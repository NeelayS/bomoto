Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=149, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8344-8399
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502361
Iteration 2/25 | Loss: 0.00135174
Iteration 3/25 | Loss: 0.00125720
Iteration 4/25 | Loss: 0.00121486
Iteration 5/25 | Loss: 0.00121032
Iteration 6/25 | Loss: 0.00120920
Iteration 7/25 | Loss: 0.00120900
Iteration 8/25 | Loss: 0.00120900
Iteration 9/25 | Loss: 0.00120900
Iteration 10/25 | Loss: 0.00120900
Iteration 11/25 | Loss: 0.00120900
Iteration 12/25 | Loss: 0.00120900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012090038508176804, 0.0012090038508176804, 0.0012090038508176804, 0.0012090038508176804, 0.0012090038508176804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012090038508176804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.90857410
Iteration 2/25 | Loss: 0.00099469
Iteration 3/25 | Loss: 0.00099466
Iteration 4/25 | Loss: 0.00099465
Iteration 5/25 | Loss: 0.00099465
Iteration 6/25 | Loss: 0.00099465
Iteration 7/25 | Loss: 0.00099465
Iteration 8/25 | Loss: 0.00099465
Iteration 9/25 | Loss: 0.00099465
Iteration 10/25 | Loss: 0.00099465
Iteration 11/25 | Loss: 0.00099465
Iteration 12/25 | Loss: 0.00099465
Iteration 13/25 | Loss: 0.00099465
Iteration 14/25 | Loss: 0.00099465
Iteration 15/25 | Loss: 0.00099465
Iteration 16/25 | Loss: 0.00099465
Iteration 17/25 | Loss: 0.00099465
Iteration 18/25 | Loss: 0.00099465
Iteration 19/25 | Loss: 0.00099465
Iteration 20/25 | Loss: 0.00099465
Iteration 21/25 | Loss: 0.00099465
Iteration 22/25 | Loss: 0.00099465
Iteration 23/25 | Loss: 0.00099465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000994651229120791, 0.000994651229120791, 0.000994651229120791, 0.000994651229120791, 0.000994651229120791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000994651229120791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099465
Iteration 2/1000 | Loss: 0.00002402
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001285
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001275
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001273
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001272
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001272
Iteration 30/1000 | Loss: 0.00001271
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001257
Iteration 48/1000 | Loss: 0.00001257
Iteration 49/1000 | Loss: 0.00001256
Iteration 50/1000 | Loss: 0.00001256
Iteration 51/1000 | Loss: 0.00001256
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001253
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001251
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001249
Iteration 75/1000 | Loss: 0.00001249
Iteration 76/1000 | Loss: 0.00001249
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001249
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001245
Iteration 100/1000 | Loss: 0.00001245
Iteration 101/1000 | Loss: 0.00001245
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001244
Iteration 105/1000 | Loss: 0.00001244
Iteration 106/1000 | Loss: 0.00001243
Iteration 107/1000 | Loss: 0.00001243
Iteration 108/1000 | Loss: 0.00001243
Iteration 109/1000 | Loss: 0.00001243
Iteration 110/1000 | Loss: 0.00001243
Iteration 111/1000 | Loss: 0.00001242
Iteration 112/1000 | Loss: 0.00001242
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001242
Iteration 115/1000 | Loss: 0.00001242
Iteration 116/1000 | Loss: 0.00001242
Iteration 117/1000 | Loss: 0.00001242
Iteration 118/1000 | Loss: 0.00001242
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001240
Iteration 130/1000 | Loss: 0.00001240
Iteration 131/1000 | Loss: 0.00001240
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001238
Iteration 143/1000 | Loss: 0.00001238
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001237
Iteration 147/1000 | Loss: 0.00001237
Iteration 148/1000 | Loss: 0.00001237
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001236
Iteration 153/1000 | Loss: 0.00001236
Iteration 154/1000 | Loss: 0.00001236
Iteration 155/1000 | Loss: 0.00001236
Iteration 156/1000 | Loss: 0.00001236
Iteration 157/1000 | Loss: 0.00001236
Iteration 158/1000 | Loss: 0.00001236
Iteration 159/1000 | Loss: 0.00001236
Iteration 160/1000 | Loss: 0.00001236
Iteration 161/1000 | Loss: 0.00001236
Iteration 162/1000 | Loss: 0.00001236
Iteration 163/1000 | Loss: 0.00001236
Iteration 164/1000 | Loss: 0.00001236
Iteration 165/1000 | Loss: 0.00001236
Iteration 166/1000 | Loss: 0.00001236
Iteration 167/1000 | Loss: 0.00001236
Iteration 168/1000 | Loss: 0.00001236
Iteration 169/1000 | Loss: 0.00001236
Iteration 170/1000 | Loss: 0.00001236
Iteration 171/1000 | Loss: 0.00001236
Iteration 172/1000 | Loss: 0.00001236
Iteration 173/1000 | Loss: 0.00001236
Iteration 174/1000 | Loss: 0.00001236
Iteration 175/1000 | Loss: 0.00001236
Iteration 176/1000 | Loss: 0.00001236
Iteration 177/1000 | Loss: 0.00001236
Iteration 178/1000 | Loss: 0.00001236
Iteration 179/1000 | Loss: 0.00001236
Iteration 180/1000 | Loss: 0.00001236
Iteration 181/1000 | Loss: 0.00001236
Iteration 182/1000 | Loss: 0.00001236
Iteration 183/1000 | Loss: 0.00001236
Iteration 184/1000 | Loss: 0.00001236
Iteration 185/1000 | Loss: 0.00001236
Iteration 186/1000 | Loss: 0.00001236
Iteration 187/1000 | Loss: 0.00001236
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.2357389095996041e-05, 1.2357389095996041e-05, 1.2357389095996041e-05, 1.2357389095996041e-05, 1.2357389095996041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2357389095996041e-05

Optimization complete. Final v2v error: 2.967006206512451 mm

Highest mean error: 3.3912723064422607 mm for frame 199

Lowest mean error: 2.645371675491333 mm for frame 151

Saving results

Total time: 47.87466788291931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493471
Iteration 2/25 | Loss: 0.00130203
Iteration 3/25 | Loss: 0.00121448
Iteration 4/25 | Loss: 0.00120172
Iteration 5/25 | Loss: 0.00119804
Iteration 6/25 | Loss: 0.00119795
Iteration 7/25 | Loss: 0.00119795
Iteration 8/25 | Loss: 0.00119795
Iteration 9/25 | Loss: 0.00119795
Iteration 10/25 | Loss: 0.00119795
Iteration 11/25 | Loss: 0.00119795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011979456758126616, 0.0011979456758126616, 0.0011979456758126616, 0.0011979456758126616, 0.0011979456758126616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011979456758126616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24087858
Iteration 2/25 | Loss: 0.00097184
Iteration 3/25 | Loss: 0.00097182
Iteration 4/25 | Loss: 0.00097182
Iteration 5/25 | Loss: 0.00097182
Iteration 6/25 | Loss: 0.00097182
Iteration 7/25 | Loss: 0.00097182
Iteration 8/25 | Loss: 0.00097182
Iteration 9/25 | Loss: 0.00097182
Iteration 10/25 | Loss: 0.00097182
Iteration 11/25 | Loss: 0.00097182
Iteration 12/25 | Loss: 0.00097182
Iteration 13/25 | Loss: 0.00097182
Iteration 14/25 | Loss: 0.00097182
Iteration 15/25 | Loss: 0.00097182
Iteration 16/25 | Loss: 0.00097182
Iteration 17/25 | Loss: 0.00097182
Iteration 18/25 | Loss: 0.00097182
Iteration 19/25 | Loss: 0.00097182
Iteration 20/25 | Loss: 0.00097182
Iteration 21/25 | Loss: 0.00097182
Iteration 22/25 | Loss: 0.00097182
Iteration 23/25 | Loss: 0.00097182
Iteration 24/25 | Loss: 0.00097182
Iteration 25/25 | Loss: 0.00097182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097182
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001223
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001188
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001167
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001166
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001165
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001144
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001141
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001140
Iteration 79/1000 | Loss: 0.00001140
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001137
Iteration 85/1000 | Loss: 0.00001137
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001131
Iteration 137/1000 | Loss: 0.00001131
Iteration 138/1000 | Loss: 0.00001131
Iteration 139/1000 | Loss: 0.00001131
Iteration 140/1000 | Loss: 0.00001131
Iteration 141/1000 | Loss: 0.00001131
Iteration 142/1000 | Loss: 0.00001131
Iteration 143/1000 | Loss: 0.00001131
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Iteration 149/1000 | Loss: 0.00001130
Iteration 150/1000 | Loss: 0.00001130
Iteration 151/1000 | Loss: 0.00001130
Iteration 152/1000 | Loss: 0.00001130
Iteration 153/1000 | Loss: 0.00001130
Iteration 154/1000 | Loss: 0.00001130
Iteration 155/1000 | Loss: 0.00001130
Iteration 156/1000 | Loss: 0.00001130
Iteration 157/1000 | Loss: 0.00001130
Iteration 158/1000 | Loss: 0.00001130
Iteration 159/1000 | Loss: 0.00001129
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Iteration 163/1000 | Loss: 0.00001129
Iteration 164/1000 | Loss: 0.00001129
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001127
Iteration 168/1000 | Loss: 0.00001127
Iteration 169/1000 | Loss: 0.00001127
Iteration 170/1000 | Loss: 0.00001127
Iteration 171/1000 | Loss: 0.00001127
Iteration 172/1000 | Loss: 0.00001127
Iteration 173/1000 | Loss: 0.00001127
Iteration 174/1000 | Loss: 0.00001127
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001127
Iteration 177/1000 | Loss: 0.00001127
Iteration 178/1000 | Loss: 0.00001127
Iteration 179/1000 | Loss: 0.00001126
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Iteration 186/1000 | Loss: 0.00001126
Iteration 187/1000 | Loss: 0.00001126
Iteration 188/1000 | Loss: 0.00001126
Iteration 189/1000 | Loss: 0.00001126
Iteration 190/1000 | Loss: 0.00001126
Iteration 191/1000 | Loss: 0.00001126
Iteration 192/1000 | Loss: 0.00001126
Iteration 193/1000 | Loss: 0.00001126
Iteration 194/1000 | Loss: 0.00001126
Iteration 195/1000 | Loss: 0.00001126
Iteration 196/1000 | Loss: 0.00001126
Iteration 197/1000 | Loss: 0.00001126
Iteration 198/1000 | Loss: 0.00001126
Iteration 199/1000 | Loss: 0.00001126
Iteration 200/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.1260023711656686e-05, 1.1260023711656686e-05, 1.1260023711656686e-05, 1.1260023711656686e-05, 1.1260023711656686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1260023711656686e-05

Optimization complete. Final v2v error: 2.9106974601745605 mm

Highest mean error: 3.2477965354919434 mm for frame 215

Lowest mean error: 2.7178773880004883 mm for frame 246

Saving results

Total time: 49.680994272232056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022103
Iteration 2/25 | Loss: 0.01022103
Iteration 3/25 | Loss: 0.00277753
Iteration 4/25 | Loss: 0.00183284
Iteration 5/25 | Loss: 0.00157374
Iteration 6/25 | Loss: 0.00147808
Iteration 7/25 | Loss: 0.00153529
Iteration 8/25 | Loss: 0.00147006
Iteration 9/25 | Loss: 0.00137801
Iteration 10/25 | Loss: 0.00134217
Iteration 11/25 | Loss: 0.00130848
Iteration 12/25 | Loss: 0.00129464
Iteration 13/25 | Loss: 0.00129896
Iteration 14/25 | Loss: 0.00129544
Iteration 15/25 | Loss: 0.00129330
Iteration 16/25 | Loss: 0.00128685
Iteration 17/25 | Loss: 0.00128682
Iteration 18/25 | Loss: 0.00128497
Iteration 19/25 | Loss: 0.00128054
Iteration 20/25 | Loss: 0.00128735
Iteration 21/25 | Loss: 0.00127381
Iteration 22/25 | Loss: 0.00127382
Iteration 23/25 | Loss: 0.00128022
Iteration 24/25 | Loss: 0.00127040
Iteration 25/25 | Loss: 0.00126828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38716888
Iteration 2/25 | Loss: 0.00166905
Iteration 3/25 | Loss: 0.00136234
Iteration 4/25 | Loss: 0.00136193
Iteration 5/25 | Loss: 0.00135519
Iteration 6/25 | Loss: 0.00135519
Iteration 7/25 | Loss: 0.00135519
Iteration 8/25 | Loss: 0.00135519
Iteration 9/25 | Loss: 0.00135518
Iteration 10/25 | Loss: 0.00135518
Iteration 11/25 | Loss: 0.00135518
Iteration 12/25 | Loss: 0.00135518
Iteration 13/25 | Loss: 0.00135518
Iteration 14/25 | Loss: 0.00135518
Iteration 15/25 | Loss: 0.00135518
Iteration 16/25 | Loss: 0.00135518
Iteration 17/25 | Loss: 0.00135518
Iteration 18/25 | Loss: 0.00135518
Iteration 19/25 | Loss: 0.00135518
Iteration 20/25 | Loss: 0.00135518
Iteration 21/25 | Loss: 0.00135518
Iteration 22/25 | Loss: 0.00135518
Iteration 23/25 | Loss: 0.00135518
Iteration 24/25 | Loss: 0.00135518
Iteration 25/25 | Loss: 0.00135518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013551829615607858, 0.0013551829615607858, 0.0013551829615607858, 0.0013551829615607858, 0.0013551829615607858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013551829615607858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135518
Iteration 2/1000 | Loss: 0.00049389
Iteration 3/1000 | Loss: 0.00050095
Iteration 4/1000 | Loss: 0.00008883
Iteration 5/1000 | Loss: 0.00033333
Iteration 6/1000 | Loss: 0.00012491
Iteration 7/1000 | Loss: 0.00015081
Iteration 8/1000 | Loss: 0.00017858
Iteration 9/1000 | Loss: 0.00021014
Iteration 10/1000 | Loss: 0.00028176
Iteration 11/1000 | Loss: 0.00036617
Iteration 12/1000 | Loss: 0.00050235
Iteration 13/1000 | Loss: 0.00021604
Iteration 14/1000 | Loss: 0.00005499
Iteration 15/1000 | Loss: 0.00004391
Iteration 16/1000 | Loss: 0.00010638
Iteration 17/1000 | Loss: 0.00005232
Iteration 18/1000 | Loss: 0.00005653
Iteration 19/1000 | Loss: 0.00004006
Iteration 20/1000 | Loss: 0.00007630
Iteration 21/1000 | Loss: 0.00005775
Iteration 22/1000 | Loss: 0.00012976
Iteration 23/1000 | Loss: 0.00009706
Iteration 24/1000 | Loss: 0.00003895
Iteration 25/1000 | Loss: 0.00008808
Iteration 26/1000 | Loss: 0.00100206
Iteration 27/1000 | Loss: 0.00092449
Iteration 28/1000 | Loss: 0.00062895
Iteration 29/1000 | Loss: 0.00070334
Iteration 30/1000 | Loss: 0.00029532
Iteration 31/1000 | Loss: 0.00018743
Iteration 32/1000 | Loss: 0.00021128
Iteration 33/1000 | Loss: 0.00046043
Iteration 34/1000 | Loss: 0.00023858
Iteration 35/1000 | Loss: 0.00010205
Iteration 36/1000 | Loss: 0.00018401
Iteration 37/1000 | Loss: 0.00016372
Iteration 38/1000 | Loss: 0.00036292
Iteration 39/1000 | Loss: 0.00037039
Iteration 40/1000 | Loss: 0.00012850
Iteration 41/1000 | Loss: 0.00006385
Iteration 42/1000 | Loss: 0.00013627
Iteration 43/1000 | Loss: 0.00010970
Iteration 44/1000 | Loss: 0.00021739
Iteration 45/1000 | Loss: 0.00013442
Iteration 46/1000 | Loss: 0.00007651
Iteration 47/1000 | Loss: 0.00002827
Iteration 48/1000 | Loss: 0.00006928
Iteration 49/1000 | Loss: 0.00016397
Iteration 50/1000 | Loss: 0.00003574
Iteration 51/1000 | Loss: 0.00009438
Iteration 52/1000 | Loss: 0.00009882
Iteration 53/1000 | Loss: 0.00010615
Iteration 54/1000 | Loss: 0.00007832
Iteration 55/1000 | Loss: 0.00005530
Iteration 56/1000 | Loss: 0.00004398
Iteration 57/1000 | Loss: 0.00003845
Iteration 58/1000 | Loss: 0.00002302
Iteration 59/1000 | Loss: 0.00005182
Iteration 60/1000 | Loss: 0.00011363
Iteration 61/1000 | Loss: 0.00028403
Iteration 62/1000 | Loss: 0.00008443
Iteration 63/1000 | Loss: 0.00010603
Iteration 64/1000 | Loss: 0.00011646
Iteration 65/1000 | Loss: 0.00010673
Iteration 66/1000 | Loss: 0.00007372
Iteration 67/1000 | Loss: 0.00012671
Iteration 68/1000 | Loss: 0.00012474
Iteration 69/1000 | Loss: 0.00013042
Iteration 70/1000 | Loss: 0.00015663
Iteration 71/1000 | Loss: 0.00016054
Iteration 72/1000 | Loss: 0.00013040
Iteration 73/1000 | Loss: 0.00070731
Iteration 74/1000 | Loss: 0.00013841
Iteration 75/1000 | Loss: 0.00002762
Iteration 76/1000 | Loss: 0.00010537
Iteration 77/1000 | Loss: 0.00025219
Iteration 78/1000 | Loss: 0.00014315
Iteration 79/1000 | Loss: 0.00008360
Iteration 80/1000 | Loss: 0.00055611
Iteration 81/1000 | Loss: 0.00042363
Iteration 82/1000 | Loss: 0.00050965
Iteration 83/1000 | Loss: 0.00082566
Iteration 84/1000 | Loss: 0.00054220
Iteration 85/1000 | Loss: 0.00025299
Iteration 86/1000 | Loss: 0.00019243
Iteration 87/1000 | Loss: 0.00003387
Iteration 88/1000 | Loss: 0.00071670
Iteration 89/1000 | Loss: 0.00049226
Iteration 90/1000 | Loss: 0.00015745
Iteration 91/1000 | Loss: 0.00008252
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001812
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00010279
Iteration 97/1000 | Loss: 0.00001676
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00019513
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00004325
Iteration 104/1000 | Loss: 0.00001996
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00002784
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001614
Iteration 110/1000 | Loss: 0.00001614
Iteration 111/1000 | Loss: 0.00001614
Iteration 112/1000 | Loss: 0.00001614
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00002845
Iteration 121/1000 | Loss: 0.00001606
Iteration 122/1000 | Loss: 0.00001605
Iteration 123/1000 | Loss: 0.00001605
Iteration 124/1000 | Loss: 0.00001605
Iteration 125/1000 | Loss: 0.00001605
Iteration 126/1000 | Loss: 0.00001605
Iteration 127/1000 | Loss: 0.00001605
Iteration 128/1000 | Loss: 0.00001605
Iteration 129/1000 | Loss: 0.00001604
Iteration 130/1000 | Loss: 0.00001604
Iteration 131/1000 | Loss: 0.00001604
Iteration 132/1000 | Loss: 0.00001604
Iteration 133/1000 | Loss: 0.00001604
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001604
Iteration 138/1000 | Loss: 0.00001604
Iteration 139/1000 | Loss: 0.00001604
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Iteration 144/1000 | Loss: 0.00001603
Iteration 145/1000 | Loss: 0.00001603
Iteration 146/1000 | Loss: 0.00001602
Iteration 147/1000 | Loss: 0.00001602
Iteration 148/1000 | Loss: 0.00001602
Iteration 149/1000 | Loss: 0.00001602
Iteration 150/1000 | Loss: 0.00003662
Iteration 151/1000 | Loss: 0.00001604
Iteration 152/1000 | Loss: 0.00001604
Iteration 153/1000 | Loss: 0.00001604
Iteration 154/1000 | Loss: 0.00001604
Iteration 155/1000 | Loss: 0.00001604
Iteration 156/1000 | Loss: 0.00001604
Iteration 157/1000 | Loss: 0.00001604
Iteration 158/1000 | Loss: 0.00001604
Iteration 159/1000 | Loss: 0.00001604
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001603
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00005784
Iteration 164/1000 | Loss: 0.00009641
Iteration 165/1000 | Loss: 0.00006861
Iteration 166/1000 | Loss: 0.00001604
Iteration 167/1000 | Loss: 0.00001603
Iteration 168/1000 | Loss: 0.00001603
Iteration 169/1000 | Loss: 0.00001603
Iteration 170/1000 | Loss: 0.00001602
Iteration 171/1000 | Loss: 0.00001602
Iteration 172/1000 | Loss: 0.00001602
Iteration 173/1000 | Loss: 0.00001602
Iteration 174/1000 | Loss: 0.00001602
Iteration 175/1000 | Loss: 0.00001601
Iteration 176/1000 | Loss: 0.00001601
Iteration 177/1000 | Loss: 0.00001601
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001600
Iteration 181/1000 | Loss: 0.00001600
Iteration 182/1000 | Loss: 0.00001600
Iteration 183/1000 | Loss: 0.00001600
Iteration 184/1000 | Loss: 0.00001600
Iteration 185/1000 | Loss: 0.00001600
Iteration 186/1000 | Loss: 0.00001599
Iteration 187/1000 | Loss: 0.00001599
Iteration 188/1000 | Loss: 0.00001599
Iteration 189/1000 | Loss: 0.00001599
Iteration 190/1000 | Loss: 0.00001599
Iteration 191/1000 | Loss: 0.00001599
Iteration 192/1000 | Loss: 0.00001599
Iteration 193/1000 | Loss: 0.00001599
Iteration 194/1000 | Loss: 0.00001599
Iteration 195/1000 | Loss: 0.00001598
Iteration 196/1000 | Loss: 0.00001598
Iteration 197/1000 | Loss: 0.00001598
Iteration 198/1000 | Loss: 0.00001598
Iteration 199/1000 | Loss: 0.00001597
Iteration 200/1000 | Loss: 0.00001597
Iteration 201/1000 | Loss: 0.00001597
Iteration 202/1000 | Loss: 0.00001597
Iteration 203/1000 | Loss: 0.00001597
Iteration 204/1000 | Loss: 0.00001597
Iteration 205/1000 | Loss: 0.00001597
Iteration 206/1000 | Loss: 0.00001597
Iteration 207/1000 | Loss: 0.00001596
Iteration 208/1000 | Loss: 0.00001596
Iteration 209/1000 | Loss: 0.00001596
Iteration 210/1000 | Loss: 0.00001596
Iteration 211/1000 | Loss: 0.00001596
Iteration 212/1000 | Loss: 0.00001596
Iteration 213/1000 | Loss: 0.00001596
Iteration 214/1000 | Loss: 0.00001596
Iteration 215/1000 | Loss: 0.00001596
Iteration 216/1000 | Loss: 0.00001596
Iteration 217/1000 | Loss: 0.00001596
Iteration 218/1000 | Loss: 0.00001596
Iteration 219/1000 | Loss: 0.00001595
Iteration 220/1000 | Loss: 0.00001595
Iteration 221/1000 | Loss: 0.00001595
Iteration 222/1000 | Loss: 0.00001595
Iteration 223/1000 | Loss: 0.00001595
Iteration 224/1000 | Loss: 0.00001595
Iteration 225/1000 | Loss: 0.00001595
Iteration 226/1000 | Loss: 0.00001595
Iteration 227/1000 | Loss: 0.00001595
Iteration 228/1000 | Loss: 0.00001595
Iteration 229/1000 | Loss: 0.00003699
Iteration 230/1000 | Loss: 0.00001598
Iteration 231/1000 | Loss: 0.00001596
Iteration 232/1000 | Loss: 0.00001595
Iteration 233/1000 | Loss: 0.00001595
Iteration 234/1000 | Loss: 0.00001594
Iteration 235/1000 | Loss: 0.00001594
Iteration 236/1000 | Loss: 0.00001594
Iteration 237/1000 | Loss: 0.00001593
Iteration 238/1000 | Loss: 0.00001593
Iteration 239/1000 | Loss: 0.00001592
Iteration 240/1000 | Loss: 0.00001592
Iteration 241/1000 | Loss: 0.00001592
Iteration 242/1000 | Loss: 0.00001591
Iteration 243/1000 | Loss: 0.00001591
Iteration 244/1000 | Loss: 0.00001591
Iteration 245/1000 | Loss: 0.00001591
Iteration 246/1000 | Loss: 0.00001591
Iteration 247/1000 | Loss: 0.00001591
Iteration 248/1000 | Loss: 0.00001591
Iteration 249/1000 | Loss: 0.00001591
Iteration 250/1000 | Loss: 0.00001591
Iteration 251/1000 | Loss: 0.00001591
Iteration 252/1000 | Loss: 0.00001591
Iteration 253/1000 | Loss: 0.00001591
Iteration 254/1000 | Loss: 0.00001591
Iteration 255/1000 | Loss: 0.00001591
Iteration 256/1000 | Loss: 0.00001591
Iteration 257/1000 | Loss: 0.00001591
Iteration 258/1000 | Loss: 0.00001591
Iteration 259/1000 | Loss: 0.00001591
Iteration 260/1000 | Loss: 0.00001591
Iteration 261/1000 | Loss: 0.00001591
Iteration 262/1000 | Loss: 0.00001591
Iteration 263/1000 | Loss: 0.00001591
Iteration 264/1000 | Loss: 0.00001591
Iteration 265/1000 | Loss: 0.00001591
Iteration 266/1000 | Loss: 0.00001591
Iteration 267/1000 | Loss: 0.00001591
Iteration 268/1000 | Loss: 0.00001591
Iteration 269/1000 | Loss: 0.00001591
Iteration 270/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.590683496033307e-05, 1.590683496033307e-05, 1.590683496033307e-05, 1.590683496033307e-05, 1.590683496033307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.590683496033307e-05

Optimization complete. Final v2v error: 3.140420913696289 mm

Highest mean error: 10.663269996643066 mm for frame 75

Lowest mean error: 2.7938036918640137 mm for frame 44

Saving results

Total time: 240.12953281402588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418623
Iteration 2/25 | Loss: 0.00128171
Iteration 3/25 | Loss: 0.00122262
Iteration 4/25 | Loss: 0.00120920
Iteration 5/25 | Loss: 0.00120616
Iteration 6/25 | Loss: 0.00120568
Iteration 7/25 | Loss: 0.00120568
Iteration 8/25 | Loss: 0.00120568
Iteration 9/25 | Loss: 0.00120568
Iteration 10/25 | Loss: 0.00120568
Iteration 11/25 | Loss: 0.00120568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012056776322424412, 0.0012056776322424412, 0.0012056776322424412, 0.0012056776322424412, 0.0012056776322424412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012056776322424412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41666245
Iteration 2/25 | Loss: 0.00096820
Iteration 3/25 | Loss: 0.00096820
Iteration 4/25 | Loss: 0.00096820
Iteration 5/25 | Loss: 0.00096820
Iteration 6/25 | Loss: 0.00096820
Iteration 7/25 | Loss: 0.00096820
Iteration 8/25 | Loss: 0.00096820
Iteration 9/25 | Loss: 0.00096820
Iteration 10/25 | Loss: 0.00096820
Iteration 11/25 | Loss: 0.00096820
Iteration 12/25 | Loss: 0.00096820
Iteration 13/25 | Loss: 0.00096819
Iteration 14/25 | Loss: 0.00096820
Iteration 15/25 | Loss: 0.00096819
Iteration 16/25 | Loss: 0.00096819
Iteration 17/25 | Loss: 0.00096819
Iteration 18/25 | Loss: 0.00096819
Iteration 19/25 | Loss: 0.00096819
Iteration 20/25 | Loss: 0.00096819
Iteration 21/25 | Loss: 0.00096819
Iteration 22/25 | Loss: 0.00096819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009681947994977236, 0.0009681947994977236, 0.0009681947994977236, 0.0009681947994977236, 0.0009681947994977236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009681947994977236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096819
Iteration 2/1000 | Loss: 0.00002647
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001572
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001468
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001406
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001361
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001332
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001321
Iteration 21/1000 | Loss: 0.00001320
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001297
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001296
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001295
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001294
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001294
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00001281
Iteration 109/1000 | Loss: 0.00001281
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2785384569724556e-05, 1.2785384569724556e-05, 1.2785384569724556e-05, 1.2785384569724556e-05, 1.2785384569724556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2785384569724556e-05

Optimization complete. Final v2v error: 3.079014539718628 mm

Highest mean error: 3.3816545009613037 mm for frame 114

Lowest mean error: 2.9844393730163574 mm for frame 139

Saving results

Total time: 38.08672642707825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741636
Iteration 2/25 | Loss: 0.00133789
Iteration 3/25 | Loss: 0.00118410
Iteration 4/25 | Loss: 0.00116885
Iteration 5/25 | Loss: 0.00116542
Iteration 6/25 | Loss: 0.00116509
Iteration 7/25 | Loss: 0.00116509
Iteration 8/25 | Loss: 0.00116509
Iteration 9/25 | Loss: 0.00116509
Iteration 10/25 | Loss: 0.00116509
Iteration 11/25 | Loss: 0.00116509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011650898959487677, 0.0011650898959487677, 0.0011650898959487677, 0.0011650898959487677, 0.0011650898959487677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011650898959487677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31241763
Iteration 2/25 | Loss: 0.00077540
Iteration 3/25 | Loss: 0.00077536
Iteration 4/25 | Loss: 0.00077536
Iteration 5/25 | Loss: 0.00077536
Iteration 6/25 | Loss: 0.00077536
Iteration 7/25 | Loss: 0.00077536
Iteration 8/25 | Loss: 0.00077536
Iteration 9/25 | Loss: 0.00077536
Iteration 10/25 | Loss: 0.00077536
Iteration 11/25 | Loss: 0.00077536
Iteration 12/25 | Loss: 0.00077536
Iteration 13/25 | Loss: 0.00077536
Iteration 14/25 | Loss: 0.00077536
Iteration 15/25 | Loss: 0.00077536
Iteration 16/25 | Loss: 0.00077536
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007753605605103076, 0.0007753605605103076, 0.0007753605605103076, 0.0007753605605103076, 0.0007753605605103076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007753605605103076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077536
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00002418
Iteration 4/1000 | Loss: 0.00002036
Iteration 5/1000 | Loss: 0.00001875
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001501
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001336
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001327
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001321
Iteration 37/1000 | Loss: 0.00001321
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001320
Iteration 40/1000 | Loss: 0.00001320
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001319
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001316
Iteration 48/1000 | Loss: 0.00001316
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001314
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001313
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001296
Iteration 94/1000 | Loss: 0.00001296
Iteration 95/1000 | Loss: 0.00001296
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001290
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001290
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001290
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001289
Iteration 153/1000 | Loss: 0.00001289
Iteration 154/1000 | Loss: 0.00001289
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001289
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001289
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001289
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001289
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2885268006357364e-05, 1.2885268006357364e-05, 1.2885268006357364e-05, 1.2885268006357364e-05, 1.2885268006357364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2885268006357364e-05

Optimization complete. Final v2v error: 3.0807979106903076 mm

Highest mean error: 3.5027639865875244 mm for frame 193

Lowest mean error: 2.763025999069214 mm for frame 65

Saving results

Total time: 49.13217854499817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972795
Iteration 2/25 | Loss: 0.00178556
Iteration 3/25 | Loss: 0.00147441
Iteration 4/25 | Loss: 0.00145270
Iteration 5/25 | Loss: 0.00144560
Iteration 6/25 | Loss: 0.00144469
Iteration 7/25 | Loss: 0.00144469
Iteration 8/25 | Loss: 0.00144469
Iteration 9/25 | Loss: 0.00144469
Iteration 10/25 | Loss: 0.00144469
Iteration 11/25 | Loss: 0.00144469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014446937711909413, 0.0014446937711909413, 0.0014446937711909413, 0.0014446937711909413, 0.0014446937711909413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014446937711909413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50807232
Iteration 2/25 | Loss: 0.00118320
Iteration 3/25 | Loss: 0.00118320
Iteration 4/25 | Loss: 0.00118320
Iteration 5/25 | Loss: 0.00118320
Iteration 6/25 | Loss: 0.00118320
Iteration 7/25 | Loss: 0.00118320
Iteration 8/25 | Loss: 0.00118320
Iteration 9/25 | Loss: 0.00118320
Iteration 10/25 | Loss: 0.00118320
Iteration 11/25 | Loss: 0.00118320
Iteration 12/25 | Loss: 0.00118320
Iteration 13/25 | Loss: 0.00118320
Iteration 14/25 | Loss: 0.00118320
Iteration 15/25 | Loss: 0.00118320
Iteration 16/25 | Loss: 0.00118320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011831974843516946, 0.0011831974843516946, 0.0011831974843516946, 0.0011831974843516946, 0.0011831974843516946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011831974843516946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118320
Iteration 2/1000 | Loss: 0.00006738
Iteration 3/1000 | Loss: 0.00004700
Iteration 4/1000 | Loss: 0.00004217
Iteration 5/1000 | Loss: 0.00004043
Iteration 6/1000 | Loss: 0.00003942
Iteration 7/1000 | Loss: 0.00003847
Iteration 8/1000 | Loss: 0.00003792
Iteration 9/1000 | Loss: 0.00003751
Iteration 10/1000 | Loss: 0.00003705
Iteration 11/1000 | Loss: 0.00003666
Iteration 12/1000 | Loss: 0.00003633
Iteration 13/1000 | Loss: 0.00003598
Iteration 14/1000 | Loss: 0.00003569
Iteration 15/1000 | Loss: 0.00003544
Iteration 16/1000 | Loss: 0.00003512
Iteration 17/1000 | Loss: 0.00003487
Iteration 18/1000 | Loss: 0.00003469
Iteration 19/1000 | Loss: 0.00003464
Iteration 20/1000 | Loss: 0.00003451
Iteration 21/1000 | Loss: 0.00003441
Iteration 22/1000 | Loss: 0.00003427
Iteration 23/1000 | Loss: 0.00003427
Iteration 24/1000 | Loss: 0.00003427
Iteration 25/1000 | Loss: 0.00003426
Iteration 26/1000 | Loss: 0.00003425
Iteration 27/1000 | Loss: 0.00003421
Iteration 28/1000 | Loss: 0.00003411
Iteration 29/1000 | Loss: 0.00003409
Iteration 30/1000 | Loss: 0.00003408
Iteration 31/1000 | Loss: 0.00003408
Iteration 32/1000 | Loss: 0.00003408
Iteration 33/1000 | Loss: 0.00003408
Iteration 34/1000 | Loss: 0.00003408
Iteration 35/1000 | Loss: 0.00003408
Iteration 36/1000 | Loss: 0.00003408
Iteration 37/1000 | Loss: 0.00003408
Iteration 38/1000 | Loss: 0.00003408
Iteration 39/1000 | Loss: 0.00003408
Iteration 40/1000 | Loss: 0.00003407
Iteration 41/1000 | Loss: 0.00003407
Iteration 42/1000 | Loss: 0.00003407
Iteration 43/1000 | Loss: 0.00003406
Iteration 44/1000 | Loss: 0.00003406
Iteration 45/1000 | Loss: 0.00003406
Iteration 46/1000 | Loss: 0.00003405
Iteration 47/1000 | Loss: 0.00003405
Iteration 48/1000 | Loss: 0.00003405
Iteration 49/1000 | Loss: 0.00003404
Iteration 50/1000 | Loss: 0.00003404
Iteration 51/1000 | Loss: 0.00003404
Iteration 52/1000 | Loss: 0.00003403
Iteration 53/1000 | Loss: 0.00003403
Iteration 54/1000 | Loss: 0.00003403
Iteration 55/1000 | Loss: 0.00003403
Iteration 56/1000 | Loss: 0.00003402
Iteration 57/1000 | Loss: 0.00003402
Iteration 58/1000 | Loss: 0.00003401
Iteration 59/1000 | Loss: 0.00003400
Iteration 60/1000 | Loss: 0.00003400
Iteration 61/1000 | Loss: 0.00003400
Iteration 62/1000 | Loss: 0.00003399
Iteration 63/1000 | Loss: 0.00003399
Iteration 64/1000 | Loss: 0.00003399
Iteration 65/1000 | Loss: 0.00003399
Iteration 66/1000 | Loss: 0.00003399
Iteration 67/1000 | Loss: 0.00003398
Iteration 68/1000 | Loss: 0.00003398
Iteration 69/1000 | Loss: 0.00003398
Iteration 70/1000 | Loss: 0.00003398
Iteration 71/1000 | Loss: 0.00003398
Iteration 72/1000 | Loss: 0.00003398
Iteration 73/1000 | Loss: 0.00003398
Iteration 74/1000 | Loss: 0.00003398
Iteration 75/1000 | Loss: 0.00003398
Iteration 76/1000 | Loss: 0.00003398
Iteration 77/1000 | Loss: 0.00003398
Iteration 78/1000 | Loss: 0.00003398
Iteration 79/1000 | Loss: 0.00003397
Iteration 80/1000 | Loss: 0.00003397
Iteration 81/1000 | Loss: 0.00003397
Iteration 82/1000 | Loss: 0.00003397
Iteration 83/1000 | Loss: 0.00003397
Iteration 84/1000 | Loss: 0.00003396
Iteration 85/1000 | Loss: 0.00003396
Iteration 86/1000 | Loss: 0.00003396
Iteration 87/1000 | Loss: 0.00003396
Iteration 88/1000 | Loss: 0.00003396
Iteration 89/1000 | Loss: 0.00003396
Iteration 90/1000 | Loss: 0.00003396
Iteration 91/1000 | Loss: 0.00003396
Iteration 92/1000 | Loss: 0.00003396
Iteration 93/1000 | Loss: 0.00003395
Iteration 94/1000 | Loss: 0.00003395
Iteration 95/1000 | Loss: 0.00003395
Iteration 96/1000 | Loss: 0.00003395
Iteration 97/1000 | Loss: 0.00003395
Iteration 98/1000 | Loss: 0.00003395
Iteration 99/1000 | Loss: 0.00003395
Iteration 100/1000 | Loss: 0.00003395
Iteration 101/1000 | Loss: 0.00003395
Iteration 102/1000 | Loss: 0.00003395
Iteration 103/1000 | Loss: 0.00003394
Iteration 104/1000 | Loss: 0.00003394
Iteration 105/1000 | Loss: 0.00003394
Iteration 106/1000 | Loss: 0.00003394
Iteration 107/1000 | Loss: 0.00003394
Iteration 108/1000 | Loss: 0.00003394
Iteration 109/1000 | Loss: 0.00003394
Iteration 110/1000 | Loss: 0.00003393
Iteration 111/1000 | Loss: 0.00003393
Iteration 112/1000 | Loss: 0.00003393
Iteration 113/1000 | Loss: 0.00003393
Iteration 114/1000 | Loss: 0.00003393
Iteration 115/1000 | Loss: 0.00003393
Iteration 116/1000 | Loss: 0.00003393
Iteration 117/1000 | Loss: 0.00003393
Iteration 118/1000 | Loss: 0.00003393
Iteration 119/1000 | Loss: 0.00003393
Iteration 120/1000 | Loss: 0.00003393
Iteration 121/1000 | Loss: 0.00003392
Iteration 122/1000 | Loss: 0.00003392
Iteration 123/1000 | Loss: 0.00003392
Iteration 124/1000 | Loss: 0.00003392
Iteration 125/1000 | Loss: 0.00003392
Iteration 126/1000 | Loss: 0.00003392
Iteration 127/1000 | Loss: 0.00003392
Iteration 128/1000 | Loss: 0.00003392
Iteration 129/1000 | Loss: 0.00003391
Iteration 130/1000 | Loss: 0.00003391
Iteration 131/1000 | Loss: 0.00003391
Iteration 132/1000 | Loss: 0.00003391
Iteration 133/1000 | Loss: 0.00003391
Iteration 134/1000 | Loss: 0.00003391
Iteration 135/1000 | Loss: 0.00003391
Iteration 136/1000 | Loss: 0.00003391
Iteration 137/1000 | Loss: 0.00003391
Iteration 138/1000 | Loss: 0.00003391
Iteration 139/1000 | Loss: 0.00003391
Iteration 140/1000 | Loss: 0.00003391
Iteration 141/1000 | Loss: 0.00003391
Iteration 142/1000 | Loss: 0.00003391
Iteration 143/1000 | Loss: 0.00003391
Iteration 144/1000 | Loss: 0.00003391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.391153586562723e-05, 3.391153586562723e-05, 3.391153586562723e-05, 3.391153586562723e-05, 3.391153586562723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.391153586562723e-05

Optimization complete. Final v2v error: 4.873171329498291 mm

Highest mean error: 5.6322126388549805 mm for frame 12

Lowest mean error: 4.512500762939453 mm for frame 44

Saving results

Total time: 49.348084449768066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530036
Iteration 2/25 | Loss: 0.00159328
Iteration 3/25 | Loss: 0.00142087
Iteration 4/25 | Loss: 0.00140499
Iteration 5/25 | Loss: 0.00139973
Iteration 6/25 | Loss: 0.00139808
Iteration 7/25 | Loss: 0.00139808
Iteration 8/25 | Loss: 0.00139808
Iteration 9/25 | Loss: 0.00139808
Iteration 10/25 | Loss: 0.00139808
Iteration 11/25 | Loss: 0.00139808
Iteration 12/25 | Loss: 0.00139808
Iteration 13/25 | Loss: 0.00139808
Iteration 14/25 | Loss: 0.00139808
Iteration 15/25 | Loss: 0.00139808
Iteration 16/25 | Loss: 0.00139808
Iteration 17/25 | Loss: 0.00139808
Iteration 18/25 | Loss: 0.00139808
Iteration 19/25 | Loss: 0.00139808
Iteration 20/25 | Loss: 0.00139808
Iteration 21/25 | Loss: 0.00139808
Iteration 22/25 | Loss: 0.00139808
Iteration 23/25 | Loss: 0.00139808
Iteration 24/25 | Loss: 0.00139808
Iteration 25/25 | Loss: 0.00139808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59230852
Iteration 2/25 | Loss: 0.00269629
Iteration 3/25 | Loss: 0.00269627
Iteration 4/25 | Loss: 0.00269627
Iteration 5/25 | Loss: 0.00269627
Iteration 6/25 | Loss: 0.00269627
Iteration 7/25 | Loss: 0.00269627
Iteration 8/25 | Loss: 0.00269627
Iteration 9/25 | Loss: 0.00269627
Iteration 10/25 | Loss: 0.00269627
Iteration 11/25 | Loss: 0.00269627
Iteration 12/25 | Loss: 0.00269627
Iteration 13/25 | Loss: 0.00269627
Iteration 14/25 | Loss: 0.00269627
Iteration 15/25 | Loss: 0.00269627
Iteration 16/25 | Loss: 0.00269627
Iteration 17/25 | Loss: 0.00269627
Iteration 18/25 | Loss: 0.00269627
Iteration 19/25 | Loss: 0.00269627
Iteration 20/25 | Loss: 0.00269627
Iteration 21/25 | Loss: 0.00269627
Iteration 22/25 | Loss: 0.00269627
Iteration 23/25 | Loss: 0.00269627
Iteration 24/25 | Loss: 0.00269627
Iteration 25/25 | Loss: 0.00269627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269627
Iteration 2/1000 | Loss: 0.00004249
Iteration 3/1000 | Loss: 0.00003185
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002585
Iteration 8/1000 | Loss: 0.00002539
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002498
Iteration 11/1000 | Loss: 0.00002477
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002458
Iteration 14/1000 | Loss: 0.00002444
Iteration 15/1000 | Loss: 0.00002435
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002421
Iteration 18/1000 | Loss: 0.00002421
Iteration 19/1000 | Loss: 0.00002418
Iteration 20/1000 | Loss: 0.00002417
Iteration 21/1000 | Loss: 0.00002417
Iteration 22/1000 | Loss: 0.00002416
Iteration 23/1000 | Loss: 0.00002416
Iteration 24/1000 | Loss: 0.00002415
Iteration 25/1000 | Loss: 0.00002415
Iteration 26/1000 | Loss: 0.00002414
Iteration 27/1000 | Loss: 0.00002414
Iteration 28/1000 | Loss: 0.00002412
Iteration 29/1000 | Loss: 0.00002411
Iteration 30/1000 | Loss: 0.00002411
Iteration 31/1000 | Loss: 0.00002411
Iteration 32/1000 | Loss: 0.00002410
Iteration 33/1000 | Loss: 0.00002409
Iteration 34/1000 | Loss: 0.00002409
Iteration 35/1000 | Loss: 0.00002408
Iteration 36/1000 | Loss: 0.00002408
Iteration 37/1000 | Loss: 0.00002408
Iteration 38/1000 | Loss: 0.00002408
Iteration 39/1000 | Loss: 0.00002408
Iteration 40/1000 | Loss: 0.00002407
Iteration 41/1000 | Loss: 0.00002407
Iteration 42/1000 | Loss: 0.00002407
Iteration 43/1000 | Loss: 0.00002407
Iteration 44/1000 | Loss: 0.00002406
Iteration 45/1000 | Loss: 0.00002406
Iteration 46/1000 | Loss: 0.00002406
Iteration 47/1000 | Loss: 0.00002406
Iteration 48/1000 | Loss: 0.00002405
Iteration 49/1000 | Loss: 0.00002405
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00002404
Iteration 52/1000 | Loss: 0.00002404
Iteration 53/1000 | Loss: 0.00002404
Iteration 54/1000 | Loss: 0.00002404
Iteration 55/1000 | Loss: 0.00002404
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002404
Iteration 59/1000 | Loss: 0.00002404
Iteration 60/1000 | Loss: 0.00002404
Iteration 61/1000 | Loss: 0.00002404
Iteration 62/1000 | Loss: 0.00002404
Iteration 63/1000 | Loss: 0.00002404
Iteration 64/1000 | Loss: 0.00002404
Iteration 65/1000 | Loss: 0.00002404
Iteration 66/1000 | Loss: 0.00002404
Iteration 67/1000 | Loss: 0.00002404
Iteration 68/1000 | Loss: 0.00002404
Iteration 69/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.4041404685704038e-05, 2.4041404685704038e-05, 2.4041404685704038e-05, 2.4041404685704038e-05, 2.4041404685704038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4041404685704038e-05

Optimization complete. Final v2v error: 4.255772113800049 mm

Highest mean error: 4.87613582611084 mm for frame 56

Lowest mean error: 3.720961332321167 mm for frame 142

Saving results

Total time: 38.31840109825134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767819
Iteration 2/25 | Loss: 0.00159936
Iteration 3/25 | Loss: 0.00147033
Iteration 4/25 | Loss: 0.00144252
Iteration 5/25 | Loss: 0.00143009
Iteration 6/25 | Loss: 0.00142672
Iteration 7/25 | Loss: 0.00142587
Iteration 8/25 | Loss: 0.00142584
Iteration 9/25 | Loss: 0.00142584
Iteration 10/25 | Loss: 0.00142584
Iteration 11/25 | Loss: 0.00142584
Iteration 12/25 | Loss: 0.00142584
Iteration 13/25 | Loss: 0.00142584
Iteration 14/25 | Loss: 0.00142584
Iteration 15/25 | Loss: 0.00142584
Iteration 16/25 | Loss: 0.00142584
Iteration 17/25 | Loss: 0.00142584
Iteration 18/25 | Loss: 0.00142584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001425841124728322, 0.001425841124728322, 0.001425841124728322, 0.001425841124728322, 0.001425841124728322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001425841124728322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18972301
Iteration 2/25 | Loss: 0.00258213
Iteration 3/25 | Loss: 0.00258212
Iteration 4/25 | Loss: 0.00258212
Iteration 5/25 | Loss: 0.00258212
Iteration 6/25 | Loss: 0.00258212
Iteration 7/25 | Loss: 0.00258212
Iteration 8/25 | Loss: 0.00258212
Iteration 9/25 | Loss: 0.00258212
Iteration 10/25 | Loss: 0.00258212
Iteration 11/25 | Loss: 0.00258212
Iteration 12/25 | Loss: 0.00258212
Iteration 13/25 | Loss: 0.00258212
Iteration 14/25 | Loss: 0.00258212
Iteration 15/25 | Loss: 0.00258212
Iteration 16/25 | Loss: 0.00258212
Iteration 17/25 | Loss: 0.00258212
Iteration 18/25 | Loss: 0.00258212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002582120941951871, 0.002582120941951871, 0.002582120941951871, 0.002582120941951871, 0.002582120941951871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002582120941951871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258212
Iteration 2/1000 | Loss: 0.00004758
Iteration 3/1000 | Loss: 0.00003383
Iteration 4/1000 | Loss: 0.00002995
Iteration 5/1000 | Loss: 0.00002808
Iteration 6/1000 | Loss: 0.00002659
Iteration 7/1000 | Loss: 0.00002603
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002548
Iteration 10/1000 | Loss: 0.00002525
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002502
Iteration 13/1000 | Loss: 0.00002494
Iteration 14/1000 | Loss: 0.00002486
Iteration 15/1000 | Loss: 0.00002477
Iteration 16/1000 | Loss: 0.00002475
Iteration 17/1000 | Loss: 0.00002470
Iteration 18/1000 | Loss: 0.00002470
Iteration 19/1000 | Loss: 0.00002470
Iteration 20/1000 | Loss: 0.00002468
Iteration 21/1000 | Loss: 0.00002466
Iteration 22/1000 | Loss: 0.00002466
Iteration 23/1000 | Loss: 0.00002466
Iteration 24/1000 | Loss: 0.00002466
Iteration 25/1000 | Loss: 0.00002466
Iteration 26/1000 | Loss: 0.00002465
Iteration 27/1000 | Loss: 0.00002465
Iteration 28/1000 | Loss: 0.00002464
Iteration 29/1000 | Loss: 0.00002463
Iteration 30/1000 | Loss: 0.00002462
Iteration 31/1000 | Loss: 0.00002462
Iteration 32/1000 | Loss: 0.00002461
Iteration 33/1000 | Loss: 0.00002461
Iteration 34/1000 | Loss: 0.00002461
Iteration 35/1000 | Loss: 0.00002460
Iteration 36/1000 | Loss: 0.00002459
Iteration 37/1000 | Loss: 0.00002459
Iteration 38/1000 | Loss: 0.00002458
Iteration 39/1000 | Loss: 0.00002458
Iteration 40/1000 | Loss: 0.00002458
Iteration 41/1000 | Loss: 0.00002458
Iteration 42/1000 | Loss: 0.00002458
Iteration 43/1000 | Loss: 0.00002458
Iteration 44/1000 | Loss: 0.00002458
Iteration 45/1000 | Loss: 0.00002458
Iteration 46/1000 | Loss: 0.00002457
Iteration 47/1000 | Loss: 0.00002456
Iteration 48/1000 | Loss: 0.00002456
Iteration 49/1000 | Loss: 0.00002456
Iteration 50/1000 | Loss: 0.00002455
Iteration 51/1000 | Loss: 0.00002455
Iteration 52/1000 | Loss: 0.00002455
Iteration 53/1000 | Loss: 0.00002455
Iteration 54/1000 | Loss: 0.00002455
Iteration 55/1000 | Loss: 0.00002455
Iteration 56/1000 | Loss: 0.00002455
Iteration 57/1000 | Loss: 0.00002455
Iteration 58/1000 | Loss: 0.00002455
Iteration 59/1000 | Loss: 0.00002454
Iteration 60/1000 | Loss: 0.00002454
Iteration 61/1000 | Loss: 0.00002454
Iteration 62/1000 | Loss: 0.00002454
Iteration 63/1000 | Loss: 0.00002453
Iteration 64/1000 | Loss: 0.00002453
Iteration 65/1000 | Loss: 0.00002453
Iteration 66/1000 | Loss: 0.00002453
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00002453
Iteration 70/1000 | Loss: 0.00002453
Iteration 71/1000 | Loss: 0.00002453
Iteration 72/1000 | Loss: 0.00002453
Iteration 73/1000 | Loss: 0.00002453
Iteration 74/1000 | Loss: 0.00002453
Iteration 75/1000 | Loss: 0.00002453
Iteration 76/1000 | Loss: 0.00002453
Iteration 77/1000 | Loss: 0.00002453
Iteration 78/1000 | Loss: 0.00002453
Iteration 79/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.4532831957913004e-05, 2.4532831957913004e-05, 2.4532831957913004e-05, 2.4532831957913004e-05, 2.4532831957913004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4532831957913004e-05

Optimization complete. Final v2v error: 4.386981010437012 mm

Highest mean error: 4.729710578918457 mm for frame 144

Lowest mean error: 4.051894664764404 mm for frame 21

Saving results

Total time: 32.94915413856506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918130
Iteration 2/25 | Loss: 0.00153352
Iteration 3/25 | Loss: 0.00142061
Iteration 4/25 | Loss: 0.00140666
Iteration 5/25 | Loss: 0.00140192
Iteration 6/25 | Loss: 0.00140126
Iteration 7/25 | Loss: 0.00140126
Iteration 8/25 | Loss: 0.00140126
Iteration 9/25 | Loss: 0.00140126
Iteration 10/25 | Loss: 0.00140126
Iteration 11/25 | Loss: 0.00140126
Iteration 12/25 | Loss: 0.00140126
Iteration 13/25 | Loss: 0.00140126
Iteration 14/25 | Loss: 0.00140126
Iteration 15/25 | Loss: 0.00140126
Iteration 16/25 | Loss: 0.00140126
Iteration 17/25 | Loss: 0.00140126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014012614265084267, 0.0014012614265084267, 0.0014012614265084267, 0.0014012614265084267, 0.0014012614265084267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014012614265084267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61194932
Iteration 2/25 | Loss: 0.00256097
Iteration 3/25 | Loss: 0.00256096
Iteration 4/25 | Loss: 0.00256096
Iteration 5/25 | Loss: 0.00256096
Iteration 6/25 | Loss: 0.00256096
Iteration 7/25 | Loss: 0.00256096
Iteration 8/25 | Loss: 0.00256096
Iteration 9/25 | Loss: 0.00256096
Iteration 10/25 | Loss: 0.00256096
Iteration 11/25 | Loss: 0.00256096
Iteration 12/25 | Loss: 0.00256096
Iteration 13/25 | Loss: 0.00256096
Iteration 14/25 | Loss: 0.00256096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025609605945646763, 0.0025609605945646763, 0.0025609605945646763, 0.0025609605945646763, 0.0025609605945646763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025609605945646763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256096
Iteration 2/1000 | Loss: 0.00003643
Iteration 3/1000 | Loss: 0.00003049
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002732
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002581
Iteration 9/1000 | Loss: 0.00002566
Iteration 10/1000 | Loss: 0.00002550
Iteration 11/1000 | Loss: 0.00002537
Iteration 12/1000 | Loss: 0.00002537
Iteration 13/1000 | Loss: 0.00002536
Iteration 14/1000 | Loss: 0.00002536
Iteration 15/1000 | Loss: 0.00002535
Iteration 16/1000 | Loss: 0.00002535
Iteration 17/1000 | Loss: 0.00002532
Iteration 18/1000 | Loss: 0.00002532
Iteration 19/1000 | Loss: 0.00002531
Iteration 20/1000 | Loss: 0.00002531
Iteration 21/1000 | Loss: 0.00002531
Iteration 22/1000 | Loss: 0.00002530
Iteration 23/1000 | Loss: 0.00002530
Iteration 24/1000 | Loss: 0.00002529
Iteration 25/1000 | Loss: 0.00002528
Iteration 26/1000 | Loss: 0.00002528
Iteration 27/1000 | Loss: 0.00002528
Iteration 28/1000 | Loss: 0.00002528
Iteration 29/1000 | Loss: 0.00002528
Iteration 30/1000 | Loss: 0.00002528
Iteration 31/1000 | Loss: 0.00002527
Iteration 32/1000 | Loss: 0.00002527
Iteration 33/1000 | Loss: 0.00002527
Iteration 34/1000 | Loss: 0.00002525
Iteration 35/1000 | Loss: 0.00002525
Iteration 36/1000 | Loss: 0.00002525
Iteration 37/1000 | Loss: 0.00002524
Iteration 38/1000 | Loss: 0.00002524
Iteration 39/1000 | Loss: 0.00002524
Iteration 40/1000 | Loss: 0.00002523
Iteration 41/1000 | Loss: 0.00002523
Iteration 42/1000 | Loss: 0.00002522
Iteration 43/1000 | Loss: 0.00002522
Iteration 44/1000 | Loss: 0.00002522
Iteration 45/1000 | Loss: 0.00002522
Iteration 46/1000 | Loss: 0.00002522
Iteration 47/1000 | Loss: 0.00002522
Iteration 48/1000 | Loss: 0.00002521
Iteration 49/1000 | Loss: 0.00002521
Iteration 50/1000 | Loss: 0.00002521
Iteration 51/1000 | Loss: 0.00002520
Iteration 52/1000 | Loss: 0.00002520
Iteration 53/1000 | Loss: 0.00002520
Iteration 54/1000 | Loss: 0.00002520
Iteration 55/1000 | Loss: 0.00002520
Iteration 56/1000 | Loss: 0.00002519
Iteration 57/1000 | Loss: 0.00002519
Iteration 58/1000 | Loss: 0.00002519
Iteration 59/1000 | Loss: 0.00002519
Iteration 60/1000 | Loss: 0.00002519
Iteration 61/1000 | Loss: 0.00002519
Iteration 62/1000 | Loss: 0.00002519
Iteration 63/1000 | Loss: 0.00002519
Iteration 64/1000 | Loss: 0.00002519
Iteration 65/1000 | Loss: 0.00002519
Iteration 66/1000 | Loss: 0.00002518
Iteration 67/1000 | Loss: 0.00002518
Iteration 68/1000 | Loss: 0.00002518
Iteration 69/1000 | Loss: 0.00002518
Iteration 70/1000 | Loss: 0.00002518
Iteration 71/1000 | Loss: 0.00002517
Iteration 72/1000 | Loss: 0.00002517
Iteration 73/1000 | Loss: 0.00002517
Iteration 74/1000 | Loss: 0.00002517
Iteration 75/1000 | Loss: 0.00002517
Iteration 76/1000 | Loss: 0.00002517
Iteration 77/1000 | Loss: 0.00002516
Iteration 78/1000 | Loss: 0.00002516
Iteration 79/1000 | Loss: 0.00002516
Iteration 80/1000 | Loss: 0.00002516
Iteration 81/1000 | Loss: 0.00002515
Iteration 82/1000 | Loss: 0.00002515
Iteration 83/1000 | Loss: 0.00002515
Iteration 84/1000 | Loss: 0.00002515
Iteration 85/1000 | Loss: 0.00002515
Iteration 86/1000 | Loss: 0.00002515
Iteration 87/1000 | Loss: 0.00002515
Iteration 88/1000 | Loss: 0.00002515
Iteration 89/1000 | Loss: 0.00002515
Iteration 90/1000 | Loss: 0.00002515
Iteration 91/1000 | Loss: 0.00002515
Iteration 92/1000 | Loss: 0.00002515
Iteration 93/1000 | Loss: 0.00002515
Iteration 94/1000 | Loss: 0.00002515
Iteration 95/1000 | Loss: 0.00002515
Iteration 96/1000 | Loss: 0.00002515
Iteration 97/1000 | Loss: 0.00002515
Iteration 98/1000 | Loss: 0.00002515
Iteration 99/1000 | Loss: 0.00002515
Iteration 100/1000 | Loss: 0.00002515
Iteration 101/1000 | Loss: 0.00002515
Iteration 102/1000 | Loss: 0.00002515
Iteration 103/1000 | Loss: 0.00002515
Iteration 104/1000 | Loss: 0.00002515
Iteration 105/1000 | Loss: 0.00002515
Iteration 106/1000 | Loss: 0.00002515
Iteration 107/1000 | Loss: 0.00002515
Iteration 108/1000 | Loss: 0.00002515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.5147865017061122e-05, 2.5147865017061122e-05, 2.5147865017061122e-05, 2.5147865017061122e-05, 2.5147865017061122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5147865017061122e-05

Optimization complete. Final v2v error: 4.437464714050293 mm

Highest mean error: 5.048882961273193 mm for frame 161

Lowest mean error: 4.034884452819824 mm for frame 129

Saving results

Total time: 33.01704788208008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524553
Iteration 2/25 | Loss: 0.00153631
Iteration 3/25 | Loss: 0.00141054
Iteration 4/25 | Loss: 0.00139758
Iteration 5/25 | Loss: 0.00139292
Iteration 6/25 | Loss: 0.00139165
Iteration 7/25 | Loss: 0.00139165
Iteration 8/25 | Loss: 0.00139165
Iteration 9/25 | Loss: 0.00139165
Iteration 10/25 | Loss: 0.00139165
Iteration 11/25 | Loss: 0.00139165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001391650061123073, 0.001391650061123073, 0.001391650061123073, 0.001391650061123073, 0.001391650061123073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001391650061123073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59428728
Iteration 2/25 | Loss: 0.00272404
Iteration 3/25 | Loss: 0.00272402
Iteration 4/25 | Loss: 0.00272402
Iteration 5/25 | Loss: 0.00272402
Iteration 6/25 | Loss: 0.00272402
Iteration 7/25 | Loss: 0.00272402
Iteration 8/25 | Loss: 0.00272402
Iteration 9/25 | Loss: 0.00272402
Iteration 10/25 | Loss: 0.00272402
Iteration 11/25 | Loss: 0.00272402
Iteration 12/25 | Loss: 0.00272402
Iteration 13/25 | Loss: 0.00272402
Iteration 14/25 | Loss: 0.00272402
Iteration 15/25 | Loss: 0.00272402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002724019344896078, 0.002724019344896078, 0.002724019344896078, 0.002724019344896078, 0.002724019344896078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002724019344896078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272402
Iteration 2/1000 | Loss: 0.00004935
Iteration 3/1000 | Loss: 0.00003474
Iteration 4/1000 | Loss: 0.00003053
Iteration 5/1000 | Loss: 0.00002859
Iteration 6/1000 | Loss: 0.00002722
Iteration 7/1000 | Loss: 0.00002648
Iteration 8/1000 | Loss: 0.00002600
Iteration 9/1000 | Loss: 0.00002565
Iteration 10/1000 | Loss: 0.00002557
Iteration 11/1000 | Loss: 0.00002537
Iteration 12/1000 | Loss: 0.00002523
Iteration 13/1000 | Loss: 0.00002518
Iteration 14/1000 | Loss: 0.00002515
Iteration 15/1000 | Loss: 0.00002514
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002511
Iteration 18/1000 | Loss: 0.00002507
Iteration 19/1000 | Loss: 0.00002500
Iteration 20/1000 | Loss: 0.00002496
Iteration 21/1000 | Loss: 0.00002495
Iteration 22/1000 | Loss: 0.00002494
Iteration 23/1000 | Loss: 0.00002493
Iteration 24/1000 | Loss: 0.00002488
Iteration 25/1000 | Loss: 0.00002485
Iteration 26/1000 | Loss: 0.00002485
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002484
Iteration 29/1000 | Loss: 0.00002483
Iteration 30/1000 | Loss: 0.00002482
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002480
Iteration 34/1000 | Loss: 0.00002480
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002479
Iteration 37/1000 | Loss: 0.00002478
Iteration 38/1000 | Loss: 0.00002478
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002477
Iteration 42/1000 | Loss: 0.00002476
Iteration 43/1000 | Loss: 0.00002476
Iteration 44/1000 | Loss: 0.00002475
Iteration 45/1000 | Loss: 0.00002474
Iteration 46/1000 | Loss: 0.00002474
Iteration 47/1000 | Loss: 0.00002474
Iteration 48/1000 | Loss: 0.00002473
Iteration 49/1000 | Loss: 0.00002473
Iteration 50/1000 | Loss: 0.00002473
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002472
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002472
Iteration 55/1000 | Loss: 0.00002471
Iteration 56/1000 | Loss: 0.00002471
Iteration 57/1000 | Loss: 0.00002471
Iteration 58/1000 | Loss: 0.00002471
Iteration 59/1000 | Loss: 0.00002471
Iteration 60/1000 | Loss: 0.00002471
Iteration 61/1000 | Loss: 0.00002471
Iteration 62/1000 | Loss: 0.00002471
Iteration 63/1000 | Loss: 0.00002470
Iteration 64/1000 | Loss: 0.00002470
Iteration 65/1000 | Loss: 0.00002470
Iteration 66/1000 | Loss: 0.00002470
Iteration 67/1000 | Loss: 0.00002470
Iteration 68/1000 | Loss: 0.00002470
Iteration 69/1000 | Loss: 0.00002470
Iteration 70/1000 | Loss: 0.00002470
Iteration 71/1000 | Loss: 0.00002470
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [2.469698119966779e-05, 2.469698119966779e-05, 2.469698119966779e-05, 2.469698119966779e-05, 2.469698119966779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.469698119966779e-05

Optimization complete. Final v2v error: 4.326213836669922 mm

Highest mean error: 4.773538589477539 mm for frame 109

Lowest mean error: 3.899090528488159 mm for frame 156

Saving results

Total time: 36.2253212928772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924942
Iteration 2/25 | Loss: 0.00187175
Iteration 3/25 | Loss: 0.00158442
Iteration 4/25 | Loss: 0.00154902
Iteration 5/25 | Loss: 0.00153688
Iteration 6/25 | Loss: 0.00153386
Iteration 7/25 | Loss: 0.00153328
Iteration 8/25 | Loss: 0.00153328
Iteration 9/25 | Loss: 0.00153328
Iteration 10/25 | Loss: 0.00153328
Iteration 11/25 | Loss: 0.00153328
Iteration 12/25 | Loss: 0.00153328
Iteration 13/25 | Loss: 0.00153328
Iteration 14/25 | Loss: 0.00153328
Iteration 15/25 | Loss: 0.00153328
Iteration 16/25 | Loss: 0.00153328
Iteration 17/25 | Loss: 0.00153328
Iteration 18/25 | Loss: 0.00153328
Iteration 19/25 | Loss: 0.00153328
Iteration 20/25 | Loss: 0.00153328
Iteration 21/25 | Loss: 0.00153328
Iteration 22/25 | Loss: 0.00153328
Iteration 23/25 | Loss: 0.00153328
Iteration 24/25 | Loss: 0.00153328
Iteration 25/25 | Loss: 0.00153328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23012543
Iteration 2/25 | Loss: 0.00326168
Iteration 3/25 | Loss: 0.00326168
Iteration 4/25 | Loss: 0.00326168
Iteration 5/25 | Loss: 0.00326168
Iteration 6/25 | Loss: 0.00326168
Iteration 7/25 | Loss: 0.00326168
Iteration 8/25 | Loss: 0.00326168
Iteration 9/25 | Loss: 0.00326168
Iteration 10/25 | Loss: 0.00326168
Iteration 11/25 | Loss: 0.00326168
Iteration 12/25 | Loss: 0.00326168
Iteration 13/25 | Loss: 0.00326168
Iteration 14/25 | Loss: 0.00326168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003261679084971547, 0.003261679084971547, 0.003261679084971547, 0.003261679084971547, 0.003261679084971547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003261679084971547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326168
Iteration 2/1000 | Loss: 0.00008201
Iteration 3/1000 | Loss: 0.00005904
Iteration 4/1000 | Loss: 0.00004920
Iteration 5/1000 | Loss: 0.00004612
Iteration 6/1000 | Loss: 0.00004449
Iteration 7/1000 | Loss: 0.00004304
Iteration 8/1000 | Loss: 0.00004185
Iteration 9/1000 | Loss: 0.00004096
Iteration 10/1000 | Loss: 0.00004016
Iteration 11/1000 | Loss: 0.00003970
Iteration 12/1000 | Loss: 0.00003926
Iteration 13/1000 | Loss: 0.00003896
Iteration 14/1000 | Loss: 0.00003864
Iteration 15/1000 | Loss: 0.00003841
Iteration 16/1000 | Loss: 0.00003818
Iteration 17/1000 | Loss: 0.00003818
Iteration 18/1000 | Loss: 0.00003817
Iteration 19/1000 | Loss: 0.00003803
Iteration 20/1000 | Loss: 0.00003787
Iteration 21/1000 | Loss: 0.00003782
Iteration 22/1000 | Loss: 0.00003781
Iteration 23/1000 | Loss: 0.00003781
Iteration 24/1000 | Loss: 0.00003771
Iteration 25/1000 | Loss: 0.00003768
Iteration 26/1000 | Loss: 0.00003767
Iteration 27/1000 | Loss: 0.00003765
Iteration 28/1000 | Loss: 0.00003765
Iteration 29/1000 | Loss: 0.00003762
Iteration 30/1000 | Loss: 0.00003761
Iteration 31/1000 | Loss: 0.00003761
Iteration 32/1000 | Loss: 0.00003761
Iteration 33/1000 | Loss: 0.00003755
Iteration 34/1000 | Loss: 0.00003752
Iteration 35/1000 | Loss: 0.00003751
Iteration 36/1000 | Loss: 0.00003751
Iteration 37/1000 | Loss: 0.00003750
Iteration 38/1000 | Loss: 0.00003746
Iteration 39/1000 | Loss: 0.00003746
Iteration 40/1000 | Loss: 0.00003745
Iteration 41/1000 | Loss: 0.00003745
Iteration 42/1000 | Loss: 0.00003745
Iteration 43/1000 | Loss: 0.00003744
Iteration 44/1000 | Loss: 0.00003740
Iteration 45/1000 | Loss: 0.00003740
Iteration 46/1000 | Loss: 0.00003740
Iteration 47/1000 | Loss: 0.00003740
Iteration 48/1000 | Loss: 0.00003740
Iteration 49/1000 | Loss: 0.00003740
Iteration 50/1000 | Loss: 0.00003739
Iteration 51/1000 | Loss: 0.00003739
Iteration 52/1000 | Loss: 0.00003738
Iteration 53/1000 | Loss: 0.00003737
Iteration 54/1000 | Loss: 0.00003736
Iteration 55/1000 | Loss: 0.00003736
Iteration 56/1000 | Loss: 0.00003736
Iteration 57/1000 | Loss: 0.00003735
Iteration 58/1000 | Loss: 0.00003735
Iteration 59/1000 | Loss: 0.00003735
Iteration 60/1000 | Loss: 0.00003734
Iteration 61/1000 | Loss: 0.00003734
Iteration 62/1000 | Loss: 0.00003734
Iteration 63/1000 | Loss: 0.00003733
Iteration 64/1000 | Loss: 0.00003733
Iteration 65/1000 | Loss: 0.00003732
Iteration 66/1000 | Loss: 0.00003732
Iteration 67/1000 | Loss: 0.00003731
Iteration 68/1000 | Loss: 0.00003731
Iteration 69/1000 | Loss: 0.00003731
Iteration 70/1000 | Loss: 0.00003731
Iteration 71/1000 | Loss: 0.00003731
Iteration 72/1000 | Loss: 0.00003731
Iteration 73/1000 | Loss: 0.00003730
Iteration 74/1000 | Loss: 0.00003730
Iteration 75/1000 | Loss: 0.00003730
Iteration 76/1000 | Loss: 0.00003730
Iteration 77/1000 | Loss: 0.00003730
Iteration 78/1000 | Loss: 0.00003730
Iteration 79/1000 | Loss: 0.00003730
Iteration 80/1000 | Loss: 0.00003730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [3.7302692362573e-05, 3.7302692362573e-05, 3.7302692362573e-05, 3.7302692362573e-05, 3.7302692362573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7302692362573e-05

Optimization complete. Final v2v error: 5.0269083976745605 mm

Highest mean error: 6.905013084411621 mm for frame 107

Lowest mean error: 4.063033103942871 mm for frame 84

Saving results

Total time: 49.8456928730011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156170
Iteration 2/25 | Loss: 0.00357735
Iteration 3/25 | Loss: 0.00234186
Iteration 4/25 | Loss: 0.00217798
Iteration 5/25 | Loss: 0.00211132
Iteration 6/25 | Loss: 0.00199919
Iteration 7/25 | Loss: 0.00193464
Iteration 8/25 | Loss: 0.00192448
Iteration 9/25 | Loss: 0.00189376
Iteration 10/25 | Loss: 0.00189830
Iteration 11/25 | Loss: 0.00187643
Iteration 12/25 | Loss: 0.00185838
Iteration 13/25 | Loss: 0.00185022
Iteration 14/25 | Loss: 0.00184635
Iteration 15/25 | Loss: 0.00185066
Iteration 16/25 | Loss: 0.00184170
Iteration 17/25 | Loss: 0.00184169
Iteration 18/25 | Loss: 0.00183664
Iteration 19/25 | Loss: 0.00182427
Iteration 20/25 | Loss: 0.00181475
Iteration 21/25 | Loss: 0.00181739
Iteration 22/25 | Loss: 0.00183079
Iteration 23/25 | Loss: 0.00180616
Iteration 24/25 | Loss: 0.00180543
Iteration 25/25 | Loss: 0.00180104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58822525
Iteration 2/25 | Loss: 0.00764766
Iteration 3/25 | Loss: 0.00744439
Iteration 4/25 | Loss: 0.00744439
Iteration 5/25 | Loss: 0.00744439
Iteration 6/25 | Loss: 0.00744439
Iteration 7/25 | Loss: 0.00744439
Iteration 8/25 | Loss: 0.00744439
Iteration 9/25 | Loss: 0.00744439
Iteration 10/25 | Loss: 0.00744439
Iteration 11/25 | Loss: 0.00744439
Iteration 12/25 | Loss: 0.00744439
Iteration 13/25 | Loss: 0.00744439
Iteration 14/25 | Loss: 0.00744439
Iteration 15/25 | Loss: 0.00744439
Iteration 16/25 | Loss: 0.00744439
Iteration 17/25 | Loss: 0.00744439
Iteration 18/25 | Loss: 0.00744439
Iteration 19/25 | Loss: 0.00744439
Iteration 20/25 | Loss: 0.00744439
Iteration 21/25 | Loss: 0.00744439
Iteration 22/25 | Loss: 0.00744439
Iteration 23/25 | Loss: 0.00744439
Iteration 24/25 | Loss: 0.00744439
Iteration 25/25 | Loss: 0.00744439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00744439
Iteration 2/1000 | Loss: 0.00436903
Iteration 3/1000 | Loss: 0.00137744
Iteration 4/1000 | Loss: 0.00102354
Iteration 5/1000 | Loss: 0.00049934
Iteration 6/1000 | Loss: 0.00059844
Iteration 7/1000 | Loss: 0.00072330
Iteration 8/1000 | Loss: 0.00193514
Iteration 9/1000 | Loss: 0.00046642
Iteration 10/1000 | Loss: 0.00224431
Iteration 11/1000 | Loss: 0.00052030
Iteration 12/1000 | Loss: 0.00029191
Iteration 13/1000 | Loss: 0.00333883
Iteration 14/1000 | Loss: 0.00203115
Iteration 15/1000 | Loss: 0.00043432
Iteration 16/1000 | Loss: 0.00017906
Iteration 17/1000 | Loss: 0.00016031
Iteration 18/1000 | Loss: 0.00213086
Iteration 19/1000 | Loss: 0.00080470
Iteration 20/1000 | Loss: 0.00062521
Iteration 21/1000 | Loss: 0.00044850
Iteration 22/1000 | Loss: 0.00088435
Iteration 23/1000 | Loss: 0.00187763
Iteration 24/1000 | Loss: 0.00181041
Iteration 25/1000 | Loss: 0.00158658
Iteration 26/1000 | Loss: 0.00037912
Iteration 27/1000 | Loss: 0.00170829
Iteration 28/1000 | Loss: 0.00186398
Iteration 29/1000 | Loss: 0.00140611
Iteration 30/1000 | Loss: 0.00099391
Iteration 31/1000 | Loss: 0.00043383
Iteration 32/1000 | Loss: 0.00053703
Iteration 33/1000 | Loss: 0.00023276
Iteration 34/1000 | Loss: 0.00024214
Iteration 35/1000 | Loss: 0.00013192
Iteration 36/1000 | Loss: 0.00092495
Iteration 37/1000 | Loss: 0.00028562
Iteration 38/1000 | Loss: 0.00062484
Iteration 39/1000 | Loss: 0.00042435
Iteration 40/1000 | Loss: 0.00055019
Iteration 41/1000 | Loss: 0.00011868
Iteration 42/1000 | Loss: 0.00015169
Iteration 43/1000 | Loss: 0.00014235
Iteration 44/1000 | Loss: 0.00013790
Iteration 45/1000 | Loss: 0.00035655
Iteration 46/1000 | Loss: 0.00089796
Iteration 47/1000 | Loss: 0.00212843
Iteration 48/1000 | Loss: 0.00092606
Iteration 49/1000 | Loss: 0.00048833
Iteration 50/1000 | Loss: 0.00093332
Iteration 51/1000 | Loss: 0.00021299
Iteration 52/1000 | Loss: 0.00020017
Iteration 53/1000 | Loss: 0.00011629
Iteration 54/1000 | Loss: 0.00009700
Iteration 55/1000 | Loss: 0.00009167
Iteration 56/1000 | Loss: 0.00008746
Iteration 57/1000 | Loss: 0.00112373
Iteration 58/1000 | Loss: 0.00040029
Iteration 59/1000 | Loss: 0.00008485
Iteration 60/1000 | Loss: 0.00111582
Iteration 61/1000 | Loss: 0.00085332
Iteration 62/1000 | Loss: 0.00346068
Iteration 63/1000 | Loss: 0.00199249
Iteration 64/1000 | Loss: 0.00330240
Iteration 65/1000 | Loss: 0.00200482
Iteration 66/1000 | Loss: 0.00162095
Iteration 67/1000 | Loss: 0.00189998
Iteration 68/1000 | Loss: 0.00154516
Iteration 69/1000 | Loss: 0.00096213
Iteration 70/1000 | Loss: 0.00188397
Iteration 71/1000 | Loss: 0.00103677
Iteration 72/1000 | Loss: 0.00078698
Iteration 73/1000 | Loss: 0.00127541
Iteration 74/1000 | Loss: 0.00427066
Iteration 75/1000 | Loss: 0.00554372
Iteration 76/1000 | Loss: 0.00202768
Iteration 77/1000 | Loss: 0.00519949
Iteration 78/1000 | Loss: 0.00503582
Iteration 79/1000 | Loss: 0.00176950
Iteration 80/1000 | Loss: 0.00073351
Iteration 81/1000 | Loss: 0.00062329
Iteration 82/1000 | Loss: 0.00034261
Iteration 83/1000 | Loss: 0.00031632
Iteration 84/1000 | Loss: 0.00196461
Iteration 85/1000 | Loss: 0.00066684
Iteration 86/1000 | Loss: 0.00015290
Iteration 87/1000 | Loss: 0.00126385
Iteration 88/1000 | Loss: 0.00081016
Iteration 89/1000 | Loss: 0.00231839
Iteration 90/1000 | Loss: 0.00110529
Iteration 91/1000 | Loss: 0.00065891
Iteration 92/1000 | Loss: 0.00032561
Iteration 93/1000 | Loss: 0.00012707
Iteration 94/1000 | Loss: 0.00011121
Iteration 95/1000 | Loss: 0.00034574
Iteration 96/1000 | Loss: 0.00022716
Iteration 97/1000 | Loss: 0.00016878
Iteration 98/1000 | Loss: 0.00116074
Iteration 99/1000 | Loss: 0.00047593
Iteration 100/1000 | Loss: 0.00016183
Iteration 101/1000 | Loss: 0.00020605
Iteration 102/1000 | Loss: 0.00048561
Iteration 103/1000 | Loss: 0.00013078
Iteration 104/1000 | Loss: 0.00011217
Iteration 105/1000 | Loss: 0.00030572
Iteration 106/1000 | Loss: 0.00021186
Iteration 107/1000 | Loss: 0.00019821
Iteration 108/1000 | Loss: 0.00028651
Iteration 109/1000 | Loss: 0.00019408
Iteration 110/1000 | Loss: 0.00034675
Iteration 111/1000 | Loss: 0.00114341
Iteration 112/1000 | Loss: 0.00037365
Iteration 113/1000 | Loss: 0.00072577
Iteration 114/1000 | Loss: 0.00012154
Iteration 115/1000 | Loss: 0.00080824
Iteration 116/1000 | Loss: 0.00028045
Iteration 117/1000 | Loss: 0.00039385
Iteration 118/1000 | Loss: 0.00013432
Iteration 119/1000 | Loss: 0.00012277
Iteration 120/1000 | Loss: 0.00015068
Iteration 121/1000 | Loss: 0.00056143
Iteration 122/1000 | Loss: 0.00027664
Iteration 123/1000 | Loss: 0.00068472
Iteration 124/1000 | Loss: 0.00031886
Iteration 125/1000 | Loss: 0.00013975
Iteration 126/1000 | Loss: 0.00012331
Iteration 127/1000 | Loss: 0.00012221
Iteration 128/1000 | Loss: 0.00081285
Iteration 129/1000 | Loss: 0.00044904
Iteration 130/1000 | Loss: 0.00076282
Iteration 131/1000 | Loss: 0.00033720
Iteration 132/1000 | Loss: 0.00025833
Iteration 133/1000 | Loss: 0.00024654
Iteration 134/1000 | Loss: 0.00091884
Iteration 135/1000 | Loss: 0.00040538
Iteration 136/1000 | Loss: 0.00078851
Iteration 137/1000 | Loss: 0.00096305
Iteration 138/1000 | Loss: 0.00069362
Iteration 139/1000 | Loss: 0.00101626
Iteration 140/1000 | Loss: 0.00056565
Iteration 141/1000 | Loss: 0.00040351
Iteration 142/1000 | Loss: 0.00061379
Iteration 143/1000 | Loss: 0.00018090
Iteration 144/1000 | Loss: 0.00014114
Iteration 145/1000 | Loss: 0.00011418
Iteration 146/1000 | Loss: 0.00015134
Iteration 147/1000 | Loss: 0.00018516
Iteration 148/1000 | Loss: 0.00015840
Iteration 149/1000 | Loss: 0.00028559
Iteration 150/1000 | Loss: 0.00015044
Iteration 151/1000 | Loss: 0.00017609
Iteration 152/1000 | Loss: 0.00013336
Iteration 153/1000 | Loss: 0.00013862
Iteration 154/1000 | Loss: 0.00010803
Iteration 155/1000 | Loss: 0.00107538
Iteration 156/1000 | Loss: 0.00019610
Iteration 157/1000 | Loss: 0.00027263
Iteration 158/1000 | Loss: 0.00088617
Iteration 159/1000 | Loss: 0.00036661
Iteration 160/1000 | Loss: 0.00016248
Iteration 161/1000 | Loss: 0.00014844
Iteration 162/1000 | Loss: 0.00022128
Iteration 163/1000 | Loss: 0.00009012
Iteration 164/1000 | Loss: 0.00012936
Iteration 165/1000 | Loss: 0.00012897
Iteration 166/1000 | Loss: 0.00070665
Iteration 167/1000 | Loss: 0.00041448
Iteration 168/1000 | Loss: 0.00042217
Iteration 169/1000 | Loss: 0.00021735
Iteration 170/1000 | Loss: 0.00030925
Iteration 171/1000 | Loss: 0.00009385
Iteration 172/1000 | Loss: 0.00012785
Iteration 173/1000 | Loss: 0.00022590
Iteration 174/1000 | Loss: 0.00012776
Iteration 175/1000 | Loss: 0.00014453
Iteration 176/1000 | Loss: 0.00013354
Iteration 177/1000 | Loss: 0.00116373
Iteration 178/1000 | Loss: 0.00030916
Iteration 179/1000 | Loss: 0.00013221
Iteration 180/1000 | Loss: 0.00013833
Iteration 181/1000 | Loss: 0.00023300
Iteration 182/1000 | Loss: 0.00022745
Iteration 183/1000 | Loss: 0.00011742
Iteration 184/1000 | Loss: 0.00017189
Iteration 185/1000 | Loss: 0.00018645
Iteration 186/1000 | Loss: 0.00020218
Iteration 187/1000 | Loss: 0.00022246
Iteration 188/1000 | Loss: 0.00027686
Iteration 189/1000 | Loss: 0.00020166
Iteration 190/1000 | Loss: 0.00025960
Iteration 191/1000 | Loss: 0.00019204
Iteration 192/1000 | Loss: 0.00033089
Iteration 193/1000 | Loss: 0.00017751
Iteration 194/1000 | Loss: 0.00019539
Iteration 195/1000 | Loss: 0.00019444
Iteration 196/1000 | Loss: 0.00022886
Iteration 197/1000 | Loss: 0.00020339
Iteration 198/1000 | Loss: 0.00018811
Iteration 199/1000 | Loss: 0.00022349
Iteration 200/1000 | Loss: 0.00020611
Iteration 201/1000 | Loss: 0.00022501
Iteration 202/1000 | Loss: 0.00018502
Iteration 203/1000 | Loss: 0.00022357
Iteration 204/1000 | Loss: 0.00088023
Iteration 205/1000 | Loss: 0.00089408
Iteration 206/1000 | Loss: 0.00022628
Iteration 207/1000 | Loss: 0.00017958
Iteration 208/1000 | Loss: 0.00013523
Iteration 209/1000 | Loss: 0.00015140
Iteration 210/1000 | Loss: 0.00018108
Iteration 211/1000 | Loss: 0.00015535
Iteration 212/1000 | Loss: 0.00024839
Iteration 213/1000 | Loss: 0.00022716
Iteration 214/1000 | Loss: 0.00040979
Iteration 215/1000 | Loss: 0.00139724
Iteration 216/1000 | Loss: 0.00204501
Iteration 217/1000 | Loss: 0.00029339
Iteration 218/1000 | Loss: 0.00059050
Iteration 219/1000 | Loss: 0.00013741
Iteration 220/1000 | Loss: 0.00013498
Iteration 221/1000 | Loss: 0.00017618
Iteration 222/1000 | Loss: 0.00014278
Iteration 223/1000 | Loss: 0.00018480
Iteration 224/1000 | Loss: 0.00013194
Iteration 225/1000 | Loss: 0.00010227
Iteration 226/1000 | Loss: 0.00016916
Iteration 227/1000 | Loss: 0.00020727
Iteration 228/1000 | Loss: 0.00024502
Iteration 229/1000 | Loss: 0.00026334
Iteration 230/1000 | Loss: 0.00024074
Iteration 231/1000 | Loss: 0.00038529
Iteration 232/1000 | Loss: 0.00017875
Iteration 233/1000 | Loss: 0.00024197
Iteration 234/1000 | Loss: 0.00019666
Iteration 235/1000 | Loss: 0.00016652
Iteration 236/1000 | Loss: 0.00024856
Iteration 237/1000 | Loss: 0.00021532
Iteration 238/1000 | Loss: 0.00023328
Iteration 239/1000 | Loss: 0.00024947
Iteration 240/1000 | Loss: 0.00026561
Iteration 241/1000 | Loss: 0.00024557
Iteration 242/1000 | Loss: 0.00024728
Iteration 243/1000 | Loss: 0.00039871
Iteration 244/1000 | Loss: 0.00017569
Iteration 245/1000 | Loss: 0.00021636
Iteration 246/1000 | Loss: 0.00098157
Iteration 247/1000 | Loss: 0.00063682
Iteration 248/1000 | Loss: 0.00012875
Iteration 249/1000 | Loss: 0.00007719
Iteration 250/1000 | Loss: 0.00021649
Iteration 251/1000 | Loss: 0.00098551
Iteration 252/1000 | Loss: 0.00053014
Iteration 253/1000 | Loss: 0.00048970
Iteration 254/1000 | Loss: 0.00008693
Iteration 255/1000 | Loss: 0.00007466
Iteration 256/1000 | Loss: 0.00007009
Iteration 257/1000 | Loss: 0.00013844
Iteration 258/1000 | Loss: 0.00122450
Iteration 259/1000 | Loss: 0.00137839
Iteration 260/1000 | Loss: 0.00068824
Iteration 261/1000 | Loss: 0.00022770
Iteration 262/1000 | Loss: 0.00024195
Iteration 263/1000 | Loss: 0.00009385
Iteration 264/1000 | Loss: 0.00040526
Iteration 265/1000 | Loss: 0.00016574
Iteration 266/1000 | Loss: 0.00011808
Iteration 267/1000 | Loss: 0.00012216
Iteration 268/1000 | Loss: 0.00015551
Iteration 269/1000 | Loss: 0.00012988
Iteration 270/1000 | Loss: 0.00008663
Iteration 271/1000 | Loss: 0.00019410
Iteration 272/1000 | Loss: 0.00006203
Iteration 273/1000 | Loss: 0.00089547
Iteration 274/1000 | Loss: 0.00036870
Iteration 275/1000 | Loss: 0.00154024
Iteration 276/1000 | Loss: 0.00057548
Iteration 277/1000 | Loss: 0.00037796
Iteration 278/1000 | Loss: 0.00006953
Iteration 279/1000 | Loss: 0.00007323
Iteration 280/1000 | Loss: 0.00074391
Iteration 281/1000 | Loss: 0.00106403
Iteration 282/1000 | Loss: 0.00058304
Iteration 283/1000 | Loss: 0.00074306
Iteration 284/1000 | Loss: 0.00068263
Iteration 285/1000 | Loss: 0.00051692
Iteration 286/1000 | Loss: 0.00039867
Iteration 287/1000 | Loss: 0.00048782
Iteration 288/1000 | Loss: 0.00038034
Iteration 289/1000 | Loss: 0.00097557
Iteration 290/1000 | Loss: 0.00039838
Iteration 291/1000 | Loss: 0.00048736
Iteration 292/1000 | Loss: 0.00070160
Iteration 293/1000 | Loss: 0.00027169
Iteration 294/1000 | Loss: 0.00044724
Iteration 295/1000 | Loss: 0.00016768
Iteration 296/1000 | Loss: 0.00049118
Iteration 297/1000 | Loss: 0.00009441
Iteration 298/1000 | Loss: 0.00009883
Iteration 299/1000 | Loss: 0.00005747
Iteration 300/1000 | Loss: 0.00009329
Iteration 301/1000 | Loss: 0.00031711
Iteration 302/1000 | Loss: 0.00013904
Iteration 303/1000 | Loss: 0.00009024
Iteration 304/1000 | Loss: 0.00009733
Iteration 305/1000 | Loss: 0.00011662
Iteration 306/1000 | Loss: 0.00012918
Iteration 307/1000 | Loss: 0.00006123
Iteration 308/1000 | Loss: 0.00015092
Iteration 309/1000 | Loss: 0.00010199
Iteration 310/1000 | Loss: 0.00010525
Iteration 311/1000 | Loss: 0.00012093
Iteration 312/1000 | Loss: 0.00007659
Iteration 313/1000 | Loss: 0.00016710
Iteration 314/1000 | Loss: 0.00007022
Iteration 315/1000 | Loss: 0.00020080
Iteration 316/1000 | Loss: 0.00006528
Iteration 317/1000 | Loss: 0.00005857
Iteration 318/1000 | Loss: 0.00005660
Iteration 319/1000 | Loss: 0.00005401
Iteration 320/1000 | Loss: 0.00005274
Iteration 321/1000 | Loss: 0.00017739
Iteration 322/1000 | Loss: 0.00019743
Iteration 323/1000 | Loss: 0.00021326
Iteration 324/1000 | Loss: 0.00005742
Iteration 325/1000 | Loss: 0.00005515
Iteration 326/1000 | Loss: 0.00005374
Iteration 327/1000 | Loss: 0.00005241
Iteration 328/1000 | Loss: 0.00005179
Iteration 329/1000 | Loss: 0.00005141
Iteration 330/1000 | Loss: 0.00005101
Iteration 331/1000 | Loss: 0.00005085
Iteration 332/1000 | Loss: 0.00005081
Iteration 333/1000 | Loss: 0.00005075
Iteration 334/1000 | Loss: 0.00005074
Iteration 335/1000 | Loss: 0.00005074
Iteration 336/1000 | Loss: 0.00005069
Iteration 337/1000 | Loss: 0.00005068
Iteration 338/1000 | Loss: 0.00005066
Iteration 339/1000 | Loss: 0.00005065
Iteration 340/1000 | Loss: 0.00005065
Iteration 341/1000 | Loss: 0.00087943
Iteration 342/1000 | Loss: 0.00063183
Iteration 343/1000 | Loss: 0.00005611
Iteration 344/1000 | Loss: 0.00005186
Iteration 345/1000 | Loss: 0.00091220
Iteration 346/1000 | Loss: 0.00049200
Iteration 347/1000 | Loss: 0.00006880
Iteration 348/1000 | Loss: 0.00005937
Iteration 349/1000 | Loss: 0.00005455
Iteration 350/1000 | Loss: 0.00005267
Iteration 351/1000 | Loss: 0.00006020
Iteration 352/1000 | Loss: 0.00005288
Iteration 353/1000 | Loss: 0.00004914
Iteration 354/1000 | Loss: 0.00004874
Iteration 355/1000 | Loss: 0.00004872
Iteration 356/1000 | Loss: 0.00004866
Iteration 357/1000 | Loss: 0.00004865
Iteration 358/1000 | Loss: 0.00004864
Iteration 359/1000 | Loss: 0.00004861
Iteration 360/1000 | Loss: 0.00004855
Iteration 361/1000 | Loss: 0.00004855
Iteration 362/1000 | Loss: 0.00004854
Iteration 363/1000 | Loss: 0.00029432
Iteration 364/1000 | Loss: 0.00006351
Iteration 365/1000 | Loss: 0.00005295
Iteration 366/1000 | Loss: 0.00007593
Iteration 367/1000 | Loss: 0.00004936
Iteration 368/1000 | Loss: 0.00004863
Iteration 369/1000 | Loss: 0.00006460
Iteration 370/1000 | Loss: 0.00004809
Iteration 371/1000 | Loss: 0.00004794
Iteration 372/1000 | Loss: 0.00004792
Iteration 373/1000 | Loss: 0.00004776
Iteration 374/1000 | Loss: 0.00004774
Iteration 375/1000 | Loss: 0.00004770
Iteration 376/1000 | Loss: 0.00008011
Iteration 377/1000 | Loss: 0.00004773
Iteration 378/1000 | Loss: 0.00004768
Iteration 379/1000 | Loss: 0.00004764
Iteration 380/1000 | Loss: 0.00004764
Iteration 381/1000 | Loss: 0.00004758
Iteration 382/1000 | Loss: 0.00004756
Iteration 383/1000 | Loss: 0.00004755
Iteration 384/1000 | Loss: 0.00004755
Iteration 385/1000 | Loss: 0.00004755
Iteration 386/1000 | Loss: 0.00004755
Iteration 387/1000 | Loss: 0.00004755
Iteration 388/1000 | Loss: 0.00004754
Iteration 389/1000 | Loss: 0.00004754
Iteration 390/1000 | Loss: 0.00004754
Iteration 391/1000 | Loss: 0.00004754
Iteration 392/1000 | Loss: 0.00004754
Iteration 393/1000 | Loss: 0.00004754
Iteration 394/1000 | Loss: 0.00004754
Iteration 395/1000 | Loss: 0.00004754
Iteration 396/1000 | Loss: 0.00004754
Iteration 397/1000 | Loss: 0.00004753
Iteration 398/1000 | Loss: 0.00004751
Iteration 399/1000 | Loss: 0.00004751
Iteration 400/1000 | Loss: 0.00004751
Iteration 401/1000 | Loss: 0.00006688
Iteration 402/1000 | Loss: 0.00004749
Iteration 403/1000 | Loss: 0.00004749
Iteration 404/1000 | Loss: 0.00004749
Iteration 405/1000 | Loss: 0.00004748
Iteration 406/1000 | Loss: 0.00004748
Iteration 407/1000 | Loss: 0.00004748
Iteration 408/1000 | Loss: 0.00004747
Iteration 409/1000 | Loss: 0.00004747
Iteration 410/1000 | Loss: 0.00004747
Iteration 411/1000 | Loss: 0.00004747
Iteration 412/1000 | Loss: 0.00004746
Iteration 413/1000 | Loss: 0.00004746
Iteration 414/1000 | Loss: 0.00004746
Iteration 415/1000 | Loss: 0.00004746
Iteration 416/1000 | Loss: 0.00004746
Iteration 417/1000 | Loss: 0.00004746
Iteration 418/1000 | Loss: 0.00004746
Iteration 419/1000 | Loss: 0.00004746
Iteration 420/1000 | Loss: 0.00004746
Iteration 421/1000 | Loss: 0.00004746
Iteration 422/1000 | Loss: 0.00004746
Iteration 423/1000 | Loss: 0.00004746
Iteration 424/1000 | Loss: 0.00004745
Iteration 425/1000 | Loss: 0.00004745
Iteration 426/1000 | Loss: 0.00004745
Iteration 427/1000 | Loss: 0.00004744
Iteration 428/1000 | Loss: 0.00004744
Iteration 429/1000 | Loss: 0.00004743
Iteration 430/1000 | Loss: 0.00004743
Iteration 431/1000 | Loss: 0.00004743
Iteration 432/1000 | Loss: 0.00004743
Iteration 433/1000 | Loss: 0.00004743
Iteration 434/1000 | Loss: 0.00004743
Iteration 435/1000 | Loss: 0.00004742
Iteration 436/1000 | Loss: 0.00004742
Iteration 437/1000 | Loss: 0.00004742
Iteration 438/1000 | Loss: 0.00004742
Iteration 439/1000 | Loss: 0.00004742
Iteration 440/1000 | Loss: 0.00004741
Iteration 441/1000 | Loss: 0.00004741
Iteration 442/1000 | Loss: 0.00004741
Iteration 443/1000 | Loss: 0.00004741
Iteration 444/1000 | Loss: 0.00004741
Iteration 445/1000 | Loss: 0.00004741
Iteration 446/1000 | Loss: 0.00004741
Iteration 447/1000 | Loss: 0.00004741
Iteration 448/1000 | Loss: 0.00004741
Iteration 449/1000 | Loss: 0.00004741
Iteration 450/1000 | Loss: 0.00004741
Iteration 451/1000 | Loss: 0.00004741
Iteration 452/1000 | Loss: 0.00004741
Iteration 453/1000 | Loss: 0.00004741
Iteration 454/1000 | Loss: 0.00004740
Iteration 455/1000 | Loss: 0.00004740
Iteration 456/1000 | Loss: 0.00004740
Iteration 457/1000 | Loss: 0.00004740
Iteration 458/1000 | Loss: 0.00004740
Iteration 459/1000 | Loss: 0.00004740
Iteration 460/1000 | Loss: 0.00004740
Iteration 461/1000 | Loss: 0.00004740
Iteration 462/1000 | Loss: 0.00004740
Iteration 463/1000 | Loss: 0.00004740
Iteration 464/1000 | Loss: 0.00004740
Iteration 465/1000 | Loss: 0.00004740
Iteration 466/1000 | Loss: 0.00004740
Iteration 467/1000 | Loss: 0.00004740
Iteration 468/1000 | Loss: 0.00004740
Iteration 469/1000 | Loss: 0.00004740
Iteration 470/1000 | Loss: 0.00004740
Iteration 471/1000 | Loss: 0.00004740
Iteration 472/1000 | Loss: 0.00004740
Iteration 473/1000 | Loss: 0.00004740
Iteration 474/1000 | Loss: 0.00004740
Iteration 475/1000 | Loss: 0.00004740
Iteration 476/1000 | Loss: 0.00004740
Iteration 477/1000 | Loss: 0.00004740
Iteration 478/1000 | Loss: 0.00004740
Iteration 479/1000 | Loss: 0.00004740
Iteration 480/1000 | Loss: 0.00004740
Iteration 481/1000 | Loss: 0.00004740
Iteration 482/1000 | Loss: 0.00004740
Iteration 483/1000 | Loss: 0.00004740
Iteration 484/1000 | Loss: 0.00004740
Iteration 485/1000 | Loss: 0.00004740
Iteration 486/1000 | Loss: 0.00004740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 486. Stopping optimization.
Last 5 losses: [4.7395915316883475e-05, 4.7395915316883475e-05, 4.7395915316883475e-05, 4.7395915316883475e-05, 4.7395915316883475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7395915316883475e-05

Optimization complete. Final v2v error: 5.3538665771484375 mm

Highest mean error: 13.371805191040039 mm for frame 49

Lowest mean error: 4.604064464569092 mm for frame 10

Saving results

Total time: 626.3113512992859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941723
Iteration 2/25 | Loss: 0.00178426
Iteration 3/25 | Loss: 0.00156571
Iteration 4/25 | Loss: 0.00152904
Iteration 5/25 | Loss: 0.00152284
Iteration 6/25 | Loss: 0.00152187
Iteration 7/25 | Loss: 0.00152176
Iteration 8/25 | Loss: 0.00152176
Iteration 9/25 | Loss: 0.00152176
Iteration 10/25 | Loss: 0.00152176
Iteration 11/25 | Loss: 0.00152176
Iteration 12/25 | Loss: 0.00152176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015217644395306706, 0.0015217644395306706, 0.0015217644395306706, 0.0015217644395306706, 0.0015217644395306706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015217644395306706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53401744
Iteration 2/25 | Loss: 0.00284682
Iteration 3/25 | Loss: 0.00284678
Iteration 4/25 | Loss: 0.00284677
Iteration 5/25 | Loss: 0.00284677
Iteration 6/25 | Loss: 0.00284677
Iteration 7/25 | Loss: 0.00284677
Iteration 8/25 | Loss: 0.00284677
Iteration 9/25 | Loss: 0.00284677
Iteration 10/25 | Loss: 0.00284677
Iteration 11/25 | Loss: 0.00284677
Iteration 12/25 | Loss: 0.00284677
Iteration 13/25 | Loss: 0.00284677
Iteration 14/25 | Loss: 0.00284677
Iteration 15/25 | Loss: 0.00284677
Iteration 16/25 | Loss: 0.00284677
Iteration 17/25 | Loss: 0.00284677
Iteration 18/25 | Loss: 0.00284677
Iteration 19/25 | Loss: 0.00284677
Iteration 20/25 | Loss: 0.00284677
Iteration 21/25 | Loss: 0.00284677
Iteration 22/25 | Loss: 0.00284677
Iteration 23/25 | Loss: 0.00284677
Iteration 24/25 | Loss: 0.00284677
Iteration 25/25 | Loss: 0.00284677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284677
Iteration 2/1000 | Loss: 0.00005726
Iteration 3/1000 | Loss: 0.00004085
Iteration 4/1000 | Loss: 0.00003613
Iteration 5/1000 | Loss: 0.00003362
Iteration 6/1000 | Loss: 0.00003232
Iteration 7/1000 | Loss: 0.00003153
Iteration 8/1000 | Loss: 0.00003078
Iteration 9/1000 | Loss: 0.00003025
Iteration 10/1000 | Loss: 0.00002990
Iteration 11/1000 | Loss: 0.00002965
Iteration 12/1000 | Loss: 0.00002943
Iteration 13/1000 | Loss: 0.00002941
Iteration 14/1000 | Loss: 0.00002922
Iteration 15/1000 | Loss: 0.00002909
Iteration 16/1000 | Loss: 0.00002903
Iteration 17/1000 | Loss: 0.00002903
Iteration 18/1000 | Loss: 0.00002892
Iteration 19/1000 | Loss: 0.00002889
Iteration 20/1000 | Loss: 0.00002889
Iteration 21/1000 | Loss: 0.00002888
Iteration 22/1000 | Loss: 0.00002888
Iteration 23/1000 | Loss: 0.00002888
Iteration 24/1000 | Loss: 0.00002888
Iteration 25/1000 | Loss: 0.00002887
Iteration 26/1000 | Loss: 0.00002887
Iteration 27/1000 | Loss: 0.00002886
Iteration 28/1000 | Loss: 0.00002885
Iteration 29/1000 | Loss: 0.00002885
Iteration 30/1000 | Loss: 0.00002885
Iteration 31/1000 | Loss: 0.00002884
Iteration 32/1000 | Loss: 0.00002884
Iteration 33/1000 | Loss: 0.00002884
Iteration 34/1000 | Loss: 0.00002884
Iteration 35/1000 | Loss: 0.00002883
Iteration 36/1000 | Loss: 0.00002882
Iteration 37/1000 | Loss: 0.00002882
Iteration 38/1000 | Loss: 0.00002882
Iteration 39/1000 | Loss: 0.00002882
Iteration 40/1000 | Loss: 0.00002881
Iteration 41/1000 | Loss: 0.00002881
Iteration 42/1000 | Loss: 0.00002881
Iteration 43/1000 | Loss: 0.00002881
Iteration 44/1000 | Loss: 0.00002881
Iteration 45/1000 | Loss: 0.00002881
Iteration 46/1000 | Loss: 0.00002881
Iteration 47/1000 | Loss: 0.00002881
Iteration 48/1000 | Loss: 0.00002881
Iteration 49/1000 | Loss: 0.00002881
Iteration 50/1000 | Loss: 0.00002880
Iteration 51/1000 | Loss: 0.00002880
Iteration 52/1000 | Loss: 0.00002880
Iteration 53/1000 | Loss: 0.00002880
Iteration 54/1000 | Loss: 0.00002879
Iteration 55/1000 | Loss: 0.00002879
Iteration 56/1000 | Loss: 0.00002879
Iteration 57/1000 | Loss: 0.00002879
Iteration 58/1000 | Loss: 0.00002879
Iteration 59/1000 | Loss: 0.00002879
Iteration 60/1000 | Loss: 0.00002879
Iteration 61/1000 | Loss: 0.00002879
Iteration 62/1000 | Loss: 0.00002878
Iteration 63/1000 | Loss: 0.00002878
Iteration 64/1000 | Loss: 0.00002878
Iteration 65/1000 | Loss: 0.00002878
Iteration 66/1000 | Loss: 0.00002878
Iteration 67/1000 | Loss: 0.00002878
Iteration 68/1000 | Loss: 0.00002878
Iteration 69/1000 | Loss: 0.00002878
Iteration 70/1000 | Loss: 0.00002878
Iteration 71/1000 | Loss: 0.00002878
Iteration 72/1000 | Loss: 0.00002878
Iteration 73/1000 | Loss: 0.00002878
Iteration 74/1000 | Loss: 0.00002878
Iteration 75/1000 | Loss: 0.00002878
Iteration 76/1000 | Loss: 0.00002878
Iteration 77/1000 | Loss: 0.00002877
Iteration 78/1000 | Loss: 0.00002877
Iteration 79/1000 | Loss: 0.00002877
Iteration 80/1000 | Loss: 0.00002877
Iteration 81/1000 | Loss: 0.00002877
Iteration 82/1000 | Loss: 0.00002877
Iteration 83/1000 | Loss: 0.00002877
Iteration 84/1000 | Loss: 0.00002877
Iteration 85/1000 | Loss: 0.00002877
Iteration 86/1000 | Loss: 0.00002877
Iteration 87/1000 | Loss: 0.00002877
Iteration 88/1000 | Loss: 0.00002877
Iteration 89/1000 | Loss: 0.00002877
Iteration 90/1000 | Loss: 0.00002877
Iteration 91/1000 | Loss: 0.00002877
Iteration 92/1000 | Loss: 0.00002876
Iteration 93/1000 | Loss: 0.00002876
Iteration 94/1000 | Loss: 0.00002876
Iteration 95/1000 | Loss: 0.00002876
Iteration 96/1000 | Loss: 0.00002876
Iteration 97/1000 | Loss: 0.00002876
Iteration 98/1000 | Loss: 0.00002876
Iteration 99/1000 | Loss: 0.00002876
Iteration 100/1000 | Loss: 0.00002876
Iteration 101/1000 | Loss: 0.00002876
Iteration 102/1000 | Loss: 0.00002876
Iteration 103/1000 | Loss: 0.00002876
Iteration 104/1000 | Loss: 0.00002875
Iteration 105/1000 | Loss: 0.00002875
Iteration 106/1000 | Loss: 0.00002875
Iteration 107/1000 | Loss: 0.00002875
Iteration 108/1000 | Loss: 0.00002875
Iteration 109/1000 | Loss: 0.00002875
Iteration 110/1000 | Loss: 0.00002875
Iteration 111/1000 | Loss: 0.00002875
Iteration 112/1000 | Loss: 0.00002875
Iteration 113/1000 | Loss: 0.00002875
Iteration 114/1000 | Loss: 0.00002875
Iteration 115/1000 | Loss: 0.00002875
Iteration 116/1000 | Loss: 0.00002875
Iteration 117/1000 | Loss: 0.00002875
Iteration 118/1000 | Loss: 0.00002875
Iteration 119/1000 | Loss: 0.00002875
Iteration 120/1000 | Loss: 0.00002875
Iteration 121/1000 | Loss: 0.00002875
Iteration 122/1000 | Loss: 0.00002875
Iteration 123/1000 | Loss: 0.00002875
Iteration 124/1000 | Loss: 0.00002875
Iteration 125/1000 | Loss: 0.00002874
Iteration 126/1000 | Loss: 0.00002874
Iteration 127/1000 | Loss: 0.00002874
Iteration 128/1000 | Loss: 0.00002874
Iteration 129/1000 | Loss: 0.00002874
Iteration 130/1000 | Loss: 0.00002874
Iteration 131/1000 | Loss: 0.00002874
Iteration 132/1000 | Loss: 0.00002874
Iteration 133/1000 | Loss: 0.00002874
Iteration 134/1000 | Loss: 0.00002874
Iteration 135/1000 | Loss: 0.00002874
Iteration 136/1000 | Loss: 0.00002874
Iteration 137/1000 | Loss: 0.00002874
Iteration 138/1000 | Loss: 0.00002874
Iteration 139/1000 | Loss: 0.00002874
Iteration 140/1000 | Loss: 0.00002874
Iteration 141/1000 | Loss: 0.00002874
Iteration 142/1000 | Loss: 0.00002874
Iteration 143/1000 | Loss: 0.00002874
Iteration 144/1000 | Loss: 0.00002874
Iteration 145/1000 | Loss: 0.00002874
Iteration 146/1000 | Loss: 0.00002874
Iteration 147/1000 | Loss: 0.00002874
Iteration 148/1000 | Loss: 0.00002874
Iteration 149/1000 | Loss: 0.00002874
Iteration 150/1000 | Loss: 0.00002874
Iteration 151/1000 | Loss: 0.00002874
Iteration 152/1000 | Loss: 0.00002874
Iteration 153/1000 | Loss: 0.00002874
Iteration 154/1000 | Loss: 0.00002874
Iteration 155/1000 | Loss: 0.00002874
Iteration 156/1000 | Loss: 0.00002874
Iteration 157/1000 | Loss: 0.00002874
Iteration 158/1000 | Loss: 0.00002874
Iteration 159/1000 | Loss: 0.00002874
Iteration 160/1000 | Loss: 0.00002874
Iteration 161/1000 | Loss: 0.00002874
Iteration 162/1000 | Loss: 0.00002874
Iteration 163/1000 | Loss: 0.00002874
Iteration 164/1000 | Loss: 0.00002874
Iteration 165/1000 | Loss: 0.00002874
Iteration 166/1000 | Loss: 0.00002874
Iteration 167/1000 | Loss: 0.00002874
Iteration 168/1000 | Loss: 0.00002874
Iteration 169/1000 | Loss: 0.00002874
Iteration 170/1000 | Loss: 0.00002874
Iteration 171/1000 | Loss: 0.00002874
Iteration 172/1000 | Loss: 0.00002874
Iteration 173/1000 | Loss: 0.00002874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.8741018468281254e-05, 2.8741018468281254e-05, 2.8741018468281254e-05, 2.8741018468281254e-05, 2.8741018468281254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8741018468281254e-05

Optimization complete. Final v2v error: 4.716846466064453 mm

Highest mean error: 5.27782678604126 mm for frame 81

Lowest mean error: 4.3039021492004395 mm for frame 48

Saving results

Total time: 38.32218337059021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869826
Iteration 2/25 | Loss: 0.00160167
Iteration 3/25 | Loss: 0.00146591
Iteration 4/25 | Loss: 0.00144423
Iteration 5/25 | Loss: 0.00143940
Iteration 6/25 | Loss: 0.00143813
Iteration 7/25 | Loss: 0.00143813
Iteration 8/25 | Loss: 0.00143813
Iteration 9/25 | Loss: 0.00143813
Iteration 10/25 | Loss: 0.00143813
Iteration 11/25 | Loss: 0.00143813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014381343498826027, 0.0014381343498826027, 0.0014381343498826027, 0.0014381343498826027, 0.0014381343498826027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014381343498826027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.88345671
Iteration 2/25 | Loss: 0.00257920
Iteration 3/25 | Loss: 0.00257917
Iteration 4/25 | Loss: 0.00257917
Iteration 5/25 | Loss: 0.00257917
Iteration 6/25 | Loss: 0.00257917
Iteration 7/25 | Loss: 0.00257917
Iteration 8/25 | Loss: 0.00257917
Iteration 9/25 | Loss: 0.00257917
Iteration 10/25 | Loss: 0.00257917
Iteration 11/25 | Loss: 0.00257917
Iteration 12/25 | Loss: 0.00257917
Iteration 13/25 | Loss: 0.00257917
Iteration 14/25 | Loss: 0.00257917
Iteration 15/25 | Loss: 0.00257917
Iteration 16/25 | Loss: 0.00257917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025791660882532597, 0.0025791660882532597, 0.0025791660882532597, 0.0025791660882532597, 0.0025791660882532597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025791660882532597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257917
Iteration 2/1000 | Loss: 0.00005629
Iteration 3/1000 | Loss: 0.00003464
Iteration 4/1000 | Loss: 0.00003028
Iteration 5/1000 | Loss: 0.00002879
Iteration 6/1000 | Loss: 0.00002756
Iteration 7/1000 | Loss: 0.00002686
Iteration 8/1000 | Loss: 0.00002638
Iteration 9/1000 | Loss: 0.00002603
Iteration 10/1000 | Loss: 0.00002573
Iteration 11/1000 | Loss: 0.00002561
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002542
Iteration 14/1000 | Loss: 0.00002541
Iteration 15/1000 | Loss: 0.00002541
Iteration 16/1000 | Loss: 0.00002539
Iteration 17/1000 | Loss: 0.00002530
Iteration 18/1000 | Loss: 0.00002527
Iteration 19/1000 | Loss: 0.00002527
Iteration 20/1000 | Loss: 0.00002526
Iteration 21/1000 | Loss: 0.00002526
Iteration 22/1000 | Loss: 0.00002522
Iteration 23/1000 | Loss: 0.00002522
Iteration 24/1000 | Loss: 0.00002521
Iteration 25/1000 | Loss: 0.00002521
Iteration 26/1000 | Loss: 0.00002518
Iteration 27/1000 | Loss: 0.00002518
Iteration 28/1000 | Loss: 0.00002517
Iteration 29/1000 | Loss: 0.00002517
Iteration 30/1000 | Loss: 0.00002517
Iteration 31/1000 | Loss: 0.00002515
Iteration 32/1000 | Loss: 0.00002515
Iteration 33/1000 | Loss: 0.00002514
Iteration 34/1000 | Loss: 0.00002514
Iteration 35/1000 | Loss: 0.00002514
Iteration 36/1000 | Loss: 0.00002513
Iteration 37/1000 | Loss: 0.00002513
Iteration 38/1000 | Loss: 0.00002512
Iteration 39/1000 | Loss: 0.00002512
Iteration 40/1000 | Loss: 0.00002512
Iteration 41/1000 | Loss: 0.00002511
Iteration 42/1000 | Loss: 0.00002511
Iteration 43/1000 | Loss: 0.00002511
Iteration 44/1000 | Loss: 0.00002511
Iteration 45/1000 | Loss: 0.00002511
Iteration 46/1000 | Loss: 0.00002511
Iteration 47/1000 | Loss: 0.00002511
Iteration 48/1000 | Loss: 0.00002511
Iteration 49/1000 | Loss: 0.00002511
Iteration 50/1000 | Loss: 0.00002511
Iteration 51/1000 | Loss: 0.00002511
Iteration 52/1000 | Loss: 0.00002511
Iteration 53/1000 | Loss: 0.00002510
Iteration 54/1000 | Loss: 0.00002510
Iteration 55/1000 | Loss: 0.00002510
Iteration 56/1000 | Loss: 0.00002510
Iteration 57/1000 | Loss: 0.00002510
Iteration 58/1000 | Loss: 0.00002510
Iteration 59/1000 | Loss: 0.00002510
Iteration 60/1000 | Loss: 0.00002510
Iteration 61/1000 | Loss: 0.00002510
Iteration 62/1000 | Loss: 0.00002510
Iteration 63/1000 | Loss: 0.00002510
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002510
Iteration 67/1000 | Loss: 0.00002510
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002509
Iteration 70/1000 | Loss: 0.00002509
Iteration 71/1000 | Loss: 0.00002509
Iteration 72/1000 | Loss: 0.00002509
Iteration 73/1000 | Loss: 0.00002509
Iteration 74/1000 | Loss: 0.00002509
Iteration 75/1000 | Loss: 0.00002509
Iteration 76/1000 | Loss: 0.00002509
Iteration 77/1000 | Loss: 0.00002509
Iteration 78/1000 | Loss: 0.00002509
Iteration 79/1000 | Loss: 0.00002509
Iteration 80/1000 | Loss: 0.00002508
Iteration 81/1000 | Loss: 0.00002508
Iteration 82/1000 | Loss: 0.00002508
Iteration 83/1000 | Loss: 0.00002508
Iteration 84/1000 | Loss: 0.00002508
Iteration 85/1000 | Loss: 0.00002508
Iteration 86/1000 | Loss: 0.00002508
Iteration 87/1000 | Loss: 0.00002508
Iteration 88/1000 | Loss: 0.00002508
Iteration 89/1000 | Loss: 0.00002508
Iteration 90/1000 | Loss: 0.00002508
Iteration 91/1000 | Loss: 0.00002508
Iteration 92/1000 | Loss: 0.00002508
Iteration 93/1000 | Loss: 0.00002508
Iteration 94/1000 | Loss: 0.00002508
Iteration 95/1000 | Loss: 0.00002508
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002507
Iteration 99/1000 | Loss: 0.00002507
Iteration 100/1000 | Loss: 0.00002507
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002507
Iteration 104/1000 | Loss: 0.00002507
Iteration 105/1000 | Loss: 0.00002507
Iteration 106/1000 | Loss: 0.00002506
Iteration 107/1000 | Loss: 0.00002506
Iteration 108/1000 | Loss: 0.00002506
Iteration 109/1000 | Loss: 0.00002506
Iteration 110/1000 | Loss: 0.00002506
Iteration 111/1000 | Loss: 0.00002506
Iteration 112/1000 | Loss: 0.00002506
Iteration 113/1000 | Loss: 0.00002506
Iteration 114/1000 | Loss: 0.00002506
Iteration 115/1000 | Loss: 0.00002506
Iteration 116/1000 | Loss: 0.00002506
Iteration 117/1000 | Loss: 0.00002506
Iteration 118/1000 | Loss: 0.00002506
Iteration 119/1000 | Loss: 0.00002506
Iteration 120/1000 | Loss: 0.00002506
Iteration 121/1000 | Loss: 0.00002505
Iteration 122/1000 | Loss: 0.00002505
Iteration 123/1000 | Loss: 0.00002505
Iteration 124/1000 | Loss: 0.00002505
Iteration 125/1000 | Loss: 0.00002505
Iteration 126/1000 | Loss: 0.00002505
Iteration 127/1000 | Loss: 0.00002505
Iteration 128/1000 | Loss: 0.00002505
Iteration 129/1000 | Loss: 0.00002504
Iteration 130/1000 | Loss: 0.00002504
Iteration 131/1000 | Loss: 0.00002504
Iteration 132/1000 | Loss: 0.00002504
Iteration 133/1000 | Loss: 0.00002504
Iteration 134/1000 | Loss: 0.00002504
Iteration 135/1000 | Loss: 0.00002504
Iteration 136/1000 | Loss: 0.00002504
Iteration 137/1000 | Loss: 0.00002504
Iteration 138/1000 | Loss: 0.00002504
Iteration 139/1000 | Loss: 0.00002504
Iteration 140/1000 | Loss: 0.00002504
Iteration 141/1000 | Loss: 0.00002504
Iteration 142/1000 | Loss: 0.00002504
Iteration 143/1000 | Loss: 0.00002504
Iteration 144/1000 | Loss: 0.00002504
Iteration 145/1000 | Loss: 0.00002504
Iteration 146/1000 | Loss: 0.00002504
Iteration 147/1000 | Loss: 0.00002504
Iteration 148/1000 | Loss: 0.00002504
Iteration 149/1000 | Loss: 0.00002504
Iteration 150/1000 | Loss: 0.00002504
Iteration 151/1000 | Loss: 0.00002504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.5043487767106853e-05, 2.5043487767106853e-05, 2.5043487767106853e-05, 2.5043487767106853e-05, 2.5043487767106853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5043487767106853e-05

Optimization complete. Final v2v error: 4.408515930175781 mm

Highest mean error: 4.8757548332214355 mm for frame 101

Lowest mean error: 3.927487850189209 mm for frame 39

Saving results

Total time: 40.27493953704834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952101
Iteration 2/25 | Loss: 0.00173572
Iteration 3/25 | Loss: 0.00147054
Iteration 4/25 | Loss: 0.00143794
Iteration 5/25 | Loss: 0.00143186
Iteration 6/25 | Loss: 0.00143135
Iteration 7/25 | Loss: 0.00143135
Iteration 8/25 | Loss: 0.00143135
Iteration 9/25 | Loss: 0.00143135
Iteration 10/25 | Loss: 0.00143135
Iteration 11/25 | Loss: 0.00143135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014313454739749432, 0.0014313454739749432, 0.0014313454739749432, 0.0014313454739749432, 0.0014313454739749432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014313454739749432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56593132
Iteration 2/25 | Loss: 0.00284397
Iteration 3/25 | Loss: 0.00284397
Iteration 4/25 | Loss: 0.00284397
Iteration 5/25 | Loss: 0.00284396
Iteration 6/25 | Loss: 0.00284396
Iteration 7/25 | Loss: 0.00284396
Iteration 8/25 | Loss: 0.00284396
Iteration 9/25 | Loss: 0.00284396
Iteration 10/25 | Loss: 0.00284396
Iteration 11/25 | Loss: 0.00284396
Iteration 12/25 | Loss: 0.00284396
Iteration 13/25 | Loss: 0.00284396
Iteration 14/25 | Loss: 0.00284396
Iteration 15/25 | Loss: 0.00284396
Iteration 16/25 | Loss: 0.00284396
Iteration 17/25 | Loss: 0.00284396
Iteration 18/25 | Loss: 0.00284396
Iteration 19/25 | Loss: 0.00284396
Iteration 20/25 | Loss: 0.00284396
Iteration 21/25 | Loss: 0.00284396
Iteration 22/25 | Loss: 0.00284396
Iteration 23/25 | Loss: 0.00284396
Iteration 24/25 | Loss: 0.00284396
Iteration 25/25 | Loss: 0.00284396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284396
Iteration 2/1000 | Loss: 0.00004126
Iteration 3/1000 | Loss: 0.00003207
Iteration 4/1000 | Loss: 0.00002809
Iteration 5/1000 | Loss: 0.00002682
Iteration 6/1000 | Loss: 0.00002562
Iteration 7/1000 | Loss: 0.00002487
Iteration 8/1000 | Loss: 0.00002430
Iteration 9/1000 | Loss: 0.00002400
Iteration 10/1000 | Loss: 0.00002375
Iteration 11/1000 | Loss: 0.00002348
Iteration 12/1000 | Loss: 0.00002334
Iteration 13/1000 | Loss: 0.00002317
Iteration 14/1000 | Loss: 0.00002314
Iteration 15/1000 | Loss: 0.00002299
Iteration 16/1000 | Loss: 0.00002293
Iteration 17/1000 | Loss: 0.00002293
Iteration 18/1000 | Loss: 0.00002288
Iteration 19/1000 | Loss: 0.00002282
Iteration 20/1000 | Loss: 0.00002279
Iteration 21/1000 | Loss: 0.00002278
Iteration 22/1000 | Loss: 0.00002278
Iteration 23/1000 | Loss: 0.00002277
Iteration 24/1000 | Loss: 0.00002276
Iteration 25/1000 | Loss: 0.00002275
Iteration 26/1000 | Loss: 0.00002275
Iteration 27/1000 | Loss: 0.00002274
Iteration 28/1000 | Loss: 0.00002274
Iteration 29/1000 | Loss: 0.00002274
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002272
Iteration 32/1000 | Loss: 0.00002272
Iteration 33/1000 | Loss: 0.00002271
Iteration 34/1000 | Loss: 0.00002271
Iteration 35/1000 | Loss: 0.00002271
Iteration 36/1000 | Loss: 0.00002271
Iteration 37/1000 | Loss: 0.00002271
Iteration 38/1000 | Loss: 0.00002271
Iteration 39/1000 | Loss: 0.00002271
Iteration 40/1000 | Loss: 0.00002271
Iteration 41/1000 | Loss: 0.00002271
Iteration 42/1000 | Loss: 0.00002270
Iteration 43/1000 | Loss: 0.00002269
Iteration 44/1000 | Loss: 0.00002269
Iteration 45/1000 | Loss: 0.00002269
Iteration 46/1000 | Loss: 0.00002269
Iteration 47/1000 | Loss: 0.00002268
Iteration 48/1000 | Loss: 0.00002268
Iteration 49/1000 | Loss: 0.00002268
Iteration 50/1000 | Loss: 0.00002268
Iteration 51/1000 | Loss: 0.00002268
Iteration 52/1000 | Loss: 0.00002268
Iteration 53/1000 | Loss: 0.00002268
Iteration 54/1000 | Loss: 0.00002268
Iteration 55/1000 | Loss: 0.00002268
Iteration 56/1000 | Loss: 0.00002268
Iteration 57/1000 | Loss: 0.00002268
Iteration 58/1000 | Loss: 0.00002268
Iteration 59/1000 | Loss: 0.00002268
Iteration 60/1000 | Loss: 0.00002267
Iteration 61/1000 | Loss: 0.00002267
Iteration 62/1000 | Loss: 0.00002266
Iteration 63/1000 | Loss: 0.00002266
Iteration 64/1000 | Loss: 0.00002266
Iteration 65/1000 | Loss: 0.00002266
Iteration 66/1000 | Loss: 0.00002266
Iteration 67/1000 | Loss: 0.00002266
Iteration 68/1000 | Loss: 0.00002266
Iteration 69/1000 | Loss: 0.00002266
Iteration 70/1000 | Loss: 0.00002265
Iteration 71/1000 | Loss: 0.00002265
Iteration 72/1000 | Loss: 0.00002265
Iteration 73/1000 | Loss: 0.00002265
Iteration 74/1000 | Loss: 0.00002265
Iteration 75/1000 | Loss: 0.00002265
Iteration 76/1000 | Loss: 0.00002265
Iteration 77/1000 | Loss: 0.00002265
Iteration 78/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.265346483909525e-05, 2.265346483909525e-05, 2.265346483909525e-05, 2.265346483909525e-05, 2.265346483909525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.265346483909525e-05

Optimization complete. Final v2v error: 4.2234954833984375 mm

Highest mean error: 4.558242321014404 mm for frame 199

Lowest mean error: 3.9276540279388428 mm for frame 170

Saving results

Total time: 38.04060363769531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00543694
Iteration 2/25 | Loss: 0.00153557
Iteration 3/25 | Loss: 0.00144561
Iteration 4/25 | Loss: 0.00142896
Iteration 5/25 | Loss: 0.00142605
Iteration 6/25 | Loss: 0.00142505
Iteration 7/25 | Loss: 0.00142500
Iteration 8/25 | Loss: 0.00142500
Iteration 9/25 | Loss: 0.00142500
Iteration 10/25 | Loss: 0.00142500
Iteration 11/25 | Loss: 0.00142500
Iteration 12/25 | Loss: 0.00142500
Iteration 13/25 | Loss: 0.00142500
Iteration 14/25 | Loss: 0.00142500
Iteration 15/25 | Loss: 0.00142500
Iteration 16/25 | Loss: 0.00142500
Iteration 17/25 | Loss: 0.00142500
Iteration 18/25 | Loss: 0.00142500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001424999674782157, 0.001424999674782157, 0.001424999674782157, 0.001424999674782157, 0.001424999674782157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001424999674782157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97584993
Iteration 2/25 | Loss: 0.00230936
Iteration 3/25 | Loss: 0.00230935
Iteration 4/25 | Loss: 0.00230935
Iteration 5/25 | Loss: 0.00230934
Iteration 6/25 | Loss: 0.00230934
Iteration 7/25 | Loss: 0.00230934
Iteration 8/25 | Loss: 0.00230934
Iteration 9/25 | Loss: 0.00230934
Iteration 10/25 | Loss: 0.00230934
Iteration 11/25 | Loss: 0.00230934
Iteration 12/25 | Loss: 0.00230934
Iteration 13/25 | Loss: 0.00230934
Iteration 14/25 | Loss: 0.00230934
Iteration 15/25 | Loss: 0.00230934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002309343544766307, 0.002309343544766307, 0.002309343544766307, 0.002309343544766307, 0.002309343544766307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002309343544766307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230934
Iteration 2/1000 | Loss: 0.00003186
Iteration 3/1000 | Loss: 0.00002608
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002384
Iteration 6/1000 | Loss: 0.00002307
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002241
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002179
Iteration 11/1000 | Loss: 0.00002164
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00002159
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002159
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00002159
Iteration 18/1000 | Loss: 0.00002158
Iteration 19/1000 | Loss: 0.00002154
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002152
Iteration 23/1000 | Loss: 0.00002151
Iteration 24/1000 | Loss: 0.00002150
Iteration 25/1000 | Loss: 0.00002150
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002149
Iteration 28/1000 | Loss: 0.00002146
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002145
Iteration 31/1000 | Loss: 0.00002145
Iteration 32/1000 | Loss: 0.00002145
Iteration 33/1000 | Loss: 0.00002145
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002144
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002143
Iteration 38/1000 | Loss: 0.00002142
Iteration 39/1000 | Loss: 0.00002142
Iteration 40/1000 | Loss: 0.00002142
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002142
Iteration 43/1000 | Loss: 0.00002142
Iteration 44/1000 | Loss: 0.00002142
Iteration 45/1000 | Loss: 0.00002142
Iteration 46/1000 | Loss: 0.00002142
Iteration 47/1000 | Loss: 0.00002142
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00002142
Iteration 50/1000 | Loss: 0.00002142
Iteration 51/1000 | Loss: 0.00002142
Iteration 52/1000 | Loss: 0.00002142
Iteration 53/1000 | Loss: 0.00002142
Iteration 54/1000 | Loss: 0.00002142
Iteration 55/1000 | Loss: 0.00002142
Iteration 56/1000 | Loss: 0.00002142
Iteration 57/1000 | Loss: 0.00002142
Iteration 58/1000 | Loss: 0.00002142
Iteration 59/1000 | Loss: 0.00002142
Iteration 60/1000 | Loss: 0.00002142
Iteration 61/1000 | Loss: 0.00002142
Iteration 62/1000 | Loss: 0.00002142
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002142
Iteration 67/1000 | Loss: 0.00002142
Iteration 68/1000 | Loss: 0.00002142
Iteration 69/1000 | Loss: 0.00002142
Iteration 70/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.1417477910290472e-05, 2.1417477910290472e-05, 2.1417477910290472e-05, 2.1417477910290472e-05, 2.1417477910290472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1417477910290472e-05

Optimization complete. Final v2v error: 4.078051567077637 mm

Highest mean error: 4.213932514190674 mm for frame 146

Lowest mean error: 3.9175283908843994 mm for frame 107

Saving results

Total time: 30.233174085617065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160785
Iteration 2/25 | Loss: 0.01160785
Iteration 3/25 | Loss: 0.01160785
Iteration 4/25 | Loss: 0.01160785
Iteration 5/25 | Loss: 0.01160785
Iteration 6/25 | Loss: 0.01160785
Iteration 7/25 | Loss: 0.01160785
Iteration 8/25 | Loss: 0.01160785
Iteration 9/25 | Loss: 0.01160785
Iteration 10/25 | Loss: 0.01160785
Iteration 11/25 | Loss: 0.01160785
Iteration 12/25 | Loss: 0.01160785
Iteration 13/25 | Loss: 0.01160785
Iteration 14/25 | Loss: 0.01160785
Iteration 15/25 | Loss: 0.01160785
Iteration 16/25 | Loss: 0.01160785
Iteration 17/25 | Loss: 0.01160785
Iteration 18/25 | Loss: 0.01160785
Iteration 19/25 | Loss: 0.01160785
Iteration 20/25 | Loss: 0.01160785
Iteration 21/25 | Loss: 0.01160785
Iteration 22/25 | Loss: 0.01160785
Iteration 23/25 | Loss: 0.01160785
Iteration 24/25 | Loss: 0.01160785
Iteration 25/25 | Loss: 0.01160785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43617272
Iteration 2/25 | Loss: 0.00556196
Iteration 3/25 | Loss: 0.00555610
Iteration 4/25 | Loss: 0.00555610
Iteration 5/25 | Loss: 0.00555610
Iteration 6/25 | Loss: 0.00555610
Iteration 7/25 | Loss: 0.00555610
Iteration 8/25 | Loss: 0.00555609
Iteration 9/25 | Loss: 0.00555609
Iteration 10/25 | Loss: 0.00555609
Iteration 11/25 | Loss: 0.00555609
Iteration 12/25 | Loss: 0.00555609
Iteration 13/25 | Loss: 0.00555609
Iteration 14/25 | Loss: 0.00555609
Iteration 15/25 | Loss: 0.00555609
Iteration 16/25 | Loss: 0.00555609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005556093528866768, 0.005556093528866768, 0.005556093528866768, 0.005556093528866768, 0.005556093528866768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005556093528866768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00555609
Iteration 2/1000 | Loss: 0.00234651
Iteration 3/1000 | Loss: 0.00043603
Iteration 4/1000 | Loss: 0.00146264
Iteration 5/1000 | Loss: 0.00034185
Iteration 6/1000 | Loss: 0.00040730
Iteration 7/1000 | Loss: 0.00008044
Iteration 8/1000 | Loss: 0.00012486
Iteration 9/1000 | Loss: 0.00044154
Iteration 10/1000 | Loss: 0.00036290
Iteration 11/1000 | Loss: 0.00022868
Iteration 12/1000 | Loss: 0.00026118
Iteration 13/1000 | Loss: 0.00007571
Iteration 14/1000 | Loss: 0.00004305
Iteration 15/1000 | Loss: 0.00040385
Iteration 16/1000 | Loss: 0.00070971
Iteration 17/1000 | Loss: 0.00056169
Iteration 18/1000 | Loss: 0.00016356
Iteration 19/1000 | Loss: 0.00034012
Iteration 20/1000 | Loss: 0.00003751
Iteration 21/1000 | Loss: 0.00006832
Iteration 22/1000 | Loss: 0.00025028
Iteration 23/1000 | Loss: 0.00097908
Iteration 24/1000 | Loss: 0.00030325
Iteration 25/1000 | Loss: 0.00018663
Iteration 26/1000 | Loss: 0.00041650
Iteration 27/1000 | Loss: 0.00006226
Iteration 28/1000 | Loss: 0.00023041
Iteration 29/1000 | Loss: 0.00007813
Iteration 30/1000 | Loss: 0.00016437
Iteration 31/1000 | Loss: 0.00008198
Iteration 32/1000 | Loss: 0.00008729
Iteration 33/1000 | Loss: 0.00010419
Iteration 34/1000 | Loss: 0.00006298
Iteration 35/1000 | Loss: 0.00002719
Iteration 36/1000 | Loss: 0.00018729
Iteration 37/1000 | Loss: 0.00006332
Iteration 38/1000 | Loss: 0.00021089
Iteration 39/1000 | Loss: 0.00006072
Iteration 40/1000 | Loss: 0.00007276
Iteration 41/1000 | Loss: 0.00002572
Iteration 42/1000 | Loss: 0.00034013
Iteration 43/1000 | Loss: 0.00011785
Iteration 44/1000 | Loss: 0.00025173
Iteration 45/1000 | Loss: 0.00021188
Iteration 46/1000 | Loss: 0.00013811
Iteration 47/1000 | Loss: 0.00011330
Iteration 48/1000 | Loss: 0.00004569
Iteration 49/1000 | Loss: 0.00004110
Iteration 50/1000 | Loss: 0.00009181
Iteration 51/1000 | Loss: 0.00002486
Iteration 52/1000 | Loss: 0.00013136
Iteration 53/1000 | Loss: 0.00015629
Iteration 54/1000 | Loss: 0.00005527
Iteration 55/1000 | Loss: 0.00002468
Iteration 56/1000 | Loss: 0.00007518
Iteration 57/1000 | Loss: 0.00012575
Iteration 58/1000 | Loss: 0.00005302
Iteration 59/1000 | Loss: 0.00002443
Iteration 60/1000 | Loss: 0.00007197
Iteration 61/1000 | Loss: 0.00002423
Iteration 62/1000 | Loss: 0.00002423
Iteration 63/1000 | Loss: 0.00002423
Iteration 64/1000 | Loss: 0.00002423
Iteration 65/1000 | Loss: 0.00002423
Iteration 66/1000 | Loss: 0.00002423
Iteration 67/1000 | Loss: 0.00002423
Iteration 68/1000 | Loss: 0.00002422
Iteration 69/1000 | Loss: 0.00002422
Iteration 70/1000 | Loss: 0.00002422
Iteration 71/1000 | Loss: 0.00011509
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00011334
Iteration 74/1000 | Loss: 0.00002431
Iteration 75/1000 | Loss: 0.00002405
Iteration 76/1000 | Loss: 0.00002404
Iteration 77/1000 | Loss: 0.00002404
Iteration 78/1000 | Loss: 0.00002404
Iteration 79/1000 | Loss: 0.00002404
Iteration 80/1000 | Loss: 0.00002404
Iteration 81/1000 | Loss: 0.00002403
Iteration 82/1000 | Loss: 0.00002403
Iteration 83/1000 | Loss: 0.00002402
Iteration 84/1000 | Loss: 0.00002402
Iteration 85/1000 | Loss: 0.00002402
Iteration 86/1000 | Loss: 0.00002402
Iteration 87/1000 | Loss: 0.00002402
Iteration 88/1000 | Loss: 0.00002402
Iteration 89/1000 | Loss: 0.00002401
Iteration 90/1000 | Loss: 0.00002401
Iteration 91/1000 | Loss: 0.00002401
Iteration 92/1000 | Loss: 0.00002401
Iteration 93/1000 | Loss: 0.00002401
Iteration 94/1000 | Loss: 0.00002401
Iteration 95/1000 | Loss: 0.00002401
Iteration 96/1000 | Loss: 0.00002401
Iteration 97/1000 | Loss: 0.00002401
Iteration 98/1000 | Loss: 0.00002400
Iteration 99/1000 | Loss: 0.00002400
Iteration 100/1000 | Loss: 0.00002400
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00002400
Iteration 103/1000 | Loss: 0.00002400
Iteration 104/1000 | Loss: 0.00002399
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002399
Iteration 107/1000 | Loss: 0.00002399
Iteration 108/1000 | Loss: 0.00002399
Iteration 109/1000 | Loss: 0.00002399
Iteration 110/1000 | Loss: 0.00002398
Iteration 111/1000 | Loss: 0.00002398
Iteration 112/1000 | Loss: 0.00002398
Iteration 113/1000 | Loss: 0.00002398
Iteration 114/1000 | Loss: 0.00002397
Iteration 115/1000 | Loss: 0.00002397
Iteration 116/1000 | Loss: 0.00002397
Iteration 117/1000 | Loss: 0.00002397
Iteration 118/1000 | Loss: 0.00002397
Iteration 119/1000 | Loss: 0.00002397
Iteration 120/1000 | Loss: 0.00002397
Iteration 121/1000 | Loss: 0.00002397
Iteration 122/1000 | Loss: 0.00002396
Iteration 123/1000 | Loss: 0.00002396
Iteration 124/1000 | Loss: 0.00002396
Iteration 125/1000 | Loss: 0.00002396
Iteration 126/1000 | Loss: 0.00002396
Iteration 127/1000 | Loss: 0.00002395
Iteration 128/1000 | Loss: 0.00002395
Iteration 129/1000 | Loss: 0.00002395
Iteration 130/1000 | Loss: 0.00002394
Iteration 131/1000 | Loss: 0.00002394
Iteration 132/1000 | Loss: 0.00002393
Iteration 133/1000 | Loss: 0.00002393
Iteration 134/1000 | Loss: 0.00002393
Iteration 135/1000 | Loss: 0.00002392
Iteration 136/1000 | Loss: 0.00002392
Iteration 137/1000 | Loss: 0.00013902
Iteration 138/1000 | Loss: 0.00010569
Iteration 139/1000 | Loss: 0.00044336
Iteration 140/1000 | Loss: 0.00012396
Iteration 141/1000 | Loss: 0.00016999
Iteration 142/1000 | Loss: 0.00003885
Iteration 143/1000 | Loss: 0.00002416
Iteration 144/1000 | Loss: 0.00007014
Iteration 145/1000 | Loss: 0.00002874
Iteration 146/1000 | Loss: 0.00002398
Iteration 147/1000 | Loss: 0.00002391
Iteration 148/1000 | Loss: 0.00002391
Iteration 149/1000 | Loss: 0.00009871
Iteration 150/1000 | Loss: 0.00002427
Iteration 151/1000 | Loss: 0.00002391
Iteration 152/1000 | Loss: 0.00004620
Iteration 153/1000 | Loss: 0.00002392
Iteration 154/1000 | Loss: 0.00002387
Iteration 155/1000 | Loss: 0.00002386
Iteration 156/1000 | Loss: 0.00002385
Iteration 157/1000 | Loss: 0.00002385
Iteration 158/1000 | Loss: 0.00002385
Iteration 159/1000 | Loss: 0.00002385
Iteration 160/1000 | Loss: 0.00002385
Iteration 161/1000 | Loss: 0.00002385
Iteration 162/1000 | Loss: 0.00002384
Iteration 163/1000 | Loss: 0.00002384
Iteration 164/1000 | Loss: 0.00002384
Iteration 165/1000 | Loss: 0.00002384
Iteration 166/1000 | Loss: 0.00002384
Iteration 167/1000 | Loss: 0.00002384
Iteration 168/1000 | Loss: 0.00002384
Iteration 169/1000 | Loss: 0.00002384
Iteration 170/1000 | Loss: 0.00002384
Iteration 171/1000 | Loss: 0.00002384
Iteration 172/1000 | Loss: 0.00002384
Iteration 173/1000 | Loss: 0.00002383
Iteration 174/1000 | Loss: 0.00002383
Iteration 175/1000 | Loss: 0.00002383
Iteration 176/1000 | Loss: 0.00002383
Iteration 177/1000 | Loss: 0.00002383
Iteration 178/1000 | Loss: 0.00002383
Iteration 179/1000 | Loss: 0.00002383
Iteration 180/1000 | Loss: 0.00002383
Iteration 181/1000 | Loss: 0.00007722
Iteration 182/1000 | Loss: 0.00002388
Iteration 183/1000 | Loss: 0.00002387
Iteration 184/1000 | Loss: 0.00002386
Iteration 185/1000 | Loss: 0.00002386
Iteration 186/1000 | Loss: 0.00002386
Iteration 187/1000 | Loss: 0.00002386
Iteration 188/1000 | Loss: 0.00002385
Iteration 189/1000 | Loss: 0.00002385
Iteration 190/1000 | Loss: 0.00002385
Iteration 191/1000 | Loss: 0.00002384
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002384
Iteration 194/1000 | Loss: 0.00002384
Iteration 195/1000 | Loss: 0.00002384
Iteration 196/1000 | Loss: 0.00002384
Iteration 197/1000 | Loss: 0.00002384
Iteration 198/1000 | Loss: 0.00002384
Iteration 199/1000 | Loss: 0.00002384
Iteration 200/1000 | Loss: 0.00002384
Iteration 201/1000 | Loss: 0.00002384
Iteration 202/1000 | Loss: 0.00002384
Iteration 203/1000 | Loss: 0.00002384
Iteration 204/1000 | Loss: 0.00002384
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002383
Iteration 208/1000 | Loss: 0.00002383
Iteration 209/1000 | Loss: 0.00002383
Iteration 210/1000 | Loss: 0.00002383
Iteration 211/1000 | Loss: 0.00002383
Iteration 212/1000 | Loss: 0.00002383
Iteration 213/1000 | Loss: 0.00002383
Iteration 214/1000 | Loss: 0.00002383
Iteration 215/1000 | Loss: 0.00002383
Iteration 216/1000 | Loss: 0.00002383
Iteration 217/1000 | Loss: 0.00002383
Iteration 218/1000 | Loss: 0.00002383
Iteration 219/1000 | Loss: 0.00002383
Iteration 220/1000 | Loss: 0.00002383
Iteration 221/1000 | Loss: 0.00002383
Iteration 222/1000 | Loss: 0.00002383
Iteration 223/1000 | Loss: 0.00002382
Iteration 224/1000 | Loss: 0.00002382
Iteration 225/1000 | Loss: 0.00002382
Iteration 226/1000 | Loss: 0.00002382
Iteration 227/1000 | Loss: 0.00002382
Iteration 228/1000 | Loss: 0.00002382
Iteration 229/1000 | Loss: 0.00002382
Iteration 230/1000 | Loss: 0.00002382
Iteration 231/1000 | Loss: 0.00002382
Iteration 232/1000 | Loss: 0.00002382
Iteration 233/1000 | Loss: 0.00002382
Iteration 234/1000 | Loss: 0.00002382
Iteration 235/1000 | Loss: 0.00002382
Iteration 236/1000 | Loss: 0.00002382
Iteration 237/1000 | Loss: 0.00002382
Iteration 238/1000 | Loss: 0.00002382
Iteration 239/1000 | Loss: 0.00002382
Iteration 240/1000 | Loss: 0.00002382
Iteration 241/1000 | Loss: 0.00002382
Iteration 242/1000 | Loss: 0.00002382
Iteration 243/1000 | Loss: 0.00002382
Iteration 244/1000 | Loss: 0.00002382
Iteration 245/1000 | Loss: 0.00002382
Iteration 246/1000 | Loss: 0.00002382
Iteration 247/1000 | Loss: 0.00002382
Iteration 248/1000 | Loss: 0.00002382
Iteration 249/1000 | Loss: 0.00002382
Iteration 250/1000 | Loss: 0.00002382
Iteration 251/1000 | Loss: 0.00002382
Iteration 252/1000 | Loss: 0.00002382
Iteration 253/1000 | Loss: 0.00002382
Iteration 254/1000 | Loss: 0.00002382
Iteration 255/1000 | Loss: 0.00002382
Iteration 256/1000 | Loss: 0.00002382
Iteration 257/1000 | Loss: 0.00002382
Iteration 258/1000 | Loss: 0.00002382
Iteration 259/1000 | Loss: 0.00002382
Iteration 260/1000 | Loss: 0.00002382
Iteration 261/1000 | Loss: 0.00002382
Iteration 262/1000 | Loss: 0.00002382
Iteration 263/1000 | Loss: 0.00002382
Iteration 264/1000 | Loss: 0.00002382
Iteration 265/1000 | Loss: 0.00002382
Iteration 266/1000 | Loss: 0.00002382
Iteration 267/1000 | Loss: 0.00002382
Iteration 268/1000 | Loss: 0.00002382
Iteration 269/1000 | Loss: 0.00002382
Iteration 270/1000 | Loss: 0.00002382
Iteration 271/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [2.3819862690288574e-05, 2.3819862690288574e-05, 2.3819862690288574e-05, 2.3819862690288574e-05, 2.3819862690288574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3819862690288574e-05

Optimization complete. Final v2v error: 4.104726791381836 mm

Highest mean error: 4.483458995819092 mm for frame 1

Lowest mean error: 3.9500534534454346 mm for frame 140

Saving results

Total time: 122.86272406578064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417925
Iteration 2/25 | Loss: 0.00144094
Iteration 3/25 | Loss: 0.00136355
Iteration 4/25 | Loss: 0.00134811
Iteration 5/25 | Loss: 0.00134273
Iteration 6/25 | Loss: 0.00134075
Iteration 7/25 | Loss: 0.00134040
Iteration 8/25 | Loss: 0.00134040
Iteration 9/25 | Loss: 0.00134040
Iteration 10/25 | Loss: 0.00134040
Iteration 11/25 | Loss: 0.00134040
Iteration 12/25 | Loss: 0.00134040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013403986813500524, 0.0013403986813500524, 0.0013403986813500524, 0.0013403986813500524, 0.0013403986813500524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013403986813500524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67675877
Iteration 2/25 | Loss: 0.00244364
Iteration 3/25 | Loss: 0.00244364
Iteration 4/25 | Loss: 0.00244364
Iteration 5/25 | Loss: 0.00244364
Iteration 6/25 | Loss: 0.00244364
Iteration 7/25 | Loss: 0.00244364
Iteration 8/25 | Loss: 0.00244364
Iteration 9/25 | Loss: 0.00244364
Iteration 10/25 | Loss: 0.00244364
Iteration 11/25 | Loss: 0.00244364
Iteration 12/25 | Loss: 0.00244364
Iteration 13/25 | Loss: 0.00244364
Iteration 14/25 | Loss: 0.00244364
Iteration 15/25 | Loss: 0.00244364
Iteration 16/25 | Loss: 0.00244364
Iteration 17/25 | Loss: 0.00244364
Iteration 18/25 | Loss: 0.00244364
Iteration 19/25 | Loss: 0.00244364
Iteration 20/25 | Loss: 0.00244364
Iteration 21/25 | Loss: 0.00244364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024436372332274914, 0.0024436372332274914, 0.0024436372332274914, 0.0024436372332274914, 0.0024436372332274914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024436372332274914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244364
Iteration 2/1000 | Loss: 0.00003788
Iteration 3/1000 | Loss: 0.00002343
Iteration 4/1000 | Loss: 0.00002157
Iteration 5/1000 | Loss: 0.00002060
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001942
Iteration 8/1000 | Loss: 0.00001921
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001899
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001880
Iteration 13/1000 | Loss: 0.00001877
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001860
Iteration 19/1000 | Loss: 0.00001860
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001858
Iteration 23/1000 | Loss: 0.00001857
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001856
Iteration 26/1000 | Loss: 0.00001856
Iteration 27/1000 | Loss: 0.00001855
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00001855
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001855
Iteration 32/1000 | Loss: 0.00001854
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001854
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001853
Iteration 41/1000 | Loss: 0.00001853
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001853
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001853
Iteration 46/1000 | Loss: 0.00001853
Iteration 47/1000 | Loss: 0.00001852
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001852
Iteration 50/1000 | Loss: 0.00001852
Iteration 51/1000 | Loss: 0.00001851
Iteration 52/1000 | Loss: 0.00001851
Iteration 53/1000 | Loss: 0.00001851
Iteration 54/1000 | Loss: 0.00001851
Iteration 55/1000 | Loss: 0.00001851
Iteration 56/1000 | Loss: 0.00001851
Iteration 57/1000 | Loss: 0.00001851
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001850
Iteration 60/1000 | Loss: 0.00001850
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001850
Iteration 63/1000 | Loss: 0.00001850
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001849
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001849
Iteration 69/1000 | Loss: 0.00001849
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001848
Iteration 72/1000 | Loss: 0.00001848
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001848
Iteration 80/1000 | Loss: 0.00001848
Iteration 81/1000 | Loss: 0.00001848
Iteration 82/1000 | Loss: 0.00001848
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001847
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001846
Iteration 95/1000 | Loss: 0.00001846
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001846
Iteration 102/1000 | Loss: 0.00001845
Iteration 103/1000 | Loss: 0.00001845
Iteration 104/1000 | Loss: 0.00001845
Iteration 105/1000 | Loss: 0.00001845
Iteration 106/1000 | Loss: 0.00001845
Iteration 107/1000 | Loss: 0.00001845
Iteration 108/1000 | Loss: 0.00001845
Iteration 109/1000 | Loss: 0.00001845
Iteration 110/1000 | Loss: 0.00001845
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001845
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001845
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.8448061382514425e-05, 1.8448061382514425e-05, 1.8448061382514425e-05, 1.8448061382514425e-05, 1.8448061382514425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8448061382514425e-05

Optimization complete. Final v2v error: 3.7951526641845703 mm

Highest mean error: 4.145691871643066 mm for frame 167

Lowest mean error: 3.5938408374786377 mm for frame 37

Saving results

Total time: 33.207595348358154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122655
Iteration 2/25 | Loss: 0.00202077
Iteration 3/25 | Loss: 0.00157342
Iteration 4/25 | Loss: 0.00148080
Iteration 5/25 | Loss: 0.00147435
Iteration 6/25 | Loss: 0.00147399
Iteration 7/25 | Loss: 0.00147399
Iteration 8/25 | Loss: 0.00147399
Iteration 9/25 | Loss: 0.00147399
Iteration 10/25 | Loss: 0.00147399
Iteration 11/25 | Loss: 0.00147399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001473993412218988, 0.001473993412218988, 0.001473993412218988, 0.001473993412218988, 0.001473993412218988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001473993412218988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59315276
Iteration 2/25 | Loss: 0.00216278
Iteration 3/25 | Loss: 0.00216278
Iteration 4/25 | Loss: 0.00216278
Iteration 5/25 | Loss: 0.00216278
Iteration 6/25 | Loss: 0.00216278
Iteration 7/25 | Loss: 0.00216278
Iteration 8/25 | Loss: 0.00216278
Iteration 9/25 | Loss: 0.00216278
Iteration 10/25 | Loss: 0.00216278
Iteration 11/25 | Loss: 0.00216278
Iteration 12/25 | Loss: 0.00216278
Iteration 13/25 | Loss: 0.00216278
Iteration 14/25 | Loss: 0.00216278
Iteration 15/25 | Loss: 0.00216278
Iteration 16/25 | Loss: 0.00216278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021627761889249086, 0.0021627761889249086, 0.0021627761889249086, 0.0021627761889249086, 0.0021627761889249086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021627761889249086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216278
Iteration 2/1000 | Loss: 0.00004013
Iteration 3/1000 | Loss: 0.00003352
Iteration 4/1000 | Loss: 0.00003086
Iteration 5/1000 | Loss: 0.00002908
Iteration 6/1000 | Loss: 0.00002819
Iteration 7/1000 | Loss: 0.00002744
Iteration 8/1000 | Loss: 0.00002708
Iteration 9/1000 | Loss: 0.00002690
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002681
Iteration 12/1000 | Loss: 0.00002673
Iteration 13/1000 | Loss: 0.00002666
Iteration 14/1000 | Loss: 0.00002664
Iteration 15/1000 | Loss: 0.00002664
Iteration 16/1000 | Loss: 0.00002664
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00002664
Iteration 19/1000 | Loss: 0.00002664
Iteration 20/1000 | Loss: 0.00002664
Iteration 21/1000 | Loss: 0.00002664
Iteration 22/1000 | Loss: 0.00002664
Iteration 23/1000 | Loss: 0.00002663
Iteration 24/1000 | Loss: 0.00002663
Iteration 25/1000 | Loss: 0.00002663
Iteration 26/1000 | Loss: 0.00002663
Iteration 27/1000 | Loss: 0.00002662
Iteration 28/1000 | Loss: 0.00002662
Iteration 29/1000 | Loss: 0.00002661
Iteration 30/1000 | Loss: 0.00002660
Iteration 31/1000 | Loss: 0.00002660
Iteration 32/1000 | Loss: 0.00002659
Iteration 33/1000 | Loss: 0.00002659
Iteration 34/1000 | Loss: 0.00002659
Iteration 35/1000 | Loss: 0.00002659
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002658
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002658
Iteration 41/1000 | Loss: 0.00002658
Iteration 42/1000 | Loss: 0.00002658
Iteration 43/1000 | Loss: 0.00002657
Iteration 44/1000 | Loss: 0.00002657
Iteration 45/1000 | Loss: 0.00002657
Iteration 46/1000 | Loss: 0.00002657
Iteration 47/1000 | Loss: 0.00002657
Iteration 48/1000 | Loss: 0.00002656
Iteration 49/1000 | Loss: 0.00002656
Iteration 50/1000 | Loss: 0.00002656
Iteration 51/1000 | Loss: 0.00002656
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002656
Iteration 55/1000 | Loss: 0.00002656
Iteration 56/1000 | Loss: 0.00002655
Iteration 57/1000 | Loss: 0.00002655
Iteration 58/1000 | Loss: 0.00002655
Iteration 59/1000 | Loss: 0.00002655
Iteration 60/1000 | Loss: 0.00002655
Iteration 61/1000 | Loss: 0.00002655
Iteration 62/1000 | Loss: 0.00002655
Iteration 63/1000 | Loss: 0.00002654
Iteration 64/1000 | Loss: 0.00002654
Iteration 65/1000 | Loss: 0.00002654
Iteration 66/1000 | Loss: 0.00002653
Iteration 67/1000 | Loss: 0.00002653
Iteration 68/1000 | Loss: 0.00002653
Iteration 69/1000 | Loss: 0.00002653
Iteration 70/1000 | Loss: 0.00002653
Iteration 71/1000 | Loss: 0.00002653
Iteration 72/1000 | Loss: 0.00002653
Iteration 73/1000 | Loss: 0.00002652
Iteration 74/1000 | Loss: 0.00002652
Iteration 75/1000 | Loss: 0.00002652
Iteration 76/1000 | Loss: 0.00002652
Iteration 77/1000 | Loss: 0.00002652
Iteration 78/1000 | Loss: 0.00002652
Iteration 79/1000 | Loss: 0.00002652
Iteration 80/1000 | Loss: 0.00002652
Iteration 81/1000 | Loss: 0.00002652
Iteration 82/1000 | Loss: 0.00002651
Iteration 83/1000 | Loss: 0.00002651
Iteration 84/1000 | Loss: 0.00002651
Iteration 85/1000 | Loss: 0.00002651
Iteration 86/1000 | Loss: 0.00002651
Iteration 87/1000 | Loss: 0.00002651
Iteration 88/1000 | Loss: 0.00002650
Iteration 89/1000 | Loss: 0.00002649
Iteration 90/1000 | Loss: 0.00002649
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002646
Iteration 94/1000 | Loss: 0.00002646
Iteration 95/1000 | Loss: 0.00002646
Iteration 96/1000 | Loss: 0.00002646
Iteration 97/1000 | Loss: 0.00002646
Iteration 98/1000 | Loss: 0.00002646
Iteration 99/1000 | Loss: 0.00002646
Iteration 100/1000 | Loss: 0.00002646
Iteration 101/1000 | Loss: 0.00002646
Iteration 102/1000 | Loss: 0.00002646
Iteration 103/1000 | Loss: 0.00002645
Iteration 104/1000 | Loss: 0.00002645
Iteration 105/1000 | Loss: 0.00002645
Iteration 106/1000 | Loss: 0.00002645
Iteration 107/1000 | Loss: 0.00002644
Iteration 108/1000 | Loss: 0.00002644
Iteration 109/1000 | Loss: 0.00002644
Iteration 110/1000 | Loss: 0.00002644
Iteration 111/1000 | Loss: 0.00002644
Iteration 112/1000 | Loss: 0.00002644
Iteration 113/1000 | Loss: 0.00002644
Iteration 114/1000 | Loss: 0.00002644
Iteration 115/1000 | Loss: 0.00002643
Iteration 116/1000 | Loss: 0.00002643
Iteration 117/1000 | Loss: 0.00002643
Iteration 118/1000 | Loss: 0.00002643
Iteration 119/1000 | Loss: 0.00002642
Iteration 120/1000 | Loss: 0.00002642
Iteration 121/1000 | Loss: 0.00002642
Iteration 122/1000 | Loss: 0.00002642
Iteration 123/1000 | Loss: 0.00002642
Iteration 124/1000 | Loss: 0.00002642
Iteration 125/1000 | Loss: 0.00002642
Iteration 126/1000 | Loss: 0.00002642
Iteration 127/1000 | Loss: 0.00002642
Iteration 128/1000 | Loss: 0.00002642
Iteration 129/1000 | Loss: 0.00002642
Iteration 130/1000 | Loss: 0.00002642
Iteration 131/1000 | Loss: 0.00002642
Iteration 132/1000 | Loss: 0.00002642
Iteration 133/1000 | Loss: 0.00002642
Iteration 134/1000 | Loss: 0.00002642
Iteration 135/1000 | Loss: 0.00002642
Iteration 136/1000 | Loss: 0.00002641
Iteration 137/1000 | Loss: 0.00002641
Iteration 138/1000 | Loss: 0.00002641
Iteration 139/1000 | Loss: 0.00002641
Iteration 140/1000 | Loss: 0.00002641
Iteration 141/1000 | Loss: 0.00002641
Iteration 142/1000 | Loss: 0.00002641
Iteration 143/1000 | Loss: 0.00002641
Iteration 144/1000 | Loss: 0.00002641
Iteration 145/1000 | Loss: 0.00002641
Iteration 146/1000 | Loss: 0.00002641
Iteration 147/1000 | Loss: 0.00002641
Iteration 148/1000 | Loss: 0.00002641
Iteration 149/1000 | Loss: 0.00002641
Iteration 150/1000 | Loss: 0.00002641
Iteration 151/1000 | Loss: 0.00002641
Iteration 152/1000 | Loss: 0.00002641
Iteration 153/1000 | Loss: 0.00002641
Iteration 154/1000 | Loss: 0.00002641
Iteration 155/1000 | Loss: 0.00002641
Iteration 156/1000 | Loss: 0.00002641
Iteration 157/1000 | Loss: 0.00002641
Iteration 158/1000 | Loss: 0.00002641
Iteration 159/1000 | Loss: 0.00002641
Iteration 160/1000 | Loss: 0.00002641
Iteration 161/1000 | Loss: 0.00002641
Iteration 162/1000 | Loss: 0.00002641
Iteration 163/1000 | Loss: 0.00002641
Iteration 164/1000 | Loss: 0.00002641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.6411255021230318e-05, 2.6411255021230318e-05, 2.6411255021230318e-05, 2.6411255021230318e-05, 2.6411255021230318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6411255021230318e-05

Optimization complete. Final v2v error: 4.319475173950195 mm

Highest mean error: 4.666691303253174 mm for frame 172

Lowest mean error: 4.0737504959106445 mm for frame 124

Saving results

Total time: 31.92686152458191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030760
Iteration 2/25 | Loss: 0.00191166
Iteration 3/25 | Loss: 0.00156372
Iteration 4/25 | Loss: 0.00152997
Iteration 5/25 | Loss: 0.00151329
Iteration 6/25 | Loss: 0.00151117
Iteration 7/25 | Loss: 0.00151087
Iteration 8/25 | Loss: 0.00151087
Iteration 9/25 | Loss: 0.00151087
Iteration 10/25 | Loss: 0.00151087
Iteration 11/25 | Loss: 0.00151087
Iteration 12/25 | Loss: 0.00151087
Iteration 13/25 | Loss: 0.00151087
Iteration 14/25 | Loss: 0.00151087
Iteration 15/25 | Loss: 0.00151087
Iteration 16/25 | Loss: 0.00151087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015108747174963355, 0.0015108747174963355, 0.0015108747174963355, 0.0015108747174963355, 0.0015108747174963355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015108747174963355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21231401
Iteration 2/25 | Loss: 0.00219939
Iteration 3/25 | Loss: 0.00219937
Iteration 4/25 | Loss: 0.00219937
Iteration 5/25 | Loss: 0.00219937
Iteration 6/25 | Loss: 0.00219937
Iteration 7/25 | Loss: 0.00219937
Iteration 8/25 | Loss: 0.00219937
Iteration 9/25 | Loss: 0.00219937
Iteration 10/25 | Loss: 0.00219937
Iteration 11/25 | Loss: 0.00219937
Iteration 12/25 | Loss: 0.00219937
Iteration 13/25 | Loss: 0.00219937
Iteration 14/25 | Loss: 0.00219937
Iteration 15/25 | Loss: 0.00219937
Iteration 16/25 | Loss: 0.00219937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021993692498654127, 0.0021993692498654127, 0.0021993692498654127, 0.0021993692498654127, 0.0021993692498654127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021993692498654127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219937
Iteration 2/1000 | Loss: 0.00006447
Iteration 3/1000 | Loss: 0.00004904
Iteration 4/1000 | Loss: 0.00004428
Iteration 5/1000 | Loss: 0.00004216
Iteration 6/1000 | Loss: 0.00004083
Iteration 7/1000 | Loss: 0.00003989
Iteration 8/1000 | Loss: 0.00003910
Iteration 9/1000 | Loss: 0.00003847
Iteration 10/1000 | Loss: 0.00003813
Iteration 11/1000 | Loss: 0.00003783
Iteration 12/1000 | Loss: 0.00003764
Iteration 13/1000 | Loss: 0.00003750
Iteration 14/1000 | Loss: 0.00003736
Iteration 15/1000 | Loss: 0.00003728
Iteration 16/1000 | Loss: 0.00003725
Iteration 17/1000 | Loss: 0.00003725
Iteration 18/1000 | Loss: 0.00003724
Iteration 19/1000 | Loss: 0.00003723
Iteration 20/1000 | Loss: 0.00003723
Iteration 21/1000 | Loss: 0.00003722
Iteration 22/1000 | Loss: 0.00003721
Iteration 23/1000 | Loss: 0.00003719
Iteration 24/1000 | Loss: 0.00003719
Iteration 25/1000 | Loss: 0.00003718
Iteration 26/1000 | Loss: 0.00003716
Iteration 27/1000 | Loss: 0.00003716
Iteration 28/1000 | Loss: 0.00003716
Iteration 29/1000 | Loss: 0.00003715
Iteration 30/1000 | Loss: 0.00003715
Iteration 31/1000 | Loss: 0.00003713
Iteration 32/1000 | Loss: 0.00003711
Iteration 33/1000 | Loss: 0.00003709
Iteration 34/1000 | Loss: 0.00003709
Iteration 35/1000 | Loss: 0.00003709
Iteration 36/1000 | Loss: 0.00003708
Iteration 37/1000 | Loss: 0.00003708
Iteration 38/1000 | Loss: 0.00003707
Iteration 39/1000 | Loss: 0.00003707
Iteration 40/1000 | Loss: 0.00003706
Iteration 41/1000 | Loss: 0.00003706
Iteration 42/1000 | Loss: 0.00003706
Iteration 43/1000 | Loss: 0.00003706
Iteration 44/1000 | Loss: 0.00003706
Iteration 45/1000 | Loss: 0.00003706
Iteration 46/1000 | Loss: 0.00003705
Iteration 47/1000 | Loss: 0.00003705
Iteration 48/1000 | Loss: 0.00003703
Iteration 49/1000 | Loss: 0.00003703
Iteration 50/1000 | Loss: 0.00003703
Iteration 51/1000 | Loss: 0.00003703
Iteration 52/1000 | Loss: 0.00003703
Iteration 53/1000 | Loss: 0.00003702
Iteration 54/1000 | Loss: 0.00003702
Iteration 55/1000 | Loss: 0.00003702
Iteration 56/1000 | Loss: 0.00003702
Iteration 57/1000 | Loss: 0.00003701
Iteration 58/1000 | Loss: 0.00003700
Iteration 59/1000 | Loss: 0.00003700
Iteration 60/1000 | Loss: 0.00003700
Iteration 61/1000 | Loss: 0.00003699
Iteration 62/1000 | Loss: 0.00003699
Iteration 63/1000 | Loss: 0.00003699
Iteration 64/1000 | Loss: 0.00003699
Iteration 65/1000 | Loss: 0.00003699
Iteration 66/1000 | Loss: 0.00003699
Iteration 67/1000 | Loss: 0.00003698
Iteration 68/1000 | Loss: 0.00003698
Iteration 69/1000 | Loss: 0.00003698
Iteration 70/1000 | Loss: 0.00003697
Iteration 71/1000 | Loss: 0.00003697
Iteration 72/1000 | Loss: 0.00003697
Iteration 73/1000 | Loss: 0.00003697
Iteration 74/1000 | Loss: 0.00003696
Iteration 75/1000 | Loss: 0.00003696
Iteration 76/1000 | Loss: 0.00003696
Iteration 77/1000 | Loss: 0.00003696
Iteration 78/1000 | Loss: 0.00003696
Iteration 79/1000 | Loss: 0.00003696
Iteration 80/1000 | Loss: 0.00003696
Iteration 81/1000 | Loss: 0.00003696
Iteration 82/1000 | Loss: 0.00003695
Iteration 83/1000 | Loss: 0.00003695
Iteration 84/1000 | Loss: 0.00003695
Iteration 85/1000 | Loss: 0.00003695
Iteration 86/1000 | Loss: 0.00003695
Iteration 87/1000 | Loss: 0.00003694
Iteration 88/1000 | Loss: 0.00003694
Iteration 89/1000 | Loss: 0.00003694
Iteration 90/1000 | Loss: 0.00003694
Iteration 91/1000 | Loss: 0.00003694
Iteration 92/1000 | Loss: 0.00003694
Iteration 93/1000 | Loss: 0.00003694
Iteration 94/1000 | Loss: 0.00003694
Iteration 95/1000 | Loss: 0.00003694
Iteration 96/1000 | Loss: 0.00003693
Iteration 97/1000 | Loss: 0.00003693
Iteration 98/1000 | Loss: 0.00003693
Iteration 99/1000 | Loss: 0.00003693
Iteration 100/1000 | Loss: 0.00003693
Iteration 101/1000 | Loss: 0.00003693
Iteration 102/1000 | Loss: 0.00003693
Iteration 103/1000 | Loss: 0.00003692
Iteration 104/1000 | Loss: 0.00003692
Iteration 105/1000 | Loss: 0.00003692
Iteration 106/1000 | Loss: 0.00003692
Iteration 107/1000 | Loss: 0.00003692
Iteration 108/1000 | Loss: 0.00003692
Iteration 109/1000 | Loss: 0.00003692
Iteration 110/1000 | Loss: 0.00003692
Iteration 111/1000 | Loss: 0.00003692
Iteration 112/1000 | Loss: 0.00003692
Iteration 113/1000 | Loss: 0.00003691
Iteration 114/1000 | Loss: 0.00003691
Iteration 115/1000 | Loss: 0.00003691
Iteration 116/1000 | Loss: 0.00003691
Iteration 117/1000 | Loss: 0.00003691
Iteration 118/1000 | Loss: 0.00003691
Iteration 119/1000 | Loss: 0.00003691
Iteration 120/1000 | Loss: 0.00003691
Iteration 121/1000 | Loss: 0.00003691
Iteration 122/1000 | Loss: 0.00003691
Iteration 123/1000 | Loss: 0.00003691
Iteration 124/1000 | Loss: 0.00003691
Iteration 125/1000 | Loss: 0.00003690
Iteration 126/1000 | Loss: 0.00003690
Iteration 127/1000 | Loss: 0.00003690
Iteration 128/1000 | Loss: 0.00003690
Iteration 129/1000 | Loss: 0.00003690
Iteration 130/1000 | Loss: 0.00003690
Iteration 131/1000 | Loss: 0.00003690
Iteration 132/1000 | Loss: 0.00003690
Iteration 133/1000 | Loss: 0.00003690
Iteration 134/1000 | Loss: 0.00003690
Iteration 135/1000 | Loss: 0.00003690
Iteration 136/1000 | Loss: 0.00003690
Iteration 137/1000 | Loss: 0.00003689
Iteration 138/1000 | Loss: 0.00003689
Iteration 139/1000 | Loss: 0.00003689
Iteration 140/1000 | Loss: 0.00003689
Iteration 141/1000 | Loss: 0.00003689
Iteration 142/1000 | Loss: 0.00003689
Iteration 143/1000 | Loss: 0.00003689
Iteration 144/1000 | Loss: 0.00003689
Iteration 145/1000 | Loss: 0.00003688
Iteration 146/1000 | Loss: 0.00003688
Iteration 147/1000 | Loss: 0.00003688
Iteration 148/1000 | Loss: 0.00003688
Iteration 149/1000 | Loss: 0.00003688
Iteration 150/1000 | Loss: 0.00003688
Iteration 151/1000 | Loss: 0.00003688
Iteration 152/1000 | Loss: 0.00003687
Iteration 153/1000 | Loss: 0.00003687
Iteration 154/1000 | Loss: 0.00003687
Iteration 155/1000 | Loss: 0.00003687
Iteration 156/1000 | Loss: 0.00003687
Iteration 157/1000 | Loss: 0.00003687
Iteration 158/1000 | Loss: 0.00003687
Iteration 159/1000 | Loss: 0.00003687
Iteration 160/1000 | Loss: 0.00003687
Iteration 161/1000 | Loss: 0.00003687
Iteration 162/1000 | Loss: 0.00003686
Iteration 163/1000 | Loss: 0.00003686
Iteration 164/1000 | Loss: 0.00003686
Iteration 165/1000 | Loss: 0.00003686
Iteration 166/1000 | Loss: 0.00003686
Iteration 167/1000 | Loss: 0.00003686
Iteration 168/1000 | Loss: 0.00003686
Iteration 169/1000 | Loss: 0.00003686
Iteration 170/1000 | Loss: 0.00003686
Iteration 171/1000 | Loss: 0.00003685
Iteration 172/1000 | Loss: 0.00003685
Iteration 173/1000 | Loss: 0.00003685
Iteration 174/1000 | Loss: 0.00003685
Iteration 175/1000 | Loss: 0.00003685
Iteration 176/1000 | Loss: 0.00003685
Iteration 177/1000 | Loss: 0.00003685
Iteration 178/1000 | Loss: 0.00003685
Iteration 179/1000 | Loss: 0.00003685
Iteration 180/1000 | Loss: 0.00003684
Iteration 181/1000 | Loss: 0.00003684
Iteration 182/1000 | Loss: 0.00003684
Iteration 183/1000 | Loss: 0.00003684
Iteration 184/1000 | Loss: 0.00003684
Iteration 185/1000 | Loss: 0.00003684
Iteration 186/1000 | Loss: 0.00003684
Iteration 187/1000 | Loss: 0.00003684
Iteration 188/1000 | Loss: 0.00003684
Iteration 189/1000 | Loss: 0.00003684
Iteration 190/1000 | Loss: 0.00003684
Iteration 191/1000 | Loss: 0.00003684
Iteration 192/1000 | Loss: 0.00003683
Iteration 193/1000 | Loss: 0.00003683
Iteration 194/1000 | Loss: 0.00003683
Iteration 195/1000 | Loss: 0.00003683
Iteration 196/1000 | Loss: 0.00003683
Iteration 197/1000 | Loss: 0.00003683
Iteration 198/1000 | Loss: 0.00003683
Iteration 199/1000 | Loss: 0.00003683
Iteration 200/1000 | Loss: 0.00003683
Iteration 201/1000 | Loss: 0.00003683
Iteration 202/1000 | Loss: 0.00003683
Iteration 203/1000 | Loss: 0.00003683
Iteration 204/1000 | Loss: 0.00003682
Iteration 205/1000 | Loss: 0.00003682
Iteration 206/1000 | Loss: 0.00003682
Iteration 207/1000 | Loss: 0.00003682
Iteration 208/1000 | Loss: 0.00003682
Iteration 209/1000 | Loss: 0.00003682
Iteration 210/1000 | Loss: 0.00003682
Iteration 211/1000 | Loss: 0.00003682
Iteration 212/1000 | Loss: 0.00003682
Iteration 213/1000 | Loss: 0.00003682
Iteration 214/1000 | Loss: 0.00003682
Iteration 215/1000 | Loss: 0.00003682
Iteration 216/1000 | Loss: 0.00003682
Iteration 217/1000 | Loss: 0.00003682
Iteration 218/1000 | Loss: 0.00003682
Iteration 219/1000 | Loss: 0.00003681
Iteration 220/1000 | Loss: 0.00003681
Iteration 221/1000 | Loss: 0.00003681
Iteration 222/1000 | Loss: 0.00003681
Iteration 223/1000 | Loss: 0.00003681
Iteration 224/1000 | Loss: 0.00003681
Iteration 225/1000 | Loss: 0.00003681
Iteration 226/1000 | Loss: 0.00003681
Iteration 227/1000 | Loss: 0.00003681
Iteration 228/1000 | Loss: 0.00003681
Iteration 229/1000 | Loss: 0.00003681
Iteration 230/1000 | Loss: 0.00003681
Iteration 231/1000 | Loss: 0.00003681
Iteration 232/1000 | Loss: 0.00003681
Iteration 233/1000 | Loss: 0.00003681
Iteration 234/1000 | Loss: 0.00003681
Iteration 235/1000 | Loss: 0.00003681
Iteration 236/1000 | Loss: 0.00003681
Iteration 237/1000 | Loss: 0.00003681
Iteration 238/1000 | Loss: 0.00003681
Iteration 239/1000 | Loss: 0.00003681
Iteration 240/1000 | Loss: 0.00003681
Iteration 241/1000 | Loss: 0.00003681
Iteration 242/1000 | Loss: 0.00003681
Iteration 243/1000 | Loss: 0.00003681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [3.681457383208908e-05, 3.681457383208908e-05, 3.681457383208908e-05, 3.681457383208908e-05, 3.681457383208908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.681457383208908e-05

Optimization complete. Final v2v error: 5.040353775024414 mm

Highest mean error: 6.093730449676514 mm for frame 100

Lowest mean error: 4.0513458251953125 mm for frame 123

Saving results

Total time: 46.31158232688904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821334
Iteration 2/25 | Loss: 0.00182728
Iteration 3/25 | Loss: 0.00161467
Iteration 4/25 | Loss: 0.00157082
Iteration 5/25 | Loss: 0.00155938
Iteration 6/25 | Loss: 0.00155910
Iteration 7/25 | Loss: 0.00155312
Iteration 8/25 | Loss: 0.00155738
Iteration 9/25 | Loss: 0.00155341
Iteration 10/25 | Loss: 0.00155227
Iteration 11/25 | Loss: 0.00155207
Iteration 12/25 | Loss: 0.00155380
Iteration 13/25 | Loss: 0.00155022
Iteration 14/25 | Loss: 0.00155398
Iteration 15/25 | Loss: 0.00155337
Iteration 16/25 | Loss: 0.00155084
Iteration 17/25 | Loss: 0.00155521
Iteration 18/25 | Loss: 0.00155256
Iteration 19/25 | Loss: 0.00154760
Iteration 20/25 | Loss: 0.00154761
Iteration 21/25 | Loss: 0.00154700
Iteration 22/25 | Loss: 0.00154889
Iteration 23/25 | Loss: 0.00154777
Iteration 24/25 | Loss: 0.00154559
Iteration 25/25 | Loss: 0.00154818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51309097
Iteration 2/25 | Loss: 0.00337324
Iteration 3/25 | Loss: 0.00337320
Iteration 4/25 | Loss: 0.00337319
Iteration 5/25 | Loss: 0.00337319
Iteration 6/25 | Loss: 0.00337319
Iteration 7/25 | Loss: 0.00337319
Iteration 8/25 | Loss: 0.00337319
Iteration 9/25 | Loss: 0.00337319
Iteration 10/25 | Loss: 0.00337319
Iteration 11/25 | Loss: 0.00337319
Iteration 12/25 | Loss: 0.00337319
Iteration 13/25 | Loss: 0.00337319
Iteration 14/25 | Loss: 0.00337319
Iteration 15/25 | Loss: 0.00337319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0033731935545802116, 0.0033731935545802116, 0.0033731935545802116, 0.0033731935545802116, 0.0033731935545802116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033731935545802116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00337319
Iteration 2/1000 | Loss: 0.00014941
Iteration 3/1000 | Loss: 0.00015708
Iteration 4/1000 | Loss: 0.00026999
Iteration 5/1000 | Loss: 0.00017540
Iteration 6/1000 | Loss: 0.00008995
Iteration 7/1000 | Loss: 0.00015950
Iteration 8/1000 | Loss: 0.00086833
Iteration 9/1000 | Loss: 0.00029788
Iteration 10/1000 | Loss: 0.00095311
Iteration 11/1000 | Loss: 0.00037758
Iteration 12/1000 | Loss: 0.00178417
Iteration 13/1000 | Loss: 0.00059523
Iteration 14/1000 | Loss: 0.00237512
Iteration 15/1000 | Loss: 0.00082661
Iteration 16/1000 | Loss: 0.00015161
Iteration 17/1000 | Loss: 0.00018661
Iteration 18/1000 | Loss: 0.00009680
Iteration 19/1000 | Loss: 0.00533455
Iteration 20/1000 | Loss: 0.00096242
Iteration 21/1000 | Loss: 0.00539450
Iteration 22/1000 | Loss: 0.00031993
Iteration 23/1000 | Loss: 0.00010400
Iteration 24/1000 | Loss: 0.00006110
Iteration 25/1000 | Loss: 0.00004871
Iteration 26/1000 | Loss: 0.00004423
Iteration 27/1000 | Loss: 0.00004219
Iteration 28/1000 | Loss: 0.00004088
Iteration 29/1000 | Loss: 0.00003966
Iteration 30/1000 | Loss: 0.00003906
Iteration 31/1000 | Loss: 0.00003869
Iteration 32/1000 | Loss: 0.00003838
Iteration 33/1000 | Loss: 0.00003802
Iteration 34/1000 | Loss: 0.00003783
Iteration 35/1000 | Loss: 0.00003770
Iteration 36/1000 | Loss: 0.00003762
Iteration 37/1000 | Loss: 0.00003760
Iteration 38/1000 | Loss: 0.00003757
Iteration 39/1000 | Loss: 0.00003757
Iteration 40/1000 | Loss: 0.00003756
Iteration 41/1000 | Loss: 0.00003754
Iteration 42/1000 | Loss: 0.00003753
Iteration 43/1000 | Loss: 0.00003753
Iteration 44/1000 | Loss: 0.00003747
Iteration 45/1000 | Loss: 0.00003746
Iteration 46/1000 | Loss: 0.00003745
Iteration 47/1000 | Loss: 0.00003742
Iteration 48/1000 | Loss: 0.00003741
Iteration 49/1000 | Loss: 0.00003741
Iteration 50/1000 | Loss: 0.00003734
Iteration 51/1000 | Loss: 0.00003733
Iteration 52/1000 | Loss: 0.00003733
Iteration 53/1000 | Loss: 0.00003733
Iteration 54/1000 | Loss: 0.00003733
Iteration 55/1000 | Loss: 0.00003733
Iteration 56/1000 | Loss: 0.00003733
Iteration 57/1000 | Loss: 0.00003733
Iteration 58/1000 | Loss: 0.00003733
Iteration 59/1000 | Loss: 0.00003733
Iteration 60/1000 | Loss: 0.00003732
Iteration 61/1000 | Loss: 0.00003732
Iteration 62/1000 | Loss: 0.00003732
Iteration 63/1000 | Loss: 0.00003732
Iteration 64/1000 | Loss: 0.00003732
Iteration 65/1000 | Loss: 0.00003732
Iteration 66/1000 | Loss: 0.00003732
Iteration 67/1000 | Loss: 0.00003731
Iteration 68/1000 | Loss: 0.00003731
Iteration 69/1000 | Loss: 0.00003729
Iteration 70/1000 | Loss: 0.00003729
Iteration 71/1000 | Loss: 0.00003729
Iteration 72/1000 | Loss: 0.00003728
Iteration 73/1000 | Loss: 0.00003728
Iteration 74/1000 | Loss: 0.00003728
Iteration 75/1000 | Loss: 0.00003728
Iteration 76/1000 | Loss: 0.00003728
Iteration 77/1000 | Loss: 0.00003728
Iteration 78/1000 | Loss: 0.00003727
Iteration 79/1000 | Loss: 0.00003727
Iteration 80/1000 | Loss: 0.00003727
Iteration 81/1000 | Loss: 0.00003726
Iteration 82/1000 | Loss: 0.00003726
Iteration 83/1000 | Loss: 0.00003726
Iteration 84/1000 | Loss: 0.00003725
Iteration 85/1000 | Loss: 0.00003725
Iteration 86/1000 | Loss: 0.00003725
Iteration 87/1000 | Loss: 0.00003725
Iteration 88/1000 | Loss: 0.00003725
Iteration 89/1000 | Loss: 0.00003724
Iteration 90/1000 | Loss: 0.00003724
Iteration 91/1000 | Loss: 0.00003724
Iteration 92/1000 | Loss: 0.00003724
Iteration 93/1000 | Loss: 0.00003724
Iteration 94/1000 | Loss: 0.00003724
Iteration 95/1000 | Loss: 0.00003724
Iteration 96/1000 | Loss: 0.00003723
Iteration 97/1000 | Loss: 0.00003723
Iteration 98/1000 | Loss: 0.00003723
Iteration 99/1000 | Loss: 0.00003723
Iteration 100/1000 | Loss: 0.00003723
Iteration 101/1000 | Loss: 0.00003722
Iteration 102/1000 | Loss: 0.00003722
Iteration 103/1000 | Loss: 0.00003722
Iteration 104/1000 | Loss: 0.00003722
Iteration 105/1000 | Loss: 0.00003722
Iteration 106/1000 | Loss: 0.00003722
Iteration 107/1000 | Loss: 0.00003722
Iteration 108/1000 | Loss: 0.00003722
Iteration 109/1000 | Loss: 0.00003722
Iteration 110/1000 | Loss: 0.00003722
Iteration 111/1000 | Loss: 0.00003722
Iteration 112/1000 | Loss: 0.00003722
Iteration 113/1000 | Loss: 0.00003722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.721996836247854e-05, 3.721996836247854e-05, 3.721996836247854e-05, 3.721996836247854e-05, 3.721996836247854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.721996836247854e-05

Optimization complete. Final v2v error: 4.666477203369141 mm

Highest mean error: 13.524467468261719 mm for frame 141

Lowest mean error: 3.835193634033203 mm for frame 37

Saving results

Total time: 117.866952419281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662197
Iteration 2/25 | Loss: 0.00147307
Iteration 3/25 | Loss: 0.00138299
Iteration 4/25 | Loss: 0.00137123
Iteration 5/25 | Loss: 0.00136937
Iteration 6/25 | Loss: 0.00136937
Iteration 7/25 | Loss: 0.00136937
Iteration 8/25 | Loss: 0.00136937
Iteration 9/25 | Loss: 0.00136937
Iteration 10/25 | Loss: 0.00136937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013693679356947541, 0.0013693679356947541, 0.0013693679356947541, 0.0013693679356947541, 0.0013693679356947541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013693679356947541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.93885851
Iteration 2/25 | Loss: 0.00250490
Iteration 3/25 | Loss: 0.00250485
Iteration 4/25 | Loss: 0.00250485
Iteration 5/25 | Loss: 0.00250485
Iteration 6/25 | Loss: 0.00250485
Iteration 7/25 | Loss: 0.00250485
Iteration 8/25 | Loss: 0.00250485
Iteration 9/25 | Loss: 0.00250485
Iteration 10/25 | Loss: 0.00250485
Iteration 11/25 | Loss: 0.00250485
Iteration 12/25 | Loss: 0.00250485
Iteration 13/25 | Loss: 0.00250485
Iteration 14/25 | Loss: 0.00250485
Iteration 15/25 | Loss: 0.00250485
Iteration 16/25 | Loss: 0.00250485
Iteration 17/25 | Loss: 0.00250485
Iteration 18/25 | Loss: 0.00250485
Iteration 19/25 | Loss: 0.00250485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002504852134734392, 0.002504852134734392, 0.002504852134734392, 0.002504852134734392, 0.002504852134734392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002504852134734392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250485
Iteration 2/1000 | Loss: 0.00003853
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002438
Iteration 7/1000 | Loss: 0.00002412
Iteration 8/1000 | Loss: 0.00002385
Iteration 9/1000 | Loss: 0.00002382
Iteration 10/1000 | Loss: 0.00002374
Iteration 11/1000 | Loss: 0.00002365
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002360
Iteration 14/1000 | Loss: 0.00002360
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002348
Iteration 18/1000 | Loss: 0.00002348
Iteration 19/1000 | Loss: 0.00002344
Iteration 20/1000 | Loss: 0.00002344
Iteration 21/1000 | Loss: 0.00002344
Iteration 22/1000 | Loss: 0.00002344
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00002344
Iteration 25/1000 | Loss: 0.00002343
Iteration 26/1000 | Loss: 0.00002343
Iteration 27/1000 | Loss: 0.00002343
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002339
Iteration 30/1000 | Loss: 0.00002339
Iteration 31/1000 | Loss: 0.00002335
Iteration 32/1000 | Loss: 0.00002335
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002333
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002332
Iteration 43/1000 | Loss: 0.00002332
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002331
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002330
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00002329
Iteration 56/1000 | Loss: 0.00002329
Iteration 57/1000 | Loss: 0.00002329
Iteration 58/1000 | Loss: 0.00002329
Iteration 59/1000 | Loss: 0.00002329
Iteration 60/1000 | Loss: 0.00002329
Iteration 61/1000 | Loss: 0.00002329
Iteration 62/1000 | Loss: 0.00002329
Iteration 63/1000 | Loss: 0.00002329
Iteration 64/1000 | Loss: 0.00002329
Iteration 65/1000 | Loss: 0.00002328
Iteration 66/1000 | Loss: 0.00002328
Iteration 67/1000 | Loss: 0.00002328
Iteration 68/1000 | Loss: 0.00002328
Iteration 69/1000 | Loss: 0.00002328
Iteration 70/1000 | Loss: 0.00002328
Iteration 71/1000 | Loss: 0.00002328
Iteration 72/1000 | Loss: 0.00002328
Iteration 73/1000 | Loss: 0.00002328
Iteration 74/1000 | Loss: 0.00002328
Iteration 75/1000 | Loss: 0.00002328
Iteration 76/1000 | Loss: 0.00002327
Iteration 77/1000 | Loss: 0.00002327
Iteration 78/1000 | Loss: 0.00002327
Iteration 79/1000 | Loss: 0.00002327
Iteration 80/1000 | Loss: 0.00002327
Iteration 81/1000 | Loss: 0.00002327
Iteration 82/1000 | Loss: 0.00002326
Iteration 83/1000 | Loss: 0.00002326
Iteration 84/1000 | Loss: 0.00002326
Iteration 85/1000 | Loss: 0.00002326
Iteration 86/1000 | Loss: 0.00002326
Iteration 87/1000 | Loss: 0.00002326
Iteration 88/1000 | Loss: 0.00002326
Iteration 89/1000 | Loss: 0.00002326
Iteration 90/1000 | Loss: 0.00002326
Iteration 91/1000 | Loss: 0.00002326
Iteration 92/1000 | Loss: 0.00002325
Iteration 93/1000 | Loss: 0.00002325
Iteration 94/1000 | Loss: 0.00002325
Iteration 95/1000 | Loss: 0.00002324
Iteration 96/1000 | Loss: 0.00002324
Iteration 97/1000 | Loss: 0.00002324
Iteration 98/1000 | Loss: 0.00002324
Iteration 99/1000 | Loss: 0.00002324
Iteration 100/1000 | Loss: 0.00002324
Iteration 101/1000 | Loss: 0.00002324
Iteration 102/1000 | Loss: 0.00002324
Iteration 103/1000 | Loss: 0.00002324
Iteration 104/1000 | Loss: 0.00002324
Iteration 105/1000 | Loss: 0.00002324
Iteration 106/1000 | Loss: 0.00002324
Iteration 107/1000 | Loss: 0.00002324
Iteration 108/1000 | Loss: 0.00002324
Iteration 109/1000 | Loss: 0.00002324
Iteration 110/1000 | Loss: 0.00002324
Iteration 111/1000 | Loss: 0.00002324
Iteration 112/1000 | Loss: 0.00002324
Iteration 113/1000 | Loss: 0.00002324
Iteration 114/1000 | Loss: 0.00002324
Iteration 115/1000 | Loss: 0.00002324
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.3238482754095457e-05, 2.3238482754095457e-05, 2.3238482754095457e-05, 2.3238482754095457e-05, 2.3238482754095457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3238482754095457e-05

Optimization complete. Final v2v error: 4.231306552886963 mm

Highest mean error: 4.8185648918151855 mm for frame 177

Lowest mean error: 3.6052119731903076 mm for frame 218

Saving results

Total time: 33.19628095626831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030157
Iteration 2/25 | Loss: 0.00183967
Iteration 3/25 | Loss: 0.00160891
Iteration 4/25 | Loss: 0.00156462
Iteration 5/25 | Loss: 0.00153632
Iteration 6/25 | Loss: 0.00150386
Iteration 7/25 | Loss: 0.00148633
Iteration 8/25 | Loss: 0.00147946
Iteration 9/25 | Loss: 0.00147932
Iteration 10/25 | Loss: 0.00152984
Iteration 11/25 | Loss: 0.00152437
Iteration 12/25 | Loss: 0.00150610
Iteration 13/25 | Loss: 0.00150633
Iteration 14/25 | Loss: 0.00150223
Iteration 15/25 | Loss: 0.00149517
Iteration 16/25 | Loss: 0.00149388
Iteration 17/25 | Loss: 0.00148780
Iteration 18/25 | Loss: 0.00148677
Iteration 19/25 | Loss: 0.00148679
Iteration 20/25 | Loss: 0.00148596
Iteration 21/25 | Loss: 0.00148300
Iteration 22/25 | Loss: 0.00147452
Iteration 23/25 | Loss: 0.00147776
Iteration 24/25 | Loss: 0.00147811
Iteration 25/25 | Loss: 0.00147202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64958727
Iteration 2/25 | Loss: 0.00354884
Iteration 3/25 | Loss: 0.00354883
Iteration 4/25 | Loss: 0.00354883
Iteration 5/25 | Loss: 0.00354883
Iteration 6/25 | Loss: 0.00354883
Iteration 7/25 | Loss: 0.00354883
Iteration 8/25 | Loss: 0.00354883
Iteration 9/25 | Loss: 0.00354883
Iteration 10/25 | Loss: 0.00354883
Iteration 11/25 | Loss: 0.00354883
Iteration 12/25 | Loss: 0.00354883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.003548825392499566, 0.003548825392499566, 0.003548825392499566, 0.003548825392499566, 0.003548825392499566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003548825392499566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00354883
Iteration 2/1000 | Loss: 0.00144804
Iteration 3/1000 | Loss: 0.00126433
Iteration 4/1000 | Loss: 0.00059124
Iteration 5/1000 | Loss: 0.00030388
Iteration 6/1000 | Loss: 0.00021860
Iteration 7/1000 | Loss: 0.00029500
Iteration 8/1000 | Loss: 0.00028525
Iteration 9/1000 | Loss: 0.00023620
Iteration 10/1000 | Loss: 0.00043290
Iteration 11/1000 | Loss: 0.00015133
Iteration 12/1000 | Loss: 0.00016036
Iteration 13/1000 | Loss: 0.00035339
Iteration 14/1000 | Loss: 0.00008166
Iteration 15/1000 | Loss: 0.00006718
Iteration 16/1000 | Loss: 0.00018001
Iteration 17/1000 | Loss: 0.00031572
Iteration 18/1000 | Loss: 0.00065859
Iteration 19/1000 | Loss: 0.00034634
Iteration 20/1000 | Loss: 0.00066683
Iteration 21/1000 | Loss: 0.00044284
Iteration 22/1000 | Loss: 0.00050387
Iteration 23/1000 | Loss: 0.00066650
Iteration 24/1000 | Loss: 0.00034454
Iteration 25/1000 | Loss: 0.00017869
Iteration 26/1000 | Loss: 0.00018633
Iteration 27/1000 | Loss: 0.00021390
Iteration 28/1000 | Loss: 0.00014408
Iteration 29/1000 | Loss: 0.00022971
Iteration 30/1000 | Loss: 0.00014845
Iteration 31/1000 | Loss: 0.00010836
Iteration 32/1000 | Loss: 0.00005556
Iteration 33/1000 | Loss: 0.00022499
Iteration 34/1000 | Loss: 0.00018774
Iteration 35/1000 | Loss: 0.00039627
Iteration 36/1000 | Loss: 0.00017535
Iteration 37/1000 | Loss: 0.00014466
Iteration 38/1000 | Loss: 0.00008818
Iteration 39/1000 | Loss: 0.00014058
Iteration 40/1000 | Loss: 0.00044147
Iteration 41/1000 | Loss: 0.00034104
Iteration 42/1000 | Loss: 0.00016151
Iteration 43/1000 | Loss: 0.00015161
Iteration 44/1000 | Loss: 0.00005379
Iteration 45/1000 | Loss: 0.00005276
Iteration 46/1000 | Loss: 0.00003995
Iteration 47/1000 | Loss: 0.00018141
Iteration 48/1000 | Loss: 0.00018118
Iteration 49/1000 | Loss: 0.00016114
Iteration 50/1000 | Loss: 0.00014218
Iteration 51/1000 | Loss: 0.00067055
Iteration 52/1000 | Loss: 0.00046718
Iteration 53/1000 | Loss: 0.00004111
Iteration 54/1000 | Loss: 0.00043886
Iteration 55/1000 | Loss: 0.00037881
Iteration 56/1000 | Loss: 0.00027238
Iteration 57/1000 | Loss: 0.00004594
Iteration 58/1000 | Loss: 0.00003800
Iteration 59/1000 | Loss: 0.00014075
Iteration 60/1000 | Loss: 0.00042265
Iteration 61/1000 | Loss: 0.00025244
Iteration 62/1000 | Loss: 0.00007561
Iteration 63/1000 | Loss: 0.00005237
Iteration 64/1000 | Loss: 0.00004423
Iteration 65/1000 | Loss: 0.00004032
Iteration 66/1000 | Loss: 0.00026748
Iteration 67/1000 | Loss: 0.00021905
Iteration 68/1000 | Loss: 0.00005129
Iteration 69/1000 | Loss: 0.00004055
Iteration 70/1000 | Loss: 0.00015034
Iteration 71/1000 | Loss: 0.00014681
Iteration 72/1000 | Loss: 0.00014354
Iteration 73/1000 | Loss: 0.00011705
Iteration 74/1000 | Loss: 0.00012676
Iteration 75/1000 | Loss: 0.00015640
Iteration 76/1000 | Loss: 0.00010539
Iteration 77/1000 | Loss: 0.00003976
Iteration 78/1000 | Loss: 0.00025481
Iteration 79/1000 | Loss: 0.00017551
Iteration 80/1000 | Loss: 0.00023676
Iteration 81/1000 | Loss: 0.00015943
Iteration 82/1000 | Loss: 0.00019285
Iteration 83/1000 | Loss: 0.00055995
Iteration 84/1000 | Loss: 0.00033542
Iteration 85/1000 | Loss: 0.00019758
Iteration 86/1000 | Loss: 0.00024951
Iteration 87/1000 | Loss: 0.00020775
Iteration 88/1000 | Loss: 0.00013593
Iteration 89/1000 | Loss: 0.00004105
Iteration 90/1000 | Loss: 0.00003935
Iteration 91/1000 | Loss: 0.00019763
Iteration 92/1000 | Loss: 0.00017805
Iteration 93/1000 | Loss: 0.00016584
Iteration 94/1000 | Loss: 0.00034613
Iteration 95/1000 | Loss: 0.00029014
Iteration 96/1000 | Loss: 0.00011858
Iteration 97/1000 | Loss: 0.00003664
Iteration 98/1000 | Loss: 0.00003397
Iteration 99/1000 | Loss: 0.00003249
Iteration 100/1000 | Loss: 0.00003164
Iteration 101/1000 | Loss: 0.00003112
Iteration 102/1000 | Loss: 0.00022343
Iteration 103/1000 | Loss: 0.00017936
Iteration 104/1000 | Loss: 0.00003197
Iteration 105/1000 | Loss: 0.00003072
Iteration 106/1000 | Loss: 0.00012239
Iteration 107/1000 | Loss: 0.00019775
Iteration 108/1000 | Loss: 0.00008996
Iteration 109/1000 | Loss: 0.00010896
Iteration 110/1000 | Loss: 0.00009221
Iteration 111/1000 | Loss: 0.00003197
Iteration 112/1000 | Loss: 0.00020874
Iteration 113/1000 | Loss: 0.00019546
Iteration 114/1000 | Loss: 0.00025478
Iteration 115/1000 | Loss: 0.00003780
Iteration 116/1000 | Loss: 0.00003470
Iteration 117/1000 | Loss: 0.00003316
Iteration 118/1000 | Loss: 0.00006455
Iteration 119/1000 | Loss: 0.00036015
Iteration 120/1000 | Loss: 0.00020234
Iteration 121/1000 | Loss: 0.00005849
Iteration 122/1000 | Loss: 0.00018028
Iteration 123/1000 | Loss: 0.00016103
Iteration 124/1000 | Loss: 0.00006642
Iteration 125/1000 | Loss: 0.00025192
Iteration 126/1000 | Loss: 0.00032719
Iteration 127/1000 | Loss: 0.00020847
Iteration 128/1000 | Loss: 0.00021444
Iteration 129/1000 | Loss: 0.00008592
Iteration 130/1000 | Loss: 0.00003249
Iteration 131/1000 | Loss: 0.00003144
Iteration 132/1000 | Loss: 0.00003074
Iteration 133/1000 | Loss: 0.00003002
Iteration 134/1000 | Loss: 0.00002970
Iteration 135/1000 | Loss: 0.00002948
Iteration 136/1000 | Loss: 0.00002931
Iteration 137/1000 | Loss: 0.00002926
Iteration 138/1000 | Loss: 0.00002919
Iteration 139/1000 | Loss: 0.00011875
Iteration 140/1000 | Loss: 0.00008787
Iteration 141/1000 | Loss: 0.00003682
Iteration 142/1000 | Loss: 0.00003278
Iteration 143/1000 | Loss: 0.00002957
Iteration 144/1000 | Loss: 0.00002923
Iteration 145/1000 | Loss: 0.00002917
Iteration 146/1000 | Loss: 0.00002916
Iteration 147/1000 | Loss: 0.00002916
Iteration 148/1000 | Loss: 0.00012363
Iteration 149/1000 | Loss: 0.00028375
Iteration 150/1000 | Loss: 0.00024341
Iteration 151/1000 | Loss: 0.00025026
Iteration 152/1000 | Loss: 0.00033538
Iteration 153/1000 | Loss: 0.00012946
Iteration 154/1000 | Loss: 0.00007999
Iteration 155/1000 | Loss: 0.00031684
Iteration 156/1000 | Loss: 0.00004167
Iteration 157/1000 | Loss: 0.00003775
Iteration 158/1000 | Loss: 0.00003551
Iteration 159/1000 | Loss: 0.00018801
Iteration 160/1000 | Loss: 0.00020393
Iteration 161/1000 | Loss: 0.00016802
Iteration 162/1000 | Loss: 0.00048578
Iteration 163/1000 | Loss: 0.00014699
Iteration 164/1000 | Loss: 0.00005437
Iteration 165/1000 | Loss: 0.00012712
Iteration 166/1000 | Loss: 0.00062683
Iteration 167/1000 | Loss: 0.00024413
Iteration 168/1000 | Loss: 0.00040085
Iteration 169/1000 | Loss: 0.00021720
Iteration 170/1000 | Loss: 0.00025938
Iteration 171/1000 | Loss: 0.00048012
Iteration 172/1000 | Loss: 0.00036741
Iteration 173/1000 | Loss: 0.00016808
Iteration 174/1000 | Loss: 0.00005732
Iteration 175/1000 | Loss: 0.00010540
Iteration 176/1000 | Loss: 0.00008777
Iteration 177/1000 | Loss: 0.00006211
Iteration 178/1000 | Loss: 0.00003591
Iteration 179/1000 | Loss: 0.00003373
Iteration 180/1000 | Loss: 0.00003201
Iteration 181/1000 | Loss: 0.00003116
Iteration 182/1000 | Loss: 0.00003052
Iteration 183/1000 | Loss: 0.00021318
Iteration 184/1000 | Loss: 0.00013108
Iteration 185/1000 | Loss: 0.00009177
Iteration 186/1000 | Loss: 0.00010486
Iteration 187/1000 | Loss: 0.00008038
Iteration 188/1000 | Loss: 0.00007432
Iteration 189/1000 | Loss: 0.00012834
Iteration 190/1000 | Loss: 0.00003171
Iteration 191/1000 | Loss: 0.00003041
Iteration 192/1000 | Loss: 0.00002910
Iteration 193/1000 | Loss: 0.00002872
Iteration 194/1000 | Loss: 0.00002848
Iteration 195/1000 | Loss: 0.00002842
Iteration 196/1000 | Loss: 0.00002832
Iteration 197/1000 | Loss: 0.00002831
Iteration 198/1000 | Loss: 0.00002822
Iteration 199/1000 | Loss: 0.00002821
Iteration 200/1000 | Loss: 0.00002820
Iteration 201/1000 | Loss: 0.00002820
Iteration 202/1000 | Loss: 0.00002819
Iteration 203/1000 | Loss: 0.00002819
Iteration 204/1000 | Loss: 0.00002819
Iteration 205/1000 | Loss: 0.00002818
Iteration 206/1000 | Loss: 0.00002818
Iteration 207/1000 | Loss: 0.00002818
Iteration 208/1000 | Loss: 0.00002818
Iteration 209/1000 | Loss: 0.00002818
Iteration 210/1000 | Loss: 0.00002818
Iteration 211/1000 | Loss: 0.00002818
Iteration 212/1000 | Loss: 0.00002818
Iteration 213/1000 | Loss: 0.00002818
Iteration 214/1000 | Loss: 0.00002818
Iteration 215/1000 | Loss: 0.00002818
Iteration 216/1000 | Loss: 0.00002818
Iteration 217/1000 | Loss: 0.00002817
Iteration 218/1000 | Loss: 0.00002817
Iteration 219/1000 | Loss: 0.00002817
Iteration 220/1000 | Loss: 0.00002816
Iteration 221/1000 | Loss: 0.00002816
Iteration 222/1000 | Loss: 0.00002816
Iteration 223/1000 | Loss: 0.00002815
Iteration 224/1000 | Loss: 0.00002815
Iteration 225/1000 | Loss: 0.00002814
Iteration 226/1000 | Loss: 0.00002814
Iteration 227/1000 | Loss: 0.00002814
Iteration 228/1000 | Loss: 0.00002813
Iteration 229/1000 | Loss: 0.00002813
Iteration 230/1000 | Loss: 0.00002813
Iteration 231/1000 | Loss: 0.00002812
Iteration 232/1000 | Loss: 0.00002812
Iteration 233/1000 | Loss: 0.00002812
Iteration 234/1000 | Loss: 0.00002811
Iteration 235/1000 | Loss: 0.00002811
Iteration 236/1000 | Loss: 0.00002810
Iteration 237/1000 | Loss: 0.00002810
Iteration 238/1000 | Loss: 0.00002810
Iteration 239/1000 | Loss: 0.00002810
Iteration 240/1000 | Loss: 0.00002809
Iteration 241/1000 | Loss: 0.00002809
Iteration 242/1000 | Loss: 0.00002809
Iteration 243/1000 | Loss: 0.00002808
Iteration 244/1000 | Loss: 0.00002808
Iteration 245/1000 | Loss: 0.00002808
Iteration 246/1000 | Loss: 0.00002807
Iteration 247/1000 | Loss: 0.00002807
Iteration 248/1000 | Loss: 0.00002807
Iteration 249/1000 | Loss: 0.00002807
Iteration 250/1000 | Loss: 0.00002807
Iteration 251/1000 | Loss: 0.00002807
Iteration 252/1000 | Loss: 0.00002807
Iteration 253/1000 | Loss: 0.00002806
Iteration 254/1000 | Loss: 0.00002806
Iteration 255/1000 | Loss: 0.00002806
Iteration 256/1000 | Loss: 0.00002805
Iteration 257/1000 | Loss: 0.00002804
Iteration 258/1000 | Loss: 0.00002803
Iteration 259/1000 | Loss: 0.00002803
Iteration 260/1000 | Loss: 0.00002803
Iteration 261/1000 | Loss: 0.00002802
Iteration 262/1000 | Loss: 0.00002802
Iteration 263/1000 | Loss: 0.00002802
Iteration 264/1000 | Loss: 0.00002802
Iteration 265/1000 | Loss: 0.00002802
Iteration 266/1000 | Loss: 0.00002802
Iteration 267/1000 | Loss: 0.00002802
Iteration 268/1000 | Loss: 0.00002801
Iteration 269/1000 | Loss: 0.00002801
Iteration 270/1000 | Loss: 0.00002801
Iteration 271/1000 | Loss: 0.00002801
Iteration 272/1000 | Loss: 0.00002801
Iteration 273/1000 | Loss: 0.00002801
Iteration 274/1000 | Loss: 0.00002801
Iteration 275/1000 | Loss: 0.00002801
Iteration 276/1000 | Loss: 0.00002801
Iteration 277/1000 | Loss: 0.00002801
Iteration 278/1000 | Loss: 0.00002800
Iteration 279/1000 | Loss: 0.00002800
Iteration 280/1000 | Loss: 0.00002799
Iteration 281/1000 | Loss: 0.00002799
Iteration 282/1000 | Loss: 0.00002799
Iteration 283/1000 | Loss: 0.00002799
Iteration 284/1000 | Loss: 0.00002799
Iteration 285/1000 | Loss: 0.00002799
Iteration 286/1000 | Loss: 0.00002798
Iteration 287/1000 | Loss: 0.00002798
Iteration 288/1000 | Loss: 0.00002798
Iteration 289/1000 | Loss: 0.00002798
Iteration 290/1000 | Loss: 0.00002798
Iteration 291/1000 | Loss: 0.00002798
Iteration 292/1000 | Loss: 0.00002798
Iteration 293/1000 | Loss: 0.00002798
Iteration 294/1000 | Loss: 0.00002798
Iteration 295/1000 | Loss: 0.00002798
Iteration 296/1000 | Loss: 0.00002797
Iteration 297/1000 | Loss: 0.00002797
Iteration 298/1000 | Loss: 0.00002797
Iteration 299/1000 | Loss: 0.00002797
Iteration 300/1000 | Loss: 0.00002797
Iteration 301/1000 | Loss: 0.00002797
Iteration 302/1000 | Loss: 0.00002797
Iteration 303/1000 | Loss: 0.00002797
Iteration 304/1000 | Loss: 0.00002797
Iteration 305/1000 | Loss: 0.00002797
Iteration 306/1000 | Loss: 0.00002797
Iteration 307/1000 | Loss: 0.00002797
Iteration 308/1000 | Loss: 0.00002797
Iteration 309/1000 | Loss: 0.00002797
Iteration 310/1000 | Loss: 0.00002797
Iteration 311/1000 | Loss: 0.00002797
Iteration 312/1000 | Loss: 0.00002797
Iteration 313/1000 | Loss: 0.00002797
Iteration 314/1000 | Loss: 0.00002797
Iteration 315/1000 | Loss: 0.00002797
Iteration 316/1000 | Loss: 0.00002797
Iteration 317/1000 | Loss: 0.00002797
Iteration 318/1000 | Loss: 0.00002797
Iteration 319/1000 | Loss: 0.00002797
Iteration 320/1000 | Loss: 0.00002797
Iteration 321/1000 | Loss: 0.00002797
Iteration 322/1000 | Loss: 0.00002797
Iteration 323/1000 | Loss: 0.00002797
Iteration 324/1000 | Loss: 0.00002797
Iteration 325/1000 | Loss: 0.00002797
Iteration 326/1000 | Loss: 0.00002797
Iteration 327/1000 | Loss: 0.00002797
Iteration 328/1000 | Loss: 0.00002797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 328. Stopping optimization.
Last 5 losses: [2.7969552320428193e-05, 2.7969552320428193e-05, 2.7969552320428193e-05, 2.7969552320428193e-05, 2.7969552320428193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7969552320428193e-05

Optimization complete. Final v2v error: 4.530923843383789 mm

Highest mean error: 9.021044731140137 mm for frame 211

Lowest mean error: 3.8036954402923584 mm for frame 152

Saving results

Total time: 368.65164160728455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959386
Iteration 2/25 | Loss: 0.00178514
Iteration 3/25 | Loss: 0.00155665
Iteration 4/25 | Loss: 0.00153036
Iteration 5/25 | Loss: 0.00152576
Iteration 6/25 | Loss: 0.00152446
Iteration 7/25 | Loss: 0.00152446
Iteration 8/25 | Loss: 0.00152446
Iteration 9/25 | Loss: 0.00152446
Iteration 10/25 | Loss: 0.00152446
Iteration 11/25 | Loss: 0.00152446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015244613168761134, 0.0015244613168761134, 0.0015244613168761134, 0.0015244613168761134, 0.0015244613168761134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015244613168761134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15143287
Iteration 2/25 | Loss: 0.00225088
Iteration 3/25 | Loss: 0.00225088
Iteration 4/25 | Loss: 0.00225088
Iteration 5/25 | Loss: 0.00225088
Iteration 6/25 | Loss: 0.00225088
Iteration 7/25 | Loss: 0.00225088
Iteration 8/25 | Loss: 0.00225088
Iteration 9/25 | Loss: 0.00225088
Iteration 10/25 | Loss: 0.00225088
Iteration 11/25 | Loss: 0.00225088
Iteration 12/25 | Loss: 0.00225087
Iteration 13/25 | Loss: 0.00225087
Iteration 14/25 | Loss: 0.00225087
Iteration 15/25 | Loss: 0.00225087
Iteration 16/25 | Loss: 0.00225087
Iteration 17/25 | Loss: 0.00225087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00225087464787066, 0.00225087464787066, 0.00225087464787066, 0.00225087464787066, 0.00225087464787066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00225087464787066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225087
Iteration 2/1000 | Loss: 0.00005120
Iteration 3/1000 | Loss: 0.00004297
Iteration 4/1000 | Loss: 0.00003985
Iteration 5/1000 | Loss: 0.00003789
Iteration 6/1000 | Loss: 0.00003715
Iteration 7/1000 | Loss: 0.00003643
Iteration 8/1000 | Loss: 0.00003619
Iteration 9/1000 | Loss: 0.00003588
Iteration 10/1000 | Loss: 0.00003562
Iteration 11/1000 | Loss: 0.00003551
Iteration 12/1000 | Loss: 0.00003550
Iteration 13/1000 | Loss: 0.00003541
Iteration 14/1000 | Loss: 0.00003535
Iteration 15/1000 | Loss: 0.00003535
Iteration 16/1000 | Loss: 0.00003535
Iteration 17/1000 | Loss: 0.00003534
Iteration 18/1000 | Loss: 0.00003534
Iteration 19/1000 | Loss: 0.00003534
Iteration 20/1000 | Loss: 0.00003534
Iteration 21/1000 | Loss: 0.00003534
Iteration 22/1000 | Loss: 0.00003534
Iteration 23/1000 | Loss: 0.00003534
Iteration 24/1000 | Loss: 0.00003534
Iteration 25/1000 | Loss: 0.00003534
Iteration 26/1000 | Loss: 0.00003533
Iteration 27/1000 | Loss: 0.00003533
Iteration 28/1000 | Loss: 0.00003533
Iteration 29/1000 | Loss: 0.00003533
Iteration 30/1000 | Loss: 0.00003533
Iteration 31/1000 | Loss: 0.00003533
Iteration 32/1000 | Loss: 0.00003533
Iteration 33/1000 | Loss: 0.00003533
Iteration 34/1000 | Loss: 0.00003532
Iteration 35/1000 | Loss: 0.00003532
Iteration 36/1000 | Loss: 0.00003530
Iteration 37/1000 | Loss: 0.00003530
Iteration 38/1000 | Loss: 0.00003530
Iteration 39/1000 | Loss: 0.00003530
Iteration 40/1000 | Loss: 0.00003530
Iteration 41/1000 | Loss: 0.00003530
Iteration 42/1000 | Loss: 0.00003530
Iteration 43/1000 | Loss: 0.00003530
Iteration 44/1000 | Loss: 0.00003530
Iteration 45/1000 | Loss: 0.00003530
Iteration 46/1000 | Loss: 0.00003529
Iteration 47/1000 | Loss: 0.00003529
Iteration 48/1000 | Loss: 0.00003529
Iteration 49/1000 | Loss: 0.00003529
Iteration 50/1000 | Loss: 0.00003529
Iteration 51/1000 | Loss: 0.00003529
Iteration 52/1000 | Loss: 0.00003528
Iteration 53/1000 | Loss: 0.00003528
Iteration 54/1000 | Loss: 0.00003528
Iteration 55/1000 | Loss: 0.00003527
Iteration 56/1000 | Loss: 0.00003527
Iteration 57/1000 | Loss: 0.00003527
Iteration 58/1000 | Loss: 0.00003527
Iteration 59/1000 | Loss: 0.00003527
Iteration 60/1000 | Loss: 0.00003526
Iteration 61/1000 | Loss: 0.00003526
Iteration 62/1000 | Loss: 0.00003526
Iteration 63/1000 | Loss: 0.00003526
Iteration 64/1000 | Loss: 0.00003526
Iteration 65/1000 | Loss: 0.00003526
Iteration 66/1000 | Loss: 0.00003526
Iteration 67/1000 | Loss: 0.00003525
Iteration 68/1000 | Loss: 0.00003525
Iteration 69/1000 | Loss: 0.00003525
Iteration 70/1000 | Loss: 0.00003525
Iteration 71/1000 | Loss: 0.00003525
Iteration 72/1000 | Loss: 0.00003525
Iteration 73/1000 | Loss: 0.00003525
Iteration 74/1000 | Loss: 0.00003525
Iteration 75/1000 | Loss: 0.00003525
Iteration 76/1000 | Loss: 0.00003525
Iteration 77/1000 | Loss: 0.00003525
Iteration 78/1000 | Loss: 0.00003525
Iteration 79/1000 | Loss: 0.00003525
Iteration 80/1000 | Loss: 0.00003525
Iteration 81/1000 | Loss: 0.00003524
Iteration 82/1000 | Loss: 0.00003524
Iteration 83/1000 | Loss: 0.00003524
Iteration 84/1000 | Loss: 0.00003524
Iteration 85/1000 | Loss: 0.00003524
Iteration 86/1000 | Loss: 0.00003524
Iteration 87/1000 | Loss: 0.00003524
Iteration 88/1000 | Loss: 0.00003523
Iteration 89/1000 | Loss: 0.00003523
Iteration 90/1000 | Loss: 0.00003523
Iteration 91/1000 | Loss: 0.00003523
Iteration 92/1000 | Loss: 0.00003523
Iteration 93/1000 | Loss: 0.00003523
Iteration 94/1000 | Loss: 0.00003522
Iteration 95/1000 | Loss: 0.00003522
Iteration 96/1000 | Loss: 0.00003522
Iteration 97/1000 | Loss: 0.00003522
Iteration 98/1000 | Loss: 0.00003522
Iteration 99/1000 | Loss: 0.00003522
Iteration 100/1000 | Loss: 0.00003522
Iteration 101/1000 | Loss: 0.00003521
Iteration 102/1000 | Loss: 0.00003521
Iteration 103/1000 | Loss: 0.00003521
Iteration 104/1000 | Loss: 0.00003521
Iteration 105/1000 | Loss: 0.00003521
Iteration 106/1000 | Loss: 0.00003521
Iteration 107/1000 | Loss: 0.00003521
Iteration 108/1000 | Loss: 0.00003521
Iteration 109/1000 | Loss: 0.00003521
Iteration 110/1000 | Loss: 0.00003521
Iteration 111/1000 | Loss: 0.00003521
Iteration 112/1000 | Loss: 0.00003521
Iteration 113/1000 | Loss: 0.00003521
Iteration 114/1000 | Loss: 0.00003520
Iteration 115/1000 | Loss: 0.00003520
Iteration 116/1000 | Loss: 0.00003520
Iteration 117/1000 | Loss: 0.00003520
Iteration 118/1000 | Loss: 0.00003520
Iteration 119/1000 | Loss: 0.00003520
Iteration 120/1000 | Loss: 0.00003519
Iteration 121/1000 | Loss: 0.00003519
Iteration 122/1000 | Loss: 0.00003519
Iteration 123/1000 | Loss: 0.00003519
Iteration 124/1000 | Loss: 0.00003519
Iteration 125/1000 | Loss: 0.00003519
Iteration 126/1000 | Loss: 0.00003519
Iteration 127/1000 | Loss: 0.00003519
Iteration 128/1000 | Loss: 0.00003519
Iteration 129/1000 | Loss: 0.00003518
Iteration 130/1000 | Loss: 0.00003518
Iteration 131/1000 | Loss: 0.00003518
Iteration 132/1000 | Loss: 0.00003518
Iteration 133/1000 | Loss: 0.00003518
Iteration 134/1000 | Loss: 0.00003518
Iteration 135/1000 | Loss: 0.00003518
Iteration 136/1000 | Loss: 0.00003518
Iteration 137/1000 | Loss: 0.00003518
Iteration 138/1000 | Loss: 0.00003518
Iteration 139/1000 | Loss: 0.00003518
Iteration 140/1000 | Loss: 0.00003518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.5178927646484226e-05, 3.5178927646484226e-05, 3.5178927646484226e-05, 3.5178927646484226e-05, 3.5178927646484226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5178927646484226e-05

Optimization complete. Final v2v error: 5.089139938354492 mm

Highest mean error: 5.152090072631836 mm for frame 26

Lowest mean error: 5.012257099151611 mm for frame 126

Saving results

Total time: 32.86376667022705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984056
Iteration 2/25 | Loss: 0.00185236
Iteration 3/25 | Loss: 0.00161508
Iteration 4/25 | Loss: 0.00157075
Iteration 5/25 | Loss: 0.00156099
Iteration 6/25 | Loss: 0.00155965
Iteration 7/25 | Loss: 0.00155965
Iteration 8/25 | Loss: 0.00155965
Iteration 9/25 | Loss: 0.00155965
Iteration 10/25 | Loss: 0.00155965
Iteration 11/25 | Loss: 0.00155965
Iteration 12/25 | Loss: 0.00155965
Iteration 13/25 | Loss: 0.00155965
Iteration 14/25 | Loss: 0.00155965
Iteration 15/25 | Loss: 0.00155965
Iteration 16/25 | Loss: 0.00155965
Iteration 17/25 | Loss: 0.00155965
Iteration 18/25 | Loss: 0.00155965
Iteration 19/25 | Loss: 0.00155965
Iteration 20/25 | Loss: 0.00155965
Iteration 21/25 | Loss: 0.00155965
Iteration 22/25 | Loss: 0.00155965
Iteration 23/25 | Loss: 0.00155965
Iteration 24/25 | Loss: 0.00155965
Iteration 25/25 | Loss: 0.00155965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56900263
Iteration 2/25 | Loss: 0.00326705
Iteration 3/25 | Loss: 0.00326705
Iteration 4/25 | Loss: 0.00326705
Iteration 5/25 | Loss: 0.00326705
Iteration 6/25 | Loss: 0.00326705
Iteration 7/25 | Loss: 0.00326705
Iteration 8/25 | Loss: 0.00326705
Iteration 9/25 | Loss: 0.00326705
Iteration 10/25 | Loss: 0.00326705
Iteration 11/25 | Loss: 0.00326705
Iteration 12/25 | Loss: 0.00326705
Iteration 13/25 | Loss: 0.00326705
Iteration 14/25 | Loss: 0.00326705
Iteration 15/25 | Loss: 0.00326705
Iteration 16/25 | Loss: 0.00326705
Iteration 17/25 | Loss: 0.00326705
Iteration 18/25 | Loss: 0.00326705
Iteration 19/25 | Loss: 0.00326705
Iteration 20/25 | Loss: 0.00326705
Iteration 21/25 | Loss: 0.00326705
Iteration 22/25 | Loss: 0.00326705
Iteration 23/25 | Loss: 0.00326705
Iteration 24/25 | Loss: 0.00326705
Iteration 25/25 | Loss: 0.00326705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326705
Iteration 2/1000 | Loss: 0.00008032
Iteration 3/1000 | Loss: 0.00005235
Iteration 4/1000 | Loss: 0.00004194
Iteration 5/1000 | Loss: 0.00003948
Iteration 6/1000 | Loss: 0.00003705
Iteration 7/1000 | Loss: 0.00003554
Iteration 8/1000 | Loss: 0.00003432
Iteration 9/1000 | Loss: 0.00003310
Iteration 10/1000 | Loss: 0.00003222
Iteration 11/1000 | Loss: 0.00003162
Iteration 12/1000 | Loss: 0.00003122
Iteration 13/1000 | Loss: 0.00003079
Iteration 14/1000 | Loss: 0.00003049
Iteration 15/1000 | Loss: 0.00003028
Iteration 16/1000 | Loss: 0.00003017
Iteration 17/1000 | Loss: 0.00003015
Iteration 18/1000 | Loss: 0.00003014
Iteration 19/1000 | Loss: 0.00003014
Iteration 20/1000 | Loss: 0.00003013
Iteration 21/1000 | Loss: 0.00003012
Iteration 22/1000 | Loss: 0.00003012
Iteration 23/1000 | Loss: 0.00003011
Iteration 24/1000 | Loss: 0.00003011
Iteration 25/1000 | Loss: 0.00003011
Iteration 26/1000 | Loss: 0.00003011
Iteration 27/1000 | Loss: 0.00003011
Iteration 28/1000 | Loss: 0.00003011
Iteration 29/1000 | Loss: 0.00003011
Iteration 30/1000 | Loss: 0.00003011
Iteration 31/1000 | Loss: 0.00003010
Iteration 32/1000 | Loss: 0.00003010
Iteration 33/1000 | Loss: 0.00003010
Iteration 34/1000 | Loss: 0.00003010
Iteration 35/1000 | Loss: 0.00003010
Iteration 36/1000 | Loss: 0.00003010
Iteration 37/1000 | Loss: 0.00003010
Iteration 38/1000 | Loss: 0.00003010
Iteration 39/1000 | Loss: 0.00003009
Iteration 40/1000 | Loss: 0.00003008
Iteration 41/1000 | Loss: 0.00003008
Iteration 42/1000 | Loss: 0.00003007
Iteration 43/1000 | Loss: 0.00003007
Iteration 44/1000 | Loss: 0.00003007
Iteration 45/1000 | Loss: 0.00003007
Iteration 46/1000 | Loss: 0.00003007
Iteration 47/1000 | Loss: 0.00003007
Iteration 48/1000 | Loss: 0.00003007
Iteration 49/1000 | Loss: 0.00003007
Iteration 50/1000 | Loss: 0.00003007
Iteration 51/1000 | Loss: 0.00003007
Iteration 52/1000 | Loss: 0.00003007
Iteration 53/1000 | Loss: 0.00003006
Iteration 54/1000 | Loss: 0.00003006
Iteration 55/1000 | Loss: 0.00003006
Iteration 56/1000 | Loss: 0.00003006
Iteration 57/1000 | Loss: 0.00003006
Iteration 58/1000 | Loss: 0.00003005
Iteration 59/1000 | Loss: 0.00003005
Iteration 60/1000 | Loss: 0.00003005
Iteration 61/1000 | Loss: 0.00003005
Iteration 62/1000 | Loss: 0.00003005
Iteration 63/1000 | Loss: 0.00003004
Iteration 64/1000 | Loss: 0.00003004
Iteration 65/1000 | Loss: 0.00003004
Iteration 66/1000 | Loss: 0.00003004
Iteration 67/1000 | Loss: 0.00003003
Iteration 68/1000 | Loss: 0.00003003
Iteration 69/1000 | Loss: 0.00003003
Iteration 70/1000 | Loss: 0.00003003
Iteration 71/1000 | Loss: 0.00003003
Iteration 72/1000 | Loss: 0.00003003
Iteration 73/1000 | Loss: 0.00003002
Iteration 74/1000 | Loss: 0.00003002
Iteration 75/1000 | Loss: 0.00003002
Iteration 76/1000 | Loss: 0.00003002
Iteration 77/1000 | Loss: 0.00003001
Iteration 78/1000 | Loss: 0.00003001
Iteration 79/1000 | Loss: 0.00003001
Iteration 80/1000 | Loss: 0.00003001
Iteration 81/1000 | Loss: 0.00003001
Iteration 82/1000 | Loss: 0.00003001
Iteration 83/1000 | Loss: 0.00003001
Iteration 84/1000 | Loss: 0.00003001
Iteration 85/1000 | Loss: 0.00003000
Iteration 86/1000 | Loss: 0.00003000
Iteration 87/1000 | Loss: 0.00003000
Iteration 88/1000 | Loss: 0.00003000
Iteration 89/1000 | Loss: 0.00003000
Iteration 90/1000 | Loss: 0.00003000
Iteration 91/1000 | Loss: 0.00003000
Iteration 92/1000 | Loss: 0.00003000
Iteration 93/1000 | Loss: 0.00003000
Iteration 94/1000 | Loss: 0.00003000
Iteration 95/1000 | Loss: 0.00003000
Iteration 96/1000 | Loss: 0.00003000
Iteration 97/1000 | Loss: 0.00003000
Iteration 98/1000 | Loss: 0.00003000
Iteration 99/1000 | Loss: 0.00003000
Iteration 100/1000 | Loss: 0.00003000
Iteration 101/1000 | Loss: 0.00003000
Iteration 102/1000 | Loss: 0.00003000
Iteration 103/1000 | Loss: 0.00003000
Iteration 104/1000 | Loss: 0.00003000
Iteration 105/1000 | Loss: 0.00003000
Iteration 106/1000 | Loss: 0.00003000
Iteration 107/1000 | Loss: 0.00003000
Iteration 108/1000 | Loss: 0.00003000
Iteration 109/1000 | Loss: 0.00003000
Iteration 110/1000 | Loss: 0.00003000
Iteration 111/1000 | Loss: 0.00003000
Iteration 112/1000 | Loss: 0.00003000
Iteration 113/1000 | Loss: 0.00003000
Iteration 114/1000 | Loss: 0.00003000
Iteration 115/1000 | Loss: 0.00003000
Iteration 116/1000 | Loss: 0.00003000
Iteration 117/1000 | Loss: 0.00003000
Iteration 118/1000 | Loss: 0.00003000
Iteration 119/1000 | Loss: 0.00003000
Iteration 120/1000 | Loss: 0.00003000
Iteration 121/1000 | Loss: 0.00003000
Iteration 122/1000 | Loss: 0.00003000
Iteration 123/1000 | Loss: 0.00003000
Iteration 124/1000 | Loss: 0.00003000
Iteration 125/1000 | Loss: 0.00003000
Iteration 126/1000 | Loss: 0.00003000
Iteration 127/1000 | Loss: 0.00003000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.9997250749147497e-05, 2.9997250749147497e-05, 2.9997250749147497e-05, 2.9997250749147497e-05, 2.9997250749147497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9997250749147497e-05

Optimization complete. Final v2v error: 4.720752716064453 mm

Highest mean error: 5.212306976318359 mm for frame 43

Lowest mean error: 4.254145622253418 mm for frame 137

Saving results

Total time: 37.68835186958313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483663
Iteration 2/25 | Loss: 0.00157208
Iteration 3/25 | Loss: 0.00147044
Iteration 4/25 | Loss: 0.00144880
Iteration 5/25 | Loss: 0.00144149
Iteration 6/25 | Loss: 0.00143917
Iteration 7/25 | Loss: 0.00143917
Iteration 8/25 | Loss: 0.00143917
Iteration 9/25 | Loss: 0.00143917
Iteration 10/25 | Loss: 0.00143917
Iteration 11/25 | Loss: 0.00143917
Iteration 12/25 | Loss: 0.00143917
Iteration 13/25 | Loss: 0.00143917
Iteration 14/25 | Loss: 0.00143917
Iteration 15/25 | Loss: 0.00143917
Iteration 16/25 | Loss: 0.00143917
Iteration 17/25 | Loss: 0.00143917
Iteration 18/25 | Loss: 0.00143917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001439169980585575, 0.001439169980585575, 0.001439169980585575, 0.001439169980585575, 0.001439169980585575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001439169980585575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73268354
Iteration 2/25 | Loss: 0.00307175
Iteration 3/25 | Loss: 0.00307175
Iteration 4/25 | Loss: 0.00307175
Iteration 5/25 | Loss: 0.00307175
Iteration 6/25 | Loss: 0.00307175
Iteration 7/25 | Loss: 0.00307175
Iteration 8/25 | Loss: 0.00307175
Iteration 9/25 | Loss: 0.00307175
Iteration 10/25 | Loss: 0.00307175
Iteration 11/25 | Loss: 0.00307175
Iteration 12/25 | Loss: 0.00307175
Iteration 13/25 | Loss: 0.00307175
Iteration 14/25 | Loss: 0.00307175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0030717465560883284, 0.0030717465560883284, 0.0030717465560883284, 0.0030717465560883284, 0.0030717465560883284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030717465560883284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00307175
Iteration 2/1000 | Loss: 0.00003713
Iteration 3/1000 | Loss: 0.00003017
Iteration 4/1000 | Loss: 0.00002811
Iteration 5/1000 | Loss: 0.00002652
Iteration 6/1000 | Loss: 0.00002547
Iteration 7/1000 | Loss: 0.00002480
Iteration 8/1000 | Loss: 0.00002429
Iteration 9/1000 | Loss: 0.00002381
Iteration 10/1000 | Loss: 0.00002353
Iteration 11/1000 | Loss: 0.00002324
Iteration 12/1000 | Loss: 0.00002315
Iteration 13/1000 | Loss: 0.00002308
Iteration 14/1000 | Loss: 0.00002298
Iteration 15/1000 | Loss: 0.00002297
Iteration 16/1000 | Loss: 0.00002296
Iteration 17/1000 | Loss: 0.00002294
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002293
Iteration 20/1000 | Loss: 0.00002293
Iteration 21/1000 | Loss: 0.00002287
Iteration 22/1000 | Loss: 0.00002287
Iteration 23/1000 | Loss: 0.00002287
Iteration 24/1000 | Loss: 0.00002287
Iteration 25/1000 | Loss: 0.00002287
Iteration 26/1000 | Loss: 0.00002287
Iteration 27/1000 | Loss: 0.00002287
Iteration 28/1000 | Loss: 0.00002286
Iteration 29/1000 | Loss: 0.00002286
Iteration 30/1000 | Loss: 0.00002286
Iteration 31/1000 | Loss: 0.00002286
Iteration 32/1000 | Loss: 0.00002285
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002284
Iteration 36/1000 | Loss: 0.00002284
Iteration 37/1000 | Loss: 0.00002284
Iteration 38/1000 | Loss: 0.00002284
Iteration 39/1000 | Loss: 0.00002284
Iteration 40/1000 | Loss: 0.00002284
Iteration 41/1000 | Loss: 0.00002283
Iteration 42/1000 | Loss: 0.00002283
Iteration 43/1000 | Loss: 0.00002283
Iteration 44/1000 | Loss: 0.00002283
Iteration 45/1000 | Loss: 0.00002282
Iteration 46/1000 | Loss: 0.00002282
Iteration 47/1000 | Loss: 0.00002282
Iteration 48/1000 | Loss: 0.00002282
Iteration 49/1000 | Loss: 0.00002282
Iteration 50/1000 | Loss: 0.00002282
Iteration 51/1000 | Loss: 0.00002282
Iteration 52/1000 | Loss: 0.00002282
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002282
Iteration 56/1000 | Loss: 0.00002282
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002281
Iteration 59/1000 | Loss: 0.00002281
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002281
Iteration 62/1000 | Loss: 0.00002281
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002280
Iteration 66/1000 | Loss: 0.00002280
Iteration 67/1000 | Loss: 0.00002280
Iteration 68/1000 | Loss: 0.00002280
Iteration 69/1000 | Loss: 0.00002280
Iteration 70/1000 | Loss: 0.00002280
Iteration 71/1000 | Loss: 0.00002280
Iteration 72/1000 | Loss: 0.00002280
Iteration 73/1000 | Loss: 0.00002280
Iteration 74/1000 | Loss: 0.00002280
Iteration 75/1000 | Loss: 0.00002280
Iteration 76/1000 | Loss: 0.00002280
Iteration 77/1000 | Loss: 0.00002279
Iteration 78/1000 | Loss: 0.00002279
Iteration 79/1000 | Loss: 0.00002279
Iteration 80/1000 | Loss: 0.00002279
Iteration 81/1000 | Loss: 0.00002279
Iteration 82/1000 | Loss: 0.00002278
Iteration 83/1000 | Loss: 0.00002278
Iteration 84/1000 | Loss: 0.00002278
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00002277
Iteration 87/1000 | Loss: 0.00002277
Iteration 88/1000 | Loss: 0.00002277
Iteration 89/1000 | Loss: 0.00002277
Iteration 90/1000 | Loss: 0.00002277
Iteration 91/1000 | Loss: 0.00002277
Iteration 92/1000 | Loss: 0.00002277
Iteration 93/1000 | Loss: 0.00002277
Iteration 94/1000 | Loss: 0.00002277
Iteration 95/1000 | Loss: 0.00002277
Iteration 96/1000 | Loss: 0.00002277
Iteration 97/1000 | Loss: 0.00002277
Iteration 98/1000 | Loss: 0.00002277
Iteration 99/1000 | Loss: 0.00002277
Iteration 100/1000 | Loss: 0.00002277
Iteration 101/1000 | Loss: 0.00002277
Iteration 102/1000 | Loss: 0.00002277
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002277
Iteration 107/1000 | Loss: 0.00002277
Iteration 108/1000 | Loss: 0.00002277
Iteration 109/1000 | Loss: 0.00002277
Iteration 110/1000 | Loss: 0.00002277
Iteration 111/1000 | Loss: 0.00002277
Iteration 112/1000 | Loss: 0.00002277
Iteration 113/1000 | Loss: 0.00002277
Iteration 114/1000 | Loss: 0.00002277
Iteration 115/1000 | Loss: 0.00002277
Iteration 116/1000 | Loss: 0.00002277
Iteration 117/1000 | Loss: 0.00002277
Iteration 118/1000 | Loss: 0.00002277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.2765596440876834e-05, 2.2765596440876834e-05, 2.2765596440876834e-05, 2.2765596440876834e-05, 2.2765596440876834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2765596440876834e-05

Optimization complete. Final v2v error: 4.2992777824401855 mm

Highest mean error: 4.505969047546387 mm for frame 92

Lowest mean error: 4.023927688598633 mm for frame 76

Saving results

Total time: 38.542168855667114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598932
Iteration 2/25 | Loss: 0.00173482
Iteration 3/25 | Loss: 0.00152755
Iteration 4/25 | Loss: 0.00148996
Iteration 5/25 | Loss: 0.00148286
Iteration 6/25 | Loss: 0.00148118
Iteration 7/25 | Loss: 0.00148088
Iteration 8/25 | Loss: 0.00148088
Iteration 9/25 | Loss: 0.00148088
Iteration 10/25 | Loss: 0.00148088
Iteration 11/25 | Loss: 0.00148088
Iteration 12/25 | Loss: 0.00148088
Iteration 13/25 | Loss: 0.00148088
Iteration 14/25 | Loss: 0.00148088
Iteration 15/25 | Loss: 0.00148088
Iteration 16/25 | Loss: 0.00148088
Iteration 17/25 | Loss: 0.00148088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001480884850025177, 0.001480884850025177, 0.001480884850025177, 0.001480884850025177, 0.001480884850025177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001480884850025177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71979225
Iteration 2/25 | Loss: 0.00243334
Iteration 3/25 | Loss: 0.00243333
Iteration 4/25 | Loss: 0.00243333
Iteration 5/25 | Loss: 0.00243333
Iteration 6/25 | Loss: 0.00243333
Iteration 7/25 | Loss: 0.00243333
Iteration 8/25 | Loss: 0.00243333
Iteration 9/25 | Loss: 0.00243333
Iteration 10/25 | Loss: 0.00243333
Iteration 11/25 | Loss: 0.00243333
Iteration 12/25 | Loss: 0.00243333
Iteration 13/25 | Loss: 0.00243333
Iteration 14/25 | Loss: 0.00243333
Iteration 15/25 | Loss: 0.00243333
Iteration 16/25 | Loss: 0.00243333
Iteration 17/25 | Loss: 0.00243333
Iteration 18/25 | Loss: 0.00243333
Iteration 19/25 | Loss: 0.00243333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002433328190818429, 0.002433328190818429, 0.002433328190818429, 0.002433328190818429, 0.002433328190818429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002433328190818429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243333
Iteration 2/1000 | Loss: 0.00006143
Iteration 3/1000 | Loss: 0.00005009
Iteration 4/1000 | Loss: 0.00004391
Iteration 5/1000 | Loss: 0.00004114
Iteration 6/1000 | Loss: 0.00003947
Iteration 7/1000 | Loss: 0.00003850
Iteration 8/1000 | Loss: 0.00003763
Iteration 9/1000 | Loss: 0.00003706
Iteration 10/1000 | Loss: 0.00003667
Iteration 11/1000 | Loss: 0.00003640
Iteration 12/1000 | Loss: 0.00003620
Iteration 13/1000 | Loss: 0.00003605
Iteration 14/1000 | Loss: 0.00003603
Iteration 15/1000 | Loss: 0.00003603
Iteration 16/1000 | Loss: 0.00003600
Iteration 17/1000 | Loss: 0.00003598
Iteration 18/1000 | Loss: 0.00003598
Iteration 19/1000 | Loss: 0.00003596
Iteration 20/1000 | Loss: 0.00003595
Iteration 21/1000 | Loss: 0.00003595
Iteration 22/1000 | Loss: 0.00003595
Iteration 23/1000 | Loss: 0.00003595
Iteration 24/1000 | Loss: 0.00003595
Iteration 25/1000 | Loss: 0.00003594
Iteration 26/1000 | Loss: 0.00003594
Iteration 27/1000 | Loss: 0.00003594
Iteration 28/1000 | Loss: 0.00003594
Iteration 29/1000 | Loss: 0.00003593
Iteration 30/1000 | Loss: 0.00003593
Iteration 31/1000 | Loss: 0.00003593
Iteration 32/1000 | Loss: 0.00003591
Iteration 33/1000 | Loss: 0.00003591
Iteration 34/1000 | Loss: 0.00003591
Iteration 35/1000 | Loss: 0.00003591
Iteration 36/1000 | Loss: 0.00003591
Iteration 37/1000 | Loss: 0.00003591
Iteration 38/1000 | Loss: 0.00003591
Iteration 39/1000 | Loss: 0.00003590
Iteration 40/1000 | Loss: 0.00003590
Iteration 41/1000 | Loss: 0.00003590
Iteration 42/1000 | Loss: 0.00003590
Iteration 43/1000 | Loss: 0.00003590
Iteration 44/1000 | Loss: 0.00003590
Iteration 45/1000 | Loss: 0.00003590
Iteration 46/1000 | Loss: 0.00003590
Iteration 47/1000 | Loss: 0.00003590
Iteration 48/1000 | Loss: 0.00003590
Iteration 49/1000 | Loss: 0.00003590
Iteration 50/1000 | Loss: 0.00003589
Iteration 51/1000 | Loss: 0.00003588
Iteration 52/1000 | Loss: 0.00003588
Iteration 53/1000 | Loss: 0.00003587
Iteration 54/1000 | Loss: 0.00003587
Iteration 55/1000 | Loss: 0.00003586
Iteration 56/1000 | Loss: 0.00003585
Iteration 57/1000 | Loss: 0.00003585
Iteration 58/1000 | Loss: 0.00003584
Iteration 59/1000 | Loss: 0.00003584
Iteration 60/1000 | Loss: 0.00003584
Iteration 61/1000 | Loss: 0.00003584
Iteration 62/1000 | Loss: 0.00003584
Iteration 63/1000 | Loss: 0.00003584
Iteration 64/1000 | Loss: 0.00003584
Iteration 65/1000 | Loss: 0.00003584
Iteration 66/1000 | Loss: 0.00003583
Iteration 67/1000 | Loss: 0.00003583
Iteration 68/1000 | Loss: 0.00003581
Iteration 69/1000 | Loss: 0.00003581
Iteration 70/1000 | Loss: 0.00003581
Iteration 71/1000 | Loss: 0.00003581
Iteration 72/1000 | Loss: 0.00003581
Iteration 73/1000 | Loss: 0.00003581
Iteration 74/1000 | Loss: 0.00003581
Iteration 75/1000 | Loss: 0.00003581
Iteration 76/1000 | Loss: 0.00003581
Iteration 77/1000 | Loss: 0.00003581
Iteration 78/1000 | Loss: 0.00003580
Iteration 79/1000 | Loss: 0.00003580
Iteration 80/1000 | Loss: 0.00003580
Iteration 81/1000 | Loss: 0.00003580
Iteration 82/1000 | Loss: 0.00003580
Iteration 83/1000 | Loss: 0.00003580
Iteration 84/1000 | Loss: 0.00003579
Iteration 85/1000 | Loss: 0.00003579
Iteration 86/1000 | Loss: 0.00003579
Iteration 87/1000 | Loss: 0.00003579
Iteration 88/1000 | Loss: 0.00003579
Iteration 89/1000 | Loss: 0.00003579
Iteration 90/1000 | Loss: 0.00003579
Iteration 91/1000 | Loss: 0.00003579
Iteration 92/1000 | Loss: 0.00003579
Iteration 93/1000 | Loss: 0.00003579
Iteration 94/1000 | Loss: 0.00003578
Iteration 95/1000 | Loss: 0.00003578
Iteration 96/1000 | Loss: 0.00003578
Iteration 97/1000 | Loss: 0.00003578
Iteration 98/1000 | Loss: 0.00003578
Iteration 99/1000 | Loss: 0.00003577
Iteration 100/1000 | Loss: 0.00003577
Iteration 101/1000 | Loss: 0.00003577
Iteration 102/1000 | Loss: 0.00003577
Iteration 103/1000 | Loss: 0.00003577
Iteration 104/1000 | Loss: 0.00003577
Iteration 105/1000 | Loss: 0.00003577
Iteration 106/1000 | Loss: 0.00003577
Iteration 107/1000 | Loss: 0.00003577
Iteration 108/1000 | Loss: 0.00003577
Iteration 109/1000 | Loss: 0.00003576
Iteration 110/1000 | Loss: 0.00003576
Iteration 111/1000 | Loss: 0.00003576
Iteration 112/1000 | Loss: 0.00003576
Iteration 113/1000 | Loss: 0.00003576
Iteration 114/1000 | Loss: 0.00003576
Iteration 115/1000 | Loss: 0.00003576
Iteration 116/1000 | Loss: 0.00003576
Iteration 117/1000 | Loss: 0.00003576
Iteration 118/1000 | Loss: 0.00003576
Iteration 119/1000 | Loss: 0.00003576
Iteration 120/1000 | Loss: 0.00003576
Iteration 121/1000 | Loss: 0.00003576
Iteration 122/1000 | Loss: 0.00003576
Iteration 123/1000 | Loss: 0.00003576
Iteration 124/1000 | Loss: 0.00003576
Iteration 125/1000 | Loss: 0.00003576
Iteration 126/1000 | Loss: 0.00003576
Iteration 127/1000 | Loss: 0.00003576
Iteration 128/1000 | Loss: 0.00003576
Iteration 129/1000 | Loss: 0.00003576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [3.575990194804035e-05, 3.575990194804035e-05, 3.575990194804035e-05, 3.575990194804035e-05, 3.575990194804035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.575990194804035e-05

Optimization complete. Final v2v error: 5.181649208068848 mm

Highest mean error: 5.533308506011963 mm for frame 127

Lowest mean error: 4.5607805252075195 mm for frame 71

Saving results

Total time: 36.05328130722046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411091
Iteration 2/25 | Loss: 0.00146298
Iteration 3/25 | Loss: 0.00137087
Iteration 4/25 | Loss: 0.00135575
Iteration 5/25 | Loss: 0.00135111
Iteration 6/25 | Loss: 0.00134940
Iteration 7/25 | Loss: 0.00134928
Iteration 8/25 | Loss: 0.00134915
Iteration 9/25 | Loss: 0.00134914
Iteration 10/25 | Loss: 0.00134914
Iteration 11/25 | Loss: 0.00134914
Iteration 12/25 | Loss: 0.00134914
Iteration 13/25 | Loss: 0.00134914
Iteration 14/25 | Loss: 0.00134914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001349139492958784, 0.001349139492958784, 0.001349139492958784, 0.001349139492958784, 0.001349139492958784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001349139492958784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89120960
Iteration 2/25 | Loss: 0.00256216
Iteration 3/25 | Loss: 0.00256216
Iteration 4/25 | Loss: 0.00256216
Iteration 5/25 | Loss: 0.00256216
Iteration 6/25 | Loss: 0.00256216
Iteration 7/25 | Loss: 0.00256216
Iteration 8/25 | Loss: 0.00256216
Iteration 9/25 | Loss: 0.00256216
Iteration 10/25 | Loss: 0.00256216
Iteration 11/25 | Loss: 0.00256216
Iteration 12/25 | Loss: 0.00256216
Iteration 13/25 | Loss: 0.00256216
Iteration 14/25 | Loss: 0.00256216
Iteration 15/25 | Loss: 0.00256216
Iteration 16/25 | Loss: 0.00256216
Iteration 17/25 | Loss: 0.00256216
Iteration 18/25 | Loss: 0.00256216
Iteration 19/25 | Loss: 0.00256216
Iteration 20/25 | Loss: 0.00256216
Iteration 21/25 | Loss: 0.00256216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002562160836532712, 0.002562160836532712, 0.002562160836532712, 0.002562160836532712, 0.002562160836532712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002562160836532712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256216
Iteration 2/1000 | Loss: 0.00004165
Iteration 3/1000 | Loss: 0.00002659
Iteration 4/1000 | Loss: 0.00002187
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001798
Iteration 12/1000 | Loss: 0.00001782
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001771
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001769
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001768
Iteration 19/1000 | Loss: 0.00001767
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001755
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001750
Iteration 30/1000 | Loss: 0.00001750
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001746
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001745
Iteration 50/1000 | Loss: 0.00001745
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001743
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001743
Iteration 56/1000 | Loss: 0.00001743
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001740
Iteration 64/1000 | Loss: 0.00001740
Iteration 65/1000 | Loss: 0.00001740
Iteration 66/1000 | Loss: 0.00001740
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001739
Iteration 69/1000 | Loss: 0.00001739
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001738
Iteration 72/1000 | Loss: 0.00001738
Iteration 73/1000 | Loss: 0.00001738
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001737
Iteration 83/1000 | Loss: 0.00001737
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001737
Iteration 87/1000 | Loss: 0.00001737
Iteration 88/1000 | Loss: 0.00001737
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001736
Iteration 92/1000 | Loss: 0.00001736
Iteration 93/1000 | Loss: 0.00001736
Iteration 94/1000 | Loss: 0.00001736
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001735
Iteration 99/1000 | Loss: 0.00001735
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001735
Iteration 106/1000 | Loss: 0.00001735
Iteration 107/1000 | Loss: 0.00001735
Iteration 108/1000 | Loss: 0.00001735
Iteration 109/1000 | Loss: 0.00001735
Iteration 110/1000 | Loss: 0.00001735
Iteration 111/1000 | Loss: 0.00001735
Iteration 112/1000 | Loss: 0.00001735
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001735
Iteration 118/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.734549368848093e-05, 1.734549368848093e-05, 1.734549368848093e-05, 1.734549368848093e-05, 1.734549368848093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.734549368848093e-05

Optimization complete. Final v2v error: 3.6659185886383057 mm

Highest mean error: 4.167795181274414 mm for frame 72

Lowest mean error: 3.520339250564575 mm for frame 118

Saving results

Total time: 34.74635171890259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608342
Iteration 2/25 | Loss: 0.00148113
Iteration 3/25 | Loss: 0.00141259
Iteration 4/25 | Loss: 0.00139935
Iteration 5/25 | Loss: 0.00139658
Iteration 6/25 | Loss: 0.00139573
Iteration 7/25 | Loss: 0.00139573
Iteration 8/25 | Loss: 0.00139573
Iteration 9/25 | Loss: 0.00139573
Iteration 10/25 | Loss: 0.00139573
Iteration 11/25 | Loss: 0.00139573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001395733910612762, 0.001395733910612762, 0.001395733910612762, 0.001395733910612762, 0.001395733910612762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001395733910612762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.61763811
Iteration 2/25 | Loss: 0.00247710
Iteration 3/25 | Loss: 0.00247709
Iteration 4/25 | Loss: 0.00247708
Iteration 5/25 | Loss: 0.00247708
Iteration 6/25 | Loss: 0.00247708
Iteration 7/25 | Loss: 0.00247708
Iteration 8/25 | Loss: 0.00247708
Iteration 9/25 | Loss: 0.00247708
Iteration 10/25 | Loss: 0.00247708
Iteration 11/25 | Loss: 0.00247708
Iteration 12/25 | Loss: 0.00247708
Iteration 13/25 | Loss: 0.00247708
Iteration 14/25 | Loss: 0.00247708
Iteration 15/25 | Loss: 0.00247708
Iteration 16/25 | Loss: 0.00247708
Iteration 17/25 | Loss: 0.00247708
Iteration 18/25 | Loss: 0.00247708
Iteration 19/25 | Loss: 0.00247708
Iteration 20/25 | Loss: 0.00247708
Iteration 21/25 | Loss: 0.00247708
Iteration 22/25 | Loss: 0.00247708
Iteration 23/25 | Loss: 0.00247708
Iteration 24/25 | Loss: 0.00247708
Iteration 25/25 | Loss: 0.00247708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247708
Iteration 2/1000 | Loss: 0.00003580
Iteration 3/1000 | Loss: 0.00002846
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002555
Iteration 6/1000 | Loss: 0.00002481
Iteration 7/1000 | Loss: 0.00002440
Iteration 8/1000 | Loss: 0.00002418
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002402
Iteration 11/1000 | Loss: 0.00002387
Iteration 12/1000 | Loss: 0.00002381
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002377
Iteration 15/1000 | Loss: 0.00002377
Iteration 16/1000 | Loss: 0.00002374
Iteration 17/1000 | Loss: 0.00002374
Iteration 18/1000 | Loss: 0.00002374
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002374
Iteration 22/1000 | Loss: 0.00002374
Iteration 23/1000 | Loss: 0.00002374
Iteration 24/1000 | Loss: 0.00002374
Iteration 25/1000 | Loss: 0.00002374
Iteration 26/1000 | Loss: 0.00002373
Iteration 27/1000 | Loss: 0.00002373
Iteration 28/1000 | Loss: 0.00002373
Iteration 29/1000 | Loss: 0.00002372
Iteration 30/1000 | Loss: 0.00002372
Iteration 31/1000 | Loss: 0.00002372
Iteration 32/1000 | Loss: 0.00002371
Iteration 33/1000 | Loss: 0.00002371
Iteration 34/1000 | Loss: 0.00002370
Iteration 35/1000 | Loss: 0.00002370
Iteration 36/1000 | Loss: 0.00002370
Iteration 37/1000 | Loss: 0.00002370
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002369
Iteration 42/1000 | Loss: 0.00002369
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00002368
Iteration 45/1000 | Loss: 0.00002368
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00002368
Iteration 48/1000 | Loss: 0.00002368
Iteration 49/1000 | Loss: 0.00002368
Iteration 50/1000 | Loss: 0.00002368
Iteration 51/1000 | Loss: 0.00002367
Iteration 52/1000 | Loss: 0.00002367
Iteration 53/1000 | Loss: 0.00002367
Iteration 54/1000 | Loss: 0.00002367
Iteration 55/1000 | Loss: 0.00002367
Iteration 56/1000 | Loss: 0.00002367
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002367
Iteration 59/1000 | Loss: 0.00002366
Iteration 60/1000 | Loss: 0.00002366
Iteration 61/1000 | Loss: 0.00002366
Iteration 62/1000 | Loss: 0.00002365
Iteration 63/1000 | Loss: 0.00002365
Iteration 64/1000 | Loss: 0.00002365
Iteration 65/1000 | Loss: 0.00002365
Iteration 66/1000 | Loss: 0.00002364
Iteration 67/1000 | Loss: 0.00002364
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002363
Iteration 73/1000 | Loss: 0.00002363
Iteration 74/1000 | Loss: 0.00002363
Iteration 75/1000 | Loss: 0.00002363
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002362
Iteration 79/1000 | Loss: 0.00002362
Iteration 80/1000 | Loss: 0.00002362
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002361
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002359
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002358
Iteration 92/1000 | Loss: 0.00002358
Iteration 93/1000 | Loss: 0.00002358
Iteration 94/1000 | Loss: 0.00002357
Iteration 95/1000 | Loss: 0.00002357
Iteration 96/1000 | Loss: 0.00002357
Iteration 97/1000 | Loss: 0.00002357
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002355
Iteration 101/1000 | Loss: 0.00002355
Iteration 102/1000 | Loss: 0.00002355
Iteration 103/1000 | Loss: 0.00002355
Iteration 104/1000 | Loss: 0.00002355
Iteration 105/1000 | Loss: 0.00002355
Iteration 106/1000 | Loss: 0.00002355
Iteration 107/1000 | Loss: 0.00002355
Iteration 108/1000 | Loss: 0.00002354
Iteration 109/1000 | Loss: 0.00002354
Iteration 110/1000 | Loss: 0.00002354
Iteration 111/1000 | Loss: 0.00002354
Iteration 112/1000 | Loss: 0.00002354
Iteration 113/1000 | Loss: 0.00002354
Iteration 114/1000 | Loss: 0.00002354
Iteration 115/1000 | Loss: 0.00002353
Iteration 116/1000 | Loss: 0.00002353
Iteration 117/1000 | Loss: 0.00002353
Iteration 118/1000 | Loss: 0.00002353
Iteration 119/1000 | Loss: 0.00002353
Iteration 120/1000 | Loss: 0.00002353
Iteration 121/1000 | Loss: 0.00002353
Iteration 122/1000 | Loss: 0.00002353
Iteration 123/1000 | Loss: 0.00002353
Iteration 124/1000 | Loss: 0.00002353
Iteration 125/1000 | Loss: 0.00002353
Iteration 126/1000 | Loss: 0.00002353
Iteration 127/1000 | Loss: 0.00002353
Iteration 128/1000 | Loss: 0.00002353
Iteration 129/1000 | Loss: 0.00002352
Iteration 130/1000 | Loss: 0.00002352
Iteration 131/1000 | Loss: 0.00002352
Iteration 132/1000 | Loss: 0.00002352
Iteration 133/1000 | Loss: 0.00002352
Iteration 134/1000 | Loss: 0.00002352
Iteration 135/1000 | Loss: 0.00002352
Iteration 136/1000 | Loss: 0.00002352
Iteration 137/1000 | Loss: 0.00002352
Iteration 138/1000 | Loss: 0.00002352
Iteration 139/1000 | Loss: 0.00002352
Iteration 140/1000 | Loss: 0.00002352
Iteration 141/1000 | Loss: 0.00002352
Iteration 142/1000 | Loss: 0.00002352
Iteration 143/1000 | Loss: 0.00002352
Iteration 144/1000 | Loss: 0.00002351
Iteration 145/1000 | Loss: 0.00002351
Iteration 146/1000 | Loss: 0.00002351
Iteration 147/1000 | Loss: 0.00002351
Iteration 148/1000 | Loss: 0.00002351
Iteration 149/1000 | Loss: 0.00002351
Iteration 150/1000 | Loss: 0.00002351
Iteration 151/1000 | Loss: 0.00002351
Iteration 152/1000 | Loss: 0.00002351
Iteration 153/1000 | Loss: 0.00002351
Iteration 154/1000 | Loss: 0.00002351
Iteration 155/1000 | Loss: 0.00002351
Iteration 156/1000 | Loss: 0.00002351
Iteration 157/1000 | Loss: 0.00002351
Iteration 158/1000 | Loss: 0.00002351
Iteration 159/1000 | Loss: 0.00002351
Iteration 160/1000 | Loss: 0.00002351
Iteration 161/1000 | Loss: 0.00002351
Iteration 162/1000 | Loss: 0.00002350
Iteration 163/1000 | Loss: 0.00002350
Iteration 164/1000 | Loss: 0.00002350
Iteration 165/1000 | Loss: 0.00002350
Iteration 166/1000 | Loss: 0.00002350
Iteration 167/1000 | Loss: 0.00002350
Iteration 168/1000 | Loss: 0.00002350
Iteration 169/1000 | Loss: 0.00002350
Iteration 170/1000 | Loss: 0.00002350
Iteration 171/1000 | Loss: 0.00002350
Iteration 172/1000 | Loss: 0.00002350
Iteration 173/1000 | Loss: 0.00002350
Iteration 174/1000 | Loss: 0.00002350
Iteration 175/1000 | Loss: 0.00002350
Iteration 176/1000 | Loss: 0.00002350
Iteration 177/1000 | Loss: 0.00002350
Iteration 178/1000 | Loss: 0.00002350
Iteration 179/1000 | Loss: 0.00002350
Iteration 180/1000 | Loss: 0.00002350
Iteration 181/1000 | Loss: 0.00002350
Iteration 182/1000 | Loss: 0.00002350
Iteration 183/1000 | Loss: 0.00002350
Iteration 184/1000 | Loss: 0.00002350
Iteration 185/1000 | Loss: 0.00002350
Iteration 186/1000 | Loss: 0.00002350
Iteration 187/1000 | Loss: 0.00002350
Iteration 188/1000 | Loss: 0.00002350
Iteration 189/1000 | Loss: 0.00002350
Iteration 190/1000 | Loss: 0.00002350
Iteration 191/1000 | Loss: 0.00002350
Iteration 192/1000 | Loss: 0.00002350
Iteration 193/1000 | Loss: 0.00002350
Iteration 194/1000 | Loss: 0.00002350
Iteration 195/1000 | Loss: 0.00002350
Iteration 196/1000 | Loss: 0.00002350
Iteration 197/1000 | Loss: 0.00002350
Iteration 198/1000 | Loss: 0.00002350
Iteration 199/1000 | Loss: 0.00002350
Iteration 200/1000 | Loss: 0.00002350
Iteration 201/1000 | Loss: 0.00002350
Iteration 202/1000 | Loss: 0.00002350
Iteration 203/1000 | Loss: 0.00002350
Iteration 204/1000 | Loss: 0.00002350
Iteration 205/1000 | Loss: 0.00002350
Iteration 206/1000 | Loss: 0.00002350
Iteration 207/1000 | Loss: 0.00002350
Iteration 208/1000 | Loss: 0.00002350
Iteration 209/1000 | Loss: 0.00002350
Iteration 210/1000 | Loss: 0.00002350
Iteration 211/1000 | Loss: 0.00002350
Iteration 212/1000 | Loss: 0.00002350
Iteration 213/1000 | Loss: 0.00002350
Iteration 214/1000 | Loss: 0.00002350
Iteration 215/1000 | Loss: 0.00002350
Iteration 216/1000 | Loss: 0.00002350
Iteration 217/1000 | Loss: 0.00002350
Iteration 218/1000 | Loss: 0.00002350
Iteration 219/1000 | Loss: 0.00002350
Iteration 220/1000 | Loss: 0.00002350
Iteration 221/1000 | Loss: 0.00002350
Iteration 222/1000 | Loss: 0.00002350
Iteration 223/1000 | Loss: 0.00002350
Iteration 224/1000 | Loss: 0.00002350
Iteration 225/1000 | Loss: 0.00002350
Iteration 226/1000 | Loss: 0.00002350
Iteration 227/1000 | Loss: 0.00002350
Iteration 228/1000 | Loss: 0.00002350
Iteration 229/1000 | Loss: 0.00002350
Iteration 230/1000 | Loss: 0.00002350
Iteration 231/1000 | Loss: 0.00002350
Iteration 232/1000 | Loss: 0.00002350
Iteration 233/1000 | Loss: 0.00002350
Iteration 234/1000 | Loss: 0.00002350
Iteration 235/1000 | Loss: 0.00002350
Iteration 236/1000 | Loss: 0.00002350
Iteration 237/1000 | Loss: 0.00002350
Iteration 238/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.3495193090639077e-05, 2.3495193090639077e-05, 2.3495193090639077e-05, 2.3495193090639077e-05, 2.3495193090639077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3495193090639077e-05

Optimization complete. Final v2v error: 4.219156265258789 mm

Highest mean error: 4.5892438888549805 mm for frame 62

Lowest mean error: 3.8932580947875977 mm for frame 27

Saving results

Total time: 36.52902579307556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439360
Iteration 2/25 | Loss: 0.00150108
Iteration 3/25 | Loss: 0.00140428
Iteration 4/25 | Loss: 0.00139233
Iteration 5/25 | Loss: 0.00138552
Iteration 6/25 | Loss: 0.00138343
Iteration 7/25 | Loss: 0.00138343
Iteration 8/25 | Loss: 0.00138343
Iteration 9/25 | Loss: 0.00138343
Iteration 10/25 | Loss: 0.00138343
Iteration 11/25 | Loss: 0.00138343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013834337005391717, 0.0013834337005391717, 0.0013834337005391717, 0.0013834337005391717, 0.0013834337005391717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013834337005391717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78945184
Iteration 2/25 | Loss: 0.00274810
Iteration 3/25 | Loss: 0.00274809
Iteration 4/25 | Loss: 0.00274809
Iteration 5/25 | Loss: 0.00274809
Iteration 6/25 | Loss: 0.00274809
Iteration 7/25 | Loss: 0.00274809
Iteration 8/25 | Loss: 0.00274809
Iteration 9/25 | Loss: 0.00274809
Iteration 10/25 | Loss: 0.00274809
Iteration 11/25 | Loss: 0.00274809
Iteration 12/25 | Loss: 0.00274809
Iteration 13/25 | Loss: 0.00274809
Iteration 14/25 | Loss: 0.00274809
Iteration 15/25 | Loss: 0.00274809
Iteration 16/25 | Loss: 0.00274809
Iteration 17/25 | Loss: 0.00274809
Iteration 18/25 | Loss: 0.00274809
Iteration 19/25 | Loss: 0.00274809
Iteration 20/25 | Loss: 0.00274809
Iteration 21/25 | Loss: 0.00274809
Iteration 22/25 | Loss: 0.00274809
Iteration 23/25 | Loss: 0.00274809
Iteration 24/25 | Loss: 0.00274809
Iteration 25/25 | Loss: 0.00274809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0027480933349579573, 0.0027480933349579573, 0.0027480933349579573, 0.0027480933349579573, 0.0027480933349579573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027480933349579573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00274809
Iteration 2/1000 | Loss: 0.00003721
Iteration 3/1000 | Loss: 0.00002736
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00002355
Iteration 6/1000 | Loss: 0.00002251
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002098
Iteration 10/1000 | Loss: 0.00002079
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002043
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00002015
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00001997
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001990
Iteration 20/1000 | Loss: 0.00001990
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001988
Iteration 24/1000 | Loss: 0.00001987
Iteration 25/1000 | Loss: 0.00001987
Iteration 26/1000 | Loss: 0.00001987
Iteration 27/1000 | Loss: 0.00001986
Iteration 28/1000 | Loss: 0.00001986
Iteration 29/1000 | Loss: 0.00001986
Iteration 30/1000 | Loss: 0.00001986
Iteration 31/1000 | Loss: 0.00001985
Iteration 32/1000 | Loss: 0.00001985
Iteration 33/1000 | Loss: 0.00001985
Iteration 34/1000 | Loss: 0.00001985
Iteration 35/1000 | Loss: 0.00001984
Iteration 36/1000 | Loss: 0.00001984
Iteration 37/1000 | Loss: 0.00001984
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001983
Iteration 44/1000 | Loss: 0.00001983
Iteration 45/1000 | Loss: 0.00001983
Iteration 46/1000 | Loss: 0.00001983
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001982
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001982
Iteration 53/1000 | Loss: 0.00001982
Iteration 54/1000 | Loss: 0.00001981
Iteration 55/1000 | Loss: 0.00001981
Iteration 56/1000 | Loss: 0.00001981
Iteration 57/1000 | Loss: 0.00001980
Iteration 58/1000 | Loss: 0.00001980
Iteration 59/1000 | Loss: 0.00001980
Iteration 60/1000 | Loss: 0.00001980
Iteration 61/1000 | Loss: 0.00001980
Iteration 62/1000 | Loss: 0.00001979
Iteration 63/1000 | Loss: 0.00001979
Iteration 64/1000 | Loss: 0.00001979
Iteration 65/1000 | Loss: 0.00001979
Iteration 66/1000 | Loss: 0.00001979
Iteration 67/1000 | Loss: 0.00001979
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001978
Iteration 70/1000 | Loss: 0.00001978
Iteration 71/1000 | Loss: 0.00001978
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001977
Iteration 74/1000 | Loss: 0.00001977
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00001976
Iteration 80/1000 | Loss: 0.00001976
Iteration 81/1000 | Loss: 0.00001976
Iteration 82/1000 | Loss: 0.00001975
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001975
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001974
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001973
Iteration 95/1000 | Loss: 0.00001973
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001972
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001971
Iteration 103/1000 | Loss: 0.00001971
Iteration 104/1000 | Loss: 0.00001971
Iteration 105/1000 | Loss: 0.00001971
Iteration 106/1000 | Loss: 0.00001971
Iteration 107/1000 | Loss: 0.00001971
Iteration 108/1000 | Loss: 0.00001971
Iteration 109/1000 | Loss: 0.00001971
Iteration 110/1000 | Loss: 0.00001971
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001970
Iteration 113/1000 | Loss: 0.00001970
Iteration 114/1000 | Loss: 0.00001970
Iteration 115/1000 | Loss: 0.00001970
Iteration 116/1000 | Loss: 0.00001970
Iteration 117/1000 | Loss: 0.00001970
Iteration 118/1000 | Loss: 0.00001970
Iteration 119/1000 | Loss: 0.00001970
Iteration 120/1000 | Loss: 0.00001970
Iteration 121/1000 | Loss: 0.00001970
Iteration 122/1000 | Loss: 0.00001970
Iteration 123/1000 | Loss: 0.00001969
Iteration 124/1000 | Loss: 0.00001969
Iteration 125/1000 | Loss: 0.00001969
Iteration 126/1000 | Loss: 0.00001969
Iteration 127/1000 | Loss: 0.00001969
Iteration 128/1000 | Loss: 0.00001969
Iteration 129/1000 | Loss: 0.00001969
Iteration 130/1000 | Loss: 0.00001969
Iteration 131/1000 | Loss: 0.00001969
Iteration 132/1000 | Loss: 0.00001969
Iteration 133/1000 | Loss: 0.00001969
Iteration 134/1000 | Loss: 0.00001969
Iteration 135/1000 | Loss: 0.00001969
Iteration 136/1000 | Loss: 0.00001969
Iteration 137/1000 | Loss: 0.00001969
Iteration 138/1000 | Loss: 0.00001969
Iteration 139/1000 | Loss: 0.00001969
Iteration 140/1000 | Loss: 0.00001969
Iteration 141/1000 | Loss: 0.00001969
Iteration 142/1000 | Loss: 0.00001969
Iteration 143/1000 | Loss: 0.00001969
Iteration 144/1000 | Loss: 0.00001969
Iteration 145/1000 | Loss: 0.00001969
Iteration 146/1000 | Loss: 0.00001969
Iteration 147/1000 | Loss: 0.00001969
Iteration 148/1000 | Loss: 0.00001969
Iteration 149/1000 | Loss: 0.00001969
Iteration 150/1000 | Loss: 0.00001969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.9694401999004185e-05, 1.9694401999004185e-05, 1.9694401999004185e-05, 1.9694401999004185e-05, 1.9694401999004185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9694401999004185e-05

Optimization complete. Final v2v error: 3.9241538047790527 mm

Highest mean error: 4.420468330383301 mm for frame 130

Lowest mean error: 3.521575450897217 mm for frame 208

Saving results

Total time: 43.42958354949951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2409/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2409/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946500
Iteration 2/25 | Loss: 0.00153923
Iteration 3/25 | Loss: 0.00142579
Iteration 4/25 | Loss: 0.00141022
Iteration 5/25 | Loss: 0.00140433
Iteration 6/25 | Loss: 0.00140276
Iteration 7/25 | Loss: 0.00140266
Iteration 8/25 | Loss: 0.00140266
Iteration 9/25 | Loss: 0.00140266
Iteration 10/25 | Loss: 0.00140266
Iteration 11/25 | Loss: 0.00140266
Iteration 12/25 | Loss: 0.00140266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014026624849066138, 0.0014026624849066138, 0.0014026624849066138, 0.0014026624849066138, 0.0014026624849066138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014026624849066138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61317182
Iteration 2/25 | Loss: 0.00255586
Iteration 3/25 | Loss: 0.00255583
Iteration 4/25 | Loss: 0.00255583
Iteration 5/25 | Loss: 0.00255583
Iteration 6/25 | Loss: 0.00255583
Iteration 7/25 | Loss: 0.00255583
Iteration 8/25 | Loss: 0.00255583
Iteration 9/25 | Loss: 0.00255583
Iteration 10/25 | Loss: 0.00255583
Iteration 11/25 | Loss: 0.00255583
Iteration 12/25 | Loss: 0.00255583
Iteration 13/25 | Loss: 0.00255583
Iteration 14/25 | Loss: 0.00255583
Iteration 15/25 | Loss: 0.00255583
Iteration 16/25 | Loss: 0.00255583
Iteration 17/25 | Loss: 0.00255583
Iteration 18/25 | Loss: 0.00255583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002555827610194683, 0.002555827610194683, 0.002555827610194683, 0.002555827610194683, 0.002555827610194683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002555827610194683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255583
Iteration 2/1000 | Loss: 0.00003754
Iteration 3/1000 | Loss: 0.00002880
Iteration 4/1000 | Loss: 0.00002531
Iteration 5/1000 | Loss: 0.00002421
Iteration 6/1000 | Loss: 0.00002327
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002224
Iteration 9/1000 | Loss: 0.00002199
Iteration 10/1000 | Loss: 0.00002186
Iteration 11/1000 | Loss: 0.00002171
Iteration 12/1000 | Loss: 0.00002170
Iteration 13/1000 | Loss: 0.00002166
Iteration 14/1000 | Loss: 0.00002165
Iteration 15/1000 | Loss: 0.00002163
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002155
Iteration 18/1000 | Loss: 0.00002152
Iteration 19/1000 | Loss: 0.00002150
Iteration 20/1000 | Loss: 0.00002147
Iteration 21/1000 | Loss: 0.00002146
Iteration 22/1000 | Loss: 0.00002144
Iteration 23/1000 | Loss: 0.00002143
Iteration 24/1000 | Loss: 0.00002142
Iteration 25/1000 | Loss: 0.00002142
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002139
Iteration 28/1000 | Loss: 0.00002139
Iteration 29/1000 | Loss: 0.00002139
Iteration 30/1000 | Loss: 0.00002138
Iteration 31/1000 | Loss: 0.00002138
Iteration 32/1000 | Loss: 0.00002137
Iteration 33/1000 | Loss: 0.00002137
Iteration 34/1000 | Loss: 0.00002136
Iteration 35/1000 | Loss: 0.00002136
Iteration 36/1000 | Loss: 0.00002136
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002135
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002135
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002135
Iteration 43/1000 | Loss: 0.00002135
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002135
Iteration 46/1000 | Loss: 0.00002135
Iteration 47/1000 | Loss: 0.00002135
Iteration 48/1000 | Loss: 0.00002135
Iteration 49/1000 | Loss: 0.00002135
Iteration 50/1000 | Loss: 0.00002135
Iteration 51/1000 | Loss: 0.00002134
Iteration 52/1000 | Loss: 0.00002134
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002133
Iteration 55/1000 | Loss: 0.00002133
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002132
Iteration 64/1000 | Loss: 0.00002132
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00002131
Iteration 67/1000 | Loss: 0.00002131
Iteration 68/1000 | Loss: 0.00002131
Iteration 69/1000 | Loss: 0.00002131
Iteration 70/1000 | Loss: 0.00002131
Iteration 71/1000 | Loss: 0.00002131
Iteration 72/1000 | Loss: 0.00002130
Iteration 73/1000 | Loss: 0.00002130
Iteration 74/1000 | Loss: 0.00002130
Iteration 75/1000 | Loss: 0.00002130
Iteration 76/1000 | Loss: 0.00002130
Iteration 77/1000 | Loss: 0.00002130
Iteration 78/1000 | Loss: 0.00002130
Iteration 79/1000 | Loss: 0.00002129
Iteration 80/1000 | Loss: 0.00002129
Iteration 81/1000 | Loss: 0.00002129
Iteration 82/1000 | Loss: 0.00002129
Iteration 83/1000 | Loss: 0.00002129
Iteration 84/1000 | Loss: 0.00002129
Iteration 85/1000 | Loss: 0.00002129
Iteration 86/1000 | Loss: 0.00002129
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002128
Iteration 89/1000 | Loss: 0.00002128
Iteration 90/1000 | Loss: 0.00002128
Iteration 91/1000 | Loss: 0.00002128
Iteration 92/1000 | Loss: 0.00002128
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002128
Iteration 96/1000 | Loss: 0.00002127
Iteration 97/1000 | Loss: 0.00002127
Iteration 98/1000 | Loss: 0.00002127
Iteration 99/1000 | Loss: 0.00002127
Iteration 100/1000 | Loss: 0.00002127
Iteration 101/1000 | Loss: 0.00002127
Iteration 102/1000 | Loss: 0.00002127
Iteration 103/1000 | Loss: 0.00002127
Iteration 104/1000 | Loss: 0.00002127
Iteration 105/1000 | Loss: 0.00002127
Iteration 106/1000 | Loss: 0.00002126
Iteration 107/1000 | Loss: 0.00002126
Iteration 108/1000 | Loss: 0.00002126
Iteration 109/1000 | Loss: 0.00002126
Iteration 110/1000 | Loss: 0.00002126
Iteration 111/1000 | Loss: 0.00002126
Iteration 112/1000 | Loss: 0.00002126
Iteration 113/1000 | Loss: 0.00002126
Iteration 114/1000 | Loss: 0.00002126
Iteration 115/1000 | Loss: 0.00002126
Iteration 116/1000 | Loss: 0.00002126
Iteration 117/1000 | Loss: 0.00002126
Iteration 118/1000 | Loss: 0.00002126
Iteration 119/1000 | Loss: 0.00002126
Iteration 120/1000 | Loss: 0.00002126
Iteration 121/1000 | Loss: 0.00002125
Iteration 122/1000 | Loss: 0.00002125
Iteration 123/1000 | Loss: 0.00002125
Iteration 124/1000 | Loss: 0.00002125
Iteration 125/1000 | Loss: 0.00002125
Iteration 126/1000 | Loss: 0.00002125
Iteration 127/1000 | Loss: 0.00002125
Iteration 128/1000 | Loss: 0.00002125
Iteration 129/1000 | Loss: 0.00002125
Iteration 130/1000 | Loss: 0.00002125
Iteration 131/1000 | Loss: 0.00002125
Iteration 132/1000 | Loss: 0.00002125
Iteration 133/1000 | Loss: 0.00002125
Iteration 134/1000 | Loss: 0.00002125
Iteration 135/1000 | Loss: 0.00002125
Iteration 136/1000 | Loss: 0.00002125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.125259197782725e-05, 2.125259197782725e-05, 2.125259197782725e-05, 2.125259197782725e-05, 2.125259197782725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.125259197782725e-05

Optimization complete. Final v2v error: 4.104184627532959 mm

Highest mean error: 4.343003749847412 mm for frame 77

Lowest mean error: 3.9424071311950684 mm for frame 15

Saving results

Total time: 34.51989722251892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005279
Iteration 2/25 | Loss: 0.01005279
Iteration 3/25 | Loss: 0.01005279
Iteration 4/25 | Loss: 0.01005279
Iteration 5/25 | Loss: 0.01005278
Iteration 6/25 | Loss: 0.01005278
Iteration 7/25 | Loss: 0.01005278
Iteration 8/25 | Loss: 0.01005278
Iteration 9/25 | Loss: 0.01005278
Iteration 10/25 | Loss: 0.01005278
Iteration 11/25 | Loss: 0.01005278
Iteration 12/25 | Loss: 0.01005277
Iteration 13/25 | Loss: 0.01005277
Iteration 14/25 | Loss: 0.01005277
Iteration 15/25 | Loss: 0.01005277
Iteration 16/25 | Loss: 0.01005277
Iteration 17/25 | Loss: 0.01005277
Iteration 18/25 | Loss: 0.01005276
Iteration 19/25 | Loss: 0.01005276
Iteration 20/25 | Loss: 0.01005276
Iteration 21/25 | Loss: 0.01005276
Iteration 22/25 | Loss: 0.01005276
Iteration 23/25 | Loss: 0.01005276
Iteration 24/25 | Loss: 0.01005276
Iteration 25/25 | Loss: 0.01005276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49024594
Iteration 2/25 | Loss: 0.17643857
Iteration 3/25 | Loss: 0.17547776
Iteration 4/25 | Loss: 0.17466532
Iteration 5/25 | Loss: 0.17466532
Iteration 6/25 | Loss: 0.17466527
Iteration 7/25 | Loss: 0.17466527
Iteration 8/25 | Loss: 0.17466527
Iteration 9/25 | Loss: 0.17466527
Iteration 10/25 | Loss: 0.17466527
Iteration 11/25 | Loss: 0.17466527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.17466527223587036, 0.17466527223587036, 0.17466527223587036, 0.17466527223587036, 0.17466527223587036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17466527223587036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17466527
Iteration 2/1000 | Loss: 0.01137324
Iteration 3/1000 | Loss: 0.00331664
Iteration 4/1000 | Loss: 0.00061337
Iteration 5/1000 | Loss: 0.00038234
Iteration 6/1000 | Loss: 0.00062816
Iteration 7/1000 | Loss: 0.00013928
Iteration 8/1000 | Loss: 0.00012384
Iteration 9/1000 | Loss: 0.00042876
Iteration 10/1000 | Loss: 0.00006669
Iteration 11/1000 | Loss: 0.00016073
Iteration 12/1000 | Loss: 0.00006699
Iteration 13/1000 | Loss: 0.00017535
Iteration 14/1000 | Loss: 0.00003852
Iteration 15/1000 | Loss: 0.00022644
Iteration 16/1000 | Loss: 0.00011583
Iteration 17/1000 | Loss: 0.00004184
Iteration 18/1000 | Loss: 0.00003086
Iteration 19/1000 | Loss: 0.00005476
Iteration 20/1000 | Loss: 0.00002843
Iteration 21/1000 | Loss: 0.00005655
Iteration 22/1000 | Loss: 0.00008729
Iteration 23/1000 | Loss: 0.00003085
Iteration 24/1000 | Loss: 0.00006643
Iteration 25/1000 | Loss: 0.00008265
Iteration 26/1000 | Loss: 0.00003442
Iteration 27/1000 | Loss: 0.00004940
Iteration 28/1000 | Loss: 0.00026798
Iteration 29/1000 | Loss: 0.00002908
Iteration 30/1000 | Loss: 0.00002433
Iteration 31/1000 | Loss: 0.00004855
Iteration 32/1000 | Loss: 0.00002380
Iteration 33/1000 | Loss: 0.00005542
Iteration 34/1000 | Loss: 0.00007064
Iteration 35/1000 | Loss: 0.00002280
Iteration 36/1000 | Loss: 0.00003699
Iteration 37/1000 | Loss: 0.00002235
Iteration 38/1000 | Loss: 0.00004320
Iteration 39/1000 | Loss: 0.00002190
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002181
Iteration 42/1000 | Loss: 0.00003607
Iteration 43/1000 | Loss: 0.00002161
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002157
Iteration 46/1000 | Loss: 0.00002154
Iteration 47/1000 | Loss: 0.00002142
Iteration 48/1000 | Loss: 0.00002135
Iteration 49/1000 | Loss: 0.00002134
Iteration 50/1000 | Loss: 0.00002132
Iteration 51/1000 | Loss: 0.00002131
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002131
Iteration 54/1000 | Loss: 0.00002130
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00004483
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002121
Iteration 64/1000 | Loss: 0.00002120
Iteration 65/1000 | Loss: 0.00002120
Iteration 66/1000 | Loss: 0.00002119
Iteration 67/1000 | Loss: 0.00002119
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002118
Iteration 70/1000 | Loss: 0.00002117
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002117
Iteration 74/1000 | Loss: 0.00002117
Iteration 75/1000 | Loss: 0.00002117
Iteration 76/1000 | Loss: 0.00002117
Iteration 77/1000 | Loss: 0.00002117
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002116
Iteration 81/1000 | Loss: 0.00002116
Iteration 82/1000 | Loss: 0.00002115
Iteration 83/1000 | Loss: 0.00002115
Iteration 84/1000 | Loss: 0.00002114
Iteration 85/1000 | Loss: 0.00002113
Iteration 86/1000 | Loss: 0.00002113
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00004635
Iteration 90/1000 | Loss: 0.00009964
Iteration 91/1000 | Loss: 0.00002728
Iteration 92/1000 | Loss: 0.00002118
Iteration 93/1000 | Loss: 0.00002116
Iteration 94/1000 | Loss: 0.00003162
Iteration 95/1000 | Loss: 0.00003502
Iteration 96/1000 | Loss: 0.00002112
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002304
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002107
Iteration 104/1000 | Loss: 0.00002107
Iteration 105/1000 | Loss: 0.00002107
Iteration 106/1000 | Loss: 0.00002107
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002107
Iteration 109/1000 | Loss: 0.00002106
Iteration 110/1000 | Loss: 0.00002106
Iteration 111/1000 | Loss: 0.00002106
Iteration 112/1000 | Loss: 0.00002106
Iteration 113/1000 | Loss: 0.00002106
Iteration 114/1000 | Loss: 0.00002106
Iteration 115/1000 | Loss: 0.00002106
Iteration 116/1000 | Loss: 0.00002106
Iteration 117/1000 | Loss: 0.00002106
Iteration 118/1000 | Loss: 0.00002106
Iteration 119/1000 | Loss: 0.00002106
Iteration 120/1000 | Loss: 0.00002106
Iteration 121/1000 | Loss: 0.00002106
Iteration 122/1000 | Loss: 0.00002106
Iteration 123/1000 | Loss: 0.00002106
Iteration 124/1000 | Loss: 0.00002106
Iteration 125/1000 | Loss: 0.00002106
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002105
Iteration 128/1000 | Loss: 0.00002105
Iteration 129/1000 | Loss: 0.00002105
Iteration 130/1000 | Loss: 0.00002105
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002105
Iteration 133/1000 | Loss: 0.00002105
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00002105
Iteration 136/1000 | Loss: 0.00002104
Iteration 137/1000 | Loss: 0.00002104
Iteration 138/1000 | Loss: 0.00002104
Iteration 139/1000 | Loss: 0.00002104
Iteration 140/1000 | Loss: 0.00002104
Iteration 141/1000 | Loss: 0.00002104
Iteration 142/1000 | Loss: 0.00002104
Iteration 143/1000 | Loss: 0.00002104
Iteration 144/1000 | Loss: 0.00002104
Iteration 145/1000 | Loss: 0.00002104
Iteration 146/1000 | Loss: 0.00002104
Iteration 147/1000 | Loss: 0.00002104
Iteration 148/1000 | Loss: 0.00002781
Iteration 149/1000 | Loss: 0.00002104
Iteration 150/1000 | Loss: 0.00002103
Iteration 151/1000 | Loss: 0.00002103
Iteration 152/1000 | Loss: 0.00002102
Iteration 153/1000 | Loss: 0.00002102
Iteration 154/1000 | Loss: 0.00002102
Iteration 155/1000 | Loss: 0.00002102
Iteration 156/1000 | Loss: 0.00002102
Iteration 157/1000 | Loss: 0.00002102
Iteration 158/1000 | Loss: 0.00002102
Iteration 159/1000 | Loss: 0.00002102
Iteration 160/1000 | Loss: 0.00002102
Iteration 161/1000 | Loss: 0.00002102
Iteration 162/1000 | Loss: 0.00002102
Iteration 163/1000 | Loss: 0.00002102
Iteration 164/1000 | Loss: 0.00002102
Iteration 165/1000 | Loss: 0.00002102
Iteration 166/1000 | Loss: 0.00002102
Iteration 167/1000 | Loss: 0.00002102
Iteration 168/1000 | Loss: 0.00002102
Iteration 169/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.102058715536259e-05, 2.102058715536259e-05, 2.102058715536259e-05, 2.102058715536259e-05, 2.102058715536259e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.102058715536259e-05

Optimization complete. Final v2v error: 3.826036214828491 mm

Highest mean error: 4.210681915283203 mm for frame 73

Lowest mean error: 3.502605676651001 mm for frame 221

Saving results

Total time: 98.78119993209839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994049
Iteration 2/25 | Loss: 0.00250827
Iteration 3/25 | Loss: 0.00185281
Iteration 4/25 | Loss: 0.00166564
Iteration 5/25 | Loss: 0.00157256
Iteration 6/25 | Loss: 0.00155319
Iteration 7/25 | Loss: 0.00150650
Iteration 8/25 | Loss: 0.00150148
Iteration 9/25 | Loss: 0.00148082
Iteration 10/25 | Loss: 0.00147871
Iteration 11/25 | Loss: 0.00146957
Iteration 12/25 | Loss: 0.00146319
Iteration 13/25 | Loss: 0.00145413
Iteration 14/25 | Loss: 0.00145645
Iteration 15/25 | Loss: 0.00145552
Iteration 16/25 | Loss: 0.00145591
Iteration 17/25 | Loss: 0.00144137
Iteration 18/25 | Loss: 0.00143549
Iteration 19/25 | Loss: 0.00143575
Iteration 20/25 | Loss: 0.00142706
Iteration 21/25 | Loss: 0.00142709
Iteration 22/25 | Loss: 0.00142269
Iteration 23/25 | Loss: 0.00142125
Iteration 24/25 | Loss: 0.00141979
Iteration 25/25 | Loss: 0.00141994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39478898
Iteration 2/25 | Loss: 0.00156723
Iteration 3/25 | Loss: 0.00151935
Iteration 4/25 | Loss: 0.00151935
Iteration 5/25 | Loss: 0.00151935
Iteration 6/25 | Loss: 0.00151935
Iteration 7/25 | Loss: 0.00151935
Iteration 8/25 | Loss: 0.00151935
Iteration 9/25 | Loss: 0.00151935
Iteration 10/25 | Loss: 0.00151935
Iteration 11/25 | Loss: 0.00151935
Iteration 12/25 | Loss: 0.00151935
Iteration 13/25 | Loss: 0.00151935
Iteration 14/25 | Loss: 0.00151935
Iteration 15/25 | Loss: 0.00151935
Iteration 16/25 | Loss: 0.00151935
Iteration 17/25 | Loss: 0.00151935
Iteration 18/25 | Loss: 0.00151935
Iteration 19/25 | Loss: 0.00151935
Iteration 20/25 | Loss: 0.00151935
Iteration 21/25 | Loss: 0.00151935
Iteration 22/25 | Loss: 0.00151935
Iteration 23/25 | Loss: 0.00151935
Iteration 24/25 | Loss: 0.00151935
Iteration 25/25 | Loss: 0.00151935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151935
Iteration 2/1000 | Loss: 0.00143397
Iteration 3/1000 | Loss: 0.00055502
Iteration 4/1000 | Loss: 0.00013513
Iteration 5/1000 | Loss: 0.00012776
Iteration 6/1000 | Loss: 0.00037204
Iteration 7/1000 | Loss: 0.00032574
Iteration 8/1000 | Loss: 0.00090584
Iteration 9/1000 | Loss: 0.00054973
Iteration 10/1000 | Loss: 0.00036829
Iteration 11/1000 | Loss: 0.00035686
Iteration 12/1000 | Loss: 0.00012879
Iteration 13/1000 | Loss: 0.00011862
Iteration 14/1000 | Loss: 0.00011340
Iteration 15/1000 | Loss: 0.00009929
Iteration 16/1000 | Loss: 0.00041383
Iteration 17/1000 | Loss: 0.00036684
Iteration 18/1000 | Loss: 0.00107562
Iteration 19/1000 | Loss: 0.00041884
Iteration 20/1000 | Loss: 0.00010421
Iteration 21/1000 | Loss: 0.00010399
Iteration 22/1000 | Loss: 0.00015315
Iteration 23/1000 | Loss: 0.00008863
Iteration 24/1000 | Loss: 0.00008167
Iteration 25/1000 | Loss: 0.00009508
Iteration 26/1000 | Loss: 0.00009279
Iteration 27/1000 | Loss: 0.00008264
Iteration 28/1000 | Loss: 0.00008245
Iteration 29/1000 | Loss: 0.00007828
Iteration 30/1000 | Loss: 0.00008341
Iteration 31/1000 | Loss: 0.00008973
Iteration 32/1000 | Loss: 0.00009686
Iteration 33/1000 | Loss: 0.00024946
Iteration 34/1000 | Loss: 0.00229850
Iteration 35/1000 | Loss: 0.00325884
Iteration 36/1000 | Loss: 0.00196040
Iteration 37/1000 | Loss: 0.00076792
Iteration 38/1000 | Loss: 0.00059625
Iteration 39/1000 | Loss: 0.00048664
Iteration 40/1000 | Loss: 0.00030336
Iteration 41/1000 | Loss: 0.00049889
Iteration 42/1000 | Loss: 0.00054956
Iteration 43/1000 | Loss: 0.00036475
Iteration 44/1000 | Loss: 0.00009997
Iteration 45/1000 | Loss: 0.00039031
Iteration 46/1000 | Loss: 0.00043795
Iteration 47/1000 | Loss: 0.00045717
Iteration 48/1000 | Loss: 0.00009966
Iteration 49/1000 | Loss: 0.00022429
Iteration 50/1000 | Loss: 0.00237806
Iteration 51/1000 | Loss: 0.00046945
Iteration 52/1000 | Loss: 0.00016049
Iteration 53/1000 | Loss: 0.00006206
Iteration 54/1000 | Loss: 0.00010974
Iteration 55/1000 | Loss: 0.00005235
Iteration 56/1000 | Loss: 0.00009543
Iteration 57/1000 | Loss: 0.00033823
Iteration 58/1000 | Loss: 0.00053156
Iteration 59/1000 | Loss: 0.00032863
Iteration 60/1000 | Loss: 0.00032042
Iteration 61/1000 | Loss: 0.00006233
Iteration 62/1000 | Loss: 0.00005451
Iteration 63/1000 | Loss: 0.00005457
Iteration 64/1000 | Loss: 0.00019835
Iteration 65/1000 | Loss: 0.00022997
Iteration 66/1000 | Loss: 0.00139027
Iteration 67/1000 | Loss: 0.00145419
Iteration 68/1000 | Loss: 0.00038480
Iteration 69/1000 | Loss: 0.00063976
Iteration 70/1000 | Loss: 0.00030116
Iteration 71/1000 | Loss: 0.00019609
Iteration 72/1000 | Loss: 0.00019222
Iteration 73/1000 | Loss: 0.00021598
Iteration 74/1000 | Loss: 0.00016550
Iteration 75/1000 | Loss: 0.00006052
Iteration 76/1000 | Loss: 0.00003823
Iteration 77/1000 | Loss: 0.00024105
Iteration 78/1000 | Loss: 0.00004289
Iteration 79/1000 | Loss: 0.00003083
Iteration 80/1000 | Loss: 0.00027900
Iteration 81/1000 | Loss: 0.00015754
Iteration 82/1000 | Loss: 0.00002709
Iteration 83/1000 | Loss: 0.00030536
Iteration 84/1000 | Loss: 0.00003836
Iteration 85/1000 | Loss: 0.00002594
Iteration 86/1000 | Loss: 0.00002281
Iteration 87/1000 | Loss: 0.00002064
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001633
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001566
Iteration 95/1000 | Loss: 0.00001544
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00002252
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00001666
Iteration 101/1000 | Loss: 0.00001515
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001446
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001445
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001436
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001435
Iteration 132/1000 | Loss: 0.00001435
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001432
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001432
Iteration 143/1000 | Loss: 0.00001432
Iteration 144/1000 | Loss: 0.00001431
Iteration 145/1000 | Loss: 0.00001431
Iteration 146/1000 | Loss: 0.00001431
Iteration 147/1000 | Loss: 0.00001431
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001429
Iteration 155/1000 | Loss: 0.00001429
Iteration 156/1000 | Loss: 0.00001428
Iteration 157/1000 | Loss: 0.00001428
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001426
Iteration 162/1000 | Loss: 0.00001426
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001425
Iteration 166/1000 | Loss: 0.00001425
Iteration 167/1000 | Loss: 0.00001425
Iteration 168/1000 | Loss: 0.00001425
Iteration 169/1000 | Loss: 0.00001425
Iteration 170/1000 | Loss: 0.00001425
Iteration 171/1000 | Loss: 0.00001424
Iteration 172/1000 | Loss: 0.00001424
Iteration 173/1000 | Loss: 0.00001424
Iteration 174/1000 | Loss: 0.00001424
Iteration 175/1000 | Loss: 0.00001424
Iteration 176/1000 | Loss: 0.00001424
Iteration 177/1000 | Loss: 0.00001424
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001423
Iteration 184/1000 | Loss: 0.00001423
Iteration 185/1000 | Loss: 0.00001423
Iteration 186/1000 | Loss: 0.00001423
Iteration 187/1000 | Loss: 0.00001423
Iteration 188/1000 | Loss: 0.00001423
Iteration 189/1000 | Loss: 0.00001423
Iteration 190/1000 | Loss: 0.00001423
Iteration 191/1000 | Loss: 0.00001423
Iteration 192/1000 | Loss: 0.00001423
Iteration 193/1000 | Loss: 0.00001423
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001423
Iteration 196/1000 | Loss: 0.00001423
Iteration 197/1000 | Loss: 0.00001423
Iteration 198/1000 | Loss: 0.00001423
Iteration 199/1000 | Loss: 0.00001423
Iteration 200/1000 | Loss: 0.00001423
Iteration 201/1000 | Loss: 0.00001423
Iteration 202/1000 | Loss: 0.00001423
Iteration 203/1000 | Loss: 0.00001423
Iteration 204/1000 | Loss: 0.00001423
Iteration 205/1000 | Loss: 0.00001423
Iteration 206/1000 | Loss: 0.00001423
Iteration 207/1000 | Loss: 0.00001423
Iteration 208/1000 | Loss: 0.00001423
Iteration 209/1000 | Loss: 0.00001423
Iteration 210/1000 | Loss: 0.00001423
Iteration 211/1000 | Loss: 0.00001423
Iteration 212/1000 | Loss: 0.00001423
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Iteration 216/1000 | Loss: 0.00001423
Iteration 217/1000 | Loss: 0.00001423
Iteration 218/1000 | Loss: 0.00001423
Iteration 219/1000 | Loss: 0.00001423
Iteration 220/1000 | Loss: 0.00001423
Iteration 221/1000 | Loss: 0.00001423
Iteration 222/1000 | Loss: 0.00001423
Iteration 223/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.4226092389435507e-05, 1.4226092389435507e-05, 1.4226092389435507e-05, 1.4226092389435507e-05, 1.4226092389435507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4226092389435507e-05

Optimization complete. Final v2v error: 3.122774124145508 mm

Highest mean error: 5.277121543884277 mm for frame 55

Lowest mean error: 2.8579366207122803 mm for frame 40

Saving results

Total time: 193.92356324195862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708001
Iteration 2/25 | Loss: 0.00164250
Iteration 3/25 | Loss: 0.00146240
Iteration 4/25 | Loss: 0.00144319
Iteration 5/25 | Loss: 0.00143975
Iteration 6/25 | Loss: 0.00142751
Iteration 7/25 | Loss: 0.00142651
Iteration 8/25 | Loss: 0.00142581
Iteration 9/25 | Loss: 0.00141893
Iteration 10/25 | Loss: 0.00141802
Iteration 11/25 | Loss: 0.00141658
Iteration 12/25 | Loss: 0.00141223
Iteration 13/25 | Loss: 0.00141157
Iteration 14/25 | Loss: 0.00141113
Iteration 15/25 | Loss: 0.00141388
Iteration 16/25 | Loss: 0.00140963
Iteration 17/25 | Loss: 0.00140869
Iteration 18/25 | Loss: 0.00140847
Iteration 19/25 | Loss: 0.00140847
Iteration 20/25 | Loss: 0.00140847
Iteration 21/25 | Loss: 0.00140847
Iteration 22/25 | Loss: 0.00140847
Iteration 23/25 | Loss: 0.00140847
Iteration 24/25 | Loss: 0.00140847
Iteration 25/25 | Loss: 0.00140847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40361667
Iteration 2/25 | Loss: 0.00107259
Iteration 3/25 | Loss: 0.00107258
Iteration 4/25 | Loss: 0.00107258
Iteration 5/25 | Loss: 0.00107258
Iteration 6/25 | Loss: 0.00107258
Iteration 7/25 | Loss: 0.00107258
Iteration 8/25 | Loss: 0.00107258
Iteration 9/25 | Loss: 0.00107258
Iteration 10/25 | Loss: 0.00107258
Iteration 11/25 | Loss: 0.00107258
Iteration 12/25 | Loss: 0.00107258
Iteration 13/25 | Loss: 0.00107258
Iteration 14/25 | Loss: 0.00107258
Iteration 15/25 | Loss: 0.00107258
Iteration 16/25 | Loss: 0.00107258
Iteration 17/25 | Loss: 0.00107258
Iteration 18/25 | Loss: 0.00107258
Iteration 19/25 | Loss: 0.00107258
Iteration 20/25 | Loss: 0.00107258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001072580344043672, 0.001072580344043672, 0.001072580344043672, 0.001072580344043672, 0.001072580344043672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001072580344043672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107258
Iteration 2/1000 | Loss: 0.00006137
Iteration 3/1000 | Loss: 0.00004292
Iteration 4/1000 | Loss: 0.00003808
Iteration 5/1000 | Loss: 0.00003560
Iteration 6/1000 | Loss: 0.00003395
Iteration 7/1000 | Loss: 0.00003297
Iteration 8/1000 | Loss: 0.00003239
Iteration 9/1000 | Loss: 0.00003190
Iteration 10/1000 | Loss: 0.00003150
Iteration 11/1000 | Loss: 0.00003120
Iteration 12/1000 | Loss: 0.00003092
Iteration 13/1000 | Loss: 0.00003070
Iteration 14/1000 | Loss: 0.00003052
Iteration 15/1000 | Loss: 0.00003030
Iteration 16/1000 | Loss: 0.00003014
Iteration 17/1000 | Loss: 0.00003008
Iteration 18/1000 | Loss: 0.00002993
Iteration 19/1000 | Loss: 0.00002981
Iteration 20/1000 | Loss: 0.00002976
Iteration 21/1000 | Loss: 0.00002974
Iteration 22/1000 | Loss: 0.00002971
Iteration 23/1000 | Loss: 0.00002969
Iteration 24/1000 | Loss: 0.00002968
Iteration 25/1000 | Loss: 0.00002968
Iteration 26/1000 | Loss: 0.00002967
Iteration 27/1000 | Loss: 0.00002963
Iteration 28/1000 | Loss: 0.00002959
Iteration 29/1000 | Loss: 0.00002959
Iteration 30/1000 | Loss: 0.00002958
Iteration 31/1000 | Loss: 0.00002958
Iteration 32/1000 | Loss: 0.00002957
Iteration 33/1000 | Loss: 0.00002956
Iteration 34/1000 | Loss: 0.00002956
Iteration 35/1000 | Loss: 0.00002956
Iteration 36/1000 | Loss: 0.00002955
Iteration 37/1000 | Loss: 0.00002954
Iteration 38/1000 | Loss: 0.00002954
Iteration 39/1000 | Loss: 0.00002953
Iteration 40/1000 | Loss: 0.00002952
Iteration 41/1000 | Loss: 0.00002952
Iteration 42/1000 | Loss: 0.00002951
Iteration 43/1000 | Loss: 0.00002951
Iteration 44/1000 | Loss: 0.00002950
Iteration 45/1000 | Loss: 0.00002950
Iteration 46/1000 | Loss: 0.00002949
Iteration 47/1000 | Loss: 0.00002948
Iteration 48/1000 | Loss: 0.00002948
Iteration 49/1000 | Loss: 0.00002947
Iteration 50/1000 | Loss: 0.00002947
Iteration 51/1000 | Loss: 0.00002946
Iteration 52/1000 | Loss: 0.00002945
Iteration 53/1000 | Loss: 0.00002944
Iteration 54/1000 | Loss: 0.00002944
Iteration 55/1000 | Loss: 0.00002943
Iteration 56/1000 | Loss: 0.00002942
Iteration 57/1000 | Loss: 0.00002941
Iteration 58/1000 | Loss: 0.00002941
Iteration 59/1000 | Loss: 0.00002940
Iteration 60/1000 | Loss: 0.00002940
Iteration 61/1000 | Loss: 0.00002939
Iteration 62/1000 | Loss: 0.00002939
Iteration 63/1000 | Loss: 0.00002939
Iteration 64/1000 | Loss: 0.00002938
Iteration 65/1000 | Loss: 0.00002938
Iteration 66/1000 | Loss: 0.00002937
Iteration 67/1000 | Loss: 0.00002937
Iteration 68/1000 | Loss: 0.00002937
Iteration 69/1000 | Loss: 0.00002936
Iteration 70/1000 | Loss: 0.00002936
Iteration 71/1000 | Loss: 0.00002936
Iteration 72/1000 | Loss: 0.00002936
Iteration 73/1000 | Loss: 0.00002935
Iteration 74/1000 | Loss: 0.00002935
Iteration 75/1000 | Loss: 0.00002934
Iteration 76/1000 | Loss: 0.00002934
Iteration 77/1000 | Loss: 0.00002934
Iteration 78/1000 | Loss: 0.00002933
Iteration 79/1000 | Loss: 0.00002933
Iteration 80/1000 | Loss: 0.00002933
Iteration 81/1000 | Loss: 0.00002932
Iteration 82/1000 | Loss: 0.00002932
Iteration 83/1000 | Loss: 0.00002931
Iteration 84/1000 | Loss: 0.00002931
Iteration 85/1000 | Loss: 0.00002930
Iteration 86/1000 | Loss: 0.00002930
Iteration 87/1000 | Loss: 0.00002929
Iteration 88/1000 | Loss: 0.00002929
Iteration 89/1000 | Loss: 0.00002928
Iteration 90/1000 | Loss: 0.00002928
Iteration 91/1000 | Loss: 0.00002928
Iteration 92/1000 | Loss: 0.00002927
Iteration 93/1000 | Loss: 0.00002927
Iteration 94/1000 | Loss: 0.00002927
Iteration 95/1000 | Loss: 0.00002926
Iteration 96/1000 | Loss: 0.00002926
Iteration 97/1000 | Loss: 0.00002926
Iteration 98/1000 | Loss: 0.00002925
Iteration 99/1000 | Loss: 0.00002925
Iteration 100/1000 | Loss: 0.00002925
Iteration 101/1000 | Loss: 0.00002924
Iteration 102/1000 | Loss: 0.00002924
Iteration 103/1000 | Loss: 0.00002923
Iteration 104/1000 | Loss: 0.00002923
Iteration 105/1000 | Loss: 0.00002923
Iteration 106/1000 | Loss: 0.00002922
Iteration 107/1000 | Loss: 0.00002922
Iteration 108/1000 | Loss: 0.00002921
Iteration 109/1000 | Loss: 0.00002921
Iteration 110/1000 | Loss: 0.00002921
Iteration 111/1000 | Loss: 0.00002921
Iteration 112/1000 | Loss: 0.00002921
Iteration 113/1000 | Loss: 0.00002920
Iteration 114/1000 | Loss: 0.00002920
Iteration 115/1000 | Loss: 0.00002920
Iteration 116/1000 | Loss: 0.00002919
Iteration 117/1000 | Loss: 0.00002919
Iteration 118/1000 | Loss: 0.00002919
Iteration 119/1000 | Loss: 0.00002918
Iteration 120/1000 | Loss: 0.00002918
Iteration 121/1000 | Loss: 0.00002918
Iteration 122/1000 | Loss: 0.00002918
Iteration 123/1000 | Loss: 0.00002917
Iteration 124/1000 | Loss: 0.00002917
Iteration 125/1000 | Loss: 0.00002917
Iteration 126/1000 | Loss: 0.00002916
Iteration 127/1000 | Loss: 0.00002916
Iteration 128/1000 | Loss: 0.00002916
Iteration 129/1000 | Loss: 0.00002916
Iteration 130/1000 | Loss: 0.00002916
Iteration 131/1000 | Loss: 0.00002915
Iteration 132/1000 | Loss: 0.00002915
Iteration 133/1000 | Loss: 0.00002915
Iteration 134/1000 | Loss: 0.00002915
Iteration 135/1000 | Loss: 0.00002914
Iteration 136/1000 | Loss: 0.00002914
Iteration 137/1000 | Loss: 0.00002914
Iteration 138/1000 | Loss: 0.00002914
Iteration 139/1000 | Loss: 0.00002914
Iteration 140/1000 | Loss: 0.00002913
Iteration 141/1000 | Loss: 0.00002913
Iteration 142/1000 | Loss: 0.00002913
Iteration 143/1000 | Loss: 0.00002913
Iteration 144/1000 | Loss: 0.00002913
Iteration 145/1000 | Loss: 0.00002913
Iteration 146/1000 | Loss: 0.00002913
Iteration 147/1000 | Loss: 0.00002912
Iteration 148/1000 | Loss: 0.00002912
Iteration 149/1000 | Loss: 0.00002912
Iteration 150/1000 | Loss: 0.00002912
Iteration 151/1000 | Loss: 0.00002912
Iteration 152/1000 | Loss: 0.00002912
Iteration 153/1000 | Loss: 0.00002911
Iteration 154/1000 | Loss: 0.00002911
Iteration 155/1000 | Loss: 0.00002911
Iteration 156/1000 | Loss: 0.00002911
Iteration 157/1000 | Loss: 0.00002911
Iteration 158/1000 | Loss: 0.00002911
Iteration 159/1000 | Loss: 0.00002911
Iteration 160/1000 | Loss: 0.00002911
Iteration 161/1000 | Loss: 0.00002911
Iteration 162/1000 | Loss: 0.00002911
Iteration 163/1000 | Loss: 0.00002911
Iteration 164/1000 | Loss: 0.00002910
Iteration 165/1000 | Loss: 0.00002910
Iteration 166/1000 | Loss: 0.00002910
Iteration 167/1000 | Loss: 0.00002910
Iteration 168/1000 | Loss: 0.00002910
Iteration 169/1000 | Loss: 0.00002910
Iteration 170/1000 | Loss: 0.00002910
Iteration 171/1000 | Loss: 0.00002910
Iteration 172/1000 | Loss: 0.00002910
Iteration 173/1000 | Loss: 0.00002910
Iteration 174/1000 | Loss: 0.00002910
Iteration 175/1000 | Loss: 0.00002910
Iteration 176/1000 | Loss: 0.00002910
Iteration 177/1000 | Loss: 0.00002910
Iteration 178/1000 | Loss: 0.00002909
Iteration 179/1000 | Loss: 0.00002909
Iteration 180/1000 | Loss: 0.00002909
Iteration 181/1000 | Loss: 0.00002909
Iteration 182/1000 | Loss: 0.00002909
Iteration 183/1000 | Loss: 0.00002909
Iteration 184/1000 | Loss: 0.00002909
Iteration 185/1000 | Loss: 0.00002909
Iteration 186/1000 | Loss: 0.00002909
Iteration 187/1000 | Loss: 0.00002909
Iteration 188/1000 | Loss: 0.00002909
Iteration 189/1000 | Loss: 0.00002909
Iteration 190/1000 | Loss: 0.00002909
Iteration 191/1000 | Loss: 0.00002908
Iteration 192/1000 | Loss: 0.00002908
Iteration 193/1000 | Loss: 0.00002908
Iteration 194/1000 | Loss: 0.00002908
Iteration 195/1000 | Loss: 0.00002908
Iteration 196/1000 | Loss: 0.00002908
Iteration 197/1000 | Loss: 0.00002908
Iteration 198/1000 | Loss: 0.00002908
Iteration 199/1000 | Loss: 0.00002908
Iteration 200/1000 | Loss: 0.00002907
Iteration 201/1000 | Loss: 0.00002907
Iteration 202/1000 | Loss: 0.00002907
Iteration 203/1000 | Loss: 0.00002907
Iteration 204/1000 | Loss: 0.00002907
Iteration 205/1000 | Loss: 0.00002907
Iteration 206/1000 | Loss: 0.00002907
Iteration 207/1000 | Loss: 0.00002907
Iteration 208/1000 | Loss: 0.00002907
Iteration 209/1000 | Loss: 0.00002907
Iteration 210/1000 | Loss: 0.00002907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.9073575205984525e-05, 2.9073575205984525e-05, 2.9073575205984525e-05, 2.9073575205984525e-05, 2.9073575205984525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9073575205984525e-05

Optimization complete. Final v2v error: 3.9672698974609375 mm

Highest mean error: 11.140865325927734 mm for frame 178

Lowest mean error: 3.0369608402252197 mm for frame 188

Saving results

Total time: 83.07694268226624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487492
Iteration 2/25 | Loss: 0.00163113
Iteration 3/25 | Loss: 0.00136692
Iteration 4/25 | Loss: 0.00134170
Iteration 5/25 | Loss: 0.00133898
Iteration 6/25 | Loss: 0.00133898
Iteration 7/25 | Loss: 0.00133898
Iteration 8/25 | Loss: 0.00133894
Iteration 9/25 | Loss: 0.00133894
Iteration 10/25 | Loss: 0.00133894
Iteration 11/25 | Loss: 0.00133894
Iteration 12/25 | Loss: 0.00133894
Iteration 13/25 | Loss: 0.00133894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013389402301982045, 0.0013389402301982045, 0.0013389402301982045, 0.0013389402301982045, 0.0013389402301982045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013389402301982045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45078850
Iteration 2/25 | Loss: 0.00068686
Iteration 3/25 | Loss: 0.00068686
Iteration 4/25 | Loss: 0.00068685
Iteration 5/25 | Loss: 0.00068685
Iteration 6/25 | Loss: 0.00068685
Iteration 7/25 | Loss: 0.00068685
Iteration 8/25 | Loss: 0.00068685
Iteration 9/25 | Loss: 0.00068685
Iteration 10/25 | Loss: 0.00068685
Iteration 11/25 | Loss: 0.00068685
Iteration 12/25 | Loss: 0.00068685
Iteration 13/25 | Loss: 0.00068685
Iteration 14/25 | Loss: 0.00068685
Iteration 15/25 | Loss: 0.00068685
Iteration 16/25 | Loss: 0.00068685
Iteration 17/25 | Loss: 0.00068685
Iteration 18/25 | Loss: 0.00068685
Iteration 19/25 | Loss: 0.00068685
Iteration 20/25 | Loss: 0.00068685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006868517375551164, 0.0006868517375551164, 0.0006868517375551164, 0.0006868517375551164, 0.0006868517375551164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006868517375551164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068685
Iteration 2/1000 | Loss: 0.00003757
Iteration 3/1000 | Loss: 0.00002594
Iteration 4/1000 | Loss: 0.00002389
Iteration 5/1000 | Loss: 0.00002266
Iteration 6/1000 | Loss: 0.00002162
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001961
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001918
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001890
Iteration 16/1000 | Loss: 0.00001889
Iteration 17/1000 | Loss: 0.00001885
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001881
Iteration 29/1000 | Loss: 0.00001881
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001880
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001869
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001868
Iteration 53/1000 | Loss: 0.00001868
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001863
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001862
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001860
Iteration 76/1000 | Loss: 0.00001860
Iteration 77/1000 | Loss: 0.00001860
Iteration 78/1000 | Loss: 0.00001859
Iteration 79/1000 | Loss: 0.00001859
Iteration 80/1000 | Loss: 0.00001859
Iteration 81/1000 | Loss: 0.00001858
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001857
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001855
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001853
Iteration 116/1000 | Loss: 0.00001853
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001851
Iteration 120/1000 | Loss: 0.00001851
Iteration 121/1000 | Loss: 0.00001851
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001850
Iteration 127/1000 | Loss: 0.00001850
Iteration 128/1000 | Loss: 0.00001850
Iteration 129/1000 | Loss: 0.00001850
Iteration 130/1000 | Loss: 0.00001850
Iteration 131/1000 | Loss: 0.00001850
Iteration 132/1000 | Loss: 0.00001849
Iteration 133/1000 | Loss: 0.00001849
Iteration 134/1000 | Loss: 0.00001849
Iteration 135/1000 | Loss: 0.00001849
Iteration 136/1000 | Loss: 0.00001849
Iteration 137/1000 | Loss: 0.00001849
Iteration 138/1000 | Loss: 0.00001849
Iteration 139/1000 | Loss: 0.00001849
Iteration 140/1000 | Loss: 0.00001849
Iteration 141/1000 | Loss: 0.00001849
Iteration 142/1000 | Loss: 0.00001849
Iteration 143/1000 | Loss: 0.00001849
Iteration 144/1000 | Loss: 0.00001849
Iteration 145/1000 | Loss: 0.00001849
Iteration 146/1000 | Loss: 0.00001849
Iteration 147/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.8493748939363286e-05, 1.8493748939363286e-05, 1.8493748939363286e-05, 1.8493748939363286e-05, 1.8493748939363286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8493748939363286e-05

Optimization complete. Final v2v error: 3.6164815425872803 mm

Highest mean error: 4.133370876312256 mm for frame 133

Lowest mean error: 3.228874683380127 mm for frame 4

Saving results

Total time: 44.75651144981384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980004
Iteration 2/25 | Loss: 0.00162342
Iteration 3/25 | Loss: 0.00143637
Iteration 4/25 | Loss: 0.00140371
Iteration 5/25 | Loss: 0.00140387
Iteration 6/25 | Loss: 0.00136543
Iteration 7/25 | Loss: 0.00133499
Iteration 8/25 | Loss: 0.00132521
Iteration 9/25 | Loss: 0.00132284
Iteration 10/25 | Loss: 0.00132203
Iteration 11/25 | Loss: 0.00132173
Iteration 12/25 | Loss: 0.00132168
Iteration 13/25 | Loss: 0.00132168
Iteration 14/25 | Loss: 0.00132168
Iteration 15/25 | Loss: 0.00132168
Iteration 16/25 | Loss: 0.00132168
Iteration 17/25 | Loss: 0.00132168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013216827064752579, 0.0013216827064752579, 0.0013216827064752579, 0.0013216827064752579, 0.0013216827064752579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013216827064752579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92359209
Iteration 2/25 | Loss: 0.00107898
Iteration 3/25 | Loss: 0.00107898
Iteration 4/25 | Loss: 0.00107898
Iteration 5/25 | Loss: 0.00107898
Iteration 6/25 | Loss: 0.00107898
Iteration 7/25 | Loss: 0.00107898
Iteration 8/25 | Loss: 0.00107898
Iteration 9/25 | Loss: 0.00107898
Iteration 10/25 | Loss: 0.00107898
Iteration 11/25 | Loss: 0.00107898
Iteration 12/25 | Loss: 0.00107898
Iteration 13/25 | Loss: 0.00107898
Iteration 14/25 | Loss: 0.00107898
Iteration 15/25 | Loss: 0.00107898
Iteration 16/25 | Loss: 0.00107898
Iteration 17/25 | Loss: 0.00107898
Iteration 18/25 | Loss: 0.00107898
Iteration 19/25 | Loss: 0.00107898
Iteration 20/25 | Loss: 0.00107898
Iteration 21/25 | Loss: 0.00107898
Iteration 22/25 | Loss: 0.00107898
Iteration 23/25 | Loss: 0.00107898
Iteration 24/25 | Loss: 0.00107898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010789766674861312, 0.0010789766674861312, 0.0010789766674861312, 0.0010789766674861312, 0.0010789766674861312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010789766674861312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107898
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00002034
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001804
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001705
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001540
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001537
Iteration 25/1000 | Loss: 0.00001537
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001530
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001525
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001524
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001520
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001516
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001515
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001508
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001507
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001506
Iteration 132/1000 | Loss: 0.00001506
Iteration 133/1000 | Loss: 0.00001506
Iteration 134/1000 | Loss: 0.00001506
Iteration 135/1000 | Loss: 0.00001506
Iteration 136/1000 | Loss: 0.00001506
Iteration 137/1000 | Loss: 0.00001506
Iteration 138/1000 | Loss: 0.00001506
Iteration 139/1000 | Loss: 0.00001506
Iteration 140/1000 | Loss: 0.00001506
Iteration 141/1000 | Loss: 0.00001506
Iteration 142/1000 | Loss: 0.00001505
Iteration 143/1000 | Loss: 0.00001505
Iteration 144/1000 | Loss: 0.00001505
Iteration 145/1000 | Loss: 0.00001505
Iteration 146/1000 | Loss: 0.00001505
Iteration 147/1000 | Loss: 0.00001505
Iteration 148/1000 | Loss: 0.00001505
Iteration 149/1000 | Loss: 0.00001505
Iteration 150/1000 | Loss: 0.00001505
Iteration 151/1000 | Loss: 0.00001505
Iteration 152/1000 | Loss: 0.00001505
Iteration 153/1000 | Loss: 0.00001505
Iteration 154/1000 | Loss: 0.00001505
Iteration 155/1000 | Loss: 0.00001505
Iteration 156/1000 | Loss: 0.00001505
Iteration 157/1000 | Loss: 0.00001505
Iteration 158/1000 | Loss: 0.00001505
Iteration 159/1000 | Loss: 0.00001505
Iteration 160/1000 | Loss: 0.00001505
Iteration 161/1000 | Loss: 0.00001505
Iteration 162/1000 | Loss: 0.00001505
Iteration 163/1000 | Loss: 0.00001505
Iteration 164/1000 | Loss: 0.00001505
Iteration 165/1000 | Loss: 0.00001505
Iteration 166/1000 | Loss: 0.00001505
Iteration 167/1000 | Loss: 0.00001505
Iteration 168/1000 | Loss: 0.00001505
Iteration 169/1000 | Loss: 0.00001505
Iteration 170/1000 | Loss: 0.00001505
Iteration 171/1000 | Loss: 0.00001505
Iteration 172/1000 | Loss: 0.00001505
Iteration 173/1000 | Loss: 0.00001505
Iteration 174/1000 | Loss: 0.00001505
Iteration 175/1000 | Loss: 0.00001505
Iteration 176/1000 | Loss: 0.00001505
Iteration 177/1000 | Loss: 0.00001505
Iteration 178/1000 | Loss: 0.00001505
Iteration 179/1000 | Loss: 0.00001505
Iteration 180/1000 | Loss: 0.00001505
Iteration 181/1000 | Loss: 0.00001505
Iteration 182/1000 | Loss: 0.00001505
Iteration 183/1000 | Loss: 0.00001505
Iteration 184/1000 | Loss: 0.00001505
Iteration 185/1000 | Loss: 0.00001505
Iteration 186/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.5045213331177365e-05, 1.5045213331177365e-05, 1.5045213331177365e-05, 1.5045213331177365e-05, 1.5045213331177365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5045213331177365e-05

Optimization complete. Final v2v error: 3.3069958686828613 mm

Highest mean error: 3.63441801071167 mm for frame 99

Lowest mean error: 2.9916861057281494 mm for frame 112

Saving results

Total time: 50.249945402145386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472160
Iteration 2/25 | Loss: 0.00161113
Iteration 3/25 | Loss: 0.00141203
Iteration 4/25 | Loss: 0.00138775
Iteration 5/25 | Loss: 0.00138418
Iteration 6/25 | Loss: 0.00138387
Iteration 7/25 | Loss: 0.00138387
Iteration 8/25 | Loss: 0.00138387
Iteration 9/25 | Loss: 0.00138387
Iteration 10/25 | Loss: 0.00138387
Iteration 11/25 | Loss: 0.00138387
Iteration 12/25 | Loss: 0.00138387
Iteration 13/25 | Loss: 0.00138387
Iteration 14/25 | Loss: 0.00138387
Iteration 15/25 | Loss: 0.00138387
Iteration 16/25 | Loss: 0.00138387
Iteration 17/25 | Loss: 0.00138387
Iteration 18/25 | Loss: 0.00138387
Iteration 19/25 | Loss: 0.00138387
Iteration 20/25 | Loss: 0.00138387
Iteration 21/25 | Loss: 0.00138387
Iteration 22/25 | Loss: 0.00138387
Iteration 23/25 | Loss: 0.00138387
Iteration 24/25 | Loss: 0.00138387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013838695595040917, 0.0013838695595040917, 0.0013838695595040917, 0.0013838695595040917, 0.0013838695595040917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013838695595040917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39400935
Iteration 2/25 | Loss: 0.00095195
Iteration 3/25 | Loss: 0.00095194
Iteration 4/25 | Loss: 0.00095194
Iteration 5/25 | Loss: 0.00095194
Iteration 6/25 | Loss: 0.00095193
Iteration 7/25 | Loss: 0.00095193
Iteration 8/25 | Loss: 0.00095193
Iteration 9/25 | Loss: 0.00095193
Iteration 10/25 | Loss: 0.00095193
Iteration 11/25 | Loss: 0.00095193
Iteration 12/25 | Loss: 0.00095193
Iteration 13/25 | Loss: 0.00095193
Iteration 14/25 | Loss: 0.00095193
Iteration 15/25 | Loss: 0.00095193
Iteration 16/25 | Loss: 0.00095193
Iteration 17/25 | Loss: 0.00095193
Iteration 18/25 | Loss: 0.00095193
Iteration 19/25 | Loss: 0.00095193
Iteration 20/25 | Loss: 0.00095193
Iteration 21/25 | Loss: 0.00095193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009519333252683282, 0.0009519333252683282, 0.0009519333252683282, 0.0009519333252683282, 0.0009519333252683282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009519333252683282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095193
Iteration 2/1000 | Loss: 0.00005482
Iteration 3/1000 | Loss: 0.00003261
Iteration 4/1000 | Loss: 0.00002989
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002662
Iteration 7/1000 | Loss: 0.00002563
Iteration 8/1000 | Loss: 0.00002501
Iteration 9/1000 | Loss: 0.00002421
Iteration 10/1000 | Loss: 0.00002373
Iteration 11/1000 | Loss: 0.00002343
Iteration 12/1000 | Loss: 0.00002322
Iteration 13/1000 | Loss: 0.00002308
Iteration 14/1000 | Loss: 0.00002291
Iteration 15/1000 | Loss: 0.00002277
Iteration 16/1000 | Loss: 0.00002275
Iteration 17/1000 | Loss: 0.00002268
Iteration 18/1000 | Loss: 0.00002266
Iteration 19/1000 | Loss: 0.00002265
Iteration 20/1000 | Loss: 0.00002262
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002262
Iteration 23/1000 | Loss: 0.00002262
Iteration 24/1000 | Loss: 0.00002262
Iteration 25/1000 | Loss: 0.00002261
Iteration 26/1000 | Loss: 0.00002261
Iteration 27/1000 | Loss: 0.00002258
Iteration 28/1000 | Loss: 0.00002258
Iteration 29/1000 | Loss: 0.00002257
Iteration 30/1000 | Loss: 0.00002256
Iteration 31/1000 | Loss: 0.00002256
Iteration 32/1000 | Loss: 0.00002256
Iteration 33/1000 | Loss: 0.00002256
Iteration 34/1000 | Loss: 0.00002256
Iteration 35/1000 | Loss: 0.00002255
Iteration 36/1000 | Loss: 0.00002253
Iteration 37/1000 | Loss: 0.00002252
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002252
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002252
Iteration 44/1000 | Loss: 0.00002252
Iteration 45/1000 | Loss: 0.00002252
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002251
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002249
Iteration 51/1000 | Loss: 0.00002249
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002248
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002248
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002246
Iteration 60/1000 | Loss: 0.00002245
Iteration 61/1000 | Loss: 0.00002245
Iteration 62/1000 | Loss: 0.00002245
Iteration 63/1000 | Loss: 0.00002244
Iteration 64/1000 | Loss: 0.00002244
Iteration 65/1000 | Loss: 0.00002244
Iteration 66/1000 | Loss: 0.00002244
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002243
Iteration 70/1000 | Loss: 0.00002241
Iteration 71/1000 | Loss: 0.00002241
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00002241
Iteration 75/1000 | Loss: 0.00002241
Iteration 76/1000 | Loss: 0.00002240
Iteration 77/1000 | Loss: 0.00002240
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002238
Iteration 81/1000 | Loss: 0.00002238
Iteration 82/1000 | Loss: 0.00002237
Iteration 83/1000 | Loss: 0.00002237
Iteration 84/1000 | Loss: 0.00002237
Iteration 85/1000 | Loss: 0.00002236
Iteration 86/1000 | Loss: 0.00002236
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002235
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002233
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002233
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002233
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002231
Iteration 105/1000 | Loss: 0.00002231
Iteration 106/1000 | Loss: 0.00002231
Iteration 107/1000 | Loss: 0.00002231
Iteration 108/1000 | Loss: 0.00002231
Iteration 109/1000 | Loss: 0.00002230
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002229
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002229
Iteration 119/1000 | Loss: 0.00002229
Iteration 120/1000 | Loss: 0.00002229
Iteration 121/1000 | Loss: 0.00002229
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002228
Iteration 124/1000 | Loss: 0.00002228
Iteration 125/1000 | Loss: 0.00002228
Iteration 126/1000 | Loss: 0.00002228
Iteration 127/1000 | Loss: 0.00002228
Iteration 128/1000 | Loss: 0.00002228
Iteration 129/1000 | Loss: 0.00002228
Iteration 130/1000 | Loss: 0.00002228
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002228
Iteration 136/1000 | Loss: 0.00002228
Iteration 137/1000 | Loss: 0.00002228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.2282796635408886e-05, 2.2282796635408886e-05, 2.2282796635408886e-05, 2.2282796635408886e-05, 2.2282796635408886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2282796635408886e-05

Optimization complete. Final v2v error: 3.9463014602661133 mm

Highest mean error: 4.241037845611572 mm for frame 166

Lowest mean error: 3.522747278213501 mm for frame 30

Saving results

Total time: 46.35117149353027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410162
Iteration 2/25 | Loss: 0.00137683
Iteration 3/25 | Loss: 0.00127924
Iteration 4/25 | Loss: 0.00126732
Iteration 5/25 | Loss: 0.00126431
Iteration 6/25 | Loss: 0.00126388
Iteration 7/25 | Loss: 0.00126388
Iteration 8/25 | Loss: 0.00126388
Iteration 9/25 | Loss: 0.00126388
Iteration 10/25 | Loss: 0.00126388
Iteration 11/25 | Loss: 0.00126388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012638847110792994, 0.0012638847110792994, 0.0012638847110792994, 0.0012638847110792994, 0.0012638847110792994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012638847110792994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40912294
Iteration 2/25 | Loss: 0.00091103
Iteration 3/25 | Loss: 0.00091102
Iteration 4/25 | Loss: 0.00091102
Iteration 5/25 | Loss: 0.00091102
Iteration 6/25 | Loss: 0.00091102
Iteration 7/25 | Loss: 0.00091102
Iteration 8/25 | Loss: 0.00091102
Iteration 9/25 | Loss: 0.00091102
Iteration 10/25 | Loss: 0.00091102
Iteration 11/25 | Loss: 0.00091102
Iteration 12/25 | Loss: 0.00091102
Iteration 13/25 | Loss: 0.00091102
Iteration 14/25 | Loss: 0.00091102
Iteration 15/25 | Loss: 0.00091102
Iteration 16/25 | Loss: 0.00091102
Iteration 17/25 | Loss: 0.00091102
Iteration 18/25 | Loss: 0.00091102
Iteration 19/25 | Loss: 0.00091102
Iteration 20/25 | Loss: 0.00091102
Iteration 21/25 | Loss: 0.00091102
Iteration 22/25 | Loss: 0.00091102
Iteration 23/25 | Loss: 0.00091102
Iteration 24/25 | Loss: 0.00091102
Iteration 25/25 | Loss: 0.00091102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091102
Iteration 2/1000 | Loss: 0.00003302
Iteration 3/1000 | Loss: 0.00002003
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001482
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001315
Iteration 8/1000 | Loss: 0.00001262
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001218
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001187
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001176
Iteration 20/1000 | Loss: 0.00001175
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001157
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001156
Iteration 35/1000 | Loss: 0.00001155
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001153
Iteration 38/1000 | Loss: 0.00001153
Iteration 39/1000 | Loss: 0.00001153
Iteration 40/1000 | Loss: 0.00001153
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001152
Iteration 43/1000 | Loss: 0.00001152
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001149
Iteration 48/1000 | Loss: 0.00001149
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001137
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001136
Iteration 70/1000 | Loss: 0.00001136
Iteration 71/1000 | Loss: 0.00001136
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001135
Iteration 75/1000 | Loss: 0.00001135
Iteration 76/1000 | Loss: 0.00001135
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001134
Iteration 80/1000 | Loss: 0.00001134
Iteration 81/1000 | Loss: 0.00001133
Iteration 82/1000 | Loss: 0.00001133
Iteration 83/1000 | Loss: 0.00001133
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001131
Iteration 89/1000 | Loss: 0.00001131
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001130
Iteration 93/1000 | Loss: 0.00001129
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001125
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001124
Iteration 104/1000 | Loss: 0.00001124
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001120
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001119
Iteration 122/1000 | Loss: 0.00001119
Iteration 123/1000 | Loss: 0.00001119
Iteration 124/1000 | Loss: 0.00001118
Iteration 125/1000 | Loss: 0.00001118
Iteration 126/1000 | Loss: 0.00001118
Iteration 127/1000 | Loss: 0.00001118
Iteration 128/1000 | Loss: 0.00001118
Iteration 129/1000 | Loss: 0.00001117
Iteration 130/1000 | Loss: 0.00001117
Iteration 131/1000 | Loss: 0.00001117
Iteration 132/1000 | Loss: 0.00001117
Iteration 133/1000 | Loss: 0.00001117
Iteration 134/1000 | Loss: 0.00001117
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001116
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001116
Iteration 140/1000 | Loss: 0.00001116
Iteration 141/1000 | Loss: 0.00001116
Iteration 142/1000 | Loss: 0.00001116
Iteration 143/1000 | Loss: 0.00001116
Iteration 144/1000 | Loss: 0.00001115
Iteration 145/1000 | Loss: 0.00001115
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001115
Iteration 148/1000 | Loss: 0.00001115
Iteration 149/1000 | Loss: 0.00001115
Iteration 150/1000 | Loss: 0.00001115
Iteration 151/1000 | Loss: 0.00001115
Iteration 152/1000 | Loss: 0.00001115
Iteration 153/1000 | Loss: 0.00001115
Iteration 154/1000 | Loss: 0.00001115
Iteration 155/1000 | Loss: 0.00001115
Iteration 156/1000 | Loss: 0.00001115
Iteration 157/1000 | Loss: 0.00001115
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001114
Iteration 160/1000 | Loss: 0.00001114
Iteration 161/1000 | Loss: 0.00001114
Iteration 162/1000 | Loss: 0.00001114
Iteration 163/1000 | Loss: 0.00001114
Iteration 164/1000 | Loss: 0.00001114
Iteration 165/1000 | Loss: 0.00001114
Iteration 166/1000 | Loss: 0.00001114
Iteration 167/1000 | Loss: 0.00001114
Iteration 168/1000 | Loss: 0.00001114
Iteration 169/1000 | Loss: 0.00001113
Iteration 170/1000 | Loss: 0.00001113
Iteration 171/1000 | Loss: 0.00001113
Iteration 172/1000 | Loss: 0.00001113
Iteration 173/1000 | Loss: 0.00001113
Iteration 174/1000 | Loss: 0.00001112
Iteration 175/1000 | Loss: 0.00001112
Iteration 176/1000 | Loss: 0.00001112
Iteration 177/1000 | Loss: 0.00001112
Iteration 178/1000 | Loss: 0.00001112
Iteration 179/1000 | Loss: 0.00001111
Iteration 180/1000 | Loss: 0.00001111
Iteration 181/1000 | Loss: 0.00001111
Iteration 182/1000 | Loss: 0.00001111
Iteration 183/1000 | Loss: 0.00001111
Iteration 184/1000 | Loss: 0.00001111
Iteration 185/1000 | Loss: 0.00001111
Iteration 186/1000 | Loss: 0.00001110
Iteration 187/1000 | Loss: 0.00001110
Iteration 188/1000 | Loss: 0.00001110
Iteration 189/1000 | Loss: 0.00001110
Iteration 190/1000 | Loss: 0.00001110
Iteration 191/1000 | Loss: 0.00001110
Iteration 192/1000 | Loss: 0.00001110
Iteration 193/1000 | Loss: 0.00001110
Iteration 194/1000 | Loss: 0.00001109
Iteration 195/1000 | Loss: 0.00001109
Iteration 196/1000 | Loss: 0.00001109
Iteration 197/1000 | Loss: 0.00001109
Iteration 198/1000 | Loss: 0.00001109
Iteration 199/1000 | Loss: 0.00001109
Iteration 200/1000 | Loss: 0.00001108
Iteration 201/1000 | Loss: 0.00001108
Iteration 202/1000 | Loss: 0.00001108
Iteration 203/1000 | Loss: 0.00001108
Iteration 204/1000 | Loss: 0.00001108
Iteration 205/1000 | Loss: 0.00001108
Iteration 206/1000 | Loss: 0.00001108
Iteration 207/1000 | Loss: 0.00001108
Iteration 208/1000 | Loss: 0.00001107
Iteration 209/1000 | Loss: 0.00001107
Iteration 210/1000 | Loss: 0.00001107
Iteration 211/1000 | Loss: 0.00001107
Iteration 212/1000 | Loss: 0.00001107
Iteration 213/1000 | Loss: 0.00001107
Iteration 214/1000 | Loss: 0.00001106
Iteration 215/1000 | Loss: 0.00001106
Iteration 216/1000 | Loss: 0.00001106
Iteration 217/1000 | Loss: 0.00001106
Iteration 218/1000 | Loss: 0.00001106
Iteration 219/1000 | Loss: 0.00001106
Iteration 220/1000 | Loss: 0.00001106
Iteration 221/1000 | Loss: 0.00001106
Iteration 222/1000 | Loss: 0.00001106
Iteration 223/1000 | Loss: 0.00001106
Iteration 224/1000 | Loss: 0.00001106
Iteration 225/1000 | Loss: 0.00001106
Iteration 226/1000 | Loss: 0.00001106
Iteration 227/1000 | Loss: 0.00001106
Iteration 228/1000 | Loss: 0.00001105
Iteration 229/1000 | Loss: 0.00001105
Iteration 230/1000 | Loss: 0.00001105
Iteration 231/1000 | Loss: 0.00001105
Iteration 232/1000 | Loss: 0.00001105
Iteration 233/1000 | Loss: 0.00001105
Iteration 234/1000 | Loss: 0.00001105
Iteration 235/1000 | Loss: 0.00001105
Iteration 236/1000 | Loss: 0.00001105
Iteration 237/1000 | Loss: 0.00001105
Iteration 238/1000 | Loss: 0.00001105
Iteration 239/1000 | Loss: 0.00001105
Iteration 240/1000 | Loss: 0.00001105
Iteration 241/1000 | Loss: 0.00001105
Iteration 242/1000 | Loss: 0.00001105
Iteration 243/1000 | Loss: 0.00001105
Iteration 244/1000 | Loss: 0.00001105
Iteration 245/1000 | Loss: 0.00001105
Iteration 246/1000 | Loss: 0.00001105
Iteration 247/1000 | Loss: 0.00001104
Iteration 248/1000 | Loss: 0.00001104
Iteration 249/1000 | Loss: 0.00001104
Iteration 250/1000 | Loss: 0.00001104
Iteration 251/1000 | Loss: 0.00001104
Iteration 252/1000 | Loss: 0.00001104
Iteration 253/1000 | Loss: 0.00001104
Iteration 254/1000 | Loss: 0.00001104
Iteration 255/1000 | Loss: 0.00001104
Iteration 256/1000 | Loss: 0.00001104
Iteration 257/1000 | Loss: 0.00001104
Iteration 258/1000 | Loss: 0.00001104
Iteration 259/1000 | Loss: 0.00001104
Iteration 260/1000 | Loss: 0.00001104
Iteration 261/1000 | Loss: 0.00001104
Iteration 262/1000 | Loss: 0.00001104
Iteration 263/1000 | Loss: 0.00001104
Iteration 264/1000 | Loss: 0.00001104
Iteration 265/1000 | Loss: 0.00001104
Iteration 266/1000 | Loss: 0.00001104
Iteration 267/1000 | Loss: 0.00001104
Iteration 268/1000 | Loss: 0.00001104
Iteration 269/1000 | Loss: 0.00001104
Iteration 270/1000 | Loss: 0.00001104
Iteration 271/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.104031252907589e-05, 1.104031252907589e-05, 1.104031252907589e-05, 1.104031252907589e-05, 1.104031252907589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.104031252907589e-05

Optimization complete. Final v2v error: 2.842083692550659 mm

Highest mean error: 3.630084991455078 mm for frame 66

Lowest mean error: 2.65725040435791 mm for frame 105

Saving results

Total time: 46.59909629821777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477176
Iteration 2/25 | Loss: 0.00139160
Iteration 3/25 | Loss: 0.00131463
Iteration 4/25 | Loss: 0.00130521
Iteration 5/25 | Loss: 0.00130379
Iteration 6/25 | Loss: 0.00130379
Iteration 7/25 | Loss: 0.00130379
Iteration 8/25 | Loss: 0.00130379
Iteration 9/25 | Loss: 0.00130379
Iteration 10/25 | Loss: 0.00130379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013037853641435504, 0.0013037853641435504, 0.0013037853641435504, 0.0013037853641435504, 0.0013037853641435504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013037853641435504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88397789
Iteration 2/25 | Loss: 0.00089492
Iteration 3/25 | Loss: 0.00089492
Iteration 4/25 | Loss: 0.00089492
Iteration 5/25 | Loss: 0.00089492
Iteration 6/25 | Loss: 0.00089492
Iteration 7/25 | Loss: 0.00089492
Iteration 8/25 | Loss: 0.00089492
Iteration 9/25 | Loss: 0.00089492
Iteration 10/25 | Loss: 0.00089492
Iteration 11/25 | Loss: 0.00089492
Iteration 12/25 | Loss: 0.00089492
Iteration 13/25 | Loss: 0.00089492
Iteration 14/25 | Loss: 0.00089492
Iteration 15/25 | Loss: 0.00089492
Iteration 16/25 | Loss: 0.00089492
Iteration 17/25 | Loss: 0.00089492
Iteration 18/25 | Loss: 0.00089492
Iteration 19/25 | Loss: 0.00089492
Iteration 20/25 | Loss: 0.00089492
Iteration 21/25 | Loss: 0.00089492
Iteration 22/25 | Loss: 0.00089492
Iteration 23/25 | Loss: 0.00089492
Iteration 24/25 | Loss: 0.00089492
Iteration 25/25 | Loss: 0.00089492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089492
Iteration 2/1000 | Loss: 0.00002404
Iteration 3/1000 | Loss: 0.00001991
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001723
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001585
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001554
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001505
Iteration 25/1000 | Loss: 0.00001504
Iteration 26/1000 | Loss: 0.00001504
Iteration 27/1000 | Loss: 0.00001504
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001501
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001482
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001473
Iteration 64/1000 | Loss: 0.00001471
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001470
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001466
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001465
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001464
Iteration 92/1000 | Loss: 0.00001464
Iteration 93/1000 | Loss: 0.00001464
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001463
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001463
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001463
Iteration 104/1000 | Loss: 0.00001463
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001463
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001463
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.4632800230174325e-05, 1.4632800230174325e-05, 1.4632800230174325e-05, 1.4632800230174325e-05, 1.4632800230174325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4632800230174325e-05

Optimization complete. Final v2v error: 3.2623450756073 mm

Highest mean error: 3.4459922313690186 mm for frame 239

Lowest mean error: 3.0420444011688232 mm for frame 210

Saving results

Total time: 42.85338735580444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774929
Iteration 2/25 | Loss: 0.00134668
Iteration 3/25 | Loss: 0.00127551
Iteration 4/25 | Loss: 0.00127026
Iteration 5/25 | Loss: 0.00126922
Iteration 6/25 | Loss: 0.00126922
Iteration 7/25 | Loss: 0.00126922
Iteration 8/25 | Loss: 0.00126922
Iteration 9/25 | Loss: 0.00126922
Iteration 10/25 | Loss: 0.00126922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012692151358351111, 0.0012692151358351111, 0.0012692151358351111, 0.0012692151358351111, 0.0012692151358351111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012692151358351111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39714003
Iteration 2/25 | Loss: 0.00081802
Iteration 3/25 | Loss: 0.00081802
Iteration 4/25 | Loss: 0.00081802
Iteration 5/25 | Loss: 0.00081802
Iteration 6/25 | Loss: 0.00081802
Iteration 7/25 | Loss: 0.00081801
Iteration 8/25 | Loss: 0.00081801
Iteration 9/25 | Loss: 0.00081801
Iteration 10/25 | Loss: 0.00081801
Iteration 11/25 | Loss: 0.00081801
Iteration 12/25 | Loss: 0.00081801
Iteration 13/25 | Loss: 0.00081801
Iteration 14/25 | Loss: 0.00081801
Iteration 15/25 | Loss: 0.00081801
Iteration 16/25 | Loss: 0.00081801
Iteration 17/25 | Loss: 0.00081801
Iteration 18/25 | Loss: 0.00081801
Iteration 19/25 | Loss: 0.00081801
Iteration 20/25 | Loss: 0.00081801
Iteration 21/25 | Loss: 0.00081801
Iteration 22/25 | Loss: 0.00081801
Iteration 23/25 | Loss: 0.00081801
Iteration 24/25 | Loss: 0.00081801
Iteration 25/25 | Loss: 0.00081801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081801
Iteration 2/1000 | Loss: 0.00002776
Iteration 3/1000 | Loss: 0.00001863
Iteration 4/1000 | Loss: 0.00001607
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001408
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001332
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001266
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001211
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001191
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001190
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001187
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001185
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001184
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001184
Iteration 105/1000 | Loss: 0.00001184
Iteration 106/1000 | Loss: 0.00001184
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001183
Iteration 110/1000 | Loss: 0.00001183
Iteration 111/1000 | Loss: 0.00001183
Iteration 112/1000 | Loss: 0.00001183
Iteration 113/1000 | Loss: 0.00001183
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001181
Iteration 122/1000 | Loss: 0.00001181
Iteration 123/1000 | Loss: 0.00001181
Iteration 124/1000 | Loss: 0.00001181
Iteration 125/1000 | Loss: 0.00001181
Iteration 126/1000 | Loss: 0.00001181
Iteration 127/1000 | Loss: 0.00001180
Iteration 128/1000 | Loss: 0.00001180
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001179
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001178
Iteration 138/1000 | Loss: 0.00001178
Iteration 139/1000 | Loss: 0.00001178
Iteration 140/1000 | Loss: 0.00001178
Iteration 141/1000 | Loss: 0.00001178
Iteration 142/1000 | Loss: 0.00001178
Iteration 143/1000 | Loss: 0.00001178
Iteration 144/1000 | Loss: 0.00001178
Iteration 145/1000 | Loss: 0.00001178
Iteration 146/1000 | Loss: 0.00001178
Iteration 147/1000 | Loss: 0.00001178
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001178
Iteration 152/1000 | Loss: 0.00001178
Iteration 153/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.1783004993048962e-05, 1.1783004993048962e-05, 1.1783004993048962e-05, 1.1783004993048962e-05, 1.1783004993048962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1783004993048962e-05

Optimization complete. Final v2v error: 2.9309639930725098 mm

Highest mean error: 3.1594409942626953 mm for frame 75

Lowest mean error: 2.789140462875366 mm for frame 201

Saving results

Total time: 38.85756325721741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00692077
Iteration 2/25 | Loss: 0.00158967
Iteration 3/25 | Loss: 0.00139163
Iteration 4/25 | Loss: 0.00138107
Iteration 5/25 | Loss: 0.00137950
Iteration 6/25 | Loss: 0.00137950
Iteration 7/25 | Loss: 0.00137950
Iteration 8/25 | Loss: 0.00137950
Iteration 9/25 | Loss: 0.00137950
Iteration 10/25 | Loss: 0.00137950
Iteration 11/25 | Loss: 0.00137950
Iteration 12/25 | Loss: 0.00137950
Iteration 13/25 | Loss: 0.00137950
Iteration 14/25 | Loss: 0.00137950
Iteration 15/25 | Loss: 0.00137950
Iteration 16/25 | Loss: 0.00137950
Iteration 17/25 | Loss: 0.00137950
Iteration 18/25 | Loss: 0.00137950
Iteration 19/25 | Loss: 0.00137950
Iteration 20/25 | Loss: 0.00137950
Iteration 21/25 | Loss: 0.00137950
Iteration 22/25 | Loss: 0.00137950
Iteration 23/25 | Loss: 0.00137950
Iteration 24/25 | Loss: 0.00137950
Iteration 25/25 | Loss: 0.00137950

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36793685
Iteration 2/25 | Loss: 0.00087255
Iteration 3/25 | Loss: 0.00087253
Iteration 4/25 | Loss: 0.00087253
Iteration 5/25 | Loss: 0.00087253
Iteration 6/25 | Loss: 0.00087253
Iteration 7/25 | Loss: 0.00087253
Iteration 8/25 | Loss: 0.00087253
Iteration 9/25 | Loss: 0.00087253
Iteration 10/25 | Loss: 0.00087252
Iteration 11/25 | Loss: 0.00087252
Iteration 12/25 | Loss: 0.00087252
Iteration 13/25 | Loss: 0.00087252
Iteration 14/25 | Loss: 0.00087252
Iteration 15/25 | Loss: 0.00087252
Iteration 16/25 | Loss: 0.00087252
Iteration 17/25 | Loss: 0.00087252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008725246298126876, 0.0008725246298126876, 0.0008725246298126876, 0.0008725246298126876, 0.0008725246298126876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008725246298126876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087252
Iteration 2/1000 | Loss: 0.00003684
Iteration 3/1000 | Loss: 0.00002629
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002292
Iteration 6/1000 | Loss: 0.00002232
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002149
Iteration 9/1000 | Loss: 0.00002114
Iteration 10/1000 | Loss: 0.00002093
Iteration 11/1000 | Loss: 0.00002067
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00002016
Iteration 14/1000 | Loss: 0.00001991
Iteration 15/1000 | Loss: 0.00001989
Iteration 16/1000 | Loss: 0.00001988
Iteration 17/1000 | Loss: 0.00001969
Iteration 18/1000 | Loss: 0.00001967
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001945
Iteration 34/1000 | Loss: 0.00001945
Iteration 35/1000 | Loss: 0.00001945
Iteration 36/1000 | Loss: 0.00001942
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001940
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001930
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001928
Iteration 71/1000 | Loss: 0.00001928
Iteration 72/1000 | Loss: 0.00001928
Iteration 73/1000 | Loss: 0.00001927
Iteration 74/1000 | Loss: 0.00001927
Iteration 75/1000 | Loss: 0.00001927
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001927
Iteration 78/1000 | Loss: 0.00001927
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001926
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001924
Iteration 87/1000 | Loss: 0.00001924
Iteration 88/1000 | Loss: 0.00001924
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001924
Iteration 91/1000 | Loss: 0.00001924
Iteration 92/1000 | Loss: 0.00001924
Iteration 93/1000 | Loss: 0.00001924
Iteration 94/1000 | Loss: 0.00001924
Iteration 95/1000 | Loss: 0.00001924
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001924
Iteration 98/1000 | Loss: 0.00001924
Iteration 99/1000 | Loss: 0.00001924
Iteration 100/1000 | Loss: 0.00001924
Iteration 101/1000 | Loss: 0.00001924
Iteration 102/1000 | Loss: 0.00001924
Iteration 103/1000 | Loss: 0.00001924
Iteration 104/1000 | Loss: 0.00001924
Iteration 105/1000 | Loss: 0.00001924
Iteration 106/1000 | Loss: 0.00001924
Iteration 107/1000 | Loss: 0.00001924
Iteration 108/1000 | Loss: 0.00001924
Iteration 109/1000 | Loss: 0.00001924
Iteration 110/1000 | Loss: 0.00001924
Iteration 111/1000 | Loss: 0.00001924
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.923687887028791e-05, 1.923687887028791e-05, 1.923687887028791e-05, 1.923687887028791e-05, 1.923687887028791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.923687887028791e-05

Optimization complete. Final v2v error: 3.7220396995544434 mm

Highest mean error: 3.887345790863037 mm for frame 91

Lowest mean error: 3.564889669418335 mm for frame 194

Saving results

Total time: 43.17627024650574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835126
Iteration 2/25 | Loss: 0.00169027
Iteration 3/25 | Loss: 0.00144826
Iteration 4/25 | Loss: 0.00141738
Iteration 5/25 | Loss: 0.00140554
Iteration 6/25 | Loss: 0.00139537
Iteration 7/25 | Loss: 0.00139402
Iteration 8/25 | Loss: 0.00139759
Iteration 9/25 | Loss: 0.00137505
Iteration 10/25 | Loss: 0.00137118
Iteration 11/25 | Loss: 0.00136281
Iteration 12/25 | Loss: 0.00136130
Iteration 13/25 | Loss: 0.00136664
Iteration 14/25 | Loss: 0.00135796
Iteration 15/25 | Loss: 0.00135680
Iteration 16/25 | Loss: 0.00135609
Iteration 17/25 | Loss: 0.00135885
Iteration 18/25 | Loss: 0.00135614
Iteration 19/25 | Loss: 0.00135186
Iteration 20/25 | Loss: 0.00134376
Iteration 21/25 | Loss: 0.00134254
Iteration 22/25 | Loss: 0.00134245
Iteration 23/25 | Loss: 0.00134245
Iteration 24/25 | Loss: 0.00134245
Iteration 25/25 | Loss: 0.00134245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95511019
Iteration 2/25 | Loss: 0.00069777
Iteration 3/25 | Loss: 0.00069776
Iteration 4/25 | Loss: 0.00069776
Iteration 5/25 | Loss: 0.00069776
Iteration 6/25 | Loss: 0.00069776
Iteration 7/25 | Loss: 0.00069776
Iteration 8/25 | Loss: 0.00069776
Iteration 9/25 | Loss: 0.00069776
Iteration 10/25 | Loss: 0.00069776
Iteration 11/25 | Loss: 0.00069776
Iteration 12/25 | Loss: 0.00069776
Iteration 13/25 | Loss: 0.00069776
Iteration 14/25 | Loss: 0.00069776
Iteration 15/25 | Loss: 0.00069776
Iteration 16/25 | Loss: 0.00069776
Iteration 17/25 | Loss: 0.00069776
Iteration 18/25 | Loss: 0.00069776
Iteration 19/25 | Loss: 0.00069776
Iteration 20/25 | Loss: 0.00069776
Iteration 21/25 | Loss: 0.00069776
Iteration 22/25 | Loss: 0.00069776
Iteration 23/25 | Loss: 0.00069776
Iteration 24/25 | Loss: 0.00069776
Iteration 25/25 | Loss: 0.00069776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069776
Iteration 2/1000 | Loss: 0.00005308
Iteration 3/1000 | Loss: 0.00004010
Iteration 4/1000 | Loss: 0.00003522
Iteration 5/1000 | Loss: 0.00003357
Iteration 6/1000 | Loss: 0.00003189
Iteration 7/1000 | Loss: 0.00003087
Iteration 8/1000 | Loss: 0.00003026
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002901
Iteration 11/1000 | Loss: 0.00044223
Iteration 12/1000 | Loss: 0.00060013
Iteration 13/1000 | Loss: 0.00004844
Iteration 14/1000 | Loss: 0.00003518
Iteration 15/1000 | Loss: 0.00002920
Iteration 16/1000 | Loss: 0.00002662
Iteration 17/1000 | Loss: 0.00002432
Iteration 18/1000 | Loss: 0.00002275
Iteration 19/1000 | Loss: 0.00002201
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002153
Iteration 22/1000 | Loss: 0.00002143
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002129
Iteration 26/1000 | Loss: 0.00002127
Iteration 27/1000 | Loss: 0.00002124
Iteration 28/1000 | Loss: 0.00002121
Iteration 29/1000 | Loss: 0.00002120
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002118
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00002117
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002111
Iteration 42/1000 | Loss: 0.00002111
Iteration 43/1000 | Loss: 0.00002111
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002111
Iteration 46/1000 | Loss: 0.00002111
Iteration 47/1000 | Loss: 0.00002111
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002109
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002108
Iteration 59/1000 | Loss: 0.00002108
Iteration 60/1000 | Loss: 0.00002108
Iteration 61/1000 | Loss: 0.00002108
Iteration 62/1000 | Loss: 0.00002108
Iteration 63/1000 | Loss: 0.00002108
Iteration 64/1000 | Loss: 0.00002108
Iteration 65/1000 | Loss: 0.00002107
Iteration 66/1000 | Loss: 0.00002107
Iteration 67/1000 | Loss: 0.00002107
Iteration 68/1000 | Loss: 0.00002107
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002107
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002106
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002106
Iteration 79/1000 | Loss: 0.00002106
Iteration 80/1000 | Loss: 0.00002106
Iteration 81/1000 | Loss: 0.00002106
Iteration 82/1000 | Loss: 0.00002106
Iteration 83/1000 | Loss: 0.00002106
Iteration 84/1000 | Loss: 0.00002106
Iteration 85/1000 | Loss: 0.00002106
Iteration 86/1000 | Loss: 0.00002106
Iteration 87/1000 | Loss: 0.00002106
Iteration 88/1000 | Loss: 0.00002105
Iteration 89/1000 | Loss: 0.00002105
Iteration 90/1000 | Loss: 0.00002105
Iteration 91/1000 | Loss: 0.00002105
Iteration 92/1000 | Loss: 0.00002105
Iteration 93/1000 | Loss: 0.00002105
Iteration 94/1000 | Loss: 0.00002105
Iteration 95/1000 | Loss: 0.00002105
Iteration 96/1000 | Loss: 0.00002105
Iteration 97/1000 | Loss: 0.00002105
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002104
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002104
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002104
Iteration 107/1000 | Loss: 0.00002104
Iteration 108/1000 | Loss: 0.00002104
Iteration 109/1000 | Loss: 0.00002103
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002103
Iteration 112/1000 | Loss: 0.00002103
Iteration 113/1000 | Loss: 0.00002103
Iteration 114/1000 | Loss: 0.00002103
Iteration 115/1000 | Loss: 0.00002103
Iteration 116/1000 | Loss: 0.00002103
Iteration 117/1000 | Loss: 0.00002103
Iteration 118/1000 | Loss: 0.00002103
Iteration 119/1000 | Loss: 0.00002103
Iteration 120/1000 | Loss: 0.00002103
Iteration 121/1000 | Loss: 0.00002103
Iteration 122/1000 | Loss: 0.00002103
Iteration 123/1000 | Loss: 0.00002103
Iteration 124/1000 | Loss: 0.00002103
Iteration 125/1000 | Loss: 0.00002103
Iteration 126/1000 | Loss: 0.00002103
Iteration 127/1000 | Loss: 0.00002103
Iteration 128/1000 | Loss: 0.00002103
Iteration 129/1000 | Loss: 0.00002103
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002103
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002103
Iteration 138/1000 | Loss: 0.00002103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.102945654769428e-05, 2.102945654769428e-05, 2.102945654769428e-05, 2.102945654769428e-05, 2.102945654769428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.102945654769428e-05

Optimization complete. Final v2v error: 3.8521578311920166 mm

Highest mean error: 4.029263973236084 mm for frame 27

Lowest mean error: 3.7129273414611816 mm for frame 4

Saving results

Total time: 78.24129986763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752501
Iteration 2/25 | Loss: 0.00200667
Iteration 3/25 | Loss: 0.00156495
Iteration 4/25 | Loss: 0.00148638
Iteration 5/25 | Loss: 0.00148039
Iteration 6/25 | Loss: 0.00148810
Iteration 7/25 | Loss: 0.00146118
Iteration 8/25 | Loss: 0.00143966
Iteration 9/25 | Loss: 0.00145791
Iteration 10/25 | Loss: 0.00146019
Iteration 11/25 | Loss: 0.00142887
Iteration 12/25 | Loss: 0.00140900
Iteration 13/25 | Loss: 0.00140400
Iteration 14/25 | Loss: 0.00140530
Iteration 15/25 | Loss: 0.00140341
Iteration 16/25 | Loss: 0.00140340
Iteration 17/25 | Loss: 0.00140340
Iteration 18/25 | Loss: 0.00140340
Iteration 19/25 | Loss: 0.00140340
Iteration 20/25 | Loss: 0.00140340
Iteration 21/25 | Loss: 0.00140340
Iteration 22/25 | Loss: 0.00140340
Iteration 23/25 | Loss: 0.00140339
Iteration 24/25 | Loss: 0.00140339
Iteration 25/25 | Loss: 0.00140339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.19816732
Iteration 2/25 | Loss: 0.00091377
Iteration 3/25 | Loss: 0.00088916
Iteration 4/25 | Loss: 0.00087615
Iteration 5/25 | Loss: 0.00087614
Iteration 6/25 | Loss: 0.00087614
Iteration 7/25 | Loss: 0.00087614
Iteration 8/25 | Loss: 0.00087614
Iteration 9/25 | Loss: 0.00087614
Iteration 10/25 | Loss: 0.00087614
Iteration 11/25 | Loss: 0.00087614
Iteration 12/25 | Loss: 0.00087614
Iteration 13/25 | Loss: 0.00087614
Iteration 14/25 | Loss: 0.00087614
Iteration 15/25 | Loss: 0.00087614
Iteration 16/25 | Loss: 0.00087614
Iteration 17/25 | Loss: 0.00087614
Iteration 18/25 | Loss: 0.00087614
Iteration 19/25 | Loss: 0.00087614
Iteration 20/25 | Loss: 0.00087614
Iteration 21/25 | Loss: 0.00087614
Iteration 22/25 | Loss: 0.00087614
Iteration 23/25 | Loss: 0.00087614
Iteration 24/25 | Loss: 0.00087614
Iteration 25/25 | Loss: 0.00087614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087614
Iteration 2/1000 | Loss: 0.00008804
Iteration 3/1000 | Loss: 0.00002726
Iteration 4/1000 | Loss: 0.00004197
Iteration 5/1000 | Loss: 0.00003306
Iteration 6/1000 | Loss: 0.00002352
Iteration 7/1000 | Loss: 0.00003350
Iteration 8/1000 | Loss: 0.00002287
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00004816
Iteration 12/1000 | Loss: 0.00002204
Iteration 13/1000 | Loss: 0.00002189
Iteration 14/1000 | Loss: 0.00003198
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002187
Iteration 17/1000 | Loss: 0.00002177
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00002166
Iteration 20/1000 | Loss: 0.00002163
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002152
Iteration 23/1000 | Loss: 0.00002985
Iteration 24/1000 | Loss: 0.00002233
Iteration 25/1000 | Loss: 0.00002359
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002151
Iteration 28/1000 | Loss: 0.00002138
Iteration 29/1000 | Loss: 0.00002138
Iteration 30/1000 | Loss: 0.00002138
Iteration 31/1000 | Loss: 0.00002138
Iteration 32/1000 | Loss: 0.00002138
Iteration 33/1000 | Loss: 0.00002138
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002138
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002136
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002134
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002131
Iteration 62/1000 | Loss: 0.00002130
Iteration 63/1000 | Loss: 0.00002130
Iteration 64/1000 | Loss: 0.00002130
Iteration 65/1000 | Loss: 0.00002130
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002129
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002126
Iteration 86/1000 | Loss: 0.00002126
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002124
Iteration 91/1000 | Loss: 0.00002124
Iteration 92/1000 | Loss: 0.00003420
Iteration 93/1000 | Loss: 0.00003420
Iteration 94/1000 | Loss: 0.00002129
Iteration 95/1000 | Loss: 0.00002443
Iteration 96/1000 | Loss: 0.00002251
Iteration 97/1000 | Loss: 0.00002118
Iteration 98/1000 | Loss: 0.00002118
Iteration 99/1000 | Loss: 0.00002118
Iteration 100/1000 | Loss: 0.00002118
Iteration 101/1000 | Loss: 0.00002117
Iteration 102/1000 | Loss: 0.00002117
Iteration 103/1000 | Loss: 0.00002117
Iteration 104/1000 | Loss: 0.00002117
Iteration 105/1000 | Loss: 0.00002117
Iteration 106/1000 | Loss: 0.00002117
Iteration 107/1000 | Loss: 0.00002117
Iteration 108/1000 | Loss: 0.00002117
Iteration 109/1000 | Loss: 0.00002117
Iteration 110/1000 | Loss: 0.00002117
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.1173291315790266e-05, 2.1173291315790266e-05, 2.1173291315790266e-05, 2.1173291315790266e-05, 2.1173291315790266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1173291315790266e-05

Optimization complete. Final v2v error: 3.6747727394104004 mm

Highest mean error: 10.433151245117188 mm for frame 131

Lowest mean error: 3.3113415241241455 mm for frame 15

Saving results

Total time: 70.38662195205688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004961
Iteration 2/25 | Loss: 0.00346133
Iteration 3/25 | Loss: 0.00302155
Iteration 4/25 | Loss: 0.00268040
Iteration 5/25 | Loss: 0.00239469
Iteration 6/25 | Loss: 0.00242898
Iteration 7/25 | Loss: 0.00214601
Iteration 8/25 | Loss: 0.00205548
Iteration 9/25 | Loss: 0.00203723
Iteration 10/25 | Loss: 0.00201587
Iteration 11/25 | Loss: 0.00199708
Iteration 12/25 | Loss: 0.00199491
Iteration 13/25 | Loss: 0.00197965
Iteration 14/25 | Loss: 0.00198654
Iteration 15/25 | Loss: 0.00197357
Iteration 16/25 | Loss: 0.00197537
Iteration 17/25 | Loss: 0.00196932
Iteration 18/25 | Loss: 0.00196859
Iteration 19/25 | Loss: 0.00195636
Iteration 20/25 | Loss: 0.00195419
Iteration 21/25 | Loss: 0.00193767
Iteration 22/25 | Loss: 0.00193671
Iteration 23/25 | Loss: 0.00194685
Iteration 24/25 | Loss: 0.00194091
Iteration 25/25 | Loss: 0.00192826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35710597
Iteration 2/25 | Loss: 0.00941934
Iteration 3/25 | Loss: 0.00758173
Iteration 4/25 | Loss: 0.00758171
Iteration 5/25 | Loss: 0.00758171
Iteration 6/25 | Loss: 0.00758171
Iteration 7/25 | Loss: 0.00758171
Iteration 8/25 | Loss: 0.00758171
Iteration 9/25 | Loss: 0.00758171
Iteration 10/25 | Loss: 0.00758171
Iteration 11/25 | Loss: 0.00758171
Iteration 12/25 | Loss: 0.00758171
Iteration 13/25 | Loss: 0.00758171
Iteration 14/25 | Loss: 0.00758171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007581708021461964, 0.007581708021461964, 0.007581708021461964, 0.007581708021461964, 0.007581708021461964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007581708021461964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00758171
Iteration 2/1000 | Loss: 0.00208419
Iteration 3/1000 | Loss: 0.00135310
Iteration 4/1000 | Loss: 0.00058443
Iteration 5/1000 | Loss: 0.00075287
Iteration 6/1000 | Loss: 0.00087427
Iteration 7/1000 | Loss: 0.00085550
Iteration 8/1000 | Loss: 0.00049968
Iteration 9/1000 | Loss: 0.00046277
Iteration 10/1000 | Loss: 0.00116736
Iteration 11/1000 | Loss: 0.00063258
Iteration 12/1000 | Loss: 0.00070459
Iteration 13/1000 | Loss: 0.00229452
Iteration 14/1000 | Loss: 0.00277276
Iteration 15/1000 | Loss: 0.00270307
Iteration 16/1000 | Loss: 0.00358583
Iteration 17/1000 | Loss: 0.00073020
Iteration 18/1000 | Loss: 0.00049764
Iteration 19/1000 | Loss: 0.00055634
Iteration 20/1000 | Loss: 0.00079808
Iteration 21/1000 | Loss: 0.00042994
Iteration 22/1000 | Loss: 0.00034922
Iteration 23/1000 | Loss: 0.00048310
Iteration 24/1000 | Loss: 0.00040220
Iteration 25/1000 | Loss: 0.00051068
Iteration 26/1000 | Loss: 0.00040744
Iteration 27/1000 | Loss: 0.00039067
Iteration 28/1000 | Loss: 0.00035642
Iteration 29/1000 | Loss: 0.00045072
Iteration 30/1000 | Loss: 0.00043493
Iteration 31/1000 | Loss: 0.00040777
Iteration 32/1000 | Loss: 0.00047963
Iteration 33/1000 | Loss: 0.00038483
Iteration 34/1000 | Loss: 0.00031983
Iteration 35/1000 | Loss: 0.00042584
Iteration 36/1000 | Loss: 0.00056304
Iteration 37/1000 | Loss: 0.00138605
Iteration 38/1000 | Loss: 0.00035392
Iteration 39/1000 | Loss: 0.00037141
Iteration 40/1000 | Loss: 0.00064477
Iteration 41/1000 | Loss: 0.00033751
Iteration 42/1000 | Loss: 0.00060453
Iteration 43/1000 | Loss: 0.00031487
Iteration 44/1000 | Loss: 0.00037350
Iteration 45/1000 | Loss: 0.00037341
Iteration 46/1000 | Loss: 0.00100383
Iteration 47/1000 | Loss: 0.00034352
Iteration 48/1000 | Loss: 0.00033079
Iteration 49/1000 | Loss: 0.00033404
Iteration 50/1000 | Loss: 0.00030752
Iteration 51/1000 | Loss: 0.00030016
Iteration 52/1000 | Loss: 0.00050004
Iteration 53/1000 | Loss: 0.00072023
Iteration 54/1000 | Loss: 0.00117207
Iteration 55/1000 | Loss: 0.00050921
Iteration 56/1000 | Loss: 0.00030042
Iteration 57/1000 | Loss: 0.00032783
Iteration 58/1000 | Loss: 0.00039246
Iteration 59/1000 | Loss: 0.00083197
Iteration 60/1000 | Loss: 0.00336470
Iteration 61/1000 | Loss: 0.00102297
Iteration 62/1000 | Loss: 0.00057581
Iteration 63/1000 | Loss: 0.00030727
Iteration 64/1000 | Loss: 0.00046073
Iteration 65/1000 | Loss: 0.00029343
Iteration 66/1000 | Loss: 0.00035500
Iteration 67/1000 | Loss: 0.00035851
Iteration 68/1000 | Loss: 0.00034975
Iteration 69/1000 | Loss: 0.00045546
Iteration 70/1000 | Loss: 0.00028244
Iteration 71/1000 | Loss: 0.00038907
Iteration 72/1000 | Loss: 0.00031689
Iteration 73/1000 | Loss: 0.00027697
Iteration 74/1000 | Loss: 0.00056901
Iteration 75/1000 | Loss: 0.00046621
Iteration 76/1000 | Loss: 0.00045675
Iteration 77/1000 | Loss: 0.00029032
Iteration 78/1000 | Loss: 0.00052469
Iteration 79/1000 | Loss: 0.00045426
Iteration 80/1000 | Loss: 0.00041324
Iteration 81/1000 | Loss: 0.00028429
Iteration 82/1000 | Loss: 0.00026818
Iteration 83/1000 | Loss: 0.00041452
Iteration 84/1000 | Loss: 0.00027772
Iteration 85/1000 | Loss: 0.00026652
Iteration 86/1000 | Loss: 0.00027412
Iteration 87/1000 | Loss: 0.00032722
Iteration 88/1000 | Loss: 0.00026265
Iteration 89/1000 | Loss: 0.00028406
Iteration 90/1000 | Loss: 0.00027156
Iteration 91/1000 | Loss: 0.00032201
Iteration 92/1000 | Loss: 0.00026583
Iteration 93/1000 | Loss: 0.00026117
Iteration 94/1000 | Loss: 0.00030316
Iteration 95/1000 | Loss: 0.00029286
Iteration 96/1000 | Loss: 0.00026067
Iteration 97/1000 | Loss: 0.00026045
Iteration 98/1000 | Loss: 0.00026019
Iteration 99/1000 | Loss: 0.00025997
Iteration 100/1000 | Loss: 0.00025976
Iteration 101/1000 | Loss: 0.00027250
Iteration 102/1000 | Loss: 0.00025961
Iteration 103/1000 | Loss: 0.00029661
Iteration 104/1000 | Loss: 0.00031800
Iteration 105/1000 | Loss: 0.00025948
Iteration 106/1000 | Loss: 0.00026124
Iteration 107/1000 | Loss: 0.00025943
Iteration 108/1000 | Loss: 0.00025943
Iteration 109/1000 | Loss: 0.00025943
Iteration 110/1000 | Loss: 0.00025942
Iteration 111/1000 | Loss: 0.00025942
Iteration 112/1000 | Loss: 0.00025942
Iteration 113/1000 | Loss: 0.00025942
Iteration 114/1000 | Loss: 0.00025942
Iteration 115/1000 | Loss: 0.00025942
Iteration 116/1000 | Loss: 0.00025942
Iteration 117/1000 | Loss: 0.00025942
Iteration 118/1000 | Loss: 0.00025942
Iteration 119/1000 | Loss: 0.00025941
Iteration 120/1000 | Loss: 0.00025940
Iteration 121/1000 | Loss: 0.00025939
Iteration 122/1000 | Loss: 0.00025939
Iteration 123/1000 | Loss: 0.00025939
Iteration 124/1000 | Loss: 0.00025939
Iteration 125/1000 | Loss: 0.00025939
Iteration 126/1000 | Loss: 0.00025939
Iteration 127/1000 | Loss: 0.00025939
Iteration 128/1000 | Loss: 0.00025939
Iteration 129/1000 | Loss: 0.00025933
Iteration 130/1000 | Loss: 0.00025929
Iteration 131/1000 | Loss: 0.00025929
Iteration 132/1000 | Loss: 0.00025929
Iteration 133/1000 | Loss: 0.00025929
Iteration 134/1000 | Loss: 0.00025929
Iteration 135/1000 | Loss: 0.00025929
Iteration 136/1000 | Loss: 0.00025929
Iteration 137/1000 | Loss: 0.00025928
Iteration 138/1000 | Loss: 0.00025928
Iteration 139/1000 | Loss: 0.00025928
Iteration 140/1000 | Loss: 0.00025928
Iteration 141/1000 | Loss: 0.00025928
Iteration 142/1000 | Loss: 0.00025928
Iteration 143/1000 | Loss: 0.00025928
Iteration 144/1000 | Loss: 0.00025928
Iteration 145/1000 | Loss: 0.00025928
Iteration 146/1000 | Loss: 0.00025926
Iteration 147/1000 | Loss: 0.00025925
Iteration 148/1000 | Loss: 0.00025925
Iteration 149/1000 | Loss: 0.00025924
Iteration 150/1000 | Loss: 0.00025924
Iteration 151/1000 | Loss: 0.00025924
Iteration 152/1000 | Loss: 0.00025924
Iteration 153/1000 | Loss: 0.00025924
Iteration 154/1000 | Loss: 0.00025924
Iteration 155/1000 | Loss: 0.00025924
Iteration 156/1000 | Loss: 0.00025924
Iteration 157/1000 | Loss: 0.00025923
Iteration 158/1000 | Loss: 0.00025923
Iteration 159/1000 | Loss: 0.00025923
Iteration 160/1000 | Loss: 0.00025923
Iteration 161/1000 | Loss: 0.00025923
Iteration 162/1000 | Loss: 0.00025923
Iteration 163/1000 | Loss: 0.00025923
Iteration 164/1000 | Loss: 0.00025923
Iteration 165/1000 | Loss: 0.00025922
Iteration 166/1000 | Loss: 0.00025922
Iteration 167/1000 | Loss: 0.00025922
Iteration 168/1000 | Loss: 0.00025921
Iteration 169/1000 | Loss: 0.00025921
Iteration 170/1000 | Loss: 0.00025921
Iteration 171/1000 | Loss: 0.00025920
Iteration 172/1000 | Loss: 0.00025920
Iteration 173/1000 | Loss: 0.00025920
Iteration 174/1000 | Loss: 0.00025920
Iteration 175/1000 | Loss: 0.00025920
Iteration 176/1000 | Loss: 0.00025920
Iteration 177/1000 | Loss: 0.00025920
Iteration 178/1000 | Loss: 0.00025920
Iteration 179/1000 | Loss: 0.00025919
Iteration 180/1000 | Loss: 0.00025919
Iteration 181/1000 | Loss: 0.00025919
Iteration 182/1000 | Loss: 0.00025919
Iteration 183/1000 | Loss: 0.00025919
Iteration 184/1000 | Loss: 0.00025919
Iteration 185/1000 | Loss: 0.00025919
Iteration 186/1000 | Loss: 0.00025919
Iteration 187/1000 | Loss: 0.00025918
Iteration 188/1000 | Loss: 0.00025918
Iteration 189/1000 | Loss: 0.00025918
Iteration 190/1000 | Loss: 0.00025918
Iteration 191/1000 | Loss: 0.00025918
Iteration 192/1000 | Loss: 0.00025918
Iteration 193/1000 | Loss: 0.00025918
Iteration 194/1000 | Loss: 0.00025917
Iteration 195/1000 | Loss: 0.00025917
Iteration 196/1000 | Loss: 0.00025917
Iteration 197/1000 | Loss: 0.00025917
Iteration 198/1000 | Loss: 0.00025917
Iteration 199/1000 | Loss: 0.00025917
Iteration 200/1000 | Loss: 0.00025917
Iteration 201/1000 | Loss: 0.00025916
Iteration 202/1000 | Loss: 0.00025916
Iteration 203/1000 | Loss: 0.00025916
Iteration 204/1000 | Loss: 0.00025916
Iteration 205/1000 | Loss: 0.00025916
Iteration 206/1000 | Loss: 0.00025916
Iteration 207/1000 | Loss: 0.00025916
Iteration 208/1000 | Loss: 0.00025916
Iteration 209/1000 | Loss: 0.00025916
Iteration 210/1000 | Loss: 0.00025916
Iteration 211/1000 | Loss: 0.00025916
Iteration 212/1000 | Loss: 0.00025916
Iteration 213/1000 | Loss: 0.00025916
Iteration 214/1000 | Loss: 0.00025916
Iteration 215/1000 | Loss: 0.00025916
Iteration 216/1000 | Loss: 0.00025916
Iteration 217/1000 | Loss: 0.00025916
Iteration 218/1000 | Loss: 0.00025916
Iteration 219/1000 | Loss: 0.00025916
Iteration 220/1000 | Loss: 0.00025916
Iteration 221/1000 | Loss: 0.00025916
Iteration 222/1000 | Loss: 0.00025915
Iteration 223/1000 | Loss: 0.00025915
Iteration 224/1000 | Loss: 0.00025915
Iteration 225/1000 | Loss: 0.00025915
Iteration 226/1000 | Loss: 0.00025915
Iteration 227/1000 | Loss: 0.00025915
Iteration 228/1000 | Loss: 0.00025915
Iteration 229/1000 | Loss: 0.00025915
Iteration 230/1000 | Loss: 0.00025915
Iteration 231/1000 | Loss: 0.00025914
Iteration 232/1000 | Loss: 0.00025914
Iteration 233/1000 | Loss: 0.00025914
Iteration 234/1000 | Loss: 0.00025914
Iteration 235/1000 | Loss: 0.00025914
Iteration 236/1000 | Loss: 0.00025914
Iteration 237/1000 | Loss: 0.00025914
Iteration 238/1000 | Loss: 0.00025914
Iteration 239/1000 | Loss: 0.00025914
Iteration 240/1000 | Loss: 0.00025914
Iteration 241/1000 | Loss: 0.00025914
Iteration 242/1000 | Loss: 0.00025914
Iteration 243/1000 | Loss: 0.00025914
Iteration 244/1000 | Loss: 0.00025914
Iteration 245/1000 | Loss: 0.00025914
Iteration 246/1000 | Loss: 0.00025914
Iteration 247/1000 | Loss: 0.00025913
Iteration 248/1000 | Loss: 0.00025913
Iteration 249/1000 | Loss: 0.00025913
Iteration 250/1000 | Loss: 0.00025913
Iteration 251/1000 | Loss: 0.00025913
Iteration 252/1000 | Loss: 0.00025913
Iteration 253/1000 | Loss: 0.00025913
Iteration 254/1000 | Loss: 0.00025913
Iteration 255/1000 | Loss: 0.00025913
Iteration 256/1000 | Loss: 0.00025913
Iteration 257/1000 | Loss: 0.00025913
Iteration 258/1000 | Loss: 0.00025913
Iteration 259/1000 | Loss: 0.00025913
Iteration 260/1000 | Loss: 0.00025913
Iteration 261/1000 | Loss: 0.00025913
Iteration 262/1000 | Loss: 0.00025913
Iteration 263/1000 | Loss: 0.00025913
Iteration 264/1000 | Loss: 0.00025913
Iteration 265/1000 | Loss: 0.00025913
Iteration 266/1000 | Loss: 0.00025913
Iteration 267/1000 | Loss: 0.00025913
Iteration 268/1000 | Loss: 0.00025913
Iteration 269/1000 | Loss: 0.00025913
Iteration 270/1000 | Loss: 0.00025913
Iteration 271/1000 | Loss: 0.00025913
Iteration 272/1000 | Loss: 0.00025913
Iteration 273/1000 | Loss: 0.00025913
Iteration 274/1000 | Loss: 0.00025913
Iteration 275/1000 | Loss: 0.00025913
Iteration 276/1000 | Loss: 0.00025913
Iteration 277/1000 | Loss: 0.00025913
Iteration 278/1000 | Loss: 0.00025913
Iteration 279/1000 | Loss: 0.00025913
Iteration 280/1000 | Loss: 0.00025913
Iteration 281/1000 | Loss: 0.00025913
Iteration 282/1000 | Loss: 0.00025913
Iteration 283/1000 | Loss: 0.00025913
Iteration 284/1000 | Loss: 0.00025913
Iteration 285/1000 | Loss: 0.00025913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [0.00025912735145539045, 0.00025912735145539045, 0.00025912735145539045, 0.00025912735145539045, 0.00025912735145539045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025912735145539045

Optimization complete. Final v2v error: 9.085294723510742 mm

Highest mean error: 12.260978698730469 mm for frame 176

Lowest mean error: 4.683788299560547 mm for frame 227

Saving results

Total time: 228.51310539245605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793353
Iteration 2/25 | Loss: 0.00150546
Iteration 3/25 | Loss: 0.00131155
Iteration 4/25 | Loss: 0.00129687
Iteration 5/25 | Loss: 0.00129154
Iteration 6/25 | Loss: 0.00129021
Iteration 7/25 | Loss: 0.00129021
Iteration 8/25 | Loss: 0.00129021
Iteration 9/25 | Loss: 0.00129021
Iteration 10/25 | Loss: 0.00129021
Iteration 11/25 | Loss: 0.00129021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012902113376185298, 0.0012902113376185298, 0.0012902113376185298, 0.0012902113376185298, 0.0012902113376185298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012902113376185298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20588779
Iteration 2/25 | Loss: 0.00096370
Iteration 3/25 | Loss: 0.00096370
Iteration 4/25 | Loss: 0.00096370
Iteration 5/25 | Loss: 0.00096370
Iteration 6/25 | Loss: 0.00096370
Iteration 7/25 | Loss: 0.00096370
Iteration 8/25 | Loss: 0.00096370
Iteration 9/25 | Loss: 0.00096370
Iteration 10/25 | Loss: 0.00096370
Iteration 11/25 | Loss: 0.00096370
Iteration 12/25 | Loss: 0.00096370
Iteration 13/25 | Loss: 0.00096370
Iteration 14/25 | Loss: 0.00096370
Iteration 15/25 | Loss: 0.00096370
Iteration 16/25 | Loss: 0.00096370
Iteration 17/25 | Loss: 0.00096370
Iteration 18/25 | Loss: 0.00096370
Iteration 19/25 | Loss: 0.00096370
Iteration 20/25 | Loss: 0.00096370
Iteration 21/25 | Loss: 0.00096370
Iteration 22/25 | Loss: 0.00096370
Iteration 23/25 | Loss: 0.00096370
Iteration 24/25 | Loss: 0.00096370
Iteration 25/25 | Loss: 0.00096370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009637009352445602, 0.0009637009352445602, 0.0009637009352445602, 0.0009637009352445602, 0.0009637009352445602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009637009352445602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096370
Iteration 2/1000 | Loss: 0.00005351
Iteration 3/1000 | Loss: 0.00003288
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002024
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001771
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001657
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001614
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001606
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001579
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001562
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001560
Iteration 41/1000 | Loss: 0.00001559
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001542
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001540
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001539
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001534
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001533
Iteration 91/1000 | Loss: 0.00001533
Iteration 92/1000 | Loss: 0.00001533
Iteration 93/1000 | Loss: 0.00001533
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001530
Iteration 101/1000 | Loss: 0.00001530
Iteration 102/1000 | Loss: 0.00001529
Iteration 103/1000 | Loss: 0.00001529
Iteration 104/1000 | Loss: 0.00001529
Iteration 105/1000 | Loss: 0.00001529
Iteration 106/1000 | Loss: 0.00001529
Iteration 107/1000 | Loss: 0.00001529
Iteration 108/1000 | Loss: 0.00001529
Iteration 109/1000 | Loss: 0.00001528
Iteration 110/1000 | Loss: 0.00001528
Iteration 111/1000 | Loss: 0.00001527
Iteration 112/1000 | Loss: 0.00001527
Iteration 113/1000 | Loss: 0.00001527
Iteration 114/1000 | Loss: 0.00001527
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001526
Iteration 117/1000 | Loss: 0.00001526
Iteration 118/1000 | Loss: 0.00001526
Iteration 119/1000 | Loss: 0.00001526
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001525
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001524
Iteration 132/1000 | Loss: 0.00001524
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001524
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00001524
Iteration 137/1000 | Loss: 0.00001524
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Iteration 141/1000 | Loss: 0.00001523
Iteration 142/1000 | Loss: 0.00001523
Iteration 143/1000 | Loss: 0.00001523
Iteration 144/1000 | Loss: 0.00001523
Iteration 145/1000 | Loss: 0.00001522
Iteration 146/1000 | Loss: 0.00001522
Iteration 147/1000 | Loss: 0.00001522
Iteration 148/1000 | Loss: 0.00001522
Iteration 149/1000 | Loss: 0.00001522
Iteration 150/1000 | Loss: 0.00001522
Iteration 151/1000 | Loss: 0.00001522
Iteration 152/1000 | Loss: 0.00001522
Iteration 153/1000 | Loss: 0.00001522
Iteration 154/1000 | Loss: 0.00001522
Iteration 155/1000 | Loss: 0.00001522
Iteration 156/1000 | Loss: 0.00001521
Iteration 157/1000 | Loss: 0.00001521
Iteration 158/1000 | Loss: 0.00001521
Iteration 159/1000 | Loss: 0.00001521
Iteration 160/1000 | Loss: 0.00001521
Iteration 161/1000 | Loss: 0.00001521
Iteration 162/1000 | Loss: 0.00001521
Iteration 163/1000 | Loss: 0.00001521
Iteration 164/1000 | Loss: 0.00001521
Iteration 165/1000 | Loss: 0.00001521
Iteration 166/1000 | Loss: 0.00001521
Iteration 167/1000 | Loss: 0.00001521
Iteration 168/1000 | Loss: 0.00001520
Iteration 169/1000 | Loss: 0.00001520
Iteration 170/1000 | Loss: 0.00001520
Iteration 171/1000 | Loss: 0.00001520
Iteration 172/1000 | Loss: 0.00001520
Iteration 173/1000 | Loss: 0.00001520
Iteration 174/1000 | Loss: 0.00001520
Iteration 175/1000 | Loss: 0.00001520
Iteration 176/1000 | Loss: 0.00001520
Iteration 177/1000 | Loss: 0.00001520
Iteration 178/1000 | Loss: 0.00001520
Iteration 179/1000 | Loss: 0.00001520
Iteration 180/1000 | Loss: 0.00001519
Iteration 181/1000 | Loss: 0.00001519
Iteration 182/1000 | Loss: 0.00001519
Iteration 183/1000 | Loss: 0.00001519
Iteration 184/1000 | Loss: 0.00001519
Iteration 185/1000 | Loss: 0.00001519
Iteration 186/1000 | Loss: 0.00001519
Iteration 187/1000 | Loss: 0.00001519
Iteration 188/1000 | Loss: 0.00001519
Iteration 189/1000 | Loss: 0.00001519
Iteration 190/1000 | Loss: 0.00001519
Iteration 191/1000 | Loss: 0.00001519
Iteration 192/1000 | Loss: 0.00001519
Iteration 193/1000 | Loss: 0.00001518
Iteration 194/1000 | Loss: 0.00001518
Iteration 195/1000 | Loss: 0.00001518
Iteration 196/1000 | Loss: 0.00001518
Iteration 197/1000 | Loss: 0.00001518
Iteration 198/1000 | Loss: 0.00001518
Iteration 199/1000 | Loss: 0.00001518
Iteration 200/1000 | Loss: 0.00001518
Iteration 201/1000 | Loss: 0.00001518
Iteration 202/1000 | Loss: 0.00001518
Iteration 203/1000 | Loss: 0.00001518
Iteration 204/1000 | Loss: 0.00001518
Iteration 205/1000 | Loss: 0.00001518
Iteration 206/1000 | Loss: 0.00001517
Iteration 207/1000 | Loss: 0.00001517
Iteration 208/1000 | Loss: 0.00001517
Iteration 209/1000 | Loss: 0.00001517
Iteration 210/1000 | Loss: 0.00001517
Iteration 211/1000 | Loss: 0.00001517
Iteration 212/1000 | Loss: 0.00001517
Iteration 213/1000 | Loss: 0.00001517
Iteration 214/1000 | Loss: 0.00001517
Iteration 215/1000 | Loss: 0.00001517
Iteration 216/1000 | Loss: 0.00001517
Iteration 217/1000 | Loss: 0.00001517
Iteration 218/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.5174777217907831e-05, 1.5174777217907831e-05, 1.5174777217907831e-05, 1.5174777217907831e-05, 1.5174777217907831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5174777217907831e-05

Optimization complete. Final v2v error: 3.24375581741333 mm

Highest mean error: 4.479367256164551 mm for frame 62

Lowest mean error: 2.7964322566986084 mm for frame 93

Saving results

Total time: 46.15140914916992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983869
Iteration 2/25 | Loss: 0.00235466
Iteration 3/25 | Loss: 0.00185222
Iteration 4/25 | Loss: 0.00177815
Iteration 5/25 | Loss: 0.00161028
Iteration 6/25 | Loss: 0.00150991
Iteration 7/25 | Loss: 0.00145382
Iteration 8/25 | Loss: 0.00141903
Iteration 9/25 | Loss: 0.00140471
Iteration 10/25 | Loss: 0.00138576
Iteration 11/25 | Loss: 0.00137178
Iteration 12/25 | Loss: 0.00136532
Iteration 13/25 | Loss: 0.00136107
Iteration 14/25 | Loss: 0.00136244
Iteration 15/25 | Loss: 0.00136085
Iteration 16/25 | Loss: 0.00136692
Iteration 17/25 | Loss: 0.00136452
Iteration 18/25 | Loss: 0.00136144
Iteration 19/25 | Loss: 0.00135691
Iteration 20/25 | Loss: 0.00136294
Iteration 21/25 | Loss: 0.00135625
Iteration 22/25 | Loss: 0.00135872
Iteration 23/25 | Loss: 0.00135195
Iteration 24/25 | Loss: 0.00135269
Iteration 25/25 | Loss: 0.00135045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39897013
Iteration 2/25 | Loss: 0.00106658
Iteration 3/25 | Loss: 0.00106001
Iteration 4/25 | Loss: 0.00106001
Iteration 5/25 | Loss: 0.00106001
Iteration 6/25 | Loss: 0.00106001
Iteration 7/25 | Loss: 0.00106001
Iteration 8/25 | Loss: 0.00106001
Iteration 9/25 | Loss: 0.00106001
Iteration 10/25 | Loss: 0.00106001
Iteration 11/25 | Loss: 0.00106001
Iteration 12/25 | Loss: 0.00106001
Iteration 13/25 | Loss: 0.00106001
Iteration 14/25 | Loss: 0.00106001
Iteration 15/25 | Loss: 0.00106001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010600098175927997, 0.0010600098175927997, 0.0010600098175927997, 0.0010600098175927997, 0.0010600098175927997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010600098175927997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106001
Iteration 2/1000 | Loss: 0.00010686
Iteration 3/1000 | Loss: 0.00008271
Iteration 4/1000 | Loss: 0.00021058
Iteration 5/1000 | Loss: 0.00015321
Iteration 6/1000 | Loss: 0.00011507
Iteration 7/1000 | Loss: 0.00012576
Iteration 8/1000 | Loss: 0.00003897
Iteration 9/1000 | Loss: 0.00005242
Iteration 10/1000 | Loss: 0.00004224
Iteration 11/1000 | Loss: 0.00003379
Iteration 12/1000 | Loss: 0.00004129
Iteration 13/1000 | Loss: 0.00004827
Iteration 14/1000 | Loss: 0.00016329
Iteration 15/1000 | Loss: 0.00015120
Iteration 16/1000 | Loss: 0.00003566
Iteration 17/1000 | Loss: 0.00003018
Iteration 18/1000 | Loss: 0.00004329
Iteration 19/1000 | Loss: 0.00004103
Iteration 20/1000 | Loss: 0.00004545
Iteration 21/1000 | Loss: 0.00004070
Iteration 22/1000 | Loss: 0.00004056
Iteration 23/1000 | Loss: 0.00004836
Iteration 24/1000 | Loss: 0.00006158
Iteration 25/1000 | Loss: 0.00004875
Iteration 26/1000 | Loss: 0.00004103
Iteration 27/1000 | Loss: 0.00003894
Iteration 28/1000 | Loss: 0.00004177
Iteration 29/1000 | Loss: 0.00004168
Iteration 30/1000 | Loss: 0.00003670
Iteration 31/1000 | Loss: 0.00003817
Iteration 32/1000 | Loss: 0.00005764
Iteration 33/1000 | Loss: 0.00005145
Iteration 34/1000 | Loss: 0.00003753
Iteration 35/1000 | Loss: 0.00003541
Iteration 36/1000 | Loss: 0.00003123
Iteration 37/1000 | Loss: 0.00003645
Iteration 38/1000 | Loss: 0.00003723
Iteration 39/1000 | Loss: 0.00004246
Iteration 40/1000 | Loss: 0.00003180
Iteration 41/1000 | Loss: 0.00003906
Iteration 42/1000 | Loss: 0.00005714
Iteration 43/1000 | Loss: 0.00005502
Iteration 44/1000 | Loss: 0.00005327
Iteration 45/1000 | Loss: 0.00005523
Iteration 46/1000 | Loss: 0.00004540
Iteration 47/1000 | Loss: 0.00004380
Iteration 48/1000 | Loss: 0.00003938
Iteration 49/1000 | Loss: 0.00005004
Iteration 50/1000 | Loss: 0.00004340
Iteration 51/1000 | Loss: 0.00003343
Iteration 52/1000 | Loss: 0.00004442
Iteration 53/1000 | Loss: 0.00003800
Iteration 54/1000 | Loss: 0.00004530
Iteration 55/1000 | Loss: 0.00003882
Iteration 56/1000 | Loss: 0.00004720
Iteration 57/1000 | Loss: 0.00004446
Iteration 58/1000 | Loss: 0.00004515
Iteration 59/1000 | Loss: 0.00004220
Iteration 60/1000 | Loss: 0.00004286
Iteration 61/1000 | Loss: 0.00004514
Iteration 62/1000 | Loss: 0.00004316
Iteration 63/1000 | Loss: 0.00015398
Iteration 64/1000 | Loss: 0.00008436
Iteration 65/1000 | Loss: 0.00007370
Iteration 66/1000 | Loss: 0.00003178
Iteration 67/1000 | Loss: 0.00003328
Iteration 68/1000 | Loss: 0.00002794
Iteration 69/1000 | Loss: 0.00002739
Iteration 70/1000 | Loss: 0.00002763
Iteration 71/1000 | Loss: 0.00002720
Iteration 72/1000 | Loss: 0.00002680
Iteration 73/1000 | Loss: 0.00002700
Iteration 74/1000 | Loss: 0.00002660
Iteration 75/1000 | Loss: 0.00002659
Iteration 76/1000 | Loss: 0.00002659
Iteration 77/1000 | Loss: 0.00002658
Iteration 78/1000 | Loss: 0.00002658
Iteration 79/1000 | Loss: 0.00002657
Iteration 80/1000 | Loss: 0.00002652
Iteration 81/1000 | Loss: 0.00002651
Iteration 82/1000 | Loss: 0.00002651
Iteration 83/1000 | Loss: 0.00002651
Iteration 84/1000 | Loss: 0.00002651
Iteration 85/1000 | Loss: 0.00002651
Iteration 86/1000 | Loss: 0.00002650
Iteration 87/1000 | Loss: 0.00002649
Iteration 88/1000 | Loss: 0.00002649
Iteration 89/1000 | Loss: 0.00002649
Iteration 90/1000 | Loss: 0.00002648
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002648
Iteration 94/1000 | Loss: 0.00002647
Iteration 95/1000 | Loss: 0.00002647
Iteration 96/1000 | Loss: 0.00002647
Iteration 97/1000 | Loss: 0.00002647
Iteration 98/1000 | Loss: 0.00002647
Iteration 99/1000 | Loss: 0.00002646
Iteration 100/1000 | Loss: 0.00002646
Iteration 101/1000 | Loss: 0.00002645
Iteration 102/1000 | Loss: 0.00002644
Iteration 103/1000 | Loss: 0.00002644
Iteration 104/1000 | Loss: 0.00002644
Iteration 105/1000 | Loss: 0.00002644
Iteration 106/1000 | Loss: 0.00002644
Iteration 107/1000 | Loss: 0.00002644
Iteration 108/1000 | Loss: 0.00002644
Iteration 109/1000 | Loss: 0.00002644
Iteration 110/1000 | Loss: 0.00002644
Iteration 111/1000 | Loss: 0.00002644
Iteration 112/1000 | Loss: 0.00002643
Iteration 113/1000 | Loss: 0.00002642
Iteration 114/1000 | Loss: 0.00002642
Iteration 115/1000 | Loss: 0.00002641
Iteration 116/1000 | Loss: 0.00002641
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002639
Iteration 119/1000 | Loss: 0.00002639
Iteration 120/1000 | Loss: 0.00002639
Iteration 121/1000 | Loss: 0.00002639
Iteration 122/1000 | Loss: 0.00002639
Iteration 123/1000 | Loss: 0.00002639
Iteration 124/1000 | Loss: 0.00002639
Iteration 125/1000 | Loss: 0.00002639
Iteration 126/1000 | Loss: 0.00002638
Iteration 127/1000 | Loss: 0.00002638
Iteration 128/1000 | Loss: 0.00002638
Iteration 129/1000 | Loss: 0.00002638
Iteration 130/1000 | Loss: 0.00002638
Iteration 131/1000 | Loss: 0.00002637
Iteration 132/1000 | Loss: 0.00002636
Iteration 133/1000 | Loss: 0.00002636
Iteration 134/1000 | Loss: 0.00002635
Iteration 135/1000 | Loss: 0.00002634
Iteration 136/1000 | Loss: 0.00002634
Iteration 137/1000 | Loss: 0.00002633
Iteration 138/1000 | Loss: 0.00002633
Iteration 139/1000 | Loss: 0.00002632
Iteration 140/1000 | Loss: 0.00002632
Iteration 141/1000 | Loss: 0.00002632
Iteration 142/1000 | Loss: 0.00002632
Iteration 143/1000 | Loss: 0.00002632
Iteration 144/1000 | Loss: 0.00002632
Iteration 145/1000 | Loss: 0.00002631
Iteration 146/1000 | Loss: 0.00002631
Iteration 147/1000 | Loss: 0.00002635
Iteration 148/1000 | Loss: 0.00013636
Iteration 149/1000 | Loss: 0.00013929
Iteration 150/1000 | Loss: 0.00012537
Iteration 151/1000 | Loss: 0.00008570
Iteration 152/1000 | Loss: 0.00013815
Iteration 153/1000 | Loss: 0.00006963
Iteration 154/1000 | Loss: 0.00087930
Iteration 155/1000 | Loss: 0.00021164
Iteration 156/1000 | Loss: 0.00002943
Iteration 157/1000 | Loss: 0.00002838
Iteration 158/1000 | Loss: 0.00002782
Iteration 159/1000 | Loss: 0.00002817
Iteration 160/1000 | Loss: 0.00002754
Iteration 161/1000 | Loss: 0.00019404
Iteration 162/1000 | Loss: 0.00055378
Iteration 163/1000 | Loss: 0.00011273
Iteration 164/1000 | Loss: 0.00002907
Iteration 165/1000 | Loss: 0.00019106
Iteration 166/1000 | Loss: 0.00007733
Iteration 167/1000 | Loss: 0.00017231
Iteration 168/1000 | Loss: 0.00006349
Iteration 169/1000 | Loss: 0.00002791
Iteration 170/1000 | Loss: 0.00017801
Iteration 171/1000 | Loss: 0.00006111
Iteration 172/1000 | Loss: 0.00016401
Iteration 173/1000 | Loss: 0.00006230
Iteration 174/1000 | Loss: 0.00002873
Iteration 175/1000 | Loss: 0.00002757
Iteration 176/1000 | Loss: 0.00058263
Iteration 177/1000 | Loss: 0.00012247
Iteration 178/1000 | Loss: 0.00009207
Iteration 179/1000 | Loss: 0.00002796
Iteration 180/1000 | Loss: 0.00053549
Iteration 181/1000 | Loss: 0.00009973
Iteration 182/1000 | Loss: 0.00010956
Iteration 183/1000 | Loss: 0.00021026
Iteration 184/1000 | Loss: 0.00003011
Iteration 185/1000 | Loss: 0.00017033
Iteration 186/1000 | Loss: 0.00016627
Iteration 187/1000 | Loss: 0.00018961
Iteration 188/1000 | Loss: 0.00003641
Iteration 189/1000 | Loss: 0.00003091
Iteration 190/1000 | Loss: 0.00003424
Iteration 191/1000 | Loss: 0.00002931
Iteration 192/1000 | Loss: 0.00002798
Iteration 193/1000 | Loss: 0.00002749
Iteration 194/1000 | Loss: 0.00002720
Iteration 195/1000 | Loss: 0.00002769
Iteration 196/1000 | Loss: 0.00002685
Iteration 197/1000 | Loss: 0.00002679
Iteration 198/1000 | Loss: 0.00002672
Iteration 199/1000 | Loss: 0.00003276
Iteration 200/1000 | Loss: 0.00002705
Iteration 201/1000 | Loss: 0.00002671
Iteration 202/1000 | Loss: 0.00002643
Iteration 203/1000 | Loss: 0.00002642
Iteration 204/1000 | Loss: 0.00002642
Iteration 205/1000 | Loss: 0.00002642
Iteration 206/1000 | Loss: 0.00002642
Iteration 207/1000 | Loss: 0.00002641
Iteration 208/1000 | Loss: 0.00002641
Iteration 209/1000 | Loss: 0.00002641
Iteration 210/1000 | Loss: 0.00002640
Iteration 211/1000 | Loss: 0.00002640
Iteration 212/1000 | Loss: 0.00002640
Iteration 213/1000 | Loss: 0.00002640
Iteration 214/1000 | Loss: 0.00002639
Iteration 215/1000 | Loss: 0.00023602
Iteration 216/1000 | Loss: 0.00012864
Iteration 217/1000 | Loss: 0.00021053
Iteration 218/1000 | Loss: 0.00010180
Iteration 219/1000 | Loss: 0.00011219
Iteration 220/1000 | Loss: 0.00002803
Iteration 221/1000 | Loss: 0.00002772
Iteration 222/1000 | Loss: 0.00002611
Iteration 223/1000 | Loss: 0.00002671
Iteration 224/1000 | Loss: 0.00002508
Iteration 225/1000 | Loss: 0.00003212
Iteration 226/1000 | Loss: 0.00002444
Iteration 227/1000 | Loss: 0.00002442
Iteration 228/1000 | Loss: 0.00002438
Iteration 229/1000 | Loss: 0.00002438
Iteration 230/1000 | Loss: 0.00002438
Iteration 231/1000 | Loss: 0.00002718
Iteration 232/1000 | Loss: 0.00002429
Iteration 233/1000 | Loss: 0.00002426
Iteration 234/1000 | Loss: 0.00002426
Iteration 235/1000 | Loss: 0.00002426
Iteration 236/1000 | Loss: 0.00002426
Iteration 237/1000 | Loss: 0.00002426
Iteration 238/1000 | Loss: 0.00002426
Iteration 239/1000 | Loss: 0.00002426
Iteration 240/1000 | Loss: 0.00002426
Iteration 241/1000 | Loss: 0.00002425
Iteration 242/1000 | Loss: 0.00002425
Iteration 243/1000 | Loss: 0.00002425
Iteration 244/1000 | Loss: 0.00002425
Iteration 245/1000 | Loss: 0.00002425
Iteration 246/1000 | Loss: 0.00002425
Iteration 247/1000 | Loss: 0.00002424
Iteration 248/1000 | Loss: 0.00002424
Iteration 249/1000 | Loss: 0.00002424
Iteration 250/1000 | Loss: 0.00002424
Iteration 251/1000 | Loss: 0.00002424
Iteration 252/1000 | Loss: 0.00002424
Iteration 253/1000 | Loss: 0.00002423
Iteration 254/1000 | Loss: 0.00002423
Iteration 255/1000 | Loss: 0.00002423
Iteration 256/1000 | Loss: 0.00002423
Iteration 257/1000 | Loss: 0.00002423
Iteration 258/1000 | Loss: 0.00002423
Iteration 259/1000 | Loss: 0.00002423
Iteration 260/1000 | Loss: 0.00002423
Iteration 261/1000 | Loss: 0.00002422
Iteration 262/1000 | Loss: 0.00002422
Iteration 263/1000 | Loss: 0.00002422
Iteration 264/1000 | Loss: 0.00002422
Iteration 265/1000 | Loss: 0.00002422
Iteration 266/1000 | Loss: 0.00002607
Iteration 267/1000 | Loss: 0.00002420
Iteration 268/1000 | Loss: 0.00002419
Iteration 269/1000 | Loss: 0.00002416
Iteration 270/1000 | Loss: 0.00002416
Iteration 271/1000 | Loss: 0.00002416
Iteration 272/1000 | Loss: 0.00002416
Iteration 273/1000 | Loss: 0.00002416
Iteration 274/1000 | Loss: 0.00002416
Iteration 275/1000 | Loss: 0.00002416
Iteration 276/1000 | Loss: 0.00002416
Iteration 277/1000 | Loss: 0.00002415
Iteration 278/1000 | Loss: 0.00002415
Iteration 279/1000 | Loss: 0.00002415
Iteration 280/1000 | Loss: 0.00002415
Iteration 281/1000 | Loss: 0.00002415
Iteration 282/1000 | Loss: 0.00002415
Iteration 283/1000 | Loss: 0.00002415
Iteration 284/1000 | Loss: 0.00002415
Iteration 285/1000 | Loss: 0.00002415
Iteration 286/1000 | Loss: 0.00002415
Iteration 287/1000 | Loss: 0.00002415
Iteration 288/1000 | Loss: 0.00002415
Iteration 289/1000 | Loss: 0.00002415
Iteration 290/1000 | Loss: 0.00002415
Iteration 291/1000 | Loss: 0.00002415
Iteration 292/1000 | Loss: 0.00002415
Iteration 293/1000 | Loss: 0.00002415
Iteration 294/1000 | Loss: 0.00002415
Iteration 295/1000 | Loss: 0.00002415
Iteration 296/1000 | Loss: 0.00002415
Iteration 297/1000 | Loss: 0.00002415
Iteration 298/1000 | Loss: 0.00002415
Iteration 299/1000 | Loss: 0.00002415
Iteration 300/1000 | Loss: 0.00002415
Iteration 301/1000 | Loss: 0.00002415
Iteration 302/1000 | Loss: 0.00002415
Iteration 303/1000 | Loss: 0.00002415
Iteration 304/1000 | Loss: 0.00002415
Iteration 305/1000 | Loss: 0.00002415
Iteration 306/1000 | Loss: 0.00002415
Iteration 307/1000 | Loss: 0.00002415
Iteration 308/1000 | Loss: 0.00002415
Iteration 309/1000 | Loss: 0.00002415
Iteration 310/1000 | Loss: 0.00002415
Iteration 311/1000 | Loss: 0.00002415
Iteration 312/1000 | Loss: 0.00002415
Iteration 313/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 313. Stopping optimization.
Last 5 losses: [2.4145329007296823e-05, 2.4145329007296823e-05, 2.4145329007296823e-05, 2.4145329007296823e-05, 2.4145329007296823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4145329007296823e-05

Optimization complete. Final v2v error: 4.012496471405029 mm

Highest mean error: 12.601523399353027 mm for frame 39

Lowest mean error: 3.3322410583496094 mm for frame 62

Saving results

Total time: 284.9778504371643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803789
Iteration 2/25 | Loss: 0.00147165
Iteration 3/25 | Loss: 0.00133241
Iteration 4/25 | Loss: 0.00132041
Iteration 5/25 | Loss: 0.00131706
Iteration 6/25 | Loss: 0.00131706
Iteration 7/25 | Loss: 0.00131706
Iteration 8/25 | Loss: 0.00131706
Iteration 9/25 | Loss: 0.00131706
Iteration 10/25 | Loss: 0.00131706
Iteration 11/25 | Loss: 0.00131706
Iteration 12/25 | Loss: 0.00131706
Iteration 13/25 | Loss: 0.00131706
Iteration 14/25 | Loss: 0.00131706
Iteration 15/25 | Loss: 0.00131706
Iteration 16/25 | Loss: 0.00131706
Iteration 17/25 | Loss: 0.00131706
Iteration 18/25 | Loss: 0.00131706
Iteration 19/25 | Loss: 0.00131706
Iteration 20/25 | Loss: 0.00131706
Iteration 21/25 | Loss: 0.00131706
Iteration 22/25 | Loss: 0.00131706
Iteration 23/25 | Loss: 0.00131706
Iteration 24/25 | Loss: 0.00131706
Iteration 25/25 | Loss: 0.00131706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.49962044
Iteration 2/25 | Loss: 0.00080894
Iteration 3/25 | Loss: 0.00080884
Iteration 4/25 | Loss: 0.00080884
Iteration 5/25 | Loss: 0.00080884
Iteration 6/25 | Loss: 0.00080884
Iteration 7/25 | Loss: 0.00080884
Iteration 8/25 | Loss: 0.00080884
Iteration 9/25 | Loss: 0.00080884
Iteration 10/25 | Loss: 0.00080884
Iteration 11/25 | Loss: 0.00080884
Iteration 12/25 | Loss: 0.00080884
Iteration 13/25 | Loss: 0.00080884
Iteration 14/25 | Loss: 0.00080884
Iteration 15/25 | Loss: 0.00080884
Iteration 16/25 | Loss: 0.00080884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008088350878097117, 0.0008088350878097117, 0.0008088350878097117, 0.0008088350878097117, 0.0008088350878097117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008088350878097117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080884
Iteration 2/1000 | Loss: 0.00003517
Iteration 3/1000 | Loss: 0.00002613
Iteration 4/1000 | Loss: 0.00002403
Iteration 5/1000 | Loss: 0.00002287
Iteration 6/1000 | Loss: 0.00002196
Iteration 7/1000 | Loss: 0.00002143
Iteration 8/1000 | Loss: 0.00002097
Iteration 9/1000 | Loss: 0.00002051
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001956
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001942
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001918
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001914
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001912
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001911
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001910
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001903
Iteration 41/1000 | Loss: 0.00001903
Iteration 42/1000 | Loss: 0.00001902
Iteration 43/1000 | Loss: 0.00001902
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001899
Iteration 48/1000 | Loss: 0.00001899
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001898
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001898
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001898
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001895
Iteration 72/1000 | Loss: 0.00001895
Iteration 73/1000 | Loss: 0.00001895
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Iteration 97/1000 | Loss: 0.00001892
Iteration 98/1000 | Loss: 0.00001892
Iteration 99/1000 | Loss: 0.00001892
Iteration 100/1000 | Loss: 0.00001892
Iteration 101/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.891996362246573e-05, 1.891996362246573e-05, 1.891996362246573e-05, 1.891996362246573e-05, 1.891996362246573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.891996362246573e-05

Optimization complete. Final v2v error: 3.679569959640503 mm

Highest mean error: 4.654654502868652 mm for frame 109

Lowest mean error: 3.2041056156158447 mm for frame 25

Saving results

Total time: 43.03288006782532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958017
Iteration 2/25 | Loss: 0.00236979
Iteration 3/25 | Loss: 0.00168707
Iteration 4/25 | Loss: 0.00151695
Iteration 5/25 | Loss: 0.00149511
Iteration 6/25 | Loss: 0.00150260
Iteration 7/25 | Loss: 0.00146460
Iteration 8/25 | Loss: 0.00142992
Iteration 9/25 | Loss: 0.00140909
Iteration 10/25 | Loss: 0.00139501
Iteration 11/25 | Loss: 0.00139035
Iteration 12/25 | Loss: 0.00139208
Iteration 13/25 | Loss: 0.00140531
Iteration 14/25 | Loss: 0.00137873
Iteration 15/25 | Loss: 0.00138273
Iteration 16/25 | Loss: 0.00137490
Iteration 17/25 | Loss: 0.00136603
Iteration 18/25 | Loss: 0.00137318
Iteration 19/25 | Loss: 0.00136996
Iteration 20/25 | Loss: 0.00136492
Iteration 21/25 | Loss: 0.00135986
Iteration 22/25 | Loss: 0.00135992
Iteration 23/25 | Loss: 0.00136162
Iteration 24/25 | Loss: 0.00135962
Iteration 25/25 | Loss: 0.00136141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29830730
Iteration 2/25 | Loss: 0.00149600
Iteration 3/25 | Loss: 0.00090399
Iteration 4/25 | Loss: 0.00090398
Iteration 5/25 | Loss: 0.00090398
Iteration 6/25 | Loss: 0.00090398
Iteration 7/25 | Loss: 0.00090398
Iteration 8/25 | Loss: 0.00090398
Iteration 9/25 | Loss: 0.00090398
Iteration 10/25 | Loss: 0.00090398
Iteration 11/25 | Loss: 0.00090398
Iteration 12/25 | Loss: 0.00090398
Iteration 13/25 | Loss: 0.00090398
Iteration 14/25 | Loss: 0.00090398
Iteration 15/25 | Loss: 0.00090398
Iteration 16/25 | Loss: 0.00090398
Iteration 17/25 | Loss: 0.00090398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009039780707098544, 0.0009039780707098544, 0.0009039780707098544, 0.0009039780707098544, 0.0009039780707098544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009039780707098544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090398
Iteration 2/1000 | Loss: 0.00061433
Iteration 3/1000 | Loss: 0.00098799
Iteration 4/1000 | Loss: 0.00019185
Iteration 5/1000 | Loss: 0.00030803
Iteration 6/1000 | Loss: 0.00033455
Iteration 7/1000 | Loss: 0.00070517
Iteration 8/1000 | Loss: 0.00111969
Iteration 9/1000 | Loss: 0.00162078
Iteration 10/1000 | Loss: 0.00032812
Iteration 11/1000 | Loss: 0.00007819
Iteration 12/1000 | Loss: 0.00005419
Iteration 13/1000 | Loss: 0.00021675
Iteration 14/1000 | Loss: 0.00086005
Iteration 15/1000 | Loss: 0.00058913
Iteration 16/1000 | Loss: 0.00074237
Iteration 17/1000 | Loss: 0.00021029
Iteration 18/1000 | Loss: 0.00005029
Iteration 19/1000 | Loss: 0.00011382
Iteration 20/1000 | Loss: 0.00004394
Iteration 21/1000 | Loss: 0.00003988
Iteration 22/1000 | Loss: 0.00025072
Iteration 23/1000 | Loss: 0.00032592
Iteration 24/1000 | Loss: 0.00022896
Iteration 25/1000 | Loss: 0.00037620
Iteration 26/1000 | Loss: 0.00077131
Iteration 27/1000 | Loss: 0.00006366
Iteration 28/1000 | Loss: 0.00006798
Iteration 29/1000 | Loss: 0.00003525
Iteration 30/1000 | Loss: 0.00013436
Iteration 31/1000 | Loss: 0.00009803
Iteration 32/1000 | Loss: 0.00003638
Iteration 33/1000 | Loss: 0.00004077
Iteration 34/1000 | Loss: 0.00004199
Iteration 35/1000 | Loss: 0.00021737
Iteration 36/1000 | Loss: 0.00016865
Iteration 37/1000 | Loss: 0.00007734
Iteration 38/1000 | Loss: 0.00003348
Iteration 39/1000 | Loss: 0.00022403
Iteration 40/1000 | Loss: 0.00014226
Iteration 41/1000 | Loss: 0.00007506
Iteration 42/1000 | Loss: 0.00007487
Iteration 43/1000 | Loss: 0.00006678
Iteration 44/1000 | Loss: 0.00016292
Iteration 45/1000 | Loss: 0.00009413
Iteration 46/1000 | Loss: 0.00008785
Iteration 47/1000 | Loss: 0.00003424
Iteration 48/1000 | Loss: 0.00003199
Iteration 49/1000 | Loss: 0.00003135
Iteration 50/1000 | Loss: 0.00003099
Iteration 51/1000 | Loss: 0.00003097
Iteration 52/1000 | Loss: 0.00011560
Iteration 53/1000 | Loss: 0.00022991
Iteration 54/1000 | Loss: 0.00015786
Iteration 55/1000 | Loss: 0.00009296
Iteration 56/1000 | Loss: 0.00014974
Iteration 57/1000 | Loss: 0.00003332
Iteration 58/1000 | Loss: 0.00003219
Iteration 59/1000 | Loss: 0.00003158
Iteration 60/1000 | Loss: 0.00003130
Iteration 61/1000 | Loss: 0.00003101
Iteration 62/1000 | Loss: 0.00003076
Iteration 63/1000 | Loss: 0.00003056
Iteration 64/1000 | Loss: 0.00018835
Iteration 65/1000 | Loss: 0.00019212
Iteration 66/1000 | Loss: 0.00037862
Iteration 67/1000 | Loss: 0.00029928
Iteration 68/1000 | Loss: 0.00045070
Iteration 69/1000 | Loss: 0.00109793
Iteration 70/1000 | Loss: 0.00004209
Iteration 71/1000 | Loss: 0.00029929
Iteration 72/1000 | Loss: 0.00007799
Iteration 73/1000 | Loss: 0.00006394
Iteration 74/1000 | Loss: 0.00003123
Iteration 75/1000 | Loss: 0.00003063
Iteration 76/1000 | Loss: 0.00003017
Iteration 77/1000 | Loss: 0.00016374
Iteration 78/1000 | Loss: 0.00010752
Iteration 79/1000 | Loss: 0.00015934
Iteration 80/1000 | Loss: 0.00018521
Iteration 81/1000 | Loss: 0.00048738
Iteration 82/1000 | Loss: 0.00004412
Iteration 83/1000 | Loss: 0.00003184
Iteration 84/1000 | Loss: 0.00003067
Iteration 85/1000 | Loss: 0.00002997
Iteration 86/1000 | Loss: 0.00013661
Iteration 87/1000 | Loss: 0.00004829
Iteration 88/1000 | Loss: 0.00007285
Iteration 89/1000 | Loss: 0.00002981
Iteration 90/1000 | Loss: 0.00002971
Iteration 91/1000 | Loss: 0.00002968
Iteration 92/1000 | Loss: 0.00002968
Iteration 93/1000 | Loss: 0.00015989
Iteration 94/1000 | Loss: 0.00015016
Iteration 95/1000 | Loss: 0.00006583
Iteration 96/1000 | Loss: 0.00002984
Iteration 97/1000 | Loss: 0.00016285
Iteration 98/1000 | Loss: 0.00003542
Iteration 99/1000 | Loss: 0.00003228
Iteration 100/1000 | Loss: 0.00019092
Iteration 101/1000 | Loss: 0.00003503
Iteration 102/1000 | Loss: 0.00003210
Iteration 103/1000 | Loss: 0.00003103
Iteration 104/1000 | Loss: 0.00014523
Iteration 105/1000 | Loss: 0.00020183
Iteration 106/1000 | Loss: 0.00003470
Iteration 107/1000 | Loss: 0.00007432
Iteration 108/1000 | Loss: 0.00003819
Iteration 109/1000 | Loss: 0.00003132
Iteration 110/1000 | Loss: 0.00003487
Iteration 111/1000 | Loss: 0.00003079
Iteration 112/1000 | Loss: 0.00012019
Iteration 113/1000 | Loss: 0.00009324
Iteration 114/1000 | Loss: 0.00010784
Iteration 115/1000 | Loss: 0.00003672
Iteration 116/1000 | Loss: 0.00003081
Iteration 117/1000 | Loss: 0.00003332
Iteration 118/1000 | Loss: 0.00024372
Iteration 119/1000 | Loss: 0.00005371
Iteration 120/1000 | Loss: 0.00003011
Iteration 121/1000 | Loss: 0.00002983
Iteration 122/1000 | Loss: 0.00002980
Iteration 123/1000 | Loss: 0.00002979
Iteration 124/1000 | Loss: 0.00002978
Iteration 125/1000 | Loss: 0.00002977
Iteration 126/1000 | Loss: 0.00002976
Iteration 127/1000 | Loss: 0.00009200
Iteration 128/1000 | Loss: 0.00003138
Iteration 129/1000 | Loss: 0.00003760
Iteration 130/1000 | Loss: 0.00002974
Iteration 131/1000 | Loss: 0.00020627
Iteration 132/1000 | Loss: 0.00004339
Iteration 133/1000 | Loss: 0.00003285
Iteration 134/1000 | Loss: 0.00003077
Iteration 135/1000 | Loss: 0.00002955
Iteration 136/1000 | Loss: 0.00008029
Iteration 137/1000 | Loss: 0.00002891
Iteration 138/1000 | Loss: 0.00002864
Iteration 139/1000 | Loss: 0.00006385
Iteration 140/1000 | Loss: 0.00002826
Iteration 141/1000 | Loss: 0.00002804
Iteration 142/1000 | Loss: 0.00002800
Iteration 143/1000 | Loss: 0.00002793
Iteration 144/1000 | Loss: 0.00002793
Iteration 145/1000 | Loss: 0.00002792
Iteration 146/1000 | Loss: 0.00002790
Iteration 147/1000 | Loss: 0.00002789
Iteration 148/1000 | Loss: 0.00002789
Iteration 149/1000 | Loss: 0.00002788
Iteration 150/1000 | Loss: 0.00002787
Iteration 151/1000 | Loss: 0.00002787
Iteration 152/1000 | Loss: 0.00002786
Iteration 153/1000 | Loss: 0.00002786
Iteration 154/1000 | Loss: 0.00002786
Iteration 155/1000 | Loss: 0.00002785
Iteration 156/1000 | Loss: 0.00002785
Iteration 157/1000 | Loss: 0.00002785
Iteration 158/1000 | Loss: 0.00002785
Iteration 159/1000 | Loss: 0.00002784
Iteration 160/1000 | Loss: 0.00002784
Iteration 161/1000 | Loss: 0.00002784
Iteration 162/1000 | Loss: 0.00002784
Iteration 163/1000 | Loss: 0.00002783
Iteration 164/1000 | Loss: 0.00002783
Iteration 165/1000 | Loss: 0.00012301
Iteration 166/1000 | Loss: 0.00003040
Iteration 167/1000 | Loss: 0.00006582
Iteration 168/1000 | Loss: 0.00002814
Iteration 169/1000 | Loss: 0.00002784
Iteration 170/1000 | Loss: 0.00002783
Iteration 171/1000 | Loss: 0.00002781
Iteration 172/1000 | Loss: 0.00002781
Iteration 173/1000 | Loss: 0.00002781
Iteration 174/1000 | Loss: 0.00002781
Iteration 175/1000 | Loss: 0.00002781
Iteration 176/1000 | Loss: 0.00002781
Iteration 177/1000 | Loss: 0.00002780
Iteration 178/1000 | Loss: 0.00002780
Iteration 179/1000 | Loss: 0.00002780
Iteration 180/1000 | Loss: 0.00002780
Iteration 181/1000 | Loss: 0.00002780
Iteration 182/1000 | Loss: 0.00002776
Iteration 183/1000 | Loss: 0.00002776
Iteration 184/1000 | Loss: 0.00002775
Iteration 185/1000 | Loss: 0.00002775
Iteration 186/1000 | Loss: 0.00002774
Iteration 187/1000 | Loss: 0.00002774
Iteration 188/1000 | Loss: 0.00002774
Iteration 189/1000 | Loss: 0.00002774
Iteration 190/1000 | Loss: 0.00002774
Iteration 191/1000 | Loss: 0.00002773
Iteration 192/1000 | Loss: 0.00002773
Iteration 193/1000 | Loss: 0.00002773
Iteration 194/1000 | Loss: 0.00002773
Iteration 195/1000 | Loss: 0.00002773
Iteration 196/1000 | Loss: 0.00002773
Iteration 197/1000 | Loss: 0.00002773
Iteration 198/1000 | Loss: 0.00002773
Iteration 199/1000 | Loss: 0.00002773
Iteration 200/1000 | Loss: 0.00002773
Iteration 201/1000 | Loss: 0.00002772
Iteration 202/1000 | Loss: 0.00002772
Iteration 203/1000 | Loss: 0.00002772
Iteration 204/1000 | Loss: 0.00002772
Iteration 205/1000 | Loss: 0.00002772
Iteration 206/1000 | Loss: 0.00002772
Iteration 207/1000 | Loss: 0.00002772
Iteration 208/1000 | Loss: 0.00002772
Iteration 209/1000 | Loss: 0.00002772
Iteration 210/1000 | Loss: 0.00002772
Iteration 211/1000 | Loss: 0.00002772
Iteration 212/1000 | Loss: 0.00002772
Iteration 213/1000 | Loss: 0.00002771
Iteration 214/1000 | Loss: 0.00002771
Iteration 215/1000 | Loss: 0.00002771
Iteration 216/1000 | Loss: 0.00002771
Iteration 217/1000 | Loss: 0.00002771
Iteration 218/1000 | Loss: 0.00002771
Iteration 219/1000 | Loss: 0.00002771
Iteration 220/1000 | Loss: 0.00002770
Iteration 221/1000 | Loss: 0.00002770
Iteration 222/1000 | Loss: 0.00002770
Iteration 223/1000 | Loss: 0.00002770
Iteration 224/1000 | Loss: 0.00002770
Iteration 225/1000 | Loss: 0.00002769
Iteration 226/1000 | Loss: 0.00002769
Iteration 227/1000 | Loss: 0.00002769
Iteration 228/1000 | Loss: 0.00002769
Iteration 229/1000 | Loss: 0.00002768
Iteration 230/1000 | Loss: 0.00002768
Iteration 231/1000 | Loss: 0.00002768
Iteration 232/1000 | Loss: 0.00002768
Iteration 233/1000 | Loss: 0.00002768
Iteration 234/1000 | Loss: 0.00002768
Iteration 235/1000 | Loss: 0.00002768
Iteration 236/1000 | Loss: 0.00002767
Iteration 237/1000 | Loss: 0.00002767
Iteration 238/1000 | Loss: 0.00002767
Iteration 239/1000 | Loss: 0.00002767
Iteration 240/1000 | Loss: 0.00002767
Iteration 241/1000 | Loss: 0.00002767
Iteration 242/1000 | Loss: 0.00002767
Iteration 243/1000 | Loss: 0.00002767
Iteration 244/1000 | Loss: 0.00002767
Iteration 245/1000 | Loss: 0.00002767
Iteration 246/1000 | Loss: 0.00002767
Iteration 247/1000 | Loss: 0.00002767
Iteration 248/1000 | Loss: 0.00002767
Iteration 249/1000 | Loss: 0.00002767
Iteration 250/1000 | Loss: 0.00002767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.767228943412192e-05, 2.767228943412192e-05, 2.767228943412192e-05, 2.767228943412192e-05, 2.767228943412192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.767228943412192e-05

Optimization complete. Final v2v error: 4.350259304046631 mm

Highest mean error: 6.161746978759766 mm for frame 9

Lowest mean error: 3.311479091644287 mm for frame 59

Saving results

Total time: 285.52919340133667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643156
Iteration 2/25 | Loss: 0.00173258
Iteration 3/25 | Loss: 0.00144303
Iteration 4/25 | Loss: 0.00136941
Iteration 5/25 | Loss: 0.00135893
Iteration 6/25 | Loss: 0.00135667
Iteration 7/25 | Loss: 0.00135572
Iteration 8/25 | Loss: 0.00135955
Iteration 9/25 | Loss: 0.00135836
Iteration 10/25 | Loss: 0.00135371
Iteration 11/25 | Loss: 0.00135832
Iteration 12/25 | Loss: 0.00135768
Iteration 13/25 | Loss: 0.00135601
Iteration 14/25 | Loss: 0.00135346
Iteration 15/25 | Loss: 0.00135329
Iteration 16/25 | Loss: 0.00135329
Iteration 17/25 | Loss: 0.00135329
Iteration 18/25 | Loss: 0.00135329
Iteration 19/25 | Loss: 0.00135329
Iteration 20/25 | Loss: 0.00135329
Iteration 21/25 | Loss: 0.00135329
Iteration 22/25 | Loss: 0.00135329
Iteration 23/25 | Loss: 0.00135329
Iteration 24/25 | Loss: 0.00135329
Iteration 25/25 | Loss: 0.00135329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93995619
Iteration 2/25 | Loss: 0.00092618
Iteration 3/25 | Loss: 0.00092613
Iteration 4/25 | Loss: 0.00092613
Iteration 5/25 | Loss: 0.00092613
Iteration 6/25 | Loss: 0.00092613
Iteration 7/25 | Loss: 0.00092613
Iteration 8/25 | Loss: 0.00092613
Iteration 9/25 | Loss: 0.00092613
Iteration 10/25 | Loss: 0.00092612
Iteration 11/25 | Loss: 0.00092612
Iteration 12/25 | Loss: 0.00092612
Iteration 13/25 | Loss: 0.00092612
Iteration 14/25 | Loss: 0.00092612
Iteration 15/25 | Loss: 0.00092612
Iteration 16/25 | Loss: 0.00092612
Iteration 17/25 | Loss: 0.00092612
Iteration 18/25 | Loss: 0.00092612
Iteration 19/25 | Loss: 0.00092612
Iteration 20/25 | Loss: 0.00092612
Iteration 21/25 | Loss: 0.00092612
Iteration 22/25 | Loss: 0.00092612
Iteration 23/25 | Loss: 0.00092612
Iteration 24/25 | Loss: 0.00092612
Iteration 25/25 | Loss: 0.00092612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092612
Iteration 2/1000 | Loss: 0.00003069
Iteration 3/1000 | Loss: 0.00007789
Iteration 4/1000 | Loss: 0.00002822
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00001929
Iteration 7/1000 | Loss: 0.00001851
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00007447
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001689
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00008374
Iteration 21/1000 | Loss: 0.00001671
Iteration 22/1000 | Loss: 0.00001646
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001636
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001636
Iteration 37/1000 | Loss: 0.00001636
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001636
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001635
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001635
Iteration 46/1000 | Loss: 0.00001635
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001634
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001631
Iteration 63/1000 | Loss: 0.00001630
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001630
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001630
Iteration 68/1000 | Loss: 0.00001630
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00001620
Iteration 74/1000 | Loss: 0.00001620
Iteration 75/1000 | Loss: 0.00001620
Iteration 76/1000 | Loss: 0.00001620
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001619
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001617
Iteration 86/1000 | Loss: 0.00001617
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001611
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001607
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001606
Iteration 109/1000 | Loss: 0.00001606
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001601
Iteration 117/1000 | Loss: 0.00001601
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001596
Iteration 142/1000 | Loss: 0.00001596
Iteration 143/1000 | Loss: 0.00001596
Iteration 144/1000 | Loss: 0.00001596
Iteration 145/1000 | Loss: 0.00001596
Iteration 146/1000 | Loss: 0.00001596
Iteration 147/1000 | Loss: 0.00001596
Iteration 148/1000 | Loss: 0.00001595
Iteration 149/1000 | Loss: 0.00001595
Iteration 150/1000 | Loss: 0.00001595
Iteration 151/1000 | Loss: 0.00001595
Iteration 152/1000 | Loss: 0.00001595
Iteration 153/1000 | Loss: 0.00001594
Iteration 154/1000 | Loss: 0.00001594
Iteration 155/1000 | Loss: 0.00001594
Iteration 156/1000 | Loss: 0.00001594
Iteration 157/1000 | Loss: 0.00001594
Iteration 158/1000 | Loss: 0.00001594
Iteration 159/1000 | Loss: 0.00001594
Iteration 160/1000 | Loss: 0.00001593
Iteration 161/1000 | Loss: 0.00001593
Iteration 162/1000 | Loss: 0.00001593
Iteration 163/1000 | Loss: 0.00001593
Iteration 164/1000 | Loss: 0.00001593
Iteration 165/1000 | Loss: 0.00001593
Iteration 166/1000 | Loss: 0.00001593
Iteration 167/1000 | Loss: 0.00001592
Iteration 168/1000 | Loss: 0.00001592
Iteration 169/1000 | Loss: 0.00001592
Iteration 170/1000 | Loss: 0.00001592
Iteration 171/1000 | Loss: 0.00001591
Iteration 172/1000 | Loss: 0.00001591
Iteration 173/1000 | Loss: 0.00001591
Iteration 174/1000 | Loss: 0.00001591
Iteration 175/1000 | Loss: 0.00001591
Iteration 176/1000 | Loss: 0.00001591
Iteration 177/1000 | Loss: 0.00001591
Iteration 178/1000 | Loss: 0.00001590
Iteration 179/1000 | Loss: 0.00001590
Iteration 180/1000 | Loss: 0.00001590
Iteration 181/1000 | Loss: 0.00001590
Iteration 182/1000 | Loss: 0.00001590
Iteration 183/1000 | Loss: 0.00001590
Iteration 184/1000 | Loss: 0.00001590
Iteration 185/1000 | Loss: 0.00001589
Iteration 186/1000 | Loss: 0.00001589
Iteration 187/1000 | Loss: 0.00001589
Iteration 188/1000 | Loss: 0.00001589
Iteration 189/1000 | Loss: 0.00001589
Iteration 190/1000 | Loss: 0.00001589
Iteration 191/1000 | Loss: 0.00001589
Iteration 192/1000 | Loss: 0.00001589
Iteration 193/1000 | Loss: 0.00001589
Iteration 194/1000 | Loss: 0.00001589
Iteration 195/1000 | Loss: 0.00001589
Iteration 196/1000 | Loss: 0.00001588
Iteration 197/1000 | Loss: 0.00001588
Iteration 198/1000 | Loss: 0.00001588
Iteration 199/1000 | Loss: 0.00001588
Iteration 200/1000 | Loss: 0.00001588
Iteration 201/1000 | Loss: 0.00001588
Iteration 202/1000 | Loss: 0.00001588
Iteration 203/1000 | Loss: 0.00001588
Iteration 204/1000 | Loss: 0.00001588
Iteration 205/1000 | Loss: 0.00001588
Iteration 206/1000 | Loss: 0.00001588
Iteration 207/1000 | Loss: 0.00001588
Iteration 208/1000 | Loss: 0.00001588
Iteration 209/1000 | Loss: 0.00001587
Iteration 210/1000 | Loss: 0.00001587
Iteration 211/1000 | Loss: 0.00001587
Iteration 212/1000 | Loss: 0.00001587
Iteration 213/1000 | Loss: 0.00001587
Iteration 214/1000 | Loss: 0.00001587
Iteration 215/1000 | Loss: 0.00001587
Iteration 216/1000 | Loss: 0.00001587
Iteration 217/1000 | Loss: 0.00001587
Iteration 218/1000 | Loss: 0.00001587
Iteration 219/1000 | Loss: 0.00001587
Iteration 220/1000 | Loss: 0.00001587
Iteration 221/1000 | Loss: 0.00001587
Iteration 222/1000 | Loss: 0.00001587
Iteration 223/1000 | Loss: 0.00001587
Iteration 224/1000 | Loss: 0.00001587
Iteration 225/1000 | Loss: 0.00001587
Iteration 226/1000 | Loss: 0.00001587
Iteration 227/1000 | Loss: 0.00001587
Iteration 228/1000 | Loss: 0.00001587
Iteration 229/1000 | Loss: 0.00001587
Iteration 230/1000 | Loss: 0.00001587
Iteration 231/1000 | Loss: 0.00001587
Iteration 232/1000 | Loss: 0.00001587
Iteration 233/1000 | Loss: 0.00001587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.5872690710239112e-05, 1.5872690710239112e-05, 1.5872690710239112e-05, 1.5872690710239112e-05, 1.5872690710239112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5872690710239112e-05

Optimization complete. Final v2v error: 3.3159193992614746 mm

Highest mean error: 4.167983055114746 mm for frame 92

Lowest mean error: 2.859079122543335 mm for frame 47

Saving results

Total time: 76.07772421836853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889518
Iteration 2/25 | Loss: 0.00166814
Iteration 3/25 | Loss: 0.00150833
Iteration 4/25 | Loss: 0.00147697
Iteration 5/25 | Loss: 0.00145394
Iteration 6/25 | Loss: 0.00142269
Iteration 7/25 | Loss: 0.00141373
Iteration 8/25 | Loss: 0.00141051
Iteration 9/25 | Loss: 0.00140728
Iteration 10/25 | Loss: 0.00140670
Iteration 11/25 | Loss: 0.00141999
Iteration 12/25 | Loss: 0.00145650
Iteration 13/25 | Loss: 0.00142765
Iteration 14/25 | Loss: 0.00141066
Iteration 15/25 | Loss: 0.00140819
Iteration 16/25 | Loss: 0.00141771
Iteration 17/25 | Loss: 0.00140711
Iteration 18/25 | Loss: 0.00139416
Iteration 19/25 | Loss: 0.00138390
Iteration 20/25 | Loss: 0.00138287
Iteration 21/25 | Loss: 0.00138415
Iteration 22/25 | Loss: 0.00138497
Iteration 23/25 | Loss: 0.00138241
Iteration 24/25 | Loss: 0.00138001
Iteration 25/25 | Loss: 0.00137930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40434146
Iteration 2/25 | Loss: 0.00105129
Iteration 3/25 | Loss: 0.00105128
Iteration 4/25 | Loss: 0.00105128
Iteration 5/25 | Loss: 0.00105128
Iteration 6/25 | Loss: 0.00105128
Iteration 7/25 | Loss: 0.00105128
Iteration 8/25 | Loss: 0.00105128
Iteration 9/25 | Loss: 0.00105128
Iteration 10/25 | Loss: 0.00105128
Iteration 11/25 | Loss: 0.00105128
Iteration 12/25 | Loss: 0.00105128
Iteration 13/25 | Loss: 0.00105128
Iteration 14/25 | Loss: 0.00105128
Iteration 15/25 | Loss: 0.00105128
Iteration 16/25 | Loss: 0.00105128
Iteration 17/25 | Loss: 0.00105128
Iteration 18/25 | Loss: 0.00105128
Iteration 19/25 | Loss: 0.00105128
Iteration 20/25 | Loss: 0.00105128
Iteration 21/25 | Loss: 0.00105128
Iteration 22/25 | Loss: 0.00105128
Iteration 23/25 | Loss: 0.00105128
Iteration 24/25 | Loss: 0.00105128
Iteration 25/25 | Loss: 0.00105128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105128
Iteration 2/1000 | Loss: 0.00008426
Iteration 3/1000 | Loss: 0.00057868
Iteration 4/1000 | Loss: 0.00039192
Iteration 5/1000 | Loss: 0.00077811
Iteration 6/1000 | Loss: 0.00043284
Iteration 7/1000 | Loss: 0.00057085
Iteration 8/1000 | Loss: 0.00039690
Iteration 9/1000 | Loss: 0.00026749
Iteration 10/1000 | Loss: 0.00016120
Iteration 11/1000 | Loss: 0.00023867
Iteration 12/1000 | Loss: 0.00027109
Iteration 13/1000 | Loss: 0.00010898
Iteration 14/1000 | Loss: 0.00007954
Iteration 15/1000 | Loss: 0.00004224
Iteration 16/1000 | Loss: 0.00005060
Iteration 17/1000 | Loss: 0.00018208
Iteration 18/1000 | Loss: 0.00013604
Iteration 19/1000 | Loss: 0.00007439
Iteration 20/1000 | Loss: 0.00003898
Iteration 21/1000 | Loss: 0.00032398
Iteration 22/1000 | Loss: 0.00019091
Iteration 23/1000 | Loss: 0.00020329
Iteration 24/1000 | Loss: 0.00013349
Iteration 25/1000 | Loss: 0.00008112
Iteration 26/1000 | Loss: 0.00004111
Iteration 27/1000 | Loss: 0.00004302
Iteration 28/1000 | Loss: 0.00004337
Iteration 29/1000 | Loss: 0.00003820
Iteration 30/1000 | Loss: 0.00003447
Iteration 31/1000 | Loss: 0.00003424
Iteration 32/1000 | Loss: 0.00004584
Iteration 33/1000 | Loss: 0.00004052
Iteration 34/1000 | Loss: 0.00004296
Iteration 35/1000 | Loss: 0.00042664
Iteration 36/1000 | Loss: 0.00033046
Iteration 37/1000 | Loss: 0.00006397
Iteration 38/1000 | Loss: 0.00032363
Iteration 39/1000 | Loss: 0.00024271
Iteration 40/1000 | Loss: 0.00006069
Iteration 41/1000 | Loss: 0.00004489
Iteration 42/1000 | Loss: 0.00007737
Iteration 43/1000 | Loss: 0.00010192
Iteration 44/1000 | Loss: 0.00004618
Iteration 45/1000 | Loss: 0.00006144
Iteration 46/1000 | Loss: 0.00003291
Iteration 47/1000 | Loss: 0.00003043
Iteration 48/1000 | Loss: 0.00002983
Iteration 49/1000 | Loss: 0.00002786
Iteration 50/1000 | Loss: 0.00002716
Iteration 51/1000 | Loss: 0.00002658
Iteration 52/1000 | Loss: 0.00002620
Iteration 53/1000 | Loss: 0.00002570
Iteration 54/1000 | Loss: 0.00002525
Iteration 55/1000 | Loss: 0.00002498
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002473
Iteration 58/1000 | Loss: 0.00002454
Iteration 59/1000 | Loss: 0.00002451
Iteration 60/1000 | Loss: 0.00002447
Iteration 61/1000 | Loss: 0.00002443
Iteration 62/1000 | Loss: 0.00002442
Iteration 63/1000 | Loss: 0.00002432
Iteration 64/1000 | Loss: 0.00002432
Iteration 65/1000 | Loss: 0.00002423
Iteration 66/1000 | Loss: 0.00002421
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002416
Iteration 69/1000 | Loss: 0.00002415
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002409
Iteration 72/1000 | Loss: 0.00002408
Iteration 73/1000 | Loss: 0.00002408
Iteration 74/1000 | Loss: 0.00002408
Iteration 75/1000 | Loss: 0.00002408
Iteration 76/1000 | Loss: 0.00002408
Iteration 77/1000 | Loss: 0.00002407
Iteration 78/1000 | Loss: 0.00002407
Iteration 79/1000 | Loss: 0.00002407
Iteration 80/1000 | Loss: 0.00002406
Iteration 81/1000 | Loss: 0.00002406
Iteration 82/1000 | Loss: 0.00002406
Iteration 83/1000 | Loss: 0.00002406
Iteration 84/1000 | Loss: 0.00002406
Iteration 85/1000 | Loss: 0.00002405
Iteration 86/1000 | Loss: 0.00002405
Iteration 87/1000 | Loss: 0.00002405
Iteration 88/1000 | Loss: 0.00002405
Iteration 89/1000 | Loss: 0.00002404
Iteration 90/1000 | Loss: 0.00002404
Iteration 91/1000 | Loss: 0.00002404
Iteration 92/1000 | Loss: 0.00002403
Iteration 93/1000 | Loss: 0.00002403
Iteration 94/1000 | Loss: 0.00002403
Iteration 95/1000 | Loss: 0.00002402
Iteration 96/1000 | Loss: 0.00002402
Iteration 97/1000 | Loss: 0.00002401
Iteration 98/1000 | Loss: 0.00002401
Iteration 99/1000 | Loss: 0.00002401
Iteration 100/1000 | Loss: 0.00002400
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00002400
Iteration 103/1000 | Loss: 0.00002399
Iteration 104/1000 | Loss: 0.00002398
Iteration 105/1000 | Loss: 0.00002398
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00002397
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002396
Iteration 110/1000 | Loss: 0.00002396
Iteration 111/1000 | Loss: 0.00002396
Iteration 112/1000 | Loss: 0.00002395
Iteration 113/1000 | Loss: 0.00002395
Iteration 114/1000 | Loss: 0.00002395
Iteration 115/1000 | Loss: 0.00002394
Iteration 116/1000 | Loss: 0.00002394
Iteration 117/1000 | Loss: 0.00002394
Iteration 118/1000 | Loss: 0.00002393
Iteration 119/1000 | Loss: 0.00002393
Iteration 120/1000 | Loss: 0.00002392
Iteration 121/1000 | Loss: 0.00002392
Iteration 122/1000 | Loss: 0.00002392
Iteration 123/1000 | Loss: 0.00002392
Iteration 124/1000 | Loss: 0.00002391
Iteration 125/1000 | Loss: 0.00002391
Iteration 126/1000 | Loss: 0.00002391
Iteration 127/1000 | Loss: 0.00002391
Iteration 128/1000 | Loss: 0.00002391
Iteration 129/1000 | Loss: 0.00002390
Iteration 130/1000 | Loss: 0.00002390
Iteration 131/1000 | Loss: 0.00002390
Iteration 132/1000 | Loss: 0.00002390
Iteration 133/1000 | Loss: 0.00002390
Iteration 134/1000 | Loss: 0.00002390
Iteration 135/1000 | Loss: 0.00002389
Iteration 136/1000 | Loss: 0.00002389
Iteration 137/1000 | Loss: 0.00002389
Iteration 138/1000 | Loss: 0.00002389
Iteration 139/1000 | Loss: 0.00002389
Iteration 140/1000 | Loss: 0.00002389
Iteration 141/1000 | Loss: 0.00002389
Iteration 142/1000 | Loss: 0.00002389
Iteration 143/1000 | Loss: 0.00002389
Iteration 144/1000 | Loss: 0.00002388
Iteration 145/1000 | Loss: 0.00002388
Iteration 146/1000 | Loss: 0.00002388
Iteration 147/1000 | Loss: 0.00002388
Iteration 148/1000 | Loss: 0.00002388
Iteration 149/1000 | Loss: 0.00002388
Iteration 150/1000 | Loss: 0.00002388
Iteration 151/1000 | Loss: 0.00002388
Iteration 152/1000 | Loss: 0.00002388
Iteration 153/1000 | Loss: 0.00002388
Iteration 154/1000 | Loss: 0.00002388
Iteration 155/1000 | Loss: 0.00002388
Iteration 156/1000 | Loss: 0.00002387
Iteration 157/1000 | Loss: 0.00002387
Iteration 158/1000 | Loss: 0.00002387
Iteration 159/1000 | Loss: 0.00002387
Iteration 160/1000 | Loss: 0.00002387
Iteration 161/1000 | Loss: 0.00002387
Iteration 162/1000 | Loss: 0.00002387
Iteration 163/1000 | Loss: 0.00002386
Iteration 164/1000 | Loss: 0.00002386
Iteration 165/1000 | Loss: 0.00002386
Iteration 166/1000 | Loss: 0.00002386
Iteration 167/1000 | Loss: 0.00002386
Iteration 168/1000 | Loss: 0.00002386
Iteration 169/1000 | Loss: 0.00002386
Iteration 170/1000 | Loss: 0.00002386
Iteration 171/1000 | Loss: 0.00002385
Iteration 172/1000 | Loss: 0.00002385
Iteration 173/1000 | Loss: 0.00002385
Iteration 174/1000 | Loss: 0.00002385
Iteration 175/1000 | Loss: 0.00002385
Iteration 176/1000 | Loss: 0.00002385
Iteration 177/1000 | Loss: 0.00002385
Iteration 178/1000 | Loss: 0.00002385
Iteration 179/1000 | Loss: 0.00002385
Iteration 180/1000 | Loss: 0.00002385
Iteration 181/1000 | Loss: 0.00002384
Iteration 182/1000 | Loss: 0.00002384
Iteration 183/1000 | Loss: 0.00002384
Iteration 184/1000 | Loss: 0.00002384
Iteration 185/1000 | Loss: 0.00002384
Iteration 186/1000 | Loss: 0.00002384
Iteration 187/1000 | Loss: 0.00002384
Iteration 188/1000 | Loss: 0.00002384
Iteration 189/1000 | Loss: 0.00002384
Iteration 190/1000 | Loss: 0.00002384
Iteration 191/1000 | Loss: 0.00002384
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002384
Iteration 194/1000 | Loss: 0.00002384
Iteration 195/1000 | Loss: 0.00002384
Iteration 196/1000 | Loss: 0.00002383
Iteration 197/1000 | Loss: 0.00002383
Iteration 198/1000 | Loss: 0.00002383
Iteration 199/1000 | Loss: 0.00002383
Iteration 200/1000 | Loss: 0.00002383
Iteration 201/1000 | Loss: 0.00002383
Iteration 202/1000 | Loss: 0.00002383
Iteration 203/1000 | Loss: 0.00002383
Iteration 204/1000 | Loss: 0.00002383
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002383
Iteration 208/1000 | Loss: 0.00002382
Iteration 209/1000 | Loss: 0.00002382
Iteration 210/1000 | Loss: 0.00002382
Iteration 211/1000 | Loss: 0.00002382
Iteration 212/1000 | Loss: 0.00002382
Iteration 213/1000 | Loss: 0.00002382
Iteration 214/1000 | Loss: 0.00002382
Iteration 215/1000 | Loss: 0.00002382
Iteration 216/1000 | Loss: 0.00002382
Iteration 217/1000 | Loss: 0.00002382
Iteration 218/1000 | Loss: 0.00002382
Iteration 219/1000 | Loss: 0.00002382
Iteration 220/1000 | Loss: 0.00002381
Iteration 221/1000 | Loss: 0.00002381
Iteration 222/1000 | Loss: 0.00002381
Iteration 223/1000 | Loss: 0.00002381
Iteration 224/1000 | Loss: 0.00002381
Iteration 225/1000 | Loss: 0.00002381
Iteration 226/1000 | Loss: 0.00002381
Iteration 227/1000 | Loss: 0.00002381
Iteration 228/1000 | Loss: 0.00002381
Iteration 229/1000 | Loss: 0.00002381
Iteration 230/1000 | Loss: 0.00002381
Iteration 231/1000 | Loss: 0.00002380
Iteration 232/1000 | Loss: 0.00002380
Iteration 233/1000 | Loss: 0.00002380
Iteration 234/1000 | Loss: 0.00002380
Iteration 235/1000 | Loss: 0.00002380
Iteration 236/1000 | Loss: 0.00002380
Iteration 237/1000 | Loss: 0.00002380
Iteration 238/1000 | Loss: 0.00002380
Iteration 239/1000 | Loss: 0.00002380
Iteration 240/1000 | Loss: 0.00002380
Iteration 241/1000 | Loss: 0.00002380
Iteration 242/1000 | Loss: 0.00002380
Iteration 243/1000 | Loss: 0.00002380
Iteration 244/1000 | Loss: 0.00002380
Iteration 245/1000 | Loss: 0.00002380
Iteration 246/1000 | Loss: 0.00002380
Iteration 247/1000 | Loss: 0.00002380
Iteration 248/1000 | Loss: 0.00002380
Iteration 249/1000 | Loss: 0.00002380
Iteration 250/1000 | Loss: 0.00002380
Iteration 251/1000 | Loss: 0.00002380
Iteration 252/1000 | Loss: 0.00002380
Iteration 253/1000 | Loss: 0.00002380
Iteration 254/1000 | Loss: 0.00002380
Iteration 255/1000 | Loss: 0.00002380
Iteration 256/1000 | Loss: 0.00002380
Iteration 257/1000 | Loss: 0.00002380
Iteration 258/1000 | Loss: 0.00002380
Iteration 259/1000 | Loss: 0.00002380
Iteration 260/1000 | Loss: 0.00002380
Iteration 261/1000 | Loss: 0.00002380
Iteration 262/1000 | Loss: 0.00002380
Iteration 263/1000 | Loss: 0.00002380
Iteration 264/1000 | Loss: 0.00002380
Iteration 265/1000 | Loss: 0.00002380
Iteration 266/1000 | Loss: 0.00002380
Iteration 267/1000 | Loss: 0.00002380
Iteration 268/1000 | Loss: 0.00002380
Iteration 269/1000 | Loss: 0.00002380
Iteration 270/1000 | Loss: 0.00002380
Iteration 271/1000 | Loss: 0.00002380
Iteration 272/1000 | Loss: 0.00002380
Iteration 273/1000 | Loss: 0.00002380
Iteration 274/1000 | Loss: 0.00002380
Iteration 275/1000 | Loss: 0.00002380
Iteration 276/1000 | Loss: 0.00002380
Iteration 277/1000 | Loss: 0.00002380
Iteration 278/1000 | Loss: 0.00002380
Iteration 279/1000 | Loss: 0.00002380
Iteration 280/1000 | Loss: 0.00002380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [2.3798764232196845e-05, 2.3798764232196845e-05, 2.3798764232196845e-05, 2.3798764232196845e-05, 2.3798764232196845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3798764232196845e-05

Optimization complete. Final v2v error: 4.0305047035217285 mm

Highest mean error: 5.128173351287842 mm for frame 212

Lowest mean error: 3.352452278137207 mm for frame 24

Saving results

Total time: 162.40610671043396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990835
Iteration 2/25 | Loss: 0.00186994
Iteration 3/25 | Loss: 0.00164864
Iteration 4/25 | Loss: 0.00161837
Iteration 5/25 | Loss: 0.00168934
Iteration 6/25 | Loss: 0.00159462
Iteration 7/25 | Loss: 0.00152318
Iteration 8/25 | Loss: 0.00159223
Iteration 9/25 | Loss: 0.00149591
Iteration 10/25 | Loss: 0.00147423
Iteration 11/25 | Loss: 0.00146918
Iteration 12/25 | Loss: 0.00140002
Iteration 13/25 | Loss: 0.00138405
Iteration 14/25 | Loss: 0.00137843
Iteration 15/25 | Loss: 0.00137401
Iteration 16/25 | Loss: 0.00137002
Iteration 17/25 | Loss: 0.00137456
Iteration 18/25 | Loss: 0.00137938
Iteration 19/25 | Loss: 0.00136860
Iteration 20/25 | Loss: 0.00136752
Iteration 21/25 | Loss: 0.00136086
Iteration 22/25 | Loss: 0.00136599
Iteration 23/25 | Loss: 0.00136152
Iteration 24/25 | Loss: 0.00136309
Iteration 25/25 | Loss: 0.00136456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51764131
Iteration 2/25 | Loss: 0.00120376
Iteration 3/25 | Loss: 0.00120376
Iteration 4/25 | Loss: 0.00120376
Iteration 5/25 | Loss: 0.00120376
Iteration 6/25 | Loss: 0.00120375
Iteration 7/25 | Loss: 0.00120375
Iteration 8/25 | Loss: 0.00120375
Iteration 9/25 | Loss: 0.00120375
Iteration 10/25 | Loss: 0.00120375
Iteration 11/25 | Loss: 0.00120375
Iteration 12/25 | Loss: 0.00120375
Iteration 13/25 | Loss: 0.00120375
Iteration 14/25 | Loss: 0.00120375
Iteration 15/25 | Loss: 0.00120375
Iteration 16/25 | Loss: 0.00120375
Iteration 17/25 | Loss: 0.00120375
Iteration 18/25 | Loss: 0.00120375
Iteration 19/25 | Loss: 0.00120375
Iteration 20/25 | Loss: 0.00120375
Iteration 21/25 | Loss: 0.00120375
Iteration 22/25 | Loss: 0.00120375
Iteration 23/25 | Loss: 0.00120375
Iteration 24/25 | Loss: 0.00120375
Iteration 25/25 | Loss: 0.00120375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120375
Iteration 2/1000 | Loss: 0.00062159
Iteration 3/1000 | Loss: 0.00028833
Iteration 4/1000 | Loss: 0.00007163
Iteration 5/1000 | Loss: 0.00042370
Iteration 6/1000 | Loss: 0.00051931
Iteration 7/1000 | Loss: 0.00024769
Iteration 8/1000 | Loss: 0.00008422
Iteration 9/1000 | Loss: 0.00021186
Iteration 10/1000 | Loss: 0.00022346
Iteration 11/1000 | Loss: 0.00019925
Iteration 12/1000 | Loss: 0.00035912
Iteration 13/1000 | Loss: 0.00027872
Iteration 14/1000 | Loss: 0.00019241
Iteration 15/1000 | Loss: 0.00008264
Iteration 16/1000 | Loss: 0.00012638
Iteration 17/1000 | Loss: 0.00007113
Iteration 18/1000 | Loss: 0.00006831
Iteration 19/1000 | Loss: 0.00026902
Iteration 20/1000 | Loss: 0.00012587
Iteration 21/1000 | Loss: 0.00007944
Iteration 22/1000 | Loss: 0.00009722
Iteration 23/1000 | Loss: 0.00028377
Iteration 24/1000 | Loss: 0.00019895
Iteration 25/1000 | Loss: 0.00029185
Iteration 26/1000 | Loss: 0.00014678
Iteration 27/1000 | Loss: 0.00016329
Iteration 28/1000 | Loss: 0.00014669
Iteration 29/1000 | Loss: 0.00005461
Iteration 30/1000 | Loss: 0.00003998
Iteration 31/1000 | Loss: 0.00013882
Iteration 32/1000 | Loss: 0.00023238
Iteration 33/1000 | Loss: 0.00024875
Iteration 34/1000 | Loss: 0.00029907
Iteration 35/1000 | Loss: 0.00020253
Iteration 36/1000 | Loss: 0.00025475
Iteration 37/1000 | Loss: 0.00037827
Iteration 38/1000 | Loss: 0.00026171
Iteration 39/1000 | Loss: 0.00035695
Iteration 40/1000 | Loss: 0.00031900
Iteration 41/1000 | Loss: 0.00025784
Iteration 42/1000 | Loss: 0.00019455
Iteration 43/1000 | Loss: 0.00020816
Iteration 44/1000 | Loss: 0.00027862
Iteration 45/1000 | Loss: 0.00042215
Iteration 46/1000 | Loss: 0.00038181
Iteration 47/1000 | Loss: 0.00025399
Iteration 48/1000 | Loss: 0.00023504
Iteration 49/1000 | Loss: 0.00029617
Iteration 50/1000 | Loss: 0.00022221
Iteration 51/1000 | Loss: 0.00006665
Iteration 52/1000 | Loss: 0.00004553
Iteration 53/1000 | Loss: 0.00020311
Iteration 54/1000 | Loss: 0.00018531
Iteration 55/1000 | Loss: 0.00016029
Iteration 56/1000 | Loss: 0.00005283
Iteration 57/1000 | Loss: 0.00012067
Iteration 58/1000 | Loss: 0.00013678
Iteration 59/1000 | Loss: 0.00015493
Iteration 60/1000 | Loss: 0.00012892
Iteration 61/1000 | Loss: 0.00022034
Iteration 62/1000 | Loss: 0.00018606
Iteration 63/1000 | Loss: 0.00026796
Iteration 64/1000 | Loss: 0.00014845
Iteration 65/1000 | Loss: 0.00012356
Iteration 66/1000 | Loss: 0.00019867
Iteration 67/1000 | Loss: 0.00079414
Iteration 68/1000 | Loss: 0.00047643
Iteration 69/1000 | Loss: 0.00012296
Iteration 70/1000 | Loss: 0.00046391
Iteration 71/1000 | Loss: 0.00083073
Iteration 72/1000 | Loss: 0.00043835
Iteration 73/1000 | Loss: 0.00059524
Iteration 74/1000 | Loss: 0.00038663
Iteration 75/1000 | Loss: 0.00045535
Iteration 76/1000 | Loss: 0.00007434
Iteration 77/1000 | Loss: 0.00004197
Iteration 78/1000 | Loss: 0.00003443
Iteration 79/1000 | Loss: 0.00018635
Iteration 80/1000 | Loss: 0.00003126
Iteration 81/1000 | Loss: 0.00004667
Iteration 82/1000 | Loss: 0.00003613
Iteration 83/1000 | Loss: 0.00004540
Iteration 84/1000 | Loss: 0.00053167
Iteration 85/1000 | Loss: 0.00007299
Iteration 86/1000 | Loss: 0.00006839
Iteration 87/1000 | Loss: 0.00030593
Iteration 88/1000 | Loss: 0.00010561
Iteration 89/1000 | Loss: 0.00004131
Iteration 90/1000 | Loss: 0.00005867
Iteration 91/1000 | Loss: 0.00009902
Iteration 92/1000 | Loss: 0.00006170
Iteration 93/1000 | Loss: 0.00003780
Iteration 94/1000 | Loss: 0.00004714
Iteration 95/1000 | Loss: 0.00004491
Iteration 96/1000 | Loss: 0.00002690
Iteration 97/1000 | Loss: 0.00005180
Iteration 98/1000 | Loss: 0.00004117
Iteration 99/1000 | Loss: 0.00003264
Iteration 100/1000 | Loss: 0.00002344
Iteration 101/1000 | Loss: 0.00002338
Iteration 102/1000 | Loss: 0.00002313
Iteration 103/1000 | Loss: 0.00002233
Iteration 104/1000 | Loss: 0.00002131
Iteration 105/1000 | Loss: 0.00002068
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001962
Iteration 109/1000 | Loss: 0.00001949
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001929
Iteration 112/1000 | Loss: 0.00001921
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001915
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001897
Iteration 117/1000 | Loss: 0.00001887
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001885
Iteration 121/1000 | Loss: 0.00001885
Iteration 122/1000 | Loss: 0.00001885
Iteration 123/1000 | Loss: 0.00001884
Iteration 124/1000 | Loss: 0.00001884
Iteration 125/1000 | Loss: 0.00001884
Iteration 126/1000 | Loss: 0.00001884
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001883
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001881
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001881
Iteration 144/1000 | Loss: 0.00001881
Iteration 145/1000 | Loss: 0.00001881
Iteration 146/1000 | Loss: 0.00001881
Iteration 147/1000 | Loss: 0.00001881
Iteration 148/1000 | Loss: 0.00001881
Iteration 149/1000 | Loss: 0.00001881
Iteration 150/1000 | Loss: 0.00001881
Iteration 151/1000 | Loss: 0.00001881
Iteration 152/1000 | Loss: 0.00001881
Iteration 153/1000 | Loss: 0.00001881
Iteration 154/1000 | Loss: 0.00001881
Iteration 155/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.8809230823535472e-05, 1.8809230823535472e-05, 1.8809230823535472e-05, 1.8809230823535472e-05, 1.8809230823535472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8809230823535472e-05

Optimization complete. Final v2v error: 3.482515811920166 mm

Highest mean error: 10.404287338256836 mm for frame 72

Lowest mean error: 3.02117657661438 mm for frame 40

Saving results

Total time: 200.28750443458557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806131
Iteration 2/25 | Loss: 0.00228031
Iteration 3/25 | Loss: 0.00193199
Iteration 4/25 | Loss: 0.00172601
Iteration 5/25 | Loss: 0.00150818
Iteration 6/25 | Loss: 0.00147809
Iteration 7/25 | Loss: 0.00147667
Iteration 8/25 | Loss: 0.00147654
Iteration 9/25 | Loss: 0.00147654
Iteration 10/25 | Loss: 0.00147654
Iteration 11/25 | Loss: 0.00147654
Iteration 12/25 | Loss: 0.00147654
Iteration 13/25 | Loss: 0.00147654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014765417436137795, 0.0014765417436137795, 0.0014765417436137795, 0.0014765417436137795, 0.0014765417436137795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014765417436137795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39375663
Iteration 2/25 | Loss: 0.00091980
Iteration 3/25 | Loss: 0.00091978
Iteration 4/25 | Loss: 0.00091978
Iteration 5/25 | Loss: 0.00091978
Iteration 6/25 | Loss: 0.00091978
Iteration 7/25 | Loss: 0.00091978
Iteration 8/25 | Loss: 0.00091978
Iteration 9/25 | Loss: 0.00091978
Iteration 10/25 | Loss: 0.00091978
Iteration 11/25 | Loss: 0.00091978
Iteration 12/25 | Loss: 0.00091978
Iteration 13/25 | Loss: 0.00091978
Iteration 14/25 | Loss: 0.00091978
Iteration 15/25 | Loss: 0.00091978
Iteration 16/25 | Loss: 0.00091978
Iteration 17/25 | Loss: 0.00091978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009197793551720679, 0.0009197793551720679, 0.0009197793551720679, 0.0009197793551720679, 0.0009197793551720679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009197793551720679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091978
Iteration 2/1000 | Loss: 0.00004593
Iteration 3/1000 | Loss: 0.00003256
Iteration 4/1000 | Loss: 0.00002935
Iteration 5/1000 | Loss: 0.00002800
Iteration 6/1000 | Loss: 0.00004099
Iteration 7/1000 | Loss: 0.00004228
Iteration 8/1000 | Loss: 0.00002956
Iteration 9/1000 | Loss: 0.00003010
Iteration 10/1000 | Loss: 0.00002826
Iteration 11/1000 | Loss: 0.00002640
Iteration 12/1000 | Loss: 0.00002604
Iteration 13/1000 | Loss: 0.00002570
Iteration 14/1000 | Loss: 0.00004011
Iteration 15/1000 | Loss: 0.00004395
Iteration 16/1000 | Loss: 0.00004103
Iteration 17/1000 | Loss: 0.00004945
Iteration 18/1000 | Loss: 0.00005034
Iteration 19/1000 | Loss: 0.00004773
Iteration 20/1000 | Loss: 0.00002973
Iteration 21/1000 | Loss: 0.00002748
Iteration 22/1000 | Loss: 0.00003360
Iteration 23/1000 | Loss: 0.00002629
Iteration 24/1000 | Loss: 0.00002608
Iteration 25/1000 | Loss: 0.00002702
Iteration 26/1000 | Loss: 0.00002496
Iteration 27/1000 | Loss: 0.00002457
Iteration 28/1000 | Loss: 0.00002414
Iteration 29/1000 | Loss: 0.00002383
Iteration 30/1000 | Loss: 0.00002358
Iteration 31/1000 | Loss: 0.00002336
Iteration 32/1000 | Loss: 0.00002335
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00002331
Iteration 35/1000 | Loss: 0.00002312
Iteration 36/1000 | Loss: 0.00002300
Iteration 37/1000 | Loss: 0.00002300
Iteration 38/1000 | Loss: 0.00002299
Iteration 39/1000 | Loss: 0.00002285
Iteration 40/1000 | Loss: 0.00002280
Iteration 41/1000 | Loss: 0.00002277
Iteration 42/1000 | Loss: 0.00002277
Iteration 43/1000 | Loss: 0.00002277
Iteration 44/1000 | Loss: 0.00002276
Iteration 45/1000 | Loss: 0.00002276
Iteration 46/1000 | Loss: 0.00002275
Iteration 47/1000 | Loss: 0.00002275
Iteration 48/1000 | Loss: 0.00002274
Iteration 49/1000 | Loss: 0.00002274
Iteration 50/1000 | Loss: 0.00002272
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002270
Iteration 54/1000 | Loss: 0.00002270
Iteration 55/1000 | Loss: 0.00002270
Iteration 56/1000 | Loss: 0.00002267
Iteration 57/1000 | Loss: 0.00002267
Iteration 58/1000 | Loss: 0.00002267
Iteration 59/1000 | Loss: 0.00002267
Iteration 60/1000 | Loss: 0.00002267
Iteration 61/1000 | Loss: 0.00002267
Iteration 62/1000 | Loss: 0.00002267
Iteration 63/1000 | Loss: 0.00002267
Iteration 64/1000 | Loss: 0.00002267
Iteration 65/1000 | Loss: 0.00002267
Iteration 66/1000 | Loss: 0.00002267
Iteration 67/1000 | Loss: 0.00002267
Iteration 68/1000 | Loss: 0.00002267
Iteration 69/1000 | Loss: 0.00002267
Iteration 70/1000 | Loss: 0.00002267
Iteration 71/1000 | Loss: 0.00002267
Iteration 72/1000 | Loss: 0.00002266
Iteration 73/1000 | Loss: 0.00002266
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002265
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002264
Iteration 78/1000 | Loss: 0.00002264
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002263
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002263
Iteration 83/1000 | Loss: 0.00002263
Iteration 84/1000 | Loss: 0.00002263
Iteration 85/1000 | Loss: 0.00002263
Iteration 86/1000 | Loss: 0.00002262
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002262
Iteration 93/1000 | Loss: 0.00002261
Iteration 94/1000 | Loss: 0.00002261
Iteration 95/1000 | Loss: 0.00002261
Iteration 96/1000 | Loss: 0.00002261
Iteration 97/1000 | Loss: 0.00002261
Iteration 98/1000 | Loss: 0.00002260
Iteration 99/1000 | Loss: 0.00002260
Iteration 100/1000 | Loss: 0.00002260
Iteration 101/1000 | Loss: 0.00002260
Iteration 102/1000 | Loss: 0.00002260
Iteration 103/1000 | Loss: 0.00002260
Iteration 104/1000 | Loss: 0.00002260
Iteration 105/1000 | Loss: 0.00002260
Iteration 106/1000 | Loss: 0.00002259
Iteration 107/1000 | Loss: 0.00002259
Iteration 108/1000 | Loss: 0.00002259
Iteration 109/1000 | Loss: 0.00002259
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002259
Iteration 112/1000 | Loss: 0.00002259
Iteration 113/1000 | Loss: 0.00002259
Iteration 114/1000 | Loss: 0.00002259
Iteration 115/1000 | Loss: 0.00002259
Iteration 116/1000 | Loss: 0.00002259
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002258
Iteration 120/1000 | Loss: 0.00002258
Iteration 121/1000 | Loss: 0.00002258
Iteration 122/1000 | Loss: 0.00002258
Iteration 123/1000 | Loss: 0.00002258
Iteration 124/1000 | Loss: 0.00002258
Iteration 125/1000 | Loss: 0.00002258
Iteration 126/1000 | Loss: 0.00002258
Iteration 127/1000 | Loss: 0.00002258
Iteration 128/1000 | Loss: 0.00002258
Iteration 129/1000 | Loss: 0.00002258
Iteration 130/1000 | Loss: 0.00002258
Iteration 131/1000 | Loss: 0.00002258
Iteration 132/1000 | Loss: 0.00002258
Iteration 133/1000 | Loss: 0.00002258
Iteration 134/1000 | Loss: 0.00002258
Iteration 135/1000 | Loss: 0.00002258
Iteration 136/1000 | Loss: 0.00002258
Iteration 137/1000 | Loss: 0.00002258
Iteration 138/1000 | Loss: 0.00002257
Iteration 139/1000 | Loss: 0.00002257
Iteration 140/1000 | Loss: 0.00002257
Iteration 141/1000 | Loss: 0.00002257
Iteration 142/1000 | Loss: 0.00002257
Iteration 143/1000 | Loss: 0.00002257
Iteration 144/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.257485903101042e-05, 2.257485903101042e-05, 2.257485903101042e-05, 2.257485903101042e-05, 2.257485903101042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.257485903101042e-05

Optimization complete. Final v2v error: 4.023040771484375 mm

Highest mean error: 4.843051910400391 mm for frame 87

Lowest mean error: 3.8436052799224854 mm for frame 10

Saving results

Total time: 75.5251932144165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771857
Iteration 2/25 | Loss: 0.00190540
Iteration 3/25 | Loss: 0.00144076
Iteration 4/25 | Loss: 0.00135214
Iteration 5/25 | Loss: 0.00131994
Iteration 6/25 | Loss: 0.00130914
Iteration 7/25 | Loss: 0.00129405
Iteration 8/25 | Loss: 0.00128805
Iteration 9/25 | Loss: 0.00128072
Iteration 10/25 | Loss: 0.00127763
Iteration 11/25 | Loss: 0.00128074
Iteration 12/25 | Loss: 0.00127221
Iteration 13/25 | Loss: 0.00127079
Iteration 14/25 | Loss: 0.00127031
Iteration 15/25 | Loss: 0.00127025
Iteration 16/25 | Loss: 0.00127024
Iteration 17/25 | Loss: 0.00127024
Iteration 18/25 | Loss: 0.00127024
Iteration 19/25 | Loss: 0.00127024
Iteration 20/25 | Loss: 0.00127024
Iteration 21/25 | Loss: 0.00127024
Iteration 22/25 | Loss: 0.00127024
Iteration 23/25 | Loss: 0.00127023
Iteration 24/25 | Loss: 0.00127023
Iteration 25/25 | Loss: 0.00127023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92350805
Iteration 2/25 | Loss: 0.00100568
Iteration 3/25 | Loss: 0.00090649
Iteration 4/25 | Loss: 0.00090649
Iteration 5/25 | Loss: 0.00090649
Iteration 6/25 | Loss: 0.00090649
Iteration 7/25 | Loss: 0.00090649
Iteration 8/25 | Loss: 0.00090649
Iteration 9/25 | Loss: 0.00090649
Iteration 10/25 | Loss: 0.00090649
Iteration 11/25 | Loss: 0.00090649
Iteration 12/25 | Loss: 0.00090649
Iteration 13/25 | Loss: 0.00090649
Iteration 14/25 | Loss: 0.00090649
Iteration 15/25 | Loss: 0.00090649
Iteration 16/25 | Loss: 0.00090649
Iteration 17/25 | Loss: 0.00090649
Iteration 18/25 | Loss: 0.00090649
Iteration 19/25 | Loss: 0.00090649
Iteration 20/25 | Loss: 0.00090649
Iteration 21/25 | Loss: 0.00090649
Iteration 22/25 | Loss: 0.00090649
Iteration 23/25 | Loss: 0.00090649
Iteration 24/25 | Loss: 0.00090649
Iteration 25/25 | Loss: 0.00090649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090649
Iteration 2/1000 | Loss: 0.00010535
Iteration 3/1000 | Loss: 0.00004160
Iteration 4/1000 | Loss: 0.00001824
Iteration 5/1000 | Loss: 0.00001666
Iteration 6/1000 | Loss: 0.00001991
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001948
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00002381
Iteration 15/1000 | Loss: 0.00001559
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001318
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001306
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001299
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001294
Iteration 73/1000 | Loss: 0.00001294
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001293
Iteration 76/1000 | Loss: 0.00001292
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001288
Iteration 92/1000 | Loss: 0.00001288
Iteration 93/1000 | Loss: 0.00001288
Iteration 94/1000 | Loss: 0.00001288
Iteration 95/1000 | Loss: 0.00001288
Iteration 96/1000 | Loss: 0.00001288
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001288
Iteration 99/1000 | Loss: 0.00001288
Iteration 100/1000 | Loss: 0.00001288
Iteration 101/1000 | Loss: 0.00001288
Iteration 102/1000 | Loss: 0.00001288
Iteration 103/1000 | Loss: 0.00001288
Iteration 104/1000 | Loss: 0.00001288
Iteration 105/1000 | Loss: 0.00001288
Iteration 106/1000 | Loss: 0.00001288
Iteration 107/1000 | Loss: 0.00001288
Iteration 108/1000 | Loss: 0.00001288
Iteration 109/1000 | Loss: 0.00001288
Iteration 110/1000 | Loss: 0.00001288
Iteration 111/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.2875284483016003e-05, 1.2875284483016003e-05, 1.2875284483016003e-05, 1.2875284483016003e-05, 1.2875284483016003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2875284483016003e-05

Optimization complete. Final v2v error: 3.0809004306793213 mm

Highest mean error: 3.3381786346435547 mm for frame 56

Lowest mean error: 2.876241683959961 mm for frame 4

Saving results

Total time: 60.084108114242554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470816
Iteration 2/25 | Loss: 0.00151345
Iteration 3/25 | Loss: 0.00134205
Iteration 4/25 | Loss: 0.00132313
Iteration 5/25 | Loss: 0.00131806
Iteration 6/25 | Loss: 0.00131745
Iteration 7/25 | Loss: 0.00131745
Iteration 8/25 | Loss: 0.00131745
Iteration 9/25 | Loss: 0.00131745
Iteration 10/25 | Loss: 0.00131745
Iteration 11/25 | Loss: 0.00131745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013174472842365503, 0.0013174472842365503, 0.0013174472842365503, 0.0013174472842365503, 0.0013174472842365503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013174472842365503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39971244
Iteration 2/25 | Loss: 0.00104177
Iteration 3/25 | Loss: 0.00104176
Iteration 4/25 | Loss: 0.00104176
Iteration 5/25 | Loss: 0.00104176
Iteration 6/25 | Loss: 0.00104176
Iteration 7/25 | Loss: 0.00104176
Iteration 8/25 | Loss: 0.00104176
Iteration 9/25 | Loss: 0.00104176
Iteration 10/25 | Loss: 0.00104176
Iteration 11/25 | Loss: 0.00104176
Iteration 12/25 | Loss: 0.00104176
Iteration 13/25 | Loss: 0.00104176
Iteration 14/25 | Loss: 0.00104176
Iteration 15/25 | Loss: 0.00104176
Iteration 16/25 | Loss: 0.00104176
Iteration 17/25 | Loss: 0.00104176
Iteration 18/25 | Loss: 0.00104176
Iteration 19/25 | Loss: 0.00104176
Iteration 20/25 | Loss: 0.00104176
Iteration 21/25 | Loss: 0.00104176
Iteration 22/25 | Loss: 0.00104176
Iteration 23/25 | Loss: 0.00104176
Iteration 24/25 | Loss: 0.00104176
Iteration 25/25 | Loss: 0.00104176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104176
Iteration 2/1000 | Loss: 0.00004514
Iteration 3/1000 | Loss: 0.00002679
Iteration 4/1000 | Loss: 0.00002414
Iteration 5/1000 | Loss: 0.00002258
Iteration 6/1000 | Loss: 0.00002159
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002039
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001976
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001954
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001931
Iteration 16/1000 | Loss: 0.00001925
Iteration 17/1000 | Loss: 0.00001916
Iteration 18/1000 | Loss: 0.00001909
Iteration 19/1000 | Loss: 0.00001908
Iteration 20/1000 | Loss: 0.00001907
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001894
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001870
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001869
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001859
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001858
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.8577913579065353e-05, 1.8577913579065353e-05, 1.8577913579065353e-05, 1.8577913579065353e-05, 1.8577913579065353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8577913579065353e-05

Optimization complete. Final v2v error: 3.6080594062805176 mm

Highest mean error: 4.541041851043701 mm for frame 215

Lowest mean error: 2.8720030784606934 mm for frame 52

Saving results

Total time: 41.691306352615356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770281
Iteration 2/25 | Loss: 0.00160887
Iteration 3/25 | Loss: 0.00137593
Iteration 4/25 | Loss: 0.00134747
Iteration 5/25 | Loss: 0.00135306
Iteration 6/25 | Loss: 0.00134122
Iteration 7/25 | Loss: 0.00133694
Iteration 8/25 | Loss: 0.00133179
Iteration 9/25 | Loss: 0.00132975
Iteration 10/25 | Loss: 0.00133089
Iteration 11/25 | Loss: 0.00133142
Iteration 12/25 | Loss: 0.00133143
Iteration 13/25 | Loss: 0.00132951
Iteration 14/25 | Loss: 0.00132582
Iteration 15/25 | Loss: 0.00132404
Iteration 16/25 | Loss: 0.00132294
Iteration 17/25 | Loss: 0.00132273
Iteration 18/25 | Loss: 0.00132265
Iteration 19/25 | Loss: 0.00132265
Iteration 20/25 | Loss: 0.00132264
Iteration 21/25 | Loss: 0.00132264
Iteration 22/25 | Loss: 0.00132264
Iteration 23/25 | Loss: 0.00132264
Iteration 24/25 | Loss: 0.00132263
Iteration 25/25 | Loss: 0.00132263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92718685
Iteration 2/25 | Loss: 0.00099873
Iteration 3/25 | Loss: 0.00099872
Iteration 4/25 | Loss: 0.00099872
Iteration 5/25 | Loss: 0.00099872
Iteration 6/25 | Loss: 0.00099872
Iteration 7/25 | Loss: 0.00099872
Iteration 8/25 | Loss: 0.00099872
Iteration 9/25 | Loss: 0.00099872
Iteration 10/25 | Loss: 0.00099872
Iteration 11/25 | Loss: 0.00099872
Iteration 12/25 | Loss: 0.00099872
Iteration 13/25 | Loss: 0.00099872
Iteration 14/25 | Loss: 0.00099872
Iteration 15/25 | Loss: 0.00099872
Iteration 16/25 | Loss: 0.00099872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000998720875941217, 0.000998720875941217, 0.000998720875941217, 0.000998720875941217, 0.000998720875941217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000998720875941217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099872
Iteration 2/1000 | Loss: 0.00003156
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002225
Iteration 5/1000 | Loss: 0.00002125
Iteration 6/1000 | Loss: 0.00002041
Iteration 7/1000 | Loss: 0.00001980
Iteration 8/1000 | Loss: 0.00001972
Iteration 9/1000 | Loss: 0.00001940
Iteration 10/1000 | Loss: 0.00001894
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001851
Iteration 13/1000 | Loss: 0.00001845
Iteration 14/1000 | Loss: 0.00001824
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001816
Iteration 17/1000 | Loss: 0.00001813
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001778
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001778
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001770
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001768
Iteration 45/1000 | Loss: 0.00001768
Iteration 46/1000 | Loss: 0.00001768
Iteration 47/1000 | Loss: 0.00001768
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001765
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001751
Iteration 74/1000 | Loss: 0.00001751
Iteration 75/1000 | Loss: 0.00001750
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001749
Iteration 81/1000 | Loss: 0.00001749
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001744
Iteration 99/1000 | Loss: 0.00001744
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001743
Iteration 103/1000 | Loss: 0.00001743
Iteration 104/1000 | Loss: 0.00001743
Iteration 105/1000 | Loss: 0.00001743
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001743
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001739
Iteration 126/1000 | Loss: 0.00001739
Iteration 127/1000 | Loss: 0.00001739
Iteration 128/1000 | Loss: 0.00001739
Iteration 129/1000 | Loss: 0.00001739
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001737
Iteration 133/1000 | Loss: 0.00001737
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001735
Iteration 137/1000 | Loss: 0.00001735
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001743
Iteration 142/1000 | Loss: 0.00001724
Iteration 143/1000 | Loss: 0.00001722
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001709
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Iteration 149/1000 | Loss: 0.00001707
Iteration 150/1000 | Loss: 0.00001707
Iteration 151/1000 | Loss: 0.00001707
Iteration 152/1000 | Loss: 0.00001707
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001705
Iteration 166/1000 | Loss: 0.00001705
Iteration 167/1000 | Loss: 0.00001705
Iteration 168/1000 | Loss: 0.00001705
Iteration 169/1000 | Loss: 0.00001705
Iteration 170/1000 | Loss: 0.00001704
Iteration 171/1000 | Loss: 0.00001704
Iteration 172/1000 | Loss: 0.00001704
Iteration 173/1000 | Loss: 0.00001704
Iteration 174/1000 | Loss: 0.00001704
Iteration 175/1000 | Loss: 0.00001704
Iteration 176/1000 | Loss: 0.00001704
Iteration 177/1000 | Loss: 0.00001704
Iteration 178/1000 | Loss: 0.00001704
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001703
Iteration 182/1000 | Loss: 0.00001703
Iteration 183/1000 | Loss: 0.00001703
Iteration 184/1000 | Loss: 0.00001703
Iteration 185/1000 | Loss: 0.00001703
Iteration 186/1000 | Loss: 0.00001703
Iteration 187/1000 | Loss: 0.00001703
Iteration 188/1000 | Loss: 0.00001703
Iteration 189/1000 | Loss: 0.00001703
Iteration 190/1000 | Loss: 0.00001703
Iteration 191/1000 | Loss: 0.00001703
Iteration 192/1000 | Loss: 0.00001703
Iteration 193/1000 | Loss: 0.00001703
Iteration 194/1000 | Loss: 0.00001703
Iteration 195/1000 | Loss: 0.00001703
Iteration 196/1000 | Loss: 0.00001703
Iteration 197/1000 | Loss: 0.00001703
Iteration 198/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.7025140550686046e-05, 1.7025140550686046e-05, 1.7025140550686046e-05, 1.7025140550686046e-05, 1.7025140550686046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7025140550686046e-05

Optimization complete. Final v2v error: 3.4340670108795166 mm

Highest mean error: 4.525746822357178 mm for frame 38

Lowest mean error: 3.0592703819274902 mm for frame 187

Saving results

Total time: 79.90740513801575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845148
Iteration 2/25 | Loss: 0.00137420
Iteration 3/25 | Loss: 0.00129361
Iteration 4/25 | Loss: 0.00128421
Iteration 5/25 | Loss: 0.00128086
Iteration 6/25 | Loss: 0.00128023
Iteration 7/25 | Loss: 0.00128023
Iteration 8/25 | Loss: 0.00128023
Iteration 9/25 | Loss: 0.00128023
Iteration 10/25 | Loss: 0.00128023
Iteration 11/25 | Loss: 0.00128023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012802305864170194, 0.0012802305864170194, 0.0012802305864170194, 0.0012802305864170194, 0.0012802305864170194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012802305864170194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04464841
Iteration 2/25 | Loss: 0.00094628
Iteration 3/25 | Loss: 0.00094628
Iteration 4/25 | Loss: 0.00094628
Iteration 5/25 | Loss: 0.00094628
Iteration 6/25 | Loss: 0.00094628
Iteration 7/25 | Loss: 0.00094628
Iteration 8/25 | Loss: 0.00094628
Iteration 9/25 | Loss: 0.00094628
Iteration 10/25 | Loss: 0.00094628
Iteration 11/25 | Loss: 0.00094628
Iteration 12/25 | Loss: 0.00094628
Iteration 13/25 | Loss: 0.00094628
Iteration 14/25 | Loss: 0.00094628
Iteration 15/25 | Loss: 0.00094628
Iteration 16/25 | Loss: 0.00094628
Iteration 17/25 | Loss: 0.00094628
Iteration 18/25 | Loss: 0.00094628
Iteration 19/25 | Loss: 0.00094628
Iteration 20/25 | Loss: 0.00094628
Iteration 21/25 | Loss: 0.00094628
Iteration 22/25 | Loss: 0.00094628
Iteration 23/25 | Loss: 0.00094628
Iteration 24/25 | Loss: 0.00094628
Iteration 25/25 | Loss: 0.00094628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094628
Iteration 2/1000 | Loss: 0.00003121
Iteration 3/1000 | Loss: 0.00001968
Iteration 4/1000 | Loss: 0.00001667
Iteration 5/1000 | Loss: 0.00001571
Iteration 6/1000 | Loss: 0.00001490
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001395
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001322
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001306
Iteration 19/1000 | Loss: 0.00001305
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001281
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001273
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001270
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001263
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001262
Iteration 70/1000 | Loss: 0.00001262
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001261
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001260
Iteration 80/1000 | Loss: 0.00001260
Iteration 81/1000 | Loss: 0.00001260
Iteration 82/1000 | Loss: 0.00001260
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001260
Iteration 85/1000 | Loss: 0.00001260
Iteration 86/1000 | Loss: 0.00001259
Iteration 87/1000 | Loss: 0.00001259
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001255
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001251
Iteration 107/1000 | Loss: 0.00001251
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001249
Iteration 110/1000 | Loss: 0.00001249
Iteration 111/1000 | Loss: 0.00001249
Iteration 112/1000 | Loss: 0.00001249
Iteration 113/1000 | Loss: 0.00001248
Iteration 114/1000 | Loss: 0.00001248
Iteration 115/1000 | Loss: 0.00001248
Iteration 116/1000 | Loss: 0.00001248
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001245
Iteration 130/1000 | Loss: 0.00001245
Iteration 131/1000 | Loss: 0.00001245
Iteration 132/1000 | Loss: 0.00001245
Iteration 133/1000 | Loss: 0.00001245
Iteration 134/1000 | Loss: 0.00001245
Iteration 135/1000 | Loss: 0.00001245
Iteration 136/1000 | Loss: 0.00001245
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001244
Iteration 139/1000 | Loss: 0.00001244
Iteration 140/1000 | Loss: 0.00001244
Iteration 141/1000 | Loss: 0.00001244
Iteration 142/1000 | Loss: 0.00001243
Iteration 143/1000 | Loss: 0.00001243
Iteration 144/1000 | Loss: 0.00001243
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001243
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001243
Iteration 154/1000 | Loss: 0.00001243
Iteration 155/1000 | Loss: 0.00001243
Iteration 156/1000 | Loss: 0.00001243
Iteration 157/1000 | Loss: 0.00001243
Iteration 158/1000 | Loss: 0.00001242
Iteration 159/1000 | Loss: 0.00001242
Iteration 160/1000 | Loss: 0.00001242
Iteration 161/1000 | Loss: 0.00001242
Iteration 162/1000 | Loss: 0.00001242
Iteration 163/1000 | Loss: 0.00001242
Iteration 164/1000 | Loss: 0.00001242
Iteration 165/1000 | Loss: 0.00001242
Iteration 166/1000 | Loss: 0.00001242
Iteration 167/1000 | Loss: 0.00001242
Iteration 168/1000 | Loss: 0.00001242
Iteration 169/1000 | Loss: 0.00001242
Iteration 170/1000 | Loss: 0.00001242
Iteration 171/1000 | Loss: 0.00001242
Iteration 172/1000 | Loss: 0.00001242
Iteration 173/1000 | Loss: 0.00001241
Iteration 174/1000 | Loss: 0.00001241
Iteration 175/1000 | Loss: 0.00001241
Iteration 176/1000 | Loss: 0.00001241
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001241
Iteration 182/1000 | Loss: 0.00001241
Iteration 183/1000 | Loss: 0.00001240
Iteration 184/1000 | Loss: 0.00001240
Iteration 185/1000 | Loss: 0.00001240
Iteration 186/1000 | Loss: 0.00001240
Iteration 187/1000 | Loss: 0.00001240
Iteration 188/1000 | Loss: 0.00001240
Iteration 189/1000 | Loss: 0.00001240
Iteration 190/1000 | Loss: 0.00001240
Iteration 191/1000 | Loss: 0.00001240
Iteration 192/1000 | Loss: 0.00001240
Iteration 193/1000 | Loss: 0.00001240
Iteration 194/1000 | Loss: 0.00001240
Iteration 195/1000 | Loss: 0.00001240
Iteration 196/1000 | Loss: 0.00001240
Iteration 197/1000 | Loss: 0.00001240
Iteration 198/1000 | Loss: 0.00001239
Iteration 199/1000 | Loss: 0.00001239
Iteration 200/1000 | Loss: 0.00001239
Iteration 201/1000 | Loss: 0.00001239
Iteration 202/1000 | Loss: 0.00001239
Iteration 203/1000 | Loss: 0.00001239
Iteration 204/1000 | Loss: 0.00001239
Iteration 205/1000 | Loss: 0.00001239
Iteration 206/1000 | Loss: 0.00001239
Iteration 207/1000 | Loss: 0.00001239
Iteration 208/1000 | Loss: 0.00001239
Iteration 209/1000 | Loss: 0.00001239
Iteration 210/1000 | Loss: 0.00001239
Iteration 211/1000 | Loss: 0.00001239
Iteration 212/1000 | Loss: 0.00001239
Iteration 213/1000 | Loss: 0.00001239
Iteration 214/1000 | Loss: 0.00001239
Iteration 215/1000 | Loss: 0.00001239
Iteration 216/1000 | Loss: 0.00001239
Iteration 217/1000 | Loss: 0.00001238
Iteration 218/1000 | Loss: 0.00001238
Iteration 219/1000 | Loss: 0.00001238
Iteration 220/1000 | Loss: 0.00001238
Iteration 221/1000 | Loss: 0.00001238
Iteration 222/1000 | Loss: 0.00001238
Iteration 223/1000 | Loss: 0.00001238
Iteration 224/1000 | Loss: 0.00001238
Iteration 225/1000 | Loss: 0.00001238
Iteration 226/1000 | Loss: 0.00001238
Iteration 227/1000 | Loss: 0.00001238
Iteration 228/1000 | Loss: 0.00001238
Iteration 229/1000 | Loss: 0.00001238
Iteration 230/1000 | Loss: 0.00001238
Iteration 231/1000 | Loss: 0.00001237
Iteration 232/1000 | Loss: 0.00001237
Iteration 233/1000 | Loss: 0.00001237
Iteration 234/1000 | Loss: 0.00001237
Iteration 235/1000 | Loss: 0.00001237
Iteration 236/1000 | Loss: 0.00001237
Iteration 237/1000 | Loss: 0.00001237
Iteration 238/1000 | Loss: 0.00001237
Iteration 239/1000 | Loss: 0.00001237
Iteration 240/1000 | Loss: 0.00001237
Iteration 241/1000 | Loss: 0.00001237
Iteration 242/1000 | Loss: 0.00001237
Iteration 243/1000 | Loss: 0.00001237
Iteration 244/1000 | Loss: 0.00001237
Iteration 245/1000 | Loss: 0.00001237
Iteration 246/1000 | Loss: 0.00001237
Iteration 247/1000 | Loss: 0.00001237
Iteration 248/1000 | Loss: 0.00001237
Iteration 249/1000 | Loss: 0.00001237
Iteration 250/1000 | Loss: 0.00001237
Iteration 251/1000 | Loss: 0.00001237
Iteration 252/1000 | Loss: 0.00001237
Iteration 253/1000 | Loss: 0.00001237
Iteration 254/1000 | Loss: 0.00001237
Iteration 255/1000 | Loss: 0.00001237
Iteration 256/1000 | Loss: 0.00001237
Iteration 257/1000 | Loss: 0.00001237
Iteration 258/1000 | Loss: 0.00001237
Iteration 259/1000 | Loss: 0.00001237
Iteration 260/1000 | Loss: 0.00001237
Iteration 261/1000 | Loss: 0.00001237
Iteration 262/1000 | Loss: 0.00001237
Iteration 263/1000 | Loss: 0.00001237
Iteration 264/1000 | Loss: 0.00001237
Iteration 265/1000 | Loss: 0.00001237
Iteration 266/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.2370065633149352e-05, 1.2370065633149352e-05, 1.2370065633149352e-05, 1.2370065633149352e-05, 1.2370065633149352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2370065633149352e-05

Optimization complete. Final v2v error: 2.994980573654175 mm

Highest mean error: 3.5509705543518066 mm for frame 53

Lowest mean error: 2.7704226970672607 mm for frame 100

Saving results

Total time: 42.67813682556152
