Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=196, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10976-11031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807824
Iteration 2/25 | Loss: 0.00180911
Iteration 3/25 | Loss: 0.00130987
Iteration 4/25 | Loss: 0.00127261
Iteration 5/25 | Loss: 0.00127001
Iteration 6/25 | Loss: 0.00127001
Iteration 7/25 | Loss: 0.00127001
Iteration 8/25 | Loss: 0.00127001
Iteration 9/25 | Loss: 0.00127001
Iteration 10/25 | Loss: 0.00127001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012700115330517292, 0.0012700115330517292, 0.0012700115330517292, 0.0012700115330517292, 0.0012700115330517292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012700115330517292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34332192
Iteration 2/25 | Loss: 0.00078217
Iteration 3/25 | Loss: 0.00078217
Iteration 4/25 | Loss: 0.00078217
Iteration 5/25 | Loss: 0.00078217
Iteration 6/25 | Loss: 0.00078217
Iteration 7/25 | Loss: 0.00078217
Iteration 8/25 | Loss: 0.00078217
Iteration 9/25 | Loss: 0.00078217
Iteration 10/25 | Loss: 0.00078217
Iteration 11/25 | Loss: 0.00078217
Iteration 12/25 | Loss: 0.00078217
Iteration 13/25 | Loss: 0.00078217
Iteration 14/25 | Loss: 0.00078217
Iteration 15/25 | Loss: 0.00078217
Iteration 16/25 | Loss: 0.00078217
Iteration 17/25 | Loss: 0.00078217
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000782167015131563, 0.000782167015131563, 0.000782167015131563, 0.000782167015131563, 0.000782167015131563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000782167015131563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078217
Iteration 2/1000 | Loss: 0.00003225
Iteration 3/1000 | Loss: 0.00002468
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002213
Iteration 6/1000 | Loss: 0.00002154
Iteration 7/1000 | Loss: 0.00002108
Iteration 8/1000 | Loss: 0.00002077
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001955
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001945
Iteration 26/1000 | Loss: 0.00001944
Iteration 27/1000 | Loss: 0.00001944
Iteration 28/1000 | Loss: 0.00001944
Iteration 29/1000 | Loss: 0.00001943
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001936
Iteration 66/1000 | Loss: 0.00001936
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001935
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001933
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001932
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001930
Iteration 85/1000 | Loss: 0.00001930
Iteration 86/1000 | Loss: 0.00001930
Iteration 87/1000 | Loss: 0.00001930
Iteration 88/1000 | Loss: 0.00001930
Iteration 89/1000 | Loss: 0.00001930
Iteration 90/1000 | Loss: 0.00001930
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001929
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001928
Iteration 100/1000 | Loss: 0.00001928
Iteration 101/1000 | Loss: 0.00001928
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001928
Iteration 107/1000 | Loss: 0.00001928
Iteration 108/1000 | Loss: 0.00001928
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001928
Iteration 111/1000 | Loss: 0.00001928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.927731864270754e-05, 1.927731864270754e-05, 1.927731864270754e-05, 1.927731864270754e-05, 1.927731864270754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.927731864270754e-05

Optimization complete. Final v2v error: 3.658263921737671 mm

Highest mean error: 3.7249326705932617 mm for frame 75

Lowest mean error: 3.2741668224334717 mm for frame 3

Saving results

Total time: 41.1082649230957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712849
Iteration 2/25 | Loss: 0.00134512
Iteration 3/25 | Loss: 0.00125920
Iteration 4/25 | Loss: 0.00124758
Iteration 5/25 | Loss: 0.00124326
Iteration 6/25 | Loss: 0.00124245
Iteration 7/25 | Loss: 0.00124245
Iteration 8/25 | Loss: 0.00124245
Iteration 9/25 | Loss: 0.00124245
Iteration 10/25 | Loss: 0.00124245
Iteration 11/25 | Loss: 0.00124245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012424500891938806, 0.0012424500891938806, 0.0012424500891938806, 0.0012424500891938806, 0.0012424500891938806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012424500891938806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07993674
Iteration 2/25 | Loss: 0.00116873
Iteration 3/25 | Loss: 0.00116872
Iteration 4/25 | Loss: 0.00116872
Iteration 5/25 | Loss: 0.00116872
Iteration 6/25 | Loss: 0.00116872
Iteration 7/25 | Loss: 0.00116872
Iteration 8/25 | Loss: 0.00116872
Iteration 9/25 | Loss: 0.00116872
Iteration 10/25 | Loss: 0.00116872
Iteration 11/25 | Loss: 0.00116872
Iteration 12/25 | Loss: 0.00116872
Iteration 13/25 | Loss: 0.00116872
Iteration 14/25 | Loss: 0.00116872
Iteration 15/25 | Loss: 0.00116872
Iteration 16/25 | Loss: 0.00116872
Iteration 17/25 | Loss: 0.00116872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011687219375744462, 0.0011687219375744462, 0.0011687219375744462, 0.0011687219375744462, 0.0011687219375744462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011687219375744462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116872
Iteration 2/1000 | Loss: 0.00003536
Iteration 3/1000 | Loss: 0.00002555
Iteration 4/1000 | Loss: 0.00002126
Iteration 5/1000 | Loss: 0.00001964
Iteration 6/1000 | Loss: 0.00001859
Iteration 7/1000 | Loss: 0.00001807
Iteration 8/1000 | Loss: 0.00001760
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001703
Iteration 11/1000 | Loss: 0.00001674
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001646
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001622
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001617
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001615
Iteration 26/1000 | Loss: 0.00001614
Iteration 27/1000 | Loss: 0.00001614
Iteration 28/1000 | Loss: 0.00001613
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001612
Iteration 31/1000 | Loss: 0.00001612
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001611
Iteration 36/1000 | Loss: 0.00001611
Iteration 37/1000 | Loss: 0.00001610
Iteration 38/1000 | Loss: 0.00001610
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001608
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001603
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001597
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001595
Iteration 60/1000 | Loss: 0.00001595
Iteration 61/1000 | Loss: 0.00001595
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001593
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001593
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001592
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001591
Iteration 72/1000 | Loss: 0.00001591
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001590
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001584
Iteration 86/1000 | Loss: 0.00001584
Iteration 87/1000 | Loss: 0.00001584
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001583
Iteration 93/1000 | Loss: 0.00001583
Iteration 94/1000 | Loss: 0.00001583
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001582
Iteration 97/1000 | Loss: 0.00001582
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001580
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001577
Iteration 111/1000 | Loss: 0.00001577
Iteration 112/1000 | Loss: 0.00001577
Iteration 113/1000 | Loss: 0.00001577
Iteration 114/1000 | Loss: 0.00001577
Iteration 115/1000 | Loss: 0.00001577
Iteration 116/1000 | Loss: 0.00001577
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001576
Iteration 120/1000 | Loss: 0.00001576
Iteration 121/1000 | Loss: 0.00001576
Iteration 122/1000 | Loss: 0.00001575
Iteration 123/1000 | Loss: 0.00001575
Iteration 124/1000 | Loss: 0.00001575
Iteration 125/1000 | Loss: 0.00001574
Iteration 126/1000 | Loss: 0.00001574
Iteration 127/1000 | Loss: 0.00001574
Iteration 128/1000 | Loss: 0.00001574
Iteration 129/1000 | Loss: 0.00001574
Iteration 130/1000 | Loss: 0.00001574
Iteration 131/1000 | Loss: 0.00001573
Iteration 132/1000 | Loss: 0.00001573
Iteration 133/1000 | Loss: 0.00001573
Iteration 134/1000 | Loss: 0.00001573
Iteration 135/1000 | Loss: 0.00001573
Iteration 136/1000 | Loss: 0.00001573
Iteration 137/1000 | Loss: 0.00001573
Iteration 138/1000 | Loss: 0.00001573
Iteration 139/1000 | Loss: 0.00001573
Iteration 140/1000 | Loss: 0.00001573
Iteration 141/1000 | Loss: 0.00001573
Iteration 142/1000 | Loss: 0.00001573
Iteration 143/1000 | Loss: 0.00001573
Iteration 144/1000 | Loss: 0.00001573
Iteration 145/1000 | Loss: 0.00001572
Iteration 146/1000 | Loss: 0.00001572
Iteration 147/1000 | Loss: 0.00001572
Iteration 148/1000 | Loss: 0.00001572
Iteration 149/1000 | Loss: 0.00001572
Iteration 150/1000 | Loss: 0.00001571
Iteration 151/1000 | Loss: 0.00001571
Iteration 152/1000 | Loss: 0.00001571
Iteration 153/1000 | Loss: 0.00001571
Iteration 154/1000 | Loss: 0.00001571
Iteration 155/1000 | Loss: 0.00001571
Iteration 156/1000 | Loss: 0.00001571
Iteration 157/1000 | Loss: 0.00001571
Iteration 158/1000 | Loss: 0.00001571
Iteration 159/1000 | Loss: 0.00001571
Iteration 160/1000 | Loss: 0.00001571
Iteration 161/1000 | Loss: 0.00001571
Iteration 162/1000 | Loss: 0.00001571
Iteration 163/1000 | Loss: 0.00001571
Iteration 164/1000 | Loss: 0.00001570
Iteration 165/1000 | Loss: 0.00001570
Iteration 166/1000 | Loss: 0.00001570
Iteration 167/1000 | Loss: 0.00001570
Iteration 168/1000 | Loss: 0.00001569
Iteration 169/1000 | Loss: 0.00001569
Iteration 170/1000 | Loss: 0.00001569
Iteration 171/1000 | Loss: 0.00001569
Iteration 172/1000 | Loss: 0.00001569
Iteration 173/1000 | Loss: 0.00001569
Iteration 174/1000 | Loss: 0.00001569
Iteration 175/1000 | Loss: 0.00001569
Iteration 176/1000 | Loss: 0.00001569
Iteration 177/1000 | Loss: 0.00001569
Iteration 178/1000 | Loss: 0.00001569
Iteration 179/1000 | Loss: 0.00001569
Iteration 180/1000 | Loss: 0.00001569
Iteration 181/1000 | Loss: 0.00001569
Iteration 182/1000 | Loss: 0.00001569
Iteration 183/1000 | Loss: 0.00001569
Iteration 184/1000 | Loss: 0.00001568
Iteration 185/1000 | Loss: 0.00001568
Iteration 186/1000 | Loss: 0.00001568
Iteration 187/1000 | Loss: 0.00001568
Iteration 188/1000 | Loss: 0.00001568
Iteration 189/1000 | Loss: 0.00001568
Iteration 190/1000 | Loss: 0.00001568
Iteration 191/1000 | Loss: 0.00001568
Iteration 192/1000 | Loss: 0.00001568
Iteration 193/1000 | Loss: 0.00001568
Iteration 194/1000 | Loss: 0.00001568
Iteration 195/1000 | Loss: 0.00001568
Iteration 196/1000 | Loss: 0.00001568
Iteration 197/1000 | Loss: 0.00001568
Iteration 198/1000 | Loss: 0.00001568
Iteration 199/1000 | Loss: 0.00001568
Iteration 200/1000 | Loss: 0.00001568
Iteration 201/1000 | Loss: 0.00001568
Iteration 202/1000 | Loss: 0.00001568
Iteration 203/1000 | Loss: 0.00001567
Iteration 204/1000 | Loss: 0.00001567
Iteration 205/1000 | Loss: 0.00001567
Iteration 206/1000 | Loss: 0.00001567
Iteration 207/1000 | Loss: 0.00001567
Iteration 208/1000 | Loss: 0.00001567
Iteration 209/1000 | Loss: 0.00001567
Iteration 210/1000 | Loss: 0.00001567
Iteration 211/1000 | Loss: 0.00001567
Iteration 212/1000 | Loss: 0.00001567
Iteration 213/1000 | Loss: 0.00001567
Iteration 214/1000 | Loss: 0.00001567
Iteration 215/1000 | Loss: 0.00001567
Iteration 216/1000 | Loss: 0.00001567
Iteration 217/1000 | Loss: 0.00001567
Iteration 218/1000 | Loss: 0.00001567
Iteration 219/1000 | Loss: 0.00001567
Iteration 220/1000 | Loss: 0.00001567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.566898026794661e-05, 1.566898026794661e-05, 1.566898026794661e-05, 1.566898026794661e-05, 1.566898026794661e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.566898026794661e-05

Optimization complete. Final v2v error: 3.3079328536987305 mm

Highest mean error: 5.035476207733154 mm for frame 98

Lowest mean error: 2.8988614082336426 mm for frame 141

Saving results

Total time: 43.15859889984131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897002
Iteration 2/25 | Loss: 0.00160812
Iteration 3/25 | Loss: 0.00135424
Iteration 4/25 | Loss: 0.00132891
Iteration 5/25 | Loss: 0.00132078
Iteration 6/25 | Loss: 0.00131873
Iteration 7/25 | Loss: 0.00131870
Iteration 8/25 | Loss: 0.00131870
Iteration 9/25 | Loss: 0.00131870
Iteration 10/25 | Loss: 0.00131870
Iteration 11/25 | Loss: 0.00131870
Iteration 12/25 | Loss: 0.00131870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001318699331022799, 0.001318699331022799, 0.001318699331022799, 0.001318699331022799, 0.001318699331022799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001318699331022799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07022560
Iteration 2/25 | Loss: 0.00093846
Iteration 3/25 | Loss: 0.00093845
Iteration 4/25 | Loss: 0.00093845
Iteration 5/25 | Loss: 0.00093845
Iteration 6/25 | Loss: 0.00093844
Iteration 7/25 | Loss: 0.00093844
Iteration 8/25 | Loss: 0.00093844
Iteration 9/25 | Loss: 0.00093844
Iteration 10/25 | Loss: 0.00093844
Iteration 11/25 | Loss: 0.00093844
Iteration 12/25 | Loss: 0.00093844
Iteration 13/25 | Loss: 0.00093844
Iteration 14/25 | Loss: 0.00093844
Iteration 15/25 | Loss: 0.00093844
Iteration 16/25 | Loss: 0.00093844
Iteration 17/25 | Loss: 0.00093844
Iteration 18/25 | Loss: 0.00093844
Iteration 19/25 | Loss: 0.00093844
Iteration 20/25 | Loss: 0.00093844
Iteration 21/25 | Loss: 0.00093844
Iteration 22/25 | Loss: 0.00093844
Iteration 23/25 | Loss: 0.00093844
Iteration 24/25 | Loss: 0.00093844
Iteration 25/25 | Loss: 0.00093844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093844
Iteration 2/1000 | Loss: 0.00004862
Iteration 3/1000 | Loss: 0.00003683
Iteration 4/1000 | Loss: 0.00003077
Iteration 5/1000 | Loss: 0.00002921
Iteration 6/1000 | Loss: 0.00002781
Iteration 7/1000 | Loss: 0.00002707
Iteration 8/1000 | Loss: 0.00002624
Iteration 9/1000 | Loss: 0.00002575
Iteration 10/1000 | Loss: 0.00002543
Iteration 11/1000 | Loss: 0.00002514
Iteration 12/1000 | Loss: 0.00002493
Iteration 13/1000 | Loss: 0.00002470
Iteration 14/1000 | Loss: 0.00002450
Iteration 15/1000 | Loss: 0.00002429
Iteration 16/1000 | Loss: 0.00002417
Iteration 17/1000 | Loss: 0.00002399
Iteration 18/1000 | Loss: 0.00002384
Iteration 19/1000 | Loss: 0.00002376
Iteration 20/1000 | Loss: 0.00002375
Iteration 21/1000 | Loss: 0.00002372
Iteration 22/1000 | Loss: 0.00002371
Iteration 23/1000 | Loss: 0.00002371
Iteration 24/1000 | Loss: 0.00002368
Iteration 25/1000 | Loss: 0.00002365
Iteration 26/1000 | Loss: 0.00002361
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002360
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002358
Iteration 31/1000 | Loss: 0.00002356
Iteration 32/1000 | Loss: 0.00002352
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002350
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002350
Iteration 39/1000 | Loss: 0.00002350
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002348
Iteration 44/1000 | Loss: 0.00002347
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002346
Iteration 51/1000 | Loss: 0.00002346
Iteration 52/1000 | Loss: 0.00002345
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00002345
Iteration 55/1000 | Loss: 0.00002344
Iteration 56/1000 | Loss: 0.00002344
Iteration 57/1000 | Loss: 0.00002343
Iteration 58/1000 | Loss: 0.00002343
Iteration 59/1000 | Loss: 0.00002343
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002342
Iteration 62/1000 | Loss: 0.00002342
Iteration 63/1000 | Loss: 0.00002341
Iteration 64/1000 | Loss: 0.00002341
Iteration 65/1000 | Loss: 0.00002341
Iteration 66/1000 | Loss: 0.00002341
Iteration 67/1000 | Loss: 0.00002341
Iteration 68/1000 | Loss: 0.00002340
Iteration 69/1000 | Loss: 0.00002340
Iteration 70/1000 | Loss: 0.00002340
Iteration 71/1000 | Loss: 0.00002340
Iteration 72/1000 | Loss: 0.00002340
Iteration 73/1000 | Loss: 0.00002340
Iteration 74/1000 | Loss: 0.00002340
Iteration 75/1000 | Loss: 0.00002340
Iteration 76/1000 | Loss: 0.00002340
Iteration 77/1000 | Loss: 0.00002340
Iteration 78/1000 | Loss: 0.00002339
Iteration 79/1000 | Loss: 0.00002339
Iteration 80/1000 | Loss: 0.00002339
Iteration 81/1000 | Loss: 0.00002339
Iteration 82/1000 | Loss: 0.00002339
Iteration 83/1000 | Loss: 0.00002339
Iteration 84/1000 | Loss: 0.00002339
Iteration 85/1000 | Loss: 0.00002339
Iteration 86/1000 | Loss: 0.00002339
Iteration 87/1000 | Loss: 0.00002339
Iteration 88/1000 | Loss: 0.00002339
Iteration 89/1000 | Loss: 0.00002338
Iteration 90/1000 | Loss: 0.00002338
Iteration 91/1000 | Loss: 0.00002338
Iteration 92/1000 | Loss: 0.00002338
Iteration 93/1000 | Loss: 0.00002338
Iteration 94/1000 | Loss: 0.00002338
Iteration 95/1000 | Loss: 0.00002338
Iteration 96/1000 | Loss: 0.00002338
Iteration 97/1000 | Loss: 0.00002338
Iteration 98/1000 | Loss: 0.00002338
Iteration 99/1000 | Loss: 0.00002338
Iteration 100/1000 | Loss: 0.00002338
Iteration 101/1000 | Loss: 0.00002338
Iteration 102/1000 | Loss: 0.00002338
Iteration 103/1000 | Loss: 0.00002338
Iteration 104/1000 | Loss: 0.00002338
Iteration 105/1000 | Loss: 0.00002338
Iteration 106/1000 | Loss: 0.00002337
Iteration 107/1000 | Loss: 0.00002337
Iteration 108/1000 | Loss: 0.00002337
Iteration 109/1000 | Loss: 0.00002337
Iteration 110/1000 | Loss: 0.00002337
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002337
Iteration 116/1000 | Loss: 0.00002337
Iteration 117/1000 | Loss: 0.00002337
Iteration 118/1000 | Loss: 0.00002336
Iteration 119/1000 | Loss: 0.00002336
Iteration 120/1000 | Loss: 0.00002336
Iteration 121/1000 | Loss: 0.00002336
Iteration 122/1000 | Loss: 0.00002336
Iteration 123/1000 | Loss: 0.00002336
Iteration 124/1000 | Loss: 0.00002336
Iteration 125/1000 | Loss: 0.00002336
Iteration 126/1000 | Loss: 0.00002336
Iteration 127/1000 | Loss: 0.00002336
Iteration 128/1000 | Loss: 0.00002336
Iteration 129/1000 | Loss: 0.00002336
Iteration 130/1000 | Loss: 0.00002336
Iteration 131/1000 | Loss: 0.00002335
Iteration 132/1000 | Loss: 0.00002335
Iteration 133/1000 | Loss: 0.00002335
Iteration 134/1000 | Loss: 0.00002335
Iteration 135/1000 | Loss: 0.00002335
Iteration 136/1000 | Loss: 0.00002335
Iteration 137/1000 | Loss: 0.00002335
Iteration 138/1000 | Loss: 0.00002335
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002335
Iteration 141/1000 | Loss: 0.00002335
Iteration 142/1000 | Loss: 0.00002335
Iteration 143/1000 | Loss: 0.00002335
Iteration 144/1000 | Loss: 0.00002335
Iteration 145/1000 | Loss: 0.00002335
Iteration 146/1000 | Loss: 0.00002334
Iteration 147/1000 | Loss: 0.00002334
Iteration 148/1000 | Loss: 0.00002334
Iteration 149/1000 | Loss: 0.00002334
Iteration 150/1000 | Loss: 0.00002334
Iteration 151/1000 | Loss: 0.00002334
Iteration 152/1000 | Loss: 0.00002334
Iteration 153/1000 | Loss: 0.00002334
Iteration 154/1000 | Loss: 0.00002334
Iteration 155/1000 | Loss: 0.00002334
Iteration 156/1000 | Loss: 0.00002334
Iteration 157/1000 | Loss: 0.00002334
Iteration 158/1000 | Loss: 0.00002334
Iteration 159/1000 | Loss: 0.00002334
Iteration 160/1000 | Loss: 0.00002333
Iteration 161/1000 | Loss: 0.00002333
Iteration 162/1000 | Loss: 0.00002333
Iteration 163/1000 | Loss: 0.00002333
Iteration 164/1000 | Loss: 0.00002333
Iteration 165/1000 | Loss: 0.00002333
Iteration 166/1000 | Loss: 0.00002333
Iteration 167/1000 | Loss: 0.00002333
Iteration 168/1000 | Loss: 0.00002333
Iteration 169/1000 | Loss: 0.00002333
Iteration 170/1000 | Loss: 0.00002333
Iteration 171/1000 | Loss: 0.00002333
Iteration 172/1000 | Loss: 0.00002333
Iteration 173/1000 | Loss: 0.00002333
Iteration 174/1000 | Loss: 0.00002333
Iteration 175/1000 | Loss: 0.00002333
Iteration 176/1000 | Loss: 0.00002333
Iteration 177/1000 | Loss: 0.00002333
Iteration 178/1000 | Loss: 0.00002333
Iteration 179/1000 | Loss: 0.00002333
Iteration 180/1000 | Loss: 0.00002332
Iteration 181/1000 | Loss: 0.00002332
Iteration 182/1000 | Loss: 0.00002332
Iteration 183/1000 | Loss: 0.00002332
Iteration 184/1000 | Loss: 0.00002332
Iteration 185/1000 | Loss: 0.00002332
Iteration 186/1000 | Loss: 0.00002332
Iteration 187/1000 | Loss: 0.00002332
Iteration 188/1000 | Loss: 0.00002332
Iteration 189/1000 | Loss: 0.00002332
Iteration 190/1000 | Loss: 0.00002332
Iteration 191/1000 | Loss: 0.00002332
Iteration 192/1000 | Loss: 0.00002332
Iteration 193/1000 | Loss: 0.00002332
Iteration 194/1000 | Loss: 0.00002332
Iteration 195/1000 | Loss: 0.00002332
Iteration 196/1000 | Loss: 0.00002332
Iteration 197/1000 | Loss: 0.00002332
Iteration 198/1000 | Loss: 0.00002332
Iteration 199/1000 | Loss: 0.00002332
Iteration 200/1000 | Loss: 0.00002332
Iteration 201/1000 | Loss: 0.00002332
Iteration 202/1000 | Loss: 0.00002332
Iteration 203/1000 | Loss: 0.00002332
Iteration 204/1000 | Loss: 0.00002332
Iteration 205/1000 | Loss: 0.00002332
Iteration 206/1000 | Loss: 0.00002332
Iteration 207/1000 | Loss: 0.00002332
Iteration 208/1000 | Loss: 0.00002332
Iteration 209/1000 | Loss: 0.00002332
Iteration 210/1000 | Loss: 0.00002332
Iteration 211/1000 | Loss: 0.00002332
Iteration 212/1000 | Loss: 0.00002332
Iteration 213/1000 | Loss: 0.00002332
Iteration 214/1000 | Loss: 0.00002332
Iteration 215/1000 | Loss: 0.00002332
Iteration 216/1000 | Loss: 0.00002332
Iteration 217/1000 | Loss: 0.00002332
Iteration 218/1000 | Loss: 0.00002332
Iteration 219/1000 | Loss: 0.00002332
Iteration 220/1000 | Loss: 0.00002332
Iteration 221/1000 | Loss: 0.00002332
Iteration 222/1000 | Loss: 0.00002332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.332101394131314e-05, 2.332101394131314e-05, 2.332101394131314e-05, 2.332101394131314e-05, 2.332101394131314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.332101394131314e-05

Optimization complete. Final v2v error: 4.020218849182129 mm

Highest mean error: 5.303459167480469 mm for frame 103

Lowest mean error: 3.263518810272217 mm for frame 119

Saving results

Total time: 50.30458664894104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977237
Iteration 2/25 | Loss: 0.00977237
Iteration 3/25 | Loss: 0.00406296
Iteration 4/25 | Loss: 0.00344434
Iteration 5/25 | Loss: 0.00249572
Iteration 6/25 | Loss: 0.00233227
Iteration 7/25 | Loss: 0.00225471
Iteration 8/25 | Loss: 0.00221355
Iteration 9/25 | Loss: 0.00222242
Iteration 10/25 | Loss: 0.00200027
Iteration 11/25 | Loss: 0.00184407
Iteration 12/25 | Loss: 0.00176992
Iteration 13/25 | Loss: 0.00171774
Iteration 14/25 | Loss: 0.00172135
Iteration 15/25 | Loss: 0.00170391
Iteration 16/25 | Loss: 0.00168741
Iteration 17/25 | Loss: 0.00168366
Iteration 18/25 | Loss: 0.00165958
Iteration 19/25 | Loss: 0.00165649
Iteration 20/25 | Loss: 0.00165058
Iteration 21/25 | Loss: 0.00164507
Iteration 22/25 | Loss: 0.00163965
Iteration 23/25 | Loss: 0.00164202
Iteration 24/25 | Loss: 0.00162683
Iteration 25/25 | Loss: 0.00162594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31211138
Iteration 2/25 | Loss: 0.00373607
Iteration 3/25 | Loss: 0.00321410
Iteration 4/25 | Loss: 0.00321410
Iteration 5/25 | Loss: 0.00321410
Iteration 6/25 | Loss: 0.00321410
Iteration 7/25 | Loss: 0.00321410
Iteration 8/25 | Loss: 0.00321410
Iteration 9/25 | Loss: 0.00321410
Iteration 10/25 | Loss: 0.00321410
Iteration 11/25 | Loss: 0.00321410
Iteration 12/25 | Loss: 0.00321410
Iteration 13/25 | Loss: 0.00321410
Iteration 14/25 | Loss: 0.00321410
Iteration 15/25 | Loss: 0.00321410
Iteration 16/25 | Loss: 0.00321410
Iteration 17/25 | Loss: 0.00321410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0032140957191586494, 0.0032140957191586494, 0.0032140957191586494, 0.0032140957191586494, 0.0032140957191586494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032140957191586494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321410
Iteration 2/1000 | Loss: 0.00331430
Iteration 3/1000 | Loss: 0.00305303
Iteration 4/1000 | Loss: 0.00508833
Iteration 5/1000 | Loss: 0.00367476
Iteration 6/1000 | Loss: 0.00310712
Iteration 7/1000 | Loss: 0.00063847
Iteration 8/1000 | Loss: 0.00107882
Iteration 9/1000 | Loss: 0.00123265
Iteration 10/1000 | Loss: 0.00095756
Iteration 11/1000 | Loss: 0.00104199
Iteration 12/1000 | Loss: 0.00064155
Iteration 13/1000 | Loss: 0.00097650
Iteration 14/1000 | Loss: 0.00066642
Iteration 15/1000 | Loss: 0.00031291
Iteration 16/1000 | Loss: 0.00083111
Iteration 17/1000 | Loss: 0.00175489
Iteration 18/1000 | Loss: 0.00056028
Iteration 19/1000 | Loss: 0.00026230
Iteration 20/1000 | Loss: 0.00055958
Iteration 21/1000 | Loss: 0.00052526
Iteration 22/1000 | Loss: 0.00026235
Iteration 23/1000 | Loss: 0.00090916
Iteration 24/1000 | Loss: 0.00034496
Iteration 25/1000 | Loss: 0.00018608
Iteration 26/1000 | Loss: 0.00014205
Iteration 27/1000 | Loss: 0.00041588
Iteration 28/1000 | Loss: 0.00036935
Iteration 29/1000 | Loss: 0.00029348
Iteration 30/1000 | Loss: 0.00020701
Iteration 31/1000 | Loss: 0.00019687
Iteration 32/1000 | Loss: 0.00011270
Iteration 33/1000 | Loss: 0.00010402
Iteration 34/1000 | Loss: 0.00015326
Iteration 35/1000 | Loss: 0.00015495
Iteration 36/1000 | Loss: 0.00035629
Iteration 37/1000 | Loss: 0.00015303
Iteration 38/1000 | Loss: 0.00038770
Iteration 39/1000 | Loss: 0.00041895
Iteration 40/1000 | Loss: 0.00024886
Iteration 41/1000 | Loss: 0.00043305
Iteration 42/1000 | Loss: 0.00040038
Iteration 43/1000 | Loss: 0.00013338
Iteration 44/1000 | Loss: 0.00022094
Iteration 45/1000 | Loss: 0.00011340
Iteration 46/1000 | Loss: 0.00008653
Iteration 47/1000 | Loss: 0.00008441
Iteration 48/1000 | Loss: 0.00021188
Iteration 49/1000 | Loss: 0.00008254
Iteration 50/1000 | Loss: 0.00019872
Iteration 51/1000 | Loss: 0.00057667
Iteration 52/1000 | Loss: 0.00014281
Iteration 53/1000 | Loss: 0.00009150
Iteration 54/1000 | Loss: 0.00016869
Iteration 55/1000 | Loss: 0.00008105
Iteration 56/1000 | Loss: 0.00019089
Iteration 57/1000 | Loss: 0.00007913
Iteration 58/1000 | Loss: 0.00027143
Iteration 59/1000 | Loss: 0.00014815
Iteration 60/1000 | Loss: 0.00008585
Iteration 61/1000 | Loss: 0.00014465
Iteration 62/1000 | Loss: 0.00035636
Iteration 63/1000 | Loss: 0.00035451
Iteration 64/1000 | Loss: 0.00015064
Iteration 65/1000 | Loss: 0.00017405
Iteration 66/1000 | Loss: 0.00008153
Iteration 67/1000 | Loss: 0.00007762
Iteration 68/1000 | Loss: 0.00021079
Iteration 69/1000 | Loss: 0.00007489
Iteration 70/1000 | Loss: 0.00011996
Iteration 71/1000 | Loss: 0.00016779
Iteration 72/1000 | Loss: 0.00012054
Iteration 73/1000 | Loss: 0.00040274
Iteration 74/1000 | Loss: 0.00022405
Iteration 75/1000 | Loss: 0.00010097
Iteration 76/1000 | Loss: 0.00046941
Iteration 77/1000 | Loss: 0.00017770
Iteration 78/1000 | Loss: 0.00007889
Iteration 79/1000 | Loss: 0.00007131
Iteration 80/1000 | Loss: 0.00007132
Iteration 81/1000 | Loss: 0.00007313
Iteration 82/1000 | Loss: 0.00013571
Iteration 83/1000 | Loss: 0.00015494
Iteration 84/1000 | Loss: 0.00008582
Iteration 85/1000 | Loss: 0.00011147
Iteration 86/1000 | Loss: 0.00007348
Iteration 87/1000 | Loss: 0.00008722
Iteration 88/1000 | Loss: 0.00012842
Iteration 89/1000 | Loss: 0.00010856
Iteration 90/1000 | Loss: 0.00010817
Iteration 91/1000 | Loss: 0.00006600
Iteration 92/1000 | Loss: 0.00006559
Iteration 93/1000 | Loss: 0.00015666
Iteration 94/1000 | Loss: 0.00006831
Iteration 95/1000 | Loss: 0.00014539
Iteration 96/1000 | Loss: 0.00009606
Iteration 97/1000 | Loss: 0.00006661
Iteration 98/1000 | Loss: 0.00006453
Iteration 99/1000 | Loss: 0.00007011
Iteration 100/1000 | Loss: 0.00027426
Iteration 101/1000 | Loss: 0.00190456
Iteration 102/1000 | Loss: 0.00047724
Iteration 103/1000 | Loss: 0.00018423
Iteration 104/1000 | Loss: 0.00018275
Iteration 105/1000 | Loss: 0.00017284
Iteration 106/1000 | Loss: 0.00014592
Iteration 107/1000 | Loss: 0.00016605
Iteration 108/1000 | Loss: 0.00042336
Iteration 109/1000 | Loss: 0.00008635
Iteration 110/1000 | Loss: 0.00017376
Iteration 111/1000 | Loss: 0.00028921
Iteration 112/1000 | Loss: 0.00034186
Iteration 113/1000 | Loss: 0.00018495
Iteration 114/1000 | Loss: 0.00013208
Iteration 115/1000 | Loss: 0.00014625
Iteration 116/1000 | Loss: 0.00005033
Iteration 117/1000 | Loss: 0.00004570
Iteration 118/1000 | Loss: 0.00040076
Iteration 119/1000 | Loss: 0.00046653
Iteration 120/1000 | Loss: 0.00178893
Iteration 121/1000 | Loss: 0.00023086
Iteration 122/1000 | Loss: 0.00006983
Iteration 123/1000 | Loss: 0.00010473
Iteration 124/1000 | Loss: 0.00004422
Iteration 125/1000 | Loss: 0.00004938
Iteration 126/1000 | Loss: 0.00004346
Iteration 127/1000 | Loss: 0.00012497
Iteration 128/1000 | Loss: 0.00004306
Iteration 129/1000 | Loss: 0.00011632
Iteration 130/1000 | Loss: 0.00004298
Iteration 131/1000 | Loss: 0.00020531
Iteration 132/1000 | Loss: 0.00007231
Iteration 133/1000 | Loss: 0.00005209
Iteration 134/1000 | Loss: 0.00006451
Iteration 135/1000 | Loss: 0.00004278
Iteration 136/1000 | Loss: 0.00004271
Iteration 137/1000 | Loss: 0.00004264
Iteration 138/1000 | Loss: 0.00004264
Iteration 139/1000 | Loss: 0.00004264
Iteration 140/1000 | Loss: 0.00010834
Iteration 141/1000 | Loss: 0.00004269
Iteration 142/1000 | Loss: 0.00004260
Iteration 143/1000 | Loss: 0.00004259
Iteration 144/1000 | Loss: 0.00004259
Iteration 145/1000 | Loss: 0.00004259
Iteration 146/1000 | Loss: 0.00004259
Iteration 147/1000 | Loss: 0.00004259
Iteration 148/1000 | Loss: 0.00004259
Iteration 149/1000 | Loss: 0.00004259
Iteration 150/1000 | Loss: 0.00004259
Iteration 151/1000 | Loss: 0.00004259
Iteration 152/1000 | Loss: 0.00004259
Iteration 153/1000 | Loss: 0.00004259
Iteration 154/1000 | Loss: 0.00004259
Iteration 155/1000 | Loss: 0.00004258
Iteration 156/1000 | Loss: 0.00004258
Iteration 157/1000 | Loss: 0.00004258
Iteration 158/1000 | Loss: 0.00004258
Iteration 159/1000 | Loss: 0.00004257
Iteration 160/1000 | Loss: 0.00004257
Iteration 161/1000 | Loss: 0.00004257
Iteration 162/1000 | Loss: 0.00004257
Iteration 163/1000 | Loss: 0.00004257
Iteration 164/1000 | Loss: 0.00004257
Iteration 165/1000 | Loss: 0.00004257
Iteration 166/1000 | Loss: 0.00004257
Iteration 167/1000 | Loss: 0.00004257
Iteration 168/1000 | Loss: 0.00004257
Iteration 169/1000 | Loss: 0.00004256
Iteration 170/1000 | Loss: 0.00004256
Iteration 171/1000 | Loss: 0.00004256
Iteration 172/1000 | Loss: 0.00004256
Iteration 173/1000 | Loss: 0.00008454
Iteration 174/1000 | Loss: 0.00004258
Iteration 175/1000 | Loss: 0.00004257
Iteration 176/1000 | Loss: 0.00004257
Iteration 177/1000 | Loss: 0.00004257
Iteration 178/1000 | Loss: 0.00004256
Iteration 179/1000 | Loss: 0.00004256
Iteration 180/1000 | Loss: 0.00004256
Iteration 181/1000 | Loss: 0.00004255
Iteration 182/1000 | Loss: 0.00004255
Iteration 183/1000 | Loss: 0.00004255
Iteration 184/1000 | Loss: 0.00004255
Iteration 185/1000 | Loss: 0.00004255
Iteration 186/1000 | Loss: 0.00004255
Iteration 187/1000 | Loss: 0.00004255
Iteration 188/1000 | Loss: 0.00004255
Iteration 189/1000 | Loss: 0.00004255
Iteration 190/1000 | Loss: 0.00004255
Iteration 191/1000 | Loss: 0.00004255
Iteration 192/1000 | Loss: 0.00004255
Iteration 193/1000 | Loss: 0.00004255
Iteration 194/1000 | Loss: 0.00004254
Iteration 195/1000 | Loss: 0.00004254
Iteration 196/1000 | Loss: 0.00004254
Iteration 197/1000 | Loss: 0.00004254
Iteration 198/1000 | Loss: 0.00004254
Iteration 199/1000 | Loss: 0.00004254
Iteration 200/1000 | Loss: 0.00004254
Iteration 201/1000 | Loss: 0.00004254
Iteration 202/1000 | Loss: 0.00004254
Iteration 203/1000 | Loss: 0.00004254
Iteration 204/1000 | Loss: 0.00004254
Iteration 205/1000 | Loss: 0.00004253
Iteration 206/1000 | Loss: 0.00004253
Iteration 207/1000 | Loss: 0.00004253
Iteration 208/1000 | Loss: 0.00004253
Iteration 209/1000 | Loss: 0.00004253
Iteration 210/1000 | Loss: 0.00004253
Iteration 211/1000 | Loss: 0.00004253
Iteration 212/1000 | Loss: 0.00004253
Iteration 213/1000 | Loss: 0.00004253
Iteration 214/1000 | Loss: 0.00004253
Iteration 215/1000 | Loss: 0.00004253
Iteration 216/1000 | Loss: 0.00004253
Iteration 217/1000 | Loss: 0.00004253
Iteration 218/1000 | Loss: 0.00004253
Iteration 219/1000 | Loss: 0.00004253
Iteration 220/1000 | Loss: 0.00004253
Iteration 221/1000 | Loss: 0.00004253
Iteration 222/1000 | Loss: 0.00004253
Iteration 223/1000 | Loss: 0.00004253
Iteration 224/1000 | Loss: 0.00004253
Iteration 225/1000 | Loss: 0.00004253
Iteration 226/1000 | Loss: 0.00004253
Iteration 227/1000 | Loss: 0.00004253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [4.253175211488269e-05, 4.253175211488269e-05, 4.253175211488269e-05, 4.253175211488269e-05, 4.253175211488269e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.253175211488269e-05

Optimization complete. Final v2v error: 4.248321533203125 mm

Highest mean error: 11.293560981750488 mm for frame 120

Lowest mean error: 3.4670236110687256 mm for frame 14

Saving results

Total time: 275.2564284801483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765292
Iteration 2/25 | Loss: 0.00161223
Iteration 3/25 | Loss: 0.00128892
Iteration 4/25 | Loss: 0.00124800
Iteration 5/25 | Loss: 0.00124104
Iteration 6/25 | Loss: 0.00123930
Iteration 7/25 | Loss: 0.00123930
Iteration 8/25 | Loss: 0.00123930
Iteration 9/25 | Loss: 0.00123930
Iteration 10/25 | Loss: 0.00123930
Iteration 11/25 | Loss: 0.00123930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012392991920933127, 0.0012392991920933127, 0.0012392991920933127, 0.0012392991920933127, 0.0012392991920933127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012392991920933127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39514720
Iteration 2/25 | Loss: 0.00105877
Iteration 3/25 | Loss: 0.00105877
Iteration 4/25 | Loss: 0.00105877
Iteration 5/25 | Loss: 0.00105877
Iteration 6/25 | Loss: 0.00105877
Iteration 7/25 | Loss: 0.00105877
Iteration 8/25 | Loss: 0.00105877
Iteration 9/25 | Loss: 0.00105877
Iteration 10/25 | Loss: 0.00105877
Iteration 11/25 | Loss: 0.00105877
Iteration 12/25 | Loss: 0.00105877
Iteration 13/25 | Loss: 0.00105877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010587673168629408, 0.0010587673168629408, 0.0010587673168629408, 0.0010587673168629408, 0.0010587673168629408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010587673168629408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105877
Iteration 2/1000 | Loss: 0.00003997
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001761
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001506
Iteration 11/1000 | Loss: 0.00001493
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001462
Iteration 39/1000 | Loss: 0.00001462
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001456
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001455
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001453
Iteration 86/1000 | Loss: 0.00001453
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001452
Iteration 89/1000 | Loss: 0.00001452
Iteration 90/1000 | Loss: 0.00001452
Iteration 91/1000 | Loss: 0.00001452
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001450
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.4504554201266728e-05, 1.4504554201266728e-05, 1.4504554201266728e-05, 1.4504554201266728e-05, 1.4504554201266728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4504554201266728e-05

Optimization complete. Final v2v error: 3.2589480876922607 mm

Highest mean error: 3.584664821624756 mm for frame 56

Lowest mean error: 2.834768533706665 mm for frame 14

Saving results

Total time: 34.03590393066406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042177
Iteration 2/25 | Loss: 0.00212584
Iteration 3/25 | Loss: 0.00149446
Iteration 4/25 | Loss: 0.00141062
Iteration 5/25 | Loss: 0.00136208
Iteration 6/25 | Loss: 0.00133010
Iteration 7/25 | Loss: 0.00128981
Iteration 8/25 | Loss: 0.00125749
Iteration 9/25 | Loss: 0.00125360
Iteration 10/25 | Loss: 0.00123844
Iteration 11/25 | Loss: 0.00123391
Iteration 12/25 | Loss: 0.00123237
Iteration 13/25 | Loss: 0.00123183
Iteration 14/25 | Loss: 0.00123474
Iteration 15/25 | Loss: 0.00123471
Iteration 16/25 | Loss: 0.00123420
Iteration 17/25 | Loss: 0.00123160
Iteration 18/25 | Loss: 0.00123073
Iteration 19/25 | Loss: 0.00123500
Iteration 20/25 | Loss: 0.00123050
Iteration 21/25 | Loss: 0.00123049
Iteration 22/25 | Loss: 0.00123512
Iteration 23/25 | Loss: 0.00123108
Iteration 24/25 | Loss: 0.00123080
Iteration 25/25 | Loss: 0.00123418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29967856
Iteration 2/25 | Loss: 0.00114375
Iteration 3/25 | Loss: 0.00114375
Iteration 4/25 | Loss: 0.00114375
Iteration 5/25 | Loss: 0.00114375
Iteration 6/25 | Loss: 0.00111943
Iteration 7/25 | Loss: 0.00111942
Iteration 8/25 | Loss: 0.00111942
Iteration 9/25 | Loss: 0.00111942
Iteration 10/25 | Loss: 0.00111942
Iteration 11/25 | Loss: 0.00111942
Iteration 12/25 | Loss: 0.00111942
Iteration 13/25 | Loss: 0.00111942
Iteration 14/25 | Loss: 0.00111942
Iteration 15/25 | Loss: 0.00111942
Iteration 16/25 | Loss: 0.00111942
Iteration 17/25 | Loss: 0.00111942
Iteration 18/25 | Loss: 0.00111942
Iteration 19/25 | Loss: 0.00111942
Iteration 20/25 | Loss: 0.00111942
Iteration 21/25 | Loss: 0.00111942
Iteration 22/25 | Loss: 0.00111942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011194198159500957, 0.0011194198159500957, 0.0011194198159500957, 0.0011194198159500957, 0.0011194198159500957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011194198159500957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111942
Iteration 2/1000 | Loss: 0.00008556
Iteration 3/1000 | Loss: 0.00005510
Iteration 4/1000 | Loss: 0.00010166
Iteration 5/1000 | Loss: 0.00004444
Iteration 6/1000 | Loss: 0.00004212
Iteration 7/1000 | Loss: 0.00004008
Iteration 8/1000 | Loss: 0.00003892
Iteration 9/1000 | Loss: 0.00003774
Iteration 10/1000 | Loss: 0.00003669
Iteration 11/1000 | Loss: 0.00003582
Iteration 12/1000 | Loss: 0.00003503
Iteration 13/1000 | Loss: 0.00003401
Iteration 14/1000 | Loss: 0.00003323
Iteration 15/1000 | Loss: 0.00172099
Iteration 16/1000 | Loss: 0.00356812
Iteration 17/1000 | Loss: 0.00006835
Iteration 18/1000 | Loss: 0.00016864
Iteration 19/1000 | Loss: 0.00020078
Iteration 20/1000 | Loss: 0.00006498
Iteration 21/1000 | Loss: 0.00032612
Iteration 22/1000 | Loss: 0.00029864
Iteration 23/1000 | Loss: 0.00030902
Iteration 24/1000 | Loss: 0.00009952
Iteration 25/1000 | Loss: 0.00003790
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00007674
Iteration 30/1000 | Loss: 0.00003031
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00002421
Iteration 33/1000 | Loss: 0.00012325
Iteration 34/1000 | Loss: 0.00011222
Iteration 35/1000 | Loss: 0.00003738
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00002077
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001054
Iteration 41/1000 | Loss: 0.00001031
Iteration 42/1000 | Loss: 0.00001025
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001022
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000995
Iteration 52/1000 | Loss: 0.00000995
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000990
Iteration 64/1000 | Loss: 0.00000990
Iteration 65/1000 | Loss: 0.00000990
Iteration 66/1000 | Loss: 0.00000990
Iteration 67/1000 | Loss: 0.00000990
Iteration 68/1000 | Loss: 0.00000990
Iteration 69/1000 | Loss: 0.00000989
Iteration 70/1000 | Loss: 0.00000989
Iteration 71/1000 | Loss: 0.00000989
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000988
Iteration 74/1000 | Loss: 0.00000988
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000984
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000983
Iteration 102/1000 | Loss: 0.00000983
Iteration 103/1000 | Loss: 0.00000983
Iteration 104/1000 | Loss: 0.00000983
Iteration 105/1000 | Loss: 0.00000983
Iteration 106/1000 | Loss: 0.00000982
Iteration 107/1000 | Loss: 0.00000982
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000982
Iteration 110/1000 | Loss: 0.00000982
Iteration 111/1000 | Loss: 0.00000982
Iteration 112/1000 | Loss: 0.00000982
Iteration 113/1000 | Loss: 0.00000982
Iteration 114/1000 | Loss: 0.00000982
Iteration 115/1000 | Loss: 0.00000981
Iteration 116/1000 | Loss: 0.00000981
Iteration 117/1000 | Loss: 0.00000981
Iteration 118/1000 | Loss: 0.00000981
Iteration 119/1000 | Loss: 0.00000981
Iteration 120/1000 | Loss: 0.00000981
Iteration 121/1000 | Loss: 0.00000981
Iteration 122/1000 | Loss: 0.00000980
Iteration 123/1000 | Loss: 0.00000980
Iteration 124/1000 | Loss: 0.00000980
Iteration 125/1000 | Loss: 0.00000980
Iteration 126/1000 | Loss: 0.00000980
Iteration 127/1000 | Loss: 0.00000980
Iteration 128/1000 | Loss: 0.00000980
Iteration 129/1000 | Loss: 0.00000980
Iteration 130/1000 | Loss: 0.00000980
Iteration 131/1000 | Loss: 0.00000980
Iteration 132/1000 | Loss: 0.00000980
Iteration 133/1000 | Loss: 0.00000980
Iteration 134/1000 | Loss: 0.00000980
Iteration 135/1000 | Loss: 0.00000980
Iteration 136/1000 | Loss: 0.00000980
Iteration 137/1000 | Loss: 0.00000980
Iteration 138/1000 | Loss: 0.00000980
Iteration 139/1000 | Loss: 0.00000980
Iteration 140/1000 | Loss: 0.00000980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [9.79952255875105e-06, 9.79952255875105e-06, 9.79952255875105e-06, 9.79952255875105e-06, 9.79952255875105e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.79952255875105e-06

Optimization complete. Final v2v error: 2.6844167709350586 mm

Highest mean error: 2.866771936416626 mm for frame 94

Lowest mean error: 2.6088685989379883 mm for frame 18

Saving results

Total time: 106.70484280586243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024290
Iteration 2/25 | Loss: 0.00165457
Iteration 3/25 | Loss: 0.00140673
Iteration 4/25 | Loss: 0.00141817
Iteration 5/25 | Loss: 0.00134606
Iteration 6/25 | Loss: 0.00127527
Iteration 7/25 | Loss: 0.00125524
Iteration 8/25 | Loss: 0.00124025
Iteration 9/25 | Loss: 0.00128040
Iteration 10/25 | Loss: 0.00122953
Iteration 11/25 | Loss: 0.00121870
Iteration 12/25 | Loss: 0.00121594
Iteration 13/25 | Loss: 0.00121598
Iteration 14/25 | Loss: 0.00122197
Iteration 15/25 | Loss: 0.00121546
Iteration 16/25 | Loss: 0.00120784
Iteration 17/25 | Loss: 0.00120663
Iteration 18/25 | Loss: 0.00121130
Iteration 19/25 | Loss: 0.00120634
Iteration 20/25 | Loss: 0.00120340
Iteration 21/25 | Loss: 0.00120317
Iteration 22/25 | Loss: 0.00120308
Iteration 23/25 | Loss: 0.00120308
Iteration 24/25 | Loss: 0.00120308
Iteration 25/25 | Loss: 0.00120307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52845967
Iteration 2/25 | Loss: 0.00109971
Iteration 3/25 | Loss: 0.00109970
Iteration 4/25 | Loss: 0.00109970
Iteration 5/25 | Loss: 0.00109970
Iteration 6/25 | Loss: 0.00109970
Iteration 7/25 | Loss: 0.00109970
Iteration 8/25 | Loss: 0.00109970
Iteration 9/25 | Loss: 0.00109970
Iteration 10/25 | Loss: 0.00109970
Iteration 11/25 | Loss: 0.00109970
Iteration 12/25 | Loss: 0.00109970
Iteration 13/25 | Loss: 0.00109970
Iteration 14/25 | Loss: 0.00109970
Iteration 15/25 | Loss: 0.00109970
Iteration 16/25 | Loss: 0.00109970
Iteration 17/25 | Loss: 0.00109970
Iteration 18/25 | Loss: 0.00109970
Iteration 19/25 | Loss: 0.00109970
Iteration 20/25 | Loss: 0.00109970
Iteration 21/25 | Loss: 0.00109970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010996999917551875, 0.0010996999917551875, 0.0010996999917551875, 0.0010996999917551875, 0.0010996999917551875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010996999917551875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109970
Iteration 2/1000 | Loss: 0.00002532
Iteration 3/1000 | Loss: 0.00001778
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00001379
Iteration 6/1000 | Loss: 0.00030275
Iteration 7/1000 | Loss: 0.00003908
Iteration 8/1000 | Loss: 0.00002224
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001261
Iteration 12/1000 | Loss: 0.00026776
Iteration 13/1000 | Loss: 0.00007708
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00020381
Iteration 16/1000 | Loss: 0.00004517
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00022894
Iteration 20/1000 | Loss: 0.00001269
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001180
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001174
Iteration 31/1000 | Loss: 0.00001174
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001174
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.1719823305611499e-05, 1.1719823305611499e-05, 1.1719823305611499e-05, 1.1719823305611499e-05, 1.1719823305611499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1719823305611499e-05

Optimization complete. Final v2v error: 2.9227395057678223 mm

Highest mean error: 3.8445253372192383 mm for frame 60

Lowest mean error: 2.685150623321533 mm for frame 136

Saving results

Total time: 70.53138136863708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806527
Iteration 2/25 | Loss: 0.00148465
Iteration 3/25 | Loss: 0.00129942
Iteration 4/25 | Loss: 0.00129449
Iteration 5/25 | Loss: 0.00129352
Iteration 6/25 | Loss: 0.00129352
Iteration 7/25 | Loss: 0.00129352
Iteration 8/25 | Loss: 0.00129352
Iteration 9/25 | Loss: 0.00129352
Iteration 10/25 | Loss: 0.00129352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001293524052016437, 0.001293524052016437, 0.001293524052016437, 0.001293524052016437, 0.001293524052016437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001293524052016437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25203288
Iteration 2/25 | Loss: 0.00087657
Iteration 3/25 | Loss: 0.00087656
Iteration 4/25 | Loss: 0.00087656
Iteration 5/25 | Loss: 0.00087656
Iteration 6/25 | Loss: 0.00087656
Iteration 7/25 | Loss: 0.00087656
Iteration 8/25 | Loss: 0.00087656
Iteration 9/25 | Loss: 0.00087656
Iteration 10/25 | Loss: 0.00087656
Iteration 11/25 | Loss: 0.00087656
Iteration 12/25 | Loss: 0.00087656
Iteration 13/25 | Loss: 0.00087656
Iteration 14/25 | Loss: 0.00087656
Iteration 15/25 | Loss: 0.00087656
Iteration 16/25 | Loss: 0.00087656
Iteration 17/25 | Loss: 0.00087656
Iteration 18/25 | Loss: 0.00087656
Iteration 19/25 | Loss: 0.00087656
Iteration 20/25 | Loss: 0.00087656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008765586535446346, 0.0008765586535446346, 0.0008765586535446346, 0.0008765586535446346, 0.0008765586535446346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008765586535446346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087656
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00002507
Iteration 4/1000 | Loss: 0.00002201
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001977
Iteration 8/1000 | Loss: 0.00001940
Iteration 9/1000 | Loss: 0.00001910
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001877
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001858
Iteration 14/1000 | Loss: 0.00001857
Iteration 15/1000 | Loss: 0.00001843
Iteration 16/1000 | Loss: 0.00001840
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001833
Iteration 19/1000 | Loss: 0.00001833
Iteration 20/1000 | Loss: 0.00001833
Iteration 21/1000 | Loss: 0.00001832
Iteration 22/1000 | Loss: 0.00001831
Iteration 23/1000 | Loss: 0.00001831
Iteration 24/1000 | Loss: 0.00001830
Iteration 25/1000 | Loss: 0.00001830
Iteration 26/1000 | Loss: 0.00001830
Iteration 27/1000 | Loss: 0.00001829
Iteration 28/1000 | Loss: 0.00001829
Iteration 29/1000 | Loss: 0.00001829
Iteration 30/1000 | Loss: 0.00001828
Iteration 31/1000 | Loss: 0.00001828
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001823
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001820
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001818
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001817
Iteration 69/1000 | Loss: 0.00001816
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001816
Iteration 73/1000 | Loss: 0.00001816
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001816
Iteration 76/1000 | Loss: 0.00001816
Iteration 77/1000 | Loss: 0.00001816
Iteration 78/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.8156728401663713e-05, 1.8156728401663713e-05, 1.8156728401663713e-05, 1.8156728401663713e-05, 1.8156728401663713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8156728401663713e-05

Optimization complete. Final v2v error: 3.5865073204040527 mm

Highest mean error: 4.018429279327393 mm for frame 58

Lowest mean error: 3.34627366065979 mm for frame 5

Saving results

Total time: 30.10873055458069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397619
Iteration 2/25 | Loss: 0.00126576
Iteration 3/25 | Loss: 0.00118747
Iteration 4/25 | Loss: 0.00117860
Iteration 5/25 | Loss: 0.00117621
Iteration 6/25 | Loss: 0.00117585
Iteration 7/25 | Loss: 0.00117585
Iteration 8/25 | Loss: 0.00117585
Iteration 9/25 | Loss: 0.00117585
Iteration 10/25 | Loss: 0.00117585
Iteration 11/25 | Loss: 0.00117585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001175850979052484, 0.001175850979052484, 0.001175850979052484, 0.001175850979052484, 0.001175850979052484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175850979052484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36132872
Iteration 2/25 | Loss: 0.00108527
Iteration 3/25 | Loss: 0.00108526
Iteration 4/25 | Loss: 0.00108526
Iteration 5/25 | Loss: 0.00108526
Iteration 6/25 | Loss: 0.00108526
Iteration 7/25 | Loss: 0.00108526
Iteration 8/25 | Loss: 0.00108526
Iteration 9/25 | Loss: 0.00108526
Iteration 10/25 | Loss: 0.00108526
Iteration 11/25 | Loss: 0.00108526
Iteration 12/25 | Loss: 0.00108526
Iteration 13/25 | Loss: 0.00108526
Iteration 14/25 | Loss: 0.00108526
Iteration 15/25 | Loss: 0.00108526
Iteration 16/25 | Loss: 0.00108526
Iteration 17/25 | Loss: 0.00108526
Iteration 18/25 | Loss: 0.00108526
Iteration 19/25 | Loss: 0.00108526
Iteration 20/25 | Loss: 0.00108526
Iteration 21/25 | Loss: 0.00108526
Iteration 22/25 | Loss: 0.00108526
Iteration 23/25 | Loss: 0.00108526
Iteration 24/25 | Loss: 0.00108526
Iteration 25/25 | Loss: 0.00108526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108526
Iteration 2/1000 | Loss: 0.00002751
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001220
Iteration 6/1000 | Loss: 0.00001137
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001052
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001006
Iteration 12/1000 | Loss: 0.00001004
Iteration 13/1000 | Loss: 0.00000999
Iteration 14/1000 | Loss: 0.00000997
Iteration 15/1000 | Loss: 0.00000995
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000991
Iteration 20/1000 | Loss: 0.00000984
Iteration 21/1000 | Loss: 0.00000978
Iteration 22/1000 | Loss: 0.00000975
Iteration 23/1000 | Loss: 0.00000974
Iteration 24/1000 | Loss: 0.00000973
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000967
Iteration 28/1000 | Loss: 0.00000967
Iteration 29/1000 | Loss: 0.00000966
Iteration 30/1000 | Loss: 0.00000965
Iteration 31/1000 | Loss: 0.00000965
Iteration 32/1000 | Loss: 0.00000965
Iteration 33/1000 | Loss: 0.00000964
Iteration 34/1000 | Loss: 0.00000964
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000961
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000959
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000957
Iteration 46/1000 | Loss: 0.00000956
Iteration 47/1000 | Loss: 0.00000956
Iteration 48/1000 | Loss: 0.00000955
Iteration 49/1000 | Loss: 0.00000955
Iteration 50/1000 | Loss: 0.00000955
Iteration 51/1000 | Loss: 0.00000954
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000954
Iteration 55/1000 | Loss: 0.00000954
Iteration 56/1000 | Loss: 0.00000954
Iteration 57/1000 | Loss: 0.00000954
Iteration 58/1000 | Loss: 0.00000953
Iteration 59/1000 | Loss: 0.00000953
Iteration 60/1000 | Loss: 0.00000952
Iteration 61/1000 | Loss: 0.00000951
Iteration 62/1000 | Loss: 0.00000951
Iteration 63/1000 | Loss: 0.00000951
Iteration 64/1000 | Loss: 0.00000950
Iteration 65/1000 | Loss: 0.00000950
Iteration 66/1000 | Loss: 0.00000950
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000949
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000948
Iteration 74/1000 | Loss: 0.00000948
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000947
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000947
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000946
Iteration 85/1000 | Loss: 0.00000946
Iteration 86/1000 | Loss: 0.00000946
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000945
Iteration 89/1000 | Loss: 0.00000945
Iteration 90/1000 | Loss: 0.00000945
Iteration 91/1000 | Loss: 0.00000945
Iteration 92/1000 | Loss: 0.00000945
Iteration 93/1000 | Loss: 0.00000944
Iteration 94/1000 | Loss: 0.00000944
Iteration 95/1000 | Loss: 0.00000944
Iteration 96/1000 | Loss: 0.00000943
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000943
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000941
Iteration 106/1000 | Loss: 0.00000941
Iteration 107/1000 | Loss: 0.00000941
Iteration 108/1000 | Loss: 0.00000941
Iteration 109/1000 | Loss: 0.00000941
Iteration 110/1000 | Loss: 0.00000940
Iteration 111/1000 | Loss: 0.00000940
Iteration 112/1000 | Loss: 0.00000940
Iteration 113/1000 | Loss: 0.00000940
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000939
Iteration 119/1000 | Loss: 0.00000939
Iteration 120/1000 | Loss: 0.00000939
Iteration 121/1000 | Loss: 0.00000939
Iteration 122/1000 | Loss: 0.00000939
Iteration 123/1000 | Loss: 0.00000939
Iteration 124/1000 | Loss: 0.00000939
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000939
Iteration 132/1000 | Loss: 0.00000939
Iteration 133/1000 | Loss: 0.00000938
Iteration 134/1000 | Loss: 0.00000938
Iteration 135/1000 | Loss: 0.00000938
Iteration 136/1000 | Loss: 0.00000938
Iteration 137/1000 | Loss: 0.00000938
Iteration 138/1000 | Loss: 0.00000938
Iteration 139/1000 | Loss: 0.00000938
Iteration 140/1000 | Loss: 0.00000938
Iteration 141/1000 | Loss: 0.00000938
Iteration 142/1000 | Loss: 0.00000938
Iteration 143/1000 | Loss: 0.00000937
Iteration 144/1000 | Loss: 0.00000937
Iteration 145/1000 | Loss: 0.00000937
Iteration 146/1000 | Loss: 0.00000936
Iteration 147/1000 | Loss: 0.00000936
Iteration 148/1000 | Loss: 0.00000936
Iteration 149/1000 | Loss: 0.00000936
Iteration 150/1000 | Loss: 0.00000936
Iteration 151/1000 | Loss: 0.00000936
Iteration 152/1000 | Loss: 0.00000936
Iteration 153/1000 | Loss: 0.00000936
Iteration 154/1000 | Loss: 0.00000936
Iteration 155/1000 | Loss: 0.00000935
Iteration 156/1000 | Loss: 0.00000935
Iteration 157/1000 | Loss: 0.00000935
Iteration 158/1000 | Loss: 0.00000935
Iteration 159/1000 | Loss: 0.00000935
Iteration 160/1000 | Loss: 0.00000935
Iteration 161/1000 | Loss: 0.00000935
Iteration 162/1000 | Loss: 0.00000935
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000934
Iteration 165/1000 | Loss: 0.00000934
Iteration 166/1000 | Loss: 0.00000934
Iteration 167/1000 | Loss: 0.00000934
Iteration 168/1000 | Loss: 0.00000934
Iteration 169/1000 | Loss: 0.00000934
Iteration 170/1000 | Loss: 0.00000934
Iteration 171/1000 | Loss: 0.00000934
Iteration 172/1000 | Loss: 0.00000934
Iteration 173/1000 | Loss: 0.00000934
Iteration 174/1000 | Loss: 0.00000934
Iteration 175/1000 | Loss: 0.00000934
Iteration 176/1000 | Loss: 0.00000934
Iteration 177/1000 | Loss: 0.00000934
Iteration 178/1000 | Loss: 0.00000933
Iteration 179/1000 | Loss: 0.00000933
Iteration 180/1000 | Loss: 0.00000933
Iteration 181/1000 | Loss: 0.00000933
Iteration 182/1000 | Loss: 0.00000933
Iteration 183/1000 | Loss: 0.00000933
Iteration 184/1000 | Loss: 0.00000933
Iteration 185/1000 | Loss: 0.00000933
Iteration 186/1000 | Loss: 0.00000933
Iteration 187/1000 | Loss: 0.00000933
Iteration 188/1000 | Loss: 0.00000933
Iteration 189/1000 | Loss: 0.00000933
Iteration 190/1000 | Loss: 0.00000932
Iteration 191/1000 | Loss: 0.00000932
Iteration 192/1000 | Loss: 0.00000932
Iteration 193/1000 | Loss: 0.00000932
Iteration 194/1000 | Loss: 0.00000932
Iteration 195/1000 | Loss: 0.00000932
Iteration 196/1000 | Loss: 0.00000932
Iteration 197/1000 | Loss: 0.00000932
Iteration 198/1000 | Loss: 0.00000932
Iteration 199/1000 | Loss: 0.00000932
Iteration 200/1000 | Loss: 0.00000932
Iteration 201/1000 | Loss: 0.00000932
Iteration 202/1000 | Loss: 0.00000932
Iteration 203/1000 | Loss: 0.00000932
Iteration 204/1000 | Loss: 0.00000932
Iteration 205/1000 | Loss: 0.00000932
Iteration 206/1000 | Loss: 0.00000932
Iteration 207/1000 | Loss: 0.00000932
Iteration 208/1000 | Loss: 0.00000931
Iteration 209/1000 | Loss: 0.00000931
Iteration 210/1000 | Loss: 0.00000931
Iteration 211/1000 | Loss: 0.00000931
Iteration 212/1000 | Loss: 0.00000931
Iteration 213/1000 | Loss: 0.00000931
Iteration 214/1000 | Loss: 0.00000931
Iteration 215/1000 | Loss: 0.00000931
Iteration 216/1000 | Loss: 0.00000931
Iteration 217/1000 | Loss: 0.00000930
Iteration 218/1000 | Loss: 0.00000930
Iteration 219/1000 | Loss: 0.00000930
Iteration 220/1000 | Loss: 0.00000930
Iteration 221/1000 | Loss: 0.00000930
Iteration 222/1000 | Loss: 0.00000930
Iteration 223/1000 | Loss: 0.00000930
Iteration 224/1000 | Loss: 0.00000930
Iteration 225/1000 | Loss: 0.00000929
Iteration 226/1000 | Loss: 0.00000929
Iteration 227/1000 | Loss: 0.00000929
Iteration 228/1000 | Loss: 0.00000929
Iteration 229/1000 | Loss: 0.00000929
Iteration 230/1000 | Loss: 0.00000929
Iteration 231/1000 | Loss: 0.00000928
Iteration 232/1000 | Loss: 0.00000928
Iteration 233/1000 | Loss: 0.00000928
Iteration 234/1000 | Loss: 0.00000928
Iteration 235/1000 | Loss: 0.00000928
Iteration 236/1000 | Loss: 0.00000928
Iteration 237/1000 | Loss: 0.00000928
Iteration 238/1000 | Loss: 0.00000927
Iteration 239/1000 | Loss: 0.00000927
Iteration 240/1000 | Loss: 0.00000927
Iteration 241/1000 | Loss: 0.00000927
Iteration 242/1000 | Loss: 0.00000927
Iteration 243/1000 | Loss: 0.00000927
Iteration 244/1000 | Loss: 0.00000927
Iteration 245/1000 | Loss: 0.00000927
Iteration 246/1000 | Loss: 0.00000927
Iteration 247/1000 | Loss: 0.00000927
Iteration 248/1000 | Loss: 0.00000927
Iteration 249/1000 | Loss: 0.00000927
Iteration 250/1000 | Loss: 0.00000927
Iteration 251/1000 | Loss: 0.00000927
Iteration 252/1000 | Loss: 0.00000927
Iteration 253/1000 | Loss: 0.00000926
Iteration 254/1000 | Loss: 0.00000926
Iteration 255/1000 | Loss: 0.00000926
Iteration 256/1000 | Loss: 0.00000926
Iteration 257/1000 | Loss: 0.00000926
Iteration 258/1000 | Loss: 0.00000926
Iteration 259/1000 | Loss: 0.00000926
Iteration 260/1000 | Loss: 0.00000926
Iteration 261/1000 | Loss: 0.00000926
Iteration 262/1000 | Loss: 0.00000926
Iteration 263/1000 | Loss: 0.00000926
Iteration 264/1000 | Loss: 0.00000926
Iteration 265/1000 | Loss: 0.00000926
Iteration 266/1000 | Loss: 0.00000926
Iteration 267/1000 | Loss: 0.00000925
Iteration 268/1000 | Loss: 0.00000925
Iteration 269/1000 | Loss: 0.00000925
Iteration 270/1000 | Loss: 0.00000925
Iteration 271/1000 | Loss: 0.00000925
Iteration 272/1000 | Loss: 0.00000925
Iteration 273/1000 | Loss: 0.00000925
Iteration 274/1000 | Loss: 0.00000925
Iteration 275/1000 | Loss: 0.00000925
Iteration 276/1000 | Loss: 0.00000925
Iteration 277/1000 | Loss: 0.00000925
Iteration 278/1000 | Loss: 0.00000925
Iteration 279/1000 | Loss: 0.00000925
Iteration 280/1000 | Loss: 0.00000925
Iteration 281/1000 | Loss: 0.00000925
Iteration 282/1000 | Loss: 0.00000925
Iteration 283/1000 | Loss: 0.00000925
Iteration 284/1000 | Loss: 0.00000925
Iteration 285/1000 | Loss: 0.00000925
Iteration 286/1000 | Loss: 0.00000924
Iteration 287/1000 | Loss: 0.00000924
Iteration 288/1000 | Loss: 0.00000924
Iteration 289/1000 | Loss: 0.00000924
Iteration 290/1000 | Loss: 0.00000924
Iteration 291/1000 | Loss: 0.00000924
Iteration 292/1000 | Loss: 0.00000924
Iteration 293/1000 | Loss: 0.00000924
Iteration 294/1000 | Loss: 0.00000924
Iteration 295/1000 | Loss: 0.00000924
Iteration 296/1000 | Loss: 0.00000924
Iteration 297/1000 | Loss: 0.00000924
Iteration 298/1000 | Loss: 0.00000924
Iteration 299/1000 | Loss: 0.00000924
Iteration 300/1000 | Loss: 0.00000924
Iteration 301/1000 | Loss: 0.00000924
Iteration 302/1000 | Loss: 0.00000924
Iteration 303/1000 | Loss: 0.00000924
Iteration 304/1000 | Loss: 0.00000924
Iteration 305/1000 | Loss: 0.00000924
Iteration 306/1000 | Loss: 0.00000924
Iteration 307/1000 | Loss: 0.00000924
Iteration 308/1000 | Loss: 0.00000924
Iteration 309/1000 | Loss: 0.00000924
Iteration 310/1000 | Loss: 0.00000924
Iteration 311/1000 | Loss: 0.00000924
Iteration 312/1000 | Loss: 0.00000924
Iteration 313/1000 | Loss: 0.00000924
Iteration 314/1000 | Loss: 0.00000924
Iteration 315/1000 | Loss: 0.00000924
Iteration 316/1000 | Loss: 0.00000924
Iteration 317/1000 | Loss: 0.00000924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [9.240688996214885e-06, 9.240688996214885e-06, 9.240688996214885e-06, 9.240688996214885e-06, 9.240688996214885e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.240688996214885e-06

Optimization complete. Final v2v error: 2.5950865745544434 mm

Highest mean error: 3.5015792846679688 mm for frame 71

Lowest mean error: 2.427459955215454 mm for frame 159

Saving results

Total time: 46.35968017578125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759466
Iteration 2/25 | Loss: 0.00150646
Iteration 3/25 | Loss: 0.00133381
Iteration 4/25 | Loss: 0.00132552
Iteration 5/25 | Loss: 0.00132314
Iteration 6/25 | Loss: 0.00132314
Iteration 7/25 | Loss: 0.00132314
Iteration 8/25 | Loss: 0.00132314
Iteration 9/25 | Loss: 0.00132314
Iteration 10/25 | Loss: 0.00132314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013231385964900255, 0.0013231385964900255, 0.0013231385964900255, 0.0013231385964900255, 0.0013231385964900255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013231385964900255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21121800
Iteration 2/25 | Loss: 0.00084800
Iteration 3/25 | Loss: 0.00084797
Iteration 4/25 | Loss: 0.00084797
Iteration 5/25 | Loss: 0.00084797
Iteration 6/25 | Loss: 0.00084797
Iteration 7/25 | Loss: 0.00084797
Iteration 8/25 | Loss: 0.00084797
Iteration 9/25 | Loss: 0.00084797
Iteration 10/25 | Loss: 0.00084797
Iteration 11/25 | Loss: 0.00084797
Iteration 12/25 | Loss: 0.00084797
Iteration 13/25 | Loss: 0.00084797
Iteration 14/25 | Loss: 0.00084797
Iteration 15/25 | Loss: 0.00084797
Iteration 16/25 | Loss: 0.00084797
Iteration 17/25 | Loss: 0.00084797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008479689131490886, 0.0008479689131490886, 0.0008479689131490886, 0.0008479689131490886, 0.0008479689131490886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008479689131490886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084797
Iteration 2/1000 | Loss: 0.00003876
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002408
Iteration 5/1000 | Loss: 0.00002237
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002033
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001948
Iteration 12/1000 | Loss: 0.00001938
Iteration 13/1000 | Loss: 0.00001921
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001916
Iteration 16/1000 | Loss: 0.00001915
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001908
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001902
Iteration 25/1000 | Loss: 0.00001902
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001883
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001855
Iteration 53/1000 | Loss: 0.00001854
Iteration 54/1000 | Loss: 0.00001854
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001852
Iteration 63/1000 | Loss: 0.00001852
Iteration 64/1000 | Loss: 0.00001852
Iteration 65/1000 | Loss: 0.00001852
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001851
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001848
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001846
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001846
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001845
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001844
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00001843
Iteration 107/1000 | Loss: 0.00001843
Iteration 108/1000 | Loss: 0.00001843
Iteration 109/1000 | Loss: 0.00001842
Iteration 110/1000 | Loss: 0.00001842
Iteration 111/1000 | Loss: 0.00001842
Iteration 112/1000 | Loss: 0.00001842
Iteration 113/1000 | Loss: 0.00001842
Iteration 114/1000 | Loss: 0.00001842
Iteration 115/1000 | Loss: 0.00001842
Iteration 116/1000 | Loss: 0.00001842
Iteration 117/1000 | Loss: 0.00001841
Iteration 118/1000 | Loss: 0.00001841
Iteration 119/1000 | Loss: 0.00001841
Iteration 120/1000 | Loss: 0.00001841
Iteration 121/1000 | Loss: 0.00001841
Iteration 122/1000 | Loss: 0.00001841
Iteration 123/1000 | Loss: 0.00001841
Iteration 124/1000 | Loss: 0.00001841
Iteration 125/1000 | Loss: 0.00001840
Iteration 126/1000 | Loss: 0.00001840
Iteration 127/1000 | Loss: 0.00001840
Iteration 128/1000 | Loss: 0.00001840
Iteration 129/1000 | Loss: 0.00001840
Iteration 130/1000 | Loss: 0.00001840
Iteration 131/1000 | Loss: 0.00001840
Iteration 132/1000 | Loss: 0.00001840
Iteration 133/1000 | Loss: 0.00001840
Iteration 134/1000 | Loss: 0.00001840
Iteration 135/1000 | Loss: 0.00001840
Iteration 136/1000 | Loss: 0.00001840
Iteration 137/1000 | Loss: 0.00001840
Iteration 138/1000 | Loss: 0.00001840
Iteration 139/1000 | Loss: 0.00001840
Iteration 140/1000 | Loss: 0.00001840
Iteration 141/1000 | Loss: 0.00001840
Iteration 142/1000 | Loss: 0.00001840
Iteration 143/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8395972801954485e-05, 1.8395972801954485e-05, 1.8395972801954485e-05, 1.8395972801954485e-05, 1.8395972801954485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8395972801954485e-05

Optimization complete. Final v2v error: 3.504772663116455 mm

Highest mean error: 4.531044006347656 mm for frame 221

Lowest mean error: 2.995435953140259 mm for frame 101

Saving results

Total time: 44.57452058792114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597534
Iteration 2/25 | Loss: 0.00141664
Iteration 3/25 | Loss: 0.00129710
Iteration 4/25 | Loss: 0.00127887
Iteration 5/25 | Loss: 0.00127363
Iteration 6/25 | Loss: 0.00127474
Iteration 7/25 | Loss: 0.00127270
Iteration 8/25 | Loss: 0.00127186
Iteration 9/25 | Loss: 0.00127168
Iteration 10/25 | Loss: 0.00127397
Iteration 11/25 | Loss: 0.00127530
Iteration 12/25 | Loss: 0.00127480
Iteration 13/25 | Loss: 0.00127460
Iteration 14/25 | Loss: 0.00127482
Iteration 15/25 | Loss: 0.00127426
Iteration 16/25 | Loss: 0.00127456
Iteration 17/25 | Loss: 0.00127456
Iteration 18/25 | Loss: 0.00127455
Iteration 19/25 | Loss: 0.00127517
Iteration 20/25 | Loss: 0.00127543
Iteration 21/25 | Loss: 0.00127527
Iteration 22/25 | Loss: 0.00127409
Iteration 23/25 | Loss: 0.00127488
Iteration 24/25 | Loss: 0.00127350
Iteration 25/25 | Loss: 0.00127344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46350145
Iteration 2/25 | Loss: 0.00130142
Iteration 3/25 | Loss: 0.00130141
Iteration 4/25 | Loss: 0.00130141
Iteration 5/25 | Loss: 0.00130141
Iteration 6/25 | Loss: 0.00130141
Iteration 7/25 | Loss: 0.00130141
Iteration 8/25 | Loss: 0.00130141
Iteration 9/25 | Loss: 0.00130141
Iteration 10/25 | Loss: 0.00130141
Iteration 11/25 | Loss: 0.00130141
Iteration 12/25 | Loss: 0.00130141
Iteration 13/25 | Loss: 0.00130141
Iteration 14/25 | Loss: 0.00130141
Iteration 15/25 | Loss: 0.00130141
Iteration 16/25 | Loss: 0.00130141
Iteration 17/25 | Loss: 0.00130141
Iteration 18/25 | Loss: 0.00130141
Iteration 19/25 | Loss: 0.00130141
Iteration 20/25 | Loss: 0.00130141
Iteration 21/25 | Loss: 0.00130141
Iteration 22/25 | Loss: 0.00130141
Iteration 23/25 | Loss: 0.00130141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013014061842113733, 0.0013014061842113733, 0.0013014061842113733, 0.0013014061842113733, 0.0013014061842113733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013014061842113733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130141
Iteration 2/1000 | Loss: 0.00006026
Iteration 3/1000 | Loss: 0.00013419
Iteration 4/1000 | Loss: 0.00008645
Iteration 5/1000 | Loss: 0.00012726
Iteration 6/1000 | Loss: 0.00010813
Iteration 7/1000 | Loss: 0.00012955
Iteration 8/1000 | Loss: 0.00011151
Iteration 9/1000 | Loss: 0.00016713
Iteration 10/1000 | Loss: 0.00012200
Iteration 11/1000 | Loss: 0.00016752
Iteration 12/1000 | Loss: 0.00012044
Iteration 13/1000 | Loss: 0.00014588
Iteration 14/1000 | Loss: 0.00012103
Iteration 15/1000 | Loss: 0.00003278
Iteration 16/1000 | Loss: 0.00011624
Iteration 17/1000 | Loss: 0.00011822
Iteration 18/1000 | Loss: 0.00009960
Iteration 19/1000 | Loss: 0.00009690
Iteration 20/1000 | Loss: 0.00002999
Iteration 21/1000 | Loss: 0.00002719
Iteration 22/1000 | Loss: 0.00002555
Iteration 23/1000 | Loss: 0.00002468
Iteration 24/1000 | Loss: 0.00003333
Iteration 25/1000 | Loss: 0.00003454
Iteration 26/1000 | Loss: 0.00002532
Iteration 27/1000 | Loss: 0.00003588
Iteration 28/1000 | Loss: 0.00004386
Iteration 29/1000 | Loss: 0.00004428
Iteration 30/1000 | Loss: 0.00002462
Iteration 31/1000 | Loss: 0.00002390
Iteration 32/1000 | Loss: 0.00003280
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002305
Iteration 35/1000 | Loss: 0.00002273
Iteration 36/1000 | Loss: 0.00002246
Iteration 37/1000 | Loss: 0.00002235
Iteration 38/1000 | Loss: 0.00002229
Iteration 39/1000 | Loss: 0.00002229
Iteration 40/1000 | Loss: 0.00002228
Iteration 41/1000 | Loss: 0.00002228
Iteration 42/1000 | Loss: 0.00002228
Iteration 43/1000 | Loss: 0.00002227
Iteration 44/1000 | Loss: 0.00002227
Iteration 45/1000 | Loss: 0.00002219
Iteration 46/1000 | Loss: 0.00002219
Iteration 47/1000 | Loss: 0.00002218
Iteration 48/1000 | Loss: 0.00002218
Iteration 49/1000 | Loss: 0.00002217
Iteration 50/1000 | Loss: 0.00002216
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00002215
Iteration 54/1000 | Loss: 0.00002215
Iteration 55/1000 | Loss: 0.00002214
Iteration 56/1000 | Loss: 0.00002214
Iteration 57/1000 | Loss: 0.00002214
Iteration 58/1000 | Loss: 0.00002214
Iteration 59/1000 | Loss: 0.00002213
Iteration 60/1000 | Loss: 0.00002213
Iteration 61/1000 | Loss: 0.00002212
Iteration 62/1000 | Loss: 0.00002212
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002208
Iteration 65/1000 | Loss: 0.00002203
Iteration 66/1000 | Loss: 0.00002202
Iteration 67/1000 | Loss: 0.00002199
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002173
Iteration 72/1000 | Loss: 0.00002173
Iteration 73/1000 | Loss: 0.00002172
Iteration 74/1000 | Loss: 0.00002172
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002171
Iteration 78/1000 | Loss: 0.00002170
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00002169
Iteration 81/1000 | Loss: 0.00002168
Iteration 82/1000 | Loss: 0.00002167
Iteration 83/1000 | Loss: 0.00002167
Iteration 84/1000 | Loss: 0.00002165
Iteration 85/1000 | Loss: 0.00002165
Iteration 86/1000 | Loss: 0.00002165
Iteration 87/1000 | Loss: 0.00002164
Iteration 88/1000 | Loss: 0.00002164
Iteration 89/1000 | Loss: 0.00002164
Iteration 90/1000 | Loss: 0.00002163
Iteration 91/1000 | Loss: 0.00002163
Iteration 92/1000 | Loss: 0.00002163
Iteration 93/1000 | Loss: 0.00002162
Iteration 94/1000 | Loss: 0.00002162
Iteration 95/1000 | Loss: 0.00002161
Iteration 96/1000 | Loss: 0.00002161
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002161
Iteration 99/1000 | Loss: 0.00002160
Iteration 100/1000 | Loss: 0.00002159
Iteration 101/1000 | Loss: 0.00002159
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002158
Iteration 104/1000 | Loss: 0.00002158
Iteration 105/1000 | Loss: 0.00002158
Iteration 106/1000 | Loss: 0.00002158
Iteration 107/1000 | Loss: 0.00002158
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002156
Iteration 111/1000 | Loss: 0.00002156
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002156
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002156
Iteration 117/1000 | Loss: 0.00002155
Iteration 118/1000 | Loss: 0.00002155
Iteration 119/1000 | Loss: 0.00002155
Iteration 120/1000 | Loss: 0.00002155
Iteration 121/1000 | Loss: 0.00002154
Iteration 122/1000 | Loss: 0.00002154
Iteration 123/1000 | Loss: 0.00002154
Iteration 124/1000 | Loss: 0.00002154
Iteration 125/1000 | Loss: 0.00002153
Iteration 126/1000 | Loss: 0.00002153
Iteration 127/1000 | Loss: 0.00002152
Iteration 128/1000 | Loss: 0.00002152
Iteration 129/1000 | Loss: 0.00002152
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002151
Iteration 132/1000 | Loss: 0.00002151
Iteration 133/1000 | Loss: 0.00002150
Iteration 134/1000 | Loss: 0.00002150
Iteration 135/1000 | Loss: 0.00002150
Iteration 136/1000 | Loss: 0.00002150
Iteration 137/1000 | Loss: 0.00002150
Iteration 138/1000 | Loss: 0.00002149
Iteration 139/1000 | Loss: 0.00002149
Iteration 140/1000 | Loss: 0.00002149
Iteration 141/1000 | Loss: 0.00002149
Iteration 142/1000 | Loss: 0.00002149
Iteration 143/1000 | Loss: 0.00002148
Iteration 144/1000 | Loss: 0.00002148
Iteration 145/1000 | Loss: 0.00002148
Iteration 146/1000 | Loss: 0.00002147
Iteration 147/1000 | Loss: 0.00002147
Iteration 148/1000 | Loss: 0.00002147
Iteration 149/1000 | Loss: 0.00002146
Iteration 150/1000 | Loss: 0.00002146
Iteration 151/1000 | Loss: 0.00002146
Iteration 152/1000 | Loss: 0.00002145
Iteration 153/1000 | Loss: 0.00002145
Iteration 154/1000 | Loss: 0.00002145
Iteration 155/1000 | Loss: 0.00002144
Iteration 156/1000 | Loss: 0.00002143
Iteration 157/1000 | Loss: 0.00002142
Iteration 158/1000 | Loss: 0.00002142
Iteration 159/1000 | Loss: 0.00002141
Iteration 160/1000 | Loss: 0.00002141
Iteration 161/1000 | Loss: 0.00002140
Iteration 162/1000 | Loss: 0.00002140
Iteration 163/1000 | Loss: 0.00002139
Iteration 164/1000 | Loss: 0.00002139
Iteration 165/1000 | Loss: 0.00002139
Iteration 166/1000 | Loss: 0.00002138
Iteration 167/1000 | Loss: 0.00002138
Iteration 168/1000 | Loss: 0.00002137
Iteration 169/1000 | Loss: 0.00002137
Iteration 170/1000 | Loss: 0.00002136
Iteration 171/1000 | Loss: 0.00002135
Iteration 172/1000 | Loss: 0.00002134
Iteration 173/1000 | Loss: 0.00002133
Iteration 174/1000 | Loss: 0.00002132
Iteration 175/1000 | Loss: 0.00002572
Iteration 176/1000 | Loss: 0.00002315
Iteration 177/1000 | Loss: 0.00002547
Iteration 178/1000 | Loss: 0.00002414
Iteration 179/1000 | Loss: 0.00002228
Iteration 180/1000 | Loss: 0.00002189
Iteration 181/1000 | Loss: 0.00002583
Iteration 182/1000 | Loss: 0.00002175
Iteration 183/1000 | Loss: 0.00002142
Iteration 184/1000 | Loss: 0.00002140
Iteration 185/1000 | Loss: 0.00002139
Iteration 186/1000 | Loss: 0.00002138
Iteration 187/1000 | Loss: 0.00002131
Iteration 188/1000 | Loss: 0.00002129
Iteration 189/1000 | Loss: 0.00002125
Iteration 190/1000 | Loss: 0.00002123
Iteration 191/1000 | Loss: 0.00002121
Iteration 192/1000 | Loss: 0.00002121
Iteration 193/1000 | Loss: 0.00002120
Iteration 194/1000 | Loss: 0.00002120
Iteration 195/1000 | Loss: 0.00002119
Iteration 196/1000 | Loss: 0.00002118
Iteration 197/1000 | Loss: 0.00002117
Iteration 198/1000 | Loss: 0.00002117
Iteration 199/1000 | Loss: 0.00002116
Iteration 200/1000 | Loss: 0.00002116
Iteration 201/1000 | Loss: 0.00002115
Iteration 202/1000 | Loss: 0.00002115
Iteration 203/1000 | Loss: 0.00002115
Iteration 204/1000 | Loss: 0.00002114
Iteration 205/1000 | Loss: 0.00002114
Iteration 206/1000 | Loss: 0.00002114
Iteration 207/1000 | Loss: 0.00002114
Iteration 208/1000 | Loss: 0.00002113
Iteration 209/1000 | Loss: 0.00002113
Iteration 210/1000 | Loss: 0.00002113
Iteration 211/1000 | Loss: 0.00002113
Iteration 212/1000 | Loss: 0.00002112
Iteration 213/1000 | Loss: 0.00002112
Iteration 214/1000 | Loss: 0.00002112
Iteration 215/1000 | Loss: 0.00002112
Iteration 216/1000 | Loss: 0.00002112
Iteration 217/1000 | Loss: 0.00002112
Iteration 218/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.1124398699612357e-05, 2.1124398699612357e-05, 2.1124398699612357e-05, 2.1124398699612357e-05, 2.1124398699612357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1124398699612357e-05

Optimization complete. Final v2v error: 3.748581647872925 mm

Highest mean error: 6.762687683105469 mm for frame 158

Lowest mean error: 2.813188076019287 mm for frame 196

Saving results

Total time: 141.3332278728485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736532
Iteration 2/25 | Loss: 0.00213826
Iteration 3/25 | Loss: 0.00172569
Iteration 4/25 | Loss: 0.00165026
Iteration 5/25 | Loss: 0.00158032
Iteration 6/25 | Loss: 0.00156436
Iteration 7/25 | Loss: 0.00153197
Iteration 8/25 | Loss: 0.00170566
Iteration 9/25 | Loss: 0.00174097
Iteration 10/25 | Loss: 0.00155405
Iteration 11/25 | Loss: 0.00140921
Iteration 12/25 | Loss: 0.00128791
Iteration 13/25 | Loss: 0.00127687
Iteration 14/25 | Loss: 0.00126230
Iteration 15/25 | Loss: 0.00126778
Iteration 16/25 | Loss: 0.00125857
Iteration 17/25 | Loss: 0.00125565
Iteration 18/25 | Loss: 0.00125914
Iteration 19/25 | Loss: 0.00125583
Iteration 20/25 | Loss: 0.00125705
Iteration 21/25 | Loss: 0.00125592
Iteration 22/25 | Loss: 0.00125462
Iteration 23/25 | Loss: 0.00125713
Iteration 24/25 | Loss: 0.00125683
Iteration 25/25 | Loss: 0.00125535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40123844
Iteration 2/25 | Loss: 0.00148675
Iteration 3/25 | Loss: 0.00123203
Iteration 4/25 | Loss: 0.00123203
Iteration 5/25 | Loss: 0.00123202
Iteration 6/25 | Loss: 0.00123202
Iteration 7/25 | Loss: 0.00123202
Iteration 8/25 | Loss: 0.00123202
Iteration 9/25 | Loss: 0.00123202
Iteration 10/25 | Loss: 0.00123202
Iteration 11/25 | Loss: 0.00123202
Iteration 12/25 | Loss: 0.00123202
Iteration 13/25 | Loss: 0.00123202
Iteration 14/25 | Loss: 0.00123202
Iteration 15/25 | Loss: 0.00123202
Iteration 16/25 | Loss: 0.00123202
Iteration 17/25 | Loss: 0.00123202
Iteration 18/25 | Loss: 0.00123202
Iteration 19/25 | Loss: 0.00123202
Iteration 20/25 | Loss: 0.00123202
Iteration 21/25 | Loss: 0.00123202
Iteration 22/25 | Loss: 0.00123202
Iteration 23/25 | Loss: 0.00123202
Iteration 24/25 | Loss: 0.00123202
Iteration 25/25 | Loss: 0.00123202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123202
Iteration 2/1000 | Loss: 0.00006667
Iteration 3/1000 | Loss: 0.00004175
Iteration 4/1000 | Loss: 0.00003427
Iteration 5/1000 | Loss: 0.00003156
Iteration 6/1000 | Loss: 0.00002943
Iteration 7/1000 | Loss: 0.00002805
Iteration 8/1000 | Loss: 0.00002714
Iteration 9/1000 | Loss: 0.00002630
Iteration 10/1000 | Loss: 0.00002565
Iteration 11/1000 | Loss: 0.00002519
Iteration 12/1000 | Loss: 0.00002485
Iteration 13/1000 | Loss: 0.00002458
Iteration 14/1000 | Loss: 0.00002440
Iteration 15/1000 | Loss: 0.00002433
Iteration 16/1000 | Loss: 0.00002417
Iteration 17/1000 | Loss: 0.00002408
Iteration 18/1000 | Loss: 0.00002408
Iteration 19/1000 | Loss: 0.00002398
Iteration 20/1000 | Loss: 0.00002394
Iteration 21/1000 | Loss: 0.00002385
Iteration 22/1000 | Loss: 0.00002378
Iteration 23/1000 | Loss: 0.00002377
Iteration 24/1000 | Loss: 0.00002377
Iteration 25/1000 | Loss: 0.00002376
Iteration 26/1000 | Loss: 0.00002374
Iteration 27/1000 | Loss: 0.00002369
Iteration 28/1000 | Loss: 0.00002369
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002359
Iteration 31/1000 | Loss: 0.00002353
Iteration 32/1000 | Loss: 0.00002351
Iteration 33/1000 | Loss: 0.00002350
Iteration 34/1000 | Loss: 0.00002350
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00002349
Iteration 37/1000 | Loss: 0.00002347
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002345
Iteration 42/1000 | Loss: 0.00002345
Iteration 43/1000 | Loss: 0.00002345
Iteration 44/1000 | Loss: 0.00002345
Iteration 45/1000 | Loss: 0.00002344
Iteration 46/1000 | Loss: 0.00002343
Iteration 47/1000 | Loss: 0.00002343
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002341
Iteration 55/1000 | Loss: 0.00002341
Iteration 56/1000 | Loss: 0.00002340
Iteration 57/1000 | Loss: 0.00002340
Iteration 58/1000 | Loss: 0.00002339
Iteration 59/1000 | Loss: 0.00002339
Iteration 60/1000 | Loss: 0.00002339
Iteration 61/1000 | Loss: 0.00002338
Iteration 62/1000 | Loss: 0.00002338
Iteration 63/1000 | Loss: 0.00002338
Iteration 64/1000 | Loss: 0.00002338
Iteration 65/1000 | Loss: 0.00002338
Iteration 66/1000 | Loss: 0.00002337
Iteration 67/1000 | Loss: 0.00002337
Iteration 68/1000 | Loss: 0.00002337
Iteration 69/1000 | Loss: 0.00002337
Iteration 70/1000 | Loss: 0.00002336
Iteration 71/1000 | Loss: 0.00002336
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002335
Iteration 74/1000 | Loss: 0.00002335
Iteration 75/1000 | Loss: 0.00002334
Iteration 76/1000 | Loss: 0.00002334
Iteration 77/1000 | Loss: 0.00002332
Iteration 78/1000 | Loss: 0.00002332
Iteration 79/1000 | Loss: 0.00002332
Iteration 80/1000 | Loss: 0.00002331
Iteration 81/1000 | Loss: 0.00002331
Iteration 82/1000 | Loss: 0.00002330
Iteration 83/1000 | Loss: 0.00002329
Iteration 84/1000 | Loss: 0.00002328
Iteration 85/1000 | Loss: 0.00002327
Iteration 86/1000 | Loss: 0.00002327
Iteration 87/1000 | Loss: 0.00002326
Iteration 88/1000 | Loss: 0.00002326
Iteration 89/1000 | Loss: 0.00002326
Iteration 90/1000 | Loss: 0.00002325
Iteration 91/1000 | Loss: 0.00002324
Iteration 92/1000 | Loss: 0.00002324
Iteration 93/1000 | Loss: 0.00002324
Iteration 94/1000 | Loss: 0.00002324
Iteration 95/1000 | Loss: 0.00002324
Iteration 96/1000 | Loss: 0.00002324
Iteration 97/1000 | Loss: 0.00002324
Iteration 98/1000 | Loss: 0.00002324
Iteration 99/1000 | Loss: 0.00002324
Iteration 100/1000 | Loss: 0.00002323
Iteration 101/1000 | Loss: 0.00002323
Iteration 102/1000 | Loss: 0.00002323
Iteration 103/1000 | Loss: 0.00002323
Iteration 104/1000 | Loss: 0.00002323
Iteration 105/1000 | Loss: 0.00002321
Iteration 106/1000 | Loss: 0.00002321
Iteration 107/1000 | Loss: 0.00002321
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002320
Iteration 112/1000 | Loss: 0.00002319
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002319
Iteration 115/1000 | Loss: 0.00002319
Iteration 116/1000 | Loss: 0.00002319
Iteration 117/1000 | Loss: 0.00002319
Iteration 118/1000 | Loss: 0.00002319
Iteration 119/1000 | Loss: 0.00002319
Iteration 120/1000 | Loss: 0.00002318
Iteration 121/1000 | Loss: 0.00002318
Iteration 122/1000 | Loss: 0.00002318
Iteration 123/1000 | Loss: 0.00002318
Iteration 124/1000 | Loss: 0.00002318
Iteration 125/1000 | Loss: 0.00002317
Iteration 126/1000 | Loss: 0.00002317
Iteration 127/1000 | Loss: 0.00002317
Iteration 128/1000 | Loss: 0.00002317
Iteration 129/1000 | Loss: 0.00002317
Iteration 130/1000 | Loss: 0.00002317
Iteration 131/1000 | Loss: 0.00002317
Iteration 132/1000 | Loss: 0.00002317
Iteration 133/1000 | Loss: 0.00002317
Iteration 134/1000 | Loss: 0.00002317
Iteration 135/1000 | Loss: 0.00002316
Iteration 136/1000 | Loss: 0.00002316
Iteration 137/1000 | Loss: 0.00002316
Iteration 138/1000 | Loss: 0.00002316
Iteration 139/1000 | Loss: 0.00002316
Iteration 140/1000 | Loss: 0.00002316
Iteration 141/1000 | Loss: 0.00002316
Iteration 142/1000 | Loss: 0.00002316
Iteration 143/1000 | Loss: 0.00002316
Iteration 144/1000 | Loss: 0.00002316
Iteration 145/1000 | Loss: 0.00002316
Iteration 146/1000 | Loss: 0.00002316
Iteration 147/1000 | Loss: 0.00002315
Iteration 148/1000 | Loss: 0.00002315
Iteration 149/1000 | Loss: 0.00002315
Iteration 150/1000 | Loss: 0.00002315
Iteration 151/1000 | Loss: 0.00002315
Iteration 152/1000 | Loss: 0.00002315
Iteration 153/1000 | Loss: 0.00002315
Iteration 154/1000 | Loss: 0.00002315
Iteration 155/1000 | Loss: 0.00002314
Iteration 156/1000 | Loss: 0.00002314
Iteration 157/1000 | Loss: 0.00002314
Iteration 158/1000 | Loss: 0.00002314
Iteration 159/1000 | Loss: 0.00002314
Iteration 160/1000 | Loss: 0.00002314
Iteration 161/1000 | Loss: 0.00002314
Iteration 162/1000 | Loss: 0.00002314
Iteration 163/1000 | Loss: 0.00002314
Iteration 164/1000 | Loss: 0.00002314
Iteration 165/1000 | Loss: 0.00002314
Iteration 166/1000 | Loss: 0.00002313
Iteration 167/1000 | Loss: 0.00002313
Iteration 168/1000 | Loss: 0.00002313
Iteration 169/1000 | Loss: 0.00002313
Iteration 170/1000 | Loss: 0.00002313
Iteration 171/1000 | Loss: 0.00002313
Iteration 172/1000 | Loss: 0.00002313
Iteration 173/1000 | Loss: 0.00002313
Iteration 174/1000 | Loss: 0.00002313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.3134400180424564e-05, 2.3134400180424564e-05, 2.3134400180424564e-05, 2.3134400180424564e-05, 2.3134400180424564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3134400180424564e-05

Optimization complete. Final v2v error: 3.8715949058532715 mm

Highest mean error: 6.20768928527832 mm for frame 158

Lowest mean error: 2.8704993724823 mm for frame 239

Saving results

Total time: 99.7373251914978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991719
Iteration 2/25 | Loss: 0.00991719
Iteration 3/25 | Loss: 0.00991719
Iteration 4/25 | Loss: 0.00273480
Iteration 5/25 | Loss: 0.00209051
Iteration 6/25 | Loss: 0.00197575
Iteration 7/25 | Loss: 0.00195959
Iteration 8/25 | Loss: 0.00192352
Iteration 9/25 | Loss: 0.00190781
Iteration 10/25 | Loss: 0.00184933
Iteration 11/25 | Loss: 0.00176569
Iteration 12/25 | Loss: 0.00172336
Iteration 13/25 | Loss: 0.00169520
Iteration 14/25 | Loss: 0.00170166
Iteration 15/25 | Loss: 0.00169801
Iteration 16/25 | Loss: 0.00163239
Iteration 17/25 | Loss: 0.00162479
Iteration 18/25 | Loss: 0.00161962
Iteration 19/25 | Loss: 0.00160116
Iteration 20/25 | Loss: 0.00158982
Iteration 21/25 | Loss: 0.00158013
Iteration 22/25 | Loss: 0.00157551
Iteration 23/25 | Loss: 0.00157807
Iteration 24/25 | Loss: 0.00156764
Iteration 25/25 | Loss: 0.00156623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33575082
Iteration 2/25 | Loss: 0.00753563
Iteration 3/25 | Loss: 0.00225899
Iteration 4/25 | Loss: 0.00225888
Iteration 5/25 | Loss: 0.00225888
Iteration 6/25 | Loss: 0.00225888
Iteration 7/25 | Loss: 0.00225888
Iteration 8/25 | Loss: 0.00225888
Iteration 9/25 | Loss: 0.00225888
Iteration 10/25 | Loss: 0.00225888
Iteration 11/25 | Loss: 0.00225888
Iteration 12/25 | Loss: 0.00225888
Iteration 13/25 | Loss: 0.00225888
Iteration 14/25 | Loss: 0.00225888
Iteration 15/25 | Loss: 0.00225888
Iteration 16/25 | Loss: 0.00225888
Iteration 17/25 | Loss: 0.00225888
Iteration 18/25 | Loss: 0.00225888
Iteration 19/25 | Loss: 0.00225888
Iteration 20/25 | Loss: 0.00225888
Iteration 21/25 | Loss: 0.00225888
Iteration 22/25 | Loss: 0.00225888
Iteration 23/25 | Loss: 0.00225888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002258879831060767, 0.002258879831060767, 0.002258879831060767, 0.002258879831060767, 0.002258879831060767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002258879831060767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225888
Iteration 2/1000 | Loss: 0.00394824
Iteration 3/1000 | Loss: 0.00470135
Iteration 4/1000 | Loss: 0.00341909
Iteration 5/1000 | Loss: 0.00242202
Iteration 6/1000 | Loss: 0.00428894
Iteration 7/1000 | Loss: 0.00093585
Iteration 8/1000 | Loss: 0.00131980
Iteration 9/1000 | Loss: 0.00291316
Iteration 10/1000 | Loss: 0.00138746
Iteration 11/1000 | Loss: 0.00203314
Iteration 12/1000 | Loss: 0.00184110
Iteration 13/1000 | Loss: 0.00200791
Iteration 14/1000 | Loss: 0.00155497
Iteration 15/1000 | Loss: 0.00338287
Iteration 16/1000 | Loss: 0.00326349
Iteration 17/1000 | Loss: 0.00214755
Iteration 18/1000 | Loss: 0.00171692
Iteration 19/1000 | Loss: 0.00082141
Iteration 20/1000 | Loss: 0.00124483
Iteration 21/1000 | Loss: 0.00460804
Iteration 22/1000 | Loss: 0.00549419
Iteration 23/1000 | Loss: 0.00586432
Iteration 24/1000 | Loss: 0.00548834
Iteration 25/1000 | Loss: 0.00523869
Iteration 26/1000 | Loss: 0.00523864
Iteration 27/1000 | Loss: 0.00251667
Iteration 28/1000 | Loss: 0.00286854
Iteration 29/1000 | Loss: 0.00109201
Iteration 30/1000 | Loss: 0.00140281
Iteration 31/1000 | Loss: 0.00149749
Iteration 32/1000 | Loss: 0.00215421
Iteration 33/1000 | Loss: 0.00154783
Iteration 34/1000 | Loss: 0.00123677
Iteration 35/1000 | Loss: 0.00081218
Iteration 36/1000 | Loss: 0.00086377
Iteration 37/1000 | Loss: 0.00249886
Iteration 38/1000 | Loss: 0.00073799
Iteration 39/1000 | Loss: 0.00032405
Iteration 40/1000 | Loss: 0.00055198
Iteration 41/1000 | Loss: 0.00107884
Iteration 42/1000 | Loss: 0.00039108
Iteration 43/1000 | Loss: 0.00097305
Iteration 44/1000 | Loss: 0.00059832
Iteration 45/1000 | Loss: 0.00125015
Iteration 46/1000 | Loss: 0.00063912
Iteration 47/1000 | Loss: 0.00029972
Iteration 48/1000 | Loss: 0.00041600
Iteration 49/1000 | Loss: 0.00061704
Iteration 50/1000 | Loss: 0.00123776
Iteration 51/1000 | Loss: 0.00108969
Iteration 52/1000 | Loss: 0.00111710
Iteration 53/1000 | Loss: 0.00018646
Iteration 54/1000 | Loss: 0.00030294
Iteration 55/1000 | Loss: 0.00059534
Iteration 56/1000 | Loss: 0.00038781
Iteration 57/1000 | Loss: 0.00024101
Iteration 58/1000 | Loss: 0.00053189
Iteration 59/1000 | Loss: 0.00032207
Iteration 60/1000 | Loss: 0.00030525
Iteration 61/1000 | Loss: 0.00029289
Iteration 62/1000 | Loss: 0.00050278
Iteration 63/1000 | Loss: 0.00027817
Iteration 64/1000 | Loss: 0.00051299
Iteration 65/1000 | Loss: 0.00056528
Iteration 66/1000 | Loss: 0.00037736
Iteration 67/1000 | Loss: 0.00080711
Iteration 68/1000 | Loss: 0.00027656
Iteration 69/1000 | Loss: 0.00101297
Iteration 70/1000 | Loss: 0.00134725
Iteration 71/1000 | Loss: 0.00103023
Iteration 72/1000 | Loss: 0.00094515
Iteration 73/1000 | Loss: 0.00108662
Iteration 74/1000 | Loss: 0.00103298
Iteration 75/1000 | Loss: 0.00095322
Iteration 76/1000 | Loss: 0.00102351
Iteration 77/1000 | Loss: 0.00132841
Iteration 78/1000 | Loss: 0.00103692
Iteration 79/1000 | Loss: 0.00125485
Iteration 80/1000 | Loss: 0.00146711
Iteration 81/1000 | Loss: 0.00152408
Iteration 82/1000 | Loss: 0.00168878
Iteration 83/1000 | Loss: 0.00054040
Iteration 84/1000 | Loss: 0.00104065
Iteration 85/1000 | Loss: 0.00083913
Iteration 86/1000 | Loss: 0.00106744
Iteration 87/1000 | Loss: 0.00054537
Iteration 88/1000 | Loss: 0.00067428
Iteration 89/1000 | Loss: 0.00108763
Iteration 90/1000 | Loss: 0.00068967
Iteration 91/1000 | Loss: 0.00114151
Iteration 92/1000 | Loss: 0.00116283
Iteration 93/1000 | Loss: 0.00087456
Iteration 94/1000 | Loss: 0.00063741
Iteration 95/1000 | Loss: 0.00113709
Iteration 96/1000 | Loss: 0.00097488
Iteration 97/1000 | Loss: 0.00112009
Iteration 98/1000 | Loss: 0.00118567
Iteration 99/1000 | Loss: 0.00114520
Iteration 100/1000 | Loss: 0.00226310
Iteration 101/1000 | Loss: 0.00091403
Iteration 102/1000 | Loss: 0.00095994
Iteration 103/1000 | Loss: 0.00048455
Iteration 104/1000 | Loss: 0.00049037
Iteration 105/1000 | Loss: 0.00088210
Iteration 106/1000 | Loss: 0.00104601
Iteration 107/1000 | Loss: 0.00108037
Iteration 108/1000 | Loss: 0.00047655
Iteration 109/1000 | Loss: 0.00087895
Iteration 110/1000 | Loss: 0.00067988
Iteration 111/1000 | Loss: 0.00097176
Iteration 112/1000 | Loss: 0.00072500
Iteration 113/1000 | Loss: 0.00159731
Iteration 114/1000 | Loss: 0.00096587
Iteration 115/1000 | Loss: 0.00038434
Iteration 116/1000 | Loss: 0.00025168
Iteration 117/1000 | Loss: 0.00041718
Iteration 118/1000 | Loss: 0.00043020
Iteration 119/1000 | Loss: 0.00045152
Iteration 120/1000 | Loss: 0.00033957
Iteration 121/1000 | Loss: 0.00032342
Iteration 122/1000 | Loss: 0.00018381
Iteration 123/1000 | Loss: 0.00033373
Iteration 124/1000 | Loss: 0.00004455
Iteration 125/1000 | Loss: 0.00027036
Iteration 126/1000 | Loss: 0.00031932
Iteration 127/1000 | Loss: 0.00053331
Iteration 128/1000 | Loss: 0.00045569
Iteration 129/1000 | Loss: 0.00030556
Iteration 130/1000 | Loss: 0.00027626
Iteration 131/1000 | Loss: 0.00012002
Iteration 132/1000 | Loss: 0.00030394
Iteration 133/1000 | Loss: 0.00031996
Iteration 134/1000 | Loss: 0.00031556
Iteration 135/1000 | Loss: 0.00024243
Iteration 136/1000 | Loss: 0.00027333
Iteration 137/1000 | Loss: 0.00020448
Iteration 138/1000 | Loss: 0.00017852
Iteration 139/1000 | Loss: 0.00020303
Iteration 140/1000 | Loss: 0.00016512
Iteration 141/1000 | Loss: 0.00016721
Iteration 142/1000 | Loss: 0.00027751
Iteration 143/1000 | Loss: 0.00022548
Iteration 144/1000 | Loss: 0.00024744
Iteration 145/1000 | Loss: 0.00026438
Iteration 146/1000 | Loss: 0.00054639
Iteration 147/1000 | Loss: 0.00009123
Iteration 148/1000 | Loss: 0.00009952
Iteration 149/1000 | Loss: 0.00019362
Iteration 150/1000 | Loss: 0.00015408
Iteration 151/1000 | Loss: 0.00026917
Iteration 152/1000 | Loss: 0.00021581
Iteration 153/1000 | Loss: 0.00007851
Iteration 154/1000 | Loss: 0.00010566
Iteration 155/1000 | Loss: 0.00012300
Iteration 156/1000 | Loss: 0.00007542
Iteration 157/1000 | Loss: 0.00015021
Iteration 158/1000 | Loss: 0.00006552
Iteration 159/1000 | Loss: 0.00012533
Iteration 160/1000 | Loss: 0.00008118
Iteration 161/1000 | Loss: 0.00011601
Iteration 162/1000 | Loss: 0.00025149
Iteration 163/1000 | Loss: 0.00012729
Iteration 164/1000 | Loss: 0.00007549
Iteration 165/1000 | Loss: 0.00012716
Iteration 166/1000 | Loss: 0.00011160
Iteration 167/1000 | Loss: 0.00010731
Iteration 168/1000 | Loss: 0.00013815
Iteration 169/1000 | Loss: 0.00011051
Iteration 170/1000 | Loss: 0.00022104
Iteration 171/1000 | Loss: 0.00021321
Iteration 172/1000 | Loss: 0.00022838
Iteration 173/1000 | Loss: 0.00049973
Iteration 174/1000 | Loss: 0.00028888
Iteration 175/1000 | Loss: 0.00004827
Iteration 176/1000 | Loss: 0.00003768
Iteration 177/1000 | Loss: 0.00003554
Iteration 178/1000 | Loss: 0.00003431
Iteration 179/1000 | Loss: 0.00003472
Iteration 180/1000 | Loss: 0.00011174
Iteration 181/1000 | Loss: 0.00068130
Iteration 182/1000 | Loss: 0.00017515
Iteration 183/1000 | Loss: 0.00007920
Iteration 184/1000 | Loss: 0.00012310
Iteration 185/1000 | Loss: 0.00019401
Iteration 186/1000 | Loss: 0.00025394
Iteration 187/1000 | Loss: 0.00004311
Iteration 188/1000 | Loss: 0.00026045
Iteration 189/1000 | Loss: 0.00019101
Iteration 190/1000 | Loss: 0.00009372
Iteration 191/1000 | Loss: 0.00003305
Iteration 192/1000 | Loss: 0.00003390
Iteration 193/1000 | Loss: 0.00003120
Iteration 194/1000 | Loss: 0.00003205
Iteration 195/1000 | Loss: 0.00002990
Iteration 196/1000 | Loss: 0.00002932
Iteration 197/1000 | Loss: 0.00002859
Iteration 198/1000 | Loss: 0.00002840
Iteration 199/1000 | Loss: 0.00024636
Iteration 200/1000 | Loss: 0.00012972
Iteration 201/1000 | Loss: 0.00028644
Iteration 202/1000 | Loss: 0.00009814
Iteration 203/1000 | Loss: 0.00013791
Iteration 204/1000 | Loss: 0.00011427
Iteration 205/1000 | Loss: 0.00010075
Iteration 206/1000 | Loss: 0.00012222
Iteration 207/1000 | Loss: 0.00003398
Iteration 208/1000 | Loss: 0.00003642
Iteration 209/1000 | Loss: 0.00003037
Iteration 210/1000 | Loss: 0.00002979
Iteration 211/1000 | Loss: 0.00006124
Iteration 212/1000 | Loss: 0.00002908
Iteration 213/1000 | Loss: 0.00002796
Iteration 214/1000 | Loss: 0.00002766
Iteration 215/1000 | Loss: 0.00003075
Iteration 216/1000 | Loss: 0.00002723
Iteration 217/1000 | Loss: 0.00022109
Iteration 218/1000 | Loss: 0.00009797
Iteration 219/1000 | Loss: 0.00002765
Iteration 220/1000 | Loss: 0.00002708
Iteration 221/1000 | Loss: 0.00023485
Iteration 222/1000 | Loss: 0.00008772
Iteration 223/1000 | Loss: 0.00003259
Iteration 224/1000 | Loss: 0.00002759
Iteration 225/1000 | Loss: 0.00002722
Iteration 226/1000 | Loss: 0.00022314
Iteration 227/1000 | Loss: 0.00029283
Iteration 228/1000 | Loss: 0.00017147
Iteration 229/1000 | Loss: 0.00010465
Iteration 230/1000 | Loss: 0.00010785
Iteration 231/1000 | Loss: 0.00004147
Iteration 232/1000 | Loss: 0.00003235
Iteration 233/1000 | Loss: 0.00002952
Iteration 234/1000 | Loss: 0.00006201
Iteration 235/1000 | Loss: 0.00002741
Iteration 236/1000 | Loss: 0.00002641
Iteration 237/1000 | Loss: 0.00002588
Iteration 238/1000 | Loss: 0.00002555
Iteration 239/1000 | Loss: 0.00002516
Iteration 240/1000 | Loss: 0.00002498
Iteration 241/1000 | Loss: 0.00002494
Iteration 242/1000 | Loss: 0.00002494
Iteration 243/1000 | Loss: 0.00002493
Iteration 244/1000 | Loss: 0.00002493
Iteration 245/1000 | Loss: 0.00002492
Iteration 246/1000 | Loss: 0.00002492
Iteration 247/1000 | Loss: 0.00002491
Iteration 248/1000 | Loss: 0.00002491
Iteration 249/1000 | Loss: 0.00002490
Iteration 250/1000 | Loss: 0.00002490
Iteration 251/1000 | Loss: 0.00002489
Iteration 252/1000 | Loss: 0.00002486
Iteration 253/1000 | Loss: 0.00002484
Iteration 254/1000 | Loss: 0.00002483
Iteration 255/1000 | Loss: 0.00002481
Iteration 256/1000 | Loss: 0.00002462
Iteration 257/1000 | Loss: 0.00002444
Iteration 258/1000 | Loss: 0.00002435
Iteration 259/1000 | Loss: 0.00002431
Iteration 260/1000 | Loss: 0.00002420
Iteration 261/1000 | Loss: 0.00006391
Iteration 262/1000 | Loss: 0.00002535
Iteration 263/1000 | Loss: 0.00003445
Iteration 264/1000 | Loss: 0.00002397
Iteration 265/1000 | Loss: 0.00002395
Iteration 266/1000 | Loss: 0.00002395
Iteration 267/1000 | Loss: 0.00002394
Iteration 268/1000 | Loss: 0.00002394
Iteration 269/1000 | Loss: 0.00002394
Iteration 270/1000 | Loss: 0.00002392
Iteration 271/1000 | Loss: 0.00002392
Iteration 272/1000 | Loss: 0.00002391
Iteration 273/1000 | Loss: 0.00002391
Iteration 274/1000 | Loss: 0.00002391
Iteration 275/1000 | Loss: 0.00002391
Iteration 276/1000 | Loss: 0.00002389
Iteration 277/1000 | Loss: 0.00002389
Iteration 278/1000 | Loss: 0.00002389
Iteration 279/1000 | Loss: 0.00002389
Iteration 280/1000 | Loss: 0.00002389
Iteration 281/1000 | Loss: 0.00002389
Iteration 282/1000 | Loss: 0.00002389
Iteration 283/1000 | Loss: 0.00002389
Iteration 284/1000 | Loss: 0.00002388
Iteration 285/1000 | Loss: 0.00002388
Iteration 286/1000 | Loss: 0.00002388
Iteration 287/1000 | Loss: 0.00002388
Iteration 288/1000 | Loss: 0.00002388
Iteration 289/1000 | Loss: 0.00002388
Iteration 290/1000 | Loss: 0.00002388
Iteration 291/1000 | Loss: 0.00002388
Iteration 292/1000 | Loss: 0.00002388
Iteration 293/1000 | Loss: 0.00002387
Iteration 294/1000 | Loss: 0.00002387
Iteration 295/1000 | Loss: 0.00002386
Iteration 296/1000 | Loss: 0.00002386
Iteration 297/1000 | Loss: 0.00002386
Iteration 298/1000 | Loss: 0.00002386
Iteration 299/1000 | Loss: 0.00002386
Iteration 300/1000 | Loss: 0.00002385
Iteration 301/1000 | Loss: 0.00002385
Iteration 302/1000 | Loss: 0.00002385
Iteration 303/1000 | Loss: 0.00002384
Iteration 304/1000 | Loss: 0.00002384
Iteration 305/1000 | Loss: 0.00002384
Iteration 306/1000 | Loss: 0.00002384
Iteration 307/1000 | Loss: 0.00002383
Iteration 308/1000 | Loss: 0.00026186
Iteration 309/1000 | Loss: 0.00047397
Iteration 310/1000 | Loss: 0.00022217
Iteration 311/1000 | Loss: 0.00041733
Iteration 312/1000 | Loss: 0.00024863
Iteration 313/1000 | Loss: 0.00013877
Iteration 314/1000 | Loss: 0.00058784
Iteration 315/1000 | Loss: 0.00017203
Iteration 316/1000 | Loss: 0.00014917
Iteration 317/1000 | Loss: 0.00003531
Iteration 318/1000 | Loss: 0.00002701
Iteration 319/1000 | Loss: 0.00002453
Iteration 320/1000 | Loss: 0.00004342
Iteration 321/1000 | Loss: 0.00002232
Iteration 322/1000 | Loss: 0.00002168
Iteration 323/1000 | Loss: 0.00002131
Iteration 324/1000 | Loss: 0.00002107
Iteration 325/1000 | Loss: 0.00002084
Iteration 326/1000 | Loss: 0.00002068
Iteration 327/1000 | Loss: 0.00002061
Iteration 328/1000 | Loss: 0.00002061
Iteration 329/1000 | Loss: 0.00007742
Iteration 330/1000 | Loss: 0.00002058
Iteration 331/1000 | Loss: 0.00002045
Iteration 332/1000 | Loss: 0.00002043
Iteration 333/1000 | Loss: 0.00002043
Iteration 334/1000 | Loss: 0.00002042
Iteration 335/1000 | Loss: 0.00002042
Iteration 336/1000 | Loss: 0.00002040
Iteration 337/1000 | Loss: 0.00002040
Iteration 338/1000 | Loss: 0.00002035
Iteration 339/1000 | Loss: 0.00002035
Iteration 340/1000 | Loss: 0.00002034
Iteration 341/1000 | Loss: 0.00002033
Iteration 342/1000 | Loss: 0.00002033
Iteration 343/1000 | Loss: 0.00002032
Iteration 344/1000 | Loss: 0.00002032
Iteration 345/1000 | Loss: 0.00002031
Iteration 346/1000 | Loss: 0.00002030
Iteration 347/1000 | Loss: 0.00002030
Iteration 348/1000 | Loss: 0.00002030
Iteration 349/1000 | Loss: 0.00002030
Iteration 350/1000 | Loss: 0.00002030
Iteration 351/1000 | Loss: 0.00002029
Iteration 352/1000 | Loss: 0.00002029
Iteration 353/1000 | Loss: 0.00002029
Iteration 354/1000 | Loss: 0.00002029
Iteration 355/1000 | Loss: 0.00002028
Iteration 356/1000 | Loss: 0.00002028
Iteration 357/1000 | Loss: 0.00002028
Iteration 358/1000 | Loss: 0.00002028
Iteration 359/1000 | Loss: 0.00002028
Iteration 360/1000 | Loss: 0.00002028
Iteration 361/1000 | Loss: 0.00002028
Iteration 362/1000 | Loss: 0.00002028
Iteration 363/1000 | Loss: 0.00002028
Iteration 364/1000 | Loss: 0.00002028
Iteration 365/1000 | Loss: 0.00002027
Iteration 366/1000 | Loss: 0.00002027
Iteration 367/1000 | Loss: 0.00002027
Iteration 368/1000 | Loss: 0.00002027
Iteration 369/1000 | Loss: 0.00002027
Iteration 370/1000 | Loss: 0.00002027
Iteration 371/1000 | Loss: 0.00002027
Iteration 372/1000 | Loss: 0.00002027
Iteration 373/1000 | Loss: 0.00002027
Iteration 374/1000 | Loss: 0.00002026
Iteration 375/1000 | Loss: 0.00002026
Iteration 376/1000 | Loss: 0.00002026
Iteration 377/1000 | Loss: 0.00002026
Iteration 378/1000 | Loss: 0.00002026
Iteration 379/1000 | Loss: 0.00002026
Iteration 380/1000 | Loss: 0.00002026
Iteration 381/1000 | Loss: 0.00002026
Iteration 382/1000 | Loss: 0.00002026
Iteration 383/1000 | Loss: 0.00002026
Iteration 384/1000 | Loss: 0.00002026
Iteration 385/1000 | Loss: 0.00002026
Iteration 386/1000 | Loss: 0.00002026
Iteration 387/1000 | Loss: 0.00002026
Iteration 388/1000 | Loss: 0.00002026
Iteration 389/1000 | Loss: 0.00002025
Iteration 390/1000 | Loss: 0.00002025
Iteration 391/1000 | Loss: 0.00002025
Iteration 392/1000 | Loss: 0.00002025
Iteration 393/1000 | Loss: 0.00002025
Iteration 394/1000 | Loss: 0.00002025
Iteration 395/1000 | Loss: 0.00002024
Iteration 396/1000 | Loss: 0.00002024
Iteration 397/1000 | Loss: 0.00002024
Iteration 398/1000 | Loss: 0.00002024
Iteration 399/1000 | Loss: 0.00002024
Iteration 400/1000 | Loss: 0.00002024
Iteration 401/1000 | Loss: 0.00002023
Iteration 402/1000 | Loss: 0.00002023
Iteration 403/1000 | Loss: 0.00002023
Iteration 404/1000 | Loss: 0.00002023
Iteration 405/1000 | Loss: 0.00002023
Iteration 406/1000 | Loss: 0.00002023
Iteration 407/1000 | Loss: 0.00002023
Iteration 408/1000 | Loss: 0.00002023
Iteration 409/1000 | Loss: 0.00002023
Iteration 410/1000 | Loss: 0.00002023
Iteration 411/1000 | Loss: 0.00002022
Iteration 412/1000 | Loss: 0.00002022
Iteration 413/1000 | Loss: 0.00002022
Iteration 414/1000 | Loss: 0.00002022
Iteration 415/1000 | Loss: 0.00002022
Iteration 416/1000 | Loss: 0.00002022
Iteration 417/1000 | Loss: 0.00002022
Iteration 418/1000 | Loss: 0.00002022
Iteration 419/1000 | Loss: 0.00002022
Iteration 420/1000 | Loss: 0.00002022
Iteration 421/1000 | Loss: 0.00002022
Iteration 422/1000 | Loss: 0.00002021
Iteration 423/1000 | Loss: 0.00002021
Iteration 424/1000 | Loss: 0.00002021
Iteration 425/1000 | Loss: 0.00002021
Iteration 426/1000 | Loss: 0.00002021
Iteration 427/1000 | Loss: 0.00002021
Iteration 428/1000 | Loss: 0.00002021
Iteration 429/1000 | Loss: 0.00002021
Iteration 430/1000 | Loss: 0.00002021
Iteration 431/1000 | Loss: 0.00002021
Iteration 432/1000 | Loss: 0.00002021
Iteration 433/1000 | Loss: 0.00002021
Iteration 434/1000 | Loss: 0.00002021
Iteration 435/1000 | Loss: 0.00002021
Iteration 436/1000 | Loss: 0.00002021
Iteration 437/1000 | Loss: 0.00002021
Iteration 438/1000 | Loss: 0.00002020
Iteration 439/1000 | Loss: 0.00002020
Iteration 440/1000 | Loss: 0.00002020
Iteration 441/1000 | Loss: 0.00002020
Iteration 442/1000 | Loss: 0.00002020
Iteration 443/1000 | Loss: 0.00002020
Iteration 444/1000 | Loss: 0.00002020
Iteration 445/1000 | Loss: 0.00002020
Iteration 446/1000 | Loss: 0.00002020
Iteration 447/1000 | Loss: 0.00002020
Iteration 448/1000 | Loss: 0.00002020
Iteration 449/1000 | Loss: 0.00002019
Iteration 450/1000 | Loss: 0.00002019
Iteration 451/1000 | Loss: 0.00002019
Iteration 452/1000 | Loss: 0.00002019
Iteration 453/1000 | Loss: 0.00002019
Iteration 454/1000 | Loss: 0.00002019
Iteration 455/1000 | Loss: 0.00002019
Iteration 456/1000 | Loss: 0.00002019
Iteration 457/1000 | Loss: 0.00002019
Iteration 458/1000 | Loss: 0.00002019
Iteration 459/1000 | Loss: 0.00002018
Iteration 460/1000 | Loss: 0.00002018
Iteration 461/1000 | Loss: 0.00002018
Iteration 462/1000 | Loss: 0.00002018
Iteration 463/1000 | Loss: 0.00002017
Iteration 464/1000 | Loss: 0.00002017
Iteration 465/1000 | Loss: 0.00002017
Iteration 466/1000 | Loss: 0.00002015
Iteration 467/1000 | Loss: 0.00002015
Iteration 468/1000 | Loss: 0.00002015
Iteration 469/1000 | Loss: 0.00002015
Iteration 470/1000 | Loss: 0.00002015
Iteration 471/1000 | Loss: 0.00002015
Iteration 472/1000 | Loss: 0.00002014
Iteration 473/1000 | Loss: 0.00002014
Iteration 474/1000 | Loss: 0.00002014
Iteration 475/1000 | Loss: 0.00002014
Iteration 476/1000 | Loss: 0.00002014
Iteration 477/1000 | Loss: 0.00002014
Iteration 478/1000 | Loss: 0.00002014
Iteration 479/1000 | Loss: 0.00002014
Iteration 480/1000 | Loss: 0.00002013
Iteration 481/1000 | Loss: 0.00002013
Iteration 482/1000 | Loss: 0.00002013
Iteration 483/1000 | Loss: 0.00002013
Iteration 484/1000 | Loss: 0.00002013
Iteration 485/1000 | Loss: 0.00002013
Iteration 486/1000 | Loss: 0.00002013
Iteration 487/1000 | Loss: 0.00002013
Iteration 488/1000 | Loss: 0.00002013
Iteration 489/1000 | Loss: 0.00002013
Iteration 490/1000 | Loss: 0.00002013
Iteration 491/1000 | Loss: 0.00002013
Iteration 492/1000 | Loss: 0.00002013
Iteration 493/1000 | Loss: 0.00002013
Iteration 494/1000 | Loss: 0.00002012
Iteration 495/1000 | Loss: 0.00002012
Iteration 496/1000 | Loss: 0.00002012
Iteration 497/1000 | Loss: 0.00002011
Iteration 498/1000 | Loss: 0.00002011
Iteration 499/1000 | Loss: 0.00002011
Iteration 500/1000 | Loss: 0.00002011
Iteration 501/1000 | Loss: 0.00002010
Iteration 502/1000 | Loss: 0.00002010
Iteration 503/1000 | Loss: 0.00002010
Iteration 504/1000 | Loss: 0.00002009
Iteration 505/1000 | Loss: 0.00002009
Iteration 506/1000 | Loss: 0.00002008
Iteration 507/1000 | Loss: 0.00002008
Iteration 508/1000 | Loss: 0.00002008
Iteration 509/1000 | Loss: 0.00002008
Iteration 510/1000 | Loss: 0.00002008
Iteration 511/1000 | Loss: 0.00002007
Iteration 512/1000 | Loss: 0.00002007
Iteration 513/1000 | Loss: 0.00002007
Iteration 514/1000 | Loss: 0.00002006
Iteration 515/1000 | Loss: 0.00002006
Iteration 516/1000 | Loss: 0.00002006
Iteration 517/1000 | Loss: 0.00002006
Iteration 518/1000 | Loss: 0.00002005
Iteration 519/1000 | Loss: 0.00002441
Iteration 520/1000 | Loss: 0.00003869
Iteration 521/1000 | Loss: 0.00002512
Iteration 522/1000 | Loss: 0.00002043
Iteration 523/1000 | Loss: 0.00002131
Iteration 524/1000 | Loss: 0.00002000
Iteration 525/1000 | Loss: 0.00001999
Iteration 526/1000 | Loss: 0.00001998
Iteration 527/1000 | Loss: 0.00001997
Iteration 528/1000 | Loss: 0.00001997
Iteration 529/1000 | Loss: 0.00001997
Iteration 530/1000 | Loss: 0.00001997
Iteration 531/1000 | Loss: 0.00001997
Iteration 532/1000 | Loss: 0.00001996
Iteration 533/1000 | Loss: 0.00001996
Iteration 534/1000 | Loss: 0.00001996
Iteration 535/1000 | Loss: 0.00001996
Iteration 536/1000 | Loss: 0.00001996
Iteration 537/1000 | Loss: 0.00001996
Iteration 538/1000 | Loss: 0.00001995
Iteration 539/1000 | Loss: 0.00001995
Iteration 540/1000 | Loss: 0.00001994
Iteration 541/1000 | Loss: 0.00001993
Iteration 542/1000 | Loss: 0.00001993
Iteration 543/1000 | Loss: 0.00001993
Iteration 544/1000 | Loss: 0.00001992
Iteration 545/1000 | Loss: 0.00001991
Iteration 546/1000 | Loss: 0.00001991
Iteration 547/1000 | Loss: 0.00001987
Iteration 548/1000 | Loss: 0.00001981
Iteration 549/1000 | Loss: 0.00001981
Iteration 550/1000 | Loss: 0.00001980
Iteration 551/1000 | Loss: 0.00001980
Iteration 552/1000 | Loss: 0.00001980
Iteration 553/1000 | Loss: 0.00001980
Iteration 554/1000 | Loss: 0.00001980
Iteration 555/1000 | Loss: 0.00001979
Iteration 556/1000 | Loss: 0.00001979
Iteration 557/1000 | Loss: 0.00001979
Iteration 558/1000 | Loss: 0.00001979
Iteration 559/1000 | Loss: 0.00001978
Iteration 560/1000 | Loss: 0.00001978
Iteration 561/1000 | Loss: 0.00001978
Iteration 562/1000 | Loss: 0.00001978
Iteration 563/1000 | Loss: 0.00001978
Iteration 564/1000 | Loss: 0.00001978
Iteration 565/1000 | Loss: 0.00001978
Iteration 566/1000 | Loss: 0.00001978
Iteration 567/1000 | Loss: 0.00001978
Iteration 568/1000 | Loss: 0.00001977
Iteration 569/1000 | Loss: 0.00001977
Iteration 570/1000 | Loss: 0.00001977
Iteration 571/1000 | Loss: 0.00001977
Iteration 572/1000 | Loss: 0.00001977
Iteration 573/1000 | Loss: 0.00001977
Iteration 574/1000 | Loss: 0.00001977
Iteration 575/1000 | Loss: 0.00001977
Iteration 576/1000 | Loss: 0.00001977
Iteration 577/1000 | Loss: 0.00001977
Iteration 578/1000 | Loss: 0.00001977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 578. Stopping optimization.
Last 5 losses: [1.9771754523389973e-05, 1.9771754523389973e-05, 1.9771754523389973e-05, 1.9771754523389973e-05, 1.9771754523389973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9771754523389973e-05

Optimization complete. Final v2v error: 3.20165753364563 mm

Highest mean error: 10.174659729003906 mm for frame 36

Lowest mean error: 2.6921544075012207 mm for frame 162

Saving results

Total time: 505.80551314353943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00641012
Iteration 2/25 | Loss: 0.00146772
Iteration 3/25 | Loss: 0.00132195
Iteration 4/25 | Loss: 0.00128330
Iteration 5/25 | Loss: 0.00127640
Iteration 6/25 | Loss: 0.00127046
Iteration 7/25 | Loss: 0.00126615
Iteration 8/25 | Loss: 0.00125821
Iteration 9/25 | Loss: 0.00125329
Iteration 10/25 | Loss: 0.00125075
Iteration 11/25 | Loss: 0.00125018
Iteration 12/25 | Loss: 0.00124779
Iteration 13/25 | Loss: 0.00124726
Iteration 14/25 | Loss: 0.00124711
Iteration 15/25 | Loss: 0.00124710
Iteration 16/25 | Loss: 0.00124710
Iteration 17/25 | Loss: 0.00124710
Iteration 18/25 | Loss: 0.00124710
Iteration 19/25 | Loss: 0.00124710
Iteration 20/25 | Loss: 0.00124710
Iteration 21/25 | Loss: 0.00124710
Iteration 22/25 | Loss: 0.00124710
Iteration 23/25 | Loss: 0.00124710
Iteration 24/25 | Loss: 0.00124709
Iteration 25/25 | Loss: 0.00124709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38725102
Iteration 2/25 | Loss: 0.00119957
Iteration 3/25 | Loss: 0.00119957
Iteration 4/25 | Loss: 0.00119957
Iteration 5/25 | Loss: 0.00119957
Iteration 6/25 | Loss: 0.00119957
Iteration 7/25 | Loss: 0.00119957
Iteration 8/25 | Loss: 0.00119957
Iteration 9/25 | Loss: 0.00119957
Iteration 10/25 | Loss: 0.00119957
Iteration 11/25 | Loss: 0.00119957
Iteration 12/25 | Loss: 0.00119957
Iteration 13/25 | Loss: 0.00119957
Iteration 14/25 | Loss: 0.00119957
Iteration 15/25 | Loss: 0.00119957
Iteration 16/25 | Loss: 0.00119957
Iteration 17/25 | Loss: 0.00119957
Iteration 18/25 | Loss: 0.00119957
Iteration 19/25 | Loss: 0.00119957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011995657114312053, 0.0011995657114312053, 0.0011995657114312053, 0.0011995657114312053, 0.0011995657114312053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011995657114312053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119957
Iteration 2/1000 | Loss: 0.00007047
Iteration 3/1000 | Loss: 0.00004953
Iteration 4/1000 | Loss: 0.00004152
Iteration 5/1000 | Loss: 0.00003756
Iteration 6/1000 | Loss: 0.00003512
Iteration 7/1000 | Loss: 0.00003342
Iteration 8/1000 | Loss: 0.00003234
Iteration 9/1000 | Loss: 0.00003133
Iteration 10/1000 | Loss: 0.00003058
Iteration 11/1000 | Loss: 0.00008860
Iteration 12/1000 | Loss: 0.00002892
Iteration 13/1000 | Loss: 0.00002773
Iteration 14/1000 | Loss: 0.00002691
Iteration 15/1000 | Loss: 0.00002628
Iteration 16/1000 | Loss: 0.00002578
Iteration 17/1000 | Loss: 0.00002547
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002516
Iteration 20/1000 | Loss: 0.00002494
Iteration 21/1000 | Loss: 0.00002476
Iteration 22/1000 | Loss: 0.00002463
Iteration 23/1000 | Loss: 0.00002462
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002435
Iteration 26/1000 | Loss: 0.00002433
Iteration 27/1000 | Loss: 0.00002423
Iteration 28/1000 | Loss: 0.00046243
Iteration 29/1000 | Loss: 0.00002467
Iteration 30/1000 | Loss: 0.00002350
Iteration 31/1000 | Loss: 0.00002289
Iteration 32/1000 | Loss: 0.00002255
Iteration 33/1000 | Loss: 0.00002236
Iteration 34/1000 | Loss: 0.00002217
Iteration 35/1000 | Loss: 0.00002210
Iteration 36/1000 | Loss: 0.00002204
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002198
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002196
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002189
Iteration 48/1000 | Loss: 0.00002188
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002184
Iteration 56/1000 | Loss: 0.00002183
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002181
Iteration 61/1000 | Loss: 0.00002181
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002179
Iteration 68/1000 | Loss: 0.00002179
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002178
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00002177
Iteration 80/1000 | Loss: 0.00002177
Iteration 81/1000 | Loss: 0.00002177
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002177
Iteration 84/1000 | Loss: 0.00002177
Iteration 85/1000 | Loss: 0.00002177
Iteration 86/1000 | Loss: 0.00002177
Iteration 87/1000 | Loss: 0.00002176
Iteration 88/1000 | Loss: 0.00002176
Iteration 89/1000 | Loss: 0.00002176
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002176
Iteration 92/1000 | Loss: 0.00002176
Iteration 93/1000 | Loss: 0.00002176
Iteration 94/1000 | Loss: 0.00002175
Iteration 95/1000 | Loss: 0.00002175
Iteration 96/1000 | Loss: 0.00002175
Iteration 97/1000 | Loss: 0.00002175
Iteration 98/1000 | Loss: 0.00002175
Iteration 99/1000 | Loss: 0.00002175
Iteration 100/1000 | Loss: 0.00002175
Iteration 101/1000 | Loss: 0.00002175
Iteration 102/1000 | Loss: 0.00002175
Iteration 103/1000 | Loss: 0.00002175
Iteration 104/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.1753025066573173e-05, 2.1753025066573173e-05, 2.1753025066573173e-05, 2.1753025066573173e-05, 2.1753025066573173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1753025066573173e-05

Optimization complete. Final v2v error: 3.8013412952423096 mm

Highest mean error: 11.54569149017334 mm for frame 63

Lowest mean error: 3.1751983165740967 mm for frame 133

Saving results

Total time: 81.52207493782043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758136
Iteration 2/25 | Loss: 0.00180152
Iteration 3/25 | Loss: 0.00143463
Iteration 4/25 | Loss: 0.00142946
Iteration 5/25 | Loss: 0.00139644
Iteration 6/25 | Loss: 0.00135999
Iteration 7/25 | Loss: 0.00135753
Iteration 8/25 | Loss: 0.00135723
Iteration 9/25 | Loss: 0.00136901
Iteration 10/25 | Loss: 0.00137168
Iteration 11/25 | Loss: 0.00136302
Iteration 12/25 | Loss: 0.00135547
Iteration 13/25 | Loss: 0.00135345
Iteration 14/25 | Loss: 0.00135290
Iteration 15/25 | Loss: 0.00135276
Iteration 16/25 | Loss: 0.00135276
Iteration 17/25 | Loss: 0.00135275
Iteration 18/25 | Loss: 0.00135275
Iteration 19/25 | Loss: 0.00135275
Iteration 20/25 | Loss: 0.00135275
Iteration 21/25 | Loss: 0.00135274
Iteration 22/25 | Loss: 0.00135273
Iteration 23/25 | Loss: 0.00135273
Iteration 24/25 | Loss: 0.00135273
Iteration 25/25 | Loss: 0.00135273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47509050
Iteration 2/25 | Loss: 0.00109835
Iteration 3/25 | Loss: 0.00109833
Iteration 4/25 | Loss: 0.00109833
Iteration 5/25 | Loss: 0.00109833
Iteration 6/25 | Loss: 0.00109832
Iteration 7/25 | Loss: 0.00109832
Iteration 8/25 | Loss: 0.00109832
Iteration 9/25 | Loss: 0.00109832
Iteration 10/25 | Loss: 0.00109832
Iteration 11/25 | Loss: 0.00109832
Iteration 12/25 | Loss: 0.00109832
Iteration 13/25 | Loss: 0.00109832
Iteration 14/25 | Loss: 0.00109832
Iteration 15/25 | Loss: 0.00109832
Iteration 16/25 | Loss: 0.00109832
Iteration 17/25 | Loss: 0.00109832
Iteration 18/25 | Loss: 0.00109832
Iteration 19/25 | Loss: 0.00109832
Iteration 20/25 | Loss: 0.00109832
Iteration 21/25 | Loss: 0.00109832
Iteration 22/25 | Loss: 0.00109832
Iteration 23/25 | Loss: 0.00109832
Iteration 24/25 | Loss: 0.00109832
Iteration 25/25 | Loss: 0.00109832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109832
Iteration 2/1000 | Loss: 0.00003143
Iteration 3/1000 | Loss: 0.00002617
Iteration 4/1000 | Loss: 0.00002476
Iteration 5/1000 | Loss: 0.00002395
Iteration 6/1000 | Loss: 0.00002342
Iteration 7/1000 | Loss: 0.00002302
Iteration 8/1000 | Loss: 0.00002250
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002179
Iteration 11/1000 | Loss: 0.00002955
Iteration 12/1000 | Loss: 0.00002358
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002109
Iteration 15/1000 | Loss: 0.00002089
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002079
Iteration 18/1000 | Loss: 0.00002078
Iteration 19/1000 | Loss: 0.00002078
Iteration 20/1000 | Loss: 0.00002066
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002053
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002038
Iteration 32/1000 | Loss: 0.00002038
Iteration 33/1000 | Loss: 0.00002037
Iteration 34/1000 | Loss: 0.00002037
Iteration 35/1000 | Loss: 0.00002036
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002036
Iteration 38/1000 | Loss: 0.00002034
Iteration 39/1000 | Loss: 0.00002034
Iteration 40/1000 | Loss: 0.00002033
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002033
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002032
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002031
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002030
Iteration 50/1000 | Loss: 0.00002029
Iteration 51/1000 | Loss: 0.00002029
Iteration 52/1000 | Loss: 0.00002029
Iteration 53/1000 | Loss: 0.00002029
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002029
Iteration 56/1000 | Loss: 0.00002029
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002026
Iteration 69/1000 | Loss: 0.00002026
Iteration 70/1000 | Loss: 0.00002026
Iteration 71/1000 | Loss: 0.00002026
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002026
Iteration 74/1000 | Loss: 0.00002026
Iteration 75/1000 | Loss: 0.00002026
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002025
Iteration 79/1000 | Loss: 0.00002025
Iteration 80/1000 | Loss: 0.00002025
Iteration 81/1000 | Loss: 0.00002025
Iteration 82/1000 | Loss: 0.00002025
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002024
Iteration 92/1000 | Loss: 0.00002024
Iteration 93/1000 | Loss: 0.00002024
Iteration 94/1000 | Loss: 0.00002024
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002024
Iteration 100/1000 | Loss: 0.00002024
Iteration 101/1000 | Loss: 0.00002024
Iteration 102/1000 | Loss: 0.00002024
Iteration 103/1000 | Loss: 0.00002024
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002023
Iteration 107/1000 | Loss: 0.00002023
Iteration 108/1000 | Loss: 0.00002023
Iteration 109/1000 | Loss: 0.00002023
Iteration 110/1000 | Loss: 0.00002023
Iteration 111/1000 | Loss: 0.00002023
Iteration 112/1000 | Loss: 0.00002023
Iteration 113/1000 | Loss: 0.00002023
Iteration 114/1000 | Loss: 0.00002023
Iteration 115/1000 | Loss: 0.00002023
Iteration 116/1000 | Loss: 0.00002023
Iteration 117/1000 | Loss: 0.00002023
Iteration 118/1000 | Loss: 0.00002022
Iteration 119/1000 | Loss: 0.00002022
Iteration 120/1000 | Loss: 0.00002022
Iteration 121/1000 | Loss: 0.00002022
Iteration 122/1000 | Loss: 0.00002022
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002021
Iteration 125/1000 | Loss: 0.00002021
Iteration 126/1000 | Loss: 0.00002021
Iteration 127/1000 | Loss: 0.00002021
Iteration 128/1000 | Loss: 0.00002020
Iteration 129/1000 | Loss: 0.00002020
Iteration 130/1000 | Loss: 0.00002020
Iteration 131/1000 | Loss: 0.00002020
Iteration 132/1000 | Loss: 0.00002020
Iteration 133/1000 | Loss: 0.00002020
Iteration 134/1000 | Loss: 0.00002020
Iteration 135/1000 | Loss: 0.00002020
Iteration 136/1000 | Loss: 0.00002020
Iteration 137/1000 | Loss: 0.00002020
Iteration 138/1000 | Loss: 0.00002020
Iteration 139/1000 | Loss: 0.00002019
Iteration 140/1000 | Loss: 0.00002019
Iteration 141/1000 | Loss: 0.00002019
Iteration 142/1000 | Loss: 0.00002019
Iteration 143/1000 | Loss: 0.00002019
Iteration 144/1000 | Loss: 0.00002019
Iteration 145/1000 | Loss: 0.00002019
Iteration 146/1000 | Loss: 0.00002019
Iteration 147/1000 | Loss: 0.00002019
Iteration 148/1000 | Loss: 0.00002019
Iteration 149/1000 | Loss: 0.00002019
Iteration 150/1000 | Loss: 0.00002019
Iteration 151/1000 | Loss: 0.00002019
Iteration 152/1000 | Loss: 0.00002019
Iteration 153/1000 | Loss: 0.00002019
Iteration 154/1000 | Loss: 0.00002019
Iteration 155/1000 | Loss: 0.00002019
Iteration 156/1000 | Loss: 0.00002019
Iteration 157/1000 | Loss: 0.00002019
Iteration 158/1000 | Loss: 0.00002019
Iteration 159/1000 | Loss: 0.00002019
Iteration 160/1000 | Loss: 0.00002019
Iteration 161/1000 | Loss: 0.00002019
Iteration 162/1000 | Loss: 0.00002019
Iteration 163/1000 | Loss: 0.00002019
Iteration 164/1000 | Loss: 0.00002019
Iteration 165/1000 | Loss: 0.00002019
Iteration 166/1000 | Loss: 0.00002019
Iteration 167/1000 | Loss: 0.00002019
Iteration 168/1000 | Loss: 0.00002019
Iteration 169/1000 | Loss: 0.00002019
Iteration 170/1000 | Loss: 0.00002019
Iteration 171/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.0190114810247906e-05, 2.0190114810247906e-05, 2.0190114810247906e-05, 2.0190114810247906e-05, 2.0190114810247906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0190114810247906e-05

Optimization complete. Final v2v error: 3.705763578414917 mm

Highest mean error: 5.087578296661377 mm for frame 231

Lowest mean error: 3.3643431663513184 mm for frame 147

Saving results

Total time: 70.48390412330627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844126
Iteration 2/25 | Loss: 0.00172136
Iteration 3/25 | Loss: 0.00156080
Iteration 4/25 | Loss: 0.00150890
Iteration 5/25 | Loss: 0.00150204
Iteration 6/25 | Loss: 0.00151752
Iteration 7/25 | Loss: 0.00150756
Iteration 8/25 | Loss: 0.00148270
Iteration 9/25 | Loss: 0.00147207
Iteration 10/25 | Loss: 0.00145673
Iteration 11/25 | Loss: 0.00144803
Iteration 12/25 | Loss: 0.00144340
Iteration 13/25 | Loss: 0.00143836
Iteration 14/25 | Loss: 0.00143512
Iteration 15/25 | Loss: 0.00143828
Iteration 16/25 | Loss: 0.00143607
Iteration 17/25 | Loss: 0.00143335
Iteration 18/25 | Loss: 0.00143041
Iteration 19/25 | Loss: 0.00142853
Iteration 20/25 | Loss: 0.00142776
Iteration 21/25 | Loss: 0.00142528
Iteration 22/25 | Loss: 0.00142758
Iteration 23/25 | Loss: 0.00142675
Iteration 24/25 | Loss: 0.00142663
Iteration 25/25 | Loss: 0.00142747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32419634
Iteration 2/25 | Loss: 0.00367859
Iteration 3/25 | Loss: 0.00367856
Iteration 4/25 | Loss: 0.00367855
Iteration 5/25 | Loss: 0.00367855
Iteration 6/25 | Loss: 0.00367855
Iteration 7/25 | Loss: 0.00367855
Iteration 8/25 | Loss: 0.00367855
Iteration 9/25 | Loss: 0.00367855
Iteration 10/25 | Loss: 0.00367855
Iteration 11/25 | Loss: 0.00367855
Iteration 12/25 | Loss: 0.00367855
Iteration 13/25 | Loss: 0.00367855
Iteration 14/25 | Loss: 0.00367855
Iteration 15/25 | Loss: 0.00367855
Iteration 16/25 | Loss: 0.00367855
Iteration 17/25 | Loss: 0.00367855
Iteration 18/25 | Loss: 0.00367855
Iteration 19/25 | Loss: 0.00367855
Iteration 20/25 | Loss: 0.00367855
Iteration 21/25 | Loss: 0.00367855
Iteration 22/25 | Loss: 0.00367855
Iteration 23/25 | Loss: 0.00367855
Iteration 24/25 | Loss: 0.00367855
Iteration 25/25 | Loss: 0.00367855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00367855
Iteration 2/1000 | Loss: 0.00031690
Iteration 3/1000 | Loss: 0.00019956
Iteration 4/1000 | Loss: 0.00023391
Iteration 5/1000 | Loss: 0.00016168
Iteration 6/1000 | Loss: 0.00019679
Iteration 7/1000 | Loss: 0.00020419
Iteration 8/1000 | Loss: 0.00014841
Iteration 9/1000 | Loss: 0.00016809
Iteration 10/1000 | Loss: 0.00024617
Iteration 11/1000 | Loss: 0.00020034
Iteration 12/1000 | Loss: 0.00013370
Iteration 13/1000 | Loss: 0.00012863
Iteration 14/1000 | Loss: 0.00020260
Iteration 15/1000 | Loss: 0.00012313
Iteration 16/1000 | Loss: 0.00012012
Iteration 17/1000 | Loss: 0.00011833
Iteration 18/1000 | Loss: 0.00011669
Iteration 19/1000 | Loss: 0.00011515
Iteration 20/1000 | Loss: 0.00011414
Iteration 21/1000 | Loss: 0.00011327
Iteration 22/1000 | Loss: 0.00011236
Iteration 23/1000 | Loss: 0.00011180
Iteration 24/1000 | Loss: 0.00011124
Iteration 25/1000 | Loss: 0.00011065
Iteration 26/1000 | Loss: 0.00011020
Iteration 27/1000 | Loss: 0.00074000
Iteration 28/1000 | Loss: 0.00016780
Iteration 29/1000 | Loss: 0.00011019
Iteration 30/1000 | Loss: 0.00072023
Iteration 31/1000 | Loss: 0.00011686
Iteration 32/1000 | Loss: 0.00011015
Iteration 33/1000 | Loss: 0.00010799
Iteration 34/1000 | Loss: 0.00139852
Iteration 35/1000 | Loss: 0.00022149
Iteration 36/1000 | Loss: 0.00011027
Iteration 37/1000 | Loss: 0.00010717
Iteration 38/1000 | Loss: 0.00010562
Iteration 39/1000 | Loss: 0.00075230
Iteration 40/1000 | Loss: 0.00023763
Iteration 41/1000 | Loss: 0.00010483
Iteration 42/1000 | Loss: 0.00134043
Iteration 43/1000 | Loss: 0.00573727
Iteration 44/1000 | Loss: 0.00681485
Iteration 45/1000 | Loss: 0.00506974
Iteration 46/1000 | Loss: 0.00074425
Iteration 47/1000 | Loss: 0.00066055
Iteration 48/1000 | Loss: 0.00019673
Iteration 49/1000 | Loss: 0.00014739
Iteration 50/1000 | Loss: 0.00012108
Iteration 51/1000 | Loss: 0.00009939
Iteration 52/1000 | Loss: 0.00008252
Iteration 53/1000 | Loss: 0.00007419
Iteration 54/1000 | Loss: 0.00046908
Iteration 55/1000 | Loss: 0.00007695
Iteration 56/1000 | Loss: 0.00006608
Iteration 57/1000 | Loss: 0.00006212
Iteration 58/1000 | Loss: 0.00005879
Iteration 59/1000 | Loss: 0.00036331
Iteration 60/1000 | Loss: 0.00007986
Iteration 61/1000 | Loss: 0.00006212
Iteration 62/1000 | Loss: 0.00005744
Iteration 63/1000 | Loss: 0.00006226
Iteration 64/1000 | Loss: 0.00005341
Iteration 65/1000 | Loss: 0.00005245
Iteration 66/1000 | Loss: 0.00005145
Iteration 67/1000 | Loss: 0.00005080
Iteration 68/1000 | Loss: 0.00005020
Iteration 69/1000 | Loss: 0.00004952
Iteration 70/1000 | Loss: 0.00004908
Iteration 71/1000 | Loss: 0.00004872
Iteration 72/1000 | Loss: 0.00004846
Iteration 73/1000 | Loss: 0.00004820
Iteration 74/1000 | Loss: 0.00004799
Iteration 75/1000 | Loss: 0.00004792
Iteration 76/1000 | Loss: 0.00004783
Iteration 77/1000 | Loss: 0.00004767
Iteration 78/1000 | Loss: 0.00004765
Iteration 79/1000 | Loss: 0.00004762
Iteration 80/1000 | Loss: 0.00004752
Iteration 81/1000 | Loss: 0.00004751
Iteration 82/1000 | Loss: 0.00004751
Iteration 83/1000 | Loss: 0.00004750
Iteration 84/1000 | Loss: 0.00004749
Iteration 85/1000 | Loss: 0.00004749
Iteration 86/1000 | Loss: 0.00004748
Iteration 87/1000 | Loss: 0.00004748
Iteration 88/1000 | Loss: 0.00004747
Iteration 89/1000 | Loss: 0.00004745
Iteration 90/1000 | Loss: 0.00004743
Iteration 91/1000 | Loss: 0.00004740
Iteration 92/1000 | Loss: 0.00004726
Iteration 93/1000 | Loss: 0.00004724
Iteration 94/1000 | Loss: 0.00004722
Iteration 95/1000 | Loss: 0.00004720
Iteration 96/1000 | Loss: 0.00004719
Iteration 97/1000 | Loss: 0.00004719
Iteration 98/1000 | Loss: 0.00004718
Iteration 99/1000 | Loss: 0.00004718
Iteration 100/1000 | Loss: 0.00004717
Iteration 101/1000 | Loss: 0.00004717
Iteration 102/1000 | Loss: 0.00004717
Iteration 103/1000 | Loss: 0.00004716
Iteration 104/1000 | Loss: 0.00004716
Iteration 105/1000 | Loss: 0.00004715
Iteration 106/1000 | Loss: 0.00004709
Iteration 107/1000 | Loss: 0.00004708
Iteration 108/1000 | Loss: 0.00004705
Iteration 109/1000 | Loss: 0.00004705
Iteration 110/1000 | Loss: 0.00004704
Iteration 111/1000 | Loss: 0.00004703
Iteration 112/1000 | Loss: 0.00004702
Iteration 113/1000 | Loss: 0.00004701
Iteration 114/1000 | Loss: 0.00004701
Iteration 115/1000 | Loss: 0.00004700
Iteration 116/1000 | Loss: 0.00004700
Iteration 117/1000 | Loss: 0.00004700
Iteration 118/1000 | Loss: 0.00004699
Iteration 119/1000 | Loss: 0.00004699
Iteration 120/1000 | Loss: 0.00004698
Iteration 121/1000 | Loss: 0.00004698
Iteration 122/1000 | Loss: 0.00004698
Iteration 123/1000 | Loss: 0.00004698
Iteration 124/1000 | Loss: 0.00004697
Iteration 125/1000 | Loss: 0.00004697
Iteration 126/1000 | Loss: 0.00004697
Iteration 127/1000 | Loss: 0.00004696
Iteration 128/1000 | Loss: 0.00004696
Iteration 129/1000 | Loss: 0.00004695
Iteration 130/1000 | Loss: 0.00004695
Iteration 131/1000 | Loss: 0.00004695
Iteration 132/1000 | Loss: 0.00004694
Iteration 133/1000 | Loss: 0.00004694
Iteration 134/1000 | Loss: 0.00004693
Iteration 135/1000 | Loss: 0.00004693
Iteration 136/1000 | Loss: 0.00004693
Iteration 137/1000 | Loss: 0.00004693
Iteration 138/1000 | Loss: 0.00004693
Iteration 139/1000 | Loss: 0.00004692
Iteration 140/1000 | Loss: 0.00004692
Iteration 141/1000 | Loss: 0.00004692
Iteration 142/1000 | Loss: 0.00004691
Iteration 143/1000 | Loss: 0.00004691
Iteration 144/1000 | Loss: 0.00004691
Iteration 145/1000 | Loss: 0.00004691
Iteration 146/1000 | Loss: 0.00004691
Iteration 147/1000 | Loss: 0.00004691
Iteration 148/1000 | Loss: 0.00004690
Iteration 149/1000 | Loss: 0.00004690
Iteration 150/1000 | Loss: 0.00004690
Iteration 151/1000 | Loss: 0.00004690
Iteration 152/1000 | Loss: 0.00004690
Iteration 153/1000 | Loss: 0.00004690
Iteration 154/1000 | Loss: 0.00004690
Iteration 155/1000 | Loss: 0.00004689
Iteration 156/1000 | Loss: 0.00004689
Iteration 157/1000 | Loss: 0.00004689
Iteration 158/1000 | Loss: 0.00004689
Iteration 159/1000 | Loss: 0.00004688
Iteration 160/1000 | Loss: 0.00004688
Iteration 161/1000 | Loss: 0.00004688
Iteration 162/1000 | Loss: 0.00004688
Iteration 163/1000 | Loss: 0.00004687
Iteration 164/1000 | Loss: 0.00004687
Iteration 165/1000 | Loss: 0.00004687
Iteration 166/1000 | Loss: 0.00004687
Iteration 167/1000 | Loss: 0.00004686
Iteration 168/1000 | Loss: 0.00004686
Iteration 169/1000 | Loss: 0.00004685
Iteration 170/1000 | Loss: 0.00004685
Iteration 171/1000 | Loss: 0.00004685
Iteration 172/1000 | Loss: 0.00004685
Iteration 173/1000 | Loss: 0.00004684
Iteration 174/1000 | Loss: 0.00004684
Iteration 175/1000 | Loss: 0.00004684
Iteration 176/1000 | Loss: 0.00004684
Iteration 177/1000 | Loss: 0.00004684
Iteration 178/1000 | Loss: 0.00004684
Iteration 179/1000 | Loss: 0.00004684
Iteration 180/1000 | Loss: 0.00004683
Iteration 181/1000 | Loss: 0.00004683
Iteration 182/1000 | Loss: 0.00004683
Iteration 183/1000 | Loss: 0.00004683
Iteration 184/1000 | Loss: 0.00004683
Iteration 185/1000 | Loss: 0.00004683
Iteration 186/1000 | Loss: 0.00004683
Iteration 187/1000 | Loss: 0.00004683
Iteration 188/1000 | Loss: 0.00004683
Iteration 189/1000 | Loss: 0.00004683
Iteration 190/1000 | Loss: 0.00004683
Iteration 191/1000 | Loss: 0.00004683
Iteration 192/1000 | Loss: 0.00004683
Iteration 193/1000 | Loss: 0.00004683
Iteration 194/1000 | Loss: 0.00004683
Iteration 195/1000 | Loss: 0.00004683
Iteration 196/1000 | Loss: 0.00004683
Iteration 197/1000 | Loss: 0.00004683
Iteration 198/1000 | Loss: 0.00004683
Iteration 199/1000 | Loss: 0.00004683
Iteration 200/1000 | Loss: 0.00004683
Iteration 201/1000 | Loss: 0.00004683
Iteration 202/1000 | Loss: 0.00004683
Iteration 203/1000 | Loss: 0.00004683
Iteration 204/1000 | Loss: 0.00004683
Iteration 205/1000 | Loss: 0.00004683
Iteration 206/1000 | Loss: 0.00004683
Iteration 207/1000 | Loss: 0.00004683
Iteration 208/1000 | Loss: 0.00004683
Iteration 209/1000 | Loss: 0.00004683
Iteration 210/1000 | Loss: 0.00004683
Iteration 211/1000 | Loss: 0.00004683
Iteration 212/1000 | Loss: 0.00004683
Iteration 213/1000 | Loss: 0.00004683
Iteration 214/1000 | Loss: 0.00004683
Iteration 215/1000 | Loss: 0.00004683
Iteration 216/1000 | Loss: 0.00004683
Iteration 217/1000 | Loss: 0.00004683
Iteration 218/1000 | Loss: 0.00004683
Iteration 219/1000 | Loss: 0.00004683
Iteration 220/1000 | Loss: 0.00004683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [4.6829281927784905e-05, 4.6829281927784905e-05, 4.6829281927784905e-05, 4.6829281927784905e-05, 4.6829281927784905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6829281927784905e-05

Optimization complete. Final v2v error: 4.1060099601745605 mm

Highest mean error: 12.181924819946289 mm for frame 165

Lowest mean error: 3.1267499923706055 mm for frame 128

Saving results

Total time: 170.05719351768494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348867
Iteration 2/25 | Loss: 0.00132208
Iteration 3/25 | Loss: 0.00121589
Iteration 4/25 | Loss: 0.00119525
Iteration 5/25 | Loss: 0.00118723
Iteration 6/25 | Loss: 0.00118552
Iteration 7/25 | Loss: 0.00118552
Iteration 8/25 | Loss: 0.00118552
Iteration 9/25 | Loss: 0.00118552
Iteration 10/25 | Loss: 0.00118552
Iteration 11/25 | Loss: 0.00118552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011855249758809805, 0.0011855249758809805, 0.0011855249758809805, 0.0011855249758809805, 0.0011855249758809805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011855249758809805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30483603
Iteration 2/25 | Loss: 0.00118840
Iteration 3/25 | Loss: 0.00118840
Iteration 4/25 | Loss: 0.00118840
Iteration 5/25 | Loss: 0.00118840
Iteration 6/25 | Loss: 0.00118840
Iteration 7/25 | Loss: 0.00118840
Iteration 8/25 | Loss: 0.00118840
Iteration 9/25 | Loss: 0.00118840
Iteration 10/25 | Loss: 0.00118840
Iteration 11/25 | Loss: 0.00118840
Iteration 12/25 | Loss: 0.00118840
Iteration 13/25 | Loss: 0.00118840
Iteration 14/25 | Loss: 0.00118840
Iteration 15/25 | Loss: 0.00118840
Iteration 16/25 | Loss: 0.00118840
Iteration 17/25 | Loss: 0.00118840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001188398222438991, 0.001188398222438991, 0.001188398222438991, 0.001188398222438991, 0.001188398222438991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001188398222438991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118840
Iteration 2/1000 | Loss: 0.00005024
Iteration 3/1000 | Loss: 0.00003332
Iteration 4/1000 | Loss: 0.00002654
Iteration 5/1000 | Loss: 0.00002380
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002050
Iteration 8/1000 | Loss: 0.00001979
Iteration 9/1000 | Loss: 0.00001922
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001824
Iteration 13/1000 | Loss: 0.00001805
Iteration 14/1000 | Loss: 0.00001791
Iteration 15/1000 | Loss: 0.00001789
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001769
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001755
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001739
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001738
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001737
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001733
Iteration 52/1000 | Loss: 0.00001733
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001727
Iteration 83/1000 | Loss: 0.00001727
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001723
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001722
Iteration 109/1000 | Loss: 0.00001722
Iteration 110/1000 | Loss: 0.00001721
Iteration 111/1000 | Loss: 0.00001721
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001720
Iteration 114/1000 | Loss: 0.00001720
Iteration 115/1000 | Loss: 0.00001720
Iteration 116/1000 | Loss: 0.00001720
Iteration 117/1000 | Loss: 0.00001720
Iteration 118/1000 | Loss: 0.00001720
Iteration 119/1000 | Loss: 0.00001720
Iteration 120/1000 | Loss: 0.00001719
Iteration 121/1000 | Loss: 0.00001719
Iteration 122/1000 | Loss: 0.00001719
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00001719
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001719
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001718
Iteration 140/1000 | Loss: 0.00001718
Iteration 141/1000 | Loss: 0.00001718
Iteration 142/1000 | Loss: 0.00001718
Iteration 143/1000 | Loss: 0.00001718
Iteration 144/1000 | Loss: 0.00001718
Iteration 145/1000 | Loss: 0.00001718
Iteration 146/1000 | Loss: 0.00001718
Iteration 147/1000 | Loss: 0.00001718
Iteration 148/1000 | Loss: 0.00001718
Iteration 149/1000 | Loss: 0.00001718
Iteration 150/1000 | Loss: 0.00001718
Iteration 151/1000 | Loss: 0.00001718
Iteration 152/1000 | Loss: 0.00001718
Iteration 153/1000 | Loss: 0.00001718
Iteration 154/1000 | Loss: 0.00001718
Iteration 155/1000 | Loss: 0.00001718
Iteration 156/1000 | Loss: 0.00001718
Iteration 157/1000 | Loss: 0.00001718
Iteration 158/1000 | Loss: 0.00001718
Iteration 159/1000 | Loss: 0.00001718
Iteration 160/1000 | Loss: 0.00001718
Iteration 161/1000 | Loss: 0.00001718
Iteration 162/1000 | Loss: 0.00001718
Iteration 163/1000 | Loss: 0.00001718
Iteration 164/1000 | Loss: 0.00001718
Iteration 165/1000 | Loss: 0.00001718
Iteration 166/1000 | Loss: 0.00001718
Iteration 167/1000 | Loss: 0.00001718
Iteration 168/1000 | Loss: 0.00001718
Iteration 169/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.7184880562126637e-05, 1.7184880562126637e-05, 1.7184880562126637e-05, 1.7184880562126637e-05, 1.7184880562126637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7184880562126637e-05

Optimization complete. Final v2v error: 3.4713871479034424 mm

Highest mean error: 4.361702919006348 mm for frame 143

Lowest mean error: 2.9315147399902344 mm for frame 179

Saving results

Total time: 47.98069405555725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808347
Iteration 2/25 | Loss: 0.00165674
Iteration 3/25 | Loss: 0.00142249
Iteration 4/25 | Loss: 0.00138452
Iteration 5/25 | Loss: 0.00136014
Iteration 6/25 | Loss: 0.00136243
Iteration 7/25 | Loss: 0.00135865
Iteration 8/25 | Loss: 0.00136057
Iteration 9/25 | Loss: 0.00134025
Iteration 10/25 | Loss: 0.00133460
Iteration 11/25 | Loss: 0.00133076
Iteration 12/25 | Loss: 0.00132904
Iteration 13/25 | Loss: 0.00133406
Iteration 14/25 | Loss: 0.00133520
Iteration 15/25 | Loss: 0.00133357
Iteration 16/25 | Loss: 0.00133528
Iteration 17/25 | Loss: 0.00133375
Iteration 18/25 | Loss: 0.00133061
Iteration 19/25 | Loss: 0.00132845
Iteration 20/25 | Loss: 0.00133184
Iteration 21/25 | Loss: 0.00133099
Iteration 22/25 | Loss: 0.00132961
Iteration 23/25 | Loss: 0.00132906
Iteration 24/25 | Loss: 0.00132882
Iteration 25/25 | Loss: 0.00133377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45822620
Iteration 2/25 | Loss: 0.00174064
Iteration 3/25 | Loss: 0.00174051
Iteration 4/25 | Loss: 0.00174050
Iteration 5/25 | Loss: 0.00174050
Iteration 6/25 | Loss: 0.00174050
Iteration 7/25 | Loss: 0.00174050
Iteration 8/25 | Loss: 0.00174050
Iteration 9/25 | Loss: 0.00174050
Iteration 10/25 | Loss: 0.00174050
Iteration 11/25 | Loss: 0.00174050
Iteration 12/25 | Loss: 0.00174050
Iteration 13/25 | Loss: 0.00174050
Iteration 14/25 | Loss: 0.00174050
Iteration 15/25 | Loss: 0.00174050
Iteration 16/25 | Loss: 0.00174050
Iteration 17/25 | Loss: 0.00174050
Iteration 18/25 | Loss: 0.00174050
Iteration 19/25 | Loss: 0.00174050
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017405023099854589, 0.0017405023099854589, 0.0017405023099854589, 0.0017405023099854589, 0.0017405023099854589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017405023099854589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174050
Iteration 2/1000 | Loss: 0.00039379
Iteration 3/1000 | Loss: 0.00028607
Iteration 4/1000 | Loss: 0.00009890
Iteration 5/1000 | Loss: 0.00012847
Iteration 6/1000 | Loss: 0.00015079
Iteration 7/1000 | Loss: 0.00021294
Iteration 8/1000 | Loss: 0.00012537
Iteration 9/1000 | Loss: 0.00020816
Iteration 10/1000 | Loss: 0.00025484
Iteration 11/1000 | Loss: 0.00021394
Iteration 12/1000 | Loss: 0.00018301
Iteration 13/1000 | Loss: 0.00021206
Iteration 14/1000 | Loss: 0.00016740
Iteration 15/1000 | Loss: 0.00021232
Iteration 16/1000 | Loss: 0.00034642
Iteration 17/1000 | Loss: 0.00030971
Iteration 18/1000 | Loss: 0.00022630
Iteration 19/1000 | Loss: 0.00014365
Iteration 20/1000 | Loss: 0.00014284
Iteration 21/1000 | Loss: 0.00009934
Iteration 22/1000 | Loss: 0.00016361
Iteration 23/1000 | Loss: 0.00063539
Iteration 24/1000 | Loss: 0.00060281
Iteration 25/1000 | Loss: 0.00066551
Iteration 26/1000 | Loss: 0.00054540
Iteration 27/1000 | Loss: 0.00047937
Iteration 28/1000 | Loss: 0.00051074
Iteration 29/1000 | Loss: 0.00032068
Iteration 30/1000 | Loss: 0.00042382
Iteration 31/1000 | Loss: 0.00008775
Iteration 32/1000 | Loss: 0.00007040
Iteration 33/1000 | Loss: 0.00006483
Iteration 34/1000 | Loss: 0.00006252
Iteration 35/1000 | Loss: 0.00006057
Iteration 36/1000 | Loss: 0.00005871
Iteration 37/1000 | Loss: 0.00005776
Iteration 38/1000 | Loss: 0.00005706
Iteration 39/1000 | Loss: 0.00005633
Iteration 40/1000 | Loss: 0.00005549
Iteration 41/1000 | Loss: 0.00069252
Iteration 42/1000 | Loss: 0.00103300
Iteration 43/1000 | Loss: 0.00006186
Iteration 44/1000 | Loss: 0.00005217
Iteration 45/1000 | Loss: 0.00053311
Iteration 46/1000 | Loss: 0.00004938
Iteration 47/1000 | Loss: 0.00004580
Iteration 48/1000 | Loss: 0.00004285
Iteration 49/1000 | Loss: 0.00004080
Iteration 50/1000 | Loss: 0.00003940
Iteration 51/1000 | Loss: 0.00003848
Iteration 52/1000 | Loss: 0.00003786
Iteration 53/1000 | Loss: 0.00003729
Iteration 54/1000 | Loss: 0.00003690
Iteration 55/1000 | Loss: 0.00003656
Iteration 56/1000 | Loss: 0.00003628
Iteration 57/1000 | Loss: 0.00003625
Iteration 58/1000 | Loss: 0.00003617
Iteration 59/1000 | Loss: 0.00003598
Iteration 60/1000 | Loss: 0.00003585
Iteration 61/1000 | Loss: 0.00003584
Iteration 62/1000 | Loss: 0.00003581
Iteration 63/1000 | Loss: 0.00003578
Iteration 64/1000 | Loss: 0.00003577
Iteration 65/1000 | Loss: 0.00003576
Iteration 66/1000 | Loss: 0.00003575
Iteration 67/1000 | Loss: 0.00003575
Iteration 68/1000 | Loss: 0.00018328
Iteration 69/1000 | Loss: 0.00003549
Iteration 70/1000 | Loss: 0.00003460
Iteration 71/1000 | Loss: 0.00003393
Iteration 72/1000 | Loss: 0.00003343
Iteration 73/1000 | Loss: 0.00003295
Iteration 74/1000 | Loss: 0.00003262
Iteration 75/1000 | Loss: 0.00003248
Iteration 76/1000 | Loss: 0.00003236
Iteration 77/1000 | Loss: 0.00003230
Iteration 78/1000 | Loss: 0.00003218
Iteration 79/1000 | Loss: 0.00003217
Iteration 80/1000 | Loss: 0.00003216
Iteration 81/1000 | Loss: 0.00003216
Iteration 82/1000 | Loss: 0.00003215
Iteration 83/1000 | Loss: 0.00003215
Iteration 84/1000 | Loss: 0.00003214
Iteration 85/1000 | Loss: 0.00003213
Iteration 86/1000 | Loss: 0.00003213
Iteration 87/1000 | Loss: 0.00003213
Iteration 88/1000 | Loss: 0.00003212
Iteration 89/1000 | Loss: 0.00003212
Iteration 90/1000 | Loss: 0.00003211
Iteration 91/1000 | Loss: 0.00003210
Iteration 92/1000 | Loss: 0.00003209
Iteration 93/1000 | Loss: 0.00003208
Iteration 94/1000 | Loss: 0.00003208
Iteration 95/1000 | Loss: 0.00003208
Iteration 96/1000 | Loss: 0.00003208
Iteration 97/1000 | Loss: 0.00003208
Iteration 98/1000 | Loss: 0.00003208
Iteration 99/1000 | Loss: 0.00003207
Iteration 100/1000 | Loss: 0.00003207
Iteration 101/1000 | Loss: 0.00003207
Iteration 102/1000 | Loss: 0.00003207
Iteration 103/1000 | Loss: 0.00003207
Iteration 104/1000 | Loss: 0.00003207
Iteration 105/1000 | Loss: 0.00003207
Iteration 106/1000 | Loss: 0.00003207
Iteration 107/1000 | Loss: 0.00003207
Iteration 108/1000 | Loss: 0.00003206
Iteration 109/1000 | Loss: 0.00003206
Iteration 110/1000 | Loss: 0.00003206
Iteration 111/1000 | Loss: 0.00003206
Iteration 112/1000 | Loss: 0.00003206
Iteration 113/1000 | Loss: 0.00003206
Iteration 114/1000 | Loss: 0.00003205
Iteration 115/1000 | Loss: 0.00003205
Iteration 116/1000 | Loss: 0.00003205
Iteration 117/1000 | Loss: 0.00003205
Iteration 118/1000 | Loss: 0.00003205
Iteration 119/1000 | Loss: 0.00003205
Iteration 120/1000 | Loss: 0.00003205
Iteration 121/1000 | Loss: 0.00003205
Iteration 122/1000 | Loss: 0.00003205
Iteration 123/1000 | Loss: 0.00003205
Iteration 124/1000 | Loss: 0.00003205
Iteration 125/1000 | Loss: 0.00003205
Iteration 126/1000 | Loss: 0.00003204
Iteration 127/1000 | Loss: 0.00003204
Iteration 128/1000 | Loss: 0.00003204
Iteration 129/1000 | Loss: 0.00003203
Iteration 130/1000 | Loss: 0.00003203
Iteration 131/1000 | Loss: 0.00003203
Iteration 132/1000 | Loss: 0.00003203
Iteration 133/1000 | Loss: 0.00003202
Iteration 134/1000 | Loss: 0.00003202
Iteration 135/1000 | Loss: 0.00003202
Iteration 136/1000 | Loss: 0.00003202
Iteration 137/1000 | Loss: 0.00003202
Iteration 138/1000 | Loss: 0.00003202
Iteration 139/1000 | Loss: 0.00003202
Iteration 140/1000 | Loss: 0.00003202
Iteration 141/1000 | Loss: 0.00003201
Iteration 142/1000 | Loss: 0.00003201
Iteration 143/1000 | Loss: 0.00003201
Iteration 144/1000 | Loss: 0.00003201
Iteration 145/1000 | Loss: 0.00003201
Iteration 146/1000 | Loss: 0.00003201
Iteration 147/1000 | Loss: 0.00003201
Iteration 148/1000 | Loss: 0.00003201
Iteration 149/1000 | Loss: 0.00003200
Iteration 150/1000 | Loss: 0.00003200
Iteration 151/1000 | Loss: 0.00003200
Iteration 152/1000 | Loss: 0.00003200
Iteration 153/1000 | Loss: 0.00003200
Iteration 154/1000 | Loss: 0.00003200
Iteration 155/1000 | Loss: 0.00003199
Iteration 156/1000 | Loss: 0.00003199
Iteration 157/1000 | Loss: 0.00003199
Iteration 158/1000 | Loss: 0.00003199
Iteration 159/1000 | Loss: 0.00003199
Iteration 160/1000 | Loss: 0.00003199
Iteration 161/1000 | Loss: 0.00003199
Iteration 162/1000 | Loss: 0.00003198
Iteration 163/1000 | Loss: 0.00003198
Iteration 164/1000 | Loss: 0.00003198
Iteration 165/1000 | Loss: 0.00003198
Iteration 166/1000 | Loss: 0.00003197
Iteration 167/1000 | Loss: 0.00003197
Iteration 168/1000 | Loss: 0.00003197
Iteration 169/1000 | Loss: 0.00003197
Iteration 170/1000 | Loss: 0.00003197
Iteration 171/1000 | Loss: 0.00003197
Iteration 172/1000 | Loss: 0.00003197
Iteration 173/1000 | Loss: 0.00003197
Iteration 174/1000 | Loss: 0.00003197
Iteration 175/1000 | Loss: 0.00003196
Iteration 176/1000 | Loss: 0.00003196
Iteration 177/1000 | Loss: 0.00003196
Iteration 178/1000 | Loss: 0.00003196
Iteration 179/1000 | Loss: 0.00003196
Iteration 180/1000 | Loss: 0.00003196
Iteration 181/1000 | Loss: 0.00003196
Iteration 182/1000 | Loss: 0.00003196
Iteration 183/1000 | Loss: 0.00003195
Iteration 184/1000 | Loss: 0.00003195
Iteration 185/1000 | Loss: 0.00003195
Iteration 186/1000 | Loss: 0.00003195
Iteration 187/1000 | Loss: 0.00003195
Iteration 188/1000 | Loss: 0.00003195
Iteration 189/1000 | Loss: 0.00003195
Iteration 190/1000 | Loss: 0.00003195
Iteration 191/1000 | Loss: 0.00003195
Iteration 192/1000 | Loss: 0.00003195
Iteration 193/1000 | Loss: 0.00003195
Iteration 194/1000 | Loss: 0.00003195
Iteration 195/1000 | Loss: 0.00003195
Iteration 196/1000 | Loss: 0.00003195
Iteration 197/1000 | Loss: 0.00003195
Iteration 198/1000 | Loss: 0.00003195
Iteration 199/1000 | Loss: 0.00003195
Iteration 200/1000 | Loss: 0.00003195
Iteration 201/1000 | Loss: 0.00003194
Iteration 202/1000 | Loss: 0.00003194
Iteration 203/1000 | Loss: 0.00003194
Iteration 204/1000 | Loss: 0.00003194
Iteration 205/1000 | Loss: 0.00003194
Iteration 206/1000 | Loss: 0.00003194
Iteration 207/1000 | Loss: 0.00003194
Iteration 208/1000 | Loss: 0.00003194
Iteration 209/1000 | Loss: 0.00003194
Iteration 210/1000 | Loss: 0.00003193
Iteration 211/1000 | Loss: 0.00003193
Iteration 212/1000 | Loss: 0.00003193
Iteration 213/1000 | Loss: 0.00003193
Iteration 214/1000 | Loss: 0.00003193
Iteration 215/1000 | Loss: 0.00003193
Iteration 216/1000 | Loss: 0.00003192
Iteration 217/1000 | Loss: 0.00003192
Iteration 218/1000 | Loss: 0.00003192
Iteration 219/1000 | Loss: 0.00003192
Iteration 220/1000 | Loss: 0.00003192
Iteration 221/1000 | Loss: 0.00003192
Iteration 222/1000 | Loss: 0.00003192
Iteration 223/1000 | Loss: 0.00003192
Iteration 224/1000 | Loss: 0.00003192
Iteration 225/1000 | Loss: 0.00003192
Iteration 226/1000 | Loss: 0.00003192
Iteration 227/1000 | Loss: 0.00003192
Iteration 228/1000 | Loss: 0.00003192
Iteration 229/1000 | Loss: 0.00003191
Iteration 230/1000 | Loss: 0.00003191
Iteration 231/1000 | Loss: 0.00003191
Iteration 232/1000 | Loss: 0.00003191
Iteration 233/1000 | Loss: 0.00003191
Iteration 234/1000 | Loss: 0.00003191
Iteration 235/1000 | Loss: 0.00003191
Iteration 236/1000 | Loss: 0.00003191
Iteration 237/1000 | Loss: 0.00003191
Iteration 238/1000 | Loss: 0.00003190
Iteration 239/1000 | Loss: 0.00003190
Iteration 240/1000 | Loss: 0.00003190
Iteration 241/1000 | Loss: 0.00003190
Iteration 242/1000 | Loss: 0.00003190
Iteration 243/1000 | Loss: 0.00003190
Iteration 244/1000 | Loss: 0.00003190
Iteration 245/1000 | Loss: 0.00003190
Iteration 246/1000 | Loss: 0.00003190
Iteration 247/1000 | Loss: 0.00003190
Iteration 248/1000 | Loss: 0.00003190
Iteration 249/1000 | Loss: 0.00003190
Iteration 250/1000 | Loss: 0.00003190
Iteration 251/1000 | Loss: 0.00003190
Iteration 252/1000 | Loss: 0.00003190
Iteration 253/1000 | Loss: 0.00003189
Iteration 254/1000 | Loss: 0.00003189
Iteration 255/1000 | Loss: 0.00003189
Iteration 256/1000 | Loss: 0.00003189
Iteration 257/1000 | Loss: 0.00003189
Iteration 258/1000 | Loss: 0.00003189
Iteration 259/1000 | Loss: 0.00003189
Iteration 260/1000 | Loss: 0.00003189
Iteration 261/1000 | Loss: 0.00003189
Iteration 262/1000 | Loss: 0.00003189
Iteration 263/1000 | Loss: 0.00003189
Iteration 264/1000 | Loss: 0.00003189
Iteration 265/1000 | Loss: 0.00003189
Iteration 266/1000 | Loss: 0.00003189
Iteration 267/1000 | Loss: 0.00003189
Iteration 268/1000 | Loss: 0.00003189
Iteration 269/1000 | Loss: 0.00003189
Iteration 270/1000 | Loss: 0.00003189
Iteration 271/1000 | Loss: 0.00003189
Iteration 272/1000 | Loss: 0.00003189
Iteration 273/1000 | Loss: 0.00003189
Iteration 274/1000 | Loss: 0.00003189
Iteration 275/1000 | Loss: 0.00003189
Iteration 276/1000 | Loss: 0.00003189
Iteration 277/1000 | Loss: 0.00003189
Iteration 278/1000 | Loss: 0.00003189
Iteration 279/1000 | Loss: 0.00003189
Iteration 280/1000 | Loss: 0.00003189
Iteration 281/1000 | Loss: 0.00003189
Iteration 282/1000 | Loss: 0.00003189
Iteration 283/1000 | Loss: 0.00003189
Iteration 284/1000 | Loss: 0.00003189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [3.189259223290719e-05, 3.189259223290719e-05, 3.189259223290719e-05, 3.189259223290719e-05, 3.189259223290719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.189259223290719e-05

Optimization complete. Final v2v error: 4.303181171417236 mm

Highest mean error: 12.575288772583008 mm for frame 65

Lowest mean error: 2.8398399353027344 mm for frame 102

Saving results

Total time: 148.3971712589264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421825
Iteration 2/25 | Loss: 0.00124562
Iteration 3/25 | Loss: 0.00117978
Iteration 4/25 | Loss: 0.00117108
Iteration 5/25 | Loss: 0.00116855
Iteration 6/25 | Loss: 0.00116796
Iteration 7/25 | Loss: 0.00116796
Iteration 8/25 | Loss: 0.00116796
Iteration 9/25 | Loss: 0.00116796
Iteration 10/25 | Loss: 0.00116796
Iteration 11/25 | Loss: 0.00116796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001167956506833434, 0.001167956506833434, 0.001167956506833434, 0.001167956506833434, 0.001167956506833434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001167956506833434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.03210568
Iteration 2/25 | Loss: 0.00099141
Iteration 3/25 | Loss: 0.00099141
Iteration 4/25 | Loss: 0.00099141
Iteration 5/25 | Loss: 0.00099141
Iteration 6/25 | Loss: 0.00099141
Iteration 7/25 | Loss: 0.00099141
Iteration 8/25 | Loss: 0.00099141
Iteration 9/25 | Loss: 0.00099141
Iteration 10/25 | Loss: 0.00099141
Iteration 11/25 | Loss: 0.00099141
Iteration 12/25 | Loss: 0.00099141
Iteration 13/25 | Loss: 0.00099141
Iteration 14/25 | Loss: 0.00099141
Iteration 15/25 | Loss: 0.00099141
Iteration 16/25 | Loss: 0.00099141
Iteration 17/25 | Loss: 0.00099141
Iteration 18/25 | Loss: 0.00099141
Iteration 19/25 | Loss: 0.00099141
Iteration 20/25 | Loss: 0.00099141
Iteration 21/25 | Loss: 0.00099141
Iteration 22/25 | Loss: 0.00099141
Iteration 23/25 | Loss: 0.00099141
Iteration 24/25 | Loss: 0.00099141
Iteration 25/25 | Loss: 0.00099141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099141
Iteration 2/1000 | Loss: 0.00001912
Iteration 3/1000 | Loss: 0.00001520
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001158
Iteration 10/1000 | Loss: 0.00001150
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001147
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001107
Iteration 26/1000 | Loss: 0.00001106
Iteration 27/1000 | Loss: 0.00001103
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001083
Iteration 37/1000 | Loss: 0.00001082
Iteration 38/1000 | Loss: 0.00001081
Iteration 39/1000 | Loss: 0.00001081
Iteration 40/1000 | Loss: 0.00001080
Iteration 41/1000 | Loss: 0.00001080
Iteration 42/1000 | Loss: 0.00001080
Iteration 43/1000 | Loss: 0.00001080
Iteration 44/1000 | Loss: 0.00001080
Iteration 45/1000 | Loss: 0.00001080
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001071
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001071
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001070
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001068
Iteration 81/1000 | Loss: 0.00001067
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001067
Iteration 87/1000 | Loss: 0.00001067
Iteration 88/1000 | Loss: 0.00001067
Iteration 89/1000 | Loss: 0.00001067
Iteration 90/1000 | Loss: 0.00001067
Iteration 91/1000 | Loss: 0.00001067
Iteration 92/1000 | Loss: 0.00001066
Iteration 93/1000 | Loss: 0.00001066
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001065
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001063
Iteration 105/1000 | Loss: 0.00001063
Iteration 106/1000 | Loss: 0.00001063
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001062
Iteration 121/1000 | Loss: 0.00001062
Iteration 122/1000 | Loss: 0.00001062
Iteration 123/1000 | Loss: 0.00001062
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001061
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001060
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001060
Iteration 140/1000 | Loss: 0.00001060
Iteration 141/1000 | Loss: 0.00001060
Iteration 142/1000 | Loss: 0.00001060
Iteration 143/1000 | Loss: 0.00001060
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001058
Iteration 152/1000 | Loss: 0.00001058
Iteration 153/1000 | Loss: 0.00001058
Iteration 154/1000 | Loss: 0.00001058
Iteration 155/1000 | Loss: 0.00001058
Iteration 156/1000 | Loss: 0.00001057
Iteration 157/1000 | Loss: 0.00001057
Iteration 158/1000 | Loss: 0.00001057
Iteration 159/1000 | Loss: 0.00001057
Iteration 160/1000 | Loss: 0.00001057
Iteration 161/1000 | Loss: 0.00001057
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001056
Iteration 164/1000 | Loss: 0.00001056
Iteration 165/1000 | Loss: 0.00001055
Iteration 166/1000 | Loss: 0.00001055
Iteration 167/1000 | Loss: 0.00001055
Iteration 168/1000 | Loss: 0.00001055
Iteration 169/1000 | Loss: 0.00001055
Iteration 170/1000 | Loss: 0.00001055
Iteration 171/1000 | Loss: 0.00001055
Iteration 172/1000 | Loss: 0.00001055
Iteration 173/1000 | Loss: 0.00001055
Iteration 174/1000 | Loss: 0.00001055
Iteration 175/1000 | Loss: 0.00001054
Iteration 176/1000 | Loss: 0.00001054
Iteration 177/1000 | Loss: 0.00001054
Iteration 178/1000 | Loss: 0.00001054
Iteration 179/1000 | Loss: 0.00001054
Iteration 180/1000 | Loss: 0.00001054
Iteration 181/1000 | Loss: 0.00001053
Iteration 182/1000 | Loss: 0.00001053
Iteration 183/1000 | Loss: 0.00001053
Iteration 184/1000 | Loss: 0.00001053
Iteration 185/1000 | Loss: 0.00001053
Iteration 186/1000 | Loss: 0.00001053
Iteration 187/1000 | Loss: 0.00001053
Iteration 188/1000 | Loss: 0.00001053
Iteration 189/1000 | Loss: 0.00001053
Iteration 190/1000 | Loss: 0.00001053
Iteration 191/1000 | Loss: 0.00001053
Iteration 192/1000 | Loss: 0.00001053
Iteration 193/1000 | Loss: 0.00001053
Iteration 194/1000 | Loss: 0.00001053
Iteration 195/1000 | Loss: 0.00001053
Iteration 196/1000 | Loss: 0.00001053
Iteration 197/1000 | Loss: 0.00001053
Iteration 198/1000 | Loss: 0.00001052
Iteration 199/1000 | Loss: 0.00001052
Iteration 200/1000 | Loss: 0.00001052
Iteration 201/1000 | Loss: 0.00001052
Iteration 202/1000 | Loss: 0.00001052
Iteration 203/1000 | Loss: 0.00001052
Iteration 204/1000 | Loss: 0.00001052
Iteration 205/1000 | Loss: 0.00001052
Iteration 206/1000 | Loss: 0.00001052
Iteration 207/1000 | Loss: 0.00001052
Iteration 208/1000 | Loss: 0.00001052
Iteration 209/1000 | Loss: 0.00001052
Iteration 210/1000 | Loss: 0.00001052
Iteration 211/1000 | Loss: 0.00001052
Iteration 212/1000 | Loss: 0.00001051
Iteration 213/1000 | Loss: 0.00001051
Iteration 214/1000 | Loss: 0.00001051
Iteration 215/1000 | Loss: 0.00001051
Iteration 216/1000 | Loss: 0.00001051
Iteration 217/1000 | Loss: 0.00001051
Iteration 218/1000 | Loss: 0.00001051
Iteration 219/1000 | Loss: 0.00001051
Iteration 220/1000 | Loss: 0.00001051
Iteration 221/1000 | Loss: 0.00001051
Iteration 222/1000 | Loss: 0.00001051
Iteration 223/1000 | Loss: 0.00001051
Iteration 224/1000 | Loss: 0.00001051
Iteration 225/1000 | Loss: 0.00001051
Iteration 226/1000 | Loss: 0.00001051
Iteration 227/1000 | Loss: 0.00001051
Iteration 228/1000 | Loss: 0.00001051
Iteration 229/1000 | Loss: 0.00001051
Iteration 230/1000 | Loss: 0.00001051
Iteration 231/1000 | Loss: 0.00001051
Iteration 232/1000 | Loss: 0.00001051
Iteration 233/1000 | Loss: 0.00001051
Iteration 234/1000 | Loss: 0.00001051
Iteration 235/1000 | Loss: 0.00001051
Iteration 236/1000 | Loss: 0.00001051
Iteration 237/1000 | Loss: 0.00001051
Iteration 238/1000 | Loss: 0.00001051
Iteration 239/1000 | Loss: 0.00001051
Iteration 240/1000 | Loss: 0.00001051
Iteration 241/1000 | Loss: 0.00001051
Iteration 242/1000 | Loss: 0.00001051
Iteration 243/1000 | Loss: 0.00001051
Iteration 244/1000 | Loss: 0.00001051
Iteration 245/1000 | Loss: 0.00001051
Iteration 246/1000 | Loss: 0.00001051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.051260915119201e-05, 1.051260915119201e-05, 1.051260915119201e-05, 1.051260915119201e-05, 1.051260915119201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.051260915119201e-05

Optimization complete. Final v2v error: 2.815499782562256 mm

Highest mean error: 3.039449453353882 mm for frame 88

Lowest mean error: 2.6682686805725098 mm for frame 175

Saving results

Total time: 43.90398049354553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455508
Iteration 2/25 | Loss: 0.00141343
Iteration 3/25 | Loss: 0.00128071
Iteration 4/25 | Loss: 0.00126839
Iteration 5/25 | Loss: 0.00126582
Iteration 6/25 | Loss: 0.00126522
Iteration 7/25 | Loss: 0.00126522
Iteration 8/25 | Loss: 0.00126522
Iteration 9/25 | Loss: 0.00126522
Iteration 10/25 | Loss: 0.00126522
Iteration 11/25 | Loss: 0.00126522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001265223021619022, 0.001265223021619022, 0.001265223021619022, 0.001265223021619022, 0.001265223021619022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001265223021619022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41911304
Iteration 2/25 | Loss: 0.00097349
Iteration 3/25 | Loss: 0.00097348
Iteration 4/25 | Loss: 0.00097348
Iteration 5/25 | Loss: 0.00097348
Iteration 6/25 | Loss: 0.00097348
Iteration 7/25 | Loss: 0.00097348
Iteration 8/25 | Loss: 0.00097348
Iteration 9/25 | Loss: 0.00097348
Iteration 10/25 | Loss: 0.00097348
Iteration 11/25 | Loss: 0.00097348
Iteration 12/25 | Loss: 0.00097348
Iteration 13/25 | Loss: 0.00097348
Iteration 14/25 | Loss: 0.00097348
Iteration 15/25 | Loss: 0.00097348
Iteration 16/25 | Loss: 0.00097348
Iteration 17/25 | Loss: 0.00097348
Iteration 18/25 | Loss: 0.00097348
Iteration 19/25 | Loss: 0.00097348
Iteration 20/25 | Loss: 0.00097348
Iteration 21/25 | Loss: 0.00097348
Iteration 22/25 | Loss: 0.00097348
Iteration 23/25 | Loss: 0.00097348
Iteration 24/25 | Loss: 0.00097348
Iteration 25/25 | Loss: 0.00097348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097348
Iteration 2/1000 | Loss: 0.00003883
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00002058
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001809
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001732
Iteration 10/1000 | Loss: 0.00001712
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001666
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001647
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001629
Iteration 21/1000 | Loss: 0.00001626
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001617
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001615
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001609
Iteration 48/1000 | Loss: 0.00001609
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001607
Iteration 53/1000 | Loss: 0.00001607
Iteration 54/1000 | Loss: 0.00001607
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001602
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001602
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001599
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001594
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001592
Iteration 76/1000 | Loss: 0.00001592
Iteration 77/1000 | Loss: 0.00001591
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001590
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001586
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001586
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001586
Iteration 122/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.5861647625570185e-05, 1.5861647625570185e-05, 1.5861647625570185e-05, 1.5861647625570185e-05, 1.5861647625570185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5861647625570185e-05

Optimization complete. Final v2v error: 3.2812180519104004 mm

Highest mean error: 3.7809150218963623 mm for frame 14

Lowest mean error: 2.502396583557129 mm for frame 0

Saving results

Total time: 37.0620653629303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793117
Iteration 2/25 | Loss: 0.00144688
Iteration 3/25 | Loss: 0.00135173
Iteration 4/25 | Loss: 0.00133917
Iteration 5/25 | Loss: 0.00133449
Iteration 6/25 | Loss: 0.00133373
Iteration 7/25 | Loss: 0.00133373
Iteration 8/25 | Loss: 0.00133373
Iteration 9/25 | Loss: 0.00133373
Iteration 10/25 | Loss: 0.00133373
Iteration 11/25 | Loss: 0.00133373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013337304117158055, 0.0013337304117158055, 0.0013337304117158055, 0.0013337304117158055, 0.0013337304117158055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013337304117158055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20538354
Iteration 2/25 | Loss: 0.00088280
Iteration 3/25 | Loss: 0.00088278
Iteration 4/25 | Loss: 0.00088278
Iteration 5/25 | Loss: 0.00088278
Iteration 6/25 | Loss: 0.00088278
Iteration 7/25 | Loss: 0.00088278
Iteration 8/25 | Loss: 0.00088278
Iteration 9/25 | Loss: 0.00088278
Iteration 10/25 | Loss: 0.00088278
Iteration 11/25 | Loss: 0.00088278
Iteration 12/25 | Loss: 0.00088278
Iteration 13/25 | Loss: 0.00088278
Iteration 14/25 | Loss: 0.00088278
Iteration 15/25 | Loss: 0.00088278
Iteration 16/25 | Loss: 0.00088278
Iteration 17/25 | Loss: 0.00088277
Iteration 18/25 | Loss: 0.00088277
Iteration 19/25 | Loss: 0.00088277
Iteration 20/25 | Loss: 0.00088277
Iteration 21/25 | Loss: 0.00088277
Iteration 22/25 | Loss: 0.00088277
Iteration 23/25 | Loss: 0.00088277
Iteration 24/25 | Loss: 0.00088277
Iteration 25/25 | Loss: 0.00088277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088277
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002559
Iteration 4/1000 | Loss: 0.00002416
Iteration 5/1000 | Loss: 0.00002345
Iteration 6/1000 | Loss: 0.00002297
Iteration 7/1000 | Loss: 0.00002255
Iteration 8/1000 | Loss: 0.00002219
Iteration 9/1000 | Loss: 0.00002190
Iteration 10/1000 | Loss: 0.00002171
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002150
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002136
Iteration 16/1000 | Loss: 0.00002136
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002124
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002117
Iteration 22/1000 | Loss: 0.00002111
Iteration 23/1000 | Loss: 0.00002111
Iteration 24/1000 | Loss: 0.00002111
Iteration 25/1000 | Loss: 0.00002109
Iteration 26/1000 | Loss: 0.00002103
Iteration 27/1000 | Loss: 0.00002103
Iteration 28/1000 | Loss: 0.00002101
Iteration 29/1000 | Loss: 0.00002098
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002097
Iteration 32/1000 | Loss: 0.00002097
Iteration 33/1000 | Loss: 0.00002096
Iteration 34/1000 | Loss: 0.00002095
Iteration 35/1000 | Loss: 0.00002094
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00002093
Iteration 38/1000 | Loss: 0.00002088
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002087
Iteration 41/1000 | Loss: 0.00002086
Iteration 42/1000 | Loss: 0.00002085
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002084
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002082
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002081
Iteration 52/1000 | Loss: 0.00002081
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002080
Iteration 56/1000 | Loss: 0.00002080
Iteration 57/1000 | Loss: 0.00002080
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002079
Iteration 60/1000 | Loss: 0.00002078
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002078
Iteration 65/1000 | Loss: 0.00002078
Iteration 66/1000 | Loss: 0.00002078
Iteration 67/1000 | Loss: 0.00002078
Iteration 68/1000 | Loss: 0.00002078
Iteration 69/1000 | Loss: 0.00002077
Iteration 70/1000 | Loss: 0.00002077
Iteration 71/1000 | Loss: 0.00002077
Iteration 72/1000 | Loss: 0.00002077
Iteration 73/1000 | Loss: 0.00002077
Iteration 74/1000 | Loss: 0.00002077
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002077
Iteration 77/1000 | Loss: 0.00002077
Iteration 78/1000 | Loss: 0.00002076
Iteration 79/1000 | Loss: 0.00002075
Iteration 80/1000 | Loss: 0.00002075
Iteration 81/1000 | Loss: 0.00002074
Iteration 82/1000 | Loss: 0.00002074
Iteration 83/1000 | Loss: 0.00002074
Iteration 84/1000 | Loss: 0.00002074
Iteration 85/1000 | Loss: 0.00002074
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002073
Iteration 88/1000 | Loss: 0.00002073
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002072
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002070
Iteration 97/1000 | Loss: 0.00002070
Iteration 98/1000 | Loss: 0.00002070
Iteration 99/1000 | Loss: 0.00002070
Iteration 100/1000 | Loss: 0.00002070
Iteration 101/1000 | Loss: 0.00002070
Iteration 102/1000 | Loss: 0.00002070
Iteration 103/1000 | Loss: 0.00002070
Iteration 104/1000 | Loss: 0.00002069
Iteration 105/1000 | Loss: 0.00002069
Iteration 106/1000 | Loss: 0.00002069
Iteration 107/1000 | Loss: 0.00002069
Iteration 108/1000 | Loss: 0.00002069
Iteration 109/1000 | Loss: 0.00002069
Iteration 110/1000 | Loss: 0.00002069
Iteration 111/1000 | Loss: 0.00002069
Iteration 112/1000 | Loss: 0.00002069
Iteration 113/1000 | Loss: 0.00002069
Iteration 114/1000 | Loss: 0.00002069
Iteration 115/1000 | Loss: 0.00002069
Iteration 116/1000 | Loss: 0.00002069
Iteration 117/1000 | Loss: 0.00002069
Iteration 118/1000 | Loss: 0.00002069
Iteration 119/1000 | Loss: 0.00002069
Iteration 120/1000 | Loss: 0.00002069
Iteration 121/1000 | Loss: 0.00002069
Iteration 122/1000 | Loss: 0.00002069
Iteration 123/1000 | Loss: 0.00002069
Iteration 124/1000 | Loss: 0.00002069
Iteration 125/1000 | Loss: 0.00002069
Iteration 126/1000 | Loss: 0.00002069
Iteration 127/1000 | Loss: 0.00002069
Iteration 128/1000 | Loss: 0.00002069
Iteration 129/1000 | Loss: 0.00002069
Iteration 130/1000 | Loss: 0.00002069
Iteration 131/1000 | Loss: 0.00002069
Iteration 132/1000 | Loss: 0.00002069
Iteration 133/1000 | Loss: 0.00002069
Iteration 134/1000 | Loss: 0.00002069
Iteration 135/1000 | Loss: 0.00002069
Iteration 136/1000 | Loss: 0.00002069
Iteration 137/1000 | Loss: 0.00002069
Iteration 138/1000 | Loss: 0.00002069
Iteration 139/1000 | Loss: 0.00002069
Iteration 140/1000 | Loss: 0.00002069
Iteration 141/1000 | Loss: 0.00002069
Iteration 142/1000 | Loss: 0.00002069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.0687135474872775e-05, 2.0687135474872775e-05, 2.0687135474872775e-05, 2.0687135474872775e-05, 2.0687135474872775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0687135474872775e-05

Optimization complete. Final v2v error: 3.793424367904663 mm

Highest mean error: 4.755836486816406 mm for frame 52

Lowest mean error: 3.3019473552703857 mm for frame 229

Saving results

Total time: 42.724847078323364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969146
Iteration 2/25 | Loss: 0.00169780
Iteration 3/25 | Loss: 0.00138208
Iteration 4/25 | Loss: 0.00133795
Iteration 5/25 | Loss: 0.00134769
Iteration 6/25 | Loss: 0.00133267
Iteration 7/25 | Loss: 0.00130912
Iteration 8/25 | Loss: 0.00129243
Iteration 9/25 | Loss: 0.00128931
Iteration 10/25 | Loss: 0.00128854
Iteration 11/25 | Loss: 0.00129125
Iteration 12/25 | Loss: 0.00128885
Iteration 13/25 | Loss: 0.00128583
Iteration 14/25 | Loss: 0.00128956
Iteration 15/25 | Loss: 0.00129453
Iteration 16/25 | Loss: 0.00129017
Iteration 17/25 | Loss: 0.00128568
Iteration 18/25 | Loss: 0.00128409
Iteration 19/25 | Loss: 0.00128284
Iteration 20/25 | Loss: 0.00128247
Iteration 21/25 | Loss: 0.00128229
Iteration 22/25 | Loss: 0.00128217
Iteration 23/25 | Loss: 0.00128212
Iteration 24/25 | Loss: 0.00128212
Iteration 25/25 | Loss: 0.00128212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.56157875
Iteration 2/25 | Loss: 0.00151763
Iteration 3/25 | Loss: 0.00151756
Iteration 4/25 | Loss: 0.00151756
Iteration 5/25 | Loss: 0.00151756
Iteration 6/25 | Loss: 0.00151756
Iteration 7/25 | Loss: 0.00151756
Iteration 8/25 | Loss: 0.00151756
Iteration 9/25 | Loss: 0.00151756
Iteration 10/25 | Loss: 0.00151756
Iteration 11/25 | Loss: 0.00151756
Iteration 12/25 | Loss: 0.00151756
Iteration 13/25 | Loss: 0.00151756
Iteration 14/25 | Loss: 0.00151756
Iteration 15/25 | Loss: 0.00151756
Iteration 16/25 | Loss: 0.00151756
Iteration 17/25 | Loss: 0.00151756
Iteration 18/25 | Loss: 0.00151756
Iteration 19/25 | Loss: 0.00151756
Iteration 20/25 | Loss: 0.00151756
Iteration 21/25 | Loss: 0.00151756
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015175607986748219, 0.0015175607986748219, 0.0015175607986748219, 0.0015175607986748219, 0.0015175607986748219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015175607986748219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151756
Iteration 2/1000 | Loss: 0.00013485
Iteration 3/1000 | Loss: 0.00007113
Iteration 4/1000 | Loss: 0.00009701
Iteration 5/1000 | Loss: 0.00004659
Iteration 6/1000 | Loss: 0.00006295
Iteration 7/1000 | Loss: 0.00008010
Iteration 8/1000 | Loss: 0.00008830
Iteration 9/1000 | Loss: 0.00003126
Iteration 10/1000 | Loss: 0.00011504
Iteration 11/1000 | Loss: 0.00011369
Iteration 12/1000 | Loss: 0.00003515
Iteration 13/1000 | Loss: 0.00007546
Iteration 14/1000 | Loss: 0.00030084
Iteration 15/1000 | Loss: 0.00007078
Iteration 16/1000 | Loss: 0.00007139
Iteration 17/1000 | Loss: 0.00006820
Iteration 18/1000 | Loss: 0.00006600
Iteration 19/1000 | Loss: 0.00004847
Iteration 20/1000 | Loss: 0.00007514
Iteration 21/1000 | Loss: 0.00005103
Iteration 22/1000 | Loss: 0.00003953
Iteration 23/1000 | Loss: 0.00004545
Iteration 24/1000 | Loss: 0.00003234
Iteration 25/1000 | Loss: 0.00002716
Iteration 26/1000 | Loss: 0.00002538
Iteration 27/1000 | Loss: 0.00002955
Iteration 28/1000 | Loss: 0.00002828
Iteration 29/1000 | Loss: 0.00002838
Iteration 30/1000 | Loss: 0.00005428
Iteration 31/1000 | Loss: 0.00004114
Iteration 32/1000 | Loss: 0.00004781
Iteration 33/1000 | Loss: 0.00004072
Iteration 34/1000 | Loss: 0.00005033
Iteration 35/1000 | Loss: 0.00004345
Iteration 36/1000 | Loss: 0.00004784
Iteration 37/1000 | Loss: 0.00004352
Iteration 38/1000 | Loss: 0.00004429
Iteration 39/1000 | Loss: 0.00004282
Iteration 40/1000 | Loss: 0.00004276
Iteration 41/1000 | Loss: 0.00004295
Iteration 42/1000 | Loss: 0.00004030
Iteration 43/1000 | Loss: 0.00004264
Iteration 44/1000 | Loss: 0.00002666
Iteration 45/1000 | Loss: 0.00004137
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002739
Iteration 51/1000 | Loss: 0.00002465
Iteration 52/1000 | Loss: 0.00001884
Iteration 53/1000 | Loss: 0.00002076
Iteration 54/1000 | Loss: 0.00001769
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001693
Iteration 58/1000 | Loss: 0.00001672
Iteration 59/1000 | Loss: 0.00001664
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001652
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00001651
Iteration 73/1000 | Loss: 0.00001651
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001650
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001648
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001644
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001642
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001633
Iteration 106/1000 | Loss: 0.00001633
Iteration 107/1000 | Loss: 0.00001632
Iteration 108/1000 | Loss: 0.00001632
Iteration 109/1000 | Loss: 0.00001632
Iteration 110/1000 | Loss: 0.00001624
Iteration 111/1000 | Loss: 0.00001624
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Iteration 131/1000 | Loss: 0.00022562
Iteration 132/1000 | Loss: 0.00002195
Iteration 133/1000 | Loss: 0.00001884
Iteration 134/1000 | Loss: 0.00004034
Iteration 135/1000 | Loss: 0.00001917
Iteration 136/1000 | Loss: 0.00003126
Iteration 137/1000 | Loss: 0.00004068
Iteration 138/1000 | Loss: 0.00003314
Iteration 139/1000 | Loss: 0.00004169
Iteration 140/1000 | Loss: 0.00003412
Iteration 141/1000 | Loss: 0.00004174
Iteration 142/1000 | Loss: 0.00002712
Iteration 143/1000 | Loss: 0.00004741
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00003853
Iteration 146/1000 | Loss: 0.00004679
Iteration 147/1000 | Loss: 0.00004048
Iteration 148/1000 | Loss: 0.00002443
Iteration 149/1000 | Loss: 0.00001897
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001721
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001631
Iteration 154/1000 | Loss: 0.00001594
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001554
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001543
Iteration 160/1000 | Loss: 0.00001543
Iteration 161/1000 | Loss: 0.00001542
Iteration 162/1000 | Loss: 0.00001542
Iteration 163/1000 | Loss: 0.00001542
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001541
Iteration 166/1000 | Loss: 0.00001541
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001541
Iteration 173/1000 | Loss: 0.00001541
Iteration 174/1000 | Loss: 0.00001541
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001538
Iteration 178/1000 | Loss: 0.00001538
Iteration 179/1000 | Loss: 0.00001538
Iteration 180/1000 | Loss: 0.00001537
Iteration 181/1000 | Loss: 0.00001535
Iteration 182/1000 | Loss: 0.00001534
Iteration 183/1000 | Loss: 0.00001534
Iteration 184/1000 | Loss: 0.00001534
Iteration 185/1000 | Loss: 0.00001533
Iteration 186/1000 | Loss: 0.00001533
Iteration 187/1000 | Loss: 0.00001533
Iteration 188/1000 | Loss: 0.00001533
Iteration 189/1000 | Loss: 0.00001533
Iteration 190/1000 | Loss: 0.00001533
Iteration 191/1000 | Loss: 0.00001533
Iteration 192/1000 | Loss: 0.00001532
Iteration 193/1000 | Loss: 0.00001532
Iteration 194/1000 | Loss: 0.00001532
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001531
Iteration 197/1000 | Loss: 0.00001531
Iteration 198/1000 | Loss: 0.00001531
Iteration 199/1000 | Loss: 0.00001531
Iteration 200/1000 | Loss: 0.00001530
Iteration 201/1000 | Loss: 0.00001529
Iteration 202/1000 | Loss: 0.00001529
Iteration 203/1000 | Loss: 0.00001529
Iteration 204/1000 | Loss: 0.00001528
Iteration 205/1000 | Loss: 0.00001527
Iteration 206/1000 | Loss: 0.00001526
Iteration 207/1000 | Loss: 0.00001526
Iteration 208/1000 | Loss: 0.00001526
Iteration 209/1000 | Loss: 0.00001525
Iteration 210/1000 | Loss: 0.00001525
Iteration 211/1000 | Loss: 0.00001524
Iteration 212/1000 | Loss: 0.00001524
Iteration 213/1000 | Loss: 0.00001524
Iteration 214/1000 | Loss: 0.00001524
Iteration 215/1000 | Loss: 0.00001524
Iteration 216/1000 | Loss: 0.00001524
Iteration 217/1000 | Loss: 0.00001523
Iteration 218/1000 | Loss: 0.00001523
Iteration 219/1000 | Loss: 0.00001523
Iteration 220/1000 | Loss: 0.00001523
Iteration 221/1000 | Loss: 0.00001523
Iteration 222/1000 | Loss: 0.00001522
Iteration 223/1000 | Loss: 0.00001522
Iteration 224/1000 | Loss: 0.00001521
Iteration 225/1000 | Loss: 0.00001521
Iteration 226/1000 | Loss: 0.00001521
Iteration 227/1000 | Loss: 0.00001521
Iteration 228/1000 | Loss: 0.00001521
Iteration 229/1000 | Loss: 0.00001520
Iteration 230/1000 | Loss: 0.00001520
Iteration 231/1000 | Loss: 0.00001520
Iteration 232/1000 | Loss: 0.00001520
Iteration 233/1000 | Loss: 0.00001519
Iteration 234/1000 | Loss: 0.00001519
Iteration 235/1000 | Loss: 0.00001519
Iteration 236/1000 | Loss: 0.00001518
Iteration 237/1000 | Loss: 0.00001517
Iteration 238/1000 | Loss: 0.00001517
Iteration 239/1000 | Loss: 0.00001517
Iteration 240/1000 | Loss: 0.00001517
Iteration 241/1000 | Loss: 0.00001517
Iteration 242/1000 | Loss: 0.00001517
Iteration 243/1000 | Loss: 0.00001517
Iteration 244/1000 | Loss: 0.00001517
Iteration 245/1000 | Loss: 0.00001517
Iteration 246/1000 | Loss: 0.00001517
Iteration 247/1000 | Loss: 0.00001516
Iteration 248/1000 | Loss: 0.00001516
Iteration 249/1000 | Loss: 0.00001515
Iteration 250/1000 | Loss: 0.00001515
Iteration 251/1000 | Loss: 0.00001515
Iteration 252/1000 | Loss: 0.00001515
Iteration 253/1000 | Loss: 0.00001515
Iteration 254/1000 | Loss: 0.00001515
Iteration 255/1000 | Loss: 0.00001515
Iteration 256/1000 | Loss: 0.00001515
Iteration 257/1000 | Loss: 0.00001515
Iteration 258/1000 | Loss: 0.00001515
Iteration 259/1000 | Loss: 0.00001515
Iteration 260/1000 | Loss: 0.00001514
Iteration 261/1000 | Loss: 0.00001514
Iteration 262/1000 | Loss: 0.00001514
Iteration 263/1000 | Loss: 0.00001514
Iteration 264/1000 | Loss: 0.00001514
Iteration 265/1000 | Loss: 0.00001514
Iteration 266/1000 | Loss: 0.00001514
Iteration 267/1000 | Loss: 0.00001514
Iteration 268/1000 | Loss: 0.00001514
Iteration 269/1000 | Loss: 0.00001514
Iteration 270/1000 | Loss: 0.00001514
Iteration 271/1000 | Loss: 0.00001514
Iteration 272/1000 | Loss: 0.00001514
Iteration 273/1000 | Loss: 0.00001514
Iteration 274/1000 | Loss: 0.00001514
Iteration 275/1000 | Loss: 0.00001514
Iteration 276/1000 | Loss: 0.00001513
Iteration 277/1000 | Loss: 0.00001513
Iteration 278/1000 | Loss: 0.00001513
Iteration 279/1000 | Loss: 0.00001513
Iteration 280/1000 | Loss: 0.00001513
Iteration 281/1000 | Loss: 0.00001513
Iteration 282/1000 | Loss: 0.00001513
Iteration 283/1000 | Loss: 0.00001513
Iteration 284/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.5133882698137313e-05, 1.5133882698137313e-05, 1.5133882698137313e-05, 1.5133882698137313e-05, 1.5133882698137313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5133882698137313e-05

Optimization complete. Final v2v error: 3.211927890777588 mm

Highest mean error: 5.486672878265381 mm for frame 33

Lowest mean error: 2.646239757537842 mm for frame 71

Saving results

Total time: 172.6109733581543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808733
Iteration 2/25 | Loss: 0.00127661
Iteration 3/25 | Loss: 0.00119242
Iteration 4/25 | Loss: 0.00118386
Iteration 5/25 | Loss: 0.00118262
Iteration 6/25 | Loss: 0.00118262
Iteration 7/25 | Loss: 0.00118262
Iteration 8/25 | Loss: 0.00118262
Iteration 9/25 | Loss: 0.00118262
Iteration 10/25 | Loss: 0.00118262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011826244881376624, 0.0011826244881376624, 0.0011826244881376624, 0.0011826244881376624, 0.0011826244881376624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011826244881376624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35149395
Iteration 2/25 | Loss: 0.00093597
Iteration 3/25 | Loss: 0.00093596
Iteration 4/25 | Loss: 0.00093596
Iteration 5/25 | Loss: 0.00093596
Iteration 6/25 | Loss: 0.00093596
Iteration 7/25 | Loss: 0.00093596
Iteration 8/25 | Loss: 0.00093596
Iteration 9/25 | Loss: 0.00093596
Iteration 10/25 | Loss: 0.00093596
Iteration 11/25 | Loss: 0.00093596
Iteration 12/25 | Loss: 0.00093596
Iteration 13/25 | Loss: 0.00093596
Iteration 14/25 | Loss: 0.00093596
Iteration 15/25 | Loss: 0.00093596
Iteration 16/25 | Loss: 0.00093596
Iteration 17/25 | Loss: 0.00093596
Iteration 18/25 | Loss: 0.00093596
Iteration 19/25 | Loss: 0.00093596
Iteration 20/25 | Loss: 0.00093596
Iteration 21/25 | Loss: 0.00093596
Iteration 22/25 | Loss: 0.00093596
Iteration 23/25 | Loss: 0.00093596
Iteration 24/25 | Loss: 0.00093596
Iteration 25/25 | Loss: 0.00093596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093596
Iteration 2/1000 | Loss: 0.00001955
Iteration 3/1000 | Loss: 0.00001476
Iteration 4/1000 | Loss: 0.00001311
Iteration 5/1000 | Loss: 0.00001228
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001116
Iteration 9/1000 | Loss: 0.00001083
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001055
Iteration 15/1000 | Loss: 0.00001053
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001047
Iteration 18/1000 | Loss: 0.00001046
Iteration 19/1000 | Loss: 0.00001042
Iteration 20/1000 | Loss: 0.00001039
Iteration 21/1000 | Loss: 0.00001038
Iteration 22/1000 | Loss: 0.00001036
Iteration 23/1000 | Loss: 0.00001036
Iteration 24/1000 | Loss: 0.00001035
Iteration 25/1000 | Loss: 0.00001035
Iteration 26/1000 | Loss: 0.00001035
Iteration 27/1000 | Loss: 0.00001034
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001032
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001030
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001026
Iteration 37/1000 | Loss: 0.00001018
Iteration 38/1000 | Loss: 0.00001014
Iteration 39/1000 | Loss: 0.00001013
Iteration 40/1000 | Loss: 0.00001012
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001006
Iteration 45/1000 | Loss: 0.00001006
Iteration 46/1000 | Loss: 0.00001006
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001004
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000998
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000997
Iteration 71/1000 | Loss: 0.00000997
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000995
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000995
Iteration 78/1000 | Loss: 0.00000995
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000994
Iteration 85/1000 | Loss: 0.00000994
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000993
Iteration 91/1000 | Loss: 0.00000993
Iteration 92/1000 | Loss: 0.00000993
Iteration 93/1000 | Loss: 0.00000993
Iteration 94/1000 | Loss: 0.00000993
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00000993
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Iteration 104/1000 | Loss: 0.00000991
Iteration 105/1000 | Loss: 0.00000990
Iteration 106/1000 | Loss: 0.00000990
Iteration 107/1000 | Loss: 0.00000990
Iteration 108/1000 | Loss: 0.00000990
Iteration 109/1000 | Loss: 0.00000989
Iteration 110/1000 | Loss: 0.00000989
Iteration 111/1000 | Loss: 0.00000989
Iteration 112/1000 | Loss: 0.00000988
Iteration 113/1000 | Loss: 0.00000988
Iteration 114/1000 | Loss: 0.00000988
Iteration 115/1000 | Loss: 0.00000987
Iteration 116/1000 | Loss: 0.00000987
Iteration 117/1000 | Loss: 0.00000987
Iteration 118/1000 | Loss: 0.00000987
Iteration 119/1000 | Loss: 0.00000987
Iteration 120/1000 | Loss: 0.00000987
Iteration 121/1000 | Loss: 0.00000986
Iteration 122/1000 | Loss: 0.00000986
Iteration 123/1000 | Loss: 0.00000986
Iteration 124/1000 | Loss: 0.00000986
Iteration 125/1000 | Loss: 0.00000985
Iteration 126/1000 | Loss: 0.00000985
Iteration 127/1000 | Loss: 0.00000985
Iteration 128/1000 | Loss: 0.00000984
Iteration 129/1000 | Loss: 0.00000984
Iteration 130/1000 | Loss: 0.00000984
Iteration 131/1000 | Loss: 0.00000984
Iteration 132/1000 | Loss: 0.00000984
Iteration 133/1000 | Loss: 0.00000983
Iteration 134/1000 | Loss: 0.00000983
Iteration 135/1000 | Loss: 0.00000983
Iteration 136/1000 | Loss: 0.00000983
Iteration 137/1000 | Loss: 0.00000982
Iteration 138/1000 | Loss: 0.00000982
Iteration 139/1000 | Loss: 0.00000982
Iteration 140/1000 | Loss: 0.00000982
Iteration 141/1000 | Loss: 0.00000982
Iteration 142/1000 | Loss: 0.00000981
Iteration 143/1000 | Loss: 0.00000981
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000980
Iteration 146/1000 | Loss: 0.00000980
Iteration 147/1000 | Loss: 0.00000980
Iteration 148/1000 | Loss: 0.00000979
Iteration 149/1000 | Loss: 0.00000979
Iteration 150/1000 | Loss: 0.00000979
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000979
Iteration 154/1000 | Loss: 0.00000979
Iteration 155/1000 | Loss: 0.00000979
Iteration 156/1000 | Loss: 0.00000979
Iteration 157/1000 | Loss: 0.00000979
Iteration 158/1000 | Loss: 0.00000979
Iteration 159/1000 | Loss: 0.00000979
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000977
Iteration 165/1000 | Loss: 0.00000977
Iteration 166/1000 | Loss: 0.00000977
Iteration 167/1000 | Loss: 0.00000977
Iteration 168/1000 | Loss: 0.00000977
Iteration 169/1000 | Loss: 0.00000977
Iteration 170/1000 | Loss: 0.00000977
Iteration 171/1000 | Loss: 0.00000977
Iteration 172/1000 | Loss: 0.00000977
Iteration 173/1000 | Loss: 0.00000977
Iteration 174/1000 | Loss: 0.00000977
Iteration 175/1000 | Loss: 0.00000977
Iteration 176/1000 | Loss: 0.00000977
Iteration 177/1000 | Loss: 0.00000976
Iteration 178/1000 | Loss: 0.00000976
Iteration 179/1000 | Loss: 0.00000976
Iteration 180/1000 | Loss: 0.00000976
Iteration 181/1000 | Loss: 0.00000976
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000975
Iteration 184/1000 | Loss: 0.00000975
Iteration 185/1000 | Loss: 0.00000975
Iteration 186/1000 | Loss: 0.00000975
Iteration 187/1000 | Loss: 0.00000975
Iteration 188/1000 | Loss: 0.00000975
Iteration 189/1000 | Loss: 0.00000975
Iteration 190/1000 | Loss: 0.00000975
Iteration 191/1000 | Loss: 0.00000975
Iteration 192/1000 | Loss: 0.00000975
Iteration 193/1000 | Loss: 0.00000975
Iteration 194/1000 | Loss: 0.00000975
Iteration 195/1000 | Loss: 0.00000975
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00000975
Iteration 198/1000 | Loss: 0.00000974
Iteration 199/1000 | Loss: 0.00000974
Iteration 200/1000 | Loss: 0.00000974
Iteration 201/1000 | Loss: 0.00000974
Iteration 202/1000 | Loss: 0.00000974
Iteration 203/1000 | Loss: 0.00000974
Iteration 204/1000 | Loss: 0.00000974
Iteration 205/1000 | Loss: 0.00000974
Iteration 206/1000 | Loss: 0.00000974
Iteration 207/1000 | Loss: 0.00000974
Iteration 208/1000 | Loss: 0.00000974
Iteration 209/1000 | Loss: 0.00000974
Iteration 210/1000 | Loss: 0.00000974
Iteration 211/1000 | Loss: 0.00000973
Iteration 212/1000 | Loss: 0.00000973
Iteration 213/1000 | Loss: 0.00000973
Iteration 214/1000 | Loss: 0.00000973
Iteration 215/1000 | Loss: 0.00000973
Iteration 216/1000 | Loss: 0.00000973
Iteration 217/1000 | Loss: 0.00000973
Iteration 218/1000 | Loss: 0.00000973
Iteration 219/1000 | Loss: 0.00000973
Iteration 220/1000 | Loss: 0.00000973
Iteration 221/1000 | Loss: 0.00000973
Iteration 222/1000 | Loss: 0.00000973
Iteration 223/1000 | Loss: 0.00000973
Iteration 224/1000 | Loss: 0.00000973
Iteration 225/1000 | Loss: 0.00000972
Iteration 226/1000 | Loss: 0.00000972
Iteration 227/1000 | Loss: 0.00000972
Iteration 228/1000 | Loss: 0.00000972
Iteration 229/1000 | Loss: 0.00000972
Iteration 230/1000 | Loss: 0.00000972
Iteration 231/1000 | Loss: 0.00000972
Iteration 232/1000 | Loss: 0.00000972
Iteration 233/1000 | Loss: 0.00000972
Iteration 234/1000 | Loss: 0.00000971
Iteration 235/1000 | Loss: 0.00000971
Iteration 236/1000 | Loss: 0.00000971
Iteration 237/1000 | Loss: 0.00000971
Iteration 238/1000 | Loss: 0.00000971
Iteration 239/1000 | Loss: 0.00000971
Iteration 240/1000 | Loss: 0.00000971
Iteration 241/1000 | Loss: 0.00000971
Iteration 242/1000 | Loss: 0.00000971
Iteration 243/1000 | Loss: 0.00000971
Iteration 244/1000 | Loss: 0.00000971
Iteration 245/1000 | Loss: 0.00000971
Iteration 246/1000 | Loss: 0.00000971
Iteration 247/1000 | Loss: 0.00000971
Iteration 248/1000 | Loss: 0.00000971
Iteration 249/1000 | Loss: 0.00000971
Iteration 250/1000 | Loss: 0.00000971
Iteration 251/1000 | Loss: 0.00000971
Iteration 252/1000 | Loss: 0.00000971
Iteration 253/1000 | Loss: 0.00000971
Iteration 254/1000 | Loss: 0.00000971
Iteration 255/1000 | Loss: 0.00000971
Iteration 256/1000 | Loss: 0.00000971
Iteration 257/1000 | Loss: 0.00000971
Iteration 258/1000 | Loss: 0.00000971
Iteration 259/1000 | Loss: 0.00000971
Iteration 260/1000 | Loss: 0.00000971
Iteration 261/1000 | Loss: 0.00000971
Iteration 262/1000 | Loss: 0.00000971
Iteration 263/1000 | Loss: 0.00000971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [9.711186066851951e-06, 9.711186066851951e-06, 9.711186066851951e-06, 9.711186066851951e-06, 9.711186066851951e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.711186066851951e-06

Optimization complete. Final v2v error: 2.655170440673828 mm

Highest mean error: 2.8160033226013184 mm for frame 56

Lowest mean error: 2.529822826385498 mm for frame 193

Saving results

Total time: 48.213377475738525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725440
Iteration 2/25 | Loss: 0.00165669
Iteration 3/25 | Loss: 0.00137679
Iteration 4/25 | Loss: 0.00134326
Iteration 5/25 | Loss: 0.00133788
Iteration 6/25 | Loss: 0.00133043
Iteration 7/25 | Loss: 0.00132744
Iteration 8/25 | Loss: 0.00132629
Iteration 9/25 | Loss: 0.00132812
Iteration 10/25 | Loss: 0.00132488
Iteration 11/25 | Loss: 0.00132334
Iteration 12/25 | Loss: 0.00132288
Iteration 13/25 | Loss: 0.00132272
Iteration 14/25 | Loss: 0.00132272
Iteration 15/25 | Loss: 0.00132272
Iteration 16/25 | Loss: 0.00132272
Iteration 17/25 | Loss: 0.00132272
Iteration 18/25 | Loss: 0.00132272
Iteration 19/25 | Loss: 0.00132272
Iteration 20/25 | Loss: 0.00132271
Iteration 21/25 | Loss: 0.00132271
Iteration 22/25 | Loss: 0.00132271
Iteration 23/25 | Loss: 0.00132271
Iteration 24/25 | Loss: 0.00132271
Iteration 25/25 | Loss: 0.00132271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30276537
Iteration 2/25 | Loss: 0.00136938
Iteration 3/25 | Loss: 0.00136938
Iteration 4/25 | Loss: 0.00136938
Iteration 5/25 | Loss: 0.00136937
Iteration 6/25 | Loss: 0.00136937
Iteration 7/25 | Loss: 0.00136937
Iteration 8/25 | Loss: 0.00136937
Iteration 9/25 | Loss: 0.00136937
Iteration 10/25 | Loss: 0.00136937
Iteration 11/25 | Loss: 0.00136937
Iteration 12/25 | Loss: 0.00136937
Iteration 13/25 | Loss: 0.00136937
Iteration 14/25 | Loss: 0.00136937
Iteration 15/25 | Loss: 0.00136937
Iteration 16/25 | Loss: 0.00136937
Iteration 17/25 | Loss: 0.00136937
Iteration 18/25 | Loss: 0.00136937
Iteration 19/25 | Loss: 0.00136937
Iteration 20/25 | Loss: 0.00136937
Iteration 21/25 | Loss: 0.00136937
Iteration 22/25 | Loss: 0.00136937
Iteration 23/25 | Loss: 0.00136937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001369372010231018, 0.001369372010231018, 0.001369372010231018, 0.001369372010231018, 0.001369372010231018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001369372010231018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136937
Iteration 2/1000 | Loss: 0.00289159
Iteration 3/1000 | Loss: 0.00011456
Iteration 4/1000 | Loss: 0.00006474
Iteration 5/1000 | Loss: 0.00004638
Iteration 6/1000 | Loss: 0.00004006
Iteration 7/1000 | Loss: 0.00003699
Iteration 8/1000 | Loss: 0.00003523
Iteration 9/1000 | Loss: 0.00113056
Iteration 10/1000 | Loss: 0.00004606
Iteration 11/1000 | Loss: 0.00003260
Iteration 12/1000 | Loss: 0.00002980
Iteration 13/1000 | Loss: 0.00002838
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00002618
Iteration 16/1000 | Loss: 0.00002568
Iteration 17/1000 | Loss: 0.00002532
Iteration 18/1000 | Loss: 0.00002495
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002441
Iteration 21/1000 | Loss: 0.00002420
Iteration 22/1000 | Loss: 0.00002414
Iteration 23/1000 | Loss: 0.00002414
Iteration 24/1000 | Loss: 0.00002413
Iteration 25/1000 | Loss: 0.00002413
Iteration 26/1000 | Loss: 0.00002409
Iteration 27/1000 | Loss: 0.00002408
Iteration 28/1000 | Loss: 0.00002405
Iteration 29/1000 | Loss: 0.00002398
Iteration 30/1000 | Loss: 0.00002398
Iteration 31/1000 | Loss: 0.00002397
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002396
Iteration 34/1000 | Loss: 0.00002396
Iteration 35/1000 | Loss: 0.00002395
Iteration 36/1000 | Loss: 0.00002393
Iteration 37/1000 | Loss: 0.00002392
Iteration 38/1000 | Loss: 0.00002390
Iteration 39/1000 | Loss: 0.00002389
Iteration 40/1000 | Loss: 0.00002384
Iteration 41/1000 | Loss: 0.00002381
Iteration 42/1000 | Loss: 0.00002380
Iteration 43/1000 | Loss: 0.00002378
Iteration 44/1000 | Loss: 0.00002374
Iteration 45/1000 | Loss: 0.00002374
Iteration 46/1000 | Loss: 0.00002373
Iteration 47/1000 | Loss: 0.00002373
Iteration 48/1000 | Loss: 0.00002372
Iteration 49/1000 | Loss: 0.00002372
Iteration 50/1000 | Loss: 0.00002371
Iteration 51/1000 | Loss: 0.00002371
Iteration 52/1000 | Loss: 0.00002370
Iteration 53/1000 | Loss: 0.00002370
Iteration 54/1000 | Loss: 0.00002369
Iteration 55/1000 | Loss: 0.00002366
Iteration 56/1000 | Loss: 0.00002366
Iteration 57/1000 | Loss: 0.00002366
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002366
Iteration 60/1000 | Loss: 0.00002365
Iteration 61/1000 | Loss: 0.00002364
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002364
Iteration 64/1000 | Loss: 0.00002364
Iteration 65/1000 | Loss: 0.00002364
Iteration 66/1000 | Loss: 0.00002364
Iteration 67/1000 | Loss: 0.00002364
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002363
Iteration 73/1000 | Loss: 0.00002363
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002362
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002362
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002361
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002359
Iteration 88/1000 | Loss: 0.00002359
Iteration 89/1000 | Loss: 0.00002358
Iteration 90/1000 | Loss: 0.00002358
Iteration 91/1000 | Loss: 0.00002358
Iteration 92/1000 | Loss: 0.00002358
Iteration 93/1000 | Loss: 0.00002358
Iteration 94/1000 | Loss: 0.00002358
Iteration 95/1000 | Loss: 0.00002357
Iteration 96/1000 | Loss: 0.00002357
Iteration 97/1000 | Loss: 0.00002357
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002355
Iteration 101/1000 | Loss: 0.00002355
Iteration 102/1000 | Loss: 0.00002355
Iteration 103/1000 | Loss: 0.00002354
Iteration 104/1000 | Loss: 0.00002354
Iteration 105/1000 | Loss: 0.00002354
Iteration 106/1000 | Loss: 0.00002354
Iteration 107/1000 | Loss: 0.00002353
Iteration 108/1000 | Loss: 0.00002353
Iteration 109/1000 | Loss: 0.00002353
Iteration 110/1000 | Loss: 0.00002352
Iteration 111/1000 | Loss: 0.00002352
Iteration 112/1000 | Loss: 0.00002352
Iteration 113/1000 | Loss: 0.00002351
Iteration 114/1000 | Loss: 0.00002351
Iteration 115/1000 | Loss: 0.00002351
Iteration 116/1000 | Loss: 0.00002351
Iteration 117/1000 | Loss: 0.00002350
Iteration 118/1000 | Loss: 0.00002350
Iteration 119/1000 | Loss: 0.00002349
Iteration 120/1000 | Loss: 0.00002349
Iteration 121/1000 | Loss: 0.00002349
Iteration 122/1000 | Loss: 0.00002349
Iteration 123/1000 | Loss: 0.00002348
Iteration 124/1000 | Loss: 0.00002348
Iteration 125/1000 | Loss: 0.00002348
Iteration 126/1000 | Loss: 0.00002348
Iteration 127/1000 | Loss: 0.00002348
Iteration 128/1000 | Loss: 0.00002347
Iteration 129/1000 | Loss: 0.00002347
Iteration 130/1000 | Loss: 0.00002347
Iteration 131/1000 | Loss: 0.00002347
Iteration 132/1000 | Loss: 0.00002346
Iteration 133/1000 | Loss: 0.00002346
Iteration 134/1000 | Loss: 0.00002346
Iteration 135/1000 | Loss: 0.00002346
Iteration 136/1000 | Loss: 0.00002346
Iteration 137/1000 | Loss: 0.00002346
Iteration 138/1000 | Loss: 0.00002346
Iteration 139/1000 | Loss: 0.00002346
Iteration 140/1000 | Loss: 0.00002346
Iteration 141/1000 | Loss: 0.00002346
Iteration 142/1000 | Loss: 0.00002345
Iteration 143/1000 | Loss: 0.00002345
Iteration 144/1000 | Loss: 0.00002345
Iteration 145/1000 | Loss: 0.00002345
Iteration 146/1000 | Loss: 0.00002345
Iteration 147/1000 | Loss: 0.00002344
Iteration 148/1000 | Loss: 0.00002344
Iteration 149/1000 | Loss: 0.00002344
Iteration 150/1000 | Loss: 0.00002344
Iteration 151/1000 | Loss: 0.00002344
Iteration 152/1000 | Loss: 0.00002344
Iteration 153/1000 | Loss: 0.00002344
Iteration 154/1000 | Loss: 0.00002344
Iteration 155/1000 | Loss: 0.00002344
Iteration 156/1000 | Loss: 0.00002344
Iteration 157/1000 | Loss: 0.00002344
Iteration 158/1000 | Loss: 0.00002344
Iteration 159/1000 | Loss: 0.00002344
Iteration 160/1000 | Loss: 0.00002344
Iteration 161/1000 | Loss: 0.00002344
Iteration 162/1000 | Loss: 0.00002344
Iteration 163/1000 | Loss: 0.00002344
Iteration 164/1000 | Loss: 0.00002344
Iteration 165/1000 | Loss: 0.00002344
Iteration 166/1000 | Loss: 0.00002344
Iteration 167/1000 | Loss: 0.00002344
Iteration 168/1000 | Loss: 0.00002344
Iteration 169/1000 | Loss: 0.00002344
Iteration 170/1000 | Loss: 0.00002344
Iteration 171/1000 | Loss: 0.00002344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.34393919527065e-05, 2.34393919527065e-05, 2.34393919527065e-05, 2.34393919527065e-05, 2.34393919527065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.34393919527065e-05

Optimization complete. Final v2v error: 3.9661526679992676 mm

Highest mean error: 6.182019233703613 mm for frame 64

Lowest mean error: 2.950796127319336 mm for frame 109

Saving results

Total time: 64.95444703102112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00685138
Iteration 2/25 | Loss: 0.00168581
Iteration 3/25 | Loss: 0.00142054
Iteration 4/25 | Loss: 0.00139853
Iteration 5/25 | Loss: 0.00139480
Iteration 6/25 | Loss: 0.00139469
Iteration 7/25 | Loss: 0.00139469
Iteration 8/25 | Loss: 0.00139469
Iteration 9/25 | Loss: 0.00139469
Iteration 10/25 | Loss: 0.00139469
Iteration 11/25 | Loss: 0.00139469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013946887338533998, 0.0013946887338533998, 0.0013946887338533998, 0.0013946887338533998, 0.0013946887338533998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013946887338533998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86159027
Iteration 2/25 | Loss: 0.00107528
Iteration 3/25 | Loss: 0.00107528
Iteration 4/25 | Loss: 0.00107528
Iteration 5/25 | Loss: 0.00107528
Iteration 6/25 | Loss: 0.00107528
Iteration 7/25 | Loss: 0.00107528
Iteration 8/25 | Loss: 0.00107528
Iteration 9/25 | Loss: 0.00107528
Iteration 10/25 | Loss: 0.00107528
Iteration 11/25 | Loss: 0.00107528
Iteration 12/25 | Loss: 0.00107528
Iteration 13/25 | Loss: 0.00107528
Iteration 14/25 | Loss: 0.00107528
Iteration 15/25 | Loss: 0.00107528
Iteration 16/25 | Loss: 0.00107528
Iteration 17/25 | Loss: 0.00107528
Iteration 18/25 | Loss: 0.00107528
Iteration 19/25 | Loss: 0.00107528
Iteration 20/25 | Loss: 0.00107528
Iteration 21/25 | Loss: 0.00107528
Iteration 22/25 | Loss: 0.00107528
Iteration 23/25 | Loss: 0.00107528
Iteration 24/25 | Loss: 0.00107528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010752750094980001, 0.0010752750094980001, 0.0010752750094980001, 0.0010752750094980001, 0.0010752750094980001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010752750094980001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107528
Iteration 2/1000 | Loss: 0.00005145
Iteration 3/1000 | Loss: 0.00003291
Iteration 4/1000 | Loss: 0.00002993
Iteration 5/1000 | Loss: 0.00002766
Iteration 6/1000 | Loss: 0.00002667
Iteration 7/1000 | Loss: 0.00002590
Iteration 8/1000 | Loss: 0.00002549
Iteration 9/1000 | Loss: 0.00002514
Iteration 10/1000 | Loss: 0.00002483
Iteration 11/1000 | Loss: 0.00002463
Iteration 12/1000 | Loss: 0.00002444
Iteration 13/1000 | Loss: 0.00002438
Iteration 14/1000 | Loss: 0.00002429
Iteration 15/1000 | Loss: 0.00002425
Iteration 16/1000 | Loss: 0.00002425
Iteration 17/1000 | Loss: 0.00002422
Iteration 18/1000 | Loss: 0.00002407
Iteration 19/1000 | Loss: 0.00002406
Iteration 20/1000 | Loss: 0.00002406
Iteration 21/1000 | Loss: 0.00002406
Iteration 22/1000 | Loss: 0.00002404
Iteration 23/1000 | Loss: 0.00002403
Iteration 24/1000 | Loss: 0.00002402
Iteration 25/1000 | Loss: 0.00002402
Iteration 26/1000 | Loss: 0.00002401
Iteration 27/1000 | Loss: 0.00002400
Iteration 28/1000 | Loss: 0.00002400
Iteration 29/1000 | Loss: 0.00002400
Iteration 30/1000 | Loss: 0.00002400
Iteration 31/1000 | Loss: 0.00002400
Iteration 32/1000 | Loss: 0.00002400
Iteration 33/1000 | Loss: 0.00002400
Iteration 34/1000 | Loss: 0.00002399
Iteration 35/1000 | Loss: 0.00002399
Iteration 36/1000 | Loss: 0.00002399
Iteration 37/1000 | Loss: 0.00002398
Iteration 38/1000 | Loss: 0.00002398
Iteration 39/1000 | Loss: 0.00002398
Iteration 40/1000 | Loss: 0.00002398
Iteration 41/1000 | Loss: 0.00002398
Iteration 42/1000 | Loss: 0.00002398
Iteration 43/1000 | Loss: 0.00002398
Iteration 44/1000 | Loss: 0.00002398
Iteration 45/1000 | Loss: 0.00002396
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002396
Iteration 49/1000 | Loss: 0.00002396
Iteration 50/1000 | Loss: 0.00002395
Iteration 51/1000 | Loss: 0.00002395
Iteration 52/1000 | Loss: 0.00002395
Iteration 53/1000 | Loss: 0.00002395
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002394
Iteration 56/1000 | Loss: 0.00002394
Iteration 57/1000 | Loss: 0.00002394
Iteration 58/1000 | Loss: 0.00002393
Iteration 59/1000 | Loss: 0.00002392
Iteration 60/1000 | Loss: 0.00002392
Iteration 61/1000 | Loss: 0.00002391
Iteration 62/1000 | Loss: 0.00002391
Iteration 63/1000 | Loss: 0.00002391
Iteration 64/1000 | Loss: 0.00002391
Iteration 65/1000 | Loss: 0.00002391
Iteration 66/1000 | Loss: 0.00002390
Iteration 67/1000 | Loss: 0.00002390
Iteration 68/1000 | Loss: 0.00002390
Iteration 69/1000 | Loss: 0.00002390
Iteration 70/1000 | Loss: 0.00002390
Iteration 71/1000 | Loss: 0.00002390
Iteration 72/1000 | Loss: 0.00002390
Iteration 73/1000 | Loss: 0.00002389
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00002389
Iteration 76/1000 | Loss: 0.00002389
Iteration 77/1000 | Loss: 0.00002389
Iteration 78/1000 | Loss: 0.00002389
Iteration 79/1000 | Loss: 0.00002389
Iteration 80/1000 | Loss: 0.00002388
Iteration 81/1000 | Loss: 0.00002388
Iteration 82/1000 | Loss: 0.00002388
Iteration 83/1000 | Loss: 0.00002388
Iteration 84/1000 | Loss: 0.00002388
Iteration 85/1000 | Loss: 0.00002388
Iteration 86/1000 | Loss: 0.00002388
Iteration 87/1000 | Loss: 0.00002388
Iteration 88/1000 | Loss: 0.00002388
Iteration 89/1000 | Loss: 0.00002388
Iteration 90/1000 | Loss: 0.00002388
Iteration 91/1000 | Loss: 0.00002388
Iteration 92/1000 | Loss: 0.00002388
Iteration 93/1000 | Loss: 0.00002388
Iteration 94/1000 | Loss: 0.00002388
Iteration 95/1000 | Loss: 0.00002388
Iteration 96/1000 | Loss: 0.00002387
Iteration 97/1000 | Loss: 0.00002387
Iteration 98/1000 | Loss: 0.00002387
Iteration 99/1000 | Loss: 0.00002386
Iteration 100/1000 | Loss: 0.00002386
Iteration 101/1000 | Loss: 0.00002386
Iteration 102/1000 | Loss: 0.00002386
Iteration 103/1000 | Loss: 0.00002386
Iteration 104/1000 | Loss: 0.00002386
Iteration 105/1000 | Loss: 0.00002386
Iteration 106/1000 | Loss: 0.00002386
Iteration 107/1000 | Loss: 0.00002386
Iteration 108/1000 | Loss: 0.00002386
Iteration 109/1000 | Loss: 0.00002385
Iteration 110/1000 | Loss: 0.00002385
Iteration 111/1000 | Loss: 0.00002385
Iteration 112/1000 | Loss: 0.00002384
Iteration 113/1000 | Loss: 0.00002384
Iteration 114/1000 | Loss: 0.00002384
Iteration 115/1000 | Loss: 0.00002384
Iteration 116/1000 | Loss: 0.00002384
Iteration 117/1000 | Loss: 0.00002384
Iteration 118/1000 | Loss: 0.00002383
Iteration 119/1000 | Loss: 0.00002383
Iteration 120/1000 | Loss: 0.00002383
Iteration 121/1000 | Loss: 0.00002383
Iteration 122/1000 | Loss: 0.00002383
Iteration 123/1000 | Loss: 0.00002383
Iteration 124/1000 | Loss: 0.00002383
Iteration 125/1000 | Loss: 0.00002383
Iteration 126/1000 | Loss: 0.00002383
Iteration 127/1000 | Loss: 0.00002383
Iteration 128/1000 | Loss: 0.00002383
Iteration 129/1000 | Loss: 0.00002383
Iteration 130/1000 | Loss: 0.00002383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.3834125386201777e-05, 2.3834125386201777e-05, 2.3834125386201777e-05, 2.3834125386201777e-05, 2.3834125386201777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3834125386201777e-05

Optimization complete. Final v2v error: 4.120182037353516 mm

Highest mean error: 4.415868282318115 mm for frame 215

Lowest mean error: 3.8241117000579834 mm for frame 131

Saving results

Total time: 39.920974016189575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901079
Iteration 2/25 | Loss: 0.00192318
Iteration 3/25 | Loss: 0.00157183
Iteration 4/25 | Loss: 0.00150387
Iteration 5/25 | Loss: 0.00150146
Iteration 6/25 | Loss: 0.00141214
Iteration 7/25 | Loss: 0.00137292
Iteration 8/25 | Loss: 0.00135661
Iteration 9/25 | Loss: 0.00135125
Iteration 10/25 | Loss: 0.00137066
Iteration 11/25 | Loss: 0.00136565
Iteration 12/25 | Loss: 0.00135361
Iteration 13/25 | Loss: 0.00133414
Iteration 14/25 | Loss: 0.00132699
Iteration 15/25 | Loss: 0.00132515
Iteration 16/25 | Loss: 0.00132467
Iteration 17/25 | Loss: 0.00132452
Iteration 18/25 | Loss: 0.00132447
Iteration 19/25 | Loss: 0.00132447
Iteration 20/25 | Loss: 0.00132447
Iteration 21/25 | Loss: 0.00132447
Iteration 22/25 | Loss: 0.00132447
Iteration 23/25 | Loss: 0.00132447
Iteration 24/25 | Loss: 0.00132447
Iteration 25/25 | Loss: 0.00132447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013244673609733582, 0.0013244673609733582, 0.0013244673609733582, 0.0013244673609733582, 0.0013244673609733582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013244673609733582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34695578
Iteration 2/25 | Loss: 0.00110729
Iteration 3/25 | Loss: 0.00110726
Iteration 4/25 | Loss: 0.00110726
Iteration 5/25 | Loss: 0.00110726
Iteration 6/25 | Loss: 0.00110726
Iteration 7/25 | Loss: 0.00110726
Iteration 8/25 | Loss: 0.00110726
Iteration 9/25 | Loss: 0.00110726
Iteration 10/25 | Loss: 0.00110726
Iteration 11/25 | Loss: 0.00110726
Iteration 12/25 | Loss: 0.00110726
Iteration 13/25 | Loss: 0.00110726
Iteration 14/25 | Loss: 0.00110726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011072595370933414, 0.0011072595370933414, 0.0011072595370933414, 0.0011072595370933414, 0.0011072595370933414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011072595370933414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110726
Iteration 2/1000 | Loss: 0.00029882
Iteration 3/1000 | Loss: 0.00038455
Iteration 4/1000 | Loss: 0.00039056
Iteration 5/1000 | Loss: 0.00021334
Iteration 6/1000 | Loss: 0.00014705
Iteration 7/1000 | Loss: 0.00010400
Iteration 8/1000 | Loss: 0.00008436
Iteration 9/1000 | Loss: 0.00007669
Iteration 10/1000 | Loss: 0.00007043
Iteration 11/1000 | Loss: 0.00016607
Iteration 12/1000 | Loss: 0.00015848
Iteration 13/1000 | Loss: 0.00010334
Iteration 14/1000 | Loss: 0.00008790
Iteration 15/1000 | Loss: 0.00014601
Iteration 16/1000 | Loss: 0.00015149
Iteration 17/1000 | Loss: 0.00011588
Iteration 18/1000 | Loss: 0.00008561
Iteration 19/1000 | Loss: 0.00007607
Iteration 20/1000 | Loss: 0.00006906
Iteration 21/1000 | Loss: 0.00006175
Iteration 22/1000 | Loss: 0.00005537
Iteration 23/1000 | Loss: 0.00005480
Iteration 24/1000 | Loss: 0.00004932
Iteration 25/1000 | Loss: 0.00005198
Iteration 26/1000 | Loss: 0.00004685
Iteration 27/1000 | Loss: 0.00004192
Iteration 28/1000 | Loss: 0.00003952
Iteration 29/1000 | Loss: 0.00005729
Iteration 30/1000 | Loss: 0.00005506
Iteration 31/1000 | Loss: 0.00004612
Iteration 32/1000 | Loss: 0.00004238
Iteration 33/1000 | Loss: 0.00006623
Iteration 34/1000 | Loss: 0.00006458
Iteration 35/1000 | Loss: 0.00006492
Iteration 36/1000 | Loss: 0.00007756
Iteration 37/1000 | Loss: 0.00006821
Iteration 38/1000 | Loss: 0.00006191
Iteration 39/1000 | Loss: 0.00005547
Iteration 40/1000 | Loss: 0.00004653
Iteration 41/1000 | Loss: 0.00003916
Iteration 42/1000 | Loss: 0.00003679
Iteration 43/1000 | Loss: 0.00003836
Iteration 44/1000 | Loss: 0.00005523
Iteration 45/1000 | Loss: 0.00006912
Iteration 46/1000 | Loss: 0.00006610
Iteration 47/1000 | Loss: 0.00005769
Iteration 48/1000 | Loss: 0.00006659
Iteration 49/1000 | Loss: 0.00006169
Iteration 50/1000 | Loss: 0.00005220
Iteration 51/1000 | Loss: 0.00003731
Iteration 52/1000 | Loss: 0.00004567
Iteration 53/1000 | Loss: 0.00006722
Iteration 54/1000 | Loss: 0.00005643
Iteration 55/1000 | Loss: 0.00004777
Iteration 56/1000 | Loss: 0.00003711
Iteration 57/1000 | Loss: 0.00003552
Iteration 58/1000 | Loss: 0.00004447
Iteration 59/1000 | Loss: 0.00009004
Iteration 60/1000 | Loss: 0.00005755
Iteration 61/1000 | Loss: 0.00004643
Iteration 62/1000 | Loss: 0.00004177
Iteration 63/1000 | Loss: 0.00005884
Iteration 64/1000 | Loss: 0.00003777
Iteration 65/1000 | Loss: 0.00003051
Iteration 66/1000 | Loss: 0.00002894
Iteration 67/1000 | Loss: 0.00002826
Iteration 68/1000 | Loss: 0.00002772
Iteration 69/1000 | Loss: 0.00002736
Iteration 70/1000 | Loss: 0.00002711
Iteration 71/1000 | Loss: 0.00007801
Iteration 72/1000 | Loss: 0.00010617
Iteration 73/1000 | Loss: 0.00005371
Iteration 74/1000 | Loss: 0.00008134
Iteration 75/1000 | Loss: 0.00005782
Iteration 76/1000 | Loss: 0.00005000
Iteration 77/1000 | Loss: 0.00003576
Iteration 78/1000 | Loss: 0.00004127
Iteration 79/1000 | Loss: 0.00004048
Iteration 80/1000 | Loss: 0.00003721
Iteration 81/1000 | Loss: 0.00003074
Iteration 82/1000 | Loss: 0.00002994
Iteration 83/1000 | Loss: 0.00004562
Iteration 84/1000 | Loss: 0.00003324
Iteration 85/1000 | Loss: 0.00002978
Iteration 86/1000 | Loss: 0.00002879
Iteration 87/1000 | Loss: 0.00002800
Iteration 88/1000 | Loss: 0.00002723
Iteration 89/1000 | Loss: 0.00002693
Iteration 90/1000 | Loss: 0.00002664
Iteration 91/1000 | Loss: 0.00002617
Iteration 92/1000 | Loss: 0.00004066
Iteration 93/1000 | Loss: 0.00003637
Iteration 94/1000 | Loss: 0.00002606
Iteration 95/1000 | Loss: 0.00002572
Iteration 96/1000 | Loss: 0.00002554
Iteration 97/1000 | Loss: 0.00002551
Iteration 98/1000 | Loss: 0.00002548
Iteration 99/1000 | Loss: 0.00003866
Iteration 100/1000 | Loss: 0.00005700
Iteration 101/1000 | Loss: 0.00005267
Iteration 102/1000 | Loss: 0.00002638
Iteration 103/1000 | Loss: 0.00003760
Iteration 104/1000 | Loss: 0.00003489
Iteration 105/1000 | Loss: 0.00003705
Iteration 106/1000 | Loss: 0.00002787
Iteration 107/1000 | Loss: 0.00002692
Iteration 108/1000 | Loss: 0.00003979
Iteration 109/1000 | Loss: 0.00003594
Iteration 110/1000 | Loss: 0.00002955
Iteration 111/1000 | Loss: 0.00003788
Iteration 112/1000 | Loss: 0.00003852
Iteration 113/1000 | Loss: 0.00003985
Iteration 114/1000 | Loss: 0.00003414
Iteration 115/1000 | Loss: 0.00003186
Iteration 116/1000 | Loss: 0.00003802
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002594
Iteration 119/1000 | Loss: 0.00002594
Iteration 120/1000 | Loss: 0.00002593
Iteration 121/1000 | Loss: 0.00002592
Iteration 122/1000 | Loss: 0.00002592
Iteration 123/1000 | Loss: 0.00002591
Iteration 124/1000 | Loss: 0.00003835
Iteration 125/1000 | Loss: 0.00003809
Iteration 126/1000 | Loss: 0.00003844
Iteration 127/1000 | Loss: 0.00003837
Iteration 128/1000 | Loss: 0.00003836
Iteration 129/1000 | Loss: 0.00003565
Iteration 130/1000 | Loss: 0.00003829
Iteration 131/1000 | Loss: 0.00003593
Iteration 132/1000 | Loss: 0.00003919
Iteration 133/1000 | Loss: 0.00003843
Iteration 134/1000 | Loss: 0.00003800
Iteration 135/1000 | Loss: 0.00003663
Iteration 136/1000 | Loss: 0.00003772
Iteration 137/1000 | Loss: 0.00003641
Iteration 138/1000 | Loss: 0.00003785
Iteration 139/1000 | Loss: 0.00003749
Iteration 140/1000 | Loss: 0.00003735
Iteration 141/1000 | Loss: 0.00003721
Iteration 142/1000 | Loss: 0.00003649
Iteration 143/1000 | Loss: 0.00003144
Iteration 144/1000 | Loss: 0.00003635
Iteration 145/1000 | Loss: 0.00003119
Iteration 146/1000 | Loss: 0.00003586
Iteration 147/1000 | Loss: 0.00003075
Iteration 148/1000 | Loss: 0.00003825
Iteration 149/1000 | Loss: 0.00003033
Iteration 150/1000 | Loss: 0.00003703
Iteration 151/1000 | Loss: 0.00003665
Iteration 152/1000 | Loss: 0.00003182
Iteration 153/1000 | Loss: 0.00002625
Iteration 154/1000 | Loss: 0.00003738
Iteration 155/1000 | Loss: 0.00003070
Iteration 156/1000 | Loss: 0.00003797
Iteration 157/1000 | Loss: 0.00002964
Iteration 158/1000 | Loss: 0.00004921
Iteration 159/1000 | Loss: 0.00002869
Iteration 160/1000 | Loss: 0.00003238
Iteration 161/1000 | Loss: 0.00003549
Iteration 162/1000 | Loss: 0.00003792
Iteration 163/1000 | Loss: 0.00003246
Iteration 164/1000 | Loss: 0.00003505
Iteration 165/1000 | Loss: 0.00003082
Iteration 166/1000 | Loss: 0.00003556
Iteration 167/1000 | Loss: 0.00003790
Iteration 168/1000 | Loss: 0.00003440
Iteration 169/1000 | Loss: 0.00004091
Iteration 170/1000 | Loss: 0.00003189
Iteration 171/1000 | Loss: 0.00004045
Iteration 172/1000 | Loss: 0.00003718
Iteration 173/1000 | Loss: 0.00003503
Iteration 174/1000 | Loss: 0.00004726
Iteration 175/1000 | Loss: 0.00004074
Iteration 176/1000 | Loss: 0.00003277
Iteration 177/1000 | Loss: 0.00004086
Iteration 178/1000 | Loss: 0.00003148
Iteration 179/1000 | Loss: 0.00004090
Iteration 180/1000 | Loss: 0.00004090
Iteration 181/1000 | Loss: 0.00003186
Iteration 182/1000 | Loss: 0.00004005
Iteration 183/1000 | Loss: 0.00003759
Iteration 184/1000 | Loss: 0.00004325
Iteration 185/1000 | Loss: 0.00003215
Iteration 186/1000 | Loss: 0.00003764
Iteration 187/1000 | Loss: 0.00003167
Iteration 188/1000 | Loss: 0.00003783
Iteration 189/1000 | Loss: 0.00003657
Iteration 190/1000 | Loss: 0.00003749
Iteration 191/1000 | Loss: 0.00003409
Iteration 192/1000 | Loss: 0.00003549
Iteration 193/1000 | Loss: 0.00003277
Iteration 194/1000 | Loss: 0.00003244
Iteration 195/1000 | Loss: 0.00003558
Iteration 196/1000 | Loss: 0.00004082
Iteration 197/1000 | Loss: 0.00003467
Iteration 198/1000 | Loss: 0.00004593
Iteration 199/1000 | Loss: 0.00003417
Iteration 200/1000 | Loss: 0.00004460
Iteration 201/1000 | Loss: 0.00003827
Iteration 202/1000 | Loss: 0.00003317
Iteration 203/1000 | Loss: 0.00003779
Iteration 204/1000 | Loss: 0.00003378
Iteration 205/1000 | Loss: 0.00003953
Iteration 206/1000 | Loss: 0.00003876
Iteration 207/1000 | Loss: 0.00003848
Iteration 208/1000 | Loss: 0.00003835
Iteration 209/1000 | Loss: 0.00003093
Iteration 210/1000 | Loss: 0.00003017
Iteration 211/1000 | Loss: 0.00003883
Iteration 212/1000 | Loss: 0.00003387
Iteration 213/1000 | Loss: 0.00003836
Iteration 214/1000 | Loss: 0.00003907
Iteration 215/1000 | Loss: 0.00003866
Iteration 216/1000 | Loss: 0.00003843
Iteration 217/1000 | Loss: 0.00003842
Iteration 218/1000 | Loss: 0.00003223
Iteration 219/1000 | Loss: 0.00003753
Iteration 220/1000 | Loss: 0.00003750
Iteration 221/1000 | Loss: 0.00003663
Iteration 222/1000 | Loss: 0.00002921
Iteration 223/1000 | Loss: 0.00003035
Iteration 224/1000 | Loss: 0.00003853
Iteration 225/1000 | Loss: 0.00002916
Iteration 226/1000 | Loss: 0.00003641
Iteration 227/1000 | Loss: 0.00003133
Iteration 228/1000 | Loss: 0.00003006
Iteration 229/1000 | Loss: 0.00003509
Iteration 230/1000 | Loss: 0.00003996
Iteration 231/1000 | Loss: 0.00002933
Iteration 232/1000 | Loss: 0.00002743
Iteration 233/1000 | Loss: 0.00002695
Iteration 234/1000 | Loss: 0.00005449
Iteration 235/1000 | Loss: 0.00004921
Iteration 236/1000 | Loss: 0.00004178
Iteration 237/1000 | Loss: 0.00003136
Iteration 238/1000 | Loss: 0.00002597
Iteration 239/1000 | Loss: 0.00002575
Iteration 240/1000 | Loss: 0.00002549
Iteration 241/1000 | Loss: 0.00002538
Iteration 242/1000 | Loss: 0.00002537
Iteration 243/1000 | Loss: 0.00002535
Iteration 244/1000 | Loss: 0.00002523
Iteration 245/1000 | Loss: 0.00002523
Iteration 246/1000 | Loss: 0.00002523
Iteration 247/1000 | Loss: 0.00002522
Iteration 248/1000 | Loss: 0.00002522
Iteration 249/1000 | Loss: 0.00002522
Iteration 250/1000 | Loss: 0.00002521
Iteration 251/1000 | Loss: 0.00002521
Iteration 252/1000 | Loss: 0.00002521
Iteration 253/1000 | Loss: 0.00002520
Iteration 254/1000 | Loss: 0.00002520
Iteration 255/1000 | Loss: 0.00002519
Iteration 256/1000 | Loss: 0.00002518
Iteration 257/1000 | Loss: 0.00002518
Iteration 258/1000 | Loss: 0.00002518
Iteration 259/1000 | Loss: 0.00002517
Iteration 260/1000 | Loss: 0.00002517
Iteration 261/1000 | Loss: 0.00002517
Iteration 262/1000 | Loss: 0.00002517
Iteration 263/1000 | Loss: 0.00002517
Iteration 264/1000 | Loss: 0.00002516
Iteration 265/1000 | Loss: 0.00002516
Iteration 266/1000 | Loss: 0.00002516
Iteration 267/1000 | Loss: 0.00002516
Iteration 268/1000 | Loss: 0.00002516
Iteration 269/1000 | Loss: 0.00002516
Iteration 270/1000 | Loss: 0.00002516
Iteration 271/1000 | Loss: 0.00002516
Iteration 272/1000 | Loss: 0.00002516
Iteration 273/1000 | Loss: 0.00002516
Iteration 274/1000 | Loss: 0.00002516
Iteration 275/1000 | Loss: 0.00002516
Iteration 276/1000 | Loss: 0.00002516
Iteration 277/1000 | Loss: 0.00002516
Iteration 278/1000 | Loss: 0.00002516
Iteration 279/1000 | Loss: 0.00002516
Iteration 280/1000 | Loss: 0.00002516
Iteration 281/1000 | Loss: 0.00002516
Iteration 282/1000 | Loss: 0.00002516
Iteration 283/1000 | Loss: 0.00002516
Iteration 284/1000 | Loss: 0.00002516
Iteration 285/1000 | Loss: 0.00002516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.5158551579806954e-05, 2.5158551579806954e-05, 2.5158551579806954e-05, 2.5158551579806954e-05, 2.5158551579806954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5158551579806954e-05

Optimization complete. Final v2v error: 4.020678520202637 mm

Highest mean error: 8.03874397277832 mm for frame 114

Lowest mean error: 3.074284791946411 mm for frame 74

Saving results

Total time: 364.75814843177795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875133
Iteration 2/25 | Loss: 0.00177812
Iteration 3/25 | Loss: 0.00140545
Iteration 4/25 | Loss: 0.00134362
Iteration 5/25 | Loss: 0.00133074
Iteration 6/25 | Loss: 0.00131407
Iteration 7/25 | Loss: 0.00130869
Iteration 8/25 | Loss: 0.00130709
Iteration 9/25 | Loss: 0.00130268
Iteration 10/25 | Loss: 0.00129477
Iteration 11/25 | Loss: 0.00129245
Iteration 12/25 | Loss: 0.00129756
Iteration 13/25 | Loss: 0.00129709
Iteration 14/25 | Loss: 0.00129498
Iteration 15/25 | Loss: 0.00129101
Iteration 16/25 | Loss: 0.00129005
Iteration 17/25 | Loss: 0.00128985
Iteration 18/25 | Loss: 0.00128980
Iteration 19/25 | Loss: 0.00128980
Iteration 20/25 | Loss: 0.00128979
Iteration 21/25 | Loss: 0.00128979
Iteration 22/25 | Loss: 0.00128979
Iteration 23/25 | Loss: 0.00128979
Iteration 24/25 | Loss: 0.00128978
Iteration 25/25 | Loss: 0.00128978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.70511532
Iteration 2/25 | Loss: 0.00090067
Iteration 3/25 | Loss: 0.00090067
Iteration 4/25 | Loss: 0.00090067
Iteration 5/25 | Loss: 0.00090067
Iteration 6/25 | Loss: 0.00090067
Iteration 7/25 | Loss: 0.00090066
Iteration 8/25 | Loss: 0.00090066
Iteration 9/25 | Loss: 0.00090066
Iteration 10/25 | Loss: 0.00090066
Iteration 11/25 | Loss: 0.00090066
Iteration 12/25 | Loss: 0.00090066
Iteration 13/25 | Loss: 0.00090066
Iteration 14/25 | Loss: 0.00090066
Iteration 15/25 | Loss: 0.00090066
Iteration 16/25 | Loss: 0.00090066
Iteration 17/25 | Loss: 0.00090066
Iteration 18/25 | Loss: 0.00090066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009006639593280852, 0.0009006639593280852, 0.0009006639593280852, 0.0009006639593280852, 0.0009006639593280852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009006639593280852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090066
Iteration 2/1000 | Loss: 0.00005387
Iteration 3/1000 | Loss: 0.00027728
Iteration 4/1000 | Loss: 0.00020176
Iteration 5/1000 | Loss: 0.00007369
Iteration 6/1000 | Loss: 0.00003329
Iteration 7/1000 | Loss: 0.00018374
Iteration 8/1000 | Loss: 0.00014697
Iteration 9/1000 | Loss: 0.00018713
Iteration 10/1000 | Loss: 0.00011694
Iteration 11/1000 | Loss: 0.00002616
Iteration 12/1000 | Loss: 0.00002454
Iteration 13/1000 | Loss: 0.00002313
Iteration 14/1000 | Loss: 0.00017636
Iteration 15/1000 | Loss: 0.00002547
Iteration 16/1000 | Loss: 0.00002309
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002132
Iteration 21/1000 | Loss: 0.00002102
Iteration 22/1000 | Loss: 0.00031655
Iteration 23/1000 | Loss: 0.00003885
Iteration 24/1000 | Loss: 0.00010429
Iteration 25/1000 | Loss: 0.00025742
Iteration 26/1000 | Loss: 0.00023278
Iteration 27/1000 | Loss: 0.00003031
Iteration 28/1000 | Loss: 0.00002719
Iteration 29/1000 | Loss: 0.00002595
Iteration 30/1000 | Loss: 0.00002527
Iteration 31/1000 | Loss: 0.00002488
Iteration 32/1000 | Loss: 0.00002687
Iteration 33/1000 | Loss: 0.00002435
Iteration 34/1000 | Loss: 0.00002517
Iteration 35/1000 | Loss: 0.00002482
Iteration 36/1000 | Loss: 0.00002432
Iteration 37/1000 | Loss: 0.00012978
Iteration 38/1000 | Loss: 0.00002315
Iteration 39/1000 | Loss: 0.00002274
Iteration 40/1000 | Loss: 0.00002242
Iteration 41/1000 | Loss: 0.00009330
Iteration 42/1000 | Loss: 0.00002136
Iteration 43/1000 | Loss: 0.00002064
Iteration 44/1000 | Loss: 0.00002027
Iteration 45/1000 | Loss: 0.00011700
Iteration 46/1000 | Loss: 0.00003294
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00004834
Iteration 50/1000 | Loss: 0.00003539
Iteration 51/1000 | Loss: 0.00001943
Iteration 52/1000 | Loss: 0.00001927
Iteration 53/1000 | Loss: 0.00001925
Iteration 54/1000 | Loss: 0.00001925
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001902
Iteration 64/1000 | Loss: 0.00003840
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001897
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001896
Iteration 78/1000 | Loss: 0.00001896
Iteration 79/1000 | Loss: 0.00001896
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001896
Iteration 82/1000 | Loss: 0.00001896
Iteration 83/1000 | Loss: 0.00001896
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001895
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001894
Iteration 89/1000 | Loss: 0.00001894
Iteration 90/1000 | Loss: 0.00001894
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001893
Iteration 93/1000 | Loss: 0.00001893
Iteration 94/1000 | Loss: 0.00001893
Iteration 95/1000 | Loss: 0.00001893
Iteration 96/1000 | Loss: 0.00001893
Iteration 97/1000 | Loss: 0.00001893
Iteration 98/1000 | Loss: 0.00001892
Iteration 99/1000 | Loss: 0.00001892
Iteration 100/1000 | Loss: 0.00001892
Iteration 101/1000 | Loss: 0.00001892
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001891
Iteration 105/1000 | Loss: 0.00001891
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001888
Iteration 124/1000 | Loss: 0.00001888
Iteration 125/1000 | Loss: 0.00001888
Iteration 126/1000 | Loss: 0.00001888
Iteration 127/1000 | Loss: 0.00001888
Iteration 128/1000 | Loss: 0.00001888
Iteration 129/1000 | Loss: 0.00001888
Iteration 130/1000 | Loss: 0.00001888
Iteration 131/1000 | Loss: 0.00001887
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001887
Iteration 134/1000 | Loss: 0.00001887
Iteration 135/1000 | Loss: 0.00001887
Iteration 136/1000 | Loss: 0.00001887
Iteration 137/1000 | Loss: 0.00001887
Iteration 138/1000 | Loss: 0.00001887
Iteration 139/1000 | Loss: 0.00001887
Iteration 140/1000 | Loss: 0.00001887
Iteration 141/1000 | Loss: 0.00001887
Iteration 142/1000 | Loss: 0.00001887
Iteration 143/1000 | Loss: 0.00001887
Iteration 144/1000 | Loss: 0.00001887
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001887
Iteration 147/1000 | Loss: 0.00001887
Iteration 148/1000 | Loss: 0.00001887
Iteration 149/1000 | Loss: 0.00001887
Iteration 150/1000 | Loss: 0.00001887
Iteration 151/1000 | Loss: 0.00001887
Iteration 152/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.886938298412133e-05, 1.886938298412133e-05, 1.886938298412133e-05, 1.886938298412133e-05, 1.886938298412133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.886938298412133e-05

Optimization complete. Final v2v error: 3.611121892929077 mm

Highest mean error: 5.662576675415039 mm for frame 99

Lowest mean error: 2.952511787414551 mm for frame 17

Saving results

Total time: 109.97013354301453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798475
Iteration 2/25 | Loss: 0.00155589
Iteration 3/25 | Loss: 0.00131172
Iteration 4/25 | Loss: 0.00127615
Iteration 5/25 | Loss: 0.00127296
Iteration 6/25 | Loss: 0.00126886
Iteration 7/25 | Loss: 0.00126440
Iteration 8/25 | Loss: 0.00126180
Iteration 9/25 | Loss: 0.00125853
Iteration 10/25 | Loss: 0.00125643
Iteration 11/25 | Loss: 0.00125493
Iteration 12/25 | Loss: 0.00125727
Iteration 13/25 | Loss: 0.00125709
Iteration 14/25 | Loss: 0.00125321
Iteration 15/25 | Loss: 0.00125233
Iteration 16/25 | Loss: 0.00125113
Iteration 17/25 | Loss: 0.00125017
Iteration 18/25 | Loss: 0.00124956
Iteration 19/25 | Loss: 0.00124938
Iteration 20/25 | Loss: 0.00124937
Iteration 21/25 | Loss: 0.00124937
Iteration 22/25 | Loss: 0.00124937
Iteration 23/25 | Loss: 0.00124937
Iteration 24/25 | Loss: 0.00124937
Iteration 25/25 | Loss: 0.00124936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41483366
Iteration 2/25 | Loss: 0.00111024
Iteration 3/25 | Loss: 0.00111023
Iteration 4/25 | Loss: 0.00111023
Iteration 5/25 | Loss: 0.00111023
Iteration 6/25 | Loss: 0.00111023
Iteration 7/25 | Loss: 0.00111023
Iteration 8/25 | Loss: 0.00111023
Iteration 9/25 | Loss: 0.00111023
Iteration 10/25 | Loss: 0.00111023
Iteration 11/25 | Loss: 0.00111023
Iteration 12/25 | Loss: 0.00111023
Iteration 13/25 | Loss: 0.00111023
Iteration 14/25 | Loss: 0.00111023
Iteration 15/25 | Loss: 0.00111023
Iteration 16/25 | Loss: 0.00111023
Iteration 17/25 | Loss: 0.00111023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011102292919531465, 0.0011102292919531465, 0.0011102292919531465, 0.0011102292919531465, 0.0011102292919531465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011102292919531465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111023
Iteration 2/1000 | Loss: 0.00007660
Iteration 3/1000 | Loss: 0.00005508
Iteration 4/1000 | Loss: 0.00005053
Iteration 5/1000 | Loss: 0.00004672
Iteration 6/1000 | Loss: 0.00004434
Iteration 7/1000 | Loss: 0.00004286
Iteration 8/1000 | Loss: 0.00004135
Iteration 9/1000 | Loss: 0.00003999
Iteration 10/1000 | Loss: 0.00003912
Iteration 11/1000 | Loss: 0.00003846
Iteration 12/1000 | Loss: 0.00090171
Iteration 13/1000 | Loss: 0.00120227
Iteration 14/1000 | Loss: 0.00047776
Iteration 15/1000 | Loss: 0.00046157
Iteration 16/1000 | Loss: 0.00006236
Iteration 17/1000 | Loss: 0.00007402
Iteration 18/1000 | Loss: 0.00033451
Iteration 19/1000 | Loss: 0.00011218
Iteration 20/1000 | Loss: 0.00011419
Iteration 21/1000 | Loss: 0.00005205
Iteration 22/1000 | Loss: 0.00004415
Iteration 23/1000 | Loss: 0.00004032
Iteration 24/1000 | Loss: 0.00003806
Iteration 25/1000 | Loss: 0.00005161
Iteration 26/1000 | Loss: 0.00035890
Iteration 27/1000 | Loss: 0.00018763
Iteration 28/1000 | Loss: 0.00073239
Iteration 29/1000 | Loss: 0.00065951
Iteration 30/1000 | Loss: 0.00101375
Iteration 31/1000 | Loss: 0.00050540
Iteration 32/1000 | Loss: 0.00022757
Iteration 33/1000 | Loss: 0.00027228
Iteration 34/1000 | Loss: 0.00056582
Iteration 35/1000 | Loss: 0.00036448
Iteration 36/1000 | Loss: 0.00054154
Iteration 37/1000 | Loss: 0.00047404
Iteration 38/1000 | Loss: 0.00055068
Iteration 39/1000 | Loss: 0.00067301
Iteration 40/1000 | Loss: 0.00020174
Iteration 41/1000 | Loss: 0.00011240
Iteration 42/1000 | Loss: 0.00008066
Iteration 43/1000 | Loss: 0.00011798
Iteration 44/1000 | Loss: 0.00016278
Iteration 45/1000 | Loss: 0.00010836
Iteration 46/1000 | Loss: 0.00010831
Iteration 47/1000 | Loss: 0.00012829
Iteration 48/1000 | Loss: 0.00011662
Iteration 49/1000 | Loss: 0.00008327
Iteration 50/1000 | Loss: 0.00006795
Iteration 51/1000 | Loss: 0.00004866
Iteration 52/1000 | Loss: 0.00005430
Iteration 53/1000 | Loss: 0.00008107
Iteration 54/1000 | Loss: 0.00005061
Iteration 55/1000 | Loss: 0.00007179
Iteration 56/1000 | Loss: 0.00015025
Iteration 57/1000 | Loss: 0.00014832
Iteration 58/1000 | Loss: 0.00017558
Iteration 59/1000 | Loss: 0.00004691
Iteration 60/1000 | Loss: 0.00017282
Iteration 61/1000 | Loss: 0.00014696
Iteration 62/1000 | Loss: 0.00018652
Iteration 63/1000 | Loss: 0.00008036
Iteration 64/1000 | Loss: 0.00016835
Iteration 65/1000 | Loss: 0.00003525
Iteration 66/1000 | Loss: 0.00002833
Iteration 67/1000 | Loss: 0.00002502
Iteration 68/1000 | Loss: 0.00002400
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002308
Iteration 71/1000 | Loss: 0.00002271
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002205
Iteration 74/1000 | Loss: 0.00002195
Iteration 75/1000 | Loss: 0.00002182
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00036972
Iteration 79/1000 | Loss: 0.00035368
Iteration 80/1000 | Loss: 0.00020624
Iteration 81/1000 | Loss: 0.00003271
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00016654
Iteration 84/1000 | Loss: 0.00003360
Iteration 85/1000 | Loss: 0.00010194
Iteration 86/1000 | Loss: 0.00009802
Iteration 87/1000 | Loss: 0.00010487
Iteration 88/1000 | Loss: 0.00009688
Iteration 89/1000 | Loss: 0.00010645
Iteration 90/1000 | Loss: 0.00008972
Iteration 91/1000 | Loss: 0.00009824
Iteration 92/1000 | Loss: 0.00008421
Iteration 93/1000 | Loss: 0.00009952
Iteration 94/1000 | Loss: 0.00009904
Iteration 95/1000 | Loss: 0.00002700
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002369
Iteration 98/1000 | Loss: 0.00002302
Iteration 99/1000 | Loss: 0.00002264
Iteration 100/1000 | Loss: 0.00027763
Iteration 101/1000 | Loss: 0.00028372
Iteration 102/1000 | Loss: 0.00017317
Iteration 103/1000 | Loss: 0.00002563
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00004012
Iteration 106/1000 | Loss: 0.00003094
Iteration 107/1000 | Loss: 0.00030956
Iteration 108/1000 | Loss: 0.00016675
Iteration 109/1000 | Loss: 0.00023897
Iteration 110/1000 | Loss: 0.00020063
Iteration 111/1000 | Loss: 0.00002966
Iteration 112/1000 | Loss: 0.00003355
Iteration 113/1000 | Loss: 0.00002495
Iteration 114/1000 | Loss: 0.00002364
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002291
Iteration 117/1000 | Loss: 0.00020450
Iteration 118/1000 | Loss: 0.00011167
Iteration 119/1000 | Loss: 0.00002520
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002158
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002146
Iteration 124/1000 | Loss: 0.00002146
Iteration 125/1000 | Loss: 0.00002146
Iteration 126/1000 | Loss: 0.00002146
Iteration 127/1000 | Loss: 0.00002146
Iteration 128/1000 | Loss: 0.00002146
Iteration 129/1000 | Loss: 0.00002146
Iteration 130/1000 | Loss: 0.00002146
Iteration 131/1000 | Loss: 0.00002146
Iteration 132/1000 | Loss: 0.00002146
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002145
Iteration 135/1000 | Loss: 0.00002241
Iteration 136/1000 | Loss: 0.00002188
Iteration 137/1000 | Loss: 0.00002172
Iteration 138/1000 | Loss: 0.00002171
Iteration 139/1000 | Loss: 0.00002171
Iteration 140/1000 | Loss: 0.00002170
Iteration 141/1000 | Loss: 0.00002164
Iteration 142/1000 | Loss: 0.00002164
Iteration 143/1000 | Loss: 0.00002164
Iteration 144/1000 | Loss: 0.00002164
Iteration 145/1000 | Loss: 0.00002164
Iteration 146/1000 | Loss: 0.00002164
Iteration 147/1000 | Loss: 0.00002163
Iteration 148/1000 | Loss: 0.00002163
Iteration 149/1000 | Loss: 0.00002163
Iteration 150/1000 | Loss: 0.00002163
Iteration 151/1000 | Loss: 0.00002163
Iteration 152/1000 | Loss: 0.00002162
Iteration 153/1000 | Loss: 0.00012674
Iteration 154/1000 | Loss: 0.00033844
Iteration 155/1000 | Loss: 0.00004053
Iteration 156/1000 | Loss: 0.00004186
Iteration 157/1000 | Loss: 0.00002747
Iteration 158/1000 | Loss: 0.00004062
Iteration 159/1000 | Loss: 0.00023316
Iteration 160/1000 | Loss: 0.00002478
Iteration 161/1000 | Loss: 0.00022531
Iteration 162/1000 | Loss: 0.00002567
Iteration 163/1000 | Loss: 0.00002250
Iteration 164/1000 | Loss: 0.00012837
Iteration 165/1000 | Loss: 0.00003196
Iteration 166/1000 | Loss: 0.00002662
Iteration 167/1000 | Loss: 0.00002298
Iteration 168/1000 | Loss: 0.00010774
Iteration 169/1000 | Loss: 0.00017971
Iteration 170/1000 | Loss: 0.00002343
Iteration 171/1000 | Loss: 0.00002163
Iteration 172/1000 | Loss: 0.00002125
Iteration 173/1000 | Loss: 0.00011510
Iteration 174/1000 | Loss: 0.00017824
Iteration 175/1000 | Loss: 0.00002138
Iteration 176/1000 | Loss: 0.00012846
Iteration 177/1000 | Loss: 0.00019537
Iteration 178/1000 | Loss: 0.00004440
Iteration 179/1000 | Loss: 0.00007994
Iteration 180/1000 | Loss: 0.00010645
Iteration 181/1000 | Loss: 0.00002153
Iteration 182/1000 | Loss: 0.00002103
Iteration 183/1000 | Loss: 0.00002100
Iteration 184/1000 | Loss: 0.00008556
Iteration 185/1000 | Loss: 0.00008574
Iteration 186/1000 | Loss: 0.00002102
Iteration 187/1000 | Loss: 0.00003789
Iteration 188/1000 | Loss: 0.00005849
Iteration 189/1000 | Loss: 0.00005236
Iteration 190/1000 | Loss: 0.00014891
Iteration 191/1000 | Loss: 0.00029876
Iteration 192/1000 | Loss: 0.00019352
Iteration 193/1000 | Loss: 0.00020734
Iteration 194/1000 | Loss: 0.00024804
Iteration 195/1000 | Loss: 0.00004530
Iteration 196/1000 | Loss: 0.00019001
Iteration 197/1000 | Loss: 0.00002563
Iteration 198/1000 | Loss: 0.00019018
Iteration 199/1000 | Loss: 0.00033946
Iteration 200/1000 | Loss: 0.00005002
Iteration 201/1000 | Loss: 0.00015764
Iteration 202/1000 | Loss: 0.00012979
Iteration 203/1000 | Loss: 0.00041437
Iteration 204/1000 | Loss: 0.00007182
Iteration 205/1000 | Loss: 0.00002098
Iteration 206/1000 | Loss: 0.00012759
Iteration 207/1000 | Loss: 0.00020766
Iteration 208/1000 | Loss: 0.00031253
Iteration 209/1000 | Loss: 0.00007234
Iteration 210/1000 | Loss: 0.00017062
Iteration 211/1000 | Loss: 0.00026998
Iteration 212/1000 | Loss: 0.00009135
Iteration 213/1000 | Loss: 0.00002364
Iteration 214/1000 | Loss: 0.00002259
Iteration 215/1000 | Loss: 0.00008083
Iteration 216/1000 | Loss: 0.00009687
Iteration 217/1000 | Loss: 0.00007843
Iteration 218/1000 | Loss: 0.00002687
Iteration 219/1000 | Loss: 0.00002296
Iteration 220/1000 | Loss: 0.00002136
Iteration 221/1000 | Loss: 0.00002043
Iteration 222/1000 | Loss: 0.00001936
Iteration 223/1000 | Loss: 0.00001897
Iteration 224/1000 | Loss: 0.00001873
Iteration 225/1000 | Loss: 0.00001850
Iteration 226/1000 | Loss: 0.00001846
Iteration 227/1000 | Loss: 0.00001842
Iteration 228/1000 | Loss: 0.00001841
Iteration 229/1000 | Loss: 0.00001841
Iteration 230/1000 | Loss: 0.00001840
Iteration 231/1000 | Loss: 0.00001839
Iteration 232/1000 | Loss: 0.00001839
Iteration 233/1000 | Loss: 0.00001838
Iteration 234/1000 | Loss: 0.00001838
Iteration 235/1000 | Loss: 0.00001837
Iteration 236/1000 | Loss: 0.00001836
Iteration 237/1000 | Loss: 0.00001835
Iteration 238/1000 | Loss: 0.00001835
Iteration 239/1000 | Loss: 0.00001835
Iteration 240/1000 | Loss: 0.00001835
Iteration 241/1000 | Loss: 0.00001835
Iteration 242/1000 | Loss: 0.00001835
Iteration 243/1000 | Loss: 0.00001835
Iteration 244/1000 | Loss: 0.00001835
Iteration 245/1000 | Loss: 0.00001835
Iteration 246/1000 | Loss: 0.00001835
Iteration 247/1000 | Loss: 0.00001835
Iteration 248/1000 | Loss: 0.00001834
Iteration 249/1000 | Loss: 0.00001834
Iteration 250/1000 | Loss: 0.00001834
Iteration 251/1000 | Loss: 0.00001834
Iteration 252/1000 | Loss: 0.00001834
Iteration 253/1000 | Loss: 0.00001833
Iteration 254/1000 | Loss: 0.00001833
Iteration 255/1000 | Loss: 0.00001832
Iteration 256/1000 | Loss: 0.00001832
Iteration 257/1000 | Loss: 0.00001832
Iteration 258/1000 | Loss: 0.00001831
Iteration 259/1000 | Loss: 0.00001831
Iteration 260/1000 | Loss: 0.00001831
Iteration 261/1000 | Loss: 0.00001831
Iteration 262/1000 | Loss: 0.00001830
Iteration 263/1000 | Loss: 0.00001830
Iteration 264/1000 | Loss: 0.00001830
Iteration 265/1000 | Loss: 0.00001830
Iteration 266/1000 | Loss: 0.00001829
Iteration 267/1000 | Loss: 0.00001829
Iteration 268/1000 | Loss: 0.00001829
Iteration 269/1000 | Loss: 0.00001829
Iteration 270/1000 | Loss: 0.00001828
Iteration 271/1000 | Loss: 0.00001828
Iteration 272/1000 | Loss: 0.00001828
Iteration 273/1000 | Loss: 0.00001828
Iteration 274/1000 | Loss: 0.00001828
Iteration 275/1000 | Loss: 0.00001828
Iteration 276/1000 | Loss: 0.00001828
Iteration 277/1000 | Loss: 0.00001828
Iteration 278/1000 | Loss: 0.00001828
Iteration 279/1000 | Loss: 0.00001828
Iteration 280/1000 | Loss: 0.00001828
Iteration 281/1000 | Loss: 0.00001828
Iteration 282/1000 | Loss: 0.00001827
Iteration 283/1000 | Loss: 0.00001827
Iteration 284/1000 | Loss: 0.00001827
Iteration 285/1000 | Loss: 0.00001827
Iteration 286/1000 | Loss: 0.00001827
Iteration 287/1000 | Loss: 0.00001827
Iteration 288/1000 | Loss: 0.00001827
Iteration 289/1000 | Loss: 0.00001827
Iteration 290/1000 | Loss: 0.00001827
Iteration 291/1000 | Loss: 0.00001827
Iteration 292/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.8272050510859117e-05, 1.8272050510859117e-05, 1.8272050510859117e-05, 1.8272050510859117e-05, 1.8272050510859117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8272050510859117e-05

Optimization complete. Final v2v error: 3.5805230140686035 mm

Highest mean error: 5.680948257446289 mm for frame 172

Lowest mean error: 2.8945469856262207 mm for frame 146

Saving results

Total time: 348.7150869369507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991234
Iteration 2/25 | Loss: 0.00164904
Iteration 3/25 | Loss: 0.00145943
Iteration 4/25 | Loss: 0.00131264
Iteration 5/25 | Loss: 0.00128933
Iteration 6/25 | Loss: 0.00127808
Iteration 7/25 | Loss: 0.00126905
Iteration 8/25 | Loss: 0.00125776
Iteration 9/25 | Loss: 0.00125032
Iteration 10/25 | Loss: 0.00124793
Iteration 11/25 | Loss: 0.00124735
Iteration 12/25 | Loss: 0.00124717
Iteration 13/25 | Loss: 0.00124709
Iteration 14/25 | Loss: 0.00124709
Iteration 15/25 | Loss: 0.00124709
Iteration 16/25 | Loss: 0.00124709
Iteration 17/25 | Loss: 0.00124709
Iteration 18/25 | Loss: 0.00124709
Iteration 19/25 | Loss: 0.00124709
Iteration 20/25 | Loss: 0.00124708
Iteration 21/25 | Loss: 0.00124708
Iteration 22/25 | Loss: 0.00124708
Iteration 23/25 | Loss: 0.00124708
Iteration 24/25 | Loss: 0.00124708
Iteration 25/25 | Loss: 0.00124708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14254284
Iteration 2/25 | Loss: 0.00115729
Iteration 3/25 | Loss: 0.00115728
Iteration 4/25 | Loss: 0.00115728
Iteration 5/25 | Loss: 0.00115728
Iteration 6/25 | Loss: 0.00115728
Iteration 7/25 | Loss: 0.00115728
Iteration 8/25 | Loss: 0.00115728
Iteration 9/25 | Loss: 0.00115728
Iteration 10/25 | Loss: 0.00115728
Iteration 11/25 | Loss: 0.00115728
Iteration 12/25 | Loss: 0.00115728
Iteration 13/25 | Loss: 0.00115728
Iteration 14/25 | Loss: 0.00115728
Iteration 15/25 | Loss: 0.00115728
Iteration 16/25 | Loss: 0.00115728
Iteration 17/25 | Loss: 0.00115728
Iteration 18/25 | Loss: 0.00115728
Iteration 19/25 | Loss: 0.00115728
Iteration 20/25 | Loss: 0.00115728
Iteration 21/25 | Loss: 0.00115728
Iteration 22/25 | Loss: 0.00115728
Iteration 23/25 | Loss: 0.00115728
Iteration 24/25 | Loss: 0.00115728
Iteration 25/25 | Loss: 0.00115728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115728
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00003150
Iteration 4/1000 | Loss: 0.00002736
Iteration 5/1000 | Loss: 0.00021079
Iteration 6/1000 | Loss: 0.00007657
Iteration 7/1000 | Loss: 0.00002965
Iteration 8/1000 | Loss: 0.00003530
Iteration 9/1000 | Loss: 0.00002247
Iteration 10/1000 | Loss: 0.00003164
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00101816
Iteration 14/1000 | Loss: 0.00010984
Iteration 15/1000 | Loss: 0.00014911
Iteration 16/1000 | Loss: 0.00004765
Iteration 17/1000 | Loss: 0.00002518
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00023535
Iteration 20/1000 | Loss: 0.00004575
Iteration 21/1000 | Loss: 0.00008227
Iteration 22/1000 | Loss: 0.00001717
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001619
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001572
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001550
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001523
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001521
Iteration 46/1000 | Loss: 0.00001521
Iteration 47/1000 | Loss: 0.00001521
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001517
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001516
Iteration 66/1000 | Loss: 0.00001516
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001516
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001509
Iteration 94/1000 | Loss: 0.00001509
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001508
Iteration 102/1000 | Loss: 0.00001508
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001506
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001505
Iteration 117/1000 | Loss: 0.00001505
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001504
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001504
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001504
Iteration 134/1000 | Loss: 0.00001504
Iteration 135/1000 | Loss: 0.00001503
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001503
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001503
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001503
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001502
Iteration 158/1000 | Loss: 0.00001502
Iteration 159/1000 | Loss: 0.00001502
Iteration 160/1000 | Loss: 0.00001502
Iteration 161/1000 | Loss: 0.00001502
Iteration 162/1000 | Loss: 0.00001502
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Iteration 170/1000 | Loss: 0.00001501
Iteration 171/1000 | Loss: 0.00001501
Iteration 172/1000 | Loss: 0.00001501
Iteration 173/1000 | Loss: 0.00001501
Iteration 174/1000 | Loss: 0.00001501
Iteration 175/1000 | Loss: 0.00001501
Iteration 176/1000 | Loss: 0.00001501
Iteration 177/1000 | Loss: 0.00001501
Iteration 178/1000 | Loss: 0.00001500
Iteration 179/1000 | Loss: 0.00001500
Iteration 180/1000 | Loss: 0.00001500
Iteration 181/1000 | Loss: 0.00001500
Iteration 182/1000 | Loss: 0.00001500
Iteration 183/1000 | Loss: 0.00001500
Iteration 184/1000 | Loss: 0.00001500
Iteration 185/1000 | Loss: 0.00001500
Iteration 186/1000 | Loss: 0.00001500
Iteration 187/1000 | Loss: 0.00001500
Iteration 188/1000 | Loss: 0.00001500
Iteration 189/1000 | Loss: 0.00001500
Iteration 190/1000 | Loss: 0.00001500
Iteration 191/1000 | Loss: 0.00001500
Iteration 192/1000 | Loss: 0.00001500
Iteration 193/1000 | Loss: 0.00001500
Iteration 194/1000 | Loss: 0.00001500
Iteration 195/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.500038069934817e-05, 1.500038069934817e-05, 1.500038069934817e-05, 1.500038069934817e-05, 1.500038069934817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.500038069934817e-05

Optimization complete. Final v2v error: 3.284555435180664 mm

Highest mean error: 4.5634613037109375 mm for frame 152

Lowest mean error: 2.7964916229248047 mm for frame 81

Saving results

Total time: 77.24431300163269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418409
Iteration 2/25 | Loss: 0.00126204
Iteration 3/25 | Loss: 0.00120345
Iteration 4/25 | Loss: 0.00119506
Iteration 5/25 | Loss: 0.00119227
Iteration 6/25 | Loss: 0.00119227
Iteration 7/25 | Loss: 0.00119227
Iteration 8/25 | Loss: 0.00119227
Iteration 9/25 | Loss: 0.00119227
Iteration 10/25 | Loss: 0.00119227
Iteration 11/25 | Loss: 0.00119227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00119226542301476, 0.00119226542301476, 0.00119226542301476, 0.00119226542301476, 0.00119226542301476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00119226542301476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.43529749
Iteration 2/25 | Loss: 0.00094017
Iteration 3/25 | Loss: 0.00094017
Iteration 4/25 | Loss: 0.00094017
Iteration 5/25 | Loss: 0.00094017
Iteration 6/25 | Loss: 0.00094017
Iteration 7/25 | Loss: 0.00094017
Iteration 8/25 | Loss: 0.00094017
Iteration 9/25 | Loss: 0.00094017
Iteration 10/25 | Loss: 0.00094017
Iteration 11/25 | Loss: 0.00094017
Iteration 12/25 | Loss: 0.00094017
Iteration 13/25 | Loss: 0.00094017
Iteration 14/25 | Loss: 0.00094017
Iteration 15/25 | Loss: 0.00094017
Iteration 16/25 | Loss: 0.00094017
Iteration 17/25 | Loss: 0.00094017
Iteration 18/25 | Loss: 0.00094017
Iteration 19/25 | Loss: 0.00094017
Iteration 20/25 | Loss: 0.00094017
Iteration 21/25 | Loss: 0.00094017
Iteration 22/25 | Loss: 0.00094017
Iteration 23/25 | Loss: 0.00094017
Iteration 24/25 | Loss: 0.00094017
Iteration 25/25 | Loss: 0.00094017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094017
Iteration 2/1000 | Loss: 0.00002363
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001441
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001185
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001163
Iteration 16/1000 | Loss: 0.00001163
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001154
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001148
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001146
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001143
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001142
Iteration 36/1000 | Loss: 0.00001141
Iteration 37/1000 | Loss: 0.00001141
Iteration 38/1000 | Loss: 0.00001140
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001132
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001129
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001118
Iteration 65/1000 | Loss: 0.00001118
Iteration 66/1000 | Loss: 0.00001118
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001117
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001106
Iteration 110/1000 | Loss: 0.00001105
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001104
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001104
Iteration 121/1000 | Loss: 0.00001104
Iteration 122/1000 | Loss: 0.00001104
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.10378032331937e-05, 1.10378032331937e-05, 1.10378032331937e-05, 1.10378032331937e-05, 1.10378032331937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.10378032331937e-05

Optimization complete. Final v2v error: 2.8167946338653564 mm

Highest mean error: 3.175402879714966 mm for frame 155

Lowest mean error: 2.6136012077331543 mm for frame 133

Saving results

Total time: 39.39612698554993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501787
Iteration 2/25 | Loss: 0.00140385
Iteration 3/25 | Loss: 0.00130497
Iteration 4/25 | Loss: 0.00129044
Iteration 5/25 | Loss: 0.00128501
Iteration 6/25 | Loss: 0.00128425
Iteration 7/25 | Loss: 0.00128425
Iteration 8/25 | Loss: 0.00128425
Iteration 9/25 | Loss: 0.00128425
Iteration 10/25 | Loss: 0.00128425
Iteration 11/25 | Loss: 0.00128425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001284251338802278, 0.001284251338802278, 0.001284251338802278, 0.001284251338802278, 0.001284251338802278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001284251338802278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62551045
Iteration 2/25 | Loss: 0.00108871
Iteration 3/25 | Loss: 0.00108871
Iteration 4/25 | Loss: 0.00108871
Iteration 5/25 | Loss: 0.00108871
Iteration 6/25 | Loss: 0.00108871
Iteration 7/25 | Loss: 0.00108870
Iteration 8/25 | Loss: 0.00108870
Iteration 9/25 | Loss: 0.00108870
Iteration 10/25 | Loss: 0.00108870
Iteration 11/25 | Loss: 0.00108870
Iteration 12/25 | Loss: 0.00108870
Iteration 13/25 | Loss: 0.00108870
Iteration 14/25 | Loss: 0.00108870
Iteration 15/25 | Loss: 0.00108870
Iteration 16/25 | Loss: 0.00108870
Iteration 17/25 | Loss: 0.00108870
Iteration 18/25 | Loss: 0.00108870
Iteration 19/25 | Loss: 0.00108870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001088703633286059, 0.001088703633286059, 0.001088703633286059, 0.001088703633286059, 0.001088703633286059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001088703633286059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108870
Iteration 2/1000 | Loss: 0.00004048
Iteration 3/1000 | Loss: 0.00002894
Iteration 4/1000 | Loss: 0.00002645
Iteration 5/1000 | Loss: 0.00002524
Iteration 6/1000 | Loss: 0.00002440
Iteration 7/1000 | Loss: 0.00002342
Iteration 8/1000 | Loss: 0.00002293
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002203
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00002134
Iteration 14/1000 | Loss: 0.00002113
Iteration 15/1000 | Loss: 0.00002095
Iteration 16/1000 | Loss: 0.00002075
Iteration 17/1000 | Loss: 0.00002057
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002024
Iteration 22/1000 | Loss: 0.00002022
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002008
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002003
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002001
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002000
Iteration 34/1000 | Loss: 0.00001999
Iteration 35/1000 | Loss: 0.00001999
Iteration 36/1000 | Loss: 0.00001999
Iteration 37/1000 | Loss: 0.00001999
Iteration 38/1000 | Loss: 0.00001999
Iteration 39/1000 | Loss: 0.00001998
Iteration 40/1000 | Loss: 0.00001998
Iteration 41/1000 | Loss: 0.00001998
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001998
Iteration 45/1000 | Loss: 0.00001998
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001993
Iteration 55/1000 | Loss: 0.00001993
Iteration 56/1000 | Loss: 0.00001993
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001992
Iteration 59/1000 | Loss: 0.00001990
Iteration 60/1000 | Loss: 0.00001990
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001985
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001984
Iteration 81/1000 | Loss: 0.00001984
Iteration 82/1000 | Loss: 0.00001984
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001984
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001984
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001982
Iteration 92/1000 | Loss: 0.00001982
Iteration 93/1000 | Loss: 0.00001982
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001982
Iteration 96/1000 | Loss: 0.00001982
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001981
Iteration 107/1000 | Loss: 0.00001981
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001981
Iteration 110/1000 | Loss: 0.00001980
Iteration 111/1000 | Loss: 0.00001980
Iteration 112/1000 | Loss: 0.00001980
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001979
Iteration 120/1000 | Loss: 0.00001979
Iteration 121/1000 | Loss: 0.00001979
Iteration 122/1000 | Loss: 0.00001979
Iteration 123/1000 | Loss: 0.00001979
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001978
Iteration 128/1000 | Loss: 0.00001978
Iteration 129/1000 | Loss: 0.00001978
Iteration 130/1000 | Loss: 0.00001977
Iteration 131/1000 | Loss: 0.00001976
Iteration 132/1000 | Loss: 0.00001976
Iteration 133/1000 | Loss: 0.00001976
Iteration 134/1000 | Loss: 0.00001974
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001972
Iteration 138/1000 | Loss: 0.00001972
Iteration 139/1000 | Loss: 0.00001971
Iteration 140/1000 | Loss: 0.00001971
Iteration 141/1000 | Loss: 0.00001971
Iteration 142/1000 | Loss: 0.00001971
Iteration 143/1000 | Loss: 0.00001971
Iteration 144/1000 | Loss: 0.00001971
Iteration 145/1000 | Loss: 0.00001971
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001970
Iteration 152/1000 | Loss: 0.00001970
Iteration 153/1000 | Loss: 0.00001969
Iteration 154/1000 | Loss: 0.00001968
Iteration 155/1000 | Loss: 0.00001968
Iteration 156/1000 | Loss: 0.00001968
Iteration 157/1000 | Loss: 0.00001968
Iteration 158/1000 | Loss: 0.00001968
Iteration 159/1000 | Loss: 0.00001968
Iteration 160/1000 | Loss: 0.00001968
Iteration 161/1000 | Loss: 0.00001968
Iteration 162/1000 | Loss: 0.00001968
Iteration 163/1000 | Loss: 0.00001968
Iteration 164/1000 | Loss: 0.00001968
Iteration 165/1000 | Loss: 0.00001968
Iteration 166/1000 | Loss: 0.00001967
Iteration 167/1000 | Loss: 0.00001967
Iteration 168/1000 | Loss: 0.00001967
Iteration 169/1000 | Loss: 0.00001967
Iteration 170/1000 | Loss: 0.00001967
Iteration 171/1000 | Loss: 0.00001967
Iteration 172/1000 | Loss: 0.00001967
Iteration 173/1000 | Loss: 0.00001966
Iteration 174/1000 | Loss: 0.00001966
Iteration 175/1000 | Loss: 0.00001966
Iteration 176/1000 | Loss: 0.00001966
Iteration 177/1000 | Loss: 0.00001966
Iteration 178/1000 | Loss: 0.00001966
Iteration 179/1000 | Loss: 0.00001966
Iteration 180/1000 | Loss: 0.00001965
Iteration 181/1000 | Loss: 0.00001965
Iteration 182/1000 | Loss: 0.00001965
Iteration 183/1000 | Loss: 0.00001965
Iteration 184/1000 | Loss: 0.00001965
Iteration 185/1000 | Loss: 0.00001965
Iteration 186/1000 | Loss: 0.00001965
Iteration 187/1000 | Loss: 0.00001965
Iteration 188/1000 | Loss: 0.00001965
Iteration 189/1000 | Loss: 0.00001965
Iteration 190/1000 | Loss: 0.00001964
Iteration 191/1000 | Loss: 0.00001964
Iteration 192/1000 | Loss: 0.00001964
Iteration 193/1000 | Loss: 0.00001964
Iteration 194/1000 | Loss: 0.00001964
Iteration 195/1000 | Loss: 0.00001964
Iteration 196/1000 | Loss: 0.00001964
Iteration 197/1000 | Loss: 0.00001963
Iteration 198/1000 | Loss: 0.00001963
Iteration 199/1000 | Loss: 0.00001963
Iteration 200/1000 | Loss: 0.00001963
Iteration 201/1000 | Loss: 0.00001963
Iteration 202/1000 | Loss: 0.00001963
Iteration 203/1000 | Loss: 0.00001963
Iteration 204/1000 | Loss: 0.00001963
Iteration 205/1000 | Loss: 0.00001963
Iteration 206/1000 | Loss: 0.00001963
Iteration 207/1000 | Loss: 0.00001963
Iteration 208/1000 | Loss: 0.00001963
Iteration 209/1000 | Loss: 0.00001963
Iteration 210/1000 | Loss: 0.00001963
Iteration 211/1000 | Loss: 0.00001963
Iteration 212/1000 | Loss: 0.00001963
Iteration 213/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.9632381736300886e-05, 1.9632381736300886e-05, 1.9632381736300886e-05, 1.9632381736300886e-05, 1.9632381736300886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9632381736300886e-05

Optimization complete. Final v2v error: 3.6919472217559814 mm

Highest mean error: 4.08199405670166 mm for frame 16

Lowest mean error: 3.558225393295288 mm for frame 209

Saving results

Total time: 58.77710723876953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031237
Iteration 2/25 | Loss: 0.00140070
Iteration 3/25 | Loss: 0.00123545
Iteration 4/25 | Loss: 0.00121612
Iteration 5/25 | Loss: 0.00121275
Iteration 6/25 | Loss: 0.00121201
Iteration 7/25 | Loss: 0.00121201
Iteration 8/25 | Loss: 0.00121201
Iteration 9/25 | Loss: 0.00121201
Iteration 10/25 | Loss: 0.00121201
Iteration 11/25 | Loss: 0.00121201
Iteration 12/25 | Loss: 0.00121201
Iteration 13/25 | Loss: 0.00121201
Iteration 14/25 | Loss: 0.00121201
Iteration 15/25 | Loss: 0.00121201
Iteration 16/25 | Loss: 0.00121201
Iteration 17/25 | Loss: 0.00121201
Iteration 18/25 | Loss: 0.00121201
Iteration 19/25 | Loss: 0.00121201
Iteration 20/25 | Loss: 0.00121201
Iteration 21/25 | Loss: 0.00121201
Iteration 22/25 | Loss: 0.00121201
Iteration 23/25 | Loss: 0.00121201
Iteration 24/25 | Loss: 0.00121201
Iteration 25/25 | Loss: 0.00121201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56482434
Iteration 2/25 | Loss: 0.00101871
Iteration 3/25 | Loss: 0.00101871
Iteration 4/25 | Loss: 0.00101871
Iteration 5/25 | Loss: 0.00101871
Iteration 6/25 | Loss: 0.00101871
Iteration 7/25 | Loss: 0.00101871
Iteration 8/25 | Loss: 0.00101871
Iteration 9/25 | Loss: 0.00101871
Iteration 10/25 | Loss: 0.00101871
Iteration 11/25 | Loss: 0.00101871
Iteration 12/25 | Loss: 0.00101871
Iteration 13/25 | Loss: 0.00101871
Iteration 14/25 | Loss: 0.00101871
Iteration 15/25 | Loss: 0.00101871
Iteration 16/25 | Loss: 0.00101871
Iteration 17/25 | Loss: 0.00101871
Iteration 18/25 | Loss: 0.00101871
Iteration 19/25 | Loss: 0.00101871
Iteration 20/25 | Loss: 0.00101871
Iteration 21/25 | Loss: 0.00101871
Iteration 22/25 | Loss: 0.00101871
Iteration 23/25 | Loss: 0.00101871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001018711132928729, 0.001018711132928729, 0.001018711132928729, 0.001018711132928729, 0.001018711132928729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001018711132928729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101871
Iteration 2/1000 | Loss: 0.00002328
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001551
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001253
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001238
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001232
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001227
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001209
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001205
Iteration 88/1000 | Loss: 0.00001205
Iteration 89/1000 | Loss: 0.00001205
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001204
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001202
Iteration 138/1000 | Loss: 0.00001202
Iteration 139/1000 | Loss: 0.00001202
Iteration 140/1000 | Loss: 0.00001202
Iteration 141/1000 | Loss: 0.00001202
Iteration 142/1000 | Loss: 0.00001202
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001202
Iteration 147/1000 | Loss: 0.00001202
Iteration 148/1000 | Loss: 0.00001202
Iteration 149/1000 | Loss: 0.00001202
Iteration 150/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.202204202854773e-05, 1.202204202854773e-05, 1.202204202854773e-05, 1.202204202854773e-05, 1.202204202854773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.202204202854773e-05

Optimization complete. Final v2v error: 2.9603006839752197 mm

Highest mean error: 3.21844744682312 mm for frame 41

Lowest mean error: 2.6755456924438477 mm for frame 204

Saving results

Total time: 39.865427017211914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805335
Iteration 2/25 | Loss: 0.00149536
Iteration 3/25 | Loss: 0.00125312
Iteration 4/25 | Loss: 0.00122271
Iteration 5/25 | Loss: 0.00121761
Iteration 6/25 | Loss: 0.00121651
Iteration 7/25 | Loss: 0.00121605
Iteration 8/25 | Loss: 0.00121579
Iteration 9/25 | Loss: 0.00121562
Iteration 10/25 | Loss: 0.00121551
Iteration 11/25 | Loss: 0.00121994
Iteration 12/25 | Loss: 0.00121724
Iteration 13/25 | Loss: 0.00121742
Iteration 14/25 | Loss: 0.00121518
Iteration 15/25 | Loss: 0.00121236
Iteration 16/25 | Loss: 0.00121128
Iteration 17/25 | Loss: 0.00121084
Iteration 18/25 | Loss: 0.00121072
Iteration 19/25 | Loss: 0.00121060
Iteration 20/25 | Loss: 0.00121045
Iteration 21/25 | Loss: 0.00121038
Iteration 22/25 | Loss: 0.00121038
Iteration 23/25 | Loss: 0.00121037
Iteration 24/25 | Loss: 0.00121037
Iteration 25/25 | Loss: 0.00121037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35463023
Iteration 2/25 | Loss: 0.00099303
Iteration 3/25 | Loss: 0.00099303
Iteration 4/25 | Loss: 0.00099303
Iteration 5/25 | Loss: 0.00099303
Iteration 6/25 | Loss: 0.00099303
Iteration 7/25 | Loss: 0.00099303
Iteration 8/25 | Loss: 0.00099303
Iteration 9/25 | Loss: 0.00099303
Iteration 10/25 | Loss: 0.00099303
Iteration 11/25 | Loss: 0.00099303
Iteration 12/25 | Loss: 0.00099303
Iteration 13/25 | Loss: 0.00099303
Iteration 14/25 | Loss: 0.00099303
Iteration 15/25 | Loss: 0.00099303
Iteration 16/25 | Loss: 0.00099303
Iteration 17/25 | Loss: 0.00099303
Iteration 18/25 | Loss: 0.00099303
Iteration 19/25 | Loss: 0.00099303
Iteration 20/25 | Loss: 0.00099303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009930282831192017, 0.0009930282831192017, 0.0009930282831192017, 0.0009930282831192017, 0.0009930282831192017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009930282831192017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099303
Iteration 2/1000 | Loss: 0.00002409
Iteration 3/1000 | Loss: 0.00001699
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001337
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001230
Iteration 8/1000 | Loss: 0.00001199
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001146
Iteration 11/1000 | Loss: 0.00001126
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001120
Iteration 14/1000 | Loss: 0.00001119
Iteration 15/1000 | Loss: 0.00001117
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001103
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001103
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001099
Iteration 36/1000 | Loss: 0.00001099
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001098
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001096
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001093
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001088
Iteration 74/1000 | Loss: 0.00001088
Iteration 75/1000 | Loss: 0.00001088
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001087
Iteration 82/1000 | Loss: 0.00001087
Iteration 83/1000 | Loss: 0.00001087
Iteration 84/1000 | Loss: 0.00001087
Iteration 85/1000 | Loss: 0.00001087
Iteration 86/1000 | Loss: 0.00001086
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001086
Iteration 89/1000 | Loss: 0.00001086
Iteration 90/1000 | Loss: 0.00001085
Iteration 91/1000 | Loss: 0.00001085
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001085
Iteration 94/1000 | Loss: 0.00001085
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001084
Iteration 98/1000 | Loss: 0.00001084
Iteration 99/1000 | Loss: 0.00001084
Iteration 100/1000 | Loss: 0.00001084
Iteration 101/1000 | Loss: 0.00001084
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001083
Iteration 106/1000 | Loss: 0.00001083
Iteration 107/1000 | Loss: 0.00001083
Iteration 108/1000 | Loss: 0.00001082
Iteration 109/1000 | Loss: 0.00001082
Iteration 110/1000 | Loss: 0.00001082
Iteration 111/1000 | Loss: 0.00001082
Iteration 112/1000 | Loss: 0.00001082
Iteration 113/1000 | Loss: 0.00001082
Iteration 114/1000 | Loss: 0.00001082
Iteration 115/1000 | Loss: 0.00001081
Iteration 116/1000 | Loss: 0.00001081
Iteration 117/1000 | Loss: 0.00001081
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001079
Iteration 129/1000 | Loss: 0.00001079
Iteration 130/1000 | Loss: 0.00001079
Iteration 131/1000 | Loss: 0.00001079
Iteration 132/1000 | Loss: 0.00001079
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001078
Iteration 139/1000 | Loss: 0.00001078
Iteration 140/1000 | Loss: 0.00001077
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001077
Iteration 146/1000 | Loss: 0.00001077
Iteration 147/1000 | Loss: 0.00001077
Iteration 148/1000 | Loss: 0.00001077
Iteration 149/1000 | Loss: 0.00001077
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001076
Iteration 153/1000 | Loss: 0.00001076
Iteration 154/1000 | Loss: 0.00001075
Iteration 155/1000 | Loss: 0.00001075
Iteration 156/1000 | Loss: 0.00001075
Iteration 157/1000 | Loss: 0.00001075
Iteration 158/1000 | Loss: 0.00001075
Iteration 159/1000 | Loss: 0.00001075
Iteration 160/1000 | Loss: 0.00001075
Iteration 161/1000 | Loss: 0.00001075
Iteration 162/1000 | Loss: 0.00001075
Iteration 163/1000 | Loss: 0.00001075
Iteration 164/1000 | Loss: 0.00001075
Iteration 165/1000 | Loss: 0.00001075
Iteration 166/1000 | Loss: 0.00001075
Iteration 167/1000 | Loss: 0.00001075
Iteration 168/1000 | Loss: 0.00001075
Iteration 169/1000 | Loss: 0.00001075
Iteration 170/1000 | Loss: 0.00001075
Iteration 171/1000 | Loss: 0.00001075
Iteration 172/1000 | Loss: 0.00001075
Iteration 173/1000 | Loss: 0.00001075
Iteration 174/1000 | Loss: 0.00001075
Iteration 175/1000 | Loss: 0.00001075
Iteration 176/1000 | Loss: 0.00001075
Iteration 177/1000 | Loss: 0.00001075
Iteration 178/1000 | Loss: 0.00001075
Iteration 179/1000 | Loss: 0.00001075
Iteration 180/1000 | Loss: 0.00001075
Iteration 181/1000 | Loss: 0.00001075
Iteration 182/1000 | Loss: 0.00001075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.0749131433840375e-05, 1.0749131433840375e-05, 1.0749131433840375e-05, 1.0749131433840375e-05, 1.0749131433840375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0749131433840375e-05

Optimization complete. Final v2v error: 2.826737880706787 mm

Highest mean error: 3.099853992462158 mm for frame 72

Lowest mean error: 2.7151355743408203 mm for frame 35

Saving results

Total time: 63.62944555282593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827195
Iteration 2/25 | Loss: 0.00137617
Iteration 3/25 | Loss: 0.00122552
Iteration 4/25 | Loss: 0.00119249
Iteration 5/25 | Loss: 0.00118971
Iteration 6/25 | Loss: 0.00119704
Iteration 7/25 | Loss: 0.00119470
Iteration 8/25 | Loss: 0.00118504
Iteration 9/25 | Loss: 0.00118460
Iteration 10/25 | Loss: 0.00118318
Iteration 11/25 | Loss: 0.00118415
Iteration 12/25 | Loss: 0.00118348
Iteration 13/25 | Loss: 0.00118308
Iteration 14/25 | Loss: 0.00118308
Iteration 15/25 | Loss: 0.00118308
Iteration 16/25 | Loss: 0.00118308
Iteration 17/25 | Loss: 0.00118308
Iteration 18/25 | Loss: 0.00118307
Iteration 19/25 | Loss: 0.00118307
Iteration 20/25 | Loss: 0.00118307
Iteration 21/25 | Loss: 0.00118307
Iteration 22/25 | Loss: 0.00118307
Iteration 23/25 | Loss: 0.00118307
Iteration 24/25 | Loss: 0.00118307
Iteration 25/25 | Loss: 0.00118307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71834946
Iteration 2/25 | Loss: 0.00100344
Iteration 3/25 | Loss: 0.00099865
Iteration 4/25 | Loss: 0.00099865
Iteration 5/25 | Loss: 0.00099865
Iteration 6/25 | Loss: 0.00099865
Iteration 7/25 | Loss: 0.00099865
Iteration 8/25 | Loss: 0.00099864
Iteration 9/25 | Loss: 0.00099864
Iteration 10/25 | Loss: 0.00099864
Iteration 11/25 | Loss: 0.00099864
Iteration 12/25 | Loss: 0.00099864
Iteration 13/25 | Loss: 0.00099864
Iteration 14/25 | Loss: 0.00099864
Iteration 15/25 | Loss: 0.00099864
Iteration 16/25 | Loss: 0.00099864
Iteration 17/25 | Loss: 0.00099864
Iteration 18/25 | Loss: 0.00099864
Iteration 19/25 | Loss: 0.00099864
Iteration 20/25 | Loss: 0.00099864
Iteration 21/25 | Loss: 0.00099864
Iteration 22/25 | Loss: 0.00099864
Iteration 23/25 | Loss: 0.00099864
Iteration 24/25 | Loss: 0.00099864
Iteration 25/25 | Loss: 0.00099864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099864
Iteration 2/1000 | Loss: 0.00003064
Iteration 3/1000 | Loss: 0.00002714
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001206
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00002401
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001099
Iteration 17/1000 | Loss: 0.00001094
Iteration 18/1000 | Loss: 0.00001094
Iteration 19/1000 | Loss: 0.00001094
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001088
Iteration 26/1000 | Loss: 0.00001088
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001086
Iteration 29/1000 | Loss: 0.00001084
Iteration 30/1000 | Loss: 0.00001083
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00002236
Iteration 34/1000 | Loss: 0.00005424
Iteration 35/1000 | Loss: 0.00002358
Iteration 36/1000 | Loss: 0.00001080
Iteration 37/1000 | Loss: 0.00001065
Iteration 38/1000 | Loss: 0.00001062
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001058
Iteration 48/1000 | Loss: 0.00001058
Iteration 49/1000 | Loss: 0.00001058
Iteration 50/1000 | Loss: 0.00001058
Iteration 51/1000 | Loss: 0.00001058
Iteration 52/1000 | Loss: 0.00001058
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001058
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001058
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.0580490197753534e-05, 1.0580490197753534e-05, 1.0580490197753534e-05, 1.0580490197753534e-05, 1.0580490197753534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0580490197753534e-05

Optimization complete. Final v2v error: 2.8169798851013184 mm

Highest mean error: 3.0703160762786865 mm for frame 173

Lowest mean error: 2.5700416564941406 mm for frame 6

Saving results

Total time: 58.87958025932312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081428
Iteration 2/25 | Loss: 0.00192392
Iteration 3/25 | Loss: 0.00147810
Iteration 4/25 | Loss: 0.00142949
Iteration 5/25 | Loss: 0.00142934
Iteration 6/25 | Loss: 0.00142902
Iteration 7/25 | Loss: 0.00142802
Iteration 8/25 | Loss: 0.00142136
Iteration 9/25 | Loss: 0.00141551
Iteration 10/25 | Loss: 0.00141373
Iteration 11/25 | Loss: 0.00141388
Iteration 12/25 | Loss: 0.00141133
Iteration 13/25 | Loss: 0.00140754
Iteration 14/25 | Loss: 0.00141066
Iteration 15/25 | Loss: 0.00141029
Iteration 16/25 | Loss: 0.00139997
Iteration 17/25 | Loss: 0.00140403
Iteration 18/25 | Loss: 0.00140082
Iteration 19/25 | Loss: 0.00139939
Iteration 20/25 | Loss: 0.00139853
Iteration 21/25 | Loss: 0.00140103
Iteration 22/25 | Loss: 0.00139531
Iteration 23/25 | Loss: 0.00139448
Iteration 24/25 | Loss: 0.00139207
Iteration 25/25 | Loss: 0.00139665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98466444
Iteration 2/25 | Loss: 0.00148499
Iteration 3/25 | Loss: 0.00148498
Iteration 4/25 | Loss: 0.00148498
Iteration 5/25 | Loss: 0.00148498
Iteration 6/25 | Loss: 0.00148498
Iteration 7/25 | Loss: 0.00148498
Iteration 8/25 | Loss: 0.00148498
Iteration 9/25 | Loss: 0.00148498
Iteration 10/25 | Loss: 0.00148498
Iteration 11/25 | Loss: 0.00148497
Iteration 12/25 | Loss: 0.00148497
Iteration 13/25 | Loss: 0.00148497
Iteration 14/25 | Loss: 0.00148497
Iteration 15/25 | Loss: 0.00148497
Iteration 16/25 | Loss: 0.00148497
Iteration 17/25 | Loss: 0.00148497
Iteration 18/25 | Loss: 0.00148497
Iteration 19/25 | Loss: 0.00148497
Iteration 20/25 | Loss: 0.00148497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001484974636696279, 0.001484974636696279, 0.001484974636696279, 0.001484974636696279, 0.001484974636696279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001484974636696279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148497
Iteration 2/1000 | Loss: 0.00017351
Iteration 3/1000 | Loss: 0.00006866
Iteration 4/1000 | Loss: 0.00043292
Iteration 5/1000 | Loss: 0.00042128
Iteration 6/1000 | Loss: 0.00048187
Iteration 7/1000 | Loss: 0.00044951
Iteration 8/1000 | Loss: 0.00045469
Iteration 9/1000 | Loss: 0.00045402
Iteration 10/1000 | Loss: 0.00046569
Iteration 11/1000 | Loss: 0.00064469
Iteration 12/1000 | Loss: 0.00056543
Iteration 13/1000 | Loss: 0.00051376
Iteration 14/1000 | Loss: 0.00045464
Iteration 15/1000 | Loss: 0.00060101
Iteration 16/1000 | Loss: 0.00055639
Iteration 17/1000 | Loss: 0.00042609
Iteration 18/1000 | Loss: 0.00035891
Iteration 19/1000 | Loss: 0.00019542
Iteration 20/1000 | Loss: 0.00039602
Iteration 21/1000 | Loss: 0.00090780
Iteration 22/1000 | Loss: 0.00080828
Iteration 23/1000 | Loss: 0.00053077
Iteration 24/1000 | Loss: 0.00070342
Iteration 25/1000 | Loss: 0.00086223
Iteration 26/1000 | Loss: 0.00054345
Iteration 27/1000 | Loss: 0.00064930
Iteration 28/1000 | Loss: 0.00045331
Iteration 29/1000 | Loss: 0.00029836
Iteration 30/1000 | Loss: 0.00029974
Iteration 31/1000 | Loss: 0.00027093
Iteration 32/1000 | Loss: 0.00075989
Iteration 33/1000 | Loss: 0.00072770
Iteration 34/1000 | Loss: 0.00064138
Iteration 35/1000 | Loss: 0.00037463
Iteration 36/1000 | Loss: 0.00020901
Iteration 37/1000 | Loss: 0.00020866
Iteration 38/1000 | Loss: 0.00019921
Iteration 39/1000 | Loss: 0.00024840
Iteration 40/1000 | Loss: 0.00027473
Iteration 41/1000 | Loss: 0.00021346
Iteration 42/1000 | Loss: 0.00017647
Iteration 43/1000 | Loss: 0.00023763
Iteration 44/1000 | Loss: 0.00017815
Iteration 45/1000 | Loss: 0.00016613
Iteration 46/1000 | Loss: 0.00017781
Iteration 47/1000 | Loss: 0.00014195
Iteration 48/1000 | Loss: 0.00014989
Iteration 49/1000 | Loss: 0.00015959
Iteration 50/1000 | Loss: 0.00014845
Iteration 51/1000 | Loss: 0.00016192
Iteration 52/1000 | Loss: 0.00010152
Iteration 53/1000 | Loss: 0.00013396
Iteration 54/1000 | Loss: 0.00008752
Iteration 55/1000 | Loss: 0.00021375
Iteration 56/1000 | Loss: 0.00016517
Iteration 57/1000 | Loss: 0.00028851
Iteration 58/1000 | Loss: 0.00018627
Iteration 59/1000 | Loss: 0.00026646
Iteration 60/1000 | Loss: 0.00025728
Iteration 61/1000 | Loss: 0.00021080
Iteration 62/1000 | Loss: 0.00020565
Iteration 63/1000 | Loss: 0.00022250
Iteration 64/1000 | Loss: 0.00008377
Iteration 65/1000 | Loss: 0.00023742
Iteration 66/1000 | Loss: 0.00015546
Iteration 67/1000 | Loss: 0.00014458
Iteration 68/1000 | Loss: 0.00013383
Iteration 69/1000 | Loss: 0.00013295
Iteration 70/1000 | Loss: 0.00019987
Iteration 71/1000 | Loss: 0.00009972
Iteration 72/1000 | Loss: 0.00011138
Iteration 73/1000 | Loss: 0.00013675
Iteration 74/1000 | Loss: 0.00010954
Iteration 75/1000 | Loss: 0.00011379
Iteration 76/1000 | Loss: 0.00013767
Iteration 77/1000 | Loss: 0.00012658
Iteration 78/1000 | Loss: 0.00011037
Iteration 79/1000 | Loss: 0.00003812
Iteration 80/1000 | Loss: 0.00025168
Iteration 81/1000 | Loss: 0.00013078
Iteration 82/1000 | Loss: 0.00013086
Iteration 83/1000 | Loss: 0.00027930
Iteration 84/1000 | Loss: 0.00047734
Iteration 85/1000 | Loss: 0.00019489
Iteration 86/1000 | Loss: 0.00069374
Iteration 87/1000 | Loss: 0.00016346
Iteration 88/1000 | Loss: 0.00026122
Iteration 89/1000 | Loss: 0.00030323
Iteration 90/1000 | Loss: 0.00027239
Iteration 91/1000 | Loss: 0.00024673
Iteration 92/1000 | Loss: 0.00020073
Iteration 93/1000 | Loss: 0.00021167
Iteration 94/1000 | Loss: 0.00016486
Iteration 95/1000 | Loss: 0.00016861
Iteration 96/1000 | Loss: 0.00012166
Iteration 97/1000 | Loss: 0.00061700
Iteration 98/1000 | Loss: 0.00020370
Iteration 99/1000 | Loss: 0.00024287
Iteration 100/1000 | Loss: 0.00032906
Iteration 101/1000 | Loss: 0.00051140
Iteration 102/1000 | Loss: 0.00004417
Iteration 103/1000 | Loss: 0.00018965
Iteration 104/1000 | Loss: 0.00028435
Iteration 105/1000 | Loss: 0.00022403
Iteration 106/1000 | Loss: 0.00025970
Iteration 107/1000 | Loss: 0.00029522
Iteration 108/1000 | Loss: 0.00035890
Iteration 109/1000 | Loss: 0.00037131
Iteration 110/1000 | Loss: 0.00026546
Iteration 111/1000 | Loss: 0.00025328
Iteration 112/1000 | Loss: 0.00009691
Iteration 113/1000 | Loss: 0.00009031
Iteration 114/1000 | Loss: 0.00003806
Iteration 115/1000 | Loss: 0.00012087
Iteration 116/1000 | Loss: 0.00015235
Iteration 117/1000 | Loss: 0.00003502
Iteration 118/1000 | Loss: 0.00008855
Iteration 119/1000 | Loss: 0.00011970
Iteration 120/1000 | Loss: 0.00009017
Iteration 121/1000 | Loss: 0.00014549
Iteration 122/1000 | Loss: 0.00009532
Iteration 123/1000 | Loss: 0.00012403
Iteration 124/1000 | Loss: 0.00009372
Iteration 125/1000 | Loss: 0.00012494
Iteration 126/1000 | Loss: 0.00011454
Iteration 127/1000 | Loss: 0.00011447
Iteration 128/1000 | Loss: 0.00009414
Iteration 129/1000 | Loss: 0.00010560
Iteration 130/1000 | Loss: 0.00009269
Iteration 131/1000 | Loss: 0.00013123
Iteration 132/1000 | Loss: 0.00009312
Iteration 133/1000 | Loss: 0.00011565
Iteration 134/1000 | Loss: 0.00009464
Iteration 135/1000 | Loss: 0.00017252
Iteration 136/1000 | Loss: 0.00017262
Iteration 137/1000 | Loss: 0.00008651
Iteration 138/1000 | Loss: 0.00007915
Iteration 139/1000 | Loss: 0.00013949
Iteration 140/1000 | Loss: 0.00012635
Iteration 141/1000 | Loss: 0.00006967
Iteration 142/1000 | Loss: 0.00003516
Iteration 143/1000 | Loss: 0.00008324
Iteration 144/1000 | Loss: 0.00005018
Iteration 145/1000 | Loss: 0.00007712
Iteration 146/1000 | Loss: 0.00004668
Iteration 147/1000 | Loss: 0.00019959
Iteration 148/1000 | Loss: 0.00020549
Iteration 149/1000 | Loss: 0.00003796
Iteration 150/1000 | Loss: 0.00003083
Iteration 151/1000 | Loss: 0.00013310
Iteration 152/1000 | Loss: 0.00002879
Iteration 153/1000 | Loss: 0.00002759
Iteration 154/1000 | Loss: 0.00002612
Iteration 155/1000 | Loss: 0.00002570
Iteration 156/1000 | Loss: 0.00002542
Iteration 157/1000 | Loss: 0.00002519
Iteration 158/1000 | Loss: 0.00002490
Iteration 159/1000 | Loss: 0.00002457
Iteration 160/1000 | Loss: 0.00002445
Iteration 161/1000 | Loss: 0.00002439
Iteration 162/1000 | Loss: 0.00002424
Iteration 163/1000 | Loss: 0.00002423
Iteration 164/1000 | Loss: 0.00002414
Iteration 165/1000 | Loss: 0.00002398
Iteration 166/1000 | Loss: 0.00002396
Iteration 167/1000 | Loss: 0.00002396
Iteration 168/1000 | Loss: 0.00002396
Iteration 169/1000 | Loss: 0.00002396
Iteration 170/1000 | Loss: 0.00002396
Iteration 171/1000 | Loss: 0.00002396
Iteration 172/1000 | Loss: 0.00002396
Iteration 173/1000 | Loss: 0.00002396
Iteration 174/1000 | Loss: 0.00002396
Iteration 175/1000 | Loss: 0.00002395
Iteration 176/1000 | Loss: 0.00002395
Iteration 177/1000 | Loss: 0.00002395
Iteration 178/1000 | Loss: 0.00002395
Iteration 179/1000 | Loss: 0.00002395
Iteration 180/1000 | Loss: 0.00002395
Iteration 181/1000 | Loss: 0.00002395
Iteration 182/1000 | Loss: 0.00002395
Iteration 183/1000 | Loss: 0.00002395
Iteration 184/1000 | Loss: 0.00002395
Iteration 185/1000 | Loss: 0.00002395
Iteration 186/1000 | Loss: 0.00002395
Iteration 187/1000 | Loss: 0.00002395
Iteration 188/1000 | Loss: 0.00002395
Iteration 189/1000 | Loss: 0.00002395
Iteration 190/1000 | Loss: 0.00002395
Iteration 191/1000 | Loss: 0.00002395
Iteration 192/1000 | Loss: 0.00002395
Iteration 193/1000 | Loss: 0.00002395
Iteration 194/1000 | Loss: 0.00002395
Iteration 195/1000 | Loss: 0.00002395
Iteration 196/1000 | Loss: 0.00002395
Iteration 197/1000 | Loss: 0.00002395
Iteration 198/1000 | Loss: 0.00002395
Iteration 199/1000 | Loss: 0.00002395
Iteration 200/1000 | Loss: 0.00002395
Iteration 201/1000 | Loss: 0.00002395
Iteration 202/1000 | Loss: 0.00002395
Iteration 203/1000 | Loss: 0.00002395
Iteration 204/1000 | Loss: 0.00002395
Iteration 205/1000 | Loss: 0.00002395
Iteration 206/1000 | Loss: 0.00002395
Iteration 207/1000 | Loss: 0.00002395
Iteration 208/1000 | Loss: 0.00002395
Iteration 209/1000 | Loss: 0.00002395
Iteration 210/1000 | Loss: 0.00002395
Iteration 211/1000 | Loss: 0.00002395
Iteration 212/1000 | Loss: 0.00002395
Iteration 213/1000 | Loss: 0.00002395
Iteration 214/1000 | Loss: 0.00002395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.3953416530275717e-05, 2.3953416530275717e-05, 2.3953416530275717e-05, 2.3953416530275717e-05, 2.3953416530275717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3953416530275717e-05

Optimization complete. Final v2v error: 4.065784931182861 mm

Highest mean error: 6.154606819152832 mm for frame 153

Lowest mean error: 3.4223499298095703 mm for frame 22

Saving results

Total time: 287.8039810657501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652458
Iteration 2/25 | Loss: 0.00128447
Iteration 3/25 | Loss: 0.00120165
Iteration 4/25 | Loss: 0.00118814
Iteration 5/25 | Loss: 0.00118361
Iteration 6/25 | Loss: 0.00118314
Iteration 7/25 | Loss: 0.00118314
Iteration 8/25 | Loss: 0.00118314
Iteration 9/25 | Loss: 0.00118314
Iteration 10/25 | Loss: 0.00118314
Iteration 11/25 | Loss: 0.00118314
Iteration 12/25 | Loss: 0.00118314
Iteration 13/25 | Loss: 0.00118314
Iteration 14/25 | Loss: 0.00118314
Iteration 15/25 | Loss: 0.00118314
Iteration 16/25 | Loss: 0.00118314
Iteration 17/25 | Loss: 0.00118314
Iteration 18/25 | Loss: 0.00118314
Iteration 19/25 | Loss: 0.00118314
Iteration 20/25 | Loss: 0.00118314
Iteration 21/25 | Loss: 0.00118314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011831382289528847, 0.0011831382289528847, 0.0011831382289528847, 0.0011831382289528847, 0.0011831382289528847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011831382289528847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.13493490
Iteration 2/25 | Loss: 0.00101078
Iteration 3/25 | Loss: 0.00101078
Iteration 4/25 | Loss: 0.00101078
Iteration 5/25 | Loss: 0.00101078
Iteration 6/25 | Loss: 0.00101078
Iteration 7/25 | Loss: 0.00101078
Iteration 8/25 | Loss: 0.00101078
Iteration 9/25 | Loss: 0.00101078
Iteration 10/25 | Loss: 0.00101078
Iteration 11/25 | Loss: 0.00101078
Iteration 12/25 | Loss: 0.00101077
Iteration 13/25 | Loss: 0.00101077
Iteration 14/25 | Loss: 0.00101077
Iteration 15/25 | Loss: 0.00101077
Iteration 16/25 | Loss: 0.00101077
Iteration 17/25 | Loss: 0.00101077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001010774984024465, 0.001010774984024465, 0.001010774984024465, 0.001010774984024465, 0.001010774984024465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001010774984024465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101077
Iteration 2/1000 | Loss: 0.00002028
Iteration 3/1000 | Loss: 0.00001500
Iteration 4/1000 | Loss: 0.00001309
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001191
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001124
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001087
Iteration 11/1000 | Loss: 0.00001070
Iteration 12/1000 | Loss: 0.00001060
Iteration 13/1000 | Loss: 0.00001046
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001030
Iteration 21/1000 | Loss: 0.00001028
Iteration 22/1000 | Loss: 0.00001028
Iteration 23/1000 | Loss: 0.00001028
Iteration 24/1000 | Loss: 0.00001028
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001023
Iteration 29/1000 | Loss: 0.00001018
Iteration 30/1000 | Loss: 0.00001018
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001015
Iteration 33/1000 | Loss: 0.00001015
Iteration 34/1000 | Loss: 0.00001014
Iteration 35/1000 | Loss: 0.00001014
Iteration 36/1000 | Loss: 0.00001013
Iteration 37/1000 | Loss: 0.00001013
Iteration 38/1000 | Loss: 0.00001012
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001008
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001001
Iteration 55/1000 | Loss: 0.00001001
Iteration 56/1000 | Loss: 0.00001001
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000999
Iteration 60/1000 | Loss: 0.00000999
Iteration 61/1000 | Loss: 0.00000999
Iteration 62/1000 | Loss: 0.00000998
Iteration 63/1000 | Loss: 0.00000998
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000997
Iteration 66/1000 | Loss: 0.00000997
Iteration 67/1000 | Loss: 0.00000996
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000995
Iteration 72/1000 | Loss: 0.00000995
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000994
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000993
Iteration 77/1000 | Loss: 0.00000993
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000992
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000992
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000991
Iteration 86/1000 | Loss: 0.00000991
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000991
Iteration 89/1000 | Loss: 0.00000991
Iteration 90/1000 | Loss: 0.00000990
Iteration 91/1000 | Loss: 0.00000990
Iteration 92/1000 | Loss: 0.00000990
Iteration 93/1000 | Loss: 0.00000990
Iteration 94/1000 | Loss: 0.00000990
Iteration 95/1000 | Loss: 0.00000990
Iteration 96/1000 | Loss: 0.00000990
Iteration 97/1000 | Loss: 0.00000989
Iteration 98/1000 | Loss: 0.00000989
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000989
Iteration 101/1000 | Loss: 0.00000988
Iteration 102/1000 | Loss: 0.00000988
Iteration 103/1000 | Loss: 0.00000988
Iteration 104/1000 | Loss: 0.00000988
Iteration 105/1000 | Loss: 0.00000988
Iteration 106/1000 | Loss: 0.00000988
Iteration 107/1000 | Loss: 0.00000988
Iteration 108/1000 | Loss: 0.00000988
Iteration 109/1000 | Loss: 0.00000988
Iteration 110/1000 | Loss: 0.00000987
Iteration 111/1000 | Loss: 0.00000987
Iteration 112/1000 | Loss: 0.00000987
Iteration 113/1000 | Loss: 0.00000987
Iteration 114/1000 | Loss: 0.00000987
Iteration 115/1000 | Loss: 0.00000987
Iteration 116/1000 | Loss: 0.00000987
Iteration 117/1000 | Loss: 0.00000987
Iteration 118/1000 | Loss: 0.00000986
Iteration 119/1000 | Loss: 0.00000986
Iteration 120/1000 | Loss: 0.00000986
Iteration 121/1000 | Loss: 0.00000986
Iteration 122/1000 | Loss: 0.00000986
Iteration 123/1000 | Loss: 0.00000986
Iteration 124/1000 | Loss: 0.00000986
Iteration 125/1000 | Loss: 0.00000986
Iteration 126/1000 | Loss: 0.00000986
Iteration 127/1000 | Loss: 0.00000986
Iteration 128/1000 | Loss: 0.00000986
Iteration 129/1000 | Loss: 0.00000986
Iteration 130/1000 | Loss: 0.00000986
Iteration 131/1000 | Loss: 0.00000986
Iteration 132/1000 | Loss: 0.00000986
Iteration 133/1000 | Loss: 0.00000986
Iteration 134/1000 | Loss: 0.00000985
Iteration 135/1000 | Loss: 0.00000985
Iteration 136/1000 | Loss: 0.00000985
Iteration 137/1000 | Loss: 0.00000985
Iteration 138/1000 | Loss: 0.00000985
Iteration 139/1000 | Loss: 0.00000985
Iteration 140/1000 | Loss: 0.00000985
Iteration 141/1000 | Loss: 0.00000985
Iteration 142/1000 | Loss: 0.00000985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [9.85288352239877e-06, 9.85288352239877e-06, 9.85288352239877e-06, 9.85288352239877e-06, 9.85288352239877e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.85288352239877e-06

Optimization complete. Final v2v error: 2.7136027812957764 mm

Highest mean error: 3.1087915897369385 mm for frame 77

Lowest mean error: 2.5483479499816895 mm for frame 106

Saving results

Total time: 37.04765558242798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416545
Iteration 2/25 | Loss: 0.00132723
Iteration 3/25 | Loss: 0.00124563
Iteration 4/25 | Loss: 0.00123152
Iteration 5/25 | Loss: 0.00122769
Iteration 6/25 | Loss: 0.00122673
Iteration 7/25 | Loss: 0.00122643
Iteration 8/25 | Loss: 0.00122643
Iteration 9/25 | Loss: 0.00122643
Iteration 10/25 | Loss: 0.00122643
Iteration 11/25 | Loss: 0.00122643
Iteration 12/25 | Loss: 0.00122643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012264266842976213, 0.0012264266842976213, 0.0012264266842976213, 0.0012264266842976213, 0.0012264266842976213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012264266842976213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38665700
Iteration 2/25 | Loss: 0.00094594
Iteration 3/25 | Loss: 0.00094594
Iteration 4/25 | Loss: 0.00094594
Iteration 5/25 | Loss: 0.00094594
Iteration 6/25 | Loss: 0.00094593
Iteration 7/25 | Loss: 0.00094593
Iteration 8/25 | Loss: 0.00094593
Iteration 9/25 | Loss: 0.00094593
Iteration 10/25 | Loss: 0.00094593
Iteration 11/25 | Loss: 0.00094593
Iteration 12/25 | Loss: 0.00094593
Iteration 13/25 | Loss: 0.00094593
Iteration 14/25 | Loss: 0.00094593
Iteration 15/25 | Loss: 0.00094593
Iteration 16/25 | Loss: 0.00094593
Iteration 17/25 | Loss: 0.00094593
Iteration 18/25 | Loss: 0.00094593
Iteration 19/25 | Loss: 0.00094593
Iteration 20/25 | Loss: 0.00094593
Iteration 21/25 | Loss: 0.00094593
Iteration 22/25 | Loss: 0.00094593
Iteration 23/25 | Loss: 0.00094593
Iteration 24/25 | Loss: 0.00094593
Iteration 25/25 | Loss: 0.00094593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094593
Iteration 2/1000 | Loss: 0.00003920
Iteration 3/1000 | Loss: 0.00002443
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001805
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001678
Iteration 8/1000 | Loss: 0.00001632
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001524
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001505
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001490
Iteration 27/1000 | Loss: 0.00001489
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001489
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001487
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001486
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001485
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001482
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001471
Iteration 104/1000 | Loss: 0.00001471
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001464
Iteration 146/1000 | Loss: 0.00001464
Iteration 147/1000 | Loss: 0.00001464
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001464
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001463
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00001463
Iteration 158/1000 | Loss: 0.00001463
Iteration 159/1000 | Loss: 0.00001463
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001462
Iteration 162/1000 | Loss: 0.00001462
Iteration 163/1000 | Loss: 0.00001462
Iteration 164/1000 | Loss: 0.00001462
Iteration 165/1000 | Loss: 0.00001462
Iteration 166/1000 | Loss: 0.00001462
Iteration 167/1000 | Loss: 0.00001462
Iteration 168/1000 | Loss: 0.00001462
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001461
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Iteration 180/1000 | Loss: 0.00001460
Iteration 181/1000 | Loss: 0.00001460
Iteration 182/1000 | Loss: 0.00001460
Iteration 183/1000 | Loss: 0.00001460
Iteration 184/1000 | Loss: 0.00001460
Iteration 185/1000 | Loss: 0.00001460
Iteration 186/1000 | Loss: 0.00001460
Iteration 187/1000 | Loss: 0.00001460
Iteration 188/1000 | Loss: 0.00001460
Iteration 189/1000 | Loss: 0.00001459
Iteration 190/1000 | Loss: 0.00001459
Iteration 191/1000 | Loss: 0.00001459
Iteration 192/1000 | Loss: 0.00001459
Iteration 193/1000 | Loss: 0.00001459
Iteration 194/1000 | Loss: 0.00001459
Iteration 195/1000 | Loss: 0.00001459
Iteration 196/1000 | Loss: 0.00001459
Iteration 197/1000 | Loss: 0.00001459
Iteration 198/1000 | Loss: 0.00001459
Iteration 199/1000 | Loss: 0.00001459
Iteration 200/1000 | Loss: 0.00001459
Iteration 201/1000 | Loss: 0.00001459
Iteration 202/1000 | Loss: 0.00001458
Iteration 203/1000 | Loss: 0.00001458
Iteration 204/1000 | Loss: 0.00001458
Iteration 205/1000 | Loss: 0.00001458
Iteration 206/1000 | Loss: 0.00001458
Iteration 207/1000 | Loss: 0.00001458
Iteration 208/1000 | Loss: 0.00001458
Iteration 209/1000 | Loss: 0.00001458
Iteration 210/1000 | Loss: 0.00001458
Iteration 211/1000 | Loss: 0.00001458
Iteration 212/1000 | Loss: 0.00001458
Iteration 213/1000 | Loss: 0.00001458
Iteration 214/1000 | Loss: 0.00001458
Iteration 215/1000 | Loss: 0.00001458
Iteration 216/1000 | Loss: 0.00001458
Iteration 217/1000 | Loss: 0.00001458
Iteration 218/1000 | Loss: 0.00001458
Iteration 219/1000 | Loss: 0.00001458
Iteration 220/1000 | Loss: 0.00001457
Iteration 221/1000 | Loss: 0.00001457
Iteration 222/1000 | Loss: 0.00001457
Iteration 223/1000 | Loss: 0.00001457
Iteration 224/1000 | Loss: 0.00001457
Iteration 225/1000 | Loss: 0.00001457
Iteration 226/1000 | Loss: 0.00001457
Iteration 227/1000 | Loss: 0.00001457
Iteration 228/1000 | Loss: 0.00001457
Iteration 229/1000 | Loss: 0.00001457
Iteration 230/1000 | Loss: 0.00001457
Iteration 231/1000 | Loss: 0.00001457
Iteration 232/1000 | Loss: 0.00001457
Iteration 233/1000 | Loss: 0.00001457
Iteration 234/1000 | Loss: 0.00001457
Iteration 235/1000 | Loss: 0.00001457
Iteration 236/1000 | Loss: 0.00001457
Iteration 237/1000 | Loss: 0.00001457
Iteration 238/1000 | Loss: 0.00001457
Iteration 239/1000 | Loss: 0.00001457
Iteration 240/1000 | Loss: 0.00001457
Iteration 241/1000 | Loss: 0.00001457
Iteration 242/1000 | Loss: 0.00001457
Iteration 243/1000 | Loss: 0.00001457
Iteration 244/1000 | Loss: 0.00001457
Iteration 245/1000 | Loss: 0.00001457
Iteration 246/1000 | Loss: 0.00001457
Iteration 247/1000 | Loss: 0.00001457
Iteration 248/1000 | Loss: 0.00001457
Iteration 249/1000 | Loss: 0.00001457
Iteration 250/1000 | Loss: 0.00001457
Iteration 251/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.4567371181328781e-05, 1.4567371181328781e-05, 1.4567371181328781e-05, 1.4567371181328781e-05, 1.4567371181328781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4567371181328781e-05

Optimization complete. Final v2v error: 3.1848604679107666 mm

Highest mean error: 4.472535133361816 mm for frame 54

Lowest mean error: 2.6522226333618164 mm for frame 114

Saving results

Total time: 44.7850878238678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808425
Iteration 2/25 | Loss: 0.00133848
Iteration 3/25 | Loss: 0.00122771
Iteration 4/25 | Loss: 0.00121539
Iteration 5/25 | Loss: 0.00121172
Iteration 6/25 | Loss: 0.00121092
Iteration 7/25 | Loss: 0.00121092
Iteration 8/25 | Loss: 0.00121092
Iteration 9/25 | Loss: 0.00121092
Iteration 10/25 | Loss: 0.00121092
Iteration 11/25 | Loss: 0.00121092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012109233066439629, 0.0012109233066439629, 0.0012109233066439629, 0.0012109233066439629, 0.0012109233066439629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012109233066439629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36435223
Iteration 2/25 | Loss: 0.00109987
Iteration 3/25 | Loss: 0.00109986
Iteration 4/25 | Loss: 0.00109986
Iteration 5/25 | Loss: 0.00109986
Iteration 6/25 | Loss: 0.00109986
Iteration 7/25 | Loss: 0.00109986
Iteration 8/25 | Loss: 0.00109986
Iteration 9/25 | Loss: 0.00109986
Iteration 10/25 | Loss: 0.00109986
Iteration 11/25 | Loss: 0.00109986
Iteration 12/25 | Loss: 0.00109986
Iteration 13/25 | Loss: 0.00109986
Iteration 14/25 | Loss: 0.00109986
Iteration 15/25 | Loss: 0.00109986
Iteration 16/25 | Loss: 0.00109986
Iteration 17/25 | Loss: 0.00109986
Iteration 18/25 | Loss: 0.00109986
Iteration 19/25 | Loss: 0.00109986
Iteration 20/25 | Loss: 0.00109986
Iteration 21/25 | Loss: 0.00109986
Iteration 22/25 | Loss: 0.00109986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010998606448993087, 0.0010998606448993087, 0.0010998606448993087, 0.0010998606448993087, 0.0010998606448993087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010998606448993087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109986
Iteration 2/1000 | Loss: 0.00002954
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001442
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001178
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001176
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001159
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001146
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001142
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001139
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001138
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001138
Iteration 48/1000 | Loss: 0.00001137
Iteration 49/1000 | Loss: 0.00001137
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001135
Iteration 55/1000 | Loss: 0.00001135
Iteration 56/1000 | Loss: 0.00001135
Iteration 57/1000 | Loss: 0.00001135
Iteration 58/1000 | Loss: 0.00001134
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001133
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001127
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001126
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001125
Iteration 78/1000 | Loss: 0.00001125
Iteration 79/1000 | Loss: 0.00001125
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001123
Iteration 85/1000 | Loss: 0.00001123
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001122
Iteration 89/1000 | Loss: 0.00001122
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001114
Iteration 122/1000 | Loss: 0.00001114
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001112
Iteration 128/1000 | Loss: 0.00001112
Iteration 129/1000 | Loss: 0.00001112
Iteration 130/1000 | Loss: 0.00001111
Iteration 131/1000 | Loss: 0.00001111
Iteration 132/1000 | Loss: 0.00001111
Iteration 133/1000 | Loss: 0.00001111
Iteration 134/1000 | Loss: 0.00001110
Iteration 135/1000 | Loss: 0.00001110
Iteration 136/1000 | Loss: 0.00001110
Iteration 137/1000 | Loss: 0.00001110
Iteration 138/1000 | Loss: 0.00001110
Iteration 139/1000 | Loss: 0.00001110
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001109
Iteration 150/1000 | Loss: 0.00001109
Iteration 151/1000 | Loss: 0.00001109
Iteration 152/1000 | Loss: 0.00001109
Iteration 153/1000 | Loss: 0.00001109
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001108
Iteration 159/1000 | Loss: 0.00001108
Iteration 160/1000 | Loss: 0.00001108
Iteration 161/1000 | Loss: 0.00001108
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001107
Iteration 171/1000 | Loss: 0.00001107
Iteration 172/1000 | Loss: 0.00001107
Iteration 173/1000 | Loss: 0.00001107
Iteration 174/1000 | Loss: 0.00001107
Iteration 175/1000 | Loss: 0.00001107
Iteration 176/1000 | Loss: 0.00001107
Iteration 177/1000 | Loss: 0.00001107
Iteration 178/1000 | Loss: 0.00001107
Iteration 179/1000 | Loss: 0.00001107
Iteration 180/1000 | Loss: 0.00001107
Iteration 181/1000 | Loss: 0.00001107
Iteration 182/1000 | Loss: 0.00001107
Iteration 183/1000 | Loss: 0.00001107
Iteration 184/1000 | Loss: 0.00001107
Iteration 185/1000 | Loss: 0.00001107
Iteration 186/1000 | Loss: 0.00001107
Iteration 187/1000 | Loss: 0.00001107
Iteration 188/1000 | Loss: 0.00001107
Iteration 189/1000 | Loss: 0.00001107
Iteration 190/1000 | Loss: 0.00001107
Iteration 191/1000 | Loss: 0.00001107
Iteration 192/1000 | Loss: 0.00001107
Iteration 193/1000 | Loss: 0.00001107
Iteration 194/1000 | Loss: 0.00001107
Iteration 195/1000 | Loss: 0.00001107
Iteration 196/1000 | Loss: 0.00001107
Iteration 197/1000 | Loss: 0.00001107
Iteration 198/1000 | Loss: 0.00001107
Iteration 199/1000 | Loss: 0.00001107
Iteration 200/1000 | Loss: 0.00001107
Iteration 201/1000 | Loss: 0.00001107
Iteration 202/1000 | Loss: 0.00001107
Iteration 203/1000 | Loss: 0.00001107
Iteration 204/1000 | Loss: 0.00001107
Iteration 205/1000 | Loss: 0.00001107
Iteration 206/1000 | Loss: 0.00001107
Iteration 207/1000 | Loss: 0.00001107
Iteration 208/1000 | Loss: 0.00001107
Iteration 209/1000 | Loss: 0.00001107
Iteration 210/1000 | Loss: 0.00001107
Iteration 211/1000 | Loss: 0.00001107
Iteration 212/1000 | Loss: 0.00001107
Iteration 213/1000 | Loss: 0.00001107
Iteration 214/1000 | Loss: 0.00001107
Iteration 215/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.1071967492171098e-05, 1.1071967492171098e-05, 1.1071967492171098e-05, 1.1071967492171098e-05, 1.1071967492171098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1071967492171098e-05

Optimization complete. Final v2v error: 2.827404260635376 mm

Highest mean error: 3.8084707260131836 mm for frame 86

Lowest mean error: 2.5333492755889893 mm for frame 20

Saving results

Total time: 41.32603645324707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903550
Iteration 2/25 | Loss: 0.00135494
Iteration 3/25 | Loss: 0.00125690
Iteration 4/25 | Loss: 0.00124078
Iteration 5/25 | Loss: 0.00123556
Iteration 6/25 | Loss: 0.00123469
Iteration 7/25 | Loss: 0.00123469
Iteration 8/25 | Loss: 0.00123469
Iteration 9/25 | Loss: 0.00123469
Iteration 10/25 | Loss: 0.00123469
Iteration 11/25 | Loss: 0.00123469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012346893781796098, 0.0012346893781796098, 0.0012346893781796098, 0.0012346893781796098, 0.0012346893781796098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012346893781796098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35193253
Iteration 2/25 | Loss: 0.00099091
Iteration 3/25 | Loss: 0.00099090
Iteration 4/25 | Loss: 0.00099090
Iteration 5/25 | Loss: 0.00099090
Iteration 6/25 | Loss: 0.00099090
Iteration 7/25 | Loss: 0.00099090
Iteration 8/25 | Loss: 0.00099090
Iteration 9/25 | Loss: 0.00099090
Iteration 10/25 | Loss: 0.00099090
Iteration 11/25 | Loss: 0.00099090
Iteration 12/25 | Loss: 0.00099090
Iteration 13/25 | Loss: 0.00099090
Iteration 14/25 | Loss: 0.00099090
Iteration 15/25 | Loss: 0.00099090
Iteration 16/25 | Loss: 0.00099090
Iteration 17/25 | Loss: 0.00099090
Iteration 18/25 | Loss: 0.00099090
Iteration 19/25 | Loss: 0.00099090
Iteration 20/25 | Loss: 0.00099090
Iteration 21/25 | Loss: 0.00099090
Iteration 22/25 | Loss: 0.00099090
Iteration 23/25 | Loss: 0.00099090
Iteration 24/25 | Loss: 0.00099090
Iteration 25/25 | Loss: 0.00099090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099090
Iteration 2/1000 | Loss: 0.00004627
Iteration 3/1000 | Loss: 0.00003062
Iteration 4/1000 | Loss: 0.00002452
Iteration 5/1000 | Loss: 0.00002283
Iteration 6/1000 | Loss: 0.00002166
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001959
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001930
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001905
Iteration 16/1000 | Loss: 0.00001904
Iteration 17/1000 | Loss: 0.00001903
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001880
Iteration 26/1000 | Loss: 0.00001878
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001876
Iteration 30/1000 | Loss: 0.00001876
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001871
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001868
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001864
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001862
Iteration 54/1000 | Loss: 0.00001862
Iteration 55/1000 | Loss: 0.00001862
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001847
Iteration 115/1000 | Loss: 0.00001847
Iteration 116/1000 | Loss: 0.00001847
Iteration 117/1000 | Loss: 0.00001847
Iteration 118/1000 | Loss: 0.00001847
Iteration 119/1000 | Loss: 0.00001847
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001844
Iteration 128/1000 | Loss: 0.00001844
Iteration 129/1000 | Loss: 0.00001844
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001841
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001841
Iteration 145/1000 | Loss: 0.00001841
Iteration 146/1000 | Loss: 0.00001840
Iteration 147/1000 | Loss: 0.00001840
Iteration 148/1000 | Loss: 0.00001840
Iteration 149/1000 | Loss: 0.00001840
Iteration 150/1000 | Loss: 0.00001840
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001839
Iteration 155/1000 | Loss: 0.00001839
Iteration 156/1000 | Loss: 0.00001839
Iteration 157/1000 | Loss: 0.00001838
Iteration 158/1000 | Loss: 0.00001838
Iteration 159/1000 | Loss: 0.00001838
Iteration 160/1000 | Loss: 0.00001838
Iteration 161/1000 | Loss: 0.00001838
Iteration 162/1000 | Loss: 0.00001838
Iteration 163/1000 | Loss: 0.00001838
Iteration 164/1000 | Loss: 0.00001838
Iteration 165/1000 | Loss: 0.00001838
Iteration 166/1000 | Loss: 0.00001837
Iteration 167/1000 | Loss: 0.00001837
Iteration 168/1000 | Loss: 0.00001837
Iteration 169/1000 | Loss: 0.00001837
Iteration 170/1000 | Loss: 0.00001837
Iteration 171/1000 | Loss: 0.00001837
Iteration 172/1000 | Loss: 0.00001837
Iteration 173/1000 | Loss: 0.00001836
Iteration 174/1000 | Loss: 0.00001836
Iteration 175/1000 | Loss: 0.00001836
Iteration 176/1000 | Loss: 0.00001836
Iteration 177/1000 | Loss: 0.00001835
Iteration 178/1000 | Loss: 0.00001835
Iteration 179/1000 | Loss: 0.00001835
Iteration 180/1000 | Loss: 0.00001834
Iteration 181/1000 | Loss: 0.00001834
Iteration 182/1000 | Loss: 0.00001834
Iteration 183/1000 | Loss: 0.00001834
Iteration 184/1000 | Loss: 0.00001834
Iteration 185/1000 | Loss: 0.00001833
Iteration 186/1000 | Loss: 0.00001833
Iteration 187/1000 | Loss: 0.00001832
Iteration 188/1000 | Loss: 0.00001832
Iteration 189/1000 | Loss: 0.00001832
Iteration 190/1000 | Loss: 0.00001832
Iteration 191/1000 | Loss: 0.00001832
Iteration 192/1000 | Loss: 0.00001832
Iteration 193/1000 | Loss: 0.00001832
Iteration 194/1000 | Loss: 0.00001832
Iteration 195/1000 | Loss: 0.00001832
Iteration 196/1000 | Loss: 0.00001832
Iteration 197/1000 | Loss: 0.00001832
Iteration 198/1000 | Loss: 0.00001832
Iteration 199/1000 | Loss: 0.00001832
Iteration 200/1000 | Loss: 0.00001831
Iteration 201/1000 | Loss: 0.00001831
Iteration 202/1000 | Loss: 0.00001831
Iteration 203/1000 | Loss: 0.00001831
Iteration 204/1000 | Loss: 0.00001831
Iteration 205/1000 | Loss: 0.00001830
Iteration 206/1000 | Loss: 0.00001830
Iteration 207/1000 | Loss: 0.00001830
Iteration 208/1000 | Loss: 0.00001830
Iteration 209/1000 | Loss: 0.00001830
Iteration 210/1000 | Loss: 0.00001829
Iteration 211/1000 | Loss: 0.00001829
Iteration 212/1000 | Loss: 0.00001829
Iteration 213/1000 | Loss: 0.00001829
Iteration 214/1000 | Loss: 0.00001829
Iteration 215/1000 | Loss: 0.00001829
Iteration 216/1000 | Loss: 0.00001829
Iteration 217/1000 | Loss: 0.00001829
Iteration 218/1000 | Loss: 0.00001829
Iteration 219/1000 | Loss: 0.00001829
Iteration 220/1000 | Loss: 0.00001829
Iteration 221/1000 | Loss: 0.00001829
Iteration 222/1000 | Loss: 0.00001829
Iteration 223/1000 | Loss: 0.00001828
Iteration 224/1000 | Loss: 0.00001828
Iteration 225/1000 | Loss: 0.00001828
Iteration 226/1000 | Loss: 0.00001828
Iteration 227/1000 | Loss: 0.00001828
Iteration 228/1000 | Loss: 0.00001828
Iteration 229/1000 | Loss: 0.00001828
Iteration 230/1000 | Loss: 0.00001828
Iteration 231/1000 | Loss: 0.00001828
Iteration 232/1000 | Loss: 0.00001828
Iteration 233/1000 | Loss: 0.00001828
Iteration 234/1000 | Loss: 0.00001828
Iteration 235/1000 | Loss: 0.00001828
Iteration 236/1000 | Loss: 0.00001828
Iteration 237/1000 | Loss: 0.00001828
Iteration 238/1000 | Loss: 0.00001828
Iteration 239/1000 | Loss: 0.00001828
Iteration 240/1000 | Loss: 0.00001828
Iteration 241/1000 | Loss: 0.00001828
Iteration 242/1000 | Loss: 0.00001828
Iteration 243/1000 | Loss: 0.00001828
Iteration 244/1000 | Loss: 0.00001828
Iteration 245/1000 | Loss: 0.00001828
Iteration 246/1000 | Loss: 0.00001828
Iteration 247/1000 | Loss: 0.00001828
Iteration 248/1000 | Loss: 0.00001828
Iteration 249/1000 | Loss: 0.00001828
Iteration 250/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.8282149540027604e-05, 1.8282149540027604e-05, 1.8282149540027604e-05, 1.8282149540027604e-05, 1.8282149540027604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8282149540027604e-05

Optimization complete. Final v2v error: 3.5564658641815186 mm

Highest mean error: 5.514239311218262 mm for frame 71

Lowest mean error: 3.0977590084075928 mm for frame 95

Saving results

Total time: 45.037936210632324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950376
Iteration 2/25 | Loss: 0.00251379
Iteration 3/25 | Loss: 0.00182374
Iteration 4/25 | Loss: 0.00170039
Iteration 5/25 | Loss: 0.00165072
Iteration 6/25 | Loss: 0.00155458
Iteration 7/25 | Loss: 0.00148769
Iteration 8/25 | Loss: 0.00143897
Iteration 9/25 | Loss: 0.00143989
Iteration 10/25 | Loss: 0.00143715
Iteration 11/25 | Loss: 0.00141574
Iteration 12/25 | Loss: 0.00141202
Iteration 13/25 | Loss: 0.00138802
Iteration 14/25 | Loss: 0.00137927
Iteration 15/25 | Loss: 0.00138285
Iteration 16/25 | Loss: 0.00138218
Iteration 17/25 | Loss: 0.00136898
Iteration 18/25 | Loss: 0.00136051
Iteration 19/25 | Loss: 0.00135492
Iteration 20/25 | Loss: 0.00136014
Iteration 21/25 | Loss: 0.00134747
Iteration 22/25 | Loss: 0.00134280
Iteration 23/25 | Loss: 0.00134010
Iteration 24/25 | Loss: 0.00133770
Iteration 25/25 | Loss: 0.00133391

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36730671
Iteration 2/25 | Loss: 0.00201554
Iteration 3/25 | Loss: 0.00168455
Iteration 4/25 | Loss: 0.00168455
Iteration 5/25 | Loss: 0.00168455
Iteration 6/25 | Loss: 0.00168455
Iteration 7/25 | Loss: 0.00168455
Iteration 8/25 | Loss: 0.00168455
Iteration 9/25 | Loss: 0.00168455
Iteration 10/25 | Loss: 0.00168455
Iteration 11/25 | Loss: 0.00168455
Iteration 12/25 | Loss: 0.00168455
Iteration 13/25 | Loss: 0.00168455
Iteration 14/25 | Loss: 0.00168455
Iteration 15/25 | Loss: 0.00168455
Iteration 16/25 | Loss: 0.00168455
Iteration 17/25 | Loss: 0.00168455
Iteration 18/25 | Loss: 0.00168455
Iteration 19/25 | Loss: 0.00168455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016845498466864228, 0.0016845498466864228, 0.0016845498466864228, 0.0016845498466864228, 0.0016845498466864228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016845498466864228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168455
Iteration 2/1000 | Loss: 0.00024933
Iteration 3/1000 | Loss: 0.00047412
Iteration 4/1000 | Loss: 0.00061520
Iteration 5/1000 | Loss: 0.00042648
Iteration 6/1000 | Loss: 0.00059363
Iteration 7/1000 | Loss: 0.00076956
Iteration 8/1000 | Loss: 0.00046218
Iteration 9/1000 | Loss: 0.00058011
Iteration 10/1000 | Loss: 0.00045454
Iteration 11/1000 | Loss: 0.00062601
Iteration 12/1000 | Loss: 0.00075100
Iteration 13/1000 | Loss: 0.00086264
Iteration 14/1000 | Loss: 0.00074450
Iteration 15/1000 | Loss: 0.00064292
Iteration 16/1000 | Loss: 0.00020172
Iteration 17/1000 | Loss: 0.00018198
Iteration 18/1000 | Loss: 0.00018940
Iteration 19/1000 | Loss: 0.00017582
Iteration 20/1000 | Loss: 0.00015043
Iteration 21/1000 | Loss: 0.00007444
Iteration 22/1000 | Loss: 0.00006902
Iteration 23/1000 | Loss: 0.00037499
Iteration 24/1000 | Loss: 0.00025646
Iteration 25/1000 | Loss: 0.00008437
Iteration 26/1000 | Loss: 0.00024740
Iteration 27/1000 | Loss: 0.00008877
Iteration 28/1000 | Loss: 0.00006385
Iteration 29/1000 | Loss: 0.00006072
Iteration 30/1000 | Loss: 0.00005808
Iteration 31/1000 | Loss: 0.00005864
Iteration 32/1000 | Loss: 0.00017747
Iteration 33/1000 | Loss: 0.00006883
Iteration 34/1000 | Loss: 0.00013566
Iteration 35/1000 | Loss: 0.00018258
Iteration 36/1000 | Loss: 0.00013463
Iteration 37/1000 | Loss: 0.00017108
Iteration 38/1000 | Loss: 0.00012508
Iteration 39/1000 | Loss: 0.00016356
Iteration 40/1000 | Loss: 0.00007042
Iteration 41/1000 | Loss: 0.00005888
Iteration 42/1000 | Loss: 0.00010330
Iteration 43/1000 | Loss: 0.00005712
Iteration 44/1000 | Loss: 0.00014961
Iteration 45/1000 | Loss: 0.00012613
Iteration 46/1000 | Loss: 0.00006164
Iteration 47/1000 | Loss: 0.00017368
Iteration 48/1000 | Loss: 0.00006407
Iteration 49/1000 | Loss: 0.00008017
Iteration 50/1000 | Loss: 0.00006039
Iteration 51/1000 | Loss: 0.00007290
Iteration 52/1000 | Loss: 0.00006335
Iteration 53/1000 | Loss: 0.00015639
Iteration 54/1000 | Loss: 0.00006623
Iteration 55/1000 | Loss: 0.00017072
Iteration 56/1000 | Loss: 0.00014695
Iteration 57/1000 | Loss: 0.00005510
Iteration 58/1000 | Loss: 0.00017800
Iteration 59/1000 | Loss: 0.00013722
Iteration 60/1000 | Loss: 0.00014017
Iteration 61/1000 | Loss: 0.00012659
Iteration 62/1000 | Loss: 0.00005467
Iteration 63/1000 | Loss: 0.00014986
Iteration 64/1000 | Loss: 0.00016988
Iteration 65/1000 | Loss: 0.00017594
Iteration 66/1000 | Loss: 0.00017912
Iteration 67/1000 | Loss: 0.00010836
Iteration 68/1000 | Loss: 0.00007065
Iteration 69/1000 | Loss: 0.00007578
Iteration 70/1000 | Loss: 0.00006565
Iteration 71/1000 | Loss: 0.00009126
Iteration 72/1000 | Loss: 0.00006464
Iteration 73/1000 | Loss: 0.00011286
Iteration 74/1000 | Loss: 0.00006463
Iteration 75/1000 | Loss: 0.00015116
Iteration 76/1000 | Loss: 0.00009721
Iteration 77/1000 | Loss: 0.00028193
Iteration 78/1000 | Loss: 0.00017356
Iteration 79/1000 | Loss: 0.00008483
Iteration 80/1000 | Loss: 0.00007436
Iteration 81/1000 | Loss: 0.00006838
Iteration 82/1000 | Loss: 0.00007276
Iteration 83/1000 | Loss: 0.00005562
Iteration 84/1000 | Loss: 0.00005433
Iteration 85/1000 | Loss: 0.00033573
Iteration 86/1000 | Loss: 0.00005364
Iteration 87/1000 | Loss: 0.00035179
Iteration 88/1000 | Loss: 0.00005226
Iteration 89/1000 | Loss: 0.00004902
Iteration 90/1000 | Loss: 0.00035906
Iteration 91/1000 | Loss: 0.00007311
Iteration 92/1000 | Loss: 0.00004804
Iteration 93/1000 | Loss: 0.00004333
Iteration 94/1000 | Loss: 0.00033099
Iteration 95/1000 | Loss: 0.00034888
Iteration 96/1000 | Loss: 0.00004282
Iteration 97/1000 | Loss: 0.00035674
Iteration 98/1000 | Loss: 0.00004075
Iteration 99/1000 | Loss: 0.00003733
Iteration 100/1000 | Loss: 0.00003511
Iteration 101/1000 | Loss: 0.00003323
Iteration 102/1000 | Loss: 0.00003217
Iteration 103/1000 | Loss: 0.00003158
Iteration 104/1000 | Loss: 0.00003119
Iteration 105/1000 | Loss: 0.00003074
Iteration 106/1000 | Loss: 0.00003041
Iteration 107/1000 | Loss: 0.00003021
Iteration 108/1000 | Loss: 0.00003002
Iteration 109/1000 | Loss: 0.00003001
Iteration 110/1000 | Loss: 0.00002996
Iteration 111/1000 | Loss: 0.00002991
Iteration 112/1000 | Loss: 0.00002985
Iteration 113/1000 | Loss: 0.00002984
Iteration 114/1000 | Loss: 0.00002982
Iteration 115/1000 | Loss: 0.00002976
Iteration 116/1000 | Loss: 0.00002972
Iteration 117/1000 | Loss: 0.00002971
Iteration 118/1000 | Loss: 0.00002971
Iteration 119/1000 | Loss: 0.00002967
Iteration 120/1000 | Loss: 0.00002955
Iteration 121/1000 | Loss: 0.00002954
Iteration 122/1000 | Loss: 0.00002954
Iteration 123/1000 | Loss: 0.00002952
Iteration 124/1000 | Loss: 0.00002951
Iteration 125/1000 | Loss: 0.00002950
Iteration 126/1000 | Loss: 0.00002950
Iteration 127/1000 | Loss: 0.00002948
Iteration 128/1000 | Loss: 0.00002942
Iteration 129/1000 | Loss: 0.00002939
Iteration 130/1000 | Loss: 0.00002939
Iteration 131/1000 | Loss: 0.00002938
Iteration 132/1000 | Loss: 0.00002938
Iteration 133/1000 | Loss: 0.00002937
Iteration 134/1000 | Loss: 0.00002937
Iteration 135/1000 | Loss: 0.00002937
Iteration 136/1000 | Loss: 0.00002936
Iteration 137/1000 | Loss: 0.00002936
Iteration 138/1000 | Loss: 0.00002935
Iteration 139/1000 | Loss: 0.00002935
Iteration 140/1000 | Loss: 0.00002935
Iteration 141/1000 | Loss: 0.00002934
Iteration 142/1000 | Loss: 0.00002934
Iteration 143/1000 | Loss: 0.00002934
Iteration 144/1000 | Loss: 0.00002933
Iteration 145/1000 | Loss: 0.00002933
Iteration 146/1000 | Loss: 0.00002932
Iteration 147/1000 | Loss: 0.00002932
Iteration 148/1000 | Loss: 0.00002932
Iteration 149/1000 | Loss: 0.00002931
Iteration 150/1000 | Loss: 0.00002931
Iteration 151/1000 | Loss: 0.00002931
Iteration 152/1000 | Loss: 0.00002930
Iteration 153/1000 | Loss: 0.00002929
Iteration 154/1000 | Loss: 0.00002929
Iteration 155/1000 | Loss: 0.00002928
Iteration 156/1000 | Loss: 0.00002928
Iteration 157/1000 | Loss: 0.00002927
Iteration 158/1000 | Loss: 0.00002927
Iteration 159/1000 | Loss: 0.00002927
Iteration 160/1000 | Loss: 0.00002927
Iteration 161/1000 | Loss: 0.00002927
Iteration 162/1000 | Loss: 0.00002927
Iteration 163/1000 | Loss: 0.00002927
Iteration 164/1000 | Loss: 0.00002927
Iteration 165/1000 | Loss: 0.00002927
Iteration 166/1000 | Loss: 0.00002926
Iteration 167/1000 | Loss: 0.00002926
Iteration 168/1000 | Loss: 0.00002926
Iteration 169/1000 | Loss: 0.00002926
Iteration 170/1000 | Loss: 0.00002926
Iteration 171/1000 | Loss: 0.00002926
Iteration 172/1000 | Loss: 0.00002926
Iteration 173/1000 | Loss: 0.00002926
Iteration 174/1000 | Loss: 0.00002926
Iteration 175/1000 | Loss: 0.00002926
Iteration 176/1000 | Loss: 0.00002926
Iteration 177/1000 | Loss: 0.00002926
Iteration 178/1000 | Loss: 0.00002925
Iteration 179/1000 | Loss: 0.00002925
Iteration 180/1000 | Loss: 0.00002925
Iteration 181/1000 | Loss: 0.00002925
Iteration 182/1000 | Loss: 0.00002925
Iteration 183/1000 | Loss: 0.00002925
Iteration 184/1000 | Loss: 0.00002925
Iteration 185/1000 | Loss: 0.00002925
Iteration 186/1000 | Loss: 0.00002924
Iteration 187/1000 | Loss: 0.00002924
Iteration 188/1000 | Loss: 0.00002924
Iteration 189/1000 | Loss: 0.00002924
Iteration 190/1000 | Loss: 0.00002924
Iteration 191/1000 | Loss: 0.00002924
Iteration 192/1000 | Loss: 0.00002924
Iteration 193/1000 | Loss: 0.00002924
Iteration 194/1000 | Loss: 0.00002923
Iteration 195/1000 | Loss: 0.00002923
Iteration 196/1000 | Loss: 0.00002923
Iteration 197/1000 | Loss: 0.00002923
Iteration 198/1000 | Loss: 0.00002923
Iteration 199/1000 | Loss: 0.00002923
Iteration 200/1000 | Loss: 0.00002922
Iteration 201/1000 | Loss: 0.00002922
Iteration 202/1000 | Loss: 0.00002921
Iteration 203/1000 | Loss: 0.00002921
Iteration 204/1000 | Loss: 0.00002921
Iteration 205/1000 | Loss: 0.00002921
Iteration 206/1000 | Loss: 0.00002920
Iteration 207/1000 | Loss: 0.00002920
Iteration 208/1000 | Loss: 0.00002920
Iteration 209/1000 | Loss: 0.00002919
Iteration 210/1000 | Loss: 0.00002919
Iteration 211/1000 | Loss: 0.00002919
Iteration 212/1000 | Loss: 0.00002919
Iteration 213/1000 | Loss: 0.00002919
Iteration 214/1000 | Loss: 0.00002919
Iteration 215/1000 | Loss: 0.00002918
Iteration 216/1000 | Loss: 0.00002918
Iteration 217/1000 | Loss: 0.00002918
Iteration 218/1000 | Loss: 0.00002918
Iteration 219/1000 | Loss: 0.00002918
Iteration 220/1000 | Loss: 0.00002918
Iteration 221/1000 | Loss: 0.00002918
Iteration 222/1000 | Loss: 0.00002918
Iteration 223/1000 | Loss: 0.00002918
Iteration 224/1000 | Loss: 0.00002918
Iteration 225/1000 | Loss: 0.00002918
Iteration 226/1000 | Loss: 0.00002918
Iteration 227/1000 | Loss: 0.00002917
Iteration 228/1000 | Loss: 0.00002917
Iteration 229/1000 | Loss: 0.00002917
Iteration 230/1000 | Loss: 0.00002917
Iteration 231/1000 | Loss: 0.00002917
Iteration 232/1000 | Loss: 0.00002917
Iteration 233/1000 | Loss: 0.00002916
Iteration 234/1000 | Loss: 0.00002916
Iteration 235/1000 | Loss: 0.00002916
Iteration 236/1000 | Loss: 0.00002916
Iteration 237/1000 | Loss: 0.00002915
Iteration 238/1000 | Loss: 0.00002915
Iteration 239/1000 | Loss: 0.00002915
Iteration 240/1000 | Loss: 0.00002915
Iteration 241/1000 | Loss: 0.00002914
Iteration 242/1000 | Loss: 0.00002914
Iteration 243/1000 | Loss: 0.00002914
Iteration 244/1000 | Loss: 0.00002914
Iteration 245/1000 | Loss: 0.00002914
Iteration 246/1000 | Loss: 0.00002914
Iteration 247/1000 | Loss: 0.00002914
Iteration 248/1000 | Loss: 0.00002913
Iteration 249/1000 | Loss: 0.00002913
Iteration 250/1000 | Loss: 0.00002913
Iteration 251/1000 | Loss: 0.00002913
Iteration 252/1000 | Loss: 0.00002913
Iteration 253/1000 | Loss: 0.00002913
Iteration 254/1000 | Loss: 0.00002913
Iteration 255/1000 | Loss: 0.00002913
Iteration 256/1000 | Loss: 0.00002913
Iteration 257/1000 | Loss: 0.00002913
Iteration 258/1000 | Loss: 0.00002913
Iteration 259/1000 | Loss: 0.00002913
Iteration 260/1000 | Loss: 0.00002913
Iteration 261/1000 | Loss: 0.00002913
Iteration 262/1000 | Loss: 0.00002913
Iteration 263/1000 | Loss: 0.00002913
Iteration 264/1000 | Loss: 0.00002913
Iteration 265/1000 | Loss: 0.00002913
Iteration 266/1000 | Loss: 0.00002913
Iteration 267/1000 | Loss: 0.00002912
Iteration 268/1000 | Loss: 0.00002912
Iteration 269/1000 | Loss: 0.00002912
Iteration 270/1000 | Loss: 0.00002912
Iteration 271/1000 | Loss: 0.00002912
Iteration 272/1000 | Loss: 0.00002912
Iteration 273/1000 | Loss: 0.00002912
Iteration 274/1000 | Loss: 0.00002912
Iteration 275/1000 | Loss: 0.00002912
Iteration 276/1000 | Loss: 0.00002912
Iteration 277/1000 | Loss: 0.00002912
Iteration 278/1000 | Loss: 0.00002912
Iteration 279/1000 | Loss: 0.00002912
Iteration 280/1000 | Loss: 0.00002912
Iteration 281/1000 | Loss: 0.00002912
Iteration 282/1000 | Loss: 0.00002912
Iteration 283/1000 | Loss: 0.00002912
Iteration 284/1000 | Loss: 0.00002912
Iteration 285/1000 | Loss: 0.00002912
Iteration 286/1000 | Loss: 0.00002912
Iteration 287/1000 | Loss: 0.00002912
Iteration 288/1000 | Loss: 0.00002912
Iteration 289/1000 | Loss: 0.00002912
Iteration 290/1000 | Loss: 0.00002912
Iteration 291/1000 | Loss: 0.00002912
Iteration 292/1000 | Loss: 0.00002912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.9118667953298427e-05, 2.9118667953298427e-05, 2.9118667953298427e-05, 2.9118667953298427e-05, 2.9118667953298427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9118667953298427e-05

Optimization complete. Final v2v error: 4.3144707679748535 mm

Highest mean error: 11.553915023803711 mm for frame 0

Lowest mean error: 3.284672975540161 mm for frame 168

Saving results

Total time: 214.70140647888184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815028
Iteration 2/25 | Loss: 0.00137768
Iteration 3/25 | Loss: 0.00125640
Iteration 4/25 | Loss: 0.00123241
Iteration 5/25 | Loss: 0.00123209
Iteration 6/25 | Loss: 0.00123679
Iteration 7/25 | Loss: 0.00124788
Iteration 8/25 | Loss: 0.00124229
Iteration 9/25 | Loss: 0.00123220
Iteration 10/25 | Loss: 0.00122764
Iteration 11/25 | Loss: 0.00122464
Iteration 12/25 | Loss: 0.00122256
Iteration 13/25 | Loss: 0.00122040
Iteration 14/25 | Loss: 0.00121917
Iteration 15/25 | Loss: 0.00121908
Iteration 16/25 | Loss: 0.00121570
Iteration 17/25 | Loss: 0.00121458
Iteration 18/25 | Loss: 0.00121287
Iteration 19/25 | Loss: 0.00121110
Iteration 20/25 | Loss: 0.00121054
Iteration 21/25 | Loss: 0.00121045
Iteration 22/25 | Loss: 0.00121045
Iteration 23/25 | Loss: 0.00121045
Iteration 24/25 | Loss: 0.00121045
Iteration 25/25 | Loss: 0.00121045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34620142
Iteration 2/25 | Loss: 0.00152363
Iteration 3/25 | Loss: 0.00152361
Iteration 4/25 | Loss: 0.00152361
Iteration 5/25 | Loss: 0.00152361
Iteration 6/25 | Loss: 0.00152361
Iteration 7/25 | Loss: 0.00152361
Iteration 8/25 | Loss: 0.00152361
Iteration 9/25 | Loss: 0.00152361
Iteration 10/25 | Loss: 0.00152361
Iteration 11/25 | Loss: 0.00152361
Iteration 12/25 | Loss: 0.00152361
Iteration 13/25 | Loss: 0.00152361
Iteration 14/25 | Loss: 0.00152361
Iteration 15/25 | Loss: 0.00152361
Iteration 16/25 | Loss: 0.00152361
Iteration 17/25 | Loss: 0.00152361
Iteration 18/25 | Loss: 0.00152361
Iteration 19/25 | Loss: 0.00152361
Iteration 20/25 | Loss: 0.00152361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001523611368611455, 0.001523611368611455, 0.001523611368611455, 0.001523611368611455, 0.001523611368611455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001523611368611455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152361
Iteration 2/1000 | Loss: 0.00008029
Iteration 3/1000 | Loss: 0.00010291
Iteration 4/1000 | Loss: 0.00008283
Iteration 5/1000 | Loss: 0.00005609
Iteration 6/1000 | Loss: 0.00003867
Iteration 7/1000 | Loss: 0.00006471
Iteration 8/1000 | Loss: 0.00005110
Iteration 9/1000 | Loss: 0.00002644
Iteration 10/1000 | Loss: 0.00003325
Iteration 11/1000 | Loss: 0.00003021
Iteration 12/1000 | Loss: 0.00004560
Iteration 13/1000 | Loss: 0.00002997
Iteration 14/1000 | Loss: 0.00003270
Iteration 15/1000 | Loss: 0.00004124
Iteration 16/1000 | Loss: 0.00004672
Iteration 17/1000 | Loss: 0.00005314
Iteration 18/1000 | Loss: 0.00003872
Iteration 19/1000 | Loss: 0.00003174
Iteration 20/1000 | Loss: 0.00003965
Iteration 21/1000 | Loss: 0.00003773
Iteration 22/1000 | Loss: 0.00003079
Iteration 23/1000 | Loss: 0.00002531
Iteration 24/1000 | Loss: 0.00003738
Iteration 25/1000 | Loss: 0.00004489
Iteration 26/1000 | Loss: 0.00003525
Iteration 27/1000 | Loss: 0.00001988
Iteration 28/1000 | Loss: 0.00003696
Iteration 29/1000 | Loss: 0.00004216
Iteration 30/1000 | Loss: 0.00002887
Iteration 31/1000 | Loss: 0.00002492
Iteration 32/1000 | Loss: 0.00003954
Iteration 33/1000 | Loss: 0.00003902
Iteration 34/1000 | Loss: 0.00004134
Iteration 35/1000 | Loss: 0.00004152
Iteration 36/1000 | Loss: 0.00003261
Iteration 37/1000 | Loss: 0.00003876
Iteration 38/1000 | Loss: 0.00003635
Iteration 39/1000 | Loss: 0.00004405
Iteration 40/1000 | Loss: 0.00005828
Iteration 41/1000 | Loss: 0.00003982
Iteration 42/1000 | Loss: 0.00004163
Iteration 43/1000 | Loss: 0.00004119
Iteration 44/1000 | Loss: 0.00005460
Iteration 45/1000 | Loss: 0.00003383
Iteration 46/1000 | Loss: 0.00004059
Iteration 47/1000 | Loss: 0.00004620
Iteration 48/1000 | Loss: 0.00004877
Iteration 49/1000 | Loss: 0.00004642
Iteration 50/1000 | Loss: 0.00005311
Iteration 51/1000 | Loss: 0.00004167
Iteration 52/1000 | Loss: 0.00004220
Iteration 53/1000 | Loss: 0.00004296
Iteration 54/1000 | Loss: 0.00003913
Iteration 55/1000 | Loss: 0.00004708
Iteration 56/1000 | Loss: 0.00004076
Iteration 57/1000 | Loss: 0.00004161
Iteration 58/1000 | Loss: 0.00003766
Iteration 59/1000 | Loss: 0.00004918
Iteration 60/1000 | Loss: 0.00004223
Iteration 61/1000 | Loss: 0.00004321
Iteration 62/1000 | Loss: 0.00005146
Iteration 63/1000 | Loss: 0.00013280
Iteration 64/1000 | Loss: 0.00020144
Iteration 65/1000 | Loss: 0.00011014
Iteration 66/1000 | Loss: 0.00003352
Iteration 67/1000 | Loss: 0.00004207
Iteration 68/1000 | Loss: 0.00003840
Iteration 69/1000 | Loss: 0.00002716
Iteration 70/1000 | Loss: 0.00002379
Iteration 71/1000 | Loss: 0.00002250
Iteration 72/1000 | Loss: 0.00002194
Iteration 73/1000 | Loss: 0.00016598
Iteration 74/1000 | Loss: 0.00010291
Iteration 75/1000 | Loss: 0.00012635
Iteration 76/1000 | Loss: 0.00002960
Iteration 77/1000 | Loss: 0.00002348
Iteration 78/1000 | Loss: 0.00002246
Iteration 79/1000 | Loss: 0.00008976
Iteration 80/1000 | Loss: 0.00008983
Iteration 81/1000 | Loss: 0.00002936
Iteration 82/1000 | Loss: 0.00006338
Iteration 83/1000 | Loss: 0.00008335
Iteration 84/1000 | Loss: 0.00002222
Iteration 85/1000 | Loss: 0.00002128
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002032
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002006
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002003
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001975
Iteration 95/1000 | Loss: 0.00001958
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001949
Iteration 99/1000 | Loss: 0.00001948
Iteration 100/1000 | Loss: 0.00003936
Iteration 101/1000 | Loss: 0.00002497
Iteration 102/1000 | Loss: 0.00002820
Iteration 103/1000 | Loss: 0.00002099
Iteration 104/1000 | Loss: 0.00002403
Iteration 105/1000 | Loss: 0.00002220
Iteration 106/1000 | Loss: 0.00001886
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001764
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001715
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001698
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001676
Iteration 119/1000 | Loss: 0.00001676
Iteration 120/1000 | Loss: 0.00001675
Iteration 121/1000 | Loss: 0.00001675
Iteration 122/1000 | Loss: 0.00001675
Iteration 123/1000 | Loss: 0.00001674
Iteration 124/1000 | Loss: 0.00001674
Iteration 125/1000 | Loss: 0.00001674
Iteration 126/1000 | Loss: 0.00001674
Iteration 127/1000 | Loss: 0.00001673
Iteration 128/1000 | Loss: 0.00001673
Iteration 129/1000 | Loss: 0.00001673
Iteration 130/1000 | Loss: 0.00001673
Iteration 131/1000 | Loss: 0.00001673
Iteration 132/1000 | Loss: 0.00001673
Iteration 133/1000 | Loss: 0.00001672
Iteration 134/1000 | Loss: 0.00001672
Iteration 135/1000 | Loss: 0.00001672
Iteration 136/1000 | Loss: 0.00001671
Iteration 137/1000 | Loss: 0.00001671
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001667
Iteration 141/1000 | Loss: 0.00001667
Iteration 142/1000 | Loss: 0.00001667
Iteration 143/1000 | Loss: 0.00001666
Iteration 144/1000 | Loss: 0.00001666
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001664
Iteration 149/1000 | Loss: 0.00001664
Iteration 150/1000 | Loss: 0.00001664
Iteration 151/1000 | Loss: 0.00001663
Iteration 152/1000 | Loss: 0.00001663
Iteration 153/1000 | Loss: 0.00001663
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001656
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001653
Iteration 165/1000 | Loss: 0.00001653
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001653
Iteration 170/1000 | Loss: 0.00001653
Iteration 171/1000 | Loss: 0.00001653
Iteration 172/1000 | Loss: 0.00001653
Iteration 173/1000 | Loss: 0.00001652
Iteration 174/1000 | Loss: 0.00001652
Iteration 175/1000 | Loss: 0.00001652
Iteration 176/1000 | Loss: 0.00001652
Iteration 177/1000 | Loss: 0.00001652
Iteration 178/1000 | Loss: 0.00001652
Iteration 179/1000 | Loss: 0.00001652
Iteration 180/1000 | Loss: 0.00001652
Iteration 181/1000 | Loss: 0.00001652
Iteration 182/1000 | Loss: 0.00001652
Iteration 183/1000 | Loss: 0.00001652
Iteration 184/1000 | Loss: 0.00001652
Iteration 185/1000 | Loss: 0.00001652
Iteration 186/1000 | Loss: 0.00001651
Iteration 187/1000 | Loss: 0.00001651
Iteration 188/1000 | Loss: 0.00001651
Iteration 189/1000 | Loss: 0.00001651
Iteration 190/1000 | Loss: 0.00001651
Iteration 191/1000 | Loss: 0.00001651
Iteration 192/1000 | Loss: 0.00001650
Iteration 193/1000 | Loss: 0.00001650
Iteration 194/1000 | Loss: 0.00001650
Iteration 195/1000 | Loss: 0.00001650
Iteration 196/1000 | Loss: 0.00001650
Iteration 197/1000 | Loss: 0.00001650
Iteration 198/1000 | Loss: 0.00001649
Iteration 199/1000 | Loss: 0.00001649
Iteration 200/1000 | Loss: 0.00001649
Iteration 201/1000 | Loss: 0.00001649
Iteration 202/1000 | Loss: 0.00001649
Iteration 203/1000 | Loss: 0.00001649
Iteration 204/1000 | Loss: 0.00001648
Iteration 205/1000 | Loss: 0.00001648
Iteration 206/1000 | Loss: 0.00001648
Iteration 207/1000 | Loss: 0.00001647
Iteration 208/1000 | Loss: 0.00001647
Iteration 209/1000 | Loss: 0.00001647
Iteration 210/1000 | Loss: 0.00001647
Iteration 211/1000 | Loss: 0.00001646
Iteration 212/1000 | Loss: 0.00001646
Iteration 213/1000 | Loss: 0.00001646
Iteration 214/1000 | Loss: 0.00001645
Iteration 215/1000 | Loss: 0.00001645
Iteration 216/1000 | Loss: 0.00001645
Iteration 217/1000 | Loss: 0.00001645
Iteration 218/1000 | Loss: 0.00001645
Iteration 219/1000 | Loss: 0.00001644
Iteration 220/1000 | Loss: 0.00001644
Iteration 221/1000 | Loss: 0.00001644
Iteration 222/1000 | Loss: 0.00001644
Iteration 223/1000 | Loss: 0.00001644
Iteration 224/1000 | Loss: 0.00001644
Iteration 225/1000 | Loss: 0.00001644
Iteration 226/1000 | Loss: 0.00001644
Iteration 227/1000 | Loss: 0.00001644
Iteration 228/1000 | Loss: 0.00001644
Iteration 229/1000 | Loss: 0.00001644
Iteration 230/1000 | Loss: 0.00001644
Iteration 231/1000 | Loss: 0.00001643
Iteration 232/1000 | Loss: 0.00001643
Iteration 233/1000 | Loss: 0.00001643
Iteration 234/1000 | Loss: 0.00001643
Iteration 235/1000 | Loss: 0.00001643
Iteration 236/1000 | Loss: 0.00001643
Iteration 237/1000 | Loss: 0.00001643
Iteration 238/1000 | Loss: 0.00001643
Iteration 239/1000 | Loss: 0.00001642
Iteration 240/1000 | Loss: 0.00001642
Iteration 241/1000 | Loss: 0.00001642
Iteration 242/1000 | Loss: 0.00001642
Iteration 243/1000 | Loss: 0.00001642
Iteration 244/1000 | Loss: 0.00001642
Iteration 245/1000 | Loss: 0.00001642
Iteration 246/1000 | Loss: 0.00001642
Iteration 247/1000 | Loss: 0.00001642
Iteration 248/1000 | Loss: 0.00001642
Iteration 249/1000 | Loss: 0.00001642
Iteration 250/1000 | Loss: 0.00001642
Iteration 251/1000 | Loss: 0.00001642
Iteration 252/1000 | Loss: 0.00001642
Iteration 253/1000 | Loss: 0.00001642
Iteration 254/1000 | Loss: 0.00001642
Iteration 255/1000 | Loss: 0.00001642
Iteration 256/1000 | Loss: 0.00001642
Iteration 257/1000 | Loss: 0.00001642
Iteration 258/1000 | Loss: 0.00001642
Iteration 259/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.6421498003182933e-05, 1.6421498003182933e-05, 1.6421498003182933e-05, 1.6421498003182933e-05, 1.6421498003182933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6421498003182933e-05

Optimization complete. Final v2v error: 3.3554859161376953 mm

Highest mean error: 5.489144325256348 mm for frame 222

Lowest mean error: 2.6635079383850098 mm for frame 11

Saving results

Total time: 218.5648229122162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969456
Iteration 2/25 | Loss: 0.00271837
Iteration 3/25 | Loss: 0.00201429
Iteration 4/25 | Loss: 0.00182649
Iteration 5/25 | Loss: 0.00181276
Iteration 6/25 | Loss: 0.00185253
Iteration 7/25 | Loss: 0.00174945
Iteration 8/25 | Loss: 0.00146920
Iteration 9/25 | Loss: 0.00149071
Iteration 10/25 | Loss: 0.00143174
Iteration 11/25 | Loss: 0.00140624
Iteration 12/25 | Loss: 0.00140164
Iteration 13/25 | Loss: 0.00130514
Iteration 14/25 | Loss: 0.00127194
Iteration 15/25 | Loss: 0.00131126
Iteration 16/25 | Loss: 0.00125368
Iteration 17/25 | Loss: 0.00124407
Iteration 18/25 | Loss: 0.00124582
Iteration 19/25 | Loss: 0.00124078
Iteration 20/25 | Loss: 0.00124029
Iteration 21/25 | Loss: 0.00123973
Iteration 22/25 | Loss: 0.00124321
Iteration 23/25 | Loss: 0.00124237
Iteration 24/25 | Loss: 0.00124249
Iteration 25/25 | Loss: 0.00123554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37778723
Iteration 2/25 | Loss: 0.00093406
Iteration 3/25 | Loss: 0.00078415
Iteration 4/25 | Loss: 0.00078415
Iteration 5/25 | Loss: 0.00078415
Iteration 6/25 | Loss: 0.00078415
Iteration 7/25 | Loss: 0.00078415
Iteration 8/25 | Loss: 0.00078415
Iteration 9/25 | Loss: 0.00078415
Iteration 10/25 | Loss: 0.00078415
Iteration 11/25 | Loss: 0.00078415
Iteration 12/25 | Loss: 0.00078415
Iteration 13/25 | Loss: 0.00078415
Iteration 14/25 | Loss: 0.00078415
Iteration 15/25 | Loss: 0.00078415
Iteration 16/25 | Loss: 0.00078415
Iteration 17/25 | Loss: 0.00078415
Iteration 18/25 | Loss: 0.00078415
Iteration 19/25 | Loss: 0.00078415
Iteration 20/25 | Loss: 0.00078415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007841464248485863, 0.0007841464248485863, 0.0007841464248485863, 0.0007841464248485863, 0.0007841464248485863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007841464248485863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078415
Iteration 2/1000 | Loss: 0.00038284
Iteration 3/1000 | Loss: 0.00018984
Iteration 4/1000 | Loss: 0.00073384
Iteration 5/1000 | Loss: 0.00003735
Iteration 6/1000 | Loss: 0.00010173
Iteration 7/1000 | Loss: 0.00003124
Iteration 8/1000 | Loss: 0.00002944
Iteration 9/1000 | Loss: 0.00002849
Iteration 10/1000 | Loss: 0.00002755
Iteration 11/1000 | Loss: 0.00002688
Iteration 12/1000 | Loss: 0.00028758
Iteration 13/1000 | Loss: 0.00051168
Iteration 14/1000 | Loss: 0.00008098
Iteration 15/1000 | Loss: 0.00004161
Iteration 16/1000 | Loss: 0.00015219
Iteration 17/1000 | Loss: 0.00011874
Iteration 18/1000 | Loss: 0.00007726
Iteration 19/1000 | Loss: 0.00003337
Iteration 20/1000 | Loss: 0.00002591
Iteration 21/1000 | Loss: 0.00002588
Iteration 22/1000 | Loss: 0.00002567
Iteration 23/1000 | Loss: 0.00002556
Iteration 24/1000 | Loss: 0.00002550
Iteration 25/1000 | Loss: 0.00059942
Iteration 26/1000 | Loss: 0.00090927
Iteration 27/1000 | Loss: 0.00004659
Iteration 28/1000 | Loss: 0.00098569
Iteration 29/1000 | Loss: 0.00003613
Iteration 30/1000 | Loss: 0.00003346
Iteration 31/1000 | Loss: 0.00012797
Iteration 32/1000 | Loss: 0.00002534
Iteration 33/1000 | Loss: 0.00004249
Iteration 34/1000 | Loss: 0.00003217
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002486
Iteration 37/1000 | Loss: 0.00002483
Iteration 38/1000 | Loss: 0.00002482
Iteration 39/1000 | Loss: 0.00002482
Iteration 40/1000 | Loss: 0.00002481
Iteration 41/1000 | Loss: 0.00002481
Iteration 42/1000 | Loss: 0.00002480
Iteration 43/1000 | Loss: 0.00002480
Iteration 44/1000 | Loss: 0.00002479
Iteration 45/1000 | Loss: 0.00002479
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002478
Iteration 49/1000 | Loss: 0.00002477
Iteration 50/1000 | Loss: 0.00002477
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002475
Iteration 53/1000 | Loss: 0.00002475
Iteration 54/1000 | Loss: 0.00002475
Iteration 55/1000 | Loss: 0.00002475
Iteration 56/1000 | Loss: 0.00002475
Iteration 57/1000 | Loss: 0.00002475
Iteration 58/1000 | Loss: 0.00002475
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002474
Iteration 61/1000 | Loss: 0.00002474
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002474
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002473
Iteration 68/1000 | Loss: 0.00002473
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002471
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002471
Iteration 77/1000 | Loss: 0.00002471
Iteration 78/1000 | Loss: 0.00002471
Iteration 79/1000 | Loss: 0.00002471
Iteration 80/1000 | Loss: 0.00002471
Iteration 81/1000 | Loss: 0.00002471
Iteration 82/1000 | Loss: 0.00002471
Iteration 83/1000 | Loss: 0.00002471
Iteration 84/1000 | Loss: 0.00002471
Iteration 85/1000 | Loss: 0.00002471
Iteration 86/1000 | Loss: 0.00002471
Iteration 87/1000 | Loss: 0.00002471
Iteration 88/1000 | Loss: 0.00002471
Iteration 89/1000 | Loss: 0.00002471
Iteration 90/1000 | Loss: 0.00002471
Iteration 91/1000 | Loss: 0.00002471
Iteration 92/1000 | Loss: 0.00002471
Iteration 93/1000 | Loss: 0.00002471
Iteration 94/1000 | Loss: 0.00002471
Iteration 95/1000 | Loss: 0.00002471
Iteration 96/1000 | Loss: 0.00002471
Iteration 97/1000 | Loss: 0.00002471
Iteration 98/1000 | Loss: 0.00002471
Iteration 99/1000 | Loss: 0.00002471
Iteration 100/1000 | Loss: 0.00002471
Iteration 101/1000 | Loss: 0.00002471
Iteration 102/1000 | Loss: 0.00002471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.4706690965103917e-05, 2.4706690965103917e-05, 2.4706690965103917e-05, 2.4706690965103917e-05, 2.4706690965103917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4706690965103917e-05

Optimization complete. Final v2v error: 4.2665910720825195 mm

Highest mean error: 4.853540420532227 mm for frame 44

Lowest mean error: 3.56937837600708 mm for frame 97

Saving results

Total time: 95.42801356315613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490457
Iteration 2/25 | Loss: 0.00142490
Iteration 3/25 | Loss: 0.00131951
Iteration 4/25 | Loss: 0.00130805
Iteration 5/25 | Loss: 0.00130494
Iteration 6/25 | Loss: 0.00130461
Iteration 7/25 | Loss: 0.00130461
Iteration 8/25 | Loss: 0.00130461
Iteration 9/25 | Loss: 0.00130461
Iteration 10/25 | Loss: 0.00130461
Iteration 11/25 | Loss: 0.00130461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013046059757471085, 0.0013046059757471085, 0.0013046059757471085, 0.0013046059757471085, 0.0013046059757471085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013046059757471085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36007011
Iteration 2/25 | Loss: 0.00102415
Iteration 3/25 | Loss: 0.00102414
Iteration 4/25 | Loss: 0.00102414
Iteration 5/25 | Loss: 0.00102414
Iteration 6/25 | Loss: 0.00102414
Iteration 7/25 | Loss: 0.00102414
Iteration 8/25 | Loss: 0.00102414
Iteration 9/25 | Loss: 0.00102414
Iteration 10/25 | Loss: 0.00102414
Iteration 11/25 | Loss: 0.00102414
Iteration 12/25 | Loss: 0.00102414
Iteration 13/25 | Loss: 0.00102414
Iteration 14/25 | Loss: 0.00102414
Iteration 15/25 | Loss: 0.00102414
Iteration 16/25 | Loss: 0.00102414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010241370182484388, 0.0010241370182484388, 0.0010241370182484388, 0.0010241370182484388, 0.0010241370182484388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010241370182484388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102414
Iteration 2/1000 | Loss: 0.00003996
Iteration 3/1000 | Loss: 0.00002730
Iteration 4/1000 | Loss: 0.00002492
Iteration 5/1000 | Loss: 0.00002353
Iteration 6/1000 | Loss: 0.00002272
Iteration 7/1000 | Loss: 0.00002216
Iteration 8/1000 | Loss: 0.00002175
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002144
Iteration 11/1000 | Loss: 0.00002133
Iteration 12/1000 | Loss: 0.00002116
Iteration 13/1000 | Loss: 0.00002112
Iteration 14/1000 | Loss: 0.00002105
Iteration 15/1000 | Loss: 0.00002102
Iteration 16/1000 | Loss: 0.00002101
Iteration 17/1000 | Loss: 0.00002100
Iteration 18/1000 | Loss: 0.00002097
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002086
Iteration 21/1000 | Loss: 0.00002085
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002081
Iteration 26/1000 | Loss: 0.00002080
Iteration 27/1000 | Loss: 0.00002080
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002077
Iteration 31/1000 | Loss: 0.00002076
Iteration 32/1000 | Loss: 0.00002076
Iteration 33/1000 | Loss: 0.00002075
Iteration 34/1000 | Loss: 0.00002075
Iteration 35/1000 | Loss: 0.00002074
Iteration 36/1000 | Loss: 0.00002074
Iteration 37/1000 | Loss: 0.00002073
Iteration 38/1000 | Loss: 0.00002073
Iteration 39/1000 | Loss: 0.00002072
Iteration 40/1000 | Loss: 0.00002072
Iteration 41/1000 | Loss: 0.00002072
Iteration 42/1000 | Loss: 0.00002071
Iteration 43/1000 | Loss: 0.00002071
Iteration 44/1000 | Loss: 0.00002071
Iteration 45/1000 | Loss: 0.00002071
Iteration 46/1000 | Loss: 0.00002071
Iteration 47/1000 | Loss: 0.00002069
Iteration 48/1000 | Loss: 0.00002068
Iteration 49/1000 | Loss: 0.00002068
Iteration 50/1000 | Loss: 0.00002068
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00002067
Iteration 53/1000 | Loss: 0.00002067
Iteration 54/1000 | Loss: 0.00002067
Iteration 55/1000 | Loss: 0.00002066
Iteration 56/1000 | Loss: 0.00002066
Iteration 57/1000 | Loss: 0.00002066
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002064
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002063
Iteration 62/1000 | Loss: 0.00002063
Iteration 63/1000 | Loss: 0.00002063
Iteration 64/1000 | Loss: 0.00002063
Iteration 65/1000 | Loss: 0.00002063
Iteration 66/1000 | Loss: 0.00002063
Iteration 67/1000 | Loss: 0.00002062
Iteration 68/1000 | Loss: 0.00002062
Iteration 69/1000 | Loss: 0.00002062
Iteration 70/1000 | Loss: 0.00002062
Iteration 71/1000 | Loss: 0.00002062
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002061
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002059
Iteration 82/1000 | Loss: 0.00002059
Iteration 83/1000 | Loss: 0.00002058
Iteration 84/1000 | Loss: 0.00002058
Iteration 85/1000 | Loss: 0.00002058
Iteration 86/1000 | Loss: 0.00002058
Iteration 87/1000 | Loss: 0.00002057
Iteration 88/1000 | Loss: 0.00002057
Iteration 89/1000 | Loss: 0.00002057
Iteration 90/1000 | Loss: 0.00002057
Iteration 91/1000 | Loss: 0.00002056
Iteration 92/1000 | Loss: 0.00002056
Iteration 93/1000 | Loss: 0.00002056
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002056
Iteration 96/1000 | Loss: 0.00002056
Iteration 97/1000 | Loss: 0.00002055
Iteration 98/1000 | Loss: 0.00002055
Iteration 99/1000 | Loss: 0.00002055
Iteration 100/1000 | Loss: 0.00002054
Iteration 101/1000 | Loss: 0.00002054
Iteration 102/1000 | Loss: 0.00002054
Iteration 103/1000 | Loss: 0.00002053
Iteration 104/1000 | Loss: 0.00002053
Iteration 105/1000 | Loss: 0.00002053
Iteration 106/1000 | Loss: 0.00002052
Iteration 107/1000 | Loss: 0.00002052
Iteration 108/1000 | Loss: 0.00002052
Iteration 109/1000 | Loss: 0.00002052
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00002051
Iteration 112/1000 | Loss: 0.00002051
Iteration 113/1000 | Loss: 0.00002051
Iteration 114/1000 | Loss: 0.00002051
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002050
Iteration 118/1000 | Loss: 0.00002050
Iteration 119/1000 | Loss: 0.00002050
Iteration 120/1000 | Loss: 0.00002050
Iteration 121/1000 | Loss: 0.00002049
Iteration 122/1000 | Loss: 0.00002049
Iteration 123/1000 | Loss: 0.00002049
Iteration 124/1000 | Loss: 0.00002049
Iteration 125/1000 | Loss: 0.00002049
Iteration 126/1000 | Loss: 0.00002049
Iteration 127/1000 | Loss: 0.00002048
Iteration 128/1000 | Loss: 0.00002048
Iteration 129/1000 | Loss: 0.00002048
Iteration 130/1000 | Loss: 0.00002048
Iteration 131/1000 | Loss: 0.00002048
Iteration 132/1000 | Loss: 0.00002048
Iteration 133/1000 | Loss: 0.00002048
Iteration 134/1000 | Loss: 0.00002048
Iteration 135/1000 | Loss: 0.00002048
Iteration 136/1000 | Loss: 0.00002047
Iteration 137/1000 | Loss: 0.00002047
Iteration 138/1000 | Loss: 0.00002047
Iteration 139/1000 | Loss: 0.00002047
Iteration 140/1000 | Loss: 0.00002047
Iteration 141/1000 | Loss: 0.00002047
Iteration 142/1000 | Loss: 0.00002047
Iteration 143/1000 | Loss: 0.00002046
Iteration 144/1000 | Loss: 0.00002046
Iteration 145/1000 | Loss: 0.00002046
Iteration 146/1000 | Loss: 0.00002046
Iteration 147/1000 | Loss: 0.00002046
Iteration 148/1000 | Loss: 0.00002046
Iteration 149/1000 | Loss: 0.00002045
Iteration 150/1000 | Loss: 0.00002045
Iteration 151/1000 | Loss: 0.00002045
Iteration 152/1000 | Loss: 0.00002045
Iteration 153/1000 | Loss: 0.00002045
Iteration 154/1000 | Loss: 0.00002045
Iteration 155/1000 | Loss: 0.00002045
Iteration 156/1000 | Loss: 0.00002045
Iteration 157/1000 | Loss: 0.00002044
Iteration 158/1000 | Loss: 0.00002044
Iteration 159/1000 | Loss: 0.00002044
Iteration 160/1000 | Loss: 0.00002044
Iteration 161/1000 | Loss: 0.00002044
Iteration 162/1000 | Loss: 0.00002044
Iteration 163/1000 | Loss: 0.00002044
Iteration 164/1000 | Loss: 0.00002044
Iteration 165/1000 | Loss: 0.00002044
Iteration 166/1000 | Loss: 0.00002043
Iteration 167/1000 | Loss: 0.00002043
Iteration 168/1000 | Loss: 0.00002043
Iteration 169/1000 | Loss: 0.00002043
Iteration 170/1000 | Loss: 0.00002043
Iteration 171/1000 | Loss: 0.00002043
Iteration 172/1000 | Loss: 0.00002043
Iteration 173/1000 | Loss: 0.00002043
Iteration 174/1000 | Loss: 0.00002043
Iteration 175/1000 | Loss: 0.00002043
Iteration 176/1000 | Loss: 0.00002043
Iteration 177/1000 | Loss: 0.00002043
Iteration 178/1000 | Loss: 0.00002042
Iteration 179/1000 | Loss: 0.00002042
Iteration 180/1000 | Loss: 0.00002042
Iteration 181/1000 | Loss: 0.00002042
Iteration 182/1000 | Loss: 0.00002042
Iteration 183/1000 | Loss: 0.00002042
Iteration 184/1000 | Loss: 0.00002042
Iteration 185/1000 | Loss: 0.00002042
Iteration 186/1000 | Loss: 0.00002042
Iteration 187/1000 | Loss: 0.00002042
Iteration 188/1000 | Loss: 0.00002042
Iteration 189/1000 | Loss: 0.00002042
Iteration 190/1000 | Loss: 0.00002042
Iteration 191/1000 | Loss: 0.00002042
Iteration 192/1000 | Loss: 0.00002041
Iteration 193/1000 | Loss: 0.00002041
Iteration 194/1000 | Loss: 0.00002041
Iteration 195/1000 | Loss: 0.00002041
Iteration 196/1000 | Loss: 0.00002041
Iteration 197/1000 | Loss: 0.00002041
Iteration 198/1000 | Loss: 0.00002041
Iteration 199/1000 | Loss: 0.00002041
Iteration 200/1000 | Loss: 0.00002041
Iteration 201/1000 | Loss: 0.00002041
Iteration 202/1000 | Loss: 0.00002041
Iteration 203/1000 | Loss: 0.00002041
Iteration 204/1000 | Loss: 0.00002041
Iteration 205/1000 | Loss: 0.00002041
Iteration 206/1000 | Loss: 0.00002041
Iteration 207/1000 | Loss: 0.00002041
Iteration 208/1000 | Loss: 0.00002040
Iteration 209/1000 | Loss: 0.00002040
Iteration 210/1000 | Loss: 0.00002040
Iteration 211/1000 | Loss: 0.00002040
Iteration 212/1000 | Loss: 0.00002040
Iteration 213/1000 | Loss: 0.00002040
Iteration 214/1000 | Loss: 0.00002040
Iteration 215/1000 | Loss: 0.00002040
Iteration 216/1000 | Loss: 0.00002040
Iteration 217/1000 | Loss: 0.00002040
Iteration 218/1000 | Loss: 0.00002040
Iteration 219/1000 | Loss: 0.00002040
Iteration 220/1000 | Loss: 0.00002040
Iteration 221/1000 | Loss: 0.00002040
Iteration 222/1000 | Loss: 0.00002040
Iteration 223/1000 | Loss: 0.00002040
Iteration 224/1000 | Loss: 0.00002040
Iteration 225/1000 | Loss: 0.00002040
Iteration 226/1000 | Loss: 0.00002040
Iteration 227/1000 | Loss: 0.00002040
Iteration 228/1000 | Loss: 0.00002040
Iteration 229/1000 | Loss: 0.00002040
Iteration 230/1000 | Loss: 0.00002040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.0397066691657528e-05, 2.0397066691657528e-05, 2.0397066691657528e-05, 2.0397066691657528e-05, 2.0397066691657528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0397066691657528e-05

Optimization complete. Final v2v error: 3.679527759552002 mm

Highest mean error: 4.2819013595581055 mm for frame 132

Lowest mean error: 3.1356518268585205 mm for frame 2

Saving results

Total time: 41.00047206878662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494446
Iteration 2/25 | Loss: 0.00133423
Iteration 3/25 | Loss: 0.00124451
Iteration 4/25 | Loss: 0.00122598
Iteration 5/25 | Loss: 0.00121966
Iteration 6/25 | Loss: 0.00121811
Iteration 7/25 | Loss: 0.00121792
Iteration 8/25 | Loss: 0.00121792
Iteration 9/25 | Loss: 0.00121792
Iteration 10/25 | Loss: 0.00121792
Iteration 11/25 | Loss: 0.00121792
Iteration 12/25 | Loss: 0.00121792
Iteration 13/25 | Loss: 0.00121792
Iteration 14/25 | Loss: 0.00121792
Iteration 15/25 | Loss: 0.00121792
Iteration 16/25 | Loss: 0.00121792
Iteration 17/25 | Loss: 0.00121792
Iteration 18/25 | Loss: 0.00121792
Iteration 19/25 | Loss: 0.00121792
Iteration 20/25 | Loss: 0.00121792
Iteration 21/25 | Loss: 0.00121792
Iteration 22/25 | Loss: 0.00121792
Iteration 23/25 | Loss: 0.00121792
Iteration 24/25 | Loss: 0.00121792
Iteration 25/25 | Loss: 0.00121792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.39489508
Iteration 2/25 | Loss: 0.00106226
Iteration 3/25 | Loss: 0.00106226
Iteration 4/25 | Loss: 0.00106226
Iteration 5/25 | Loss: 0.00106226
Iteration 6/25 | Loss: 0.00106226
Iteration 7/25 | Loss: 0.00106226
Iteration 8/25 | Loss: 0.00106226
Iteration 9/25 | Loss: 0.00106226
Iteration 10/25 | Loss: 0.00106226
Iteration 11/25 | Loss: 0.00106226
Iteration 12/25 | Loss: 0.00106226
Iteration 13/25 | Loss: 0.00106226
Iteration 14/25 | Loss: 0.00106226
Iteration 15/25 | Loss: 0.00106226
Iteration 16/25 | Loss: 0.00106226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010622574482113123, 0.0010622574482113123, 0.0010622574482113123, 0.0010622574482113123, 0.0010622574482113123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010622574482113123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106226
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00002653
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002049
Iteration 6/1000 | Loss: 0.00001972
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001877
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001783
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001724
Iteration 16/1000 | Loss: 0.00001711
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001707
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001702
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001689
Iteration 50/1000 | Loss: 0.00001688
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001687
Iteration 60/1000 | Loss: 0.00001687
Iteration 61/1000 | Loss: 0.00001687
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001679
Iteration 89/1000 | Loss: 0.00001679
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001676
Iteration 100/1000 | Loss: 0.00001676
Iteration 101/1000 | Loss: 0.00001676
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001676
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001675
Iteration 108/1000 | Loss: 0.00001675
Iteration 109/1000 | Loss: 0.00001675
Iteration 110/1000 | Loss: 0.00001674
Iteration 111/1000 | Loss: 0.00001674
Iteration 112/1000 | Loss: 0.00001674
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001673
Iteration 115/1000 | Loss: 0.00001673
Iteration 116/1000 | Loss: 0.00001673
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001672
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001671
Iteration 127/1000 | Loss: 0.00001671
Iteration 128/1000 | Loss: 0.00001671
Iteration 129/1000 | Loss: 0.00001671
Iteration 130/1000 | Loss: 0.00001671
Iteration 131/1000 | Loss: 0.00001671
Iteration 132/1000 | Loss: 0.00001671
Iteration 133/1000 | Loss: 0.00001671
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001669
Iteration 138/1000 | Loss: 0.00001669
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001669
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001668
Iteration 145/1000 | Loss: 0.00001668
Iteration 146/1000 | Loss: 0.00001668
Iteration 147/1000 | Loss: 0.00001668
Iteration 148/1000 | Loss: 0.00001668
Iteration 149/1000 | Loss: 0.00001668
Iteration 150/1000 | Loss: 0.00001667
Iteration 151/1000 | Loss: 0.00001667
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001667
Iteration 154/1000 | Loss: 0.00001666
Iteration 155/1000 | Loss: 0.00001666
Iteration 156/1000 | Loss: 0.00001666
Iteration 157/1000 | Loss: 0.00001666
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001665
Iteration 160/1000 | Loss: 0.00001665
Iteration 161/1000 | Loss: 0.00001665
Iteration 162/1000 | Loss: 0.00001665
Iteration 163/1000 | Loss: 0.00001665
Iteration 164/1000 | Loss: 0.00001665
Iteration 165/1000 | Loss: 0.00001665
Iteration 166/1000 | Loss: 0.00001665
Iteration 167/1000 | Loss: 0.00001665
Iteration 168/1000 | Loss: 0.00001665
Iteration 169/1000 | Loss: 0.00001665
Iteration 170/1000 | Loss: 0.00001665
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001664
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001664
Iteration 176/1000 | Loss: 0.00001664
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001664
Iteration 180/1000 | Loss: 0.00001664
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001664
Iteration 184/1000 | Loss: 0.00001664
Iteration 185/1000 | Loss: 0.00001664
Iteration 186/1000 | Loss: 0.00001664
Iteration 187/1000 | Loss: 0.00001664
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001664
Iteration 190/1000 | Loss: 0.00001664
Iteration 191/1000 | Loss: 0.00001664
Iteration 192/1000 | Loss: 0.00001664
Iteration 193/1000 | Loss: 0.00001664
Iteration 194/1000 | Loss: 0.00001664
Iteration 195/1000 | Loss: 0.00001664
Iteration 196/1000 | Loss: 0.00001664
Iteration 197/1000 | Loss: 0.00001664
Iteration 198/1000 | Loss: 0.00001664
Iteration 199/1000 | Loss: 0.00001664
Iteration 200/1000 | Loss: 0.00001664
Iteration 201/1000 | Loss: 0.00001664
Iteration 202/1000 | Loss: 0.00001664
Iteration 203/1000 | Loss: 0.00001664
Iteration 204/1000 | Loss: 0.00001664
Iteration 205/1000 | Loss: 0.00001664
Iteration 206/1000 | Loss: 0.00001664
Iteration 207/1000 | Loss: 0.00001664
Iteration 208/1000 | Loss: 0.00001664
Iteration 209/1000 | Loss: 0.00001664
Iteration 210/1000 | Loss: 0.00001664
Iteration 211/1000 | Loss: 0.00001664
Iteration 212/1000 | Loss: 0.00001664
Iteration 213/1000 | Loss: 0.00001664
Iteration 214/1000 | Loss: 0.00001664
Iteration 215/1000 | Loss: 0.00001664
Iteration 216/1000 | Loss: 0.00001664
Iteration 217/1000 | Loss: 0.00001664
Iteration 218/1000 | Loss: 0.00001664
Iteration 219/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.663986586208921e-05, 1.663986586208921e-05, 1.663986586208921e-05, 1.663986586208921e-05, 1.663986586208921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.663986586208921e-05

Optimization complete. Final v2v error: 3.4406490325927734 mm

Highest mean error: 4.102329730987549 mm for frame 36

Lowest mean error: 2.9447708129882812 mm for frame 132

Saving results

Total time: 43.52031445503235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790564
Iteration 2/25 | Loss: 0.00192028
Iteration 3/25 | Loss: 0.00151841
Iteration 4/25 | Loss: 0.00154772
Iteration 5/25 | Loss: 0.00149245
Iteration 6/25 | Loss: 0.00140114
Iteration 7/25 | Loss: 0.00137612
Iteration 8/25 | Loss: 0.00135425
Iteration 9/25 | Loss: 0.00136491
Iteration 10/25 | Loss: 0.00134849
Iteration 11/25 | Loss: 0.00133781
Iteration 12/25 | Loss: 0.00133339
Iteration 13/25 | Loss: 0.00132759
Iteration 14/25 | Loss: 0.00132565
Iteration 15/25 | Loss: 0.00132467
Iteration 16/25 | Loss: 0.00132405
Iteration 17/25 | Loss: 0.00132364
Iteration 18/25 | Loss: 0.00132200
Iteration 19/25 | Loss: 0.00132139
Iteration 20/25 | Loss: 0.00132096
Iteration 21/25 | Loss: 0.00132057
Iteration 22/25 | Loss: 0.00132058
Iteration 23/25 | Loss: 0.00131981
Iteration 24/25 | Loss: 0.00131847
Iteration 25/25 | Loss: 0.00131814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36253476
Iteration 2/25 | Loss: 0.00133024
Iteration 3/25 | Loss: 0.00133023
Iteration 4/25 | Loss: 0.00133023
Iteration 5/25 | Loss: 0.00133023
Iteration 6/25 | Loss: 0.00133022
Iteration 7/25 | Loss: 0.00133022
Iteration 8/25 | Loss: 0.00133022
Iteration 9/25 | Loss: 0.00133022
Iteration 10/25 | Loss: 0.00133022
Iteration 11/25 | Loss: 0.00133022
Iteration 12/25 | Loss: 0.00133022
Iteration 13/25 | Loss: 0.00133022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001330223516561091, 0.001330223516561091, 0.001330223516561091, 0.001330223516561091, 0.001330223516561091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001330223516561091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133022
Iteration 2/1000 | Loss: 0.00010011
Iteration 3/1000 | Loss: 0.00056941
Iteration 4/1000 | Loss: 0.00242989
Iteration 5/1000 | Loss: 0.00008323
Iteration 6/1000 | Loss: 0.00004883
Iteration 7/1000 | Loss: 0.00003647
Iteration 8/1000 | Loss: 0.00003150
Iteration 9/1000 | Loss: 0.00002896
Iteration 10/1000 | Loss: 0.00002743
Iteration 11/1000 | Loss: 0.00002665
Iteration 12/1000 | Loss: 0.00002597
Iteration 13/1000 | Loss: 0.00002543
Iteration 14/1000 | Loss: 0.00002499
Iteration 15/1000 | Loss: 0.00002469
Iteration 16/1000 | Loss: 0.00002444
Iteration 17/1000 | Loss: 0.00002424
Iteration 18/1000 | Loss: 0.00002410
Iteration 19/1000 | Loss: 0.00002406
Iteration 20/1000 | Loss: 0.00002394
Iteration 21/1000 | Loss: 0.00002388
Iteration 22/1000 | Loss: 0.00002388
Iteration 23/1000 | Loss: 0.00002387
Iteration 24/1000 | Loss: 0.00002387
Iteration 25/1000 | Loss: 0.00002386
Iteration 26/1000 | Loss: 0.00002384
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002378
Iteration 29/1000 | Loss: 0.00002378
Iteration 30/1000 | Loss: 0.00002377
Iteration 31/1000 | Loss: 0.00002376
Iteration 32/1000 | Loss: 0.00002375
Iteration 33/1000 | Loss: 0.00002375
Iteration 34/1000 | Loss: 0.00002374
Iteration 35/1000 | Loss: 0.00002373
Iteration 36/1000 | Loss: 0.00002373
Iteration 37/1000 | Loss: 0.00002367
Iteration 38/1000 | Loss: 0.00002367
Iteration 39/1000 | Loss: 0.00002365
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002360
Iteration 42/1000 | Loss: 0.00002356
Iteration 43/1000 | Loss: 0.00002356
Iteration 44/1000 | Loss: 0.00002356
Iteration 45/1000 | Loss: 0.00002356
Iteration 46/1000 | Loss: 0.00002355
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002354
Iteration 50/1000 | Loss: 0.00002353
Iteration 51/1000 | Loss: 0.00002353
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002352
Iteration 54/1000 | Loss: 0.00002352
Iteration 55/1000 | Loss: 0.00002352
Iteration 56/1000 | Loss: 0.00002352
Iteration 57/1000 | Loss: 0.00002351
Iteration 58/1000 | Loss: 0.00002351
Iteration 59/1000 | Loss: 0.00002351
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002350
Iteration 64/1000 | Loss: 0.00002350
Iteration 65/1000 | Loss: 0.00002350
Iteration 66/1000 | Loss: 0.00002350
Iteration 67/1000 | Loss: 0.00002349
Iteration 68/1000 | Loss: 0.00002349
Iteration 69/1000 | Loss: 0.00002348
Iteration 70/1000 | Loss: 0.00002348
Iteration 71/1000 | Loss: 0.00002348
Iteration 72/1000 | Loss: 0.00002348
Iteration 73/1000 | Loss: 0.00002347
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002346
Iteration 77/1000 | Loss: 0.00002346
Iteration 78/1000 | Loss: 0.00002346
Iteration 79/1000 | Loss: 0.00002346
Iteration 80/1000 | Loss: 0.00002346
Iteration 81/1000 | Loss: 0.00002346
Iteration 82/1000 | Loss: 0.00002345
Iteration 83/1000 | Loss: 0.00002345
Iteration 84/1000 | Loss: 0.00002345
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002342
Iteration 89/1000 | Loss: 0.00002342
Iteration 90/1000 | Loss: 0.00002342
Iteration 91/1000 | Loss: 0.00002342
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002340
Iteration 97/1000 | Loss: 0.00002340
Iteration 98/1000 | Loss: 0.00002340
Iteration 99/1000 | Loss: 0.00002340
Iteration 100/1000 | Loss: 0.00002340
Iteration 101/1000 | Loss: 0.00002340
Iteration 102/1000 | Loss: 0.00002340
Iteration 103/1000 | Loss: 0.00002340
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002340
Iteration 107/1000 | Loss: 0.00002340
Iteration 108/1000 | Loss: 0.00002339
Iteration 109/1000 | Loss: 0.00002339
Iteration 110/1000 | Loss: 0.00002339
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002339
Iteration 113/1000 | Loss: 0.00002338
Iteration 114/1000 | Loss: 0.00002338
Iteration 115/1000 | Loss: 0.00002338
Iteration 116/1000 | Loss: 0.00002338
Iteration 117/1000 | Loss: 0.00002338
Iteration 118/1000 | Loss: 0.00002338
Iteration 119/1000 | Loss: 0.00002338
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002338
Iteration 122/1000 | Loss: 0.00002338
Iteration 123/1000 | Loss: 0.00002338
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002337
Iteration 127/1000 | Loss: 0.00002337
Iteration 128/1000 | Loss: 0.00002337
Iteration 129/1000 | Loss: 0.00002337
Iteration 130/1000 | Loss: 0.00002337
Iteration 131/1000 | Loss: 0.00002337
Iteration 132/1000 | Loss: 0.00002337
Iteration 133/1000 | Loss: 0.00002337
Iteration 134/1000 | Loss: 0.00002337
Iteration 135/1000 | Loss: 0.00002337
Iteration 136/1000 | Loss: 0.00002337
Iteration 137/1000 | Loss: 0.00002337
Iteration 138/1000 | Loss: 0.00002337
Iteration 139/1000 | Loss: 0.00002337
Iteration 140/1000 | Loss: 0.00002336
Iteration 141/1000 | Loss: 0.00002336
Iteration 142/1000 | Loss: 0.00002336
Iteration 143/1000 | Loss: 0.00002336
Iteration 144/1000 | Loss: 0.00002336
Iteration 145/1000 | Loss: 0.00002336
Iteration 146/1000 | Loss: 0.00002336
Iteration 147/1000 | Loss: 0.00002336
Iteration 148/1000 | Loss: 0.00002336
Iteration 149/1000 | Loss: 0.00002336
Iteration 150/1000 | Loss: 0.00002336
Iteration 151/1000 | Loss: 0.00002336
Iteration 152/1000 | Loss: 0.00002336
Iteration 153/1000 | Loss: 0.00002336
Iteration 154/1000 | Loss: 0.00002336
Iteration 155/1000 | Loss: 0.00002336
Iteration 156/1000 | Loss: 0.00002335
Iteration 157/1000 | Loss: 0.00002335
Iteration 158/1000 | Loss: 0.00002335
Iteration 159/1000 | Loss: 0.00002335
Iteration 160/1000 | Loss: 0.00002335
Iteration 161/1000 | Loss: 0.00002335
Iteration 162/1000 | Loss: 0.00002335
Iteration 163/1000 | Loss: 0.00002335
Iteration 164/1000 | Loss: 0.00002335
Iteration 165/1000 | Loss: 0.00002335
Iteration 166/1000 | Loss: 0.00002335
Iteration 167/1000 | Loss: 0.00002335
Iteration 168/1000 | Loss: 0.00002335
Iteration 169/1000 | Loss: 0.00002335
Iteration 170/1000 | Loss: 0.00002335
Iteration 171/1000 | Loss: 0.00002334
Iteration 172/1000 | Loss: 0.00002334
Iteration 173/1000 | Loss: 0.00002334
Iteration 174/1000 | Loss: 0.00002334
Iteration 175/1000 | Loss: 0.00002334
Iteration 176/1000 | Loss: 0.00002334
Iteration 177/1000 | Loss: 0.00002334
Iteration 178/1000 | Loss: 0.00002334
Iteration 179/1000 | Loss: 0.00002334
Iteration 180/1000 | Loss: 0.00002334
Iteration 181/1000 | Loss: 0.00002334
Iteration 182/1000 | Loss: 0.00002334
Iteration 183/1000 | Loss: 0.00002334
Iteration 184/1000 | Loss: 0.00002334
Iteration 185/1000 | Loss: 0.00002334
Iteration 186/1000 | Loss: 0.00002334
Iteration 187/1000 | Loss: 0.00002334
Iteration 188/1000 | Loss: 0.00002334
Iteration 189/1000 | Loss: 0.00002334
Iteration 190/1000 | Loss: 0.00002334
Iteration 191/1000 | Loss: 0.00002334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.334055352548603e-05, 2.334055352548603e-05, 2.334055352548603e-05, 2.334055352548603e-05, 2.334055352548603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.334055352548603e-05

Optimization complete. Final v2v error: 4.031010627746582 mm

Highest mean error: 4.6302313804626465 mm for frame 212

Lowest mean error: 3.4408419132232666 mm for frame 239

Saving results

Total time: 95.53618454933167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070628
Iteration 2/25 | Loss: 0.00210376
Iteration 3/25 | Loss: 0.00153903
Iteration 4/25 | Loss: 0.00148814
Iteration 5/25 | Loss: 0.00147053
Iteration 6/25 | Loss: 0.00146096
Iteration 7/25 | Loss: 0.00142686
Iteration 8/25 | Loss: 0.00141243
Iteration 9/25 | Loss: 0.00141223
Iteration 10/25 | Loss: 0.00139187
Iteration 11/25 | Loss: 0.00137844
Iteration 12/25 | Loss: 0.00137472
Iteration 13/25 | Loss: 0.00138170
Iteration 14/25 | Loss: 0.00137438
Iteration 15/25 | Loss: 0.00137174
Iteration 16/25 | Loss: 0.00137177
Iteration 17/25 | Loss: 0.00137119
Iteration 18/25 | Loss: 0.00136972
Iteration 19/25 | Loss: 0.00136939
Iteration 20/25 | Loss: 0.00136914
Iteration 21/25 | Loss: 0.00136893
Iteration 22/25 | Loss: 0.00136888
Iteration 23/25 | Loss: 0.00136888
Iteration 24/25 | Loss: 0.00136888
Iteration 25/25 | Loss: 0.00136888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66943967
Iteration 2/25 | Loss: 0.00093463
Iteration 3/25 | Loss: 0.00093460
Iteration 4/25 | Loss: 0.00093460
Iteration 5/25 | Loss: 0.00093460
Iteration 6/25 | Loss: 0.00093460
Iteration 7/25 | Loss: 0.00093460
Iteration 8/25 | Loss: 0.00093460
Iteration 9/25 | Loss: 0.00093460
Iteration 10/25 | Loss: 0.00093460
Iteration 11/25 | Loss: 0.00093460
Iteration 12/25 | Loss: 0.00093460
Iteration 13/25 | Loss: 0.00093460
Iteration 14/25 | Loss: 0.00093460
Iteration 15/25 | Loss: 0.00093460
Iteration 16/25 | Loss: 0.00093460
Iteration 17/25 | Loss: 0.00093459
Iteration 18/25 | Loss: 0.00093459
Iteration 19/25 | Loss: 0.00093459
Iteration 20/25 | Loss: 0.00093459
Iteration 21/25 | Loss: 0.00093459
Iteration 22/25 | Loss: 0.00093459
Iteration 23/25 | Loss: 0.00093459
Iteration 24/25 | Loss: 0.00093459
Iteration 25/25 | Loss: 0.00093459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093459
Iteration 2/1000 | Loss: 0.00006855
Iteration 3/1000 | Loss: 0.00011761
Iteration 4/1000 | Loss: 0.00004350
Iteration 5/1000 | Loss: 0.00008966
Iteration 6/1000 | Loss: 0.00003922
Iteration 7/1000 | Loss: 0.00003826
Iteration 8/1000 | Loss: 0.00012711
Iteration 9/1000 | Loss: 0.00003662
Iteration 10/1000 | Loss: 0.00003587
Iteration 11/1000 | Loss: 0.00010948
Iteration 12/1000 | Loss: 0.00003797
Iteration 13/1000 | Loss: 0.00003623
Iteration 14/1000 | Loss: 0.00003496
Iteration 15/1000 | Loss: 0.00012240
Iteration 16/1000 | Loss: 0.00003690
Iteration 17/1000 | Loss: 0.00003473
Iteration 18/1000 | Loss: 0.00003398
Iteration 19/1000 | Loss: 0.00003378
Iteration 20/1000 | Loss: 0.00003352
Iteration 21/1000 | Loss: 0.00003333
Iteration 22/1000 | Loss: 0.00003318
Iteration 23/1000 | Loss: 0.00003317
Iteration 24/1000 | Loss: 0.00003305
Iteration 25/1000 | Loss: 0.00003294
Iteration 26/1000 | Loss: 0.00003292
Iteration 27/1000 | Loss: 0.00003291
Iteration 28/1000 | Loss: 0.00003291
Iteration 29/1000 | Loss: 0.00003291
Iteration 30/1000 | Loss: 0.00003291
Iteration 31/1000 | Loss: 0.00003291
Iteration 32/1000 | Loss: 0.00003291
Iteration 33/1000 | Loss: 0.00003291
Iteration 34/1000 | Loss: 0.00003291
Iteration 35/1000 | Loss: 0.00003291
Iteration 36/1000 | Loss: 0.00003290
Iteration 37/1000 | Loss: 0.00003290
Iteration 38/1000 | Loss: 0.00003290
Iteration 39/1000 | Loss: 0.00003289
Iteration 40/1000 | Loss: 0.00003284
Iteration 41/1000 | Loss: 0.00003284
Iteration 42/1000 | Loss: 0.00003284
Iteration 43/1000 | Loss: 0.00003274
Iteration 44/1000 | Loss: 0.00003271
Iteration 45/1000 | Loss: 0.00003269
Iteration 46/1000 | Loss: 0.00003269
Iteration 47/1000 | Loss: 0.00003265
Iteration 48/1000 | Loss: 0.00003265
Iteration 49/1000 | Loss: 0.00003261
Iteration 50/1000 | Loss: 0.00007989
Iteration 51/1000 | Loss: 0.00003279
Iteration 52/1000 | Loss: 0.00003253
Iteration 53/1000 | Loss: 0.00003253
Iteration 54/1000 | Loss: 0.00003253
Iteration 55/1000 | Loss: 0.00003253
Iteration 56/1000 | Loss: 0.00003253
Iteration 57/1000 | Loss: 0.00003252
Iteration 58/1000 | Loss: 0.00003252
Iteration 59/1000 | Loss: 0.00003252
Iteration 60/1000 | Loss: 0.00003252
Iteration 61/1000 | Loss: 0.00003252
Iteration 62/1000 | Loss: 0.00003252
Iteration 63/1000 | Loss: 0.00003252
Iteration 64/1000 | Loss: 0.00003252
Iteration 65/1000 | Loss: 0.00003252
Iteration 66/1000 | Loss: 0.00003252
Iteration 67/1000 | Loss: 0.00003251
Iteration 68/1000 | Loss: 0.00003251
Iteration 69/1000 | Loss: 0.00003250
Iteration 70/1000 | Loss: 0.00003250
Iteration 71/1000 | Loss: 0.00003250
Iteration 72/1000 | Loss: 0.00003250
Iteration 73/1000 | Loss: 0.00003250
Iteration 74/1000 | Loss: 0.00003250
Iteration 75/1000 | Loss: 0.00003250
Iteration 76/1000 | Loss: 0.00003250
Iteration 77/1000 | Loss: 0.00003250
Iteration 78/1000 | Loss: 0.00003250
Iteration 79/1000 | Loss: 0.00003249
Iteration 80/1000 | Loss: 0.00003248
Iteration 81/1000 | Loss: 0.00003248
Iteration 82/1000 | Loss: 0.00003248
Iteration 83/1000 | Loss: 0.00003247
Iteration 84/1000 | Loss: 0.00003247
Iteration 85/1000 | Loss: 0.00003247
Iteration 86/1000 | Loss: 0.00003247
Iteration 87/1000 | Loss: 0.00003247
Iteration 88/1000 | Loss: 0.00003247
Iteration 89/1000 | Loss: 0.00003246
Iteration 90/1000 | Loss: 0.00007354
Iteration 91/1000 | Loss: 0.00003432
Iteration 92/1000 | Loss: 0.00004993
Iteration 93/1000 | Loss: 0.00003861
Iteration 94/1000 | Loss: 0.00003618
Iteration 95/1000 | Loss: 0.00003244
Iteration 96/1000 | Loss: 0.00003244
Iteration 97/1000 | Loss: 0.00003243
Iteration 98/1000 | Loss: 0.00003243
Iteration 99/1000 | Loss: 0.00003243
Iteration 100/1000 | Loss: 0.00003243
Iteration 101/1000 | Loss: 0.00003243
Iteration 102/1000 | Loss: 0.00003243
Iteration 103/1000 | Loss: 0.00003243
Iteration 104/1000 | Loss: 0.00003243
Iteration 105/1000 | Loss: 0.00003243
Iteration 106/1000 | Loss: 0.00003243
Iteration 107/1000 | Loss: 0.00003243
Iteration 108/1000 | Loss: 0.00003243
Iteration 109/1000 | Loss: 0.00003243
Iteration 110/1000 | Loss: 0.00003243
Iteration 111/1000 | Loss: 0.00003243
Iteration 112/1000 | Loss: 0.00003242
Iteration 113/1000 | Loss: 0.00003242
Iteration 114/1000 | Loss: 0.00003242
Iteration 115/1000 | Loss: 0.00003241
Iteration 116/1000 | Loss: 0.00003241
Iteration 117/1000 | Loss: 0.00003241
Iteration 118/1000 | Loss: 0.00003241
Iteration 119/1000 | Loss: 0.00003241
Iteration 120/1000 | Loss: 0.00003240
Iteration 121/1000 | Loss: 0.00003240
Iteration 122/1000 | Loss: 0.00003240
Iteration 123/1000 | Loss: 0.00003239
Iteration 124/1000 | Loss: 0.00003239
Iteration 125/1000 | Loss: 0.00003238
Iteration 126/1000 | Loss: 0.00003234
Iteration 127/1000 | Loss: 0.00003234
Iteration 128/1000 | Loss: 0.00003234
Iteration 129/1000 | Loss: 0.00003234
Iteration 130/1000 | Loss: 0.00003234
Iteration 131/1000 | Loss: 0.00003234
Iteration 132/1000 | Loss: 0.00003234
Iteration 133/1000 | Loss: 0.00003234
Iteration 134/1000 | Loss: 0.00003234
Iteration 135/1000 | Loss: 0.00003234
Iteration 136/1000 | Loss: 0.00003233
Iteration 137/1000 | Loss: 0.00003233
Iteration 138/1000 | Loss: 0.00003233
Iteration 139/1000 | Loss: 0.00003233
Iteration 140/1000 | Loss: 0.00003233
Iteration 141/1000 | Loss: 0.00003232
Iteration 142/1000 | Loss: 0.00003232
Iteration 143/1000 | Loss: 0.00003232
Iteration 144/1000 | Loss: 0.00003232
Iteration 145/1000 | Loss: 0.00003232
Iteration 146/1000 | Loss: 0.00003232
Iteration 147/1000 | Loss: 0.00003232
Iteration 148/1000 | Loss: 0.00003232
Iteration 149/1000 | Loss: 0.00003231
Iteration 150/1000 | Loss: 0.00003231
Iteration 151/1000 | Loss: 0.00003231
Iteration 152/1000 | Loss: 0.00003231
Iteration 153/1000 | Loss: 0.00003231
Iteration 154/1000 | Loss: 0.00003231
Iteration 155/1000 | Loss: 0.00003231
Iteration 156/1000 | Loss: 0.00003231
Iteration 157/1000 | Loss: 0.00003230
Iteration 158/1000 | Loss: 0.00003230
Iteration 159/1000 | Loss: 0.00003230
Iteration 160/1000 | Loss: 0.00003230
Iteration 161/1000 | Loss: 0.00003230
Iteration 162/1000 | Loss: 0.00003230
Iteration 163/1000 | Loss: 0.00003230
Iteration 164/1000 | Loss: 0.00003230
Iteration 165/1000 | Loss: 0.00003230
Iteration 166/1000 | Loss: 0.00003230
Iteration 167/1000 | Loss: 0.00003230
Iteration 168/1000 | Loss: 0.00003230
Iteration 169/1000 | Loss: 0.00003230
Iteration 170/1000 | Loss: 0.00003230
Iteration 171/1000 | Loss: 0.00003229
Iteration 172/1000 | Loss: 0.00003229
Iteration 173/1000 | Loss: 0.00003229
Iteration 174/1000 | Loss: 0.00003229
Iteration 175/1000 | Loss: 0.00003229
Iteration 176/1000 | Loss: 0.00003229
Iteration 177/1000 | Loss: 0.00003229
Iteration 178/1000 | Loss: 0.00003229
Iteration 179/1000 | Loss: 0.00003229
Iteration 180/1000 | Loss: 0.00003229
Iteration 181/1000 | Loss: 0.00003228
Iteration 182/1000 | Loss: 0.00003228
Iteration 183/1000 | Loss: 0.00003228
Iteration 184/1000 | Loss: 0.00003228
Iteration 185/1000 | Loss: 0.00003228
Iteration 186/1000 | Loss: 0.00003228
Iteration 187/1000 | Loss: 0.00003228
Iteration 188/1000 | Loss: 0.00003228
Iteration 189/1000 | Loss: 0.00003228
Iteration 190/1000 | Loss: 0.00003228
Iteration 191/1000 | Loss: 0.00003228
Iteration 192/1000 | Loss: 0.00003228
Iteration 193/1000 | Loss: 0.00003228
Iteration 194/1000 | Loss: 0.00003228
Iteration 195/1000 | Loss: 0.00003228
Iteration 196/1000 | Loss: 0.00003227
Iteration 197/1000 | Loss: 0.00003227
Iteration 198/1000 | Loss: 0.00003227
Iteration 199/1000 | Loss: 0.00003227
Iteration 200/1000 | Loss: 0.00003227
Iteration 201/1000 | Loss: 0.00003227
Iteration 202/1000 | Loss: 0.00003227
Iteration 203/1000 | Loss: 0.00003227
Iteration 204/1000 | Loss: 0.00003227
Iteration 205/1000 | Loss: 0.00003227
Iteration 206/1000 | Loss: 0.00003227
Iteration 207/1000 | Loss: 0.00003227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [3.227305205655284e-05, 3.227305205655284e-05, 3.227305205655284e-05, 3.227305205655284e-05, 3.227305205655284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.227305205655284e-05

Optimization complete. Final v2v error: 4.659494876861572 mm

Highest mean error: 5.659998416900635 mm for frame 171

Lowest mean error: 3.5985257625579834 mm for frame 204

Saving results

Total time: 108.48246264457703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735658
Iteration 2/25 | Loss: 0.00153934
Iteration 3/25 | Loss: 0.00133861
Iteration 4/25 | Loss: 0.00132170
Iteration 5/25 | Loss: 0.00131943
Iteration 6/25 | Loss: 0.00131943
Iteration 7/25 | Loss: 0.00131943
Iteration 8/25 | Loss: 0.00131943
Iteration 9/25 | Loss: 0.00131943
Iteration 10/25 | Loss: 0.00131943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013194320490583777, 0.0013194320490583777, 0.0013194320490583777, 0.0013194320490583777, 0.0013194320490583777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013194320490583777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57241678
Iteration 2/25 | Loss: 0.00104763
Iteration 3/25 | Loss: 0.00104763
Iteration 4/25 | Loss: 0.00104763
Iteration 5/25 | Loss: 0.00104763
Iteration 6/25 | Loss: 0.00104763
Iteration 7/25 | Loss: 0.00104763
Iteration 8/25 | Loss: 0.00104763
Iteration 9/25 | Loss: 0.00104763
Iteration 10/25 | Loss: 0.00104763
Iteration 11/25 | Loss: 0.00104763
Iteration 12/25 | Loss: 0.00104763
Iteration 13/25 | Loss: 0.00104763
Iteration 14/25 | Loss: 0.00104763
Iteration 15/25 | Loss: 0.00104763
Iteration 16/25 | Loss: 0.00104763
Iteration 17/25 | Loss: 0.00104763
Iteration 18/25 | Loss: 0.00104763
Iteration 19/25 | Loss: 0.00104763
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010476275347173214, 0.0010476275347173214, 0.0010476275347173214, 0.0010476275347173214, 0.0010476275347173214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010476275347173214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104763
Iteration 2/1000 | Loss: 0.00003375
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002275
Iteration 5/1000 | Loss: 0.00002162
Iteration 6/1000 | Loss: 0.00002098
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00002041
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001948
Iteration 12/1000 | Loss: 0.00001935
Iteration 13/1000 | Loss: 0.00001929
Iteration 14/1000 | Loss: 0.00001918
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001911
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001878
Iteration 22/1000 | Loss: 0.00001873
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001864
Iteration 32/1000 | Loss: 0.00001864
Iteration 33/1000 | Loss: 0.00001859
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001857
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001856
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001854
Iteration 43/1000 | Loss: 0.00001853
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001851
Iteration 49/1000 | Loss: 0.00001851
Iteration 50/1000 | Loss: 0.00001850
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001849
Iteration 53/1000 | Loss: 0.00001849
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001846
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001840
Iteration 70/1000 | Loss: 0.00001840
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001839
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001838
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001837
Iteration 77/1000 | Loss: 0.00001837
Iteration 78/1000 | Loss: 0.00001837
Iteration 79/1000 | Loss: 0.00001836
Iteration 80/1000 | Loss: 0.00001836
Iteration 81/1000 | Loss: 0.00001836
Iteration 82/1000 | Loss: 0.00001835
Iteration 83/1000 | Loss: 0.00001835
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001834
Iteration 86/1000 | Loss: 0.00001834
Iteration 87/1000 | Loss: 0.00001834
Iteration 88/1000 | Loss: 0.00001833
Iteration 89/1000 | Loss: 0.00001833
Iteration 90/1000 | Loss: 0.00001832
Iteration 91/1000 | Loss: 0.00001832
Iteration 92/1000 | Loss: 0.00001832
Iteration 93/1000 | Loss: 0.00001832
Iteration 94/1000 | Loss: 0.00001831
Iteration 95/1000 | Loss: 0.00001831
Iteration 96/1000 | Loss: 0.00001831
Iteration 97/1000 | Loss: 0.00001830
Iteration 98/1000 | Loss: 0.00001830
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001825
Iteration 112/1000 | Loss: 0.00001825
Iteration 113/1000 | Loss: 0.00001825
Iteration 114/1000 | Loss: 0.00001825
Iteration 115/1000 | Loss: 0.00001825
Iteration 116/1000 | Loss: 0.00001825
Iteration 117/1000 | Loss: 0.00001824
Iteration 118/1000 | Loss: 0.00001824
Iteration 119/1000 | Loss: 0.00001824
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Iteration 123/1000 | Loss: 0.00001824
Iteration 124/1000 | Loss: 0.00001824
Iteration 125/1000 | Loss: 0.00001824
Iteration 126/1000 | Loss: 0.00001824
Iteration 127/1000 | Loss: 0.00001824
Iteration 128/1000 | Loss: 0.00001824
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001822
Iteration 135/1000 | Loss: 0.00001822
Iteration 136/1000 | Loss: 0.00001822
Iteration 137/1000 | Loss: 0.00001822
Iteration 138/1000 | Loss: 0.00001822
Iteration 139/1000 | Loss: 0.00001822
Iteration 140/1000 | Loss: 0.00001822
Iteration 141/1000 | Loss: 0.00001822
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001822
Iteration 144/1000 | Loss: 0.00001822
Iteration 145/1000 | Loss: 0.00001821
Iteration 146/1000 | Loss: 0.00001821
Iteration 147/1000 | Loss: 0.00001821
Iteration 148/1000 | Loss: 0.00001821
Iteration 149/1000 | Loss: 0.00001821
Iteration 150/1000 | Loss: 0.00001821
Iteration 151/1000 | Loss: 0.00001821
Iteration 152/1000 | Loss: 0.00001821
Iteration 153/1000 | Loss: 0.00001821
Iteration 154/1000 | Loss: 0.00001821
Iteration 155/1000 | Loss: 0.00001821
Iteration 156/1000 | Loss: 0.00001820
Iteration 157/1000 | Loss: 0.00001820
Iteration 158/1000 | Loss: 0.00001820
Iteration 159/1000 | Loss: 0.00001820
Iteration 160/1000 | Loss: 0.00001820
Iteration 161/1000 | Loss: 0.00001820
Iteration 162/1000 | Loss: 0.00001820
Iteration 163/1000 | Loss: 0.00001820
Iteration 164/1000 | Loss: 0.00001820
Iteration 165/1000 | Loss: 0.00001820
Iteration 166/1000 | Loss: 0.00001820
Iteration 167/1000 | Loss: 0.00001819
Iteration 168/1000 | Loss: 0.00001819
Iteration 169/1000 | Loss: 0.00001819
Iteration 170/1000 | Loss: 0.00001819
Iteration 171/1000 | Loss: 0.00001819
Iteration 172/1000 | Loss: 0.00001819
Iteration 173/1000 | Loss: 0.00001819
Iteration 174/1000 | Loss: 0.00001819
Iteration 175/1000 | Loss: 0.00001819
Iteration 176/1000 | Loss: 0.00001819
Iteration 177/1000 | Loss: 0.00001819
Iteration 178/1000 | Loss: 0.00001819
Iteration 179/1000 | Loss: 0.00001819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.8187416571890935e-05, 1.8187416571890935e-05, 1.8187416571890935e-05, 1.8187416571890935e-05, 1.8187416571890935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8187416571890935e-05

Optimization complete. Final v2v error: 3.4953606128692627 mm

Highest mean error: 4.496843338012695 mm for frame 188

Lowest mean error: 2.904536008834839 mm for frame 16

Saving results

Total time: 47.83425951004028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832758
Iteration 2/25 | Loss: 0.00126766
Iteration 3/25 | Loss: 0.00119929
Iteration 4/25 | Loss: 0.00118620
Iteration 5/25 | Loss: 0.00118157
Iteration 6/25 | Loss: 0.00118130
Iteration 7/25 | Loss: 0.00118130
Iteration 8/25 | Loss: 0.00118125
Iteration 9/25 | Loss: 0.00118125
Iteration 10/25 | Loss: 0.00118125
Iteration 11/25 | Loss: 0.00118125
Iteration 12/25 | Loss: 0.00118125
Iteration 13/25 | Loss: 0.00118125
Iteration 14/25 | Loss: 0.00118125
Iteration 15/25 | Loss: 0.00118125
Iteration 16/25 | Loss: 0.00118125
Iteration 17/25 | Loss: 0.00118125
Iteration 18/25 | Loss: 0.00118125
Iteration 19/25 | Loss: 0.00118125
Iteration 20/25 | Loss: 0.00118125
Iteration 21/25 | Loss: 0.00118125
Iteration 22/25 | Loss: 0.00118125
Iteration 23/25 | Loss: 0.00118125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011812455486506224, 0.0011812455486506224, 0.0011812455486506224, 0.0011812455486506224, 0.0011812455486506224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011812455486506224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80562735
Iteration 2/25 | Loss: 0.00098591
Iteration 3/25 | Loss: 0.00098591
Iteration 4/25 | Loss: 0.00098591
Iteration 5/25 | Loss: 0.00098591
Iteration 6/25 | Loss: 0.00098591
Iteration 7/25 | Loss: 0.00098591
Iteration 8/25 | Loss: 0.00098591
Iteration 9/25 | Loss: 0.00098590
Iteration 10/25 | Loss: 0.00098590
Iteration 11/25 | Loss: 0.00098590
Iteration 12/25 | Loss: 0.00098590
Iteration 13/25 | Loss: 0.00098590
Iteration 14/25 | Loss: 0.00098590
Iteration 15/25 | Loss: 0.00098590
Iteration 16/25 | Loss: 0.00098590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009859042475000024, 0.0009859042475000024, 0.0009859042475000024, 0.0009859042475000024, 0.0009859042475000024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009859042475000024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098590
Iteration 2/1000 | Loss: 0.00001944
Iteration 3/1000 | Loss: 0.00001556
Iteration 4/1000 | Loss: 0.00001436
Iteration 5/1000 | Loss: 0.00001337
Iteration 6/1000 | Loss: 0.00001285
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001141
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001134
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001128
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001123
Iteration 41/1000 | Loss: 0.00001122
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001118
Iteration 47/1000 | Loss: 0.00001117
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001100
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001093
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001092
Iteration 118/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.0917417057498824e-05, 1.0917417057498824e-05, 1.0917417057498824e-05, 1.0917417057498824e-05, 1.0917417057498824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0917417057498824e-05

Optimization complete. Final v2v error: 2.813937187194824 mm

Highest mean error: 3.498861074447632 mm for frame 84

Lowest mean error: 2.6210947036743164 mm for frame 124

Saving results

Total time: 35.67918276786804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841651
Iteration 2/25 | Loss: 0.00167681
Iteration 3/25 | Loss: 0.00137170
Iteration 4/25 | Loss: 0.00134200
Iteration 5/25 | Loss: 0.00133616
Iteration 6/25 | Loss: 0.00133596
Iteration 7/25 | Loss: 0.00133596
Iteration 8/25 | Loss: 0.00133596
Iteration 9/25 | Loss: 0.00133596
Iteration 10/25 | Loss: 0.00133596
Iteration 11/25 | Loss: 0.00133596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013359583681449294, 0.0013359583681449294, 0.0013359583681449294, 0.0013359583681449294, 0.0013359583681449294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013359583681449294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91807032
Iteration 2/25 | Loss: 0.00106655
Iteration 3/25 | Loss: 0.00106655
Iteration 4/25 | Loss: 0.00106655
Iteration 5/25 | Loss: 0.00106655
Iteration 6/25 | Loss: 0.00106655
Iteration 7/25 | Loss: 0.00106655
Iteration 8/25 | Loss: 0.00106655
Iteration 9/25 | Loss: 0.00106655
Iteration 10/25 | Loss: 0.00106655
Iteration 11/25 | Loss: 0.00106655
Iteration 12/25 | Loss: 0.00106655
Iteration 13/25 | Loss: 0.00106655
Iteration 14/25 | Loss: 0.00106655
Iteration 15/25 | Loss: 0.00106655
Iteration 16/25 | Loss: 0.00106655
Iteration 17/25 | Loss: 0.00106655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001066545839421451, 0.001066545839421451, 0.001066545839421451, 0.001066545839421451, 0.001066545839421451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001066545839421451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106655
Iteration 2/1000 | Loss: 0.00003515
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00002012
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001857
Iteration 7/1000 | Loss: 0.00001814
Iteration 8/1000 | Loss: 0.00001787
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001702
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001701
Iteration 19/1000 | Loss: 0.00001701
Iteration 20/1000 | Loss: 0.00001700
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001685
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001680
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001679
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001677
Iteration 49/1000 | Loss: 0.00001677
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001676
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001675
Iteration 62/1000 | Loss: 0.00001675
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001674
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001673
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001673
Iteration 106/1000 | Loss: 0.00001673
Iteration 107/1000 | Loss: 0.00001673
Iteration 108/1000 | Loss: 0.00001673
Iteration 109/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.6733843949623406e-05, 1.6733843949623406e-05, 1.6733843949623406e-05, 1.6733843949623406e-05, 1.6733843949623406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6733843949623406e-05

Optimization complete. Final v2v error: 3.410827875137329 mm

Highest mean error: 3.806830883026123 mm for frame 0

Lowest mean error: 3.1417694091796875 mm for frame 76

Saving results

Total time: 34.96826481819153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781564
Iteration 2/25 | Loss: 0.00142514
Iteration 3/25 | Loss: 0.00132075
Iteration 4/25 | Loss: 0.00129093
Iteration 5/25 | Loss: 0.00128238
Iteration 6/25 | Loss: 0.00128132
Iteration 7/25 | Loss: 0.00128064
Iteration 8/25 | Loss: 0.00129145
Iteration 9/25 | Loss: 0.00127284
Iteration 10/25 | Loss: 0.00128453
Iteration 11/25 | Loss: 0.00127083
Iteration 12/25 | Loss: 0.00127074
Iteration 13/25 | Loss: 0.00127074
Iteration 14/25 | Loss: 0.00127074
Iteration 15/25 | Loss: 0.00127074
Iteration 16/25 | Loss: 0.00127074
Iteration 17/25 | Loss: 0.00127074
Iteration 18/25 | Loss: 0.00127074
Iteration 19/25 | Loss: 0.00127074
Iteration 20/25 | Loss: 0.00127074
Iteration 21/25 | Loss: 0.00127074
Iteration 22/25 | Loss: 0.00127074
Iteration 23/25 | Loss: 0.00127073
Iteration 24/25 | Loss: 0.00127073
Iteration 25/25 | Loss: 0.00127073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.90199280
Iteration 2/25 | Loss: 0.00089783
Iteration 3/25 | Loss: 0.00089776
Iteration 4/25 | Loss: 0.00089776
Iteration 5/25 | Loss: 0.00089776
Iteration 6/25 | Loss: 0.00089776
Iteration 7/25 | Loss: 0.00089776
Iteration 8/25 | Loss: 0.00089776
Iteration 9/25 | Loss: 0.00089776
Iteration 10/25 | Loss: 0.00089776
Iteration 11/25 | Loss: 0.00089776
Iteration 12/25 | Loss: 0.00089776
Iteration 13/25 | Loss: 0.00089776
Iteration 14/25 | Loss: 0.00089776
Iteration 15/25 | Loss: 0.00089776
Iteration 16/25 | Loss: 0.00089776
Iteration 17/25 | Loss: 0.00089776
Iteration 18/25 | Loss: 0.00089776
Iteration 19/25 | Loss: 0.00089776
Iteration 20/25 | Loss: 0.00089776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008977564284577966, 0.0008977564284577966, 0.0008977564284577966, 0.0008977564284577966, 0.0008977564284577966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008977564284577966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089776
Iteration 2/1000 | Loss: 0.00002778
Iteration 3/1000 | Loss: 0.00002064
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00019609
Iteration 6/1000 | Loss: 0.00016863
Iteration 7/1000 | Loss: 0.00055381
Iteration 8/1000 | Loss: 0.00005959
Iteration 9/1000 | Loss: 0.00003782
Iteration 10/1000 | Loss: 0.00002385
Iteration 11/1000 | Loss: 0.00037784
Iteration 12/1000 | Loss: 0.00072483
Iteration 13/1000 | Loss: 0.00039010
Iteration 14/1000 | Loss: 0.00002380
Iteration 15/1000 | Loss: 0.00002072
Iteration 16/1000 | Loss: 0.00016313
Iteration 17/1000 | Loss: 0.00014871
Iteration 18/1000 | Loss: 0.00001977
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001860
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00017433
Iteration 23/1000 | Loss: 0.00002381
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00001920
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001840
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00025731
Iteration 30/1000 | Loss: 0.00007206
Iteration 31/1000 | Loss: 0.00025172
Iteration 32/1000 | Loss: 0.00008035
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001794
Iteration 35/1000 | Loss: 0.00024466
Iteration 36/1000 | Loss: 0.00069099
Iteration 37/1000 | Loss: 0.00020602
Iteration 38/1000 | Loss: 0.00012588
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00018552
Iteration 41/1000 | Loss: 0.00013166
Iteration 42/1000 | Loss: 0.00019682
Iteration 43/1000 | Loss: 0.00013339
Iteration 44/1000 | Loss: 0.00016604
Iteration 45/1000 | Loss: 0.00010847
Iteration 46/1000 | Loss: 0.00021799
Iteration 47/1000 | Loss: 0.00021290
Iteration 48/1000 | Loss: 0.00044151
Iteration 49/1000 | Loss: 0.00006310
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001582
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001542
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001531
Iteration 66/1000 | Loss: 0.00001531
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001530
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001529
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001528
Iteration 77/1000 | Loss: 0.00001528
Iteration 78/1000 | Loss: 0.00001528
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001527
Iteration 90/1000 | Loss: 0.00001527
Iteration 91/1000 | Loss: 0.00001527
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001526
Iteration 106/1000 | Loss: 0.00001526
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001526
Iteration 117/1000 | Loss: 0.00001526
Iteration 118/1000 | Loss: 0.00001526
Iteration 119/1000 | Loss: 0.00001526
Iteration 120/1000 | Loss: 0.00001526
Iteration 121/1000 | Loss: 0.00001526
Iteration 122/1000 | Loss: 0.00001526
Iteration 123/1000 | Loss: 0.00001526
Iteration 124/1000 | Loss: 0.00001526
Iteration 125/1000 | Loss: 0.00001526
Iteration 126/1000 | Loss: 0.00001526
Iteration 127/1000 | Loss: 0.00001526
Iteration 128/1000 | Loss: 0.00001526
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001526
Iteration 133/1000 | Loss: 0.00001526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.5264722605934367e-05, 1.5264722605934367e-05, 1.5264722605934367e-05, 1.5264722605934367e-05, 1.5264722605934367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5264722605934367e-05

Optimization complete. Final v2v error: 3.2759392261505127 mm

Highest mean error: 4.230191230773926 mm for frame 51

Lowest mean error: 2.7388219833374023 mm for frame 1

Saving results

Total time: 104.93574237823486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405757
Iteration 2/25 | Loss: 0.00135970
Iteration 3/25 | Loss: 0.00123043
Iteration 4/25 | Loss: 0.00121353
Iteration 5/25 | Loss: 0.00120905
Iteration 6/25 | Loss: 0.00120762
Iteration 7/25 | Loss: 0.00120762
Iteration 8/25 | Loss: 0.00120762
Iteration 9/25 | Loss: 0.00120762
Iteration 10/25 | Loss: 0.00120762
Iteration 11/25 | Loss: 0.00120762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012076228158548474, 0.0012076228158548474, 0.0012076228158548474, 0.0012076228158548474, 0.0012076228158548474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012076228158548474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40248835
Iteration 2/25 | Loss: 0.00114436
Iteration 3/25 | Loss: 0.00114436
Iteration 4/25 | Loss: 0.00114436
Iteration 5/25 | Loss: 0.00114436
Iteration 6/25 | Loss: 0.00114436
Iteration 7/25 | Loss: 0.00114436
Iteration 8/25 | Loss: 0.00114436
Iteration 9/25 | Loss: 0.00114436
Iteration 10/25 | Loss: 0.00114436
Iteration 11/25 | Loss: 0.00114436
Iteration 12/25 | Loss: 0.00114436
Iteration 13/25 | Loss: 0.00114436
Iteration 14/25 | Loss: 0.00114436
Iteration 15/25 | Loss: 0.00114436
Iteration 16/25 | Loss: 0.00114436
Iteration 17/25 | Loss: 0.00114436
Iteration 18/25 | Loss: 0.00114436
Iteration 19/25 | Loss: 0.00114436
Iteration 20/25 | Loss: 0.00114436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011443556286394596, 0.0011443556286394596, 0.0011443556286394596, 0.0011443556286394596, 0.0011443556286394596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011443556286394596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114436
Iteration 2/1000 | Loss: 0.00004216
Iteration 3/1000 | Loss: 0.00002661
Iteration 4/1000 | Loss: 0.00001940
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001543
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001412
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001406
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001340
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001326
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001326
Iteration 142/1000 | Loss: 0.00001326
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001324
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001322
Iteration 174/1000 | Loss: 0.00001322
Iteration 175/1000 | Loss: 0.00001322
Iteration 176/1000 | Loss: 0.00001322
Iteration 177/1000 | Loss: 0.00001322
Iteration 178/1000 | Loss: 0.00001322
Iteration 179/1000 | Loss: 0.00001322
Iteration 180/1000 | Loss: 0.00001321
Iteration 181/1000 | Loss: 0.00001321
Iteration 182/1000 | Loss: 0.00001321
Iteration 183/1000 | Loss: 0.00001321
Iteration 184/1000 | Loss: 0.00001321
Iteration 185/1000 | Loss: 0.00001321
Iteration 186/1000 | Loss: 0.00001321
Iteration 187/1000 | Loss: 0.00001321
Iteration 188/1000 | Loss: 0.00001320
Iteration 189/1000 | Loss: 0.00001320
Iteration 190/1000 | Loss: 0.00001320
Iteration 191/1000 | Loss: 0.00001320
Iteration 192/1000 | Loss: 0.00001320
Iteration 193/1000 | Loss: 0.00001320
Iteration 194/1000 | Loss: 0.00001320
Iteration 195/1000 | Loss: 0.00001320
Iteration 196/1000 | Loss: 0.00001320
Iteration 197/1000 | Loss: 0.00001320
Iteration 198/1000 | Loss: 0.00001320
Iteration 199/1000 | Loss: 0.00001320
Iteration 200/1000 | Loss: 0.00001320
Iteration 201/1000 | Loss: 0.00001320
Iteration 202/1000 | Loss: 0.00001320
Iteration 203/1000 | Loss: 0.00001319
Iteration 204/1000 | Loss: 0.00001319
Iteration 205/1000 | Loss: 0.00001319
Iteration 206/1000 | Loss: 0.00001319
Iteration 207/1000 | Loss: 0.00001319
Iteration 208/1000 | Loss: 0.00001319
Iteration 209/1000 | Loss: 0.00001319
Iteration 210/1000 | Loss: 0.00001319
Iteration 211/1000 | Loss: 0.00001319
Iteration 212/1000 | Loss: 0.00001318
Iteration 213/1000 | Loss: 0.00001318
Iteration 214/1000 | Loss: 0.00001318
Iteration 215/1000 | Loss: 0.00001318
Iteration 216/1000 | Loss: 0.00001318
Iteration 217/1000 | Loss: 0.00001318
Iteration 218/1000 | Loss: 0.00001317
Iteration 219/1000 | Loss: 0.00001317
Iteration 220/1000 | Loss: 0.00001317
Iteration 221/1000 | Loss: 0.00001317
Iteration 222/1000 | Loss: 0.00001317
Iteration 223/1000 | Loss: 0.00001316
Iteration 224/1000 | Loss: 0.00001316
Iteration 225/1000 | Loss: 0.00001316
Iteration 226/1000 | Loss: 0.00001316
Iteration 227/1000 | Loss: 0.00001315
Iteration 228/1000 | Loss: 0.00001315
Iteration 229/1000 | Loss: 0.00001315
Iteration 230/1000 | Loss: 0.00001315
Iteration 231/1000 | Loss: 0.00001315
Iteration 232/1000 | Loss: 0.00001315
Iteration 233/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.3154011867300142e-05, 1.3154011867300142e-05, 1.3154011867300142e-05, 1.3154011867300142e-05, 1.3154011867300142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3154011867300142e-05

Optimization complete. Final v2v error: 3.097930431365967 mm

Highest mean error: 3.839749574661255 mm for frame 107

Lowest mean error: 2.5899322032928467 mm for frame 161

Saving results

Total time: 45.25120687484741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907953
Iteration 2/25 | Loss: 0.00281239
Iteration 3/25 | Loss: 0.00186884
Iteration 4/25 | Loss: 0.00183549
Iteration 5/25 | Loss: 0.00163667
Iteration 6/25 | Loss: 0.00161881
Iteration 7/25 | Loss: 0.00160645
Iteration 8/25 | Loss: 0.00159100
Iteration 9/25 | Loss: 0.00158586
Iteration 10/25 | Loss: 0.00158368
Iteration 11/25 | Loss: 0.00158008
Iteration 12/25 | Loss: 0.00157395
Iteration 13/25 | Loss: 0.00156939
Iteration 14/25 | Loss: 0.00156791
Iteration 15/25 | Loss: 0.00156633
Iteration 16/25 | Loss: 0.00156568
Iteration 17/25 | Loss: 0.00156526
Iteration 18/25 | Loss: 0.00156487
Iteration 19/25 | Loss: 0.00156557
Iteration 20/25 | Loss: 0.00156517
Iteration 21/25 | Loss: 0.00156371
Iteration 22/25 | Loss: 0.00156339
Iteration 23/25 | Loss: 0.00156304
Iteration 24/25 | Loss: 0.00156345
Iteration 25/25 | Loss: 0.00156524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14617729
Iteration 2/25 | Loss: 0.00756952
Iteration 3/25 | Loss: 0.00374485
Iteration 4/25 | Loss: 0.00351076
Iteration 5/25 | Loss: 0.00351076
Iteration 6/25 | Loss: 0.00351076
Iteration 7/25 | Loss: 0.00351076
Iteration 8/25 | Loss: 0.00351076
Iteration 9/25 | Loss: 0.00351076
Iteration 10/25 | Loss: 0.00351076
Iteration 11/25 | Loss: 0.00351076
Iteration 12/25 | Loss: 0.00351076
Iteration 13/25 | Loss: 0.00351076
Iteration 14/25 | Loss: 0.00351076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0035107582807540894, 0.0035107582807540894, 0.0035107582807540894, 0.0035107582807540894, 0.0035107582807540894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035107582807540894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351076
Iteration 2/1000 | Loss: 0.00971446
Iteration 3/1000 | Loss: 0.00969570
Iteration 4/1000 | Loss: 0.00149764
Iteration 5/1000 | Loss: 0.00042356
Iteration 6/1000 | Loss: 0.00111846
Iteration 7/1000 | Loss: 0.00097875
Iteration 8/1000 | Loss: 0.00157220
Iteration 9/1000 | Loss: 0.00060259
Iteration 10/1000 | Loss: 0.00131567
Iteration 11/1000 | Loss: 0.00088690
Iteration 12/1000 | Loss: 0.00089832
Iteration 13/1000 | Loss: 0.00276887
Iteration 14/1000 | Loss: 0.00340765
Iteration 15/1000 | Loss: 0.00182768
Iteration 16/1000 | Loss: 0.00188153
Iteration 17/1000 | Loss: 0.00096958
Iteration 18/1000 | Loss: 0.00079702
Iteration 19/1000 | Loss: 0.00100208
Iteration 20/1000 | Loss: 0.00033613
Iteration 21/1000 | Loss: 0.00244766
Iteration 22/1000 | Loss: 0.00115022
Iteration 23/1000 | Loss: 0.00155028
Iteration 24/1000 | Loss: 0.00123443
Iteration 25/1000 | Loss: 0.00276776
Iteration 26/1000 | Loss: 0.00085987
Iteration 27/1000 | Loss: 0.00077106
Iteration 28/1000 | Loss: 0.00194396
Iteration 29/1000 | Loss: 0.00068230
Iteration 30/1000 | Loss: 0.00088779
Iteration 31/1000 | Loss: 0.00247567
Iteration 32/1000 | Loss: 0.00041095
Iteration 33/1000 | Loss: 0.00009309
Iteration 34/1000 | Loss: 0.00036121
Iteration 35/1000 | Loss: 0.00050788
Iteration 36/1000 | Loss: 0.00017096
Iteration 37/1000 | Loss: 0.00038850
Iteration 38/1000 | Loss: 0.00028787
Iteration 39/1000 | Loss: 0.00018920
Iteration 40/1000 | Loss: 0.00078707
Iteration 41/1000 | Loss: 0.00075988
Iteration 42/1000 | Loss: 0.00114995
Iteration 43/1000 | Loss: 0.00023473
Iteration 44/1000 | Loss: 0.00054535
Iteration 45/1000 | Loss: 0.00008512
Iteration 46/1000 | Loss: 0.00006348
Iteration 47/1000 | Loss: 0.00060286
Iteration 48/1000 | Loss: 0.00127636
Iteration 49/1000 | Loss: 0.00035402
Iteration 50/1000 | Loss: 0.00032297
Iteration 51/1000 | Loss: 0.00031794
Iteration 52/1000 | Loss: 0.00049687
Iteration 53/1000 | Loss: 0.00044955
Iteration 54/1000 | Loss: 0.00018598
Iteration 55/1000 | Loss: 0.00019057
Iteration 56/1000 | Loss: 0.00005262
Iteration 57/1000 | Loss: 0.00005589
Iteration 58/1000 | Loss: 0.00025229
Iteration 59/1000 | Loss: 0.00003970
Iteration 60/1000 | Loss: 0.00108458
Iteration 61/1000 | Loss: 0.00033722
Iteration 62/1000 | Loss: 0.00036856
Iteration 63/1000 | Loss: 0.00040254
Iteration 64/1000 | Loss: 0.00006265
Iteration 65/1000 | Loss: 0.00015269
Iteration 66/1000 | Loss: 0.00062500
Iteration 67/1000 | Loss: 0.00039891
Iteration 68/1000 | Loss: 0.00028048
Iteration 69/1000 | Loss: 0.00004584
Iteration 70/1000 | Loss: 0.00003707
Iteration 71/1000 | Loss: 0.00015353
Iteration 72/1000 | Loss: 0.00010042
Iteration 73/1000 | Loss: 0.00006835
Iteration 74/1000 | Loss: 0.00004445
Iteration 75/1000 | Loss: 0.00005813
Iteration 76/1000 | Loss: 0.00086737
Iteration 77/1000 | Loss: 0.00033339
Iteration 78/1000 | Loss: 0.00014886
Iteration 79/1000 | Loss: 0.00003311
Iteration 80/1000 | Loss: 0.00026371
Iteration 81/1000 | Loss: 0.00004890
Iteration 82/1000 | Loss: 0.00019089
Iteration 83/1000 | Loss: 0.00009350
Iteration 84/1000 | Loss: 0.00012547
Iteration 85/1000 | Loss: 0.00002800
Iteration 86/1000 | Loss: 0.00002672
Iteration 87/1000 | Loss: 0.00002619
Iteration 88/1000 | Loss: 0.00044437
Iteration 89/1000 | Loss: 0.00068282
Iteration 90/1000 | Loss: 0.00034651
Iteration 91/1000 | Loss: 0.00058736
Iteration 92/1000 | Loss: 0.00048907
Iteration 93/1000 | Loss: 0.00041706
Iteration 94/1000 | Loss: 0.00028306
Iteration 95/1000 | Loss: 0.00066442
Iteration 96/1000 | Loss: 0.00038794
Iteration 97/1000 | Loss: 0.00050575
Iteration 98/1000 | Loss: 0.00015058
Iteration 99/1000 | Loss: 0.00005951
Iteration 100/1000 | Loss: 0.00011063
Iteration 101/1000 | Loss: 0.00016345
Iteration 102/1000 | Loss: 0.00003207
Iteration 103/1000 | Loss: 0.00004545
Iteration 104/1000 | Loss: 0.00004138
Iteration 105/1000 | Loss: 0.00002641
Iteration 106/1000 | Loss: 0.00022863
Iteration 107/1000 | Loss: 0.00002461
Iteration 108/1000 | Loss: 0.00003088
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00030814
Iteration 111/1000 | Loss: 0.00002090
Iteration 112/1000 | Loss: 0.00002039
Iteration 113/1000 | Loss: 0.00001999
Iteration 114/1000 | Loss: 0.00001966
Iteration 115/1000 | Loss: 0.00001941
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001922
Iteration 120/1000 | Loss: 0.00001919
Iteration 121/1000 | Loss: 0.00001919
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001915
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001914
Iteration 129/1000 | Loss: 0.00001913
Iteration 130/1000 | Loss: 0.00001913
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001913
Iteration 134/1000 | Loss: 0.00001912
Iteration 135/1000 | Loss: 0.00001912
Iteration 136/1000 | Loss: 0.00001912
Iteration 137/1000 | Loss: 0.00001908
Iteration 138/1000 | Loss: 0.00001908
Iteration 139/1000 | Loss: 0.00001907
Iteration 140/1000 | Loss: 0.00001907
Iteration 141/1000 | Loss: 0.00001895
Iteration 142/1000 | Loss: 0.00001895
Iteration 143/1000 | Loss: 0.00001895
Iteration 144/1000 | Loss: 0.00001894
Iteration 145/1000 | Loss: 0.00001894
Iteration 146/1000 | Loss: 0.00001894
Iteration 147/1000 | Loss: 0.00001894
Iteration 148/1000 | Loss: 0.00001893
Iteration 149/1000 | Loss: 0.00001893
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001892
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Iteration 158/1000 | Loss: 0.00001892
Iteration 159/1000 | Loss: 0.00001892
Iteration 160/1000 | Loss: 0.00001892
Iteration 161/1000 | Loss: 0.00001891
Iteration 162/1000 | Loss: 0.00001891
Iteration 163/1000 | Loss: 0.00001891
Iteration 164/1000 | Loss: 0.00001891
Iteration 165/1000 | Loss: 0.00001891
Iteration 166/1000 | Loss: 0.00001891
Iteration 167/1000 | Loss: 0.00001891
Iteration 168/1000 | Loss: 0.00001891
Iteration 169/1000 | Loss: 0.00001891
Iteration 170/1000 | Loss: 0.00001891
Iteration 171/1000 | Loss: 0.00001890
Iteration 172/1000 | Loss: 0.00001890
Iteration 173/1000 | Loss: 0.00001889
Iteration 174/1000 | Loss: 0.00001888
Iteration 175/1000 | Loss: 0.00001888
Iteration 176/1000 | Loss: 0.00001888
Iteration 177/1000 | Loss: 0.00001887
Iteration 178/1000 | Loss: 0.00001887
Iteration 179/1000 | Loss: 0.00001887
Iteration 180/1000 | Loss: 0.00001887
Iteration 181/1000 | Loss: 0.00001887
Iteration 182/1000 | Loss: 0.00001886
Iteration 183/1000 | Loss: 0.00001886
Iteration 184/1000 | Loss: 0.00001886
Iteration 185/1000 | Loss: 0.00001886
Iteration 186/1000 | Loss: 0.00001886
Iteration 187/1000 | Loss: 0.00001885
Iteration 188/1000 | Loss: 0.00001885
Iteration 189/1000 | Loss: 0.00001885
Iteration 190/1000 | Loss: 0.00001885
Iteration 191/1000 | Loss: 0.00001885
Iteration 192/1000 | Loss: 0.00001884
Iteration 193/1000 | Loss: 0.00001884
Iteration 194/1000 | Loss: 0.00001884
Iteration 195/1000 | Loss: 0.00001884
Iteration 196/1000 | Loss: 0.00001884
Iteration 197/1000 | Loss: 0.00001884
Iteration 198/1000 | Loss: 0.00001883
Iteration 199/1000 | Loss: 0.00001883
Iteration 200/1000 | Loss: 0.00001883
Iteration 201/1000 | Loss: 0.00001883
Iteration 202/1000 | Loss: 0.00001883
Iteration 203/1000 | Loss: 0.00001882
Iteration 204/1000 | Loss: 0.00001882
Iteration 205/1000 | Loss: 0.00001882
Iteration 206/1000 | Loss: 0.00001882
Iteration 207/1000 | Loss: 0.00001882
Iteration 208/1000 | Loss: 0.00001882
Iteration 209/1000 | Loss: 0.00001882
Iteration 210/1000 | Loss: 0.00001882
Iteration 211/1000 | Loss: 0.00001881
Iteration 212/1000 | Loss: 0.00001881
Iteration 213/1000 | Loss: 0.00001881
Iteration 214/1000 | Loss: 0.00001881
Iteration 215/1000 | Loss: 0.00001881
Iteration 216/1000 | Loss: 0.00001881
Iteration 217/1000 | Loss: 0.00001881
Iteration 218/1000 | Loss: 0.00001881
Iteration 219/1000 | Loss: 0.00001881
Iteration 220/1000 | Loss: 0.00001881
Iteration 221/1000 | Loss: 0.00001880
Iteration 222/1000 | Loss: 0.00001880
Iteration 223/1000 | Loss: 0.00001880
Iteration 224/1000 | Loss: 0.00001880
Iteration 225/1000 | Loss: 0.00001880
Iteration 226/1000 | Loss: 0.00001880
Iteration 227/1000 | Loss: 0.00001880
Iteration 228/1000 | Loss: 0.00001879
Iteration 229/1000 | Loss: 0.00001879
Iteration 230/1000 | Loss: 0.00001879
Iteration 231/1000 | Loss: 0.00001879
Iteration 232/1000 | Loss: 0.00001879
Iteration 233/1000 | Loss: 0.00001879
Iteration 234/1000 | Loss: 0.00001879
Iteration 235/1000 | Loss: 0.00001879
Iteration 236/1000 | Loss: 0.00001879
Iteration 237/1000 | Loss: 0.00001879
Iteration 238/1000 | Loss: 0.00001879
Iteration 239/1000 | Loss: 0.00001879
Iteration 240/1000 | Loss: 0.00001879
Iteration 241/1000 | Loss: 0.00001879
Iteration 242/1000 | Loss: 0.00001879
Iteration 243/1000 | Loss: 0.00001878
Iteration 244/1000 | Loss: 0.00001878
Iteration 245/1000 | Loss: 0.00001878
Iteration 246/1000 | Loss: 0.00001878
Iteration 247/1000 | Loss: 0.00001878
Iteration 248/1000 | Loss: 0.00001878
Iteration 249/1000 | Loss: 0.00001878
Iteration 250/1000 | Loss: 0.00001878
Iteration 251/1000 | Loss: 0.00001878
Iteration 252/1000 | Loss: 0.00001878
Iteration 253/1000 | Loss: 0.00001878
Iteration 254/1000 | Loss: 0.00001878
Iteration 255/1000 | Loss: 0.00001878
Iteration 256/1000 | Loss: 0.00001878
Iteration 257/1000 | Loss: 0.00001878
Iteration 258/1000 | Loss: 0.00001877
Iteration 259/1000 | Loss: 0.00001877
Iteration 260/1000 | Loss: 0.00001877
Iteration 261/1000 | Loss: 0.00001877
Iteration 262/1000 | Loss: 0.00001877
Iteration 263/1000 | Loss: 0.00001877
Iteration 264/1000 | Loss: 0.00001877
Iteration 265/1000 | Loss: 0.00001877
Iteration 266/1000 | Loss: 0.00001877
Iteration 267/1000 | Loss: 0.00001877
Iteration 268/1000 | Loss: 0.00001877
Iteration 269/1000 | Loss: 0.00001877
Iteration 270/1000 | Loss: 0.00001877
Iteration 271/1000 | Loss: 0.00001877
Iteration 272/1000 | Loss: 0.00001877
Iteration 273/1000 | Loss: 0.00001877
Iteration 274/1000 | Loss: 0.00001877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [1.8767721485346556e-05, 1.8767721485346556e-05, 1.8767721485346556e-05, 1.8767721485346556e-05, 1.8767721485346556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8767721485346556e-05

Optimization complete. Final v2v error: 3.459223747253418 mm

Highest mean error: 5.338268756866455 mm for frame 70

Lowest mean error: 2.73455548286438 mm for frame 28

Saving results

Total time: 232.3457269668579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557355
Iteration 2/25 | Loss: 0.00146338
Iteration 3/25 | Loss: 0.00135516
Iteration 4/25 | Loss: 0.00134311
Iteration 5/25 | Loss: 0.00134041
Iteration 6/25 | Loss: 0.00134041
Iteration 7/25 | Loss: 0.00134041
Iteration 8/25 | Loss: 0.00134041
Iteration 9/25 | Loss: 0.00134041
Iteration 10/25 | Loss: 0.00134041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013404100900515914, 0.0013404100900515914, 0.0013404100900515914, 0.0013404100900515914, 0.0013404100900515914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013404100900515914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38792872
Iteration 2/25 | Loss: 0.00107600
Iteration 3/25 | Loss: 0.00107600
Iteration 4/25 | Loss: 0.00107600
Iteration 5/25 | Loss: 0.00107600
Iteration 6/25 | Loss: 0.00107600
Iteration 7/25 | Loss: 0.00107600
Iteration 8/25 | Loss: 0.00107600
Iteration 9/25 | Loss: 0.00107600
Iteration 10/25 | Loss: 0.00107600
Iteration 11/25 | Loss: 0.00107600
Iteration 12/25 | Loss: 0.00107600
Iteration 13/25 | Loss: 0.00107600
Iteration 14/25 | Loss: 0.00107600
Iteration 15/25 | Loss: 0.00107600
Iteration 16/25 | Loss: 0.00107600
Iteration 17/25 | Loss: 0.00107600
Iteration 18/25 | Loss: 0.00107600
Iteration 19/25 | Loss: 0.00107600
Iteration 20/25 | Loss: 0.00107600
Iteration 21/25 | Loss: 0.00107600
Iteration 22/25 | Loss: 0.00107600
Iteration 23/25 | Loss: 0.00107600
Iteration 24/25 | Loss: 0.00107600
Iteration 25/25 | Loss: 0.00107600

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107600
Iteration 2/1000 | Loss: 0.00004063
Iteration 3/1000 | Loss: 0.00002885
Iteration 4/1000 | Loss: 0.00002672
Iteration 5/1000 | Loss: 0.00002564
Iteration 6/1000 | Loss: 0.00002494
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002410
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002342
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002335
Iteration 14/1000 | Loss: 0.00002333
Iteration 15/1000 | Loss: 0.00002320
Iteration 16/1000 | Loss: 0.00002301
Iteration 17/1000 | Loss: 0.00002290
Iteration 18/1000 | Loss: 0.00002284
Iteration 19/1000 | Loss: 0.00002280
Iteration 20/1000 | Loss: 0.00002278
Iteration 21/1000 | Loss: 0.00002273
Iteration 22/1000 | Loss: 0.00002269
Iteration 23/1000 | Loss: 0.00002269
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002266
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002263
Iteration 32/1000 | Loss: 0.00002262
Iteration 33/1000 | Loss: 0.00002262
Iteration 34/1000 | Loss: 0.00002262
Iteration 35/1000 | Loss: 0.00002261
Iteration 36/1000 | Loss: 0.00002260
Iteration 37/1000 | Loss: 0.00002260
Iteration 38/1000 | Loss: 0.00002260
Iteration 39/1000 | Loss: 0.00002259
Iteration 40/1000 | Loss: 0.00002259
Iteration 41/1000 | Loss: 0.00002258
Iteration 42/1000 | Loss: 0.00002258
Iteration 43/1000 | Loss: 0.00002258
Iteration 44/1000 | Loss: 0.00002257
Iteration 45/1000 | Loss: 0.00002257
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002255
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00002254
Iteration 52/1000 | Loss: 0.00002254
Iteration 53/1000 | Loss: 0.00002254
Iteration 54/1000 | Loss: 0.00002254
Iteration 55/1000 | Loss: 0.00002254
Iteration 56/1000 | Loss: 0.00002254
Iteration 57/1000 | Loss: 0.00002253
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002246
Iteration 61/1000 | Loss: 0.00002245
Iteration 62/1000 | Loss: 0.00002244
Iteration 63/1000 | Loss: 0.00002243
Iteration 64/1000 | Loss: 0.00002243
Iteration 65/1000 | Loss: 0.00002243
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002242
Iteration 69/1000 | Loss: 0.00002242
Iteration 70/1000 | Loss: 0.00002242
Iteration 71/1000 | Loss: 0.00002241
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00002240
Iteration 75/1000 | Loss: 0.00002240
Iteration 76/1000 | Loss: 0.00002240
Iteration 77/1000 | Loss: 0.00002240
Iteration 78/1000 | Loss: 0.00002240
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00002239
Iteration 82/1000 | Loss: 0.00002239
Iteration 83/1000 | Loss: 0.00002239
Iteration 84/1000 | Loss: 0.00002239
Iteration 85/1000 | Loss: 0.00002238
Iteration 86/1000 | Loss: 0.00002238
Iteration 87/1000 | Loss: 0.00002238
Iteration 88/1000 | Loss: 0.00002238
Iteration 89/1000 | Loss: 0.00002237
Iteration 90/1000 | Loss: 0.00002237
Iteration 91/1000 | Loss: 0.00002237
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002236
Iteration 94/1000 | Loss: 0.00002236
Iteration 95/1000 | Loss: 0.00002236
Iteration 96/1000 | Loss: 0.00002236
Iteration 97/1000 | Loss: 0.00002236
Iteration 98/1000 | Loss: 0.00002236
Iteration 99/1000 | Loss: 0.00002236
Iteration 100/1000 | Loss: 0.00002236
Iteration 101/1000 | Loss: 0.00002236
Iteration 102/1000 | Loss: 0.00002236
Iteration 103/1000 | Loss: 0.00002236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.2360678485711105e-05, 2.2360678485711105e-05, 2.2360678485711105e-05, 2.2360678485711105e-05, 2.2360678485711105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2360678485711105e-05

Optimization complete. Final v2v error: 3.8822484016418457 mm

Highest mean error: 4.0925374031066895 mm for frame 83

Lowest mean error: 3.5836005210876465 mm for frame 15

Saving results

Total time: 40.70903944969177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671197
Iteration 2/25 | Loss: 0.00147770
Iteration 3/25 | Loss: 0.00133101
Iteration 4/25 | Loss: 0.00130565
Iteration 5/25 | Loss: 0.00130060
Iteration 6/25 | Loss: 0.00129995
Iteration 7/25 | Loss: 0.00129995
Iteration 8/25 | Loss: 0.00129995
Iteration 9/25 | Loss: 0.00129995
Iteration 10/25 | Loss: 0.00129995
Iteration 11/25 | Loss: 0.00129995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012999525060877204, 0.0012999525060877204, 0.0012999525060877204, 0.0012999525060877204, 0.0012999525060877204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012999525060877204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85501504
Iteration 2/25 | Loss: 0.00088914
Iteration 3/25 | Loss: 0.00088912
Iteration 4/25 | Loss: 0.00088912
Iteration 5/25 | Loss: 0.00088912
Iteration 6/25 | Loss: 0.00088912
Iteration 7/25 | Loss: 0.00088912
Iteration 8/25 | Loss: 0.00088912
Iteration 9/25 | Loss: 0.00088912
Iteration 10/25 | Loss: 0.00088912
Iteration 11/25 | Loss: 0.00088912
Iteration 12/25 | Loss: 0.00088912
Iteration 13/25 | Loss: 0.00088912
Iteration 14/25 | Loss: 0.00088912
Iteration 15/25 | Loss: 0.00088912
Iteration 16/25 | Loss: 0.00088912
Iteration 17/25 | Loss: 0.00088912
Iteration 18/25 | Loss: 0.00088912
Iteration 19/25 | Loss: 0.00088912
Iteration 20/25 | Loss: 0.00088912
Iteration 21/25 | Loss: 0.00088912
Iteration 22/25 | Loss: 0.00088912
Iteration 23/25 | Loss: 0.00088912
Iteration 24/25 | Loss: 0.00088912
Iteration 25/25 | Loss: 0.00088912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088912
Iteration 2/1000 | Loss: 0.00004515
Iteration 3/1000 | Loss: 0.00002742
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002323
Iteration 6/1000 | Loss: 0.00002260
Iteration 7/1000 | Loss: 0.00002206
Iteration 8/1000 | Loss: 0.00002176
Iteration 9/1000 | Loss: 0.00002152
Iteration 10/1000 | Loss: 0.00002135
Iteration 11/1000 | Loss: 0.00002122
Iteration 12/1000 | Loss: 0.00002116
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002101
Iteration 15/1000 | Loss: 0.00002093
Iteration 16/1000 | Loss: 0.00002090
Iteration 17/1000 | Loss: 0.00002090
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002089
Iteration 20/1000 | Loss: 0.00002089
Iteration 21/1000 | Loss: 0.00002089
Iteration 22/1000 | Loss: 0.00002089
Iteration 23/1000 | Loss: 0.00002089
Iteration 24/1000 | Loss: 0.00002089
Iteration 25/1000 | Loss: 0.00002089
Iteration 26/1000 | Loss: 0.00002089
Iteration 27/1000 | Loss: 0.00002089
Iteration 28/1000 | Loss: 0.00002088
Iteration 29/1000 | Loss: 0.00002088
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002086
Iteration 34/1000 | Loss: 0.00002086
Iteration 35/1000 | Loss: 0.00002085
Iteration 36/1000 | Loss: 0.00002085
Iteration 37/1000 | Loss: 0.00002085
Iteration 38/1000 | Loss: 0.00002084
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002082
Iteration 49/1000 | Loss: 0.00002082
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002081
Iteration 52/1000 | Loss: 0.00002081
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002080
Iteration 56/1000 | Loss: 0.00002080
Iteration 57/1000 | Loss: 0.00002080
Iteration 58/1000 | Loss: 0.00002080
Iteration 59/1000 | Loss: 0.00002080
Iteration 60/1000 | Loss: 0.00002079
Iteration 61/1000 | Loss: 0.00002079
Iteration 62/1000 | Loss: 0.00002079
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002078
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002077
Iteration 68/1000 | Loss: 0.00002076
Iteration 69/1000 | Loss: 0.00002076
Iteration 70/1000 | Loss: 0.00002076
Iteration 71/1000 | Loss: 0.00002076
Iteration 72/1000 | Loss: 0.00002075
Iteration 73/1000 | Loss: 0.00002075
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002074
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002073
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002073
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002073
Iteration 88/1000 | Loss: 0.00002072
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002072
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002070
Iteration 97/1000 | Loss: 0.00002070
Iteration 98/1000 | Loss: 0.00002070
Iteration 99/1000 | Loss: 0.00002070
Iteration 100/1000 | Loss: 0.00002070
Iteration 101/1000 | Loss: 0.00002069
Iteration 102/1000 | Loss: 0.00002069
Iteration 103/1000 | Loss: 0.00002069
Iteration 104/1000 | Loss: 0.00002069
Iteration 105/1000 | Loss: 0.00002069
Iteration 106/1000 | Loss: 0.00002068
Iteration 107/1000 | Loss: 0.00002068
Iteration 108/1000 | Loss: 0.00002068
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002065
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002064
Iteration 128/1000 | Loss: 0.00002064
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002063
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002062
Iteration 140/1000 | Loss: 0.00002062
Iteration 141/1000 | Loss: 0.00002062
Iteration 142/1000 | Loss: 0.00002062
Iteration 143/1000 | Loss: 0.00002062
Iteration 144/1000 | Loss: 0.00002062
Iteration 145/1000 | Loss: 0.00002061
Iteration 146/1000 | Loss: 0.00002061
Iteration 147/1000 | Loss: 0.00002061
Iteration 148/1000 | Loss: 0.00002061
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002061
Iteration 152/1000 | Loss: 0.00002061
Iteration 153/1000 | Loss: 0.00002061
Iteration 154/1000 | Loss: 0.00002061
Iteration 155/1000 | Loss: 0.00002061
Iteration 156/1000 | Loss: 0.00002061
Iteration 157/1000 | Loss: 0.00002060
Iteration 158/1000 | Loss: 0.00002060
Iteration 159/1000 | Loss: 0.00002060
Iteration 160/1000 | Loss: 0.00002060
Iteration 161/1000 | Loss: 0.00002060
Iteration 162/1000 | Loss: 0.00002060
Iteration 163/1000 | Loss: 0.00002060
Iteration 164/1000 | Loss: 0.00002060
Iteration 165/1000 | Loss: 0.00002060
Iteration 166/1000 | Loss: 0.00002060
Iteration 167/1000 | Loss: 0.00002060
Iteration 168/1000 | Loss: 0.00002060
Iteration 169/1000 | Loss: 0.00002059
Iteration 170/1000 | Loss: 0.00002059
Iteration 171/1000 | Loss: 0.00002059
Iteration 172/1000 | Loss: 0.00002059
Iteration 173/1000 | Loss: 0.00002059
Iteration 174/1000 | Loss: 0.00002059
Iteration 175/1000 | Loss: 0.00002059
Iteration 176/1000 | Loss: 0.00002059
Iteration 177/1000 | Loss: 0.00002059
Iteration 178/1000 | Loss: 0.00002059
Iteration 179/1000 | Loss: 0.00002059
Iteration 180/1000 | Loss: 0.00002059
Iteration 181/1000 | Loss: 0.00002059
Iteration 182/1000 | Loss: 0.00002059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.0591303837136365e-05, 2.0591303837136365e-05, 2.0591303837136365e-05, 2.0591303837136365e-05, 2.0591303837136365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0591303837136365e-05

Optimization complete. Final v2v error: 3.7694029808044434 mm

Highest mean error: 4.924281597137451 mm for frame 98

Lowest mean error: 3.1122210025787354 mm for frame 7

Saving results

Total time: 44.62887501716614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423750
Iteration 2/25 | Loss: 0.00150922
Iteration 3/25 | Loss: 0.00130165
Iteration 4/25 | Loss: 0.00127682
Iteration 5/25 | Loss: 0.00127400
Iteration 6/25 | Loss: 0.00127352
Iteration 7/25 | Loss: 0.00127352
Iteration 8/25 | Loss: 0.00127352
Iteration 9/25 | Loss: 0.00127352
Iteration 10/25 | Loss: 0.00127352
Iteration 11/25 | Loss: 0.00127352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012735174968838692, 0.0012735174968838692, 0.0012735174968838692, 0.0012735174968838692, 0.0012735174968838692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012735174968838692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35872757
Iteration 2/25 | Loss: 0.00069061
Iteration 3/25 | Loss: 0.00069061
Iteration 4/25 | Loss: 0.00069061
Iteration 5/25 | Loss: 0.00069061
Iteration 6/25 | Loss: 0.00069061
Iteration 7/25 | Loss: 0.00069061
Iteration 8/25 | Loss: 0.00069061
Iteration 9/25 | Loss: 0.00069061
Iteration 10/25 | Loss: 0.00069061
Iteration 11/25 | Loss: 0.00069061
Iteration 12/25 | Loss: 0.00069060
Iteration 13/25 | Loss: 0.00069060
Iteration 14/25 | Loss: 0.00069060
Iteration 15/25 | Loss: 0.00069060
Iteration 16/25 | Loss: 0.00069060
Iteration 17/25 | Loss: 0.00069060
Iteration 18/25 | Loss: 0.00069060
Iteration 19/25 | Loss: 0.00069060
Iteration 20/25 | Loss: 0.00069060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000690604851115495, 0.000690604851115495, 0.000690604851115495, 0.000690604851115495, 0.000690604851115495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000690604851115495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069060
Iteration 2/1000 | Loss: 0.00003379
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00002471
Iteration 5/1000 | Loss: 0.00002398
Iteration 6/1000 | Loss: 0.00002307
Iteration 7/1000 | Loss: 0.00002235
Iteration 8/1000 | Loss: 0.00002178
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002042
Iteration 13/1000 | Loss: 0.00002037
Iteration 14/1000 | Loss: 0.00002036
Iteration 15/1000 | Loss: 0.00002035
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002018
Iteration 18/1000 | Loss: 0.00002016
Iteration 19/1000 | Loss: 0.00002013
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002011
Iteration 22/1000 | Loss: 0.00002010
Iteration 23/1000 | Loss: 0.00002010
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002002
Iteration 32/1000 | Loss: 0.00002002
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00001999
Iteration 39/1000 | Loss: 0.00001999
Iteration 40/1000 | Loss: 0.00001999
Iteration 41/1000 | Loss: 0.00001998
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001997
Iteration 45/1000 | Loss: 0.00001997
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001996
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001992
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001992
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001992
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001991
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001991
Iteration 72/1000 | Loss: 0.00001991
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001990
Iteration 76/1000 | Loss: 0.00001990
Iteration 77/1000 | Loss: 0.00001990
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001990
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001989
Iteration 84/1000 | Loss: 0.00001989
Iteration 85/1000 | Loss: 0.00001989
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001989
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001988
Iteration 94/1000 | Loss: 0.00001988
Iteration 95/1000 | Loss: 0.00001988
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001987
Iteration 99/1000 | Loss: 0.00001987
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001986
Iteration 102/1000 | Loss: 0.00001986
Iteration 103/1000 | Loss: 0.00001986
Iteration 104/1000 | Loss: 0.00001985
Iteration 105/1000 | Loss: 0.00001985
Iteration 106/1000 | Loss: 0.00001985
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001984
Iteration 109/1000 | Loss: 0.00001984
Iteration 110/1000 | Loss: 0.00001984
Iteration 111/1000 | Loss: 0.00001984
Iteration 112/1000 | Loss: 0.00001984
Iteration 113/1000 | Loss: 0.00001983
Iteration 114/1000 | Loss: 0.00001983
Iteration 115/1000 | Loss: 0.00001983
Iteration 116/1000 | Loss: 0.00001983
Iteration 117/1000 | Loss: 0.00001983
Iteration 118/1000 | Loss: 0.00001983
Iteration 119/1000 | Loss: 0.00001983
Iteration 120/1000 | Loss: 0.00001983
Iteration 121/1000 | Loss: 0.00001983
Iteration 122/1000 | Loss: 0.00001983
Iteration 123/1000 | Loss: 0.00001983
Iteration 124/1000 | Loss: 0.00001983
Iteration 125/1000 | Loss: 0.00001983
Iteration 126/1000 | Loss: 0.00001983
Iteration 127/1000 | Loss: 0.00001982
Iteration 128/1000 | Loss: 0.00001982
Iteration 129/1000 | Loss: 0.00001982
Iteration 130/1000 | Loss: 0.00001982
Iteration 131/1000 | Loss: 0.00001982
Iteration 132/1000 | Loss: 0.00001982
Iteration 133/1000 | Loss: 0.00001982
Iteration 134/1000 | Loss: 0.00001982
Iteration 135/1000 | Loss: 0.00001982
Iteration 136/1000 | Loss: 0.00001982
Iteration 137/1000 | Loss: 0.00001982
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001982
Iteration 143/1000 | Loss: 0.00001982
Iteration 144/1000 | Loss: 0.00001982
Iteration 145/1000 | Loss: 0.00001982
Iteration 146/1000 | Loss: 0.00001982
Iteration 147/1000 | Loss: 0.00001982
Iteration 148/1000 | Loss: 0.00001981
Iteration 149/1000 | Loss: 0.00001981
Iteration 150/1000 | Loss: 0.00001981
Iteration 151/1000 | Loss: 0.00001981
Iteration 152/1000 | Loss: 0.00001981
Iteration 153/1000 | Loss: 0.00001981
Iteration 154/1000 | Loss: 0.00001981
Iteration 155/1000 | Loss: 0.00001981
Iteration 156/1000 | Loss: 0.00001981
Iteration 157/1000 | Loss: 0.00001981
Iteration 158/1000 | Loss: 0.00001981
Iteration 159/1000 | Loss: 0.00001981
Iteration 160/1000 | Loss: 0.00001981
Iteration 161/1000 | Loss: 0.00001981
Iteration 162/1000 | Loss: 0.00001981
Iteration 163/1000 | Loss: 0.00001981
Iteration 164/1000 | Loss: 0.00001981
Iteration 165/1000 | Loss: 0.00001981
Iteration 166/1000 | Loss: 0.00001981
Iteration 167/1000 | Loss: 0.00001981
Iteration 168/1000 | Loss: 0.00001981
Iteration 169/1000 | Loss: 0.00001981
Iteration 170/1000 | Loss: 0.00001981
Iteration 171/1000 | Loss: 0.00001981
Iteration 172/1000 | Loss: 0.00001981
Iteration 173/1000 | Loss: 0.00001981
Iteration 174/1000 | Loss: 0.00001981
Iteration 175/1000 | Loss: 0.00001981
Iteration 176/1000 | Loss: 0.00001981
Iteration 177/1000 | Loss: 0.00001981
Iteration 178/1000 | Loss: 0.00001981
Iteration 179/1000 | Loss: 0.00001981
Iteration 180/1000 | Loss: 0.00001981
Iteration 181/1000 | Loss: 0.00001981
Iteration 182/1000 | Loss: 0.00001981
Iteration 183/1000 | Loss: 0.00001981
Iteration 184/1000 | Loss: 0.00001981
Iteration 185/1000 | Loss: 0.00001981
Iteration 186/1000 | Loss: 0.00001981
Iteration 187/1000 | Loss: 0.00001981
Iteration 188/1000 | Loss: 0.00001981
Iteration 189/1000 | Loss: 0.00001981
Iteration 190/1000 | Loss: 0.00001981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.9807481294265017e-05, 1.9807481294265017e-05, 1.9807481294265017e-05, 1.9807481294265017e-05, 1.9807481294265017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9807481294265017e-05

Optimization complete. Final v2v error: 3.725233554840088 mm

Highest mean error: 4.095973968505859 mm for frame 67

Lowest mean error: 3.225895881652832 mm for frame 1

Saving results

Total time: 40.36332440376282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620824
Iteration 2/25 | Loss: 0.00126273
Iteration 3/25 | Loss: 0.00120017
Iteration 4/25 | Loss: 0.00118925
Iteration 5/25 | Loss: 0.00118564
Iteration 6/25 | Loss: 0.00118550
Iteration 7/25 | Loss: 0.00118550
Iteration 8/25 | Loss: 0.00118550
Iteration 9/25 | Loss: 0.00118550
Iteration 10/25 | Loss: 0.00118550
Iteration 11/25 | Loss: 0.00118550
Iteration 12/25 | Loss: 0.00118550
Iteration 13/25 | Loss: 0.00118550
Iteration 14/25 | Loss: 0.00118550
Iteration 15/25 | Loss: 0.00118550
Iteration 16/25 | Loss: 0.00118550
Iteration 17/25 | Loss: 0.00118550
Iteration 18/25 | Loss: 0.00118550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011855035554617643, 0.0011855035554617643, 0.0011855035554617643, 0.0011855035554617643, 0.0011855035554617643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011855035554617643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93198943
Iteration 2/25 | Loss: 0.00096576
Iteration 3/25 | Loss: 0.00096576
Iteration 4/25 | Loss: 0.00096576
Iteration 5/25 | Loss: 0.00096576
Iteration 6/25 | Loss: 0.00096576
Iteration 7/25 | Loss: 0.00096576
Iteration 8/25 | Loss: 0.00096576
Iteration 9/25 | Loss: 0.00096576
Iteration 10/25 | Loss: 0.00096576
Iteration 11/25 | Loss: 0.00096576
Iteration 12/25 | Loss: 0.00096576
Iteration 13/25 | Loss: 0.00096576
Iteration 14/25 | Loss: 0.00096576
Iteration 15/25 | Loss: 0.00096576
Iteration 16/25 | Loss: 0.00096576
Iteration 17/25 | Loss: 0.00096576
Iteration 18/25 | Loss: 0.00096576
Iteration 19/25 | Loss: 0.00096576
Iteration 20/25 | Loss: 0.00096576
Iteration 21/25 | Loss: 0.00096576
Iteration 22/25 | Loss: 0.00096576
Iteration 23/25 | Loss: 0.00096576
Iteration 24/25 | Loss: 0.00096576
Iteration 25/25 | Loss: 0.00096576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009657568298280239, 0.0009657568298280239, 0.0009657568298280239, 0.0009657568298280239, 0.0009657568298280239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009657568298280239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096576
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001381
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001155
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Iteration 94/1000 | Loss: 0.00001152
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001148
Iteration 107/1000 | Loss: 0.00001148
Iteration 108/1000 | Loss: 0.00001148
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001145
Iteration 124/1000 | Loss: 0.00001145
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001145
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.1449906196503434e-05, 1.1449906196503434e-05, 1.1449906196503434e-05, 1.1449906196503434e-05, 1.1449906196503434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1449906196503434e-05

Optimization complete. Final v2v error: 2.93589186668396 mm

Highest mean error: 3.3634490966796875 mm for frame 106

Lowest mean error: 2.725369691848755 mm for frame 143

Saving results

Total time: 39.65156960487366
