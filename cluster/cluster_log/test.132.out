Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=132, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7392-7447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954947
Iteration 2/25 | Loss: 0.00228672
Iteration 3/25 | Loss: 0.00148750
Iteration 4/25 | Loss: 0.00137032
Iteration 5/25 | Loss: 0.00134563
Iteration 6/25 | Loss: 0.00130675
Iteration 7/25 | Loss: 0.00130440
Iteration 8/25 | Loss: 0.00130891
Iteration 9/25 | Loss: 0.00130518
Iteration 10/25 | Loss: 0.00129688
Iteration 11/25 | Loss: 0.00130037
Iteration 12/25 | Loss: 0.00129584
Iteration 13/25 | Loss: 0.00129361
Iteration 14/25 | Loss: 0.00129217
Iteration 15/25 | Loss: 0.00129178
Iteration 16/25 | Loss: 0.00129177
Iteration 17/25 | Loss: 0.00129177
Iteration 18/25 | Loss: 0.00129177
Iteration 19/25 | Loss: 0.00129177
Iteration 20/25 | Loss: 0.00129176
Iteration 21/25 | Loss: 0.00129176
Iteration 22/25 | Loss: 0.00129176
Iteration 23/25 | Loss: 0.00129176
Iteration 24/25 | Loss: 0.00129176
Iteration 25/25 | Loss: 0.00129176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92951012
Iteration 2/25 | Loss: 0.00094410
Iteration 3/25 | Loss: 0.00092772
Iteration 4/25 | Loss: 0.00092771
Iteration 5/25 | Loss: 0.00092771
Iteration 6/25 | Loss: 0.00092771
Iteration 7/25 | Loss: 0.00092771
Iteration 8/25 | Loss: 0.00092771
Iteration 9/25 | Loss: 0.00092771
Iteration 10/25 | Loss: 0.00092771
Iteration 11/25 | Loss: 0.00092771
Iteration 12/25 | Loss: 0.00092771
Iteration 13/25 | Loss: 0.00092771
Iteration 14/25 | Loss: 0.00092771
Iteration 15/25 | Loss: 0.00092771
Iteration 16/25 | Loss: 0.00092771
Iteration 17/25 | Loss: 0.00092771
Iteration 18/25 | Loss: 0.00092771
Iteration 19/25 | Loss: 0.00092771
Iteration 20/25 | Loss: 0.00092771
Iteration 21/25 | Loss: 0.00092771
Iteration 22/25 | Loss: 0.00092771
Iteration 23/25 | Loss: 0.00092771
Iteration 24/25 | Loss: 0.00092771
Iteration 25/25 | Loss: 0.00092771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092771
Iteration 2/1000 | Loss: 0.00012027
Iteration 3/1000 | Loss: 0.00002810
Iteration 4/1000 | Loss: 0.00015129
Iteration 5/1000 | Loss: 0.00006068
Iteration 6/1000 | Loss: 0.00027030
Iteration 7/1000 | Loss: 0.00010835
Iteration 8/1000 | Loss: 0.00018653
Iteration 9/1000 | Loss: 0.00002256
Iteration 10/1000 | Loss: 0.00005554
Iteration 11/1000 | Loss: 0.00003697
Iteration 12/1000 | Loss: 0.00011672
Iteration 13/1000 | Loss: 0.00002147
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00005961
Iteration 16/1000 | Loss: 0.00004436
Iteration 17/1000 | Loss: 0.00002316
Iteration 18/1000 | Loss: 0.00007854
Iteration 19/1000 | Loss: 0.00042707
Iteration 20/1000 | Loss: 0.00004137
Iteration 21/1000 | Loss: 0.00006616
Iteration 22/1000 | Loss: 0.00005245
Iteration 23/1000 | Loss: 0.00003153
Iteration 24/1000 | Loss: 0.00002520
Iteration 25/1000 | Loss: 0.00001993
Iteration 26/1000 | Loss: 0.00002301
Iteration 27/1000 | Loss: 0.00002893
Iteration 28/1000 | Loss: 0.00001948
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001947
Iteration 31/1000 | Loss: 0.00001947
Iteration 32/1000 | Loss: 0.00001947
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001945
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001943
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001942
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00010144
Iteration 44/1000 | Loss: 0.00006002
Iteration 45/1000 | Loss: 0.00007787
Iteration 46/1000 | Loss: 0.00004574
Iteration 47/1000 | Loss: 0.00008669
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00003028
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001927
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001925
Iteration 67/1000 | Loss: 0.00001925
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00002879
Iteration 70/1000 | Loss: 0.00003553
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001923
Iteration 78/1000 | Loss: 0.00001923
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001923
Iteration 81/1000 | Loss: 0.00001923
Iteration 82/1000 | Loss: 0.00001923
Iteration 83/1000 | Loss: 0.00001923
Iteration 84/1000 | Loss: 0.00001923
Iteration 85/1000 | Loss: 0.00001923
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.922553201438859e-05, 1.922553201438859e-05, 1.922553201438859e-05, 1.922553201438859e-05, 1.922553201438859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.922553201438859e-05

Optimization complete. Final v2v error: 3.7862660884857178 mm

Highest mean error: 4.30580997467041 mm for frame 73

Lowest mean error: 3.4193201065063477 mm for frame 106

Saving results

Total time: 95.71916580200195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425036
Iteration 2/25 | Loss: 0.00129803
Iteration 3/25 | Loss: 0.00124936
Iteration 4/25 | Loss: 0.00124362
Iteration 5/25 | Loss: 0.00124167
Iteration 6/25 | Loss: 0.00124146
Iteration 7/25 | Loss: 0.00124146
Iteration 8/25 | Loss: 0.00124146
Iteration 9/25 | Loss: 0.00124146
Iteration 10/25 | Loss: 0.00124146
Iteration 11/25 | Loss: 0.00124146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012414620723575354, 0.0012414620723575354, 0.0012414620723575354, 0.0012414620723575354, 0.0012414620723575354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012414620723575354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44786119
Iteration 2/25 | Loss: 0.00082423
Iteration 3/25 | Loss: 0.00082423
Iteration 4/25 | Loss: 0.00082423
Iteration 5/25 | Loss: 0.00082423
Iteration 6/25 | Loss: 0.00082423
Iteration 7/25 | Loss: 0.00082423
Iteration 8/25 | Loss: 0.00082423
Iteration 9/25 | Loss: 0.00082423
Iteration 10/25 | Loss: 0.00082422
Iteration 11/25 | Loss: 0.00082422
Iteration 12/25 | Loss: 0.00082422
Iteration 13/25 | Loss: 0.00082422
Iteration 14/25 | Loss: 0.00082422
Iteration 15/25 | Loss: 0.00082422
Iteration 16/25 | Loss: 0.00082422
Iteration 17/25 | Loss: 0.00082422
Iteration 18/25 | Loss: 0.00082422
Iteration 19/25 | Loss: 0.00082422
Iteration 20/25 | Loss: 0.00082422
Iteration 21/25 | Loss: 0.00082422
Iteration 22/25 | Loss: 0.00082422
Iteration 23/25 | Loss: 0.00082422
Iteration 24/25 | Loss: 0.00082422
Iteration 25/25 | Loss: 0.00082422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082422
Iteration 2/1000 | Loss: 0.00002689
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001501
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001410
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001384
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001345
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001315
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001314
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001309
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001308
Iteration 60/1000 | Loss: 0.00001308
Iteration 61/1000 | Loss: 0.00001307
Iteration 62/1000 | Loss: 0.00001307
Iteration 63/1000 | Loss: 0.00001307
Iteration 64/1000 | Loss: 0.00001306
Iteration 65/1000 | Loss: 0.00001306
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001305
Iteration 69/1000 | Loss: 0.00001305
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001305
Iteration 73/1000 | Loss: 0.00001305
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001304
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001304
Iteration 84/1000 | Loss: 0.00001304
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001302
Iteration 90/1000 | Loss: 0.00001302
Iteration 91/1000 | Loss: 0.00001302
Iteration 92/1000 | Loss: 0.00001301
Iteration 93/1000 | Loss: 0.00001301
Iteration 94/1000 | Loss: 0.00001301
Iteration 95/1000 | Loss: 0.00001301
Iteration 96/1000 | Loss: 0.00001301
Iteration 97/1000 | Loss: 0.00001301
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001298
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001297
Iteration 119/1000 | Loss: 0.00001297
Iteration 120/1000 | Loss: 0.00001297
Iteration 121/1000 | Loss: 0.00001297
Iteration 122/1000 | Loss: 0.00001297
Iteration 123/1000 | Loss: 0.00001297
Iteration 124/1000 | Loss: 0.00001297
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001293
Iteration 136/1000 | Loss: 0.00001293
Iteration 137/1000 | Loss: 0.00001293
Iteration 138/1000 | Loss: 0.00001293
Iteration 139/1000 | Loss: 0.00001293
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001293
Iteration 146/1000 | Loss: 0.00001292
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001292
Iteration 149/1000 | Loss: 0.00001292
Iteration 150/1000 | Loss: 0.00001292
Iteration 151/1000 | Loss: 0.00001292
Iteration 152/1000 | Loss: 0.00001292
Iteration 153/1000 | Loss: 0.00001292
Iteration 154/1000 | Loss: 0.00001292
Iteration 155/1000 | Loss: 0.00001292
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001291
Iteration 163/1000 | Loss: 0.00001291
Iteration 164/1000 | Loss: 0.00001291
Iteration 165/1000 | Loss: 0.00001291
Iteration 166/1000 | Loss: 0.00001291
Iteration 167/1000 | Loss: 0.00001291
Iteration 168/1000 | Loss: 0.00001291
Iteration 169/1000 | Loss: 0.00001291
Iteration 170/1000 | Loss: 0.00001291
Iteration 171/1000 | Loss: 0.00001290
Iteration 172/1000 | Loss: 0.00001290
Iteration 173/1000 | Loss: 0.00001290
Iteration 174/1000 | Loss: 0.00001290
Iteration 175/1000 | Loss: 0.00001290
Iteration 176/1000 | Loss: 0.00001290
Iteration 177/1000 | Loss: 0.00001290
Iteration 178/1000 | Loss: 0.00001290
Iteration 179/1000 | Loss: 0.00001290
Iteration 180/1000 | Loss: 0.00001290
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001290
Iteration 183/1000 | Loss: 0.00001290
Iteration 184/1000 | Loss: 0.00001290
Iteration 185/1000 | Loss: 0.00001290
Iteration 186/1000 | Loss: 0.00001290
Iteration 187/1000 | Loss: 0.00001290
Iteration 188/1000 | Loss: 0.00001289
Iteration 189/1000 | Loss: 0.00001289
Iteration 190/1000 | Loss: 0.00001289
Iteration 191/1000 | Loss: 0.00001289
Iteration 192/1000 | Loss: 0.00001289
Iteration 193/1000 | Loss: 0.00001289
Iteration 194/1000 | Loss: 0.00001289
Iteration 195/1000 | Loss: 0.00001289
Iteration 196/1000 | Loss: 0.00001289
Iteration 197/1000 | Loss: 0.00001289
Iteration 198/1000 | Loss: 0.00001289
Iteration 199/1000 | Loss: 0.00001289
Iteration 200/1000 | Loss: 0.00001289
Iteration 201/1000 | Loss: 0.00001289
Iteration 202/1000 | Loss: 0.00001289
Iteration 203/1000 | Loss: 0.00001289
Iteration 204/1000 | Loss: 0.00001289
Iteration 205/1000 | Loss: 0.00001289
Iteration 206/1000 | Loss: 0.00001289
Iteration 207/1000 | Loss: 0.00001289
Iteration 208/1000 | Loss: 0.00001289
Iteration 209/1000 | Loss: 0.00001289
Iteration 210/1000 | Loss: 0.00001289
Iteration 211/1000 | Loss: 0.00001289
Iteration 212/1000 | Loss: 0.00001289
Iteration 213/1000 | Loss: 0.00001289
Iteration 214/1000 | Loss: 0.00001289
Iteration 215/1000 | Loss: 0.00001289
Iteration 216/1000 | Loss: 0.00001289
Iteration 217/1000 | Loss: 0.00001289
Iteration 218/1000 | Loss: 0.00001289
Iteration 219/1000 | Loss: 0.00001289
Iteration 220/1000 | Loss: 0.00001289
Iteration 221/1000 | Loss: 0.00001289
Iteration 222/1000 | Loss: 0.00001289
Iteration 223/1000 | Loss: 0.00001289
Iteration 224/1000 | Loss: 0.00001289
Iteration 225/1000 | Loss: 0.00001289
Iteration 226/1000 | Loss: 0.00001289
Iteration 227/1000 | Loss: 0.00001289
Iteration 228/1000 | Loss: 0.00001289
Iteration 229/1000 | Loss: 0.00001289
Iteration 230/1000 | Loss: 0.00001289
Iteration 231/1000 | Loss: 0.00001289
Iteration 232/1000 | Loss: 0.00001289
Iteration 233/1000 | Loss: 0.00001289
Iteration 234/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.2887569937447552e-05, 1.2887569937447552e-05, 1.2887569937447552e-05, 1.2887569937447552e-05, 1.2887569937447552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2887569937447552e-05

Optimization complete. Final v2v error: 3.0757548809051514 mm

Highest mean error: 3.15740704536438 mm for frame 39

Lowest mean error: 2.982236385345459 mm for frame 127

Saving results

Total time: 39.09936833381653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397465
Iteration 2/25 | Loss: 0.00123329
Iteration 3/25 | Loss: 0.00119444
Iteration 4/25 | Loss: 0.00118921
Iteration 5/25 | Loss: 0.00118748
Iteration 6/25 | Loss: 0.00118739
Iteration 7/25 | Loss: 0.00118739
Iteration 8/25 | Loss: 0.00118739
Iteration 9/25 | Loss: 0.00118739
Iteration 10/25 | Loss: 0.00118739
Iteration 11/25 | Loss: 0.00118739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011873884359374642, 0.0011873884359374642, 0.0011873884359374642, 0.0011873884359374642, 0.0011873884359374642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011873884359374642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42476368
Iteration 2/25 | Loss: 0.00062904
Iteration 3/25 | Loss: 0.00062904
Iteration 4/25 | Loss: 0.00062903
Iteration 5/25 | Loss: 0.00062903
Iteration 6/25 | Loss: 0.00062903
Iteration 7/25 | Loss: 0.00062903
Iteration 8/25 | Loss: 0.00062903
Iteration 9/25 | Loss: 0.00062903
Iteration 10/25 | Loss: 0.00062903
Iteration 11/25 | Loss: 0.00062903
Iteration 12/25 | Loss: 0.00062903
Iteration 13/25 | Loss: 0.00062903
Iteration 14/25 | Loss: 0.00062903
Iteration 15/25 | Loss: 0.00062903
Iteration 16/25 | Loss: 0.00062903
Iteration 17/25 | Loss: 0.00062903
Iteration 18/25 | Loss: 0.00062903
Iteration 19/25 | Loss: 0.00062903
Iteration 20/25 | Loss: 0.00062903
Iteration 21/25 | Loss: 0.00062903
Iteration 22/25 | Loss: 0.00062903
Iteration 23/25 | Loss: 0.00062903
Iteration 24/25 | Loss: 0.00062903
Iteration 25/25 | Loss: 0.00062903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062903
Iteration 2/1000 | Loss: 0.00002479
Iteration 3/1000 | Loss: 0.00001737
Iteration 4/1000 | Loss: 0.00001582
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001195
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001179
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001178
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001177
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001159
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001156
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001154
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001141
Iteration 145/1000 | Loss: 0.00001141
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001141
Iteration 148/1000 | Loss: 0.00001140
Iteration 149/1000 | Loss: 0.00001140
Iteration 150/1000 | Loss: 0.00001140
Iteration 151/1000 | Loss: 0.00001140
Iteration 152/1000 | Loss: 0.00001140
Iteration 153/1000 | Loss: 0.00001140
Iteration 154/1000 | Loss: 0.00001140
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001139
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001138
Iteration 172/1000 | Loss: 0.00001138
Iteration 173/1000 | Loss: 0.00001138
Iteration 174/1000 | Loss: 0.00001138
Iteration 175/1000 | Loss: 0.00001138
Iteration 176/1000 | Loss: 0.00001138
Iteration 177/1000 | Loss: 0.00001138
Iteration 178/1000 | Loss: 0.00001138
Iteration 179/1000 | Loss: 0.00001138
Iteration 180/1000 | Loss: 0.00001138
Iteration 181/1000 | Loss: 0.00001138
Iteration 182/1000 | Loss: 0.00001138
Iteration 183/1000 | Loss: 0.00001138
Iteration 184/1000 | Loss: 0.00001138
Iteration 185/1000 | Loss: 0.00001138
Iteration 186/1000 | Loss: 0.00001137
Iteration 187/1000 | Loss: 0.00001137
Iteration 188/1000 | Loss: 0.00001137
Iteration 189/1000 | Loss: 0.00001137
Iteration 190/1000 | Loss: 0.00001137
Iteration 191/1000 | Loss: 0.00001137
Iteration 192/1000 | Loss: 0.00001137
Iteration 193/1000 | Loss: 0.00001137
Iteration 194/1000 | Loss: 0.00001137
Iteration 195/1000 | Loss: 0.00001137
Iteration 196/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.1370588254067115e-05, 1.1370588254067115e-05, 1.1370588254067115e-05, 1.1370588254067115e-05, 1.1370588254067115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1370588254067115e-05

Optimization complete. Final v2v error: 2.887545347213745 mm

Highest mean error: 3.0651369094848633 mm for frame 138

Lowest mean error: 2.7993874549865723 mm for frame 40

Saving results

Total time: 39.28312540054321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994659
Iteration 2/25 | Loss: 0.00248863
Iteration 3/25 | Loss: 0.00207917
Iteration 4/25 | Loss: 0.00197486
Iteration 5/25 | Loss: 0.00183889
Iteration 6/25 | Loss: 0.00159522
Iteration 7/25 | Loss: 0.00140183
Iteration 8/25 | Loss: 0.00137823
Iteration 9/25 | Loss: 0.00137556
Iteration 10/25 | Loss: 0.00137510
Iteration 11/25 | Loss: 0.00137510
Iteration 12/25 | Loss: 0.00137510
Iteration 13/25 | Loss: 0.00137510
Iteration 14/25 | Loss: 0.00137510
Iteration 15/25 | Loss: 0.00137510
Iteration 16/25 | Loss: 0.00137510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013750962680205703, 0.0013750962680205703, 0.0013750962680205703, 0.0013750962680205703, 0.0013750962680205703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013750962680205703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42067838
Iteration 2/25 | Loss: 0.00067523
Iteration 3/25 | Loss: 0.00067522
Iteration 4/25 | Loss: 0.00067522
Iteration 5/25 | Loss: 0.00067522
Iteration 6/25 | Loss: 0.00067522
Iteration 7/25 | Loss: 0.00067522
Iteration 8/25 | Loss: 0.00067522
Iteration 9/25 | Loss: 0.00067522
Iteration 10/25 | Loss: 0.00067522
Iteration 11/25 | Loss: 0.00067522
Iteration 12/25 | Loss: 0.00067522
Iteration 13/25 | Loss: 0.00067522
Iteration 14/25 | Loss: 0.00067522
Iteration 15/25 | Loss: 0.00067522
Iteration 16/25 | Loss: 0.00067522
Iteration 17/25 | Loss: 0.00067522
Iteration 18/25 | Loss: 0.00067522
Iteration 19/25 | Loss: 0.00067522
Iteration 20/25 | Loss: 0.00067522
Iteration 21/25 | Loss: 0.00067522
Iteration 22/25 | Loss: 0.00067522
Iteration 23/25 | Loss: 0.00067522
Iteration 24/25 | Loss: 0.00067522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006752209737896919, 0.0006752209737896919, 0.0006752209737896919, 0.0006752209737896919, 0.0006752209737896919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006752209737896919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067522
Iteration 2/1000 | Loss: 0.00004690
Iteration 3/1000 | Loss: 0.00003333
Iteration 4/1000 | Loss: 0.00003176
Iteration 5/1000 | Loss: 0.00003043
Iteration 6/1000 | Loss: 0.00002977
Iteration 7/1000 | Loss: 0.00002920
Iteration 8/1000 | Loss: 0.00002888
Iteration 9/1000 | Loss: 0.00002860
Iteration 10/1000 | Loss: 0.00002843
Iteration 11/1000 | Loss: 0.00002840
Iteration 12/1000 | Loss: 0.00002839
Iteration 13/1000 | Loss: 0.00002836
Iteration 14/1000 | Loss: 0.00002835
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002819
Iteration 17/1000 | Loss: 0.00002818
Iteration 18/1000 | Loss: 0.00002818
Iteration 19/1000 | Loss: 0.00002818
Iteration 20/1000 | Loss: 0.00002817
Iteration 21/1000 | Loss: 0.00002804
Iteration 22/1000 | Loss: 0.00002801
Iteration 23/1000 | Loss: 0.00002799
Iteration 24/1000 | Loss: 0.00002798
Iteration 25/1000 | Loss: 0.00002798
Iteration 26/1000 | Loss: 0.00002798
Iteration 27/1000 | Loss: 0.00002798
Iteration 28/1000 | Loss: 0.00002798
Iteration 29/1000 | Loss: 0.00002798
Iteration 30/1000 | Loss: 0.00002793
Iteration 31/1000 | Loss: 0.00002793
Iteration 32/1000 | Loss: 0.00002793
Iteration 33/1000 | Loss: 0.00002793
Iteration 34/1000 | Loss: 0.00002793
Iteration 35/1000 | Loss: 0.00002792
Iteration 36/1000 | Loss: 0.00002790
Iteration 37/1000 | Loss: 0.00002790
Iteration 38/1000 | Loss: 0.00002786
Iteration 39/1000 | Loss: 0.00002782
Iteration 40/1000 | Loss: 0.00002782
Iteration 41/1000 | Loss: 0.00002781
Iteration 42/1000 | Loss: 0.00002781
Iteration 43/1000 | Loss: 0.00002781
Iteration 44/1000 | Loss: 0.00002780
Iteration 45/1000 | Loss: 0.00002780
Iteration 46/1000 | Loss: 0.00002780
Iteration 47/1000 | Loss: 0.00002780
Iteration 48/1000 | Loss: 0.00002780
Iteration 49/1000 | Loss: 0.00002779
Iteration 50/1000 | Loss: 0.00002779
Iteration 51/1000 | Loss: 0.00002779
Iteration 52/1000 | Loss: 0.00002779
Iteration 53/1000 | Loss: 0.00002779
Iteration 54/1000 | Loss: 0.00002779
Iteration 55/1000 | Loss: 0.00002779
Iteration 56/1000 | Loss: 0.00002779
Iteration 57/1000 | Loss: 0.00002779
Iteration 58/1000 | Loss: 0.00002778
Iteration 59/1000 | Loss: 0.00002778
Iteration 60/1000 | Loss: 0.00002778
Iteration 61/1000 | Loss: 0.00002778
Iteration 62/1000 | Loss: 0.00002778
Iteration 63/1000 | Loss: 0.00002778
Iteration 64/1000 | Loss: 0.00002778
Iteration 65/1000 | Loss: 0.00002778
Iteration 66/1000 | Loss: 0.00002777
Iteration 67/1000 | Loss: 0.00002777
Iteration 68/1000 | Loss: 0.00002777
Iteration 69/1000 | Loss: 0.00002776
Iteration 70/1000 | Loss: 0.00002776
Iteration 71/1000 | Loss: 0.00002776
Iteration 72/1000 | Loss: 0.00002776
Iteration 73/1000 | Loss: 0.00002776
Iteration 74/1000 | Loss: 0.00002776
Iteration 75/1000 | Loss: 0.00002776
Iteration 76/1000 | Loss: 0.00002776
Iteration 77/1000 | Loss: 0.00002776
Iteration 78/1000 | Loss: 0.00002776
Iteration 79/1000 | Loss: 0.00002776
Iteration 80/1000 | Loss: 0.00002776
Iteration 81/1000 | Loss: 0.00002776
Iteration 82/1000 | Loss: 0.00002775
Iteration 83/1000 | Loss: 0.00002775
Iteration 84/1000 | Loss: 0.00002775
Iteration 85/1000 | Loss: 0.00002775
Iteration 86/1000 | Loss: 0.00002775
Iteration 87/1000 | Loss: 0.00002775
Iteration 88/1000 | Loss: 0.00002775
Iteration 89/1000 | Loss: 0.00002775
Iteration 90/1000 | Loss: 0.00002774
Iteration 91/1000 | Loss: 0.00002774
Iteration 92/1000 | Loss: 0.00002774
Iteration 93/1000 | Loss: 0.00002774
Iteration 94/1000 | Loss: 0.00002774
Iteration 95/1000 | Loss: 0.00002774
Iteration 96/1000 | Loss: 0.00002773
Iteration 97/1000 | Loss: 0.00002773
Iteration 98/1000 | Loss: 0.00002773
Iteration 99/1000 | Loss: 0.00002773
Iteration 100/1000 | Loss: 0.00002773
Iteration 101/1000 | Loss: 0.00002773
Iteration 102/1000 | Loss: 0.00002773
Iteration 103/1000 | Loss: 0.00002773
Iteration 104/1000 | Loss: 0.00002773
Iteration 105/1000 | Loss: 0.00002773
Iteration 106/1000 | Loss: 0.00002773
Iteration 107/1000 | Loss: 0.00002773
Iteration 108/1000 | Loss: 0.00002773
Iteration 109/1000 | Loss: 0.00002773
Iteration 110/1000 | Loss: 0.00002773
Iteration 111/1000 | Loss: 0.00002773
Iteration 112/1000 | Loss: 0.00002773
Iteration 113/1000 | Loss: 0.00002773
Iteration 114/1000 | Loss: 0.00002773
Iteration 115/1000 | Loss: 0.00002773
Iteration 116/1000 | Loss: 0.00002773
Iteration 117/1000 | Loss: 0.00002773
Iteration 118/1000 | Loss: 0.00002773
Iteration 119/1000 | Loss: 0.00002773
Iteration 120/1000 | Loss: 0.00002773
Iteration 121/1000 | Loss: 0.00002773
Iteration 122/1000 | Loss: 0.00002773
Iteration 123/1000 | Loss: 0.00002773
Iteration 124/1000 | Loss: 0.00002773
Iteration 125/1000 | Loss: 0.00002773
Iteration 126/1000 | Loss: 0.00002773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.773001506284345e-05, 2.773001506284345e-05, 2.773001506284345e-05, 2.773001506284345e-05, 2.773001506284345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.773001506284345e-05

Optimization complete. Final v2v error: 4.525918006896973 mm

Highest mean error: 4.64749813079834 mm for frame 40

Lowest mean error: 4.41253662109375 mm for frame 183

Saving results

Total time: 47.97895383834839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776910
Iteration 2/25 | Loss: 0.00157513
Iteration 3/25 | Loss: 0.00132008
Iteration 4/25 | Loss: 0.00128578
Iteration 5/25 | Loss: 0.00127941
Iteration 6/25 | Loss: 0.00127789
Iteration 7/25 | Loss: 0.00127789
Iteration 8/25 | Loss: 0.00127789
Iteration 9/25 | Loss: 0.00127789
Iteration 10/25 | Loss: 0.00127789
Iteration 11/25 | Loss: 0.00127789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012778869131579995, 0.0012778869131579995, 0.0012778869131579995, 0.0012778869131579995, 0.0012778869131579995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012778869131579995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41555119
Iteration 2/25 | Loss: 0.00068072
Iteration 3/25 | Loss: 0.00068071
Iteration 4/25 | Loss: 0.00068071
Iteration 5/25 | Loss: 0.00068071
Iteration 6/25 | Loss: 0.00068071
Iteration 7/25 | Loss: 0.00068071
Iteration 8/25 | Loss: 0.00068071
Iteration 9/25 | Loss: 0.00068071
Iteration 10/25 | Loss: 0.00068071
Iteration 11/25 | Loss: 0.00068071
Iteration 12/25 | Loss: 0.00068071
Iteration 13/25 | Loss: 0.00068071
Iteration 14/25 | Loss: 0.00068071
Iteration 15/25 | Loss: 0.00068071
Iteration 16/25 | Loss: 0.00068071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006807107129134238, 0.0006807107129134238, 0.0006807107129134238, 0.0006807107129134238, 0.0006807107129134238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006807107129134238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068071
Iteration 2/1000 | Loss: 0.00003828
Iteration 3/1000 | Loss: 0.00002793
Iteration 4/1000 | Loss: 0.00002463
Iteration 5/1000 | Loss: 0.00002337
Iteration 6/1000 | Loss: 0.00002216
Iteration 7/1000 | Loss: 0.00002154
Iteration 8/1000 | Loss: 0.00002108
Iteration 9/1000 | Loss: 0.00002061
Iteration 10/1000 | Loss: 0.00002021
Iteration 11/1000 | Loss: 0.00002020
Iteration 12/1000 | Loss: 0.00002001
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001984
Iteration 15/1000 | Loss: 0.00001978
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001956
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001953
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001951
Iteration 28/1000 | Loss: 0.00001950
Iteration 29/1000 | Loss: 0.00001950
Iteration 30/1000 | Loss: 0.00001950
Iteration 31/1000 | Loss: 0.00001949
Iteration 32/1000 | Loss: 0.00001949
Iteration 33/1000 | Loss: 0.00001949
Iteration 34/1000 | Loss: 0.00001948
Iteration 35/1000 | Loss: 0.00001948
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001944
Iteration 49/1000 | Loss: 0.00001943
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001941
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001938
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001936
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001936
Iteration 71/1000 | Loss: 0.00001935
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001933
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001932
Iteration 93/1000 | Loss: 0.00001932
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001928
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001927
Iteration 104/1000 | Loss: 0.00001927
Iteration 105/1000 | Loss: 0.00001927
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001926
Iteration 113/1000 | Loss: 0.00001926
Iteration 114/1000 | Loss: 0.00001926
Iteration 115/1000 | Loss: 0.00001926
Iteration 116/1000 | Loss: 0.00001926
Iteration 117/1000 | Loss: 0.00001926
Iteration 118/1000 | Loss: 0.00001925
Iteration 119/1000 | Loss: 0.00001925
Iteration 120/1000 | Loss: 0.00001925
Iteration 121/1000 | Loss: 0.00001925
Iteration 122/1000 | Loss: 0.00001925
Iteration 123/1000 | Loss: 0.00001925
Iteration 124/1000 | Loss: 0.00001925
Iteration 125/1000 | Loss: 0.00001924
Iteration 126/1000 | Loss: 0.00001924
Iteration 127/1000 | Loss: 0.00001924
Iteration 128/1000 | Loss: 0.00001924
Iteration 129/1000 | Loss: 0.00001924
Iteration 130/1000 | Loss: 0.00001924
Iteration 131/1000 | Loss: 0.00001924
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001923
Iteration 136/1000 | Loss: 0.00001923
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001923
Iteration 139/1000 | Loss: 0.00001923
Iteration 140/1000 | Loss: 0.00001923
Iteration 141/1000 | Loss: 0.00001922
Iteration 142/1000 | Loss: 0.00001922
Iteration 143/1000 | Loss: 0.00001922
Iteration 144/1000 | Loss: 0.00001922
Iteration 145/1000 | Loss: 0.00001922
Iteration 146/1000 | Loss: 0.00001922
Iteration 147/1000 | Loss: 0.00001922
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001922
Iteration 151/1000 | Loss: 0.00001921
Iteration 152/1000 | Loss: 0.00001921
Iteration 153/1000 | Loss: 0.00001921
Iteration 154/1000 | Loss: 0.00001921
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001921
Iteration 159/1000 | Loss: 0.00001921
Iteration 160/1000 | Loss: 0.00001921
Iteration 161/1000 | Loss: 0.00001920
Iteration 162/1000 | Loss: 0.00001920
Iteration 163/1000 | Loss: 0.00001920
Iteration 164/1000 | Loss: 0.00001920
Iteration 165/1000 | Loss: 0.00001920
Iteration 166/1000 | Loss: 0.00001920
Iteration 167/1000 | Loss: 0.00001920
Iteration 168/1000 | Loss: 0.00001920
Iteration 169/1000 | Loss: 0.00001920
Iteration 170/1000 | Loss: 0.00001920
Iteration 171/1000 | Loss: 0.00001920
Iteration 172/1000 | Loss: 0.00001920
Iteration 173/1000 | Loss: 0.00001920
Iteration 174/1000 | Loss: 0.00001920
Iteration 175/1000 | Loss: 0.00001920
Iteration 176/1000 | Loss: 0.00001920
Iteration 177/1000 | Loss: 0.00001920
Iteration 178/1000 | Loss: 0.00001920
Iteration 179/1000 | Loss: 0.00001920
Iteration 180/1000 | Loss: 0.00001920
Iteration 181/1000 | Loss: 0.00001920
Iteration 182/1000 | Loss: 0.00001920
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.9201741451979615e-05, 1.9201741451979615e-05, 1.9201741451979615e-05, 1.9201741451979615e-05, 1.9201741451979615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9201741451979615e-05

Optimization complete. Final v2v error: 3.691377878189087 mm

Highest mean error: 4.18095064163208 mm for frame 184

Lowest mean error: 3.418895959854126 mm for frame 90

Saving results

Total time: 45.488096714019775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904670
Iteration 2/25 | Loss: 0.00231087
Iteration 3/25 | Loss: 0.00186309
Iteration 4/25 | Loss: 0.00182026
Iteration 5/25 | Loss: 0.00176123
Iteration 6/25 | Loss: 0.00166904
Iteration 7/25 | Loss: 0.00160419
Iteration 8/25 | Loss: 0.00158412
Iteration 9/25 | Loss: 0.00155774
Iteration 10/25 | Loss: 0.00153195
Iteration 11/25 | Loss: 0.00152780
Iteration 12/25 | Loss: 0.00152830
Iteration 13/25 | Loss: 0.00151998
Iteration 14/25 | Loss: 0.00151759
Iteration 15/25 | Loss: 0.00151755
Iteration 16/25 | Loss: 0.00150477
Iteration 17/25 | Loss: 0.00149323
Iteration 18/25 | Loss: 0.00147888
Iteration 19/25 | Loss: 0.00147813
Iteration 20/25 | Loss: 0.00147446
Iteration 21/25 | Loss: 0.00147646
Iteration 22/25 | Loss: 0.00148121
Iteration 23/25 | Loss: 0.00147413
Iteration 24/25 | Loss: 0.00147519
Iteration 25/25 | Loss: 0.00147587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89327252
Iteration 2/25 | Loss: 0.00223031
Iteration 3/25 | Loss: 0.00215782
Iteration 4/25 | Loss: 0.00215780
Iteration 5/25 | Loss: 0.00215780
Iteration 6/25 | Loss: 0.00215780
Iteration 7/25 | Loss: 0.00215780
Iteration 8/25 | Loss: 0.00215780
Iteration 9/25 | Loss: 0.00215780
Iteration 10/25 | Loss: 0.00215780
Iteration 11/25 | Loss: 0.00215780
Iteration 12/25 | Loss: 0.00215780
Iteration 13/25 | Loss: 0.00215780
Iteration 14/25 | Loss: 0.00215780
Iteration 15/25 | Loss: 0.00215780
Iteration 16/25 | Loss: 0.00215780
Iteration 17/25 | Loss: 0.00215780
Iteration 18/25 | Loss: 0.00215780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002157800365239382, 0.002157800365239382, 0.002157800365239382, 0.002157800365239382, 0.002157800365239382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002157800365239382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215780
Iteration 2/1000 | Loss: 0.00040427
Iteration 3/1000 | Loss: 0.00038948
Iteration 4/1000 | Loss: 0.00042875
Iteration 5/1000 | Loss: 0.00018904
Iteration 6/1000 | Loss: 0.00012071
Iteration 7/1000 | Loss: 0.00011988
Iteration 8/1000 | Loss: 0.00011997
Iteration 9/1000 | Loss: 0.00010243
Iteration 10/1000 | Loss: 0.00014409
Iteration 11/1000 | Loss: 0.00022099
Iteration 12/1000 | Loss: 0.00011205
Iteration 13/1000 | Loss: 0.00018052
Iteration 14/1000 | Loss: 0.00024404
Iteration 15/1000 | Loss: 0.00007406
Iteration 16/1000 | Loss: 0.00006900
Iteration 17/1000 | Loss: 0.00006992
Iteration 18/1000 | Loss: 0.00007264
Iteration 19/1000 | Loss: 0.00018862
Iteration 20/1000 | Loss: 0.00007573
Iteration 21/1000 | Loss: 0.00024186
Iteration 22/1000 | Loss: 0.00035244
Iteration 23/1000 | Loss: 0.00016305
Iteration 24/1000 | Loss: 0.00020584
Iteration 25/1000 | Loss: 0.00020243
Iteration 26/1000 | Loss: 0.00007014
Iteration 27/1000 | Loss: 0.00006568
Iteration 28/1000 | Loss: 0.00021978
Iteration 29/1000 | Loss: 0.00017862
Iteration 30/1000 | Loss: 0.00022220
Iteration 31/1000 | Loss: 0.00024428
Iteration 32/1000 | Loss: 0.00008212
Iteration 33/1000 | Loss: 0.00006689
Iteration 34/1000 | Loss: 0.00007418
Iteration 35/1000 | Loss: 0.00007570
Iteration 36/1000 | Loss: 0.00007542
Iteration 37/1000 | Loss: 0.00007237
Iteration 38/1000 | Loss: 0.00008144
Iteration 39/1000 | Loss: 0.00026205
Iteration 40/1000 | Loss: 0.00025491
Iteration 41/1000 | Loss: 0.00014362
Iteration 42/1000 | Loss: 0.00007420
Iteration 43/1000 | Loss: 0.00007079
Iteration 44/1000 | Loss: 0.00007071
Iteration 45/1000 | Loss: 0.00006890
Iteration 46/1000 | Loss: 0.00006548
Iteration 47/1000 | Loss: 0.00032018
Iteration 48/1000 | Loss: 0.00009011
Iteration 49/1000 | Loss: 0.00007571
Iteration 50/1000 | Loss: 0.00006219
Iteration 51/1000 | Loss: 0.00006793
Iteration 52/1000 | Loss: 0.00006336
Iteration 53/1000 | Loss: 0.00007095
Iteration 54/1000 | Loss: 0.00006244
Iteration 55/1000 | Loss: 0.00007131
Iteration 56/1000 | Loss: 0.00006141
Iteration 57/1000 | Loss: 0.00006397
Iteration 58/1000 | Loss: 0.00005894
Iteration 59/1000 | Loss: 0.00005880
Iteration 60/1000 | Loss: 0.00006429
Iteration 61/1000 | Loss: 0.00006624
Iteration 62/1000 | Loss: 0.00006413
Iteration 63/1000 | Loss: 0.00006482
Iteration 64/1000 | Loss: 0.00006688
Iteration 65/1000 | Loss: 0.00006669
Iteration 66/1000 | Loss: 0.00005908
Iteration 67/1000 | Loss: 0.00006718
Iteration 68/1000 | Loss: 0.00006588
Iteration 69/1000 | Loss: 0.00007568
Iteration 70/1000 | Loss: 0.00006557
Iteration 71/1000 | Loss: 0.00006686
Iteration 72/1000 | Loss: 0.00006077
Iteration 73/1000 | Loss: 0.00006756
Iteration 74/1000 | Loss: 0.00006001
Iteration 75/1000 | Loss: 0.00006627
Iteration 76/1000 | Loss: 0.00006328
Iteration 77/1000 | Loss: 0.00006650
Iteration 78/1000 | Loss: 0.00006298
Iteration 79/1000 | Loss: 0.00006610
Iteration 80/1000 | Loss: 0.00006337
Iteration 81/1000 | Loss: 0.00006613
Iteration 82/1000 | Loss: 0.00006574
Iteration 83/1000 | Loss: 0.00006667
Iteration 84/1000 | Loss: 0.00006487
Iteration 85/1000 | Loss: 0.00006609
Iteration 86/1000 | Loss: 0.00006398
Iteration 87/1000 | Loss: 0.00006561
Iteration 88/1000 | Loss: 0.00007125
Iteration 89/1000 | Loss: 0.00006520
Iteration 90/1000 | Loss: 0.00007117
Iteration 91/1000 | Loss: 0.00006722
Iteration 92/1000 | Loss: 0.00006205
Iteration 93/1000 | Loss: 0.00006532
Iteration 94/1000 | Loss: 0.00006532
Iteration 95/1000 | Loss: 0.00006555
Iteration 96/1000 | Loss: 0.00006156
Iteration 97/1000 | Loss: 0.00006608
Iteration 98/1000 | Loss: 0.00006328
Iteration 99/1000 | Loss: 0.00006786
Iteration 100/1000 | Loss: 0.00006592
Iteration 101/1000 | Loss: 0.00006651
Iteration 102/1000 | Loss: 0.00006290
Iteration 103/1000 | Loss: 0.00006581
Iteration 104/1000 | Loss: 0.00006505
Iteration 105/1000 | Loss: 0.00006563
Iteration 106/1000 | Loss: 0.00006438
Iteration 107/1000 | Loss: 0.00006694
Iteration 108/1000 | Loss: 0.00006824
Iteration 109/1000 | Loss: 0.00006804
Iteration 110/1000 | Loss: 0.00006711
Iteration 111/1000 | Loss: 0.00007854
Iteration 112/1000 | Loss: 0.00006755
Iteration 113/1000 | Loss: 0.00006611
Iteration 114/1000 | Loss: 0.00006333
Iteration 115/1000 | Loss: 0.00006708
Iteration 116/1000 | Loss: 0.00006751
Iteration 117/1000 | Loss: 0.00006647
Iteration 118/1000 | Loss: 0.00006580
Iteration 119/1000 | Loss: 0.00006637
Iteration 120/1000 | Loss: 0.00006895
Iteration 121/1000 | Loss: 0.00006480
Iteration 122/1000 | Loss: 0.00006539
Iteration 123/1000 | Loss: 0.00006644
Iteration 124/1000 | Loss: 0.00006929
Iteration 125/1000 | Loss: 0.00006498
Iteration 126/1000 | Loss: 0.00007034
Iteration 127/1000 | Loss: 0.00005976
Iteration 128/1000 | Loss: 0.00006589
Iteration 129/1000 | Loss: 0.00006542
Iteration 130/1000 | Loss: 0.00006618
Iteration 131/1000 | Loss: 0.00006477
Iteration 132/1000 | Loss: 0.00007037
Iteration 133/1000 | Loss: 0.00006808
Iteration 134/1000 | Loss: 0.00006612
Iteration 135/1000 | Loss: 0.00006444
Iteration 136/1000 | Loss: 0.00006731
Iteration 137/1000 | Loss: 0.00006704
Iteration 138/1000 | Loss: 0.00006602
Iteration 139/1000 | Loss: 0.00006548
Iteration 140/1000 | Loss: 0.00006643
Iteration 141/1000 | Loss: 0.00006528
Iteration 142/1000 | Loss: 0.00006539
Iteration 143/1000 | Loss: 0.00006317
Iteration 144/1000 | Loss: 0.00006718
Iteration 145/1000 | Loss: 0.00006314
Iteration 146/1000 | Loss: 0.00006592
Iteration 147/1000 | Loss: 0.00006912
Iteration 148/1000 | Loss: 0.00006873
Iteration 149/1000 | Loss: 0.00007928
Iteration 150/1000 | Loss: 0.00006095
Iteration 151/1000 | Loss: 0.00006863
Iteration 152/1000 | Loss: 0.00006525
Iteration 153/1000 | Loss: 0.00006809
Iteration 154/1000 | Loss: 0.00005841
Iteration 155/1000 | Loss: 0.00006741
Iteration 156/1000 | Loss: 0.00006448
Iteration 157/1000 | Loss: 0.00006766
Iteration 158/1000 | Loss: 0.00006771
Iteration 159/1000 | Loss: 0.00006168
Iteration 160/1000 | Loss: 0.00006194
Iteration 161/1000 | Loss: 0.00006437
Iteration 162/1000 | Loss: 0.00005938
Iteration 163/1000 | Loss: 0.00006622
Iteration 164/1000 | Loss: 0.00006414
Iteration 165/1000 | Loss: 0.00006479
Iteration 166/1000 | Loss: 0.00006839
Iteration 167/1000 | Loss: 0.00006569
Iteration 168/1000 | Loss: 0.00006274
Iteration 169/1000 | Loss: 0.00006592
Iteration 170/1000 | Loss: 0.00006654
Iteration 171/1000 | Loss: 0.00007750
Iteration 172/1000 | Loss: 0.00006672
Iteration 173/1000 | Loss: 0.00007073
Iteration 174/1000 | Loss: 0.00006657
Iteration 175/1000 | Loss: 0.00006299
Iteration 176/1000 | Loss: 0.00006559
Iteration 177/1000 | Loss: 0.00007373
Iteration 178/1000 | Loss: 0.00006915
Iteration 179/1000 | Loss: 0.00006697
Iteration 180/1000 | Loss: 0.00007824
Iteration 181/1000 | Loss: 0.00006636
Iteration 182/1000 | Loss: 0.00006780
Iteration 183/1000 | Loss: 0.00006681
Iteration 184/1000 | Loss: 0.00007898
Iteration 185/1000 | Loss: 0.00006726
Iteration 186/1000 | Loss: 0.00007263
Iteration 187/1000 | Loss: 0.00006598
Iteration 188/1000 | Loss: 0.00007198
Iteration 189/1000 | Loss: 0.00006524
Iteration 190/1000 | Loss: 0.00007286
Iteration 191/1000 | Loss: 0.00007410
Iteration 192/1000 | Loss: 0.00006726
Iteration 193/1000 | Loss: 0.00007158
Iteration 194/1000 | Loss: 0.00006475
Iteration 195/1000 | Loss: 0.00007032
Iteration 196/1000 | Loss: 0.00006535
Iteration 197/1000 | Loss: 0.00007263
Iteration 198/1000 | Loss: 0.00006550
Iteration 199/1000 | Loss: 0.00006499
Iteration 200/1000 | Loss: 0.00006908
Iteration 201/1000 | Loss: 0.00005685
Iteration 202/1000 | Loss: 0.00005547
Iteration 203/1000 | Loss: 0.00005483
Iteration 204/1000 | Loss: 0.00005429
Iteration 205/1000 | Loss: 0.00005381
Iteration 206/1000 | Loss: 0.00005366
Iteration 207/1000 | Loss: 0.00005365
Iteration 208/1000 | Loss: 0.00005365
Iteration 209/1000 | Loss: 0.00005363
Iteration 210/1000 | Loss: 0.00005355
Iteration 211/1000 | Loss: 0.00005350
Iteration 212/1000 | Loss: 0.00005350
Iteration 213/1000 | Loss: 0.00005350
Iteration 214/1000 | Loss: 0.00005349
Iteration 215/1000 | Loss: 0.00005343
Iteration 216/1000 | Loss: 0.00005334
Iteration 217/1000 | Loss: 0.00027642
Iteration 218/1000 | Loss: 0.00017724
Iteration 219/1000 | Loss: 0.00005364
Iteration 220/1000 | Loss: 0.00005308
Iteration 221/1000 | Loss: 0.00070193
Iteration 222/1000 | Loss: 0.00026015
Iteration 223/1000 | Loss: 0.00023230
Iteration 224/1000 | Loss: 0.00017409
Iteration 225/1000 | Loss: 0.00020786
Iteration 226/1000 | Loss: 0.00077124
Iteration 227/1000 | Loss: 0.00024831
Iteration 228/1000 | Loss: 0.00007604
Iteration 229/1000 | Loss: 0.00017008
Iteration 230/1000 | Loss: 0.00059132
Iteration 231/1000 | Loss: 0.00017757
Iteration 232/1000 | Loss: 0.00007023
Iteration 233/1000 | Loss: 0.00005646
Iteration 234/1000 | Loss: 0.00005358
Iteration 235/1000 | Loss: 0.00047876
Iteration 236/1000 | Loss: 0.00013846
Iteration 237/1000 | Loss: 0.00006562
Iteration 238/1000 | Loss: 0.00039721
Iteration 239/1000 | Loss: 0.00026468
Iteration 240/1000 | Loss: 0.00007527
Iteration 241/1000 | Loss: 0.00005719
Iteration 242/1000 | Loss: 0.00005449
Iteration 243/1000 | Loss: 0.00005303
Iteration 244/1000 | Loss: 0.00005213
Iteration 245/1000 | Loss: 0.00005169
Iteration 246/1000 | Loss: 0.00005149
Iteration 247/1000 | Loss: 0.00005145
Iteration 248/1000 | Loss: 0.00027193
Iteration 249/1000 | Loss: 0.00007353
Iteration 250/1000 | Loss: 0.00020326
Iteration 251/1000 | Loss: 0.00005302
Iteration 252/1000 | Loss: 0.00026719
Iteration 253/1000 | Loss: 0.00008952
Iteration 254/1000 | Loss: 0.00010013
Iteration 255/1000 | Loss: 0.00005134
Iteration 256/1000 | Loss: 0.00047897
Iteration 257/1000 | Loss: 0.00007797
Iteration 258/1000 | Loss: 0.00005311
Iteration 259/1000 | Loss: 0.00004890
Iteration 260/1000 | Loss: 0.00004774
Iteration 261/1000 | Loss: 0.00004729
Iteration 262/1000 | Loss: 0.00004690
Iteration 263/1000 | Loss: 0.00004651
Iteration 264/1000 | Loss: 0.00004628
Iteration 265/1000 | Loss: 0.00004613
Iteration 266/1000 | Loss: 0.00004610
Iteration 267/1000 | Loss: 0.00022662
Iteration 268/1000 | Loss: 0.00011759
Iteration 269/1000 | Loss: 0.00004708
Iteration 270/1000 | Loss: 0.00004632
Iteration 271/1000 | Loss: 0.00004607
Iteration 272/1000 | Loss: 0.00004604
Iteration 273/1000 | Loss: 0.00004596
Iteration 274/1000 | Loss: 0.00004592
Iteration 275/1000 | Loss: 0.00004592
Iteration 276/1000 | Loss: 0.00004592
Iteration 277/1000 | Loss: 0.00004592
Iteration 278/1000 | Loss: 0.00004592
Iteration 279/1000 | Loss: 0.00004592
Iteration 280/1000 | Loss: 0.00004592
Iteration 281/1000 | Loss: 0.00004592
Iteration 282/1000 | Loss: 0.00004592
Iteration 283/1000 | Loss: 0.00004591
Iteration 284/1000 | Loss: 0.00004591
Iteration 285/1000 | Loss: 0.00004591
Iteration 286/1000 | Loss: 0.00004591
Iteration 287/1000 | Loss: 0.00004590
Iteration 288/1000 | Loss: 0.00004589
Iteration 289/1000 | Loss: 0.00023299
Iteration 290/1000 | Loss: 0.00005481
Iteration 291/1000 | Loss: 0.00004603
Iteration 292/1000 | Loss: 0.00004492
Iteration 293/1000 | Loss: 0.00004458
Iteration 294/1000 | Loss: 0.00004453
Iteration 295/1000 | Loss: 0.00004441
Iteration 296/1000 | Loss: 0.00004434
Iteration 297/1000 | Loss: 0.00004429
Iteration 298/1000 | Loss: 0.00004423
Iteration 299/1000 | Loss: 0.00004420
Iteration 300/1000 | Loss: 0.00004419
Iteration 301/1000 | Loss: 0.00004418
Iteration 302/1000 | Loss: 0.00004418
Iteration 303/1000 | Loss: 0.00004418
Iteration 304/1000 | Loss: 0.00004417
Iteration 305/1000 | Loss: 0.00004417
Iteration 306/1000 | Loss: 0.00004417
Iteration 307/1000 | Loss: 0.00004416
Iteration 308/1000 | Loss: 0.00004416
Iteration 309/1000 | Loss: 0.00004415
Iteration 310/1000 | Loss: 0.00004415
Iteration 311/1000 | Loss: 0.00004414
Iteration 312/1000 | Loss: 0.00004414
Iteration 313/1000 | Loss: 0.00004414
Iteration 314/1000 | Loss: 0.00004413
Iteration 315/1000 | Loss: 0.00004413
Iteration 316/1000 | Loss: 0.00004413
Iteration 317/1000 | Loss: 0.00004413
Iteration 318/1000 | Loss: 0.00004413
Iteration 319/1000 | Loss: 0.00004413
Iteration 320/1000 | Loss: 0.00004412
Iteration 321/1000 | Loss: 0.00004412
Iteration 322/1000 | Loss: 0.00004412
Iteration 323/1000 | Loss: 0.00004412
Iteration 324/1000 | Loss: 0.00004412
Iteration 325/1000 | Loss: 0.00004412
Iteration 326/1000 | Loss: 0.00004412
Iteration 327/1000 | Loss: 0.00004411
Iteration 328/1000 | Loss: 0.00004411
Iteration 329/1000 | Loss: 0.00004411
Iteration 330/1000 | Loss: 0.00004411
Iteration 331/1000 | Loss: 0.00004410
Iteration 332/1000 | Loss: 0.00004410
Iteration 333/1000 | Loss: 0.00004410
Iteration 334/1000 | Loss: 0.00004410
Iteration 335/1000 | Loss: 0.00004410
Iteration 336/1000 | Loss: 0.00004409
Iteration 337/1000 | Loss: 0.00004409
Iteration 338/1000 | Loss: 0.00004409
Iteration 339/1000 | Loss: 0.00004409
Iteration 340/1000 | Loss: 0.00004409
Iteration 341/1000 | Loss: 0.00004409
Iteration 342/1000 | Loss: 0.00004409
Iteration 343/1000 | Loss: 0.00004408
Iteration 344/1000 | Loss: 0.00004408
Iteration 345/1000 | Loss: 0.00004408
Iteration 346/1000 | Loss: 0.00004408
Iteration 347/1000 | Loss: 0.00004408
Iteration 348/1000 | Loss: 0.00004408
Iteration 349/1000 | Loss: 0.00004408
Iteration 350/1000 | Loss: 0.00004408
Iteration 351/1000 | Loss: 0.00004408
Iteration 352/1000 | Loss: 0.00004408
Iteration 353/1000 | Loss: 0.00004408
Iteration 354/1000 | Loss: 0.00004408
Iteration 355/1000 | Loss: 0.00004408
Iteration 356/1000 | Loss: 0.00004408
Iteration 357/1000 | Loss: 0.00004407
Iteration 358/1000 | Loss: 0.00004407
Iteration 359/1000 | Loss: 0.00004407
Iteration 360/1000 | Loss: 0.00004407
Iteration 361/1000 | Loss: 0.00004407
Iteration 362/1000 | Loss: 0.00004407
Iteration 363/1000 | Loss: 0.00004407
Iteration 364/1000 | Loss: 0.00004407
Iteration 365/1000 | Loss: 0.00004407
Iteration 366/1000 | Loss: 0.00004407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [4.4074171455577016e-05, 4.4074171455577016e-05, 4.4074171455577016e-05, 4.4074171455577016e-05, 4.4074171455577016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4074171455577016e-05

Optimization complete. Final v2v error: 4.695785999298096 mm

Highest mean error: 12.052948951721191 mm for frame 14

Lowest mean error: 3.6553797721862793 mm for frame 88

Saving results

Total time: 487.73945665359497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779293
Iteration 2/25 | Loss: 0.00132056
Iteration 3/25 | Loss: 0.00122893
Iteration 4/25 | Loss: 0.00121763
Iteration 5/25 | Loss: 0.00121527
Iteration 6/25 | Loss: 0.00121527
Iteration 7/25 | Loss: 0.00121527
Iteration 8/25 | Loss: 0.00121527
Iteration 9/25 | Loss: 0.00121527
Iteration 10/25 | Loss: 0.00121527
Iteration 11/25 | Loss: 0.00121527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012152688577771187, 0.0012152688577771187, 0.0012152688577771187, 0.0012152688577771187, 0.0012152688577771187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012152688577771187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43266535
Iteration 2/25 | Loss: 0.00073602
Iteration 3/25 | Loss: 0.00073602
Iteration 4/25 | Loss: 0.00073602
Iteration 5/25 | Loss: 0.00073602
Iteration 6/25 | Loss: 0.00073601
Iteration 7/25 | Loss: 0.00073601
Iteration 8/25 | Loss: 0.00073601
Iteration 9/25 | Loss: 0.00073601
Iteration 10/25 | Loss: 0.00073601
Iteration 11/25 | Loss: 0.00073601
Iteration 12/25 | Loss: 0.00073601
Iteration 13/25 | Loss: 0.00073601
Iteration 14/25 | Loss: 0.00073601
Iteration 15/25 | Loss: 0.00073601
Iteration 16/25 | Loss: 0.00073601
Iteration 17/25 | Loss: 0.00073601
Iteration 18/25 | Loss: 0.00073601
Iteration 19/25 | Loss: 0.00073601
Iteration 20/25 | Loss: 0.00073601
Iteration 21/25 | Loss: 0.00073601
Iteration 22/25 | Loss: 0.00073601
Iteration 23/25 | Loss: 0.00073601
Iteration 24/25 | Loss: 0.00073601
Iteration 25/25 | Loss: 0.00073601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073601
Iteration 2/1000 | Loss: 0.00002459
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001204
Iteration 14/1000 | Loss: 0.00001203
Iteration 15/1000 | Loss: 0.00001202
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001181
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001178
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001177
Iteration 23/1000 | Loss: 0.00001172
Iteration 24/1000 | Loss: 0.00001172
Iteration 25/1000 | Loss: 0.00001172
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001172
Iteration 29/1000 | Loss: 0.00001171
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001170
Iteration 32/1000 | Loss: 0.00001169
Iteration 33/1000 | Loss: 0.00001169
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001168
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001168
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001167
Iteration 40/1000 | Loss: 0.00001166
Iteration 41/1000 | Loss: 0.00001166
Iteration 42/1000 | Loss: 0.00001166
Iteration 43/1000 | Loss: 0.00001166
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001164
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001164
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001163
Iteration 52/1000 | Loss: 0.00001163
Iteration 53/1000 | Loss: 0.00001163
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001162
Iteration 62/1000 | Loss: 0.00001162
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001145
Iteration 99/1000 | Loss: 0.00001145
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001144
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001141
Iteration 121/1000 | Loss: 0.00001140
Iteration 122/1000 | Loss: 0.00001140
Iteration 123/1000 | Loss: 0.00001140
Iteration 124/1000 | Loss: 0.00001140
Iteration 125/1000 | Loss: 0.00001140
Iteration 126/1000 | Loss: 0.00001140
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001138
Iteration 134/1000 | Loss: 0.00001138
Iteration 135/1000 | Loss: 0.00001138
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001136
Iteration 140/1000 | Loss: 0.00001136
Iteration 141/1000 | Loss: 0.00001136
Iteration 142/1000 | Loss: 0.00001136
Iteration 143/1000 | Loss: 0.00001136
Iteration 144/1000 | Loss: 0.00001136
Iteration 145/1000 | Loss: 0.00001136
Iteration 146/1000 | Loss: 0.00001135
Iteration 147/1000 | Loss: 0.00001135
Iteration 148/1000 | Loss: 0.00001135
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001135
Iteration 154/1000 | Loss: 0.00001135
Iteration 155/1000 | Loss: 0.00001135
Iteration 156/1000 | Loss: 0.00001135
Iteration 157/1000 | Loss: 0.00001135
Iteration 158/1000 | Loss: 0.00001135
Iteration 159/1000 | Loss: 0.00001134
Iteration 160/1000 | Loss: 0.00001134
Iteration 161/1000 | Loss: 0.00001134
Iteration 162/1000 | Loss: 0.00001134
Iteration 163/1000 | Loss: 0.00001134
Iteration 164/1000 | Loss: 0.00001134
Iteration 165/1000 | Loss: 0.00001134
Iteration 166/1000 | Loss: 0.00001134
Iteration 167/1000 | Loss: 0.00001134
Iteration 168/1000 | Loss: 0.00001134
Iteration 169/1000 | Loss: 0.00001133
Iteration 170/1000 | Loss: 0.00001133
Iteration 171/1000 | Loss: 0.00001133
Iteration 172/1000 | Loss: 0.00001133
Iteration 173/1000 | Loss: 0.00001133
Iteration 174/1000 | Loss: 0.00001133
Iteration 175/1000 | Loss: 0.00001133
Iteration 176/1000 | Loss: 0.00001133
Iteration 177/1000 | Loss: 0.00001133
Iteration 178/1000 | Loss: 0.00001133
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001132
Iteration 183/1000 | Loss: 0.00001132
Iteration 184/1000 | Loss: 0.00001132
Iteration 185/1000 | Loss: 0.00001132
Iteration 186/1000 | Loss: 0.00001132
Iteration 187/1000 | Loss: 0.00001132
Iteration 188/1000 | Loss: 0.00001131
Iteration 189/1000 | Loss: 0.00001131
Iteration 190/1000 | Loss: 0.00001131
Iteration 191/1000 | Loss: 0.00001131
Iteration 192/1000 | Loss: 0.00001131
Iteration 193/1000 | Loss: 0.00001131
Iteration 194/1000 | Loss: 0.00001131
Iteration 195/1000 | Loss: 0.00001130
Iteration 196/1000 | Loss: 0.00001130
Iteration 197/1000 | Loss: 0.00001130
Iteration 198/1000 | Loss: 0.00001130
Iteration 199/1000 | Loss: 0.00001130
Iteration 200/1000 | Loss: 0.00001129
Iteration 201/1000 | Loss: 0.00001129
Iteration 202/1000 | Loss: 0.00001129
Iteration 203/1000 | Loss: 0.00001129
Iteration 204/1000 | Loss: 0.00001129
Iteration 205/1000 | Loss: 0.00001129
Iteration 206/1000 | Loss: 0.00001128
Iteration 207/1000 | Loss: 0.00001128
Iteration 208/1000 | Loss: 0.00001128
Iteration 209/1000 | Loss: 0.00001128
Iteration 210/1000 | Loss: 0.00001128
Iteration 211/1000 | Loss: 0.00001128
Iteration 212/1000 | Loss: 0.00001128
Iteration 213/1000 | Loss: 0.00001128
Iteration 214/1000 | Loss: 0.00001128
Iteration 215/1000 | Loss: 0.00001128
Iteration 216/1000 | Loss: 0.00001127
Iteration 217/1000 | Loss: 0.00001127
Iteration 218/1000 | Loss: 0.00001127
Iteration 219/1000 | Loss: 0.00001127
Iteration 220/1000 | Loss: 0.00001127
Iteration 221/1000 | Loss: 0.00001127
Iteration 222/1000 | Loss: 0.00001127
Iteration 223/1000 | Loss: 0.00001127
Iteration 224/1000 | Loss: 0.00001127
Iteration 225/1000 | Loss: 0.00001127
Iteration 226/1000 | Loss: 0.00001127
Iteration 227/1000 | Loss: 0.00001127
Iteration 228/1000 | Loss: 0.00001127
Iteration 229/1000 | Loss: 0.00001127
Iteration 230/1000 | Loss: 0.00001127
Iteration 231/1000 | Loss: 0.00001127
Iteration 232/1000 | Loss: 0.00001127
Iteration 233/1000 | Loss: 0.00001127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.1267889021837618e-05, 1.1267889021837618e-05, 1.1267889021837618e-05, 1.1267889021837618e-05, 1.1267889021837618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1267889021837618e-05

Optimization complete. Final v2v error: 2.862067699432373 mm

Highest mean error: 3.0909628868103027 mm for frame 75

Lowest mean error: 2.720407247543335 mm for frame 194

Saving results

Total time: 43.27583456039429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857100
Iteration 2/25 | Loss: 0.00137269
Iteration 3/25 | Loss: 0.00129478
Iteration 4/25 | Loss: 0.00128598
Iteration 5/25 | Loss: 0.00128346
Iteration 6/25 | Loss: 0.00128346
Iteration 7/25 | Loss: 0.00128346
Iteration 8/25 | Loss: 0.00128346
Iteration 9/25 | Loss: 0.00128346
Iteration 10/25 | Loss: 0.00128346
Iteration 11/25 | Loss: 0.00128346
Iteration 12/25 | Loss: 0.00128346
Iteration 13/25 | Loss: 0.00128346
Iteration 14/25 | Loss: 0.00128346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012834606459364295, 0.0012834606459364295, 0.0012834606459364295, 0.0012834606459364295, 0.0012834606459364295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012834606459364295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35780859
Iteration 2/25 | Loss: 0.00071504
Iteration 3/25 | Loss: 0.00071495
Iteration 4/25 | Loss: 0.00071495
Iteration 5/25 | Loss: 0.00071495
Iteration 6/25 | Loss: 0.00071495
Iteration 7/25 | Loss: 0.00071495
Iteration 8/25 | Loss: 0.00071494
Iteration 9/25 | Loss: 0.00071494
Iteration 10/25 | Loss: 0.00071494
Iteration 11/25 | Loss: 0.00071494
Iteration 12/25 | Loss: 0.00071494
Iteration 13/25 | Loss: 0.00071494
Iteration 14/25 | Loss: 0.00071494
Iteration 15/25 | Loss: 0.00071494
Iteration 16/25 | Loss: 0.00071494
Iteration 17/25 | Loss: 0.00071494
Iteration 18/25 | Loss: 0.00071494
Iteration 19/25 | Loss: 0.00071494
Iteration 20/25 | Loss: 0.00071494
Iteration 21/25 | Loss: 0.00071494
Iteration 22/25 | Loss: 0.00071494
Iteration 23/25 | Loss: 0.00071494
Iteration 24/25 | Loss: 0.00071494
Iteration 25/25 | Loss: 0.00071494
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007149434532038867, 0.0007149434532038867, 0.0007149434532038867, 0.0007149434532038867, 0.0007149434532038867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007149434532038867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071494
Iteration 2/1000 | Loss: 0.00004081
Iteration 3/1000 | Loss: 0.00002369
Iteration 4/1000 | Loss: 0.00002056
Iteration 5/1000 | Loss: 0.00001938
Iteration 6/1000 | Loss: 0.00001808
Iteration 7/1000 | Loss: 0.00001738
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001662
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001621
Iteration 12/1000 | Loss: 0.00001604
Iteration 13/1000 | Loss: 0.00001604
Iteration 14/1000 | Loss: 0.00001585
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001574
Iteration 17/1000 | Loss: 0.00001571
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001569
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001561
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001561
Iteration 36/1000 | Loss: 0.00001561
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001560
Iteration 46/1000 | Loss: 0.00001560
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001555
Iteration 66/1000 | Loss: 0.00001555
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001550
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001546
Iteration 110/1000 | Loss: 0.00001546
Iteration 111/1000 | Loss: 0.00001546
Iteration 112/1000 | Loss: 0.00001546
Iteration 113/1000 | Loss: 0.00001546
Iteration 114/1000 | Loss: 0.00001546
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001543
Iteration 132/1000 | Loss: 0.00001543
Iteration 133/1000 | Loss: 0.00001543
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001543
Iteration 137/1000 | Loss: 0.00001543
Iteration 138/1000 | Loss: 0.00001543
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001542
Iteration 142/1000 | Loss: 0.00001542
Iteration 143/1000 | Loss: 0.00001542
Iteration 144/1000 | Loss: 0.00001541
Iteration 145/1000 | Loss: 0.00001541
Iteration 146/1000 | Loss: 0.00001541
Iteration 147/1000 | Loss: 0.00001541
Iteration 148/1000 | Loss: 0.00001541
Iteration 149/1000 | Loss: 0.00001541
Iteration 150/1000 | Loss: 0.00001541
Iteration 151/1000 | Loss: 0.00001541
Iteration 152/1000 | Loss: 0.00001541
Iteration 153/1000 | Loss: 0.00001541
Iteration 154/1000 | Loss: 0.00001541
Iteration 155/1000 | Loss: 0.00001541
Iteration 156/1000 | Loss: 0.00001541
Iteration 157/1000 | Loss: 0.00001540
Iteration 158/1000 | Loss: 0.00001540
Iteration 159/1000 | Loss: 0.00001540
Iteration 160/1000 | Loss: 0.00001540
Iteration 161/1000 | Loss: 0.00001540
Iteration 162/1000 | Loss: 0.00001540
Iteration 163/1000 | Loss: 0.00001540
Iteration 164/1000 | Loss: 0.00001540
Iteration 165/1000 | Loss: 0.00001540
Iteration 166/1000 | Loss: 0.00001540
Iteration 167/1000 | Loss: 0.00001540
Iteration 168/1000 | Loss: 0.00001540
Iteration 169/1000 | Loss: 0.00001540
Iteration 170/1000 | Loss: 0.00001540
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.540447374281939e-05, 1.540447374281939e-05, 1.540447374281939e-05, 1.540447374281939e-05, 1.540447374281939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.540447374281939e-05

Optimization complete. Final v2v error: 3.331446886062622 mm

Highest mean error: 3.6370627880096436 mm for frame 220

Lowest mean error: 3.1681861877441406 mm for frame 79

Saving results

Total time: 42.93307280540466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076506
Iteration 2/25 | Loss: 0.01076505
Iteration 3/25 | Loss: 0.00365145
Iteration 4/25 | Loss: 0.00188624
Iteration 5/25 | Loss: 0.00178908
Iteration 6/25 | Loss: 0.00175607
Iteration 7/25 | Loss: 0.00174963
Iteration 8/25 | Loss: 0.00184017
Iteration 9/25 | Loss: 0.00185038
Iteration 10/25 | Loss: 0.00163103
Iteration 11/25 | Loss: 0.00141190
Iteration 12/25 | Loss: 0.00137587
Iteration 13/25 | Loss: 0.00136355
Iteration 14/25 | Loss: 0.00135145
Iteration 15/25 | Loss: 0.00134987
Iteration 16/25 | Loss: 0.00134997
Iteration 17/25 | Loss: 0.00135033
Iteration 18/25 | Loss: 0.00134995
Iteration 19/25 | Loss: 0.00134995
Iteration 20/25 | Loss: 0.00134995
Iteration 21/25 | Loss: 0.00134995
Iteration 22/25 | Loss: 0.00134995
Iteration 23/25 | Loss: 0.00134995
Iteration 24/25 | Loss: 0.00134995
Iteration 25/25 | Loss: 0.00135173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56651312
Iteration 2/25 | Loss: 0.00075104
Iteration 3/25 | Loss: 0.00074073
Iteration 4/25 | Loss: 0.00074073
Iteration 5/25 | Loss: 0.00074073
Iteration 6/25 | Loss: 0.00074072
Iteration 7/25 | Loss: 0.00074072
Iteration 8/25 | Loss: 0.00074072
Iteration 9/25 | Loss: 0.00074072
Iteration 10/25 | Loss: 0.00074072
Iteration 11/25 | Loss: 0.00074072
Iteration 12/25 | Loss: 0.00074072
Iteration 13/25 | Loss: 0.00074072
Iteration 14/25 | Loss: 0.00074072
Iteration 15/25 | Loss: 0.00074072
Iteration 16/25 | Loss: 0.00074072
Iteration 17/25 | Loss: 0.00074072
Iteration 18/25 | Loss: 0.00074072
Iteration 19/25 | Loss: 0.00074072
Iteration 20/25 | Loss: 0.00074072
Iteration 21/25 | Loss: 0.00074072
Iteration 22/25 | Loss: 0.00074072
Iteration 23/25 | Loss: 0.00074072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000740723917260766, 0.000740723917260766, 0.000740723917260766, 0.000740723917260766, 0.000740723917260766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000740723917260766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074072
Iteration 2/1000 | Loss: 0.00009857
Iteration 3/1000 | Loss: 0.00018635
Iteration 4/1000 | Loss: 0.00026113
Iteration 5/1000 | Loss: 0.00003753
Iteration 6/1000 | Loss: 0.00002318
Iteration 7/1000 | Loss: 0.00002651
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002121
Iteration 10/1000 | Loss: 0.00003141
Iteration 11/1000 | Loss: 0.00002171
Iteration 12/1000 | Loss: 0.00002131
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00001977
Iteration 15/1000 | Loss: 0.00003863
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00002082
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002756
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001941
Iteration 24/1000 | Loss: 0.00002072
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00002045
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001953
Iteration 31/1000 | Loss: 0.00001953
Iteration 32/1000 | Loss: 0.00002242
Iteration 33/1000 | Loss: 0.00001982
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001892
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001890
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001890
Iteration 41/1000 | Loss: 0.00001890
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001890
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001890
Iteration 50/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [1.889551458589267e-05, 1.889551458589267e-05, 1.889551458589267e-05, 1.889551458589267e-05, 1.889551458589267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.889551458589267e-05

Optimization complete. Final v2v error: 3.718773365020752 mm

Highest mean error: 4.167244911193848 mm for frame 106

Lowest mean error: 3.5557162761688232 mm for frame 120

Saving results

Total time: 81.25324010848999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792688
Iteration 2/25 | Loss: 0.00191223
Iteration 3/25 | Loss: 0.00143034
Iteration 4/25 | Loss: 0.00137423
Iteration 5/25 | Loss: 0.00136731
Iteration 6/25 | Loss: 0.00136664
Iteration 7/25 | Loss: 0.00136664
Iteration 8/25 | Loss: 0.00136664
Iteration 9/25 | Loss: 0.00136664
Iteration 10/25 | Loss: 0.00136664
Iteration 11/25 | Loss: 0.00136664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013666355516761541, 0.0013666355516761541, 0.0013666355516761541, 0.0013666355516761541, 0.0013666355516761541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013666355516761541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30654454
Iteration 2/25 | Loss: 0.00065847
Iteration 3/25 | Loss: 0.00065844
Iteration 4/25 | Loss: 0.00065844
Iteration 5/25 | Loss: 0.00065844
Iteration 6/25 | Loss: 0.00065844
Iteration 7/25 | Loss: 0.00065844
Iteration 8/25 | Loss: 0.00065844
Iteration 9/25 | Loss: 0.00065843
Iteration 10/25 | Loss: 0.00065843
Iteration 11/25 | Loss: 0.00065843
Iteration 12/25 | Loss: 0.00065843
Iteration 13/25 | Loss: 0.00065843
Iteration 14/25 | Loss: 0.00065843
Iteration 15/25 | Loss: 0.00065843
Iteration 16/25 | Loss: 0.00065843
Iteration 17/25 | Loss: 0.00065843
Iteration 18/25 | Loss: 0.00065843
Iteration 19/25 | Loss: 0.00065843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006584346992895007, 0.0006584346992895007, 0.0006584346992895007, 0.0006584346992895007, 0.0006584346992895007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006584346992895007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065843
Iteration 2/1000 | Loss: 0.00004327
Iteration 3/1000 | Loss: 0.00003080
Iteration 4/1000 | Loss: 0.00002840
Iteration 5/1000 | Loss: 0.00002684
Iteration 6/1000 | Loss: 0.00002585
Iteration 7/1000 | Loss: 0.00002542
Iteration 8/1000 | Loss: 0.00002489
Iteration 9/1000 | Loss: 0.00002444
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002391
Iteration 12/1000 | Loss: 0.00002369
Iteration 13/1000 | Loss: 0.00002353
Iteration 14/1000 | Loss: 0.00002351
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002344
Iteration 17/1000 | Loss: 0.00002344
Iteration 18/1000 | Loss: 0.00002343
Iteration 19/1000 | Loss: 0.00002343
Iteration 20/1000 | Loss: 0.00002341
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002341
Iteration 23/1000 | Loss: 0.00002341
Iteration 24/1000 | Loss: 0.00002341
Iteration 25/1000 | Loss: 0.00002341
Iteration 26/1000 | Loss: 0.00002341
Iteration 27/1000 | Loss: 0.00002341
Iteration 28/1000 | Loss: 0.00002341
Iteration 29/1000 | Loss: 0.00002341
Iteration 30/1000 | Loss: 0.00002341
Iteration 31/1000 | Loss: 0.00002341
Iteration 32/1000 | Loss: 0.00002341
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002341
Iteration 35/1000 | Loss: 0.00002341
Iteration 36/1000 | Loss: 0.00002341
Iteration 37/1000 | Loss: 0.00002341
Iteration 38/1000 | Loss: 0.00002341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [2.3406348191201687e-05, 2.3406348191201687e-05, 2.3406348191201687e-05, 2.3406348191201687e-05, 2.3406348191201687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3406348191201687e-05

Optimization complete. Final v2v error: 4.045378684997559 mm

Highest mean error: 4.414155006408691 mm for frame 125

Lowest mean error: 3.821709156036377 mm for frame 214

Saving results

Total time: 32.21261215209961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832270
Iteration 2/25 | Loss: 0.00140230
Iteration 3/25 | Loss: 0.00131439
Iteration 4/25 | Loss: 0.00129858
Iteration 5/25 | Loss: 0.00129365
Iteration 6/25 | Loss: 0.00129324
Iteration 7/25 | Loss: 0.00129324
Iteration 8/25 | Loss: 0.00129324
Iteration 9/25 | Loss: 0.00129324
Iteration 10/25 | Loss: 0.00129324
Iteration 11/25 | Loss: 0.00129324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012932440731674433, 0.0012932440731674433, 0.0012932440731674433, 0.0012932440731674433, 0.0012932440731674433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012932440731674433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39763033
Iteration 2/25 | Loss: 0.00072521
Iteration 3/25 | Loss: 0.00072520
Iteration 4/25 | Loss: 0.00072520
Iteration 5/25 | Loss: 0.00072520
Iteration 6/25 | Loss: 0.00072520
Iteration 7/25 | Loss: 0.00072520
Iteration 8/25 | Loss: 0.00072520
Iteration 9/25 | Loss: 0.00072520
Iteration 10/25 | Loss: 0.00072520
Iteration 11/25 | Loss: 0.00072520
Iteration 12/25 | Loss: 0.00072520
Iteration 13/25 | Loss: 0.00072520
Iteration 14/25 | Loss: 0.00072520
Iteration 15/25 | Loss: 0.00072520
Iteration 16/25 | Loss: 0.00072520
Iteration 17/25 | Loss: 0.00072520
Iteration 18/25 | Loss: 0.00072520
Iteration 19/25 | Loss: 0.00072520
Iteration 20/25 | Loss: 0.00072520
Iteration 21/25 | Loss: 0.00072520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007251995266415179, 0.0007251995266415179, 0.0007251995266415179, 0.0007251995266415179, 0.0007251995266415179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007251995266415179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072520
Iteration 2/1000 | Loss: 0.00004408
Iteration 3/1000 | Loss: 0.00003091
Iteration 4/1000 | Loss: 0.00002704
Iteration 5/1000 | Loss: 0.00002542
Iteration 6/1000 | Loss: 0.00002441
Iteration 7/1000 | Loss: 0.00002340
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002216
Iteration 11/1000 | Loss: 0.00002190
Iteration 12/1000 | Loss: 0.00002174
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002156
Iteration 16/1000 | Loss: 0.00002145
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002143
Iteration 19/1000 | Loss: 0.00002136
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002129
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002127
Iteration 25/1000 | Loss: 0.00002126
Iteration 26/1000 | Loss: 0.00002124
Iteration 27/1000 | Loss: 0.00002123
Iteration 28/1000 | Loss: 0.00002117
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002112
Iteration 32/1000 | Loss: 0.00002112
Iteration 33/1000 | Loss: 0.00002111
Iteration 34/1000 | Loss: 0.00002107
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002106
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002105
Iteration 39/1000 | Loss: 0.00002105
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002103
Iteration 43/1000 | Loss: 0.00002103
Iteration 44/1000 | Loss: 0.00002102
Iteration 45/1000 | Loss: 0.00002102
Iteration 46/1000 | Loss: 0.00002102
Iteration 47/1000 | Loss: 0.00002102
Iteration 48/1000 | Loss: 0.00002101
Iteration 49/1000 | Loss: 0.00002101
Iteration 50/1000 | Loss: 0.00002101
Iteration 51/1000 | Loss: 0.00002101
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002101
Iteration 56/1000 | Loss: 0.00002101
Iteration 57/1000 | Loss: 0.00002100
Iteration 58/1000 | Loss: 0.00002100
Iteration 59/1000 | Loss: 0.00002100
Iteration 60/1000 | Loss: 0.00002100
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002098
Iteration 63/1000 | Loss: 0.00002098
Iteration 64/1000 | Loss: 0.00002098
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002097
Iteration 68/1000 | Loss: 0.00002096
Iteration 69/1000 | Loss: 0.00002096
Iteration 70/1000 | Loss: 0.00002096
Iteration 71/1000 | Loss: 0.00002095
Iteration 72/1000 | Loss: 0.00002095
Iteration 73/1000 | Loss: 0.00002095
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002092
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002092
Iteration 86/1000 | Loss: 0.00002092
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002090
Iteration 91/1000 | Loss: 0.00002090
Iteration 92/1000 | Loss: 0.00002090
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002090
Iteration 95/1000 | Loss: 0.00002090
Iteration 96/1000 | Loss: 0.00002090
Iteration 97/1000 | Loss: 0.00002090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.089797089865897e-05, 2.089797089865897e-05, 2.089797089865897e-05, 2.089797089865897e-05, 2.089797089865897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.089797089865897e-05

Optimization complete. Final v2v error: 3.777069568634033 mm

Highest mean error: 4.696405410766602 mm for frame 191

Lowest mean error: 3.0460317134857178 mm for frame 11

Saving results

Total time: 42.261107206344604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749611
Iteration 2/25 | Loss: 0.00175594
Iteration 3/25 | Loss: 0.00138036
Iteration 4/25 | Loss: 0.00133460
Iteration 5/25 | Loss: 0.00132846
Iteration 6/25 | Loss: 0.00130308
Iteration 7/25 | Loss: 0.00130307
Iteration 8/25 | Loss: 0.00129964
Iteration 9/25 | Loss: 0.00129450
Iteration 10/25 | Loss: 0.00129425
Iteration 11/25 | Loss: 0.00129082
Iteration 12/25 | Loss: 0.00128934
Iteration 13/25 | Loss: 0.00129154
Iteration 14/25 | Loss: 0.00128921
Iteration 15/25 | Loss: 0.00128755
Iteration 16/25 | Loss: 0.00128662
Iteration 17/25 | Loss: 0.00128616
Iteration 18/25 | Loss: 0.00128577
Iteration 19/25 | Loss: 0.00128971
Iteration 20/25 | Loss: 0.00128856
Iteration 21/25 | Loss: 0.00128690
Iteration 22/25 | Loss: 0.00128627
Iteration 23/25 | Loss: 0.00128582
Iteration 24/25 | Loss: 0.00128554
Iteration 25/25 | Loss: 0.00128508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66816568
Iteration 2/25 | Loss: 0.00081560
Iteration 3/25 | Loss: 0.00081556
Iteration 4/25 | Loss: 0.00081556
Iteration 5/25 | Loss: 0.00081556
Iteration 6/25 | Loss: 0.00081556
Iteration 7/25 | Loss: 0.00081556
Iteration 8/25 | Loss: 0.00081556
Iteration 9/25 | Loss: 0.00081556
Iteration 10/25 | Loss: 0.00081556
Iteration 11/25 | Loss: 0.00081556
Iteration 12/25 | Loss: 0.00081556
Iteration 13/25 | Loss: 0.00081556
Iteration 14/25 | Loss: 0.00081556
Iteration 15/25 | Loss: 0.00081556
Iteration 16/25 | Loss: 0.00081556
Iteration 17/25 | Loss: 0.00081556
Iteration 18/25 | Loss: 0.00081556
Iteration 19/25 | Loss: 0.00081556
Iteration 20/25 | Loss: 0.00081556
Iteration 21/25 | Loss: 0.00081556
Iteration 22/25 | Loss: 0.00081556
Iteration 23/25 | Loss: 0.00081556
Iteration 24/25 | Loss: 0.00081556
Iteration 25/25 | Loss: 0.00081556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081556
Iteration 2/1000 | Loss: 0.00003028
Iteration 3/1000 | Loss: 0.00002578
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002041
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001882
Iteration 8/1000 | Loss: 0.00001854
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001746
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001738
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001725
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001716
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001704
Iteration 121/1000 | Loss: 0.00001704
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001701
Iteration 131/1000 | Loss: 0.00001701
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001700
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001700
Iteration 137/1000 | Loss: 0.00001700
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001699
Iteration 142/1000 | Loss: 0.00001699
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001697
Iteration 148/1000 | Loss: 0.00001697
Iteration 149/1000 | Loss: 0.00001697
Iteration 150/1000 | Loss: 0.00001697
Iteration 151/1000 | Loss: 0.00001697
Iteration 152/1000 | Loss: 0.00001697
Iteration 153/1000 | Loss: 0.00001696
Iteration 154/1000 | Loss: 0.00001696
Iteration 155/1000 | Loss: 0.00001696
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001696
Iteration 158/1000 | Loss: 0.00001696
Iteration 159/1000 | Loss: 0.00001696
Iteration 160/1000 | Loss: 0.00001696
Iteration 161/1000 | Loss: 0.00001696
Iteration 162/1000 | Loss: 0.00001695
Iteration 163/1000 | Loss: 0.00001695
Iteration 164/1000 | Loss: 0.00001695
Iteration 165/1000 | Loss: 0.00001695
Iteration 166/1000 | Loss: 0.00001695
Iteration 167/1000 | Loss: 0.00001695
Iteration 168/1000 | Loss: 0.00001695
Iteration 169/1000 | Loss: 0.00001695
Iteration 170/1000 | Loss: 0.00001695
Iteration 171/1000 | Loss: 0.00001695
Iteration 172/1000 | Loss: 0.00001695
Iteration 173/1000 | Loss: 0.00001695
Iteration 174/1000 | Loss: 0.00001695
Iteration 175/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.694713682809379e-05, 1.694713682809379e-05, 1.694713682809379e-05, 1.694713682809379e-05, 1.694713682809379e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.694713682809379e-05

Optimization complete. Final v2v error: 3.4645190238952637 mm

Highest mean error: 3.9298510551452637 mm for frame 49

Lowest mean error: 2.9842541217803955 mm for frame 207

Saving results

Total time: 87.21177124977112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062521
Iteration 2/25 | Loss: 0.00181580
Iteration 3/25 | Loss: 0.00151775
Iteration 4/25 | Loss: 0.00144372
Iteration 5/25 | Loss: 0.00146149
Iteration 6/25 | Loss: 0.00140488
Iteration 7/25 | Loss: 0.00132323
Iteration 8/25 | Loss: 0.00129234
Iteration 9/25 | Loss: 0.00127224
Iteration 10/25 | Loss: 0.00126720
Iteration 11/25 | Loss: 0.00126600
Iteration 12/25 | Loss: 0.00126467
Iteration 13/25 | Loss: 0.00126383
Iteration 14/25 | Loss: 0.00126344
Iteration 15/25 | Loss: 0.00126325
Iteration 16/25 | Loss: 0.00126280
Iteration 17/25 | Loss: 0.00126160
Iteration 18/25 | Loss: 0.00126131
Iteration 19/25 | Loss: 0.00126124
Iteration 20/25 | Loss: 0.00126124
Iteration 21/25 | Loss: 0.00126124
Iteration 22/25 | Loss: 0.00126124
Iteration 23/25 | Loss: 0.00126123
Iteration 24/25 | Loss: 0.00126123
Iteration 25/25 | Loss: 0.00126123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57966626
Iteration 2/25 | Loss: 0.00084731
Iteration 3/25 | Loss: 0.00084731
Iteration 4/25 | Loss: 0.00084730
Iteration 5/25 | Loss: 0.00084730
Iteration 6/25 | Loss: 0.00084730
Iteration 7/25 | Loss: 0.00084730
Iteration 8/25 | Loss: 0.00084730
Iteration 9/25 | Loss: 0.00084730
Iteration 10/25 | Loss: 0.00084730
Iteration 11/25 | Loss: 0.00084730
Iteration 12/25 | Loss: 0.00084730
Iteration 13/25 | Loss: 0.00084730
Iteration 14/25 | Loss: 0.00084730
Iteration 15/25 | Loss: 0.00084730
Iteration 16/25 | Loss: 0.00084730
Iteration 17/25 | Loss: 0.00084730
Iteration 18/25 | Loss: 0.00084730
Iteration 19/25 | Loss: 0.00084730
Iteration 20/25 | Loss: 0.00084730
Iteration 21/25 | Loss: 0.00084730
Iteration 22/25 | Loss: 0.00084730
Iteration 23/25 | Loss: 0.00084730
Iteration 24/25 | Loss: 0.00084730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008473019115626812, 0.0008473019115626812, 0.0008473019115626812, 0.0008473019115626812, 0.0008473019115626812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008473019115626812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084730
Iteration 2/1000 | Loss: 0.00003393
Iteration 3/1000 | Loss: 0.00005679
Iteration 4/1000 | Loss: 0.00002351
Iteration 5/1000 | Loss: 0.00004316
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00005294
Iteration 8/1000 | Loss: 0.00002104
Iteration 9/1000 | Loss: 0.00002053
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00002015
Iteration 12/1000 | Loss: 0.00005193
Iteration 13/1000 | Loss: 0.00003042
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00003714
Iteration 17/1000 | Loss: 0.00003714
Iteration 18/1000 | Loss: 0.00001989
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001941
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001933
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00001913
Iteration 38/1000 | Loss: 0.00001913
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001906
Iteration 48/1000 | Loss: 0.00001906
Iteration 49/1000 | Loss: 0.00001906
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001905
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001903
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001901
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001899
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001898
Iteration 72/1000 | Loss: 0.00001898
Iteration 73/1000 | Loss: 0.00001898
Iteration 74/1000 | Loss: 0.00001897
Iteration 75/1000 | Loss: 0.00001897
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001896
Iteration 78/1000 | Loss: 0.00001896
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001895
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001894
Iteration 84/1000 | Loss: 0.00001894
Iteration 85/1000 | Loss: 0.00001894
Iteration 86/1000 | Loss: 0.00001894
Iteration 87/1000 | Loss: 0.00001894
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001889
Iteration 108/1000 | Loss: 0.00001889
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001888
Iteration 116/1000 | Loss: 0.00001888
Iteration 117/1000 | Loss: 0.00001888
Iteration 118/1000 | Loss: 0.00001888
Iteration 119/1000 | Loss: 0.00001888
Iteration 120/1000 | Loss: 0.00001888
Iteration 121/1000 | Loss: 0.00001888
Iteration 122/1000 | Loss: 0.00001888
Iteration 123/1000 | Loss: 0.00001888
Iteration 124/1000 | Loss: 0.00001888
Iteration 125/1000 | Loss: 0.00001888
Iteration 126/1000 | Loss: 0.00001888
Iteration 127/1000 | Loss: 0.00001888
Iteration 128/1000 | Loss: 0.00001888
Iteration 129/1000 | Loss: 0.00001888
Iteration 130/1000 | Loss: 0.00001888
Iteration 131/1000 | Loss: 0.00001888
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001887
Iteration 134/1000 | Loss: 0.00001887
Iteration 135/1000 | Loss: 0.00001887
Iteration 136/1000 | Loss: 0.00001887
Iteration 137/1000 | Loss: 0.00001887
Iteration 138/1000 | Loss: 0.00001887
Iteration 139/1000 | Loss: 0.00001887
Iteration 140/1000 | Loss: 0.00001887
Iteration 141/1000 | Loss: 0.00001887
Iteration 142/1000 | Loss: 0.00001887
Iteration 143/1000 | Loss: 0.00001887
Iteration 144/1000 | Loss: 0.00001887
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001887
Iteration 147/1000 | Loss: 0.00001887
Iteration 148/1000 | Loss: 0.00001887
Iteration 149/1000 | Loss: 0.00001887
Iteration 150/1000 | Loss: 0.00001887
Iteration 151/1000 | Loss: 0.00001887
Iteration 152/1000 | Loss: 0.00001887
Iteration 153/1000 | Loss: 0.00001887
Iteration 154/1000 | Loss: 0.00001886
Iteration 155/1000 | Loss: 0.00001886
Iteration 156/1000 | Loss: 0.00001886
Iteration 157/1000 | Loss: 0.00001886
Iteration 158/1000 | Loss: 0.00001886
Iteration 159/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.8864970115828328e-05, 1.8864970115828328e-05, 1.8864970115828328e-05, 1.8864970115828328e-05, 1.8864970115828328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8864970115828328e-05

Optimization complete. Final v2v error: 3.608792304992676 mm

Highest mean error: 4.976145267486572 mm for frame 81

Lowest mean error: 3.104607582092285 mm for frame 13

Saving results

Total time: 69.84708094596863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499603
Iteration 2/25 | Loss: 0.00133766
Iteration 3/25 | Loss: 0.00125755
Iteration 4/25 | Loss: 0.00124616
Iteration 5/25 | Loss: 0.00124419
Iteration 6/25 | Loss: 0.00124355
Iteration 7/25 | Loss: 0.00124355
Iteration 8/25 | Loss: 0.00124355
Iteration 9/25 | Loss: 0.00124355
Iteration 10/25 | Loss: 0.00124355
Iteration 11/25 | Loss: 0.00124355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012435547541826963, 0.0012435547541826963, 0.0012435547541826963, 0.0012435547541826963, 0.0012435547541826963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012435547541826963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04470158
Iteration 2/25 | Loss: 0.00059217
Iteration 3/25 | Loss: 0.00059217
Iteration 4/25 | Loss: 0.00059217
Iteration 5/25 | Loss: 0.00059217
Iteration 6/25 | Loss: 0.00059217
Iteration 7/25 | Loss: 0.00059217
Iteration 8/25 | Loss: 0.00059217
Iteration 9/25 | Loss: 0.00059217
Iteration 10/25 | Loss: 0.00059217
Iteration 11/25 | Loss: 0.00059217
Iteration 12/25 | Loss: 0.00059217
Iteration 13/25 | Loss: 0.00059217
Iteration 14/25 | Loss: 0.00059217
Iteration 15/25 | Loss: 0.00059217
Iteration 16/25 | Loss: 0.00059217
Iteration 17/25 | Loss: 0.00059217
Iteration 18/25 | Loss: 0.00059217
Iteration 19/25 | Loss: 0.00059217
Iteration 20/25 | Loss: 0.00059217
Iteration 21/25 | Loss: 0.00059217
Iteration 22/25 | Loss: 0.00059217
Iteration 23/25 | Loss: 0.00059217
Iteration 24/25 | Loss: 0.00059217
Iteration 25/25 | Loss: 0.00059217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059217
Iteration 2/1000 | Loss: 0.00005104
Iteration 3/1000 | Loss: 0.00003346
Iteration 4/1000 | Loss: 0.00002744
Iteration 5/1000 | Loss: 0.00002564
Iteration 6/1000 | Loss: 0.00002448
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002283
Iteration 9/1000 | Loss: 0.00002237
Iteration 10/1000 | Loss: 0.00002194
Iteration 11/1000 | Loss: 0.00002165
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002085
Iteration 15/1000 | Loss: 0.00002085
Iteration 16/1000 | Loss: 0.00002078
Iteration 17/1000 | Loss: 0.00002044
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002021
Iteration 20/1000 | Loss: 0.00002011
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002006
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00002002
Iteration 27/1000 | Loss: 0.00002001
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001992
Iteration 33/1000 | Loss: 0.00001987
Iteration 34/1000 | Loss: 0.00001986
Iteration 35/1000 | Loss: 0.00001986
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001980
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001979
Iteration 55/1000 | Loss: 0.00001979
Iteration 56/1000 | Loss: 0.00001979
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001978
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001978
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001977
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001976
Iteration 71/1000 | Loss: 0.00001976
Iteration 72/1000 | Loss: 0.00001976
Iteration 73/1000 | Loss: 0.00001976
Iteration 74/1000 | Loss: 0.00001976
Iteration 75/1000 | Loss: 0.00001976
Iteration 76/1000 | Loss: 0.00001976
Iteration 77/1000 | Loss: 0.00001976
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00001976
Iteration 80/1000 | Loss: 0.00001975
Iteration 81/1000 | Loss: 0.00001975
Iteration 82/1000 | Loss: 0.00001975
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001975
Iteration 85/1000 | Loss: 0.00001975
Iteration 86/1000 | Loss: 0.00001975
Iteration 87/1000 | Loss: 0.00001975
Iteration 88/1000 | Loss: 0.00001975
Iteration 89/1000 | Loss: 0.00001975
Iteration 90/1000 | Loss: 0.00001975
Iteration 91/1000 | Loss: 0.00001975
Iteration 92/1000 | Loss: 0.00001974
Iteration 93/1000 | Loss: 0.00001974
Iteration 94/1000 | Loss: 0.00001974
Iteration 95/1000 | Loss: 0.00001974
Iteration 96/1000 | Loss: 0.00001974
Iteration 97/1000 | Loss: 0.00001974
Iteration 98/1000 | Loss: 0.00001974
Iteration 99/1000 | Loss: 0.00001974
Iteration 100/1000 | Loss: 0.00001974
Iteration 101/1000 | Loss: 0.00001974
Iteration 102/1000 | Loss: 0.00001974
Iteration 103/1000 | Loss: 0.00001974
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.9735973182832822e-05, 1.9735973182832822e-05, 1.9735973182832822e-05, 1.9735973182832822e-05, 1.9735973182832822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9735973182832822e-05

Optimization complete. Final v2v error: 3.752471685409546 mm

Highest mean error: 3.787285327911377 mm for frame 5

Lowest mean error: 3.7278738021850586 mm for frame 41

Saving results

Total time: 38.08389687538147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806981
Iteration 2/25 | Loss: 0.00157616
Iteration 3/25 | Loss: 0.00127569
Iteration 4/25 | Loss: 0.00125046
Iteration 5/25 | Loss: 0.00124766
Iteration 6/25 | Loss: 0.00124718
Iteration 7/25 | Loss: 0.00124718
Iteration 8/25 | Loss: 0.00124718
Iteration 9/25 | Loss: 0.00124718
Iteration 10/25 | Loss: 0.00124718
Iteration 11/25 | Loss: 0.00124718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012471823720261455, 0.0012471823720261455, 0.0012471823720261455, 0.0012471823720261455, 0.0012471823720261455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012471823720261455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43367970
Iteration 2/25 | Loss: 0.00076000
Iteration 3/25 | Loss: 0.00076000
Iteration 4/25 | Loss: 0.00076000
Iteration 5/25 | Loss: 0.00076000
Iteration 6/25 | Loss: 0.00076000
Iteration 7/25 | Loss: 0.00076000
Iteration 8/25 | Loss: 0.00076000
Iteration 9/25 | Loss: 0.00076000
Iteration 10/25 | Loss: 0.00076000
Iteration 11/25 | Loss: 0.00076000
Iteration 12/25 | Loss: 0.00076000
Iteration 13/25 | Loss: 0.00076000
Iteration 14/25 | Loss: 0.00076000
Iteration 15/25 | Loss: 0.00076000
Iteration 16/25 | Loss: 0.00076000
Iteration 17/25 | Loss: 0.00076000
Iteration 18/25 | Loss: 0.00076000
Iteration 19/25 | Loss: 0.00076000
Iteration 20/25 | Loss: 0.00076000
Iteration 21/25 | Loss: 0.00076000
Iteration 22/25 | Loss: 0.00076000
Iteration 23/25 | Loss: 0.00076000
Iteration 24/25 | Loss: 0.00076000
Iteration 25/25 | Loss: 0.00076000

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076000
Iteration 2/1000 | Loss: 0.00002732
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001645
Iteration 5/1000 | Loss: 0.00001551
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001420
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001337
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001320
Iteration 19/1000 | Loss: 0.00001320
Iteration 20/1000 | Loss: 0.00001319
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001307
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001306
Iteration 43/1000 | Loss: 0.00001306
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001290
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001288
Iteration 135/1000 | Loss: 0.00001288
Iteration 136/1000 | Loss: 0.00001288
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001287
Iteration 141/1000 | Loss: 0.00001287
Iteration 142/1000 | Loss: 0.00001287
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001286
Iteration 157/1000 | Loss: 0.00001286
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001284
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001283
Iteration 162/1000 | Loss: 0.00001283
Iteration 163/1000 | Loss: 0.00001283
Iteration 164/1000 | Loss: 0.00001283
Iteration 165/1000 | Loss: 0.00001283
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001282
Iteration 169/1000 | Loss: 0.00001282
Iteration 170/1000 | Loss: 0.00001282
Iteration 171/1000 | Loss: 0.00001281
Iteration 172/1000 | Loss: 0.00001281
Iteration 173/1000 | Loss: 0.00001281
Iteration 174/1000 | Loss: 0.00001281
Iteration 175/1000 | Loss: 0.00001281
Iteration 176/1000 | Loss: 0.00001281
Iteration 177/1000 | Loss: 0.00001281
Iteration 178/1000 | Loss: 0.00001281
Iteration 179/1000 | Loss: 0.00001280
Iteration 180/1000 | Loss: 0.00001280
Iteration 181/1000 | Loss: 0.00001280
Iteration 182/1000 | Loss: 0.00001280
Iteration 183/1000 | Loss: 0.00001280
Iteration 184/1000 | Loss: 0.00001280
Iteration 185/1000 | Loss: 0.00001280
Iteration 186/1000 | Loss: 0.00001280
Iteration 187/1000 | Loss: 0.00001279
Iteration 188/1000 | Loss: 0.00001279
Iteration 189/1000 | Loss: 0.00001279
Iteration 190/1000 | Loss: 0.00001278
Iteration 191/1000 | Loss: 0.00001278
Iteration 192/1000 | Loss: 0.00001278
Iteration 193/1000 | Loss: 0.00001278
Iteration 194/1000 | Loss: 0.00001278
Iteration 195/1000 | Loss: 0.00001278
Iteration 196/1000 | Loss: 0.00001278
Iteration 197/1000 | Loss: 0.00001278
Iteration 198/1000 | Loss: 0.00001278
Iteration 199/1000 | Loss: 0.00001277
Iteration 200/1000 | Loss: 0.00001277
Iteration 201/1000 | Loss: 0.00001277
Iteration 202/1000 | Loss: 0.00001277
Iteration 203/1000 | Loss: 0.00001277
Iteration 204/1000 | Loss: 0.00001277
Iteration 205/1000 | Loss: 0.00001277
Iteration 206/1000 | Loss: 0.00001277
Iteration 207/1000 | Loss: 0.00001277
Iteration 208/1000 | Loss: 0.00001277
Iteration 209/1000 | Loss: 0.00001277
Iteration 210/1000 | Loss: 0.00001276
Iteration 211/1000 | Loss: 0.00001276
Iteration 212/1000 | Loss: 0.00001276
Iteration 213/1000 | Loss: 0.00001276
Iteration 214/1000 | Loss: 0.00001276
Iteration 215/1000 | Loss: 0.00001276
Iteration 216/1000 | Loss: 0.00001276
Iteration 217/1000 | Loss: 0.00001276
Iteration 218/1000 | Loss: 0.00001276
Iteration 219/1000 | Loss: 0.00001276
Iteration 220/1000 | Loss: 0.00001276
Iteration 221/1000 | Loss: 0.00001276
Iteration 222/1000 | Loss: 0.00001276
Iteration 223/1000 | Loss: 0.00001276
Iteration 224/1000 | Loss: 0.00001276
Iteration 225/1000 | Loss: 0.00001276
Iteration 226/1000 | Loss: 0.00001276
Iteration 227/1000 | Loss: 0.00001276
Iteration 228/1000 | Loss: 0.00001276
Iteration 229/1000 | Loss: 0.00001276
Iteration 230/1000 | Loss: 0.00001276
Iteration 231/1000 | Loss: 0.00001276
Iteration 232/1000 | Loss: 0.00001276
Iteration 233/1000 | Loss: 0.00001276
Iteration 234/1000 | Loss: 0.00001276
Iteration 235/1000 | Loss: 0.00001276
Iteration 236/1000 | Loss: 0.00001276
Iteration 237/1000 | Loss: 0.00001276
Iteration 238/1000 | Loss: 0.00001276
Iteration 239/1000 | Loss: 0.00001276
Iteration 240/1000 | Loss: 0.00001276
Iteration 241/1000 | Loss: 0.00001276
Iteration 242/1000 | Loss: 0.00001276
Iteration 243/1000 | Loss: 0.00001276
Iteration 244/1000 | Loss: 0.00001276
Iteration 245/1000 | Loss: 0.00001276
Iteration 246/1000 | Loss: 0.00001276
Iteration 247/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.2762022379320115e-05, 1.2762022379320115e-05, 1.2762022379320115e-05, 1.2762022379320115e-05, 1.2762022379320115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2762022379320115e-05

Optimization complete. Final v2v error: 3.0540952682495117 mm

Highest mean error: 3.3361785411834717 mm for frame 59

Lowest mean error: 2.8939521312713623 mm for frame 4

Saving results

Total time: 41.176087856292725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023707
Iteration 2/25 | Loss: 0.00256942
Iteration 3/25 | Loss: 0.00179698
Iteration 4/25 | Loss: 0.00160773
Iteration 5/25 | Loss: 0.00157915
Iteration 6/25 | Loss: 0.00148959
Iteration 7/25 | Loss: 0.00146760
Iteration 8/25 | Loss: 0.00138110
Iteration 9/25 | Loss: 0.00128787
Iteration 10/25 | Loss: 0.00130203
Iteration 11/25 | Loss: 0.00128912
Iteration 12/25 | Loss: 0.00125257
Iteration 13/25 | Loss: 0.00124847
Iteration 14/25 | Loss: 0.00124285
Iteration 15/25 | Loss: 0.00123633
Iteration 16/25 | Loss: 0.00123203
Iteration 17/25 | Loss: 0.00123031
Iteration 18/25 | Loss: 0.00122969
Iteration 19/25 | Loss: 0.00122951
Iteration 20/25 | Loss: 0.00122951
Iteration 21/25 | Loss: 0.00122950
Iteration 22/25 | Loss: 0.00122950
Iteration 23/25 | Loss: 0.00122950
Iteration 24/25 | Loss: 0.00122950
Iteration 25/25 | Loss: 0.00122949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00296187
Iteration 2/25 | Loss: 0.00079958
Iteration 3/25 | Loss: 0.00079958
Iteration 4/25 | Loss: 0.00073984
Iteration 5/25 | Loss: 0.00073984
Iteration 6/25 | Loss: 0.00073984
Iteration 7/25 | Loss: 0.00073984
Iteration 8/25 | Loss: 0.00073984
Iteration 9/25 | Loss: 0.00073984
Iteration 10/25 | Loss: 0.00073984
Iteration 11/25 | Loss: 0.00073984
Iteration 12/25 | Loss: 0.00073984
Iteration 13/25 | Loss: 0.00073984
Iteration 14/25 | Loss: 0.00073984
Iteration 15/25 | Loss: 0.00073984
Iteration 16/25 | Loss: 0.00073984
Iteration 17/25 | Loss: 0.00073984
Iteration 18/25 | Loss: 0.00073984
Iteration 19/25 | Loss: 0.00073984
Iteration 20/25 | Loss: 0.00073984
Iteration 21/25 | Loss: 0.00073984
Iteration 22/25 | Loss: 0.00073984
Iteration 23/25 | Loss: 0.00073984
Iteration 24/25 | Loss: 0.00073984
Iteration 25/25 | Loss: 0.00073984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073984
Iteration 2/1000 | Loss: 0.00005066
Iteration 3/1000 | Loss: 0.00016966
Iteration 4/1000 | Loss: 0.00031828
Iteration 5/1000 | Loss: 0.00129289
Iteration 6/1000 | Loss: 0.00007411
Iteration 7/1000 | Loss: 0.00005826
Iteration 8/1000 | Loss: 0.00029267
Iteration 9/1000 | Loss: 0.00009446
Iteration 10/1000 | Loss: 0.00003396
Iteration 11/1000 | Loss: 0.00009825
Iteration 12/1000 | Loss: 0.00057422
Iteration 13/1000 | Loss: 0.00004640
Iteration 14/1000 | Loss: 0.00003940
Iteration 15/1000 | Loss: 0.00008866
Iteration 16/1000 | Loss: 0.00002321
Iteration 17/1000 | Loss: 0.00008798
Iteration 18/1000 | Loss: 0.00008980
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00008518
Iteration 21/1000 | Loss: 0.00002432
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002240
Iteration 24/1000 | Loss: 0.00001877
Iteration 25/1000 | Loss: 0.00010540
Iteration 26/1000 | Loss: 0.00045362
Iteration 27/1000 | Loss: 0.00004641
Iteration 28/1000 | Loss: 0.00002423
Iteration 29/1000 | Loss: 0.00003995
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00008685
Iteration 32/1000 | Loss: 0.00003236
Iteration 33/1000 | Loss: 0.00002790
Iteration 34/1000 | Loss: 0.00039860
Iteration 35/1000 | Loss: 0.00019634
Iteration 36/1000 | Loss: 0.00012262
Iteration 37/1000 | Loss: 0.00007395
Iteration 38/1000 | Loss: 0.00014064
Iteration 39/1000 | Loss: 0.00003132
Iteration 40/1000 | Loss: 0.00002339
Iteration 41/1000 | Loss: 0.00004334
Iteration 42/1000 | Loss: 0.00010049
Iteration 43/1000 | Loss: 0.00004260
Iteration 44/1000 | Loss: 0.00001749
Iteration 45/1000 | Loss: 0.00012823
Iteration 46/1000 | Loss: 0.00128155
Iteration 47/1000 | Loss: 0.00002006
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00006397
Iteration 50/1000 | Loss: 0.00036083
Iteration 51/1000 | Loss: 0.00111525
Iteration 52/1000 | Loss: 0.00011597
Iteration 53/1000 | Loss: 0.00004477
Iteration 54/1000 | Loss: 0.00004418
Iteration 55/1000 | Loss: 0.00019026
Iteration 56/1000 | Loss: 0.00004809
Iteration 57/1000 | Loss: 0.00010627
Iteration 58/1000 | Loss: 0.00001972
Iteration 59/1000 | Loss: 0.00004672
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001672
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00004971
Iteration 65/1000 | Loss: 0.00002429
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00002144
Iteration 78/1000 | Loss: 0.00002332
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00002581
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00014573
Iteration 87/1000 | Loss: 0.00004157
Iteration 88/1000 | Loss: 0.00010690
Iteration 89/1000 | Loss: 0.00003371
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00004535
Iteration 92/1000 | Loss: 0.00003222
Iteration 93/1000 | Loss: 0.00004088
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00003386
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00009651
Iteration 98/1000 | Loss: 0.00004356
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00002107
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00002863
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001844
Iteration 106/1000 | Loss: 0.00004113
Iteration 107/1000 | Loss: 0.00015591
Iteration 108/1000 | Loss: 0.00002370
Iteration 109/1000 | Loss: 0.00002092
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001715
Iteration 134/1000 | Loss: 0.00001715
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001612
Iteration 137/1000 | Loss: 0.00001612
Iteration 138/1000 | Loss: 0.00001612
Iteration 139/1000 | Loss: 0.00001612
Iteration 140/1000 | Loss: 0.00001611
Iteration 141/1000 | Loss: 0.00001611
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001611
Iteration 146/1000 | Loss: 0.00001611
Iteration 147/1000 | Loss: 0.00001611
Iteration 148/1000 | Loss: 0.00001611
Iteration 149/1000 | Loss: 0.00001611
Iteration 150/1000 | Loss: 0.00001611
Iteration 151/1000 | Loss: 0.00001611
Iteration 152/1000 | Loss: 0.00001611
Iteration 153/1000 | Loss: 0.00001611
Iteration 154/1000 | Loss: 0.00001611
Iteration 155/1000 | Loss: 0.00001611
Iteration 156/1000 | Loss: 0.00001611
Iteration 157/1000 | Loss: 0.00001611
Iteration 158/1000 | Loss: 0.00001611
Iteration 159/1000 | Loss: 0.00001610
Iteration 160/1000 | Loss: 0.00001610
Iteration 161/1000 | Loss: 0.00001610
Iteration 162/1000 | Loss: 0.00001610
Iteration 163/1000 | Loss: 0.00001610
Iteration 164/1000 | Loss: 0.00001610
Iteration 165/1000 | Loss: 0.00003873
Iteration 166/1000 | Loss: 0.00003547
Iteration 167/1000 | Loss: 0.00001634
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001615
Iteration 170/1000 | Loss: 0.00001615
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001614
Iteration 173/1000 | Loss: 0.00001614
Iteration 174/1000 | Loss: 0.00001614
Iteration 175/1000 | Loss: 0.00001614
Iteration 176/1000 | Loss: 0.00001614
Iteration 177/1000 | Loss: 0.00002747
Iteration 178/1000 | Loss: 0.00001778
Iteration 179/1000 | Loss: 0.00001686
Iteration 180/1000 | Loss: 0.00001611
Iteration 181/1000 | Loss: 0.00004793
Iteration 182/1000 | Loss: 0.00001612
Iteration 183/1000 | Loss: 0.00001610
Iteration 184/1000 | Loss: 0.00001610
Iteration 185/1000 | Loss: 0.00001610
Iteration 186/1000 | Loss: 0.00001609
Iteration 187/1000 | Loss: 0.00001609
Iteration 188/1000 | Loss: 0.00001609
Iteration 189/1000 | Loss: 0.00001609
Iteration 190/1000 | Loss: 0.00001609
Iteration 191/1000 | Loss: 0.00001609
Iteration 192/1000 | Loss: 0.00001609
Iteration 193/1000 | Loss: 0.00001609
Iteration 194/1000 | Loss: 0.00001609
Iteration 195/1000 | Loss: 0.00001608
Iteration 196/1000 | Loss: 0.00001608
Iteration 197/1000 | Loss: 0.00001608
Iteration 198/1000 | Loss: 0.00001608
Iteration 199/1000 | Loss: 0.00001607
Iteration 200/1000 | Loss: 0.00001607
Iteration 201/1000 | Loss: 0.00001607
Iteration 202/1000 | Loss: 0.00001607
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Iteration 206/1000 | Loss: 0.00001606
Iteration 207/1000 | Loss: 0.00001606
Iteration 208/1000 | Loss: 0.00001606
Iteration 209/1000 | Loss: 0.00001606
Iteration 210/1000 | Loss: 0.00001606
Iteration 211/1000 | Loss: 0.00001606
Iteration 212/1000 | Loss: 0.00001606
Iteration 213/1000 | Loss: 0.00001606
Iteration 214/1000 | Loss: 0.00001606
Iteration 215/1000 | Loss: 0.00001606
Iteration 216/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.606283512955997e-05, 1.606283512955997e-05, 1.606283512955997e-05, 1.606283512955997e-05, 1.606283512955997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.606283512955997e-05

Optimization complete. Final v2v error: 3.381441116333008 mm

Highest mean error: 4.0477681159973145 mm for frame 142

Lowest mean error: 3.1077544689178467 mm for frame 35

Saving results

Total time: 176.59242463111877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00679567
Iteration 2/25 | Loss: 0.00173513
Iteration 3/25 | Loss: 0.00147817
Iteration 4/25 | Loss: 0.00143828
Iteration 5/25 | Loss: 0.00142950
Iteration 6/25 | Loss: 0.00142756
Iteration 7/25 | Loss: 0.00142755
Iteration 8/25 | Loss: 0.00142755
Iteration 9/25 | Loss: 0.00142755
Iteration 10/25 | Loss: 0.00142756
Iteration 11/25 | Loss: 0.00142755
Iteration 12/25 | Loss: 0.00142755
Iteration 13/25 | Loss: 0.00142755
Iteration 14/25 | Loss: 0.00142756
Iteration 15/25 | Loss: 0.00142755
Iteration 16/25 | Loss: 0.00142756
Iteration 17/25 | Loss: 0.00142756
Iteration 18/25 | Loss: 0.00142756
Iteration 19/25 | Loss: 0.00142755
Iteration 20/25 | Loss: 0.00142755
Iteration 21/25 | Loss: 0.00142755
Iteration 22/25 | Loss: 0.00142755
Iteration 23/25 | Loss: 0.00142755
Iteration 24/25 | Loss: 0.00142755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014275549910962582, 0.0014275549910962582, 0.0014275549910962582, 0.0014275549910962582, 0.0014275549910962582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014275549910962582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23559177
Iteration 2/25 | Loss: 0.00098501
Iteration 3/25 | Loss: 0.00098500
Iteration 4/25 | Loss: 0.00098500
Iteration 5/25 | Loss: 0.00098500
Iteration 6/25 | Loss: 0.00098500
Iteration 7/25 | Loss: 0.00098500
Iteration 8/25 | Loss: 0.00098500
Iteration 9/25 | Loss: 0.00098500
Iteration 10/25 | Loss: 0.00098500
Iteration 11/25 | Loss: 0.00098500
Iteration 12/25 | Loss: 0.00098500
Iteration 13/25 | Loss: 0.00098500
Iteration 14/25 | Loss: 0.00098500
Iteration 15/25 | Loss: 0.00098500
Iteration 16/25 | Loss: 0.00098500
Iteration 17/25 | Loss: 0.00098500
Iteration 18/25 | Loss: 0.00098500
Iteration 19/25 | Loss: 0.00098500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000985001097433269, 0.000985001097433269, 0.000985001097433269, 0.000985001097433269, 0.000985001097433269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000985001097433269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098500
Iteration 2/1000 | Loss: 0.00007116
Iteration 3/1000 | Loss: 0.00004643
Iteration 4/1000 | Loss: 0.00004196
Iteration 5/1000 | Loss: 0.00003902
Iteration 6/1000 | Loss: 0.00003715
Iteration 7/1000 | Loss: 0.00003585
Iteration 8/1000 | Loss: 0.00003470
Iteration 9/1000 | Loss: 0.00003403
Iteration 10/1000 | Loss: 0.00003349
Iteration 11/1000 | Loss: 0.00003306
Iteration 12/1000 | Loss: 0.00003270
Iteration 13/1000 | Loss: 0.00003237
Iteration 14/1000 | Loss: 0.00003216
Iteration 15/1000 | Loss: 0.00003200
Iteration 16/1000 | Loss: 0.00003188
Iteration 17/1000 | Loss: 0.00003184
Iteration 18/1000 | Loss: 0.00003180
Iteration 19/1000 | Loss: 0.00003178
Iteration 20/1000 | Loss: 0.00003177
Iteration 21/1000 | Loss: 0.00003177
Iteration 22/1000 | Loss: 0.00003173
Iteration 23/1000 | Loss: 0.00003173
Iteration 24/1000 | Loss: 0.00003170
Iteration 25/1000 | Loss: 0.00003169
Iteration 26/1000 | Loss: 0.00003168
Iteration 27/1000 | Loss: 0.00003168
Iteration 28/1000 | Loss: 0.00003164
Iteration 29/1000 | Loss: 0.00003164
Iteration 30/1000 | Loss: 0.00003162
Iteration 31/1000 | Loss: 0.00003162
Iteration 32/1000 | Loss: 0.00003161
Iteration 33/1000 | Loss: 0.00003160
Iteration 34/1000 | Loss: 0.00003160
Iteration 35/1000 | Loss: 0.00003159
Iteration 36/1000 | Loss: 0.00003159
Iteration 37/1000 | Loss: 0.00003159
Iteration 38/1000 | Loss: 0.00003159
Iteration 39/1000 | Loss: 0.00003158
Iteration 40/1000 | Loss: 0.00003158
Iteration 41/1000 | Loss: 0.00003158
Iteration 42/1000 | Loss: 0.00003158
Iteration 43/1000 | Loss: 0.00003158
Iteration 44/1000 | Loss: 0.00003158
Iteration 45/1000 | Loss: 0.00003158
Iteration 46/1000 | Loss: 0.00003157
Iteration 47/1000 | Loss: 0.00003155
Iteration 48/1000 | Loss: 0.00003154
Iteration 49/1000 | Loss: 0.00003154
Iteration 50/1000 | Loss: 0.00003154
Iteration 51/1000 | Loss: 0.00003153
Iteration 52/1000 | Loss: 0.00003153
Iteration 53/1000 | Loss: 0.00003153
Iteration 54/1000 | Loss: 0.00003152
Iteration 55/1000 | Loss: 0.00003151
Iteration 56/1000 | Loss: 0.00003151
Iteration 57/1000 | Loss: 0.00003151
Iteration 58/1000 | Loss: 0.00003150
Iteration 59/1000 | Loss: 0.00003150
Iteration 60/1000 | Loss: 0.00003150
Iteration 61/1000 | Loss: 0.00003148
Iteration 62/1000 | Loss: 0.00003148
Iteration 63/1000 | Loss: 0.00003148
Iteration 64/1000 | Loss: 0.00003147
Iteration 65/1000 | Loss: 0.00003146
Iteration 66/1000 | Loss: 0.00003146
Iteration 67/1000 | Loss: 0.00003146
Iteration 68/1000 | Loss: 0.00003145
Iteration 69/1000 | Loss: 0.00003145
Iteration 70/1000 | Loss: 0.00003145
Iteration 71/1000 | Loss: 0.00003145
Iteration 72/1000 | Loss: 0.00003144
Iteration 73/1000 | Loss: 0.00003144
Iteration 74/1000 | Loss: 0.00003144
Iteration 75/1000 | Loss: 0.00003144
Iteration 76/1000 | Loss: 0.00003144
Iteration 77/1000 | Loss: 0.00003144
Iteration 78/1000 | Loss: 0.00003144
Iteration 79/1000 | Loss: 0.00003143
Iteration 80/1000 | Loss: 0.00003143
Iteration 81/1000 | Loss: 0.00003142
Iteration 82/1000 | Loss: 0.00003142
Iteration 83/1000 | Loss: 0.00003142
Iteration 84/1000 | Loss: 0.00003141
Iteration 85/1000 | Loss: 0.00003141
Iteration 86/1000 | Loss: 0.00003141
Iteration 87/1000 | Loss: 0.00003141
Iteration 88/1000 | Loss: 0.00003140
Iteration 89/1000 | Loss: 0.00003140
Iteration 90/1000 | Loss: 0.00003140
Iteration 91/1000 | Loss: 0.00003140
Iteration 92/1000 | Loss: 0.00003140
Iteration 93/1000 | Loss: 0.00003140
Iteration 94/1000 | Loss: 0.00003140
Iteration 95/1000 | Loss: 0.00003140
Iteration 96/1000 | Loss: 0.00003140
Iteration 97/1000 | Loss: 0.00003140
Iteration 98/1000 | Loss: 0.00003139
Iteration 99/1000 | Loss: 0.00003139
Iteration 100/1000 | Loss: 0.00003139
Iteration 101/1000 | Loss: 0.00003139
Iteration 102/1000 | Loss: 0.00003139
Iteration 103/1000 | Loss: 0.00003138
Iteration 104/1000 | Loss: 0.00003138
Iteration 105/1000 | Loss: 0.00003138
Iteration 106/1000 | Loss: 0.00003138
Iteration 107/1000 | Loss: 0.00003138
Iteration 108/1000 | Loss: 0.00003138
Iteration 109/1000 | Loss: 0.00003138
Iteration 110/1000 | Loss: 0.00003138
Iteration 111/1000 | Loss: 0.00003138
Iteration 112/1000 | Loss: 0.00003137
Iteration 113/1000 | Loss: 0.00003137
Iteration 114/1000 | Loss: 0.00003137
Iteration 115/1000 | Loss: 0.00003137
Iteration 116/1000 | Loss: 0.00003137
Iteration 117/1000 | Loss: 0.00003137
Iteration 118/1000 | Loss: 0.00003137
Iteration 119/1000 | Loss: 0.00003137
Iteration 120/1000 | Loss: 0.00003136
Iteration 121/1000 | Loss: 0.00003136
Iteration 122/1000 | Loss: 0.00003136
Iteration 123/1000 | Loss: 0.00003136
Iteration 124/1000 | Loss: 0.00003136
Iteration 125/1000 | Loss: 0.00003136
Iteration 126/1000 | Loss: 0.00003136
Iteration 127/1000 | Loss: 0.00003136
Iteration 128/1000 | Loss: 0.00003136
Iteration 129/1000 | Loss: 0.00003136
Iteration 130/1000 | Loss: 0.00003136
Iteration 131/1000 | Loss: 0.00003136
Iteration 132/1000 | Loss: 0.00003135
Iteration 133/1000 | Loss: 0.00003135
Iteration 134/1000 | Loss: 0.00003135
Iteration 135/1000 | Loss: 0.00003135
Iteration 136/1000 | Loss: 0.00003135
Iteration 137/1000 | Loss: 0.00003135
Iteration 138/1000 | Loss: 0.00003135
Iteration 139/1000 | Loss: 0.00003135
Iteration 140/1000 | Loss: 0.00003135
Iteration 141/1000 | Loss: 0.00003135
Iteration 142/1000 | Loss: 0.00003135
Iteration 143/1000 | Loss: 0.00003135
Iteration 144/1000 | Loss: 0.00003135
Iteration 145/1000 | Loss: 0.00003135
Iteration 146/1000 | Loss: 0.00003135
Iteration 147/1000 | Loss: 0.00003135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [3.134945291094482e-05, 3.134945291094482e-05, 3.134945291094482e-05, 3.134945291094482e-05, 3.134945291094482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.134945291094482e-05

Optimization complete. Final v2v error: 4.690854072570801 mm

Highest mean error: 5.117326736450195 mm for frame 88

Lowest mean error: 4.2254791259765625 mm for frame 188

Saving results

Total time: 50.275389194488525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814082
Iteration 2/25 | Loss: 0.00148786
Iteration 3/25 | Loss: 0.00130343
Iteration 4/25 | Loss: 0.00129794
Iteration 5/25 | Loss: 0.00129682
Iteration 6/25 | Loss: 0.00129681
Iteration 7/25 | Loss: 0.00129681
Iteration 8/25 | Loss: 0.00129681
Iteration 9/25 | Loss: 0.00129681
Iteration 10/25 | Loss: 0.00129681
Iteration 11/25 | Loss: 0.00129681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012968116207048297, 0.0012968116207048297, 0.0012968116207048297, 0.0012968116207048297, 0.0012968116207048297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012968116207048297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03549969
Iteration 2/25 | Loss: 0.00059155
Iteration 3/25 | Loss: 0.00059155
Iteration 4/25 | Loss: 0.00059155
Iteration 5/25 | Loss: 0.00059155
Iteration 6/25 | Loss: 0.00059155
Iteration 7/25 | Loss: 0.00059155
Iteration 8/25 | Loss: 0.00059155
Iteration 9/25 | Loss: 0.00059154
Iteration 10/25 | Loss: 0.00059154
Iteration 11/25 | Loss: 0.00059154
Iteration 12/25 | Loss: 0.00059154
Iteration 13/25 | Loss: 0.00059154
Iteration 14/25 | Loss: 0.00059154
Iteration 15/25 | Loss: 0.00059154
Iteration 16/25 | Loss: 0.00059154
Iteration 17/25 | Loss: 0.00059154
Iteration 18/25 | Loss: 0.00059154
Iteration 19/25 | Loss: 0.00059154
Iteration 20/25 | Loss: 0.00059154
Iteration 21/25 | Loss: 0.00059154
Iteration 22/25 | Loss: 0.00059154
Iteration 23/25 | Loss: 0.00059154
Iteration 24/25 | Loss: 0.00059154
Iteration 25/25 | Loss: 0.00059154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059154
Iteration 2/1000 | Loss: 0.00003508
Iteration 3/1000 | Loss: 0.00002775
Iteration 4/1000 | Loss: 0.00002554
Iteration 5/1000 | Loss: 0.00002469
Iteration 6/1000 | Loss: 0.00002413
Iteration 7/1000 | Loss: 0.00002354
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002285
Iteration 10/1000 | Loss: 0.00002261
Iteration 11/1000 | Loss: 0.00002244
Iteration 12/1000 | Loss: 0.00002234
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002228
Iteration 15/1000 | Loss: 0.00002217
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002216
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002212
Iteration 22/1000 | Loss: 0.00002212
Iteration 23/1000 | Loss: 0.00002212
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002204
Iteration 30/1000 | Loss: 0.00002204
Iteration 31/1000 | Loss: 0.00002202
Iteration 32/1000 | Loss: 0.00002202
Iteration 33/1000 | Loss: 0.00002202
Iteration 34/1000 | Loss: 0.00002201
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002197
Iteration 38/1000 | Loss: 0.00002197
Iteration 39/1000 | Loss: 0.00002197
Iteration 40/1000 | Loss: 0.00002194
Iteration 41/1000 | Loss: 0.00002192
Iteration 42/1000 | Loss: 0.00002192
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002191
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002191
Iteration 48/1000 | Loss: 0.00002191
Iteration 49/1000 | Loss: 0.00002191
Iteration 50/1000 | Loss: 0.00002190
Iteration 51/1000 | Loss: 0.00002190
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002189
Iteration 54/1000 | Loss: 0.00002188
Iteration 55/1000 | Loss: 0.00002188
Iteration 56/1000 | Loss: 0.00002187
Iteration 57/1000 | Loss: 0.00002187
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002180
Iteration 61/1000 | Loss: 0.00002180
Iteration 62/1000 | Loss: 0.00002180
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002177
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00002177
Iteration 80/1000 | Loss: 0.00002177
Iteration 81/1000 | Loss: 0.00002177
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002177
Iteration 84/1000 | Loss: 0.00002176
Iteration 85/1000 | Loss: 0.00002176
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002173
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002172
Iteration 91/1000 | Loss: 0.00002172
Iteration 92/1000 | Loss: 0.00002172
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002171
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002170
Iteration 101/1000 | Loss: 0.00002170
Iteration 102/1000 | Loss: 0.00002170
Iteration 103/1000 | Loss: 0.00002170
Iteration 104/1000 | Loss: 0.00002170
Iteration 105/1000 | Loss: 0.00002170
Iteration 106/1000 | Loss: 0.00002169
Iteration 107/1000 | Loss: 0.00002169
Iteration 108/1000 | Loss: 0.00002169
Iteration 109/1000 | Loss: 0.00002169
Iteration 110/1000 | Loss: 0.00002169
Iteration 111/1000 | Loss: 0.00002169
Iteration 112/1000 | Loss: 0.00002169
Iteration 113/1000 | Loss: 0.00002169
Iteration 114/1000 | Loss: 0.00002169
Iteration 115/1000 | Loss: 0.00002169
Iteration 116/1000 | Loss: 0.00002168
Iteration 117/1000 | Loss: 0.00002168
Iteration 118/1000 | Loss: 0.00002168
Iteration 119/1000 | Loss: 0.00002168
Iteration 120/1000 | Loss: 0.00002168
Iteration 121/1000 | Loss: 0.00002168
Iteration 122/1000 | Loss: 0.00002168
Iteration 123/1000 | Loss: 0.00002168
Iteration 124/1000 | Loss: 0.00002167
Iteration 125/1000 | Loss: 0.00002167
Iteration 126/1000 | Loss: 0.00002167
Iteration 127/1000 | Loss: 0.00002167
Iteration 128/1000 | Loss: 0.00002167
Iteration 129/1000 | Loss: 0.00002167
Iteration 130/1000 | Loss: 0.00002167
Iteration 131/1000 | Loss: 0.00002167
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002167
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00002167
Iteration 137/1000 | Loss: 0.00002166
Iteration 138/1000 | Loss: 0.00002166
Iteration 139/1000 | Loss: 0.00002166
Iteration 140/1000 | Loss: 0.00002166
Iteration 141/1000 | Loss: 0.00002166
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002166
Iteration 144/1000 | Loss: 0.00002166
Iteration 145/1000 | Loss: 0.00002166
Iteration 146/1000 | Loss: 0.00002166
Iteration 147/1000 | Loss: 0.00002166
Iteration 148/1000 | Loss: 0.00002166
Iteration 149/1000 | Loss: 0.00002166
Iteration 150/1000 | Loss: 0.00002166
Iteration 151/1000 | Loss: 0.00002166
Iteration 152/1000 | Loss: 0.00002166
Iteration 153/1000 | Loss: 0.00002166
Iteration 154/1000 | Loss: 0.00002166
Iteration 155/1000 | Loss: 0.00002166
Iteration 156/1000 | Loss: 0.00002166
Iteration 157/1000 | Loss: 0.00002166
Iteration 158/1000 | Loss: 0.00002166
Iteration 159/1000 | Loss: 0.00002166
Iteration 160/1000 | Loss: 0.00002166
Iteration 161/1000 | Loss: 0.00002166
Iteration 162/1000 | Loss: 0.00002166
Iteration 163/1000 | Loss: 0.00002166
Iteration 164/1000 | Loss: 0.00002166
Iteration 165/1000 | Loss: 0.00002166
Iteration 166/1000 | Loss: 0.00002166
Iteration 167/1000 | Loss: 0.00002166
Iteration 168/1000 | Loss: 0.00002166
Iteration 169/1000 | Loss: 0.00002166
Iteration 170/1000 | Loss: 0.00002166
Iteration 171/1000 | Loss: 0.00002166
Iteration 172/1000 | Loss: 0.00002166
Iteration 173/1000 | Loss: 0.00002166
Iteration 174/1000 | Loss: 0.00002166
Iteration 175/1000 | Loss: 0.00002166
Iteration 176/1000 | Loss: 0.00002166
Iteration 177/1000 | Loss: 0.00002166
Iteration 178/1000 | Loss: 0.00002166
Iteration 179/1000 | Loss: 0.00002166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.165804835385643e-05, 2.165804835385643e-05, 2.165804835385643e-05, 2.165804835385643e-05, 2.165804835385643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.165804835385643e-05

Optimization complete. Final v2v error: 3.824824810028076 mm

Highest mean error: 3.9561922550201416 mm for frame 108

Lowest mean error: 3.676013708114624 mm for frame 36

Saving results

Total time: 38.37800097465515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991919
Iteration 2/25 | Loss: 0.00169416
Iteration 3/25 | Loss: 0.00148381
Iteration 4/25 | Loss: 0.00140658
Iteration 5/25 | Loss: 0.00136167
Iteration 6/25 | Loss: 0.00136744
Iteration 7/25 | Loss: 0.00130479
Iteration 8/25 | Loss: 0.00129595
Iteration 9/25 | Loss: 0.00128849
Iteration 10/25 | Loss: 0.00128855
Iteration 11/25 | Loss: 0.00128543
Iteration 12/25 | Loss: 0.00128298
Iteration 13/25 | Loss: 0.00128096
Iteration 14/25 | Loss: 0.00128015
Iteration 15/25 | Loss: 0.00127996
Iteration 16/25 | Loss: 0.00127993
Iteration 17/25 | Loss: 0.00127993
Iteration 18/25 | Loss: 0.00127992
Iteration 19/25 | Loss: 0.00127992
Iteration 20/25 | Loss: 0.00127992
Iteration 21/25 | Loss: 0.00127992
Iteration 22/25 | Loss: 0.00127992
Iteration 23/25 | Loss: 0.00127992
Iteration 24/25 | Loss: 0.00127992
Iteration 25/25 | Loss: 0.00127992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22361231
Iteration 2/25 | Loss: 0.00100246
Iteration 3/25 | Loss: 0.00100246
Iteration 4/25 | Loss: 0.00100245
Iteration 5/25 | Loss: 0.00100245
Iteration 6/25 | Loss: 0.00100245
Iteration 7/25 | Loss: 0.00100245
Iteration 8/25 | Loss: 0.00100245
Iteration 9/25 | Loss: 0.00100245
Iteration 10/25 | Loss: 0.00100245
Iteration 11/25 | Loss: 0.00100245
Iteration 12/25 | Loss: 0.00100245
Iteration 13/25 | Loss: 0.00100245
Iteration 14/25 | Loss: 0.00100245
Iteration 15/25 | Loss: 0.00100245
Iteration 16/25 | Loss: 0.00100245
Iteration 17/25 | Loss: 0.00100245
Iteration 18/25 | Loss: 0.00100245
Iteration 19/25 | Loss: 0.00100245
Iteration 20/25 | Loss: 0.00100245
Iteration 21/25 | Loss: 0.00100245
Iteration 22/25 | Loss: 0.00100245
Iteration 23/25 | Loss: 0.00100245
Iteration 24/25 | Loss: 0.00100245
Iteration 25/25 | Loss: 0.00100245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100245
Iteration 2/1000 | Loss: 0.00004997
Iteration 3/1000 | Loss: 0.00019268
Iteration 4/1000 | Loss: 0.00003603
Iteration 5/1000 | Loss: 0.00002937
Iteration 6/1000 | Loss: 0.00002719
Iteration 7/1000 | Loss: 0.00002567
Iteration 8/1000 | Loss: 0.00011977
Iteration 9/1000 | Loss: 0.00038774
Iteration 10/1000 | Loss: 0.00041718
Iteration 11/1000 | Loss: 0.00002550
Iteration 12/1000 | Loss: 0.00002374
Iteration 13/1000 | Loss: 0.00036649
Iteration 14/1000 | Loss: 0.00003568
Iteration 15/1000 | Loss: 0.00008052
Iteration 16/1000 | Loss: 0.00002869
Iteration 17/1000 | Loss: 0.00007628
Iteration 18/1000 | Loss: 0.00002830
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00076639
Iteration 21/1000 | Loss: 0.00152385
Iteration 22/1000 | Loss: 0.00082182
Iteration 23/1000 | Loss: 0.00059090
Iteration 24/1000 | Loss: 0.00003624
Iteration 25/1000 | Loss: 0.00013906
Iteration 26/1000 | Loss: 0.00003227
Iteration 27/1000 | Loss: 0.00002310
Iteration 28/1000 | Loss: 0.00058144
Iteration 29/1000 | Loss: 0.00031905
Iteration 30/1000 | Loss: 0.00046633
Iteration 31/1000 | Loss: 0.00024181
Iteration 32/1000 | Loss: 0.00004466
Iteration 33/1000 | Loss: 0.00003119
Iteration 34/1000 | Loss: 0.00002423
Iteration 35/1000 | Loss: 0.00002123
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001759
Iteration 43/1000 | Loss: 0.00017656
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001742
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001728
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001696
Iteration 111/1000 | Loss: 0.00001696
Iteration 112/1000 | Loss: 0.00001696
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001692
Iteration 128/1000 | Loss: 0.00001692
Iteration 129/1000 | Loss: 0.00001692
Iteration 130/1000 | Loss: 0.00001692
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001691
Iteration 134/1000 | Loss: 0.00001691
Iteration 135/1000 | Loss: 0.00001691
Iteration 136/1000 | Loss: 0.00001691
Iteration 137/1000 | Loss: 0.00001691
Iteration 138/1000 | Loss: 0.00001691
Iteration 139/1000 | Loss: 0.00001691
Iteration 140/1000 | Loss: 0.00001691
Iteration 141/1000 | Loss: 0.00001690
Iteration 142/1000 | Loss: 0.00001690
Iteration 143/1000 | Loss: 0.00001690
Iteration 144/1000 | Loss: 0.00001690
Iteration 145/1000 | Loss: 0.00001690
Iteration 146/1000 | Loss: 0.00001690
Iteration 147/1000 | Loss: 0.00001690
Iteration 148/1000 | Loss: 0.00001690
Iteration 149/1000 | Loss: 0.00001690
Iteration 150/1000 | Loss: 0.00001690
Iteration 151/1000 | Loss: 0.00001690
Iteration 152/1000 | Loss: 0.00001690
Iteration 153/1000 | Loss: 0.00001689
Iteration 154/1000 | Loss: 0.00001689
Iteration 155/1000 | Loss: 0.00001689
Iteration 156/1000 | Loss: 0.00001689
Iteration 157/1000 | Loss: 0.00001689
Iteration 158/1000 | Loss: 0.00001689
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001689
Iteration 161/1000 | Loss: 0.00001689
Iteration 162/1000 | Loss: 0.00001689
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001689
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001689
Iteration 172/1000 | Loss: 0.00001689
Iteration 173/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.6891623090486974e-05, 1.6891623090486974e-05, 1.6891623090486974e-05, 1.6891623090486974e-05, 1.6891623090486974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6891623090486974e-05

Optimization complete. Final v2v error: 3.4774112701416016 mm

Highest mean error: 4.729997158050537 mm for frame 152

Lowest mean error: 2.9773764610290527 mm for frame 81

Saving results

Total time: 107.71026396751404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452320
Iteration 2/25 | Loss: 0.00127163
Iteration 3/25 | Loss: 0.00120806
Iteration 4/25 | Loss: 0.00119779
Iteration 5/25 | Loss: 0.00119467
Iteration 6/25 | Loss: 0.00119406
Iteration 7/25 | Loss: 0.00119406
Iteration 8/25 | Loss: 0.00119406
Iteration 9/25 | Loss: 0.00119406
Iteration 10/25 | Loss: 0.00119406
Iteration 11/25 | Loss: 0.00119406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011940556578338146, 0.0011940556578338146, 0.0011940556578338146, 0.0011940556578338146, 0.0011940556578338146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011940556578338146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57156801
Iteration 2/25 | Loss: 0.00074472
Iteration 3/25 | Loss: 0.00074472
Iteration 4/25 | Loss: 0.00074472
Iteration 5/25 | Loss: 0.00074472
Iteration 6/25 | Loss: 0.00074472
Iteration 7/25 | Loss: 0.00074472
Iteration 8/25 | Loss: 0.00074472
Iteration 9/25 | Loss: 0.00074472
Iteration 10/25 | Loss: 0.00074472
Iteration 11/25 | Loss: 0.00074472
Iteration 12/25 | Loss: 0.00074472
Iteration 13/25 | Loss: 0.00074472
Iteration 14/25 | Loss: 0.00074472
Iteration 15/25 | Loss: 0.00074472
Iteration 16/25 | Loss: 0.00074472
Iteration 17/25 | Loss: 0.00074472
Iteration 18/25 | Loss: 0.00074472
Iteration 19/25 | Loss: 0.00074472
Iteration 20/25 | Loss: 0.00074472
Iteration 21/25 | Loss: 0.00074472
Iteration 22/25 | Loss: 0.00074472
Iteration 23/25 | Loss: 0.00074472
Iteration 24/25 | Loss: 0.00074472
Iteration 25/25 | Loss: 0.00074472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074472
Iteration 2/1000 | Loss: 0.00002676
Iteration 3/1000 | Loss: 0.00001786
Iteration 4/1000 | Loss: 0.00001466
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001307
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001229
Iteration 11/1000 | Loss: 0.00001220
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001187
Iteration 19/1000 | Loss: 0.00001181
Iteration 20/1000 | Loss: 0.00001177
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001146
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001146
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001144
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001141
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001136
Iteration 71/1000 | Loss: 0.00001136
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001135
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001131
Iteration 85/1000 | Loss: 0.00001131
Iteration 86/1000 | Loss: 0.00001131
Iteration 87/1000 | Loss: 0.00001131
Iteration 88/1000 | Loss: 0.00001131
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001129
Iteration 93/1000 | Loss: 0.00001129
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001128
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001126
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001123
Iteration 122/1000 | Loss: 0.00001123
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001123
Iteration 125/1000 | Loss: 0.00001123
Iteration 126/1000 | Loss: 0.00001123
Iteration 127/1000 | Loss: 0.00001123
Iteration 128/1000 | Loss: 0.00001123
Iteration 129/1000 | Loss: 0.00001123
Iteration 130/1000 | Loss: 0.00001123
Iteration 131/1000 | Loss: 0.00001122
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001120
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001119
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001119
Iteration 149/1000 | Loss: 0.00001119
Iteration 150/1000 | Loss: 0.00001119
Iteration 151/1000 | Loss: 0.00001119
Iteration 152/1000 | Loss: 0.00001118
Iteration 153/1000 | Loss: 0.00001118
Iteration 154/1000 | Loss: 0.00001118
Iteration 155/1000 | Loss: 0.00001118
Iteration 156/1000 | Loss: 0.00001118
Iteration 157/1000 | Loss: 0.00001118
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001117
Iteration 160/1000 | Loss: 0.00001117
Iteration 161/1000 | Loss: 0.00001117
Iteration 162/1000 | Loss: 0.00001117
Iteration 163/1000 | Loss: 0.00001117
Iteration 164/1000 | Loss: 0.00001117
Iteration 165/1000 | Loss: 0.00001117
Iteration 166/1000 | Loss: 0.00001117
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001116
Iteration 174/1000 | Loss: 0.00001116
Iteration 175/1000 | Loss: 0.00001116
Iteration 176/1000 | Loss: 0.00001116
Iteration 177/1000 | Loss: 0.00001116
Iteration 178/1000 | Loss: 0.00001116
Iteration 179/1000 | Loss: 0.00001116
Iteration 180/1000 | Loss: 0.00001116
Iteration 181/1000 | Loss: 0.00001116
Iteration 182/1000 | Loss: 0.00001116
Iteration 183/1000 | Loss: 0.00001116
Iteration 184/1000 | Loss: 0.00001116
Iteration 185/1000 | Loss: 0.00001116
Iteration 186/1000 | Loss: 0.00001116
Iteration 187/1000 | Loss: 0.00001116
Iteration 188/1000 | Loss: 0.00001116
Iteration 189/1000 | Loss: 0.00001116
Iteration 190/1000 | Loss: 0.00001116
Iteration 191/1000 | Loss: 0.00001116
Iteration 192/1000 | Loss: 0.00001116
Iteration 193/1000 | Loss: 0.00001116
Iteration 194/1000 | Loss: 0.00001116
Iteration 195/1000 | Loss: 0.00001116
Iteration 196/1000 | Loss: 0.00001116
Iteration 197/1000 | Loss: 0.00001116
Iteration 198/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.1156492291775066e-05, 1.1156492291775066e-05, 1.1156492291775066e-05, 1.1156492291775066e-05, 1.1156492291775066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1156492291775066e-05

Optimization complete. Final v2v error: 2.877155303955078 mm

Highest mean error: 3.0334317684173584 mm for frame 40

Lowest mean error: 2.7406423091888428 mm for frame 104

Saving results

Total time: 38.55433249473572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556591
Iteration 2/25 | Loss: 0.00137156
Iteration 3/25 | Loss: 0.00129691
Iteration 4/25 | Loss: 0.00128456
Iteration 5/25 | Loss: 0.00128101
Iteration 6/25 | Loss: 0.00128087
Iteration 7/25 | Loss: 0.00128087
Iteration 8/25 | Loss: 0.00128087
Iteration 9/25 | Loss: 0.00128087
Iteration 10/25 | Loss: 0.00128087
Iteration 11/25 | Loss: 0.00128087
Iteration 12/25 | Loss: 0.00128087
Iteration 13/25 | Loss: 0.00128087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012808714527636766, 0.0012808714527636766, 0.0012808714527636766, 0.0012808714527636766, 0.0012808714527636766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012808714527636766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39547718
Iteration 2/25 | Loss: 0.00077398
Iteration 3/25 | Loss: 0.00077393
Iteration 4/25 | Loss: 0.00077393
Iteration 5/25 | Loss: 0.00077393
Iteration 6/25 | Loss: 0.00077393
Iteration 7/25 | Loss: 0.00077393
Iteration 8/25 | Loss: 0.00077393
Iteration 9/25 | Loss: 0.00077393
Iteration 10/25 | Loss: 0.00077393
Iteration 11/25 | Loss: 0.00077393
Iteration 12/25 | Loss: 0.00077393
Iteration 13/25 | Loss: 0.00077393
Iteration 14/25 | Loss: 0.00077393
Iteration 15/25 | Loss: 0.00077393
Iteration 16/25 | Loss: 0.00077393
Iteration 17/25 | Loss: 0.00077393
Iteration 18/25 | Loss: 0.00077393
Iteration 19/25 | Loss: 0.00077393
Iteration 20/25 | Loss: 0.00077393
Iteration 21/25 | Loss: 0.00077393
Iteration 22/25 | Loss: 0.00077393
Iteration 23/25 | Loss: 0.00077392
Iteration 24/25 | Loss: 0.00077393
Iteration 25/25 | Loss: 0.00077393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077393
Iteration 2/1000 | Loss: 0.00002864
Iteration 3/1000 | Loss: 0.00001961
Iteration 4/1000 | Loss: 0.00001756
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001527
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001349
Iteration 26/1000 | Loss: 0.00001349
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001348
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001347
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001346
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001345
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001345
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001344
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001342
Iteration 81/1000 | Loss: 0.00001342
Iteration 82/1000 | Loss: 0.00001342
Iteration 83/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.3418056369118858e-05, 1.3418056369118858e-05, 1.3418056369118858e-05, 1.3418056369118858e-05, 1.3418056369118858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3418056369118858e-05

Optimization complete. Final v2v error: 3.121696949005127 mm

Highest mean error: 3.2610373497009277 mm for frame 148

Lowest mean error: 3.029115676879883 mm for frame 96

Saving results

Total time: 30.894347667694092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846531
Iteration 2/25 | Loss: 0.00170423
Iteration 3/25 | Loss: 0.00145339
Iteration 4/25 | Loss: 0.00139882
Iteration 5/25 | Loss: 0.00138536
Iteration 6/25 | Loss: 0.00138325
Iteration 7/25 | Loss: 0.00137846
Iteration 8/25 | Loss: 0.00137614
Iteration 9/25 | Loss: 0.00137490
Iteration 10/25 | Loss: 0.00137427
Iteration 11/25 | Loss: 0.00137406
Iteration 12/25 | Loss: 0.00137375
Iteration 13/25 | Loss: 0.00137680
Iteration 14/25 | Loss: 0.00137565
Iteration 15/25 | Loss: 0.00137312
Iteration 16/25 | Loss: 0.00137114
Iteration 17/25 | Loss: 0.00137017
Iteration 18/25 | Loss: 0.00136971
Iteration 19/25 | Loss: 0.00136935
Iteration 20/25 | Loss: 0.00136925
Iteration 21/25 | Loss: 0.00136921
Iteration 22/25 | Loss: 0.00136921
Iteration 23/25 | Loss: 0.00136921
Iteration 24/25 | Loss: 0.00136921
Iteration 25/25 | Loss: 0.00136921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36193633
Iteration 2/25 | Loss: 0.00125147
Iteration 3/25 | Loss: 0.00125145
Iteration 4/25 | Loss: 0.00125145
Iteration 5/25 | Loss: 0.00125145
Iteration 6/25 | Loss: 0.00125145
Iteration 7/25 | Loss: 0.00125145
Iteration 8/25 | Loss: 0.00125145
Iteration 9/25 | Loss: 0.00125145
Iteration 10/25 | Loss: 0.00125145
Iteration 11/25 | Loss: 0.00125145
Iteration 12/25 | Loss: 0.00125145
Iteration 13/25 | Loss: 0.00125145
Iteration 14/25 | Loss: 0.00125145
Iteration 15/25 | Loss: 0.00125145
Iteration 16/25 | Loss: 0.00125145
Iteration 17/25 | Loss: 0.00125145
Iteration 18/25 | Loss: 0.00125145
Iteration 19/25 | Loss: 0.00125145
Iteration 20/25 | Loss: 0.00125145
Iteration 21/25 | Loss: 0.00125145
Iteration 22/25 | Loss: 0.00125145
Iteration 23/25 | Loss: 0.00125145
Iteration 24/25 | Loss: 0.00125145
Iteration 25/25 | Loss: 0.00125145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125145
Iteration 2/1000 | Loss: 0.00015898
Iteration 3/1000 | Loss: 0.00011674
Iteration 4/1000 | Loss: 0.00009975
Iteration 5/1000 | Loss: 0.00009193
Iteration 6/1000 | Loss: 0.00008635
Iteration 7/1000 | Loss: 0.00008274
Iteration 8/1000 | Loss: 0.00010011
Iteration 9/1000 | Loss: 0.00067813
Iteration 10/1000 | Loss: 0.00007677
Iteration 11/1000 | Loss: 0.00009618
Iteration 12/1000 | Loss: 0.00006988
Iteration 13/1000 | Loss: 0.00008979
Iteration 14/1000 | Loss: 0.00006581
Iteration 15/1000 | Loss: 0.00009506
Iteration 16/1000 | Loss: 0.00006253
Iteration 17/1000 | Loss: 0.00006027
Iteration 18/1000 | Loss: 0.00005861
Iteration 19/1000 | Loss: 0.00005730
Iteration 20/1000 | Loss: 0.00005628
Iteration 21/1000 | Loss: 0.00005556
Iteration 22/1000 | Loss: 0.00005500
Iteration 23/1000 | Loss: 0.00005444
Iteration 24/1000 | Loss: 0.00005399
Iteration 25/1000 | Loss: 0.00005353
Iteration 26/1000 | Loss: 0.00005322
Iteration 27/1000 | Loss: 0.00005294
Iteration 28/1000 | Loss: 0.00005263
Iteration 29/1000 | Loss: 0.00005234
Iteration 30/1000 | Loss: 0.00005212
Iteration 31/1000 | Loss: 0.00005192
Iteration 32/1000 | Loss: 0.00005177
Iteration 33/1000 | Loss: 0.00005174
Iteration 34/1000 | Loss: 0.00005164
Iteration 35/1000 | Loss: 0.00005157
Iteration 36/1000 | Loss: 0.00005156
Iteration 37/1000 | Loss: 0.00005154
Iteration 38/1000 | Loss: 0.00005154
Iteration 39/1000 | Loss: 0.00005149
Iteration 40/1000 | Loss: 0.00005142
Iteration 41/1000 | Loss: 0.00005138
Iteration 42/1000 | Loss: 0.00005137
Iteration 43/1000 | Loss: 0.00005131
Iteration 44/1000 | Loss: 0.00005119
Iteration 45/1000 | Loss: 0.00005118
Iteration 46/1000 | Loss: 0.00005112
Iteration 47/1000 | Loss: 0.00005111
Iteration 48/1000 | Loss: 0.00005110
Iteration 49/1000 | Loss: 0.00066729
Iteration 50/1000 | Loss: 0.00355166
Iteration 51/1000 | Loss: 0.00259140
Iteration 52/1000 | Loss: 0.00228529
Iteration 53/1000 | Loss: 0.00208266
Iteration 54/1000 | Loss: 0.00112562
Iteration 55/1000 | Loss: 0.00183235
Iteration 56/1000 | Loss: 0.00143825
Iteration 57/1000 | Loss: 0.00106170
Iteration 58/1000 | Loss: 0.00077758
Iteration 59/1000 | Loss: 0.00087910
Iteration 60/1000 | Loss: 0.00005919
Iteration 61/1000 | Loss: 0.00004903
Iteration 62/1000 | Loss: 0.00004611
Iteration 63/1000 | Loss: 0.00004393
Iteration 64/1000 | Loss: 0.00004200
Iteration 65/1000 | Loss: 0.00004088
Iteration 66/1000 | Loss: 0.00003958
Iteration 67/1000 | Loss: 0.00003875
Iteration 68/1000 | Loss: 0.00003798
Iteration 69/1000 | Loss: 0.00067263
Iteration 70/1000 | Loss: 0.00067262
Iteration 71/1000 | Loss: 0.00005418
Iteration 72/1000 | Loss: 0.00004093
Iteration 73/1000 | Loss: 0.00003809
Iteration 74/1000 | Loss: 0.00003707
Iteration 75/1000 | Loss: 0.00003661
Iteration 76/1000 | Loss: 0.00003616
Iteration 77/1000 | Loss: 0.00067857
Iteration 78/1000 | Loss: 0.00038482
Iteration 79/1000 | Loss: 0.00060589
Iteration 80/1000 | Loss: 0.00031038
Iteration 81/1000 | Loss: 0.00003681
Iteration 82/1000 | Loss: 0.00003565
Iteration 83/1000 | Loss: 0.00003560
Iteration 84/1000 | Loss: 0.00003558
Iteration 85/1000 | Loss: 0.00003554
Iteration 86/1000 | Loss: 0.00003549
Iteration 87/1000 | Loss: 0.00003545
Iteration 88/1000 | Loss: 0.00003544
Iteration 89/1000 | Loss: 0.00063264
Iteration 90/1000 | Loss: 0.00044508
Iteration 91/1000 | Loss: 0.00025536
Iteration 92/1000 | Loss: 0.00003840
Iteration 93/1000 | Loss: 0.00003624
Iteration 94/1000 | Loss: 0.00003537
Iteration 95/1000 | Loss: 0.00003476
Iteration 96/1000 | Loss: 0.00003428
Iteration 97/1000 | Loss: 0.00003388
Iteration 98/1000 | Loss: 0.00003362
Iteration 99/1000 | Loss: 0.00003356
Iteration 100/1000 | Loss: 0.00003336
Iteration 101/1000 | Loss: 0.00003328
Iteration 102/1000 | Loss: 0.00003327
Iteration 103/1000 | Loss: 0.00003325
Iteration 104/1000 | Loss: 0.00003325
Iteration 105/1000 | Loss: 0.00003324
Iteration 106/1000 | Loss: 0.00003323
Iteration 107/1000 | Loss: 0.00003323
Iteration 108/1000 | Loss: 0.00003323
Iteration 109/1000 | Loss: 0.00003322
Iteration 110/1000 | Loss: 0.00003322
Iteration 111/1000 | Loss: 0.00003322
Iteration 112/1000 | Loss: 0.00003321
Iteration 113/1000 | Loss: 0.00003321
Iteration 114/1000 | Loss: 0.00003321
Iteration 115/1000 | Loss: 0.00003320
Iteration 116/1000 | Loss: 0.00003318
Iteration 117/1000 | Loss: 0.00003318
Iteration 118/1000 | Loss: 0.00003318
Iteration 119/1000 | Loss: 0.00003317
Iteration 120/1000 | Loss: 0.00003316
Iteration 121/1000 | Loss: 0.00003316
Iteration 122/1000 | Loss: 0.00003315
Iteration 123/1000 | Loss: 0.00003315
Iteration 124/1000 | Loss: 0.00003315
Iteration 125/1000 | Loss: 0.00003314
Iteration 126/1000 | Loss: 0.00003314
Iteration 127/1000 | Loss: 0.00003314
Iteration 128/1000 | Loss: 0.00003313
Iteration 129/1000 | Loss: 0.00003313
Iteration 130/1000 | Loss: 0.00003313
Iteration 131/1000 | Loss: 0.00003312
Iteration 132/1000 | Loss: 0.00003312
Iteration 133/1000 | Loss: 0.00003312
Iteration 134/1000 | Loss: 0.00003311
Iteration 135/1000 | Loss: 0.00003311
Iteration 136/1000 | Loss: 0.00003311
Iteration 137/1000 | Loss: 0.00003310
Iteration 138/1000 | Loss: 0.00003310
Iteration 139/1000 | Loss: 0.00003310
Iteration 140/1000 | Loss: 0.00003310
Iteration 141/1000 | Loss: 0.00003310
Iteration 142/1000 | Loss: 0.00003310
Iteration 143/1000 | Loss: 0.00003309
Iteration 144/1000 | Loss: 0.00003309
Iteration 145/1000 | Loss: 0.00003309
Iteration 146/1000 | Loss: 0.00003309
Iteration 147/1000 | Loss: 0.00003308
Iteration 148/1000 | Loss: 0.00003308
Iteration 149/1000 | Loss: 0.00003307
Iteration 150/1000 | Loss: 0.00003307
Iteration 151/1000 | Loss: 0.00003307
Iteration 152/1000 | Loss: 0.00003307
Iteration 153/1000 | Loss: 0.00003307
Iteration 154/1000 | Loss: 0.00003307
Iteration 155/1000 | Loss: 0.00003307
Iteration 156/1000 | Loss: 0.00003307
Iteration 157/1000 | Loss: 0.00003307
Iteration 158/1000 | Loss: 0.00003307
Iteration 159/1000 | Loss: 0.00003307
Iteration 160/1000 | Loss: 0.00003307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.3068645279854536e-05, 3.3068645279854536e-05, 3.3068645279854536e-05, 3.3068645279854536e-05, 3.3068645279854536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3068645279854536e-05

Optimization complete. Final v2v error: 4.62212610244751 mm

Highest mean error: 6.977418422698975 mm for frame 58

Lowest mean error: 4.032072067260742 mm for frame 116

Saving results

Total time: 180.40573024749756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541901
Iteration 2/25 | Loss: 0.00147892
Iteration 3/25 | Loss: 0.00133699
Iteration 4/25 | Loss: 0.00132441
Iteration 5/25 | Loss: 0.00132246
Iteration 6/25 | Loss: 0.00132246
Iteration 7/25 | Loss: 0.00132246
Iteration 8/25 | Loss: 0.00132246
Iteration 9/25 | Loss: 0.00132246
Iteration 10/25 | Loss: 0.00132246
Iteration 11/25 | Loss: 0.00132246
Iteration 12/25 | Loss: 0.00132246
Iteration 13/25 | Loss: 0.00132246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013224586145952344, 0.0013224586145952344, 0.0013224586145952344, 0.0013224586145952344, 0.0013224586145952344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013224586145952344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88721603
Iteration 2/25 | Loss: 0.00084112
Iteration 3/25 | Loss: 0.00084111
Iteration 4/25 | Loss: 0.00084111
Iteration 5/25 | Loss: 0.00084111
Iteration 6/25 | Loss: 0.00084111
Iteration 7/25 | Loss: 0.00084111
Iteration 8/25 | Loss: 0.00084111
Iteration 9/25 | Loss: 0.00084111
Iteration 10/25 | Loss: 0.00084111
Iteration 11/25 | Loss: 0.00084111
Iteration 12/25 | Loss: 0.00084111
Iteration 13/25 | Loss: 0.00084111
Iteration 14/25 | Loss: 0.00084111
Iteration 15/25 | Loss: 0.00084111
Iteration 16/25 | Loss: 0.00084111
Iteration 17/25 | Loss: 0.00084111
Iteration 18/25 | Loss: 0.00084111
Iteration 19/25 | Loss: 0.00084111
Iteration 20/25 | Loss: 0.00084111
Iteration 21/25 | Loss: 0.00084111
Iteration 22/25 | Loss: 0.00084111
Iteration 23/25 | Loss: 0.00084111
Iteration 24/25 | Loss: 0.00084111
Iteration 25/25 | Loss: 0.00084111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084111
Iteration 2/1000 | Loss: 0.00003317
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002258
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002095
Iteration 7/1000 | Loss: 0.00002042
Iteration 8/1000 | Loss: 0.00002012
Iteration 9/1000 | Loss: 0.00001974
Iteration 10/1000 | Loss: 0.00001944
Iteration 11/1000 | Loss: 0.00001917
Iteration 12/1000 | Loss: 0.00001889
Iteration 13/1000 | Loss: 0.00001868
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001782
Iteration 19/1000 | Loss: 0.00001771
Iteration 20/1000 | Loss: 0.00001771
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001765
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001765
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001764
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001761
Iteration 36/1000 | Loss: 0.00001761
Iteration 37/1000 | Loss: 0.00001760
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001757
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001753
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001752
Iteration 69/1000 | Loss: 0.00001752
Iteration 70/1000 | Loss: 0.00001752
Iteration 71/1000 | Loss: 0.00001752
Iteration 72/1000 | Loss: 0.00001752
Iteration 73/1000 | Loss: 0.00001752
Iteration 74/1000 | Loss: 0.00001751
Iteration 75/1000 | Loss: 0.00001751
Iteration 76/1000 | Loss: 0.00001751
Iteration 77/1000 | Loss: 0.00001751
Iteration 78/1000 | Loss: 0.00001751
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001750
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001748
Iteration 95/1000 | Loss: 0.00001748
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001747
Iteration 99/1000 | Loss: 0.00001747
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001746
Iteration 116/1000 | Loss: 0.00001746
Iteration 117/1000 | Loss: 0.00001746
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Iteration 134/1000 | Loss: 0.00001743
Iteration 135/1000 | Loss: 0.00001743
Iteration 136/1000 | Loss: 0.00001743
Iteration 137/1000 | Loss: 0.00001743
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001742
Iteration 140/1000 | Loss: 0.00001742
Iteration 141/1000 | Loss: 0.00001742
Iteration 142/1000 | Loss: 0.00001742
Iteration 143/1000 | Loss: 0.00001742
Iteration 144/1000 | Loss: 0.00001741
Iteration 145/1000 | Loss: 0.00001741
Iteration 146/1000 | Loss: 0.00001741
Iteration 147/1000 | Loss: 0.00001741
Iteration 148/1000 | Loss: 0.00001741
Iteration 149/1000 | Loss: 0.00001741
Iteration 150/1000 | Loss: 0.00001741
Iteration 151/1000 | Loss: 0.00001741
Iteration 152/1000 | Loss: 0.00001741
Iteration 153/1000 | Loss: 0.00001741
Iteration 154/1000 | Loss: 0.00001741
Iteration 155/1000 | Loss: 0.00001741
Iteration 156/1000 | Loss: 0.00001741
Iteration 157/1000 | Loss: 0.00001741
Iteration 158/1000 | Loss: 0.00001741
Iteration 159/1000 | Loss: 0.00001741
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001741
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001741
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Iteration 172/1000 | Loss: 0.00001741
Iteration 173/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.7411688531865366e-05, 1.7411688531865366e-05, 1.7411688531865366e-05, 1.7411688531865366e-05, 1.7411688531865366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7411688531865366e-05

Optimization complete. Final v2v error: 3.4948394298553467 mm

Highest mean error: 3.7837905883789062 mm for frame 57

Lowest mean error: 3.1023895740509033 mm for frame 218

Saving results

Total time: 50.56025981903076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803301
Iteration 2/25 | Loss: 0.00177558
Iteration 3/25 | Loss: 0.00141657
Iteration 4/25 | Loss: 0.00137284
Iteration 5/25 | Loss: 0.00136643
Iteration 6/25 | Loss: 0.00136472
Iteration 7/25 | Loss: 0.00136444
Iteration 8/25 | Loss: 0.00136444
Iteration 9/25 | Loss: 0.00136444
Iteration 10/25 | Loss: 0.00136444
Iteration 11/25 | Loss: 0.00136444
Iteration 12/25 | Loss: 0.00136444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001364437979646027, 0.001364437979646027, 0.001364437979646027, 0.001364437979646027, 0.001364437979646027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001364437979646027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47586608
Iteration 2/25 | Loss: 0.00077130
Iteration 3/25 | Loss: 0.00077130
Iteration 4/25 | Loss: 0.00077129
Iteration 5/25 | Loss: 0.00077129
Iteration 6/25 | Loss: 0.00077129
Iteration 7/25 | Loss: 0.00077129
Iteration 8/25 | Loss: 0.00077129
Iteration 9/25 | Loss: 0.00077129
Iteration 10/25 | Loss: 0.00077129
Iteration 11/25 | Loss: 0.00077129
Iteration 12/25 | Loss: 0.00077129
Iteration 13/25 | Loss: 0.00077129
Iteration 14/25 | Loss: 0.00077129
Iteration 15/25 | Loss: 0.00077129
Iteration 16/25 | Loss: 0.00077129
Iteration 17/25 | Loss: 0.00077129
Iteration 18/25 | Loss: 0.00077129
Iteration 19/25 | Loss: 0.00077129
Iteration 20/25 | Loss: 0.00077129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007712933584116399, 0.0007712933584116399, 0.0007712933584116399, 0.0007712933584116399, 0.0007712933584116399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007712933584116399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077129
Iteration 2/1000 | Loss: 0.00006331
Iteration 3/1000 | Loss: 0.00004485
Iteration 4/1000 | Loss: 0.00003726
Iteration 5/1000 | Loss: 0.00003417
Iteration 6/1000 | Loss: 0.00003288
Iteration 7/1000 | Loss: 0.00003174
Iteration 8/1000 | Loss: 0.00003093
Iteration 9/1000 | Loss: 0.00003022
Iteration 10/1000 | Loss: 0.00002974
Iteration 11/1000 | Loss: 0.00002940
Iteration 12/1000 | Loss: 0.00002918
Iteration 13/1000 | Loss: 0.00002902
Iteration 14/1000 | Loss: 0.00002890
Iteration 15/1000 | Loss: 0.00002889
Iteration 16/1000 | Loss: 0.00002889
Iteration 17/1000 | Loss: 0.00002885
Iteration 18/1000 | Loss: 0.00002885
Iteration 19/1000 | Loss: 0.00002884
Iteration 20/1000 | Loss: 0.00002884
Iteration 21/1000 | Loss: 0.00002883
Iteration 22/1000 | Loss: 0.00002882
Iteration 23/1000 | Loss: 0.00002878
Iteration 24/1000 | Loss: 0.00002878
Iteration 25/1000 | Loss: 0.00002877
Iteration 26/1000 | Loss: 0.00002877
Iteration 27/1000 | Loss: 0.00002876
Iteration 28/1000 | Loss: 0.00002874
Iteration 29/1000 | Loss: 0.00002872
Iteration 30/1000 | Loss: 0.00002872
Iteration 31/1000 | Loss: 0.00002869
Iteration 32/1000 | Loss: 0.00002865
Iteration 33/1000 | Loss: 0.00002864
Iteration 34/1000 | Loss: 0.00002864
Iteration 35/1000 | Loss: 0.00002859
Iteration 36/1000 | Loss: 0.00002859
Iteration 37/1000 | Loss: 0.00002859
Iteration 38/1000 | Loss: 0.00002859
Iteration 39/1000 | Loss: 0.00002858
Iteration 40/1000 | Loss: 0.00002858
Iteration 41/1000 | Loss: 0.00002854
Iteration 42/1000 | Loss: 0.00002854
Iteration 43/1000 | Loss: 0.00002854
Iteration 44/1000 | Loss: 0.00002854
Iteration 45/1000 | Loss: 0.00002854
Iteration 46/1000 | Loss: 0.00002854
Iteration 47/1000 | Loss: 0.00002854
Iteration 48/1000 | Loss: 0.00002854
Iteration 49/1000 | Loss: 0.00002854
Iteration 50/1000 | Loss: 0.00002854
Iteration 51/1000 | Loss: 0.00002854
Iteration 52/1000 | Loss: 0.00002853
Iteration 53/1000 | Loss: 0.00002853
Iteration 54/1000 | Loss: 0.00002853
Iteration 55/1000 | Loss: 0.00002853
Iteration 56/1000 | Loss: 0.00002852
Iteration 57/1000 | Loss: 0.00002852
Iteration 58/1000 | Loss: 0.00002852
Iteration 59/1000 | Loss: 0.00002851
Iteration 60/1000 | Loss: 0.00002851
Iteration 61/1000 | Loss: 0.00002851
Iteration 62/1000 | Loss: 0.00002851
Iteration 63/1000 | Loss: 0.00002850
Iteration 64/1000 | Loss: 0.00002850
Iteration 65/1000 | Loss: 0.00002850
Iteration 66/1000 | Loss: 0.00002850
Iteration 67/1000 | Loss: 0.00002850
Iteration 68/1000 | Loss: 0.00002849
Iteration 69/1000 | Loss: 0.00002849
Iteration 70/1000 | Loss: 0.00002849
Iteration 71/1000 | Loss: 0.00002848
Iteration 72/1000 | Loss: 0.00002848
Iteration 73/1000 | Loss: 0.00002848
Iteration 74/1000 | Loss: 0.00002848
Iteration 75/1000 | Loss: 0.00002848
Iteration 76/1000 | Loss: 0.00002848
Iteration 77/1000 | Loss: 0.00002848
Iteration 78/1000 | Loss: 0.00002848
Iteration 79/1000 | Loss: 0.00002848
Iteration 80/1000 | Loss: 0.00002848
Iteration 81/1000 | Loss: 0.00002848
Iteration 82/1000 | Loss: 0.00002847
Iteration 83/1000 | Loss: 0.00002847
Iteration 84/1000 | Loss: 0.00002847
Iteration 85/1000 | Loss: 0.00002847
Iteration 86/1000 | Loss: 0.00002847
Iteration 87/1000 | Loss: 0.00002847
Iteration 88/1000 | Loss: 0.00002846
Iteration 89/1000 | Loss: 0.00002845
Iteration 90/1000 | Loss: 0.00002845
Iteration 91/1000 | Loss: 0.00002845
Iteration 92/1000 | Loss: 0.00002845
Iteration 93/1000 | Loss: 0.00002845
Iteration 94/1000 | Loss: 0.00002845
Iteration 95/1000 | Loss: 0.00002845
Iteration 96/1000 | Loss: 0.00002845
Iteration 97/1000 | Loss: 0.00002845
Iteration 98/1000 | Loss: 0.00002844
Iteration 99/1000 | Loss: 0.00002844
Iteration 100/1000 | Loss: 0.00002844
Iteration 101/1000 | Loss: 0.00002844
Iteration 102/1000 | Loss: 0.00002844
Iteration 103/1000 | Loss: 0.00002844
Iteration 104/1000 | Loss: 0.00002844
Iteration 105/1000 | Loss: 0.00002844
Iteration 106/1000 | Loss: 0.00002844
Iteration 107/1000 | Loss: 0.00002844
Iteration 108/1000 | Loss: 0.00002844
Iteration 109/1000 | Loss: 0.00002843
Iteration 110/1000 | Loss: 0.00002843
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00002843
Iteration 113/1000 | Loss: 0.00002843
Iteration 114/1000 | Loss: 0.00002843
Iteration 115/1000 | Loss: 0.00002843
Iteration 116/1000 | Loss: 0.00002843
Iteration 117/1000 | Loss: 0.00002842
Iteration 118/1000 | Loss: 0.00002842
Iteration 119/1000 | Loss: 0.00002842
Iteration 120/1000 | Loss: 0.00002842
Iteration 121/1000 | Loss: 0.00002842
Iteration 122/1000 | Loss: 0.00002842
Iteration 123/1000 | Loss: 0.00002842
Iteration 124/1000 | Loss: 0.00002842
Iteration 125/1000 | Loss: 0.00002841
Iteration 126/1000 | Loss: 0.00002841
Iteration 127/1000 | Loss: 0.00002841
Iteration 128/1000 | Loss: 0.00002841
Iteration 129/1000 | Loss: 0.00002841
Iteration 130/1000 | Loss: 0.00002841
Iteration 131/1000 | Loss: 0.00002841
Iteration 132/1000 | Loss: 0.00002841
Iteration 133/1000 | Loss: 0.00002841
Iteration 134/1000 | Loss: 0.00002841
Iteration 135/1000 | Loss: 0.00002841
Iteration 136/1000 | Loss: 0.00002841
Iteration 137/1000 | Loss: 0.00002840
Iteration 138/1000 | Loss: 0.00002840
Iteration 139/1000 | Loss: 0.00002840
Iteration 140/1000 | Loss: 0.00002840
Iteration 141/1000 | Loss: 0.00002840
Iteration 142/1000 | Loss: 0.00002840
Iteration 143/1000 | Loss: 0.00002840
Iteration 144/1000 | Loss: 0.00002840
Iteration 145/1000 | Loss: 0.00002840
Iteration 146/1000 | Loss: 0.00002840
Iteration 147/1000 | Loss: 0.00002840
Iteration 148/1000 | Loss: 0.00002840
Iteration 149/1000 | Loss: 0.00002840
Iteration 150/1000 | Loss: 0.00002840
Iteration 151/1000 | Loss: 0.00002840
Iteration 152/1000 | Loss: 0.00002840
Iteration 153/1000 | Loss: 0.00002840
Iteration 154/1000 | Loss: 0.00002840
Iteration 155/1000 | Loss: 0.00002840
Iteration 156/1000 | Loss: 0.00002840
Iteration 157/1000 | Loss: 0.00002840
Iteration 158/1000 | Loss: 0.00002840
Iteration 159/1000 | Loss: 0.00002840
Iteration 160/1000 | Loss: 0.00002840
Iteration 161/1000 | Loss: 0.00002840
Iteration 162/1000 | Loss: 0.00002840
Iteration 163/1000 | Loss: 0.00002840
Iteration 164/1000 | Loss: 0.00002840
Iteration 165/1000 | Loss: 0.00002840
Iteration 166/1000 | Loss: 0.00002840
Iteration 167/1000 | Loss: 0.00002840
Iteration 168/1000 | Loss: 0.00002840
Iteration 169/1000 | Loss: 0.00002840
Iteration 170/1000 | Loss: 0.00002840
Iteration 171/1000 | Loss: 0.00002840
Iteration 172/1000 | Loss: 0.00002840
Iteration 173/1000 | Loss: 0.00002840
Iteration 174/1000 | Loss: 0.00002840
Iteration 175/1000 | Loss: 0.00002840
Iteration 176/1000 | Loss: 0.00002840
Iteration 177/1000 | Loss: 0.00002840
Iteration 178/1000 | Loss: 0.00002840
Iteration 179/1000 | Loss: 0.00002840
Iteration 180/1000 | Loss: 0.00002840
Iteration 181/1000 | Loss: 0.00002840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.8399084840202704e-05, 2.8399084840202704e-05, 2.8399084840202704e-05, 2.8399084840202704e-05, 2.8399084840202704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8399084840202704e-05

Optimization complete. Final v2v error: 4.463771343231201 mm

Highest mean error: 5.191047668457031 mm for frame 36

Lowest mean error: 4.008215427398682 mm for frame 46

Saving results

Total time: 39.82693910598755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996808
Iteration 2/25 | Loss: 0.00193989
Iteration 3/25 | Loss: 0.00155534
Iteration 4/25 | Loss: 0.00148706
Iteration 5/25 | Loss: 0.00146980
Iteration 6/25 | Loss: 0.00140917
Iteration 7/25 | Loss: 0.00138101
Iteration 8/25 | Loss: 0.00136821
Iteration 9/25 | Loss: 0.00136572
Iteration 10/25 | Loss: 0.00136686
Iteration 11/25 | Loss: 0.00135564
Iteration 12/25 | Loss: 0.00134814
Iteration 13/25 | Loss: 0.00134434
Iteration 14/25 | Loss: 0.00134274
Iteration 15/25 | Loss: 0.00134305
Iteration 16/25 | Loss: 0.00134201
Iteration 17/25 | Loss: 0.00134282
Iteration 18/25 | Loss: 0.00133846
Iteration 19/25 | Loss: 0.00134296
Iteration 20/25 | Loss: 0.00133874
Iteration 21/25 | Loss: 0.00133689
Iteration 22/25 | Loss: 0.00133970
Iteration 23/25 | Loss: 0.00133623
Iteration 24/25 | Loss: 0.00133874
Iteration 25/25 | Loss: 0.00133890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59732533
Iteration 2/25 | Loss: 0.00124647
Iteration 3/25 | Loss: 0.00121523
Iteration 4/25 | Loss: 0.00121523
Iteration 5/25 | Loss: 0.00121523
Iteration 6/25 | Loss: 0.00121523
Iteration 7/25 | Loss: 0.00121523
Iteration 8/25 | Loss: 0.00121523
Iteration 9/25 | Loss: 0.00121523
Iteration 10/25 | Loss: 0.00121523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012152281124144793, 0.0012152281124144793, 0.0012152281124144793, 0.0012152281124144793, 0.0012152281124144793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012152281124144793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121523
Iteration 2/1000 | Loss: 0.00078244
Iteration 3/1000 | Loss: 0.00009727
Iteration 4/1000 | Loss: 0.00005921
Iteration 5/1000 | Loss: 0.00005921
Iteration 6/1000 | Loss: 0.00004533
Iteration 7/1000 | Loss: 0.00007724
Iteration 8/1000 | Loss: 0.00003989
Iteration 9/1000 | Loss: 0.00049305
Iteration 10/1000 | Loss: 0.00005864
Iteration 11/1000 | Loss: 0.00005370
Iteration 12/1000 | Loss: 0.00005242
Iteration 13/1000 | Loss: 0.00005252
Iteration 14/1000 | Loss: 0.00005360
Iteration 15/1000 | Loss: 0.00005514
Iteration 16/1000 | Loss: 0.00005308
Iteration 17/1000 | Loss: 0.00005471
Iteration 18/1000 | Loss: 0.00007629
Iteration 19/1000 | Loss: 0.00006283
Iteration 20/1000 | Loss: 0.00008153
Iteration 21/1000 | Loss: 0.00003456
Iteration 22/1000 | Loss: 0.00005866
Iteration 23/1000 | Loss: 0.00006219
Iteration 24/1000 | Loss: 0.00004646
Iteration 25/1000 | Loss: 0.00007136
Iteration 26/1000 | Loss: 0.00004744
Iteration 27/1000 | Loss: 0.00005173
Iteration 28/1000 | Loss: 0.00003197
Iteration 29/1000 | Loss: 0.00004297
Iteration 30/1000 | Loss: 0.00005570
Iteration 31/1000 | Loss: 0.00006233
Iteration 32/1000 | Loss: 0.00003336
Iteration 33/1000 | Loss: 0.00003083
Iteration 34/1000 | Loss: 0.00002811
Iteration 35/1000 | Loss: 0.00002732
Iteration 36/1000 | Loss: 0.00002682
Iteration 37/1000 | Loss: 0.00036391
Iteration 38/1000 | Loss: 0.00029546
Iteration 39/1000 | Loss: 0.00031407
Iteration 40/1000 | Loss: 0.00003676
Iteration 41/1000 | Loss: 0.00002673
Iteration 42/1000 | Loss: 0.00002453
Iteration 43/1000 | Loss: 0.00002377
Iteration 44/1000 | Loss: 0.00002320
Iteration 45/1000 | Loss: 0.00002267
Iteration 46/1000 | Loss: 0.00027634
Iteration 47/1000 | Loss: 0.00027332
Iteration 48/1000 | Loss: 0.00003052
Iteration 49/1000 | Loss: 0.00002412
Iteration 50/1000 | Loss: 0.00002240
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002088
Iteration 53/1000 | Loss: 0.00002029
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001979
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001937
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001921
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001920
Iteration 68/1000 | Loss: 0.00001920
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001916
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001910
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001910
Iteration 103/1000 | Loss: 0.00001910
Iteration 104/1000 | Loss: 0.00001910
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001909
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001909
Iteration 113/1000 | Loss: 0.00001909
Iteration 114/1000 | Loss: 0.00001909
Iteration 115/1000 | Loss: 0.00001909
Iteration 116/1000 | Loss: 0.00001909
Iteration 117/1000 | Loss: 0.00001909
Iteration 118/1000 | Loss: 0.00001909
Iteration 119/1000 | Loss: 0.00001909
Iteration 120/1000 | Loss: 0.00001909
Iteration 121/1000 | Loss: 0.00001909
Iteration 122/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.908844569697976e-05, 1.908844569697976e-05, 1.908844569697976e-05, 1.908844569697976e-05, 1.908844569697976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.908844569697976e-05

Optimization complete. Final v2v error: 3.4909045696258545 mm

Highest mean error: 6.201359748840332 mm for frame 50

Lowest mean error: 2.702643632888794 mm for frame 2

Saving results

Total time: 130.53556561470032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962016
Iteration 2/25 | Loss: 0.00175049
Iteration 3/25 | Loss: 0.00143567
Iteration 4/25 | Loss: 0.00141623
Iteration 5/25 | Loss: 0.00141012
Iteration 6/25 | Loss: 0.00140983
Iteration 7/25 | Loss: 0.00140983
Iteration 8/25 | Loss: 0.00140983
Iteration 9/25 | Loss: 0.00140983
Iteration 10/25 | Loss: 0.00140983
Iteration 11/25 | Loss: 0.00140983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014098315732553601, 0.0014098315732553601, 0.0014098315732553601, 0.0014098315732553601, 0.0014098315732553601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014098315732553601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78720725
Iteration 2/25 | Loss: 0.00087272
Iteration 3/25 | Loss: 0.00087272
Iteration 4/25 | Loss: 0.00087271
Iteration 5/25 | Loss: 0.00087271
Iteration 6/25 | Loss: 0.00087271
Iteration 7/25 | Loss: 0.00087271
Iteration 8/25 | Loss: 0.00087271
Iteration 9/25 | Loss: 0.00087271
Iteration 10/25 | Loss: 0.00087271
Iteration 11/25 | Loss: 0.00087271
Iteration 12/25 | Loss: 0.00087271
Iteration 13/25 | Loss: 0.00087271
Iteration 14/25 | Loss: 0.00087271
Iteration 15/25 | Loss: 0.00087271
Iteration 16/25 | Loss: 0.00087271
Iteration 17/25 | Loss: 0.00087271
Iteration 18/25 | Loss: 0.00087271
Iteration 19/25 | Loss: 0.00087271
Iteration 20/25 | Loss: 0.00087271
Iteration 21/25 | Loss: 0.00087271
Iteration 22/25 | Loss: 0.00087271
Iteration 23/25 | Loss: 0.00087271
Iteration 24/25 | Loss: 0.00087271
Iteration 25/25 | Loss: 0.00087271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087271
Iteration 2/1000 | Loss: 0.00005916
Iteration 3/1000 | Loss: 0.00004511
Iteration 4/1000 | Loss: 0.00003946
Iteration 5/1000 | Loss: 0.00003813
Iteration 6/1000 | Loss: 0.00003666
Iteration 7/1000 | Loss: 0.00003583
Iteration 8/1000 | Loss: 0.00003499
Iteration 9/1000 | Loss: 0.00003448
Iteration 10/1000 | Loss: 0.00003408
Iteration 11/1000 | Loss: 0.00003351
Iteration 12/1000 | Loss: 0.00003311
Iteration 13/1000 | Loss: 0.00003274
Iteration 14/1000 | Loss: 0.00003234
Iteration 15/1000 | Loss: 0.00003202
Iteration 16/1000 | Loss: 0.00003171
Iteration 17/1000 | Loss: 0.00003148
Iteration 18/1000 | Loss: 0.00003129
Iteration 19/1000 | Loss: 0.00003115
Iteration 20/1000 | Loss: 0.00003108
Iteration 21/1000 | Loss: 0.00003097
Iteration 22/1000 | Loss: 0.00003089
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003076
Iteration 25/1000 | Loss: 0.00003071
Iteration 26/1000 | Loss: 0.00003071
Iteration 27/1000 | Loss: 0.00003070
Iteration 28/1000 | Loss: 0.00003069
Iteration 29/1000 | Loss: 0.00003068
Iteration 30/1000 | Loss: 0.00003068
Iteration 31/1000 | Loss: 0.00003067
Iteration 32/1000 | Loss: 0.00003067
Iteration 33/1000 | Loss: 0.00003067
Iteration 34/1000 | Loss: 0.00003067
Iteration 35/1000 | Loss: 0.00003067
Iteration 36/1000 | Loss: 0.00003066
Iteration 37/1000 | Loss: 0.00003066
Iteration 38/1000 | Loss: 0.00003066
Iteration 39/1000 | Loss: 0.00003066
Iteration 40/1000 | Loss: 0.00003065
Iteration 41/1000 | Loss: 0.00003065
Iteration 42/1000 | Loss: 0.00003064
Iteration 43/1000 | Loss: 0.00003064
Iteration 44/1000 | Loss: 0.00003064
Iteration 45/1000 | Loss: 0.00003064
Iteration 46/1000 | Loss: 0.00003064
Iteration 47/1000 | Loss: 0.00003064
Iteration 48/1000 | Loss: 0.00003064
Iteration 49/1000 | Loss: 0.00003064
Iteration 50/1000 | Loss: 0.00003064
Iteration 51/1000 | Loss: 0.00003063
Iteration 52/1000 | Loss: 0.00003063
Iteration 53/1000 | Loss: 0.00003063
Iteration 54/1000 | Loss: 0.00003063
Iteration 55/1000 | Loss: 0.00003063
Iteration 56/1000 | Loss: 0.00003062
Iteration 57/1000 | Loss: 0.00003062
Iteration 58/1000 | Loss: 0.00003062
Iteration 59/1000 | Loss: 0.00003062
Iteration 60/1000 | Loss: 0.00003062
Iteration 61/1000 | Loss: 0.00003062
Iteration 62/1000 | Loss: 0.00003062
Iteration 63/1000 | Loss: 0.00003062
Iteration 64/1000 | Loss: 0.00003062
Iteration 65/1000 | Loss: 0.00003062
Iteration 66/1000 | Loss: 0.00003062
Iteration 67/1000 | Loss: 0.00003061
Iteration 68/1000 | Loss: 0.00003061
Iteration 69/1000 | Loss: 0.00003061
Iteration 70/1000 | Loss: 0.00003061
Iteration 71/1000 | Loss: 0.00003061
Iteration 72/1000 | Loss: 0.00003061
Iteration 73/1000 | Loss: 0.00003061
Iteration 74/1000 | Loss: 0.00003061
Iteration 75/1000 | Loss: 0.00003061
Iteration 76/1000 | Loss: 0.00003061
Iteration 77/1000 | Loss: 0.00003060
Iteration 78/1000 | Loss: 0.00003060
Iteration 79/1000 | Loss: 0.00003060
Iteration 80/1000 | Loss: 0.00003060
Iteration 81/1000 | Loss: 0.00003059
Iteration 82/1000 | Loss: 0.00003059
Iteration 83/1000 | Loss: 0.00003059
Iteration 84/1000 | Loss: 0.00003059
Iteration 85/1000 | Loss: 0.00003059
Iteration 86/1000 | Loss: 0.00003059
Iteration 87/1000 | Loss: 0.00003058
Iteration 88/1000 | Loss: 0.00003058
Iteration 89/1000 | Loss: 0.00003058
Iteration 90/1000 | Loss: 0.00003058
Iteration 91/1000 | Loss: 0.00003058
Iteration 92/1000 | Loss: 0.00003057
Iteration 93/1000 | Loss: 0.00003057
Iteration 94/1000 | Loss: 0.00003057
Iteration 95/1000 | Loss: 0.00003057
Iteration 96/1000 | Loss: 0.00003057
Iteration 97/1000 | Loss: 0.00003056
Iteration 98/1000 | Loss: 0.00003056
Iteration 99/1000 | Loss: 0.00003056
Iteration 100/1000 | Loss: 0.00003056
Iteration 101/1000 | Loss: 0.00003056
Iteration 102/1000 | Loss: 0.00003055
Iteration 103/1000 | Loss: 0.00003055
Iteration 104/1000 | Loss: 0.00003055
Iteration 105/1000 | Loss: 0.00003055
Iteration 106/1000 | Loss: 0.00003055
Iteration 107/1000 | Loss: 0.00003055
Iteration 108/1000 | Loss: 0.00003055
Iteration 109/1000 | Loss: 0.00003055
Iteration 110/1000 | Loss: 0.00003055
Iteration 111/1000 | Loss: 0.00003054
Iteration 112/1000 | Loss: 0.00003054
Iteration 113/1000 | Loss: 0.00003054
Iteration 114/1000 | Loss: 0.00003054
Iteration 115/1000 | Loss: 0.00003054
Iteration 116/1000 | Loss: 0.00003054
Iteration 117/1000 | Loss: 0.00003053
Iteration 118/1000 | Loss: 0.00003053
Iteration 119/1000 | Loss: 0.00003053
Iteration 120/1000 | Loss: 0.00003053
Iteration 121/1000 | Loss: 0.00003053
Iteration 122/1000 | Loss: 0.00003053
Iteration 123/1000 | Loss: 0.00003052
Iteration 124/1000 | Loss: 0.00003052
Iteration 125/1000 | Loss: 0.00003052
Iteration 126/1000 | Loss: 0.00003052
Iteration 127/1000 | Loss: 0.00003052
Iteration 128/1000 | Loss: 0.00003051
Iteration 129/1000 | Loss: 0.00003051
Iteration 130/1000 | Loss: 0.00003051
Iteration 131/1000 | Loss: 0.00003051
Iteration 132/1000 | Loss: 0.00003051
Iteration 133/1000 | Loss: 0.00003051
Iteration 134/1000 | Loss: 0.00003051
Iteration 135/1000 | Loss: 0.00003051
Iteration 136/1000 | Loss: 0.00003051
Iteration 137/1000 | Loss: 0.00003051
Iteration 138/1000 | Loss: 0.00003051
Iteration 139/1000 | Loss: 0.00003050
Iteration 140/1000 | Loss: 0.00003050
Iteration 141/1000 | Loss: 0.00003050
Iteration 142/1000 | Loss: 0.00003050
Iteration 143/1000 | Loss: 0.00003050
Iteration 144/1000 | Loss: 0.00003050
Iteration 145/1000 | Loss: 0.00003050
Iteration 146/1000 | Loss: 0.00003050
Iteration 147/1000 | Loss: 0.00003050
Iteration 148/1000 | Loss: 0.00003050
Iteration 149/1000 | Loss: 0.00003050
Iteration 150/1000 | Loss: 0.00003050
Iteration 151/1000 | Loss: 0.00003050
Iteration 152/1000 | Loss: 0.00003050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.050327541131992e-05, 3.050327541131992e-05, 3.050327541131992e-05, 3.050327541131992e-05, 3.050327541131992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.050327541131992e-05

Optimization complete. Final v2v error: 4.6860198974609375 mm

Highest mean error: 5.2164177894592285 mm for frame 191

Lowest mean error: 4.182087421417236 mm for frame 35

Saving results

Total time: 50.94724655151367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402381
Iteration 2/25 | Loss: 0.00140565
Iteration 3/25 | Loss: 0.00125733
Iteration 4/25 | Loss: 0.00123610
Iteration 5/25 | Loss: 0.00122898
Iteration 6/25 | Loss: 0.00122743
Iteration 7/25 | Loss: 0.00122743
Iteration 8/25 | Loss: 0.00122743
Iteration 9/25 | Loss: 0.00122743
Iteration 10/25 | Loss: 0.00122743
Iteration 11/25 | Loss: 0.00122743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012274349573999643, 0.0012274349573999643, 0.0012274349573999643, 0.0012274349573999643, 0.0012274349573999643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012274349573999643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44382012
Iteration 2/25 | Loss: 0.00073064
Iteration 3/25 | Loss: 0.00073064
Iteration 4/25 | Loss: 0.00073064
Iteration 5/25 | Loss: 0.00073064
Iteration 6/25 | Loss: 0.00073064
Iteration 7/25 | Loss: 0.00073064
Iteration 8/25 | Loss: 0.00073064
Iteration 9/25 | Loss: 0.00073064
Iteration 10/25 | Loss: 0.00073063
Iteration 11/25 | Loss: 0.00073063
Iteration 12/25 | Loss: 0.00073063
Iteration 13/25 | Loss: 0.00073063
Iteration 14/25 | Loss: 0.00073063
Iteration 15/25 | Loss: 0.00073063
Iteration 16/25 | Loss: 0.00073063
Iteration 17/25 | Loss: 0.00073063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007306347833946347, 0.0007306347833946347, 0.0007306347833946347, 0.0007306347833946347, 0.0007306347833946347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007306347833946347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073063
Iteration 2/1000 | Loss: 0.00004404
Iteration 3/1000 | Loss: 0.00003055
Iteration 4/1000 | Loss: 0.00002627
Iteration 5/1000 | Loss: 0.00002375
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00001950
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001842
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001710
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001708
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001692
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001686
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001685
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001683
Iteration 46/1000 | Loss: 0.00001683
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001669
Iteration 102/1000 | Loss: 0.00001669
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001668
Iteration 106/1000 | Loss: 0.00001668
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001667
Iteration 110/1000 | Loss: 0.00001667
Iteration 111/1000 | Loss: 0.00001667
Iteration 112/1000 | Loss: 0.00001667
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001666
Iteration 116/1000 | Loss: 0.00001666
Iteration 117/1000 | Loss: 0.00001666
Iteration 118/1000 | Loss: 0.00001666
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001665
Iteration 122/1000 | Loss: 0.00001665
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Iteration 126/1000 | Loss: 0.00001664
Iteration 127/1000 | Loss: 0.00001664
Iteration 128/1000 | Loss: 0.00001664
Iteration 129/1000 | Loss: 0.00001664
Iteration 130/1000 | Loss: 0.00001664
Iteration 131/1000 | Loss: 0.00001664
Iteration 132/1000 | Loss: 0.00001664
Iteration 133/1000 | Loss: 0.00001664
Iteration 134/1000 | Loss: 0.00001663
Iteration 135/1000 | Loss: 0.00001663
Iteration 136/1000 | Loss: 0.00001663
Iteration 137/1000 | Loss: 0.00001663
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001663
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001663
Iteration 142/1000 | Loss: 0.00001663
Iteration 143/1000 | Loss: 0.00001663
Iteration 144/1000 | Loss: 0.00001663
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001662
Iteration 147/1000 | Loss: 0.00001662
Iteration 148/1000 | Loss: 0.00001662
Iteration 149/1000 | Loss: 0.00001662
Iteration 150/1000 | Loss: 0.00001662
Iteration 151/1000 | Loss: 0.00001662
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001661
Iteration 155/1000 | Loss: 0.00001661
Iteration 156/1000 | Loss: 0.00001661
Iteration 157/1000 | Loss: 0.00001661
Iteration 158/1000 | Loss: 0.00001661
Iteration 159/1000 | Loss: 0.00001661
Iteration 160/1000 | Loss: 0.00001661
Iteration 161/1000 | Loss: 0.00001661
Iteration 162/1000 | Loss: 0.00001661
Iteration 163/1000 | Loss: 0.00001661
Iteration 164/1000 | Loss: 0.00001661
Iteration 165/1000 | Loss: 0.00001661
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Iteration 168/1000 | Loss: 0.00001661
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001660
Iteration 171/1000 | Loss: 0.00001660
Iteration 172/1000 | Loss: 0.00001660
Iteration 173/1000 | Loss: 0.00001660
Iteration 174/1000 | Loss: 0.00001660
Iteration 175/1000 | Loss: 0.00001660
Iteration 176/1000 | Loss: 0.00001660
Iteration 177/1000 | Loss: 0.00001660
Iteration 178/1000 | Loss: 0.00001660
Iteration 179/1000 | Loss: 0.00001660
Iteration 180/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6604524716967717e-05, 1.6604524716967717e-05, 1.6604524716967717e-05, 1.6604524716967717e-05, 1.6604524716967717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6604524716967717e-05

Optimization complete. Final v2v error: 3.4778144359588623 mm

Highest mean error: 4.4853692054748535 mm for frame 5

Lowest mean error: 2.880261182785034 mm for frame 129

Saving results

Total time: 50.978612184524536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492029
Iteration 2/25 | Loss: 0.00145624
Iteration 3/25 | Loss: 0.00131969
Iteration 4/25 | Loss: 0.00130427
Iteration 5/25 | Loss: 0.00130029
Iteration 6/25 | Loss: 0.00129966
Iteration 7/25 | Loss: 0.00129966
Iteration 8/25 | Loss: 0.00129966
Iteration 9/25 | Loss: 0.00129966
Iteration 10/25 | Loss: 0.00129966
Iteration 11/25 | Loss: 0.00129966
Iteration 12/25 | Loss: 0.00129944
Iteration 13/25 | Loss: 0.00129944
Iteration 14/25 | Loss: 0.00129944
Iteration 15/25 | Loss: 0.00129944
Iteration 16/25 | Loss: 0.00129944
Iteration 17/25 | Loss: 0.00129944
Iteration 18/25 | Loss: 0.00129944
Iteration 19/25 | Loss: 0.00129944
Iteration 20/25 | Loss: 0.00129944
Iteration 21/25 | Loss: 0.00129944
Iteration 22/25 | Loss: 0.00129944
Iteration 23/25 | Loss: 0.00129944
Iteration 24/25 | Loss: 0.00129944
Iteration 25/25 | Loss: 0.00129944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43683922
Iteration 2/25 | Loss: 0.00077893
Iteration 3/25 | Loss: 0.00077891
Iteration 4/25 | Loss: 0.00077891
Iteration 5/25 | Loss: 0.00077891
Iteration 6/25 | Loss: 0.00077890
Iteration 7/25 | Loss: 0.00077890
Iteration 8/25 | Loss: 0.00077890
Iteration 9/25 | Loss: 0.00077890
Iteration 10/25 | Loss: 0.00077890
Iteration 11/25 | Loss: 0.00077890
Iteration 12/25 | Loss: 0.00077890
Iteration 13/25 | Loss: 0.00077890
Iteration 14/25 | Loss: 0.00077890
Iteration 15/25 | Loss: 0.00077890
Iteration 16/25 | Loss: 0.00077890
Iteration 17/25 | Loss: 0.00077890
Iteration 18/25 | Loss: 0.00077890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007789033697918057, 0.0007789033697918057, 0.0007789033697918057, 0.0007789033697918057, 0.0007789033697918057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007789033697918057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077890
Iteration 2/1000 | Loss: 0.00004800
Iteration 3/1000 | Loss: 0.00003113
Iteration 4/1000 | Loss: 0.00002677
Iteration 5/1000 | Loss: 0.00002473
Iteration 6/1000 | Loss: 0.00002294
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002083
Iteration 9/1000 | Loss: 0.00002033
Iteration 10/1000 | Loss: 0.00001989
Iteration 11/1000 | Loss: 0.00001959
Iteration 12/1000 | Loss: 0.00001935
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001910
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001887
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001868
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001863
Iteration 62/1000 | Loss: 0.00001863
Iteration 63/1000 | Loss: 0.00001863
Iteration 64/1000 | Loss: 0.00001862
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001862
Iteration 67/1000 | Loss: 0.00001861
Iteration 68/1000 | Loss: 0.00001861
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001860
Iteration 71/1000 | Loss: 0.00001860
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001854
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001853
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001852
Iteration 105/1000 | Loss: 0.00001852
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001851
Iteration 110/1000 | Loss: 0.00001851
Iteration 111/1000 | Loss: 0.00001851
Iteration 112/1000 | Loss: 0.00001851
Iteration 113/1000 | Loss: 0.00001851
Iteration 114/1000 | Loss: 0.00001851
Iteration 115/1000 | Loss: 0.00001851
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001850
Iteration 127/1000 | Loss: 0.00001849
Iteration 128/1000 | Loss: 0.00001849
Iteration 129/1000 | Loss: 0.00001849
Iteration 130/1000 | Loss: 0.00001849
Iteration 131/1000 | Loss: 0.00001849
Iteration 132/1000 | Loss: 0.00001849
Iteration 133/1000 | Loss: 0.00001849
Iteration 134/1000 | Loss: 0.00001848
Iteration 135/1000 | Loss: 0.00001848
Iteration 136/1000 | Loss: 0.00001848
Iteration 137/1000 | Loss: 0.00001848
Iteration 138/1000 | Loss: 0.00001848
Iteration 139/1000 | Loss: 0.00001848
Iteration 140/1000 | Loss: 0.00001848
Iteration 141/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.848374813562259e-05, 1.848374813562259e-05, 1.848374813562259e-05, 1.848374813562259e-05, 1.848374813562259e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.848374813562259e-05

Optimization complete. Final v2v error: 3.6442580223083496 mm

Highest mean error: 4.492090702056885 mm for frame 135

Lowest mean error: 3.1666793823242188 mm for frame 89

Saving results

Total time: 40.19665718078613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430237
Iteration 2/25 | Loss: 0.00138357
Iteration 3/25 | Loss: 0.00125164
Iteration 4/25 | Loss: 0.00124226
Iteration 5/25 | Loss: 0.00124092
Iteration 6/25 | Loss: 0.00124092
Iteration 7/25 | Loss: 0.00124092
Iteration 8/25 | Loss: 0.00124092
Iteration 9/25 | Loss: 0.00124092
Iteration 10/25 | Loss: 0.00124092
Iteration 11/25 | Loss: 0.00124092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012409220216795802, 0.0012409220216795802, 0.0012409220216795802, 0.0012409220216795802, 0.0012409220216795802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012409220216795802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23810387
Iteration 2/25 | Loss: 0.00051617
Iteration 3/25 | Loss: 0.00051617
Iteration 4/25 | Loss: 0.00051617
Iteration 5/25 | Loss: 0.00051617
Iteration 6/25 | Loss: 0.00051616
Iteration 7/25 | Loss: 0.00051616
Iteration 8/25 | Loss: 0.00051616
Iteration 9/25 | Loss: 0.00051616
Iteration 10/25 | Loss: 0.00051616
Iteration 11/25 | Loss: 0.00051616
Iteration 12/25 | Loss: 0.00051616
Iteration 13/25 | Loss: 0.00051616
Iteration 14/25 | Loss: 0.00051616
Iteration 15/25 | Loss: 0.00051616
Iteration 16/25 | Loss: 0.00051616
Iteration 17/25 | Loss: 0.00051616
Iteration 18/25 | Loss: 0.00051616
Iteration 19/25 | Loss: 0.00051616
Iteration 20/25 | Loss: 0.00051616
Iteration 21/25 | Loss: 0.00051616
Iteration 22/25 | Loss: 0.00051616
Iteration 23/25 | Loss: 0.00051616
Iteration 24/25 | Loss: 0.00051616
Iteration 25/25 | Loss: 0.00051616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051616
Iteration 2/1000 | Loss: 0.00003389
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001862
Iteration 5/1000 | Loss: 0.00001731
Iteration 6/1000 | Loss: 0.00001629
Iteration 7/1000 | Loss: 0.00001544
Iteration 8/1000 | Loss: 0.00001494
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001355
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001349
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.3489519915310666e-05, 1.3489519915310666e-05, 1.3489519915310666e-05, 1.3489519915310666e-05, 1.3489519915310666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3489519915310666e-05

Optimization complete. Final v2v error: 3.157461643218994 mm

Highest mean error: 3.1944451332092285 mm for frame 111

Lowest mean error: 3.1230826377868652 mm for frame 12

Saving results

Total time: 26.61945390701294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045990
Iteration 2/25 | Loss: 0.00271895
Iteration 3/25 | Loss: 0.00187093
Iteration 4/25 | Loss: 0.00184693
Iteration 5/25 | Loss: 0.00177572
Iteration 6/25 | Loss: 0.00162294
Iteration 7/25 | Loss: 0.00160228
Iteration 8/25 | Loss: 0.00152208
Iteration 9/25 | Loss: 0.00151520
Iteration 10/25 | Loss: 0.00143391
Iteration 11/25 | Loss: 0.00137364
Iteration 12/25 | Loss: 0.00134656
Iteration 13/25 | Loss: 0.00129954
Iteration 14/25 | Loss: 0.00127757
Iteration 15/25 | Loss: 0.00127898
Iteration 16/25 | Loss: 0.00124893
Iteration 17/25 | Loss: 0.00124743
Iteration 18/25 | Loss: 0.00124575
Iteration 19/25 | Loss: 0.00126137
Iteration 20/25 | Loss: 0.00122649
Iteration 21/25 | Loss: 0.00123043
Iteration 22/25 | Loss: 0.00122576
Iteration 23/25 | Loss: 0.00122247
Iteration 24/25 | Loss: 0.00122709
Iteration 25/25 | Loss: 0.00122343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48807240
Iteration 2/25 | Loss: 0.00132973
Iteration 3/25 | Loss: 0.00089198
Iteration 4/25 | Loss: 0.00089198
Iteration 5/25 | Loss: 0.00089198
Iteration 6/25 | Loss: 0.00089198
Iteration 7/25 | Loss: 0.00089198
Iteration 8/25 | Loss: 0.00089198
Iteration 9/25 | Loss: 0.00089198
Iteration 10/25 | Loss: 0.00089198
Iteration 11/25 | Loss: 0.00089198
Iteration 12/25 | Loss: 0.00089198
Iteration 13/25 | Loss: 0.00089198
Iteration 14/25 | Loss: 0.00089198
Iteration 15/25 | Loss: 0.00089198
Iteration 16/25 | Loss: 0.00089198
Iteration 17/25 | Loss: 0.00089198
Iteration 18/25 | Loss: 0.00089198
Iteration 19/25 | Loss: 0.00089198
Iteration 20/25 | Loss: 0.00089198
Iteration 21/25 | Loss: 0.00089198
Iteration 22/25 | Loss: 0.00089198
Iteration 23/25 | Loss: 0.00089198
Iteration 24/25 | Loss: 0.00089198
Iteration 25/25 | Loss: 0.00089198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089198
Iteration 2/1000 | Loss: 0.00081708
Iteration 3/1000 | Loss: 0.00039628
Iteration 4/1000 | Loss: 0.00037320
Iteration 5/1000 | Loss: 0.00005026
Iteration 6/1000 | Loss: 0.00013690
Iteration 7/1000 | Loss: 0.00038605
Iteration 8/1000 | Loss: 0.00003822
Iteration 9/1000 | Loss: 0.00032560
Iteration 10/1000 | Loss: 0.00036221
Iteration 11/1000 | Loss: 0.00016101
Iteration 12/1000 | Loss: 0.00006487
Iteration 13/1000 | Loss: 0.00009979
Iteration 14/1000 | Loss: 0.00002761
Iteration 15/1000 | Loss: 0.00021589
Iteration 16/1000 | Loss: 0.00042210
Iteration 17/1000 | Loss: 0.00071928
Iteration 18/1000 | Loss: 0.00172995
Iteration 19/1000 | Loss: 0.00013140
Iteration 20/1000 | Loss: 0.00082366
Iteration 21/1000 | Loss: 0.00031271
Iteration 22/1000 | Loss: 0.00007915
Iteration 23/1000 | Loss: 0.00024692
Iteration 24/1000 | Loss: 0.00004046
Iteration 25/1000 | Loss: 0.00002584
Iteration 26/1000 | Loss: 0.00003641
Iteration 27/1000 | Loss: 0.00002516
Iteration 28/1000 | Loss: 0.00016320
Iteration 29/1000 | Loss: 0.00178177
Iteration 30/1000 | Loss: 0.00007350
Iteration 31/1000 | Loss: 0.00002964
Iteration 32/1000 | Loss: 0.00002486
Iteration 33/1000 | Loss: 0.00014289
Iteration 34/1000 | Loss: 0.00002506
Iteration 35/1000 | Loss: 0.00004589
Iteration 36/1000 | Loss: 0.00002439
Iteration 37/1000 | Loss: 0.00003767
Iteration 38/1000 | Loss: 0.00018991
Iteration 39/1000 | Loss: 0.00002846
Iteration 40/1000 | Loss: 0.00002536
Iteration 41/1000 | Loss: 0.00002333
Iteration 42/1000 | Loss: 0.00007815
Iteration 43/1000 | Loss: 0.00008333
Iteration 44/1000 | Loss: 0.00002562
Iteration 45/1000 | Loss: 0.00003372
Iteration 46/1000 | Loss: 0.00002283
Iteration 47/1000 | Loss: 0.00009146
Iteration 48/1000 | Loss: 0.00002225
Iteration 49/1000 | Loss: 0.00002236
Iteration 50/1000 | Loss: 0.00002209
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002205
Iteration 53/1000 | Loss: 0.00002205
Iteration 54/1000 | Loss: 0.00002212
Iteration 55/1000 | Loss: 0.00002204
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002204
Iteration 58/1000 | Loss: 0.00002204
Iteration 59/1000 | Loss: 0.00002204
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00008699
Iteration 64/1000 | Loss: 0.00002909
Iteration 65/1000 | Loss: 0.00010198
Iteration 66/1000 | Loss: 0.00003071
Iteration 67/1000 | Loss: 0.00042907
Iteration 68/1000 | Loss: 0.00050308
Iteration 69/1000 | Loss: 0.00048199
Iteration 70/1000 | Loss: 0.00013639
Iteration 71/1000 | Loss: 0.00060359
Iteration 72/1000 | Loss: 0.00004484
Iteration 73/1000 | Loss: 0.00005815
Iteration 74/1000 | Loss: 0.00003525
Iteration 75/1000 | Loss: 0.00005129
Iteration 76/1000 | Loss: 0.00002957
Iteration 77/1000 | Loss: 0.00005025
Iteration 78/1000 | Loss: 0.00002726
Iteration 79/1000 | Loss: 0.00002467
Iteration 80/1000 | Loss: 0.00002294
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002174
Iteration 84/1000 | Loss: 0.00002171
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002170
Iteration 87/1000 | Loss: 0.00003706
Iteration 88/1000 | Loss: 0.00002174
Iteration 89/1000 | Loss: 0.00002156
Iteration 90/1000 | Loss: 0.00002156
Iteration 91/1000 | Loss: 0.00002155
Iteration 92/1000 | Loss: 0.00002155
Iteration 93/1000 | Loss: 0.00002155
Iteration 94/1000 | Loss: 0.00002155
Iteration 95/1000 | Loss: 0.00002154
Iteration 96/1000 | Loss: 0.00002154
Iteration 97/1000 | Loss: 0.00002154
Iteration 98/1000 | Loss: 0.00002154
Iteration 99/1000 | Loss: 0.00002153
Iteration 100/1000 | Loss: 0.00002153
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002153
Iteration 103/1000 | Loss: 0.00002153
Iteration 104/1000 | Loss: 0.00002153
Iteration 105/1000 | Loss: 0.00002153
Iteration 106/1000 | Loss: 0.00002153
Iteration 107/1000 | Loss: 0.00002152
Iteration 108/1000 | Loss: 0.00002152
Iteration 109/1000 | Loss: 0.00002152
Iteration 110/1000 | Loss: 0.00002152
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002152
Iteration 113/1000 | Loss: 0.00002152
Iteration 114/1000 | Loss: 0.00002152
Iteration 115/1000 | Loss: 0.00002152
Iteration 116/1000 | Loss: 0.00002152
Iteration 117/1000 | Loss: 0.00002152
Iteration 118/1000 | Loss: 0.00002152
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002151
Iteration 121/1000 | Loss: 0.00002151
Iteration 122/1000 | Loss: 0.00002151
Iteration 123/1000 | Loss: 0.00002151
Iteration 124/1000 | Loss: 0.00002151
Iteration 125/1000 | Loss: 0.00002816
Iteration 126/1000 | Loss: 0.00002153
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002151
Iteration 132/1000 | Loss: 0.00002151
Iteration 133/1000 | Loss: 0.00002151
Iteration 134/1000 | Loss: 0.00002151
Iteration 135/1000 | Loss: 0.00002151
Iteration 136/1000 | Loss: 0.00002151
Iteration 137/1000 | Loss: 0.00002150
Iteration 138/1000 | Loss: 0.00002150
Iteration 139/1000 | Loss: 0.00002150
Iteration 140/1000 | Loss: 0.00002150
Iteration 141/1000 | Loss: 0.00002150
Iteration 142/1000 | Loss: 0.00002150
Iteration 143/1000 | Loss: 0.00002150
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00002149
Iteration 146/1000 | Loss: 0.00002149
Iteration 147/1000 | Loss: 0.00002901
Iteration 148/1000 | Loss: 0.00002150
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002150
Iteration 151/1000 | Loss: 0.00002149
Iteration 152/1000 | Loss: 0.00002149
Iteration 153/1000 | Loss: 0.00002149
Iteration 154/1000 | Loss: 0.00002149
Iteration 155/1000 | Loss: 0.00002149
Iteration 156/1000 | Loss: 0.00002149
Iteration 157/1000 | Loss: 0.00002149
Iteration 158/1000 | Loss: 0.00002149
Iteration 159/1000 | Loss: 0.00002149
Iteration 160/1000 | Loss: 0.00002149
Iteration 161/1000 | Loss: 0.00002149
Iteration 162/1000 | Loss: 0.00002149
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002148
Iteration 165/1000 | Loss: 0.00002148
Iteration 166/1000 | Loss: 0.00002147
Iteration 167/1000 | Loss: 0.00002147
Iteration 168/1000 | Loss: 0.00002147
Iteration 169/1000 | Loss: 0.00002204
Iteration 170/1000 | Loss: 0.00002146
Iteration 171/1000 | Loss: 0.00002145
Iteration 172/1000 | Loss: 0.00002144
Iteration 173/1000 | Loss: 0.00002144
Iteration 174/1000 | Loss: 0.00002144
Iteration 175/1000 | Loss: 0.00002144
Iteration 176/1000 | Loss: 0.00002144
Iteration 177/1000 | Loss: 0.00002144
Iteration 178/1000 | Loss: 0.00002144
Iteration 179/1000 | Loss: 0.00002144
Iteration 180/1000 | Loss: 0.00002144
Iteration 181/1000 | Loss: 0.00002144
Iteration 182/1000 | Loss: 0.00002144
Iteration 183/1000 | Loss: 0.00002144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.144223435607273e-05, 2.144223435607273e-05, 2.144223435607273e-05, 2.144223435607273e-05, 2.144223435607273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.144223435607273e-05

Optimization complete. Final v2v error: 3.908815860748291 mm

Highest mean error: 5.292627334594727 mm for frame 93

Lowest mean error: 3.1406595706939697 mm for frame 134

Saving results

Total time: 179.09669303894043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957183
Iteration 2/25 | Loss: 0.00293720
Iteration 3/25 | Loss: 0.00213311
Iteration 4/25 | Loss: 0.00184038
Iteration 5/25 | Loss: 0.00175386
Iteration 6/25 | Loss: 0.00144341
Iteration 7/25 | Loss: 0.00137187
Iteration 8/25 | Loss: 0.00136307
Iteration 9/25 | Loss: 0.00135637
Iteration 10/25 | Loss: 0.00136365
Iteration 11/25 | Loss: 0.00135993
Iteration 12/25 | Loss: 0.00134136
Iteration 13/25 | Loss: 0.00133543
Iteration 14/25 | Loss: 0.00133277
Iteration 15/25 | Loss: 0.00133214
Iteration 16/25 | Loss: 0.00133184
Iteration 17/25 | Loss: 0.00133177
Iteration 18/25 | Loss: 0.00133176
Iteration 19/25 | Loss: 0.00133176
Iteration 20/25 | Loss: 0.00133176
Iteration 21/25 | Loss: 0.00133176
Iteration 22/25 | Loss: 0.00133176
Iteration 23/25 | Loss: 0.00133175
Iteration 24/25 | Loss: 0.00133175
Iteration 25/25 | Loss: 0.00133175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44660676
Iteration 2/25 | Loss: 0.00062259
Iteration 3/25 | Loss: 0.00062258
Iteration 4/25 | Loss: 0.00062258
Iteration 5/25 | Loss: 0.00062258
Iteration 6/25 | Loss: 0.00062258
Iteration 7/25 | Loss: 0.00062258
Iteration 8/25 | Loss: 0.00062258
Iteration 9/25 | Loss: 0.00062258
Iteration 10/25 | Loss: 0.00062258
Iteration 11/25 | Loss: 0.00062258
Iteration 12/25 | Loss: 0.00062258
Iteration 13/25 | Loss: 0.00062258
Iteration 14/25 | Loss: 0.00062258
Iteration 15/25 | Loss: 0.00062258
Iteration 16/25 | Loss: 0.00062258
Iteration 17/25 | Loss: 0.00062258
Iteration 18/25 | Loss: 0.00062258
Iteration 19/25 | Loss: 0.00062258
Iteration 20/25 | Loss: 0.00062258
Iteration 21/25 | Loss: 0.00062258
Iteration 22/25 | Loss: 0.00062258
Iteration 23/25 | Loss: 0.00062258
Iteration 24/25 | Loss: 0.00062258
Iteration 25/25 | Loss: 0.00062258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006225789547897875, 0.0006225789547897875, 0.0006225789547897875, 0.0006225789547897875, 0.0006225789547897875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006225789547897875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062258
Iteration 2/1000 | Loss: 0.00003812
Iteration 3/1000 | Loss: 0.00002837
Iteration 4/1000 | Loss: 0.00002610
Iteration 5/1000 | Loss: 0.00002491
Iteration 6/1000 | Loss: 0.00002423
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002346
Iteration 9/1000 | Loss: 0.00002324
Iteration 10/1000 | Loss: 0.00002323
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002274
Iteration 13/1000 | Loss: 0.00002271
Iteration 14/1000 | Loss: 0.00002265
Iteration 15/1000 | Loss: 0.00002257
Iteration 16/1000 | Loss: 0.00002238
Iteration 17/1000 | Loss: 0.00002231
Iteration 18/1000 | Loss: 0.00002222
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00002218
Iteration 21/1000 | Loss: 0.00002218
Iteration 22/1000 | Loss: 0.00002217
Iteration 23/1000 | Loss: 0.00002217
Iteration 24/1000 | Loss: 0.00002217
Iteration 25/1000 | Loss: 0.00002216
Iteration 26/1000 | Loss: 0.00002216
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002215
Iteration 29/1000 | Loss: 0.00002215
Iteration 30/1000 | Loss: 0.00002215
Iteration 31/1000 | Loss: 0.00002215
Iteration 32/1000 | Loss: 0.00002215
Iteration 33/1000 | Loss: 0.00002215
Iteration 34/1000 | Loss: 0.00002214
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00002214
Iteration 37/1000 | Loss: 0.00002214
Iteration 38/1000 | Loss: 0.00002214
Iteration 39/1000 | Loss: 0.00002214
Iteration 40/1000 | Loss: 0.00002214
Iteration 41/1000 | Loss: 0.00002214
Iteration 42/1000 | Loss: 0.00002214
Iteration 43/1000 | Loss: 0.00002213
Iteration 44/1000 | Loss: 0.00002213
Iteration 45/1000 | Loss: 0.00002213
Iteration 46/1000 | Loss: 0.00002213
Iteration 47/1000 | Loss: 0.00002212
Iteration 48/1000 | Loss: 0.00002212
Iteration 49/1000 | Loss: 0.00002212
Iteration 50/1000 | Loss: 0.00002212
Iteration 51/1000 | Loss: 0.00002211
Iteration 52/1000 | Loss: 0.00002211
Iteration 53/1000 | Loss: 0.00002211
Iteration 54/1000 | Loss: 0.00002211
Iteration 55/1000 | Loss: 0.00002211
Iteration 56/1000 | Loss: 0.00002211
Iteration 57/1000 | Loss: 0.00002211
Iteration 58/1000 | Loss: 0.00002211
Iteration 59/1000 | Loss: 0.00002211
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002210
Iteration 62/1000 | Loss: 0.00002210
Iteration 63/1000 | Loss: 0.00002210
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002210
Iteration 66/1000 | Loss: 0.00002210
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002210
Iteration 69/1000 | Loss: 0.00002210
Iteration 70/1000 | Loss: 0.00002210
Iteration 71/1000 | Loss: 0.00002209
Iteration 72/1000 | Loss: 0.00002209
Iteration 73/1000 | Loss: 0.00002209
Iteration 74/1000 | Loss: 0.00002209
Iteration 75/1000 | Loss: 0.00002209
Iteration 76/1000 | Loss: 0.00002209
Iteration 77/1000 | Loss: 0.00002209
Iteration 78/1000 | Loss: 0.00002209
Iteration 79/1000 | Loss: 0.00002209
Iteration 80/1000 | Loss: 0.00002208
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002208
Iteration 83/1000 | Loss: 0.00002208
Iteration 84/1000 | Loss: 0.00002208
Iteration 85/1000 | Loss: 0.00002208
Iteration 86/1000 | Loss: 0.00002208
Iteration 87/1000 | Loss: 0.00002208
Iteration 88/1000 | Loss: 0.00002207
Iteration 89/1000 | Loss: 0.00002207
Iteration 90/1000 | Loss: 0.00002207
Iteration 91/1000 | Loss: 0.00002207
Iteration 92/1000 | Loss: 0.00002207
Iteration 93/1000 | Loss: 0.00002207
Iteration 94/1000 | Loss: 0.00002207
Iteration 95/1000 | Loss: 0.00002207
Iteration 96/1000 | Loss: 0.00002207
Iteration 97/1000 | Loss: 0.00002207
Iteration 98/1000 | Loss: 0.00002207
Iteration 99/1000 | Loss: 0.00002206
Iteration 100/1000 | Loss: 0.00002206
Iteration 101/1000 | Loss: 0.00002206
Iteration 102/1000 | Loss: 0.00002206
Iteration 103/1000 | Loss: 0.00002206
Iteration 104/1000 | Loss: 0.00002206
Iteration 105/1000 | Loss: 0.00002206
Iteration 106/1000 | Loss: 0.00002206
Iteration 107/1000 | Loss: 0.00002206
Iteration 108/1000 | Loss: 0.00002206
Iteration 109/1000 | Loss: 0.00002206
Iteration 110/1000 | Loss: 0.00002206
Iteration 111/1000 | Loss: 0.00002206
Iteration 112/1000 | Loss: 0.00002205
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002205
Iteration 115/1000 | Loss: 0.00002205
Iteration 116/1000 | Loss: 0.00002205
Iteration 117/1000 | Loss: 0.00002205
Iteration 118/1000 | Loss: 0.00002205
Iteration 119/1000 | Loss: 0.00002205
Iteration 120/1000 | Loss: 0.00002205
Iteration 121/1000 | Loss: 0.00002205
Iteration 122/1000 | Loss: 0.00002205
Iteration 123/1000 | Loss: 0.00002205
Iteration 124/1000 | Loss: 0.00002205
Iteration 125/1000 | Loss: 0.00002205
Iteration 126/1000 | Loss: 0.00002205
Iteration 127/1000 | Loss: 0.00002205
Iteration 128/1000 | Loss: 0.00002205
Iteration 129/1000 | Loss: 0.00002205
Iteration 130/1000 | Loss: 0.00002205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.2054813598515466e-05, 2.2054813598515466e-05, 2.2054813598515466e-05, 2.2054813598515466e-05, 2.2054813598515466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2054813598515466e-05

Optimization complete. Final v2v error: 3.9786477088928223 mm

Highest mean error: 4.121790409088135 mm for frame 0

Lowest mean error: 3.730433225631714 mm for frame 15

Saving results

Total time: 56.92684030532837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778845
Iteration 2/25 | Loss: 0.00143482
Iteration 3/25 | Loss: 0.00131695
Iteration 4/25 | Loss: 0.00130896
Iteration 5/25 | Loss: 0.00130704
Iteration 6/25 | Loss: 0.00130704
Iteration 7/25 | Loss: 0.00130704
Iteration 8/25 | Loss: 0.00130704
Iteration 9/25 | Loss: 0.00130704
Iteration 10/25 | Loss: 0.00130704
Iteration 11/25 | Loss: 0.00130704
Iteration 12/25 | Loss: 0.00130704
Iteration 13/25 | Loss: 0.00130704
Iteration 14/25 | Loss: 0.00130704
Iteration 15/25 | Loss: 0.00130704
Iteration 16/25 | Loss: 0.00130704
Iteration 17/25 | Loss: 0.00130704
Iteration 18/25 | Loss: 0.00130704
Iteration 19/25 | Loss: 0.00130704
Iteration 20/25 | Loss: 0.00130704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001307043363340199, 0.001307043363340199, 0.001307043363340199, 0.001307043363340199, 0.001307043363340199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001307043363340199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37226069
Iteration 2/25 | Loss: 0.00060424
Iteration 3/25 | Loss: 0.00060418
Iteration 4/25 | Loss: 0.00060418
Iteration 5/25 | Loss: 0.00060418
Iteration 6/25 | Loss: 0.00060418
Iteration 7/25 | Loss: 0.00060418
Iteration 8/25 | Loss: 0.00060418
Iteration 9/25 | Loss: 0.00060418
Iteration 10/25 | Loss: 0.00060418
Iteration 11/25 | Loss: 0.00060418
Iteration 12/25 | Loss: 0.00060418
Iteration 13/25 | Loss: 0.00060418
Iteration 14/25 | Loss: 0.00060418
Iteration 15/25 | Loss: 0.00060418
Iteration 16/25 | Loss: 0.00060418
Iteration 17/25 | Loss: 0.00060418
Iteration 18/25 | Loss: 0.00060418
Iteration 19/25 | Loss: 0.00060418
Iteration 20/25 | Loss: 0.00060418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006041811429895461, 0.0006041811429895461, 0.0006041811429895461, 0.0006041811429895461, 0.0006041811429895461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006041811429895461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060418
Iteration 2/1000 | Loss: 0.00003045
Iteration 3/1000 | Loss: 0.00002129
Iteration 4/1000 | Loss: 0.00001905
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001723
Iteration 8/1000 | Loss: 0.00001694
Iteration 9/1000 | Loss: 0.00001664
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001608
Iteration 16/1000 | Loss: 0.00001607
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001601
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001600
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001589
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001586
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001585
Iteration 41/1000 | Loss: 0.00001584
Iteration 42/1000 | Loss: 0.00001583
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001567
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001562
Iteration 81/1000 | Loss: 0.00001562
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001558
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001557
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001556
Iteration 102/1000 | Loss: 0.00001556
Iteration 103/1000 | Loss: 0.00001556
Iteration 104/1000 | Loss: 0.00001556
Iteration 105/1000 | Loss: 0.00001556
Iteration 106/1000 | Loss: 0.00001556
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001556
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.555805283715017e-05, 1.555805283715017e-05, 1.555805283715017e-05, 1.555805283715017e-05, 1.555805283715017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555805283715017e-05

Optimization complete. Final v2v error: 3.3416969776153564 mm

Highest mean error: 3.5115201473236084 mm for frame 46

Lowest mean error: 3.2536780834198 mm for frame 103

Saving results

Total time: 35.31178092956543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809637
Iteration 2/25 | Loss: 0.00162081
Iteration 3/25 | Loss: 0.00136110
Iteration 4/25 | Loss: 0.00131357
Iteration 5/25 | Loss: 0.00130233
Iteration 6/25 | Loss: 0.00129812
Iteration 7/25 | Loss: 0.00129753
Iteration 8/25 | Loss: 0.00130579
Iteration 9/25 | Loss: 0.00130187
Iteration 10/25 | Loss: 0.00131256
Iteration 11/25 | Loss: 0.00131220
Iteration 12/25 | Loss: 0.00131129
Iteration 13/25 | Loss: 0.00131409
Iteration 14/25 | Loss: 0.00130514
Iteration 15/25 | Loss: 0.00130566
Iteration 16/25 | Loss: 0.00131048
Iteration 17/25 | Loss: 0.00130998
Iteration 18/25 | Loss: 0.00130570
Iteration 19/25 | Loss: 0.00130443
Iteration 20/25 | Loss: 0.00130009
Iteration 21/25 | Loss: 0.00130319
Iteration 22/25 | Loss: 0.00130370
Iteration 23/25 | Loss: 0.00130008
Iteration 24/25 | Loss: 0.00130080
Iteration 25/25 | Loss: 0.00129833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43171883
Iteration 2/25 | Loss: 0.00090260
Iteration 3/25 | Loss: 0.00090260
Iteration 4/25 | Loss: 0.00090260
Iteration 5/25 | Loss: 0.00090260
Iteration 6/25 | Loss: 0.00090260
Iteration 7/25 | Loss: 0.00090260
Iteration 8/25 | Loss: 0.00090260
Iteration 9/25 | Loss: 0.00090260
Iteration 10/25 | Loss: 0.00090260
Iteration 11/25 | Loss: 0.00090260
Iteration 12/25 | Loss: 0.00090260
Iteration 13/25 | Loss: 0.00090260
Iteration 14/25 | Loss: 0.00090260
Iteration 15/25 | Loss: 0.00090260
Iteration 16/25 | Loss: 0.00090260
Iteration 17/25 | Loss: 0.00090260
Iteration 18/25 | Loss: 0.00090260
Iteration 19/25 | Loss: 0.00090260
Iteration 20/25 | Loss: 0.00090260
Iteration 21/25 | Loss: 0.00090260
Iteration 22/25 | Loss: 0.00090260
Iteration 23/25 | Loss: 0.00090260
Iteration 24/25 | Loss: 0.00090260
Iteration 25/25 | Loss: 0.00090260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090260
Iteration 2/1000 | Loss: 0.00009488
Iteration 3/1000 | Loss: 0.00013523
Iteration 4/1000 | Loss: 0.00017697
Iteration 5/1000 | Loss: 0.00013177
Iteration 6/1000 | Loss: 0.00014449
Iteration 7/1000 | Loss: 0.00025174
Iteration 8/1000 | Loss: 0.00014999
Iteration 9/1000 | Loss: 0.00008051
Iteration 10/1000 | Loss: 0.00018024
Iteration 11/1000 | Loss: 0.00010160
Iteration 12/1000 | Loss: 0.00012035
Iteration 13/1000 | Loss: 0.00020164
Iteration 14/1000 | Loss: 0.00022994
Iteration 15/1000 | Loss: 0.00024185
Iteration 16/1000 | Loss: 0.00024103
Iteration 17/1000 | Loss: 0.00026024
Iteration 18/1000 | Loss: 0.00024798
Iteration 19/1000 | Loss: 0.00025315
Iteration 20/1000 | Loss: 0.00014110
Iteration 21/1000 | Loss: 0.00012843
Iteration 22/1000 | Loss: 0.00008242
Iteration 23/1000 | Loss: 0.00007948
Iteration 24/1000 | Loss: 0.00008866
Iteration 25/1000 | Loss: 0.00010203
Iteration 26/1000 | Loss: 0.00009970
Iteration 27/1000 | Loss: 0.00011737
Iteration 28/1000 | Loss: 0.00010655
Iteration 29/1000 | Loss: 0.00014686
Iteration 30/1000 | Loss: 0.00010948
Iteration 31/1000 | Loss: 0.00009385
Iteration 32/1000 | Loss: 0.00011689
Iteration 33/1000 | Loss: 0.00009784
Iteration 34/1000 | Loss: 0.00011745
Iteration 35/1000 | Loss: 0.00013097
Iteration 36/1000 | Loss: 0.00012388
Iteration 37/1000 | Loss: 0.00012285
Iteration 38/1000 | Loss: 0.00013353
Iteration 39/1000 | Loss: 0.00013439
Iteration 40/1000 | Loss: 0.00014139
Iteration 41/1000 | Loss: 0.00014654
Iteration 42/1000 | Loss: 0.00015654
Iteration 43/1000 | Loss: 0.00015909
Iteration 44/1000 | Loss: 0.00017223
Iteration 45/1000 | Loss: 0.00008130
Iteration 46/1000 | Loss: 0.00012198
Iteration 47/1000 | Loss: 0.00008939
Iteration 48/1000 | Loss: 0.00012068
Iteration 49/1000 | Loss: 0.00011049
Iteration 50/1000 | Loss: 0.00010111
Iteration 51/1000 | Loss: 0.00012191
Iteration 52/1000 | Loss: 0.00010013
Iteration 53/1000 | Loss: 0.00009339
Iteration 54/1000 | Loss: 0.00007775
Iteration 55/1000 | Loss: 0.00004820
Iteration 56/1000 | Loss: 0.00007941
Iteration 57/1000 | Loss: 0.00008082
Iteration 58/1000 | Loss: 0.00007118
Iteration 59/1000 | Loss: 0.00009358
Iteration 60/1000 | Loss: 0.00011888
Iteration 61/1000 | Loss: 0.00008322
Iteration 62/1000 | Loss: 0.00007463
Iteration 63/1000 | Loss: 0.00005717
Iteration 64/1000 | Loss: 0.00006593
Iteration 65/1000 | Loss: 0.00007742
Iteration 66/1000 | Loss: 0.00009359
Iteration 67/1000 | Loss: 0.00009187
Iteration 68/1000 | Loss: 0.00008616
Iteration 69/1000 | Loss: 0.00006747
Iteration 70/1000 | Loss: 0.00006466
Iteration 71/1000 | Loss: 0.00005266
Iteration 72/1000 | Loss: 0.00004882
Iteration 73/1000 | Loss: 0.00006044
Iteration 74/1000 | Loss: 0.00006954
Iteration 75/1000 | Loss: 0.00010227
Iteration 76/1000 | Loss: 0.00006065
Iteration 77/1000 | Loss: 0.00007081
Iteration 78/1000 | Loss: 0.00007516
Iteration 79/1000 | Loss: 0.00006897
Iteration 80/1000 | Loss: 0.00007971
Iteration 81/1000 | Loss: 0.00008790
Iteration 82/1000 | Loss: 0.00008367
Iteration 83/1000 | Loss: 0.00008898
Iteration 84/1000 | Loss: 0.00009616
Iteration 85/1000 | Loss: 0.00009175
Iteration 86/1000 | Loss: 0.00008943
Iteration 87/1000 | Loss: 0.00008485
Iteration 88/1000 | Loss: 0.00009067
Iteration 89/1000 | Loss: 0.00009601
Iteration 90/1000 | Loss: 0.00009866
Iteration 91/1000 | Loss: 0.00009574
Iteration 92/1000 | Loss: 0.00008252
Iteration 93/1000 | Loss: 0.00004833
Iteration 94/1000 | Loss: 0.00005811
Iteration 95/1000 | Loss: 0.00010035
Iteration 96/1000 | Loss: 0.00008878
Iteration 97/1000 | Loss: 0.00004760
Iteration 98/1000 | Loss: 0.00008177
Iteration 99/1000 | Loss: 0.00012699
Iteration 100/1000 | Loss: 0.00009698
Iteration 101/1000 | Loss: 0.00009016
Iteration 102/1000 | Loss: 0.00004362
Iteration 103/1000 | Loss: 0.00005027
Iteration 104/1000 | Loss: 0.00006857
Iteration 105/1000 | Loss: 0.00005857
Iteration 106/1000 | Loss: 0.00007093
Iteration 107/1000 | Loss: 0.00005988
Iteration 108/1000 | Loss: 0.00009338
Iteration 109/1000 | Loss: 0.00007643
Iteration 110/1000 | Loss: 0.00008284
Iteration 111/1000 | Loss: 0.00011178
Iteration 112/1000 | Loss: 0.00029579
Iteration 113/1000 | Loss: 0.00013214
Iteration 114/1000 | Loss: 0.00004877
Iteration 115/1000 | Loss: 0.00003606
Iteration 116/1000 | Loss: 0.00020380
Iteration 117/1000 | Loss: 0.00004689
Iteration 118/1000 | Loss: 0.00003054
Iteration 119/1000 | Loss: 0.00003576
Iteration 120/1000 | Loss: 0.00002532
Iteration 121/1000 | Loss: 0.00002280
Iteration 122/1000 | Loss: 0.00002165
Iteration 123/1000 | Loss: 0.00002378
Iteration 124/1000 | Loss: 0.00002225
Iteration 125/1000 | Loss: 0.00002146
Iteration 126/1000 | Loss: 0.00002008
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00003179
Iteration 129/1000 | Loss: 0.00001995
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00005239
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001898
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00002688
Iteration 138/1000 | Loss: 0.00002458
Iteration 139/1000 | Loss: 0.00001952
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00002406
Iteration 142/1000 | Loss: 0.00002863
Iteration 143/1000 | Loss: 0.00002317
Iteration 144/1000 | Loss: 0.00002423
Iteration 145/1000 | Loss: 0.00002044
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001561
Iteration 149/1000 | Loss: 0.00002215
Iteration 150/1000 | Loss: 0.00002406
Iteration 151/1000 | Loss: 0.00002238
Iteration 152/1000 | Loss: 0.00002349
Iteration 153/1000 | Loss: 0.00002167
Iteration 154/1000 | Loss: 0.00002499
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00002259
Iteration 157/1000 | Loss: 0.00002279
Iteration 158/1000 | Loss: 0.00002245
Iteration 159/1000 | Loss: 0.00002833
Iteration 160/1000 | Loss: 0.00002737
Iteration 161/1000 | Loss: 0.00001878
Iteration 162/1000 | Loss: 0.00002366
Iteration 163/1000 | Loss: 0.00002437
Iteration 164/1000 | Loss: 0.00002392
Iteration 165/1000 | Loss: 0.00002615
Iteration 166/1000 | Loss: 0.00002273
Iteration 167/1000 | Loss: 0.00002331
Iteration 168/1000 | Loss: 0.00002246
Iteration 169/1000 | Loss: 0.00002485
Iteration 170/1000 | Loss: 0.00004369
Iteration 171/1000 | Loss: 0.00002948
Iteration 172/1000 | Loss: 0.00001830
Iteration 173/1000 | Loss: 0.00001604
Iteration 174/1000 | Loss: 0.00004287
Iteration 175/1000 | Loss: 0.00002056
Iteration 176/1000 | Loss: 0.00003253
Iteration 177/1000 | Loss: 0.00003257
Iteration 178/1000 | Loss: 0.00003021
Iteration 179/1000 | Loss: 0.00003167
Iteration 180/1000 | Loss: 0.00003486
Iteration 181/1000 | Loss: 0.00001703
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001483
Iteration 184/1000 | Loss: 0.00001438
Iteration 185/1000 | Loss: 0.00001402
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001376
Iteration 191/1000 | Loss: 0.00001376
Iteration 192/1000 | Loss: 0.00001370
Iteration 193/1000 | Loss: 0.00001367
Iteration 194/1000 | Loss: 0.00001366
Iteration 195/1000 | Loss: 0.00001365
Iteration 196/1000 | Loss: 0.00001364
Iteration 197/1000 | Loss: 0.00001364
Iteration 198/1000 | Loss: 0.00001363
Iteration 199/1000 | Loss: 0.00001363
Iteration 200/1000 | Loss: 0.00001362
Iteration 201/1000 | Loss: 0.00001356
Iteration 202/1000 | Loss: 0.00001356
Iteration 203/1000 | Loss: 0.00001355
Iteration 204/1000 | Loss: 0.00001352
Iteration 205/1000 | Loss: 0.00001351
Iteration 206/1000 | Loss: 0.00001351
Iteration 207/1000 | Loss: 0.00001350
Iteration 208/1000 | Loss: 0.00001350
Iteration 209/1000 | Loss: 0.00001350
Iteration 210/1000 | Loss: 0.00001349
Iteration 211/1000 | Loss: 0.00001349
Iteration 212/1000 | Loss: 0.00001349
Iteration 213/1000 | Loss: 0.00001349
Iteration 214/1000 | Loss: 0.00001349
Iteration 215/1000 | Loss: 0.00001349
Iteration 216/1000 | Loss: 0.00001348
Iteration 217/1000 | Loss: 0.00001348
Iteration 218/1000 | Loss: 0.00001348
Iteration 219/1000 | Loss: 0.00001348
Iteration 220/1000 | Loss: 0.00001347
Iteration 221/1000 | Loss: 0.00001347
Iteration 222/1000 | Loss: 0.00001347
Iteration 223/1000 | Loss: 0.00001346
Iteration 224/1000 | Loss: 0.00001346
Iteration 225/1000 | Loss: 0.00001346
Iteration 226/1000 | Loss: 0.00001346
Iteration 227/1000 | Loss: 0.00001346
Iteration 228/1000 | Loss: 0.00001346
Iteration 229/1000 | Loss: 0.00001346
Iteration 230/1000 | Loss: 0.00001345
Iteration 231/1000 | Loss: 0.00001345
Iteration 232/1000 | Loss: 0.00001345
Iteration 233/1000 | Loss: 0.00001345
Iteration 234/1000 | Loss: 0.00001345
Iteration 235/1000 | Loss: 0.00001345
Iteration 236/1000 | Loss: 0.00001344
Iteration 237/1000 | Loss: 0.00001344
Iteration 238/1000 | Loss: 0.00001344
Iteration 239/1000 | Loss: 0.00001344
Iteration 240/1000 | Loss: 0.00001344
Iteration 241/1000 | Loss: 0.00001343
Iteration 242/1000 | Loss: 0.00001343
Iteration 243/1000 | Loss: 0.00001342
Iteration 244/1000 | Loss: 0.00001342
Iteration 245/1000 | Loss: 0.00001342
Iteration 246/1000 | Loss: 0.00001341
Iteration 247/1000 | Loss: 0.00001341
Iteration 248/1000 | Loss: 0.00001341
Iteration 249/1000 | Loss: 0.00001341
Iteration 250/1000 | Loss: 0.00001340
Iteration 251/1000 | Loss: 0.00001340
Iteration 252/1000 | Loss: 0.00001340
Iteration 253/1000 | Loss: 0.00001340
Iteration 254/1000 | Loss: 0.00001340
Iteration 255/1000 | Loss: 0.00001340
Iteration 256/1000 | Loss: 0.00001340
Iteration 257/1000 | Loss: 0.00001340
Iteration 258/1000 | Loss: 0.00001340
Iteration 259/1000 | Loss: 0.00001340
Iteration 260/1000 | Loss: 0.00001340
Iteration 261/1000 | Loss: 0.00001340
Iteration 262/1000 | Loss: 0.00001340
Iteration 263/1000 | Loss: 0.00001339
Iteration 264/1000 | Loss: 0.00001339
Iteration 265/1000 | Loss: 0.00001339
Iteration 266/1000 | Loss: 0.00001338
Iteration 267/1000 | Loss: 0.00001338
Iteration 268/1000 | Loss: 0.00001338
Iteration 269/1000 | Loss: 0.00001337
Iteration 270/1000 | Loss: 0.00001337
Iteration 271/1000 | Loss: 0.00001337
Iteration 272/1000 | Loss: 0.00001337
Iteration 273/1000 | Loss: 0.00001336
Iteration 274/1000 | Loss: 0.00001336
Iteration 275/1000 | Loss: 0.00001336
Iteration 276/1000 | Loss: 0.00001335
Iteration 277/1000 | Loss: 0.00001335
Iteration 278/1000 | Loss: 0.00001335
Iteration 279/1000 | Loss: 0.00001335
Iteration 280/1000 | Loss: 0.00001334
Iteration 281/1000 | Loss: 0.00001334
Iteration 282/1000 | Loss: 0.00001334
Iteration 283/1000 | Loss: 0.00001334
Iteration 284/1000 | Loss: 0.00001334
Iteration 285/1000 | Loss: 0.00001334
Iteration 286/1000 | Loss: 0.00001333
Iteration 287/1000 | Loss: 0.00001333
Iteration 288/1000 | Loss: 0.00001332
Iteration 289/1000 | Loss: 0.00001332
Iteration 290/1000 | Loss: 0.00001331
Iteration 291/1000 | Loss: 0.00001330
Iteration 292/1000 | Loss: 0.00001329
Iteration 293/1000 | Loss: 0.00001329
Iteration 294/1000 | Loss: 0.00001329
Iteration 295/1000 | Loss: 0.00001328
Iteration 296/1000 | Loss: 0.00001328
Iteration 297/1000 | Loss: 0.00001328
Iteration 298/1000 | Loss: 0.00001328
Iteration 299/1000 | Loss: 0.00001328
Iteration 300/1000 | Loss: 0.00001328
Iteration 301/1000 | Loss: 0.00001326
Iteration 302/1000 | Loss: 0.00001326
Iteration 303/1000 | Loss: 0.00001326
Iteration 304/1000 | Loss: 0.00001325
Iteration 305/1000 | Loss: 0.00001325
Iteration 306/1000 | Loss: 0.00001325
Iteration 307/1000 | Loss: 0.00001325
Iteration 308/1000 | Loss: 0.00001325
Iteration 309/1000 | Loss: 0.00001325
Iteration 310/1000 | Loss: 0.00001325
Iteration 311/1000 | Loss: 0.00001325
Iteration 312/1000 | Loss: 0.00001324
Iteration 313/1000 | Loss: 0.00001324
Iteration 314/1000 | Loss: 0.00001324
Iteration 315/1000 | Loss: 0.00001324
Iteration 316/1000 | Loss: 0.00001324
Iteration 317/1000 | Loss: 0.00001324
Iteration 318/1000 | Loss: 0.00001324
Iteration 319/1000 | Loss: 0.00001324
Iteration 320/1000 | Loss: 0.00001324
Iteration 321/1000 | Loss: 0.00001324
Iteration 322/1000 | Loss: 0.00001324
Iteration 323/1000 | Loss: 0.00001324
Iteration 324/1000 | Loss: 0.00001323
Iteration 325/1000 | Loss: 0.00001323
Iteration 326/1000 | Loss: 0.00001322
Iteration 327/1000 | Loss: 0.00001321
Iteration 328/1000 | Loss: 0.00001321
Iteration 329/1000 | Loss: 0.00001321
Iteration 330/1000 | Loss: 0.00001320
Iteration 331/1000 | Loss: 0.00001319
Iteration 332/1000 | Loss: 0.00001319
Iteration 333/1000 | Loss: 0.00001319
Iteration 334/1000 | Loss: 0.00001319
Iteration 335/1000 | Loss: 0.00001318
Iteration 336/1000 | Loss: 0.00001318
Iteration 337/1000 | Loss: 0.00001317
Iteration 338/1000 | Loss: 0.00001317
Iteration 339/1000 | Loss: 0.00001317
Iteration 340/1000 | Loss: 0.00001316
Iteration 341/1000 | Loss: 0.00001316
Iteration 342/1000 | Loss: 0.00001316
Iteration 343/1000 | Loss: 0.00001316
Iteration 344/1000 | Loss: 0.00001316
Iteration 345/1000 | Loss: 0.00001316
Iteration 346/1000 | Loss: 0.00001315
Iteration 347/1000 | Loss: 0.00001315
Iteration 348/1000 | Loss: 0.00001315
Iteration 349/1000 | Loss: 0.00001314
Iteration 350/1000 | Loss: 0.00001314
Iteration 351/1000 | Loss: 0.00001314
Iteration 352/1000 | Loss: 0.00001314
Iteration 353/1000 | Loss: 0.00001314
Iteration 354/1000 | Loss: 0.00001313
Iteration 355/1000 | Loss: 0.00001313
Iteration 356/1000 | Loss: 0.00001313
Iteration 357/1000 | Loss: 0.00001313
Iteration 358/1000 | Loss: 0.00001312
Iteration 359/1000 | Loss: 0.00001312
Iteration 360/1000 | Loss: 0.00001312
Iteration 361/1000 | Loss: 0.00001312
Iteration 362/1000 | Loss: 0.00001312
Iteration 363/1000 | Loss: 0.00001312
Iteration 364/1000 | Loss: 0.00001312
Iteration 365/1000 | Loss: 0.00001311
Iteration 366/1000 | Loss: 0.00001311
Iteration 367/1000 | Loss: 0.00001311
Iteration 368/1000 | Loss: 0.00001311
Iteration 369/1000 | Loss: 0.00001311
Iteration 370/1000 | Loss: 0.00001311
Iteration 371/1000 | Loss: 0.00001311
Iteration 372/1000 | Loss: 0.00001310
Iteration 373/1000 | Loss: 0.00001310
Iteration 374/1000 | Loss: 0.00001310
Iteration 375/1000 | Loss: 0.00001310
Iteration 376/1000 | Loss: 0.00001310
Iteration 377/1000 | Loss: 0.00001310
Iteration 378/1000 | Loss: 0.00001310
Iteration 379/1000 | Loss: 0.00001309
Iteration 380/1000 | Loss: 0.00001309
Iteration 381/1000 | Loss: 0.00001309
Iteration 382/1000 | Loss: 0.00001309
Iteration 383/1000 | Loss: 0.00001309
Iteration 384/1000 | Loss: 0.00001309
Iteration 385/1000 | Loss: 0.00001308
Iteration 386/1000 | Loss: 0.00001308
Iteration 387/1000 | Loss: 0.00001308
Iteration 388/1000 | Loss: 0.00001308
Iteration 389/1000 | Loss: 0.00001308
Iteration 390/1000 | Loss: 0.00001308
Iteration 391/1000 | Loss: 0.00001307
Iteration 392/1000 | Loss: 0.00001307
Iteration 393/1000 | Loss: 0.00001307
Iteration 394/1000 | Loss: 0.00001307
Iteration 395/1000 | Loss: 0.00001307
Iteration 396/1000 | Loss: 0.00001307
Iteration 397/1000 | Loss: 0.00001307
Iteration 398/1000 | Loss: 0.00001307
Iteration 399/1000 | Loss: 0.00001307
Iteration 400/1000 | Loss: 0.00001307
Iteration 401/1000 | Loss: 0.00001307
Iteration 402/1000 | Loss: 0.00001307
Iteration 403/1000 | Loss: 0.00001307
Iteration 404/1000 | Loss: 0.00001307
Iteration 405/1000 | Loss: 0.00001307
Iteration 406/1000 | Loss: 0.00001307
Iteration 407/1000 | Loss: 0.00001307
Iteration 408/1000 | Loss: 0.00001307
Iteration 409/1000 | Loss: 0.00001306
Iteration 410/1000 | Loss: 0.00001306
Iteration 411/1000 | Loss: 0.00001306
Iteration 412/1000 | Loss: 0.00001306
Iteration 413/1000 | Loss: 0.00001306
Iteration 414/1000 | Loss: 0.00001306
Iteration 415/1000 | Loss: 0.00001306
Iteration 416/1000 | Loss: 0.00001306
Iteration 417/1000 | Loss: 0.00001306
Iteration 418/1000 | Loss: 0.00001306
Iteration 419/1000 | Loss: 0.00001306
Iteration 420/1000 | Loss: 0.00001306
Iteration 421/1000 | Loss: 0.00001306
Iteration 422/1000 | Loss: 0.00001306
Iteration 423/1000 | Loss: 0.00001306
Iteration 424/1000 | Loss: 0.00001306
Iteration 425/1000 | Loss: 0.00001306
Iteration 426/1000 | Loss: 0.00001306
Iteration 427/1000 | Loss: 0.00001306
Iteration 428/1000 | Loss: 0.00001306
Iteration 429/1000 | Loss: 0.00001306
Iteration 430/1000 | Loss: 0.00001306
Iteration 431/1000 | Loss: 0.00001305
Iteration 432/1000 | Loss: 0.00001305
Iteration 433/1000 | Loss: 0.00001305
Iteration 434/1000 | Loss: 0.00001305
Iteration 435/1000 | Loss: 0.00001305
Iteration 436/1000 | Loss: 0.00001305
Iteration 437/1000 | Loss: 0.00001305
Iteration 438/1000 | Loss: 0.00001305
Iteration 439/1000 | Loss: 0.00001305
Iteration 440/1000 | Loss: 0.00001305
Iteration 441/1000 | Loss: 0.00001305
Iteration 442/1000 | Loss: 0.00001305
Iteration 443/1000 | Loss: 0.00001305
Iteration 444/1000 | Loss: 0.00001305
Iteration 445/1000 | Loss: 0.00001305
Iteration 446/1000 | Loss: 0.00001305
Iteration 447/1000 | Loss: 0.00001305
Iteration 448/1000 | Loss: 0.00001305
Iteration 449/1000 | Loss: 0.00001305
Iteration 450/1000 | Loss: 0.00001305
Iteration 451/1000 | Loss: 0.00001305
Iteration 452/1000 | Loss: 0.00001305
Iteration 453/1000 | Loss: 0.00001305
Iteration 454/1000 | Loss: 0.00001305
Iteration 455/1000 | Loss: 0.00001305
Iteration 456/1000 | Loss: 0.00001305
Iteration 457/1000 | Loss: 0.00001305
Iteration 458/1000 | Loss: 0.00001304
Iteration 459/1000 | Loss: 0.00001304
Iteration 460/1000 | Loss: 0.00001304
Iteration 461/1000 | Loss: 0.00001304
Iteration 462/1000 | Loss: 0.00001304
Iteration 463/1000 | Loss: 0.00001304
Iteration 464/1000 | Loss: 0.00001304
Iteration 465/1000 | Loss: 0.00001304
Iteration 466/1000 | Loss: 0.00001304
Iteration 467/1000 | Loss: 0.00001304
Iteration 468/1000 | Loss: 0.00001304
Iteration 469/1000 | Loss: 0.00001304
Iteration 470/1000 | Loss: 0.00001304
Iteration 471/1000 | Loss: 0.00001304
Iteration 472/1000 | Loss: 0.00001304
Iteration 473/1000 | Loss: 0.00001304
Iteration 474/1000 | Loss: 0.00001304
Iteration 475/1000 | Loss: 0.00001304
Iteration 476/1000 | Loss: 0.00001304
Iteration 477/1000 | Loss: 0.00001304
Iteration 478/1000 | Loss: 0.00001304
Iteration 479/1000 | Loss: 0.00001304
Iteration 480/1000 | Loss: 0.00001304
Iteration 481/1000 | Loss: 0.00001304
Iteration 482/1000 | Loss: 0.00001304
Iteration 483/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 483. Stopping optimization.
Last 5 losses: [1.3036235031904653e-05, 1.3036235031904653e-05, 1.3036235031904653e-05, 1.3036235031904653e-05, 1.3036235031904653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3036235031904653e-05

Optimization complete. Final v2v error: 3.0981054306030273 mm

Highest mean error: 3.870499849319458 mm for frame 122

Lowest mean error: 2.9148776531219482 mm for frame 17

Saving results

Total time: 371.2681143283844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108719
Iteration 2/25 | Loss: 0.00221608
Iteration 3/25 | Loss: 0.00158646
Iteration 4/25 | Loss: 0.00150989
Iteration 5/25 | Loss: 0.00151774
Iteration 6/25 | Loss: 0.00142700
Iteration 7/25 | Loss: 0.00132799
Iteration 8/25 | Loss: 0.00120447
Iteration 9/25 | Loss: 0.00123218
Iteration 10/25 | Loss: 0.00116680
Iteration 11/25 | Loss: 0.00112874
Iteration 12/25 | Loss: 0.00108368
Iteration 13/25 | Loss: 0.00104387
Iteration 14/25 | Loss: 0.00106326
Iteration 15/25 | Loss: 0.00099631
Iteration 16/25 | Loss: 0.00098335
Iteration 17/25 | Loss: 0.00097604
Iteration 18/25 | Loss: 0.00097963
Iteration 19/25 | Loss: 0.00097751
Iteration 20/25 | Loss: 0.00097739
Iteration 21/25 | Loss: 0.00097851
Iteration 22/25 | Loss: 0.00097696
Iteration 23/25 | Loss: 0.00097628
Iteration 24/25 | Loss: 0.00097559
Iteration 25/25 | Loss: 0.00097577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35301840
Iteration 2/25 | Loss: 0.00178470
Iteration 3/25 | Loss: 0.00176278
Iteration 4/25 | Loss: 0.00176278
Iteration 5/25 | Loss: 0.00176278
Iteration 6/25 | Loss: 0.00176278
Iteration 7/25 | Loss: 0.00176278
Iteration 8/25 | Loss: 0.00176278
Iteration 9/25 | Loss: 0.00176278
Iteration 10/25 | Loss: 0.00176278
Iteration 11/25 | Loss: 0.00176278
Iteration 12/25 | Loss: 0.00176278
Iteration 13/25 | Loss: 0.00176278
Iteration 14/25 | Loss: 0.00176278
Iteration 15/25 | Loss: 0.00176278
Iteration 16/25 | Loss: 0.00176278
Iteration 17/25 | Loss: 0.00176278
Iteration 18/25 | Loss: 0.00176278
Iteration 19/25 | Loss: 0.00176278
Iteration 20/25 | Loss: 0.00176278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001762776984833181, 0.001762776984833181, 0.001762776984833181, 0.001762776984833181, 0.001762776984833181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001762776984833181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176278
Iteration 2/1000 | Loss: 0.00014118
Iteration 3/1000 | Loss: 0.00075539
Iteration 4/1000 | Loss: 0.00103226
Iteration 5/1000 | Loss: 0.00189368
Iteration 6/1000 | Loss: 0.00095441
Iteration 7/1000 | Loss: 0.00075009
Iteration 8/1000 | Loss: 0.00057336
Iteration 9/1000 | Loss: 0.00035332
Iteration 10/1000 | Loss: 0.00031160
Iteration 11/1000 | Loss: 0.00036846
Iteration 12/1000 | Loss: 0.00025553
Iteration 13/1000 | Loss: 0.00028630
Iteration 14/1000 | Loss: 0.00034032
Iteration 15/1000 | Loss: 0.00035209
Iteration 16/1000 | Loss: 0.00027354
Iteration 17/1000 | Loss: 0.00013415
Iteration 18/1000 | Loss: 0.00016891
Iteration 19/1000 | Loss: 0.00036632
Iteration 20/1000 | Loss: 0.00017067
Iteration 21/1000 | Loss: 0.00037907
Iteration 22/1000 | Loss: 0.00036266
Iteration 23/1000 | Loss: 0.00036383
Iteration 24/1000 | Loss: 0.00038767
Iteration 25/1000 | Loss: 0.00032054
Iteration 26/1000 | Loss: 0.00025877
Iteration 27/1000 | Loss: 0.00035550
Iteration 28/1000 | Loss: 0.00045665
Iteration 29/1000 | Loss: 0.00045422
Iteration 30/1000 | Loss: 0.00028955
Iteration 31/1000 | Loss: 0.00030422
Iteration 32/1000 | Loss: 0.00032538
Iteration 33/1000 | Loss: 0.00025585
Iteration 34/1000 | Loss: 0.00019082
Iteration 35/1000 | Loss: 0.00042569
Iteration 36/1000 | Loss: 0.00042280
Iteration 37/1000 | Loss: 0.00028954
Iteration 38/1000 | Loss: 0.00015243
Iteration 39/1000 | Loss: 0.00005384
Iteration 40/1000 | Loss: 0.00017847
Iteration 41/1000 | Loss: 0.00011453
Iteration 42/1000 | Loss: 0.00003943
Iteration 43/1000 | Loss: 0.00017020
Iteration 44/1000 | Loss: 0.00019405
Iteration 45/1000 | Loss: 0.00004435
Iteration 46/1000 | Loss: 0.00029333
Iteration 47/1000 | Loss: 0.00006551
Iteration 48/1000 | Loss: 0.00039109
Iteration 49/1000 | Loss: 0.00019001
Iteration 50/1000 | Loss: 0.00016092
Iteration 51/1000 | Loss: 0.00003022
Iteration 52/1000 | Loss: 0.00003824
Iteration 53/1000 | Loss: 0.00005605
Iteration 54/1000 | Loss: 0.00003742
Iteration 55/1000 | Loss: 0.00003016
Iteration 56/1000 | Loss: 0.00002080
Iteration 57/1000 | Loss: 0.00002034
Iteration 58/1000 | Loss: 0.00005877
Iteration 59/1000 | Loss: 0.00003949
Iteration 60/1000 | Loss: 0.00002158
Iteration 61/1000 | Loss: 0.00003436
Iteration 62/1000 | Loss: 0.00005101
Iteration 63/1000 | Loss: 0.00002791
Iteration 64/1000 | Loss: 0.00008450
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00003205
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00005137
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00003431
Iteration 84/1000 | Loss: 0.00002081
Iteration 85/1000 | Loss: 0.00001923
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001920
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001919
Iteration 96/1000 | Loss: 0.00001919
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001919
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001917
Iteration 110/1000 | Loss: 0.00001917
Iteration 111/1000 | Loss: 0.00001917
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001917
Iteration 116/1000 | Loss: 0.00001917
Iteration 117/1000 | Loss: 0.00001917
Iteration 118/1000 | Loss: 0.00001917
Iteration 119/1000 | Loss: 0.00001917
Iteration 120/1000 | Loss: 0.00001916
Iteration 121/1000 | Loss: 0.00001916
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001915
Iteration 124/1000 | Loss: 0.00001915
Iteration 125/1000 | Loss: 0.00001915
Iteration 126/1000 | Loss: 0.00001914
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001914
Iteration 129/1000 | Loss: 0.00001914
Iteration 130/1000 | Loss: 0.00001914
Iteration 131/1000 | Loss: 0.00001914
Iteration 132/1000 | Loss: 0.00001914
Iteration 133/1000 | Loss: 0.00001914
Iteration 134/1000 | Loss: 0.00001913
Iteration 135/1000 | Loss: 0.00001913
Iteration 136/1000 | Loss: 0.00001913
Iteration 137/1000 | Loss: 0.00001913
Iteration 138/1000 | Loss: 0.00001913
Iteration 139/1000 | Loss: 0.00001913
Iteration 140/1000 | Loss: 0.00001913
Iteration 141/1000 | Loss: 0.00001913
Iteration 142/1000 | Loss: 0.00001913
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001912
Iteration 145/1000 | Loss: 0.00001912
Iteration 146/1000 | Loss: 0.00001912
Iteration 147/1000 | Loss: 0.00001912
Iteration 148/1000 | Loss: 0.00001912
Iteration 149/1000 | Loss: 0.00001912
Iteration 150/1000 | Loss: 0.00001911
Iteration 151/1000 | Loss: 0.00001911
Iteration 152/1000 | Loss: 0.00001911
Iteration 153/1000 | Loss: 0.00001911
Iteration 154/1000 | Loss: 0.00001911
Iteration 155/1000 | Loss: 0.00001911
Iteration 156/1000 | Loss: 0.00001911
Iteration 157/1000 | Loss: 0.00001910
Iteration 158/1000 | Loss: 0.00001910
Iteration 159/1000 | Loss: 0.00001910
Iteration 160/1000 | Loss: 0.00001910
Iteration 161/1000 | Loss: 0.00001910
Iteration 162/1000 | Loss: 0.00001910
Iteration 163/1000 | Loss: 0.00001910
Iteration 164/1000 | Loss: 0.00001910
Iteration 165/1000 | Loss: 0.00001910
Iteration 166/1000 | Loss: 0.00001910
Iteration 167/1000 | Loss: 0.00001910
Iteration 168/1000 | Loss: 0.00001910
Iteration 169/1000 | Loss: 0.00001910
Iteration 170/1000 | Loss: 0.00001910
Iteration 171/1000 | Loss: 0.00001910
Iteration 172/1000 | Loss: 0.00001910
Iteration 173/1000 | Loss: 0.00001909
Iteration 174/1000 | Loss: 0.00001909
Iteration 175/1000 | Loss: 0.00001909
Iteration 176/1000 | Loss: 0.00001909
Iteration 177/1000 | Loss: 0.00001909
Iteration 178/1000 | Loss: 0.00001909
Iteration 179/1000 | Loss: 0.00001909
Iteration 180/1000 | Loss: 0.00001909
Iteration 181/1000 | Loss: 0.00001909
Iteration 182/1000 | Loss: 0.00001909
Iteration 183/1000 | Loss: 0.00001909
Iteration 184/1000 | Loss: 0.00001909
Iteration 185/1000 | Loss: 0.00001909
Iteration 186/1000 | Loss: 0.00001909
Iteration 187/1000 | Loss: 0.00001909
Iteration 188/1000 | Loss: 0.00001909
Iteration 189/1000 | Loss: 0.00001909
Iteration 190/1000 | Loss: 0.00001909
Iteration 191/1000 | Loss: 0.00001909
Iteration 192/1000 | Loss: 0.00001909
Iteration 193/1000 | Loss: 0.00001909
Iteration 194/1000 | Loss: 0.00001909
Iteration 195/1000 | Loss: 0.00001908
Iteration 196/1000 | Loss: 0.00001908
Iteration 197/1000 | Loss: 0.00001908
Iteration 198/1000 | Loss: 0.00001908
Iteration 199/1000 | Loss: 0.00001908
Iteration 200/1000 | Loss: 0.00001908
Iteration 201/1000 | Loss: 0.00001908
Iteration 202/1000 | Loss: 0.00001908
Iteration 203/1000 | Loss: 0.00001908
Iteration 204/1000 | Loss: 0.00001908
Iteration 205/1000 | Loss: 0.00001908
Iteration 206/1000 | Loss: 0.00001908
Iteration 207/1000 | Loss: 0.00001908
Iteration 208/1000 | Loss: 0.00001908
Iteration 209/1000 | Loss: 0.00001908
Iteration 210/1000 | Loss: 0.00001907
Iteration 211/1000 | Loss: 0.00001907
Iteration 212/1000 | Loss: 0.00001907
Iteration 213/1000 | Loss: 0.00001907
Iteration 214/1000 | Loss: 0.00001907
Iteration 215/1000 | Loss: 0.00001907
Iteration 216/1000 | Loss: 0.00001907
Iteration 217/1000 | Loss: 0.00001907
Iteration 218/1000 | Loss: 0.00001907
Iteration 219/1000 | Loss: 0.00001907
Iteration 220/1000 | Loss: 0.00001907
Iteration 221/1000 | Loss: 0.00001907
Iteration 222/1000 | Loss: 0.00001907
Iteration 223/1000 | Loss: 0.00001907
Iteration 224/1000 | Loss: 0.00001907
Iteration 225/1000 | Loss: 0.00001907
Iteration 226/1000 | Loss: 0.00001907
Iteration 227/1000 | Loss: 0.00001907
Iteration 228/1000 | Loss: 0.00001907
Iteration 229/1000 | Loss: 0.00001907
Iteration 230/1000 | Loss: 0.00001907
Iteration 231/1000 | Loss: 0.00001907
Iteration 232/1000 | Loss: 0.00001907
Iteration 233/1000 | Loss: 0.00001907
Iteration 234/1000 | Loss: 0.00001907
Iteration 235/1000 | Loss: 0.00001907
Iteration 236/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.906606303236913e-05, 1.906606303236913e-05, 1.906606303236913e-05, 1.906606303236913e-05, 1.906606303236913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.906606303236913e-05

Optimization complete. Final v2v error: 3.719841480255127 mm

Highest mean error: 5.066448211669922 mm for frame 69

Lowest mean error: 3.251905679702759 mm for frame 145

Saving results

Total time: 155.98046398162842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085714
Iteration 2/25 | Loss: 0.00306450
Iteration 3/25 | Loss: 0.00205295
Iteration 4/25 | Loss: 0.00179431
Iteration 5/25 | Loss: 0.00172114
Iteration 6/25 | Loss: 0.00165617
Iteration 7/25 | Loss: 0.00161531
Iteration 8/25 | Loss: 0.00147646
Iteration 9/25 | Loss: 0.00143193
Iteration 10/25 | Loss: 0.00139844
Iteration 11/25 | Loss: 0.00135562
Iteration 12/25 | Loss: 0.00132725
Iteration 13/25 | Loss: 0.00130282
Iteration 14/25 | Loss: 0.00127521
Iteration 15/25 | Loss: 0.00126377
Iteration 16/25 | Loss: 0.00125143
Iteration 17/25 | Loss: 0.00123625
Iteration 18/25 | Loss: 0.00122625
Iteration 19/25 | Loss: 0.00122964
Iteration 20/25 | Loss: 0.00122181
Iteration 21/25 | Loss: 0.00121161
Iteration 22/25 | Loss: 0.00120635
Iteration 23/25 | Loss: 0.00120104
Iteration 24/25 | Loss: 0.00120502
Iteration 25/25 | Loss: 0.00120667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27802908
Iteration 2/25 | Loss: 0.00548024
Iteration 3/25 | Loss: 0.00405186
Iteration 4/25 | Loss: 0.00405186
Iteration 5/25 | Loss: 0.00405186
Iteration 6/25 | Loss: 0.00405185
Iteration 7/25 | Loss: 0.00405185
Iteration 8/25 | Loss: 0.00405185
Iteration 9/25 | Loss: 0.00405185
Iteration 10/25 | Loss: 0.00405185
Iteration 11/25 | Loss: 0.00405185
Iteration 12/25 | Loss: 0.00405185
Iteration 13/25 | Loss: 0.00405185
Iteration 14/25 | Loss: 0.00405185
Iteration 15/25 | Loss: 0.00405185
Iteration 16/25 | Loss: 0.00405185
Iteration 17/25 | Loss: 0.00405185
Iteration 18/25 | Loss: 0.00405185
Iteration 19/25 | Loss: 0.00405185
Iteration 20/25 | Loss: 0.00405185
Iteration 21/25 | Loss: 0.00405185
Iteration 22/25 | Loss: 0.00405185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004051853436976671, 0.004051853436976671, 0.004051853436976671, 0.004051853436976671, 0.004051853436976671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004051853436976671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00405185
Iteration 2/1000 | Loss: 0.00248568
Iteration 3/1000 | Loss: 0.00100371
Iteration 4/1000 | Loss: 0.00101738
Iteration 5/1000 | Loss: 0.00060853
Iteration 6/1000 | Loss: 0.00031448
Iteration 7/1000 | Loss: 0.00083957
Iteration 8/1000 | Loss: 0.00074363
Iteration 9/1000 | Loss: 0.00030233
Iteration 10/1000 | Loss: 0.00024059
Iteration 11/1000 | Loss: 0.00028614
Iteration 12/1000 | Loss: 0.00016781
Iteration 13/1000 | Loss: 0.00017916
Iteration 14/1000 | Loss: 0.00026163
Iteration 15/1000 | Loss: 0.00041247
Iteration 16/1000 | Loss: 0.00042421
Iteration 17/1000 | Loss: 0.00051307
Iteration 18/1000 | Loss: 0.00014925
Iteration 19/1000 | Loss: 0.00017540
Iteration 20/1000 | Loss: 0.00014747
Iteration 21/1000 | Loss: 0.00012737
Iteration 22/1000 | Loss: 0.00019408
Iteration 23/1000 | Loss: 0.00012378
Iteration 24/1000 | Loss: 0.00012771
Iteration 25/1000 | Loss: 0.00025032
Iteration 26/1000 | Loss: 0.00047253
Iteration 27/1000 | Loss: 0.00445743
Iteration 28/1000 | Loss: 0.00141911
Iteration 29/1000 | Loss: 0.00288498
Iteration 30/1000 | Loss: 0.00117713
Iteration 31/1000 | Loss: 0.00135446
Iteration 32/1000 | Loss: 0.00205079
Iteration 33/1000 | Loss: 0.00032687
Iteration 34/1000 | Loss: 0.00042151
Iteration 35/1000 | Loss: 0.00128332
Iteration 36/1000 | Loss: 0.00078621
Iteration 37/1000 | Loss: 0.00037601
Iteration 38/1000 | Loss: 0.00015502
Iteration 39/1000 | Loss: 0.00030356
Iteration 40/1000 | Loss: 0.00032634
Iteration 41/1000 | Loss: 0.00018774
Iteration 42/1000 | Loss: 0.00037821
Iteration 43/1000 | Loss: 0.00056796
Iteration 44/1000 | Loss: 0.00052601
Iteration 45/1000 | Loss: 0.00032434
Iteration 46/1000 | Loss: 0.00023417
Iteration 47/1000 | Loss: 0.00009249
Iteration 48/1000 | Loss: 0.00010231
Iteration 49/1000 | Loss: 0.00044769
Iteration 50/1000 | Loss: 0.00031641
Iteration 51/1000 | Loss: 0.00044856
Iteration 52/1000 | Loss: 0.00019037
Iteration 53/1000 | Loss: 0.00011460
Iteration 54/1000 | Loss: 0.00008505
Iteration 55/1000 | Loss: 0.00026113
Iteration 56/1000 | Loss: 0.00012506
Iteration 57/1000 | Loss: 0.00061942
Iteration 58/1000 | Loss: 0.00036148
Iteration 59/1000 | Loss: 0.00021673
Iteration 60/1000 | Loss: 0.00055486
Iteration 61/1000 | Loss: 0.00064037
Iteration 62/1000 | Loss: 0.00018220
Iteration 63/1000 | Loss: 0.00007371
Iteration 64/1000 | Loss: 0.00007047
Iteration 65/1000 | Loss: 0.00009718
Iteration 66/1000 | Loss: 0.00019477
Iteration 67/1000 | Loss: 0.00006641
Iteration 68/1000 | Loss: 0.00007741
Iteration 69/1000 | Loss: 0.00007557
Iteration 70/1000 | Loss: 0.00008785
Iteration 71/1000 | Loss: 0.00005969
Iteration 72/1000 | Loss: 0.00005886
Iteration 73/1000 | Loss: 0.00008209
Iteration 74/1000 | Loss: 0.00006164
Iteration 75/1000 | Loss: 0.00005823
Iteration 76/1000 | Loss: 0.00007289
Iteration 77/1000 | Loss: 0.00127193
Iteration 78/1000 | Loss: 0.00205949
Iteration 79/1000 | Loss: 0.00136383
Iteration 80/1000 | Loss: 0.00048759
Iteration 81/1000 | Loss: 0.00056634
Iteration 82/1000 | Loss: 0.00009308
Iteration 83/1000 | Loss: 0.00006477
Iteration 84/1000 | Loss: 0.00026114
Iteration 85/1000 | Loss: 0.00041148
Iteration 86/1000 | Loss: 0.00007224
Iteration 87/1000 | Loss: 0.00032783
Iteration 88/1000 | Loss: 0.00018788
Iteration 89/1000 | Loss: 0.00006323
Iteration 90/1000 | Loss: 0.00005672
Iteration 91/1000 | Loss: 0.00043843
Iteration 92/1000 | Loss: 0.00017292
Iteration 93/1000 | Loss: 0.00017776
Iteration 94/1000 | Loss: 0.00007018
Iteration 95/1000 | Loss: 0.00006545
Iteration 96/1000 | Loss: 0.00007126
Iteration 97/1000 | Loss: 0.00086813
Iteration 98/1000 | Loss: 0.00013607
Iteration 99/1000 | Loss: 0.00023089
Iteration 100/1000 | Loss: 0.00015334
Iteration 101/1000 | Loss: 0.00005801
Iteration 102/1000 | Loss: 0.00015678
Iteration 103/1000 | Loss: 0.00009131
Iteration 104/1000 | Loss: 0.00013432
Iteration 105/1000 | Loss: 0.00009796
Iteration 106/1000 | Loss: 0.00015966
Iteration 107/1000 | Loss: 0.00010314
Iteration 108/1000 | Loss: 0.00005326
Iteration 109/1000 | Loss: 0.00004896
Iteration 110/1000 | Loss: 0.00004800
Iteration 111/1000 | Loss: 0.00042622
Iteration 112/1000 | Loss: 0.00017887
Iteration 113/1000 | Loss: 0.00010953
Iteration 114/1000 | Loss: 0.00014089
Iteration 115/1000 | Loss: 0.00011362
Iteration 116/1000 | Loss: 0.00024102
Iteration 117/1000 | Loss: 0.00010140
Iteration 118/1000 | Loss: 0.00005563
Iteration 119/1000 | Loss: 0.00006425
Iteration 120/1000 | Loss: 0.00005190
Iteration 121/1000 | Loss: 0.00004710
Iteration 122/1000 | Loss: 0.00004635
Iteration 123/1000 | Loss: 0.00004639
Iteration 124/1000 | Loss: 0.00006194
Iteration 125/1000 | Loss: 0.00004822
Iteration 126/1000 | Loss: 0.00005934
Iteration 127/1000 | Loss: 0.00006305
Iteration 128/1000 | Loss: 0.00004807
Iteration 129/1000 | Loss: 0.00004563
Iteration 130/1000 | Loss: 0.00004497
Iteration 131/1000 | Loss: 0.00004461
Iteration 132/1000 | Loss: 0.00004656
Iteration 133/1000 | Loss: 0.00007052
Iteration 134/1000 | Loss: 0.00005005
Iteration 135/1000 | Loss: 0.00004492
Iteration 136/1000 | Loss: 0.00004444
Iteration 137/1000 | Loss: 0.00004415
Iteration 138/1000 | Loss: 0.00004641
Iteration 139/1000 | Loss: 0.00004409
Iteration 140/1000 | Loss: 0.00004392
Iteration 141/1000 | Loss: 0.00004382
Iteration 142/1000 | Loss: 0.00004381
Iteration 143/1000 | Loss: 0.00042638
Iteration 144/1000 | Loss: 0.00051041
Iteration 145/1000 | Loss: 0.00031328
Iteration 146/1000 | Loss: 0.00004405
Iteration 147/1000 | Loss: 0.00042950
Iteration 148/1000 | Loss: 0.00036153
Iteration 149/1000 | Loss: 0.00004334
Iteration 150/1000 | Loss: 0.00004206
Iteration 151/1000 | Loss: 0.00005383
Iteration 152/1000 | Loss: 0.00004759
Iteration 153/1000 | Loss: 0.00004203
Iteration 154/1000 | Loss: 0.00004128
Iteration 155/1000 | Loss: 0.00004128
Iteration 156/1000 | Loss: 0.00004128
Iteration 157/1000 | Loss: 0.00004128
Iteration 158/1000 | Loss: 0.00004127
Iteration 159/1000 | Loss: 0.00004127
Iteration 160/1000 | Loss: 0.00004127
Iteration 161/1000 | Loss: 0.00004127
Iteration 162/1000 | Loss: 0.00004127
Iteration 163/1000 | Loss: 0.00004152
Iteration 164/1000 | Loss: 0.00004152
Iteration 165/1000 | Loss: 0.00004678
Iteration 166/1000 | Loss: 0.00004120
Iteration 167/1000 | Loss: 0.00004117
Iteration 168/1000 | Loss: 0.00004113
Iteration 169/1000 | Loss: 0.00004112
Iteration 170/1000 | Loss: 0.00004112
Iteration 171/1000 | Loss: 0.00004111
Iteration 172/1000 | Loss: 0.00004104
Iteration 173/1000 | Loss: 0.00004101
Iteration 174/1000 | Loss: 0.00004102
Iteration 175/1000 | Loss: 0.00004087
Iteration 176/1000 | Loss: 0.00004086
Iteration 177/1000 | Loss: 0.00004086
Iteration 178/1000 | Loss: 0.00004086
Iteration 179/1000 | Loss: 0.00004086
Iteration 180/1000 | Loss: 0.00004086
Iteration 181/1000 | Loss: 0.00004086
Iteration 182/1000 | Loss: 0.00004086
Iteration 183/1000 | Loss: 0.00004086
Iteration 184/1000 | Loss: 0.00004086
Iteration 185/1000 | Loss: 0.00004086
Iteration 186/1000 | Loss: 0.00004085
Iteration 187/1000 | Loss: 0.00004085
Iteration 188/1000 | Loss: 0.00005794
Iteration 189/1000 | Loss: 0.00004629
Iteration 190/1000 | Loss: 0.00004995
Iteration 191/1000 | Loss: 0.00014719
Iteration 192/1000 | Loss: 0.00033587
Iteration 193/1000 | Loss: 0.00005797
Iteration 194/1000 | Loss: 0.00005048
Iteration 195/1000 | Loss: 0.00004918
Iteration 196/1000 | Loss: 0.00005097
Iteration 197/1000 | Loss: 0.00004130
Iteration 198/1000 | Loss: 0.00054020
Iteration 199/1000 | Loss: 0.00027731
Iteration 200/1000 | Loss: 0.00003958
Iteration 201/1000 | Loss: 0.00004818
Iteration 202/1000 | Loss: 0.00004214
Iteration 203/1000 | Loss: 0.00005456
Iteration 204/1000 | Loss: 0.00004844
Iteration 205/1000 | Loss: 0.00004406
Iteration 206/1000 | Loss: 0.00004020
Iteration 207/1000 | Loss: 0.00004063
Iteration 208/1000 | Loss: 0.00004137
Iteration 209/1000 | Loss: 0.00003851
Iteration 210/1000 | Loss: 0.00003850
Iteration 211/1000 | Loss: 0.00003849
Iteration 212/1000 | Loss: 0.00003842
Iteration 213/1000 | Loss: 0.00003903
Iteration 214/1000 | Loss: 0.00003857
Iteration 215/1000 | Loss: 0.00003826
Iteration 216/1000 | Loss: 0.00003823
Iteration 217/1000 | Loss: 0.00003823
Iteration 218/1000 | Loss: 0.00003822
Iteration 219/1000 | Loss: 0.00004135
Iteration 220/1000 | Loss: 0.00003823
Iteration 221/1000 | Loss: 0.00003817
Iteration 222/1000 | Loss: 0.00003817
Iteration 223/1000 | Loss: 0.00003816
Iteration 224/1000 | Loss: 0.00003816
Iteration 225/1000 | Loss: 0.00003816
Iteration 226/1000 | Loss: 0.00003816
Iteration 227/1000 | Loss: 0.00003816
Iteration 228/1000 | Loss: 0.00003816
Iteration 229/1000 | Loss: 0.00003816
Iteration 230/1000 | Loss: 0.00003816
Iteration 231/1000 | Loss: 0.00003816
Iteration 232/1000 | Loss: 0.00003816
Iteration 233/1000 | Loss: 0.00003816
Iteration 234/1000 | Loss: 0.00003816
Iteration 235/1000 | Loss: 0.00003815
Iteration 236/1000 | Loss: 0.00003815
Iteration 237/1000 | Loss: 0.00003815
Iteration 238/1000 | Loss: 0.00003815
Iteration 239/1000 | Loss: 0.00003815
Iteration 240/1000 | Loss: 0.00003815
Iteration 241/1000 | Loss: 0.00003815
Iteration 242/1000 | Loss: 0.00003815
Iteration 243/1000 | Loss: 0.00003815
Iteration 244/1000 | Loss: 0.00003815
Iteration 245/1000 | Loss: 0.00003814
Iteration 246/1000 | Loss: 0.00003814
Iteration 247/1000 | Loss: 0.00003814
Iteration 248/1000 | Loss: 0.00003814
Iteration 249/1000 | Loss: 0.00003814
Iteration 250/1000 | Loss: 0.00003814
Iteration 251/1000 | Loss: 0.00003814
Iteration 252/1000 | Loss: 0.00003814
Iteration 253/1000 | Loss: 0.00003814
Iteration 254/1000 | Loss: 0.00003814
Iteration 255/1000 | Loss: 0.00003814
Iteration 256/1000 | Loss: 0.00003814
Iteration 257/1000 | Loss: 0.00003814
Iteration 258/1000 | Loss: 0.00003814
Iteration 259/1000 | Loss: 0.00003814
Iteration 260/1000 | Loss: 0.00003813
Iteration 261/1000 | Loss: 0.00003813
Iteration 262/1000 | Loss: 0.00003813
Iteration 263/1000 | Loss: 0.00003813
Iteration 264/1000 | Loss: 0.00003813
Iteration 265/1000 | Loss: 0.00003813
Iteration 266/1000 | Loss: 0.00003813
Iteration 267/1000 | Loss: 0.00003813
Iteration 268/1000 | Loss: 0.00003813
Iteration 269/1000 | Loss: 0.00003813
Iteration 270/1000 | Loss: 0.00003813
Iteration 271/1000 | Loss: 0.00003813
Iteration 272/1000 | Loss: 0.00003813
Iteration 273/1000 | Loss: 0.00003813
Iteration 274/1000 | Loss: 0.00003812
Iteration 275/1000 | Loss: 0.00003812
Iteration 276/1000 | Loss: 0.00003812
Iteration 277/1000 | Loss: 0.00003812
Iteration 278/1000 | Loss: 0.00003812
Iteration 279/1000 | Loss: 0.00003812
Iteration 280/1000 | Loss: 0.00003812
Iteration 281/1000 | Loss: 0.00003812
Iteration 282/1000 | Loss: 0.00003812
Iteration 283/1000 | Loss: 0.00003812
Iteration 284/1000 | Loss: 0.00003812
Iteration 285/1000 | Loss: 0.00003812
Iteration 286/1000 | Loss: 0.00003812
Iteration 287/1000 | Loss: 0.00003894
Iteration 288/1000 | Loss: 0.00003894
Iteration 289/1000 | Loss: 0.00003812
Iteration 290/1000 | Loss: 0.00003811
Iteration 291/1000 | Loss: 0.00003811
Iteration 292/1000 | Loss: 0.00003810
Iteration 293/1000 | Loss: 0.00003810
Iteration 294/1000 | Loss: 0.00003810
Iteration 295/1000 | Loss: 0.00003810
Iteration 296/1000 | Loss: 0.00003810
Iteration 297/1000 | Loss: 0.00003810
Iteration 298/1000 | Loss: 0.00003810
Iteration 299/1000 | Loss: 0.00003810
Iteration 300/1000 | Loss: 0.00003810
Iteration 301/1000 | Loss: 0.00003810
Iteration 302/1000 | Loss: 0.00003810
Iteration 303/1000 | Loss: 0.00003810
Iteration 304/1000 | Loss: 0.00003809
Iteration 305/1000 | Loss: 0.00003809
Iteration 306/1000 | Loss: 0.00003809
Iteration 307/1000 | Loss: 0.00003809
Iteration 308/1000 | Loss: 0.00003809
Iteration 309/1000 | Loss: 0.00003809
Iteration 310/1000 | Loss: 0.00003809
Iteration 311/1000 | Loss: 0.00003809
Iteration 312/1000 | Loss: 0.00003809
Iteration 313/1000 | Loss: 0.00003809
Iteration 314/1000 | Loss: 0.00003809
Iteration 315/1000 | Loss: 0.00003809
Iteration 316/1000 | Loss: 0.00003809
Iteration 317/1000 | Loss: 0.00003809
Iteration 318/1000 | Loss: 0.00003809
Iteration 319/1000 | Loss: 0.00003809
Iteration 320/1000 | Loss: 0.00003809
Iteration 321/1000 | Loss: 0.00003809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [3.808971814578399e-05, 3.808971814578399e-05, 3.808971814578399e-05, 3.808971814578399e-05, 3.808971814578399e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.808971814578399e-05

Optimization complete. Final v2v error: 4.1361236572265625 mm

Highest mean error: 13.559762954711914 mm for frame 172

Lowest mean error: 3.3075451850891113 mm for frame 29

Saving results

Total time: 357.4436767101288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419796
Iteration 2/25 | Loss: 0.00132965
Iteration 3/25 | Loss: 0.00100340
Iteration 4/25 | Loss: 0.00094343
Iteration 5/25 | Loss: 0.00093236
Iteration 6/25 | Loss: 0.00092853
Iteration 7/25 | Loss: 0.00092740
Iteration 8/25 | Loss: 0.00092740
Iteration 9/25 | Loss: 0.00092740
Iteration 10/25 | Loss: 0.00092740
Iteration 11/25 | Loss: 0.00092740
Iteration 12/25 | Loss: 0.00092740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009273967589251697, 0.0009273967589251697, 0.0009273967589251697, 0.0009273967589251697, 0.0009273967589251697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009273967589251697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26104856
Iteration 2/25 | Loss: 0.00099473
Iteration 3/25 | Loss: 0.00099472
Iteration 4/25 | Loss: 0.00099472
Iteration 5/25 | Loss: 0.00099472
Iteration 6/25 | Loss: 0.00099472
Iteration 7/25 | Loss: 0.00099472
Iteration 8/25 | Loss: 0.00099472
Iteration 9/25 | Loss: 0.00099472
Iteration 10/25 | Loss: 0.00099472
Iteration 11/25 | Loss: 0.00099472
Iteration 12/25 | Loss: 0.00099472
Iteration 13/25 | Loss: 0.00099472
Iteration 14/25 | Loss: 0.00099472
Iteration 15/25 | Loss: 0.00099472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009947230573743582, 0.0009947230573743582, 0.0009947230573743582, 0.0009947230573743582, 0.0009947230573743582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009947230573743582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099472
Iteration 2/1000 | Loss: 0.00003705
Iteration 3/1000 | Loss: 0.00002374
Iteration 4/1000 | Loss: 0.00001940
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001640
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001587
Iteration 12/1000 | Loss: 0.00001584
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001569
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001553
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001546
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001536
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001534
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001532
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001529
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001527
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001527
Iteration 58/1000 | Loss: 0.00001527
Iteration 59/1000 | Loss: 0.00001527
Iteration 60/1000 | Loss: 0.00001527
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001526
Iteration 64/1000 | Loss: 0.00001526
Iteration 65/1000 | Loss: 0.00001526
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001522
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001520
Iteration 112/1000 | Loss: 0.00001520
Iteration 113/1000 | Loss: 0.00001520
Iteration 114/1000 | Loss: 0.00001520
Iteration 115/1000 | Loss: 0.00001520
Iteration 116/1000 | Loss: 0.00001520
Iteration 117/1000 | Loss: 0.00001520
Iteration 118/1000 | Loss: 0.00001520
Iteration 119/1000 | Loss: 0.00001520
Iteration 120/1000 | Loss: 0.00001520
Iteration 121/1000 | Loss: 0.00001520
Iteration 122/1000 | Loss: 0.00001520
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001520
Iteration 126/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.519654324511066e-05, 1.519654324511066e-05, 1.519654324511066e-05, 1.519654324511066e-05, 1.519654324511066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.519654324511066e-05

Optimization complete. Final v2v error: 3.4061121940612793 mm

Highest mean error: 4.127060890197754 mm for frame 77

Lowest mean error: 3.128697156906128 mm for frame 15

Saving results

Total time: 38.16016244888306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00933050
Iteration 2/25 | Loss: 0.00144830
Iteration 3/25 | Loss: 0.00112144
Iteration 4/25 | Loss: 0.00106613
Iteration 5/25 | Loss: 0.00105325
Iteration 6/25 | Loss: 0.00105109
Iteration 7/25 | Loss: 0.00104805
Iteration 8/25 | Loss: 0.00104620
Iteration 9/25 | Loss: 0.00104549
Iteration 10/25 | Loss: 0.00104525
Iteration 11/25 | Loss: 0.00104525
Iteration 12/25 | Loss: 0.00104525
Iteration 13/25 | Loss: 0.00104525
Iteration 14/25 | Loss: 0.00104525
Iteration 15/25 | Loss: 0.00104525
Iteration 16/25 | Loss: 0.00104525
Iteration 17/25 | Loss: 0.00104525
Iteration 18/25 | Loss: 0.00104525
Iteration 19/25 | Loss: 0.00104525
Iteration 20/25 | Loss: 0.00104525
Iteration 21/25 | Loss: 0.00104525
Iteration 22/25 | Loss: 0.00104525
Iteration 23/25 | Loss: 0.00104525
Iteration 24/25 | Loss: 0.00104525
Iteration 25/25 | Loss: 0.00104524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84879273
Iteration 2/25 | Loss: 0.00077079
Iteration 3/25 | Loss: 0.00077079
Iteration 4/25 | Loss: 0.00077079
Iteration 5/25 | Loss: 0.00077079
Iteration 6/25 | Loss: 0.00077079
Iteration 7/25 | Loss: 0.00077079
Iteration 8/25 | Loss: 0.00077079
Iteration 9/25 | Loss: 0.00077078
Iteration 10/25 | Loss: 0.00077078
Iteration 11/25 | Loss: 0.00077078
Iteration 12/25 | Loss: 0.00077078
Iteration 13/25 | Loss: 0.00077078
Iteration 14/25 | Loss: 0.00077078
Iteration 15/25 | Loss: 0.00077078
Iteration 16/25 | Loss: 0.00077078
Iteration 17/25 | Loss: 0.00077078
Iteration 18/25 | Loss: 0.00077078
Iteration 19/25 | Loss: 0.00077078
Iteration 20/25 | Loss: 0.00077078
Iteration 21/25 | Loss: 0.00077078
Iteration 22/25 | Loss: 0.00077078
Iteration 23/25 | Loss: 0.00077078
Iteration 24/25 | Loss: 0.00077078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007707840413786471, 0.0007707840413786471, 0.0007707840413786471, 0.0007707840413786471, 0.0007707840413786471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007707840413786471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077078
Iteration 2/1000 | Loss: 0.00006410
Iteration 3/1000 | Loss: 0.00005020
Iteration 4/1000 | Loss: 0.00004548
Iteration 5/1000 | Loss: 0.00004255
Iteration 6/1000 | Loss: 0.00004145
Iteration 7/1000 | Loss: 0.00004022
Iteration 8/1000 | Loss: 0.00003943
Iteration 9/1000 | Loss: 0.00003894
Iteration 10/1000 | Loss: 0.00003853
Iteration 11/1000 | Loss: 0.00003827
Iteration 12/1000 | Loss: 0.00003814
Iteration 13/1000 | Loss: 0.00003812
Iteration 14/1000 | Loss: 0.00003812
Iteration 15/1000 | Loss: 0.00003806
Iteration 16/1000 | Loss: 0.00003805
Iteration 17/1000 | Loss: 0.00003801
Iteration 18/1000 | Loss: 0.00003799
Iteration 19/1000 | Loss: 0.00003799
Iteration 20/1000 | Loss: 0.00003799
Iteration 21/1000 | Loss: 0.00003799
Iteration 22/1000 | Loss: 0.00003799
Iteration 23/1000 | Loss: 0.00003799
Iteration 24/1000 | Loss: 0.00003799
Iteration 25/1000 | Loss: 0.00003799
Iteration 26/1000 | Loss: 0.00003799
Iteration 27/1000 | Loss: 0.00003799
Iteration 28/1000 | Loss: 0.00003799
Iteration 29/1000 | Loss: 0.00003798
Iteration 30/1000 | Loss: 0.00003798
Iteration 31/1000 | Loss: 0.00003798
Iteration 32/1000 | Loss: 0.00003798
Iteration 33/1000 | Loss: 0.00003798
Iteration 34/1000 | Loss: 0.00003798
Iteration 35/1000 | Loss: 0.00003797
Iteration 36/1000 | Loss: 0.00003796
Iteration 37/1000 | Loss: 0.00003796
Iteration 38/1000 | Loss: 0.00003795
Iteration 39/1000 | Loss: 0.00003793
Iteration 40/1000 | Loss: 0.00003793
Iteration 41/1000 | Loss: 0.00003793
Iteration 42/1000 | Loss: 0.00003793
Iteration 43/1000 | Loss: 0.00003793
Iteration 44/1000 | Loss: 0.00003793
Iteration 45/1000 | Loss: 0.00003793
Iteration 46/1000 | Loss: 0.00003793
Iteration 47/1000 | Loss: 0.00003793
Iteration 48/1000 | Loss: 0.00003793
Iteration 49/1000 | Loss: 0.00003793
Iteration 50/1000 | Loss: 0.00003793
Iteration 51/1000 | Loss: 0.00003792
Iteration 52/1000 | Loss: 0.00003792
Iteration 53/1000 | Loss: 0.00003791
Iteration 54/1000 | Loss: 0.00003791
Iteration 55/1000 | Loss: 0.00003791
Iteration 56/1000 | Loss: 0.00003791
Iteration 57/1000 | Loss: 0.00003791
Iteration 58/1000 | Loss: 0.00003791
Iteration 59/1000 | Loss: 0.00003791
Iteration 60/1000 | Loss: 0.00003790
Iteration 61/1000 | Loss: 0.00003790
Iteration 62/1000 | Loss: 0.00003790
Iteration 63/1000 | Loss: 0.00003790
Iteration 64/1000 | Loss: 0.00003790
Iteration 65/1000 | Loss: 0.00003790
Iteration 66/1000 | Loss: 0.00003790
Iteration 67/1000 | Loss: 0.00003790
Iteration 68/1000 | Loss: 0.00003790
Iteration 69/1000 | Loss: 0.00003790
Iteration 70/1000 | Loss: 0.00003789
Iteration 71/1000 | Loss: 0.00003789
Iteration 72/1000 | Loss: 0.00003789
Iteration 73/1000 | Loss: 0.00003789
Iteration 74/1000 | Loss: 0.00003789
Iteration 75/1000 | Loss: 0.00003789
Iteration 76/1000 | Loss: 0.00003789
Iteration 77/1000 | Loss: 0.00003789
Iteration 78/1000 | Loss: 0.00003789
Iteration 79/1000 | Loss: 0.00003789
Iteration 80/1000 | Loss: 0.00003789
Iteration 81/1000 | Loss: 0.00003789
Iteration 82/1000 | Loss: 0.00003789
Iteration 83/1000 | Loss: 0.00003789
Iteration 84/1000 | Loss: 0.00003789
Iteration 85/1000 | Loss: 0.00003789
Iteration 86/1000 | Loss: 0.00003788
Iteration 87/1000 | Loss: 0.00003788
Iteration 88/1000 | Loss: 0.00003788
Iteration 89/1000 | Loss: 0.00003788
Iteration 90/1000 | Loss: 0.00003788
Iteration 91/1000 | Loss: 0.00003788
Iteration 92/1000 | Loss: 0.00003788
Iteration 93/1000 | Loss: 0.00003788
Iteration 94/1000 | Loss: 0.00003788
Iteration 95/1000 | Loss: 0.00003788
Iteration 96/1000 | Loss: 0.00003788
Iteration 97/1000 | Loss: 0.00003788
Iteration 98/1000 | Loss: 0.00003788
Iteration 99/1000 | Loss: 0.00003787
Iteration 100/1000 | Loss: 0.00003787
Iteration 101/1000 | Loss: 0.00003787
Iteration 102/1000 | Loss: 0.00003787
Iteration 103/1000 | Loss: 0.00003787
Iteration 104/1000 | Loss: 0.00003787
Iteration 105/1000 | Loss: 0.00003787
Iteration 106/1000 | Loss: 0.00003787
Iteration 107/1000 | Loss: 0.00003787
Iteration 108/1000 | Loss: 0.00003787
Iteration 109/1000 | Loss: 0.00003787
Iteration 110/1000 | Loss: 0.00003787
Iteration 111/1000 | Loss: 0.00003786
Iteration 112/1000 | Loss: 0.00003786
Iteration 113/1000 | Loss: 0.00003786
Iteration 114/1000 | Loss: 0.00003786
Iteration 115/1000 | Loss: 0.00003786
Iteration 116/1000 | Loss: 0.00003786
Iteration 117/1000 | Loss: 0.00003786
Iteration 118/1000 | Loss: 0.00003786
Iteration 119/1000 | Loss: 0.00003786
Iteration 120/1000 | Loss: 0.00003786
Iteration 121/1000 | Loss: 0.00003786
Iteration 122/1000 | Loss: 0.00003786
Iteration 123/1000 | Loss: 0.00003786
Iteration 124/1000 | Loss: 0.00003786
Iteration 125/1000 | Loss: 0.00003786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.78571858163923e-05, 3.78571858163923e-05, 3.78571858163923e-05, 3.78571858163923e-05, 3.78571858163923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.78571858163923e-05

Optimization complete. Final v2v error: 5.175361633300781 mm

Highest mean error: 5.249733924865723 mm for frame 21

Lowest mean error: 5.058438777923584 mm for frame 89

Saving results

Total time: 42.37996697425842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062855
Iteration 2/25 | Loss: 0.00174512
Iteration 3/25 | Loss: 0.00124880
Iteration 4/25 | Loss: 0.00117314
Iteration 5/25 | Loss: 0.00107290
Iteration 6/25 | Loss: 0.00104164
Iteration 7/25 | Loss: 0.00103466
Iteration 8/25 | Loss: 0.00101566
Iteration 9/25 | Loss: 0.00100909
Iteration 10/25 | Loss: 0.00100870
Iteration 11/25 | Loss: 0.00100336
Iteration 12/25 | Loss: 0.00099572
Iteration 13/25 | Loss: 0.00099475
Iteration 14/25 | Loss: 0.00099510
Iteration 15/25 | Loss: 0.00100243
Iteration 16/25 | Loss: 0.00100433
Iteration 17/25 | Loss: 0.00099297
Iteration 18/25 | Loss: 0.00098499
Iteration 19/25 | Loss: 0.00098283
Iteration 20/25 | Loss: 0.00098213
Iteration 21/25 | Loss: 0.00098157
Iteration 22/25 | Loss: 0.00098472
Iteration 23/25 | Loss: 0.00098342
Iteration 24/25 | Loss: 0.00098430
Iteration 25/25 | Loss: 0.00098327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18994641
Iteration 2/25 | Loss: 0.00141260
Iteration 3/25 | Loss: 0.00141259
Iteration 4/25 | Loss: 0.00102025
Iteration 5/25 | Loss: 0.00102025
Iteration 6/25 | Loss: 0.00102025
Iteration 7/25 | Loss: 0.00102025
Iteration 8/25 | Loss: 0.00102025
Iteration 9/25 | Loss: 0.00102025
Iteration 10/25 | Loss: 0.00102025
Iteration 11/25 | Loss: 0.00102025
Iteration 12/25 | Loss: 0.00102025
Iteration 13/25 | Loss: 0.00102025
Iteration 14/25 | Loss: 0.00102025
Iteration 15/25 | Loss: 0.00102025
Iteration 16/25 | Loss: 0.00102025
Iteration 17/25 | Loss: 0.00102025
Iteration 18/25 | Loss: 0.00102025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010202479315921664, 0.0010202479315921664, 0.0010202479315921664, 0.0010202479315921664, 0.0010202479315921664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010202479315921664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102025
Iteration 2/1000 | Loss: 0.00044117
Iteration 3/1000 | Loss: 0.00030149
Iteration 4/1000 | Loss: 0.00005895
Iteration 5/1000 | Loss: 0.00004437
Iteration 6/1000 | Loss: 0.00035410
Iteration 7/1000 | Loss: 0.00020958
Iteration 8/1000 | Loss: 0.00031822
Iteration 9/1000 | Loss: 0.00003053
Iteration 10/1000 | Loss: 0.00002744
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002396
Iteration 13/1000 | Loss: 0.00018860
Iteration 14/1000 | Loss: 0.00002539
Iteration 15/1000 | Loss: 0.00010770
Iteration 16/1000 | Loss: 0.00002558
Iteration 17/1000 | Loss: 0.00048831
Iteration 18/1000 | Loss: 0.00003000
Iteration 19/1000 | Loss: 0.00040647
Iteration 20/1000 | Loss: 0.00031028
Iteration 21/1000 | Loss: 0.00010680
Iteration 22/1000 | Loss: 0.00008273
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00002214
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002174
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002154
Iteration 29/1000 | Loss: 0.00002151
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002124
Iteration 32/1000 | Loss: 0.00002119
Iteration 33/1000 | Loss: 0.00002119
Iteration 34/1000 | Loss: 0.00002119
Iteration 35/1000 | Loss: 0.00002119
Iteration 36/1000 | Loss: 0.00002119
Iteration 37/1000 | Loss: 0.00002119
Iteration 38/1000 | Loss: 0.00002119
Iteration 39/1000 | Loss: 0.00002119
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002119
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002118
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002116
Iteration 49/1000 | Loss: 0.00002116
Iteration 50/1000 | Loss: 0.00002116
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002115
Iteration 53/1000 | Loss: 0.00002113
Iteration 54/1000 | Loss: 0.00002113
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002112
Iteration 57/1000 | Loss: 0.00002112
Iteration 58/1000 | Loss: 0.00002112
Iteration 59/1000 | Loss: 0.00002111
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002110
Iteration 62/1000 | Loss: 0.00002110
Iteration 63/1000 | Loss: 0.00002110
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002109
Iteration 67/1000 | Loss: 0.00002109
Iteration 68/1000 | Loss: 0.00002109
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002106
Iteration 79/1000 | Loss: 0.00002106
Iteration 80/1000 | Loss: 0.00002106
Iteration 81/1000 | Loss: 0.00002106
Iteration 82/1000 | Loss: 0.00002105
Iteration 83/1000 | Loss: 0.00002105
Iteration 84/1000 | Loss: 0.00002105
Iteration 85/1000 | Loss: 0.00002105
Iteration 86/1000 | Loss: 0.00002105
Iteration 87/1000 | Loss: 0.00002104
Iteration 88/1000 | Loss: 0.00002104
Iteration 89/1000 | Loss: 0.00002104
Iteration 90/1000 | Loss: 0.00002104
Iteration 91/1000 | Loss: 0.00002104
Iteration 92/1000 | Loss: 0.00002104
Iteration 93/1000 | Loss: 0.00002104
Iteration 94/1000 | Loss: 0.00002104
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002104
Iteration 97/1000 | Loss: 0.00002104
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002103
Iteration 100/1000 | Loss: 0.00002103
Iteration 101/1000 | Loss: 0.00002103
Iteration 102/1000 | Loss: 0.00002103
Iteration 103/1000 | Loss: 0.00002103
Iteration 104/1000 | Loss: 0.00002103
Iteration 105/1000 | Loss: 0.00002103
Iteration 106/1000 | Loss: 0.00002102
Iteration 107/1000 | Loss: 0.00002102
Iteration 108/1000 | Loss: 0.00002101
Iteration 109/1000 | Loss: 0.00002101
Iteration 110/1000 | Loss: 0.00002101
Iteration 111/1000 | Loss: 0.00002101
Iteration 112/1000 | Loss: 0.00002101
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002099
Iteration 128/1000 | Loss: 0.00002099
Iteration 129/1000 | Loss: 0.00002099
Iteration 130/1000 | Loss: 0.00002099
Iteration 131/1000 | Loss: 0.00002099
Iteration 132/1000 | Loss: 0.00002099
Iteration 133/1000 | Loss: 0.00002099
Iteration 134/1000 | Loss: 0.00002099
Iteration 135/1000 | Loss: 0.00002099
Iteration 136/1000 | Loss: 0.00002098
Iteration 137/1000 | Loss: 0.00002098
Iteration 138/1000 | Loss: 0.00002098
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002098
Iteration 141/1000 | Loss: 0.00002097
Iteration 142/1000 | Loss: 0.00002097
Iteration 143/1000 | Loss: 0.00002097
Iteration 144/1000 | Loss: 0.00002097
Iteration 145/1000 | Loss: 0.00002097
Iteration 146/1000 | Loss: 0.00002097
Iteration 147/1000 | Loss: 0.00002096
Iteration 148/1000 | Loss: 0.00002096
Iteration 149/1000 | Loss: 0.00002096
Iteration 150/1000 | Loss: 0.00002096
Iteration 151/1000 | Loss: 0.00002095
Iteration 152/1000 | Loss: 0.00002095
Iteration 153/1000 | Loss: 0.00002095
Iteration 154/1000 | Loss: 0.00002095
Iteration 155/1000 | Loss: 0.00002095
Iteration 156/1000 | Loss: 0.00002095
Iteration 157/1000 | Loss: 0.00002095
Iteration 158/1000 | Loss: 0.00002094
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002094
Iteration 165/1000 | Loss: 0.00002094
Iteration 166/1000 | Loss: 0.00002094
Iteration 167/1000 | Loss: 0.00002094
Iteration 168/1000 | Loss: 0.00002094
Iteration 169/1000 | Loss: 0.00002094
Iteration 170/1000 | Loss: 0.00002093
Iteration 171/1000 | Loss: 0.00002093
Iteration 172/1000 | Loss: 0.00002093
Iteration 173/1000 | Loss: 0.00002093
Iteration 174/1000 | Loss: 0.00002093
Iteration 175/1000 | Loss: 0.00002093
Iteration 176/1000 | Loss: 0.00002093
Iteration 177/1000 | Loss: 0.00002093
Iteration 178/1000 | Loss: 0.00002093
Iteration 179/1000 | Loss: 0.00002093
Iteration 180/1000 | Loss: 0.00002093
Iteration 181/1000 | Loss: 0.00002092
Iteration 182/1000 | Loss: 0.00002092
Iteration 183/1000 | Loss: 0.00002092
Iteration 184/1000 | Loss: 0.00002092
Iteration 185/1000 | Loss: 0.00002092
Iteration 186/1000 | Loss: 0.00002092
Iteration 187/1000 | Loss: 0.00002092
Iteration 188/1000 | Loss: 0.00002092
Iteration 189/1000 | Loss: 0.00002092
Iteration 190/1000 | Loss: 0.00002092
Iteration 191/1000 | Loss: 0.00002092
Iteration 192/1000 | Loss: 0.00002092
Iteration 193/1000 | Loss: 0.00002092
Iteration 194/1000 | Loss: 0.00002091
Iteration 195/1000 | Loss: 0.00002091
Iteration 196/1000 | Loss: 0.00002091
Iteration 197/1000 | Loss: 0.00002091
Iteration 198/1000 | Loss: 0.00002091
Iteration 199/1000 | Loss: 0.00002091
Iteration 200/1000 | Loss: 0.00002091
Iteration 201/1000 | Loss: 0.00002091
Iteration 202/1000 | Loss: 0.00002091
Iteration 203/1000 | Loss: 0.00002091
Iteration 204/1000 | Loss: 0.00002091
Iteration 205/1000 | Loss: 0.00002091
Iteration 206/1000 | Loss: 0.00002091
Iteration 207/1000 | Loss: 0.00002091
Iteration 208/1000 | Loss: 0.00002091
Iteration 209/1000 | Loss: 0.00002091
Iteration 210/1000 | Loss: 0.00002091
Iteration 211/1000 | Loss: 0.00002091
Iteration 212/1000 | Loss: 0.00002091
Iteration 213/1000 | Loss: 0.00002091
Iteration 214/1000 | Loss: 0.00002091
Iteration 215/1000 | Loss: 0.00002091
Iteration 216/1000 | Loss: 0.00002091
Iteration 217/1000 | Loss: 0.00002091
Iteration 218/1000 | Loss: 0.00002091
Iteration 219/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.090608177240938e-05, 2.090608177240938e-05, 2.090608177240938e-05, 2.090608177240938e-05, 2.090608177240938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.090608177240938e-05

Optimization complete. Final v2v error: 3.987077236175537 mm

Highest mean error: 10.705705642700195 mm for frame 154

Lowest mean error: 3.4167449474334717 mm for frame 239

Saving results

Total time: 114.54004001617432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846615
Iteration 2/25 | Loss: 0.00127136
Iteration 3/25 | Loss: 0.00097566
Iteration 4/25 | Loss: 0.00095755
Iteration 5/25 | Loss: 0.00095416
Iteration 6/25 | Loss: 0.00095340
Iteration 7/25 | Loss: 0.00095340
Iteration 8/25 | Loss: 0.00095340
Iteration 9/25 | Loss: 0.00095340
Iteration 10/25 | Loss: 0.00095340
Iteration 11/25 | Loss: 0.00095340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009534012642689049, 0.0009534012642689049, 0.0009534012642689049, 0.0009534012642689049, 0.0009534012642689049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009534012642689049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24333131
Iteration 2/25 | Loss: 0.00088795
Iteration 3/25 | Loss: 0.00088794
Iteration 4/25 | Loss: 0.00088794
Iteration 5/25 | Loss: 0.00088794
Iteration 6/25 | Loss: 0.00088794
Iteration 7/25 | Loss: 0.00088794
Iteration 8/25 | Loss: 0.00088794
Iteration 9/25 | Loss: 0.00088794
Iteration 10/25 | Loss: 0.00088794
Iteration 11/25 | Loss: 0.00088794
Iteration 12/25 | Loss: 0.00088794
Iteration 13/25 | Loss: 0.00088794
Iteration 14/25 | Loss: 0.00088794
Iteration 15/25 | Loss: 0.00088794
Iteration 16/25 | Loss: 0.00088794
Iteration 17/25 | Loss: 0.00088794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008879409288056195, 0.0008879409288056195, 0.0008879409288056195, 0.0008879409288056195, 0.0008879409288056195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008879409288056195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088794
Iteration 2/1000 | Loss: 0.00003765
Iteration 3/1000 | Loss: 0.00002944
Iteration 4/1000 | Loss: 0.00002547
Iteration 5/1000 | Loss: 0.00002325
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002113
Iteration 9/1000 | Loss: 0.00002078
Iteration 10/1000 | Loss: 0.00002045
Iteration 11/1000 | Loss: 0.00002040
Iteration 12/1000 | Loss: 0.00002034
Iteration 13/1000 | Loss: 0.00002024
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00002000
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001999
Iteration 19/1000 | Loss: 0.00001999
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001988
Iteration 22/1000 | Loss: 0.00001987
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001980
Iteration 26/1000 | Loss: 0.00001980
Iteration 27/1000 | Loss: 0.00001979
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001977
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001976
Iteration 34/1000 | Loss: 0.00001976
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001975
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001974
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001966
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001966
Iteration 67/1000 | Loss: 0.00001966
Iteration 68/1000 | Loss: 0.00001966
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001964
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001962
Iteration 103/1000 | Loss: 0.00001962
Iteration 104/1000 | Loss: 0.00001962
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001961
Iteration 112/1000 | Loss: 0.00001961
Iteration 113/1000 | Loss: 0.00001961
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001961
Iteration 118/1000 | Loss: 0.00001961
Iteration 119/1000 | Loss: 0.00001961
Iteration 120/1000 | Loss: 0.00001961
Iteration 121/1000 | Loss: 0.00001961
Iteration 122/1000 | Loss: 0.00001961
Iteration 123/1000 | Loss: 0.00001961
Iteration 124/1000 | Loss: 0.00001961
Iteration 125/1000 | Loss: 0.00001961
Iteration 126/1000 | Loss: 0.00001961
Iteration 127/1000 | Loss: 0.00001961
Iteration 128/1000 | Loss: 0.00001961
Iteration 129/1000 | Loss: 0.00001961
Iteration 130/1000 | Loss: 0.00001961
Iteration 131/1000 | Loss: 0.00001961
Iteration 132/1000 | Loss: 0.00001961
Iteration 133/1000 | Loss: 0.00001961
Iteration 134/1000 | Loss: 0.00001961
Iteration 135/1000 | Loss: 0.00001961
Iteration 136/1000 | Loss: 0.00001961
Iteration 137/1000 | Loss: 0.00001961
Iteration 138/1000 | Loss: 0.00001961
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001961
Iteration 141/1000 | Loss: 0.00001961
Iteration 142/1000 | Loss: 0.00001961
Iteration 143/1000 | Loss: 0.00001961
Iteration 144/1000 | Loss: 0.00001961
Iteration 145/1000 | Loss: 0.00001961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.961147427209653e-05, 1.961147427209653e-05, 1.961147427209653e-05, 1.961147427209653e-05, 1.961147427209653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.961147427209653e-05

Optimization complete. Final v2v error: 3.8463735580444336 mm

Highest mean error: 4.229597091674805 mm for frame 122

Lowest mean error: 3.2492380142211914 mm for frame 3

Saving results

Total time: 37.330249547958374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847348
Iteration 2/25 | Loss: 0.00109560
Iteration 3/25 | Loss: 0.00095353
Iteration 4/25 | Loss: 0.00093207
Iteration 5/25 | Loss: 0.00092705
Iteration 6/25 | Loss: 0.00092601
Iteration 7/25 | Loss: 0.00092601
Iteration 8/25 | Loss: 0.00092601
Iteration 9/25 | Loss: 0.00092601
Iteration 10/25 | Loss: 0.00092601
Iteration 11/25 | Loss: 0.00092601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009260103106498718, 0.0009260103106498718, 0.0009260103106498718, 0.0009260103106498718, 0.0009260103106498718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009260103106498718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25725436
Iteration 2/25 | Loss: 0.00097695
Iteration 3/25 | Loss: 0.00097695
Iteration 4/25 | Loss: 0.00097695
Iteration 5/25 | Loss: 0.00097695
Iteration 6/25 | Loss: 0.00097695
Iteration 7/25 | Loss: 0.00097695
Iteration 8/25 | Loss: 0.00097695
Iteration 9/25 | Loss: 0.00097695
Iteration 10/25 | Loss: 0.00097695
Iteration 11/25 | Loss: 0.00097695
Iteration 12/25 | Loss: 0.00097695
Iteration 13/25 | Loss: 0.00097695
Iteration 14/25 | Loss: 0.00097695
Iteration 15/25 | Loss: 0.00097695
Iteration 16/25 | Loss: 0.00097695
Iteration 17/25 | Loss: 0.00097695
Iteration 18/25 | Loss: 0.00097695
Iteration 19/25 | Loss: 0.00097695
Iteration 20/25 | Loss: 0.00097695
Iteration 21/25 | Loss: 0.00097695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009769491152837873, 0.0009769491152837873, 0.0009769491152837873, 0.0009769491152837873, 0.0009769491152837873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009769491152837873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097695
Iteration 2/1000 | Loss: 0.00003694
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00001773
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001515
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001510
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001509
Iteration 18/1000 | Loss: 0.00001508
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001493
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001490
Iteration 41/1000 | Loss: 0.00001489
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001488
Iteration 46/1000 | Loss: 0.00001488
Iteration 47/1000 | Loss: 0.00001487
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001487
Iteration 51/1000 | Loss: 0.00001486
Iteration 52/1000 | Loss: 0.00001486
Iteration 53/1000 | Loss: 0.00001486
Iteration 54/1000 | Loss: 0.00001486
Iteration 55/1000 | Loss: 0.00001486
Iteration 56/1000 | Loss: 0.00001486
Iteration 57/1000 | Loss: 0.00001486
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001484
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001483
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001482
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001481
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001480
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001479
Iteration 93/1000 | Loss: 0.00001479
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001479
Iteration 101/1000 | Loss: 0.00001479
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001478
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001477
Iteration 114/1000 | Loss: 0.00001477
Iteration 115/1000 | Loss: 0.00001477
Iteration 116/1000 | Loss: 0.00001477
Iteration 117/1000 | Loss: 0.00001477
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001477
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001477
Iteration 122/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4773811017221306e-05, 1.4773811017221306e-05, 1.4773811017221306e-05, 1.4773811017221306e-05, 1.4773811017221306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4773811017221306e-05

Optimization complete. Final v2v error: 3.337665319442749 mm

Highest mean error: 3.7640933990478516 mm for frame 118

Lowest mean error: 3.0019125938415527 mm for frame 205

Saving results

Total time: 32.92100286483765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072375
Iteration 2/25 | Loss: 0.00303052
Iteration 3/25 | Loss: 0.00247101
Iteration 4/25 | Loss: 0.00221294
Iteration 5/25 | Loss: 0.00207711
Iteration 6/25 | Loss: 0.00203350
Iteration 7/25 | Loss: 0.00193275
Iteration 8/25 | Loss: 0.00188994
Iteration 9/25 | Loss: 0.00176330
Iteration 10/25 | Loss: 0.00172952
Iteration 11/25 | Loss: 0.00170846
Iteration 12/25 | Loss: 0.00166478
Iteration 13/25 | Loss: 0.00165494
Iteration 14/25 | Loss: 0.00164912
Iteration 15/25 | Loss: 0.00164713
Iteration 16/25 | Loss: 0.00164645
Iteration 17/25 | Loss: 0.00164613
Iteration 18/25 | Loss: 0.00164594
Iteration 19/25 | Loss: 0.00164567
Iteration 20/25 | Loss: 0.00164542
Iteration 21/25 | Loss: 0.00164524
Iteration 22/25 | Loss: 0.00164513
Iteration 23/25 | Loss: 0.00164505
Iteration 24/25 | Loss: 0.00164504
Iteration 25/25 | Loss: 0.00164504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24683857
Iteration 2/25 | Loss: 0.00922627
Iteration 3/25 | Loss: 0.00922626
Iteration 4/25 | Loss: 0.00922626
Iteration 5/25 | Loss: 0.00922626
Iteration 6/25 | Loss: 0.00922626
Iteration 7/25 | Loss: 0.00922626
Iteration 8/25 | Loss: 0.00922626
Iteration 9/25 | Loss: 0.00922626
Iteration 10/25 | Loss: 0.00922626
Iteration 11/25 | Loss: 0.00922626
Iteration 12/25 | Loss: 0.00922626
Iteration 13/25 | Loss: 0.00922626
Iteration 14/25 | Loss: 0.00922626
Iteration 15/25 | Loss: 0.00922626
Iteration 16/25 | Loss: 0.00922626
Iteration 17/25 | Loss: 0.00922626
Iteration 18/25 | Loss: 0.00922626
Iteration 19/25 | Loss: 0.00922626
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.009226257912814617, 0.009226257912814617, 0.009226257912814617, 0.009226257912814617, 0.009226257912814617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009226257912814617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00922626
Iteration 2/1000 | Loss: 0.00324878
Iteration 3/1000 | Loss: 0.00785689
Iteration 4/1000 | Loss: 0.00518601
Iteration 5/1000 | Loss: 0.00719417
Iteration 6/1000 | Loss: 0.00228711
Iteration 7/1000 | Loss: 0.00129657
Iteration 8/1000 | Loss: 0.00098281
Iteration 9/1000 | Loss: 0.00068112
Iteration 10/1000 | Loss: 0.00059418
Iteration 11/1000 | Loss: 0.00050290
Iteration 12/1000 | Loss: 0.00045138
Iteration 13/1000 | Loss: 0.00041547
Iteration 14/1000 | Loss: 0.00038392
Iteration 15/1000 | Loss: 0.00035813
Iteration 16/1000 | Loss: 0.00034156
Iteration 17/1000 | Loss: 0.00032684
Iteration 18/1000 | Loss: 0.00436208
Iteration 19/1000 | Loss: 0.00565980
Iteration 20/1000 | Loss: 0.00491995
Iteration 21/1000 | Loss: 0.00279867
Iteration 22/1000 | Loss: 0.00218994
Iteration 23/1000 | Loss: 0.00171151
Iteration 24/1000 | Loss: 0.00059207
Iteration 25/1000 | Loss: 0.00036351
Iteration 26/1000 | Loss: 0.00028435
Iteration 27/1000 | Loss: 0.00168388
Iteration 28/1000 | Loss: 0.00202754
Iteration 29/1000 | Loss: 0.00198115
Iteration 30/1000 | Loss: 0.00233608
Iteration 31/1000 | Loss: 0.00094818
Iteration 32/1000 | Loss: 0.00082249
Iteration 33/1000 | Loss: 0.00044955
Iteration 34/1000 | Loss: 0.00109818
Iteration 35/1000 | Loss: 0.00076955
Iteration 36/1000 | Loss: 0.00078350
Iteration 37/1000 | Loss: 0.00023483
Iteration 38/1000 | Loss: 0.00020036
Iteration 39/1000 | Loss: 0.00140290
Iteration 40/1000 | Loss: 0.00038910
Iteration 41/1000 | Loss: 0.00016573
Iteration 42/1000 | Loss: 0.00101725
Iteration 43/1000 | Loss: 0.00020285
Iteration 44/1000 | Loss: 0.00045350
Iteration 45/1000 | Loss: 0.00044165
Iteration 46/1000 | Loss: 0.00013854
Iteration 47/1000 | Loss: 0.00012760
Iteration 48/1000 | Loss: 0.00063692
Iteration 49/1000 | Loss: 0.00040805
Iteration 50/1000 | Loss: 0.00011154
Iteration 51/1000 | Loss: 0.00105054
Iteration 52/1000 | Loss: 0.00126111
Iteration 53/1000 | Loss: 0.00013594
Iteration 54/1000 | Loss: 0.00009940
Iteration 55/1000 | Loss: 0.00008543
Iteration 56/1000 | Loss: 0.00007643
Iteration 57/1000 | Loss: 0.00105077
Iteration 58/1000 | Loss: 0.00010125
Iteration 59/1000 | Loss: 0.00007990
Iteration 60/1000 | Loss: 0.00039615
Iteration 61/1000 | Loss: 0.00006826
Iteration 62/1000 | Loss: 0.00037789
Iteration 63/1000 | Loss: 0.00040160
Iteration 64/1000 | Loss: 0.00013708
Iteration 65/1000 | Loss: 0.00007340
Iteration 66/1000 | Loss: 0.00006144
Iteration 67/1000 | Loss: 0.00005788
Iteration 68/1000 | Loss: 0.00037710
Iteration 69/1000 | Loss: 0.00051220
Iteration 70/1000 | Loss: 0.00072478
Iteration 71/1000 | Loss: 0.00034151
Iteration 72/1000 | Loss: 0.00006297
Iteration 73/1000 | Loss: 0.00005263
Iteration 74/1000 | Loss: 0.00036930
Iteration 75/1000 | Loss: 0.00089358
Iteration 76/1000 | Loss: 0.00014252
Iteration 77/1000 | Loss: 0.00017813
Iteration 78/1000 | Loss: 0.00013179
Iteration 79/1000 | Loss: 0.00018699
Iteration 80/1000 | Loss: 0.00024371
Iteration 81/1000 | Loss: 0.00016059
Iteration 82/1000 | Loss: 0.00017879
Iteration 83/1000 | Loss: 0.00005655
Iteration 84/1000 | Loss: 0.00004840
Iteration 85/1000 | Loss: 0.00004450
Iteration 86/1000 | Loss: 0.00004145
Iteration 87/1000 | Loss: 0.00003932
Iteration 88/1000 | Loss: 0.00003818
Iteration 89/1000 | Loss: 0.00003743
Iteration 90/1000 | Loss: 0.00003668
Iteration 91/1000 | Loss: 0.00003555
Iteration 92/1000 | Loss: 0.00003416
Iteration 93/1000 | Loss: 0.00003290
Iteration 94/1000 | Loss: 0.00003234
Iteration 95/1000 | Loss: 0.00003169
Iteration 96/1000 | Loss: 0.00003140
Iteration 97/1000 | Loss: 0.00003128
Iteration 98/1000 | Loss: 0.00003110
Iteration 99/1000 | Loss: 0.00003109
Iteration 100/1000 | Loss: 0.00003109
Iteration 101/1000 | Loss: 0.00003107
Iteration 102/1000 | Loss: 0.00003106
Iteration 103/1000 | Loss: 0.00003101
Iteration 104/1000 | Loss: 0.00003101
Iteration 105/1000 | Loss: 0.00003095
Iteration 106/1000 | Loss: 0.00003095
Iteration 107/1000 | Loss: 0.00003094
Iteration 108/1000 | Loss: 0.00003094
Iteration 109/1000 | Loss: 0.00003093
Iteration 110/1000 | Loss: 0.00003093
Iteration 111/1000 | Loss: 0.00003092
Iteration 112/1000 | Loss: 0.00003092
Iteration 113/1000 | Loss: 0.00003091
Iteration 114/1000 | Loss: 0.00003091
Iteration 115/1000 | Loss: 0.00003090
Iteration 116/1000 | Loss: 0.00003090
Iteration 117/1000 | Loss: 0.00003090
Iteration 118/1000 | Loss: 0.00003089
Iteration 119/1000 | Loss: 0.00003089
Iteration 120/1000 | Loss: 0.00003088
Iteration 121/1000 | Loss: 0.00003088
Iteration 122/1000 | Loss: 0.00003088
Iteration 123/1000 | Loss: 0.00003087
Iteration 124/1000 | Loss: 0.00003087
Iteration 125/1000 | Loss: 0.00003087
Iteration 126/1000 | Loss: 0.00003087
Iteration 127/1000 | Loss: 0.00003087
Iteration 128/1000 | Loss: 0.00003087
Iteration 129/1000 | Loss: 0.00003087
Iteration 130/1000 | Loss: 0.00003087
Iteration 131/1000 | Loss: 0.00003087
Iteration 132/1000 | Loss: 0.00003087
Iteration 133/1000 | Loss: 0.00003087
Iteration 134/1000 | Loss: 0.00003086
Iteration 135/1000 | Loss: 0.00003086
Iteration 136/1000 | Loss: 0.00003086
Iteration 137/1000 | Loss: 0.00003085
Iteration 138/1000 | Loss: 0.00003085
Iteration 139/1000 | Loss: 0.00003085
Iteration 140/1000 | Loss: 0.00003084
Iteration 141/1000 | Loss: 0.00003084
Iteration 142/1000 | Loss: 0.00003084
Iteration 143/1000 | Loss: 0.00003082
Iteration 144/1000 | Loss: 0.00003082
Iteration 145/1000 | Loss: 0.00003082
Iteration 146/1000 | Loss: 0.00003081
Iteration 147/1000 | Loss: 0.00003081
Iteration 148/1000 | Loss: 0.00003081
Iteration 149/1000 | Loss: 0.00003081
Iteration 150/1000 | Loss: 0.00003081
Iteration 151/1000 | Loss: 0.00003080
Iteration 152/1000 | Loss: 0.00003080
Iteration 153/1000 | Loss: 0.00003080
Iteration 154/1000 | Loss: 0.00003079
Iteration 155/1000 | Loss: 0.00003079
Iteration 156/1000 | Loss: 0.00003079
Iteration 157/1000 | Loss: 0.00003079
Iteration 158/1000 | Loss: 0.00003078
Iteration 159/1000 | Loss: 0.00003078
Iteration 160/1000 | Loss: 0.00003078
Iteration 161/1000 | Loss: 0.00003078
Iteration 162/1000 | Loss: 0.00003077
Iteration 163/1000 | Loss: 0.00003077
Iteration 164/1000 | Loss: 0.00003077
Iteration 165/1000 | Loss: 0.00003077
Iteration 166/1000 | Loss: 0.00003077
Iteration 167/1000 | Loss: 0.00003076
Iteration 168/1000 | Loss: 0.00003076
Iteration 169/1000 | Loss: 0.00003076
Iteration 170/1000 | Loss: 0.00003076
Iteration 171/1000 | Loss: 0.00003076
Iteration 172/1000 | Loss: 0.00003076
Iteration 173/1000 | Loss: 0.00003076
Iteration 174/1000 | Loss: 0.00003076
Iteration 175/1000 | Loss: 0.00003076
Iteration 176/1000 | Loss: 0.00003075
Iteration 177/1000 | Loss: 0.00003075
Iteration 178/1000 | Loss: 0.00003075
Iteration 179/1000 | Loss: 0.00003075
Iteration 180/1000 | Loss: 0.00003075
Iteration 181/1000 | Loss: 0.00003075
Iteration 182/1000 | Loss: 0.00003075
Iteration 183/1000 | Loss: 0.00003075
Iteration 184/1000 | Loss: 0.00003075
Iteration 185/1000 | Loss: 0.00003075
Iteration 186/1000 | Loss: 0.00003074
Iteration 187/1000 | Loss: 0.00003074
Iteration 188/1000 | Loss: 0.00003074
Iteration 189/1000 | Loss: 0.00003074
Iteration 190/1000 | Loss: 0.00003073
Iteration 191/1000 | Loss: 0.00003073
Iteration 192/1000 | Loss: 0.00003073
Iteration 193/1000 | Loss: 0.00003073
Iteration 194/1000 | Loss: 0.00003073
Iteration 195/1000 | Loss: 0.00003073
Iteration 196/1000 | Loss: 0.00003073
Iteration 197/1000 | Loss: 0.00003073
Iteration 198/1000 | Loss: 0.00003073
Iteration 199/1000 | Loss: 0.00003072
Iteration 200/1000 | Loss: 0.00003072
Iteration 201/1000 | Loss: 0.00003072
Iteration 202/1000 | Loss: 0.00003072
Iteration 203/1000 | Loss: 0.00003072
Iteration 204/1000 | Loss: 0.00003072
Iteration 205/1000 | Loss: 0.00003072
Iteration 206/1000 | Loss: 0.00003072
Iteration 207/1000 | Loss: 0.00003072
Iteration 208/1000 | Loss: 0.00003072
Iteration 209/1000 | Loss: 0.00003072
Iteration 210/1000 | Loss: 0.00003072
Iteration 211/1000 | Loss: 0.00003071
Iteration 212/1000 | Loss: 0.00003071
Iteration 213/1000 | Loss: 0.00003071
Iteration 214/1000 | Loss: 0.00003071
Iteration 215/1000 | Loss: 0.00003071
Iteration 216/1000 | Loss: 0.00003071
Iteration 217/1000 | Loss: 0.00003071
Iteration 218/1000 | Loss: 0.00003071
Iteration 219/1000 | Loss: 0.00003071
Iteration 220/1000 | Loss: 0.00003071
Iteration 221/1000 | Loss: 0.00003071
Iteration 222/1000 | Loss: 0.00003071
Iteration 223/1000 | Loss: 0.00003071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [3.0711547879036516e-05, 3.0711547879036516e-05, 3.0711547879036516e-05, 3.0711547879036516e-05, 3.0711547879036516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0711547879036516e-05

Optimization complete. Final v2v error: 4.28961706161499 mm

Highest mean error: 11.523384094238281 mm for frame 95

Lowest mean error: 3.490659236907959 mm for frame 0

Saving results

Total time: 191.51289653778076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467907
Iteration 2/25 | Loss: 0.00111140
Iteration 3/25 | Loss: 0.00102024
Iteration 4/25 | Loss: 0.00099270
Iteration 5/25 | Loss: 0.00098490
Iteration 6/25 | Loss: 0.00098356
Iteration 7/25 | Loss: 0.00098356
Iteration 8/25 | Loss: 0.00098356
Iteration 9/25 | Loss: 0.00098356
Iteration 10/25 | Loss: 0.00098356
Iteration 11/25 | Loss: 0.00098356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000983564998023212, 0.000983564998023212, 0.000983564998023212, 0.000983564998023212, 0.000983564998023212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983564998023212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21738112
Iteration 2/25 | Loss: 0.00119556
Iteration 3/25 | Loss: 0.00119554
Iteration 4/25 | Loss: 0.00119554
Iteration 5/25 | Loss: 0.00119554
Iteration 6/25 | Loss: 0.00119554
Iteration 7/25 | Loss: 0.00119554
Iteration 8/25 | Loss: 0.00119554
Iteration 9/25 | Loss: 0.00119554
Iteration 10/25 | Loss: 0.00119554
Iteration 11/25 | Loss: 0.00119554
Iteration 12/25 | Loss: 0.00119554
Iteration 13/25 | Loss: 0.00119554
Iteration 14/25 | Loss: 0.00119554
Iteration 15/25 | Loss: 0.00119554
Iteration 16/25 | Loss: 0.00119554
Iteration 17/25 | Loss: 0.00119554
Iteration 18/25 | Loss: 0.00119554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011955394875258207, 0.0011955394875258207, 0.0011955394875258207, 0.0011955394875258207, 0.0011955394875258207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011955394875258207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119554
Iteration 2/1000 | Loss: 0.00004998
Iteration 3/1000 | Loss: 0.00002909
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002189
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002091
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001979
Iteration 12/1000 | Loss: 0.00001968
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001963
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001935
Iteration 20/1000 | Loss: 0.00001934
Iteration 21/1000 | Loss: 0.00001934
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001926
Iteration 33/1000 | Loss: 0.00001925
Iteration 34/1000 | Loss: 0.00001925
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001920
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001919
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001909
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001909
Iteration 85/1000 | Loss: 0.00001909
Iteration 86/1000 | Loss: 0.00001909
Iteration 87/1000 | Loss: 0.00001909
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001908
Iteration 93/1000 | Loss: 0.00001908
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001907
Iteration 96/1000 | Loss: 0.00001907
Iteration 97/1000 | Loss: 0.00001907
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001906
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001905
Iteration 111/1000 | Loss: 0.00001905
Iteration 112/1000 | Loss: 0.00001905
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001904
Iteration 115/1000 | Loss: 0.00001904
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001904
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001903
Iteration 131/1000 | Loss: 0.00001903
Iteration 132/1000 | Loss: 0.00001903
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001902
Iteration 145/1000 | Loss: 0.00001902
Iteration 146/1000 | Loss: 0.00001902
Iteration 147/1000 | Loss: 0.00001902
Iteration 148/1000 | Loss: 0.00001902
Iteration 149/1000 | Loss: 0.00001902
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001902
Iteration 162/1000 | Loss: 0.00001902
Iteration 163/1000 | Loss: 0.00001902
Iteration 164/1000 | Loss: 0.00001902
Iteration 165/1000 | Loss: 0.00001902
Iteration 166/1000 | Loss: 0.00001902
Iteration 167/1000 | Loss: 0.00001901
Iteration 168/1000 | Loss: 0.00001901
Iteration 169/1000 | Loss: 0.00001901
Iteration 170/1000 | Loss: 0.00001901
Iteration 171/1000 | Loss: 0.00001901
Iteration 172/1000 | Loss: 0.00001901
Iteration 173/1000 | Loss: 0.00001901
Iteration 174/1000 | Loss: 0.00001901
Iteration 175/1000 | Loss: 0.00001901
Iteration 176/1000 | Loss: 0.00001901
Iteration 177/1000 | Loss: 0.00001901
Iteration 178/1000 | Loss: 0.00001901
Iteration 179/1000 | Loss: 0.00001901
Iteration 180/1000 | Loss: 0.00001901
Iteration 181/1000 | Loss: 0.00001901
Iteration 182/1000 | Loss: 0.00001901
Iteration 183/1000 | Loss: 0.00001901
Iteration 184/1000 | Loss: 0.00001901
Iteration 185/1000 | Loss: 0.00001901
Iteration 186/1000 | Loss: 0.00001901
Iteration 187/1000 | Loss: 0.00001901
Iteration 188/1000 | Loss: 0.00001901
Iteration 189/1000 | Loss: 0.00001901
Iteration 190/1000 | Loss: 0.00001901
Iteration 191/1000 | Loss: 0.00001901
Iteration 192/1000 | Loss: 0.00001901
Iteration 193/1000 | Loss: 0.00001901
Iteration 194/1000 | Loss: 0.00001901
Iteration 195/1000 | Loss: 0.00001901
Iteration 196/1000 | Loss: 0.00001901
Iteration 197/1000 | Loss: 0.00001901
Iteration 198/1000 | Loss: 0.00001901
Iteration 199/1000 | Loss: 0.00001901
Iteration 200/1000 | Loss: 0.00001901
Iteration 201/1000 | Loss: 0.00001901
Iteration 202/1000 | Loss: 0.00001901
Iteration 203/1000 | Loss: 0.00001901
Iteration 204/1000 | Loss: 0.00001901
Iteration 205/1000 | Loss: 0.00001901
Iteration 206/1000 | Loss: 0.00001901
Iteration 207/1000 | Loss: 0.00001901
Iteration 208/1000 | Loss: 0.00001901
Iteration 209/1000 | Loss: 0.00001901
Iteration 210/1000 | Loss: 0.00001901
Iteration 211/1000 | Loss: 0.00001901
Iteration 212/1000 | Loss: 0.00001901
Iteration 213/1000 | Loss: 0.00001901
Iteration 214/1000 | Loss: 0.00001901
Iteration 215/1000 | Loss: 0.00001901
Iteration 216/1000 | Loss: 0.00001901
Iteration 217/1000 | Loss: 0.00001901
Iteration 218/1000 | Loss: 0.00001901
Iteration 219/1000 | Loss: 0.00001901
Iteration 220/1000 | Loss: 0.00001901
Iteration 221/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.900637653307058e-05, 1.900637653307058e-05, 1.900637653307058e-05, 1.900637653307058e-05, 1.900637653307058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.900637653307058e-05

Optimization complete. Final v2v error: 3.72295880317688 mm

Highest mean error: 5.511430263519287 mm for frame 222

Lowest mean error: 3.4557368755340576 mm for frame 70

Saving results

Total time: 46.987603187561035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493150
Iteration 2/25 | Loss: 0.00114349
Iteration 3/25 | Loss: 0.00104475
Iteration 4/25 | Loss: 0.00101734
Iteration 5/25 | Loss: 0.00100876
Iteration 6/25 | Loss: 0.00100724
Iteration 7/25 | Loss: 0.00100691
Iteration 8/25 | Loss: 0.00100691
Iteration 9/25 | Loss: 0.00100691
Iteration 10/25 | Loss: 0.00100691
Iteration 11/25 | Loss: 0.00100691
Iteration 12/25 | Loss: 0.00100691
Iteration 13/25 | Loss: 0.00100691
Iteration 14/25 | Loss: 0.00100691
Iteration 15/25 | Loss: 0.00100691
Iteration 16/25 | Loss: 0.00100691
Iteration 17/25 | Loss: 0.00100691
Iteration 18/25 | Loss: 0.00100691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010069124400615692, 0.0010069124400615692, 0.0010069124400615692, 0.0010069124400615692, 0.0010069124400615692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010069124400615692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37855959
Iteration 2/25 | Loss: 0.00087447
Iteration 3/25 | Loss: 0.00087446
Iteration 4/25 | Loss: 0.00087446
Iteration 5/25 | Loss: 0.00087446
Iteration 6/25 | Loss: 0.00087446
Iteration 7/25 | Loss: 0.00087446
Iteration 8/25 | Loss: 0.00087446
Iteration 9/25 | Loss: 0.00087446
Iteration 10/25 | Loss: 0.00087446
Iteration 11/25 | Loss: 0.00087446
Iteration 12/25 | Loss: 0.00087446
Iteration 13/25 | Loss: 0.00087446
Iteration 14/25 | Loss: 0.00087446
Iteration 15/25 | Loss: 0.00087446
Iteration 16/25 | Loss: 0.00087446
Iteration 17/25 | Loss: 0.00087446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008744621300138533, 0.0008744621300138533, 0.0008744621300138533, 0.0008744621300138533, 0.0008744621300138533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008744621300138533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087446
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00003466
Iteration 4/1000 | Loss: 0.00003105
Iteration 5/1000 | Loss: 0.00002928
Iteration 6/1000 | Loss: 0.00002818
Iteration 7/1000 | Loss: 0.00002750
Iteration 8/1000 | Loss: 0.00002688
Iteration 9/1000 | Loss: 0.00002656
Iteration 10/1000 | Loss: 0.00002630
Iteration 11/1000 | Loss: 0.00002610
Iteration 12/1000 | Loss: 0.00002595
Iteration 13/1000 | Loss: 0.00002595
Iteration 14/1000 | Loss: 0.00002595
Iteration 15/1000 | Loss: 0.00002594
Iteration 16/1000 | Loss: 0.00002593
Iteration 17/1000 | Loss: 0.00002591
Iteration 18/1000 | Loss: 0.00002591
Iteration 19/1000 | Loss: 0.00002591
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002590
Iteration 22/1000 | Loss: 0.00002590
Iteration 23/1000 | Loss: 0.00002588
Iteration 24/1000 | Loss: 0.00002587
Iteration 25/1000 | Loss: 0.00002587
Iteration 26/1000 | Loss: 0.00002587
Iteration 27/1000 | Loss: 0.00002586
Iteration 28/1000 | Loss: 0.00002586
Iteration 29/1000 | Loss: 0.00002586
Iteration 30/1000 | Loss: 0.00002586
Iteration 31/1000 | Loss: 0.00002586
Iteration 32/1000 | Loss: 0.00002586
Iteration 33/1000 | Loss: 0.00002586
Iteration 34/1000 | Loss: 0.00002585
Iteration 35/1000 | Loss: 0.00002585
Iteration 36/1000 | Loss: 0.00002584
Iteration 37/1000 | Loss: 0.00002582
Iteration 38/1000 | Loss: 0.00002582
Iteration 39/1000 | Loss: 0.00002582
Iteration 40/1000 | Loss: 0.00002582
Iteration 41/1000 | Loss: 0.00002582
Iteration 42/1000 | Loss: 0.00002582
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002581
Iteration 50/1000 | Loss: 0.00002581
Iteration 51/1000 | Loss: 0.00002581
Iteration 52/1000 | Loss: 0.00002580
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002578
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002578
Iteration 58/1000 | Loss: 0.00002577
Iteration 59/1000 | Loss: 0.00002577
Iteration 60/1000 | Loss: 0.00002577
Iteration 61/1000 | Loss: 0.00002576
Iteration 62/1000 | Loss: 0.00002576
Iteration 63/1000 | Loss: 0.00002575
Iteration 64/1000 | Loss: 0.00002575
Iteration 65/1000 | Loss: 0.00002575
Iteration 66/1000 | Loss: 0.00002575
Iteration 67/1000 | Loss: 0.00002575
Iteration 68/1000 | Loss: 0.00002574
Iteration 69/1000 | Loss: 0.00002574
Iteration 70/1000 | Loss: 0.00002574
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00002574
Iteration 73/1000 | Loss: 0.00002573
Iteration 74/1000 | Loss: 0.00002573
Iteration 75/1000 | Loss: 0.00002573
Iteration 76/1000 | Loss: 0.00002572
Iteration 77/1000 | Loss: 0.00002572
Iteration 78/1000 | Loss: 0.00002572
Iteration 79/1000 | Loss: 0.00002572
Iteration 80/1000 | Loss: 0.00002571
Iteration 81/1000 | Loss: 0.00002571
Iteration 82/1000 | Loss: 0.00002571
Iteration 83/1000 | Loss: 0.00002571
Iteration 84/1000 | Loss: 0.00002570
Iteration 85/1000 | Loss: 0.00002570
Iteration 86/1000 | Loss: 0.00002569
Iteration 87/1000 | Loss: 0.00002569
Iteration 88/1000 | Loss: 0.00002569
Iteration 89/1000 | Loss: 0.00002569
Iteration 90/1000 | Loss: 0.00002569
Iteration 91/1000 | Loss: 0.00002569
Iteration 92/1000 | Loss: 0.00002569
Iteration 93/1000 | Loss: 0.00002569
Iteration 94/1000 | Loss: 0.00002569
Iteration 95/1000 | Loss: 0.00002569
Iteration 96/1000 | Loss: 0.00002569
Iteration 97/1000 | Loss: 0.00002568
Iteration 98/1000 | Loss: 0.00002568
Iteration 99/1000 | Loss: 0.00002568
Iteration 100/1000 | Loss: 0.00002568
Iteration 101/1000 | Loss: 0.00002568
Iteration 102/1000 | Loss: 0.00002568
Iteration 103/1000 | Loss: 0.00002568
Iteration 104/1000 | Loss: 0.00002568
Iteration 105/1000 | Loss: 0.00002568
Iteration 106/1000 | Loss: 0.00002568
Iteration 107/1000 | Loss: 0.00002568
Iteration 108/1000 | Loss: 0.00002568
Iteration 109/1000 | Loss: 0.00002568
Iteration 110/1000 | Loss: 0.00002568
Iteration 111/1000 | Loss: 0.00002568
Iteration 112/1000 | Loss: 0.00002568
Iteration 113/1000 | Loss: 0.00002568
Iteration 114/1000 | Loss: 0.00002567
Iteration 115/1000 | Loss: 0.00002567
Iteration 116/1000 | Loss: 0.00002567
Iteration 117/1000 | Loss: 0.00002567
Iteration 118/1000 | Loss: 0.00002567
Iteration 119/1000 | Loss: 0.00002567
Iteration 120/1000 | Loss: 0.00002567
Iteration 121/1000 | Loss: 0.00002567
Iteration 122/1000 | Loss: 0.00002567
Iteration 123/1000 | Loss: 0.00002567
Iteration 124/1000 | Loss: 0.00002567
Iteration 125/1000 | Loss: 0.00002567
Iteration 126/1000 | Loss: 0.00002567
Iteration 127/1000 | Loss: 0.00002567
Iteration 128/1000 | Loss: 0.00002567
Iteration 129/1000 | Loss: 0.00002567
Iteration 130/1000 | Loss: 0.00002567
Iteration 131/1000 | Loss: 0.00002567
Iteration 132/1000 | Loss: 0.00002567
Iteration 133/1000 | Loss: 0.00002567
Iteration 134/1000 | Loss: 0.00002567
Iteration 135/1000 | Loss: 0.00002567
Iteration 136/1000 | Loss: 0.00002567
Iteration 137/1000 | Loss: 0.00002567
Iteration 138/1000 | Loss: 0.00002567
Iteration 139/1000 | Loss: 0.00002567
Iteration 140/1000 | Loss: 0.00002567
Iteration 141/1000 | Loss: 0.00002567
Iteration 142/1000 | Loss: 0.00002567
Iteration 143/1000 | Loss: 0.00002567
Iteration 144/1000 | Loss: 0.00002567
Iteration 145/1000 | Loss: 0.00002567
Iteration 146/1000 | Loss: 0.00002567
Iteration 147/1000 | Loss: 0.00002567
Iteration 148/1000 | Loss: 0.00002567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.5666166038718075e-05, 2.5666166038718075e-05, 2.5666166038718075e-05, 2.5666166038718075e-05, 2.5666166038718075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5666166038718075e-05

Optimization complete. Final v2v error: 4.3700995445251465 mm

Highest mean error: 4.839615345001221 mm for frame 26

Lowest mean error: 3.9076735973358154 mm for frame 3

Saving results

Total time: 34.89342188835144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517340
Iteration 2/25 | Loss: 0.00116029
Iteration 3/25 | Loss: 0.00100899
Iteration 4/25 | Loss: 0.00098236
Iteration 5/25 | Loss: 0.00097665
Iteration 6/25 | Loss: 0.00097592
Iteration 7/25 | Loss: 0.00097592
Iteration 8/25 | Loss: 0.00097592
Iteration 9/25 | Loss: 0.00097592
Iteration 10/25 | Loss: 0.00097592
Iteration 11/25 | Loss: 0.00097592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009759185486473143, 0.0009759185486473143, 0.0009759185486473143, 0.0009759185486473143, 0.0009759185486473143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009759185486473143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74404395
Iteration 2/25 | Loss: 0.00073314
Iteration 3/25 | Loss: 0.00073313
Iteration 4/25 | Loss: 0.00073313
Iteration 5/25 | Loss: 0.00073313
Iteration 6/25 | Loss: 0.00073313
Iteration 7/25 | Loss: 0.00073313
Iteration 8/25 | Loss: 0.00073313
Iteration 9/25 | Loss: 0.00073313
Iteration 10/25 | Loss: 0.00073313
Iteration 11/25 | Loss: 0.00073313
Iteration 12/25 | Loss: 0.00073313
Iteration 13/25 | Loss: 0.00073313
Iteration 14/25 | Loss: 0.00073313
Iteration 15/25 | Loss: 0.00073313
Iteration 16/25 | Loss: 0.00073313
Iteration 17/25 | Loss: 0.00073313
Iteration 18/25 | Loss: 0.00073313
Iteration 19/25 | Loss: 0.00073313
Iteration 20/25 | Loss: 0.00073313
Iteration 21/25 | Loss: 0.00073313
Iteration 22/25 | Loss: 0.00073313
Iteration 23/25 | Loss: 0.00073313
Iteration 24/25 | Loss: 0.00073313
Iteration 25/25 | Loss: 0.00073313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073313
Iteration 2/1000 | Loss: 0.00003602
Iteration 3/1000 | Loss: 0.00002918
Iteration 4/1000 | Loss: 0.00002697
Iteration 5/1000 | Loss: 0.00002560
Iteration 6/1000 | Loss: 0.00002491
Iteration 7/1000 | Loss: 0.00002439
Iteration 8/1000 | Loss: 0.00002386
Iteration 9/1000 | Loss: 0.00002365
Iteration 10/1000 | Loss: 0.00002338
Iteration 11/1000 | Loss: 0.00002337
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002334
Iteration 15/1000 | Loss: 0.00002334
Iteration 16/1000 | Loss: 0.00002333
Iteration 17/1000 | Loss: 0.00002331
Iteration 18/1000 | Loss: 0.00002327
Iteration 19/1000 | Loss: 0.00002327
Iteration 20/1000 | Loss: 0.00002323
Iteration 21/1000 | Loss: 0.00002323
Iteration 22/1000 | Loss: 0.00002322
Iteration 23/1000 | Loss: 0.00002322
Iteration 24/1000 | Loss: 0.00002320
Iteration 25/1000 | Loss: 0.00002318
Iteration 26/1000 | Loss: 0.00002318
Iteration 27/1000 | Loss: 0.00002318
Iteration 28/1000 | Loss: 0.00002317
Iteration 29/1000 | Loss: 0.00002315
Iteration 30/1000 | Loss: 0.00002315
Iteration 31/1000 | Loss: 0.00002315
Iteration 32/1000 | Loss: 0.00002315
Iteration 33/1000 | Loss: 0.00002315
Iteration 34/1000 | Loss: 0.00002315
Iteration 35/1000 | Loss: 0.00002315
Iteration 36/1000 | Loss: 0.00002315
Iteration 37/1000 | Loss: 0.00002315
Iteration 38/1000 | Loss: 0.00002315
Iteration 39/1000 | Loss: 0.00002315
Iteration 40/1000 | Loss: 0.00002315
Iteration 41/1000 | Loss: 0.00002315
Iteration 42/1000 | Loss: 0.00002314
Iteration 43/1000 | Loss: 0.00002314
Iteration 44/1000 | Loss: 0.00002314
Iteration 45/1000 | Loss: 0.00002314
Iteration 46/1000 | Loss: 0.00002314
Iteration 47/1000 | Loss: 0.00002314
Iteration 48/1000 | Loss: 0.00002314
Iteration 49/1000 | Loss: 0.00002313
Iteration 50/1000 | Loss: 0.00002313
Iteration 51/1000 | Loss: 0.00002312
Iteration 52/1000 | Loss: 0.00002312
Iteration 53/1000 | Loss: 0.00002312
Iteration 54/1000 | Loss: 0.00002312
Iteration 55/1000 | Loss: 0.00002312
Iteration 56/1000 | Loss: 0.00002311
Iteration 57/1000 | Loss: 0.00002311
Iteration 58/1000 | Loss: 0.00002310
Iteration 59/1000 | Loss: 0.00002310
Iteration 60/1000 | Loss: 0.00002310
Iteration 61/1000 | Loss: 0.00002310
Iteration 62/1000 | Loss: 0.00002309
Iteration 63/1000 | Loss: 0.00002309
Iteration 64/1000 | Loss: 0.00002309
Iteration 65/1000 | Loss: 0.00002309
Iteration 66/1000 | Loss: 0.00002309
Iteration 67/1000 | Loss: 0.00002307
Iteration 68/1000 | Loss: 0.00002307
Iteration 69/1000 | Loss: 0.00002307
Iteration 70/1000 | Loss: 0.00002307
Iteration 71/1000 | Loss: 0.00002307
Iteration 72/1000 | Loss: 0.00002307
Iteration 73/1000 | Loss: 0.00002307
Iteration 74/1000 | Loss: 0.00002307
Iteration 75/1000 | Loss: 0.00002307
Iteration 76/1000 | Loss: 0.00002307
Iteration 77/1000 | Loss: 0.00002306
Iteration 78/1000 | Loss: 0.00002306
Iteration 79/1000 | Loss: 0.00002306
Iteration 80/1000 | Loss: 0.00002306
Iteration 81/1000 | Loss: 0.00002305
Iteration 82/1000 | Loss: 0.00002305
Iteration 83/1000 | Loss: 0.00002305
Iteration 84/1000 | Loss: 0.00002305
Iteration 85/1000 | Loss: 0.00002305
Iteration 86/1000 | Loss: 0.00002305
Iteration 87/1000 | Loss: 0.00002304
Iteration 88/1000 | Loss: 0.00002304
Iteration 89/1000 | Loss: 0.00002304
Iteration 90/1000 | Loss: 0.00002304
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002304
Iteration 95/1000 | Loss: 0.00002304
Iteration 96/1000 | Loss: 0.00002304
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002303
Iteration 99/1000 | Loss: 0.00002303
Iteration 100/1000 | Loss: 0.00002303
Iteration 101/1000 | Loss: 0.00002302
Iteration 102/1000 | Loss: 0.00002302
Iteration 103/1000 | Loss: 0.00002302
Iteration 104/1000 | Loss: 0.00002302
Iteration 105/1000 | Loss: 0.00002302
Iteration 106/1000 | Loss: 0.00002302
Iteration 107/1000 | Loss: 0.00002301
Iteration 108/1000 | Loss: 0.00002301
Iteration 109/1000 | Loss: 0.00002301
Iteration 110/1000 | Loss: 0.00002301
Iteration 111/1000 | Loss: 0.00002301
Iteration 112/1000 | Loss: 0.00002301
Iteration 113/1000 | Loss: 0.00002301
Iteration 114/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.3014705220703036e-05, 2.3014705220703036e-05, 2.3014705220703036e-05, 2.3014705220703036e-05, 2.3014705220703036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3014705220703036e-05

Optimization complete. Final v2v error: 4.185027122497559 mm

Highest mean error: 4.465170383453369 mm for frame 9

Lowest mean error: 3.989323139190674 mm for frame 135

Saving results

Total time: 31.951181888580322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082836
Iteration 2/25 | Loss: 0.01082836
Iteration 3/25 | Loss: 0.00357195
Iteration 4/25 | Loss: 0.00231657
Iteration 5/25 | Loss: 0.00163095
Iteration 6/25 | Loss: 0.00163653
Iteration 7/25 | Loss: 0.00129454
Iteration 8/25 | Loss: 0.00114365
Iteration 9/25 | Loss: 0.00109763
Iteration 10/25 | Loss: 0.00106133
Iteration 11/25 | Loss: 0.00104551
Iteration 12/25 | Loss: 0.00104935
Iteration 13/25 | Loss: 0.00104397
Iteration 14/25 | Loss: 0.00103845
Iteration 15/25 | Loss: 0.00103668
Iteration 16/25 | Loss: 0.00103583
Iteration 17/25 | Loss: 0.00103528
Iteration 18/25 | Loss: 0.00103507
Iteration 19/25 | Loss: 0.00103498
Iteration 20/25 | Loss: 0.00103494
Iteration 21/25 | Loss: 0.00103494
Iteration 22/25 | Loss: 0.00103493
Iteration 23/25 | Loss: 0.00103493
Iteration 24/25 | Loss: 0.00103491
Iteration 25/25 | Loss: 0.00103491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23364568
Iteration 2/25 | Loss: 0.00099250
Iteration 3/25 | Loss: 0.00099250
Iteration 4/25 | Loss: 0.00099250
Iteration 5/25 | Loss: 0.00099250
Iteration 6/25 | Loss: 0.00099250
Iteration 7/25 | Loss: 0.00099250
Iteration 8/25 | Loss: 0.00099250
Iteration 9/25 | Loss: 0.00099249
Iteration 10/25 | Loss: 0.00099249
Iteration 11/25 | Loss: 0.00099249
Iteration 12/25 | Loss: 0.00099249
Iteration 13/25 | Loss: 0.00099249
Iteration 14/25 | Loss: 0.00099249
Iteration 15/25 | Loss: 0.00099249
Iteration 16/25 | Loss: 0.00099249
Iteration 17/25 | Loss: 0.00099249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009924941696226597, 0.0009924941696226597, 0.0009924941696226597, 0.0009924941696226597, 0.0009924941696226597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009924941696226597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099249
Iteration 2/1000 | Loss: 0.00030068
Iteration 3/1000 | Loss: 0.00008484
Iteration 4/1000 | Loss: 0.00006330
Iteration 5/1000 | Loss: 0.00008427
Iteration 6/1000 | Loss: 0.00005898
Iteration 7/1000 | Loss: 0.00008716
Iteration 8/1000 | Loss: 0.00015456
Iteration 9/1000 | Loss: 0.00012769
Iteration 10/1000 | Loss: 0.00005310
Iteration 11/1000 | Loss: 0.00004740
Iteration 12/1000 | Loss: 0.00004403
Iteration 13/1000 | Loss: 0.00023474
Iteration 14/1000 | Loss: 0.00004518
Iteration 15/1000 | Loss: 0.00019268
Iteration 16/1000 | Loss: 0.00006870
Iteration 17/1000 | Loss: 0.00004505
Iteration 18/1000 | Loss: 0.00004125
Iteration 19/1000 | Loss: 0.00011048
Iteration 20/1000 | Loss: 0.00014332
Iteration 21/1000 | Loss: 0.00007270
Iteration 22/1000 | Loss: 0.00019190
Iteration 23/1000 | Loss: 0.00007222
Iteration 24/1000 | Loss: 0.00007915
Iteration 25/1000 | Loss: 0.00005745
Iteration 26/1000 | Loss: 0.00005350
Iteration 27/1000 | Loss: 0.00006683
Iteration 28/1000 | Loss: 0.00012667
Iteration 29/1000 | Loss: 0.00006261
Iteration 30/1000 | Loss: 0.00018620
Iteration 31/1000 | Loss: 0.00013006
Iteration 32/1000 | Loss: 0.00009975
Iteration 33/1000 | Loss: 0.00004368
Iteration 34/1000 | Loss: 0.00012479
Iteration 35/1000 | Loss: 0.00006995
Iteration 36/1000 | Loss: 0.00003559
Iteration 37/1000 | Loss: 0.00010694
Iteration 38/1000 | Loss: 0.00011717
Iteration 39/1000 | Loss: 0.00014358
Iteration 40/1000 | Loss: 0.00006107
Iteration 41/1000 | Loss: 0.00010178
Iteration 42/1000 | Loss: 0.00008548
Iteration 43/1000 | Loss: 0.00008586
Iteration 44/1000 | Loss: 0.00011609
Iteration 45/1000 | Loss: 0.00011669
Iteration 46/1000 | Loss: 0.00010589
Iteration 47/1000 | Loss: 0.00005497
Iteration 48/1000 | Loss: 0.00007422
Iteration 49/1000 | Loss: 0.00003422
Iteration 50/1000 | Loss: 0.00003330
Iteration 51/1000 | Loss: 0.00003769
Iteration 52/1000 | Loss: 0.00006390
Iteration 53/1000 | Loss: 0.00006372
Iteration 54/1000 | Loss: 0.00006186
Iteration 55/1000 | Loss: 0.00005429
Iteration 56/1000 | Loss: 0.00003305
Iteration 57/1000 | Loss: 0.00007638
Iteration 58/1000 | Loss: 0.00012070
Iteration 59/1000 | Loss: 0.00008624
Iteration 60/1000 | Loss: 0.00006092
Iteration 61/1000 | Loss: 0.00007871
Iteration 62/1000 | Loss: 0.00009005
Iteration 63/1000 | Loss: 0.00006727
Iteration 64/1000 | Loss: 0.00008116
Iteration 65/1000 | Loss: 0.00010547
Iteration 66/1000 | Loss: 0.00008052
Iteration 67/1000 | Loss: 0.00010887
Iteration 68/1000 | Loss: 0.00010567
Iteration 69/1000 | Loss: 0.00009060
Iteration 70/1000 | Loss: 0.00006489
Iteration 71/1000 | Loss: 0.00003365
Iteration 72/1000 | Loss: 0.00008994
Iteration 73/1000 | Loss: 0.00011592
Iteration 74/1000 | Loss: 0.00008945
Iteration 75/1000 | Loss: 0.00011321
Iteration 76/1000 | Loss: 0.00012725
Iteration 77/1000 | Loss: 0.00011221
Iteration 78/1000 | Loss: 0.00019147
Iteration 79/1000 | Loss: 0.00004875
Iteration 80/1000 | Loss: 0.00012797
Iteration 81/1000 | Loss: 0.00005804
Iteration 82/1000 | Loss: 0.00006337
Iteration 83/1000 | Loss: 0.00012350
Iteration 84/1000 | Loss: 0.00007999
Iteration 85/1000 | Loss: 0.00006939
Iteration 86/1000 | Loss: 0.00007144
Iteration 87/1000 | Loss: 0.00008536
Iteration 88/1000 | Loss: 0.00009271
Iteration 89/1000 | Loss: 0.00008625
Iteration 90/1000 | Loss: 0.00009822
Iteration 91/1000 | Loss: 0.00004625
Iteration 92/1000 | Loss: 0.00007160
Iteration 93/1000 | Loss: 0.00004882
Iteration 94/1000 | Loss: 0.00004063
Iteration 95/1000 | Loss: 0.00003759
Iteration 96/1000 | Loss: 0.00003498
Iteration 97/1000 | Loss: 0.00003369
Iteration 98/1000 | Loss: 0.00003285
Iteration 99/1000 | Loss: 0.00003252
Iteration 100/1000 | Loss: 0.00011252
Iteration 101/1000 | Loss: 0.00003567
Iteration 102/1000 | Loss: 0.00003260
Iteration 103/1000 | Loss: 0.00003140
Iteration 104/1000 | Loss: 0.00003053
Iteration 105/1000 | Loss: 0.00003007
Iteration 106/1000 | Loss: 0.00002978
Iteration 107/1000 | Loss: 0.00002954
Iteration 108/1000 | Loss: 0.00002930
Iteration 109/1000 | Loss: 0.00002929
Iteration 110/1000 | Loss: 0.00002922
Iteration 111/1000 | Loss: 0.00002915
Iteration 112/1000 | Loss: 0.00002912
Iteration 113/1000 | Loss: 0.00002911
Iteration 114/1000 | Loss: 0.00002911
Iteration 115/1000 | Loss: 0.00002900
Iteration 116/1000 | Loss: 0.00002894
Iteration 117/1000 | Loss: 0.00002893
Iteration 118/1000 | Loss: 0.00002892
Iteration 119/1000 | Loss: 0.00002892
Iteration 120/1000 | Loss: 0.00002891
Iteration 121/1000 | Loss: 0.00002891
Iteration 122/1000 | Loss: 0.00002889
Iteration 123/1000 | Loss: 0.00002888
Iteration 124/1000 | Loss: 0.00002887
Iteration 125/1000 | Loss: 0.00002885
Iteration 126/1000 | Loss: 0.00002885
Iteration 127/1000 | Loss: 0.00002883
Iteration 128/1000 | Loss: 0.00002883
Iteration 129/1000 | Loss: 0.00002882
Iteration 130/1000 | Loss: 0.00002882
Iteration 131/1000 | Loss: 0.00002882
Iteration 132/1000 | Loss: 0.00002882
Iteration 133/1000 | Loss: 0.00002882
Iteration 134/1000 | Loss: 0.00002882
Iteration 135/1000 | Loss: 0.00002882
Iteration 136/1000 | Loss: 0.00002882
Iteration 137/1000 | Loss: 0.00002882
Iteration 138/1000 | Loss: 0.00002882
Iteration 139/1000 | Loss: 0.00002882
Iteration 140/1000 | Loss: 0.00002882
Iteration 141/1000 | Loss: 0.00002882
Iteration 142/1000 | Loss: 0.00002881
Iteration 143/1000 | Loss: 0.00002881
Iteration 144/1000 | Loss: 0.00002881
Iteration 145/1000 | Loss: 0.00002881
Iteration 146/1000 | Loss: 0.00002881
Iteration 147/1000 | Loss: 0.00002881
Iteration 148/1000 | Loss: 0.00002881
Iteration 149/1000 | Loss: 0.00002881
Iteration 150/1000 | Loss: 0.00002881
Iteration 151/1000 | Loss: 0.00002881
Iteration 152/1000 | Loss: 0.00002880
Iteration 153/1000 | Loss: 0.00002880
Iteration 154/1000 | Loss: 0.00002880
Iteration 155/1000 | Loss: 0.00002880
Iteration 156/1000 | Loss: 0.00002880
Iteration 157/1000 | Loss: 0.00002880
Iteration 158/1000 | Loss: 0.00002880
Iteration 159/1000 | Loss: 0.00002880
Iteration 160/1000 | Loss: 0.00002880
Iteration 161/1000 | Loss: 0.00002880
Iteration 162/1000 | Loss: 0.00002880
Iteration 163/1000 | Loss: 0.00002880
Iteration 164/1000 | Loss: 0.00002880
Iteration 165/1000 | Loss: 0.00002880
Iteration 166/1000 | Loss: 0.00002880
Iteration 167/1000 | Loss: 0.00002880
Iteration 168/1000 | Loss: 0.00002880
Iteration 169/1000 | Loss: 0.00002880
Iteration 170/1000 | Loss: 0.00002880
Iteration 171/1000 | Loss: 0.00002880
Iteration 172/1000 | Loss: 0.00002880
Iteration 173/1000 | Loss: 0.00002880
Iteration 174/1000 | Loss: 0.00002880
Iteration 175/1000 | Loss: 0.00002880
Iteration 176/1000 | Loss: 0.00002880
Iteration 177/1000 | Loss: 0.00002880
Iteration 178/1000 | Loss: 0.00002880
Iteration 179/1000 | Loss: 0.00002880
Iteration 180/1000 | Loss: 0.00002880
Iteration 181/1000 | Loss: 0.00002880
Iteration 182/1000 | Loss: 0.00002880
Iteration 183/1000 | Loss: 0.00002880
Iteration 184/1000 | Loss: 0.00002880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.879672683775425e-05, 2.879672683775425e-05, 2.879672683775425e-05, 2.879672683775425e-05, 2.879672683775425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.879672683775425e-05

Optimization complete. Final v2v error: 4.583423614501953 mm

Highest mean error: 11.134933471679688 mm for frame 56

Lowest mean error: 4.052964687347412 mm for frame 138

Saving results

Total time: 219.7142415046692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874642
Iteration 2/25 | Loss: 0.00105443
Iteration 3/25 | Loss: 0.00092718
Iteration 4/25 | Loss: 0.00090965
Iteration 5/25 | Loss: 0.00090593
Iteration 6/25 | Loss: 0.00090470
Iteration 7/25 | Loss: 0.00090470
Iteration 8/25 | Loss: 0.00090470
Iteration 9/25 | Loss: 0.00090470
Iteration 10/25 | Loss: 0.00090470
Iteration 11/25 | Loss: 0.00090470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009047037456184626, 0.0009047037456184626, 0.0009047037456184626, 0.0009047037456184626, 0.0009047037456184626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009047037456184626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26613331
Iteration 2/25 | Loss: 0.00096096
Iteration 3/25 | Loss: 0.00096096
Iteration 4/25 | Loss: 0.00096096
Iteration 5/25 | Loss: 0.00096096
Iteration 6/25 | Loss: 0.00096096
Iteration 7/25 | Loss: 0.00096096
Iteration 8/25 | Loss: 0.00096096
Iteration 9/25 | Loss: 0.00096096
Iteration 10/25 | Loss: 0.00096096
Iteration 11/25 | Loss: 0.00096096
Iteration 12/25 | Loss: 0.00096096
Iteration 13/25 | Loss: 0.00096096
Iteration 14/25 | Loss: 0.00096096
Iteration 15/25 | Loss: 0.00096096
Iteration 16/25 | Loss: 0.00096096
Iteration 17/25 | Loss: 0.00096096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009609573753550649, 0.0009609573753550649, 0.0009609573753550649, 0.0009609573753550649, 0.0009609573753550649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009609573753550649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096096
Iteration 2/1000 | Loss: 0.00003644
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001642
Iteration 9/1000 | Loss: 0.00001615
Iteration 10/1000 | Loss: 0.00001600
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001599
Iteration 13/1000 | Loss: 0.00001598
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001595
Iteration 20/1000 | Loss: 0.00001595
Iteration 21/1000 | Loss: 0.00001594
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001586
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001573
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001568
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001567
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001558
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001553
Iteration 93/1000 | Loss: 0.00001553
Iteration 94/1000 | Loss: 0.00001553
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001552
Iteration 100/1000 | Loss: 0.00001552
Iteration 101/1000 | Loss: 0.00001552
Iteration 102/1000 | Loss: 0.00001552
Iteration 103/1000 | Loss: 0.00001552
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001550
Iteration 129/1000 | Loss: 0.00001550
Iteration 130/1000 | Loss: 0.00001550
Iteration 131/1000 | Loss: 0.00001550
Iteration 132/1000 | Loss: 0.00001550
Iteration 133/1000 | Loss: 0.00001550
Iteration 134/1000 | Loss: 0.00001550
Iteration 135/1000 | Loss: 0.00001550
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001549
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001549
Iteration 143/1000 | Loss: 0.00001549
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001548
Iteration 150/1000 | Loss: 0.00001548
Iteration 151/1000 | Loss: 0.00001548
Iteration 152/1000 | Loss: 0.00001548
Iteration 153/1000 | Loss: 0.00001548
Iteration 154/1000 | Loss: 0.00001548
Iteration 155/1000 | Loss: 0.00001548
Iteration 156/1000 | Loss: 0.00001548
Iteration 157/1000 | Loss: 0.00001548
Iteration 158/1000 | Loss: 0.00001548
Iteration 159/1000 | Loss: 0.00001548
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Iteration 162/1000 | Loss: 0.00001547
Iteration 163/1000 | Loss: 0.00001547
Iteration 164/1000 | Loss: 0.00001547
Iteration 165/1000 | Loss: 0.00001547
Iteration 166/1000 | Loss: 0.00001547
Iteration 167/1000 | Loss: 0.00001547
Iteration 168/1000 | Loss: 0.00001547
Iteration 169/1000 | Loss: 0.00001547
Iteration 170/1000 | Loss: 0.00001547
Iteration 171/1000 | Loss: 0.00001547
Iteration 172/1000 | Loss: 0.00001547
Iteration 173/1000 | Loss: 0.00001547
Iteration 174/1000 | Loss: 0.00001547
Iteration 175/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.5472871382371522e-05, 1.5472871382371522e-05, 1.5472871382371522e-05, 1.5472871382371522e-05, 1.5472871382371522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5472871382371522e-05

Optimization complete. Final v2v error: 3.436387300491333 mm

Highest mean error: 3.7688941955566406 mm for frame 88

Lowest mean error: 3.1070189476013184 mm for frame 21

Saving results

Total time: 36.563114166259766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396048
Iteration 2/25 | Loss: 0.00116416
Iteration 3/25 | Loss: 0.00100223
Iteration 4/25 | Loss: 0.00097922
Iteration 5/25 | Loss: 0.00097186
Iteration 6/25 | Loss: 0.00097094
Iteration 7/25 | Loss: 0.00097094
Iteration 8/25 | Loss: 0.00097094
Iteration 9/25 | Loss: 0.00097094
Iteration 10/25 | Loss: 0.00097094
Iteration 11/25 | Loss: 0.00097094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009709377191029489, 0.0009709377191029489, 0.0009709377191029489, 0.0009709377191029489, 0.0009709377191029489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009709377191029489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24516332
Iteration 2/25 | Loss: 0.00110924
Iteration 3/25 | Loss: 0.00110924
Iteration 4/25 | Loss: 0.00110924
Iteration 5/25 | Loss: 0.00110924
Iteration 6/25 | Loss: 0.00110924
Iteration 7/25 | Loss: 0.00110924
Iteration 8/25 | Loss: 0.00110924
Iteration 9/25 | Loss: 0.00110924
Iteration 10/25 | Loss: 0.00110924
Iteration 11/25 | Loss: 0.00110924
Iteration 12/25 | Loss: 0.00110924
Iteration 13/25 | Loss: 0.00110924
Iteration 14/25 | Loss: 0.00110924
Iteration 15/25 | Loss: 0.00110924
Iteration 16/25 | Loss: 0.00110924
Iteration 17/25 | Loss: 0.00110924
Iteration 18/25 | Loss: 0.00110924
Iteration 19/25 | Loss: 0.00110924
Iteration 20/25 | Loss: 0.00110924
Iteration 21/25 | Loss: 0.00110924
Iteration 22/25 | Loss: 0.00110924
Iteration 23/25 | Loss: 0.00110924
Iteration 24/25 | Loss: 0.00110924
Iteration 25/25 | Loss: 0.00110924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110924
Iteration 2/1000 | Loss: 0.00003948
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002287
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002048
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001932
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001931
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001924
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001915
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001911
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001909
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001909
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001906
Iteration 48/1000 | Loss: 0.00001906
Iteration 49/1000 | Loss: 0.00001906
Iteration 50/1000 | Loss: 0.00001905
Iteration 51/1000 | Loss: 0.00001905
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001902
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001902
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001902
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001902
Iteration 77/1000 | Loss: 0.00001902
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001901
Iteration 84/1000 | Loss: 0.00001901
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001900
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001900
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001898
Iteration 100/1000 | Loss: 0.00001898
Iteration 101/1000 | Loss: 0.00001898
Iteration 102/1000 | Loss: 0.00001898
Iteration 103/1000 | Loss: 0.00001898
Iteration 104/1000 | Loss: 0.00001898
Iteration 105/1000 | Loss: 0.00001898
Iteration 106/1000 | Loss: 0.00001897
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001897
Iteration 112/1000 | Loss: 0.00001897
Iteration 113/1000 | Loss: 0.00001897
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001896
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001896
Iteration 120/1000 | Loss: 0.00001896
Iteration 121/1000 | Loss: 0.00001895
Iteration 122/1000 | Loss: 0.00001895
Iteration 123/1000 | Loss: 0.00001895
Iteration 124/1000 | Loss: 0.00001895
Iteration 125/1000 | Loss: 0.00001895
Iteration 126/1000 | Loss: 0.00001895
Iteration 127/1000 | Loss: 0.00001895
Iteration 128/1000 | Loss: 0.00001895
Iteration 129/1000 | Loss: 0.00001895
Iteration 130/1000 | Loss: 0.00001894
Iteration 131/1000 | Loss: 0.00001894
Iteration 132/1000 | Loss: 0.00001894
Iteration 133/1000 | Loss: 0.00001894
Iteration 134/1000 | Loss: 0.00001894
Iteration 135/1000 | Loss: 0.00001894
Iteration 136/1000 | Loss: 0.00001894
Iteration 137/1000 | Loss: 0.00001894
Iteration 138/1000 | Loss: 0.00001894
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001893
Iteration 143/1000 | Loss: 0.00001893
Iteration 144/1000 | Loss: 0.00001893
Iteration 145/1000 | Loss: 0.00001893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.89334914466599e-05, 1.89334914466599e-05, 1.89334914466599e-05, 1.89334914466599e-05, 1.89334914466599e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.89334914466599e-05

Optimization complete. Final v2v error: 3.7449958324432373 mm

Highest mean error: 3.9232261180877686 mm for frame 107

Lowest mean error: 3.4124319553375244 mm for frame 12

Saving results

Total time: 33.42902493476868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902078
Iteration 2/25 | Loss: 0.00177816
Iteration 3/25 | Loss: 0.00127698
Iteration 4/25 | Loss: 0.00114652
Iteration 5/25 | Loss: 0.00111873
Iteration 6/25 | Loss: 0.00111462
Iteration 7/25 | Loss: 0.00111232
Iteration 8/25 | Loss: 0.00111066
Iteration 9/25 | Loss: 0.00110862
Iteration 10/25 | Loss: 0.00110553
Iteration 11/25 | Loss: 0.00110510
Iteration 12/25 | Loss: 0.00110506
Iteration 13/25 | Loss: 0.00110506
Iteration 14/25 | Loss: 0.00110506
Iteration 15/25 | Loss: 0.00110506
Iteration 16/25 | Loss: 0.00110506
Iteration 17/25 | Loss: 0.00110506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001105062197893858, 0.001105062197893858, 0.001105062197893858, 0.001105062197893858, 0.001105062197893858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105062197893858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23585582
Iteration 2/25 | Loss: 0.00076600
Iteration 3/25 | Loss: 0.00069216
Iteration 4/25 | Loss: 0.00069216
Iteration 5/25 | Loss: 0.00069216
Iteration 6/25 | Loss: 0.00069216
Iteration 7/25 | Loss: 0.00069216
Iteration 8/25 | Loss: 0.00069216
Iteration 9/25 | Loss: 0.00069216
Iteration 10/25 | Loss: 0.00069216
Iteration 11/25 | Loss: 0.00069216
Iteration 12/25 | Loss: 0.00069216
Iteration 13/25 | Loss: 0.00069216
Iteration 14/25 | Loss: 0.00069216
Iteration 15/25 | Loss: 0.00069216
Iteration 16/25 | Loss: 0.00069216
Iteration 17/25 | Loss: 0.00069216
Iteration 18/25 | Loss: 0.00069216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006921602180227637, 0.0006921602180227637, 0.0006921602180227637, 0.0006921602180227637, 0.0006921602180227637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006921602180227637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069216
Iteration 2/1000 | Loss: 0.00007985
Iteration 3/1000 | Loss: 0.00005588
Iteration 4/1000 | Loss: 0.00008361
Iteration 5/1000 | Loss: 0.00004902
Iteration 6/1000 | Loss: 0.00004743
Iteration 7/1000 | Loss: 0.00007868
Iteration 8/1000 | Loss: 0.00005513
Iteration 9/1000 | Loss: 0.00005404
Iteration 10/1000 | Loss: 0.00004525
Iteration 11/1000 | Loss: 0.00016557
Iteration 12/1000 | Loss: 0.00005550
Iteration 13/1000 | Loss: 0.00007675
Iteration 14/1000 | Loss: 0.00013349
Iteration 15/1000 | Loss: 0.00004341
Iteration 16/1000 | Loss: 0.00004279
Iteration 17/1000 | Loss: 0.00004234
Iteration 18/1000 | Loss: 0.00008446
Iteration 19/1000 | Loss: 0.00004222
Iteration 20/1000 | Loss: 0.00004197
Iteration 21/1000 | Loss: 0.00004195
Iteration 22/1000 | Loss: 0.00004187
Iteration 23/1000 | Loss: 0.00004187
Iteration 24/1000 | Loss: 0.00004185
Iteration 25/1000 | Loss: 0.00004185
Iteration 26/1000 | Loss: 0.00004179
Iteration 27/1000 | Loss: 0.00004174
Iteration 28/1000 | Loss: 0.00004174
Iteration 29/1000 | Loss: 0.00004174
Iteration 30/1000 | Loss: 0.00004173
Iteration 31/1000 | Loss: 0.00004173
Iteration 32/1000 | Loss: 0.00004173
Iteration 33/1000 | Loss: 0.00004173
Iteration 34/1000 | Loss: 0.00004173
Iteration 35/1000 | Loss: 0.00004173
Iteration 36/1000 | Loss: 0.00004173
Iteration 37/1000 | Loss: 0.00004172
Iteration 38/1000 | Loss: 0.00004172
Iteration 39/1000 | Loss: 0.00004172
Iteration 40/1000 | Loss: 0.00004172
Iteration 41/1000 | Loss: 0.00004172
Iteration 42/1000 | Loss: 0.00004172
Iteration 43/1000 | Loss: 0.00004171
Iteration 44/1000 | Loss: 0.00004171
Iteration 45/1000 | Loss: 0.00004171
Iteration 46/1000 | Loss: 0.00004171
Iteration 47/1000 | Loss: 0.00004171
Iteration 48/1000 | Loss: 0.00004171
Iteration 49/1000 | Loss: 0.00004171
Iteration 50/1000 | Loss: 0.00004170
Iteration 51/1000 | Loss: 0.00004170
Iteration 52/1000 | Loss: 0.00004170
Iteration 53/1000 | Loss: 0.00004169
Iteration 54/1000 | Loss: 0.00004169
Iteration 55/1000 | Loss: 0.00004169
Iteration 56/1000 | Loss: 0.00004169
Iteration 57/1000 | Loss: 0.00004169
Iteration 58/1000 | Loss: 0.00004169
Iteration 59/1000 | Loss: 0.00004168
Iteration 60/1000 | Loss: 0.00004168
Iteration 61/1000 | Loss: 0.00004168
Iteration 62/1000 | Loss: 0.00004168
Iteration 63/1000 | Loss: 0.00004167
Iteration 64/1000 | Loss: 0.00004166
Iteration 65/1000 | Loss: 0.00004166
Iteration 66/1000 | Loss: 0.00004166
Iteration 67/1000 | Loss: 0.00004166
Iteration 68/1000 | Loss: 0.00004165
Iteration 69/1000 | Loss: 0.00004165
Iteration 70/1000 | Loss: 0.00004165
Iteration 71/1000 | Loss: 0.00004165
Iteration 72/1000 | Loss: 0.00004165
Iteration 73/1000 | Loss: 0.00004164
Iteration 74/1000 | Loss: 0.00004164
Iteration 75/1000 | Loss: 0.00004164
Iteration 76/1000 | Loss: 0.00004164
Iteration 77/1000 | Loss: 0.00004164
Iteration 78/1000 | Loss: 0.00004164
Iteration 79/1000 | Loss: 0.00004164
Iteration 80/1000 | Loss: 0.00004164
Iteration 81/1000 | Loss: 0.00004164
Iteration 82/1000 | Loss: 0.00004164
Iteration 83/1000 | Loss: 0.00004163
Iteration 84/1000 | Loss: 0.00004163
Iteration 85/1000 | Loss: 0.00004163
Iteration 86/1000 | Loss: 0.00004163
Iteration 87/1000 | Loss: 0.00004162
Iteration 88/1000 | Loss: 0.00004162
Iteration 89/1000 | Loss: 0.00004162
Iteration 90/1000 | Loss: 0.00004161
Iteration 91/1000 | Loss: 0.00004161
Iteration 92/1000 | Loss: 0.00004161
Iteration 93/1000 | Loss: 0.00004161
Iteration 94/1000 | Loss: 0.00004161
Iteration 95/1000 | Loss: 0.00004161
Iteration 96/1000 | Loss: 0.00004160
Iteration 97/1000 | Loss: 0.00004160
Iteration 98/1000 | Loss: 0.00004160
Iteration 99/1000 | Loss: 0.00004160
Iteration 100/1000 | Loss: 0.00004160
Iteration 101/1000 | Loss: 0.00004160
Iteration 102/1000 | Loss: 0.00004160
Iteration 103/1000 | Loss: 0.00004160
Iteration 104/1000 | Loss: 0.00004160
Iteration 105/1000 | Loss: 0.00004160
Iteration 106/1000 | Loss: 0.00004160
Iteration 107/1000 | Loss: 0.00004159
Iteration 108/1000 | Loss: 0.00004159
Iteration 109/1000 | Loss: 0.00004159
Iteration 110/1000 | Loss: 0.00004159
Iteration 111/1000 | Loss: 0.00004159
Iteration 112/1000 | Loss: 0.00004159
Iteration 113/1000 | Loss: 0.00004159
Iteration 114/1000 | Loss: 0.00004159
Iteration 115/1000 | Loss: 0.00004159
Iteration 116/1000 | Loss: 0.00004159
Iteration 117/1000 | Loss: 0.00004158
Iteration 118/1000 | Loss: 0.00004158
Iteration 119/1000 | Loss: 0.00004158
Iteration 120/1000 | Loss: 0.00004158
Iteration 121/1000 | Loss: 0.00004158
Iteration 122/1000 | Loss: 0.00004158
Iteration 123/1000 | Loss: 0.00004158
Iteration 124/1000 | Loss: 0.00004158
Iteration 125/1000 | Loss: 0.00004158
Iteration 126/1000 | Loss: 0.00004158
Iteration 127/1000 | Loss: 0.00004158
Iteration 128/1000 | Loss: 0.00004158
Iteration 129/1000 | Loss: 0.00004158
Iteration 130/1000 | Loss: 0.00004157
Iteration 131/1000 | Loss: 0.00004157
Iteration 132/1000 | Loss: 0.00004157
Iteration 133/1000 | Loss: 0.00004156
Iteration 134/1000 | Loss: 0.00004156
Iteration 135/1000 | Loss: 0.00004156
Iteration 136/1000 | Loss: 0.00004156
Iteration 137/1000 | Loss: 0.00004156
Iteration 138/1000 | Loss: 0.00004156
Iteration 139/1000 | Loss: 0.00004156
Iteration 140/1000 | Loss: 0.00004156
Iteration 141/1000 | Loss: 0.00004156
Iteration 142/1000 | Loss: 0.00004156
Iteration 143/1000 | Loss: 0.00004155
Iteration 144/1000 | Loss: 0.00004155
Iteration 145/1000 | Loss: 0.00004154
Iteration 146/1000 | Loss: 0.00004154
Iteration 147/1000 | Loss: 0.00004154
Iteration 148/1000 | Loss: 0.00004154
Iteration 149/1000 | Loss: 0.00004154
Iteration 150/1000 | Loss: 0.00004154
Iteration 151/1000 | Loss: 0.00004154
Iteration 152/1000 | Loss: 0.00004153
Iteration 153/1000 | Loss: 0.00004153
Iteration 154/1000 | Loss: 0.00004153
Iteration 155/1000 | Loss: 0.00004153
Iteration 156/1000 | Loss: 0.00004153
Iteration 157/1000 | Loss: 0.00004153
Iteration 158/1000 | Loss: 0.00004153
Iteration 159/1000 | Loss: 0.00004153
Iteration 160/1000 | Loss: 0.00004153
Iteration 161/1000 | Loss: 0.00004153
Iteration 162/1000 | Loss: 0.00004153
Iteration 163/1000 | Loss: 0.00004153
Iteration 164/1000 | Loss: 0.00004153
Iteration 165/1000 | Loss: 0.00004153
Iteration 166/1000 | Loss: 0.00004153
Iteration 167/1000 | Loss: 0.00004153
Iteration 168/1000 | Loss: 0.00004153
Iteration 169/1000 | Loss: 0.00004153
Iteration 170/1000 | Loss: 0.00004153
Iteration 171/1000 | Loss: 0.00004153
Iteration 172/1000 | Loss: 0.00004153
Iteration 173/1000 | Loss: 0.00004153
Iteration 174/1000 | Loss: 0.00004153
Iteration 175/1000 | Loss: 0.00004153
Iteration 176/1000 | Loss: 0.00004153
Iteration 177/1000 | Loss: 0.00004153
Iteration 178/1000 | Loss: 0.00004153
Iteration 179/1000 | Loss: 0.00004153
Iteration 180/1000 | Loss: 0.00004153
Iteration 181/1000 | Loss: 0.00004153
Iteration 182/1000 | Loss: 0.00004153
Iteration 183/1000 | Loss: 0.00004153
Iteration 184/1000 | Loss: 0.00004153
Iteration 185/1000 | Loss: 0.00004153
Iteration 186/1000 | Loss: 0.00004153
Iteration 187/1000 | Loss: 0.00004153
Iteration 188/1000 | Loss: 0.00004153
Iteration 189/1000 | Loss: 0.00004153
Iteration 190/1000 | Loss: 0.00004153
Iteration 191/1000 | Loss: 0.00004153
Iteration 192/1000 | Loss: 0.00004153
Iteration 193/1000 | Loss: 0.00004153
Iteration 194/1000 | Loss: 0.00004153
Iteration 195/1000 | Loss: 0.00004153
Iteration 196/1000 | Loss: 0.00004153
Iteration 197/1000 | Loss: 0.00004153
Iteration 198/1000 | Loss: 0.00004153
Iteration 199/1000 | Loss: 0.00004153
Iteration 200/1000 | Loss: 0.00004153
Iteration 201/1000 | Loss: 0.00004153
Iteration 202/1000 | Loss: 0.00004153
Iteration 203/1000 | Loss: 0.00004153
Iteration 204/1000 | Loss: 0.00004153
Iteration 205/1000 | Loss: 0.00004153
Iteration 206/1000 | Loss: 0.00004153
Iteration 207/1000 | Loss: 0.00004153
Iteration 208/1000 | Loss: 0.00004153
Iteration 209/1000 | Loss: 0.00004153
Iteration 210/1000 | Loss: 0.00004153
Iteration 211/1000 | Loss: 0.00004153
Iteration 212/1000 | Loss: 0.00004153
Iteration 213/1000 | Loss: 0.00004153
Iteration 214/1000 | Loss: 0.00004153
Iteration 215/1000 | Loss: 0.00004153
Iteration 216/1000 | Loss: 0.00004153
Iteration 217/1000 | Loss: 0.00004153
Iteration 218/1000 | Loss: 0.00004153
Iteration 219/1000 | Loss: 0.00004153
Iteration 220/1000 | Loss: 0.00004153
Iteration 221/1000 | Loss: 0.00004153
Iteration 222/1000 | Loss: 0.00004153
Iteration 223/1000 | Loss: 0.00004153
Iteration 224/1000 | Loss: 0.00004153
Iteration 225/1000 | Loss: 0.00004153
Iteration 226/1000 | Loss: 0.00004153
Iteration 227/1000 | Loss: 0.00004153
Iteration 228/1000 | Loss: 0.00004153
Iteration 229/1000 | Loss: 0.00004153
Iteration 230/1000 | Loss: 0.00004153
Iteration 231/1000 | Loss: 0.00004153
Iteration 232/1000 | Loss: 0.00004153
Iteration 233/1000 | Loss: 0.00004153
Iteration 234/1000 | Loss: 0.00004153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [4.1533967305440456e-05, 4.1533967305440456e-05, 4.1533967305440456e-05, 4.1533967305440456e-05, 4.1533967305440456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1533967305440456e-05

Optimization complete. Final v2v error: 5.376988410949707 mm

Highest mean error: 6.250786304473877 mm for frame 224

Lowest mean error: 4.636031150817871 mm for frame 0

Saving results

Total time: 71.40053510665894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470573
Iteration 2/25 | Loss: 0.00131089
Iteration 3/25 | Loss: 0.00096195
Iteration 4/25 | Loss: 0.00094072
Iteration 5/25 | Loss: 0.00093488
Iteration 6/25 | Loss: 0.00093330
Iteration 7/25 | Loss: 0.00093327
Iteration 8/25 | Loss: 0.00093327
Iteration 9/25 | Loss: 0.00093327
Iteration 10/25 | Loss: 0.00093327
Iteration 11/25 | Loss: 0.00093327
Iteration 12/25 | Loss: 0.00093327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009332738700322807, 0.0009332738700322807, 0.0009332738700322807, 0.0009332738700322807, 0.0009332738700322807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009332738700322807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33179533
Iteration 2/25 | Loss: 0.00088246
Iteration 3/25 | Loss: 0.00088246
Iteration 4/25 | Loss: 0.00088246
Iteration 5/25 | Loss: 0.00088246
Iteration 6/25 | Loss: 0.00088246
Iteration 7/25 | Loss: 0.00088246
Iteration 8/25 | Loss: 0.00088246
Iteration 9/25 | Loss: 0.00088246
Iteration 10/25 | Loss: 0.00088246
Iteration 11/25 | Loss: 0.00088246
Iteration 12/25 | Loss: 0.00088246
Iteration 13/25 | Loss: 0.00088246
Iteration 14/25 | Loss: 0.00088246
Iteration 15/25 | Loss: 0.00088246
Iteration 16/25 | Loss: 0.00088246
Iteration 17/25 | Loss: 0.00088246
Iteration 18/25 | Loss: 0.00088246
Iteration 19/25 | Loss: 0.00088246
Iteration 20/25 | Loss: 0.00088246
Iteration 21/25 | Loss: 0.00088246
Iteration 22/25 | Loss: 0.00088246
Iteration 23/25 | Loss: 0.00088246
Iteration 24/25 | Loss: 0.00088246
Iteration 25/25 | Loss: 0.00088246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088246
Iteration 2/1000 | Loss: 0.00003865
Iteration 3/1000 | Loss: 0.00002519
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001941
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001863
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001824
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001793
Iteration 13/1000 | Loss: 0.00001793
Iteration 14/1000 | Loss: 0.00001793
Iteration 15/1000 | Loss: 0.00001792
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001790
Iteration 22/1000 | Loss: 0.00001789
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001786
Iteration 38/1000 | Loss: 0.00001786
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001776
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001775
Iteration 91/1000 | Loss: 0.00001775
Iteration 92/1000 | Loss: 0.00001775
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001773
Iteration 102/1000 | Loss: 0.00001773
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001773
Iteration 107/1000 | Loss: 0.00001773
Iteration 108/1000 | Loss: 0.00001772
Iteration 109/1000 | Loss: 0.00001772
Iteration 110/1000 | Loss: 0.00001772
Iteration 111/1000 | Loss: 0.00001772
Iteration 112/1000 | Loss: 0.00001772
Iteration 113/1000 | Loss: 0.00001772
Iteration 114/1000 | Loss: 0.00001772
Iteration 115/1000 | Loss: 0.00001772
Iteration 116/1000 | Loss: 0.00001772
Iteration 117/1000 | Loss: 0.00001772
Iteration 118/1000 | Loss: 0.00001772
Iteration 119/1000 | Loss: 0.00001772
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001771
Iteration 123/1000 | Loss: 0.00001771
Iteration 124/1000 | Loss: 0.00001771
Iteration 125/1000 | Loss: 0.00001771
Iteration 126/1000 | Loss: 0.00001771
Iteration 127/1000 | Loss: 0.00001771
Iteration 128/1000 | Loss: 0.00001771
Iteration 129/1000 | Loss: 0.00001771
Iteration 130/1000 | Loss: 0.00001771
Iteration 131/1000 | Loss: 0.00001771
Iteration 132/1000 | Loss: 0.00001771
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00001771
Iteration 135/1000 | Loss: 0.00001770
Iteration 136/1000 | Loss: 0.00001770
Iteration 137/1000 | Loss: 0.00001770
Iteration 138/1000 | Loss: 0.00001770
Iteration 139/1000 | Loss: 0.00001770
Iteration 140/1000 | Loss: 0.00001770
Iteration 141/1000 | Loss: 0.00001770
Iteration 142/1000 | Loss: 0.00001770
Iteration 143/1000 | Loss: 0.00001770
Iteration 144/1000 | Loss: 0.00001770
Iteration 145/1000 | Loss: 0.00001770
Iteration 146/1000 | Loss: 0.00001770
Iteration 147/1000 | Loss: 0.00001770
Iteration 148/1000 | Loss: 0.00001770
Iteration 149/1000 | Loss: 0.00001770
Iteration 150/1000 | Loss: 0.00001770
Iteration 151/1000 | Loss: 0.00001770
Iteration 152/1000 | Loss: 0.00001770
Iteration 153/1000 | Loss: 0.00001770
Iteration 154/1000 | Loss: 0.00001770
Iteration 155/1000 | Loss: 0.00001770
Iteration 156/1000 | Loss: 0.00001770
Iteration 157/1000 | Loss: 0.00001770
Iteration 158/1000 | Loss: 0.00001770
Iteration 159/1000 | Loss: 0.00001770
Iteration 160/1000 | Loss: 0.00001770
Iteration 161/1000 | Loss: 0.00001770
Iteration 162/1000 | Loss: 0.00001770
Iteration 163/1000 | Loss: 0.00001770
Iteration 164/1000 | Loss: 0.00001770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7697233488433994e-05, 1.7697233488433994e-05, 1.7697233488433994e-05, 1.7697233488433994e-05, 1.7697233488433994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7697233488433994e-05

Optimization complete. Final v2v error: 3.5570790767669678 mm

Highest mean error: 4.292380332946777 mm for frame 23

Lowest mean error: 3.0169122219085693 mm for frame 102

Saving results

Total time: 35.59027314186096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840705
Iteration 2/25 | Loss: 0.00104284
Iteration 3/25 | Loss: 0.00096119
Iteration 4/25 | Loss: 0.00094157
Iteration 5/25 | Loss: 0.00093315
Iteration 6/25 | Loss: 0.00093167
Iteration 7/25 | Loss: 0.00093147
Iteration 8/25 | Loss: 0.00093147
Iteration 9/25 | Loss: 0.00093147
Iteration 10/25 | Loss: 0.00093147
Iteration 11/25 | Loss: 0.00093147
Iteration 12/25 | Loss: 0.00093147
Iteration 13/25 | Loss: 0.00093146
Iteration 14/25 | Loss: 0.00093146
Iteration 15/25 | Loss: 0.00093146
Iteration 16/25 | Loss: 0.00093146
Iteration 17/25 | Loss: 0.00093146
Iteration 18/25 | Loss: 0.00093146
Iteration 19/25 | Loss: 0.00093146
Iteration 20/25 | Loss: 0.00093146
Iteration 21/25 | Loss: 0.00093146
Iteration 22/25 | Loss: 0.00093146
Iteration 23/25 | Loss: 0.00093146
Iteration 24/25 | Loss: 0.00093146
Iteration 25/25 | Loss: 0.00093146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29622030
Iteration 2/25 | Loss: 0.00089684
Iteration 3/25 | Loss: 0.00089684
Iteration 4/25 | Loss: 0.00089684
Iteration 5/25 | Loss: 0.00089684
Iteration 6/25 | Loss: 0.00089684
Iteration 7/25 | Loss: 0.00089684
Iteration 8/25 | Loss: 0.00089684
Iteration 9/25 | Loss: 0.00089684
Iteration 10/25 | Loss: 0.00089684
Iteration 11/25 | Loss: 0.00089684
Iteration 12/25 | Loss: 0.00089684
Iteration 13/25 | Loss: 0.00089684
Iteration 14/25 | Loss: 0.00089684
Iteration 15/25 | Loss: 0.00089684
Iteration 16/25 | Loss: 0.00089684
Iteration 17/25 | Loss: 0.00089684
Iteration 18/25 | Loss: 0.00089684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008968375041149557, 0.0008968375041149557, 0.0008968375041149557, 0.0008968375041149557, 0.0008968375041149557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008968375041149557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089684
Iteration 2/1000 | Loss: 0.00004357
Iteration 3/1000 | Loss: 0.00002481
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001772
Iteration 9/1000 | Loss: 0.00001760
Iteration 10/1000 | Loss: 0.00001750
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00001745
Iteration 13/1000 | Loss: 0.00001744
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001740
Iteration 16/1000 | Loss: 0.00001739
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001729
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001725
Iteration 25/1000 | Loss: 0.00001723
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001722
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001718
Iteration 34/1000 | Loss: 0.00001718
Iteration 35/1000 | Loss: 0.00001718
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001717
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001713
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001713
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.7124670193879865e-05, 1.7124670193879865e-05, 1.7124670193879865e-05, 1.7124670193879865e-05, 1.7124670193879865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7124670193879865e-05

Optimization complete. Final v2v error: 3.578331470489502 mm

Highest mean error: 3.713745594024658 mm for frame 99

Lowest mean error: 3.4780824184417725 mm for frame 44

Saving results

Total time: 28.313737869262695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753972
Iteration 2/25 | Loss: 0.00115544
Iteration 3/25 | Loss: 0.00101835
Iteration 4/25 | Loss: 0.00100682
Iteration 5/25 | Loss: 0.00100202
Iteration 6/25 | Loss: 0.00100111
Iteration 7/25 | Loss: 0.00100111
Iteration 8/25 | Loss: 0.00100111
Iteration 9/25 | Loss: 0.00100111
Iteration 10/25 | Loss: 0.00100111
Iteration 11/25 | Loss: 0.00100111
Iteration 12/25 | Loss: 0.00100111
Iteration 13/25 | Loss: 0.00100111
Iteration 14/25 | Loss: 0.00100111
Iteration 15/25 | Loss: 0.00100111
Iteration 16/25 | Loss: 0.00100111
Iteration 17/25 | Loss: 0.00100111
Iteration 18/25 | Loss: 0.00100111
Iteration 19/25 | Loss: 0.00100111
Iteration 20/25 | Loss: 0.00100111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010011106496676803, 0.0010011106496676803, 0.0010011106496676803, 0.0010011106496676803, 0.0010011106496676803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010011106496676803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28656292
Iteration 2/25 | Loss: 0.00092940
Iteration 3/25 | Loss: 0.00092940
Iteration 4/25 | Loss: 0.00092940
Iteration 5/25 | Loss: 0.00092940
Iteration 6/25 | Loss: 0.00092940
Iteration 7/25 | Loss: 0.00092940
Iteration 8/25 | Loss: 0.00092940
Iteration 9/25 | Loss: 0.00092940
Iteration 10/25 | Loss: 0.00092940
Iteration 11/25 | Loss: 0.00092940
Iteration 12/25 | Loss: 0.00092940
Iteration 13/25 | Loss: 0.00092940
Iteration 14/25 | Loss: 0.00092940
Iteration 15/25 | Loss: 0.00092940
Iteration 16/25 | Loss: 0.00092940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009293990442529321, 0.0009293990442529321, 0.0009293990442529321, 0.0009293990442529321, 0.0009293990442529321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009293990442529321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092940
Iteration 2/1000 | Loss: 0.00004522
Iteration 3/1000 | Loss: 0.00002866
Iteration 4/1000 | Loss: 0.00002597
Iteration 5/1000 | Loss: 0.00002458
Iteration 6/1000 | Loss: 0.00002364
Iteration 7/1000 | Loss: 0.00002318
Iteration 8/1000 | Loss: 0.00002270
Iteration 9/1000 | Loss: 0.00002236
Iteration 10/1000 | Loss: 0.00002201
Iteration 11/1000 | Loss: 0.00002181
Iteration 12/1000 | Loss: 0.00002164
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002148
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002140
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002134
Iteration 20/1000 | Loss: 0.00002134
Iteration 21/1000 | Loss: 0.00002133
Iteration 22/1000 | Loss: 0.00002132
Iteration 23/1000 | Loss: 0.00002131
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002129
Iteration 26/1000 | Loss: 0.00002129
Iteration 27/1000 | Loss: 0.00002129
Iteration 28/1000 | Loss: 0.00002129
Iteration 29/1000 | Loss: 0.00002129
Iteration 30/1000 | Loss: 0.00002128
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002128
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002127
Iteration 36/1000 | Loss: 0.00002127
Iteration 37/1000 | Loss: 0.00002126
Iteration 38/1000 | Loss: 0.00002126
Iteration 39/1000 | Loss: 0.00002126
Iteration 40/1000 | Loss: 0.00002126
Iteration 41/1000 | Loss: 0.00002125
Iteration 42/1000 | Loss: 0.00002125
Iteration 43/1000 | Loss: 0.00002125
Iteration 44/1000 | Loss: 0.00002125
Iteration 45/1000 | Loss: 0.00002124
Iteration 46/1000 | Loss: 0.00002124
Iteration 47/1000 | Loss: 0.00002124
Iteration 48/1000 | Loss: 0.00002124
Iteration 49/1000 | Loss: 0.00002124
Iteration 50/1000 | Loss: 0.00002124
Iteration 51/1000 | Loss: 0.00002124
Iteration 52/1000 | Loss: 0.00002124
Iteration 53/1000 | Loss: 0.00002124
Iteration 54/1000 | Loss: 0.00002124
Iteration 55/1000 | Loss: 0.00002124
Iteration 56/1000 | Loss: 0.00002124
Iteration 57/1000 | Loss: 0.00002123
Iteration 58/1000 | Loss: 0.00002123
Iteration 59/1000 | Loss: 0.00002123
Iteration 60/1000 | Loss: 0.00002123
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002122
Iteration 71/1000 | Loss: 0.00002122
Iteration 72/1000 | Loss: 0.00002122
Iteration 73/1000 | Loss: 0.00002122
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002121
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00002120
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002120
Iteration 90/1000 | Loss: 0.00002120
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002120
Iteration 97/1000 | Loss: 0.00002120
Iteration 98/1000 | Loss: 0.00002120
Iteration 99/1000 | Loss: 0.00002120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.119986129400786e-05, 2.119986129400786e-05, 2.119986129400786e-05, 2.119986129400786e-05, 2.119986129400786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.119986129400786e-05

Optimization complete. Final v2v error: 4.035437107086182 mm

Highest mean error: 5.336476802825928 mm for frame 239

Lowest mean error: 3.497025966644287 mm for frame 15

Saving results

Total time: 39.63410401344299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844555
Iteration 2/25 | Loss: 0.00151678
Iteration 3/25 | Loss: 0.00120348
Iteration 4/25 | Loss: 0.00111742
Iteration 5/25 | Loss: 0.00113131
Iteration 6/25 | Loss: 0.00114274
Iteration 7/25 | Loss: 0.00107051
Iteration 8/25 | Loss: 0.00104439
Iteration 9/25 | Loss: 0.00103055
Iteration 10/25 | Loss: 0.00102720
Iteration 11/25 | Loss: 0.00102660
Iteration 12/25 | Loss: 0.00102650
Iteration 13/25 | Loss: 0.00102646
Iteration 14/25 | Loss: 0.00102646
Iteration 15/25 | Loss: 0.00102646
Iteration 16/25 | Loss: 0.00102646
Iteration 17/25 | Loss: 0.00102646
Iteration 18/25 | Loss: 0.00102645
Iteration 19/25 | Loss: 0.00102645
Iteration 20/25 | Loss: 0.00102645
Iteration 21/25 | Loss: 0.00102645
Iteration 22/25 | Loss: 0.00102645
Iteration 23/25 | Loss: 0.00102645
Iteration 24/25 | Loss: 0.00102645
Iteration 25/25 | Loss: 0.00102645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24128509
Iteration 2/25 | Loss: 0.00132303
Iteration 3/25 | Loss: 0.00132302
Iteration 4/25 | Loss: 0.00132302
Iteration 5/25 | Loss: 0.00132302
Iteration 6/25 | Loss: 0.00132302
Iteration 7/25 | Loss: 0.00132302
Iteration 8/25 | Loss: 0.00132302
Iteration 9/25 | Loss: 0.00132302
Iteration 10/25 | Loss: 0.00132302
Iteration 11/25 | Loss: 0.00132302
Iteration 12/25 | Loss: 0.00132302
Iteration 13/25 | Loss: 0.00132302
Iteration 14/25 | Loss: 0.00132302
Iteration 15/25 | Loss: 0.00132302
Iteration 16/25 | Loss: 0.00132302
Iteration 17/25 | Loss: 0.00132302
Iteration 18/25 | Loss: 0.00132302
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013230193872004747, 0.0013230193872004747, 0.0013230193872004747, 0.0013230193872004747, 0.0013230193872004747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013230193872004747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132302
Iteration 2/1000 | Loss: 0.00007935
Iteration 3/1000 | Loss: 0.00005549
Iteration 4/1000 | Loss: 0.00004264
Iteration 5/1000 | Loss: 0.00003776
Iteration 6/1000 | Loss: 0.00003520
Iteration 7/1000 | Loss: 0.00003383
Iteration 8/1000 | Loss: 0.00003249
Iteration 9/1000 | Loss: 0.00003154
Iteration 10/1000 | Loss: 0.00003097
Iteration 11/1000 | Loss: 0.00003066
Iteration 12/1000 | Loss: 0.00003042
Iteration 13/1000 | Loss: 0.00003036
Iteration 14/1000 | Loss: 0.00003019
Iteration 15/1000 | Loss: 0.00003009
Iteration 16/1000 | Loss: 0.00003003
Iteration 17/1000 | Loss: 0.00003003
Iteration 18/1000 | Loss: 0.00003002
Iteration 19/1000 | Loss: 0.00003001
Iteration 20/1000 | Loss: 0.00003000
Iteration 21/1000 | Loss: 0.00002999
Iteration 22/1000 | Loss: 0.00002998
Iteration 23/1000 | Loss: 0.00002997
Iteration 24/1000 | Loss: 0.00002996
Iteration 25/1000 | Loss: 0.00002993
Iteration 26/1000 | Loss: 0.00002993
Iteration 27/1000 | Loss: 0.00002993
Iteration 28/1000 | Loss: 0.00002993
Iteration 29/1000 | Loss: 0.00002993
Iteration 30/1000 | Loss: 0.00002992
Iteration 31/1000 | Loss: 0.00002992
Iteration 32/1000 | Loss: 0.00002988
Iteration 33/1000 | Loss: 0.00002988
Iteration 34/1000 | Loss: 0.00002987
Iteration 35/1000 | Loss: 0.00002983
Iteration 36/1000 | Loss: 0.00002978
Iteration 37/1000 | Loss: 0.00002977
Iteration 38/1000 | Loss: 0.00002976
Iteration 39/1000 | Loss: 0.00002966
Iteration 40/1000 | Loss: 0.00002951
Iteration 41/1000 | Loss: 0.00002941
Iteration 42/1000 | Loss: 0.00002932
Iteration 43/1000 | Loss: 0.00002931
Iteration 44/1000 | Loss: 0.00002930
Iteration 45/1000 | Loss: 0.00002927
Iteration 46/1000 | Loss: 0.00002927
Iteration 47/1000 | Loss: 0.00002927
Iteration 48/1000 | Loss: 0.00002926
Iteration 49/1000 | Loss: 0.00002926
Iteration 50/1000 | Loss: 0.00002926
Iteration 51/1000 | Loss: 0.00002925
Iteration 52/1000 | Loss: 0.00002925
Iteration 53/1000 | Loss: 0.00002925
Iteration 54/1000 | Loss: 0.00002925
Iteration 55/1000 | Loss: 0.00002924
Iteration 56/1000 | Loss: 0.00002923
Iteration 57/1000 | Loss: 0.00002923
Iteration 58/1000 | Loss: 0.00002921
Iteration 59/1000 | Loss: 0.00002920
Iteration 60/1000 | Loss: 0.00002907
Iteration 61/1000 | Loss: 0.00002907
Iteration 62/1000 | Loss: 0.00002906
Iteration 63/1000 | Loss: 0.00002904
Iteration 64/1000 | Loss: 0.00002904
Iteration 65/1000 | Loss: 0.00002903
Iteration 66/1000 | Loss: 0.00030782
Iteration 67/1000 | Loss: 0.00041464
Iteration 68/1000 | Loss: 0.00018420
Iteration 69/1000 | Loss: 0.00004959
Iteration 70/1000 | Loss: 0.00004132
Iteration 71/1000 | Loss: 0.00004738
Iteration 72/1000 | Loss: 0.00003487
Iteration 73/1000 | Loss: 0.00003345
Iteration 74/1000 | Loss: 0.00003260
Iteration 75/1000 | Loss: 0.00003179
Iteration 76/1000 | Loss: 0.00003139
Iteration 77/1000 | Loss: 0.00003091
Iteration 78/1000 | Loss: 0.00003065
Iteration 79/1000 | Loss: 0.00003040
Iteration 80/1000 | Loss: 0.00036822
Iteration 81/1000 | Loss: 0.00004065
Iteration 82/1000 | Loss: 0.00003613
Iteration 83/1000 | Loss: 0.00005476
Iteration 84/1000 | Loss: 0.00003301
Iteration 85/1000 | Loss: 0.00003072
Iteration 86/1000 | Loss: 0.00003004
Iteration 87/1000 | Loss: 0.00002957
Iteration 88/1000 | Loss: 0.00002921
Iteration 89/1000 | Loss: 0.00002873
Iteration 90/1000 | Loss: 0.00002840
Iteration 91/1000 | Loss: 0.00002823
Iteration 92/1000 | Loss: 0.00002812
Iteration 93/1000 | Loss: 0.00002809
Iteration 94/1000 | Loss: 0.00002809
Iteration 95/1000 | Loss: 0.00002804
Iteration 96/1000 | Loss: 0.00002804
Iteration 97/1000 | Loss: 0.00002804
Iteration 98/1000 | Loss: 0.00002804
Iteration 99/1000 | Loss: 0.00002804
Iteration 100/1000 | Loss: 0.00002804
Iteration 101/1000 | Loss: 0.00002803
Iteration 102/1000 | Loss: 0.00002802
Iteration 103/1000 | Loss: 0.00002801
Iteration 104/1000 | Loss: 0.00002801
Iteration 105/1000 | Loss: 0.00002800
Iteration 106/1000 | Loss: 0.00002800
Iteration 107/1000 | Loss: 0.00002800
Iteration 108/1000 | Loss: 0.00002800
Iteration 109/1000 | Loss: 0.00002800
Iteration 110/1000 | Loss: 0.00002800
Iteration 111/1000 | Loss: 0.00002800
Iteration 112/1000 | Loss: 0.00002800
Iteration 113/1000 | Loss: 0.00002800
Iteration 114/1000 | Loss: 0.00002799
Iteration 115/1000 | Loss: 0.00002799
Iteration 116/1000 | Loss: 0.00002798
Iteration 117/1000 | Loss: 0.00002798
Iteration 118/1000 | Loss: 0.00002797
Iteration 119/1000 | Loss: 0.00002797
Iteration 120/1000 | Loss: 0.00002797
Iteration 121/1000 | Loss: 0.00002797
Iteration 122/1000 | Loss: 0.00002796
Iteration 123/1000 | Loss: 0.00002796
Iteration 124/1000 | Loss: 0.00002796
Iteration 125/1000 | Loss: 0.00002795
Iteration 126/1000 | Loss: 0.00002795
Iteration 127/1000 | Loss: 0.00002795
Iteration 128/1000 | Loss: 0.00002795
Iteration 129/1000 | Loss: 0.00002795
Iteration 130/1000 | Loss: 0.00002795
Iteration 131/1000 | Loss: 0.00002795
Iteration 132/1000 | Loss: 0.00002795
Iteration 133/1000 | Loss: 0.00002795
Iteration 134/1000 | Loss: 0.00002795
Iteration 135/1000 | Loss: 0.00002795
Iteration 136/1000 | Loss: 0.00002795
Iteration 137/1000 | Loss: 0.00002795
Iteration 138/1000 | Loss: 0.00002795
Iteration 139/1000 | Loss: 0.00002795
Iteration 140/1000 | Loss: 0.00002795
Iteration 141/1000 | Loss: 0.00002795
Iteration 142/1000 | Loss: 0.00002795
Iteration 143/1000 | Loss: 0.00002795
Iteration 144/1000 | Loss: 0.00002795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.7945863621425815e-05, 2.7945863621425815e-05, 2.7945863621425815e-05, 2.7945863621425815e-05, 2.7945863621425815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7945863621425815e-05

Optimization complete. Final v2v error: 4.435120105743408 mm

Highest mean error: 4.795767307281494 mm for frame 79

Lowest mean error: 4.178309440612793 mm for frame 122

Saving results

Total time: 94.98444032669067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499830
Iteration 2/25 | Loss: 0.00130208
Iteration 3/25 | Loss: 0.00100883
Iteration 4/25 | Loss: 0.00097201
Iteration 5/25 | Loss: 0.00096542
Iteration 6/25 | Loss: 0.00096361
Iteration 7/25 | Loss: 0.00096326
Iteration 8/25 | Loss: 0.00096326
Iteration 9/25 | Loss: 0.00096326
Iteration 10/25 | Loss: 0.00096326
Iteration 11/25 | Loss: 0.00096326
Iteration 12/25 | Loss: 0.00096326
Iteration 13/25 | Loss: 0.00096326
Iteration 14/25 | Loss: 0.00096326
Iteration 15/25 | Loss: 0.00096326
Iteration 16/25 | Loss: 0.00096326
Iteration 17/25 | Loss: 0.00096326
Iteration 18/25 | Loss: 0.00096326
Iteration 19/25 | Loss: 0.00096326
Iteration 20/25 | Loss: 0.00096326
Iteration 21/25 | Loss: 0.00096326
Iteration 22/25 | Loss: 0.00096326
Iteration 23/25 | Loss: 0.00096326
Iteration 24/25 | Loss: 0.00096326
Iteration 25/25 | Loss: 0.00096326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39406264
Iteration 2/25 | Loss: 0.00068615
Iteration 3/25 | Loss: 0.00068612
Iteration 4/25 | Loss: 0.00068612
Iteration 5/25 | Loss: 0.00068612
Iteration 6/25 | Loss: 0.00068612
Iteration 7/25 | Loss: 0.00068612
Iteration 8/25 | Loss: 0.00068612
Iteration 9/25 | Loss: 0.00068612
Iteration 10/25 | Loss: 0.00068612
Iteration 11/25 | Loss: 0.00068612
Iteration 12/25 | Loss: 0.00068612
Iteration 13/25 | Loss: 0.00068612
Iteration 14/25 | Loss: 0.00068612
Iteration 15/25 | Loss: 0.00068612
Iteration 16/25 | Loss: 0.00068612
Iteration 17/25 | Loss: 0.00068612
Iteration 18/25 | Loss: 0.00068612
Iteration 19/25 | Loss: 0.00068612
Iteration 20/25 | Loss: 0.00068612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006861197180114686, 0.0006861197180114686, 0.0006861197180114686, 0.0006861197180114686, 0.0006861197180114686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006861197180114686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068612
Iteration 2/1000 | Loss: 0.00004965
Iteration 3/1000 | Loss: 0.00003325
Iteration 4/1000 | Loss: 0.00002751
Iteration 5/1000 | Loss: 0.00002511
Iteration 6/1000 | Loss: 0.00002343
Iteration 7/1000 | Loss: 0.00002269
Iteration 8/1000 | Loss: 0.00002240
Iteration 9/1000 | Loss: 0.00002208
Iteration 10/1000 | Loss: 0.00002187
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002156
Iteration 15/1000 | Loss: 0.00002149
Iteration 16/1000 | Loss: 0.00002147
Iteration 17/1000 | Loss: 0.00002146
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002136
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002135
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002133
Iteration 24/1000 | Loss: 0.00002132
Iteration 25/1000 | Loss: 0.00002132
Iteration 26/1000 | Loss: 0.00002132
Iteration 27/1000 | Loss: 0.00002132
Iteration 28/1000 | Loss: 0.00002132
Iteration 29/1000 | Loss: 0.00002132
Iteration 30/1000 | Loss: 0.00002132
Iteration 31/1000 | Loss: 0.00002131
Iteration 32/1000 | Loss: 0.00002131
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002129
Iteration 37/1000 | Loss: 0.00002129
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002128
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002128
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002127
Iteration 49/1000 | Loss: 0.00002127
Iteration 50/1000 | Loss: 0.00002127
Iteration 51/1000 | Loss: 0.00002127
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002126
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002126
Iteration 60/1000 | Loss: 0.00002126
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002125
Iteration 63/1000 | Loss: 0.00002125
Iteration 64/1000 | Loss: 0.00002125
Iteration 65/1000 | Loss: 0.00002125
Iteration 66/1000 | Loss: 0.00002125
Iteration 67/1000 | Loss: 0.00002125
Iteration 68/1000 | Loss: 0.00002124
Iteration 69/1000 | Loss: 0.00002124
Iteration 70/1000 | Loss: 0.00002124
Iteration 71/1000 | Loss: 0.00002124
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002123
Iteration 79/1000 | Loss: 0.00002123
Iteration 80/1000 | Loss: 0.00002123
Iteration 81/1000 | Loss: 0.00002123
Iteration 82/1000 | Loss: 0.00002123
Iteration 83/1000 | Loss: 0.00002122
Iteration 84/1000 | Loss: 0.00002122
Iteration 85/1000 | Loss: 0.00002122
Iteration 86/1000 | Loss: 0.00002122
Iteration 87/1000 | Loss: 0.00002122
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002119
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002119
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002118
Iteration 104/1000 | Loss: 0.00002118
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00002118
Iteration 107/1000 | Loss: 0.00002118
Iteration 108/1000 | Loss: 0.00002118
Iteration 109/1000 | Loss: 0.00002118
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002118
Iteration 112/1000 | Loss: 0.00002118
Iteration 113/1000 | Loss: 0.00002118
Iteration 114/1000 | Loss: 0.00002118
Iteration 115/1000 | Loss: 0.00002118
Iteration 116/1000 | Loss: 0.00002118
Iteration 117/1000 | Loss: 0.00002118
Iteration 118/1000 | Loss: 0.00002118
Iteration 119/1000 | Loss: 0.00002118
Iteration 120/1000 | Loss: 0.00002118
Iteration 121/1000 | Loss: 0.00002118
Iteration 122/1000 | Loss: 0.00002118
Iteration 123/1000 | Loss: 0.00002118
Iteration 124/1000 | Loss: 0.00002118
Iteration 125/1000 | Loss: 0.00002118
Iteration 126/1000 | Loss: 0.00002118
Iteration 127/1000 | Loss: 0.00002118
Iteration 128/1000 | Loss: 0.00002118
Iteration 129/1000 | Loss: 0.00002118
Iteration 130/1000 | Loss: 0.00002118
Iteration 131/1000 | Loss: 0.00002118
Iteration 132/1000 | Loss: 0.00002118
Iteration 133/1000 | Loss: 0.00002118
Iteration 134/1000 | Loss: 0.00002118
Iteration 135/1000 | Loss: 0.00002118
Iteration 136/1000 | Loss: 0.00002118
Iteration 137/1000 | Loss: 0.00002118
Iteration 138/1000 | Loss: 0.00002118
Iteration 139/1000 | Loss: 0.00002118
Iteration 140/1000 | Loss: 0.00002118
Iteration 141/1000 | Loss: 0.00002118
Iteration 142/1000 | Loss: 0.00002118
Iteration 143/1000 | Loss: 0.00002118
Iteration 144/1000 | Loss: 0.00002118
Iteration 145/1000 | Loss: 0.00002118
Iteration 146/1000 | Loss: 0.00002118
Iteration 147/1000 | Loss: 0.00002118
Iteration 148/1000 | Loss: 0.00002118
Iteration 149/1000 | Loss: 0.00002118
Iteration 150/1000 | Loss: 0.00002118
Iteration 151/1000 | Loss: 0.00002118
Iteration 152/1000 | Loss: 0.00002118
Iteration 153/1000 | Loss: 0.00002118
Iteration 154/1000 | Loss: 0.00002118
Iteration 155/1000 | Loss: 0.00002118
Iteration 156/1000 | Loss: 0.00002118
Iteration 157/1000 | Loss: 0.00002118
Iteration 158/1000 | Loss: 0.00002118
Iteration 159/1000 | Loss: 0.00002118
Iteration 160/1000 | Loss: 0.00002118
Iteration 161/1000 | Loss: 0.00002118
Iteration 162/1000 | Loss: 0.00002118
Iteration 163/1000 | Loss: 0.00002118
Iteration 164/1000 | Loss: 0.00002118
Iteration 165/1000 | Loss: 0.00002118
Iteration 166/1000 | Loss: 0.00002118
Iteration 167/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.1175777874304913e-05, 2.1175777874304913e-05, 2.1175777874304913e-05, 2.1175777874304913e-05, 2.1175777874304913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1175777874304913e-05

Optimization complete. Final v2v error: 4.059731483459473 mm

Highest mean error: 4.486017227172852 mm for frame 68

Lowest mean error: 3.410409688949585 mm for frame 94

Saving results

Total time: 37.353485107421875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430952
Iteration 2/25 | Loss: 0.00113979
Iteration 3/25 | Loss: 0.00093847
Iteration 4/25 | Loss: 0.00091625
Iteration 5/25 | Loss: 0.00091026
Iteration 6/25 | Loss: 0.00090903
Iteration 7/25 | Loss: 0.00090872
Iteration 8/25 | Loss: 0.00090871
Iteration 9/25 | Loss: 0.00090871
Iteration 10/25 | Loss: 0.00090871
Iteration 11/25 | Loss: 0.00090871
Iteration 12/25 | Loss: 0.00090871
Iteration 13/25 | Loss: 0.00090871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009087130310945213, 0.0009087130310945213, 0.0009087130310945213, 0.0009087130310945213, 0.0009087130310945213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009087130310945213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31209266
Iteration 2/25 | Loss: 0.00076798
Iteration 3/25 | Loss: 0.00076798
Iteration 4/25 | Loss: 0.00076798
Iteration 5/25 | Loss: 0.00076798
Iteration 6/25 | Loss: 0.00076798
Iteration 7/25 | Loss: 0.00076798
Iteration 8/25 | Loss: 0.00076798
Iteration 9/25 | Loss: 0.00076798
Iteration 10/25 | Loss: 0.00076798
Iteration 11/25 | Loss: 0.00076798
Iteration 12/25 | Loss: 0.00076798
Iteration 13/25 | Loss: 0.00076798
Iteration 14/25 | Loss: 0.00076798
Iteration 15/25 | Loss: 0.00076798
Iteration 16/25 | Loss: 0.00076798
Iteration 17/25 | Loss: 0.00076798
Iteration 18/25 | Loss: 0.00076798
Iteration 19/25 | Loss: 0.00076798
Iteration 20/25 | Loss: 0.00076798
Iteration 21/25 | Loss: 0.00076798
Iteration 22/25 | Loss: 0.00076798
Iteration 23/25 | Loss: 0.00076798
Iteration 24/25 | Loss: 0.00076798
Iteration 25/25 | Loss: 0.00076798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076798
Iteration 2/1000 | Loss: 0.00003949
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001744
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001638
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00001635
Iteration 16/1000 | Loss: 0.00001634
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001632
Iteration 19/1000 | Loss: 0.00001631
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001628
Iteration 22/1000 | Loss: 0.00001628
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001627
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001625
Iteration 33/1000 | Loss: 0.00001625
Iteration 34/1000 | Loss: 0.00001625
Iteration 35/1000 | Loss: 0.00001624
Iteration 36/1000 | Loss: 0.00001624
Iteration 37/1000 | Loss: 0.00001623
Iteration 38/1000 | Loss: 0.00001623
Iteration 39/1000 | Loss: 0.00001623
Iteration 40/1000 | Loss: 0.00001623
Iteration 41/1000 | Loss: 0.00001622
Iteration 42/1000 | Loss: 0.00001622
Iteration 43/1000 | Loss: 0.00001622
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001621
Iteration 46/1000 | Loss: 0.00001621
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001620
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001619
Iteration 60/1000 | Loss: 0.00001619
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001618
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.6179867088794708e-05, 1.6179867088794708e-05, 1.6179867088794708e-05, 1.6179867088794708e-05, 1.6179867088794708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6179867088794708e-05

Optimization complete. Final v2v error: 3.446518898010254 mm

Highest mean error: 4.489630222320557 mm for frame 41

Lowest mean error: 3.0894391536712646 mm for frame 2

Saving results

Total time: 32.97653412818909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988594
Iteration 2/25 | Loss: 0.00390534
Iteration 3/25 | Loss: 0.00291375
Iteration 4/25 | Loss: 0.00231311
Iteration 5/25 | Loss: 0.00226308
Iteration 6/25 | Loss: 0.00232748
Iteration 7/25 | Loss: 0.00190054
Iteration 8/25 | Loss: 0.00185871
Iteration 9/25 | Loss: 0.00171560
Iteration 10/25 | Loss: 0.00164658
Iteration 11/25 | Loss: 0.00171517
Iteration 12/25 | Loss: 0.00161013
Iteration 13/25 | Loss: 0.00154813
Iteration 14/25 | Loss: 0.00152679
Iteration 15/25 | Loss: 0.00151692
Iteration 16/25 | Loss: 0.00151832
Iteration 17/25 | Loss: 0.00149502
Iteration 18/25 | Loss: 0.00149978
Iteration 19/25 | Loss: 0.00150428
Iteration 20/25 | Loss: 0.00148443
Iteration 21/25 | Loss: 0.00148567
Iteration 22/25 | Loss: 0.00147777
Iteration 23/25 | Loss: 0.00145942
Iteration 24/25 | Loss: 0.00146730
Iteration 25/25 | Loss: 0.00147018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.91726065
Iteration 2/25 | Loss: 0.01588448
Iteration 3/25 | Loss: 0.00934273
Iteration 4/25 | Loss: 0.00747244
Iteration 5/25 | Loss: 0.00747243
Iteration 6/25 | Loss: 0.00747243
Iteration 7/25 | Loss: 0.00747243
Iteration 8/25 | Loss: 0.00747243
Iteration 9/25 | Loss: 0.00747243
Iteration 10/25 | Loss: 0.00747243
Iteration 11/25 | Loss: 0.00747243
Iteration 12/25 | Loss: 0.00747243
Iteration 13/25 | Loss: 0.00747243
Iteration 14/25 | Loss: 0.00747243
Iteration 15/25 | Loss: 0.00747243
Iteration 16/25 | Loss: 0.00747243
Iteration 17/25 | Loss: 0.00747243
Iteration 18/25 | Loss: 0.00747243
Iteration 19/25 | Loss: 0.00747243
Iteration 20/25 | Loss: 0.00747243
Iteration 21/25 | Loss: 0.00747243
Iteration 22/25 | Loss: 0.00747243
Iteration 23/25 | Loss: 0.00747243
Iteration 24/25 | Loss: 0.00747243
Iteration 25/25 | Loss: 0.00747243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00747243
Iteration 2/1000 | Loss: 0.00511660
Iteration 3/1000 | Loss: 0.00300290
Iteration 4/1000 | Loss: 0.00596847
Iteration 5/1000 | Loss: 0.00463619
Iteration 6/1000 | Loss: 0.00336972
Iteration 7/1000 | Loss: 0.00741385
Iteration 8/1000 | Loss: 0.00296255
Iteration 9/1000 | Loss: 0.00420528
Iteration 10/1000 | Loss: 0.00265048
Iteration 11/1000 | Loss: 0.00236841
Iteration 12/1000 | Loss: 0.00416735
Iteration 13/1000 | Loss: 0.00255518
Iteration 14/1000 | Loss: 0.00288299
Iteration 15/1000 | Loss: 0.00315440
Iteration 16/1000 | Loss: 0.00521826
Iteration 17/1000 | Loss: 0.00254595
Iteration 18/1000 | Loss: 0.00489145
Iteration 19/1000 | Loss: 0.00314841
Iteration 20/1000 | Loss: 0.00422088
Iteration 21/1000 | Loss: 0.00232886
Iteration 22/1000 | Loss: 0.00799139
Iteration 23/1000 | Loss: 0.01047355
Iteration 24/1000 | Loss: 0.00415441
Iteration 25/1000 | Loss: 0.00432971
Iteration 26/1000 | Loss: 0.00610708
Iteration 27/1000 | Loss: 0.00551192
Iteration 28/1000 | Loss: 0.00294441
Iteration 29/1000 | Loss: 0.00555386
Iteration 30/1000 | Loss: 0.00412306
Iteration 31/1000 | Loss: 0.00425868
Iteration 32/1000 | Loss: 0.00292459
Iteration 33/1000 | Loss: 0.00225526
Iteration 34/1000 | Loss: 0.00439893
Iteration 35/1000 | Loss: 0.00599659
Iteration 36/1000 | Loss: 0.00482574
Iteration 37/1000 | Loss: 0.00306021
Iteration 38/1000 | Loss: 0.00377377
Iteration 39/1000 | Loss: 0.00194687
Iteration 40/1000 | Loss: 0.00366337
Iteration 41/1000 | Loss: 0.00402877
Iteration 42/1000 | Loss: 0.00582814
Iteration 43/1000 | Loss: 0.00369116
Iteration 44/1000 | Loss: 0.00460276
Iteration 45/1000 | Loss: 0.00299092
Iteration 46/1000 | Loss: 0.00268714
Iteration 47/1000 | Loss: 0.00366895
Iteration 48/1000 | Loss: 0.00254000
Iteration 49/1000 | Loss: 0.00267006
Iteration 50/1000 | Loss: 0.00478244
Iteration 51/1000 | Loss: 0.00364741
Iteration 52/1000 | Loss: 0.00430782
Iteration 53/1000 | Loss: 0.00284958
Iteration 54/1000 | Loss: 0.00443307
Iteration 55/1000 | Loss: 0.00438556
Iteration 56/1000 | Loss: 0.00450136
Iteration 57/1000 | Loss: 0.00220640
Iteration 58/1000 | Loss: 0.00442283
Iteration 59/1000 | Loss: 0.00306359
Iteration 60/1000 | Loss: 0.00329762
Iteration 61/1000 | Loss: 0.00389089
Iteration 62/1000 | Loss: 0.00347512
Iteration 63/1000 | Loss: 0.00480929
Iteration 64/1000 | Loss: 0.00342054
Iteration 65/1000 | Loss: 0.00413211
Iteration 66/1000 | Loss: 0.00431988
Iteration 67/1000 | Loss: 0.00362127
Iteration 68/1000 | Loss: 0.00371435
Iteration 69/1000 | Loss: 0.00571157
Iteration 70/1000 | Loss: 0.00341404
Iteration 71/1000 | Loss: 0.00376557
Iteration 72/1000 | Loss: 0.00388990
Iteration 73/1000 | Loss: 0.00430074
Iteration 74/1000 | Loss: 0.00298094
Iteration 75/1000 | Loss: 0.00217809
Iteration 76/1000 | Loss: 0.00334662
Iteration 77/1000 | Loss: 0.00172289
Iteration 78/1000 | Loss: 0.00214740
Iteration 79/1000 | Loss: 0.00232951
Iteration 80/1000 | Loss: 0.00263905
Iteration 81/1000 | Loss: 0.00211282
Iteration 82/1000 | Loss: 0.00218574
Iteration 83/1000 | Loss: 0.00178971
Iteration 84/1000 | Loss: 0.00145486
Iteration 85/1000 | Loss: 0.00429405
Iteration 86/1000 | Loss: 0.00278140
Iteration 87/1000 | Loss: 0.00168557
Iteration 88/1000 | Loss: 0.00315415
Iteration 89/1000 | Loss: 0.00213518
Iteration 90/1000 | Loss: 0.00172610
Iteration 91/1000 | Loss: 0.00183161
Iteration 92/1000 | Loss: 0.00149004
Iteration 93/1000 | Loss: 0.00165070
Iteration 94/1000 | Loss: 0.00220562
Iteration 95/1000 | Loss: 0.00192166
Iteration 96/1000 | Loss: 0.00507674
Iteration 97/1000 | Loss: 0.00220611
Iteration 98/1000 | Loss: 0.00176525
Iteration 99/1000 | Loss: 0.00368996
Iteration 100/1000 | Loss: 0.00218802
Iteration 101/1000 | Loss: 0.00127915
Iteration 102/1000 | Loss: 0.00156038
Iteration 103/1000 | Loss: 0.00154383
Iteration 104/1000 | Loss: 0.00231555
Iteration 105/1000 | Loss: 0.00268406
Iteration 106/1000 | Loss: 0.00234117
Iteration 107/1000 | Loss: 0.00140606
Iteration 108/1000 | Loss: 0.00215759
Iteration 109/1000 | Loss: 0.00337373
Iteration 110/1000 | Loss: 0.00238978
Iteration 111/1000 | Loss: 0.00053045
Iteration 112/1000 | Loss: 0.00108795
Iteration 113/1000 | Loss: 0.00101689
Iteration 114/1000 | Loss: 0.00088082
Iteration 115/1000 | Loss: 0.00082431
Iteration 116/1000 | Loss: 0.00042466
Iteration 117/1000 | Loss: 0.00106746
Iteration 118/1000 | Loss: 0.00072660
Iteration 119/1000 | Loss: 0.00093540
Iteration 120/1000 | Loss: 0.00103651
Iteration 121/1000 | Loss: 0.00130441
Iteration 122/1000 | Loss: 0.00130881
Iteration 123/1000 | Loss: 0.00121238
Iteration 124/1000 | Loss: 0.00298171
Iteration 125/1000 | Loss: 0.00090966
Iteration 126/1000 | Loss: 0.00064951
Iteration 127/1000 | Loss: 0.00164119
Iteration 128/1000 | Loss: 0.00033954
Iteration 129/1000 | Loss: 0.00021062
Iteration 130/1000 | Loss: 0.00079063
Iteration 131/1000 | Loss: 0.00169946
Iteration 132/1000 | Loss: 0.00035091
Iteration 133/1000 | Loss: 0.00178799
Iteration 134/1000 | Loss: 0.00022821
Iteration 135/1000 | Loss: 0.00014748
Iteration 136/1000 | Loss: 0.00148858
Iteration 137/1000 | Loss: 0.00019288
Iteration 138/1000 | Loss: 0.00080999
Iteration 139/1000 | Loss: 0.00032118
Iteration 140/1000 | Loss: 0.00050762
Iteration 141/1000 | Loss: 0.00035292
Iteration 142/1000 | Loss: 0.00066928
Iteration 143/1000 | Loss: 0.00050777
Iteration 144/1000 | Loss: 0.00033477
Iteration 145/1000 | Loss: 0.00015753
Iteration 146/1000 | Loss: 0.00051731
Iteration 147/1000 | Loss: 0.00034817
Iteration 148/1000 | Loss: 0.00046282
Iteration 149/1000 | Loss: 0.00033475
Iteration 150/1000 | Loss: 0.00023384
Iteration 151/1000 | Loss: 0.00038858
Iteration 152/1000 | Loss: 0.00047260
Iteration 153/1000 | Loss: 0.00068320
Iteration 154/1000 | Loss: 0.00019807
Iteration 155/1000 | Loss: 0.00022607
Iteration 156/1000 | Loss: 0.00019763
Iteration 157/1000 | Loss: 0.00012015
Iteration 158/1000 | Loss: 0.00011359
Iteration 159/1000 | Loss: 0.00041472
Iteration 160/1000 | Loss: 0.00060390
Iteration 161/1000 | Loss: 0.00046211
Iteration 162/1000 | Loss: 0.00034924
Iteration 163/1000 | Loss: 0.00028327
Iteration 164/1000 | Loss: 0.00025058
Iteration 165/1000 | Loss: 0.00024452
Iteration 166/1000 | Loss: 0.00040209
Iteration 167/1000 | Loss: 0.00031771
Iteration 168/1000 | Loss: 0.00018530
Iteration 169/1000 | Loss: 0.00056189
Iteration 170/1000 | Loss: 0.00031019
Iteration 171/1000 | Loss: 0.00131370
Iteration 172/1000 | Loss: 0.00070574
Iteration 173/1000 | Loss: 0.00083832
Iteration 174/1000 | Loss: 0.00424114
Iteration 175/1000 | Loss: 0.00291551
Iteration 176/1000 | Loss: 0.00195162
Iteration 177/1000 | Loss: 0.00067304
Iteration 178/1000 | Loss: 0.00278808
Iteration 179/1000 | Loss: 0.00680145
Iteration 180/1000 | Loss: 0.00756053
Iteration 181/1000 | Loss: 0.00577084
Iteration 182/1000 | Loss: 0.00409171
Iteration 183/1000 | Loss: 0.00240863
Iteration 184/1000 | Loss: 0.00186451
Iteration 185/1000 | Loss: 0.00245678
Iteration 186/1000 | Loss: 0.00123041
Iteration 187/1000 | Loss: 0.00122855
Iteration 188/1000 | Loss: 0.00209330
Iteration 189/1000 | Loss: 0.00172030
Iteration 190/1000 | Loss: 0.00212997
Iteration 191/1000 | Loss: 0.00277641
Iteration 192/1000 | Loss: 0.00404063
Iteration 193/1000 | Loss: 0.00338895
Iteration 194/1000 | Loss: 0.00191430
Iteration 195/1000 | Loss: 0.00157003
Iteration 196/1000 | Loss: 0.00197224
Iteration 197/1000 | Loss: 0.00183591
Iteration 198/1000 | Loss: 0.00187259
Iteration 199/1000 | Loss: 0.00158704
Iteration 200/1000 | Loss: 0.00171073
Iteration 201/1000 | Loss: 0.00148096
Iteration 202/1000 | Loss: 0.00134164
Iteration 203/1000 | Loss: 0.00240510
Iteration 204/1000 | Loss: 0.00136143
Iteration 205/1000 | Loss: 0.00273303
Iteration 206/1000 | Loss: 0.00282480
Iteration 207/1000 | Loss: 0.00263103
Iteration 208/1000 | Loss: 0.00280581
Iteration 209/1000 | Loss: 0.00169823
Iteration 210/1000 | Loss: 0.00254257
Iteration 211/1000 | Loss: 0.00299105
Iteration 212/1000 | Loss: 0.00230937
Iteration 213/1000 | Loss: 0.00185424
Iteration 214/1000 | Loss: 0.00121316
Iteration 215/1000 | Loss: 0.00189504
Iteration 216/1000 | Loss: 0.00159521
Iteration 217/1000 | Loss: 0.00119224
Iteration 218/1000 | Loss: 0.00371919
Iteration 219/1000 | Loss: 0.00137022
Iteration 220/1000 | Loss: 0.00082833
Iteration 221/1000 | Loss: 0.00072758
Iteration 222/1000 | Loss: 0.00098819
Iteration 223/1000 | Loss: 0.00104979
Iteration 224/1000 | Loss: 0.00069312
Iteration 225/1000 | Loss: 0.00081912
Iteration 226/1000 | Loss: 0.00205614
Iteration 227/1000 | Loss: 0.00056959
Iteration 228/1000 | Loss: 0.00103106
Iteration 229/1000 | Loss: 0.00068558
Iteration 230/1000 | Loss: 0.00109846
Iteration 231/1000 | Loss: 0.00066990
Iteration 232/1000 | Loss: 0.00165907
Iteration 233/1000 | Loss: 0.00102683
Iteration 234/1000 | Loss: 0.00162848
Iteration 235/1000 | Loss: 0.00121166
Iteration 236/1000 | Loss: 0.00091415
Iteration 237/1000 | Loss: 0.00060587
Iteration 238/1000 | Loss: 0.00074825
Iteration 239/1000 | Loss: 0.00102952
Iteration 240/1000 | Loss: 0.00090802
Iteration 241/1000 | Loss: 0.00037069
Iteration 242/1000 | Loss: 0.00059724
Iteration 243/1000 | Loss: 0.00059992
Iteration 244/1000 | Loss: 0.00055614
Iteration 245/1000 | Loss: 0.00052270
Iteration 246/1000 | Loss: 0.00056836
Iteration 247/1000 | Loss: 0.00086198
Iteration 248/1000 | Loss: 0.00072652
Iteration 249/1000 | Loss: 0.00056336
Iteration 250/1000 | Loss: 0.00104982
Iteration 251/1000 | Loss: 0.00110908
Iteration 252/1000 | Loss: 0.00063187
Iteration 253/1000 | Loss: 0.00064477
Iteration 254/1000 | Loss: 0.00050497
Iteration 255/1000 | Loss: 0.00053028
Iteration 256/1000 | Loss: 0.00123235
Iteration 257/1000 | Loss: 0.00022883
Iteration 258/1000 | Loss: 0.00017685
Iteration 259/1000 | Loss: 0.00077752
Iteration 260/1000 | Loss: 0.00037242
Iteration 261/1000 | Loss: 0.00028750
Iteration 262/1000 | Loss: 0.00030754
Iteration 263/1000 | Loss: 0.00016995
Iteration 264/1000 | Loss: 0.00057886
Iteration 265/1000 | Loss: 0.00033973
Iteration 266/1000 | Loss: 0.00009342
Iteration 267/1000 | Loss: 0.00039383
Iteration 268/1000 | Loss: 0.00028324
Iteration 269/1000 | Loss: 0.00045752
Iteration 270/1000 | Loss: 0.00044622
Iteration 271/1000 | Loss: 0.00012978
Iteration 272/1000 | Loss: 0.00007430
Iteration 273/1000 | Loss: 0.00031653
Iteration 274/1000 | Loss: 0.00033144
Iteration 275/1000 | Loss: 0.00025770
Iteration 276/1000 | Loss: 0.00031881
Iteration 277/1000 | Loss: 0.00021057
Iteration 278/1000 | Loss: 0.00026461
Iteration 279/1000 | Loss: 0.00040131
Iteration 280/1000 | Loss: 0.00047157
Iteration 281/1000 | Loss: 0.00018945
Iteration 282/1000 | Loss: 0.00008070
Iteration 283/1000 | Loss: 0.00006754
Iteration 284/1000 | Loss: 0.00006386
Iteration 285/1000 | Loss: 0.00006124
Iteration 286/1000 | Loss: 0.00016875
Iteration 287/1000 | Loss: 0.00011937
Iteration 288/1000 | Loss: 0.00027911
Iteration 289/1000 | Loss: 0.00165143
Iteration 290/1000 | Loss: 0.00217860
Iteration 291/1000 | Loss: 0.00070247
Iteration 292/1000 | Loss: 0.00140764
Iteration 293/1000 | Loss: 0.00114000
Iteration 294/1000 | Loss: 0.00108093
Iteration 295/1000 | Loss: 0.00010072
Iteration 296/1000 | Loss: 0.00060661
Iteration 297/1000 | Loss: 0.00008201
Iteration 298/1000 | Loss: 0.00005963
Iteration 299/1000 | Loss: 0.00020002
Iteration 300/1000 | Loss: 0.00011370
Iteration 301/1000 | Loss: 0.00005105
Iteration 302/1000 | Loss: 0.00023604
Iteration 303/1000 | Loss: 0.00005538
Iteration 304/1000 | Loss: 0.00025972
Iteration 305/1000 | Loss: 0.00024873
Iteration 306/1000 | Loss: 0.00005646
Iteration 307/1000 | Loss: 0.00005133
Iteration 308/1000 | Loss: 0.00004887
Iteration 309/1000 | Loss: 0.00013543
Iteration 310/1000 | Loss: 0.00006009
Iteration 311/1000 | Loss: 0.00006595
Iteration 312/1000 | Loss: 0.00004432
Iteration 313/1000 | Loss: 0.00004327
Iteration 314/1000 | Loss: 0.00004284
Iteration 315/1000 | Loss: 0.00004225
Iteration 316/1000 | Loss: 0.00004186
Iteration 317/1000 | Loss: 0.00034594
Iteration 318/1000 | Loss: 0.00004715
Iteration 319/1000 | Loss: 0.00004303
Iteration 320/1000 | Loss: 0.00004153
Iteration 321/1000 | Loss: 0.00004046
Iteration 322/1000 | Loss: 0.00003977
Iteration 323/1000 | Loss: 0.00003937
Iteration 324/1000 | Loss: 0.00003925
Iteration 325/1000 | Loss: 0.00003919
Iteration 326/1000 | Loss: 0.00003917
Iteration 327/1000 | Loss: 0.00003916
Iteration 328/1000 | Loss: 0.00003915
Iteration 329/1000 | Loss: 0.00003914
Iteration 330/1000 | Loss: 0.00003914
Iteration 331/1000 | Loss: 0.00003914
Iteration 332/1000 | Loss: 0.00003914
Iteration 333/1000 | Loss: 0.00003913
Iteration 334/1000 | Loss: 0.00003913
Iteration 335/1000 | Loss: 0.00003913
Iteration 336/1000 | Loss: 0.00003912
Iteration 337/1000 | Loss: 0.00003912
Iteration 338/1000 | Loss: 0.00003912
Iteration 339/1000 | Loss: 0.00003912
Iteration 340/1000 | Loss: 0.00003911
Iteration 341/1000 | Loss: 0.00003911
Iteration 342/1000 | Loss: 0.00003910
Iteration 343/1000 | Loss: 0.00003909
Iteration 344/1000 | Loss: 0.00003909
Iteration 345/1000 | Loss: 0.00003909
Iteration 346/1000 | Loss: 0.00003908
Iteration 347/1000 | Loss: 0.00003908
Iteration 348/1000 | Loss: 0.00003908
Iteration 349/1000 | Loss: 0.00003907
Iteration 350/1000 | Loss: 0.00003907
Iteration 351/1000 | Loss: 0.00003906
Iteration 352/1000 | Loss: 0.00003906
Iteration 353/1000 | Loss: 0.00003906
Iteration 354/1000 | Loss: 0.00003905
Iteration 355/1000 | Loss: 0.00003905
Iteration 356/1000 | Loss: 0.00003905
Iteration 357/1000 | Loss: 0.00003905
Iteration 358/1000 | Loss: 0.00003904
Iteration 359/1000 | Loss: 0.00003904
Iteration 360/1000 | Loss: 0.00003904
Iteration 361/1000 | Loss: 0.00003904
Iteration 362/1000 | Loss: 0.00003904
Iteration 363/1000 | Loss: 0.00003904
Iteration 364/1000 | Loss: 0.00003904
Iteration 365/1000 | Loss: 0.00003904
Iteration 366/1000 | Loss: 0.00003904
Iteration 367/1000 | Loss: 0.00003904
Iteration 368/1000 | Loss: 0.00003904
Iteration 369/1000 | Loss: 0.00003904
Iteration 370/1000 | Loss: 0.00003903
Iteration 371/1000 | Loss: 0.00003903
Iteration 372/1000 | Loss: 0.00003903
Iteration 373/1000 | Loss: 0.00003903
Iteration 374/1000 | Loss: 0.00003902
Iteration 375/1000 | Loss: 0.00003902
Iteration 376/1000 | Loss: 0.00003902
Iteration 377/1000 | Loss: 0.00003902
Iteration 378/1000 | Loss: 0.00003902
Iteration 379/1000 | Loss: 0.00003902
Iteration 380/1000 | Loss: 0.00003902
Iteration 381/1000 | Loss: 0.00003901
Iteration 382/1000 | Loss: 0.00003901
Iteration 383/1000 | Loss: 0.00003901
Iteration 384/1000 | Loss: 0.00003901
Iteration 385/1000 | Loss: 0.00003901
Iteration 386/1000 | Loss: 0.00003900
Iteration 387/1000 | Loss: 0.00003900
Iteration 388/1000 | Loss: 0.00003900
Iteration 389/1000 | Loss: 0.00003899
Iteration 390/1000 | Loss: 0.00003899
Iteration 391/1000 | Loss: 0.00003899
Iteration 392/1000 | Loss: 0.00003898
Iteration 393/1000 | Loss: 0.00003897
Iteration 394/1000 | Loss: 0.00003897
Iteration 395/1000 | Loss: 0.00003897
Iteration 396/1000 | Loss: 0.00003897
Iteration 397/1000 | Loss: 0.00003896
Iteration 398/1000 | Loss: 0.00003896
Iteration 399/1000 | Loss: 0.00003895
Iteration 400/1000 | Loss: 0.00003895
Iteration 401/1000 | Loss: 0.00003895
Iteration 402/1000 | Loss: 0.00003894
Iteration 403/1000 | Loss: 0.00003894
Iteration 404/1000 | Loss: 0.00003894
Iteration 405/1000 | Loss: 0.00003894
Iteration 406/1000 | Loss: 0.00003893
Iteration 407/1000 | Loss: 0.00003893
Iteration 408/1000 | Loss: 0.00003893
Iteration 409/1000 | Loss: 0.00003893
Iteration 410/1000 | Loss: 0.00003893
Iteration 411/1000 | Loss: 0.00003893
Iteration 412/1000 | Loss: 0.00003893
Iteration 413/1000 | Loss: 0.00003892
Iteration 414/1000 | Loss: 0.00003892
Iteration 415/1000 | Loss: 0.00003892
Iteration 416/1000 | Loss: 0.00003892
Iteration 417/1000 | Loss: 0.00003892
Iteration 418/1000 | Loss: 0.00003892
Iteration 419/1000 | Loss: 0.00003892
Iteration 420/1000 | Loss: 0.00003892
Iteration 421/1000 | Loss: 0.00003892
Iteration 422/1000 | Loss: 0.00003891
Iteration 423/1000 | Loss: 0.00003891
Iteration 424/1000 | Loss: 0.00003891
Iteration 425/1000 | Loss: 0.00003891
Iteration 426/1000 | Loss: 0.00003890
Iteration 427/1000 | Loss: 0.00003890
Iteration 428/1000 | Loss: 0.00003890
Iteration 429/1000 | Loss: 0.00003890
Iteration 430/1000 | Loss: 0.00003890
Iteration 431/1000 | Loss: 0.00003890
Iteration 432/1000 | Loss: 0.00003889
Iteration 433/1000 | Loss: 0.00003889
Iteration 434/1000 | Loss: 0.00003889
Iteration 435/1000 | Loss: 0.00003889
Iteration 436/1000 | Loss: 0.00003889
Iteration 437/1000 | Loss: 0.00003889
Iteration 438/1000 | Loss: 0.00003888
Iteration 439/1000 | Loss: 0.00003888
Iteration 440/1000 | Loss: 0.00003888
Iteration 441/1000 | Loss: 0.00003888
Iteration 442/1000 | Loss: 0.00003888
Iteration 443/1000 | Loss: 0.00003887
Iteration 444/1000 | Loss: 0.00003887
Iteration 445/1000 | Loss: 0.00003887
Iteration 446/1000 | Loss: 0.00003887
Iteration 447/1000 | Loss: 0.00003887
Iteration 448/1000 | Loss: 0.00003887
Iteration 449/1000 | Loss: 0.00003887
Iteration 450/1000 | Loss: 0.00003887
Iteration 451/1000 | Loss: 0.00003886
Iteration 452/1000 | Loss: 0.00003886
Iteration 453/1000 | Loss: 0.00003886
Iteration 454/1000 | Loss: 0.00003886
Iteration 455/1000 | Loss: 0.00003886
Iteration 456/1000 | Loss: 0.00003886
Iteration 457/1000 | Loss: 0.00003886
Iteration 458/1000 | Loss: 0.00003886
Iteration 459/1000 | Loss: 0.00003885
Iteration 460/1000 | Loss: 0.00003885
Iteration 461/1000 | Loss: 0.00003885
Iteration 462/1000 | Loss: 0.00003885
Iteration 463/1000 | Loss: 0.00003885
Iteration 464/1000 | Loss: 0.00003885
Iteration 465/1000 | Loss: 0.00003885
Iteration 466/1000 | Loss: 0.00003885
Iteration 467/1000 | Loss: 0.00003884
Iteration 468/1000 | Loss: 0.00003884
Iteration 469/1000 | Loss: 0.00003884
Iteration 470/1000 | Loss: 0.00003884
Iteration 471/1000 | Loss: 0.00003884
Iteration 472/1000 | Loss: 0.00003883
Iteration 473/1000 | Loss: 0.00003883
Iteration 474/1000 | Loss: 0.00003883
Iteration 475/1000 | Loss: 0.00003882
Iteration 476/1000 | Loss: 0.00003882
Iteration 477/1000 | Loss: 0.00003882
Iteration 478/1000 | Loss: 0.00003882
Iteration 479/1000 | Loss: 0.00003881
Iteration 480/1000 | Loss: 0.00003881
Iteration 481/1000 | Loss: 0.00003881
Iteration 482/1000 | Loss: 0.00003880
Iteration 483/1000 | Loss: 0.00003880
Iteration 484/1000 | Loss: 0.00003880
Iteration 485/1000 | Loss: 0.00003880
Iteration 486/1000 | Loss: 0.00003879
Iteration 487/1000 | Loss: 0.00003879
Iteration 488/1000 | Loss: 0.00003879
Iteration 489/1000 | Loss: 0.00003879
Iteration 490/1000 | Loss: 0.00003879
Iteration 491/1000 | Loss: 0.00003879
Iteration 492/1000 | Loss: 0.00003878
Iteration 493/1000 | Loss: 0.00003878
Iteration 494/1000 | Loss: 0.00003878
Iteration 495/1000 | Loss: 0.00003878
Iteration 496/1000 | Loss: 0.00003878
Iteration 497/1000 | Loss: 0.00003878
Iteration 498/1000 | Loss: 0.00003877
Iteration 499/1000 | Loss: 0.00003877
Iteration 500/1000 | Loss: 0.00003877
Iteration 501/1000 | Loss: 0.00003877
Iteration 502/1000 | Loss: 0.00003877
Iteration 503/1000 | Loss: 0.00003877
Iteration 504/1000 | Loss: 0.00003877
Iteration 505/1000 | Loss: 0.00003877
Iteration 506/1000 | Loss: 0.00003877
Iteration 507/1000 | Loss: 0.00003877
Iteration 508/1000 | Loss: 0.00003877
Iteration 509/1000 | Loss: 0.00003877
Iteration 510/1000 | Loss: 0.00003877
Iteration 511/1000 | Loss: 0.00003877
Iteration 512/1000 | Loss: 0.00003877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 512. Stopping optimization.
Last 5 losses: [3.8773516280343756e-05, 3.8773516280343756e-05, 3.8773516280343756e-05, 3.8773516280343756e-05, 3.8773516280343756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8773516280343756e-05

Optimization complete. Final v2v error: 4.181176662445068 mm

Highest mean error: 14.673291206359863 mm for frame 79

Lowest mean error: 3.1784608364105225 mm for frame 5

Saving results

Total time: 591.2427496910095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1252/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1252/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403967
Iteration 2/25 | Loss: 0.00107015
Iteration 3/25 | Loss: 0.00097296
Iteration 4/25 | Loss: 0.00095445
Iteration 5/25 | Loss: 0.00094642
Iteration 6/25 | Loss: 0.00094471
Iteration 7/25 | Loss: 0.00094469
Iteration 8/25 | Loss: 0.00094469
Iteration 9/25 | Loss: 0.00094469
Iteration 10/25 | Loss: 0.00094469
Iteration 11/25 | Loss: 0.00094469
Iteration 12/25 | Loss: 0.00094469
Iteration 13/25 | Loss: 0.00094469
Iteration 14/25 | Loss: 0.00094469
Iteration 15/25 | Loss: 0.00094469
Iteration 16/25 | Loss: 0.00094469
Iteration 17/25 | Loss: 0.00094469
Iteration 18/25 | Loss: 0.00094469
Iteration 19/25 | Loss: 0.00094469
Iteration 20/25 | Loss: 0.00094469
Iteration 21/25 | Loss: 0.00094469
Iteration 22/25 | Loss: 0.00094469
Iteration 23/25 | Loss: 0.00094469
Iteration 24/25 | Loss: 0.00094469
Iteration 25/25 | Loss: 0.00094469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27175689
Iteration 2/25 | Loss: 0.00096698
Iteration 3/25 | Loss: 0.00096698
Iteration 4/25 | Loss: 0.00096698
Iteration 5/25 | Loss: 0.00096698
Iteration 6/25 | Loss: 0.00096698
Iteration 7/25 | Loss: 0.00096698
Iteration 8/25 | Loss: 0.00096698
Iteration 9/25 | Loss: 0.00096698
Iteration 10/25 | Loss: 0.00096698
Iteration 11/25 | Loss: 0.00096698
Iteration 12/25 | Loss: 0.00096698
Iteration 13/25 | Loss: 0.00096698
Iteration 14/25 | Loss: 0.00096698
Iteration 15/25 | Loss: 0.00096698
Iteration 16/25 | Loss: 0.00096698
Iteration 17/25 | Loss: 0.00096698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009669808787293732, 0.0009669808787293732, 0.0009669808787293732, 0.0009669808787293732, 0.0009669808787293732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009669808787293732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096698
Iteration 2/1000 | Loss: 0.00004614
Iteration 3/1000 | Loss: 0.00002558
Iteration 4/1000 | Loss: 0.00002323
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002089
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001980
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001942
Iteration 15/1000 | Loss: 0.00001942
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001941
Iteration 18/1000 | Loss: 0.00001941
Iteration 19/1000 | Loss: 0.00001941
Iteration 20/1000 | Loss: 0.00001941
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001939
Iteration 24/1000 | Loss: 0.00001939
Iteration 25/1000 | Loss: 0.00001939
Iteration 26/1000 | Loss: 0.00001938
Iteration 27/1000 | Loss: 0.00001938
Iteration 28/1000 | Loss: 0.00001938
Iteration 29/1000 | Loss: 0.00001938
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001936
Iteration 41/1000 | Loss: 0.00001936
Iteration 42/1000 | Loss: 0.00001936
Iteration 43/1000 | Loss: 0.00001936
Iteration 44/1000 | Loss: 0.00001936
Iteration 45/1000 | Loss: 0.00001936
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001936
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001935
Iteration 53/1000 | Loss: 0.00001935
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001934
Iteration 56/1000 | Loss: 0.00001934
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001933
Iteration 61/1000 | Loss: 0.00001933
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001929
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001929
Iteration 98/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.9290986529085785e-05, 1.9290986529085785e-05, 1.9290986529085785e-05, 1.9290986529085785e-05, 1.9290986529085785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9290986529085785e-05

Optimization complete. Final v2v error: 3.8403005599975586 mm

Highest mean error: 4.1960673332214355 mm for frame 78

Lowest mean error: 3.6175122261047363 mm for frame 0

Saving results

Total time: 31.127025842666626
