Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=63, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3528-3583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043701
Iteration 2/25 | Loss: 0.00241826
Iteration 3/25 | Loss: 0.00159464
Iteration 4/25 | Loss: 0.00147897
Iteration 5/25 | Loss: 0.00144525
Iteration 6/25 | Loss: 0.00141316
Iteration 7/25 | Loss: 0.00136085
Iteration 8/25 | Loss: 0.00133817
Iteration 9/25 | Loss: 0.00132144
Iteration 10/25 | Loss: 0.00132254
Iteration 11/25 | Loss: 0.00132441
Iteration 12/25 | Loss: 0.00132505
Iteration 13/25 | Loss: 0.00130539
Iteration 14/25 | Loss: 0.00130136
Iteration 15/25 | Loss: 0.00129715
Iteration 16/25 | Loss: 0.00129595
Iteration 17/25 | Loss: 0.00129522
Iteration 18/25 | Loss: 0.00129485
Iteration 19/25 | Loss: 0.00129466
Iteration 20/25 | Loss: 0.00129444
Iteration 21/25 | Loss: 0.00129439
Iteration 22/25 | Loss: 0.00129438
Iteration 23/25 | Loss: 0.00129438
Iteration 24/25 | Loss: 0.00129438
Iteration 25/25 | Loss: 0.00129438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50339353
Iteration 2/25 | Loss: 0.00172622
Iteration 3/25 | Loss: 0.00171480
Iteration 4/25 | Loss: 0.00171480
Iteration 5/25 | Loss: 0.00171480
Iteration 6/25 | Loss: 0.00171480
Iteration 7/25 | Loss: 0.00171480
Iteration 8/25 | Loss: 0.00171480
Iteration 9/25 | Loss: 0.00171480
Iteration 10/25 | Loss: 0.00171480
Iteration 11/25 | Loss: 0.00171480
Iteration 12/25 | Loss: 0.00171480
Iteration 13/25 | Loss: 0.00171480
Iteration 14/25 | Loss: 0.00171480
Iteration 15/25 | Loss: 0.00171480
Iteration 16/25 | Loss: 0.00171480
Iteration 17/25 | Loss: 0.00171480
Iteration 18/25 | Loss: 0.00171480
Iteration 19/25 | Loss: 0.00171480
Iteration 20/25 | Loss: 0.00171480
Iteration 21/25 | Loss: 0.00171480
Iteration 22/25 | Loss: 0.00171480
Iteration 23/25 | Loss: 0.00171480
Iteration 24/25 | Loss: 0.00171480
Iteration 25/25 | Loss: 0.00171480

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171480
Iteration 2/1000 | Loss: 0.00024838
Iteration 3/1000 | Loss: 0.00027626
Iteration 4/1000 | Loss: 0.00009637
Iteration 5/1000 | Loss: 0.00041500
Iteration 6/1000 | Loss: 0.00022262
Iteration 7/1000 | Loss: 0.00013348
Iteration 8/1000 | Loss: 0.00014946
Iteration 9/1000 | Loss: 0.00130865
Iteration 10/1000 | Loss: 0.00077841
Iteration 11/1000 | Loss: 0.00089440
Iteration 12/1000 | Loss: 0.00121300
Iteration 13/1000 | Loss: 0.00172442
Iteration 14/1000 | Loss: 0.00049567
Iteration 15/1000 | Loss: 0.00017594
Iteration 16/1000 | Loss: 0.00004390
Iteration 17/1000 | Loss: 0.00007032
Iteration 18/1000 | Loss: 0.00005680
Iteration 19/1000 | Loss: 0.00003406
Iteration 20/1000 | Loss: 0.00003044
Iteration 21/1000 | Loss: 0.00031798
Iteration 22/1000 | Loss: 0.00002936
Iteration 23/1000 | Loss: 0.00002745
Iteration 24/1000 | Loss: 0.00003519
Iteration 25/1000 | Loss: 0.00002553
Iteration 26/1000 | Loss: 0.00023398
Iteration 27/1000 | Loss: 0.00019795
Iteration 28/1000 | Loss: 0.00007964
Iteration 29/1000 | Loss: 0.00022394
Iteration 30/1000 | Loss: 0.00013670
Iteration 31/1000 | Loss: 0.00008343
Iteration 32/1000 | Loss: 0.00004472
Iteration 33/1000 | Loss: 0.00008682
Iteration 34/1000 | Loss: 0.00021963
Iteration 35/1000 | Loss: 0.00061350
Iteration 36/1000 | Loss: 0.00030029
Iteration 37/1000 | Loss: 0.00015585
Iteration 38/1000 | Loss: 0.00014046
Iteration 39/1000 | Loss: 0.00011110
Iteration 40/1000 | Loss: 0.00014026
Iteration 41/1000 | Loss: 0.00012735
Iteration 42/1000 | Loss: 0.00008119
Iteration 43/1000 | Loss: 0.00002780
Iteration 44/1000 | Loss: 0.00002595
Iteration 45/1000 | Loss: 0.00012748
Iteration 46/1000 | Loss: 0.00011601
Iteration 47/1000 | Loss: 0.00003341
Iteration 48/1000 | Loss: 0.00009077
Iteration 49/1000 | Loss: 0.00011275
Iteration 50/1000 | Loss: 0.00002416
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002269
Iteration 53/1000 | Loss: 0.00002246
Iteration 54/1000 | Loss: 0.00006660
Iteration 55/1000 | Loss: 0.00003008
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002190
Iteration 58/1000 | Loss: 0.00002166
Iteration 59/1000 | Loss: 0.00006561
Iteration 60/1000 | Loss: 0.00003142
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00006050
Iteration 64/1000 | Loss: 0.00021993
Iteration 65/1000 | Loss: 0.00004599
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00006214
Iteration 68/1000 | Loss: 0.00004673
Iteration 69/1000 | Loss: 0.00002097
Iteration 70/1000 | Loss: 0.00006159
Iteration 71/1000 | Loss: 0.00004442
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00008249
Iteration 74/1000 | Loss: 0.00011498
Iteration 75/1000 | Loss: 0.00008350
Iteration 76/1000 | Loss: 0.00002836
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002076
Iteration 80/1000 | Loss: 0.00002075
Iteration 81/1000 | Loss: 0.00002075
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002072
Iteration 85/1000 | Loss: 0.00002072
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002068
Iteration 94/1000 | Loss: 0.00013066
Iteration 95/1000 | Loss: 0.00008510
Iteration 96/1000 | Loss: 0.00002472
Iteration 97/1000 | Loss: 0.00049522
Iteration 98/1000 | Loss: 0.00145461
Iteration 99/1000 | Loss: 0.00155011
Iteration 100/1000 | Loss: 0.00103458
Iteration 101/1000 | Loss: 0.00165749
Iteration 102/1000 | Loss: 0.00123986
Iteration 103/1000 | Loss: 0.00083726
Iteration 104/1000 | Loss: 0.00025713
Iteration 105/1000 | Loss: 0.00014080
Iteration 106/1000 | Loss: 0.00017783
Iteration 107/1000 | Loss: 0.00003326
Iteration 108/1000 | Loss: 0.00003124
Iteration 109/1000 | Loss: 0.00002805
Iteration 110/1000 | Loss: 0.00013898
Iteration 111/1000 | Loss: 0.00008751
Iteration 112/1000 | Loss: 0.00009611
Iteration 113/1000 | Loss: 0.00005710
Iteration 114/1000 | Loss: 0.00003006
Iteration 115/1000 | Loss: 0.00002732
Iteration 116/1000 | Loss: 0.00002639
Iteration 117/1000 | Loss: 0.00008900
Iteration 118/1000 | Loss: 0.00005049
Iteration 119/1000 | Loss: 0.00011896
Iteration 120/1000 | Loss: 0.00012144
Iteration 121/1000 | Loss: 0.00022601
Iteration 122/1000 | Loss: 0.00009791
Iteration 123/1000 | Loss: 0.00011697
Iteration 124/1000 | Loss: 0.00003898
Iteration 125/1000 | Loss: 0.00005288
Iteration 126/1000 | Loss: 0.00004983
Iteration 127/1000 | Loss: 0.00004881
Iteration 128/1000 | Loss: 0.00034835
Iteration 129/1000 | Loss: 0.00011015
Iteration 130/1000 | Loss: 0.00010483
Iteration 131/1000 | Loss: 0.00024537
Iteration 132/1000 | Loss: 0.00003968
Iteration 133/1000 | Loss: 0.00019734
Iteration 134/1000 | Loss: 0.00002960
Iteration 135/1000 | Loss: 0.00014313
Iteration 136/1000 | Loss: 0.00009272
Iteration 137/1000 | Loss: 0.00011366
Iteration 138/1000 | Loss: 0.00005315
Iteration 139/1000 | Loss: 0.00014988
Iteration 140/1000 | Loss: 0.00042506
Iteration 141/1000 | Loss: 0.00061820
Iteration 142/1000 | Loss: 0.00031787
Iteration 143/1000 | Loss: 0.00011856
Iteration 144/1000 | Loss: 0.00055854
Iteration 145/1000 | Loss: 0.00032782
Iteration 146/1000 | Loss: 0.00021997
Iteration 147/1000 | Loss: 0.00015667
Iteration 148/1000 | Loss: 0.00006666
Iteration 149/1000 | Loss: 0.00035322
Iteration 150/1000 | Loss: 0.00007545
Iteration 151/1000 | Loss: 0.00023550
Iteration 152/1000 | Loss: 0.00017530
Iteration 153/1000 | Loss: 0.00013081
Iteration 154/1000 | Loss: 0.00011305
Iteration 155/1000 | Loss: 0.00013706
Iteration 156/1000 | Loss: 0.00013070
Iteration 157/1000 | Loss: 0.00011113
Iteration 158/1000 | Loss: 0.00005278
Iteration 159/1000 | Loss: 0.00011709
Iteration 160/1000 | Loss: 0.00003509
Iteration 161/1000 | Loss: 0.00002570
Iteration 162/1000 | Loss: 0.00007270
Iteration 163/1000 | Loss: 0.00018284
Iteration 164/1000 | Loss: 0.00010753
Iteration 165/1000 | Loss: 0.00007061
Iteration 166/1000 | Loss: 0.00013463
Iteration 167/1000 | Loss: 0.00013157
Iteration 168/1000 | Loss: 0.00008504
Iteration 169/1000 | Loss: 0.00013748
Iteration 170/1000 | Loss: 0.00010360
Iteration 171/1000 | Loss: 0.00011966
Iteration 172/1000 | Loss: 0.00009123
Iteration 173/1000 | Loss: 0.00007900
Iteration 174/1000 | Loss: 0.00005215
Iteration 175/1000 | Loss: 0.00003623
Iteration 176/1000 | Loss: 0.00009364
Iteration 177/1000 | Loss: 0.00003985
Iteration 178/1000 | Loss: 0.00003268
Iteration 179/1000 | Loss: 0.00005314
Iteration 180/1000 | Loss: 0.00005368
Iteration 181/1000 | Loss: 0.00009970
Iteration 182/1000 | Loss: 0.00009235
Iteration 183/1000 | Loss: 0.00008505
Iteration 184/1000 | Loss: 0.00007273
Iteration 185/1000 | Loss: 0.00008562
Iteration 186/1000 | Loss: 0.00006075
Iteration 187/1000 | Loss: 0.00006384
Iteration 188/1000 | Loss: 0.00006818
Iteration 189/1000 | Loss: 0.00006754
Iteration 190/1000 | Loss: 0.00008830
Iteration 191/1000 | Loss: 0.00008776
Iteration 192/1000 | Loss: 0.00002654
Iteration 193/1000 | Loss: 0.00002360
Iteration 194/1000 | Loss: 0.00002103
Iteration 195/1000 | Loss: 0.00002037
Iteration 196/1000 | Loss: 0.00001913
Iteration 197/1000 | Loss: 0.00002146
Iteration 198/1000 | Loss: 0.00001792
Iteration 199/1000 | Loss: 0.00002126
Iteration 200/1000 | Loss: 0.00001762
Iteration 201/1000 | Loss: 0.00002105
Iteration 202/1000 | Loss: 0.00007968
Iteration 203/1000 | Loss: 0.00015742
Iteration 204/1000 | Loss: 0.00002057
Iteration 205/1000 | Loss: 0.00001815
Iteration 206/1000 | Loss: 0.00001735
Iteration 207/1000 | Loss: 0.00001712
Iteration 208/1000 | Loss: 0.00001710
Iteration 209/1000 | Loss: 0.00001709
Iteration 210/1000 | Loss: 0.00001706
Iteration 211/1000 | Loss: 0.00001706
Iteration 212/1000 | Loss: 0.00001706
Iteration 213/1000 | Loss: 0.00001705
Iteration 214/1000 | Loss: 0.00001705
Iteration 215/1000 | Loss: 0.00001704
Iteration 216/1000 | Loss: 0.00001704
Iteration 217/1000 | Loss: 0.00001984
Iteration 218/1000 | Loss: 0.00001705
Iteration 219/1000 | Loss: 0.00001700
Iteration 220/1000 | Loss: 0.00001697
Iteration 221/1000 | Loss: 0.00001696
Iteration 222/1000 | Loss: 0.00001696
Iteration 223/1000 | Loss: 0.00001772
Iteration 224/1000 | Loss: 0.00001691
Iteration 225/1000 | Loss: 0.00001691
Iteration 226/1000 | Loss: 0.00001690
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001690
Iteration 229/1000 | Loss: 0.00001690
Iteration 230/1000 | Loss: 0.00001690
Iteration 231/1000 | Loss: 0.00001690
Iteration 232/1000 | Loss: 0.00001690
Iteration 233/1000 | Loss: 0.00001690
Iteration 234/1000 | Loss: 0.00001689
Iteration 235/1000 | Loss: 0.00001688
Iteration 236/1000 | Loss: 0.00001807
Iteration 237/1000 | Loss: 0.00001680
Iteration 238/1000 | Loss: 0.00001680
Iteration 239/1000 | Loss: 0.00001680
Iteration 240/1000 | Loss: 0.00001680
Iteration 241/1000 | Loss: 0.00001680
Iteration 242/1000 | Loss: 0.00001680
Iteration 243/1000 | Loss: 0.00001680
Iteration 244/1000 | Loss: 0.00001680
Iteration 245/1000 | Loss: 0.00001680
Iteration 246/1000 | Loss: 0.00001680
Iteration 247/1000 | Loss: 0.00001680
Iteration 248/1000 | Loss: 0.00001680
Iteration 249/1000 | Loss: 0.00001680
Iteration 250/1000 | Loss: 0.00001680
Iteration 251/1000 | Loss: 0.00001680
Iteration 252/1000 | Loss: 0.00001680
Iteration 253/1000 | Loss: 0.00001680
Iteration 254/1000 | Loss: 0.00001680
Iteration 255/1000 | Loss: 0.00001680
Iteration 256/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.6796975614852272e-05, 1.6796975614852272e-05, 1.6796975614852272e-05, 1.6796975614852272e-05, 1.6796975614852272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6796975614852272e-05

Optimization complete. Final v2v error: 3.1871464252471924 mm

Highest mean error: 9.953614234924316 mm for frame 212

Lowest mean error: 2.671531915664673 mm for frame 104

Saving results

Total time: 346.90091466903687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946276
Iteration 2/25 | Loss: 0.00207386
Iteration 3/25 | Loss: 0.00154396
Iteration 4/25 | Loss: 0.00152811
Iteration 5/25 | Loss: 0.00152526
Iteration 6/25 | Loss: 0.00152498
Iteration 7/25 | Loss: 0.00152498
Iteration 8/25 | Loss: 0.00152498
Iteration 9/25 | Loss: 0.00152498
Iteration 10/25 | Loss: 0.00152498
Iteration 11/25 | Loss: 0.00152498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015249787829816341, 0.0015249787829816341, 0.0015249787829816341, 0.0015249787829816341, 0.0015249787829816341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015249787829816341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66936731
Iteration 2/25 | Loss: 0.00107511
Iteration 3/25 | Loss: 0.00107511
Iteration 4/25 | Loss: 0.00107511
Iteration 5/25 | Loss: 0.00107511
Iteration 6/25 | Loss: 0.00107511
Iteration 7/25 | Loss: 0.00107511
Iteration 8/25 | Loss: 0.00107511
Iteration 9/25 | Loss: 0.00107511
Iteration 10/25 | Loss: 0.00107511
Iteration 11/25 | Loss: 0.00107511
Iteration 12/25 | Loss: 0.00107511
Iteration 13/25 | Loss: 0.00107511
Iteration 14/25 | Loss: 0.00107511
Iteration 15/25 | Loss: 0.00107511
Iteration 16/25 | Loss: 0.00107511
Iteration 17/25 | Loss: 0.00107511
Iteration 18/25 | Loss: 0.00107511
Iteration 19/25 | Loss: 0.00107511
Iteration 20/25 | Loss: 0.00107511
Iteration 21/25 | Loss: 0.00107511
Iteration 22/25 | Loss: 0.00107511
Iteration 23/25 | Loss: 0.00107511
Iteration 24/25 | Loss: 0.00107511
Iteration 25/25 | Loss: 0.00107511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010751105146482587, 0.0010751105146482587, 0.0010751105146482587, 0.0010751105146482587, 0.0010751105146482587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010751105146482587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107511
Iteration 2/1000 | Loss: 0.00007969
Iteration 3/1000 | Loss: 0.00005347
Iteration 4/1000 | Loss: 0.00004473
Iteration 5/1000 | Loss: 0.00004274
Iteration 6/1000 | Loss: 0.00004143
Iteration 7/1000 | Loss: 0.00004038
Iteration 8/1000 | Loss: 0.00003944
Iteration 9/1000 | Loss: 0.00003853
Iteration 10/1000 | Loss: 0.00003780
Iteration 11/1000 | Loss: 0.00003720
Iteration 12/1000 | Loss: 0.00003668
Iteration 13/1000 | Loss: 0.00003643
Iteration 14/1000 | Loss: 0.00003612
Iteration 15/1000 | Loss: 0.00003579
Iteration 16/1000 | Loss: 0.00003556
Iteration 17/1000 | Loss: 0.00003533
Iteration 18/1000 | Loss: 0.00003514
Iteration 19/1000 | Loss: 0.00003505
Iteration 20/1000 | Loss: 0.00003492
Iteration 21/1000 | Loss: 0.00003489
Iteration 22/1000 | Loss: 0.00003471
Iteration 23/1000 | Loss: 0.00003470
Iteration 24/1000 | Loss: 0.00003469
Iteration 25/1000 | Loss: 0.00003456
Iteration 26/1000 | Loss: 0.00003448
Iteration 27/1000 | Loss: 0.00003439
Iteration 28/1000 | Loss: 0.00003436
Iteration 29/1000 | Loss: 0.00003436
Iteration 30/1000 | Loss: 0.00003436
Iteration 31/1000 | Loss: 0.00003436
Iteration 32/1000 | Loss: 0.00003436
Iteration 33/1000 | Loss: 0.00003436
Iteration 34/1000 | Loss: 0.00003436
Iteration 35/1000 | Loss: 0.00003436
Iteration 36/1000 | Loss: 0.00003435
Iteration 37/1000 | Loss: 0.00003435
Iteration 38/1000 | Loss: 0.00003435
Iteration 39/1000 | Loss: 0.00003435
Iteration 40/1000 | Loss: 0.00003434
Iteration 41/1000 | Loss: 0.00003433
Iteration 42/1000 | Loss: 0.00003432
Iteration 43/1000 | Loss: 0.00003431
Iteration 44/1000 | Loss: 0.00003426
Iteration 45/1000 | Loss: 0.00003426
Iteration 46/1000 | Loss: 0.00003425
Iteration 47/1000 | Loss: 0.00003424
Iteration 48/1000 | Loss: 0.00003424
Iteration 49/1000 | Loss: 0.00003424
Iteration 50/1000 | Loss: 0.00003424
Iteration 51/1000 | Loss: 0.00003424
Iteration 52/1000 | Loss: 0.00003424
Iteration 53/1000 | Loss: 0.00003423
Iteration 54/1000 | Loss: 0.00003423
Iteration 55/1000 | Loss: 0.00003423
Iteration 56/1000 | Loss: 0.00003423
Iteration 57/1000 | Loss: 0.00003423
Iteration 58/1000 | Loss: 0.00003423
Iteration 59/1000 | Loss: 0.00003423
Iteration 60/1000 | Loss: 0.00003423
Iteration 61/1000 | Loss: 0.00003423
Iteration 62/1000 | Loss: 0.00003423
Iteration 63/1000 | Loss: 0.00003423
Iteration 64/1000 | Loss: 0.00003423
Iteration 65/1000 | Loss: 0.00003422
Iteration 66/1000 | Loss: 0.00003422
Iteration 67/1000 | Loss: 0.00003422
Iteration 68/1000 | Loss: 0.00003421
Iteration 69/1000 | Loss: 0.00003421
Iteration 70/1000 | Loss: 0.00003421
Iteration 71/1000 | Loss: 0.00003421
Iteration 72/1000 | Loss: 0.00003421
Iteration 73/1000 | Loss: 0.00003420
Iteration 74/1000 | Loss: 0.00003420
Iteration 75/1000 | Loss: 0.00003420
Iteration 76/1000 | Loss: 0.00003420
Iteration 77/1000 | Loss: 0.00003420
Iteration 78/1000 | Loss: 0.00003420
Iteration 79/1000 | Loss: 0.00003420
Iteration 80/1000 | Loss: 0.00003420
Iteration 81/1000 | Loss: 0.00003419
Iteration 82/1000 | Loss: 0.00003418
Iteration 83/1000 | Loss: 0.00003418
Iteration 84/1000 | Loss: 0.00003418
Iteration 85/1000 | Loss: 0.00003417
Iteration 86/1000 | Loss: 0.00003417
Iteration 87/1000 | Loss: 0.00003417
Iteration 88/1000 | Loss: 0.00003416
Iteration 89/1000 | Loss: 0.00003416
Iteration 90/1000 | Loss: 0.00003416
Iteration 91/1000 | Loss: 0.00003416
Iteration 92/1000 | Loss: 0.00003416
Iteration 93/1000 | Loss: 0.00003416
Iteration 94/1000 | Loss: 0.00003416
Iteration 95/1000 | Loss: 0.00003416
Iteration 96/1000 | Loss: 0.00003416
Iteration 97/1000 | Loss: 0.00003415
Iteration 98/1000 | Loss: 0.00003414
Iteration 99/1000 | Loss: 0.00003414
Iteration 100/1000 | Loss: 0.00003414
Iteration 101/1000 | Loss: 0.00003414
Iteration 102/1000 | Loss: 0.00003414
Iteration 103/1000 | Loss: 0.00003414
Iteration 104/1000 | Loss: 0.00003414
Iteration 105/1000 | Loss: 0.00003413
Iteration 106/1000 | Loss: 0.00003413
Iteration 107/1000 | Loss: 0.00003413
Iteration 108/1000 | Loss: 0.00003413
Iteration 109/1000 | Loss: 0.00003412
Iteration 110/1000 | Loss: 0.00003412
Iteration 111/1000 | Loss: 0.00003412
Iteration 112/1000 | Loss: 0.00003412
Iteration 113/1000 | Loss: 0.00003411
Iteration 114/1000 | Loss: 0.00003411
Iteration 115/1000 | Loss: 0.00003411
Iteration 116/1000 | Loss: 0.00003411
Iteration 117/1000 | Loss: 0.00003410
Iteration 118/1000 | Loss: 0.00003410
Iteration 119/1000 | Loss: 0.00003410
Iteration 120/1000 | Loss: 0.00003410
Iteration 121/1000 | Loss: 0.00003410
Iteration 122/1000 | Loss: 0.00003409
Iteration 123/1000 | Loss: 0.00003409
Iteration 124/1000 | Loss: 0.00003409
Iteration 125/1000 | Loss: 0.00003409
Iteration 126/1000 | Loss: 0.00003409
Iteration 127/1000 | Loss: 0.00003409
Iteration 128/1000 | Loss: 0.00003408
Iteration 129/1000 | Loss: 0.00003408
Iteration 130/1000 | Loss: 0.00003408
Iteration 131/1000 | Loss: 0.00003408
Iteration 132/1000 | Loss: 0.00003408
Iteration 133/1000 | Loss: 0.00003408
Iteration 134/1000 | Loss: 0.00003407
Iteration 135/1000 | Loss: 0.00003407
Iteration 136/1000 | Loss: 0.00003407
Iteration 137/1000 | Loss: 0.00003407
Iteration 138/1000 | Loss: 0.00003407
Iteration 139/1000 | Loss: 0.00003407
Iteration 140/1000 | Loss: 0.00003407
Iteration 141/1000 | Loss: 0.00003407
Iteration 142/1000 | Loss: 0.00003407
Iteration 143/1000 | Loss: 0.00003407
Iteration 144/1000 | Loss: 0.00003407
Iteration 145/1000 | Loss: 0.00003407
Iteration 146/1000 | Loss: 0.00003407
Iteration 147/1000 | Loss: 0.00003407
Iteration 148/1000 | Loss: 0.00003407
Iteration 149/1000 | Loss: 0.00003406
Iteration 150/1000 | Loss: 0.00003406
Iteration 151/1000 | Loss: 0.00003406
Iteration 152/1000 | Loss: 0.00003406
Iteration 153/1000 | Loss: 0.00003406
Iteration 154/1000 | Loss: 0.00003406
Iteration 155/1000 | Loss: 0.00003406
Iteration 156/1000 | Loss: 0.00003406
Iteration 157/1000 | Loss: 0.00003406
Iteration 158/1000 | Loss: 0.00003406
Iteration 159/1000 | Loss: 0.00003406
Iteration 160/1000 | Loss: 0.00003406
Iteration 161/1000 | Loss: 0.00003406
Iteration 162/1000 | Loss: 0.00003406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.406332325539552e-05, 3.406332325539552e-05, 3.406332325539552e-05, 3.406332325539552e-05, 3.406332325539552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.406332325539552e-05

Optimization complete. Final v2v error: 4.837888717651367 mm

Highest mean error: 5.489775657653809 mm for frame 58

Lowest mean error: 4.416079044342041 mm for frame 111

Saving results

Total time: 50.38607406616211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417093
Iteration 2/25 | Loss: 0.00129502
Iteration 3/25 | Loss: 0.00122788
Iteration 4/25 | Loss: 0.00121749
Iteration 5/25 | Loss: 0.00121429
Iteration 6/25 | Loss: 0.00121360
Iteration 7/25 | Loss: 0.00121360
Iteration 8/25 | Loss: 0.00121360
Iteration 9/25 | Loss: 0.00121360
Iteration 10/25 | Loss: 0.00121360
Iteration 11/25 | Loss: 0.00121360
Iteration 12/25 | Loss: 0.00121360
Iteration 13/25 | Loss: 0.00121360
Iteration 14/25 | Loss: 0.00121360
Iteration 15/25 | Loss: 0.00121360
Iteration 16/25 | Loss: 0.00121360
Iteration 17/25 | Loss: 0.00121360
Iteration 18/25 | Loss: 0.00121360
Iteration 19/25 | Loss: 0.00121360
Iteration 20/25 | Loss: 0.00121360
Iteration 21/25 | Loss: 0.00121360
Iteration 22/25 | Loss: 0.00121360
Iteration 23/25 | Loss: 0.00121360
Iteration 24/25 | Loss: 0.00121360
Iteration 25/25 | Loss: 0.00121360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012135979486629367, 0.0012135979486629367, 0.0012135979486629367, 0.0012135979486629367, 0.0012135979486629367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012135979486629367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37894583
Iteration 2/25 | Loss: 0.00096239
Iteration 3/25 | Loss: 0.00096239
Iteration 4/25 | Loss: 0.00096239
Iteration 5/25 | Loss: 0.00096239
Iteration 6/25 | Loss: 0.00096239
Iteration 7/25 | Loss: 0.00096239
Iteration 8/25 | Loss: 0.00096239
Iteration 9/25 | Loss: 0.00096239
Iteration 10/25 | Loss: 0.00096239
Iteration 11/25 | Loss: 0.00096239
Iteration 12/25 | Loss: 0.00096238
Iteration 13/25 | Loss: 0.00096238
Iteration 14/25 | Loss: 0.00096238
Iteration 15/25 | Loss: 0.00096238
Iteration 16/25 | Loss: 0.00096238
Iteration 17/25 | Loss: 0.00096238
Iteration 18/25 | Loss: 0.00096238
Iteration 19/25 | Loss: 0.00096238
Iteration 20/25 | Loss: 0.00096238
Iteration 21/25 | Loss: 0.00096238
Iteration 22/25 | Loss: 0.00096238
Iteration 23/25 | Loss: 0.00096238
Iteration 24/25 | Loss: 0.00096238
Iteration 25/25 | Loss: 0.00096238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096238
Iteration 2/1000 | Loss: 0.00002779
Iteration 3/1000 | Loss: 0.00001894
Iteration 4/1000 | Loss: 0.00001598
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001316
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001238
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001194
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001192
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001191
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001174
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001172
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001172
Iteration 115/1000 | Loss: 0.00001172
Iteration 116/1000 | Loss: 0.00001172
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1715264918166213e-05, 1.1715264918166213e-05, 1.1715264918166213e-05, 1.1715264918166213e-05, 1.1715264918166213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1715264918166213e-05

Optimization complete. Final v2v error: 2.9512228965759277 mm

Highest mean error: 3.4553022384643555 mm for frame 57

Lowest mean error: 2.708658456802368 mm for frame 122

Saving results

Total time: 37.98315787315369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959817
Iteration 2/25 | Loss: 0.00959817
Iteration 3/25 | Loss: 0.00243179
Iteration 4/25 | Loss: 0.00186817
Iteration 5/25 | Loss: 0.00181294
Iteration 6/25 | Loss: 0.00165395
Iteration 7/25 | Loss: 0.00153346
Iteration 8/25 | Loss: 0.00148161
Iteration 9/25 | Loss: 0.00143973
Iteration 10/25 | Loss: 0.00144815
Iteration 11/25 | Loss: 0.00139799
Iteration 12/25 | Loss: 0.00137650
Iteration 13/25 | Loss: 0.00134811
Iteration 14/25 | Loss: 0.00134750
Iteration 15/25 | Loss: 0.00131622
Iteration 16/25 | Loss: 0.00129544
Iteration 17/25 | Loss: 0.00129524
Iteration 18/25 | Loss: 0.00129427
Iteration 19/25 | Loss: 0.00128882
Iteration 20/25 | Loss: 0.00129061
Iteration 21/25 | Loss: 0.00128908
Iteration 22/25 | Loss: 0.00128673
Iteration 23/25 | Loss: 0.00128558
Iteration 24/25 | Loss: 0.00128624
Iteration 25/25 | Loss: 0.00128465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35375941
Iteration 2/25 | Loss: 0.00192077
Iteration 3/25 | Loss: 0.00146964
Iteration 4/25 | Loss: 0.00146964
Iteration 5/25 | Loss: 0.00146964
Iteration 6/25 | Loss: 0.00146964
Iteration 7/25 | Loss: 0.00146964
Iteration 8/25 | Loss: 0.00146964
Iteration 9/25 | Loss: 0.00146964
Iteration 10/25 | Loss: 0.00146964
Iteration 11/25 | Loss: 0.00146964
Iteration 12/25 | Loss: 0.00146964
Iteration 13/25 | Loss: 0.00146964
Iteration 14/25 | Loss: 0.00146964
Iteration 15/25 | Loss: 0.00146964
Iteration 16/25 | Loss: 0.00146964
Iteration 17/25 | Loss: 0.00146964
Iteration 18/25 | Loss: 0.00146964
Iteration 19/25 | Loss: 0.00146964
Iteration 20/25 | Loss: 0.00146964
Iteration 21/25 | Loss: 0.00146964
Iteration 22/25 | Loss: 0.00146964
Iteration 23/25 | Loss: 0.00146964
Iteration 24/25 | Loss: 0.00146964
Iteration 25/25 | Loss: 0.00146964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014696373837068677, 0.0014696373837068677, 0.0014696373837068677, 0.0014696373837068677, 0.0014696373837068677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014696373837068677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146964
Iteration 2/1000 | Loss: 0.00069688
Iteration 3/1000 | Loss: 0.00097691
Iteration 4/1000 | Loss: 0.00019248
Iteration 5/1000 | Loss: 0.00012005
Iteration 6/1000 | Loss: 0.00051716
Iteration 7/1000 | Loss: 0.00016799
Iteration 8/1000 | Loss: 0.00004583
Iteration 9/1000 | Loss: 0.00004771
Iteration 10/1000 | Loss: 0.00009431
Iteration 11/1000 | Loss: 0.00007052
Iteration 12/1000 | Loss: 0.00003264
Iteration 13/1000 | Loss: 0.00003226
Iteration 14/1000 | Loss: 0.00043683
Iteration 15/1000 | Loss: 0.00012600
Iteration 16/1000 | Loss: 0.00004295
Iteration 17/1000 | Loss: 0.00004808
Iteration 18/1000 | Loss: 0.00004428
Iteration 19/1000 | Loss: 0.00009824
Iteration 20/1000 | Loss: 0.00002898
Iteration 21/1000 | Loss: 0.00003229
Iteration 22/1000 | Loss: 0.00004993
Iteration 23/1000 | Loss: 0.00002393
Iteration 24/1000 | Loss: 0.00002517
Iteration 25/1000 | Loss: 0.00002468
Iteration 26/1000 | Loss: 0.00002270
Iteration 27/1000 | Loss: 0.00002255
Iteration 28/1000 | Loss: 0.00002041
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00002336
Iteration 31/1000 | Loss: 0.00001984
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001926
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001920
Iteration 38/1000 | Loss: 0.00001955
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001915
Iteration 41/1000 | Loss: 0.00001915
Iteration 42/1000 | Loss: 0.00001914
Iteration 43/1000 | Loss: 0.00001914
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001914
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001920
Iteration 57/1000 | Loss: 0.00001920
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001911
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001911
Iteration 67/1000 | Loss: 0.00001910
Iteration 68/1000 | Loss: 0.00001910
Iteration 69/1000 | Loss: 0.00001910
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002718
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001898
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001894
Iteration 98/1000 | Loss: 0.00004576
Iteration 99/1000 | Loss: 0.00007201
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001904
Iteration 102/1000 | Loss: 0.00001862
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00001797
Iteration 105/1000 | Loss: 0.00002531
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00002977
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001667
Iteration 110/1000 | Loss: 0.00001667
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001655
Iteration 122/1000 | Loss: 0.00001654
Iteration 123/1000 | Loss: 0.00001654
Iteration 124/1000 | Loss: 0.00001654
Iteration 125/1000 | Loss: 0.00001654
Iteration 126/1000 | Loss: 0.00001652
Iteration 127/1000 | Loss: 0.00001652
Iteration 128/1000 | Loss: 0.00001652
Iteration 129/1000 | Loss: 0.00001652
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001652
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001651
Iteration 134/1000 | Loss: 0.00001651
Iteration 135/1000 | Loss: 0.00001650
Iteration 136/1000 | Loss: 0.00001650
Iteration 137/1000 | Loss: 0.00001650
Iteration 138/1000 | Loss: 0.00001650
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001648
Iteration 145/1000 | Loss: 0.00001648
Iteration 146/1000 | Loss: 0.00001895
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001646
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001648
Iteration 154/1000 | Loss: 0.00001648
Iteration 155/1000 | Loss: 0.00002017
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001934
Iteration 158/1000 | Loss: 0.00001679
Iteration 159/1000 | Loss: 0.00001679
Iteration 160/1000 | Loss: 0.00001640
Iteration 161/1000 | Loss: 0.00001638
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001636
Iteration 168/1000 | Loss: 0.00001636
Iteration 169/1000 | Loss: 0.00001636
Iteration 170/1000 | Loss: 0.00001636
Iteration 171/1000 | Loss: 0.00001636
Iteration 172/1000 | Loss: 0.00001636
Iteration 173/1000 | Loss: 0.00001636
Iteration 174/1000 | Loss: 0.00001636
Iteration 175/1000 | Loss: 0.00001636
Iteration 176/1000 | Loss: 0.00001636
Iteration 177/1000 | Loss: 0.00001636
Iteration 178/1000 | Loss: 0.00001636
Iteration 179/1000 | Loss: 0.00001636
Iteration 180/1000 | Loss: 0.00001636
Iteration 181/1000 | Loss: 0.00001636
Iteration 182/1000 | Loss: 0.00001636
Iteration 183/1000 | Loss: 0.00001636
Iteration 184/1000 | Loss: 0.00001636
Iteration 185/1000 | Loss: 0.00001636
Iteration 186/1000 | Loss: 0.00001636
Iteration 187/1000 | Loss: 0.00001636
Iteration 188/1000 | Loss: 0.00001636
Iteration 189/1000 | Loss: 0.00001636
Iteration 190/1000 | Loss: 0.00001636
Iteration 191/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6356703781639226e-05, 1.6356703781639226e-05, 1.6356703781639226e-05, 1.6356703781639226e-05, 1.6356703781639226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6356703781639226e-05

Optimization complete. Final v2v error: 3.2681021690368652 mm

Highest mean error: 10.968984603881836 mm for frame 43

Lowest mean error: 2.7894744873046875 mm for frame 83

Saving results

Total time: 141.06932306289673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594009
Iteration 2/25 | Loss: 0.00153190
Iteration 3/25 | Loss: 0.00132774
Iteration 4/25 | Loss: 0.00128091
Iteration 5/25 | Loss: 0.00127213
Iteration 6/25 | Loss: 0.00127168
Iteration 7/25 | Loss: 0.00127047
Iteration 8/25 | Loss: 0.00126741
Iteration 9/25 | Loss: 0.00126589
Iteration 10/25 | Loss: 0.00126491
Iteration 11/25 | Loss: 0.00126444
Iteration 12/25 | Loss: 0.00126423
Iteration 13/25 | Loss: 0.00126412
Iteration 14/25 | Loss: 0.00126411
Iteration 15/25 | Loss: 0.00126411
Iteration 16/25 | Loss: 0.00126410
Iteration 17/25 | Loss: 0.00126410
Iteration 18/25 | Loss: 0.00126410
Iteration 19/25 | Loss: 0.00126410
Iteration 20/25 | Loss: 0.00126410
Iteration 21/25 | Loss: 0.00126410
Iteration 22/25 | Loss: 0.00126410
Iteration 23/25 | Loss: 0.00126410
Iteration 24/25 | Loss: 0.00126410
Iteration 25/25 | Loss: 0.00126409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.62855148
Iteration 2/25 | Loss: 0.00093203
Iteration 3/25 | Loss: 0.00092092
Iteration 4/25 | Loss: 0.00092092
Iteration 5/25 | Loss: 0.00092092
Iteration 6/25 | Loss: 0.00092092
Iteration 7/25 | Loss: 0.00092092
Iteration 8/25 | Loss: 0.00092092
Iteration 9/25 | Loss: 0.00092092
Iteration 10/25 | Loss: 0.00092092
Iteration 11/25 | Loss: 0.00092092
Iteration 12/25 | Loss: 0.00092092
Iteration 13/25 | Loss: 0.00092092
Iteration 14/25 | Loss: 0.00092092
Iteration 15/25 | Loss: 0.00092092
Iteration 16/25 | Loss: 0.00092092
Iteration 17/25 | Loss: 0.00092092
Iteration 18/25 | Loss: 0.00092092
Iteration 19/25 | Loss: 0.00092092
Iteration 20/25 | Loss: 0.00092092
Iteration 21/25 | Loss: 0.00092092
Iteration 22/25 | Loss: 0.00092092
Iteration 23/25 | Loss: 0.00092092
Iteration 24/25 | Loss: 0.00092092
Iteration 25/25 | Loss: 0.00092092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092092
Iteration 2/1000 | Loss: 0.00005612
Iteration 3/1000 | Loss: 0.00002237
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001899
Iteration 6/1000 | Loss: 0.00001811
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001590
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001589
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001575
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001573
Iteration 43/1000 | Loss: 0.00001573
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001568
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001564
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001560
Iteration 77/1000 | Loss: 0.00001560
Iteration 78/1000 | Loss: 0.00001560
Iteration 79/1000 | Loss: 0.00001560
Iteration 80/1000 | Loss: 0.00001559
Iteration 81/1000 | Loss: 0.00001558
Iteration 82/1000 | Loss: 0.00001558
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001550
Iteration 120/1000 | Loss: 0.00001550
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001548
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001547
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001547
Iteration 143/1000 | Loss: 0.00001547
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001545
Iteration 156/1000 | Loss: 0.00001545
Iteration 157/1000 | Loss: 0.00001545
Iteration 158/1000 | Loss: 0.00001545
Iteration 159/1000 | Loss: 0.00001545
Iteration 160/1000 | Loss: 0.00001545
Iteration 161/1000 | Loss: 0.00001545
Iteration 162/1000 | Loss: 0.00001545
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001544
Iteration 168/1000 | Loss: 0.00001544
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001544
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001543
Iteration 177/1000 | Loss: 0.00001543
Iteration 178/1000 | Loss: 0.00001543
Iteration 179/1000 | Loss: 0.00001543
Iteration 180/1000 | Loss: 0.00001543
Iteration 181/1000 | Loss: 0.00001543
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001542
Iteration 184/1000 | Loss: 0.00001542
Iteration 185/1000 | Loss: 0.00001542
Iteration 186/1000 | Loss: 0.00001542
Iteration 187/1000 | Loss: 0.00001542
Iteration 188/1000 | Loss: 0.00001542
Iteration 189/1000 | Loss: 0.00001542
Iteration 190/1000 | Loss: 0.00001542
Iteration 191/1000 | Loss: 0.00001542
Iteration 192/1000 | Loss: 0.00001542
Iteration 193/1000 | Loss: 0.00001542
Iteration 194/1000 | Loss: 0.00001541
Iteration 195/1000 | Loss: 0.00001541
Iteration 196/1000 | Loss: 0.00001541
Iteration 197/1000 | Loss: 0.00001541
Iteration 198/1000 | Loss: 0.00001541
Iteration 199/1000 | Loss: 0.00001541
Iteration 200/1000 | Loss: 0.00001541
Iteration 201/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.5412764696520753e-05, 1.5412764696520753e-05, 1.5412764696520753e-05, 1.5412764696520753e-05, 1.5412764696520753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5412764696520753e-05

Optimization complete. Final v2v error: 3.320452928543091 mm

Highest mean error: 3.8032777309417725 mm for frame 173

Lowest mean error: 3.0428833961486816 mm for frame 198

Saving results

Total time: 62.82429766654968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793921
Iteration 2/25 | Loss: 0.00163777
Iteration 3/25 | Loss: 0.00141570
Iteration 4/25 | Loss: 0.00140268
Iteration 5/25 | Loss: 0.00140025
Iteration 6/25 | Loss: 0.00140025
Iteration 7/25 | Loss: 0.00140025
Iteration 8/25 | Loss: 0.00140025
Iteration 9/25 | Loss: 0.00140025
Iteration 10/25 | Loss: 0.00140025
Iteration 11/25 | Loss: 0.00140025
Iteration 12/25 | Loss: 0.00140025
Iteration 13/25 | Loss: 0.00140025
Iteration 14/25 | Loss: 0.00140025
Iteration 15/25 | Loss: 0.00140025
Iteration 16/25 | Loss: 0.00140025
Iteration 17/25 | Loss: 0.00140025
Iteration 18/25 | Loss: 0.00140025
Iteration 19/25 | Loss: 0.00140025
Iteration 20/25 | Loss: 0.00140025
Iteration 21/25 | Loss: 0.00140025
Iteration 22/25 | Loss: 0.00140025
Iteration 23/25 | Loss: 0.00140025
Iteration 24/25 | Loss: 0.00140025
Iteration 25/25 | Loss: 0.00140025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23266101
Iteration 2/25 | Loss: 0.00081879
Iteration 3/25 | Loss: 0.00081879
Iteration 4/25 | Loss: 0.00081879
Iteration 5/25 | Loss: 0.00081879
Iteration 6/25 | Loss: 0.00081879
Iteration 7/25 | Loss: 0.00081879
Iteration 8/25 | Loss: 0.00081879
Iteration 9/25 | Loss: 0.00081879
Iteration 10/25 | Loss: 0.00081879
Iteration 11/25 | Loss: 0.00081879
Iteration 12/25 | Loss: 0.00081879
Iteration 13/25 | Loss: 0.00081879
Iteration 14/25 | Loss: 0.00081879
Iteration 15/25 | Loss: 0.00081879
Iteration 16/25 | Loss: 0.00081879
Iteration 17/25 | Loss: 0.00081879
Iteration 18/25 | Loss: 0.00081879
Iteration 19/25 | Loss: 0.00081879
Iteration 20/25 | Loss: 0.00081879
Iteration 21/25 | Loss: 0.00081879
Iteration 22/25 | Loss: 0.00081879
Iteration 23/25 | Loss: 0.00081879
Iteration 24/25 | Loss: 0.00081879
Iteration 25/25 | Loss: 0.00081879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081879
Iteration 2/1000 | Loss: 0.00003537
Iteration 3/1000 | Loss: 0.00002778
Iteration 4/1000 | Loss: 0.00002559
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00002369
Iteration 7/1000 | Loss: 0.00002300
Iteration 8/1000 | Loss: 0.00002264
Iteration 9/1000 | Loss: 0.00002239
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002208
Iteration 12/1000 | Loss: 0.00002204
Iteration 13/1000 | Loss: 0.00002196
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002193
Iteration 17/1000 | Loss: 0.00002192
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002190
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002189
Iteration 23/1000 | Loss: 0.00002188
Iteration 24/1000 | Loss: 0.00002185
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002184
Iteration 27/1000 | Loss: 0.00002184
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002183
Iteration 31/1000 | Loss: 0.00002183
Iteration 32/1000 | Loss: 0.00002182
Iteration 33/1000 | Loss: 0.00002182
Iteration 34/1000 | Loss: 0.00002182
Iteration 35/1000 | Loss: 0.00002181
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002180
Iteration 38/1000 | Loss: 0.00002180
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002179
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002179
Iteration 47/1000 | Loss: 0.00002179
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002178
Iteration 51/1000 | Loss: 0.00002178
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002178
Iteration 55/1000 | Loss: 0.00002177
Iteration 56/1000 | Loss: 0.00002177
Iteration 57/1000 | Loss: 0.00002177
Iteration 58/1000 | Loss: 0.00002177
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002176
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002176
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002175
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002175
Iteration 79/1000 | Loss: 0.00002175
Iteration 80/1000 | Loss: 0.00002175
Iteration 81/1000 | Loss: 0.00002175
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.1748646759078838e-05, 2.1748646759078838e-05, 2.1748646759078838e-05, 2.1748646759078838e-05, 2.1748646759078838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1748646759078838e-05

Optimization complete. Final v2v error: 3.8996429443359375 mm

Highest mean error: 4.17067289352417 mm for frame 10

Lowest mean error: 3.6691219806671143 mm for frame 214

Saving results

Total time: 32.27624177932739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553064
Iteration 2/25 | Loss: 0.00146232
Iteration 3/25 | Loss: 0.00132116
Iteration 4/25 | Loss: 0.00130262
Iteration 5/25 | Loss: 0.00129777
Iteration 6/25 | Loss: 0.00129692
Iteration 7/25 | Loss: 0.00129692
Iteration 8/25 | Loss: 0.00129692
Iteration 9/25 | Loss: 0.00129692
Iteration 10/25 | Loss: 0.00129692
Iteration 11/25 | Loss: 0.00129692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012969160452485085, 0.0012969160452485085, 0.0012969160452485085, 0.0012969160452485085, 0.0012969160452485085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012969160452485085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71033192
Iteration 2/25 | Loss: 0.00125737
Iteration 3/25 | Loss: 0.00125735
Iteration 4/25 | Loss: 0.00125735
Iteration 5/25 | Loss: 0.00125734
Iteration 6/25 | Loss: 0.00125734
Iteration 7/25 | Loss: 0.00125734
Iteration 8/25 | Loss: 0.00125734
Iteration 9/25 | Loss: 0.00125734
Iteration 10/25 | Loss: 0.00125734
Iteration 11/25 | Loss: 0.00125734
Iteration 12/25 | Loss: 0.00125734
Iteration 13/25 | Loss: 0.00125734
Iteration 14/25 | Loss: 0.00125734
Iteration 15/25 | Loss: 0.00125734
Iteration 16/25 | Loss: 0.00125734
Iteration 17/25 | Loss: 0.00125734
Iteration 18/25 | Loss: 0.00125734
Iteration 19/25 | Loss: 0.00125734
Iteration 20/25 | Loss: 0.00125734
Iteration 21/25 | Loss: 0.00125734
Iteration 22/25 | Loss: 0.00125734
Iteration 23/25 | Loss: 0.00125734
Iteration 24/25 | Loss: 0.00125734
Iteration 25/25 | Loss: 0.00125734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012573425192385912, 0.0012573425192385912, 0.0012573425192385912, 0.0012573425192385912, 0.0012573425192385912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012573425192385912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125734
Iteration 2/1000 | Loss: 0.00005183
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002824
Iteration 5/1000 | Loss: 0.00002647
Iteration 6/1000 | Loss: 0.00002499
Iteration 7/1000 | Loss: 0.00002394
Iteration 8/1000 | Loss: 0.00002329
Iteration 9/1000 | Loss: 0.00002279
Iteration 10/1000 | Loss: 0.00002234
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002161
Iteration 13/1000 | Loss: 0.00002140
Iteration 14/1000 | Loss: 0.00002124
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002111
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002108
Iteration 19/1000 | Loss: 0.00002107
Iteration 20/1000 | Loss: 0.00002106
Iteration 21/1000 | Loss: 0.00002104
Iteration 22/1000 | Loss: 0.00002104
Iteration 23/1000 | Loss: 0.00002103
Iteration 24/1000 | Loss: 0.00002102
Iteration 25/1000 | Loss: 0.00002102
Iteration 26/1000 | Loss: 0.00002101
Iteration 27/1000 | Loss: 0.00002101
Iteration 28/1000 | Loss: 0.00002100
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002098
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002097
Iteration 34/1000 | Loss: 0.00002097
Iteration 35/1000 | Loss: 0.00002096
Iteration 36/1000 | Loss: 0.00002096
Iteration 37/1000 | Loss: 0.00002095
Iteration 38/1000 | Loss: 0.00002094
Iteration 39/1000 | Loss: 0.00002092
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002091
Iteration 42/1000 | Loss: 0.00002091
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002089
Iteration 45/1000 | Loss: 0.00002086
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002082
Iteration 51/1000 | Loss: 0.00002081
Iteration 52/1000 | Loss: 0.00002081
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002080
Iteration 55/1000 | Loss: 0.00002080
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00002078
Iteration 59/1000 | Loss: 0.00002077
Iteration 60/1000 | Loss: 0.00002077
Iteration 61/1000 | Loss: 0.00002077
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002076
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002076
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002074
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002073
Iteration 74/1000 | Loss: 0.00002073
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002072
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002070
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002070
Iteration 94/1000 | Loss: 0.00002070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.0695728380815126e-05, 2.0695728380815126e-05, 2.0695728380815126e-05, 2.0695728380815126e-05, 2.0695728380815126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0695728380815126e-05

Optimization complete. Final v2v error: 3.7670352458953857 mm

Highest mean error: 4.502294540405273 mm for frame 226

Lowest mean error: 3.3280744552612305 mm for frame 67

Saving results

Total time: 42.37530207633972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721331
Iteration 2/25 | Loss: 0.00152121
Iteration 3/25 | Loss: 0.00140918
Iteration 4/25 | Loss: 0.00139665
Iteration 5/25 | Loss: 0.00139171
Iteration 6/25 | Loss: 0.00139095
Iteration 7/25 | Loss: 0.00139095
Iteration 8/25 | Loss: 0.00139095
Iteration 9/25 | Loss: 0.00139095
Iteration 10/25 | Loss: 0.00139095
Iteration 11/25 | Loss: 0.00139095
Iteration 12/25 | Loss: 0.00139095
Iteration 13/25 | Loss: 0.00139095
Iteration 14/25 | Loss: 0.00139095
Iteration 15/25 | Loss: 0.00139095
Iteration 16/25 | Loss: 0.00139095
Iteration 17/25 | Loss: 0.00139095
Iteration 18/25 | Loss: 0.00139095
Iteration 19/25 | Loss: 0.00139095
Iteration 20/25 | Loss: 0.00139095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013909466797485948, 0.0013909466797485948, 0.0013909466797485948, 0.0013909466797485948, 0.0013909466797485948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013909466797485948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79221642
Iteration 2/25 | Loss: 0.00104929
Iteration 3/25 | Loss: 0.00104924
Iteration 4/25 | Loss: 0.00104924
Iteration 5/25 | Loss: 0.00104924
Iteration 6/25 | Loss: 0.00104924
Iteration 7/25 | Loss: 0.00104924
Iteration 8/25 | Loss: 0.00104924
Iteration 9/25 | Loss: 0.00104924
Iteration 10/25 | Loss: 0.00104924
Iteration 11/25 | Loss: 0.00104924
Iteration 12/25 | Loss: 0.00104923
Iteration 13/25 | Loss: 0.00104923
Iteration 14/25 | Loss: 0.00104923
Iteration 15/25 | Loss: 0.00104923
Iteration 16/25 | Loss: 0.00104923
Iteration 17/25 | Loss: 0.00104923
Iteration 18/25 | Loss: 0.00104923
Iteration 19/25 | Loss: 0.00104923
Iteration 20/25 | Loss: 0.00104923
Iteration 21/25 | Loss: 0.00104923
Iteration 22/25 | Loss: 0.00104923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010492348810657859, 0.0010492348810657859, 0.0010492348810657859, 0.0010492348810657859, 0.0010492348810657859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010492348810657859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104923
Iteration 2/1000 | Loss: 0.00007960
Iteration 3/1000 | Loss: 0.00004918
Iteration 4/1000 | Loss: 0.00004008
Iteration 5/1000 | Loss: 0.00003646
Iteration 6/1000 | Loss: 0.00003479
Iteration 7/1000 | Loss: 0.00003407
Iteration 8/1000 | Loss: 0.00003339
Iteration 9/1000 | Loss: 0.00003276
Iteration 10/1000 | Loss: 0.00003221
Iteration 11/1000 | Loss: 0.00003175
Iteration 12/1000 | Loss: 0.00003139
Iteration 13/1000 | Loss: 0.00003111
Iteration 14/1000 | Loss: 0.00003088
Iteration 15/1000 | Loss: 0.00003063
Iteration 16/1000 | Loss: 0.00003036
Iteration 17/1000 | Loss: 0.00003018
Iteration 18/1000 | Loss: 0.00003006
Iteration 19/1000 | Loss: 0.00003005
Iteration 20/1000 | Loss: 0.00002999
Iteration 21/1000 | Loss: 0.00002986
Iteration 22/1000 | Loss: 0.00002974
Iteration 23/1000 | Loss: 0.00002971
Iteration 24/1000 | Loss: 0.00002971
Iteration 25/1000 | Loss: 0.00002965
Iteration 26/1000 | Loss: 0.00002963
Iteration 27/1000 | Loss: 0.00002963
Iteration 28/1000 | Loss: 0.00002962
Iteration 29/1000 | Loss: 0.00002960
Iteration 30/1000 | Loss: 0.00002959
Iteration 31/1000 | Loss: 0.00002959
Iteration 32/1000 | Loss: 0.00002958
Iteration 33/1000 | Loss: 0.00002958
Iteration 34/1000 | Loss: 0.00002958
Iteration 35/1000 | Loss: 0.00002957
Iteration 36/1000 | Loss: 0.00002957
Iteration 37/1000 | Loss: 0.00002957
Iteration 38/1000 | Loss: 0.00002957
Iteration 39/1000 | Loss: 0.00002956
Iteration 40/1000 | Loss: 0.00002956
Iteration 41/1000 | Loss: 0.00002956
Iteration 42/1000 | Loss: 0.00002955
Iteration 43/1000 | Loss: 0.00002955
Iteration 44/1000 | Loss: 0.00002954
Iteration 45/1000 | Loss: 0.00002954
Iteration 46/1000 | Loss: 0.00002954
Iteration 47/1000 | Loss: 0.00002954
Iteration 48/1000 | Loss: 0.00002952
Iteration 49/1000 | Loss: 0.00002952
Iteration 50/1000 | Loss: 0.00002952
Iteration 51/1000 | Loss: 0.00002952
Iteration 52/1000 | Loss: 0.00002952
Iteration 53/1000 | Loss: 0.00002952
Iteration 54/1000 | Loss: 0.00002952
Iteration 55/1000 | Loss: 0.00002952
Iteration 56/1000 | Loss: 0.00002952
Iteration 57/1000 | Loss: 0.00002952
Iteration 58/1000 | Loss: 0.00002950
Iteration 59/1000 | Loss: 0.00002950
Iteration 60/1000 | Loss: 0.00002949
Iteration 61/1000 | Loss: 0.00002949
Iteration 62/1000 | Loss: 0.00002948
Iteration 63/1000 | Loss: 0.00002947
Iteration 64/1000 | Loss: 0.00002947
Iteration 65/1000 | Loss: 0.00002947
Iteration 66/1000 | Loss: 0.00002946
Iteration 67/1000 | Loss: 0.00002946
Iteration 68/1000 | Loss: 0.00002945
Iteration 69/1000 | Loss: 0.00002945
Iteration 70/1000 | Loss: 0.00002945
Iteration 71/1000 | Loss: 0.00002945
Iteration 72/1000 | Loss: 0.00002945
Iteration 73/1000 | Loss: 0.00002944
Iteration 74/1000 | Loss: 0.00002944
Iteration 75/1000 | Loss: 0.00002944
Iteration 76/1000 | Loss: 0.00002944
Iteration 77/1000 | Loss: 0.00002944
Iteration 78/1000 | Loss: 0.00002944
Iteration 79/1000 | Loss: 0.00002944
Iteration 80/1000 | Loss: 0.00002944
Iteration 81/1000 | Loss: 0.00002944
Iteration 82/1000 | Loss: 0.00002944
Iteration 83/1000 | Loss: 0.00002943
Iteration 84/1000 | Loss: 0.00002943
Iteration 85/1000 | Loss: 0.00002943
Iteration 86/1000 | Loss: 0.00002943
Iteration 87/1000 | Loss: 0.00002942
Iteration 88/1000 | Loss: 0.00002942
Iteration 89/1000 | Loss: 0.00002941
Iteration 90/1000 | Loss: 0.00002941
Iteration 91/1000 | Loss: 0.00002941
Iteration 92/1000 | Loss: 0.00002941
Iteration 93/1000 | Loss: 0.00002941
Iteration 94/1000 | Loss: 0.00002941
Iteration 95/1000 | Loss: 0.00002941
Iteration 96/1000 | Loss: 0.00002940
Iteration 97/1000 | Loss: 0.00002940
Iteration 98/1000 | Loss: 0.00002940
Iteration 99/1000 | Loss: 0.00002940
Iteration 100/1000 | Loss: 0.00002940
Iteration 101/1000 | Loss: 0.00002940
Iteration 102/1000 | Loss: 0.00002940
Iteration 103/1000 | Loss: 0.00002939
Iteration 104/1000 | Loss: 0.00002939
Iteration 105/1000 | Loss: 0.00002939
Iteration 106/1000 | Loss: 0.00002938
Iteration 107/1000 | Loss: 0.00002938
Iteration 108/1000 | Loss: 0.00002938
Iteration 109/1000 | Loss: 0.00002938
Iteration 110/1000 | Loss: 0.00002938
Iteration 111/1000 | Loss: 0.00002938
Iteration 112/1000 | Loss: 0.00002937
Iteration 113/1000 | Loss: 0.00002937
Iteration 114/1000 | Loss: 0.00002937
Iteration 115/1000 | Loss: 0.00002936
Iteration 116/1000 | Loss: 0.00002936
Iteration 117/1000 | Loss: 0.00002936
Iteration 118/1000 | Loss: 0.00002936
Iteration 119/1000 | Loss: 0.00002936
Iteration 120/1000 | Loss: 0.00002935
Iteration 121/1000 | Loss: 0.00002935
Iteration 122/1000 | Loss: 0.00002935
Iteration 123/1000 | Loss: 0.00002935
Iteration 124/1000 | Loss: 0.00002935
Iteration 125/1000 | Loss: 0.00002935
Iteration 126/1000 | Loss: 0.00002935
Iteration 127/1000 | Loss: 0.00002935
Iteration 128/1000 | Loss: 0.00002935
Iteration 129/1000 | Loss: 0.00002935
Iteration 130/1000 | Loss: 0.00002934
Iteration 131/1000 | Loss: 0.00002934
Iteration 132/1000 | Loss: 0.00002934
Iteration 133/1000 | Loss: 0.00002934
Iteration 134/1000 | Loss: 0.00002934
Iteration 135/1000 | Loss: 0.00002934
Iteration 136/1000 | Loss: 0.00002934
Iteration 137/1000 | Loss: 0.00002934
Iteration 138/1000 | Loss: 0.00002934
Iteration 139/1000 | Loss: 0.00002934
Iteration 140/1000 | Loss: 0.00002933
Iteration 141/1000 | Loss: 0.00002933
Iteration 142/1000 | Loss: 0.00002933
Iteration 143/1000 | Loss: 0.00002933
Iteration 144/1000 | Loss: 0.00002933
Iteration 145/1000 | Loss: 0.00002933
Iteration 146/1000 | Loss: 0.00002933
Iteration 147/1000 | Loss: 0.00002933
Iteration 148/1000 | Loss: 0.00002933
Iteration 149/1000 | Loss: 0.00002932
Iteration 150/1000 | Loss: 0.00002932
Iteration 151/1000 | Loss: 0.00002932
Iteration 152/1000 | Loss: 0.00002932
Iteration 153/1000 | Loss: 0.00002932
Iteration 154/1000 | Loss: 0.00002932
Iteration 155/1000 | Loss: 0.00002932
Iteration 156/1000 | Loss: 0.00002932
Iteration 157/1000 | Loss: 0.00002932
Iteration 158/1000 | Loss: 0.00002932
Iteration 159/1000 | Loss: 0.00002932
Iteration 160/1000 | Loss: 0.00002931
Iteration 161/1000 | Loss: 0.00002931
Iteration 162/1000 | Loss: 0.00002931
Iteration 163/1000 | Loss: 0.00002931
Iteration 164/1000 | Loss: 0.00002931
Iteration 165/1000 | Loss: 0.00002931
Iteration 166/1000 | Loss: 0.00002930
Iteration 167/1000 | Loss: 0.00002930
Iteration 168/1000 | Loss: 0.00002930
Iteration 169/1000 | Loss: 0.00002930
Iteration 170/1000 | Loss: 0.00002929
Iteration 171/1000 | Loss: 0.00002929
Iteration 172/1000 | Loss: 0.00002929
Iteration 173/1000 | Loss: 0.00002929
Iteration 174/1000 | Loss: 0.00002929
Iteration 175/1000 | Loss: 0.00002929
Iteration 176/1000 | Loss: 0.00002929
Iteration 177/1000 | Loss: 0.00002928
Iteration 178/1000 | Loss: 0.00002928
Iteration 179/1000 | Loss: 0.00002928
Iteration 180/1000 | Loss: 0.00002928
Iteration 181/1000 | Loss: 0.00002928
Iteration 182/1000 | Loss: 0.00002928
Iteration 183/1000 | Loss: 0.00002928
Iteration 184/1000 | Loss: 0.00002928
Iteration 185/1000 | Loss: 0.00002927
Iteration 186/1000 | Loss: 0.00002927
Iteration 187/1000 | Loss: 0.00002927
Iteration 188/1000 | Loss: 0.00002927
Iteration 189/1000 | Loss: 0.00002927
Iteration 190/1000 | Loss: 0.00002927
Iteration 191/1000 | Loss: 0.00002927
Iteration 192/1000 | Loss: 0.00002927
Iteration 193/1000 | Loss: 0.00002927
Iteration 194/1000 | Loss: 0.00002926
Iteration 195/1000 | Loss: 0.00002926
Iteration 196/1000 | Loss: 0.00002926
Iteration 197/1000 | Loss: 0.00002926
Iteration 198/1000 | Loss: 0.00002926
Iteration 199/1000 | Loss: 0.00002926
Iteration 200/1000 | Loss: 0.00002926
Iteration 201/1000 | Loss: 0.00002926
Iteration 202/1000 | Loss: 0.00002925
Iteration 203/1000 | Loss: 0.00002925
Iteration 204/1000 | Loss: 0.00002925
Iteration 205/1000 | Loss: 0.00002925
Iteration 206/1000 | Loss: 0.00002925
Iteration 207/1000 | Loss: 0.00002924
Iteration 208/1000 | Loss: 0.00002924
Iteration 209/1000 | Loss: 0.00002924
Iteration 210/1000 | Loss: 0.00002924
Iteration 211/1000 | Loss: 0.00002924
Iteration 212/1000 | Loss: 0.00002924
Iteration 213/1000 | Loss: 0.00002924
Iteration 214/1000 | Loss: 0.00002923
Iteration 215/1000 | Loss: 0.00002923
Iteration 216/1000 | Loss: 0.00002923
Iteration 217/1000 | Loss: 0.00002923
Iteration 218/1000 | Loss: 0.00002923
Iteration 219/1000 | Loss: 0.00002923
Iteration 220/1000 | Loss: 0.00002923
Iteration 221/1000 | Loss: 0.00002922
Iteration 222/1000 | Loss: 0.00002922
Iteration 223/1000 | Loss: 0.00002922
Iteration 224/1000 | Loss: 0.00002922
Iteration 225/1000 | Loss: 0.00002922
Iteration 226/1000 | Loss: 0.00002922
Iteration 227/1000 | Loss: 0.00002922
Iteration 228/1000 | Loss: 0.00002921
Iteration 229/1000 | Loss: 0.00002921
Iteration 230/1000 | Loss: 0.00002921
Iteration 231/1000 | Loss: 0.00002921
Iteration 232/1000 | Loss: 0.00002921
Iteration 233/1000 | Loss: 0.00002921
Iteration 234/1000 | Loss: 0.00002921
Iteration 235/1000 | Loss: 0.00002921
Iteration 236/1000 | Loss: 0.00002921
Iteration 237/1000 | Loss: 0.00002920
Iteration 238/1000 | Loss: 0.00002920
Iteration 239/1000 | Loss: 0.00002920
Iteration 240/1000 | Loss: 0.00002920
Iteration 241/1000 | Loss: 0.00002920
Iteration 242/1000 | Loss: 0.00002920
Iteration 243/1000 | Loss: 0.00002920
Iteration 244/1000 | Loss: 0.00002920
Iteration 245/1000 | Loss: 0.00002920
Iteration 246/1000 | Loss: 0.00002920
Iteration 247/1000 | Loss: 0.00002920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.920048609666992e-05, 2.920048609666992e-05, 2.920048609666992e-05, 2.920048609666992e-05, 2.920048609666992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.920048609666992e-05

Optimization complete. Final v2v error: 4.315773010253906 mm

Highest mean error: 5.729137420654297 mm for frame 127

Lowest mean error: 3.4031758308410645 mm for frame 0

Saving results

Total time: 53.50954556465149
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838200
Iteration 2/25 | Loss: 0.00129364
Iteration 3/25 | Loss: 0.00122989
Iteration 4/25 | Loss: 0.00121864
Iteration 5/25 | Loss: 0.00121456
Iteration 6/25 | Loss: 0.00121351
Iteration 7/25 | Loss: 0.00121348
Iteration 8/25 | Loss: 0.00121348
Iteration 9/25 | Loss: 0.00121348
Iteration 10/25 | Loss: 0.00121348
Iteration 11/25 | Loss: 0.00121348
Iteration 12/25 | Loss: 0.00121348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012134762946516275, 0.0012134762946516275, 0.0012134762946516275, 0.0012134762946516275, 0.0012134762946516275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012134762946516275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99514055
Iteration 2/25 | Loss: 0.00100521
Iteration 3/25 | Loss: 0.00100521
Iteration 4/25 | Loss: 0.00100521
Iteration 5/25 | Loss: 0.00100521
Iteration 6/25 | Loss: 0.00100521
Iteration 7/25 | Loss: 0.00100521
Iteration 8/25 | Loss: 0.00100521
Iteration 9/25 | Loss: 0.00100521
Iteration 10/25 | Loss: 0.00100521
Iteration 11/25 | Loss: 0.00100520
Iteration 12/25 | Loss: 0.00100520
Iteration 13/25 | Loss: 0.00100520
Iteration 14/25 | Loss: 0.00100520
Iteration 15/25 | Loss: 0.00100520
Iteration 16/25 | Loss: 0.00100520
Iteration 17/25 | Loss: 0.00100520
Iteration 18/25 | Loss: 0.00100520
Iteration 19/25 | Loss: 0.00100520
Iteration 20/25 | Loss: 0.00100520
Iteration 21/25 | Loss: 0.00100520
Iteration 22/25 | Loss: 0.00100520
Iteration 23/25 | Loss: 0.00100520
Iteration 24/25 | Loss: 0.00100520
Iteration 25/25 | Loss: 0.00100520

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100520
Iteration 2/1000 | Loss: 0.00002573
Iteration 3/1000 | Loss: 0.00001813
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001451
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001356
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001229
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001198
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001196
Iteration 120/1000 | Loss: 0.00001196
Iteration 121/1000 | Loss: 0.00001196
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001194
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001194
Iteration 146/1000 | Loss: 0.00001194
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001193
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001193
Iteration 158/1000 | Loss: 0.00001193
Iteration 159/1000 | Loss: 0.00001193
Iteration 160/1000 | Loss: 0.00001193
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001192
Iteration 164/1000 | Loss: 0.00001192
Iteration 165/1000 | Loss: 0.00001192
Iteration 166/1000 | Loss: 0.00001192
Iteration 167/1000 | Loss: 0.00001192
Iteration 168/1000 | Loss: 0.00001192
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Iteration 177/1000 | Loss: 0.00001192
Iteration 178/1000 | Loss: 0.00001192
Iteration 179/1000 | Loss: 0.00001192
Iteration 180/1000 | Loss: 0.00001192
Iteration 181/1000 | Loss: 0.00001192
Iteration 182/1000 | Loss: 0.00001192
Iteration 183/1000 | Loss: 0.00001192
Iteration 184/1000 | Loss: 0.00001192
Iteration 185/1000 | Loss: 0.00001192
Iteration 186/1000 | Loss: 0.00001192
Iteration 187/1000 | Loss: 0.00001191
Iteration 188/1000 | Loss: 0.00001191
Iteration 189/1000 | Loss: 0.00001191
Iteration 190/1000 | Loss: 0.00001191
Iteration 191/1000 | Loss: 0.00001191
Iteration 192/1000 | Loss: 0.00001191
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Iteration 204/1000 | Loss: 0.00001191
Iteration 205/1000 | Loss: 0.00001191
Iteration 206/1000 | Loss: 0.00001191
Iteration 207/1000 | Loss: 0.00001191
Iteration 208/1000 | Loss: 0.00001191
Iteration 209/1000 | Loss: 0.00001191
Iteration 210/1000 | Loss: 0.00001191
Iteration 211/1000 | Loss: 0.00001191
Iteration 212/1000 | Loss: 0.00001191
Iteration 213/1000 | Loss: 0.00001191
Iteration 214/1000 | Loss: 0.00001191
Iteration 215/1000 | Loss: 0.00001191
Iteration 216/1000 | Loss: 0.00001191
Iteration 217/1000 | Loss: 0.00001191
Iteration 218/1000 | Loss: 0.00001191
Iteration 219/1000 | Loss: 0.00001191
Iteration 220/1000 | Loss: 0.00001191
Iteration 221/1000 | Loss: 0.00001191
Iteration 222/1000 | Loss: 0.00001191
Iteration 223/1000 | Loss: 0.00001191
Iteration 224/1000 | Loss: 0.00001191
Iteration 225/1000 | Loss: 0.00001191
Iteration 226/1000 | Loss: 0.00001191
Iteration 227/1000 | Loss: 0.00001191
Iteration 228/1000 | Loss: 0.00001191
Iteration 229/1000 | Loss: 0.00001191
Iteration 230/1000 | Loss: 0.00001191
Iteration 231/1000 | Loss: 0.00001191
Iteration 232/1000 | Loss: 0.00001191
Iteration 233/1000 | Loss: 0.00001191
Iteration 234/1000 | Loss: 0.00001191
Iteration 235/1000 | Loss: 0.00001191
Iteration 236/1000 | Loss: 0.00001191
Iteration 237/1000 | Loss: 0.00001191
Iteration 238/1000 | Loss: 0.00001191
Iteration 239/1000 | Loss: 0.00001191
Iteration 240/1000 | Loss: 0.00001191
Iteration 241/1000 | Loss: 0.00001191
Iteration 242/1000 | Loss: 0.00001191
Iteration 243/1000 | Loss: 0.00001191
Iteration 244/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.1911945875908714e-05, 1.1911945875908714e-05, 1.1911945875908714e-05, 1.1911945875908714e-05, 1.1911945875908714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1911945875908714e-05

Optimization complete. Final v2v error: 2.939476251602173 mm

Highest mean error: 3.467817783355713 mm for frame 61

Lowest mean error: 2.707087755203247 mm for frame 99

Saving results

Total time: 40.87264800071716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399927
Iteration 2/25 | Loss: 0.00126569
Iteration 3/25 | Loss: 0.00120563
Iteration 4/25 | Loss: 0.00119845
Iteration 5/25 | Loss: 0.00119707
Iteration 6/25 | Loss: 0.00119707
Iteration 7/25 | Loss: 0.00119707
Iteration 8/25 | Loss: 0.00119707
Iteration 9/25 | Loss: 0.00119707
Iteration 10/25 | Loss: 0.00119707
Iteration 11/25 | Loss: 0.00119707
Iteration 12/25 | Loss: 0.00119707
Iteration 13/25 | Loss: 0.00119707
Iteration 14/25 | Loss: 0.00119707
Iteration 15/25 | Loss: 0.00119707
Iteration 16/25 | Loss: 0.00119707
Iteration 17/25 | Loss: 0.00119707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011970706982538104, 0.0011970706982538104, 0.0011970706982538104, 0.0011970706982538104, 0.0011970706982538104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011970706982538104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97254777
Iteration 2/25 | Loss: 0.00089875
Iteration 3/25 | Loss: 0.00089874
Iteration 4/25 | Loss: 0.00089874
Iteration 5/25 | Loss: 0.00089874
Iteration 6/25 | Loss: 0.00089874
Iteration 7/25 | Loss: 0.00089874
Iteration 8/25 | Loss: 0.00089874
Iteration 9/25 | Loss: 0.00089874
Iteration 10/25 | Loss: 0.00089874
Iteration 11/25 | Loss: 0.00089874
Iteration 12/25 | Loss: 0.00089874
Iteration 13/25 | Loss: 0.00089874
Iteration 14/25 | Loss: 0.00089874
Iteration 15/25 | Loss: 0.00089874
Iteration 16/25 | Loss: 0.00089874
Iteration 17/25 | Loss: 0.00089874
Iteration 18/25 | Loss: 0.00089874
Iteration 19/25 | Loss: 0.00089874
Iteration 20/25 | Loss: 0.00089874
Iteration 21/25 | Loss: 0.00089874
Iteration 22/25 | Loss: 0.00089874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008987373439595103, 0.0008987373439595103, 0.0008987373439595103, 0.0008987373439595103, 0.0008987373439595103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008987373439595103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089874
Iteration 2/1000 | Loss: 0.00002046
Iteration 3/1000 | Loss: 0.00001546
Iteration 4/1000 | Loss: 0.00001418
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001291
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001181
Iteration 14/1000 | Loss: 0.00001180
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001162
Iteration 17/1000 | Loss: 0.00001159
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001129
Iteration 36/1000 | Loss: 0.00001129
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001124
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001123
Iteration 44/1000 | Loss: 0.00001123
Iteration 45/1000 | Loss: 0.00001123
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001122
Iteration 49/1000 | Loss: 0.00001122
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001118
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001107
Iteration 87/1000 | Loss: 0.00001107
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001107
Iteration 93/1000 | Loss: 0.00001107
Iteration 94/1000 | Loss: 0.00001106
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001103
Iteration 102/1000 | Loss: 0.00001103
Iteration 103/1000 | Loss: 0.00001103
Iteration 104/1000 | Loss: 0.00001103
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001102
Iteration 109/1000 | Loss: 0.00001102
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001101
Iteration 113/1000 | Loss: 0.00001101
Iteration 114/1000 | Loss: 0.00001100
Iteration 115/1000 | Loss: 0.00001100
Iteration 116/1000 | Loss: 0.00001100
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001099
Iteration 119/1000 | Loss: 0.00001099
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001098
Iteration 129/1000 | Loss: 0.00001098
Iteration 130/1000 | Loss: 0.00001098
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001097
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001097
Iteration 141/1000 | Loss: 0.00001097
Iteration 142/1000 | Loss: 0.00001097
Iteration 143/1000 | Loss: 0.00001096
Iteration 144/1000 | Loss: 0.00001096
Iteration 145/1000 | Loss: 0.00001096
Iteration 146/1000 | Loss: 0.00001096
Iteration 147/1000 | Loss: 0.00001096
Iteration 148/1000 | Loss: 0.00001096
Iteration 149/1000 | Loss: 0.00001096
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001095
Iteration 152/1000 | Loss: 0.00001095
Iteration 153/1000 | Loss: 0.00001095
Iteration 154/1000 | Loss: 0.00001095
Iteration 155/1000 | Loss: 0.00001095
Iteration 156/1000 | Loss: 0.00001095
Iteration 157/1000 | Loss: 0.00001094
Iteration 158/1000 | Loss: 0.00001094
Iteration 159/1000 | Loss: 0.00001094
Iteration 160/1000 | Loss: 0.00001094
Iteration 161/1000 | Loss: 0.00001094
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001094
Iteration 164/1000 | Loss: 0.00001094
Iteration 165/1000 | Loss: 0.00001094
Iteration 166/1000 | Loss: 0.00001094
Iteration 167/1000 | Loss: 0.00001094
Iteration 168/1000 | Loss: 0.00001093
Iteration 169/1000 | Loss: 0.00001093
Iteration 170/1000 | Loss: 0.00001093
Iteration 171/1000 | Loss: 0.00001093
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001093
Iteration 174/1000 | Loss: 0.00001093
Iteration 175/1000 | Loss: 0.00001093
Iteration 176/1000 | Loss: 0.00001093
Iteration 177/1000 | Loss: 0.00001093
Iteration 178/1000 | Loss: 0.00001092
Iteration 179/1000 | Loss: 0.00001092
Iteration 180/1000 | Loss: 0.00001092
Iteration 181/1000 | Loss: 0.00001092
Iteration 182/1000 | Loss: 0.00001092
Iteration 183/1000 | Loss: 0.00001092
Iteration 184/1000 | Loss: 0.00001092
Iteration 185/1000 | Loss: 0.00001092
Iteration 186/1000 | Loss: 0.00001092
Iteration 187/1000 | Loss: 0.00001092
Iteration 188/1000 | Loss: 0.00001092
Iteration 189/1000 | Loss: 0.00001092
Iteration 190/1000 | Loss: 0.00001092
Iteration 191/1000 | Loss: 0.00001092
Iteration 192/1000 | Loss: 0.00001092
Iteration 193/1000 | Loss: 0.00001092
Iteration 194/1000 | Loss: 0.00001092
Iteration 195/1000 | Loss: 0.00001092
Iteration 196/1000 | Loss: 0.00001092
Iteration 197/1000 | Loss: 0.00001092
Iteration 198/1000 | Loss: 0.00001092
Iteration 199/1000 | Loss: 0.00001092
Iteration 200/1000 | Loss: 0.00001092
Iteration 201/1000 | Loss: 0.00001092
Iteration 202/1000 | Loss: 0.00001092
Iteration 203/1000 | Loss: 0.00001092
Iteration 204/1000 | Loss: 0.00001092
Iteration 205/1000 | Loss: 0.00001092
Iteration 206/1000 | Loss: 0.00001092
Iteration 207/1000 | Loss: 0.00001092
Iteration 208/1000 | Loss: 0.00001092
Iteration 209/1000 | Loss: 0.00001092
Iteration 210/1000 | Loss: 0.00001092
Iteration 211/1000 | Loss: 0.00001092
Iteration 212/1000 | Loss: 0.00001092
Iteration 213/1000 | Loss: 0.00001092
Iteration 214/1000 | Loss: 0.00001092
Iteration 215/1000 | Loss: 0.00001092
Iteration 216/1000 | Loss: 0.00001092
Iteration 217/1000 | Loss: 0.00001092
Iteration 218/1000 | Loss: 0.00001092
Iteration 219/1000 | Loss: 0.00001092
Iteration 220/1000 | Loss: 0.00001092
Iteration 221/1000 | Loss: 0.00001092
Iteration 222/1000 | Loss: 0.00001092
Iteration 223/1000 | Loss: 0.00001092
Iteration 224/1000 | Loss: 0.00001092
Iteration 225/1000 | Loss: 0.00001092
Iteration 226/1000 | Loss: 0.00001092
Iteration 227/1000 | Loss: 0.00001092
Iteration 228/1000 | Loss: 0.00001092
Iteration 229/1000 | Loss: 0.00001092
Iteration 230/1000 | Loss: 0.00001092
Iteration 231/1000 | Loss: 0.00001092
Iteration 232/1000 | Loss: 0.00001092
Iteration 233/1000 | Loss: 0.00001092
Iteration 234/1000 | Loss: 0.00001092
Iteration 235/1000 | Loss: 0.00001092
Iteration 236/1000 | Loss: 0.00001092
Iteration 237/1000 | Loss: 0.00001092
Iteration 238/1000 | Loss: 0.00001092
Iteration 239/1000 | Loss: 0.00001092
Iteration 240/1000 | Loss: 0.00001092
Iteration 241/1000 | Loss: 0.00001092
Iteration 242/1000 | Loss: 0.00001092
Iteration 243/1000 | Loss: 0.00001092
Iteration 244/1000 | Loss: 0.00001092
Iteration 245/1000 | Loss: 0.00001092
Iteration 246/1000 | Loss: 0.00001092
Iteration 247/1000 | Loss: 0.00001092
Iteration 248/1000 | Loss: 0.00001092
Iteration 249/1000 | Loss: 0.00001092
Iteration 250/1000 | Loss: 0.00001092
Iteration 251/1000 | Loss: 0.00001092
Iteration 252/1000 | Loss: 0.00001092
Iteration 253/1000 | Loss: 0.00001092
Iteration 254/1000 | Loss: 0.00001092
Iteration 255/1000 | Loss: 0.00001092
Iteration 256/1000 | Loss: 0.00001092
Iteration 257/1000 | Loss: 0.00001092
Iteration 258/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.0923289664788172e-05, 1.0923289664788172e-05, 1.0923289664788172e-05, 1.0923289664788172e-05, 1.0923289664788172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0923289664788172e-05

Optimization complete. Final v2v error: 2.849534034729004 mm

Highest mean error: 3.179504871368408 mm for frame 82

Lowest mean error: 2.7242157459259033 mm for frame 144

Saving results

Total time: 41.58415961265564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891342
Iteration 2/25 | Loss: 0.00185250
Iteration 3/25 | Loss: 0.00149736
Iteration 4/25 | Loss: 0.00147181
Iteration 5/25 | Loss: 0.00146870
Iteration 6/25 | Loss: 0.00146542
Iteration 7/25 | Loss: 0.00146393
Iteration 8/25 | Loss: 0.00146353
Iteration 9/25 | Loss: 0.00146335
Iteration 10/25 | Loss: 0.00146334
Iteration 11/25 | Loss: 0.00146334
Iteration 12/25 | Loss: 0.00146333
Iteration 13/25 | Loss: 0.00146333
Iteration 14/25 | Loss: 0.00146333
Iteration 15/25 | Loss: 0.00146333
Iteration 16/25 | Loss: 0.00146333
Iteration 17/25 | Loss: 0.00146333
Iteration 18/25 | Loss: 0.00146333
Iteration 19/25 | Loss: 0.00146333
Iteration 20/25 | Loss: 0.00146333
Iteration 21/25 | Loss: 0.00146333
Iteration 22/25 | Loss: 0.00146333
Iteration 23/25 | Loss: 0.00146332
Iteration 24/25 | Loss: 0.00146332
Iteration 25/25 | Loss: 0.00146332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54826367
Iteration 2/25 | Loss: 0.00092726
Iteration 3/25 | Loss: 0.00092726
Iteration 4/25 | Loss: 0.00092726
Iteration 5/25 | Loss: 0.00092726
Iteration 6/25 | Loss: 0.00092726
Iteration 7/25 | Loss: 0.00092726
Iteration 8/25 | Loss: 0.00092726
Iteration 9/25 | Loss: 0.00092726
Iteration 10/25 | Loss: 0.00092726
Iteration 11/25 | Loss: 0.00092726
Iteration 12/25 | Loss: 0.00092726
Iteration 13/25 | Loss: 0.00092726
Iteration 14/25 | Loss: 0.00092726
Iteration 15/25 | Loss: 0.00092726
Iteration 16/25 | Loss: 0.00092726
Iteration 17/25 | Loss: 0.00092726
Iteration 18/25 | Loss: 0.00092726
Iteration 19/25 | Loss: 0.00092726
Iteration 20/25 | Loss: 0.00092726
Iteration 21/25 | Loss: 0.00092726
Iteration 22/25 | Loss: 0.00092726
Iteration 23/25 | Loss: 0.00092726
Iteration 24/25 | Loss: 0.00092726
Iteration 25/25 | Loss: 0.00092726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092726
Iteration 2/1000 | Loss: 0.00006042
Iteration 3/1000 | Loss: 0.00003913
Iteration 4/1000 | Loss: 0.00003357
Iteration 5/1000 | Loss: 0.00003173
Iteration 6/1000 | Loss: 0.00003072
Iteration 7/1000 | Loss: 0.00002991
Iteration 8/1000 | Loss: 0.00002938
Iteration 9/1000 | Loss: 0.00002881
Iteration 10/1000 | Loss: 0.00002837
Iteration 11/1000 | Loss: 0.00002806
Iteration 12/1000 | Loss: 0.00002778
Iteration 13/1000 | Loss: 0.00002755
Iteration 14/1000 | Loss: 0.00002739
Iteration 15/1000 | Loss: 0.00002734
Iteration 16/1000 | Loss: 0.00002730
Iteration 17/1000 | Loss: 0.00002724
Iteration 18/1000 | Loss: 0.00002722
Iteration 19/1000 | Loss: 0.00002722
Iteration 20/1000 | Loss: 0.00002721
Iteration 21/1000 | Loss: 0.00002721
Iteration 22/1000 | Loss: 0.00002721
Iteration 23/1000 | Loss: 0.00002721
Iteration 24/1000 | Loss: 0.00002720
Iteration 25/1000 | Loss: 0.00002719
Iteration 26/1000 | Loss: 0.00002719
Iteration 27/1000 | Loss: 0.00002719
Iteration 28/1000 | Loss: 0.00002719
Iteration 29/1000 | Loss: 0.00002719
Iteration 30/1000 | Loss: 0.00002719
Iteration 31/1000 | Loss: 0.00002719
Iteration 32/1000 | Loss: 0.00002719
Iteration 33/1000 | Loss: 0.00002719
Iteration 34/1000 | Loss: 0.00002719
Iteration 35/1000 | Loss: 0.00002719
Iteration 36/1000 | Loss: 0.00002718
Iteration 37/1000 | Loss: 0.00002718
Iteration 38/1000 | Loss: 0.00002718
Iteration 39/1000 | Loss: 0.00002718
Iteration 40/1000 | Loss: 0.00002718
Iteration 41/1000 | Loss: 0.00002717
Iteration 42/1000 | Loss: 0.00002717
Iteration 43/1000 | Loss: 0.00002717
Iteration 44/1000 | Loss: 0.00002717
Iteration 45/1000 | Loss: 0.00002717
Iteration 46/1000 | Loss: 0.00002716
Iteration 47/1000 | Loss: 0.00002716
Iteration 48/1000 | Loss: 0.00002716
Iteration 49/1000 | Loss: 0.00002716
Iteration 50/1000 | Loss: 0.00002716
Iteration 51/1000 | Loss: 0.00002715
Iteration 52/1000 | Loss: 0.00002715
Iteration 53/1000 | Loss: 0.00002715
Iteration 54/1000 | Loss: 0.00002715
Iteration 55/1000 | Loss: 0.00002715
Iteration 56/1000 | Loss: 0.00002715
Iteration 57/1000 | Loss: 0.00002714
Iteration 58/1000 | Loss: 0.00002714
Iteration 59/1000 | Loss: 0.00002714
Iteration 60/1000 | Loss: 0.00002714
Iteration 61/1000 | Loss: 0.00002714
Iteration 62/1000 | Loss: 0.00002713
Iteration 63/1000 | Loss: 0.00002713
Iteration 64/1000 | Loss: 0.00002713
Iteration 65/1000 | Loss: 0.00002713
Iteration 66/1000 | Loss: 0.00002713
Iteration 67/1000 | Loss: 0.00002713
Iteration 68/1000 | Loss: 0.00002712
Iteration 69/1000 | Loss: 0.00002712
Iteration 70/1000 | Loss: 0.00002712
Iteration 71/1000 | Loss: 0.00002712
Iteration 72/1000 | Loss: 0.00002712
Iteration 73/1000 | Loss: 0.00002712
Iteration 74/1000 | Loss: 0.00002712
Iteration 75/1000 | Loss: 0.00002711
Iteration 76/1000 | Loss: 0.00002711
Iteration 77/1000 | Loss: 0.00002711
Iteration 78/1000 | Loss: 0.00002711
Iteration 79/1000 | Loss: 0.00002711
Iteration 80/1000 | Loss: 0.00002711
Iteration 81/1000 | Loss: 0.00002710
Iteration 82/1000 | Loss: 0.00002710
Iteration 83/1000 | Loss: 0.00002710
Iteration 84/1000 | Loss: 0.00002709
Iteration 85/1000 | Loss: 0.00002709
Iteration 86/1000 | Loss: 0.00002709
Iteration 87/1000 | Loss: 0.00002708
Iteration 88/1000 | Loss: 0.00002708
Iteration 89/1000 | Loss: 0.00002707
Iteration 90/1000 | Loss: 0.00002707
Iteration 91/1000 | Loss: 0.00002707
Iteration 92/1000 | Loss: 0.00002707
Iteration 93/1000 | Loss: 0.00002706
Iteration 94/1000 | Loss: 0.00002706
Iteration 95/1000 | Loss: 0.00002706
Iteration 96/1000 | Loss: 0.00002705
Iteration 97/1000 | Loss: 0.00002705
Iteration 98/1000 | Loss: 0.00002705
Iteration 99/1000 | Loss: 0.00002704
Iteration 100/1000 | Loss: 0.00002704
Iteration 101/1000 | Loss: 0.00002704
Iteration 102/1000 | Loss: 0.00002704
Iteration 103/1000 | Loss: 0.00002704
Iteration 104/1000 | Loss: 0.00002704
Iteration 105/1000 | Loss: 0.00002704
Iteration 106/1000 | Loss: 0.00002704
Iteration 107/1000 | Loss: 0.00002704
Iteration 108/1000 | Loss: 0.00002703
Iteration 109/1000 | Loss: 0.00002703
Iteration 110/1000 | Loss: 0.00002703
Iteration 111/1000 | Loss: 0.00002703
Iteration 112/1000 | Loss: 0.00002703
Iteration 113/1000 | Loss: 0.00002703
Iteration 114/1000 | Loss: 0.00002703
Iteration 115/1000 | Loss: 0.00002703
Iteration 116/1000 | Loss: 0.00002703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.703325662878342e-05, 2.703325662878342e-05, 2.703325662878342e-05, 2.703325662878342e-05, 2.703325662878342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.703325662878342e-05

Optimization complete. Final v2v error: 4.350632667541504 mm

Highest mean error: 4.943002700805664 mm for frame 17

Lowest mean error: 4.125959396362305 mm for frame 31

Saving results

Total time: 45.49879598617554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969101
Iteration 2/25 | Loss: 0.00969101
Iteration 3/25 | Loss: 0.00330739
Iteration 4/25 | Loss: 0.00229610
Iteration 5/25 | Loss: 0.00199863
Iteration 6/25 | Loss: 0.00195088
Iteration 7/25 | Loss: 0.00174990
Iteration 8/25 | Loss: 0.00167023
Iteration 9/25 | Loss: 0.00155679
Iteration 10/25 | Loss: 0.00155312
Iteration 11/25 | Loss: 0.00155271
Iteration 12/25 | Loss: 0.00151839
Iteration 13/25 | Loss: 0.00152322
Iteration 14/25 | Loss: 0.00151447
Iteration 15/25 | Loss: 0.00151144
Iteration 16/25 | Loss: 0.00151094
Iteration 17/25 | Loss: 0.00151074
Iteration 18/25 | Loss: 0.00151069
Iteration 19/25 | Loss: 0.00151069
Iteration 20/25 | Loss: 0.00151069
Iteration 21/25 | Loss: 0.00151068
Iteration 22/25 | Loss: 0.00151068
Iteration 23/25 | Loss: 0.00151068
Iteration 24/25 | Loss: 0.00151068
Iteration 25/25 | Loss: 0.00151068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34339929
Iteration 2/25 | Loss: 0.00171561
Iteration 3/25 | Loss: 0.00129886
Iteration 4/25 | Loss: 0.00129886
Iteration 5/25 | Loss: 0.00129886
Iteration 6/25 | Loss: 0.00129886
Iteration 7/25 | Loss: 0.00129886
Iteration 8/25 | Loss: 0.00129886
Iteration 9/25 | Loss: 0.00129886
Iteration 10/25 | Loss: 0.00129886
Iteration 11/25 | Loss: 0.00129885
Iteration 12/25 | Loss: 0.00129885
Iteration 13/25 | Loss: 0.00129885
Iteration 14/25 | Loss: 0.00129885
Iteration 15/25 | Loss: 0.00129885
Iteration 16/25 | Loss: 0.00129885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012988547096028924, 0.0012988547096028924, 0.0012988547096028924, 0.0012988547096028924, 0.0012988547096028924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012988547096028924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129885
Iteration 2/1000 | Loss: 0.00082505
Iteration 3/1000 | Loss: 0.00010220
Iteration 4/1000 | Loss: 0.00022141
Iteration 5/1000 | Loss: 0.00008342
Iteration 6/1000 | Loss: 0.00020600
Iteration 7/1000 | Loss: 0.00007916
Iteration 8/1000 | Loss: 0.00007716
Iteration 9/1000 | Loss: 0.00007544
Iteration 10/1000 | Loss: 0.00029831
Iteration 11/1000 | Loss: 0.00007305
Iteration 12/1000 | Loss: 0.00007157
Iteration 13/1000 | Loss: 0.00007020
Iteration 14/1000 | Loss: 0.00006862
Iteration 15/1000 | Loss: 0.00006711
Iteration 16/1000 | Loss: 0.00006609
Iteration 17/1000 | Loss: 0.00006535
Iteration 18/1000 | Loss: 0.00046251
Iteration 19/1000 | Loss: 0.00008654
Iteration 20/1000 | Loss: 0.00023643
Iteration 21/1000 | Loss: 0.00011838
Iteration 22/1000 | Loss: 0.00006523
Iteration 23/1000 | Loss: 0.00008345
Iteration 24/1000 | Loss: 0.00006408
Iteration 25/1000 | Loss: 0.00038131
Iteration 26/1000 | Loss: 0.00022398
Iteration 27/1000 | Loss: 0.00007417
Iteration 28/1000 | Loss: 0.00013696
Iteration 29/1000 | Loss: 0.00153552
Iteration 30/1000 | Loss: 0.00330598
Iteration 31/1000 | Loss: 0.00129942
Iteration 32/1000 | Loss: 0.00065835
Iteration 33/1000 | Loss: 0.00111645
Iteration 34/1000 | Loss: 0.00041935
Iteration 35/1000 | Loss: 0.00074928
Iteration 36/1000 | Loss: 0.00038856
Iteration 37/1000 | Loss: 0.00070606
Iteration 38/1000 | Loss: 0.00046947
Iteration 39/1000 | Loss: 0.00130638
Iteration 40/1000 | Loss: 0.00027456
Iteration 41/1000 | Loss: 0.00023172
Iteration 42/1000 | Loss: 0.00030825
Iteration 43/1000 | Loss: 0.00018000
Iteration 44/1000 | Loss: 0.00007350
Iteration 45/1000 | Loss: 0.00036115
Iteration 46/1000 | Loss: 0.00012687
Iteration 47/1000 | Loss: 0.00064961
Iteration 48/1000 | Loss: 0.00087578
Iteration 49/1000 | Loss: 0.00021394
Iteration 50/1000 | Loss: 0.00022966
Iteration 51/1000 | Loss: 0.00008884
Iteration 52/1000 | Loss: 0.00009664
Iteration 53/1000 | Loss: 0.00022822
Iteration 54/1000 | Loss: 0.00026897
Iteration 55/1000 | Loss: 0.00015335
Iteration 56/1000 | Loss: 0.00048305
Iteration 57/1000 | Loss: 0.00009464
Iteration 58/1000 | Loss: 0.00011329
Iteration 59/1000 | Loss: 0.00009911
Iteration 60/1000 | Loss: 0.00008111
Iteration 61/1000 | Loss: 0.00024354
Iteration 62/1000 | Loss: 0.00041566
Iteration 63/1000 | Loss: 0.00025871
Iteration 64/1000 | Loss: 0.00034646
Iteration 65/1000 | Loss: 0.00021028
Iteration 66/1000 | Loss: 0.00057918
Iteration 67/1000 | Loss: 0.00025393
Iteration 68/1000 | Loss: 0.00042185
Iteration 69/1000 | Loss: 0.00021764
Iteration 70/1000 | Loss: 0.00015298
Iteration 71/1000 | Loss: 0.00020212
Iteration 72/1000 | Loss: 0.00086900
Iteration 73/1000 | Loss: 0.00014401
Iteration 74/1000 | Loss: 0.00023406
Iteration 75/1000 | Loss: 0.00009487
Iteration 76/1000 | Loss: 0.00134673
Iteration 77/1000 | Loss: 0.00040719
Iteration 78/1000 | Loss: 0.00017925
Iteration 79/1000 | Loss: 0.00072052
Iteration 80/1000 | Loss: 0.00108556
Iteration 81/1000 | Loss: 0.00063609
Iteration 82/1000 | Loss: 0.00013468
Iteration 83/1000 | Loss: 0.00011186
Iteration 84/1000 | Loss: 0.00058923
Iteration 85/1000 | Loss: 0.00038399
Iteration 86/1000 | Loss: 0.00042224
Iteration 87/1000 | Loss: 0.00012786
Iteration 88/1000 | Loss: 0.00013961
Iteration 89/1000 | Loss: 0.00004759
Iteration 90/1000 | Loss: 0.00037227
Iteration 91/1000 | Loss: 0.00017041
Iteration 92/1000 | Loss: 0.00086042
Iteration 93/1000 | Loss: 0.00043769
Iteration 94/1000 | Loss: 0.00009629
Iteration 95/1000 | Loss: 0.00005098
Iteration 96/1000 | Loss: 0.00024555
Iteration 97/1000 | Loss: 0.00004313
Iteration 98/1000 | Loss: 0.00004207
Iteration 99/1000 | Loss: 0.00012392
Iteration 100/1000 | Loss: 0.00093784
Iteration 101/1000 | Loss: 0.00006272
Iteration 102/1000 | Loss: 0.00004167
Iteration 103/1000 | Loss: 0.00013095
Iteration 104/1000 | Loss: 0.00004014
Iteration 105/1000 | Loss: 0.00003969
Iteration 106/1000 | Loss: 0.00003945
Iteration 107/1000 | Loss: 0.00003940
Iteration 108/1000 | Loss: 0.00003926
Iteration 109/1000 | Loss: 0.00003923
Iteration 110/1000 | Loss: 0.00003923
Iteration 111/1000 | Loss: 0.00003922
Iteration 112/1000 | Loss: 0.00003905
Iteration 113/1000 | Loss: 0.00003892
Iteration 114/1000 | Loss: 0.00003890
Iteration 115/1000 | Loss: 0.00003886
Iteration 116/1000 | Loss: 0.00003877
Iteration 117/1000 | Loss: 0.00003877
Iteration 118/1000 | Loss: 0.00003876
Iteration 119/1000 | Loss: 0.00003876
Iteration 120/1000 | Loss: 0.00003876
Iteration 121/1000 | Loss: 0.00003875
Iteration 122/1000 | Loss: 0.00003874
Iteration 123/1000 | Loss: 0.00003872
Iteration 124/1000 | Loss: 0.00003871
Iteration 125/1000 | Loss: 0.00003871
Iteration 126/1000 | Loss: 0.00003870
Iteration 127/1000 | Loss: 0.00003870
Iteration 128/1000 | Loss: 0.00003869
Iteration 129/1000 | Loss: 0.00003869
Iteration 130/1000 | Loss: 0.00003869
Iteration 131/1000 | Loss: 0.00003869
Iteration 132/1000 | Loss: 0.00003868
Iteration 133/1000 | Loss: 0.00003868
Iteration 134/1000 | Loss: 0.00003868
Iteration 135/1000 | Loss: 0.00003867
Iteration 136/1000 | Loss: 0.00003867
Iteration 137/1000 | Loss: 0.00003866
Iteration 138/1000 | Loss: 0.00003866
Iteration 139/1000 | Loss: 0.00003866
Iteration 140/1000 | Loss: 0.00003866
Iteration 141/1000 | Loss: 0.00003865
Iteration 142/1000 | Loss: 0.00003865
Iteration 143/1000 | Loss: 0.00003864
Iteration 144/1000 | Loss: 0.00003864
Iteration 145/1000 | Loss: 0.00003864
Iteration 146/1000 | Loss: 0.00003864
Iteration 147/1000 | Loss: 0.00003864
Iteration 148/1000 | Loss: 0.00003863
Iteration 149/1000 | Loss: 0.00003863
Iteration 150/1000 | Loss: 0.00003863
Iteration 151/1000 | Loss: 0.00003863
Iteration 152/1000 | Loss: 0.00003863
Iteration 153/1000 | Loss: 0.00003863
Iteration 154/1000 | Loss: 0.00003863
Iteration 155/1000 | Loss: 0.00003863
Iteration 156/1000 | Loss: 0.00003863
Iteration 157/1000 | Loss: 0.00003862
Iteration 158/1000 | Loss: 0.00003862
Iteration 159/1000 | Loss: 0.00003862
Iteration 160/1000 | Loss: 0.00003862
Iteration 161/1000 | Loss: 0.00003862
Iteration 162/1000 | Loss: 0.00003862
Iteration 163/1000 | Loss: 0.00003861
Iteration 164/1000 | Loss: 0.00003861
Iteration 165/1000 | Loss: 0.00003861
Iteration 166/1000 | Loss: 0.00003861
Iteration 167/1000 | Loss: 0.00003860
Iteration 168/1000 | Loss: 0.00003860
Iteration 169/1000 | Loss: 0.00003860
Iteration 170/1000 | Loss: 0.00003860
Iteration 171/1000 | Loss: 0.00003859
Iteration 172/1000 | Loss: 0.00003859
Iteration 173/1000 | Loss: 0.00003859
Iteration 174/1000 | Loss: 0.00003859
Iteration 175/1000 | Loss: 0.00003859
Iteration 176/1000 | Loss: 0.00003859
Iteration 177/1000 | Loss: 0.00003859
Iteration 178/1000 | Loss: 0.00003859
Iteration 179/1000 | Loss: 0.00003858
Iteration 180/1000 | Loss: 0.00003858
Iteration 181/1000 | Loss: 0.00003858
Iteration 182/1000 | Loss: 0.00003858
Iteration 183/1000 | Loss: 0.00003857
Iteration 184/1000 | Loss: 0.00003857
Iteration 185/1000 | Loss: 0.00003857
Iteration 186/1000 | Loss: 0.00003854
Iteration 187/1000 | Loss: 0.00003854
Iteration 188/1000 | Loss: 0.00003854
Iteration 189/1000 | Loss: 0.00003854
Iteration 190/1000 | Loss: 0.00003854
Iteration 191/1000 | Loss: 0.00003854
Iteration 192/1000 | Loss: 0.00003854
Iteration 193/1000 | Loss: 0.00003853
Iteration 194/1000 | Loss: 0.00003853
Iteration 195/1000 | Loss: 0.00003853
Iteration 196/1000 | Loss: 0.00003853
Iteration 197/1000 | Loss: 0.00003853
Iteration 198/1000 | Loss: 0.00003853
Iteration 199/1000 | Loss: 0.00003853
Iteration 200/1000 | Loss: 0.00003851
Iteration 201/1000 | Loss: 0.00003851
Iteration 202/1000 | Loss: 0.00003850
Iteration 203/1000 | Loss: 0.00003849
Iteration 204/1000 | Loss: 0.00003849
Iteration 205/1000 | Loss: 0.00003848
Iteration 206/1000 | Loss: 0.00003848
Iteration 207/1000 | Loss: 0.00003848
Iteration 208/1000 | Loss: 0.00003848
Iteration 209/1000 | Loss: 0.00003848
Iteration 210/1000 | Loss: 0.00003848
Iteration 211/1000 | Loss: 0.00003847
Iteration 212/1000 | Loss: 0.00003847
Iteration 213/1000 | Loss: 0.00003847
Iteration 214/1000 | Loss: 0.00003847
Iteration 215/1000 | Loss: 0.00003847
Iteration 216/1000 | Loss: 0.00003847
Iteration 217/1000 | Loss: 0.00003846
Iteration 218/1000 | Loss: 0.00003846
Iteration 219/1000 | Loss: 0.00003846
Iteration 220/1000 | Loss: 0.00003846
Iteration 221/1000 | Loss: 0.00003845
Iteration 222/1000 | Loss: 0.00003845
Iteration 223/1000 | Loss: 0.00003845
Iteration 224/1000 | Loss: 0.00003844
Iteration 225/1000 | Loss: 0.00003844
Iteration 226/1000 | Loss: 0.00003843
Iteration 227/1000 | Loss: 0.00003843
Iteration 228/1000 | Loss: 0.00003842
Iteration 229/1000 | Loss: 0.00003842
Iteration 230/1000 | Loss: 0.00003842
Iteration 231/1000 | Loss: 0.00003841
Iteration 232/1000 | Loss: 0.00003841
Iteration 233/1000 | Loss: 0.00003840
Iteration 234/1000 | Loss: 0.00003840
Iteration 235/1000 | Loss: 0.00003840
Iteration 236/1000 | Loss: 0.00003840
Iteration 237/1000 | Loss: 0.00003839
Iteration 238/1000 | Loss: 0.00003839
Iteration 239/1000 | Loss: 0.00003838
Iteration 240/1000 | Loss: 0.00003837
Iteration 241/1000 | Loss: 0.00003836
Iteration 242/1000 | Loss: 0.00003834
Iteration 243/1000 | Loss: 0.00003832
Iteration 244/1000 | Loss: 0.00003831
Iteration 245/1000 | Loss: 0.00003831
Iteration 246/1000 | Loss: 0.00003819
Iteration 247/1000 | Loss: 0.00003818
Iteration 248/1000 | Loss: 0.00003812
Iteration 249/1000 | Loss: 0.00003812
Iteration 250/1000 | Loss: 0.00003811
Iteration 251/1000 | Loss: 0.00003811
Iteration 252/1000 | Loss: 0.00003811
Iteration 253/1000 | Loss: 0.00003811
Iteration 254/1000 | Loss: 0.00003809
Iteration 255/1000 | Loss: 0.00003809
Iteration 256/1000 | Loss: 0.00003809
Iteration 257/1000 | Loss: 0.00003808
Iteration 258/1000 | Loss: 0.00003807
Iteration 259/1000 | Loss: 0.00003807
Iteration 260/1000 | Loss: 0.00003806
Iteration 261/1000 | Loss: 0.00003805
Iteration 262/1000 | Loss: 0.00003805
Iteration 263/1000 | Loss: 0.00003805
Iteration 264/1000 | Loss: 0.00003804
Iteration 265/1000 | Loss: 0.00003804
Iteration 266/1000 | Loss: 0.00003804
Iteration 267/1000 | Loss: 0.00003804
Iteration 268/1000 | Loss: 0.00003804
Iteration 269/1000 | Loss: 0.00003804
Iteration 270/1000 | Loss: 0.00003804
Iteration 271/1000 | Loss: 0.00003804
Iteration 272/1000 | Loss: 0.00003804
Iteration 273/1000 | Loss: 0.00003803
Iteration 274/1000 | Loss: 0.00003803
Iteration 275/1000 | Loss: 0.00003803
Iteration 276/1000 | Loss: 0.00003803
Iteration 277/1000 | Loss: 0.00003803
Iteration 278/1000 | Loss: 0.00003803
Iteration 279/1000 | Loss: 0.00003803
Iteration 280/1000 | Loss: 0.00003802
Iteration 281/1000 | Loss: 0.00003802
Iteration 282/1000 | Loss: 0.00003802
Iteration 283/1000 | Loss: 0.00003802
Iteration 284/1000 | Loss: 0.00003802
Iteration 285/1000 | Loss: 0.00003802
Iteration 286/1000 | Loss: 0.00003802
Iteration 287/1000 | Loss: 0.00003802
Iteration 288/1000 | Loss: 0.00003802
Iteration 289/1000 | Loss: 0.00003802
Iteration 290/1000 | Loss: 0.00003801
Iteration 291/1000 | Loss: 0.00003801
Iteration 292/1000 | Loss: 0.00003801
Iteration 293/1000 | Loss: 0.00003801
Iteration 294/1000 | Loss: 0.00026976
Iteration 295/1000 | Loss: 0.00026976
Iteration 296/1000 | Loss: 0.00009028
Iteration 297/1000 | Loss: 0.00005154
Iteration 298/1000 | Loss: 0.00003841
Iteration 299/1000 | Loss: 0.00003802
Iteration 300/1000 | Loss: 0.00003800
Iteration 301/1000 | Loss: 0.00003797
Iteration 302/1000 | Loss: 0.00003797
Iteration 303/1000 | Loss: 0.00003797
Iteration 304/1000 | Loss: 0.00003797
Iteration 305/1000 | Loss: 0.00003797
Iteration 306/1000 | Loss: 0.00003797
Iteration 307/1000 | Loss: 0.00003797
Iteration 308/1000 | Loss: 0.00003797
Iteration 309/1000 | Loss: 0.00003797
Iteration 310/1000 | Loss: 0.00003796
Iteration 311/1000 | Loss: 0.00003796
Iteration 312/1000 | Loss: 0.00003796
Iteration 313/1000 | Loss: 0.00003795
Iteration 314/1000 | Loss: 0.00003795
Iteration 315/1000 | Loss: 0.00003795
Iteration 316/1000 | Loss: 0.00003795
Iteration 317/1000 | Loss: 0.00003795
Iteration 318/1000 | Loss: 0.00003795
Iteration 319/1000 | Loss: 0.00003794
Iteration 320/1000 | Loss: 0.00003794
Iteration 321/1000 | Loss: 0.00003794
Iteration 322/1000 | Loss: 0.00003794
Iteration 323/1000 | Loss: 0.00003793
Iteration 324/1000 | Loss: 0.00003793
Iteration 325/1000 | Loss: 0.00003793
Iteration 326/1000 | Loss: 0.00003793
Iteration 327/1000 | Loss: 0.00003793
Iteration 328/1000 | Loss: 0.00003793
Iteration 329/1000 | Loss: 0.00003793
Iteration 330/1000 | Loss: 0.00003792
Iteration 331/1000 | Loss: 0.00003792
Iteration 332/1000 | Loss: 0.00003792
Iteration 333/1000 | Loss: 0.00003792
Iteration 334/1000 | Loss: 0.00003792
Iteration 335/1000 | Loss: 0.00003792
Iteration 336/1000 | Loss: 0.00003792
Iteration 337/1000 | Loss: 0.00003792
Iteration 338/1000 | Loss: 0.00003792
Iteration 339/1000 | Loss: 0.00003792
Iteration 340/1000 | Loss: 0.00003792
Iteration 341/1000 | Loss: 0.00003792
Iteration 342/1000 | Loss: 0.00003792
Iteration 343/1000 | Loss: 0.00003792
Iteration 344/1000 | Loss: 0.00003792
Iteration 345/1000 | Loss: 0.00003792
Iteration 346/1000 | Loss: 0.00003792
Iteration 347/1000 | Loss: 0.00003792
Iteration 348/1000 | Loss: 0.00003792
Iteration 349/1000 | Loss: 0.00003792
Iteration 350/1000 | Loss: 0.00003792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [3.791704148170538e-05, 3.791704148170538e-05, 3.791704148170538e-05, 3.791704148170538e-05, 3.791704148170538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.791704148170538e-05

Optimization complete. Final v2v error: 4.865819454193115 mm

Highest mean error: 13.092134475708008 mm for frame 112

Lowest mean error: 4.6238322257995605 mm for frame 239

Saving results

Total time: 237.67383289337158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062431
Iteration 2/25 | Loss: 0.01062431
Iteration 3/25 | Loss: 0.01062431
Iteration 4/25 | Loss: 0.01062431
Iteration 5/25 | Loss: 0.01062430
Iteration 6/25 | Loss: 0.00587754
Iteration 7/25 | Loss: 0.00378334
Iteration 8/25 | Loss: 0.00323183
Iteration 9/25 | Loss: 0.00307834
Iteration 10/25 | Loss: 0.00210259
Iteration 11/25 | Loss: 0.00182293
Iteration 12/25 | Loss: 0.00170262
Iteration 13/25 | Loss: 0.00158899
Iteration 14/25 | Loss: 0.00154534
Iteration 15/25 | Loss: 0.00144883
Iteration 16/25 | Loss: 0.00143087
Iteration 17/25 | Loss: 0.00141834
Iteration 18/25 | Loss: 0.00140698
Iteration 19/25 | Loss: 0.00141006
Iteration 20/25 | Loss: 0.00140435
Iteration 21/25 | Loss: 0.00140543
Iteration 22/25 | Loss: 0.00140551
Iteration 23/25 | Loss: 0.00140289
Iteration 24/25 | Loss: 0.00140992
Iteration 25/25 | Loss: 0.00140488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56195867
Iteration 2/25 | Loss: 0.00139027
Iteration 3/25 | Loss: 0.00099324
Iteration 4/25 | Loss: 0.00099323
Iteration 5/25 | Loss: 0.00099323
Iteration 6/25 | Loss: 0.00099323
Iteration 7/25 | Loss: 0.00099323
Iteration 8/25 | Loss: 0.00099323
Iteration 9/25 | Loss: 0.00099323
Iteration 10/25 | Loss: 0.00099323
Iteration 11/25 | Loss: 0.00099323
Iteration 12/25 | Loss: 0.00099323
Iteration 13/25 | Loss: 0.00099323
Iteration 14/25 | Loss: 0.00099323
Iteration 15/25 | Loss: 0.00099323
Iteration 16/25 | Loss: 0.00099323
Iteration 17/25 | Loss: 0.00099323
Iteration 18/25 | Loss: 0.00099323
Iteration 19/25 | Loss: 0.00099323
Iteration 20/25 | Loss: 0.00099323
Iteration 21/25 | Loss: 0.00099323
Iteration 22/25 | Loss: 0.00099323
Iteration 23/25 | Loss: 0.00099323
Iteration 24/25 | Loss: 0.00099323
Iteration 25/25 | Loss: 0.00099323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099323
Iteration 2/1000 | Loss: 0.00067835
Iteration 3/1000 | Loss: 0.00005689
Iteration 4/1000 | Loss: 0.00003511
Iteration 5/1000 | Loss: 0.00003056
Iteration 6/1000 | Loss: 0.00002936
Iteration 7/1000 | Loss: 0.00002883
Iteration 8/1000 | Loss: 0.00002838
Iteration 9/1000 | Loss: 0.00002791
Iteration 10/1000 | Loss: 0.00002759
Iteration 11/1000 | Loss: 0.00002725
Iteration 12/1000 | Loss: 0.00170903
Iteration 13/1000 | Loss: 0.00003535
Iteration 14/1000 | Loss: 0.00003234
Iteration 15/1000 | Loss: 0.00003107
Iteration 16/1000 | Loss: 0.00005042
Iteration 17/1000 | Loss: 0.00002377
Iteration 18/1000 | Loss: 0.00002263
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002137
Iteration 21/1000 | Loss: 0.00002608
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002283
Iteration 24/1000 | Loss: 0.00002205
Iteration 25/1000 | Loss: 0.00002141
Iteration 26/1000 | Loss: 0.00002022
Iteration 27/1000 | Loss: 0.00001977
Iteration 28/1000 | Loss: 0.00001960
Iteration 29/1000 | Loss: 0.00001952
Iteration 30/1000 | Loss: 0.00001951
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001949
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001946
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001944
Iteration 42/1000 | Loss: 0.00001944
Iteration 43/1000 | Loss: 0.00001944
Iteration 44/1000 | Loss: 0.00001944
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001942
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001942
Iteration 55/1000 | Loss: 0.00001942
Iteration 56/1000 | Loss: 0.00001942
Iteration 57/1000 | Loss: 0.00001942
Iteration 58/1000 | Loss: 0.00001942
Iteration 59/1000 | Loss: 0.00001942
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001940
Iteration 64/1000 | Loss: 0.00001940
Iteration 65/1000 | Loss: 0.00001940
Iteration 66/1000 | Loss: 0.00001940
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001939
Iteration 76/1000 | Loss: 0.00001939
Iteration 77/1000 | Loss: 0.00001939
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001938
Iteration 80/1000 | Loss: 0.00001938
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001937
Iteration 96/1000 | Loss: 0.00001937
Iteration 97/1000 | Loss: 0.00001937
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001937
Iteration 107/1000 | Loss: 0.00001937
Iteration 108/1000 | Loss: 0.00001937
Iteration 109/1000 | Loss: 0.00001937
Iteration 110/1000 | Loss: 0.00001937
Iteration 111/1000 | Loss: 0.00001937
Iteration 112/1000 | Loss: 0.00001937
Iteration 113/1000 | Loss: 0.00001937
Iteration 114/1000 | Loss: 0.00001937
Iteration 115/1000 | Loss: 0.00001937
Iteration 116/1000 | Loss: 0.00001937
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001937
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001937
Iteration 127/1000 | Loss: 0.00001937
Iteration 128/1000 | Loss: 0.00001937
Iteration 129/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.9365681509952992e-05, 1.9365681509952992e-05, 1.9365681509952992e-05, 1.9365681509952992e-05, 1.9365681509952992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9365681509952992e-05

Optimization complete. Final v2v error: 3.8127059936523438 mm

Highest mean error: 4.401823043823242 mm for frame 84

Lowest mean error: 3.6636738777160645 mm for frame 31

Saving results

Total time: 93.57198715209961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007132
Iteration 2/25 | Loss: 0.00372114
Iteration 3/25 | Loss: 0.00299161
Iteration 4/25 | Loss: 0.00276148
Iteration 5/25 | Loss: 0.00234537
Iteration 6/25 | Loss: 0.00221254
Iteration 7/25 | Loss: 0.00209725
Iteration 8/25 | Loss: 0.00200052
Iteration 9/25 | Loss: 0.00198248
Iteration 10/25 | Loss: 0.00197498
Iteration 11/25 | Loss: 0.00195609
Iteration 12/25 | Loss: 0.00194740
Iteration 13/25 | Loss: 0.00194633
Iteration 14/25 | Loss: 0.00194366
Iteration 15/25 | Loss: 0.00193315
Iteration 16/25 | Loss: 0.00192859
Iteration 17/25 | Loss: 0.00192664
Iteration 18/25 | Loss: 0.00192405
Iteration 19/25 | Loss: 0.00191885
Iteration 20/25 | Loss: 0.00191592
Iteration 21/25 | Loss: 0.00191737
Iteration 22/25 | Loss: 0.00192348
Iteration 23/25 | Loss: 0.00192307
Iteration 24/25 | Loss: 0.00191017
Iteration 25/25 | Loss: 0.00191088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31662941
Iteration 2/25 | Loss: 0.01086762
Iteration 3/25 | Loss: 0.00858294
Iteration 4/25 | Loss: 0.00858292
Iteration 5/25 | Loss: 0.00858292
Iteration 6/25 | Loss: 0.00858292
Iteration 7/25 | Loss: 0.00858292
Iteration 8/25 | Loss: 0.00858292
Iteration 9/25 | Loss: 0.00858292
Iteration 10/25 | Loss: 0.00858292
Iteration 11/25 | Loss: 0.00858292
Iteration 12/25 | Loss: 0.00858292
Iteration 13/25 | Loss: 0.00858292
Iteration 14/25 | Loss: 0.00858292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.008582917973399162, 0.008582917973399162, 0.008582917973399162, 0.008582917973399162, 0.008582917973399162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008582917973399162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00858292
Iteration 2/1000 | Loss: 0.00218718
Iteration 3/1000 | Loss: 0.00119926
Iteration 4/1000 | Loss: 0.00074779
Iteration 5/1000 | Loss: 0.00087274
Iteration 6/1000 | Loss: 0.00152621
Iteration 7/1000 | Loss: 0.00044723
Iteration 8/1000 | Loss: 0.00065720
Iteration 9/1000 | Loss: 0.00093201
Iteration 10/1000 | Loss: 0.00198129
Iteration 11/1000 | Loss: 0.00343429
Iteration 12/1000 | Loss: 0.00084631
Iteration 13/1000 | Loss: 0.00070104
Iteration 14/1000 | Loss: 0.00043848
Iteration 15/1000 | Loss: 0.00045265
Iteration 16/1000 | Loss: 0.00058785
Iteration 17/1000 | Loss: 0.00093531
Iteration 18/1000 | Loss: 0.00076361
Iteration 19/1000 | Loss: 0.00076648
Iteration 20/1000 | Loss: 0.00044553
Iteration 21/1000 | Loss: 0.00071766
Iteration 22/1000 | Loss: 0.00041378
Iteration 23/1000 | Loss: 0.00037154
Iteration 24/1000 | Loss: 0.00091704
Iteration 25/1000 | Loss: 0.00039550
Iteration 26/1000 | Loss: 0.00033371
Iteration 27/1000 | Loss: 0.00039386
Iteration 28/1000 | Loss: 0.00032752
Iteration 29/1000 | Loss: 0.00048089
Iteration 30/1000 | Loss: 0.00033244
Iteration 31/1000 | Loss: 0.00051496
Iteration 32/1000 | Loss: 0.00076141
Iteration 33/1000 | Loss: 0.00037924
Iteration 34/1000 | Loss: 0.00031924
Iteration 35/1000 | Loss: 0.00031457
Iteration 36/1000 | Loss: 0.00049773
Iteration 37/1000 | Loss: 0.00031092
Iteration 38/1000 | Loss: 0.00081772
Iteration 39/1000 | Loss: 0.00057863
Iteration 40/1000 | Loss: 0.00031488
Iteration 41/1000 | Loss: 0.00056826
Iteration 42/1000 | Loss: 0.00114061
Iteration 43/1000 | Loss: 0.00075049
Iteration 44/1000 | Loss: 0.00058189
Iteration 45/1000 | Loss: 0.00033577
Iteration 46/1000 | Loss: 0.00034809
Iteration 47/1000 | Loss: 0.00038141
Iteration 48/1000 | Loss: 0.00033362
Iteration 49/1000 | Loss: 0.00035941
Iteration 50/1000 | Loss: 0.00049233
Iteration 51/1000 | Loss: 0.00044323
Iteration 52/1000 | Loss: 0.00044311
Iteration 53/1000 | Loss: 0.00086483
Iteration 54/1000 | Loss: 0.00030714
Iteration 55/1000 | Loss: 0.00029542
Iteration 56/1000 | Loss: 0.00029337
Iteration 57/1000 | Loss: 0.00034095
Iteration 58/1000 | Loss: 0.00095394
Iteration 59/1000 | Loss: 0.00043696
Iteration 60/1000 | Loss: 0.00030138
Iteration 61/1000 | Loss: 0.00029458
Iteration 62/1000 | Loss: 0.00039725
Iteration 63/1000 | Loss: 0.00028373
Iteration 64/1000 | Loss: 0.00080431
Iteration 65/1000 | Loss: 0.00029862
Iteration 66/1000 | Loss: 0.00073849
Iteration 67/1000 | Loss: 0.00129882
Iteration 68/1000 | Loss: 0.00081364
Iteration 69/1000 | Loss: 0.00041513
Iteration 70/1000 | Loss: 0.00031140
Iteration 71/1000 | Loss: 0.00031824
Iteration 72/1000 | Loss: 0.00075501
Iteration 73/1000 | Loss: 0.00030470
Iteration 74/1000 | Loss: 0.00037701
Iteration 75/1000 | Loss: 0.00027448
Iteration 76/1000 | Loss: 0.00027530
Iteration 77/1000 | Loss: 0.00027127
Iteration 78/1000 | Loss: 0.00088582
Iteration 79/1000 | Loss: 0.00036555
Iteration 80/1000 | Loss: 0.00035013
Iteration 81/1000 | Loss: 0.00030657
Iteration 82/1000 | Loss: 0.00042594
Iteration 83/1000 | Loss: 0.00037695
Iteration 84/1000 | Loss: 0.00032644
Iteration 85/1000 | Loss: 0.00044915
Iteration 86/1000 | Loss: 0.00031101
Iteration 87/1000 | Loss: 0.00026730
Iteration 88/1000 | Loss: 0.00026628
Iteration 89/1000 | Loss: 0.00039433
Iteration 90/1000 | Loss: 0.00026537
Iteration 91/1000 | Loss: 0.00070006
Iteration 92/1000 | Loss: 0.00026653
Iteration 93/1000 | Loss: 0.00028266
Iteration 94/1000 | Loss: 0.00040979
Iteration 95/1000 | Loss: 0.00026405
Iteration 96/1000 | Loss: 0.00026329
Iteration 97/1000 | Loss: 0.00032781
Iteration 98/1000 | Loss: 0.00043039
Iteration 99/1000 | Loss: 0.00043422
Iteration 100/1000 | Loss: 0.00030037
Iteration 101/1000 | Loss: 0.00029070
Iteration 102/1000 | Loss: 0.00026232
Iteration 103/1000 | Loss: 0.00026213
Iteration 104/1000 | Loss: 0.00029014
Iteration 105/1000 | Loss: 0.00045819
Iteration 106/1000 | Loss: 0.00076471
Iteration 107/1000 | Loss: 0.00033039
Iteration 108/1000 | Loss: 0.00027774
Iteration 109/1000 | Loss: 0.00026184
Iteration 110/1000 | Loss: 0.00040238
Iteration 111/1000 | Loss: 0.00032667
Iteration 112/1000 | Loss: 0.00039133
Iteration 113/1000 | Loss: 0.00058053
Iteration 114/1000 | Loss: 0.00026836
Iteration 115/1000 | Loss: 0.00026650
Iteration 116/1000 | Loss: 0.00030534
Iteration 117/1000 | Loss: 0.00026180
Iteration 118/1000 | Loss: 0.00028481
Iteration 119/1000 | Loss: 0.00026486
Iteration 120/1000 | Loss: 0.00026923
Iteration 121/1000 | Loss: 0.00026155
Iteration 122/1000 | Loss: 0.00035456
Iteration 123/1000 | Loss: 0.00027059
Iteration 124/1000 | Loss: 0.00026348
Iteration 125/1000 | Loss: 0.00026130
Iteration 126/1000 | Loss: 0.00026119
Iteration 127/1000 | Loss: 0.00026117
Iteration 128/1000 | Loss: 0.00035317
Iteration 129/1000 | Loss: 0.00049216
Iteration 130/1000 | Loss: 0.00029365
Iteration 131/1000 | Loss: 0.00027060
Iteration 132/1000 | Loss: 0.00028213
Iteration 133/1000 | Loss: 0.00026132
Iteration 134/1000 | Loss: 0.00029976
Iteration 135/1000 | Loss: 0.00026262
Iteration 136/1000 | Loss: 0.00034036
Iteration 137/1000 | Loss: 0.00026329
Iteration 138/1000 | Loss: 0.00033930
Iteration 139/1000 | Loss: 0.00046124
Iteration 140/1000 | Loss: 0.00029384
Iteration 141/1000 | Loss: 0.00029391
Iteration 142/1000 | Loss: 0.00027627
Iteration 143/1000 | Loss: 0.00026112
Iteration 144/1000 | Loss: 0.00026088
Iteration 145/1000 | Loss: 0.00026087
Iteration 146/1000 | Loss: 0.00026086
Iteration 147/1000 | Loss: 0.00026086
Iteration 148/1000 | Loss: 0.00026086
Iteration 149/1000 | Loss: 0.00026086
Iteration 150/1000 | Loss: 0.00026085
Iteration 151/1000 | Loss: 0.00026085
Iteration 152/1000 | Loss: 0.00026085
Iteration 153/1000 | Loss: 0.00026085
Iteration 154/1000 | Loss: 0.00026170
Iteration 155/1000 | Loss: 0.00026104
Iteration 156/1000 | Loss: 0.00027799
Iteration 157/1000 | Loss: 0.00026299
Iteration 158/1000 | Loss: 0.00026349
Iteration 159/1000 | Loss: 0.00026084
Iteration 160/1000 | Loss: 0.00026084
Iteration 161/1000 | Loss: 0.00026084
Iteration 162/1000 | Loss: 0.00026084
Iteration 163/1000 | Loss: 0.00026084
Iteration 164/1000 | Loss: 0.00026084
Iteration 165/1000 | Loss: 0.00026084
Iteration 166/1000 | Loss: 0.00026084
Iteration 167/1000 | Loss: 0.00026084
Iteration 168/1000 | Loss: 0.00026084
Iteration 169/1000 | Loss: 0.00026084
Iteration 170/1000 | Loss: 0.00026084
Iteration 171/1000 | Loss: 0.00026084
Iteration 172/1000 | Loss: 0.00026084
Iteration 173/1000 | Loss: 0.00026084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [0.00026083903503604233, 0.00026083903503604233, 0.00026083903503604233, 0.00026083903503604233, 0.00026083903503604233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026083903503604233

Optimization complete. Final v2v error: 9.103832244873047 mm

Highest mean error: 12.357891082763672 mm for frame 187

Lowest mean error: 4.739559650421143 mm for frame 227

Saving results

Total time: 278.88197588920593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812255
Iteration 2/25 | Loss: 0.00139448
Iteration 3/25 | Loss: 0.00128148
Iteration 4/25 | Loss: 0.00125712
Iteration 5/25 | Loss: 0.00125686
Iteration 6/25 | Loss: 0.00125792
Iteration 7/25 | Loss: 0.00127287
Iteration 8/25 | Loss: 0.00126977
Iteration 9/25 | Loss: 0.00127071
Iteration 10/25 | Loss: 0.00125966
Iteration 11/25 | Loss: 0.00125582
Iteration 12/25 | Loss: 0.00125285
Iteration 13/25 | Loss: 0.00124507
Iteration 14/25 | Loss: 0.00124277
Iteration 15/25 | Loss: 0.00124210
Iteration 16/25 | Loss: 0.00124170
Iteration 17/25 | Loss: 0.00123979
Iteration 18/25 | Loss: 0.00124017
Iteration 19/25 | Loss: 0.00124019
Iteration 20/25 | Loss: 0.00123994
Iteration 21/25 | Loss: 0.00123744
Iteration 22/25 | Loss: 0.00123686
Iteration 23/25 | Loss: 0.00123654
Iteration 24/25 | Loss: 0.00123868
Iteration 25/25 | Loss: 0.00123768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35070038
Iteration 2/25 | Loss: 0.00131301
Iteration 3/25 | Loss: 0.00131299
Iteration 4/25 | Loss: 0.00131299
Iteration 5/25 | Loss: 0.00131299
Iteration 6/25 | Loss: 0.00131299
Iteration 7/25 | Loss: 0.00131299
Iteration 8/25 | Loss: 0.00131299
Iteration 9/25 | Loss: 0.00131299
Iteration 10/25 | Loss: 0.00131299
Iteration 11/25 | Loss: 0.00131299
Iteration 12/25 | Loss: 0.00131299
Iteration 13/25 | Loss: 0.00131299
Iteration 14/25 | Loss: 0.00131299
Iteration 15/25 | Loss: 0.00131299
Iteration 16/25 | Loss: 0.00131299
Iteration 17/25 | Loss: 0.00131299
Iteration 18/25 | Loss: 0.00131299
Iteration 19/25 | Loss: 0.00131299
Iteration 20/25 | Loss: 0.00131299
Iteration 21/25 | Loss: 0.00131299
Iteration 22/25 | Loss: 0.00131299
Iteration 23/25 | Loss: 0.00131299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013129869475960732, 0.0013129869475960732, 0.0013129869475960732, 0.0013129869475960732, 0.0013129869475960732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013129869475960732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131299
Iteration 2/1000 | Loss: 0.00006593
Iteration 3/1000 | Loss: 0.00006234
Iteration 4/1000 | Loss: 0.00003436
Iteration 5/1000 | Loss: 0.00004926
Iteration 6/1000 | Loss: 0.00003203
Iteration 7/1000 | Loss: 0.00002649
Iteration 8/1000 | Loss: 0.00002427
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00003573
Iteration 11/1000 | Loss: 0.00002629
Iteration 12/1000 | Loss: 0.00004305
Iteration 13/1000 | Loss: 0.00003738
Iteration 14/1000 | Loss: 0.00003978
Iteration 15/1000 | Loss: 0.00003546
Iteration 16/1000 | Loss: 0.00002152
Iteration 17/1000 | Loss: 0.00001973
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001854
Iteration 20/1000 | Loss: 0.00001854
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001850
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001833
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001795
Iteration 28/1000 | Loss: 0.00001776
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001766
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001763
Iteration 36/1000 | Loss: 0.00001763
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001755
Iteration 42/1000 | Loss: 0.00001755
Iteration 43/1000 | Loss: 0.00001755
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001753
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001752
Iteration 53/1000 | Loss: 0.00001752
Iteration 54/1000 | Loss: 0.00001752
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001751
Iteration 59/1000 | Loss: 0.00001751
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001750
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001749
Iteration 64/1000 | Loss: 0.00001749
Iteration 65/1000 | Loss: 0.00001749
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001748
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001747
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001747
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001746
Iteration 84/1000 | Loss: 0.00001746
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001745
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001739
Iteration 130/1000 | Loss: 0.00001739
Iteration 131/1000 | Loss: 0.00001739
Iteration 132/1000 | Loss: 0.00001739
Iteration 133/1000 | Loss: 0.00001738
Iteration 134/1000 | Loss: 0.00001738
Iteration 135/1000 | Loss: 0.00001738
Iteration 136/1000 | Loss: 0.00001738
Iteration 137/1000 | Loss: 0.00001738
Iteration 138/1000 | Loss: 0.00001738
Iteration 139/1000 | Loss: 0.00001738
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001737
Iteration 146/1000 | Loss: 0.00001737
Iteration 147/1000 | Loss: 0.00001737
Iteration 148/1000 | Loss: 0.00001737
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001737
Iteration 157/1000 | Loss: 0.00001736
Iteration 158/1000 | Loss: 0.00001736
Iteration 159/1000 | Loss: 0.00001736
Iteration 160/1000 | Loss: 0.00001736
Iteration 161/1000 | Loss: 0.00001736
Iteration 162/1000 | Loss: 0.00001736
Iteration 163/1000 | Loss: 0.00001736
Iteration 164/1000 | Loss: 0.00001736
Iteration 165/1000 | Loss: 0.00001735
Iteration 166/1000 | Loss: 0.00001735
Iteration 167/1000 | Loss: 0.00001735
Iteration 168/1000 | Loss: 0.00001735
Iteration 169/1000 | Loss: 0.00001735
Iteration 170/1000 | Loss: 0.00001735
Iteration 171/1000 | Loss: 0.00001735
Iteration 172/1000 | Loss: 0.00001734
Iteration 173/1000 | Loss: 0.00001734
Iteration 174/1000 | Loss: 0.00001734
Iteration 175/1000 | Loss: 0.00001734
Iteration 176/1000 | Loss: 0.00001734
Iteration 177/1000 | Loss: 0.00001734
Iteration 178/1000 | Loss: 0.00001733
Iteration 179/1000 | Loss: 0.00001733
Iteration 180/1000 | Loss: 0.00001733
Iteration 181/1000 | Loss: 0.00001733
Iteration 182/1000 | Loss: 0.00001733
Iteration 183/1000 | Loss: 0.00001732
Iteration 184/1000 | Loss: 0.00001732
Iteration 185/1000 | Loss: 0.00001732
Iteration 186/1000 | Loss: 0.00001732
Iteration 187/1000 | Loss: 0.00001732
Iteration 188/1000 | Loss: 0.00001732
Iteration 189/1000 | Loss: 0.00001731
Iteration 190/1000 | Loss: 0.00001731
Iteration 191/1000 | Loss: 0.00001731
Iteration 192/1000 | Loss: 0.00001731
Iteration 193/1000 | Loss: 0.00001731
Iteration 194/1000 | Loss: 0.00001731
Iteration 195/1000 | Loss: 0.00001731
Iteration 196/1000 | Loss: 0.00001731
Iteration 197/1000 | Loss: 0.00001730
Iteration 198/1000 | Loss: 0.00001730
Iteration 199/1000 | Loss: 0.00001730
Iteration 200/1000 | Loss: 0.00001730
Iteration 201/1000 | Loss: 0.00001730
Iteration 202/1000 | Loss: 0.00001730
Iteration 203/1000 | Loss: 0.00001730
Iteration 204/1000 | Loss: 0.00001730
Iteration 205/1000 | Loss: 0.00001730
Iteration 206/1000 | Loss: 0.00001730
Iteration 207/1000 | Loss: 0.00001730
Iteration 208/1000 | Loss: 0.00001730
Iteration 209/1000 | Loss: 0.00001730
Iteration 210/1000 | Loss: 0.00001729
Iteration 211/1000 | Loss: 0.00001729
Iteration 212/1000 | Loss: 0.00001729
Iteration 213/1000 | Loss: 0.00001729
Iteration 214/1000 | Loss: 0.00001729
Iteration 215/1000 | Loss: 0.00001729
Iteration 216/1000 | Loss: 0.00001729
Iteration 217/1000 | Loss: 0.00001729
Iteration 218/1000 | Loss: 0.00001729
Iteration 219/1000 | Loss: 0.00001729
Iteration 220/1000 | Loss: 0.00001729
Iteration 221/1000 | Loss: 0.00001729
Iteration 222/1000 | Loss: 0.00001729
Iteration 223/1000 | Loss: 0.00001728
Iteration 224/1000 | Loss: 0.00001728
Iteration 225/1000 | Loss: 0.00001728
Iteration 226/1000 | Loss: 0.00001728
Iteration 227/1000 | Loss: 0.00001728
Iteration 228/1000 | Loss: 0.00001728
Iteration 229/1000 | Loss: 0.00001728
Iteration 230/1000 | Loss: 0.00001728
Iteration 231/1000 | Loss: 0.00001728
Iteration 232/1000 | Loss: 0.00001727
Iteration 233/1000 | Loss: 0.00001727
Iteration 234/1000 | Loss: 0.00001727
Iteration 235/1000 | Loss: 0.00001727
Iteration 236/1000 | Loss: 0.00001727
Iteration 237/1000 | Loss: 0.00001726
Iteration 238/1000 | Loss: 0.00001726
Iteration 239/1000 | Loss: 0.00001726
Iteration 240/1000 | Loss: 0.00001726
Iteration 241/1000 | Loss: 0.00001726
Iteration 242/1000 | Loss: 0.00001726
Iteration 243/1000 | Loss: 0.00001725
Iteration 244/1000 | Loss: 0.00001725
Iteration 245/1000 | Loss: 0.00001725
Iteration 246/1000 | Loss: 0.00001725
Iteration 247/1000 | Loss: 0.00001725
Iteration 248/1000 | Loss: 0.00001725
Iteration 249/1000 | Loss: 0.00001725
Iteration 250/1000 | Loss: 0.00001725
Iteration 251/1000 | Loss: 0.00001725
Iteration 252/1000 | Loss: 0.00001725
Iteration 253/1000 | Loss: 0.00001725
Iteration 254/1000 | Loss: 0.00001724
Iteration 255/1000 | Loss: 0.00001724
Iteration 256/1000 | Loss: 0.00001724
Iteration 257/1000 | Loss: 0.00001724
Iteration 258/1000 | Loss: 0.00001724
Iteration 259/1000 | Loss: 0.00001724
Iteration 260/1000 | Loss: 0.00001724
Iteration 261/1000 | Loss: 0.00001724
Iteration 262/1000 | Loss: 0.00001724
Iteration 263/1000 | Loss: 0.00001724
Iteration 264/1000 | Loss: 0.00001724
Iteration 265/1000 | Loss: 0.00001724
Iteration 266/1000 | Loss: 0.00001724
Iteration 267/1000 | Loss: 0.00001724
Iteration 268/1000 | Loss: 0.00001724
Iteration 269/1000 | Loss: 0.00001724
Iteration 270/1000 | Loss: 0.00001724
Iteration 271/1000 | Loss: 0.00001724
Iteration 272/1000 | Loss: 0.00001724
Iteration 273/1000 | Loss: 0.00001724
Iteration 274/1000 | Loss: 0.00001724
Iteration 275/1000 | Loss: 0.00001723
Iteration 276/1000 | Loss: 0.00001723
Iteration 277/1000 | Loss: 0.00001723
Iteration 278/1000 | Loss: 0.00001723
Iteration 279/1000 | Loss: 0.00001723
Iteration 280/1000 | Loss: 0.00001723
Iteration 281/1000 | Loss: 0.00001723
Iteration 282/1000 | Loss: 0.00001723
Iteration 283/1000 | Loss: 0.00001723
Iteration 284/1000 | Loss: 0.00001723
Iteration 285/1000 | Loss: 0.00001723
Iteration 286/1000 | Loss: 0.00001723
Iteration 287/1000 | Loss: 0.00001723
Iteration 288/1000 | Loss: 0.00001723
Iteration 289/1000 | Loss: 0.00001723
Iteration 290/1000 | Loss: 0.00001723
Iteration 291/1000 | Loss: 0.00001723
Iteration 292/1000 | Loss: 0.00001723
Iteration 293/1000 | Loss: 0.00001723
Iteration 294/1000 | Loss: 0.00001723
Iteration 295/1000 | Loss: 0.00001723
Iteration 296/1000 | Loss: 0.00001723
Iteration 297/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.7227603166247718e-05, 1.7227603166247718e-05, 1.7227603166247718e-05, 1.7227603166247718e-05, 1.7227603166247718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7227603166247718e-05

Optimization complete. Final v2v error: 3.4616901874542236 mm

Highest mean error: 5.50945520401001 mm for frame 66

Lowest mean error: 2.7677481174468994 mm for frame 11

Saving results

Total time: 110.43272757530212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714469
Iteration 2/25 | Loss: 0.00158050
Iteration 3/25 | Loss: 0.00143078
Iteration 4/25 | Loss: 0.00140509
Iteration 5/25 | Loss: 0.00139852
Iteration 6/25 | Loss: 0.00139699
Iteration 7/25 | Loss: 0.00139699
Iteration 8/25 | Loss: 0.00139699
Iteration 9/25 | Loss: 0.00139699
Iteration 10/25 | Loss: 0.00139699
Iteration 11/25 | Loss: 0.00139699
Iteration 12/25 | Loss: 0.00139699
Iteration 13/25 | Loss: 0.00139699
Iteration 14/25 | Loss: 0.00139699
Iteration 15/25 | Loss: 0.00139699
Iteration 16/25 | Loss: 0.00139699
Iteration 17/25 | Loss: 0.00139699
Iteration 18/25 | Loss: 0.00139699
Iteration 19/25 | Loss: 0.00139699
Iteration 20/25 | Loss: 0.00139699
Iteration 21/25 | Loss: 0.00139699
Iteration 22/25 | Loss: 0.00139699
Iteration 23/25 | Loss: 0.00139699
Iteration 24/25 | Loss: 0.00139699
Iteration 25/25 | Loss: 0.00139699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21417892
Iteration 2/25 | Loss: 0.00105824
Iteration 3/25 | Loss: 0.00105802
Iteration 4/25 | Loss: 0.00105802
Iteration 5/25 | Loss: 0.00105802
Iteration 6/25 | Loss: 0.00105802
Iteration 7/25 | Loss: 0.00105802
Iteration 8/25 | Loss: 0.00105802
Iteration 9/25 | Loss: 0.00105802
Iteration 10/25 | Loss: 0.00105802
Iteration 11/25 | Loss: 0.00105802
Iteration 12/25 | Loss: 0.00105802
Iteration 13/25 | Loss: 0.00105802
Iteration 14/25 | Loss: 0.00105802
Iteration 15/25 | Loss: 0.00105802
Iteration 16/25 | Loss: 0.00105802
Iteration 17/25 | Loss: 0.00105802
Iteration 18/25 | Loss: 0.00105802
Iteration 19/25 | Loss: 0.00105802
Iteration 20/25 | Loss: 0.00105802
Iteration 21/25 | Loss: 0.00105802
Iteration 22/25 | Loss: 0.00105802
Iteration 23/25 | Loss: 0.00105802
Iteration 24/25 | Loss: 0.00105802
Iteration 25/25 | Loss: 0.00105802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105802
Iteration 2/1000 | Loss: 0.00009014
Iteration 3/1000 | Loss: 0.00005859
Iteration 4/1000 | Loss: 0.00004584
Iteration 5/1000 | Loss: 0.00004030
Iteration 6/1000 | Loss: 0.00003869
Iteration 7/1000 | Loss: 0.00003743
Iteration 8/1000 | Loss: 0.00003669
Iteration 9/1000 | Loss: 0.00003609
Iteration 10/1000 | Loss: 0.00003564
Iteration 11/1000 | Loss: 0.00003527
Iteration 12/1000 | Loss: 0.00003502
Iteration 13/1000 | Loss: 0.00003476
Iteration 14/1000 | Loss: 0.00003452
Iteration 15/1000 | Loss: 0.00003451
Iteration 16/1000 | Loss: 0.00003436
Iteration 17/1000 | Loss: 0.00003425
Iteration 18/1000 | Loss: 0.00003424
Iteration 19/1000 | Loss: 0.00003422
Iteration 20/1000 | Loss: 0.00003422
Iteration 21/1000 | Loss: 0.00003421
Iteration 22/1000 | Loss: 0.00003415
Iteration 23/1000 | Loss: 0.00003412
Iteration 24/1000 | Loss: 0.00003411
Iteration 25/1000 | Loss: 0.00003411
Iteration 26/1000 | Loss: 0.00003410
Iteration 27/1000 | Loss: 0.00003408
Iteration 28/1000 | Loss: 0.00003407
Iteration 29/1000 | Loss: 0.00003406
Iteration 30/1000 | Loss: 0.00003406
Iteration 31/1000 | Loss: 0.00003405
Iteration 32/1000 | Loss: 0.00003405
Iteration 33/1000 | Loss: 0.00003405
Iteration 34/1000 | Loss: 0.00003404
Iteration 35/1000 | Loss: 0.00003403
Iteration 36/1000 | Loss: 0.00003402
Iteration 37/1000 | Loss: 0.00003400
Iteration 38/1000 | Loss: 0.00003400
Iteration 39/1000 | Loss: 0.00003399
Iteration 40/1000 | Loss: 0.00003399
Iteration 41/1000 | Loss: 0.00003398
Iteration 42/1000 | Loss: 0.00003397
Iteration 43/1000 | Loss: 0.00003396
Iteration 44/1000 | Loss: 0.00003395
Iteration 45/1000 | Loss: 0.00003394
Iteration 46/1000 | Loss: 0.00003394
Iteration 47/1000 | Loss: 0.00003393
Iteration 48/1000 | Loss: 0.00003393
Iteration 49/1000 | Loss: 0.00003392
Iteration 50/1000 | Loss: 0.00003392
Iteration 51/1000 | Loss: 0.00003392
Iteration 52/1000 | Loss: 0.00003392
Iteration 53/1000 | Loss: 0.00003392
Iteration 54/1000 | Loss: 0.00003391
Iteration 55/1000 | Loss: 0.00003391
Iteration 56/1000 | Loss: 0.00003390
Iteration 57/1000 | Loss: 0.00003390
Iteration 58/1000 | Loss: 0.00003390
Iteration 59/1000 | Loss: 0.00003390
Iteration 60/1000 | Loss: 0.00003389
Iteration 61/1000 | Loss: 0.00003389
Iteration 62/1000 | Loss: 0.00003389
Iteration 63/1000 | Loss: 0.00003388
Iteration 64/1000 | Loss: 0.00003388
Iteration 65/1000 | Loss: 0.00003388
Iteration 66/1000 | Loss: 0.00003387
Iteration 67/1000 | Loss: 0.00003387
Iteration 68/1000 | Loss: 0.00003387
Iteration 69/1000 | Loss: 0.00003387
Iteration 70/1000 | Loss: 0.00003387
Iteration 71/1000 | Loss: 0.00003387
Iteration 72/1000 | Loss: 0.00003387
Iteration 73/1000 | Loss: 0.00003387
Iteration 74/1000 | Loss: 0.00003387
Iteration 75/1000 | Loss: 0.00003387
Iteration 76/1000 | Loss: 0.00003387
Iteration 77/1000 | Loss: 0.00003387
Iteration 78/1000 | Loss: 0.00003387
Iteration 79/1000 | Loss: 0.00003387
Iteration 80/1000 | Loss: 0.00003387
Iteration 81/1000 | Loss: 0.00003387
Iteration 82/1000 | Loss: 0.00003387
Iteration 83/1000 | Loss: 0.00003386
Iteration 84/1000 | Loss: 0.00003386
Iteration 85/1000 | Loss: 0.00003386
Iteration 86/1000 | Loss: 0.00003386
Iteration 87/1000 | Loss: 0.00003386
Iteration 88/1000 | Loss: 0.00003386
Iteration 89/1000 | Loss: 0.00003386
Iteration 90/1000 | Loss: 0.00003386
Iteration 91/1000 | Loss: 0.00003386
Iteration 92/1000 | Loss: 0.00003386
Iteration 93/1000 | Loss: 0.00003386
Iteration 94/1000 | Loss: 0.00003386
Iteration 95/1000 | Loss: 0.00003386
Iteration 96/1000 | Loss: 0.00003386
Iteration 97/1000 | Loss: 0.00003386
Iteration 98/1000 | Loss: 0.00003386
Iteration 99/1000 | Loss: 0.00003386
Iteration 100/1000 | Loss: 0.00003386
Iteration 101/1000 | Loss: 0.00003386
Iteration 102/1000 | Loss: 0.00003386
Iteration 103/1000 | Loss: 0.00003386
Iteration 104/1000 | Loss: 0.00003386
Iteration 105/1000 | Loss: 0.00003386
Iteration 106/1000 | Loss: 0.00003386
Iteration 107/1000 | Loss: 0.00003386
Iteration 108/1000 | Loss: 0.00003386
Iteration 109/1000 | Loss: 0.00003386
Iteration 110/1000 | Loss: 0.00003386
Iteration 111/1000 | Loss: 0.00003386
Iteration 112/1000 | Loss: 0.00003386
Iteration 113/1000 | Loss: 0.00003386
Iteration 114/1000 | Loss: 0.00003386
Iteration 115/1000 | Loss: 0.00003386
Iteration 116/1000 | Loss: 0.00003386
Iteration 117/1000 | Loss: 0.00003386
Iteration 118/1000 | Loss: 0.00003386
Iteration 119/1000 | Loss: 0.00003386
Iteration 120/1000 | Loss: 0.00003386
Iteration 121/1000 | Loss: 0.00003386
Iteration 122/1000 | Loss: 0.00003386
Iteration 123/1000 | Loss: 0.00003386
Iteration 124/1000 | Loss: 0.00003386
Iteration 125/1000 | Loss: 0.00003386
Iteration 126/1000 | Loss: 0.00003386
Iteration 127/1000 | Loss: 0.00003386
Iteration 128/1000 | Loss: 0.00003386
Iteration 129/1000 | Loss: 0.00003386
Iteration 130/1000 | Loss: 0.00003386
Iteration 131/1000 | Loss: 0.00003386
Iteration 132/1000 | Loss: 0.00003386
Iteration 133/1000 | Loss: 0.00003386
Iteration 134/1000 | Loss: 0.00003386
Iteration 135/1000 | Loss: 0.00003386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [3.3862081181723624e-05, 3.3862081181723624e-05, 3.3862081181723624e-05, 3.3862081181723624e-05, 3.3862081181723624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3862081181723624e-05

Optimization complete. Final v2v error: 4.718692302703857 mm

Highest mean error: 5.653896808624268 mm for frame 108

Lowest mean error: 3.5430941581726074 mm for frame 16

Saving results

Total time: 38.568785429000854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498853
Iteration 2/25 | Loss: 0.00145275
Iteration 3/25 | Loss: 0.00127803
Iteration 4/25 | Loss: 0.00126867
Iteration 5/25 | Loss: 0.00126671
Iteration 6/25 | Loss: 0.00126651
Iteration 7/25 | Loss: 0.00126651
Iteration 8/25 | Loss: 0.00126651
Iteration 9/25 | Loss: 0.00126651
Iteration 10/25 | Loss: 0.00126651
Iteration 11/25 | Loss: 0.00126651
Iteration 12/25 | Loss: 0.00126651
Iteration 13/25 | Loss: 0.00126651
Iteration 14/25 | Loss: 0.00126651
Iteration 15/25 | Loss: 0.00126651
Iteration 16/25 | Loss: 0.00126649
Iteration 17/25 | Loss: 0.00126649
Iteration 18/25 | Loss: 0.00126649
Iteration 19/25 | Loss: 0.00126649
Iteration 20/25 | Loss: 0.00126649
Iteration 21/25 | Loss: 0.00126649
Iteration 22/25 | Loss: 0.00126649
Iteration 23/25 | Loss: 0.00126649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012664867099374533, 0.0012664867099374533, 0.0012664867099374533, 0.0012664867099374533, 0.0012664867099374533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012664867099374533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37677145
Iteration 2/25 | Loss: 0.00091505
Iteration 3/25 | Loss: 0.00091504
Iteration 4/25 | Loss: 0.00091504
Iteration 5/25 | Loss: 0.00091504
Iteration 6/25 | Loss: 0.00091504
Iteration 7/25 | Loss: 0.00091504
Iteration 8/25 | Loss: 0.00091504
Iteration 9/25 | Loss: 0.00091504
Iteration 10/25 | Loss: 0.00091504
Iteration 11/25 | Loss: 0.00091504
Iteration 12/25 | Loss: 0.00091504
Iteration 13/25 | Loss: 0.00091504
Iteration 14/25 | Loss: 0.00091504
Iteration 15/25 | Loss: 0.00091504
Iteration 16/25 | Loss: 0.00091504
Iteration 17/25 | Loss: 0.00091504
Iteration 18/25 | Loss: 0.00091504
Iteration 19/25 | Loss: 0.00091504
Iteration 20/25 | Loss: 0.00091504
Iteration 21/25 | Loss: 0.00091504
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009150427649728954, 0.0009150427649728954, 0.0009150427649728954, 0.0009150427649728954, 0.0009150427649728954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009150427649728954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091504
Iteration 2/1000 | Loss: 0.00004465
Iteration 3/1000 | Loss: 0.00002563
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001880
Iteration 7/1000 | Loss: 0.00001804
Iteration 8/1000 | Loss: 0.00001743
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001672
Iteration 12/1000 | Loss: 0.00001665
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001646
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001639
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001637
Iteration 21/1000 | Loss: 0.00001637
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001633
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001632
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001624
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001616
Iteration 57/1000 | Loss: 0.00001616
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001613
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001611
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001604
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001602
Iteration 89/1000 | Loss: 0.00001602
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001600
Iteration 95/1000 | Loss: 0.00001600
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001599
Iteration 99/1000 | Loss: 0.00001599
Iteration 100/1000 | Loss: 0.00001599
Iteration 101/1000 | Loss: 0.00001599
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001597
Iteration 106/1000 | Loss: 0.00001597
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001597
Iteration 109/1000 | Loss: 0.00001596
Iteration 110/1000 | Loss: 0.00001596
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001593
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001593
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001592
Iteration 120/1000 | Loss: 0.00001591
Iteration 121/1000 | Loss: 0.00001591
Iteration 122/1000 | Loss: 0.00001591
Iteration 123/1000 | Loss: 0.00001590
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001590
Iteration 127/1000 | Loss: 0.00001590
Iteration 128/1000 | Loss: 0.00001590
Iteration 129/1000 | Loss: 0.00001590
Iteration 130/1000 | Loss: 0.00001590
Iteration 131/1000 | Loss: 0.00001589
Iteration 132/1000 | Loss: 0.00001589
Iteration 133/1000 | Loss: 0.00001589
Iteration 134/1000 | Loss: 0.00001589
Iteration 135/1000 | Loss: 0.00001589
Iteration 136/1000 | Loss: 0.00001588
Iteration 137/1000 | Loss: 0.00001588
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001587
Iteration 140/1000 | Loss: 0.00001587
Iteration 141/1000 | Loss: 0.00001587
Iteration 142/1000 | Loss: 0.00001587
Iteration 143/1000 | Loss: 0.00001587
Iteration 144/1000 | Loss: 0.00001587
Iteration 145/1000 | Loss: 0.00001587
Iteration 146/1000 | Loss: 0.00001587
Iteration 147/1000 | Loss: 0.00001586
Iteration 148/1000 | Loss: 0.00001586
Iteration 149/1000 | Loss: 0.00001586
Iteration 150/1000 | Loss: 0.00001586
Iteration 151/1000 | Loss: 0.00001586
Iteration 152/1000 | Loss: 0.00001586
Iteration 153/1000 | Loss: 0.00001586
Iteration 154/1000 | Loss: 0.00001586
Iteration 155/1000 | Loss: 0.00001586
Iteration 156/1000 | Loss: 0.00001585
Iteration 157/1000 | Loss: 0.00001585
Iteration 158/1000 | Loss: 0.00001585
Iteration 159/1000 | Loss: 0.00001585
Iteration 160/1000 | Loss: 0.00001585
Iteration 161/1000 | Loss: 0.00001585
Iteration 162/1000 | Loss: 0.00001585
Iteration 163/1000 | Loss: 0.00001585
Iteration 164/1000 | Loss: 0.00001585
Iteration 165/1000 | Loss: 0.00001585
Iteration 166/1000 | Loss: 0.00001585
Iteration 167/1000 | Loss: 0.00001585
Iteration 168/1000 | Loss: 0.00001585
Iteration 169/1000 | Loss: 0.00001584
Iteration 170/1000 | Loss: 0.00001584
Iteration 171/1000 | Loss: 0.00001584
Iteration 172/1000 | Loss: 0.00001584
Iteration 173/1000 | Loss: 0.00001584
Iteration 174/1000 | Loss: 0.00001584
Iteration 175/1000 | Loss: 0.00001584
Iteration 176/1000 | Loss: 0.00001584
Iteration 177/1000 | Loss: 0.00001584
Iteration 178/1000 | Loss: 0.00001583
Iteration 179/1000 | Loss: 0.00001583
Iteration 180/1000 | Loss: 0.00001583
Iteration 181/1000 | Loss: 0.00001583
Iteration 182/1000 | Loss: 0.00001583
Iteration 183/1000 | Loss: 0.00001583
Iteration 184/1000 | Loss: 0.00001583
Iteration 185/1000 | Loss: 0.00001583
Iteration 186/1000 | Loss: 0.00001582
Iteration 187/1000 | Loss: 0.00001582
Iteration 188/1000 | Loss: 0.00001582
Iteration 189/1000 | Loss: 0.00001582
Iteration 190/1000 | Loss: 0.00001582
Iteration 191/1000 | Loss: 0.00001582
Iteration 192/1000 | Loss: 0.00001582
Iteration 193/1000 | Loss: 0.00001582
Iteration 194/1000 | Loss: 0.00001582
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001581
Iteration 198/1000 | Loss: 0.00001581
Iteration 199/1000 | Loss: 0.00001581
Iteration 200/1000 | Loss: 0.00001581
Iteration 201/1000 | Loss: 0.00001581
Iteration 202/1000 | Loss: 0.00001581
Iteration 203/1000 | Loss: 0.00001580
Iteration 204/1000 | Loss: 0.00001580
Iteration 205/1000 | Loss: 0.00001580
Iteration 206/1000 | Loss: 0.00001580
Iteration 207/1000 | Loss: 0.00001580
Iteration 208/1000 | Loss: 0.00001580
Iteration 209/1000 | Loss: 0.00001579
Iteration 210/1000 | Loss: 0.00001579
Iteration 211/1000 | Loss: 0.00001579
Iteration 212/1000 | Loss: 0.00001579
Iteration 213/1000 | Loss: 0.00001579
Iteration 214/1000 | Loss: 0.00001579
Iteration 215/1000 | Loss: 0.00001579
Iteration 216/1000 | Loss: 0.00001579
Iteration 217/1000 | Loss: 0.00001579
Iteration 218/1000 | Loss: 0.00001579
Iteration 219/1000 | Loss: 0.00001579
Iteration 220/1000 | Loss: 0.00001579
Iteration 221/1000 | Loss: 0.00001579
Iteration 222/1000 | Loss: 0.00001579
Iteration 223/1000 | Loss: 0.00001579
Iteration 224/1000 | Loss: 0.00001579
Iteration 225/1000 | Loss: 0.00001579
Iteration 226/1000 | Loss: 0.00001579
Iteration 227/1000 | Loss: 0.00001579
Iteration 228/1000 | Loss: 0.00001579
Iteration 229/1000 | Loss: 0.00001579
Iteration 230/1000 | Loss: 0.00001579
Iteration 231/1000 | Loss: 0.00001579
Iteration 232/1000 | Loss: 0.00001579
Iteration 233/1000 | Loss: 0.00001579
Iteration 234/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.5786279618623666e-05, 1.5786279618623666e-05, 1.5786279618623666e-05, 1.5786279618623666e-05, 1.5786279618623666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5786279618623666e-05

Optimization complete. Final v2v error: 3.2332561016082764 mm

Highest mean error: 4.414517402648926 mm for frame 61

Lowest mean error: 2.8178622722625732 mm for frame 147

Saving results

Total time: 43.18888020515442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957631
Iteration 2/25 | Loss: 0.00957631
Iteration 3/25 | Loss: 0.00957631
Iteration 4/25 | Loss: 0.00362369
Iteration 5/25 | Loss: 0.00261497
Iteration 6/25 | Loss: 0.00252003
Iteration 7/25 | Loss: 0.00223449
Iteration 8/25 | Loss: 0.00223403
Iteration 9/25 | Loss: 0.00188733
Iteration 10/25 | Loss: 0.00169555
Iteration 11/25 | Loss: 0.00162402
Iteration 12/25 | Loss: 0.00157289
Iteration 13/25 | Loss: 0.00156785
Iteration 14/25 | Loss: 0.00155576
Iteration 15/25 | Loss: 0.00148605
Iteration 16/25 | Loss: 0.00145800
Iteration 17/25 | Loss: 0.00145260
Iteration 18/25 | Loss: 0.00146709
Iteration 19/25 | Loss: 0.00143018
Iteration 20/25 | Loss: 0.00140589
Iteration 21/25 | Loss: 0.00140449
Iteration 22/25 | Loss: 0.00140489
Iteration 23/25 | Loss: 0.00140042
Iteration 24/25 | Loss: 0.00139899
Iteration 25/25 | Loss: 0.00139494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34314823
Iteration 2/25 | Loss: 0.00172584
Iteration 3/25 | Loss: 0.00139100
Iteration 4/25 | Loss: 0.00139094
Iteration 5/25 | Loss: 0.00139094
Iteration 6/25 | Loss: 0.00139094
Iteration 7/25 | Loss: 0.00139094
Iteration 8/25 | Loss: 0.00139094
Iteration 9/25 | Loss: 0.00139094
Iteration 10/25 | Loss: 0.00139094
Iteration 11/25 | Loss: 0.00139094
Iteration 12/25 | Loss: 0.00139094
Iteration 13/25 | Loss: 0.00139094
Iteration 14/25 | Loss: 0.00139094
Iteration 15/25 | Loss: 0.00139094
Iteration 16/25 | Loss: 0.00139094
Iteration 17/25 | Loss: 0.00139094
Iteration 18/25 | Loss: 0.00139094
Iteration 19/25 | Loss: 0.00139094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013909433037042618, 0.0013909433037042618, 0.0013909433037042618, 0.0013909433037042618, 0.0013909433037042618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013909433037042618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139094
Iteration 2/1000 | Loss: 0.00025427
Iteration 3/1000 | Loss: 0.00023438
Iteration 4/1000 | Loss: 0.00011335
Iteration 5/1000 | Loss: 0.00007776
Iteration 6/1000 | Loss: 0.00050302
Iteration 7/1000 | Loss: 0.00005715
Iteration 8/1000 | Loss: 0.00026284
Iteration 9/1000 | Loss: 0.00094905
Iteration 10/1000 | Loss: 0.00036543
Iteration 11/1000 | Loss: 0.00006934
Iteration 12/1000 | Loss: 0.00019264
Iteration 13/1000 | Loss: 0.00081556
Iteration 14/1000 | Loss: 0.00010922
Iteration 15/1000 | Loss: 0.00011586
Iteration 16/1000 | Loss: 0.00006401
Iteration 17/1000 | Loss: 0.00006359
Iteration 18/1000 | Loss: 0.00006622
Iteration 19/1000 | Loss: 0.00006501
Iteration 20/1000 | Loss: 0.00006505
Iteration 21/1000 | Loss: 0.00006578
Iteration 22/1000 | Loss: 0.00026561
Iteration 23/1000 | Loss: 0.00186355
Iteration 24/1000 | Loss: 0.00150316
Iteration 25/1000 | Loss: 0.00044541
Iteration 26/1000 | Loss: 0.00085753
Iteration 27/1000 | Loss: 0.00068180
Iteration 28/1000 | Loss: 0.00011457
Iteration 29/1000 | Loss: 0.00008975
Iteration 30/1000 | Loss: 0.00004505
Iteration 31/1000 | Loss: 0.00059420
Iteration 32/1000 | Loss: 0.00036389
Iteration 33/1000 | Loss: 0.00047817
Iteration 34/1000 | Loss: 0.00007587
Iteration 35/1000 | Loss: 0.00005002
Iteration 36/1000 | Loss: 0.00002978
Iteration 37/1000 | Loss: 0.00036529
Iteration 38/1000 | Loss: 0.00065554
Iteration 39/1000 | Loss: 0.00014250
Iteration 40/1000 | Loss: 0.00002651
Iteration 41/1000 | Loss: 0.00002428
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00023004
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00017394
Iteration 46/1000 | Loss: 0.00011634
Iteration 47/1000 | Loss: 0.00094481
Iteration 48/1000 | Loss: 0.00114816
Iteration 49/1000 | Loss: 0.00018308
Iteration 50/1000 | Loss: 0.00022493
Iteration 51/1000 | Loss: 0.00002724
Iteration 52/1000 | Loss: 0.00009271
Iteration 53/1000 | Loss: 0.00004176
Iteration 54/1000 | Loss: 0.00007947
Iteration 55/1000 | Loss: 0.00002228
Iteration 56/1000 | Loss: 0.00010834
Iteration 57/1000 | Loss: 0.00016354
Iteration 58/1000 | Loss: 0.00115207
Iteration 59/1000 | Loss: 0.00003138
Iteration 60/1000 | Loss: 0.00018504
Iteration 61/1000 | Loss: 0.00020168
Iteration 62/1000 | Loss: 0.00006921
Iteration 63/1000 | Loss: 0.00008656
Iteration 64/1000 | Loss: 0.00003104
Iteration 65/1000 | Loss: 0.00002139
Iteration 66/1000 | Loss: 0.00004118
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00003745
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00003032
Iteration 71/1000 | Loss: 0.00002075
Iteration 72/1000 | Loss: 0.00002110
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002031
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002021
Iteration 77/1000 | Loss: 0.00002020
Iteration 78/1000 | Loss: 0.00002002
Iteration 79/1000 | Loss: 0.00016733
Iteration 80/1000 | Loss: 0.00002016
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001950
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001902
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001875
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001866
Iteration 95/1000 | Loss: 0.00014916
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00010448
Iteration 98/1000 | Loss: 0.00002337
Iteration 99/1000 | Loss: 0.00002449
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00017257
Iteration 102/1000 | Loss: 0.00013506
Iteration 103/1000 | Loss: 0.00004233
Iteration 104/1000 | Loss: 0.00001810
Iteration 105/1000 | Loss: 0.00017348
Iteration 106/1000 | Loss: 0.00015031
Iteration 107/1000 | Loss: 0.00012417
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00001974
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001894
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001873
Iteration 116/1000 | Loss: 0.00001873
Iteration 117/1000 | Loss: 0.00001873
Iteration 118/1000 | Loss: 0.00001869
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001867
Iteration 122/1000 | Loss: 0.00001866
Iteration 123/1000 | Loss: 0.00001866
Iteration 124/1000 | Loss: 0.00001865
Iteration 125/1000 | Loss: 0.00001864
Iteration 126/1000 | Loss: 0.00001864
Iteration 127/1000 | Loss: 0.00001864
Iteration 128/1000 | Loss: 0.00001863
Iteration 129/1000 | Loss: 0.00001863
Iteration 130/1000 | Loss: 0.00001863
Iteration 131/1000 | Loss: 0.00001863
Iteration 132/1000 | Loss: 0.00001863
Iteration 133/1000 | Loss: 0.00001863
Iteration 134/1000 | Loss: 0.00001863
Iteration 135/1000 | Loss: 0.00001862
Iteration 136/1000 | Loss: 0.00001862
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001861
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001860
Iteration 141/1000 | Loss: 0.00001860
Iteration 142/1000 | Loss: 0.00001860
Iteration 143/1000 | Loss: 0.00001860
Iteration 144/1000 | Loss: 0.00001860
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001859
Iteration 147/1000 | Loss: 0.00001859
Iteration 148/1000 | Loss: 0.00001859
Iteration 149/1000 | Loss: 0.00001859
Iteration 150/1000 | Loss: 0.00001859
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001858
Iteration 153/1000 | Loss: 0.00001858
Iteration 154/1000 | Loss: 0.00001858
Iteration 155/1000 | Loss: 0.00001855
Iteration 156/1000 | Loss: 0.00001854
Iteration 157/1000 | Loss: 0.00001852
Iteration 158/1000 | Loss: 0.00001852
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001850
Iteration 169/1000 | Loss: 0.00001850
Iteration 170/1000 | Loss: 0.00001850
Iteration 171/1000 | Loss: 0.00001850
Iteration 172/1000 | Loss: 0.00001850
Iteration 173/1000 | Loss: 0.00001850
Iteration 174/1000 | Loss: 0.00001850
Iteration 175/1000 | Loss: 0.00001849
Iteration 176/1000 | Loss: 0.00001849
Iteration 177/1000 | Loss: 0.00001849
Iteration 178/1000 | Loss: 0.00001849
Iteration 179/1000 | Loss: 0.00001849
Iteration 180/1000 | Loss: 0.00001849
Iteration 181/1000 | Loss: 0.00001849
Iteration 182/1000 | Loss: 0.00001849
Iteration 183/1000 | Loss: 0.00001849
Iteration 184/1000 | Loss: 0.00001849
Iteration 185/1000 | Loss: 0.00001849
Iteration 186/1000 | Loss: 0.00001849
Iteration 187/1000 | Loss: 0.00001849
Iteration 188/1000 | Loss: 0.00001849
Iteration 189/1000 | Loss: 0.00001849
Iteration 190/1000 | Loss: 0.00001849
Iteration 191/1000 | Loss: 0.00001848
Iteration 192/1000 | Loss: 0.00001848
Iteration 193/1000 | Loss: 0.00001848
Iteration 194/1000 | Loss: 0.00001848
Iteration 195/1000 | Loss: 0.00001847
Iteration 196/1000 | Loss: 0.00001847
Iteration 197/1000 | Loss: 0.00001847
Iteration 198/1000 | Loss: 0.00001847
Iteration 199/1000 | Loss: 0.00001847
Iteration 200/1000 | Loss: 0.00001847
Iteration 201/1000 | Loss: 0.00001847
Iteration 202/1000 | Loss: 0.00001847
Iteration 203/1000 | Loss: 0.00001846
Iteration 204/1000 | Loss: 0.00001846
Iteration 205/1000 | Loss: 0.00001846
Iteration 206/1000 | Loss: 0.00001846
Iteration 207/1000 | Loss: 0.00001846
Iteration 208/1000 | Loss: 0.00001846
Iteration 209/1000 | Loss: 0.00001846
Iteration 210/1000 | Loss: 0.00001846
Iteration 211/1000 | Loss: 0.00001846
Iteration 212/1000 | Loss: 0.00001846
Iteration 213/1000 | Loss: 0.00001846
Iteration 214/1000 | Loss: 0.00001846
Iteration 215/1000 | Loss: 0.00001846
Iteration 216/1000 | Loss: 0.00001845
Iteration 217/1000 | Loss: 0.00001845
Iteration 218/1000 | Loss: 0.00001845
Iteration 219/1000 | Loss: 0.00001845
Iteration 220/1000 | Loss: 0.00001845
Iteration 221/1000 | Loss: 0.00001845
Iteration 222/1000 | Loss: 0.00001845
Iteration 223/1000 | Loss: 0.00001844
Iteration 224/1000 | Loss: 0.00001844
Iteration 225/1000 | Loss: 0.00001843
Iteration 226/1000 | Loss: 0.00001843
Iteration 227/1000 | Loss: 0.00001843
Iteration 228/1000 | Loss: 0.00001843
Iteration 229/1000 | Loss: 0.00001843
Iteration 230/1000 | Loss: 0.00001843
Iteration 231/1000 | Loss: 0.00001843
Iteration 232/1000 | Loss: 0.00001843
Iteration 233/1000 | Loss: 0.00001843
Iteration 234/1000 | Loss: 0.00001843
Iteration 235/1000 | Loss: 0.00001843
Iteration 236/1000 | Loss: 0.00001843
Iteration 237/1000 | Loss: 0.00001842
Iteration 238/1000 | Loss: 0.00001842
Iteration 239/1000 | Loss: 0.00001842
Iteration 240/1000 | Loss: 0.00001842
Iteration 241/1000 | Loss: 0.00001842
Iteration 242/1000 | Loss: 0.00001842
Iteration 243/1000 | Loss: 0.00001842
Iteration 244/1000 | Loss: 0.00001842
Iteration 245/1000 | Loss: 0.00001842
Iteration 246/1000 | Loss: 0.00001842
Iteration 247/1000 | Loss: 0.00001842
Iteration 248/1000 | Loss: 0.00001842
Iteration 249/1000 | Loss: 0.00001842
Iteration 250/1000 | Loss: 0.00001842
Iteration 251/1000 | Loss: 0.00001842
Iteration 252/1000 | Loss: 0.00001842
Iteration 253/1000 | Loss: 0.00001842
Iteration 254/1000 | Loss: 0.00001842
Iteration 255/1000 | Loss: 0.00001842
Iteration 256/1000 | Loss: 0.00001842
Iteration 257/1000 | Loss: 0.00001842
Iteration 258/1000 | Loss: 0.00001841
Iteration 259/1000 | Loss: 0.00001841
Iteration 260/1000 | Loss: 0.00001841
Iteration 261/1000 | Loss: 0.00001841
Iteration 262/1000 | Loss: 0.00001841
Iteration 263/1000 | Loss: 0.00001841
Iteration 264/1000 | Loss: 0.00001841
Iteration 265/1000 | Loss: 0.00001841
Iteration 266/1000 | Loss: 0.00001841
Iteration 267/1000 | Loss: 0.00001841
Iteration 268/1000 | Loss: 0.00001841
Iteration 269/1000 | Loss: 0.00001841
Iteration 270/1000 | Loss: 0.00001841
Iteration 271/1000 | Loss: 0.00001841
Iteration 272/1000 | Loss: 0.00001841
Iteration 273/1000 | Loss: 0.00001841
Iteration 274/1000 | Loss: 0.00001841
Iteration 275/1000 | Loss: 0.00001840
Iteration 276/1000 | Loss: 0.00001840
Iteration 277/1000 | Loss: 0.00001840
Iteration 278/1000 | Loss: 0.00001840
Iteration 279/1000 | Loss: 0.00001840
Iteration 280/1000 | Loss: 0.00001840
Iteration 281/1000 | Loss: 0.00001840
Iteration 282/1000 | Loss: 0.00001840
Iteration 283/1000 | Loss: 0.00001840
Iteration 284/1000 | Loss: 0.00001840
Iteration 285/1000 | Loss: 0.00001839
Iteration 286/1000 | Loss: 0.00001839
Iteration 287/1000 | Loss: 0.00001839
Iteration 288/1000 | Loss: 0.00001839
Iteration 289/1000 | Loss: 0.00001839
Iteration 290/1000 | Loss: 0.00001838
Iteration 291/1000 | Loss: 0.00001838
Iteration 292/1000 | Loss: 0.00001838
Iteration 293/1000 | Loss: 0.00001838
Iteration 294/1000 | Loss: 0.00001838
Iteration 295/1000 | Loss: 0.00001838
Iteration 296/1000 | Loss: 0.00001838
Iteration 297/1000 | Loss: 0.00001838
Iteration 298/1000 | Loss: 0.00001838
Iteration 299/1000 | Loss: 0.00001837
Iteration 300/1000 | Loss: 0.00001837
Iteration 301/1000 | Loss: 0.00001837
Iteration 302/1000 | Loss: 0.00001837
Iteration 303/1000 | Loss: 0.00001837
Iteration 304/1000 | Loss: 0.00001837
Iteration 305/1000 | Loss: 0.00001837
Iteration 306/1000 | Loss: 0.00001837
Iteration 307/1000 | Loss: 0.00001836
Iteration 308/1000 | Loss: 0.00001836
Iteration 309/1000 | Loss: 0.00001836
Iteration 310/1000 | Loss: 0.00001836
Iteration 311/1000 | Loss: 0.00001836
Iteration 312/1000 | Loss: 0.00001836
Iteration 313/1000 | Loss: 0.00001836
Iteration 314/1000 | Loss: 0.00001836
Iteration 315/1000 | Loss: 0.00001836
Iteration 316/1000 | Loss: 0.00001836
Iteration 317/1000 | Loss: 0.00001836
Iteration 318/1000 | Loss: 0.00001836
Iteration 319/1000 | Loss: 0.00001836
Iteration 320/1000 | Loss: 0.00001836
Iteration 321/1000 | Loss: 0.00001836
Iteration 322/1000 | Loss: 0.00001836
Iteration 323/1000 | Loss: 0.00001836
Iteration 324/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [1.835681905504316e-05, 1.835681905504316e-05, 1.835681905504316e-05, 1.835681905504316e-05, 1.835681905504316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.835681905504316e-05

Optimization complete. Final v2v error: 3.400726795196533 mm

Highest mean error: 10.384987831115723 mm for frame 35

Lowest mean error: 3.105980396270752 mm for frame 149

Saving results

Total time: 230.6042399406433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605442
Iteration 2/25 | Loss: 0.00129644
Iteration 3/25 | Loss: 0.00123565
Iteration 4/25 | Loss: 0.00122836
Iteration 5/25 | Loss: 0.00122706
Iteration 6/25 | Loss: 0.00122706
Iteration 7/25 | Loss: 0.00122706
Iteration 8/25 | Loss: 0.00122706
Iteration 9/25 | Loss: 0.00122706
Iteration 10/25 | Loss: 0.00122706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012270607985556126, 0.0012270607985556126, 0.0012270607985556126, 0.0012270607985556126, 0.0012270607985556126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012270607985556126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.72235775
Iteration 2/25 | Loss: 0.00092133
Iteration 3/25 | Loss: 0.00092130
Iteration 4/25 | Loss: 0.00092130
Iteration 5/25 | Loss: 0.00092130
Iteration 6/25 | Loss: 0.00092130
Iteration 7/25 | Loss: 0.00092130
Iteration 8/25 | Loss: 0.00092130
Iteration 9/25 | Loss: 0.00092130
Iteration 10/25 | Loss: 0.00092130
Iteration 11/25 | Loss: 0.00092130
Iteration 12/25 | Loss: 0.00092130
Iteration 13/25 | Loss: 0.00092130
Iteration 14/25 | Loss: 0.00092130
Iteration 15/25 | Loss: 0.00092130
Iteration 16/25 | Loss: 0.00092130
Iteration 17/25 | Loss: 0.00092130
Iteration 18/25 | Loss: 0.00092130
Iteration 19/25 | Loss: 0.00092130
Iteration 20/25 | Loss: 0.00092130
Iteration 21/25 | Loss: 0.00092130
Iteration 22/25 | Loss: 0.00092130
Iteration 23/25 | Loss: 0.00092130
Iteration 24/25 | Loss: 0.00092130
Iteration 25/25 | Loss: 0.00092130

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092130
Iteration 2/1000 | Loss: 0.00002643
Iteration 3/1000 | Loss: 0.00001780
Iteration 4/1000 | Loss: 0.00001535
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001182
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001176
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001175
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001171
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001155
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001154
Iteration 102/1000 | Loss: 0.00001154
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001150
Iteration 114/1000 | Loss: 0.00001150
Iteration 115/1000 | Loss: 0.00001150
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001149
Iteration 120/1000 | Loss: 0.00001149
Iteration 121/1000 | Loss: 0.00001149
Iteration 122/1000 | Loss: 0.00001149
Iteration 123/1000 | Loss: 0.00001149
Iteration 124/1000 | Loss: 0.00001149
Iteration 125/1000 | Loss: 0.00001148
Iteration 126/1000 | Loss: 0.00001148
Iteration 127/1000 | Loss: 0.00001148
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001146
Iteration 150/1000 | Loss: 0.00001146
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001146
Iteration 155/1000 | Loss: 0.00001146
Iteration 156/1000 | Loss: 0.00001146
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Iteration 159/1000 | Loss: 0.00001146
Iteration 160/1000 | Loss: 0.00001146
Iteration 161/1000 | Loss: 0.00001146
Iteration 162/1000 | Loss: 0.00001146
Iteration 163/1000 | Loss: 0.00001146
Iteration 164/1000 | Loss: 0.00001145
Iteration 165/1000 | Loss: 0.00001145
Iteration 166/1000 | Loss: 0.00001145
Iteration 167/1000 | Loss: 0.00001145
Iteration 168/1000 | Loss: 0.00001145
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Iteration 173/1000 | Loss: 0.00001145
Iteration 174/1000 | Loss: 0.00001145
Iteration 175/1000 | Loss: 0.00001145
Iteration 176/1000 | Loss: 0.00001145
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001145
Iteration 181/1000 | Loss: 0.00001145
Iteration 182/1000 | Loss: 0.00001145
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Iteration 189/1000 | Loss: 0.00001145
Iteration 190/1000 | Loss: 0.00001145
Iteration 191/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.1451144928287249e-05, 1.1451144928287249e-05, 1.1451144928287249e-05, 1.1451144928287249e-05, 1.1451144928287249e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1451144928287249e-05

Optimization complete. Final v2v error: 2.899191379547119 mm

Highest mean error: 3.1114003658294678 mm for frame 141

Lowest mean error: 2.6975088119506836 mm for frame 225

Saving results

Total time: 46.63097834587097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401426
Iteration 2/25 | Loss: 0.00125828
Iteration 3/25 | Loss: 0.00120369
Iteration 4/25 | Loss: 0.00119597
Iteration 5/25 | Loss: 0.00119452
Iteration 6/25 | Loss: 0.00119452
Iteration 7/25 | Loss: 0.00119452
Iteration 8/25 | Loss: 0.00119452
Iteration 9/25 | Loss: 0.00119452
Iteration 10/25 | Loss: 0.00119452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011945209698751569, 0.0011945209698751569, 0.0011945209698751569, 0.0011945209698751569, 0.0011945209698751569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011945209698751569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14346433
Iteration 2/25 | Loss: 0.00087945
Iteration 3/25 | Loss: 0.00087944
Iteration 4/25 | Loss: 0.00087944
Iteration 5/25 | Loss: 0.00087944
Iteration 6/25 | Loss: 0.00087944
Iteration 7/25 | Loss: 0.00087944
Iteration 8/25 | Loss: 0.00087944
Iteration 9/25 | Loss: 0.00087944
Iteration 10/25 | Loss: 0.00087944
Iteration 11/25 | Loss: 0.00087944
Iteration 12/25 | Loss: 0.00087944
Iteration 13/25 | Loss: 0.00087944
Iteration 14/25 | Loss: 0.00087944
Iteration 15/25 | Loss: 0.00087944
Iteration 16/25 | Loss: 0.00087944
Iteration 17/25 | Loss: 0.00087944
Iteration 18/25 | Loss: 0.00087944
Iteration 19/25 | Loss: 0.00087944
Iteration 20/25 | Loss: 0.00087944
Iteration 21/25 | Loss: 0.00087944
Iteration 22/25 | Loss: 0.00087944
Iteration 23/25 | Loss: 0.00087944
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008794395253062248, 0.0008794395253062248, 0.0008794395253062248, 0.0008794395253062248, 0.0008794395253062248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008794395253062248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087944
Iteration 2/1000 | Loss: 0.00001920
Iteration 3/1000 | Loss: 0.00001505
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001333
Iteration 6/1000 | Loss: 0.00001283
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001201
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001178
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001122
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001118
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001117
Iteration 37/1000 | Loss: 0.00001116
Iteration 38/1000 | Loss: 0.00001116
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001113
Iteration 49/1000 | Loss: 0.00001113
Iteration 50/1000 | Loss: 0.00001113
Iteration 51/1000 | Loss: 0.00001113
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001113
Iteration 56/1000 | Loss: 0.00001113
Iteration 57/1000 | Loss: 0.00001113
Iteration 58/1000 | Loss: 0.00001113
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001110
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001107
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.0987913810822647e-05, 1.0987913810822647e-05, 1.0987913810822647e-05, 1.0987913810822647e-05, 1.0987913810822647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0987913810822647e-05

Optimization complete. Final v2v error: 2.8649322986602783 mm

Highest mean error: 3.325157642364502 mm for frame 209

Lowest mean error: 2.75331711769104 mm for frame 75

Saving results

Total time: 36.943012714385986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426511
Iteration 2/25 | Loss: 0.00141609
Iteration 3/25 | Loss: 0.00129896
Iteration 4/25 | Loss: 0.00128765
Iteration 5/25 | Loss: 0.00128526
Iteration 6/25 | Loss: 0.00128463
Iteration 7/25 | Loss: 0.00128463
Iteration 8/25 | Loss: 0.00128463
Iteration 9/25 | Loss: 0.00128463
Iteration 10/25 | Loss: 0.00128463
Iteration 11/25 | Loss: 0.00128463
Iteration 12/25 | Loss: 0.00128463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012846270110458136, 0.0012846270110458136, 0.0012846270110458136, 0.0012846270110458136, 0.0012846270110458136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012846270110458136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39889455
Iteration 2/25 | Loss: 0.00098042
Iteration 3/25 | Loss: 0.00098041
Iteration 4/25 | Loss: 0.00098041
Iteration 5/25 | Loss: 0.00098041
Iteration 6/25 | Loss: 0.00098041
Iteration 7/25 | Loss: 0.00098041
Iteration 8/25 | Loss: 0.00098041
Iteration 9/25 | Loss: 0.00098041
Iteration 10/25 | Loss: 0.00098041
Iteration 11/25 | Loss: 0.00098041
Iteration 12/25 | Loss: 0.00098041
Iteration 13/25 | Loss: 0.00098041
Iteration 14/25 | Loss: 0.00098041
Iteration 15/25 | Loss: 0.00098041
Iteration 16/25 | Loss: 0.00098041
Iteration 17/25 | Loss: 0.00098041
Iteration 18/25 | Loss: 0.00098041
Iteration 19/25 | Loss: 0.00098041
Iteration 20/25 | Loss: 0.00098041
Iteration 21/25 | Loss: 0.00098041
Iteration 22/25 | Loss: 0.00098041
Iteration 23/25 | Loss: 0.00098041
Iteration 24/25 | Loss: 0.00098041
Iteration 25/25 | Loss: 0.00098041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098041
Iteration 2/1000 | Loss: 0.00004261
Iteration 3/1000 | Loss: 0.00002408
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001986
Iteration 6/1000 | Loss: 0.00001909
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001819
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001721
Iteration 22/1000 | Loss: 0.00001717
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001707
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001706
Iteration 29/1000 | Loss: 0.00001706
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001704
Iteration 33/1000 | Loss: 0.00001703
Iteration 34/1000 | Loss: 0.00001703
Iteration 35/1000 | Loss: 0.00001703
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001700
Iteration 45/1000 | Loss: 0.00001700
Iteration 46/1000 | Loss: 0.00001700
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001700
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001698
Iteration 55/1000 | Loss: 0.00001698
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001696
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001695
Iteration 73/1000 | Loss: 0.00001695
Iteration 74/1000 | Loss: 0.00001694
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001694
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001692
Iteration 82/1000 | Loss: 0.00001692
Iteration 83/1000 | Loss: 0.00001692
Iteration 84/1000 | Loss: 0.00001692
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001692
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001685
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001683
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001681
Iteration 178/1000 | Loss: 0.00001681
Iteration 179/1000 | Loss: 0.00001681
Iteration 180/1000 | Loss: 0.00001681
Iteration 181/1000 | Loss: 0.00001681
Iteration 182/1000 | Loss: 0.00001681
Iteration 183/1000 | Loss: 0.00001681
Iteration 184/1000 | Loss: 0.00001681
Iteration 185/1000 | Loss: 0.00001681
Iteration 186/1000 | Loss: 0.00001681
Iteration 187/1000 | Loss: 0.00001681
Iteration 188/1000 | Loss: 0.00001681
Iteration 189/1000 | Loss: 0.00001681
Iteration 190/1000 | Loss: 0.00001681
Iteration 191/1000 | Loss: 0.00001681
Iteration 192/1000 | Loss: 0.00001681
Iteration 193/1000 | Loss: 0.00001681
Iteration 194/1000 | Loss: 0.00001681
Iteration 195/1000 | Loss: 0.00001681
Iteration 196/1000 | Loss: 0.00001681
Iteration 197/1000 | Loss: 0.00001681
Iteration 198/1000 | Loss: 0.00001681
Iteration 199/1000 | Loss: 0.00001681
Iteration 200/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.6805353880045004e-05, 1.6805353880045004e-05, 1.6805353880045004e-05, 1.6805353880045004e-05, 1.6805353880045004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6805353880045004e-05

Optimization complete. Final v2v error: 3.354910135269165 mm

Highest mean error: 4.604203701019287 mm for frame 38

Lowest mean error: 2.9408512115478516 mm for frame 95

Saving results

Total time: 39.86883354187012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814666
Iteration 2/25 | Loss: 0.00128812
Iteration 3/25 | Loss: 0.00122378
Iteration 4/25 | Loss: 0.00121741
Iteration 5/25 | Loss: 0.00121528
Iteration 6/25 | Loss: 0.00121528
Iteration 7/25 | Loss: 0.00121528
Iteration 8/25 | Loss: 0.00121528
Iteration 9/25 | Loss: 0.00121528
Iteration 10/25 | Loss: 0.00121528
Iteration 11/25 | Loss: 0.00121528
Iteration 12/25 | Loss: 0.00121528
Iteration 13/25 | Loss: 0.00121528
Iteration 14/25 | Loss: 0.00121528
Iteration 15/25 | Loss: 0.00121528
Iteration 16/25 | Loss: 0.00121528
Iteration 17/25 | Loss: 0.00121528
Iteration 18/25 | Loss: 0.00121528
Iteration 19/25 | Loss: 0.00121528
Iteration 20/25 | Loss: 0.00121528
Iteration 21/25 | Loss: 0.00121528
Iteration 22/25 | Loss: 0.00121528
Iteration 23/25 | Loss: 0.00121528
Iteration 24/25 | Loss: 0.00121528
Iteration 25/25 | Loss: 0.00121528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.23739624
Iteration 2/25 | Loss: 0.00099302
Iteration 3/25 | Loss: 0.00099301
Iteration 4/25 | Loss: 0.00099301
Iteration 5/25 | Loss: 0.00099301
Iteration 6/25 | Loss: 0.00099301
Iteration 7/25 | Loss: 0.00099301
Iteration 8/25 | Loss: 0.00099301
Iteration 9/25 | Loss: 0.00099301
Iteration 10/25 | Loss: 0.00099301
Iteration 11/25 | Loss: 0.00099301
Iteration 12/25 | Loss: 0.00099301
Iteration 13/25 | Loss: 0.00099301
Iteration 14/25 | Loss: 0.00099301
Iteration 15/25 | Loss: 0.00099301
Iteration 16/25 | Loss: 0.00099301
Iteration 17/25 | Loss: 0.00099301
Iteration 18/25 | Loss: 0.00099301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000993005814962089, 0.000993005814962089, 0.000993005814962089, 0.000993005814962089, 0.000993005814962089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000993005814962089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099301
Iteration 2/1000 | Loss: 0.00002485
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001592
Iteration 6/1000 | Loss: 0.00001543
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001431
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001385
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001345
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001336
Iteration 40/1000 | Loss: 0.00001335
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001315
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001309
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001306
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001303
Iteration 100/1000 | Loss: 0.00001303
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001303
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001301
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001300
Iteration 142/1000 | Loss: 0.00001300
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.3004637366975658e-05, 1.3004637366975658e-05, 1.3004637366975658e-05, 1.3004637366975658e-05, 1.3004637366975658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3004637366975658e-05

Optimization complete. Final v2v error: 3.0910866260528564 mm

Highest mean error: 3.4099676609039307 mm for frame 114

Lowest mean error: 2.7797086238861084 mm for frame 139

Saving results

Total time: 38.32019782066345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420874
Iteration 2/25 | Loss: 0.00129540
Iteration 3/25 | Loss: 0.00123113
Iteration 4/25 | Loss: 0.00122090
Iteration 5/25 | Loss: 0.00121707
Iteration 6/25 | Loss: 0.00121624
Iteration 7/25 | Loss: 0.00121624
Iteration 8/25 | Loss: 0.00121624
Iteration 9/25 | Loss: 0.00121624
Iteration 10/25 | Loss: 0.00121624
Iteration 11/25 | Loss: 0.00121624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012162384809926152, 0.0012162384809926152, 0.0012162384809926152, 0.0012162384809926152, 0.0012162384809926152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012162384809926152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70404530
Iteration 2/25 | Loss: 0.00101777
Iteration 3/25 | Loss: 0.00101777
Iteration 4/25 | Loss: 0.00101777
Iteration 5/25 | Loss: 0.00101777
Iteration 6/25 | Loss: 0.00101777
Iteration 7/25 | Loss: 0.00101776
Iteration 8/25 | Loss: 0.00101776
Iteration 9/25 | Loss: 0.00101776
Iteration 10/25 | Loss: 0.00101776
Iteration 11/25 | Loss: 0.00101776
Iteration 12/25 | Loss: 0.00101776
Iteration 13/25 | Loss: 0.00101776
Iteration 14/25 | Loss: 0.00101776
Iteration 15/25 | Loss: 0.00101776
Iteration 16/25 | Loss: 0.00101776
Iteration 17/25 | Loss: 0.00101776
Iteration 18/25 | Loss: 0.00101776
Iteration 19/25 | Loss: 0.00101776
Iteration 20/25 | Loss: 0.00101776
Iteration 21/25 | Loss: 0.00101776
Iteration 22/25 | Loss: 0.00101776
Iteration 23/25 | Loss: 0.00101776
Iteration 24/25 | Loss: 0.00101776
Iteration 25/25 | Loss: 0.00101776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101776
Iteration 2/1000 | Loss: 0.00002993
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00001298
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001296
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00001294
Iteration 22/1000 | Loss: 0.00001293
Iteration 23/1000 | Loss: 0.00001292
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001285
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001284
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001282
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001280
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001278
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001277
Iteration 43/1000 | Loss: 0.00001277
Iteration 44/1000 | Loss: 0.00001277
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001276
Iteration 48/1000 | Loss: 0.00001276
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001272
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001270
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001269
Iteration 70/1000 | Loss: 0.00001269
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001269
Iteration 78/1000 | Loss: 0.00001269
Iteration 79/1000 | Loss: 0.00001269
Iteration 80/1000 | Loss: 0.00001269
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001268
Iteration 83/1000 | Loss: 0.00001268
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001266
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001263
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001261
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001258
Iteration 101/1000 | Loss: 0.00001258
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001254
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001249
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001247
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Iteration 141/1000 | Loss: 0.00001245
Iteration 142/1000 | Loss: 0.00001245
Iteration 143/1000 | Loss: 0.00001245
Iteration 144/1000 | Loss: 0.00001244
Iteration 145/1000 | Loss: 0.00001244
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001242
Iteration 150/1000 | Loss: 0.00001242
Iteration 151/1000 | Loss: 0.00001242
Iteration 152/1000 | Loss: 0.00001242
Iteration 153/1000 | Loss: 0.00001241
Iteration 154/1000 | Loss: 0.00001241
Iteration 155/1000 | Loss: 0.00001241
Iteration 156/1000 | Loss: 0.00001241
Iteration 157/1000 | Loss: 0.00001240
Iteration 158/1000 | Loss: 0.00001240
Iteration 159/1000 | Loss: 0.00001240
Iteration 160/1000 | Loss: 0.00001240
Iteration 161/1000 | Loss: 0.00001240
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001239
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001239
Iteration 166/1000 | Loss: 0.00001239
Iteration 167/1000 | Loss: 0.00001239
Iteration 168/1000 | Loss: 0.00001238
Iteration 169/1000 | Loss: 0.00001238
Iteration 170/1000 | Loss: 0.00001238
Iteration 171/1000 | Loss: 0.00001238
Iteration 172/1000 | Loss: 0.00001238
Iteration 173/1000 | Loss: 0.00001238
Iteration 174/1000 | Loss: 0.00001238
Iteration 175/1000 | Loss: 0.00001238
Iteration 176/1000 | Loss: 0.00001238
Iteration 177/1000 | Loss: 0.00001238
Iteration 178/1000 | Loss: 0.00001237
Iteration 179/1000 | Loss: 0.00001237
Iteration 180/1000 | Loss: 0.00001237
Iteration 181/1000 | Loss: 0.00001237
Iteration 182/1000 | Loss: 0.00001237
Iteration 183/1000 | Loss: 0.00001237
Iteration 184/1000 | Loss: 0.00001237
Iteration 185/1000 | Loss: 0.00001237
Iteration 186/1000 | Loss: 0.00001237
Iteration 187/1000 | Loss: 0.00001237
Iteration 188/1000 | Loss: 0.00001237
Iteration 189/1000 | Loss: 0.00001237
Iteration 190/1000 | Loss: 0.00001237
Iteration 191/1000 | Loss: 0.00001237
Iteration 192/1000 | Loss: 0.00001237
Iteration 193/1000 | Loss: 0.00001237
Iteration 194/1000 | Loss: 0.00001237
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Iteration 199/1000 | Loss: 0.00001236
Iteration 200/1000 | Loss: 0.00001236
Iteration 201/1000 | Loss: 0.00001236
Iteration 202/1000 | Loss: 0.00001236
Iteration 203/1000 | Loss: 0.00001236
Iteration 204/1000 | Loss: 0.00001236
Iteration 205/1000 | Loss: 0.00001236
Iteration 206/1000 | Loss: 0.00001236
Iteration 207/1000 | Loss: 0.00001236
Iteration 208/1000 | Loss: 0.00001236
Iteration 209/1000 | Loss: 0.00001236
Iteration 210/1000 | Loss: 0.00001236
Iteration 211/1000 | Loss: 0.00001236
Iteration 212/1000 | Loss: 0.00001236
Iteration 213/1000 | Loss: 0.00001236
Iteration 214/1000 | Loss: 0.00001236
Iteration 215/1000 | Loss: 0.00001236
Iteration 216/1000 | Loss: 0.00001236
Iteration 217/1000 | Loss: 0.00001236
Iteration 218/1000 | Loss: 0.00001236
Iteration 219/1000 | Loss: 0.00001236
Iteration 220/1000 | Loss: 0.00001236
Iteration 221/1000 | Loss: 0.00001236
Iteration 222/1000 | Loss: 0.00001236
Iteration 223/1000 | Loss: 0.00001236
Iteration 224/1000 | Loss: 0.00001236
Iteration 225/1000 | Loss: 0.00001236
Iteration 226/1000 | Loss: 0.00001236
Iteration 227/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.2358616004348733e-05, 1.2358616004348733e-05, 1.2358616004348733e-05, 1.2358616004348733e-05, 1.2358616004348733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2358616004348733e-05

Optimization complete. Final v2v error: 3.007709264755249 mm

Highest mean error: 3.9229137897491455 mm for frame 74

Lowest mean error: 2.723113536834717 mm for frame 96

Saving results

Total time: 44.065138816833496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783712
Iteration 2/25 | Loss: 0.00174593
Iteration 3/25 | Loss: 0.00139958
Iteration 4/25 | Loss: 0.00136009
Iteration 5/25 | Loss: 0.00135846
Iteration 6/25 | Loss: 0.00136148
Iteration 7/25 | Loss: 0.00134988
Iteration 8/25 | Loss: 0.00134544
Iteration 9/25 | Loss: 0.00133954
Iteration 10/25 | Loss: 0.00133949
Iteration 11/25 | Loss: 0.00133949
Iteration 12/25 | Loss: 0.00133949
Iteration 13/25 | Loss: 0.00133949
Iteration 14/25 | Loss: 0.00133949
Iteration 15/25 | Loss: 0.00133948
Iteration 16/25 | Loss: 0.00133948
Iteration 17/25 | Loss: 0.00133948
Iteration 18/25 | Loss: 0.00133948
Iteration 19/25 | Loss: 0.00133948
Iteration 20/25 | Loss: 0.00133948
Iteration 21/25 | Loss: 0.00133948
Iteration 22/25 | Loss: 0.00133948
Iteration 23/25 | Loss: 0.00133948
Iteration 24/25 | Loss: 0.00133948
Iteration 25/25 | Loss: 0.00133948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69179034
Iteration 2/25 | Loss: 0.00079945
Iteration 3/25 | Loss: 0.00079941
Iteration 4/25 | Loss: 0.00079941
Iteration 5/25 | Loss: 0.00079941
Iteration 6/25 | Loss: 0.00079941
Iteration 7/25 | Loss: 0.00079941
Iteration 8/25 | Loss: 0.00079941
Iteration 9/25 | Loss: 0.00079941
Iteration 10/25 | Loss: 0.00079941
Iteration 11/25 | Loss: 0.00079941
Iteration 12/25 | Loss: 0.00079941
Iteration 13/25 | Loss: 0.00079941
Iteration 14/25 | Loss: 0.00079941
Iteration 15/25 | Loss: 0.00079941
Iteration 16/25 | Loss: 0.00079941
Iteration 17/25 | Loss: 0.00079941
Iteration 18/25 | Loss: 0.00079941
Iteration 19/25 | Loss: 0.00079941
Iteration 20/25 | Loss: 0.00079941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007994103943929076, 0.0007994103943929076, 0.0007994103943929076, 0.0007994103943929076, 0.0007994103943929076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007994103943929076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079941
Iteration 2/1000 | Loss: 0.00003611
Iteration 3/1000 | Loss: 0.00002505
Iteration 4/1000 | Loss: 0.00002258
Iteration 5/1000 | Loss: 0.00012390
Iteration 6/1000 | Loss: 0.00022823
Iteration 7/1000 | Loss: 0.00004676
Iteration 8/1000 | Loss: 0.00002083
Iteration 9/1000 | Loss: 0.00012160
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00001979
Iteration 12/1000 | Loss: 0.00001951
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001917
Iteration 16/1000 | Loss: 0.00001914
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001904
Iteration 19/1000 | Loss: 0.00001902
Iteration 20/1000 | Loss: 0.00001902
Iteration 21/1000 | Loss: 0.00001900
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001895
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001894
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001894
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001893
Iteration 31/1000 | Loss: 0.00001892
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001888
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001887
Iteration 43/1000 | Loss: 0.00001887
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001886
Iteration 46/1000 | Loss: 0.00001886
Iteration 47/1000 | Loss: 0.00001886
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001886
Iteration 50/1000 | Loss: 0.00001885
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001885
Iteration 56/1000 | Loss: 0.00001884
Iteration 57/1000 | Loss: 0.00001884
Iteration 58/1000 | Loss: 0.00001884
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001883
Iteration 61/1000 | Loss: 0.00001883
Iteration 62/1000 | Loss: 0.00001883
Iteration 63/1000 | Loss: 0.00001882
Iteration 64/1000 | Loss: 0.00001882
Iteration 65/1000 | Loss: 0.00001882
Iteration 66/1000 | Loss: 0.00001882
Iteration 67/1000 | Loss: 0.00001882
Iteration 68/1000 | Loss: 0.00001881
Iteration 69/1000 | Loss: 0.00001881
Iteration 70/1000 | Loss: 0.00001880
Iteration 71/1000 | Loss: 0.00001879
Iteration 72/1000 | Loss: 0.00001879
Iteration 73/1000 | Loss: 0.00001879
Iteration 74/1000 | Loss: 0.00001879
Iteration 75/1000 | Loss: 0.00001879
Iteration 76/1000 | Loss: 0.00001879
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001879
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001879
Iteration 81/1000 | Loss: 0.00001879
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001878
Iteration 84/1000 | Loss: 0.00001878
Iteration 85/1000 | Loss: 0.00001876
Iteration 86/1000 | Loss: 0.00001876
Iteration 87/1000 | Loss: 0.00001876
Iteration 88/1000 | Loss: 0.00001876
Iteration 89/1000 | Loss: 0.00001876
Iteration 90/1000 | Loss: 0.00001876
Iteration 91/1000 | Loss: 0.00001876
Iteration 92/1000 | Loss: 0.00001876
Iteration 93/1000 | Loss: 0.00001876
Iteration 94/1000 | Loss: 0.00001875
Iteration 95/1000 | Loss: 0.00001875
Iteration 96/1000 | Loss: 0.00001875
Iteration 97/1000 | Loss: 0.00001875
Iteration 98/1000 | Loss: 0.00001874
Iteration 99/1000 | Loss: 0.00001874
Iteration 100/1000 | Loss: 0.00001874
Iteration 101/1000 | Loss: 0.00001874
Iteration 102/1000 | Loss: 0.00001874
Iteration 103/1000 | Loss: 0.00001873
Iteration 104/1000 | Loss: 0.00001873
Iteration 105/1000 | Loss: 0.00001873
Iteration 106/1000 | Loss: 0.00001873
Iteration 107/1000 | Loss: 0.00001873
Iteration 108/1000 | Loss: 0.00001873
Iteration 109/1000 | Loss: 0.00001873
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001872
Iteration 115/1000 | Loss: 0.00001872
Iteration 116/1000 | Loss: 0.00001872
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001871
Iteration 125/1000 | Loss: 0.00001871
Iteration 126/1000 | Loss: 0.00001871
Iteration 127/1000 | Loss: 0.00001871
Iteration 128/1000 | Loss: 0.00001871
Iteration 129/1000 | Loss: 0.00001871
Iteration 130/1000 | Loss: 0.00001871
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001870
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001870
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001869
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001869
Iteration 148/1000 | Loss: 0.00001869
Iteration 149/1000 | Loss: 0.00001869
Iteration 150/1000 | Loss: 0.00001869
Iteration 151/1000 | Loss: 0.00001869
Iteration 152/1000 | Loss: 0.00001869
Iteration 153/1000 | Loss: 0.00001868
Iteration 154/1000 | Loss: 0.00001868
Iteration 155/1000 | Loss: 0.00001868
Iteration 156/1000 | Loss: 0.00001868
Iteration 157/1000 | Loss: 0.00001868
Iteration 158/1000 | Loss: 0.00001868
Iteration 159/1000 | Loss: 0.00001868
Iteration 160/1000 | Loss: 0.00001868
Iteration 161/1000 | Loss: 0.00001868
Iteration 162/1000 | Loss: 0.00001868
Iteration 163/1000 | Loss: 0.00001868
Iteration 164/1000 | Loss: 0.00001868
Iteration 165/1000 | Loss: 0.00001868
Iteration 166/1000 | Loss: 0.00001868
Iteration 167/1000 | Loss: 0.00001868
Iteration 168/1000 | Loss: 0.00001867
Iteration 169/1000 | Loss: 0.00001867
Iteration 170/1000 | Loss: 0.00001867
Iteration 171/1000 | Loss: 0.00001867
Iteration 172/1000 | Loss: 0.00001867
Iteration 173/1000 | Loss: 0.00001867
Iteration 174/1000 | Loss: 0.00001867
Iteration 175/1000 | Loss: 0.00001867
Iteration 176/1000 | Loss: 0.00001867
Iteration 177/1000 | Loss: 0.00001867
Iteration 178/1000 | Loss: 0.00001867
Iteration 179/1000 | Loss: 0.00001867
Iteration 180/1000 | Loss: 0.00001867
Iteration 181/1000 | Loss: 0.00001867
Iteration 182/1000 | Loss: 0.00001867
Iteration 183/1000 | Loss: 0.00001867
Iteration 184/1000 | Loss: 0.00001867
Iteration 185/1000 | Loss: 0.00001867
Iteration 186/1000 | Loss: 0.00001867
Iteration 187/1000 | Loss: 0.00001867
Iteration 188/1000 | Loss: 0.00001867
Iteration 189/1000 | Loss: 0.00001867
Iteration 190/1000 | Loss: 0.00001867
Iteration 191/1000 | Loss: 0.00001867
Iteration 192/1000 | Loss: 0.00001867
Iteration 193/1000 | Loss: 0.00001867
Iteration 194/1000 | Loss: 0.00001867
Iteration 195/1000 | Loss: 0.00001867
Iteration 196/1000 | Loss: 0.00001867
Iteration 197/1000 | Loss: 0.00001867
Iteration 198/1000 | Loss: 0.00001867
Iteration 199/1000 | Loss: 0.00001867
Iteration 200/1000 | Loss: 0.00001867
Iteration 201/1000 | Loss: 0.00001867
Iteration 202/1000 | Loss: 0.00001867
Iteration 203/1000 | Loss: 0.00001867
Iteration 204/1000 | Loss: 0.00001867
Iteration 205/1000 | Loss: 0.00001867
Iteration 206/1000 | Loss: 0.00001867
Iteration 207/1000 | Loss: 0.00001867
Iteration 208/1000 | Loss: 0.00001867
Iteration 209/1000 | Loss: 0.00001867
Iteration 210/1000 | Loss: 0.00001867
Iteration 211/1000 | Loss: 0.00001867
Iteration 212/1000 | Loss: 0.00001867
Iteration 213/1000 | Loss: 0.00001867
Iteration 214/1000 | Loss: 0.00001867
Iteration 215/1000 | Loss: 0.00001867
Iteration 216/1000 | Loss: 0.00001867
Iteration 217/1000 | Loss: 0.00001867
Iteration 218/1000 | Loss: 0.00001867
Iteration 219/1000 | Loss: 0.00001867
Iteration 220/1000 | Loss: 0.00001867
Iteration 221/1000 | Loss: 0.00001867
Iteration 222/1000 | Loss: 0.00001867
Iteration 223/1000 | Loss: 0.00001867
Iteration 224/1000 | Loss: 0.00001867
Iteration 225/1000 | Loss: 0.00001867
Iteration 226/1000 | Loss: 0.00001867
Iteration 227/1000 | Loss: 0.00001867
Iteration 228/1000 | Loss: 0.00001867
Iteration 229/1000 | Loss: 0.00001867
Iteration 230/1000 | Loss: 0.00001867
Iteration 231/1000 | Loss: 0.00001867
Iteration 232/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.866512502601836e-05, 1.866512502601836e-05, 1.866512502601836e-05, 1.866512502601836e-05, 1.866512502601836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.866512502601836e-05

Optimization complete. Final v2v error: 3.6105568408966064 mm

Highest mean error: 3.9822394847869873 mm for frame 134

Lowest mean error: 3.1454215049743652 mm for frame 178

Saving results

Total time: 53.619967460632324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400645
Iteration 2/25 | Loss: 0.00133114
Iteration 3/25 | Loss: 0.00124086
Iteration 4/25 | Loss: 0.00123345
Iteration 5/25 | Loss: 0.00123226
Iteration 6/25 | Loss: 0.00123226
Iteration 7/25 | Loss: 0.00123226
Iteration 8/25 | Loss: 0.00123226
Iteration 9/25 | Loss: 0.00123226
Iteration 10/25 | Loss: 0.00123226
Iteration 11/25 | Loss: 0.00123226
Iteration 12/25 | Loss: 0.00123226
Iteration 13/25 | Loss: 0.00123226
Iteration 14/25 | Loss: 0.00123226
Iteration 15/25 | Loss: 0.00123226
Iteration 16/25 | Loss: 0.00123226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012322573456913233, 0.0012322573456913233, 0.0012322573456913233, 0.0012322573456913233, 0.0012322573456913233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012322573456913233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59235978
Iteration 2/25 | Loss: 0.00083904
Iteration 3/25 | Loss: 0.00083904
Iteration 4/25 | Loss: 0.00083904
Iteration 5/25 | Loss: 0.00083904
Iteration 6/25 | Loss: 0.00083904
Iteration 7/25 | Loss: 0.00083904
Iteration 8/25 | Loss: 0.00083904
Iteration 9/25 | Loss: 0.00083904
Iteration 10/25 | Loss: 0.00083904
Iteration 11/25 | Loss: 0.00083904
Iteration 12/25 | Loss: 0.00083904
Iteration 13/25 | Loss: 0.00083904
Iteration 14/25 | Loss: 0.00083904
Iteration 15/25 | Loss: 0.00083904
Iteration 16/25 | Loss: 0.00083904
Iteration 17/25 | Loss: 0.00083904
Iteration 18/25 | Loss: 0.00083904
Iteration 19/25 | Loss: 0.00083904
Iteration 20/25 | Loss: 0.00083904
Iteration 21/25 | Loss: 0.00083904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008390357834286988, 0.0008390357834286988, 0.0008390357834286988, 0.0008390357834286988, 0.0008390357834286988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008390357834286988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083904
Iteration 2/1000 | Loss: 0.00002640
Iteration 3/1000 | Loss: 0.00001690
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001290
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001167
Iteration 19/1000 | Loss: 0.00001167
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001165
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001163
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001157
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001157
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001140
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001130
Iteration 55/1000 | Loss: 0.00001130
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001129
Iteration 59/1000 | Loss: 0.00001129
Iteration 60/1000 | Loss: 0.00001129
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001123
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001115
Iteration 102/1000 | Loss: 0.00001115
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001114
Iteration 105/1000 | Loss: 0.00001114
Iteration 106/1000 | Loss: 0.00001114
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001106
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001104
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001104
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001104
Iteration 155/1000 | Loss: 0.00001104
Iteration 156/1000 | Loss: 0.00001104
Iteration 157/1000 | Loss: 0.00001104
Iteration 158/1000 | Loss: 0.00001104
Iteration 159/1000 | Loss: 0.00001104
Iteration 160/1000 | Loss: 0.00001103
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001103
Iteration 163/1000 | Loss: 0.00001103
Iteration 164/1000 | Loss: 0.00001103
Iteration 165/1000 | Loss: 0.00001103
Iteration 166/1000 | Loss: 0.00001103
Iteration 167/1000 | Loss: 0.00001103
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001102
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001102
Iteration 179/1000 | Loss: 0.00001102
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001102
Iteration 188/1000 | Loss: 0.00001102
Iteration 189/1000 | Loss: 0.00001102
Iteration 190/1000 | Loss: 0.00001102
Iteration 191/1000 | Loss: 0.00001102
Iteration 192/1000 | Loss: 0.00001102
Iteration 193/1000 | Loss: 0.00001102
Iteration 194/1000 | Loss: 0.00001102
Iteration 195/1000 | Loss: 0.00001102
Iteration 196/1000 | Loss: 0.00001102
Iteration 197/1000 | Loss: 0.00001102
Iteration 198/1000 | Loss: 0.00001102
Iteration 199/1000 | Loss: 0.00001102
Iteration 200/1000 | Loss: 0.00001102
Iteration 201/1000 | Loss: 0.00001102
Iteration 202/1000 | Loss: 0.00001102
Iteration 203/1000 | Loss: 0.00001102
Iteration 204/1000 | Loss: 0.00001102
Iteration 205/1000 | Loss: 0.00001102
Iteration 206/1000 | Loss: 0.00001102
Iteration 207/1000 | Loss: 0.00001102
Iteration 208/1000 | Loss: 0.00001102
Iteration 209/1000 | Loss: 0.00001102
Iteration 210/1000 | Loss: 0.00001102
Iteration 211/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.101715042750584e-05, 1.101715042750584e-05, 1.101715042750584e-05, 1.101715042750584e-05, 1.101715042750584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.101715042750584e-05

Optimization complete. Final v2v error: 2.8513541221618652 mm

Highest mean error: 3.0925581455230713 mm for frame 105

Lowest mean error: 2.719090223312378 mm for frame 198

Saving results

Total time: 48.28341102600098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031467
Iteration 2/25 | Loss: 0.00299198
Iteration 3/25 | Loss: 0.00200131
Iteration 4/25 | Loss: 0.00207355
Iteration 5/25 | Loss: 0.00180202
Iteration 6/25 | Loss: 0.00173605
Iteration 7/25 | Loss: 0.00169269
Iteration 8/25 | Loss: 0.00164372
Iteration 9/25 | Loss: 0.00160914
Iteration 10/25 | Loss: 0.00157784
Iteration 11/25 | Loss: 0.00156787
Iteration 12/25 | Loss: 0.00154077
Iteration 13/25 | Loss: 0.00153627
Iteration 14/25 | Loss: 0.00154345
Iteration 15/25 | Loss: 0.00152543
Iteration 16/25 | Loss: 0.00152609
Iteration 17/25 | Loss: 0.00152016
Iteration 18/25 | Loss: 0.00151574
Iteration 19/25 | Loss: 0.00151481
Iteration 20/25 | Loss: 0.00151440
Iteration 21/25 | Loss: 0.00151656
Iteration 22/25 | Loss: 0.00151393
Iteration 23/25 | Loss: 0.00151215
Iteration 24/25 | Loss: 0.00151138
Iteration 25/25 | Loss: 0.00151124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32447278
Iteration 2/25 | Loss: 0.00382776
Iteration 3/25 | Loss: 0.00298514
Iteration 4/25 | Loss: 0.00298514
Iteration 5/25 | Loss: 0.00298514
Iteration 6/25 | Loss: 0.00298514
Iteration 7/25 | Loss: 0.00298514
Iteration 8/25 | Loss: 0.00298514
Iteration 9/25 | Loss: 0.00298514
Iteration 10/25 | Loss: 0.00298514
Iteration 11/25 | Loss: 0.00298514
Iteration 12/25 | Loss: 0.00298514
Iteration 13/25 | Loss: 0.00298514
Iteration 14/25 | Loss: 0.00298514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0029851370491087437, 0.0029851370491087437, 0.0029851370491087437, 0.0029851370491087437, 0.0029851370491087437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029851370491087437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298514
Iteration 2/1000 | Loss: 0.00222576
Iteration 3/1000 | Loss: 0.00277229
Iteration 4/1000 | Loss: 0.00113136
Iteration 5/1000 | Loss: 0.00288977
Iteration 6/1000 | Loss: 0.00262598
Iteration 7/1000 | Loss: 0.00068991
Iteration 8/1000 | Loss: 0.00208824
Iteration 9/1000 | Loss: 0.00264050
Iteration 10/1000 | Loss: 0.00055090
Iteration 11/1000 | Loss: 0.00172281
Iteration 12/1000 | Loss: 0.00053328
Iteration 13/1000 | Loss: 0.00143602
Iteration 14/1000 | Loss: 0.00129212
Iteration 15/1000 | Loss: 0.00136707
Iteration 16/1000 | Loss: 0.00101288
Iteration 17/1000 | Loss: 0.00034542
Iteration 18/1000 | Loss: 0.00047497
Iteration 19/1000 | Loss: 0.00226010
Iteration 20/1000 | Loss: 0.00041230
Iteration 21/1000 | Loss: 0.00084680
Iteration 22/1000 | Loss: 0.00016643
Iteration 23/1000 | Loss: 0.00014473
Iteration 24/1000 | Loss: 0.00012965
Iteration 25/1000 | Loss: 0.00061795
Iteration 26/1000 | Loss: 0.00687435
Iteration 27/1000 | Loss: 0.00498552
Iteration 28/1000 | Loss: 0.00655275
Iteration 29/1000 | Loss: 0.00788303
Iteration 30/1000 | Loss: 0.00551634
Iteration 31/1000 | Loss: 0.00483418
Iteration 32/1000 | Loss: 0.00111900
Iteration 33/1000 | Loss: 0.00384163
Iteration 34/1000 | Loss: 0.00098007
Iteration 35/1000 | Loss: 0.00082079
Iteration 36/1000 | Loss: 0.00053724
Iteration 37/1000 | Loss: 0.00135583
Iteration 38/1000 | Loss: 0.00086514
Iteration 39/1000 | Loss: 0.00153435
Iteration 40/1000 | Loss: 0.00119772
Iteration 41/1000 | Loss: 0.00151299
Iteration 42/1000 | Loss: 0.00107649
Iteration 43/1000 | Loss: 0.00170550
Iteration 44/1000 | Loss: 0.00093255
Iteration 45/1000 | Loss: 0.00088037
Iteration 46/1000 | Loss: 0.00068828
Iteration 47/1000 | Loss: 0.00162856
Iteration 48/1000 | Loss: 0.00033737
Iteration 49/1000 | Loss: 0.00061169
Iteration 50/1000 | Loss: 0.00078314
Iteration 51/1000 | Loss: 0.00065747
Iteration 52/1000 | Loss: 0.00051788
Iteration 53/1000 | Loss: 0.00057023
Iteration 54/1000 | Loss: 0.00020860
Iteration 55/1000 | Loss: 0.00022719
Iteration 56/1000 | Loss: 0.00023847
Iteration 57/1000 | Loss: 0.00026300
Iteration 58/1000 | Loss: 0.00072526
Iteration 59/1000 | Loss: 0.00053734
Iteration 60/1000 | Loss: 0.00046206
Iteration 61/1000 | Loss: 0.00027557
Iteration 62/1000 | Loss: 0.00024778
Iteration 63/1000 | Loss: 0.00029676
Iteration 64/1000 | Loss: 0.00021184
Iteration 65/1000 | Loss: 0.00028248
Iteration 66/1000 | Loss: 0.00027141
Iteration 67/1000 | Loss: 0.00027299
Iteration 68/1000 | Loss: 0.00023112
Iteration 69/1000 | Loss: 0.00003845
Iteration 70/1000 | Loss: 0.00016105
Iteration 71/1000 | Loss: 0.00005111
Iteration 72/1000 | Loss: 0.00036904
Iteration 73/1000 | Loss: 0.00004284
Iteration 74/1000 | Loss: 0.00046491
Iteration 75/1000 | Loss: 0.00016730
Iteration 76/1000 | Loss: 0.00037016
Iteration 77/1000 | Loss: 0.00019330
Iteration 78/1000 | Loss: 0.00003545
Iteration 79/1000 | Loss: 0.00004531
Iteration 80/1000 | Loss: 0.00003211
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00009014
Iteration 83/1000 | Loss: 0.00009470
Iteration 84/1000 | Loss: 0.00013332
Iteration 85/1000 | Loss: 0.00023838
Iteration 86/1000 | Loss: 0.00010413
Iteration 87/1000 | Loss: 0.00006864
Iteration 88/1000 | Loss: 0.00010563
Iteration 89/1000 | Loss: 0.00056232
Iteration 90/1000 | Loss: 0.00023615
Iteration 91/1000 | Loss: 0.00007740
Iteration 92/1000 | Loss: 0.00023761
Iteration 93/1000 | Loss: 0.00013438
Iteration 94/1000 | Loss: 0.00029759
Iteration 95/1000 | Loss: 0.00022812
Iteration 96/1000 | Loss: 0.00033186
Iteration 97/1000 | Loss: 0.00033698
Iteration 98/1000 | Loss: 0.00034478
Iteration 99/1000 | Loss: 0.00016150
Iteration 100/1000 | Loss: 0.00032744
Iteration 101/1000 | Loss: 0.00033998
Iteration 102/1000 | Loss: 0.00061854
Iteration 103/1000 | Loss: 0.00044636
Iteration 104/1000 | Loss: 0.00038150
Iteration 105/1000 | Loss: 0.00003353
Iteration 106/1000 | Loss: 0.00053901
Iteration 107/1000 | Loss: 0.00002896
Iteration 108/1000 | Loss: 0.00002676
Iteration 109/1000 | Loss: 0.00002568
Iteration 110/1000 | Loss: 0.00002422
Iteration 111/1000 | Loss: 0.00070982
Iteration 112/1000 | Loss: 0.00011160
Iteration 113/1000 | Loss: 0.00015770
Iteration 114/1000 | Loss: 0.00004018
Iteration 115/1000 | Loss: 0.00002622
Iteration 116/1000 | Loss: 0.00002820
Iteration 117/1000 | Loss: 0.00018669
Iteration 118/1000 | Loss: 0.00002284
Iteration 119/1000 | Loss: 0.00002096
Iteration 120/1000 | Loss: 0.00002752
Iteration 121/1000 | Loss: 0.00002091
Iteration 122/1000 | Loss: 0.00002006
Iteration 123/1000 | Loss: 0.00011665
Iteration 124/1000 | Loss: 0.00078206
Iteration 125/1000 | Loss: 0.00014367
Iteration 126/1000 | Loss: 0.00005202
Iteration 127/1000 | Loss: 0.00001997
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00002263
Iteration 130/1000 | Loss: 0.00004278
Iteration 131/1000 | Loss: 0.00023057
Iteration 132/1000 | Loss: 0.00018678
Iteration 133/1000 | Loss: 0.00043290
Iteration 134/1000 | Loss: 0.00076666
Iteration 135/1000 | Loss: 0.00014945
Iteration 136/1000 | Loss: 0.00040951
Iteration 137/1000 | Loss: 0.00006420
Iteration 138/1000 | Loss: 0.00023621
Iteration 139/1000 | Loss: 0.00026579
Iteration 140/1000 | Loss: 0.00026073
Iteration 141/1000 | Loss: 0.00024514
Iteration 142/1000 | Loss: 0.00041982
Iteration 143/1000 | Loss: 0.00019677
Iteration 144/1000 | Loss: 0.00018199
Iteration 145/1000 | Loss: 0.00028728
Iteration 146/1000 | Loss: 0.00039682
Iteration 147/1000 | Loss: 0.00028350
Iteration 148/1000 | Loss: 0.00020485
Iteration 149/1000 | Loss: 0.00008956
Iteration 150/1000 | Loss: 0.00024777
Iteration 151/1000 | Loss: 0.00033817
Iteration 152/1000 | Loss: 0.00024501
Iteration 153/1000 | Loss: 0.00018324
Iteration 154/1000 | Loss: 0.00002608
Iteration 155/1000 | Loss: 0.00030482
Iteration 156/1000 | Loss: 0.00002058
Iteration 157/1000 | Loss: 0.00014352
Iteration 158/1000 | Loss: 0.00009330
Iteration 159/1000 | Loss: 0.00002262
Iteration 160/1000 | Loss: 0.00001975
Iteration 161/1000 | Loss: 0.00001961
Iteration 162/1000 | Loss: 0.00005499
Iteration 163/1000 | Loss: 0.00032861
Iteration 164/1000 | Loss: 0.00005442
Iteration 165/1000 | Loss: 0.00003771
Iteration 166/1000 | Loss: 0.00006191
Iteration 167/1000 | Loss: 0.00026604
Iteration 168/1000 | Loss: 0.00004624
Iteration 169/1000 | Loss: 0.00024647
Iteration 170/1000 | Loss: 0.00006077
Iteration 171/1000 | Loss: 0.00028856
Iteration 172/1000 | Loss: 0.00003322
Iteration 173/1000 | Loss: 0.00027598
Iteration 174/1000 | Loss: 0.00046117
Iteration 175/1000 | Loss: 0.00035153
Iteration 176/1000 | Loss: 0.00053825
Iteration 177/1000 | Loss: 0.00004988
Iteration 178/1000 | Loss: 0.00025426
Iteration 179/1000 | Loss: 0.00019079
Iteration 180/1000 | Loss: 0.00046338
Iteration 181/1000 | Loss: 0.00036780
Iteration 182/1000 | Loss: 0.00028501
Iteration 183/1000 | Loss: 0.00029700
Iteration 184/1000 | Loss: 0.00024416
Iteration 185/1000 | Loss: 0.00045173
Iteration 186/1000 | Loss: 0.00016801
Iteration 187/1000 | Loss: 0.00004512
Iteration 188/1000 | Loss: 0.00018461
Iteration 189/1000 | Loss: 0.00027734
Iteration 190/1000 | Loss: 0.00003058
Iteration 191/1000 | Loss: 0.00010003
Iteration 192/1000 | Loss: 0.00022163
Iteration 193/1000 | Loss: 0.00013106
Iteration 194/1000 | Loss: 0.00036069
Iteration 195/1000 | Loss: 0.00025049
Iteration 196/1000 | Loss: 0.00029028
Iteration 197/1000 | Loss: 0.00009053
Iteration 198/1000 | Loss: 0.00028215
Iteration 199/1000 | Loss: 0.00002451
Iteration 200/1000 | Loss: 0.00002765
Iteration 201/1000 | Loss: 0.00001979
Iteration 202/1000 | Loss: 0.00001957
Iteration 203/1000 | Loss: 0.00004249
Iteration 204/1000 | Loss: 0.00001948
Iteration 205/1000 | Loss: 0.00001931
Iteration 206/1000 | Loss: 0.00002279
Iteration 207/1000 | Loss: 0.00001965
Iteration 208/1000 | Loss: 0.00001922
Iteration 209/1000 | Loss: 0.00001922
Iteration 210/1000 | Loss: 0.00001922
Iteration 211/1000 | Loss: 0.00001922
Iteration 212/1000 | Loss: 0.00001921
Iteration 213/1000 | Loss: 0.00001921
Iteration 214/1000 | Loss: 0.00001921
Iteration 215/1000 | Loss: 0.00001921
Iteration 216/1000 | Loss: 0.00001921
Iteration 217/1000 | Loss: 0.00001921
Iteration 218/1000 | Loss: 0.00001921
Iteration 219/1000 | Loss: 0.00001921
Iteration 220/1000 | Loss: 0.00001921
Iteration 221/1000 | Loss: 0.00001921
Iteration 222/1000 | Loss: 0.00001921
Iteration 223/1000 | Loss: 0.00001921
Iteration 224/1000 | Loss: 0.00001921
Iteration 225/1000 | Loss: 0.00001920
Iteration 226/1000 | Loss: 0.00001920
Iteration 227/1000 | Loss: 0.00001919
Iteration 228/1000 | Loss: 0.00001918
Iteration 229/1000 | Loss: 0.00001932
Iteration 230/1000 | Loss: 0.00001906
Iteration 231/1000 | Loss: 0.00001906
Iteration 232/1000 | Loss: 0.00001906
Iteration 233/1000 | Loss: 0.00001905
Iteration 234/1000 | Loss: 0.00001905
Iteration 235/1000 | Loss: 0.00001905
Iteration 236/1000 | Loss: 0.00001905
Iteration 237/1000 | Loss: 0.00001905
Iteration 238/1000 | Loss: 0.00001905
Iteration 239/1000 | Loss: 0.00001905
Iteration 240/1000 | Loss: 0.00001905
Iteration 241/1000 | Loss: 0.00001905
Iteration 242/1000 | Loss: 0.00001905
Iteration 243/1000 | Loss: 0.00001905
Iteration 244/1000 | Loss: 0.00001905
Iteration 245/1000 | Loss: 0.00001905
Iteration 246/1000 | Loss: 0.00001904
Iteration 247/1000 | Loss: 0.00001904
Iteration 248/1000 | Loss: 0.00001904
Iteration 249/1000 | Loss: 0.00001904
Iteration 250/1000 | Loss: 0.00001904
Iteration 251/1000 | Loss: 0.00001904
Iteration 252/1000 | Loss: 0.00001903
Iteration 253/1000 | Loss: 0.00001903
Iteration 254/1000 | Loss: 0.00001903
Iteration 255/1000 | Loss: 0.00001902
Iteration 256/1000 | Loss: 0.00001902
Iteration 257/1000 | Loss: 0.00001902
Iteration 258/1000 | Loss: 0.00001902
Iteration 259/1000 | Loss: 0.00001902
Iteration 260/1000 | Loss: 0.00001941
Iteration 261/1000 | Loss: 0.00001902
Iteration 262/1000 | Loss: 0.00001901
Iteration 263/1000 | Loss: 0.00001900
Iteration 264/1000 | Loss: 0.00001900
Iteration 265/1000 | Loss: 0.00001899
Iteration 266/1000 | Loss: 0.00001899
Iteration 267/1000 | Loss: 0.00001899
Iteration 268/1000 | Loss: 0.00001899
Iteration 269/1000 | Loss: 0.00001899
Iteration 270/1000 | Loss: 0.00001899
Iteration 271/1000 | Loss: 0.00001899
Iteration 272/1000 | Loss: 0.00001898
Iteration 273/1000 | Loss: 0.00001898
Iteration 274/1000 | Loss: 0.00001898
Iteration 275/1000 | Loss: 0.00001898
Iteration 276/1000 | Loss: 0.00001898
Iteration 277/1000 | Loss: 0.00001898
Iteration 278/1000 | Loss: 0.00001916
Iteration 279/1000 | Loss: 0.00001896
Iteration 280/1000 | Loss: 0.00001896
Iteration 281/1000 | Loss: 0.00001895
Iteration 282/1000 | Loss: 0.00001895
Iteration 283/1000 | Loss: 0.00001894
Iteration 284/1000 | Loss: 0.00001894
Iteration 285/1000 | Loss: 0.00001893
Iteration 286/1000 | Loss: 0.00001893
Iteration 287/1000 | Loss: 0.00001893
Iteration 288/1000 | Loss: 0.00001893
Iteration 289/1000 | Loss: 0.00001892
Iteration 290/1000 | Loss: 0.00001892
Iteration 291/1000 | Loss: 0.00001892
Iteration 292/1000 | Loss: 0.00001891
Iteration 293/1000 | Loss: 0.00001891
Iteration 294/1000 | Loss: 0.00001891
Iteration 295/1000 | Loss: 0.00001891
Iteration 296/1000 | Loss: 0.00001994
Iteration 297/1000 | Loss: 0.00001887
Iteration 298/1000 | Loss: 0.00001887
Iteration 299/1000 | Loss: 0.00001887
Iteration 300/1000 | Loss: 0.00001887
Iteration 301/1000 | Loss: 0.00001886
Iteration 302/1000 | Loss: 0.00001886
Iteration 303/1000 | Loss: 0.00001886
Iteration 304/1000 | Loss: 0.00001886
Iteration 305/1000 | Loss: 0.00001886
Iteration 306/1000 | Loss: 0.00001886
Iteration 307/1000 | Loss: 0.00001886
Iteration 308/1000 | Loss: 0.00001886
Iteration 309/1000 | Loss: 0.00001886
Iteration 310/1000 | Loss: 0.00001886
Iteration 311/1000 | Loss: 0.00001886
Iteration 312/1000 | Loss: 0.00001886
Iteration 313/1000 | Loss: 0.00001886
Iteration 314/1000 | Loss: 0.00001886
Iteration 315/1000 | Loss: 0.00001886
Iteration 316/1000 | Loss: 0.00001886
Iteration 317/1000 | Loss: 0.00001886
Iteration 318/1000 | Loss: 0.00001886
Iteration 319/1000 | Loss: 0.00001886
Iteration 320/1000 | Loss: 0.00001886
Iteration 321/1000 | Loss: 0.00001886
Iteration 322/1000 | Loss: 0.00001886
Iteration 323/1000 | Loss: 0.00001886
Iteration 324/1000 | Loss: 0.00001886
Iteration 325/1000 | Loss: 0.00001886
Iteration 326/1000 | Loss: 0.00001886
Iteration 327/1000 | Loss: 0.00001886
Iteration 328/1000 | Loss: 0.00001886
Iteration 329/1000 | Loss: 0.00001886
Iteration 330/1000 | Loss: 0.00001886
Iteration 331/1000 | Loss: 0.00001886
Iteration 332/1000 | Loss: 0.00001886
Iteration 333/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [1.88606463780161e-05, 1.88606463780161e-05, 1.88606463780161e-05, 1.88606463780161e-05, 1.88606463780161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.88606463780161e-05

Optimization complete. Final v2v error: 3.3908965587615967 mm

Highest mean error: 13.560422897338867 mm for frame 142

Lowest mean error: 2.846477746963501 mm for frame 120

Saving results

Total time: 377.9280927181244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438136
Iteration 2/25 | Loss: 0.00146068
Iteration 3/25 | Loss: 0.00132871
Iteration 4/25 | Loss: 0.00131316
Iteration 5/25 | Loss: 0.00130948
Iteration 6/25 | Loss: 0.00130856
Iteration 7/25 | Loss: 0.00130856
Iteration 8/25 | Loss: 0.00130856
Iteration 9/25 | Loss: 0.00130856
Iteration 10/25 | Loss: 0.00130856
Iteration 11/25 | Loss: 0.00130856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013085631653666496, 0.0013085631653666496, 0.0013085631653666496, 0.0013085631653666496, 0.0013085631653666496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013085631653666496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97266293
Iteration 2/25 | Loss: 0.00088042
Iteration 3/25 | Loss: 0.00088039
Iteration 4/25 | Loss: 0.00088039
Iteration 5/25 | Loss: 0.00088039
Iteration 6/25 | Loss: 0.00088038
Iteration 7/25 | Loss: 0.00088038
Iteration 8/25 | Loss: 0.00088038
Iteration 9/25 | Loss: 0.00088038
Iteration 10/25 | Loss: 0.00088038
Iteration 11/25 | Loss: 0.00088038
Iteration 12/25 | Loss: 0.00088038
Iteration 13/25 | Loss: 0.00088038
Iteration 14/25 | Loss: 0.00088038
Iteration 15/25 | Loss: 0.00088038
Iteration 16/25 | Loss: 0.00088038
Iteration 17/25 | Loss: 0.00088038
Iteration 18/25 | Loss: 0.00088038
Iteration 19/25 | Loss: 0.00088038
Iteration 20/25 | Loss: 0.00088038
Iteration 21/25 | Loss: 0.00088038
Iteration 22/25 | Loss: 0.00088038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008803830714896321, 0.0008803830714896321, 0.0008803830714896321, 0.0008803830714896321, 0.0008803830714896321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008803830714896321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088038
Iteration 2/1000 | Loss: 0.00004179
Iteration 3/1000 | Loss: 0.00002903
Iteration 4/1000 | Loss: 0.00002673
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002502
Iteration 7/1000 | Loss: 0.00002419
Iteration 8/1000 | Loss: 0.00002370
Iteration 9/1000 | Loss: 0.00002334
Iteration 10/1000 | Loss: 0.00002299
Iteration 11/1000 | Loss: 0.00002273
Iteration 12/1000 | Loss: 0.00002255
Iteration 13/1000 | Loss: 0.00002240
Iteration 14/1000 | Loss: 0.00002236
Iteration 15/1000 | Loss: 0.00002228
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002221
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002213
Iteration 20/1000 | Loss: 0.00002210
Iteration 21/1000 | Loss: 0.00002203
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002202
Iteration 24/1000 | Loss: 0.00002202
Iteration 25/1000 | Loss: 0.00002201
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002200
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002199
Iteration 32/1000 | Loss: 0.00002199
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002199
Iteration 36/1000 | Loss: 0.00002199
Iteration 37/1000 | Loss: 0.00002199
Iteration 38/1000 | Loss: 0.00002199
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002198
Iteration 44/1000 | Loss: 0.00002198
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002197
Iteration 48/1000 | Loss: 0.00002197
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002195
Iteration 53/1000 | Loss: 0.00002195
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002194
Iteration 58/1000 | Loss: 0.00002194
Iteration 59/1000 | Loss: 0.00002194
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002193
Iteration 63/1000 | Loss: 0.00002192
Iteration 64/1000 | Loss: 0.00002192
Iteration 65/1000 | Loss: 0.00002192
Iteration 66/1000 | Loss: 0.00002192
Iteration 67/1000 | Loss: 0.00002191
Iteration 68/1000 | Loss: 0.00002191
Iteration 69/1000 | Loss: 0.00002191
Iteration 70/1000 | Loss: 0.00002191
Iteration 71/1000 | Loss: 0.00002191
Iteration 72/1000 | Loss: 0.00002191
Iteration 73/1000 | Loss: 0.00002190
Iteration 74/1000 | Loss: 0.00002190
Iteration 75/1000 | Loss: 0.00002190
Iteration 76/1000 | Loss: 0.00002190
Iteration 77/1000 | Loss: 0.00002190
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002190
Iteration 80/1000 | Loss: 0.00002189
Iteration 81/1000 | Loss: 0.00002189
Iteration 82/1000 | Loss: 0.00002189
Iteration 83/1000 | Loss: 0.00002189
Iteration 84/1000 | Loss: 0.00002189
Iteration 85/1000 | Loss: 0.00002189
Iteration 86/1000 | Loss: 0.00002189
Iteration 87/1000 | Loss: 0.00002189
Iteration 88/1000 | Loss: 0.00002188
Iteration 89/1000 | Loss: 0.00002188
Iteration 90/1000 | Loss: 0.00002188
Iteration 91/1000 | Loss: 0.00002188
Iteration 92/1000 | Loss: 0.00002187
Iteration 93/1000 | Loss: 0.00002187
Iteration 94/1000 | Loss: 0.00002187
Iteration 95/1000 | Loss: 0.00002187
Iteration 96/1000 | Loss: 0.00002187
Iteration 97/1000 | Loss: 0.00002187
Iteration 98/1000 | Loss: 0.00002186
Iteration 99/1000 | Loss: 0.00002186
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002186
Iteration 102/1000 | Loss: 0.00002186
Iteration 103/1000 | Loss: 0.00002186
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002185
Iteration 110/1000 | Loss: 0.00002185
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002184
Iteration 115/1000 | Loss: 0.00002184
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002184
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002184
Iteration 121/1000 | Loss: 0.00002184
Iteration 122/1000 | Loss: 0.00002184
Iteration 123/1000 | Loss: 0.00002184
Iteration 124/1000 | Loss: 0.00002184
Iteration 125/1000 | Loss: 0.00002184
Iteration 126/1000 | Loss: 0.00002184
Iteration 127/1000 | Loss: 0.00002184
Iteration 128/1000 | Loss: 0.00002184
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002184
Iteration 132/1000 | Loss: 0.00002184
Iteration 133/1000 | Loss: 0.00002184
Iteration 134/1000 | Loss: 0.00002184
Iteration 135/1000 | Loss: 0.00002184
Iteration 136/1000 | Loss: 0.00002183
Iteration 137/1000 | Loss: 0.00002183
Iteration 138/1000 | Loss: 0.00002183
Iteration 139/1000 | Loss: 0.00002183
Iteration 140/1000 | Loss: 0.00002183
Iteration 141/1000 | Loss: 0.00002183
Iteration 142/1000 | Loss: 0.00002183
Iteration 143/1000 | Loss: 0.00002183
Iteration 144/1000 | Loss: 0.00002183
Iteration 145/1000 | Loss: 0.00002183
Iteration 146/1000 | Loss: 0.00002183
Iteration 147/1000 | Loss: 0.00002183
Iteration 148/1000 | Loss: 0.00002183
Iteration 149/1000 | Loss: 0.00002183
Iteration 150/1000 | Loss: 0.00002183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.1832867787452415e-05, 2.1832867787452415e-05, 2.1832867787452415e-05, 2.1832867787452415e-05, 2.1832867787452415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1832867787452415e-05

Optimization complete. Final v2v error: 3.966672897338867 mm

Highest mean error: 4.440761566162109 mm for frame 60

Lowest mean error: 3.496795892715454 mm for frame 2

Saving results

Total time: 38.2688422203064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790454
Iteration 2/25 | Loss: 0.00179960
Iteration 3/25 | Loss: 0.00159200
Iteration 4/25 | Loss: 0.00158399
Iteration 5/25 | Loss: 0.00158128
Iteration 6/25 | Loss: 0.00158045
Iteration 7/25 | Loss: 0.00158045
Iteration 8/25 | Loss: 0.00158045
Iteration 9/25 | Loss: 0.00158045
Iteration 10/25 | Loss: 0.00158045
Iteration 11/25 | Loss: 0.00158045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015804520808160305, 0.0015804520808160305, 0.0015804520808160305, 0.0015804520808160305, 0.0015804520808160305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015804520808160305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60382515
Iteration 2/25 | Loss: 0.00121503
Iteration 3/25 | Loss: 0.00121503
Iteration 4/25 | Loss: 0.00121503
Iteration 5/25 | Loss: 0.00121503
Iteration 6/25 | Loss: 0.00121503
Iteration 7/25 | Loss: 0.00121503
Iteration 8/25 | Loss: 0.00121503
Iteration 9/25 | Loss: 0.00121503
Iteration 10/25 | Loss: 0.00121503
Iteration 11/25 | Loss: 0.00121503
Iteration 12/25 | Loss: 0.00121503
Iteration 13/25 | Loss: 0.00121503
Iteration 14/25 | Loss: 0.00121503
Iteration 15/25 | Loss: 0.00121503
Iteration 16/25 | Loss: 0.00121503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012150283437222242, 0.0012150283437222242, 0.0012150283437222242, 0.0012150283437222242, 0.0012150283437222242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012150283437222242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121503
Iteration 2/1000 | Loss: 0.00009458
Iteration 3/1000 | Loss: 0.00006083
Iteration 4/1000 | Loss: 0.00005094
Iteration 5/1000 | Loss: 0.00004777
Iteration 6/1000 | Loss: 0.00004617
Iteration 7/1000 | Loss: 0.00004529
Iteration 8/1000 | Loss: 0.00004437
Iteration 9/1000 | Loss: 0.00004348
Iteration 10/1000 | Loss: 0.00004275
Iteration 11/1000 | Loss: 0.00004209
Iteration 12/1000 | Loss: 0.00004162
Iteration 13/1000 | Loss: 0.00004132
Iteration 14/1000 | Loss: 0.00004081
Iteration 15/1000 | Loss: 0.00004048
Iteration 16/1000 | Loss: 0.00004017
Iteration 17/1000 | Loss: 0.00003995
Iteration 18/1000 | Loss: 0.00003980
Iteration 19/1000 | Loss: 0.00003965
Iteration 20/1000 | Loss: 0.00003961
Iteration 21/1000 | Loss: 0.00003958
Iteration 22/1000 | Loss: 0.00003955
Iteration 23/1000 | Loss: 0.00003951
Iteration 24/1000 | Loss: 0.00003951
Iteration 25/1000 | Loss: 0.00003951
Iteration 26/1000 | Loss: 0.00003951
Iteration 27/1000 | Loss: 0.00003951
Iteration 28/1000 | Loss: 0.00003951
Iteration 29/1000 | Loss: 0.00003951
Iteration 30/1000 | Loss: 0.00003951
Iteration 31/1000 | Loss: 0.00003951
Iteration 32/1000 | Loss: 0.00003950
Iteration 33/1000 | Loss: 0.00003950
Iteration 34/1000 | Loss: 0.00003950
Iteration 35/1000 | Loss: 0.00003950
Iteration 36/1000 | Loss: 0.00003950
Iteration 37/1000 | Loss: 0.00003950
Iteration 38/1000 | Loss: 0.00003950
Iteration 39/1000 | Loss: 0.00003950
Iteration 40/1000 | Loss: 0.00003950
Iteration 41/1000 | Loss: 0.00003944
Iteration 42/1000 | Loss: 0.00003940
Iteration 43/1000 | Loss: 0.00003939
Iteration 44/1000 | Loss: 0.00003938
Iteration 45/1000 | Loss: 0.00003936
Iteration 46/1000 | Loss: 0.00003936
Iteration 47/1000 | Loss: 0.00003936
Iteration 48/1000 | Loss: 0.00003936
Iteration 49/1000 | Loss: 0.00003936
Iteration 50/1000 | Loss: 0.00003936
Iteration 51/1000 | Loss: 0.00003936
Iteration 52/1000 | Loss: 0.00003935
Iteration 53/1000 | Loss: 0.00003935
Iteration 54/1000 | Loss: 0.00003934
Iteration 55/1000 | Loss: 0.00003934
Iteration 56/1000 | Loss: 0.00003932
Iteration 57/1000 | Loss: 0.00003930
Iteration 58/1000 | Loss: 0.00003930
Iteration 59/1000 | Loss: 0.00003929
Iteration 60/1000 | Loss: 0.00003929
Iteration 61/1000 | Loss: 0.00003928
Iteration 62/1000 | Loss: 0.00003928
Iteration 63/1000 | Loss: 0.00003926
Iteration 64/1000 | Loss: 0.00003925
Iteration 65/1000 | Loss: 0.00003925
Iteration 66/1000 | Loss: 0.00003924
Iteration 67/1000 | Loss: 0.00003924
Iteration 68/1000 | Loss: 0.00003924
Iteration 69/1000 | Loss: 0.00003924
Iteration 70/1000 | Loss: 0.00003924
Iteration 71/1000 | Loss: 0.00003924
Iteration 72/1000 | Loss: 0.00003924
Iteration 73/1000 | Loss: 0.00003924
Iteration 74/1000 | Loss: 0.00003924
Iteration 75/1000 | Loss: 0.00003924
Iteration 76/1000 | Loss: 0.00003923
Iteration 77/1000 | Loss: 0.00003921
Iteration 78/1000 | Loss: 0.00003921
Iteration 79/1000 | Loss: 0.00003921
Iteration 80/1000 | Loss: 0.00003920
Iteration 81/1000 | Loss: 0.00003920
Iteration 82/1000 | Loss: 0.00003920
Iteration 83/1000 | Loss: 0.00003920
Iteration 84/1000 | Loss: 0.00003920
Iteration 85/1000 | Loss: 0.00003920
Iteration 86/1000 | Loss: 0.00003920
Iteration 87/1000 | Loss: 0.00003920
Iteration 88/1000 | Loss: 0.00003920
Iteration 89/1000 | Loss: 0.00003920
Iteration 90/1000 | Loss: 0.00003919
Iteration 91/1000 | Loss: 0.00003919
Iteration 92/1000 | Loss: 0.00003919
Iteration 93/1000 | Loss: 0.00003919
Iteration 94/1000 | Loss: 0.00003919
Iteration 95/1000 | Loss: 0.00003918
Iteration 96/1000 | Loss: 0.00003918
Iteration 97/1000 | Loss: 0.00003917
Iteration 98/1000 | Loss: 0.00003917
Iteration 99/1000 | Loss: 0.00003917
Iteration 100/1000 | Loss: 0.00003917
Iteration 101/1000 | Loss: 0.00003917
Iteration 102/1000 | Loss: 0.00003917
Iteration 103/1000 | Loss: 0.00003917
Iteration 104/1000 | Loss: 0.00003917
Iteration 105/1000 | Loss: 0.00003916
Iteration 106/1000 | Loss: 0.00003916
Iteration 107/1000 | Loss: 0.00003916
Iteration 108/1000 | Loss: 0.00003916
Iteration 109/1000 | Loss: 0.00003916
Iteration 110/1000 | Loss: 0.00003916
Iteration 111/1000 | Loss: 0.00003915
Iteration 112/1000 | Loss: 0.00003915
Iteration 113/1000 | Loss: 0.00003915
Iteration 114/1000 | Loss: 0.00003915
Iteration 115/1000 | Loss: 0.00003915
Iteration 116/1000 | Loss: 0.00003915
Iteration 117/1000 | Loss: 0.00003915
Iteration 118/1000 | Loss: 0.00003915
Iteration 119/1000 | Loss: 0.00003914
Iteration 120/1000 | Loss: 0.00003914
Iteration 121/1000 | Loss: 0.00003914
Iteration 122/1000 | Loss: 0.00003914
Iteration 123/1000 | Loss: 0.00003914
Iteration 124/1000 | Loss: 0.00003914
Iteration 125/1000 | Loss: 0.00003914
Iteration 126/1000 | Loss: 0.00003914
Iteration 127/1000 | Loss: 0.00003913
Iteration 128/1000 | Loss: 0.00003913
Iteration 129/1000 | Loss: 0.00003912
Iteration 130/1000 | Loss: 0.00003912
Iteration 131/1000 | Loss: 0.00003912
Iteration 132/1000 | Loss: 0.00003912
Iteration 133/1000 | Loss: 0.00003912
Iteration 134/1000 | Loss: 0.00003911
Iteration 135/1000 | Loss: 0.00003911
Iteration 136/1000 | Loss: 0.00003911
Iteration 137/1000 | Loss: 0.00003911
Iteration 138/1000 | Loss: 0.00003910
Iteration 139/1000 | Loss: 0.00003910
Iteration 140/1000 | Loss: 0.00003910
Iteration 141/1000 | Loss: 0.00003910
Iteration 142/1000 | Loss: 0.00003910
Iteration 143/1000 | Loss: 0.00003909
Iteration 144/1000 | Loss: 0.00003909
Iteration 145/1000 | Loss: 0.00003909
Iteration 146/1000 | Loss: 0.00003909
Iteration 147/1000 | Loss: 0.00003909
Iteration 148/1000 | Loss: 0.00003908
Iteration 149/1000 | Loss: 0.00003908
Iteration 150/1000 | Loss: 0.00003908
Iteration 151/1000 | Loss: 0.00003908
Iteration 152/1000 | Loss: 0.00003907
Iteration 153/1000 | Loss: 0.00003907
Iteration 154/1000 | Loss: 0.00003907
Iteration 155/1000 | Loss: 0.00003906
Iteration 156/1000 | Loss: 0.00003906
Iteration 157/1000 | Loss: 0.00003906
Iteration 158/1000 | Loss: 0.00003906
Iteration 159/1000 | Loss: 0.00003906
Iteration 160/1000 | Loss: 0.00003906
Iteration 161/1000 | Loss: 0.00003905
Iteration 162/1000 | Loss: 0.00003905
Iteration 163/1000 | Loss: 0.00003905
Iteration 164/1000 | Loss: 0.00003904
Iteration 165/1000 | Loss: 0.00003904
Iteration 166/1000 | Loss: 0.00003904
Iteration 167/1000 | Loss: 0.00003904
Iteration 168/1000 | Loss: 0.00003904
Iteration 169/1000 | Loss: 0.00003903
Iteration 170/1000 | Loss: 0.00003903
Iteration 171/1000 | Loss: 0.00003903
Iteration 172/1000 | Loss: 0.00003903
Iteration 173/1000 | Loss: 0.00003902
Iteration 174/1000 | Loss: 0.00003902
Iteration 175/1000 | Loss: 0.00003902
Iteration 176/1000 | Loss: 0.00003902
Iteration 177/1000 | Loss: 0.00003902
Iteration 178/1000 | Loss: 0.00003902
Iteration 179/1000 | Loss: 0.00003901
Iteration 180/1000 | Loss: 0.00003901
Iteration 181/1000 | Loss: 0.00003901
Iteration 182/1000 | Loss: 0.00003901
Iteration 183/1000 | Loss: 0.00003901
Iteration 184/1000 | Loss: 0.00003900
Iteration 185/1000 | Loss: 0.00003900
Iteration 186/1000 | Loss: 0.00003900
Iteration 187/1000 | Loss: 0.00003900
Iteration 188/1000 | Loss: 0.00003900
Iteration 189/1000 | Loss: 0.00003900
Iteration 190/1000 | Loss: 0.00003900
Iteration 191/1000 | Loss: 0.00003899
Iteration 192/1000 | Loss: 0.00003899
Iteration 193/1000 | Loss: 0.00003899
Iteration 194/1000 | Loss: 0.00003899
Iteration 195/1000 | Loss: 0.00003899
Iteration 196/1000 | Loss: 0.00003899
Iteration 197/1000 | Loss: 0.00003899
Iteration 198/1000 | Loss: 0.00003899
Iteration 199/1000 | Loss: 0.00003899
Iteration 200/1000 | Loss: 0.00003898
Iteration 201/1000 | Loss: 0.00003898
Iteration 202/1000 | Loss: 0.00003898
Iteration 203/1000 | Loss: 0.00003898
Iteration 204/1000 | Loss: 0.00003898
Iteration 205/1000 | Loss: 0.00003898
Iteration 206/1000 | Loss: 0.00003897
Iteration 207/1000 | Loss: 0.00003897
Iteration 208/1000 | Loss: 0.00003897
Iteration 209/1000 | Loss: 0.00003897
Iteration 210/1000 | Loss: 0.00003897
Iteration 211/1000 | Loss: 0.00003897
Iteration 212/1000 | Loss: 0.00003897
Iteration 213/1000 | Loss: 0.00003897
Iteration 214/1000 | Loss: 0.00003897
Iteration 215/1000 | Loss: 0.00003897
Iteration 216/1000 | Loss: 0.00003897
Iteration 217/1000 | Loss: 0.00003897
Iteration 218/1000 | Loss: 0.00003897
Iteration 219/1000 | Loss: 0.00003897
Iteration 220/1000 | Loss: 0.00003897
Iteration 221/1000 | Loss: 0.00003897
Iteration 222/1000 | Loss: 0.00003897
Iteration 223/1000 | Loss: 0.00003897
Iteration 224/1000 | Loss: 0.00003897
Iteration 225/1000 | Loss: 0.00003897
Iteration 226/1000 | Loss: 0.00003897
Iteration 227/1000 | Loss: 0.00003897
Iteration 228/1000 | Loss: 0.00003897
Iteration 229/1000 | Loss: 0.00003897
Iteration 230/1000 | Loss: 0.00003897
Iteration 231/1000 | Loss: 0.00003897
Iteration 232/1000 | Loss: 0.00003897
Iteration 233/1000 | Loss: 0.00003897
Iteration 234/1000 | Loss: 0.00003897
Iteration 235/1000 | Loss: 0.00003897
Iteration 236/1000 | Loss: 0.00003897
Iteration 237/1000 | Loss: 0.00003897
Iteration 238/1000 | Loss: 0.00003897
Iteration 239/1000 | Loss: 0.00003897
Iteration 240/1000 | Loss: 0.00003897
Iteration 241/1000 | Loss: 0.00003897
Iteration 242/1000 | Loss: 0.00003897
Iteration 243/1000 | Loss: 0.00003897
Iteration 244/1000 | Loss: 0.00003897
Iteration 245/1000 | Loss: 0.00003897
Iteration 246/1000 | Loss: 0.00003897
Iteration 247/1000 | Loss: 0.00003897
Iteration 248/1000 | Loss: 0.00003897
Iteration 249/1000 | Loss: 0.00003897
Iteration 250/1000 | Loss: 0.00003897
Iteration 251/1000 | Loss: 0.00003897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [3.8967144064372405e-05, 3.8967144064372405e-05, 3.8967144064372405e-05, 3.8967144064372405e-05, 3.8967144064372405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8967144064372405e-05

Optimization complete. Final v2v error: 5.059504508972168 mm

Highest mean error: 5.571484088897705 mm for frame 32

Lowest mean error: 4.017144203186035 mm for frame 11

Saving results

Total time: 54.07306885719299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412564
Iteration 2/25 | Loss: 0.00133119
Iteration 3/25 | Loss: 0.00123585
Iteration 4/25 | Loss: 0.00122688
Iteration 5/25 | Loss: 0.00122445
Iteration 6/25 | Loss: 0.00122387
Iteration 7/25 | Loss: 0.00122387
Iteration 8/25 | Loss: 0.00122387
Iteration 9/25 | Loss: 0.00122387
Iteration 10/25 | Loss: 0.00122387
Iteration 11/25 | Loss: 0.00122387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001223867293447256, 0.001223867293447256, 0.001223867293447256, 0.001223867293447256, 0.001223867293447256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001223867293447256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51870418
Iteration 2/25 | Loss: 0.00095535
Iteration 3/25 | Loss: 0.00095535
Iteration 4/25 | Loss: 0.00095535
Iteration 5/25 | Loss: 0.00095535
Iteration 6/25 | Loss: 0.00095535
Iteration 7/25 | Loss: 0.00095535
Iteration 8/25 | Loss: 0.00095535
Iteration 9/25 | Loss: 0.00095535
Iteration 10/25 | Loss: 0.00095535
Iteration 11/25 | Loss: 0.00095535
Iteration 12/25 | Loss: 0.00095535
Iteration 13/25 | Loss: 0.00095535
Iteration 14/25 | Loss: 0.00095535
Iteration 15/25 | Loss: 0.00095534
Iteration 16/25 | Loss: 0.00095534
Iteration 17/25 | Loss: 0.00095534
Iteration 18/25 | Loss: 0.00095534
Iteration 19/25 | Loss: 0.00095534
Iteration 20/25 | Loss: 0.00095534
Iteration 21/25 | Loss: 0.00095534
Iteration 22/25 | Loss: 0.00095534
Iteration 23/25 | Loss: 0.00095534
Iteration 24/25 | Loss: 0.00095534
Iteration 25/25 | Loss: 0.00095534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095534
Iteration 2/1000 | Loss: 0.00002632
Iteration 3/1000 | Loss: 0.00001854
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001390
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001271
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001266
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001260
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001253
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001247
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001242
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001222
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001221
Iteration 96/1000 | Loss: 0.00001221
Iteration 97/1000 | Loss: 0.00001221
Iteration 98/1000 | Loss: 0.00001221
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001218
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001218
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001218
Iteration 122/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.2175594747532159e-05, 1.2175594747532159e-05, 1.2175594747532159e-05, 1.2175594747532159e-05, 1.2175594747532159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2175594747532159e-05

Optimization complete. Final v2v error: 2.9435667991638184 mm

Highest mean error: 4.455745697021484 mm for frame 168

Lowest mean error: 2.6868717670440674 mm for frame 144

Saving results

Total time: 39.22836494445801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039289
Iteration 2/25 | Loss: 0.00227351
Iteration 3/25 | Loss: 0.00161958
Iteration 4/25 | Loss: 0.00152631
Iteration 5/25 | Loss: 0.00135184
Iteration 6/25 | Loss: 0.00130986
Iteration 7/25 | Loss: 0.00125994
Iteration 8/25 | Loss: 0.00126622
Iteration 9/25 | Loss: 0.00123858
Iteration 10/25 | Loss: 0.00123242
Iteration 11/25 | Loss: 0.00123271
Iteration 12/25 | Loss: 0.00122234
Iteration 13/25 | Loss: 0.00122227
Iteration 14/25 | Loss: 0.00122227
Iteration 15/25 | Loss: 0.00122227
Iteration 16/25 | Loss: 0.00122226
Iteration 17/25 | Loss: 0.00122226
Iteration 18/25 | Loss: 0.00122226
Iteration 19/25 | Loss: 0.00122226
Iteration 20/25 | Loss: 0.00122226
Iteration 21/25 | Loss: 0.00122226
Iteration 22/25 | Loss: 0.00122226
Iteration 23/25 | Loss: 0.00122226
Iteration 24/25 | Loss: 0.00122226
Iteration 25/25 | Loss: 0.00122226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32600915
Iteration 2/25 | Loss: 0.00101839
Iteration 3/25 | Loss: 0.00101839
Iteration 4/25 | Loss: 0.00101839
Iteration 5/25 | Loss: 0.00101839
Iteration 6/25 | Loss: 0.00101839
Iteration 7/25 | Loss: 0.00101839
Iteration 8/25 | Loss: 0.00101839
Iteration 9/25 | Loss: 0.00101839
Iteration 10/25 | Loss: 0.00101839
Iteration 11/25 | Loss: 0.00101839
Iteration 12/25 | Loss: 0.00101839
Iteration 13/25 | Loss: 0.00101839
Iteration 14/25 | Loss: 0.00101839
Iteration 15/25 | Loss: 0.00101839
Iteration 16/25 | Loss: 0.00101839
Iteration 17/25 | Loss: 0.00101839
Iteration 18/25 | Loss: 0.00101839
Iteration 19/25 | Loss: 0.00101839
Iteration 20/25 | Loss: 0.00101839
Iteration 21/25 | Loss: 0.00101839
Iteration 22/25 | Loss: 0.00101839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010183864505961537, 0.0010183864505961537, 0.0010183864505961537, 0.0010183864505961537, 0.0010183864505961537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010183864505961537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101839
Iteration 2/1000 | Loss: 0.00002621
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00001658
Iteration 7/1000 | Loss: 0.00001603
Iteration 8/1000 | Loss: 0.00001580
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001422
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001400
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001382
Iteration 46/1000 | Loss: 0.00001382
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001373
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001373
Iteration 72/1000 | Loss: 0.00001373
Iteration 73/1000 | Loss: 0.00001373
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001371
Iteration 82/1000 | Loss: 0.00001371
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001371
Iteration 85/1000 | Loss: 0.00001371
Iteration 86/1000 | Loss: 0.00001371
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001371
Iteration 92/1000 | Loss: 0.00001371
Iteration 93/1000 | Loss: 0.00001371
Iteration 94/1000 | Loss: 0.00001371
Iteration 95/1000 | Loss: 0.00001371
Iteration 96/1000 | Loss: 0.00001371
Iteration 97/1000 | Loss: 0.00001371
Iteration 98/1000 | Loss: 0.00001371
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001371
Iteration 108/1000 | Loss: 0.00001371
Iteration 109/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.370963582303375e-05, 1.370963582303375e-05, 1.370963582303375e-05, 1.370963582303375e-05, 1.370963582303375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.370963582303375e-05

Optimization complete. Final v2v error: 3.1657755374908447 mm

Highest mean error: 3.2913997173309326 mm for frame 65

Lowest mean error: 3.0444514751434326 mm for frame 116

Saving results

Total time: 50.13271141052246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964975
Iteration 2/25 | Loss: 0.00163131
Iteration 3/25 | Loss: 0.00146704
Iteration 4/25 | Loss: 0.00138953
Iteration 5/25 | Loss: 0.00138735
Iteration 6/25 | Loss: 0.00143285
Iteration 7/25 | Loss: 0.00136977
Iteration 8/25 | Loss: 0.00135124
Iteration 9/25 | Loss: 0.00129790
Iteration 10/25 | Loss: 0.00132398
Iteration 11/25 | Loss: 0.00131336
Iteration 12/25 | Loss: 0.00130870
Iteration 13/25 | Loss: 0.00130740
Iteration 14/25 | Loss: 0.00130609
Iteration 15/25 | Loss: 0.00127328
Iteration 16/25 | Loss: 0.00126755
Iteration 17/25 | Loss: 0.00126787
Iteration 18/25 | Loss: 0.00126760
Iteration 19/25 | Loss: 0.00126789
Iteration 20/25 | Loss: 0.00126696
Iteration 21/25 | Loss: 0.00126738
Iteration 22/25 | Loss: 0.00126744
Iteration 23/25 | Loss: 0.00126702
Iteration 24/25 | Loss: 0.00126689
Iteration 25/25 | Loss: 0.00126815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98104119
Iteration 2/25 | Loss: 0.00109871
Iteration 3/25 | Loss: 0.00109870
Iteration 4/25 | Loss: 0.00109870
Iteration 5/25 | Loss: 0.00109870
Iteration 6/25 | Loss: 0.00109870
Iteration 7/25 | Loss: 0.00109870
Iteration 8/25 | Loss: 0.00109870
Iteration 9/25 | Loss: 0.00109870
Iteration 10/25 | Loss: 0.00109870
Iteration 11/25 | Loss: 0.00109870
Iteration 12/25 | Loss: 0.00109870
Iteration 13/25 | Loss: 0.00109870
Iteration 14/25 | Loss: 0.00109870
Iteration 15/25 | Loss: 0.00109870
Iteration 16/25 | Loss: 0.00109870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001098700682632625, 0.001098700682632625, 0.001098700682632625, 0.001098700682632625, 0.001098700682632625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098700682632625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109870
Iteration 2/1000 | Loss: 0.00005797
Iteration 3/1000 | Loss: 0.00003986
Iteration 4/1000 | Loss: 0.00003639
Iteration 5/1000 | Loss: 0.00004308
Iteration 6/1000 | Loss: 0.00003960
Iteration 7/1000 | Loss: 0.00003895
Iteration 8/1000 | Loss: 0.00003910
Iteration 9/1000 | Loss: 0.00003026
Iteration 10/1000 | Loss: 0.00003220
Iteration 11/1000 | Loss: 0.00003678
Iteration 12/1000 | Loss: 0.00003849
Iteration 13/1000 | Loss: 0.00003814
Iteration 14/1000 | Loss: 0.00004309
Iteration 15/1000 | Loss: 0.00004779
Iteration 16/1000 | Loss: 0.00003060
Iteration 17/1000 | Loss: 0.00004143
Iteration 18/1000 | Loss: 0.00002593
Iteration 19/1000 | Loss: 0.00003446
Iteration 20/1000 | Loss: 0.00003494
Iteration 21/1000 | Loss: 0.00003232
Iteration 22/1000 | Loss: 0.00003923
Iteration 23/1000 | Loss: 0.00003978
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002306
Iteration 26/1000 | Loss: 0.00002279
Iteration 27/1000 | Loss: 0.00003705
Iteration 28/1000 | Loss: 0.00003005
Iteration 29/1000 | Loss: 0.00003133
Iteration 30/1000 | Loss: 0.00003678
Iteration 31/1000 | Loss: 0.00003901
Iteration 32/1000 | Loss: 0.00005501
Iteration 33/1000 | Loss: 0.00003424
Iteration 34/1000 | Loss: 0.00003746
Iteration 35/1000 | Loss: 0.00003879
Iteration 36/1000 | Loss: 0.00005660
Iteration 37/1000 | Loss: 0.00003493
Iteration 38/1000 | Loss: 0.00002770
Iteration 39/1000 | Loss: 0.00004040
Iteration 40/1000 | Loss: 0.00003730
Iteration 41/1000 | Loss: 0.00003934
Iteration 42/1000 | Loss: 0.00003816
Iteration 43/1000 | Loss: 0.00002901
Iteration 44/1000 | Loss: 0.00003837
Iteration 45/1000 | Loss: 0.00004602
Iteration 46/1000 | Loss: 0.00003536
Iteration 47/1000 | Loss: 0.00002154
Iteration 48/1000 | Loss: 0.00003147
Iteration 49/1000 | Loss: 0.00002020
Iteration 50/1000 | Loss: 0.00003142
Iteration 51/1000 | Loss: 0.00003514
Iteration 52/1000 | Loss: 0.00003009
Iteration 53/1000 | Loss: 0.00004217
Iteration 54/1000 | Loss: 0.00003531
Iteration 55/1000 | Loss: 0.00003788
Iteration 56/1000 | Loss: 0.00004564
Iteration 57/1000 | Loss: 0.00003513
Iteration 58/1000 | Loss: 0.00004458
Iteration 59/1000 | Loss: 0.00003982
Iteration 60/1000 | Loss: 0.00003951
Iteration 61/1000 | Loss: 0.00003634
Iteration 62/1000 | Loss: 0.00003165
Iteration 63/1000 | Loss: 0.00003848
Iteration 64/1000 | Loss: 0.00003012
Iteration 65/1000 | Loss: 0.00003633
Iteration 66/1000 | Loss: 0.00002936
Iteration 67/1000 | Loss: 0.00003590
Iteration 68/1000 | Loss: 0.00002910
Iteration 69/1000 | Loss: 0.00003300
Iteration 70/1000 | Loss: 0.00003014
Iteration 71/1000 | Loss: 0.00003662
Iteration 72/1000 | Loss: 0.00004124
Iteration 73/1000 | Loss: 0.00003631
Iteration 74/1000 | Loss: 0.00004189
Iteration 75/1000 | Loss: 0.00003954
Iteration 76/1000 | Loss: 0.00004293
Iteration 77/1000 | Loss: 0.00003599
Iteration 78/1000 | Loss: 0.00004204
Iteration 79/1000 | Loss: 0.00003647
Iteration 80/1000 | Loss: 0.00003040
Iteration 81/1000 | Loss: 0.00003627
Iteration 82/1000 | Loss: 0.00004170
Iteration 83/1000 | Loss: 0.00003687
Iteration 84/1000 | Loss: 0.00004134
Iteration 85/1000 | Loss: 0.00003700
Iteration 86/1000 | Loss: 0.00004172
Iteration 87/1000 | Loss: 0.00004095
Iteration 88/1000 | Loss: 0.00004097
Iteration 89/1000 | Loss: 0.00003026
Iteration 90/1000 | Loss: 0.00003357
Iteration 91/1000 | Loss: 0.00003658
Iteration 92/1000 | Loss: 0.00003694
Iteration 93/1000 | Loss: 0.00004046
Iteration 94/1000 | Loss: 0.00003806
Iteration 95/1000 | Loss: 0.00003832
Iteration 96/1000 | Loss: 0.00003986
Iteration 97/1000 | Loss: 0.00003699
Iteration 98/1000 | Loss: 0.00004153
Iteration 99/1000 | Loss: 0.00003660
Iteration 100/1000 | Loss: 0.00004239
Iteration 101/1000 | Loss: 0.00003691
Iteration 102/1000 | Loss: 0.00004055
Iteration 103/1000 | Loss: 0.00003850
Iteration 104/1000 | Loss: 0.00003959
Iteration 105/1000 | Loss: 0.00003646
Iteration 106/1000 | Loss: 0.00004339
Iteration 107/1000 | Loss: 0.00003597
Iteration 108/1000 | Loss: 0.00004081
Iteration 109/1000 | Loss: 0.00003589
Iteration 110/1000 | Loss: 0.00003990
Iteration 111/1000 | Loss: 0.00004475
Iteration 112/1000 | Loss: 0.00003749
Iteration 113/1000 | Loss: 0.00004195
Iteration 114/1000 | Loss: 0.00003715
Iteration 115/1000 | Loss: 0.00003770
Iteration 116/1000 | Loss: 0.00003996
Iteration 117/1000 | Loss: 0.00003904
Iteration 118/1000 | Loss: 0.00003997
Iteration 119/1000 | Loss: 0.00004845
Iteration 120/1000 | Loss: 0.00004067
Iteration 121/1000 | Loss: 0.00003894
Iteration 122/1000 | Loss: 0.00005693
Iteration 123/1000 | Loss: 0.00003629
Iteration 124/1000 | Loss: 0.00004116
Iteration 125/1000 | Loss: 0.00003529
Iteration 126/1000 | Loss: 0.00003774
Iteration 127/1000 | Loss: 0.00003412
Iteration 128/1000 | Loss: 0.00003910
Iteration 129/1000 | Loss: 0.00003597
Iteration 130/1000 | Loss: 0.00004115
Iteration 131/1000 | Loss: 0.00003641
Iteration 132/1000 | Loss: 0.00003935
Iteration 133/1000 | Loss: 0.00003709
Iteration 134/1000 | Loss: 0.00003852
Iteration 135/1000 | Loss: 0.00003644
Iteration 136/1000 | Loss: 0.00003692
Iteration 137/1000 | Loss: 0.00004304
Iteration 138/1000 | Loss: 0.00003649
Iteration 139/1000 | Loss: 0.00003781
Iteration 140/1000 | Loss: 0.00003984
Iteration 141/1000 | Loss: 0.00003695
Iteration 142/1000 | Loss: 0.00003888
Iteration 143/1000 | Loss: 0.00003673
Iteration 144/1000 | Loss: 0.00003959
Iteration 145/1000 | Loss: 0.00004101
Iteration 146/1000 | Loss: 0.00003508
Iteration 147/1000 | Loss: 0.00004075
Iteration 148/1000 | Loss: 0.00003613
Iteration 149/1000 | Loss: 0.00004379
Iteration 150/1000 | Loss: 0.00003574
Iteration 151/1000 | Loss: 0.00003674
Iteration 152/1000 | Loss: 0.00003087
Iteration 153/1000 | Loss: 0.00003949
Iteration 154/1000 | Loss: 0.00004086
Iteration 155/1000 | Loss: 0.00002765
Iteration 156/1000 | Loss: 0.00004026
Iteration 157/1000 | Loss: 0.00002607
Iteration 158/1000 | Loss: 0.00002505
Iteration 159/1000 | Loss: 0.00003830
Iteration 160/1000 | Loss: 0.00003092
Iteration 161/1000 | Loss: 0.00004169
Iteration 162/1000 | Loss: 0.00004224
Iteration 163/1000 | Loss: 0.00003863
Iteration 164/1000 | Loss: 0.00003517
Iteration 165/1000 | Loss: 0.00003751
Iteration 166/1000 | Loss: 0.00003787
Iteration 167/1000 | Loss: 0.00003594
Iteration 168/1000 | Loss: 0.00003664
Iteration 169/1000 | Loss: 0.00002921
Iteration 170/1000 | Loss: 0.00004270
Iteration 171/1000 | Loss: 0.00004049
Iteration 172/1000 | Loss: 0.00003882
Iteration 173/1000 | Loss: 0.00003100
Iteration 174/1000 | Loss: 0.00003719
Iteration 175/1000 | Loss: 0.00003971
Iteration 176/1000 | Loss: 0.00002623
Iteration 177/1000 | Loss: 0.00004221
Iteration 178/1000 | Loss: 0.00004032
Iteration 179/1000 | Loss: 0.00003949
Iteration 180/1000 | Loss: 0.00003991
Iteration 181/1000 | Loss: 0.00004404
Iteration 182/1000 | Loss: 0.00004014
Iteration 183/1000 | Loss: 0.00003901
Iteration 184/1000 | Loss: 0.00003949
Iteration 185/1000 | Loss: 0.00003948
Iteration 186/1000 | Loss: 0.00003945
Iteration 187/1000 | Loss: 0.00004292
Iteration 188/1000 | Loss: 0.00003392
Iteration 189/1000 | Loss: 0.00005003
Iteration 190/1000 | Loss: 0.00003447
Iteration 191/1000 | Loss: 0.00004678
Iteration 192/1000 | Loss: 0.00004785
Iteration 193/1000 | Loss: 0.00005118
Iteration 194/1000 | Loss: 0.00002196
Iteration 195/1000 | Loss: 0.00002950
Iteration 196/1000 | Loss: 0.00004437
Iteration 197/1000 | Loss: 0.00005294
Iteration 198/1000 | Loss: 0.00004580
Iteration 199/1000 | Loss: 0.00005660
Iteration 200/1000 | Loss: 0.00004867
Iteration 201/1000 | Loss: 0.00004602
Iteration 202/1000 | Loss: 0.00004426
Iteration 203/1000 | Loss: 0.00003251
Iteration 204/1000 | Loss: 0.00003864
Iteration 205/1000 | Loss: 0.00004422
Iteration 206/1000 | Loss: 0.00006110
Iteration 207/1000 | Loss: 0.00003217
Iteration 208/1000 | Loss: 0.00003719
Iteration 209/1000 | Loss: 0.00003754
Iteration 210/1000 | Loss: 0.00003778
Iteration 211/1000 | Loss: 0.00003490
Iteration 212/1000 | Loss: 0.00004848
Iteration 213/1000 | Loss: 0.00004146
Iteration 214/1000 | Loss: 0.00004481
Iteration 215/1000 | Loss: 0.00003817
Iteration 216/1000 | Loss: 0.00004384
Iteration 217/1000 | Loss: 0.00002531
Iteration 218/1000 | Loss: 0.00003846
Iteration 219/1000 | Loss: 0.00002644
Iteration 220/1000 | Loss: 0.00002475
Iteration 221/1000 | Loss: 0.00002503
Iteration 222/1000 | Loss: 0.00002284
Iteration 223/1000 | Loss: 0.00002100
Iteration 224/1000 | Loss: 0.00001995
Iteration 225/1000 | Loss: 0.00001947
Iteration 226/1000 | Loss: 0.00001918
Iteration 227/1000 | Loss: 0.00001910
Iteration 228/1000 | Loss: 0.00001909
Iteration 229/1000 | Loss: 0.00001909
Iteration 230/1000 | Loss: 0.00001908
Iteration 231/1000 | Loss: 0.00001908
Iteration 232/1000 | Loss: 0.00001906
Iteration 233/1000 | Loss: 0.00001905
Iteration 234/1000 | Loss: 0.00001897
Iteration 235/1000 | Loss: 0.00001894
Iteration 236/1000 | Loss: 0.00001893
Iteration 237/1000 | Loss: 0.00001893
Iteration 238/1000 | Loss: 0.00001893
Iteration 239/1000 | Loss: 0.00001892
Iteration 240/1000 | Loss: 0.00001892
Iteration 241/1000 | Loss: 0.00001891
Iteration 242/1000 | Loss: 0.00001891
Iteration 243/1000 | Loss: 0.00001890
Iteration 244/1000 | Loss: 0.00001890
Iteration 245/1000 | Loss: 0.00001889
Iteration 246/1000 | Loss: 0.00001884
Iteration 247/1000 | Loss: 0.00001880
Iteration 248/1000 | Loss: 0.00001880
Iteration 249/1000 | Loss: 0.00001880
Iteration 250/1000 | Loss: 0.00001880
Iteration 251/1000 | Loss: 0.00001880
Iteration 252/1000 | Loss: 0.00001880
Iteration 253/1000 | Loss: 0.00001880
Iteration 254/1000 | Loss: 0.00001880
Iteration 255/1000 | Loss: 0.00001880
Iteration 256/1000 | Loss: 0.00001880
Iteration 257/1000 | Loss: 0.00001880
Iteration 258/1000 | Loss: 0.00001879
Iteration 259/1000 | Loss: 0.00001879
Iteration 260/1000 | Loss: 0.00001879
Iteration 261/1000 | Loss: 0.00001879
Iteration 262/1000 | Loss: 0.00001879
Iteration 263/1000 | Loss: 0.00001878
Iteration 264/1000 | Loss: 0.00001878
Iteration 265/1000 | Loss: 0.00001877
Iteration 266/1000 | Loss: 0.00001877
Iteration 267/1000 | Loss: 0.00001877
Iteration 268/1000 | Loss: 0.00001876
Iteration 269/1000 | Loss: 0.00001876
Iteration 270/1000 | Loss: 0.00001876
Iteration 271/1000 | Loss: 0.00001876
Iteration 272/1000 | Loss: 0.00001875
Iteration 273/1000 | Loss: 0.00001875
Iteration 274/1000 | Loss: 0.00001875
Iteration 275/1000 | Loss: 0.00001875
Iteration 276/1000 | Loss: 0.00001875
Iteration 277/1000 | Loss: 0.00001875
Iteration 278/1000 | Loss: 0.00001875
Iteration 279/1000 | Loss: 0.00001875
Iteration 280/1000 | Loss: 0.00001875
Iteration 281/1000 | Loss: 0.00001875
Iteration 282/1000 | Loss: 0.00001874
Iteration 283/1000 | Loss: 0.00001874
Iteration 284/1000 | Loss: 0.00001874
Iteration 285/1000 | Loss: 0.00001874
Iteration 286/1000 | Loss: 0.00001874
Iteration 287/1000 | Loss: 0.00001874
Iteration 288/1000 | Loss: 0.00001874
Iteration 289/1000 | Loss: 0.00001874
Iteration 290/1000 | Loss: 0.00001874
Iteration 291/1000 | Loss: 0.00001874
Iteration 292/1000 | Loss: 0.00001873
Iteration 293/1000 | Loss: 0.00001873
Iteration 294/1000 | Loss: 0.00001873
Iteration 295/1000 | Loss: 0.00001873
Iteration 296/1000 | Loss: 0.00001873
Iteration 297/1000 | Loss: 0.00001873
Iteration 298/1000 | Loss: 0.00001873
Iteration 299/1000 | Loss: 0.00001873
Iteration 300/1000 | Loss: 0.00001873
Iteration 301/1000 | Loss: 0.00001873
Iteration 302/1000 | Loss: 0.00001873
Iteration 303/1000 | Loss: 0.00001873
Iteration 304/1000 | Loss: 0.00001873
Iteration 305/1000 | Loss: 0.00001873
Iteration 306/1000 | Loss: 0.00001873
Iteration 307/1000 | Loss: 0.00001873
Iteration 308/1000 | Loss: 0.00001873
Iteration 309/1000 | Loss: 0.00001872
Iteration 310/1000 | Loss: 0.00001872
Iteration 311/1000 | Loss: 0.00001872
Iteration 312/1000 | Loss: 0.00001872
Iteration 313/1000 | Loss: 0.00001872
Iteration 314/1000 | Loss: 0.00001872
Iteration 315/1000 | Loss: 0.00001872
Iteration 316/1000 | Loss: 0.00001872
Iteration 317/1000 | Loss: 0.00001872
Iteration 318/1000 | Loss: 0.00001872
Iteration 319/1000 | Loss: 0.00001872
Iteration 320/1000 | Loss: 0.00001872
Iteration 321/1000 | Loss: 0.00001871
Iteration 322/1000 | Loss: 0.00001871
Iteration 323/1000 | Loss: 0.00001871
Iteration 324/1000 | Loss: 0.00001871
Iteration 325/1000 | Loss: 0.00001870
Iteration 326/1000 | Loss: 0.00001870
Iteration 327/1000 | Loss: 0.00001870
Iteration 328/1000 | Loss: 0.00001869
Iteration 329/1000 | Loss: 0.00001869
Iteration 330/1000 | Loss: 0.00001869
Iteration 331/1000 | Loss: 0.00001869
Iteration 332/1000 | Loss: 0.00001869
Iteration 333/1000 | Loss: 0.00001869
Iteration 334/1000 | Loss: 0.00001868
Iteration 335/1000 | Loss: 0.00001868
Iteration 336/1000 | Loss: 0.00001868
Iteration 337/1000 | Loss: 0.00001868
Iteration 338/1000 | Loss: 0.00001868
Iteration 339/1000 | Loss: 0.00001868
Iteration 340/1000 | Loss: 0.00001868
Iteration 341/1000 | Loss: 0.00001868
Iteration 342/1000 | Loss: 0.00001868
Iteration 343/1000 | Loss: 0.00001868
Iteration 344/1000 | Loss: 0.00001868
Iteration 345/1000 | Loss: 0.00001867
Iteration 346/1000 | Loss: 0.00001867
Iteration 347/1000 | Loss: 0.00001867
Iteration 348/1000 | Loss: 0.00001867
Iteration 349/1000 | Loss: 0.00001867
Iteration 350/1000 | Loss: 0.00001867
Iteration 351/1000 | Loss: 0.00001867
Iteration 352/1000 | Loss: 0.00001867
Iteration 353/1000 | Loss: 0.00001867
Iteration 354/1000 | Loss: 0.00001867
Iteration 355/1000 | Loss: 0.00001867
Iteration 356/1000 | Loss: 0.00001867
Iteration 357/1000 | Loss: 0.00001867
Iteration 358/1000 | Loss: 0.00001867
Iteration 359/1000 | Loss: 0.00001867
Iteration 360/1000 | Loss: 0.00001867
Iteration 361/1000 | Loss: 0.00001867
Iteration 362/1000 | Loss: 0.00001867
Iteration 363/1000 | Loss: 0.00001867
Iteration 364/1000 | Loss: 0.00001867
Iteration 365/1000 | Loss: 0.00001867
Iteration 366/1000 | Loss: 0.00001867
Iteration 367/1000 | Loss: 0.00001867
Iteration 368/1000 | Loss: 0.00001867
Iteration 369/1000 | Loss: 0.00001867
Iteration 370/1000 | Loss: 0.00001867
Iteration 371/1000 | Loss: 0.00001867
Iteration 372/1000 | Loss: 0.00001867
Iteration 373/1000 | Loss: 0.00001867
Iteration 374/1000 | Loss: 0.00001867
Iteration 375/1000 | Loss: 0.00001867
Iteration 376/1000 | Loss: 0.00001867
Iteration 377/1000 | Loss: 0.00001867
Iteration 378/1000 | Loss: 0.00001867
Iteration 379/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 379. Stopping optimization.
Last 5 losses: [1.867117316578515e-05, 1.867117316578515e-05, 1.867117316578515e-05, 1.867117316578515e-05, 1.867117316578515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.867117316578515e-05

Optimization complete. Final v2v error: 3.4064059257507324 mm

Highest mean error: 12.024291038513184 mm for frame 88

Lowest mean error: 2.8684327602386475 mm for frame 32

Saving results

Total time: 369.20753145217896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843699
Iteration 2/25 | Loss: 0.00130133
Iteration 3/25 | Loss: 0.00123952
Iteration 4/25 | Loss: 0.00122761
Iteration 5/25 | Loss: 0.00122407
Iteration 6/25 | Loss: 0.00122365
Iteration 7/25 | Loss: 0.00122365
Iteration 8/25 | Loss: 0.00122365
Iteration 9/25 | Loss: 0.00122365
Iteration 10/25 | Loss: 0.00122365
Iteration 11/25 | Loss: 0.00122365
Iteration 12/25 | Loss: 0.00122365
Iteration 13/25 | Loss: 0.00122365
Iteration 14/25 | Loss: 0.00122365
Iteration 15/25 | Loss: 0.00122365
Iteration 16/25 | Loss: 0.00122365
Iteration 17/25 | Loss: 0.00122365
Iteration 18/25 | Loss: 0.00122365
Iteration 19/25 | Loss: 0.00122365
Iteration 20/25 | Loss: 0.00122365
Iteration 21/25 | Loss: 0.00122365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012236523907631636, 0.0012236523907631636, 0.0012236523907631636, 0.0012236523907631636, 0.0012236523907631636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012236523907631636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42994821
Iteration 2/25 | Loss: 0.00095551
Iteration 3/25 | Loss: 0.00095551
Iteration 4/25 | Loss: 0.00095551
Iteration 5/25 | Loss: 0.00095551
Iteration 6/25 | Loss: 0.00095551
Iteration 7/25 | Loss: 0.00095551
Iteration 8/25 | Loss: 0.00095551
Iteration 9/25 | Loss: 0.00095551
Iteration 10/25 | Loss: 0.00095551
Iteration 11/25 | Loss: 0.00095551
Iteration 12/25 | Loss: 0.00095551
Iteration 13/25 | Loss: 0.00095551
Iteration 14/25 | Loss: 0.00095551
Iteration 15/25 | Loss: 0.00095551
Iteration 16/25 | Loss: 0.00095551
Iteration 17/25 | Loss: 0.00095551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000955505296587944, 0.000955505296587944, 0.000955505296587944, 0.000955505296587944, 0.000955505296587944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000955505296587944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095551
Iteration 2/1000 | Loss: 0.00002760
Iteration 3/1000 | Loss: 0.00001983
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001482
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001361
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001333
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001332
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001322
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001315
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001308
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001298
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001279
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001272
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001270
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001269
Iteration 104/1000 | Loss: 0.00001269
Iteration 105/1000 | Loss: 0.00001269
Iteration 106/1000 | Loss: 0.00001269
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001268
Iteration 110/1000 | Loss: 0.00001268
Iteration 111/1000 | Loss: 0.00001268
Iteration 112/1000 | Loss: 0.00001267
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001264
Iteration 139/1000 | Loss: 0.00001264
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001264
Iteration 143/1000 | Loss: 0.00001264
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001263
Iteration 150/1000 | Loss: 0.00001263
Iteration 151/1000 | Loss: 0.00001263
Iteration 152/1000 | Loss: 0.00001263
Iteration 153/1000 | Loss: 0.00001263
Iteration 154/1000 | Loss: 0.00001263
Iteration 155/1000 | Loss: 0.00001263
Iteration 156/1000 | Loss: 0.00001263
Iteration 157/1000 | Loss: 0.00001263
Iteration 158/1000 | Loss: 0.00001263
Iteration 159/1000 | Loss: 0.00001263
Iteration 160/1000 | Loss: 0.00001263
Iteration 161/1000 | Loss: 0.00001263
Iteration 162/1000 | Loss: 0.00001263
Iteration 163/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.263160720554879e-05, 1.263160720554879e-05, 1.263160720554879e-05, 1.263160720554879e-05, 1.263160720554879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.263160720554879e-05

Optimization complete. Final v2v error: 3.0434694290161133 mm

Highest mean error: 3.412144184112549 mm for frame 83

Lowest mean error: 2.9576311111450195 mm for frame 35

Saving results

Total time: 41.42840814590454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879607
Iteration 2/25 | Loss: 0.00185938
Iteration 3/25 | Loss: 0.00147690
Iteration 4/25 | Loss: 0.00136343
Iteration 5/25 | Loss: 0.00138963
Iteration 6/25 | Loss: 0.00133542
Iteration 7/25 | Loss: 0.00129897
Iteration 8/25 | Loss: 0.00127235
Iteration 9/25 | Loss: 0.00125708
Iteration 10/25 | Loss: 0.00126426
Iteration 11/25 | Loss: 0.00125593
Iteration 12/25 | Loss: 0.00125261
Iteration 13/25 | Loss: 0.00125253
Iteration 14/25 | Loss: 0.00125253
Iteration 15/25 | Loss: 0.00125253
Iteration 16/25 | Loss: 0.00125252
Iteration 17/25 | Loss: 0.00125252
Iteration 18/25 | Loss: 0.00125252
Iteration 19/25 | Loss: 0.00125252
Iteration 20/25 | Loss: 0.00125252
Iteration 21/25 | Loss: 0.00125252
Iteration 22/25 | Loss: 0.00125251
Iteration 23/25 | Loss: 0.00125251
Iteration 24/25 | Loss: 0.00125251
Iteration 25/25 | Loss: 0.00125251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39239848
Iteration 2/25 | Loss: 0.00095426
Iteration 3/25 | Loss: 0.00095426
Iteration 4/25 | Loss: 0.00095425
Iteration 5/25 | Loss: 0.00095425
Iteration 6/25 | Loss: 0.00095425
Iteration 7/25 | Loss: 0.00095425
Iteration 8/25 | Loss: 0.00095425
Iteration 9/25 | Loss: 0.00095425
Iteration 10/25 | Loss: 0.00095425
Iteration 11/25 | Loss: 0.00095425
Iteration 12/25 | Loss: 0.00095425
Iteration 13/25 | Loss: 0.00095425
Iteration 14/25 | Loss: 0.00095425
Iteration 15/25 | Loss: 0.00095425
Iteration 16/25 | Loss: 0.00095425
Iteration 17/25 | Loss: 0.00095425
Iteration 18/25 | Loss: 0.00095425
Iteration 19/25 | Loss: 0.00095425
Iteration 20/25 | Loss: 0.00095425
Iteration 21/25 | Loss: 0.00095425
Iteration 22/25 | Loss: 0.00095425
Iteration 23/25 | Loss: 0.00095425
Iteration 24/25 | Loss: 0.00095425
Iteration 25/25 | Loss: 0.00095425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095425
Iteration 2/1000 | Loss: 0.00003450
Iteration 3/1000 | Loss: 0.00002472
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00007815
Iteration 7/1000 | Loss: 0.00002032
Iteration 8/1000 | Loss: 0.00001979
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001883
Iteration 12/1000 | Loss: 0.00055237
Iteration 13/1000 | Loss: 0.00002460
Iteration 14/1000 | Loss: 0.00014512
Iteration 15/1000 | Loss: 0.00001774
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001597
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001542
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001535
Iteration 32/1000 | Loss: 0.00001535
Iteration 33/1000 | Loss: 0.00001534
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001529
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001526
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001523
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00007175
Iteration 50/1000 | Loss: 0.00002642
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001518
Iteration 54/1000 | Loss: 0.00001518
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001517
Iteration 59/1000 | Loss: 0.00001517
Iteration 60/1000 | Loss: 0.00001517
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001516
Iteration 69/1000 | Loss: 0.00001516
Iteration 70/1000 | Loss: 0.00001516
Iteration 71/1000 | Loss: 0.00001516
Iteration 72/1000 | Loss: 0.00001516
Iteration 73/1000 | Loss: 0.00001516
Iteration 74/1000 | Loss: 0.00001516
Iteration 75/1000 | Loss: 0.00001516
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001515
Iteration 83/1000 | Loss: 0.00001515
Iteration 84/1000 | Loss: 0.00001515
Iteration 85/1000 | Loss: 0.00001515
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001515
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001514
Iteration 99/1000 | Loss: 0.00001514
Iteration 100/1000 | Loss: 0.00001514
Iteration 101/1000 | Loss: 0.00001514
Iteration 102/1000 | Loss: 0.00001514
Iteration 103/1000 | Loss: 0.00001514
Iteration 104/1000 | Loss: 0.00001514
Iteration 105/1000 | Loss: 0.00001514
Iteration 106/1000 | Loss: 0.00001514
Iteration 107/1000 | Loss: 0.00001514
Iteration 108/1000 | Loss: 0.00001514
Iteration 109/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5143114069360308e-05, 1.5143114069360308e-05, 1.5143114069360308e-05, 1.5143114069360308e-05, 1.5143114069360308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5143114069360308e-05

Optimization complete. Final v2v error: 3.34700083732605 mm

Highest mean error: 4.5744194984436035 mm for frame 75

Lowest mean error: 3.077955722808838 mm for frame 105

Saving results

Total time: 62.80926251411438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815534
Iteration 2/25 | Loss: 0.00130064
Iteration 3/25 | Loss: 0.00122076
Iteration 4/25 | Loss: 0.00121260
Iteration 5/25 | Loss: 0.00121096
Iteration 6/25 | Loss: 0.00121086
Iteration 7/25 | Loss: 0.00121086
Iteration 8/25 | Loss: 0.00121086
Iteration 9/25 | Loss: 0.00121086
Iteration 10/25 | Loss: 0.00121086
Iteration 11/25 | Loss: 0.00121086
Iteration 12/25 | Loss: 0.00121086
Iteration 13/25 | Loss: 0.00121086
Iteration 14/25 | Loss: 0.00121086
Iteration 15/25 | Loss: 0.00121086
Iteration 16/25 | Loss: 0.00121086
Iteration 17/25 | Loss: 0.00121086
Iteration 18/25 | Loss: 0.00121086
Iteration 19/25 | Loss: 0.00121086
Iteration 20/25 | Loss: 0.00121086
Iteration 21/25 | Loss: 0.00121086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012108635855838656, 0.0012108635855838656, 0.0012108635855838656, 0.0012108635855838656, 0.0012108635855838656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012108635855838656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34954476
Iteration 2/25 | Loss: 0.00083497
Iteration 3/25 | Loss: 0.00083495
Iteration 4/25 | Loss: 0.00083495
Iteration 5/25 | Loss: 0.00083495
Iteration 6/25 | Loss: 0.00083495
Iteration 7/25 | Loss: 0.00083495
Iteration 8/25 | Loss: 0.00083495
Iteration 9/25 | Loss: 0.00083495
Iteration 10/25 | Loss: 0.00083495
Iteration 11/25 | Loss: 0.00083495
Iteration 12/25 | Loss: 0.00083495
Iteration 13/25 | Loss: 0.00083495
Iteration 14/25 | Loss: 0.00083495
Iteration 15/25 | Loss: 0.00083495
Iteration 16/25 | Loss: 0.00083495
Iteration 17/25 | Loss: 0.00083495
Iteration 18/25 | Loss: 0.00083495
Iteration 19/25 | Loss: 0.00083495
Iteration 20/25 | Loss: 0.00083495
Iteration 21/25 | Loss: 0.00083495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008349463460035622, 0.0008349463460035622, 0.0008349463460035622, 0.0008349463460035622, 0.0008349463460035622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008349463460035622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083495
Iteration 2/1000 | Loss: 0.00002196
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00001163
Iteration 10/1000 | Loss: 0.00001151
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001110
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001087
Iteration 16/1000 | Loss: 0.00001086
Iteration 17/1000 | Loss: 0.00001082
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001072
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001067
Iteration 34/1000 | Loss: 0.00001066
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001065
Iteration 38/1000 | Loss: 0.00001065
Iteration 39/1000 | Loss: 0.00001065
Iteration 40/1000 | Loss: 0.00001064
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001062
Iteration 44/1000 | Loss: 0.00001062
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001061
Iteration 47/1000 | Loss: 0.00001061
Iteration 48/1000 | Loss: 0.00001060
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001057
Iteration 51/1000 | Loss: 0.00001057
Iteration 52/1000 | Loss: 0.00001057
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001056
Iteration 56/1000 | Loss: 0.00001056
Iteration 57/1000 | Loss: 0.00001056
Iteration 58/1000 | Loss: 0.00001055
Iteration 59/1000 | Loss: 0.00001055
Iteration 60/1000 | Loss: 0.00001055
Iteration 61/1000 | Loss: 0.00001055
Iteration 62/1000 | Loss: 0.00001054
Iteration 63/1000 | Loss: 0.00001054
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001053
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001053
Iteration 68/1000 | Loss: 0.00001053
Iteration 69/1000 | Loss: 0.00001053
Iteration 70/1000 | Loss: 0.00001052
Iteration 71/1000 | Loss: 0.00001052
Iteration 72/1000 | Loss: 0.00001052
Iteration 73/1000 | Loss: 0.00001052
Iteration 74/1000 | Loss: 0.00001051
Iteration 75/1000 | Loss: 0.00001051
Iteration 76/1000 | Loss: 0.00001050
Iteration 77/1000 | Loss: 0.00001050
Iteration 78/1000 | Loss: 0.00001049
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001048
Iteration 81/1000 | Loss: 0.00001048
Iteration 82/1000 | Loss: 0.00001047
Iteration 83/1000 | Loss: 0.00001047
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001041
Iteration 92/1000 | Loss: 0.00001041
Iteration 93/1000 | Loss: 0.00001041
Iteration 94/1000 | Loss: 0.00001040
Iteration 95/1000 | Loss: 0.00001039
Iteration 96/1000 | Loss: 0.00001039
Iteration 97/1000 | Loss: 0.00001039
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001038
Iteration 102/1000 | Loss: 0.00001038
Iteration 103/1000 | Loss: 0.00001038
Iteration 104/1000 | Loss: 0.00001038
Iteration 105/1000 | Loss: 0.00001038
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001037
Iteration 109/1000 | Loss: 0.00001037
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001035
Iteration 116/1000 | Loss: 0.00001035
Iteration 117/1000 | Loss: 0.00001035
Iteration 118/1000 | Loss: 0.00001034
Iteration 119/1000 | Loss: 0.00001034
Iteration 120/1000 | Loss: 0.00001034
Iteration 121/1000 | Loss: 0.00001034
Iteration 122/1000 | Loss: 0.00001033
Iteration 123/1000 | Loss: 0.00001033
Iteration 124/1000 | Loss: 0.00001033
Iteration 125/1000 | Loss: 0.00001033
Iteration 126/1000 | Loss: 0.00001032
Iteration 127/1000 | Loss: 0.00001032
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001032
Iteration 131/1000 | Loss: 0.00001032
Iteration 132/1000 | Loss: 0.00001032
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001032
Iteration 137/1000 | Loss: 0.00001032
Iteration 138/1000 | Loss: 0.00001032
Iteration 139/1000 | Loss: 0.00001032
Iteration 140/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0320659384888131e-05, 1.0320659384888131e-05, 1.0320659384888131e-05, 1.0320659384888131e-05, 1.0320659384888131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0320659384888131e-05

Optimization complete. Final v2v error: 2.7700371742248535 mm

Highest mean error: 2.8992624282836914 mm for frame 26

Lowest mean error: 2.6711809635162354 mm for frame 43

Saving results

Total time: 35.807230710983276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486145
Iteration 2/25 | Loss: 0.00138884
Iteration 3/25 | Loss: 0.00128932
Iteration 4/25 | Loss: 0.00127281
Iteration 5/25 | Loss: 0.00126761
Iteration 6/25 | Loss: 0.00126658
Iteration 7/25 | Loss: 0.00126658
Iteration 8/25 | Loss: 0.00126658
Iteration 9/25 | Loss: 0.00126658
Iteration 10/25 | Loss: 0.00126658
Iteration 11/25 | Loss: 0.00126658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012665796093642712, 0.0012665796093642712, 0.0012665796093642712, 0.0012665796093642712, 0.0012665796093642712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012665796093642712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40830505
Iteration 2/25 | Loss: 0.00113465
Iteration 3/25 | Loss: 0.00113464
Iteration 4/25 | Loss: 0.00113464
Iteration 5/25 | Loss: 0.00113464
Iteration 6/25 | Loss: 0.00113464
Iteration 7/25 | Loss: 0.00113464
Iteration 8/25 | Loss: 0.00113464
Iteration 9/25 | Loss: 0.00113464
Iteration 10/25 | Loss: 0.00113464
Iteration 11/25 | Loss: 0.00113464
Iteration 12/25 | Loss: 0.00113463
Iteration 13/25 | Loss: 0.00113463
Iteration 14/25 | Loss: 0.00113463
Iteration 15/25 | Loss: 0.00113463
Iteration 16/25 | Loss: 0.00113463
Iteration 17/25 | Loss: 0.00113463
Iteration 18/25 | Loss: 0.00113463
Iteration 19/25 | Loss: 0.00113463
Iteration 20/25 | Loss: 0.00113463
Iteration 21/25 | Loss: 0.00113463
Iteration 22/25 | Loss: 0.00113463
Iteration 23/25 | Loss: 0.00113463
Iteration 24/25 | Loss: 0.00113463
Iteration 25/25 | Loss: 0.00113463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113463
Iteration 2/1000 | Loss: 0.00003392
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00001851
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001700
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001643
Iteration 11/1000 | Loss: 0.00001633
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001608
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001593
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001591
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001581
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001581
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001576
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001571
Iteration 68/1000 | Loss: 0.00001571
Iteration 69/1000 | Loss: 0.00001571
Iteration 70/1000 | Loss: 0.00001570
Iteration 71/1000 | Loss: 0.00001570
Iteration 72/1000 | Loss: 0.00001570
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001569
Iteration 75/1000 | Loss: 0.00001569
Iteration 76/1000 | Loss: 0.00001569
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001568
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001565
Iteration 91/1000 | Loss: 0.00001565
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001563
Iteration 101/1000 | Loss: 0.00001563
Iteration 102/1000 | Loss: 0.00001563
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001562
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001562
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001561
Iteration 112/1000 | Loss: 0.00001561
Iteration 113/1000 | Loss: 0.00001561
Iteration 114/1000 | Loss: 0.00001561
Iteration 115/1000 | Loss: 0.00001561
Iteration 116/1000 | Loss: 0.00001561
Iteration 117/1000 | Loss: 0.00001561
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001560
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001560
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.5593384887324646e-05, 1.5593384887324646e-05, 1.5593384887324646e-05, 1.5593384887324646e-05, 1.5593384887324646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5593384887324646e-05

Optimization complete. Final v2v error: 3.3247594833374023 mm

Highest mean error: 3.814039707183838 mm for frame 47

Lowest mean error: 2.801815986633301 mm for frame 41

Saving results

Total time: 42.11705994606018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069404
Iteration 2/25 | Loss: 0.00262652
Iteration 3/25 | Loss: 0.00185188
Iteration 4/25 | Loss: 0.00169082
Iteration 5/25 | Loss: 0.00165879
Iteration 6/25 | Loss: 0.00166091
Iteration 7/25 | Loss: 0.00158233
Iteration 8/25 | Loss: 0.00153629
Iteration 9/25 | Loss: 0.00150241
Iteration 10/25 | Loss: 0.00150923
Iteration 11/25 | Loss: 0.00148694
Iteration 12/25 | Loss: 0.00148263
Iteration 13/25 | Loss: 0.00147152
Iteration 14/25 | Loss: 0.00146599
Iteration 15/25 | Loss: 0.00141740
Iteration 16/25 | Loss: 0.00140448
Iteration 17/25 | Loss: 0.00141829
Iteration 18/25 | Loss: 0.00141844
Iteration 19/25 | Loss: 0.00141285
Iteration 20/25 | Loss: 0.00139860
Iteration 21/25 | Loss: 0.00138944
Iteration 22/25 | Loss: 0.00137150
Iteration 23/25 | Loss: 0.00136772
Iteration 24/25 | Loss: 0.00136402
Iteration 25/25 | Loss: 0.00136089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09067261
Iteration 2/25 | Loss: 0.00107083
Iteration 3/25 | Loss: 0.00107083
Iteration 4/25 | Loss: 0.00107083
Iteration 5/25 | Loss: 0.00107083
Iteration 6/25 | Loss: 0.00107083
Iteration 7/25 | Loss: 0.00107083
Iteration 8/25 | Loss: 0.00107083
Iteration 9/25 | Loss: 0.00107083
Iteration 10/25 | Loss: 0.00107083
Iteration 11/25 | Loss: 0.00107083
Iteration 12/25 | Loss: 0.00107083
Iteration 13/25 | Loss: 0.00107083
Iteration 14/25 | Loss: 0.00107083
Iteration 15/25 | Loss: 0.00107083
Iteration 16/25 | Loss: 0.00107083
Iteration 17/25 | Loss: 0.00107083
Iteration 18/25 | Loss: 0.00107083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010708278277888894, 0.0010708278277888894, 0.0010708278277888894, 0.0010708278277888894, 0.0010708278277888894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010708278277888894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107083
Iteration 2/1000 | Loss: 0.00010570
Iteration 3/1000 | Loss: 0.00008314
Iteration 4/1000 | Loss: 0.00008667
Iteration 5/1000 | Loss: 0.00007170
Iteration 6/1000 | Loss: 0.00007208
Iteration 7/1000 | Loss: 0.00005838
Iteration 8/1000 | Loss: 0.00007796
Iteration 9/1000 | Loss: 0.00006926
Iteration 10/1000 | Loss: 0.00005284
Iteration 11/1000 | Loss: 0.00005235
Iteration 12/1000 | Loss: 0.00005241
Iteration 13/1000 | Loss: 0.00006647
Iteration 14/1000 | Loss: 0.00007323
Iteration 15/1000 | Loss: 0.00006341
Iteration 16/1000 | Loss: 0.00006585
Iteration 17/1000 | Loss: 0.00006328
Iteration 18/1000 | Loss: 0.00007428
Iteration 19/1000 | Loss: 0.00006427
Iteration 20/1000 | Loss: 0.00007523
Iteration 21/1000 | Loss: 0.00007169
Iteration 22/1000 | Loss: 0.00006790
Iteration 23/1000 | Loss: 0.00006848
Iteration 24/1000 | Loss: 0.00008099
Iteration 25/1000 | Loss: 0.00006257
Iteration 26/1000 | Loss: 0.00006992
Iteration 27/1000 | Loss: 0.00022692
Iteration 28/1000 | Loss: 0.00007971
Iteration 29/1000 | Loss: 0.00005449
Iteration 30/1000 | Loss: 0.00011467
Iteration 31/1000 | Loss: 0.00006744
Iteration 32/1000 | Loss: 0.00007427
Iteration 33/1000 | Loss: 0.00007428
Iteration 34/1000 | Loss: 0.00006783
Iteration 35/1000 | Loss: 0.00006831
Iteration 36/1000 | Loss: 0.00019344
Iteration 37/1000 | Loss: 0.00022251
Iteration 38/1000 | Loss: 0.00020290
Iteration 39/1000 | Loss: 0.00004959
Iteration 40/1000 | Loss: 0.00005824
Iteration 41/1000 | Loss: 0.00022638
Iteration 42/1000 | Loss: 0.00031546
Iteration 43/1000 | Loss: 0.00038507
Iteration 44/1000 | Loss: 0.00005674
Iteration 45/1000 | Loss: 0.00094717
Iteration 46/1000 | Loss: 0.00017478
Iteration 47/1000 | Loss: 0.00005446
Iteration 48/1000 | Loss: 0.00004333
Iteration 49/1000 | Loss: 0.00003407
Iteration 50/1000 | Loss: 0.00002823
Iteration 51/1000 | Loss: 0.00002591
Iteration 52/1000 | Loss: 0.00002445
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002274
Iteration 55/1000 | Loss: 0.00002205
Iteration 56/1000 | Loss: 0.00002127
Iteration 57/1000 | Loss: 0.00002077
Iteration 58/1000 | Loss: 0.00002777
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002022
Iteration 61/1000 | Loss: 0.00002016
Iteration 62/1000 | Loss: 0.00001994
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001980
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001974
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001971
Iteration 72/1000 | Loss: 0.00001968
Iteration 73/1000 | Loss: 0.00001967
Iteration 74/1000 | Loss: 0.00001963
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001958
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001957
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001955
Iteration 87/1000 | Loss: 0.00001954
Iteration 88/1000 | Loss: 0.00001954
Iteration 89/1000 | Loss: 0.00001954
Iteration 90/1000 | Loss: 0.00001953
Iteration 91/1000 | Loss: 0.00001953
Iteration 92/1000 | Loss: 0.00001953
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001952
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001949
Iteration 104/1000 | Loss: 0.00001949
Iteration 105/1000 | Loss: 0.00001949
Iteration 106/1000 | Loss: 0.00001948
Iteration 107/1000 | Loss: 0.00001948
Iteration 108/1000 | Loss: 0.00001948
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001947
Iteration 113/1000 | Loss: 0.00001947
Iteration 114/1000 | Loss: 0.00001947
Iteration 115/1000 | Loss: 0.00001946
Iteration 116/1000 | Loss: 0.00001946
Iteration 117/1000 | Loss: 0.00001945
Iteration 118/1000 | Loss: 0.00001945
Iteration 119/1000 | Loss: 0.00001945
Iteration 120/1000 | Loss: 0.00001945
Iteration 121/1000 | Loss: 0.00001944
Iteration 122/1000 | Loss: 0.00001944
Iteration 123/1000 | Loss: 0.00001944
Iteration 124/1000 | Loss: 0.00001944
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001942
Iteration 127/1000 | Loss: 0.00001942
Iteration 128/1000 | Loss: 0.00001942
Iteration 129/1000 | Loss: 0.00001942
Iteration 130/1000 | Loss: 0.00001942
Iteration 131/1000 | Loss: 0.00001941
Iteration 132/1000 | Loss: 0.00001941
Iteration 133/1000 | Loss: 0.00001940
Iteration 134/1000 | Loss: 0.00001940
Iteration 135/1000 | Loss: 0.00001940
Iteration 136/1000 | Loss: 0.00001939
Iteration 137/1000 | Loss: 0.00001939
Iteration 138/1000 | Loss: 0.00001939
Iteration 139/1000 | Loss: 0.00001939
Iteration 140/1000 | Loss: 0.00001939
Iteration 141/1000 | Loss: 0.00001939
Iteration 142/1000 | Loss: 0.00001939
Iteration 143/1000 | Loss: 0.00001939
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001938
Iteration 146/1000 | Loss: 0.00001938
Iteration 147/1000 | Loss: 0.00001937
Iteration 148/1000 | Loss: 0.00001937
Iteration 149/1000 | Loss: 0.00001937
Iteration 150/1000 | Loss: 0.00001937
Iteration 151/1000 | Loss: 0.00001937
Iteration 152/1000 | Loss: 0.00001937
Iteration 153/1000 | Loss: 0.00001937
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00001937
Iteration 156/1000 | Loss: 0.00001936
Iteration 157/1000 | Loss: 0.00001936
Iteration 158/1000 | Loss: 0.00001936
Iteration 159/1000 | Loss: 0.00001936
Iteration 160/1000 | Loss: 0.00001936
Iteration 161/1000 | Loss: 0.00001935
Iteration 162/1000 | Loss: 0.00001935
Iteration 163/1000 | Loss: 0.00001935
Iteration 164/1000 | Loss: 0.00001935
Iteration 165/1000 | Loss: 0.00001935
Iteration 166/1000 | Loss: 0.00001935
Iteration 167/1000 | Loss: 0.00001935
Iteration 168/1000 | Loss: 0.00001935
Iteration 169/1000 | Loss: 0.00001935
Iteration 170/1000 | Loss: 0.00001935
Iteration 171/1000 | Loss: 0.00001934
Iteration 172/1000 | Loss: 0.00001934
Iteration 173/1000 | Loss: 0.00001933
Iteration 174/1000 | Loss: 0.00001933
Iteration 175/1000 | Loss: 0.00001933
Iteration 176/1000 | Loss: 0.00001933
Iteration 177/1000 | Loss: 0.00001933
Iteration 178/1000 | Loss: 0.00001932
Iteration 179/1000 | Loss: 0.00001932
Iteration 180/1000 | Loss: 0.00001932
Iteration 181/1000 | Loss: 0.00001932
Iteration 182/1000 | Loss: 0.00001932
Iteration 183/1000 | Loss: 0.00001931
Iteration 184/1000 | Loss: 0.00001931
Iteration 185/1000 | Loss: 0.00001931
Iteration 186/1000 | Loss: 0.00001931
Iteration 187/1000 | Loss: 0.00001931
Iteration 188/1000 | Loss: 0.00001931
Iteration 189/1000 | Loss: 0.00001930
Iteration 190/1000 | Loss: 0.00001930
Iteration 191/1000 | Loss: 0.00001930
Iteration 192/1000 | Loss: 0.00001930
Iteration 193/1000 | Loss: 0.00001930
Iteration 194/1000 | Loss: 0.00001930
Iteration 195/1000 | Loss: 0.00001930
Iteration 196/1000 | Loss: 0.00001930
Iteration 197/1000 | Loss: 0.00001930
Iteration 198/1000 | Loss: 0.00001930
Iteration 199/1000 | Loss: 0.00001930
Iteration 200/1000 | Loss: 0.00001930
Iteration 201/1000 | Loss: 0.00001930
Iteration 202/1000 | Loss: 0.00001930
Iteration 203/1000 | Loss: 0.00001930
Iteration 204/1000 | Loss: 0.00001930
Iteration 205/1000 | Loss: 0.00001930
Iteration 206/1000 | Loss: 0.00001930
Iteration 207/1000 | Loss: 0.00001930
Iteration 208/1000 | Loss: 0.00001930
Iteration 209/1000 | Loss: 0.00001930
Iteration 210/1000 | Loss: 0.00001930
Iteration 211/1000 | Loss: 0.00001930
Iteration 212/1000 | Loss: 0.00001930
Iteration 213/1000 | Loss: 0.00001930
Iteration 214/1000 | Loss: 0.00001930
Iteration 215/1000 | Loss: 0.00001930
Iteration 216/1000 | Loss: 0.00001930
Iteration 217/1000 | Loss: 0.00001930
Iteration 218/1000 | Loss: 0.00001930
Iteration 219/1000 | Loss: 0.00001930
Iteration 220/1000 | Loss: 0.00001930
Iteration 221/1000 | Loss: 0.00001930
Iteration 222/1000 | Loss: 0.00001930
Iteration 223/1000 | Loss: 0.00001930
Iteration 224/1000 | Loss: 0.00001930
Iteration 225/1000 | Loss: 0.00001930
Iteration 226/1000 | Loss: 0.00001930
Iteration 227/1000 | Loss: 0.00001930
Iteration 228/1000 | Loss: 0.00001930
Iteration 229/1000 | Loss: 0.00001930
Iteration 230/1000 | Loss: 0.00001930
Iteration 231/1000 | Loss: 0.00001930
Iteration 232/1000 | Loss: 0.00001930
Iteration 233/1000 | Loss: 0.00001930
Iteration 234/1000 | Loss: 0.00001930
Iteration 235/1000 | Loss: 0.00001930
Iteration 236/1000 | Loss: 0.00001930
Iteration 237/1000 | Loss: 0.00001930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.929865902638994e-05, 1.929865902638994e-05, 1.929865902638994e-05, 1.929865902638994e-05, 1.929865902638994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.929865902638994e-05

Optimization complete. Final v2v error: 3.744717836380005 mm

Highest mean error: 4.350769996643066 mm for frame 99

Lowest mean error: 3.573016881942749 mm for frame 130

Saving results

Total time: 140.11031532287598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765290
Iteration 2/25 | Loss: 0.00176345
Iteration 3/25 | Loss: 0.00141916
Iteration 4/25 | Loss: 0.00139398
Iteration 5/25 | Loss: 0.00139208
Iteration 6/25 | Loss: 0.00139208
Iteration 7/25 | Loss: 0.00139208
Iteration 8/25 | Loss: 0.00139208
Iteration 9/25 | Loss: 0.00139208
Iteration 10/25 | Loss: 0.00139208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001392081961967051, 0.001392081961967051, 0.001392081961967051, 0.001392081961967051, 0.001392081961967051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001392081961967051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35102820
Iteration 2/25 | Loss: 0.00089017
Iteration 3/25 | Loss: 0.00089015
Iteration 4/25 | Loss: 0.00089014
Iteration 5/25 | Loss: 0.00089014
Iteration 6/25 | Loss: 0.00089014
Iteration 7/25 | Loss: 0.00089014
Iteration 8/25 | Loss: 0.00089014
Iteration 9/25 | Loss: 0.00089014
Iteration 10/25 | Loss: 0.00089014
Iteration 11/25 | Loss: 0.00089014
Iteration 12/25 | Loss: 0.00089014
Iteration 13/25 | Loss: 0.00089014
Iteration 14/25 | Loss: 0.00089014
Iteration 15/25 | Loss: 0.00089014
Iteration 16/25 | Loss: 0.00089014
Iteration 17/25 | Loss: 0.00089014
Iteration 18/25 | Loss: 0.00089014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008901424007490277, 0.0008901424007490277, 0.0008901424007490277, 0.0008901424007490277, 0.0008901424007490277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008901424007490277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089014
Iteration 2/1000 | Loss: 0.00005129
Iteration 3/1000 | Loss: 0.00003495
Iteration 4/1000 | Loss: 0.00003060
Iteration 5/1000 | Loss: 0.00002884
Iteration 6/1000 | Loss: 0.00002780
Iteration 7/1000 | Loss: 0.00002729
Iteration 8/1000 | Loss: 0.00002692
Iteration 9/1000 | Loss: 0.00002664
Iteration 10/1000 | Loss: 0.00002638
Iteration 11/1000 | Loss: 0.00002609
Iteration 12/1000 | Loss: 0.00002599
Iteration 13/1000 | Loss: 0.00002580
Iteration 14/1000 | Loss: 0.00002578
Iteration 15/1000 | Loss: 0.00002568
Iteration 16/1000 | Loss: 0.00002554
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00002544
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002540
Iteration 21/1000 | Loss: 0.00002539
Iteration 22/1000 | Loss: 0.00002539
Iteration 23/1000 | Loss: 0.00002538
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002537
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002536
Iteration 28/1000 | Loss: 0.00002536
Iteration 29/1000 | Loss: 0.00002536
Iteration 30/1000 | Loss: 0.00002536
Iteration 31/1000 | Loss: 0.00002536
Iteration 32/1000 | Loss: 0.00002535
Iteration 33/1000 | Loss: 0.00002535
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002534
Iteration 40/1000 | Loss: 0.00002534
Iteration 41/1000 | Loss: 0.00002534
Iteration 42/1000 | Loss: 0.00002533
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002533
Iteration 45/1000 | Loss: 0.00002533
Iteration 46/1000 | Loss: 0.00002533
Iteration 47/1000 | Loss: 0.00002532
Iteration 48/1000 | Loss: 0.00002532
Iteration 49/1000 | Loss: 0.00002532
Iteration 50/1000 | Loss: 0.00002532
Iteration 51/1000 | Loss: 0.00002532
Iteration 52/1000 | Loss: 0.00002532
Iteration 53/1000 | Loss: 0.00002532
Iteration 54/1000 | Loss: 0.00002532
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00002531
Iteration 59/1000 | Loss: 0.00002530
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002530
Iteration 62/1000 | Loss: 0.00002529
Iteration 63/1000 | Loss: 0.00002529
Iteration 64/1000 | Loss: 0.00002529
Iteration 65/1000 | Loss: 0.00002529
Iteration 66/1000 | Loss: 0.00002529
Iteration 67/1000 | Loss: 0.00002529
Iteration 68/1000 | Loss: 0.00002529
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002529
Iteration 71/1000 | Loss: 0.00002529
Iteration 72/1000 | Loss: 0.00002529
Iteration 73/1000 | Loss: 0.00002529
Iteration 74/1000 | Loss: 0.00002529
Iteration 75/1000 | Loss: 0.00002529
Iteration 76/1000 | Loss: 0.00002529
Iteration 77/1000 | Loss: 0.00002529
Iteration 78/1000 | Loss: 0.00002529
Iteration 79/1000 | Loss: 0.00002529
Iteration 80/1000 | Loss: 0.00002529
Iteration 81/1000 | Loss: 0.00002529
Iteration 82/1000 | Loss: 0.00002529
Iteration 83/1000 | Loss: 0.00002529
Iteration 84/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.5286472009611316e-05, 2.5286472009611316e-05, 2.5286472009611316e-05, 2.5286472009611316e-05, 2.5286472009611316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5286472009611316e-05

Optimization complete. Final v2v error: 4.233193397521973 mm

Highest mean error: 4.4240264892578125 mm for frame 54

Lowest mean error: 4.0723185539245605 mm for frame 109

Saving results

Total time: 32.29171180725098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897337
Iteration 2/25 | Loss: 0.00166871
Iteration 3/25 | Loss: 0.00140095
Iteration 4/25 | Loss: 0.00137358
Iteration 5/25 | Loss: 0.00136463
Iteration 6/25 | Loss: 0.00136364
Iteration 7/25 | Loss: 0.00136364
Iteration 8/25 | Loss: 0.00136364
Iteration 9/25 | Loss: 0.00136364
Iteration 10/25 | Loss: 0.00136364
Iteration 11/25 | Loss: 0.00136364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001363635528832674, 0.001363635528832674, 0.001363635528832674, 0.001363635528832674, 0.001363635528832674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001363635528832674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05321336
Iteration 2/25 | Loss: 0.00119680
Iteration 3/25 | Loss: 0.00119679
Iteration 4/25 | Loss: 0.00119679
Iteration 5/25 | Loss: 0.00119679
Iteration 6/25 | Loss: 0.00119679
Iteration 7/25 | Loss: 0.00119679
Iteration 8/25 | Loss: 0.00119679
Iteration 9/25 | Loss: 0.00119679
Iteration 10/25 | Loss: 0.00119679
Iteration 11/25 | Loss: 0.00119679
Iteration 12/25 | Loss: 0.00119679
Iteration 13/25 | Loss: 0.00119679
Iteration 14/25 | Loss: 0.00119679
Iteration 15/25 | Loss: 0.00119679
Iteration 16/25 | Loss: 0.00119679
Iteration 17/25 | Loss: 0.00119679
Iteration 18/25 | Loss: 0.00119679
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001196789089590311, 0.001196789089590311, 0.001196789089590311, 0.001196789089590311, 0.001196789089590311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196789089590311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119679
Iteration 2/1000 | Loss: 0.00007479
Iteration 3/1000 | Loss: 0.00004305
Iteration 4/1000 | Loss: 0.00003904
Iteration 5/1000 | Loss: 0.00003686
Iteration 6/1000 | Loss: 0.00003547
Iteration 7/1000 | Loss: 0.00003447
Iteration 8/1000 | Loss: 0.00003368
Iteration 9/1000 | Loss: 0.00003317
Iteration 10/1000 | Loss: 0.00003275
Iteration 11/1000 | Loss: 0.00003245
Iteration 12/1000 | Loss: 0.00003215
Iteration 13/1000 | Loss: 0.00003186
Iteration 14/1000 | Loss: 0.00003161
Iteration 15/1000 | Loss: 0.00003136
Iteration 16/1000 | Loss: 0.00003115
Iteration 17/1000 | Loss: 0.00003095
Iteration 18/1000 | Loss: 0.00003076
Iteration 19/1000 | Loss: 0.00003063
Iteration 20/1000 | Loss: 0.00003059
Iteration 21/1000 | Loss: 0.00003052
Iteration 22/1000 | Loss: 0.00003049
Iteration 23/1000 | Loss: 0.00003042
Iteration 24/1000 | Loss: 0.00003041
Iteration 25/1000 | Loss: 0.00003041
Iteration 26/1000 | Loss: 0.00003040
Iteration 27/1000 | Loss: 0.00003039
Iteration 28/1000 | Loss: 0.00003038
Iteration 29/1000 | Loss: 0.00003037
Iteration 30/1000 | Loss: 0.00003029
Iteration 31/1000 | Loss: 0.00003027
Iteration 32/1000 | Loss: 0.00003027
Iteration 33/1000 | Loss: 0.00003027
Iteration 34/1000 | Loss: 0.00003026
Iteration 35/1000 | Loss: 0.00003025
Iteration 36/1000 | Loss: 0.00003025
Iteration 37/1000 | Loss: 0.00003025
Iteration 38/1000 | Loss: 0.00003025
Iteration 39/1000 | Loss: 0.00003024
Iteration 40/1000 | Loss: 0.00003023
Iteration 41/1000 | Loss: 0.00003023
Iteration 42/1000 | Loss: 0.00003023
Iteration 43/1000 | Loss: 0.00003023
Iteration 44/1000 | Loss: 0.00003023
Iteration 45/1000 | Loss: 0.00003022
Iteration 46/1000 | Loss: 0.00003022
Iteration 47/1000 | Loss: 0.00003021
Iteration 48/1000 | Loss: 0.00003021
Iteration 49/1000 | Loss: 0.00003021
Iteration 50/1000 | Loss: 0.00003021
Iteration 51/1000 | Loss: 0.00003021
Iteration 52/1000 | Loss: 0.00003021
Iteration 53/1000 | Loss: 0.00003020
Iteration 54/1000 | Loss: 0.00003020
Iteration 55/1000 | Loss: 0.00003020
Iteration 56/1000 | Loss: 0.00003020
Iteration 57/1000 | Loss: 0.00003020
Iteration 58/1000 | Loss: 0.00003020
Iteration 59/1000 | Loss: 0.00003020
Iteration 60/1000 | Loss: 0.00003019
Iteration 61/1000 | Loss: 0.00003019
Iteration 62/1000 | Loss: 0.00003019
Iteration 63/1000 | Loss: 0.00003019
Iteration 64/1000 | Loss: 0.00003019
Iteration 65/1000 | Loss: 0.00003018
Iteration 66/1000 | Loss: 0.00003018
Iteration 67/1000 | Loss: 0.00003018
Iteration 68/1000 | Loss: 0.00003018
Iteration 69/1000 | Loss: 0.00003018
Iteration 70/1000 | Loss: 0.00003018
Iteration 71/1000 | Loss: 0.00003017
Iteration 72/1000 | Loss: 0.00003017
Iteration 73/1000 | Loss: 0.00003017
Iteration 74/1000 | Loss: 0.00003017
Iteration 75/1000 | Loss: 0.00003017
Iteration 76/1000 | Loss: 0.00003017
Iteration 77/1000 | Loss: 0.00003017
Iteration 78/1000 | Loss: 0.00003017
Iteration 79/1000 | Loss: 0.00003017
Iteration 80/1000 | Loss: 0.00003017
Iteration 81/1000 | Loss: 0.00003017
Iteration 82/1000 | Loss: 0.00003017
Iteration 83/1000 | Loss: 0.00003016
Iteration 84/1000 | Loss: 0.00003016
Iteration 85/1000 | Loss: 0.00003016
Iteration 86/1000 | Loss: 0.00003016
Iteration 87/1000 | Loss: 0.00003016
Iteration 88/1000 | Loss: 0.00003016
Iteration 89/1000 | Loss: 0.00003016
Iteration 90/1000 | Loss: 0.00003016
Iteration 91/1000 | Loss: 0.00003016
Iteration 92/1000 | Loss: 0.00003016
Iteration 93/1000 | Loss: 0.00003016
Iteration 94/1000 | Loss: 0.00003016
Iteration 95/1000 | Loss: 0.00003016
Iteration 96/1000 | Loss: 0.00003016
Iteration 97/1000 | Loss: 0.00003016
Iteration 98/1000 | Loss: 0.00003016
Iteration 99/1000 | Loss: 0.00003016
Iteration 100/1000 | Loss: 0.00003016
Iteration 101/1000 | Loss: 0.00003016
Iteration 102/1000 | Loss: 0.00003016
Iteration 103/1000 | Loss: 0.00003016
Iteration 104/1000 | Loss: 0.00003016
Iteration 105/1000 | Loss: 0.00003016
Iteration 106/1000 | Loss: 0.00003016
Iteration 107/1000 | Loss: 0.00003016
Iteration 108/1000 | Loss: 0.00003016
Iteration 109/1000 | Loss: 0.00003016
Iteration 110/1000 | Loss: 0.00003016
Iteration 111/1000 | Loss: 0.00003016
Iteration 112/1000 | Loss: 0.00003016
Iteration 113/1000 | Loss: 0.00003016
Iteration 114/1000 | Loss: 0.00003016
Iteration 115/1000 | Loss: 0.00003016
Iteration 116/1000 | Loss: 0.00003016
Iteration 117/1000 | Loss: 0.00003016
Iteration 118/1000 | Loss: 0.00003016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.015982838405762e-05, 3.015982838405762e-05, 3.015982838405762e-05, 3.015982838405762e-05, 3.015982838405762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.015982838405762e-05

Optimization complete. Final v2v error: 4.512857437133789 mm

Highest mean error: 5.721754550933838 mm for frame 145

Lowest mean error: 3.803389310836792 mm for frame 45

Saving results

Total time: 51.754502296447754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801342
Iteration 2/25 | Loss: 0.00147804
Iteration 3/25 | Loss: 0.00131200
Iteration 4/25 | Loss: 0.00130538
Iteration 5/25 | Loss: 0.00130538
Iteration 6/25 | Loss: 0.00130538
Iteration 7/25 | Loss: 0.00130538
Iteration 8/25 | Loss: 0.00130538
Iteration 9/25 | Loss: 0.00130538
Iteration 10/25 | Loss: 0.00130538
Iteration 11/25 | Loss: 0.00130538
Iteration 12/25 | Loss: 0.00130538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001305382582359016, 0.001305382582359016, 0.001305382582359016, 0.001305382582359016, 0.001305382582359016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001305382582359016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26324773
Iteration 2/25 | Loss: 0.00081355
Iteration 3/25 | Loss: 0.00081354
Iteration 4/25 | Loss: 0.00081354
Iteration 5/25 | Loss: 0.00081354
Iteration 6/25 | Loss: 0.00081353
Iteration 7/25 | Loss: 0.00081353
Iteration 8/25 | Loss: 0.00081353
Iteration 9/25 | Loss: 0.00081353
Iteration 10/25 | Loss: 0.00081353
Iteration 11/25 | Loss: 0.00081353
Iteration 12/25 | Loss: 0.00081353
Iteration 13/25 | Loss: 0.00081353
Iteration 14/25 | Loss: 0.00081353
Iteration 15/25 | Loss: 0.00081353
Iteration 16/25 | Loss: 0.00081353
Iteration 17/25 | Loss: 0.00081353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008135331445373595, 0.0008135331445373595, 0.0008135331445373595, 0.0008135331445373595, 0.0008135331445373595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008135331445373595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081353
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002166
Iteration 6/1000 | Loss: 0.00002095
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00001985
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001933
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001883
Iteration 13/1000 | Loss: 0.00001879
Iteration 14/1000 | Loss: 0.00001859
Iteration 15/1000 | Loss: 0.00001854
Iteration 16/1000 | Loss: 0.00001854
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001845
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001844
Iteration 30/1000 | Loss: 0.00001844
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001828
Iteration 34/1000 | Loss: 0.00001828
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001817
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001814
Iteration 46/1000 | Loss: 0.00001814
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001811
Iteration 51/1000 | Loss: 0.00001811
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001807
Iteration 61/1000 | Loss: 0.00001807
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001803
Iteration 75/1000 | Loss: 0.00001803
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001803
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001803
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.8021524738287553e-05, 1.8021524738287553e-05, 1.8021524738287553e-05, 1.8021524738287553e-05, 1.8021524738287553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8021524738287553e-05

Optimization complete. Final v2v error: 3.6006808280944824 mm

Highest mean error: 3.7388179302215576 mm for frame 212

Lowest mean error: 3.477226495742798 mm for frame 62

Saving results

Total time: 37.377716064453125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769557
Iteration 2/25 | Loss: 0.00190951
Iteration 3/25 | Loss: 0.00132733
Iteration 4/25 | Loss: 0.00128275
Iteration 5/25 | Loss: 0.00125547
Iteration 6/25 | Loss: 0.00124721
Iteration 7/25 | Loss: 0.00123234
Iteration 8/25 | Loss: 0.00121101
Iteration 9/25 | Loss: 0.00122980
Iteration 10/25 | Loss: 0.00120541
Iteration 11/25 | Loss: 0.00120755
Iteration 12/25 | Loss: 0.00121249
Iteration 13/25 | Loss: 0.00120288
Iteration 14/25 | Loss: 0.00120276
Iteration 15/25 | Loss: 0.00120276
Iteration 16/25 | Loss: 0.00120276
Iteration 17/25 | Loss: 0.00120276
Iteration 18/25 | Loss: 0.00120276
Iteration 19/25 | Loss: 0.00120275
Iteration 20/25 | Loss: 0.00120275
Iteration 21/25 | Loss: 0.00120275
Iteration 22/25 | Loss: 0.00120275
Iteration 23/25 | Loss: 0.00120275
Iteration 24/25 | Loss: 0.00120275
Iteration 25/25 | Loss: 0.00120275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41356432
Iteration 2/25 | Loss: 0.00112119
Iteration 3/25 | Loss: 0.00111802
Iteration 4/25 | Loss: 0.00111801
Iteration 5/25 | Loss: 0.00111801
Iteration 6/25 | Loss: 0.00108039
Iteration 7/25 | Loss: 0.00108038
Iteration 8/25 | Loss: 0.00108038
Iteration 9/25 | Loss: 0.00108038
Iteration 10/25 | Loss: 0.00108038
Iteration 11/25 | Loss: 0.00108038
Iteration 12/25 | Loss: 0.00108038
Iteration 13/25 | Loss: 0.00108038
Iteration 14/25 | Loss: 0.00108038
Iteration 15/25 | Loss: 0.00108038
Iteration 16/25 | Loss: 0.00108038
Iteration 17/25 | Loss: 0.00108038
Iteration 18/25 | Loss: 0.00108038
Iteration 19/25 | Loss: 0.00108038
Iteration 20/25 | Loss: 0.00108038
Iteration 21/25 | Loss: 0.00108038
Iteration 22/25 | Loss: 0.00108038
Iteration 23/25 | Loss: 0.00108038
Iteration 24/25 | Loss: 0.00108038
Iteration 25/25 | Loss: 0.00108038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108038
Iteration 2/1000 | Loss: 0.00002565
Iteration 3/1000 | Loss: 0.00015360
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00006381
Iteration 6/1000 | Loss: 0.00030977
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001207
Iteration 12/1000 | Loss: 0.00001202
Iteration 13/1000 | Loss: 0.00001200
Iteration 14/1000 | Loss: 0.00006930
Iteration 15/1000 | Loss: 0.00034267
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00013356
Iteration 20/1000 | Loss: 0.00001700
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001118
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001114
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00003701
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001103
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001100
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001095
Iteration 59/1000 | Loss: 0.00001095
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001093
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001092
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001091
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001091
Iteration 79/1000 | Loss: 0.00001091
Iteration 80/1000 | Loss: 0.00001091
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001087
Iteration 86/1000 | Loss: 0.00006391
Iteration 87/1000 | Loss: 0.00030872
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00002299
Iteration 90/1000 | Loss: 0.00003923
Iteration 91/1000 | Loss: 0.00002715
Iteration 92/1000 | Loss: 0.00001098
Iteration 93/1000 | Loss: 0.00001095
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001094
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001090
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001089
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001085
Iteration 123/1000 | Loss: 0.00003657
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001081
Iteration 126/1000 | Loss: 0.00001081
Iteration 127/1000 | Loss: 0.00001081
Iteration 128/1000 | Loss: 0.00001081
Iteration 129/1000 | Loss: 0.00001081
Iteration 130/1000 | Loss: 0.00001081
Iteration 131/1000 | Loss: 0.00001081
Iteration 132/1000 | Loss: 0.00001080
Iteration 133/1000 | Loss: 0.00001080
Iteration 134/1000 | Loss: 0.00001080
Iteration 135/1000 | Loss: 0.00001080
Iteration 136/1000 | Loss: 0.00001079
Iteration 137/1000 | Loss: 0.00001079
Iteration 138/1000 | Loss: 0.00001079
Iteration 139/1000 | Loss: 0.00001079
Iteration 140/1000 | Loss: 0.00001079
Iteration 141/1000 | Loss: 0.00001079
Iteration 142/1000 | Loss: 0.00001078
Iteration 143/1000 | Loss: 0.00001078
Iteration 144/1000 | Loss: 0.00001078
Iteration 145/1000 | Loss: 0.00001078
Iteration 146/1000 | Loss: 0.00001078
Iteration 147/1000 | Loss: 0.00001078
Iteration 148/1000 | Loss: 0.00001078
Iteration 149/1000 | Loss: 0.00001078
Iteration 150/1000 | Loss: 0.00001078
Iteration 151/1000 | Loss: 0.00001078
Iteration 152/1000 | Loss: 0.00001078
Iteration 153/1000 | Loss: 0.00001078
Iteration 154/1000 | Loss: 0.00001078
Iteration 155/1000 | Loss: 0.00001078
Iteration 156/1000 | Loss: 0.00001078
Iteration 157/1000 | Loss: 0.00001078
Iteration 158/1000 | Loss: 0.00001078
Iteration 159/1000 | Loss: 0.00001078
Iteration 160/1000 | Loss: 0.00001078
Iteration 161/1000 | Loss: 0.00001078
Iteration 162/1000 | Loss: 0.00001078
Iteration 163/1000 | Loss: 0.00001078
Iteration 164/1000 | Loss: 0.00001078
Iteration 165/1000 | Loss: 0.00001078
Iteration 166/1000 | Loss: 0.00001078
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001078
Iteration 169/1000 | Loss: 0.00001078
Iteration 170/1000 | Loss: 0.00001078
Iteration 171/1000 | Loss: 0.00001078
Iteration 172/1000 | Loss: 0.00001078
Iteration 173/1000 | Loss: 0.00001078
Iteration 174/1000 | Loss: 0.00001078
Iteration 175/1000 | Loss: 0.00001078
Iteration 176/1000 | Loss: 0.00001078
Iteration 177/1000 | Loss: 0.00001078
Iteration 178/1000 | Loss: 0.00001078
Iteration 179/1000 | Loss: 0.00001078
Iteration 180/1000 | Loss: 0.00001078
Iteration 181/1000 | Loss: 0.00001078
Iteration 182/1000 | Loss: 0.00001078
Iteration 183/1000 | Loss: 0.00001078
Iteration 184/1000 | Loss: 0.00001078
Iteration 185/1000 | Loss: 0.00001078
Iteration 186/1000 | Loss: 0.00001078
Iteration 187/1000 | Loss: 0.00001078
Iteration 188/1000 | Loss: 0.00001078
Iteration 189/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.0780013326439075e-05, 1.0780013326439075e-05, 1.0780013326439075e-05, 1.0780013326439075e-05, 1.0780013326439075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0780013326439075e-05

Optimization complete. Final v2v error: 2.8480074405670166 mm

Highest mean error: 3.1685218811035156 mm for frame 58

Lowest mean error: 2.6349148750305176 mm for frame 154

Saving results

Total time: 80.63183259963989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437065
Iteration 2/25 | Loss: 0.00134285
Iteration 3/25 | Loss: 0.00124552
Iteration 4/25 | Loss: 0.00123109
Iteration 5/25 | Loss: 0.00122704
Iteration 6/25 | Loss: 0.00122607
Iteration 7/25 | Loss: 0.00122607
Iteration 8/25 | Loss: 0.00122607
Iteration 9/25 | Loss: 0.00122607
Iteration 10/25 | Loss: 0.00122607
Iteration 11/25 | Loss: 0.00122607
Iteration 12/25 | Loss: 0.00122607
Iteration 13/25 | Loss: 0.00122607
Iteration 14/25 | Loss: 0.00122607
Iteration 15/25 | Loss: 0.00122607
Iteration 16/25 | Loss: 0.00122607
Iteration 17/25 | Loss: 0.00122607
Iteration 18/25 | Loss: 0.00122607
Iteration 19/25 | Loss: 0.00122607
Iteration 20/25 | Loss: 0.00122607
Iteration 21/25 | Loss: 0.00122607
Iteration 22/25 | Loss: 0.00122607
Iteration 23/25 | Loss: 0.00122607
Iteration 24/25 | Loss: 0.00122607
Iteration 25/25 | Loss: 0.00122607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33374691
Iteration 2/25 | Loss: 0.00099021
Iteration 3/25 | Loss: 0.00099019
Iteration 4/25 | Loss: 0.00099019
Iteration 5/25 | Loss: 0.00099019
Iteration 6/25 | Loss: 0.00099019
Iteration 7/25 | Loss: 0.00099019
Iteration 8/25 | Loss: 0.00099019
Iteration 9/25 | Loss: 0.00099019
Iteration 10/25 | Loss: 0.00099019
Iteration 11/25 | Loss: 0.00099019
Iteration 12/25 | Loss: 0.00099019
Iteration 13/25 | Loss: 0.00099019
Iteration 14/25 | Loss: 0.00099019
Iteration 15/25 | Loss: 0.00099019
Iteration 16/25 | Loss: 0.00099019
Iteration 17/25 | Loss: 0.00099019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009901871671900153, 0.0009901871671900153, 0.0009901871671900153, 0.0009901871671900153, 0.0009901871671900153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009901871671900153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099019
Iteration 2/1000 | Loss: 0.00003120
Iteration 3/1000 | Loss: 0.00001783
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001356
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001339
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001286
Iteration 18/1000 | Loss: 0.00001285
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001275
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001265
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001224
Iteration 123/1000 | Loss: 0.00001224
Iteration 124/1000 | Loss: 0.00001224
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001221
Iteration 131/1000 | Loss: 0.00001221
Iteration 132/1000 | Loss: 0.00001220
Iteration 133/1000 | Loss: 0.00001220
Iteration 134/1000 | Loss: 0.00001220
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001217
Iteration 144/1000 | Loss: 0.00001217
Iteration 145/1000 | Loss: 0.00001217
Iteration 146/1000 | Loss: 0.00001217
Iteration 147/1000 | Loss: 0.00001217
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001216
Iteration 150/1000 | Loss: 0.00001216
Iteration 151/1000 | Loss: 0.00001215
Iteration 152/1000 | Loss: 0.00001215
Iteration 153/1000 | Loss: 0.00001215
Iteration 154/1000 | Loss: 0.00001215
Iteration 155/1000 | Loss: 0.00001215
Iteration 156/1000 | Loss: 0.00001215
Iteration 157/1000 | Loss: 0.00001215
Iteration 158/1000 | Loss: 0.00001215
Iteration 159/1000 | Loss: 0.00001214
Iteration 160/1000 | Loss: 0.00001214
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001214
Iteration 164/1000 | Loss: 0.00001214
Iteration 165/1000 | Loss: 0.00001214
Iteration 166/1000 | Loss: 0.00001214
Iteration 167/1000 | Loss: 0.00001214
Iteration 168/1000 | Loss: 0.00001214
Iteration 169/1000 | Loss: 0.00001214
Iteration 170/1000 | Loss: 0.00001214
Iteration 171/1000 | Loss: 0.00001213
Iteration 172/1000 | Loss: 0.00001213
Iteration 173/1000 | Loss: 0.00001213
Iteration 174/1000 | Loss: 0.00001213
Iteration 175/1000 | Loss: 0.00001213
Iteration 176/1000 | Loss: 0.00001213
Iteration 177/1000 | Loss: 0.00001213
Iteration 178/1000 | Loss: 0.00001213
Iteration 179/1000 | Loss: 0.00001213
Iteration 180/1000 | Loss: 0.00001212
Iteration 181/1000 | Loss: 0.00001212
Iteration 182/1000 | Loss: 0.00001212
Iteration 183/1000 | Loss: 0.00001212
Iteration 184/1000 | Loss: 0.00001212
Iteration 185/1000 | Loss: 0.00001212
Iteration 186/1000 | Loss: 0.00001212
Iteration 187/1000 | Loss: 0.00001212
Iteration 188/1000 | Loss: 0.00001212
Iteration 189/1000 | Loss: 0.00001212
Iteration 190/1000 | Loss: 0.00001212
Iteration 191/1000 | Loss: 0.00001211
Iteration 192/1000 | Loss: 0.00001211
Iteration 193/1000 | Loss: 0.00001211
Iteration 194/1000 | Loss: 0.00001211
Iteration 195/1000 | Loss: 0.00001211
Iteration 196/1000 | Loss: 0.00001210
Iteration 197/1000 | Loss: 0.00001210
Iteration 198/1000 | Loss: 0.00001210
Iteration 199/1000 | Loss: 0.00001210
Iteration 200/1000 | Loss: 0.00001210
Iteration 201/1000 | Loss: 0.00001210
Iteration 202/1000 | Loss: 0.00001210
Iteration 203/1000 | Loss: 0.00001210
Iteration 204/1000 | Loss: 0.00001210
Iteration 205/1000 | Loss: 0.00001210
Iteration 206/1000 | Loss: 0.00001210
Iteration 207/1000 | Loss: 0.00001210
Iteration 208/1000 | Loss: 0.00001210
Iteration 209/1000 | Loss: 0.00001209
Iteration 210/1000 | Loss: 0.00001209
Iteration 211/1000 | Loss: 0.00001209
Iteration 212/1000 | Loss: 0.00001209
Iteration 213/1000 | Loss: 0.00001209
Iteration 214/1000 | Loss: 0.00001209
Iteration 215/1000 | Loss: 0.00001208
Iteration 216/1000 | Loss: 0.00001208
Iteration 217/1000 | Loss: 0.00001208
Iteration 218/1000 | Loss: 0.00001208
Iteration 219/1000 | Loss: 0.00001208
Iteration 220/1000 | Loss: 0.00001208
Iteration 221/1000 | Loss: 0.00001208
Iteration 222/1000 | Loss: 0.00001208
Iteration 223/1000 | Loss: 0.00001208
Iteration 224/1000 | Loss: 0.00001208
Iteration 225/1000 | Loss: 0.00001208
Iteration 226/1000 | Loss: 0.00001208
Iteration 227/1000 | Loss: 0.00001208
Iteration 228/1000 | Loss: 0.00001207
Iteration 229/1000 | Loss: 0.00001207
Iteration 230/1000 | Loss: 0.00001207
Iteration 231/1000 | Loss: 0.00001207
Iteration 232/1000 | Loss: 0.00001207
Iteration 233/1000 | Loss: 0.00001207
Iteration 234/1000 | Loss: 0.00001207
Iteration 235/1000 | Loss: 0.00001207
Iteration 236/1000 | Loss: 0.00001207
Iteration 237/1000 | Loss: 0.00001207
Iteration 238/1000 | Loss: 0.00001207
Iteration 239/1000 | Loss: 0.00001206
Iteration 240/1000 | Loss: 0.00001206
Iteration 241/1000 | Loss: 0.00001206
Iteration 242/1000 | Loss: 0.00001206
Iteration 243/1000 | Loss: 0.00001206
Iteration 244/1000 | Loss: 0.00001206
Iteration 245/1000 | Loss: 0.00001206
Iteration 246/1000 | Loss: 0.00001206
Iteration 247/1000 | Loss: 0.00001206
Iteration 248/1000 | Loss: 0.00001206
Iteration 249/1000 | Loss: 0.00001206
Iteration 250/1000 | Loss: 0.00001206
Iteration 251/1000 | Loss: 0.00001206
Iteration 252/1000 | Loss: 0.00001205
Iteration 253/1000 | Loss: 0.00001205
Iteration 254/1000 | Loss: 0.00001205
Iteration 255/1000 | Loss: 0.00001205
Iteration 256/1000 | Loss: 0.00001205
Iteration 257/1000 | Loss: 0.00001205
Iteration 258/1000 | Loss: 0.00001205
Iteration 259/1000 | Loss: 0.00001205
Iteration 260/1000 | Loss: 0.00001205
Iteration 261/1000 | Loss: 0.00001205
Iteration 262/1000 | Loss: 0.00001205
Iteration 263/1000 | Loss: 0.00001205
Iteration 264/1000 | Loss: 0.00001205
Iteration 265/1000 | Loss: 0.00001205
Iteration 266/1000 | Loss: 0.00001204
Iteration 267/1000 | Loss: 0.00001204
Iteration 268/1000 | Loss: 0.00001204
Iteration 269/1000 | Loss: 0.00001204
Iteration 270/1000 | Loss: 0.00001204
Iteration 271/1000 | Loss: 0.00001204
Iteration 272/1000 | Loss: 0.00001204
Iteration 273/1000 | Loss: 0.00001204
Iteration 274/1000 | Loss: 0.00001204
Iteration 275/1000 | Loss: 0.00001204
Iteration 276/1000 | Loss: 0.00001204
Iteration 277/1000 | Loss: 0.00001204
Iteration 278/1000 | Loss: 0.00001204
Iteration 279/1000 | Loss: 0.00001204
Iteration 280/1000 | Loss: 0.00001203
Iteration 281/1000 | Loss: 0.00001203
Iteration 282/1000 | Loss: 0.00001203
Iteration 283/1000 | Loss: 0.00001203
Iteration 284/1000 | Loss: 0.00001203
Iteration 285/1000 | Loss: 0.00001203
Iteration 286/1000 | Loss: 0.00001203
Iteration 287/1000 | Loss: 0.00001203
Iteration 288/1000 | Loss: 0.00001203
Iteration 289/1000 | Loss: 0.00001203
Iteration 290/1000 | Loss: 0.00001203
Iteration 291/1000 | Loss: 0.00001203
Iteration 292/1000 | Loss: 0.00001203
Iteration 293/1000 | Loss: 0.00001203
Iteration 294/1000 | Loss: 0.00001203
Iteration 295/1000 | Loss: 0.00001203
Iteration 296/1000 | Loss: 0.00001202
Iteration 297/1000 | Loss: 0.00001202
Iteration 298/1000 | Loss: 0.00001202
Iteration 299/1000 | Loss: 0.00001202
Iteration 300/1000 | Loss: 0.00001202
Iteration 301/1000 | Loss: 0.00001202
Iteration 302/1000 | Loss: 0.00001202
Iteration 303/1000 | Loss: 0.00001202
Iteration 304/1000 | Loss: 0.00001202
Iteration 305/1000 | Loss: 0.00001202
Iteration 306/1000 | Loss: 0.00001202
Iteration 307/1000 | Loss: 0.00001202
Iteration 308/1000 | Loss: 0.00001202
Iteration 309/1000 | Loss: 0.00001202
Iteration 310/1000 | Loss: 0.00001202
Iteration 311/1000 | Loss: 0.00001202
Iteration 312/1000 | Loss: 0.00001202
Iteration 313/1000 | Loss: 0.00001202
Iteration 314/1000 | Loss: 0.00001202
Iteration 315/1000 | Loss: 0.00001202
Iteration 316/1000 | Loss: 0.00001202
Iteration 317/1000 | Loss: 0.00001202
Iteration 318/1000 | Loss: 0.00001202
Iteration 319/1000 | Loss: 0.00001202
Iteration 320/1000 | Loss: 0.00001202
Iteration 321/1000 | Loss: 0.00001202
Iteration 322/1000 | Loss: 0.00001202
Iteration 323/1000 | Loss: 0.00001202
Iteration 324/1000 | Loss: 0.00001202
Iteration 325/1000 | Loss: 0.00001202
Iteration 326/1000 | Loss: 0.00001202
Iteration 327/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [1.2018751476716716e-05, 1.2018751476716716e-05, 1.2018751476716716e-05, 1.2018751476716716e-05, 1.2018751476716716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2018751476716716e-05

Optimization complete. Final v2v error: 2.9667630195617676 mm

Highest mean error: 3.731025218963623 mm for frame 62

Lowest mean error: 2.5666913986206055 mm for frame 29

Saving results

Total time: 46.63508439064026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963926
Iteration 2/25 | Loss: 0.00332622
Iteration 3/25 | Loss: 0.00213470
Iteration 4/25 | Loss: 0.00210760
Iteration 5/25 | Loss: 0.00195234
Iteration 6/25 | Loss: 0.00175441
Iteration 7/25 | Loss: 0.00168858
Iteration 8/25 | Loss: 0.00165388
Iteration 9/25 | Loss: 0.00160623
Iteration 10/25 | Loss: 0.00158911
Iteration 11/25 | Loss: 0.00155439
Iteration 12/25 | Loss: 0.00154851
Iteration 13/25 | Loss: 0.00154105
Iteration 14/25 | Loss: 0.00153346
Iteration 15/25 | Loss: 0.00153154
Iteration 16/25 | Loss: 0.00153539
Iteration 17/25 | Loss: 0.00153368
Iteration 18/25 | Loss: 0.00153105
Iteration 19/25 | Loss: 0.00153019
Iteration 20/25 | Loss: 0.00153410
Iteration 21/25 | Loss: 0.00153472
Iteration 22/25 | Loss: 0.00153038
Iteration 23/25 | Loss: 0.00152798
Iteration 24/25 | Loss: 0.00152680
Iteration 25/25 | Loss: 0.00152661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33427632
Iteration 2/25 | Loss: 0.00362758
Iteration 3/25 | Loss: 0.00294123
Iteration 4/25 | Loss: 0.00294123
Iteration 5/25 | Loss: 0.00294123
Iteration 6/25 | Loss: 0.00294123
Iteration 7/25 | Loss: 0.00294122
Iteration 8/25 | Loss: 0.00294122
Iteration 9/25 | Loss: 0.00294122
Iteration 10/25 | Loss: 0.00294122
Iteration 11/25 | Loss: 0.00294122
Iteration 12/25 | Loss: 0.00294122
Iteration 13/25 | Loss: 0.00294122
Iteration 14/25 | Loss: 0.00294122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0029412242583930492, 0.0029412242583930492, 0.0029412242583930492, 0.0029412242583930492, 0.0029412242583930492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029412242583930492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294122
Iteration 2/1000 | Loss: 0.00161132
Iteration 3/1000 | Loss: 0.00155524
Iteration 4/1000 | Loss: 0.00192267
Iteration 5/1000 | Loss: 0.00077445
Iteration 6/1000 | Loss: 0.00215583
Iteration 7/1000 | Loss: 0.00108443
Iteration 8/1000 | Loss: 0.00032548
Iteration 9/1000 | Loss: 0.00025162
Iteration 10/1000 | Loss: 0.00049114
Iteration 11/1000 | Loss: 0.00022534
Iteration 12/1000 | Loss: 0.00089130
Iteration 13/1000 | Loss: 0.00019143
Iteration 14/1000 | Loss: 0.00225693
Iteration 15/1000 | Loss: 0.00019643
Iteration 16/1000 | Loss: 0.00070477
Iteration 17/1000 | Loss: 0.00101916
Iteration 18/1000 | Loss: 0.00058325
Iteration 19/1000 | Loss: 0.00032838
Iteration 20/1000 | Loss: 0.00021995
Iteration 21/1000 | Loss: 0.00009917
Iteration 22/1000 | Loss: 0.00038468
Iteration 23/1000 | Loss: 0.00087603
Iteration 24/1000 | Loss: 0.00014671
Iteration 25/1000 | Loss: 0.00055106
Iteration 26/1000 | Loss: 0.00072206
Iteration 27/1000 | Loss: 0.00013928
Iteration 28/1000 | Loss: 0.00017625
Iteration 29/1000 | Loss: 0.00044576
Iteration 30/1000 | Loss: 0.00055918
Iteration 31/1000 | Loss: 0.00036720
Iteration 32/1000 | Loss: 0.00039102
Iteration 33/1000 | Loss: 0.00031782
Iteration 34/1000 | Loss: 0.00021207
Iteration 35/1000 | Loss: 0.00018486
Iteration 36/1000 | Loss: 0.00025600
Iteration 37/1000 | Loss: 0.00024624
Iteration 38/1000 | Loss: 0.00006684
Iteration 39/1000 | Loss: 0.00017964
Iteration 40/1000 | Loss: 0.00014398
Iteration 41/1000 | Loss: 0.00006582
Iteration 42/1000 | Loss: 0.00006372
Iteration 43/1000 | Loss: 0.00024089
Iteration 44/1000 | Loss: 0.00027654
Iteration 45/1000 | Loss: 0.00008382
Iteration 46/1000 | Loss: 0.00039398
Iteration 47/1000 | Loss: 0.00006453
Iteration 48/1000 | Loss: 0.00006163
Iteration 49/1000 | Loss: 0.00006037
Iteration 50/1000 | Loss: 0.00005919
Iteration 51/1000 | Loss: 0.00005826
Iteration 52/1000 | Loss: 0.00043172
Iteration 53/1000 | Loss: 0.00542620
Iteration 54/1000 | Loss: 0.00421854
Iteration 55/1000 | Loss: 0.00461275
Iteration 56/1000 | Loss: 0.00255901
Iteration 57/1000 | Loss: 0.00253450
Iteration 58/1000 | Loss: 0.00314940
Iteration 59/1000 | Loss: 0.00385306
Iteration 60/1000 | Loss: 0.00243040
Iteration 61/1000 | Loss: 0.00286354
Iteration 62/1000 | Loss: 0.00422426
Iteration 63/1000 | Loss: 0.00107629
Iteration 64/1000 | Loss: 0.00067944
Iteration 65/1000 | Loss: 0.00052637
Iteration 66/1000 | Loss: 0.00028525
Iteration 67/1000 | Loss: 0.00058715
Iteration 68/1000 | Loss: 0.00016071
Iteration 69/1000 | Loss: 0.00009582
Iteration 70/1000 | Loss: 0.00006894
Iteration 71/1000 | Loss: 0.00011222
Iteration 72/1000 | Loss: 0.00010976
Iteration 73/1000 | Loss: 0.00022447
Iteration 74/1000 | Loss: 0.00007438
Iteration 75/1000 | Loss: 0.00017636
Iteration 76/1000 | Loss: 0.00005387
Iteration 77/1000 | Loss: 0.00006829
Iteration 78/1000 | Loss: 0.00030551
Iteration 79/1000 | Loss: 0.00035701
Iteration 80/1000 | Loss: 0.00015473
Iteration 81/1000 | Loss: 0.00026840
Iteration 82/1000 | Loss: 0.00015602
Iteration 83/1000 | Loss: 0.00032586
Iteration 84/1000 | Loss: 0.00020024
Iteration 85/1000 | Loss: 0.00017082
Iteration 86/1000 | Loss: 0.00024361
Iteration 87/1000 | Loss: 0.00016650
Iteration 88/1000 | Loss: 0.00006719
Iteration 89/1000 | Loss: 0.00024054
Iteration 90/1000 | Loss: 0.00033345
Iteration 91/1000 | Loss: 0.00006979
Iteration 92/1000 | Loss: 0.00005510
Iteration 93/1000 | Loss: 0.00005617
Iteration 94/1000 | Loss: 0.00004796
Iteration 95/1000 | Loss: 0.00004562
Iteration 96/1000 | Loss: 0.00003564
Iteration 97/1000 | Loss: 0.00035199
Iteration 98/1000 | Loss: 0.00003500
Iteration 99/1000 | Loss: 0.00005799
Iteration 100/1000 | Loss: 0.00011449
Iteration 101/1000 | Loss: 0.00002955
Iteration 102/1000 | Loss: 0.00003402
Iteration 103/1000 | Loss: 0.00003279
Iteration 104/1000 | Loss: 0.00032732
Iteration 105/1000 | Loss: 0.00007787
Iteration 106/1000 | Loss: 0.00003982
Iteration 107/1000 | Loss: 0.00003333
Iteration 108/1000 | Loss: 0.00016111
Iteration 109/1000 | Loss: 0.00019007
Iteration 110/1000 | Loss: 0.00004422
Iteration 111/1000 | Loss: 0.00003808
Iteration 112/1000 | Loss: 0.00002694
Iteration 113/1000 | Loss: 0.00004233
Iteration 114/1000 | Loss: 0.00011523
Iteration 115/1000 | Loss: 0.00011779
Iteration 116/1000 | Loss: 0.00026601
Iteration 117/1000 | Loss: 0.00004793
Iteration 118/1000 | Loss: 0.00030903
Iteration 119/1000 | Loss: 0.00004948
Iteration 120/1000 | Loss: 0.00004413
Iteration 121/1000 | Loss: 0.00005226
Iteration 122/1000 | Loss: 0.00002712
Iteration 123/1000 | Loss: 0.00003503
Iteration 124/1000 | Loss: 0.00012860
Iteration 125/1000 | Loss: 0.00004047
Iteration 126/1000 | Loss: 0.00003047
Iteration 127/1000 | Loss: 0.00003118
Iteration 128/1000 | Loss: 0.00005809
Iteration 129/1000 | Loss: 0.00003857
Iteration 130/1000 | Loss: 0.00005510
Iteration 131/1000 | Loss: 0.00002755
Iteration 132/1000 | Loss: 0.00007936
Iteration 133/1000 | Loss: 0.00002790
Iteration 134/1000 | Loss: 0.00003730
Iteration 135/1000 | Loss: 0.00003069
Iteration 136/1000 | Loss: 0.00002329
Iteration 137/1000 | Loss: 0.00002282
Iteration 138/1000 | Loss: 0.00002242
Iteration 139/1000 | Loss: 0.00002210
Iteration 140/1000 | Loss: 0.00009366
Iteration 141/1000 | Loss: 0.00038150
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002176
Iteration 144/1000 | Loss: 0.00002813
Iteration 145/1000 | Loss: 0.00002202
Iteration 146/1000 | Loss: 0.00002171
Iteration 147/1000 | Loss: 0.00002148
Iteration 148/1000 | Loss: 0.00002130
Iteration 149/1000 | Loss: 0.00002125
Iteration 150/1000 | Loss: 0.00002125
Iteration 151/1000 | Loss: 0.00002122
Iteration 152/1000 | Loss: 0.00002122
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002121
Iteration 155/1000 | Loss: 0.00002121
Iteration 156/1000 | Loss: 0.00002121
Iteration 157/1000 | Loss: 0.00002120
Iteration 158/1000 | Loss: 0.00002120
Iteration 159/1000 | Loss: 0.00002120
Iteration 160/1000 | Loss: 0.00002114
Iteration 161/1000 | Loss: 0.00002114
Iteration 162/1000 | Loss: 0.00002113
Iteration 163/1000 | Loss: 0.00002113
Iteration 164/1000 | Loss: 0.00002113
Iteration 165/1000 | Loss: 0.00002113
Iteration 166/1000 | Loss: 0.00002113
Iteration 167/1000 | Loss: 0.00002113
Iteration 168/1000 | Loss: 0.00002113
Iteration 169/1000 | Loss: 0.00002113
Iteration 170/1000 | Loss: 0.00002111
Iteration 171/1000 | Loss: 0.00002110
Iteration 172/1000 | Loss: 0.00002110
Iteration 173/1000 | Loss: 0.00002109
Iteration 174/1000 | Loss: 0.00002109
Iteration 175/1000 | Loss: 0.00002109
Iteration 176/1000 | Loss: 0.00002108
Iteration 177/1000 | Loss: 0.00002108
Iteration 178/1000 | Loss: 0.00002108
Iteration 179/1000 | Loss: 0.00002108
Iteration 180/1000 | Loss: 0.00002107
Iteration 181/1000 | Loss: 0.00002107
Iteration 182/1000 | Loss: 0.00002106
Iteration 183/1000 | Loss: 0.00002106
Iteration 184/1000 | Loss: 0.00002105
Iteration 185/1000 | Loss: 0.00002105
Iteration 186/1000 | Loss: 0.00002105
Iteration 187/1000 | Loss: 0.00002105
Iteration 188/1000 | Loss: 0.00002105
Iteration 189/1000 | Loss: 0.00002105
Iteration 190/1000 | Loss: 0.00002104
Iteration 191/1000 | Loss: 0.00002104
Iteration 192/1000 | Loss: 0.00002104
Iteration 193/1000 | Loss: 0.00002104
Iteration 194/1000 | Loss: 0.00002104
Iteration 195/1000 | Loss: 0.00002104
Iteration 196/1000 | Loss: 0.00002104
Iteration 197/1000 | Loss: 0.00002104
Iteration 198/1000 | Loss: 0.00002104
Iteration 199/1000 | Loss: 0.00002104
Iteration 200/1000 | Loss: 0.00002104
Iteration 201/1000 | Loss: 0.00002104
Iteration 202/1000 | Loss: 0.00002103
Iteration 203/1000 | Loss: 0.00002103
Iteration 204/1000 | Loss: 0.00002103
Iteration 205/1000 | Loss: 0.00002103
Iteration 206/1000 | Loss: 0.00002103
Iteration 207/1000 | Loss: 0.00002103
Iteration 208/1000 | Loss: 0.00002103
Iteration 209/1000 | Loss: 0.00002103
Iteration 210/1000 | Loss: 0.00002102
Iteration 211/1000 | Loss: 0.00002102
Iteration 212/1000 | Loss: 0.00002102
Iteration 213/1000 | Loss: 0.00002102
Iteration 214/1000 | Loss: 0.00002102
Iteration 215/1000 | Loss: 0.00002102
Iteration 216/1000 | Loss: 0.00002102
Iteration 217/1000 | Loss: 0.00002102
Iteration 218/1000 | Loss: 0.00002102
Iteration 219/1000 | Loss: 0.00002102
Iteration 220/1000 | Loss: 0.00002101
Iteration 221/1000 | Loss: 0.00002101
Iteration 222/1000 | Loss: 0.00002101
Iteration 223/1000 | Loss: 0.00002101
Iteration 224/1000 | Loss: 0.00002100
Iteration 225/1000 | Loss: 0.00002100
Iteration 226/1000 | Loss: 0.00002100
Iteration 227/1000 | Loss: 0.00002100
Iteration 228/1000 | Loss: 0.00002099
Iteration 229/1000 | Loss: 0.00011102
Iteration 230/1000 | Loss: 0.00002135
Iteration 231/1000 | Loss: 0.00002108
Iteration 232/1000 | Loss: 0.00002473
Iteration 233/1000 | Loss: 0.00009273
Iteration 234/1000 | Loss: 0.00018259
Iteration 235/1000 | Loss: 0.00002266
Iteration 236/1000 | Loss: 0.00009039
Iteration 237/1000 | Loss: 0.00009039
Iteration 238/1000 | Loss: 0.00009543
Iteration 239/1000 | Loss: 0.00009635
Iteration 240/1000 | Loss: 0.00029280
Iteration 241/1000 | Loss: 0.00005087
Iteration 242/1000 | Loss: 0.00002133
Iteration 243/1000 | Loss: 0.00002101
Iteration 244/1000 | Loss: 0.00002099
Iteration 245/1000 | Loss: 0.00002098
Iteration 246/1000 | Loss: 0.00002097
Iteration 247/1000 | Loss: 0.00002097
Iteration 248/1000 | Loss: 0.00002096
Iteration 249/1000 | Loss: 0.00002096
Iteration 250/1000 | Loss: 0.00002096
Iteration 251/1000 | Loss: 0.00002096
Iteration 252/1000 | Loss: 0.00002095
Iteration 253/1000 | Loss: 0.00002095
Iteration 254/1000 | Loss: 0.00002095
Iteration 255/1000 | Loss: 0.00002095
Iteration 256/1000 | Loss: 0.00002095
Iteration 257/1000 | Loss: 0.00002094
Iteration 258/1000 | Loss: 0.00002094
Iteration 259/1000 | Loss: 0.00002094
Iteration 260/1000 | Loss: 0.00002093
Iteration 261/1000 | Loss: 0.00002093
Iteration 262/1000 | Loss: 0.00010593
Iteration 263/1000 | Loss: 0.00002102
Iteration 264/1000 | Loss: 0.00002095
Iteration 265/1000 | Loss: 0.00002093
Iteration 266/1000 | Loss: 0.00002093
Iteration 267/1000 | Loss: 0.00002092
Iteration 268/1000 | Loss: 0.00002091
Iteration 269/1000 | Loss: 0.00002091
Iteration 270/1000 | Loss: 0.00002091
Iteration 271/1000 | Loss: 0.00002091
Iteration 272/1000 | Loss: 0.00002090
Iteration 273/1000 | Loss: 0.00002090
Iteration 274/1000 | Loss: 0.00002090
Iteration 275/1000 | Loss: 0.00002090
Iteration 276/1000 | Loss: 0.00002089
Iteration 277/1000 | Loss: 0.00002089
Iteration 278/1000 | Loss: 0.00002089
Iteration 279/1000 | Loss: 0.00002089
Iteration 280/1000 | Loss: 0.00002089
Iteration 281/1000 | Loss: 0.00002089
Iteration 282/1000 | Loss: 0.00002089
Iteration 283/1000 | Loss: 0.00002088
Iteration 284/1000 | Loss: 0.00002088
Iteration 285/1000 | Loss: 0.00002088
Iteration 286/1000 | Loss: 0.00002088
Iteration 287/1000 | Loss: 0.00002088
Iteration 288/1000 | Loss: 0.00002088
Iteration 289/1000 | Loss: 0.00002088
Iteration 290/1000 | Loss: 0.00002088
Iteration 291/1000 | Loss: 0.00002088
Iteration 292/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.0883762772427872e-05, 2.0883762772427872e-05, 2.0883762772427872e-05, 2.0883762772427872e-05, 2.0883762772427872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0883762772427872e-05

Optimization complete. Final v2v error: 3.6996586322784424 mm

Highest mean error: 5.326514720916748 mm for frame 0

Lowest mean error: 3.3450000286102295 mm for frame 127

Saving results

Total time: 297.7591609954834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980861
Iteration 2/25 | Loss: 0.00402692
Iteration 3/25 | Loss: 0.00230146
Iteration 4/25 | Loss: 0.00199923
Iteration 5/25 | Loss: 0.00194746
Iteration 6/25 | Loss: 0.00202179
Iteration 7/25 | Loss: 0.00187923
Iteration 8/25 | Loss: 0.00174433
Iteration 9/25 | Loss: 0.00169504
Iteration 10/25 | Loss: 0.00167038
Iteration 11/25 | Loss: 0.00164783
Iteration 12/25 | Loss: 0.00163034
Iteration 13/25 | Loss: 0.00160583
Iteration 14/25 | Loss: 0.00159307
Iteration 15/25 | Loss: 0.00157837
Iteration 16/25 | Loss: 0.00157059
Iteration 17/25 | Loss: 0.00155163
Iteration 18/25 | Loss: 0.00155357
Iteration 19/25 | Loss: 0.00154493
Iteration 20/25 | Loss: 0.00154255
Iteration 21/25 | Loss: 0.00154479
Iteration 22/25 | Loss: 0.00154466
Iteration 23/25 | Loss: 0.00154354
Iteration 24/25 | Loss: 0.00154363
Iteration 25/25 | Loss: 0.00154355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35369503
Iteration 2/25 | Loss: 0.00589549
Iteration 3/25 | Loss: 0.00381632
Iteration 4/25 | Loss: 0.00381632
Iteration 5/25 | Loss: 0.00381632
Iteration 6/25 | Loss: 0.00381632
Iteration 7/25 | Loss: 0.00381632
Iteration 8/25 | Loss: 0.00381631
Iteration 9/25 | Loss: 0.00381631
Iteration 10/25 | Loss: 0.00381631
Iteration 11/25 | Loss: 0.00381631
Iteration 12/25 | Loss: 0.00381631
Iteration 13/25 | Loss: 0.00381631
Iteration 14/25 | Loss: 0.00381631
Iteration 15/25 | Loss: 0.00381631
Iteration 16/25 | Loss: 0.00381631
Iteration 17/25 | Loss: 0.00381631
Iteration 18/25 | Loss: 0.00381631
Iteration 19/25 | Loss: 0.00381631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003816313575953245, 0.003816313575953245, 0.003816313575953245, 0.003816313575953245, 0.003816313575953245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003816313575953245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00381631
Iteration 2/1000 | Loss: 0.00327923
Iteration 3/1000 | Loss: 0.00049533
Iteration 4/1000 | Loss: 0.00047490
Iteration 5/1000 | Loss: 0.00059750
Iteration 6/1000 | Loss: 0.00035227
Iteration 7/1000 | Loss: 0.00051880
Iteration 8/1000 | Loss: 0.00050002
Iteration 9/1000 | Loss: 0.00024398
Iteration 10/1000 | Loss: 0.00027652
Iteration 11/1000 | Loss: 0.00046889
Iteration 12/1000 | Loss: 0.00044278
Iteration 13/1000 | Loss: 0.00045617
Iteration 14/1000 | Loss: 0.00108207
Iteration 15/1000 | Loss: 0.00133491
Iteration 16/1000 | Loss: 0.00192558
Iteration 17/1000 | Loss: 0.00117707
Iteration 18/1000 | Loss: 0.00042964
Iteration 19/1000 | Loss: 0.00030614
Iteration 20/1000 | Loss: 0.00024132
Iteration 21/1000 | Loss: 0.00023181
Iteration 22/1000 | Loss: 0.00053497
Iteration 23/1000 | Loss: 0.00067023
Iteration 24/1000 | Loss: 0.00060607
Iteration 25/1000 | Loss: 0.00064332
Iteration 26/1000 | Loss: 0.00040605
Iteration 27/1000 | Loss: 0.00020200
Iteration 28/1000 | Loss: 0.00022108
Iteration 29/1000 | Loss: 0.00037775
Iteration 30/1000 | Loss: 0.00041974
Iteration 31/1000 | Loss: 0.00026599
Iteration 32/1000 | Loss: 0.00027606
Iteration 33/1000 | Loss: 0.00038156
Iteration 34/1000 | Loss: 0.00019324
Iteration 35/1000 | Loss: 0.00026017
Iteration 36/1000 | Loss: 0.00019860
Iteration 37/1000 | Loss: 0.00019467
Iteration 38/1000 | Loss: 0.00027359
Iteration 39/1000 | Loss: 0.00017549
Iteration 40/1000 | Loss: 0.00021542
Iteration 41/1000 | Loss: 0.00021554
Iteration 42/1000 | Loss: 0.00022700
Iteration 43/1000 | Loss: 0.00018302
Iteration 44/1000 | Loss: 0.00022299
Iteration 45/1000 | Loss: 0.00022008
Iteration 46/1000 | Loss: 0.00022028
Iteration 47/1000 | Loss: 0.00022501
Iteration 48/1000 | Loss: 0.00026582
Iteration 49/1000 | Loss: 0.00020819
Iteration 50/1000 | Loss: 0.00015408
Iteration 51/1000 | Loss: 0.00018318
Iteration 52/1000 | Loss: 0.00021582
Iteration 53/1000 | Loss: 0.00019306
Iteration 54/1000 | Loss: 0.00021278
Iteration 55/1000 | Loss: 0.00089819
Iteration 56/1000 | Loss: 0.00059090
Iteration 57/1000 | Loss: 0.00031828
Iteration 58/1000 | Loss: 0.00019184
Iteration 59/1000 | Loss: 0.00014006
Iteration 60/1000 | Loss: 0.00014901
Iteration 61/1000 | Loss: 0.00014403
Iteration 62/1000 | Loss: 0.00014234
Iteration 63/1000 | Loss: 0.00014105
Iteration 64/1000 | Loss: 0.00014002
Iteration 65/1000 | Loss: 0.00046443
Iteration 66/1000 | Loss: 0.00032875
Iteration 67/1000 | Loss: 0.00012207
Iteration 68/1000 | Loss: 0.00044246
Iteration 69/1000 | Loss: 0.00029355
Iteration 70/1000 | Loss: 0.00070486
Iteration 71/1000 | Loss: 0.00056260
Iteration 72/1000 | Loss: 0.00047638
Iteration 73/1000 | Loss: 0.00015497
Iteration 74/1000 | Loss: 0.00014399
Iteration 75/1000 | Loss: 0.00014232
Iteration 76/1000 | Loss: 0.00012769
Iteration 77/1000 | Loss: 0.00016282
Iteration 78/1000 | Loss: 0.00013824
Iteration 79/1000 | Loss: 0.00014223
Iteration 80/1000 | Loss: 0.00013056
Iteration 81/1000 | Loss: 0.00013609
Iteration 82/1000 | Loss: 0.00012664
Iteration 83/1000 | Loss: 0.00049627
Iteration 84/1000 | Loss: 0.00018806
Iteration 85/1000 | Loss: 0.00014597
Iteration 86/1000 | Loss: 0.00014345
Iteration 87/1000 | Loss: 0.00049748
Iteration 88/1000 | Loss: 0.00014328
Iteration 89/1000 | Loss: 0.00017247
Iteration 90/1000 | Loss: 0.00016454
Iteration 91/1000 | Loss: 0.00014085
Iteration 92/1000 | Loss: 0.00011254
Iteration 93/1000 | Loss: 0.00043509
Iteration 94/1000 | Loss: 0.00011115
Iteration 95/1000 | Loss: 0.00010805
Iteration 96/1000 | Loss: 0.00010628
Iteration 97/1000 | Loss: 0.00010463
Iteration 98/1000 | Loss: 0.00010366
Iteration 99/1000 | Loss: 0.00010281
Iteration 100/1000 | Loss: 0.00011172
Iteration 101/1000 | Loss: 0.00010444
Iteration 102/1000 | Loss: 0.00010282
Iteration 103/1000 | Loss: 0.00010209
Iteration 104/1000 | Loss: 0.00010157
Iteration 105/1000 | Loss: 0.00010095
Iteration 106/1000 | Loss: 0.00010040
Iteration 107/1000 | Loss: 0.00016502
Iteration 108/1000 | Loss: 0.00053213
Iteration 109/1000 | Loss: 0.00040308
Iteration 110/1000 | Loss: 0.00028263
Iteration 111/1000 | Loss: 0.00033984
Iteration 112/1000 | Loss: 0.00034557
Iteration 113/1000 | Loss: 0.00010416
Iteration 114/1000 | Loss: 0.00009945
Iteration 115/1000 | Loss: 0.00009624
Iteration 116/1000 | Loss: 0.00009414
Iteration 117/1000 | Loss: 0.00009282
Iteration 118/1000 | Loss: 0.00009205
Iteration 119/1000 | Loss: 0.00009132
Iteration 120/1000 | Loss: 0.00009085
Iteration 121/1000 | Loss: 0.00009031
Iteration 122/1000 | Loss: 0.00009005
Iteration 123/1000 | Loss: 0.00008966
Iteration 124/1000 | Loss: 0.00008931
Iteration 125/1000 | Loss: 0.00008904
Iteration 126/1000 | Loss: 0.00008879
Iteration 127/1000 | Loss: 0.00008855
Iteration 128/1000 | Loss: 0.00008829
Iteration 129/1000 | Loss: 0.00008810
Iteration 130/1000 | Loss: 0.00008790
Iteration 131/1000 | Loss: 0.00017683
Iteration 132/1000 | Loss: 0.00009254
Iteration 133/1000 | Loss: 0.00008961
Iteration 134/1000 | Loss: 0.00008821
Iteration 135/1000 | Loss: 0.00008699
Iteration 136/1000 | Loss: 0.00008632
Iteration 137/1000 | Loss: 0.00017711
Iteration 138/1000 | Loss: 0.00009364
Iteration 139/1000 | Loss: 0.00017274
Iteration 140/1000 | Loss: 0.00012643
Iteration 141/1000 | Loss: 0.00009065
Iteration 142/1000 | Loss: 0.00027802
Iteration 143/1000 | Loss: 0.00021890
Iteration 144/1000 | Loss: 0.00029340
Iteration 145/1000 | Loss: 0.00008922
Iteration 146/1000 | Loss: 0.00008594
Iteration 147/1000 | Loss: 0.00008474
Iteration 148/1000 | Loss: 0.00013946
Iteration 149/1000 | Loss: 0.00008337
Iteration 150/1000 | Loss: 0.00012275
Iteration 151/1000 | Loss: 0.00017401
Iteration 152/1000 | Loss: 0.00028141
Iteration 153/1000 | Loss: 0.00018711
Iteration 154/1000 | Loss: 0.00013602
Iteration 155/1000 | Loss: 0.00038003
Iteration 156/1000 | Loss: 0.00009307
Iteration 157/1000 | Loss: 0.00008825
Iteration 158/1000 | Loss: 0.00014420
Iteration 159/1000 | Loss: 0.00017111
Iteration 160/1000 | Loss: 0.00008474
Iteration 161/1000 | Loss: 0.00008197
Iteration 162/1000 | Loss: 0.00023296
Iteration 163/1000 | Loss: 0.00017397
Iteration 164/1000 | Loss: 0.00012341
Iteration 165/1000 | Loss: 0.00008756
Iteration 166/1000 | Loss: 0.00007856
Iteration 167/1000 | Loss: 0.00007706
Iteration 168/1000 | Loss: 0.00029106
Iteration 169/1000 | Loss: 0.00025272
Iteration 170/1000 | Loss: 0.00056212
Iteration 171/1000 | Loss: 0.00007674
Iteration 172/1000 | Loss: 0.00007396
Iteration 173/1000 | Loss: 0.00007319
Iteration 174/1000 | Loss: 0.00029243
Iteration 175/1000 | Loss: 0.00027910
Iteration 176/1000 | Loss: 0.00039739
Iteration 177/1000 | Loss: 0.00047007
Iteration 178/1000 | Loss: 0.00032375
Iteration 179/1000 | Loss: 0.00009139
Iteration 180/1000 | Loss: 0.00013329
Iteration 181/1000 | Loss: 0.00009421
Iteration 182/1000 | Loss: 0.00022463
Iteration 183/1000 | Loss: 0.00007104
Iteration 184/1000 | Loss: 0.00006977
Iteration 185/1000 | Loss: 0.00046394
Iteration 186/1000 | Loss: 0.00143438
Iteration 187/1000 | Loss: 0.00029268
Iteration 188/1000 | Loss: 0.00023671
Iteration 189/1000 | Loss: 0.00026612
Iteration 190/1000 | Loss: 0.00014327
Iteration 191/1000 | Loss: 0.00012683
Iteration 192/1000 | Loss: 0.00027784
Iteration 193/1000 | Loss: 0.00022012
Iteration 194/1000 | Loss: 0.00023483
Iteration 195/1000 | Loss: 0.00017029
Iteration 196/1000 | Loss: 0.00008188
Iteration 197/1000 | Loss: 0.00007886
Iteration 198/1000 | Loss: 0.00047051
Iteration 199/1000 | Loss: 0.00049774
Iteration 200/1000 | Loss: 0.00033960
Iteration 201/1000 | Loss: 0.00022829
Iteration 202/1000 | Loss: 0.00008091
Iteration 203/1000 | Loss: 0.00028709
Iteration 204/1000 | Loss: 0.00063571
Iteration 205/1000 | Loss: 0.00029141
Iteration 206/1000 | Loss: 0.00021732
Iteration 207/1000 | Loss: 0.00034385
Iteration 208/1000 | Loss: 0.00027926
Iteration 209/1000 | Loss: 0.00056718
Iteration 210/1000 | Loss: 0.00030802
Iteration 211/1000 | Loss: 0.00030165
Iteration 212/1000 | Loss: 0.00008848
Iteration 213/1000 | Loss: 0.00020362
Iteration 214/1000 | Loss: 0.00029312
Iteration 215/1000 | Loss: 0.00026701
Iteration 216/1000 | Loss: 0.00036813
Iteration 217/1000 | Loss: 0.00011721
Iteration 218/1000 | Loss: 0.00018164
Iteration 219/1000 | Loss: 0.00028606
Iteration 220/1000 | Loss: 0.00036467
Iteration 221/1000 | Loss: 0.00020807
Iteration 222/1000 | Loss: 0.00023822
Iteration 223/1000 | Loss: 0.00029730
Iteration 224/1000 | Loss: 0.00016867
Iteration 225/1000 | Loss: 0.00007406
Iteration 226/1000 | Loss: 0.00008660
Iteration 227/1000 | Loss: 0.00007250
Iteration 228/1000 | Loss: 0.00012397
Iteration 229/1000 | Loss: 0.00007579
Iteration 230/1000 | Loss: 0.00007126
Iteration 231/1000 | Loss: 0.00009863
Iteration 232/1000 | Loss: 0.00007069
Iteration 233/1000 | Loss: 0.00007026
Iteration 234/1000 | Loss: 0.00006982
Iteration 235/1000 | Loss: 0.00029919
Iteration 236/1000 | Loss: 0.00009059
Iteration 237/1000 | Loss: 0.00007623
Iteration 238/1000 | Loss: 0.00007174
Iteration 239/1000 | Loss: 0.00006990
Iteration 240/1000 | Loss: 0.00006849
Iteration 241/1000 | Loss: 0.00006796
Iteration 242/1000 | Loss: 0.00006749
Iteration 243/1000 | Loss: 0.00006723
Iteration 244/1000 | Loss: 0.00027528
Iteration 245/1000 | Loss: 0.00019424
Iteration 246/1000 | Loss: 0.00028181
Iteration 247/1000 | Loss: 0.00019882
Iteration 248/1000 | Loss: 0.00008320
Iteration 249/1000 | Loss: 0.00028926
Iteration 250/1000 | Loss: 0.00007993
Iteration 251/1000 | Loss: 0.00015667
Iteration 252/1000 | Loss: 0.00006869
Iteration 253/1000 | Loss: 0.00006734
Iteration 254/1000 | Loss: 0.00006621
Iteration 255/1000 | Loss: 0.00006541
Iteration 256/1000 | Loss: 0.00013139
Iteration 257/1000 | Loss: 0.00013137
Iteration 258/1000 | Loss: 0.00098940
Iteration 259/1000 | Loss: 0.00063647
Iteration 260/1000 | Loss: 0.00022856
Iteration 261/1000 | Loss: 0.00007905
Iteration 262/1000 | Loss: 0.00006910
Iteration 263/1000 | Loss: 0.00006364
Iteration 264/1000 | Loss: 0.00006125
Iteration 265/1000 | Loss: 0.00005966
Iteration 266/1000 | Loss: 0.00005876
Iteration 267/1000 | Loss: 0.00005807
Iteration 268/1000 | Loss: 0.00005739
Iteration 269/1000 | Loss: 0.00005685
Iteration 270/1000 | Loss: 0.00049919
Iteration 271/1000 | Loss: 0.00034221
Iteration 272/1000 | Loss: 0.00035586
Iteration 273/1000 | Loss: 0.00035352
Iteration 274/1000 | Loss: 0.00005873
Iteration 275/1000 | Loss: 0.00005699
Iteration 276/1000 | Loss: 0.00046844
Iteration 277/1000 | Loss: 0.00029071
Iteration 278/1000 | Loss: 0.00015743
Iteration 279/1000 | Loss: 0.00045289
Iteration 280/1000 | Loss: 0.00022882
Iteration 281/1000 | Loss: 0.00007133
Iteration 282/1000 | Loss: 0.00006563
Iteration 283/1000 | Loss: 0.00006251
Iteration 284/1000 | Loss: 0.00009932
Iteration 285/1000 | Loss: 0.00005932
Iteration 286/1000 | Loss: 0.00010412
Iteration 287/1000 | Loss: 0.00026781
Iteration 288/1000 | Loss: 0.00022554
Iteration 289/1000 | Loss: 0.00016918
Iteration 290/1000 | Loss: 0.00008001
Iteration 291/1000 | Loss: 0.00006217
Iteration 292/1000 | Loss: 0.00005974
Iteration 293/1000 | Loss: 0.00005881
Iteration 294/1000 | Loss: 0.00025432
Iteration 295/1000 | Loss: 0.00005839
Iteration 296/1000 | Loss: 0.00012995
Iteration 297/1000 | Loss: 0.00012991
Iteration 298/1000 | Loss: 0.00034211
Iteration 299/1000 | Loss: 0.00021084
Iteration 300/1000 | Loss: 0.00022757
Iteration 301/1000 | Loss: 0.00010254
Iteration 302/1000 | Loss: 0.00005943
Iteration 303/1000 | Loss: 0.00011396
Iteration 304/1000 | Loss: 0.00005898
Iteration 305/1000 | Loss: 0.00005799
Iteration 306/1000 | Loss: 0.00010713
Iteration 307/1000 | Loss: 0.00036312
Iteration 308/1000 | Loss: 0.00022005
Iteration 309/1000 | Loss: 0.00011637
Iteration 310/1000 | Loss: 0.00013410
Iteration 311/1000 | Loss: 0.00006677
Iteration 312/1000 | Loss: 0.00005878
Iteration 313/1000 | Loss: 0.00005712
Iteration 314/1000 | Loss: 0.00027714
Iteration 315/1000 | Loss: 0.00021115
Iteration 316/1000 | Loss: 0.00027238
Iteration 317/1000 | Loss: 0.00014830
Iteration 318/1000 | Loss: 0.00007120
Iteration 319/1000 | Loss: 0.00006066
Iteration 320/1000 | Loss: 0.00005720
Iteration 321/1000 | Loss: 0.00005499
Iteration 322/1000 | Loss: 0.00005400
Iteration 323/1000 | Loss: 0.00005297
Iteration 324/1000 | Loss: 0.00005262
Iteration 325/1000 | Loss: 0.00005241
Iteration 326/1000 | Loss: 0.00005211
Iteration 327/1000 | Loss: 0.00005183
Iteration 328/1000 | Loss: 0.00005166
Iteration 329/1000 | Loss: 0.00005161
Iteration 330/1000 | Loss: 0.00005157
Iteration 331/1000 | Loss: 0.00005144
Iteration 332/1000 | Loss: 0.00005140
Iteration 333/1000 | Loss: 0.00005121
Iteration 334/1000 | Loss: 0.00005112
Iteration 335/1000 | Loss: 0.00024830
Iteration 336/1000 | Loss: 0.00018667
Iteration 337/1000 | Loss: 0.00029106
Iteration 338/1000 | Loss: 0.00014870
Iteration 339/1000 | Loss: 0.00024393
Iteration 340/1000 | Loss: 0.00015817
Iteration 341/1000 | Loss: 0.00019917
Iteration 342/1000 | Loss: 0.00026760
Iteration 343/1000 | Loss: 0.00009705
Iteration 344/1000 | Loss: 0.00006185
Iteration 345/1000 | Loss: 0.00013567
Iteration 346/1000 | Loss: 0.00005377
Iteration 347/1000 | Loss: 0.00005067
Iteration 348/1000 | Loss: 0.00011585
Iteration 349/1000 | Loss: 0.00013673
Iteration 350/1000 | Loss: 0.00005075
Iteration 351/1000 | Loss: 0.00006285
Iteration 352/1000 | Loss: 0.00004808
Iteration 353/1000 | Loss: 0.00009319
Iteration 354/1000 | Loss: 0.00004765
Iteration 355/1000 | Loss: 0.00004713
Iteration 356/1000 | Loss: 0.00004679
Iteration 357/1000 | Loss: 0.00004643
Iteration 358/1000 | Loss: 0.00004614
Iteration 359/1000 | Loss: 0.00026325
Iteration 360/1000 | Loss: 0.00016537
Iteration 361/1000 | Loss: 0.00008522
Iteration 362/1000 | Loss: 0.00026860
Iteration 363/1000 | Loss: 0.00014361
Iteration 364/1000 | Loss: 0.00014369
Iteration 365/1000 | Loss: 0.00004857
Iteration 366/1000 | Loss: 0.00004633
Iteration 367/1000 | Loss: 0.00005687
Iteration 368/1000 | Loss: 0.00004481
Iteration 369/1000 | Loss: 0.00004442
Iteration 370/1000 | Loss: 0.00004420
Iteration 371/1000 | Loss: 0.00004401
Iteration 372/1000 | Loss: 0.00004400
Iteration 373/1000 | Loss: 0.00004398
Iteration 374/1000 | Loss: 0.00004388
Iteration 375/1000 | Loss: 0.00026260
Iteration 376/1000 | Loss: 0.00019492
Iteration 377/1000 | Loss: 0.00004399
Iteration 378/1000 | Loss: 0.00026311
Iteration 379/1000 | Loss: 0.00021038
Iteration 380/1000 | Loss: 0.00052449
Iteration 381/1000 | Loss: 0.00027769
Iteration 382/1000 | Loss: 0.00040549
Iteration 383/1000 | Loss: 0.00027515
Iteration 384/1000 | Loss: 0.00032786
Iteration 385/1000 | Loss: 0.00017699
Iteration 386/1000 | Loss: 0.00026953
Iteration 387/1000 | Loss: 0.00033964
Iteration 388/1000 | Loss: 0.00008191
Iteration 389/1000 | Loss: 0.00005007
Iteration 390/1000 | Loss: 0.00004627
Iteration 391/1000 | Loss: 0.00004484
Iteration 392/1000 | Loss: 0.00004415
Iteration 393/1000 | Loss: 0.00023326
Iteration 394/1000 | Loss: 0.00088544
Iteration 395/1000 | Loss: 0.00005269
Iteration 396/1000 | Loss: 0.00023200
Iteration 397/1000 | Loss: 0.00013328
Iteration 398/1000 | Loss: 0.00004449
Iteration 399/1000 | Loss: 0.00004388
Iteration 400/1000 | Loss: 0.00004378
Iteration 401/1000 | Loss: 0.00004375
Iteration 402/1000 | Loss: 0.00004374
Iteration 403/1000 | Loss: 0.00004374
Iteration 404/1000 | Loss: 0.00004374
Iteration 405/1000 | Loss: 0.00004373
Iteration 406/1000 | Loss: 0.00004373
Iteration 407/1000 | Loss: 0.00004373
Iteration 408/1000 | Loss: 0.00004372
Iteration 409/1000 | Loss: 0.00004372
Iteration 410/1000 | Loss: 0.00004372
Iteration 411/1000 | Loss: 0.00022978
Iteration 412/1000 | Loss: 0.00014088
Iteration 413/1000 | Loss: 0.00004393
Iteration 414/1000 | Loss: 0.00044639
Iteration 415/1000 | Loss: 0.00021352
Iteration 416/1000 | Loss: 0.00004443
Iteration 417/1000 | Loss: 0.00004374
Iteration 418/1000 | Loss: 0.00023076
Iteration 419/1000 | Loss: 0.00012789
Iteration 420/1000 | Loss: 0.00004566
Iteration 421/1000 | Loss: 0.00004421
Iteration 422/1000 | Loss: 0.00004380
Iteration 423/1000 | Loss: 0.00047850
Iteration 424/1000 | Loss: 0.00014159
Iteration 425/1000 | Loss: 0.00010383
Iteration 426/1000 | Loss: 0.00023870
Iteration 427/1000 | Loss: 0.00014190
Iteration 428/1000 | Loss: 0.00018899
Iteration 429/1000 | Loss: 0.00015242
Iteration 430/1000 | Loss: 0.00004439
Iteration 431/1000 | Loss: 0.00004396
Iteration 432/1000 | Loss: 0.00047651
Iteration 433/1000 | Loss: 0.00014518
Iteration 434/1000 | Loss: 0.00004419
Iteration 435/1000 | Loss: 0.00023661
Iteration 436/1000 | Loss: 0.00018504
Iteration 437/1000 | Loss: 0.00010629
Iteration 438/1000 | Loss: 0.00048259
Iteration 439/1000 | Loss: 0.00014345
Iteration 440/1000 | Loss: 0.00019992
Iteration 441/1000 | Loss: 0.00005153
Iteration 442/1000 | Loss: 0.00004653
Iteration 443/1000 | Loss: 0.00025934
Iteration 444/1000 | Loss: 0.00015063
Iteration 445/1000 | Loss: 0.00031053
Iteration 446/1000 | Loss: 0.00016024
Iteration 447/1000 | Loss: 0.00005544
Iteration 448/1000 | Loss: 0.00022511
Iteration 449/1000 | Loss: 0.00004732
Iteration 450/1000 | Loss: 0.00004950
Iteration 451/1000 | Loss: 0.00004545
Iteration 452/1000 | Loss: 0.00026207
Iteration 453/1000 | Loss: 0.00014958
Iteration 454/1000 | Loss: 0.00022224
Iteration 455/1000 | Loss: 0.00007221
Iteration 456/1000 | Loss: 0.00005398
Iteration 457/1000 | Loss: 0.00018106
Iteration 458/1000 | Loss: 0.00039576
Iteration 459/1000 | Loss: 0.00034243
Iteration 460/1000 | Loss: 0.00028127
Iteration 461/1000 | Loss: 0.00027087
Iteration 462/1000 | Loss: 0.00042736
Iteration 463/1000 | Loss: 0.00039989
Iteration 464/1000 | Loss: 0.00053821
Iteration 465/1000 | Loss: 0.00030526
Iteration 466/1000 | Loss: 0.00006500
Iteration 467/1000 | Loss: 0.00008623
Iteration 468/1000 | Loss: 0.00008572
Iteration 469/1000 | Loss: 0.00005270
Iteration 470/1000 | Loss: 0.00021545
Iteration 471/1000 | Loss: 0.00008017
Iteration 472/1000 | Loss: 0.00011256
Iteration 473/1000 | Loss: 0.00009335
Iteration 474/1000 | Loss: 0.00010183
Iteration 475/1000 | Loss: 0.00017854
Iteration 476/1000 | Loss: 0.00005513
Iteration 477/1000 | Loss: 0.00004830
Iteration 478/1000 | Loss: 0.00004591
Iteration 479/1000 | Loss: 0.00011462
Iteration 480/1000 | Loss: 0.00004535
Iteration 481/1000 | Loss: 0.00004498
Iteration 482/1000 | Loss: 0.00004482
Iteration 483/1000 | Loss: 0.00004478
Iteration 484/1000 | Loss: 0.00004463
Iteration 485/1000 | Loss: 0.00004449
Iteration 486/1000 | Loss: 0.00004449
Iteration 487/1000 | Loss: 0.00004448
Iteration 488/1000 | Loss: 0.00004448
Iteration 489/1000 | Loss: 0.00004448
Iteration 490/1000 | Loss: 0.00004448
Iteration 491/1000 | Loss: 0.00004448
Iteration 492/1000 | Loss: 0.00004448
Iteration 493/1000 | Loss: 0.00004448
Iteration 494/1000 | Loss: 0.00004448
Iteration 495/1000 | Loss: 0.00004448
Iteration 496/1000 | Loss: 0.00004443
Iteration 497/1000 | Loss: 0.00004443
Iteration 498/1000 | Loss: 0.00059203
Iteration 499/1000 | Loss: 0.00021990
Iteration 500/1000 | Loss: 0.00006408
Iteration 501/1000 | Loss: 0.00004828
Iteration 502/1000 | Loss: 0.00004600
Iteration 503/1000 | Loss: 0.00004466
Iteration 504/1000 | Loss: 0.00004347
Iteration 505/1000 | Loss: 0.00004289
Iteration 506/1000 | Loss: 0.00004245
Iteration 507/1000 | Loss: 0.00004221
Iteration 508/1000 | Loss: 0.00004218
Iteration 509/1000 | Loss: 0.00004217
Iteration 510/1000 | Loss: 0.00004216
Iteration 511/1000 | Loss: 0.00004216
Iteration 512/1000 | Loss: 0.00004215
Iteration 513/1000 | Loss: 0.00004214
Iteration 514/1000 | Loss: 0.00004214
Iteration 515/1000 | Loss: 0.00004212
Iteration 516/1000 | Loss: 0.00004211
Iteration 517/1000 | Loss: 0.00004200
Iteration 518/1000 | Loss: 0.00004199
Iteration 519/1000 | Loss: 0.00004199
Iteration 520/1000 | Loss: 0.00004198
Iteration 521/1000 | Loss: 0.00004197
Iteration 522/1000 | Loss: 0.00004197
Iteration 523/1000 | Loss: 0.00004197
Iteration 524/1000 | Loss: 0.00004196
Iteration 525/1000 | Loss: 0.00004196
Iteration 526/1000 | Loss: 0.00004196
Iteration 527/1000 | Loss: 0.00004196
Iteration 528/1000 | Loss: 0.00004196
Iteration 529/1000 | Loss: 0.00004196
Iteration 530/1000 | Loss: 0.00004196
Iteration 531/1000 | Loss: 0.00004196
Iteration 532/1000 | Loss: 0.00004196
Iteration 533/1000 | Loss: 0.00004195
Iteration 534/1000 | Loss: 0.00004195
Iteration 535/1000 | Loss: 0.00004194
Iteration 536/1000 | Loss: 0.00004193
Iteration 537/1000 | Loss: 0.00004193
Iteration 538/1000 | Loss: 0.00004192
Iteration 539/1000 | Loss: 0.00004192
Iteration 540/1000 | Loss: 0.00004192
Iteration 541/1000 | Loss: 0.00004192
Iteration 542/1000 | Loss: 0.00004192
Iteration 543/1000 | Loss: 0.00004191
Iteration 544/1000 | Loss: 0.00004191
Iteration 545/1000 | Loss: 0.00004191
Iteration 546/1000 | Loss: 0.00004191
Iteration 547/1000 | Loss: 0.00004191
Iteration 548/1000 | Loss: 0.00004191
Iteration 549/1000 | Loss: 0.00004191
Iteration 550/1000 | Loss: 0.00004191
Iteration 551/1000 | Loss: 0.00004191
Iteration 552/1000 | Loss: 0.00004190
Iteration 553/1000 | Loss: 0.00004190
Iteration 554/1000 | Loss: 0.00004190
Iteration 555/1000 | Loss: 0.00004190
Iteration 556/1000 | Loss: 0.00004190
Iteration 557/1000 | Loss: 0.00004189
Iteration 558/1000 | Loss: 0.00004189
Iteration 559/1000 | Loss: 0.00004188
Iteration 560/1000 | Loss: 0.00004188
Iteration 561/1000 | Loss: 0.00004187
Iteration 562/1000 | Loss: 0.00004187
Iteration 563/1000 | Loss: 0.00004186
Iteration 564/1000 | Loss: 0.00004185
Iteration 565/1000 | Loss: 0.00004185
Iteration 566/1000 | Loss: 0.00004184
Iteration 567/1000 | Loss: 0.00004184
Iteration 568/1000 | Loss: 0.00004184
Iteration 569/1000 | Loss: 0.00004184
Iteration 570/1000 | Loss: 0.00004184
Iteration 571/1000 | Loss: 0.00004183
Iteration 572/1000 | Loss: 0.00004183
Iteration 573/1000 | Loss: 0.00004183
Iteration 574/1000 | Loss: 0.00004183
Iteration 575/1000 | Loss: 0.00004182
Iteration 576/1000 | Loss: 0.00004182
Iteration 577/1000 | Loss: 0.00004181
Iteration 578/1000 | Loss: 0.00004181
Iteration 579/1000 | Loss: 0.00004180
Iteration 580/1000 | Loss: 0.00004178
Iteration 581/1000 | Loss: 0.00004178
Iteration 582/1000 | Loss: 0.00004178
Iteration 583/1000 | Loss: 0.00004178
Iteration 584/1000 | Loss: 0.00004174
Iteration 585/1000 | Loss: 0.00004172
Iteration 586/1000 | Loss: 0.00004172
Iteration 587/1000 | Loss: 0.00004172
Iteration 588/1000 | Loss: 0.00004172
Iteration 589/1000 | Loss: 0.00004171
Iteration 590/1000 | Loss: 0.00004171
Iteration 591/1000 | Loss: 0.00004171
Iteration 592/1000 | Loss: 0.00004171
Iteration 593/1000 | Loss: 0.00004171
Iteration 594/1000 | Loss: 0.00004171
Iteration 595/1000 | Loss: 0.00004171
Iteration 596/1000 | Loss: 0.00004171
Iteration 597/1000 | Loss: 0.00004171
Iteration 598/1000 | Loss: 0.00004171
Iteration 599/1000 | Loss: 0.00004171
Iteration 600/1000 | Loss: 0.00004171
Iteration 601/1000 | Loss: 0.00004170
Iteration 602/1000 | Loss: 0.00004170
Iteration 603/1000 | Loss: 0.00004169
Iteration 604/1000 | Loss: 0.00004169
Iteration 605/1000 | Loss: 0.00004169
Iteration 606/1000 | Loss: 0.00004169
Iteration 607/1000 | Loss: 0.00004169
Iteration 608/1000 | Loss: 0.00004169
Iteration 609/1000 | Loss: 0.00004169
Iteration 610/1000 | Loss: 0.00004169
Iteration 611/1000 | Loss: 0.00004168
Iteration 612/1000 | Loss: 0.00012075
Iteration 613/1000 | Loss: 0.00004934
Iteration 614/1000 | Loss: 0.00004462
Iteration 615/1000 | Loss: 0.00004297
Iteration 616/1000 | Loss: 0.00011182
Iteration 617/1000 | Loss: 0.00004797
Iteration 618/1000 | Loss: 0.00004323
Iteration 619/1000 | Loss: 0.00004195
Iteration 620/1000 | Loss: 0.00004040
Iteration 621/1000 | Loss: 0.00011767
Iteration 622/1000 | Loss: 0.00004728
Iteration 623/1000 | Loss: 0.00004267
Iteration 624/1000 | Loss: 0.00004053
Iteration 625/1000 | Loss: 0.00003908
Iteration 626/1000 | Loss: 0.00003821
Iteration 627/1000 | Loss: 0.00003746
Iteration 628/1000 | Loss: 0.00003724
Iteration 629/1000 | Loss: 0.00003696
Iteration 630/1000 | Loss: 0.00024722
Iteration 631/1000 | Loss: 0.00016885
Iteration 632/1000 | Loss: 0.00022355
Iteration 633/1000 | Loss: 0.00018619
Iteration 634/1000 | Loss: 0.00033832
Iteration 635/1000 | Loss: 0.00005478
Iteration 636/1000 | Loss: 0.00003912
Iteration 637/1000 | Loss: 0.00003843
Iteration 638/1000 | Loss: 0.00003796
Iteration 639/1000 | Loss: 0.00003764
Iteration 640/1000 | Loss: 0.00003745
Iteration 641/1000 | Loss: 0.00003742
Iteration 642/1000 | Loss: 0.00003737
Iteration 643/1000 | Loss: 0.00003736
Iteration 644/1000 | Loss: 0.00003736
Iteration 645/1000 | Loss: 0.00003732
Iteration 646/1000 | Loss: 0.00003725
Iteration 647/1000 | Loss: 0.00003724
Iteration 648/1000 | Loss: 0.00003723
Iteration 649/1000 | Loss: 0.00003723
Iteration 650/1000 | Loss: 0.00003722
Iteration 651/1000 | Loss: 0.00003716
Iteration 652/1000 | Loss: 0.00003711
Iteration 653/1000 | Loss: 0.00003711
Iteration 654/1000 | Loss: 0.00003711
Iteration 655/1000 | Loss: 0.00003711
Iteration 656/1000 | Loss: 0.00003711
Iteration 657/1000 | Loss: 0.00003711
Iteration 658/1000 | Loss: 0.00003711
Iteration 659/1000 | Loss: 0.00003711
Iteration 660/1000 | Loss: 0.00003711
Iteration 661/1000 | Loss: 0.00003711
Iteration 662/1000 | Loss: 0.00003711
Iteration 663/1000 | Loss: 0.00003711
Iteration 664/1000 | Loss: 0.00003710
Iteration 665/1000 | Loss: 0.00003710
Iteration 666/1000 | Loss: 0.00003710
Iteration 667/1000 | Loss: 0.00003710
Iteration 668/1000 | Loss: 0.00003710
Iteration 669/1000 | Loss: 0.00003710
Iteration 670/1000 | Loss: 0.00003710
Iteration 671/1000 | Loss: 0.00003708
Iteration 672/1000 | Loss: 0.00003708
Iteration 673/1000 | Loss: 0.00003708
Iteration 674/1000 | Loss: 0.00003708
Iteration 675/1000 | Loss: 0.00003708
Iteration 676/1000 | Loss: 0.00003708
Iteration 677/1000 | Loss: 0.00003708
Iteration 678/1000 | Loss: 0.00003708
Iteration 679/1000 | Loss: 0.00003708
Iteration 680/1000 | Loss: 0.00003708
Iteration 681/1000 | Loss: 0.00003708
Iteration 682/1000 | Loss: 0.00003707
Iteration 683/1000 | Loss: 0.00003707
Iteration 684/1000 | Loss: 0.00003707
Iteration 685/1000 | Loss: 0.00003707
Iteration 686/1000 | Loss: 0.00003707
Iteration 687/1000 | Loss: 0.00003707
Iteration 688/1000 | Loss: 0.00003707
Iteration 689/1000 | Loss: 0.00003707
Iteration 690/1000 | Loss: 0.00003707
Iteration 691/1000 | Loss: 0.00003707
Iteration 692/1000 | Loss: 0.00003707
Iteration 693/1000 | Loss: 0.00003706
Iteration 694/1000 | Loss: 0.00003705
Iteration 695/1000 | Loss: 0.00003705
Iteration 696/1000 | Loss: 0.00003704
Iteration 697/1000 | Loss: 0.00003704
Iteration 698/1000 | Loss: 0.00003704
Iteration 699/1000 | Loss: 0.00003704
Iteration 700/1000 | Loss: 0.00003704
Iteration 701/1000 | Loss: 0.00003704
Iteration 702/1000 | Loss: 0.00003704
Iteration 703/1000 | Loss: 0.00003704
Iteration 704/1000 | Loss: 0.00003703
Iteration 705/1000 | Loss: 0.00003703
Iteration 706/1000 | Loss: 0.00003703
Iteration 707/1000 | Loss: 0.00003703
Iteration 708/1000 | Loss: 0.00003703
Iteration 709/1000 | Loss: 0.00003702
Iteration 710/1000 | Loss: 0.00003702
Iteration 711/1000 | Loss: 0.00003702
Iteration 712/1000 | Loss: 0.00003702
Iteration 713/1000 | Loss: 0.00003702
Iteration 714/1000 | Loss: 0.00003702
Iteration 715/1000 | Loss: 0.00003702
Iteration 716/1000 | Loss: 0.00003702
Iteration 717/1000 | Loss: 0.00003701
Iteration 718/1000 | Loss: 0.00003701
Iteration 719/1000 | Loss: 0.00003701
Iteration 720/1000 | Loss: 0.00003701
Iteration 721/1000 | Loss: 0.00003701
Iteration 722/1000 | Loss: 0.00003701
Iteration 723/1000 | Loss: 0.00003701
Iteration 724/1000 | Loss: 0.00003701
Iteration 725/1000 | Loss: 0.00003701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 725. Stopping optimization.
Last 5 losses: [3.701274181366898e-05, 3.701274181366898e-05, 3.701274181366898e-05, 3.701274181366898e-05, 3.701274181366898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.701274181366898e-05

Optimization complete. Final v2v error: 3.6534245014190674 mm

Highest mean error: 10.948241233825684 mm for frame 17

Lowest mean error: 2.8157050609588623 mm for frame 71

Saving results

Total time: 857.9907715320587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034163
Iteration 2/25 | Loss: 0.01034163
Iteration 3/25 | Loss: 0.01034162
Iteration 4/25 | Loss: 0.01034162
Iteration 5/25 | Loss: 0.01034162
Iteration 6/25 | Loss: 0.01034162
Iteration 7/25 | Loss: 0.01034162
Iteration 8/25 | Loss: 0.01034162
Iteration 9/25 | Loss: 0.01034162
Iteration 10/25 | Loss: 0.01034162
Iteration 11/25 | Loss: 0.01034162
Iteration 12/25 | Loss: 0.01034161
Iteration 13/25 | Loss: 0.01034161
Iteration 14/25 | Loss: 0.01034161
Iteration 15/25 | Loss: 0.01034161
Iteration 16/25 | Loss: 0.01034161
Iteration 17/25 | Loss: 0.01034161
Iteration 18/25 | Loss: 0.01034161
Iteration 19/25 | Loss: 0.01034161
Iteration 20/25 | Loss: 0.01034161
Iteration 21/25 | Loss: 0.01034161
Iteration 22/25 | Loss: 0.01034161
Iteration 23/25 | Loss: 0.01034161
Iteration 24/25 | Loss: 0.01034160
Iteration 25/25 | Loss: 0.01034160

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32223821
Iteration 2/25 | Loss: 0.17090040
Iteration 3/25 | Loss: 0.17044541
Iteration 4/25 | Loss: 0.17035829
Iteration 5/25 | Loss: 0.17035818
Iteration 6/25 | Loss: 0.17033361
Iteration 7/25 | Loss: 0.17033356
Iteration 8/25 | Loss: 0.17033352
Iteration 9/25 | Loss: 0.17033352
Iteration 10/25 | Loss: 0.17033352
Iteration 11/25 | Loss: 0.17033350
Iteration 12/25 | Loss: 0.17033350
Iteration 13/25 | Loss: 0.17033350
Iteration 14/25 | Loss: 0.17033347
Iteration 15/25 | Loss: 0.17033347
Iteration 16/25 | Loss: 0.17033347
Iteration 17/25 | Loss: 0.17033347
Iteration 18/25 | Loss: 0.17033347
Iteration 19/25 | Loss: 0.17033347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.17033347487449646, 0.17033347487449646, 0.17033347487449646, 0.17033347487449646, 0.17033347487449646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17033347487449646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17033347
Iteration 2/1000 | Loss: 0.00159113
Iteration 3/1000 | Loss: 0.00214900
Iteration 4/1000 | Loss: 0.00036373
Iteration 5/1000 | Loss: 0.00105256
Iteration 6/1000 | Loss: 0.00007661
Iteration 7/1000 | Loss: 0.00057338
Iteration 8/1000 | Loss: 0.00004647
Iteration 9/1000 | Loss: 0.00009283
Iteration 10/1000 | Loss: 0.00003341
Iteration 11/1000 | Loss: 0.00002786
Iteration 12/1000 | Loss: 0.00002443
Iteration 13/1000 | Loss: 0.00002224
Iteration 14/1000 | Loss: 0.00002096
Iteration 15/1000 | Loss: 0.00001969
Iteration 16/1000 | Loss: 0.00011472
Iteration 17/1000 | Loss: 0.00043233
Iteration 18/1000 | Loss: 0.00002085
Iteration 19/1000 | Loss: 0.00006200
Iteration 20/1000 | Loss: 0.00001775
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00014564
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001626
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001528
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001469
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001432
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001429
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001417
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001416
Iteration 104/1000 | Loss: 0.00001416
Iteration 105/1000 | Loss: 0.00001416
Iteration 106/1000 | Loss: 0.00001416
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001414
Iteration 115/1000 | Loss: 0.00011580
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001413
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001408
Iteration 135/1000 | Loss: 0.00001408
Iteration 136/1000 | Loss: 0.00001408
Iteration 137/1000 | Loss: 0.00001408
Iteration 138/1000 | Loss: 0.00001408
Iteration 139/1000 | Loss: 0.00001408
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001407
Iteration 148/1000 | Loss: 0.00001407
Iteration 149/1000 | Loss: 0.00001407
Iteration 150/1000 | Loss: 0.00001407
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00009641
Iteration 159/1000 | Loss: 0.00001524
Iteration 160/1000 | Loss: 0.00001417
Iteration 161/1000 | Loss: 0.00003438
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001408
Iteration 164/1000 | Loss: 0.00001408
Iteration 165/1000 | Loss: 0.00001408
Iteration 166/1000 | Loss: 0.00001408
Iteration 167/1000 | Loss: 0.00001408
Iteration 168/1000 | Loss: 0.00001408
Iteration 169/1000 | Loss: 0.00001408
Iteration 170/1000 | Loss: 0.00001408
Iteration 171/1000 | Loss: 0.00001407
Iteration 172/1000 | Loss: 0.00001407
Iteration 173/1000 | Loss: 0.00001406
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001405
Iteration 177/1000 | Loss: 0.00001405
Iteration 178/1000 | Loss: 0.00001404
Iteration 179/1000 | Loss: 0.00001404
Iteration 180/1000 | Loss: 0.00001404
Iteration 181/1000 | Loss: 0.00001404
Iteration 182/1000 | Loss: 0.00001404
Iteration 183/1000 | Loss: 0.00001404
Iteration 184/1000 | Loss: 0.00001404
Iteration 185/1000 | Loss: 0.00001404
Iteration 186/1000 | Loss: 0.00001404
Iteration 187/1000 | Loss: 0.00001404
Iteration 188/1000 | Loss: 0.00001404
Iteration 189/1000 | Loss: 0.00001404
Iteration 190/1000 | Loss: 0.00001404
Iteration 191/1000 | Loss: 0.00001404
Iteration 192/1000 | Loss: 0.00001404
Iteration 193/1000 | Loss: 0.00001404
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001403
Iteration 199/1000 | Loss: 0.00001403
Iteration 200/1000 | Loss: 0.00001403
Iteration 201/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.4034181731403805e-05, 1.4034181731403805e-05, 1.4034181731403805e-05, 1.4034181731403805e-05, 1.4034181731403805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4034181731403805e-05

Optimization complete. Final v2v error: 3.1484713554382324 mm

Highest mean error: 3.3643507957458496 mm for frame 128

Lowest mean error: 2.8425133228302 mm for frame 6

Saving results

Total time: 82.45746803283691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857934
Iteration 2/25 | Loss: 0.00151490
Iteration 3/25 | Loss: 0.00130494
Iteration 4/25 | Loss: 0.00128671
Iteration 5/25 | Loss: 0.00128483
Iteration 6/25 | Loss: 0.00128483
Iteration 7/25 | Loss: 0.00128483
Iteration 8/25 | Loss: 0.00128483
Iteration 9/25 | Loss: 0.00128483
Iteration 10/25 | Loss: 0.00128483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012848342303186655, 0.0012848342303186655, 0.0012848342303186655, 0.0012848342303186655, 0.0012848342303186655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012848342303186655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91697776
Iteration 2/25 | Loss: 0.00065782
Iteration 3/25 | Loss: 0.00065781
Iteration 4/25 | Loss: 0.00065781
Iteration 5/25 | Loss: 0.00065781
Iteration 6/25 | Loss: 0.00065781
Iteration 7/25 | Loss: 0.00065781
Iteration 8/25 | Loss: 0.00065781
Iteration 9/25 | Loss: 0.00065781
Iteration 10/25 | Loss: 0.00065781
Iteration 11/25 | Loss: 0.00065781
Iteration 12/25 | Loss: 0.00065781
Iteration 13/25 | Loss: 0.00065781
Iteration 14/25 | Loss: 0.00065781
Iteration 15/25 | Loss: 0.00065781
Iteration 16/25 | Loss: 0.00065781
Iteration 17/25 | Loss: 0.00065781
Iteration 18/25 | Loss: 0.00065781
Iteration 19/25 | Loss: 0.00065781
Iteration 20/25 | Loss: 0.00065781
Iteration 21/25 | Loss: 0.00065781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006578093743883073, 0.0006578093743883073, 0.0006578093743883073, 0.0006578093743883073, 0.0006578093743883073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006578093743883073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065781
Iteration 2/1000 | Loss: 0.00003433
Iteration 3/1000 | Loss: 0.00002515
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002181
Iteration 6/1000 | Loss: 0.00002095
Iteration 7/1000 | Loss: 0.00002029
Iteration 8/1000 | Loss: 0.00001992
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001919
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001891
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001873
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001871
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001871
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001870
Iteration 54/1000 | Loss: 0.00001870
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001870
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001864
Iteration 89/1000 | Loss: 0.00001864
Iteration 90/1000 | Loss: 0.00001864
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001863
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001862
Iteration 101/1000 | Loss: 0.00001862
Iteration 102/1000 | Loss: 0.00001862
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001861
Iteration 115/1000 | Loss: 0.00001861
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001860
Iteration 118/1000 | Loss: 0.00001860
Iteration 119/1000 | Loss: 0.00001860
Iteration 120/1000 | Loss: 0.00001860
Iteration 121/1000 | Loss: 0.00001860
Iteration 122/1000 | Loss: 0.00001860
Iteration 123/1000 | Loss: 0.00001860
Iteration 124/1000 | Loss: 0.00001860
Iteration 125/1000 | Loss: 0.00001860
Iteration 126/1000 | Loss: 0.00001860
Iteration 127/1000 | Loss: 0.00001860
Iteration 128/1000 | Loss: 0.00001860
Iteration 129/1000 | Loss: 0.00001859
Iteration 130/1000 | Loss: 0.00001859
Iteration 131/1000 | Loss: 0.00001859
Iteration 132/1000 | Loss: 0.00001859
Iteration 133/1000 | Loss: 0.00001858
Iteration 134/1000 | Loss: 0.00001858
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001858
Iteration 137/1000 | Loss: 0.00001858
Iteration 138/1000 | Loss: 0.00001858
Iteration 139/1000 | Loss: 0.00001858
Iteration 140/1000 | Loss: 0.00001858
Iteration 141/1000 | Loss: 0.00001857
Iteration 142/1000 | Loss: 0.00001857
Iteration 143/1000 | Loss: 0.00001857
Iteration 144/1000 | Loss: 0.00001857
Iteration 145/1000 | Loss: 0.00001857
Iteration 146/1000 | Loss: 0.00001857
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001857
Iteration 155/1000 | Loss: 0.00001857
Iteration 156/1000 | Loss: 0.00001857
Iteration 157/1000 | Loss: 0.00001857
Iteration 158/1000 | Loss: 0.00001857
Iteration 159/1000 | Loss: 0.00001856
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00001856
Iteration 170/1000 | Loss: 0.00001856
Iteration 171/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.856189192039892e-05, 1.856189192039892e-05, 1.856189192039892e-05, 1.856189192039892e-05, 1.856189192039892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.856189192039892e-05

Optimization complete. Final v2v error: 3.619535446166992 mm

Highest mean error: 3.829392910003662 mm for frame 34

Lowest mean error: 3.4735889434814453 mm for frame 91

Saving results

Total time: 35.41303586959839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491124
Iteration 2/25 | Loss: 0.00148710
Iteration 3/25 | Loss: 0.00135360
Iteration 4/25 | Loss: 0.00133892
Iteration 5/25 | Loss: 0.00133555
Iteration 6/25 | Loss: 0.00133504
Iteration 7/25 | Loss: 0.00133504
Iteration 8/25 | Loss: 0.00133504
Iteration 9/25 | Loss: 0.00133504
Iteration 10/25 | Loss: 0.00133504
Iteration 11/25 | Loss: 0.00133504
Iteration 12/25 | Loss: 0.00133504
Iteration 13/25 | Loss: 0.00133504
Iteration 14/25 | Loss: 0.00133504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001335042528808117, 0.001335042528808117, 0.001335042528808117, 0.001335042528808117, 0.001335042528808117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001335042528808117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36425376
Iteration 2/25 | Loss: 0.00101226
Iteration 3/25 | Loss: 0.00101225
Iteration 4/25 | Loss: 0.00101225
Iteration 5/25 | Loss: 0.00101225
Iteration 6/25 | Loss: 0.00101224
Iteration 7/25 | Loss: 0.00101224
Iteration 8/25 | Loss: 0.00101224
Iteration 9/25 | Loss: 0.00101224
Iteration 10/25 | Loss: 0.00101224
Iteration 11/25 | Loss: 0.00101224
Iteration 12/25 | Loss: 0.00101224
Iteration 13/25 | Loss: 0.00101224
Iteration 14/25 | Loss: 0.00101224
Iteration 15/25 | Loss: 0.00101224
Iteration 16/25 | Loss: 0.00101224
Iteration 17/25 | Loss: 0.00101224
Iteration 18/25 | Loss: 0.00101224
Iteration 19/25 | Loss: 0.00101224
Iteration 20/25 | Loss: 0.00101224
Iteration 21/25 | Loss: 0.00101224
Iteration 22/25 | Loss: 0.00101224
Iteration 23/25 | Loss: 0.00101224
Iteration 24/25 | Loss: 0.00101224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010122429812327027, 0.0010122429812327027, 0.0010122429812327027, 0.0010122429812327027, 0.0010122429812327027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010122429812327027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101224
Iteration 2/1000 | Loss: 0.00004333
Iteration 3/1000 | Loss: 0.00002868
Iteration 4/1000 | Loss: 0.00002630
Iteration 5/1000 | Loss: 0.00002474
Iteration 6/1000 | Loss: 0.00002386
Iteration 7/1000 | Loss: 0.00002313
Iteration 8/1000 | Loss: 0.00002275
Iteration 9/1000 | Loss: 0.00002242
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002209
Iteration 13/1000 | Loss: 0.00002195
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002183
Iteration 16/1000 | Loss: 0.00002180
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002174
Iteration 22/1000 | Loss: 0.00002170
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002166
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002163
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002160
Iteration 33/1000 | Loss: 0.00002158
Iteration 34/1000 | Loss: 0.00002158
Iteration 35/1000 | Loss: 0.00002155
Iteration 36/1000 | Loss: 0.00002155
Iteration 37/1000 | Loss: 0.00002154
Iteration 38/1000 | Loss: 0.00002153
Iteration 39/1000 | Loss: 0.00002152
Iteration 40/1000 | Loss: 0.00002152
Iteration 41/1000 | Loss: 0.00002152
Iteration 42/1000 | Loss: 0.00002151
Iteration 43/1000 | Loss: 0.00002150
Iteration 44/1000 | Loss: 0.00002150
Iteration 45/1000 | Loss: 0.00002149
Iteration 46/1000 | Loss: 0.00002148
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002148
Iteration 49/1000 | Loss: 0.00002148
Iteration 50/1000 | Loss: 0.00002147
Iteration 51/1000 | Loss: 0.00002147
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002147
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002146
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00002146
Iteration 58/1000 | Loss: 0.00002146
Iteration 59/1000 | Loss: 0.00002145
Iteration 60/1000 | Loss: 0.00002145
Iteration 61/1000 | Loss: 0.00002144
Iteration 62/1000 | Loss: 0.00002144
Iteration 63/1000 | Loss: 0.00002144
Iteration 64/1000 | Loss: 0.00002144
Iteration 65/1000 | Loss: 0.00002144
Iteration 66/1000 | Loss: 0.00002143
Iteration 67/1000 | Loss: 0.00002143
Iteration 68/1000 | Loss: 0.00002143
Iteration 69/1000 | Loss: 0.00002143
Iteration 70/1000 | Loss: 0.00002143
Iteration 71/1000 | Loss: 0.00002143
Iteration 72/1000 | Loss: 0.00002143
Iteration 73/1000 | Loss: 0.00002143
Iteration 74/1000 | Loss: 0.00002142
Iteration 75/1000 | Loss: 0.00002142
Iteration 76/1000 | Loss: 0.00002142
Iteration 77/1000 | Loss: 0.00002141
Iteration 78/1000 | Loss: 0.00002141
Iteration 79/1000 | Loss: 0.00002141
Iteration 80/1000 | Loss: 0.00002141
Iteration 81/1000 | Loss: 0.00002141
Iteration 82/1000 | Loss: 0.00002141
Iteration 83/1000 | Loss: 0.00002140
Iteration 84/1000 | Loss: 0.00002140
Iteration 85/1000 | Loss: 0.00002140
Iteration 86/1000 | Loss: 0.00002139
Iteration 87/1000 | Loss: 0.00002139
Iteration 88/1000 | Loss: 0.00002139
Iteration 89/1000 | Loss: 0.00002139
Iteration 90/1000 | Loss: 0.00002138
Iteration 91/1000 | Loss: 0.00002138
Iteration 92/1000 | Loss: 0.00002138
Iteration 93/1000 | Loss: 0.00002137
Iteration 94/1000 | Loss: 0.00002137
Iteration 95/1000 | Loss: 0.00002136
Iteration 96/1000 | Loss: 0.00002136
Iteration 97/1000 | Loss: 0.00002135
Iteration 98/1000 | Loss: 0.00002135
Iteration 99/1000 | Loss: 0.00002135
Iteration 100/1000 | Loss: 0.00002135
Iteration 101/1000 | Loss: 0.00002134
Iteration 102/1000 | Loss: 0.00002134
Iteration 103/1000 | Loss: 0.00002134
Iteration 104/1000 | Loss: 0.00002134
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002133
Iteration 109/1000 | Loss: 0.00002132
Iteration 110/1000 | Loss: 0.00002132
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002132
Iteration 113/1000 | Loss: 0.00002132
Iteration 114/1000 | Loss: 0.00002132
Iteration 115/1000 | Loss: 0.00002132
Iteration 116/1000 | Loss: 0.00002132
Iteration 117/1000 | Loss: 0.00002132
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002131
Iteration 120/1000 | Loss: 0.00002131
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002130
Iteration 124/1000 | Loss: 0.00002130
Iteration 125/1000 | Loss: 0.00002130
Iteration 126/1000 | Loss: 0.00002130
Iteration 127/1000 | Loss: 0.00002130
Iteration 128/1000 | Loss: 0.00002130
Iteration 129/1000 | Loss: 0.00002130
Iteration 130/1000 | Loss: 0.00002130
Iteration 131/1000 | Loss: 0.00002130
Iteration 132/1000 | Loss: 0.00002130
Iteration 133/1000 | Loss: 0.00002129
Iteration 134/1000 | Loss: 0.00002129
Iteration 135/1000 | Loss: 0.00002129
Iteration 136/1000 | Loss: 0.00002129
Iteration 137/1000 | Loss: 0.00002129
Iteration 138/1000 | Loss: 0.00002129
Iteration 139/1000 | Loss: 0.00002129
Iteration 140/1000 | Loss: 0.00002129
Iteration 141/1000 | Loss: 0.00002129
Iteration 142/1000 | Loss: 0.00002128
Iteration 143/1000 | Loss: 0.00002128
Iteration 144/1000 | Loss: 0.00002128
Iteration 145/1000 | Loss: 0.00002128
Iteration 146/1000 | Loss: 0.00002127
Iteration 147/1000 | Loss: 0.00002127
Iteration 148/1000 | Loss: 0.00002127
Iteration 149/1000 | Loss: 0.00002127
Iteration 150/1000 | Loss: 0.00002127
Iteration 151/1000 | Loss: 0.00002127
Iteration 152/1000 | Loss: 0.00002126
Iteration 153/1000 | Loss: 0.00002126
Iteration 154/1000 | Loss: 0.00002126
Iteration 155/1000 | Loss: 0.00002126
Iteration 156/1000 | Loss: 0.00002126
Iteration 157/1000 | Loss: 0.00002126
Iteration 158/1000 | Loss: 0.00002126
Iteration 159/1000 | Loss: 0.00002126
Iteration 160/1000 | Loss: 0.00002126
Iteration 161/1000 | Loss: 0.00002126
Iteration 162/1000 | Loss: 0.00002126
Iteration 163/1000 | Loss: 0.00002126
Iteration 164/1000 | Loss: 0.00002126
Iteration 165/1000 | Loss: 0.00002126
Iteration 166/1000 | Loss: 0.00002126
Iteration 167/1000 | Loss: 0.00002126
Iteration 168/1000 | Loss: 0.00002126
Iteration 169/1000 | Loss: 0.00002126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.126274375768844e-05, 2.126274375768844e-05, 2.126274375768844e-05, 2.126274375768844e-05, 2.126274375768844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.126274375768844e-05

Optimization complete. Final v2v error: 3.7766377925872803 mm

Highest mean error: 4.40681266784668 mm for frame 132

Lowest mean error: 3.247299909591675 mm for frame 0

Saving results

Total time: 39.652093172073364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591574
Iteration 2/25 | Loss: 0.00163159
Iteration 3/25 | Loss: 0.00135083
Iteration 4/25 | Loss: 0.00132379
Iteration 5/25 | Loss: 0.00131974
Iteration 6/25 | Loss: 0.00131851
Iteration 7/25 | Loss: 0.00131828
Iteration 8/25 | Loss: 0.00131828
Iteration 9/25 | Loss: 0.00131828
Iteration 10/25 | Loss: 0.00131828
Iteration 11/25 | Loss: 0.00131828
Iteration 12/25 | Loss: 0.00131828
Iteration 13/25 | Loss: 0.00131827
Iteration 14/25 | Loss: 0.00131827
Iteration 15/25 | Loss: 0.00131827
Iteration 16/25 | Loss: 0.00131827
Iteration 17/25 | Loss: 0.00131827
Iteration 18/25 | Loss: 0.00131827
Iteration 19/25 | Loss: 0.00131827
Iteration 20/25 | Loss: 0.00131827
Iteration 21/25 | Loss: 0.00131827
Iteration 22/25 | Loss: 0.00131827
Iteration 23/25 | Loss: 0.00131827
Iteration 24/25 | Loss: 0.00131827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001318267430178821, 0.001318267430178821, 0.001318267430178821, 0.001318267430178821, 0.001318267430178821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001318267430178821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17831361
Iteration 2/25 | Loss: 0.00081693
Iteration 3/25 | Loss: 0.00081693
Iteration 4/25 | Loss: 0.00081693
Iteration 5/25 | Loss: 0.00081693
Iteration 6/25 | Loss: 0.00081693
Iteration 7/25 | Loss: 0.00081693
Iteration 8/25 | Loss: 0.00081693
Iteration 9/25 | Loss: 0.00081693
Iteration 10/25 | Loss: 0.00081693
Iteration 11/25 | Loss: 0.00081693
Iteration 12/25 | Loss: 0.00081693
Iteration 13/25 | Loss: 0.00081693
Iteration 14/25 | Loss: 0.00081693
Iteration 15/25 | Loss: 0.00081693
Iteration 16/25 | Loss: 0.00081693
Iteration 17/25 | Loss: 0.00081693
Iteration 18/25 | Loss: 0.00081693
Iteration 19/25 | Loss: 0.00081693
Iteration 20/25 | Loss: 0.00081693
Iteration 21/25 | Loss: 0.00081693
Iteration 22/25 | Loss: 0.00081693
Iteration 23/25 | Loss: 0.00081693
Iteration 24/25 | Loss: 0.00081693
Iteration 25/25 | Loss: 0.00081693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081693
Iteration 2/1000 | Loss: 0.00004877
Iteration 3/1000 | Loss: 0.00003094
Iteration 4/1000 | Loss: 0.00002439
Iteration 5/1000 | Loss: 0.00002288
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002081
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002008
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00001965
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001960
Iteration 17/1000 | Loss: 0.00001960
Iteration 18/1000 | Loss: 0.00001959
Iteration 19/1000 | Loss: 0.00001959
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001953
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001948
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001935
Iteration 28/1000 | Loss: 0.00001934
Iteration 29/1000 | Loss: 0.00001934
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001933
Iteration 32/1000 | Loss: 0.00001933
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001933
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001932
Iteration 39/1000 | Loss: 0.00001932
Iteration 40/1000 | Loss: 0.00001932
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001926
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001920
Iteration 68/1000 | Loss: 0.00001920
Iteration 69/1000 | Loss: 0.00001920
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001917
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001915
Iteration 89/1000 | Loss: 0.00001915
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001914
Iteration 94/1000 | Loss: 0.00001914
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001913
Iteration 100/1000 | Loss: 0.00001913
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001912
Iteration 108/1000 | Loss: 0.00001912
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001912
Iteration 117/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.912083098432049e-05, 1.912083098432049e-05, 1.912083098432049e-05, 1.912083098432049e-05, 1.912083098432049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.912083098432049e-05

Optimization complete. Final v2v error: 3.6158483028411865 mm

Highest mean error: 5.286028861999512 mm for frame 58

Lowest mean error: 3.1908528804779053 mm for frame 130

Saving results

Total time: 36.876246213912964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817458
Iteration 2/25 | Loss: 0.00169166
Iteration 3/25 | Loss: 0.00139963
Iteration 4/25 | Loss: 0.00135993
Iteration 5/25 | Loss: 0.00136060
Iteration 6/25 | Loss: 0.00134947
Iteration 7/25 | Loss: 0.00134027
Iteration 8/25 | Loss: 0.00133745
Iteration 9/25 | Loss: 0.00133674
Iteration 10/25 | Loss: 0.00133651
Iteration 11/25 | Loss: 0.00133642
Iteration 12/25 | Loss: 0.00133641
Iteration 13/25 | Loss: 0.00133641
Iteration 14/25 | Loss: 0.00133641
Iteration 15/25 | Loss: 0.00133641
Iteration 16/25 | Loss: 0.00133641
Iteration 17/25 | Loss: 0.00133641
Iteration 18/25 | Loss: 0.00133641
Iteration 19/25 | Loss: 0.00133641
Iteration 20/25 | Loss: 0.00133641
Iteration 21/25 | Loss: 0.00133641
Iteration 22/25 | Loss: 0.00133640
Iteration 23/25 | Loss: 0.00133640
Iteration 24/25 | Loss: 0.00133640
Iteration 25/25 | Loss: 0.00133640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37114191
Iteration 2/25 | Loss: 0.00107632
Iteration 3/25 | Loss: 0.00107631
Iteration 4/25 | Loss: 0.00107631
Iteration 5/25 | Loss: 0.00107631
Iteration 6/25 | Loss: 0.00107631
Iteration 7/25 | Loss: 0.00107631
Iteration 8/25 | Loss: 0.00107631
Iteration 9/25 | Loss: 0.00107631
Iteration 10/25 | Loss: 0.00107631
Iteration 11/25 | Loss: 0.00107631
Iteration 12/25 | Loss: 0.00107631
Iteration 13/25 | Loss: 0.00107631
Iteration 14/25 | Loss: 0.00107631
Iteration 15/25 | Loss: 0.00107631
Iteration 16/25 | Loss: 0.00107631
Iteration 17/25 | Loss: 0.00107631
Iteration 18/25 | Loss: 0.00107631
Iteration 19/25 | Loss: 0.00107631
Iteration 20/25 | Loss: 0.00107631
Iteration 21/25 | Loss: 0.00107631
Iteration 22/25 | Loss: 0.00107631
Iteration 23/25 | Loss: 0.00107631
Iteration 24/25 | Loss: 0.00107631
Iteration 25/25 | Loss: 0.00107631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010763061000034213, 0.0010763061000034213, 0.0010763061000034213, 0.0010763061000034213, 0.0010763061000034213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010763061000034213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107631
Iteration 2/1000 | Loss: 0.00004553
Iteration 3/1000 | Loss: 0.00003244
Iteration 4/1000 | Loss: 0.00002823
Iteration 5/1000 | Loss: 0.00002632
Iteration 6/1000 | Loss: 0.00002528
Iteration 7/1000 | Loss: 0.00002448
Iteration 8/1000 | Loss: 0.00002373
Iteration 9/1000 | Loss: 0.00002331
Iteration 10/1000 | Loss: 0.00002296
Iteration 11/1000 | Loss: 0.00002267
Iteration 12/1000 | Loss: 0.00002250
Iteration 13/1000 | Loss: 0.00002228
Iteration 14/1000 | Loss: 0.00002210
Iteration 15/1000 | Loss: 0.00002202
Iteration 16/1000 | Loss: 0.00002196
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00002187
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002180
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002176
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002175
Iteration 28/1000 | Loss: 0.00002174
Iteration 29/1000 | Loss: 0.00002173
Iteration 30/1000 | Loss: 0.00002173
Iteration 31/1000 | Loss: 0.00002172
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002168
Iteration 34/1000 | Loss: 0.00002168
Iteration 35/1000 | Loss: 0.00002168
Iteration 36/1000 | Loss: 0.00002168
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002162
Iteration 43/1000 | Loss: 0.00002162
Iteration 44/1000 | Loss: 0.00002162
Iteration 45/1000 | Loss: 0.00002161
Iteration 46/1000 | Loss: 0.00002161
Iteration 47/1000 | Loss: 0.00002161
Iteration 48/1000 | Loss: 0.00002161
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002161
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00002160
Iteration 55/1000 | Loss: 0.00002160
Iteration 56/1000 | Loss: 0.00002160
Iteration 57/1000 | Loss: 0.00002160
Iteration 58/1000 | Loss: 0.00002160
Iteration 59/1000 | Loss: 0.00002159
Iteration 60/1000 | Loss: 0.00002159
Iteration 61/1000 | Loss: 0.00002159
Iteration 62/1000 | Loss: 0.00002159
Iteration 63/1000 | Loss: 0.00002159
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002158
Iteration 67/1000 | Loss: 0.00002157
Iteration 68/1000 | Loss: 0.00002157
Iteration 69/1000 | Loss: 0.00002157
Iteration 70/1000 | Loss: 0.00002157
Iteration 71/1000 | Loss: 0.00002157
Iteration 72/1000 | Loss: 0.00002157
Iteration 73/1000 | Loss: 0.00002156
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002155
Iteration 78/1000 | Loss: 0.00002155
Iteration 79/1000 | Loss: 0.00002154
Iteration 80/1000 | Loss: 0.00002154
Iteration 81/1000 | Loss: 0.00002154
Iteration 82/1000 | Loss: 0.00002154
Iteration 83/1000 | Loss: 0.00002154
Iteration 84/1000 | Loss: 0.00002154
Iteration 85/1000 | Loss: 0.00002154
Iteration 86/1000 | Loss: 0.00002154
Iteration 87/1000 | Loss: 0.00002154
Iteration 88/1000 | Loss: 0.00002153
Iteration 89/1000 | Loss: 0.00002153
Iteration 90/1000 | Loss: 0.00002153
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002152
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00002152
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002150
Iteration 102/1000 | Loss: 0.00002150
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002150
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002149
Iteration 107/1000 | Loss: 0.00002149
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002149
Iteration 111/1000 | Loss: 0.00002149
Iteration 112/1000 | Loss: 0.00002149
Iteration 113/1000 | Loss: 0.00002149
Iteration 114/1000 | Loss: 0.00002149
Iteration 115/1000 | Loss: 0.00002149
Iteration 116/1000 | Loss: 0.00002149
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002148
Iteration 120/1000 | Loss: 0.00002148
Iteration 121/1000 | Loss: 0.00002148
Iteration 122/1000 | Loss: 0.00002148
Iteration 123/1000 | Loss: 0.00002147
Iteration 124/1000 | Loss: 0.00002147
Iteration 125/1000 | Loss: 0.00002147
Iteration 126/1000 | Loss: 0.00002147
Iteration 127/1000 | Loss: 0.00002147
Iteration 128/1000 | Loss: 0.00002147
Iteration 129/1000 | Loss: 0.00002147
Iteration 130/1000 | Loss: 0.00002147
Iteration 131/1000 | Loss: 0.00002146
Iteration 132/1000 | Loss: 0.00002146
Iteration 133/1000 | Loss: 0.00002146
Iteration 134/1000 | Loss: 0.00002146
Iteration 135/1000 | Loss: 0.00002146
Iteration 136/1000 | Loss: 0.00002146
Iteration 137/1000 | Loss: 0.00002146
Iteration 138/1000 | Loss: 0.00002146
Iteration 139/1000 | Loss: 0.00002146
Iteration 140/1000 | Loss: 0.00002145
Iteration 141/1000 | Loss: 0.00002145
Iteration 142/1000 | Loss: 0.00002145
Iteration 143/1000 | Loss: 0.00002145
Iteration 144/1000 | Loss: 0.00002145
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002145
Iteration 147/1000 | Loss: 0.00002145
Iteration 148/1000 | Loss: 0.00002145
Iteration 149/1000 | Loss: 0.00002145
Iteration 150/1000 | Loss: 0.00002145
Iteration 151/1000 | Loss: 0.00002145
Iteration 152/1000 | Loss: 0.00002145
Iteration 153/1000 | Loss: 0.00002145
Iteration 154/1000 | Loss: 0.00002145
Iteration 155/1000 | Loss: 0.00002145
Iteration 156/1000 | Loss: 0.00002145
Iteration 157/1000 | Loss: 0.00002145
Iteration 158/1000 | Loss: 0.00002145
Iteration 159/1000 | Loss: 0.00002145
Iteration 160/1000 | Loss: 0.00002145
Iteration 161/1000 | Loss: 0.00002145
Iteration 162/1000 | Loss: 0.00002145
Iteration 163/1000 | Loss: 0.00002145
Iteration 164/1000 | Loss: 0.00002145
Iteration 165/1000 | Loss: 0.00002145
Iteration 166/1000 | Loss: 0.00002145
Iteration 167/1000 | Loss: 0.00002145
Iteration 168/1000 | Loss: 0.00002145
Iteration 169/1000 | Loss: 0.00002145
Iteration 170/1000 | Loss: 0.00002145
Iteration 171/1000 | Loss: 0.00002145
Iteration 172/1000 | Loss: 0.00002145
Iteration 173/1000 | Loss: 0.00002145
Iteration 174/1000 | Loss: 0.00002145
Iteration 175/1000 | Loss: 0.00002145
Iteration 176/1000 | Loss: 0.00002145
Iteration 177/1000 | Loss: 0.00002145
Iteration 178/1000 | Loss: 0.00002145
Iteration 179/1000 | Loss: 0.00002145
Iteration 180/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.1447765902848914e-05, 2.1447765902848914e-05, 2.1447765902848914e-05, 2.1447765902848914e-05, 2.1447765902848914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1447765902848914e-05

Optimization complete. Final v2v error: 3.839054584503174 mm

Highest mean error: 5.07052755355835 mm for frame 57

Lowest mean error: 3.009420156478882 mm for frame 74

Saving results

Total time: 52.44941425323486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905251
Iteration 2/25 | Loss: 0.00274040
Iteration 3/25 | Loss: 0.00182639
Iteration 4/25 | Loss: 0.00185173
Iteration 5/25 | Loss: 0.00165695
Iteration 6/25 | Loss: 0.00163957
Iteration 7/25 | Loss: 0.00162681
Iteration 8/25 | Loss: 0.00161881
Iteration 9/25 | Loss: 0.00161946
Iteration 10/25 | Loss: 0.00162259
Iteration 11/25 | Loss: 0.00161044
Iteration 12/25 | Loss: 0.00160602
Iteration 13/25 | Loss: 0.00160013
Iteration 14/25 | Loss: 0.00159784
Iteration 15/25 | Loss: 0.00159620
Iteration 16/25 | Loss: 0.00159780
Iteration 17/25 | Loss: 0.00159711
Iteration 18/25 | Loss: 0.00159641
Iteration 19/25 | Loss: 0.00159381
Iteration 20/25 | Loss: 0.00159529
Iteration 21/25 | Loss: 0.00159565
Iteration 22/25 | Loss: 0.00159236
Iteration 23/25 | Loss: 0.00159126
Iteration 24/25 | Loss: 0.00159079
Iteration 25/25 | Loss: 0.00159055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15801716
Iteration 2/25 | Loss: 0.00674064
Iteration 3/25 | Loss: 0.00330644
Iteration 4/25 | Loss: 0.00330644
Iteration 5/25 | Loss: 0.00330644
Iteration 6/25 | Loss: 0.00330644
Iteration 7/25 | Loss: 0.00330644
Iteration 8/25 | Loss: 0.00330644
Iteration 9/25 | Loss: 0.00330644
Iteration 10/25 | Loss: 0.00330644
Iteration 11/25 | Loss: 0.00330643
Iteration 12/25 | Loss: 0.00330643
Iteration 13/25 | Loss: 0.00330643
Iteration 14/25 | Loss: 0.00330643
Iteration 15/25 | Loss: 0.00330643
Iteration 16/25 | Loss: 0.00330643
Iteration 17/25 | Loss: 0.00330643
Iteration 18/25 | Loss: 0.00330643
Iteration 19/25 | Loss: 0.00330643
Iteration 20/25 | Loss: 0.00330643
Iteration 21/25 | Loss: 0.00330643
Iteration 22/25 | Loss: 0.00330643
Iteration 23/25 | Loss: 0.00330643
Iteration 24/25 | Loss: 0.00330643
Iteration 25/25 | Loss: 0.00330643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00330643
Iteration 2/1000 | Loss: 0.01070324
Iteration 3/1000 | Loss: 0.00776270
Iteration 4/1000 | Loss: 0.00294486
Iteration 5/1000 | Loss: 0.00110735
Iteration 6/1000 | Loss: 0.00056778
Iteration 7/1000 | Loss: 0.00201967
Iteration 8/1000 | Loss: 0.00596341
Iteration 9/1000 | Loss: 0.00127435
Iteration 10/1000 | Loss: 0.00113164
Iteration 11/1000 | Loss: 0.00107314
Iteration 12/1000 | Loss: 0.00071698
Iteration 13/1000 | Loss: 0.00042823
Iteration 14/1000 | Loss: 0.00016850
Iteration 15/1000 | Loss: 0.00578000
Iteration 16/1000 | Loss: 0.00216940
Iteration 17/1000 | Loss: 0.00626062
Iteration 18/1000 | Loss: 0.00254082
Iteration 19/1000 | Loss: 0.00162405
Iteration 20/1000 | Loss: 0.00067335
Iteration 21/1000 | Loss: 0.00093958
Iteration 22/1000 | Loss: 0.00079739
Iteration 23/1000 | Loss: 0.00025668
Iteration 24/1000 | Loss: 0.00031830
Iteration 25/1000 | Loss: 0.00019168
Iteration 26/1000 | Loss: 0.00110117
Iteration 27/1000 | Loss: 0.00012402
Iteration 28/1000 | Loss: 0.00120422
Iteration 29/1000 | Loss: 0.00014903
Iteration 30/1000 | Loss: 0.00005675
Iteration 31/1000 | Loss: 0.00030725
Iteration 32/1000 | Loss: 0.00004381
Iteration 33/1000 | Loss: 0.00058004
Iteration 34/1000 | Loss: 0.00053964
Iteration 35/1000 | Loss: 0.00036209
Iteration 36/1000 | Loss: 0.00133847
Iteration 37/1000 | Loss: 0.00032408
Iteration 38/1000 | Loss: 0.00032251
Iteration 39/1000 | Loss: 0.00009609
Iteration 40/1000 | Loss: 0.00007572
Iteration 41/1000 | Loss: 0.00006147
Iteration 42/1000 | Loss: 0.00004707
Iteration 43/1000 | Loss: 0.00057924
Iteration 44/1000 | Loss: 0.00034989
Iteration 45/1000 | Loss: 0.00028963
Iteration 46/1000 | Loss: 0.00046192
Iteration 47/1000 | Loss: 0.00010953
Iteration 48/1000 | Loss: 0.00003171
Iteration 49/1000 | Loss: 0.00002940
Iteration 50/1000 | Loss: 0.00003295
Iteration 51/1000 | Loss: 0.00037933
Iteration 52/1000 | Loss: 0.00015155
Iteration 53/1000 | Loss: 0.00045615
Iteration 54/1000 | Loss: 0.00013256
Iteration 55/1000 | Loss: 0.00007929
Iteration 56/1000 | Loss: 0.00011240
Iteration 57/1000 | Loss: 0.00102956
Iteration 58/1000 | Loss: 0.00006662
Iteration 59/1000 | Loss: 0.00002602
Iteration 60/1000 | Loss: 0.00020021
Iteration 61/1000 | Loss: 0.00008264
Iteration 62/1000 | Loss: 0.00002371
Iteration 63/1000 | Loss: 0.00021704
Iteration 64/1000 | Loss: 0.00017986
Iteration 65/1000 | Loss: 0.00002284
Iteration 66/1000 | Loss: 0.00067563
Iteration 67/1000 | Loss: 0.00002268
Iteration 68/1000 | Loss: 0.00002128
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002055
Iteration 71/1000 | Loss: 0.00035831
Iteration 72/1000 | Loss: 0.00008116
Iteration 73/1000 | Loss: 0.00030955
Iteration 74/1000 | Loss: 0.00006674
Iteration 75/1000 | Loss: 0.00009923
Iteration 76/1000 | Loss: 0.00006251
Iteration 77/1000 | Loss: 0.00002369
Iteration 78/1000 | Loss: 0.00002870
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00001951
Iteration 85/1000 | Loss: 0.00001951
Iteration 86/1000 | Loss: 0.00001950
Iteration 87/1000 | Loss: 0.00001950
Iteration 88/1000 | Loss: 0.00001946
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001941
Iteration 91/1000 | Loss: 0.00001940
Iteration 92/1000 | Loss: 0.00001940
Iteration 93/1000 | Loss: 0.00001939
Iteration 94/1000 | Loss: 0.00001939
Iteration 95/1000 | Loss: 0.00001938
Iteration 96/1000 | Loss: 0.00002861
Iteration 97/1000 | Loss: 0.00001974
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001921
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001919
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00002375
Iteration 113/1000 | Loss: 0.00002322
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001914
Iteration 120/1000 | Loss: 0.00001914
Iteration 121/1000 | Loss: 0.00001914
Iteration 122/1000 | Loss: 0.00001914
Iteration 123/1000 | Loss: 0.00001913
Iteration 124/1000 | Loss: 0.00001913
Iteration 125/1000 | Loss: 0.00001913
Iteration 126/1000 | Loss: 0.00001913
Iteration 127/1000 | Loss: 0.00001913
Iteration 128/1000 | Loss: 0.00001912
Iteration 129/1000 | Loss: 0.00001912
Iteration 130/1000 | Loss: 0.00001912
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001911
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001911
Iteration 136/1000 | Loss: 0.00001911
Iteration 137/1000 | Loss: 0.00001911
Iteration 138/1000 | Loss: 0.00001911
Iteration 139/1000 | Loss: 0.00001911
Iteration 140/1000 | Loss: 0.00001911
Iteration 141/1000 | Loss: 0.00001911
Iteration 142/1000 | Loss: 0.00001910
Iteration 143/1000 | Loss: 0.00001910
Iteration 144/1000 | Loss: 0.00001910
Iteration 145/1000 | Loss: 0.00001910
Iteration 146/1000 | Loss: 0.00001910
Iteration 147/1000 | Loss: 0.00001910
Iteration 148/1000 | Loss: 0.00001910
Iteration 149/1000 | Loss: 0.00001910
Iteration 150/1000 | Loss: 0.00001910
Iteration 151/1000 | Loss: 0.00001910
Iteration 152/1000 | Loss: 0.00001909
Iteration 153/1000 | Loss: 0.00001909
Iteration 154/1000 | Loss: 0.00001909
Iteration 155/1000 | Loss: 0.00001909
Iteration 156/1000 | Loss: 0.00001909
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Iteration 159/1000 | Loss: 0.00001909
Iteration 160/1000 | Loss: 0.00001909
Iteration 161/1000 | Loss: 0.00001909
Iteration 162/1000 | Loss: 0.00001909
Iteration 163/1000 | Loss: 0.00001908
Iteration 164/1000 | Loss: 0.00001908
Iteration 165/1000 | Loss: 0.00001908
Iteration 166/1000 | Loss: 0.00001908
Iteration 167/1000 | Loss: 0.00001907
Iteration 168/1000 | Loss: 0.00001907
Iteration 169/1000 | Loss: 0.00001907
Iteration 170/1000 | Loss: 0.00001907
Iteration 171/1000 | Loss: 0.00001907
Iteration 172/1000 | Loss: 0.00001906
Iteration 173/1000 | Loss: 0.00001906
Iteration 174/1000 | Loss: 0.00001906
Iteration 175/1000 | Loss: 0.00001906
Iteration 176/1000 | Loss: 0.00001906
Iteration 177/1000 | Loss: 0.00001906
Iteration 178/1000 | Loss: 0.00001906
Iteration 179/1000 | Loss: 0.00001906
Iteration 180/1000 | Loss: 0.00001906
Iteration 181/1000 | Loss: 0.00001906
Iteration 182/1000 | Loss: 0.00001906
Iteration 183/1000 | Loss: 0.00001906
Iteration 184/1000 | Loss: 0.00001906
Iteration 185/1000 | Loss: 0.00001906
Iteration 186/1000 | Loss: 0.00001906
Iteration 187/1000 | Loss: 0.00001906
Iteration 188/1000 | Loss: 0.00001906
Iteration 189/1000 | Loss: 0.00001906
Iteration 190/1000 | Loss: 0.00001906
Iteration 191/1000 | Loss: 0.00001906
Iteration 192/1000 | Loss: 0.00001906
Iteration 193/1000 | Loss: 0.00001906
Iteration 194/1000 | Loss: 0.00001906
Iteration 195/1000 | Loss: 0.00001906
Iteration 196/1000 | Loss: 0.00001906
Iteration 197/1000 | Loss: 0.00001906
Iteration 198/1000 | Loss: 0.00001906
Iteration 199/1000 | Loss: 0.00001906
Iteration 200/1000 | Loss: 0.00001906
Iteration 201/1000 | Loss: 0.00001906
Iteration 202/1000 | Loss: 0.00001906
Iteration 203/1000 | Loss: 0.00001906
Iteration 204/1000 | Loss: 0.00001906
Iteration 205/1000 | Loss: 0.00001906
Iteration 206/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.9063860236201435e-05, 1.9063860236201435e-05, 1.9063860236201435e-05, 1.9063860236201435e-05, 1.9063860236201435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9063860236201435e-05

Optimization complete. Final v2v error: 3.520399570465088 mm

Highest mean error: 5.187597274780273 mm for frame 65

Lowest mean error: 2.816563367843628 mm for frame 28

Saving results

Total time: 183.8856828212738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982157
Iteration 2/25 | Loss: 0.00322551
Iteration 3/25 | Loss: 0.00247796
Iteration 4/25 | Loss: 0.00215894
Iteration 5/25 | Loss: 0.00237501
Iteration 6/25 | Loss: 0.00227223
Iteration 7/25 | Loss: 0.00207593
Iteration 8/25 | Loss: 0.00189390
Iteration 9/25 | Loss: 0.00185656
Iteration 10/25 | Loss: 0.00176017
Iteration 11/25 | Loss: 0.00174189
Iteration 12/25 | Loss: 0.00170549
Iteration 13/25 | Loss: 0.00169567
Iteration 14/25 | Loss: 0.00168936
Iteration 15/25 | Loss: 0.00168835
Iteration 16/25 | Loss: 0.00169101
Iteration 17/25 | Loss: 0.00168510
Iteration 18/25 | Loss: 0.00168190
Iteration 19/25 | Loss: 0.00167964
Iteration 20/25 | Loss: 0.00167784
Iteration 21/25 | Loss: 0.00167699
Iteration 22/25 | Loss: 0.00167543
Iteration 23/25 | Loss: 0.00167402
Iteration 24/25 | Loss: 0.00167297
Iteration 25/25 | Loss: 0.00167147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36780715
Iteration 2/25 | Loss: 0.00646607
Iteration 3/25 | Loss: 0.00473340
Iteration 4/25 | Loss: 0.00473340
Iteration 5/25 | Loss: 0.00473340
Iteration 6/25 | Loss: 0.00473340
Iteration 7/25 | Loss: 0.00473340
Iteration 8/25 | Loss: 0.00473339
Iteration 9/25 | Loss: 0.00473339
Iteration 10/25 | Loss: 0.00473339
Iteration 11/25 | Loss: 0.00473339
Iteration 12/25 | Loss: 0.00473339
Iteration 13/25 | Loss: 0.00473339
Iteration 14/25 | Loss: 0.00473339
Iteration 15/25 | Loss: 0.00473339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0047333939000964165, 0.0047333939000964165, 0.0047333939000964165, 0.0047333939000964165, 0.0047333939000964165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047333939000964165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00473339
Iteration 2/1000 | Loss: 0.00089327
Iteration 3/1000 | Loss: 0.00197216
Iteration 4/1000 | Loss: 0.00088162
Iteration 5/1000 | Loss: 0.00130000
Iteration 6/1000 | Loss: 0.00146761
Iteration 7/1000 | Loss: 0.00112311
Iteration 8/1000 | Loss: 0.00203107
Iteration 9/1000 | Loss: 0.00065761
Iteration 10/1000 | Loss: 0.00420468
Iteration 11/1000 | Loss: 0.00323030
Iteration 12/1000 | Loss: 0.00033148
Iteration 13/1000 | Loss: 0.00198678
Iteration 14/1000 | Loss: 0.00534056
Iteration 15/1000 | Loss: 0.00131537
Iteration 16/1000 | Loss: 0.00112194
Iteration 17/1000 | Loss: 0.00202853
Iteration 18/1000 | Loss: 0.00169593
Iteration 19/1000 | Loss: 0.00106124
Iteration 20/1000 | Loss: 0.00067774
Iteration 21/1000 | Loss: 0.00015356
Iteration 22/1000 | Loss: 0.00231560
Iteration 23/1000 | Loss: 0.00256386
Iteration 24/1000 | Loss: 0.00273006
Iteration 25/1000 | Loss: 0.00365219
Iteration 26/1000 | Loss: 0.00377799
Iteration 27/1000 | Loss: 0.00223801
Iteration 28/1000 | Loss: 0.00253246
Iteration 29/1000 | Loss: 0.00227348
Iteration 30/1000 | Loss: 0.00211297
Iteration 31/1000 | Loss: 0.00122160
Iteration 32/1000 | Loss: 0.00130608
Iteration 33/1000 | Loss: 0.00163255
Iteration 34/1000 | Loss: 0.00074219
Iteration 35/1000 | Loss: 0.00085727
Iteration 36/1000 | Loss: 0.00179191
Iteration 37/1000 | Loss: 0.00592356
Iteration 38/1000 | Loss: 0.00151464
Iteration 39/1000 | Loss: 0.00021365
Iteration 40/1000 | Loss: 0.00046792
Iteration 41/1000 | Loss: 0.00236302
Iteration 42/1000 | Loss: 0.00245727
Iteration 43/1000 | Loss: 0.00111023
Iteration 44/1000 | Loss: 0.00234202
Iteration 45/1000 | Loss: 0.00234956
Iteration 46/1000 | Loss: 0.00030159
Iteration 47/1000 | Loss: 0.00079097
Iteration 48/1000 | Loss: 0.00103956
Iteration 49/1000 | Loss: 0.00079276
Iteration 50/1000 | Loss: 0.00073059
Iteration 51/1000 | Loss: 0.00066406
Iteration 52/1000 | Loss: 0.00053493
Iteration 53/1000 | Loss: 0.00071199
Iteration 54/1000 | Loss: 0.00091688
Iteration 55/1000 | Loss: 0.00073153
Iteration 56/1000 | Loss: 0.00046221
Iteration 57/1000 | Loss: 0.00045783
Iteration 58/1000 | Loss: 0.00129311
Iteration 59/1000 | Loss: 0.00072091
Iteration 60/1000 | Loss: 0.00106218
Iteration 61/1000 | Loss: 0.00214248
Iteration 62/1000 | Loss: 0.00088651
Iteration 63/1000 | Loss: 0.00047389
Iteration 64/1000 | Loss: 0.00055789
Iteration 65/1000 | Loss: 0.00061369
Iteration 66/1000 | Loss: 0.00090873
Iteration 67/1000 | Loss: 0.00047731
Iteration 68/1000 | Loss: 0.00186798
Iteration 69/1000 | Loss: 0.00154832
Iteration 70/1000 | Loss: 0.00084360
Iteration 71/1000 | Loss: 0.00150826
Iteration 72/1000 | Loss: 0.00113891
Iteration 73/1000 | Loss: 0.00057373
Iteration 74/1000 | Loss: 0.00071699
Iteration 75/1000 | Loss: 0.00113803
Iteration 76/1000 | Loss: 0.00070049
Iteration 77/1000 | Loss: 0.00084828
Iteration 78/1000 | Loss: 0.00020555
Iteration 79/1000 | Loss: 0.00050423
Iteration 80/1000 | Loss: 0.00059310
Iteration 81/1000 | Loss: 0.00084197
Iteration 82/1000 | Loss: 0.00091646
Iteration 83/1000 | Loss: 0.00032797
Iteration 84/1000 | Loss: 0.00021785
Iteration 85/1000 | Loss: 0.00050475
Iteration 86/1000 | Loss: 0.00052585
Iteration 87/1000 | Loss: 0.00020408
Iteration 88/1000 | Loss: 0.00049995
Iteration 89/1000 | Loss: 0.00045619
Iteration 90/1000 | Loss: 0.00053414
Iteration 91/1000 | Loss: 0.00058002
Iteration 92/1000 | Loss: 0.00057526
Iteration 93/1000 | Loss: 0.00055079
Iteration 94/1000 | Loss: 0.00013715
Iteration 95/1000 | Loss: 0.00032513
Iteration 96/1000 | Loss: 0.00042142
Iteration 97/1000 | Loss: 0.00045745
Iteration 98/1000 | Loss: 0.00062963
Iteration 99/1000 | Loss: 0.00045437
Iteration 100/1000 | Loss: 0.00009490
Iteration 101/1000 | Loss: 0.00026284
Iteration 102/1000 | Loss: 0.00031733
Iteration 103/1000 | Loss: 0.00074945
Iteration 104/1000 | Loss: 0.00043630
Iteration 105/1000 | Loss: 0.00162185
Iteration 106/1000 | Loss: 0.00094588
Iteration 107/1000 | Loss: 0.00094145
Iteration 108/1000 | Loss: 0.00099019
Iteration 109/1000 | Loss: 0.00087700
Iteration 110/1000 | Loss: 0.00056873
Iteration 111/1000 | Loss: 0.00046510
Iteration 112/1000 | Loss: 0.00026217
Iteration 113/1000 | Loss: 0.00011205
Iteration 114/1000 | Loss: 0.00055211
Iteration 115/1000 | Loss: 0.00049425
Iteration 116/1000 | Loss: 0.00060969
Iteration 117/1000 | Loss: 0.00046448
Iteration 118/1000 | Loss: 0.00057648
Iteration 119/1000 | Loss: 0.00072412
Iteration 120/1000 | Loss: 0.00041024
Iteration 121/1000 | Loss: 0.00040497
Iteration 122/1000 | Loss: 0.00044958
Iteration 123/1000 | Loss: 0.00037496
Iteration 124/1000 | Loss: 0.00006353
Iteration 125/1000 | Loss: 0.00005656
Iteration 126/1000 | Loss: 0.00069250
Iteration 127/1000 | Loss: 0.00005931
Iteration 128/1000 | Loss: 0.00005439
Iteration 129/1000 | Loss: 0.00005175
Iteration 130/1000 | Loss: 0.00004606
Iteration 131/1000 | Loss: 0.00017560
Iteration 132/1000 | Loss: 0.00005264
Iteration 133/1000 | Loss: 0.00015630
Iteration 134/1000 | Loss: 0.00016713
Iteration 135/1000 | Loss: 0.00086558
Iteration 136/1000 | Loss: 0.00040030
Iteration 137/1000 | Loss: 0.00016524
Iteration 138/1000 | Loss: 0.00013591
Iteration 139/1000 | Loss: 0.00015001
Iteration 140/1000 | Loss: 0.00005731
Iteration 141/1000 | Loss: 0.00008807
Iteration 142/1000 | Loss: 0.00004170
Iteration 143/1000 | Loss: 0.00013934
Iteration 144/1000 | Loss: 0.00004461
Iteration 145/1000 | Loss: 0.00015238
Iteration 146/1000 | Loss: 0.00004840
Iteration 147/1000 | Loss: 0.00004129
Iteration 148/1000 | Loss: 0.00003978
Iteration 149/1000 | Loss: 0.00011847
Iteration 150/1000 | Loss: 0.00005061
Iteration 151/1000 | Loss: 0.00008876
Iteration 152/1000 | Loss: 0.00008888
Iteration 153/1000 | Loss: 0.00007492
Iteration 154/1000 | Loss: 0.00006614
Iteration 155/1000 | Loss: 0.00068903
Iteration 156/1000 | Loss: 0.00080223
Iteration 157/1000 | Loss: 0.00088820
Iteration 158/1000 | Loss: 0.00006650
Iteration 159/1000 | Loss: 0.00004154
Iteration 160/1000 | Loss: 0.00003854
Iteration 161/1000 | Loss: 0.00003678
Iteration 162/1000 | Loss: 0.00003567
Iteration 163/1000 | Loss: 0.00026418
Iteration 164/1000 | Loss: 0.00131718
Iteration 165/1000 | Loss: 0.00043216
Iteration 166/1000 | Loss: 0.00030168
Iteration 167/1000 | Loss: 0.00011256
Iteration 168/1000 | Loss: 0.00004316
Iteration 169/1000 | Loss: 0.00015348
Iteration 170/1000 | Loss: 0.00016767
Iteration 171/1000 | Loss: 0.00003405
Iteration 172/1000 | Loss: 0.00003277
Iteration 173/1000 | Loss: 0.00003218
Iteration 174/1000 | Loss: 0.00016152
Iteration 175/1000 | Loss: 0.00012188
Iteration 176/1000 | Loss: 0.00014685
Iteration 177/1000 | Loss: 0.00010527
Iteration 178/1000 | Loss: 0.00026568
Iteration 179/1000 | Loss: 0.00028027
Iteration 180/1000 | Loss: 0.00025864
Iteration 181/1000 | Loss: 0.00027034
Iteration 182/1000 | Loss: 0.00022188
Iteration 183/1000 | Loss: 0.00025070
Iteration 184/1000 | Loss: 0.00016049
Iteration 185/1000 | Loss: 0.00022876
Iteration 186/1000 | Loss: 0.00021061
Iteration 187/1000 | Loss: 0.00022015
Iteration 188/1000 | Loss: 0.00024299
Iteration 189/1000 | Loss: 0.00008236
Iteration 190/1000 | Loss: 0.00010761
Iteration 191/1000 | Loss: 0.00012014
Iteration 192/1000 | Loss: 0.00006651
Iteration 193/1000 | Loss: 0.00008000
Iteration 194/1000 | Loss: 0.00018727
Iteration 195/1000 | Loss: 0.00017801
Iteration 196/1000 | Loss: 0.00019380
Iteration 197/1000 | Loss: 0.00016446
Iteration 198/1000 | Loss: 0.00015175
Iteration 199/1000 | Loss: 0.00014656
Iteration 200/1000 | Loss: 0.00014969
Iteration 201/1000 | Loss: 0.00014220
Iteration 202/1000 | Loss: 0.00014501
Iteration 203/1000 | Loss: 0.00011894
Iteration 204/1000 | Loss: 0.00009069
Iteration 205/1000 | Loss: 0.00011540
Iteration 206/1000 | Loss: 0.00003970
Iteration 207/1000 | Loss: 0.00013778
Iteration 208/1000 | Loss: 0.00014305
Iteration 209/1000 | Loss: 0.00003875
Iteration 210/1000 | Loss: 0.00003374
Iteration 211/1000 | Loss: 0.00003280
Iteration 212/1000 | Loss: 0.00003190
Iteration 213/1000 | Loss: 0.00003104
Iteration 214/1000 | Loss: 0.00003064
Iteration 215/1000 | Loss: 0.00003035
Iteration 216/1000 | Loss: 0.00003032
Iteration 217/1000 | Loss: 0.00003027
Iteration 218/1000 | Loss: 0.00003017
Iteration 219/1000 | Loss: 0.00002996
Iteration 220/1000 | Loss: 0.00002994
Iteration 221/1000 | Loss: 0.00002994
Iteration 222/1000 | Loss: 0.00002983
Iteration 223/1000 | Loss: 0.00002978
Iteration 224/1000 | Loss: 0.00002978
Iteration 225/1000 | Loss: 0.00002977
Iteration 226/1000 | Loss: 0.00002977
Iteration 227/1000 | Loss: 0.00002976
Iteration 228/1000 | Loss: 0.00002976
Iteration 229/1000 | Loss: 0.00002975
Iteration 230/1000 | Loss: 0.00002975
Iteration 231/1000 | Loss: 0.00002974
Iteration 232/1000 | Loss: 0.00002974
Iteration 233/1000 | Loss: 0.00002973
Iteration 234/1000 | Loss: 0.00002973
Iteration 235/1000 | Loss: 0.00002972
Iteration 236/1000 | Loss: 0.00002971
Iteration 237/1000 | Loss: 0.00002971
Iteration 238/1000 | Loss: 0.00002971
Iteration 239/1000 | Loss: 0.00002971
Iteration 240/1000 | Loss: 0.00002970
Iteration 241/1000 | Loss: 0.00002969
Iteration 242/1000 | Loss: 0.00002969
Iteration 243/1000 | Loss: 0.00002968
Iteration 244/1000 | Loss: 0.00002967
Iteration 245/1000 | Loss: 0.00002964
Iteration 246/1000 | Loss: 0.00002962
Iteration 247/1000 | Loss: 0.00048587
Iteration 248/1000 | Loss: 0.00088471
Iteration 249/1000 | Loss: 0.00007904
Iteration 250/1000 | Loss: 0.00003833
Iteration 251/1000 | Loss: 0.00003069
Iteration 252/1000 | Loss: 0.00002804
Iteration 253/1000 | Loss: 0.00002661
Iteration 254/1000 | Loss: 0.00002554
Iteration 255/1000 | Loss: 0.00002468
Iteration 256/1000 | Loss: 0.00002412
Iteration 257/1000 | Loss: 0.00002376
Iteration 258/1000 | Loss: 0.00002352
Iteration 259/1000 | Loss: 0.00002347
Iteration 260/1000 | Loss: 0.00002342
Iteration 261/1000 | Loss: 0.00002342
Iteration 262/1000 | Loss: 0.00002342
Iteration 263/1000 | Loss: 0.00002341
Iteration 264/1000 | Loss: 0.00002341
Iteration 265/1000 | Loss: 0.00002338
Iteration 266/1000 | Loss: 0.00002337
Iteration 267/1000 | Loss: 0.00002330
Iteration 268/1000 | Loss: 0.00002329
Iteration 269/1000 | Loss: 0.00002329
Iteration 270/1000 | Loss: 0.00002328
Iteration 271/1000 | Loss: 0.00002328
Iteration 272/1000 | Loss: 0.00002327
Iteration 273/1000 | Loss: 0.00002327
Iteration 274/1000 | Loss: 0.00002327
Iteration 275/1000 | Loss: 0.00002326
Iteration 276/1000 | Loss: 0.00002326
Iteration 277/1000 | Loss: 0.00002326
Iteration 278/1000 | Loss: 0.00002326
Iteration 279/1000 | Loss: 0.00002326
Iteration 280/1000 | Loss: 0.00002325
Iteration 281/1000 | Loss: 0.00002325
Iteration 282/1000 | Loss: 0.00002325
Iteration 283/1000 | Loss: 0.00002325
Iteration 284/1000 | Loss: 0.00002325
Iteration 285/1000 | Loss: 0.00002325
Iteration 286/1000 | Loss: 0.00002324
Iteration 287/1000 | Loss: 0.00002324
Iteration 288/1000 | Loss: 0.00002324
Iteration 289/1000 | Loss: 0.00002324
Iteration 290/1000 | Loss: 0.00002324
Iteration 291/1000 | Loss: 0.00002323
Iteration 292/1000 | Loss: 0.00002323
Iteration 293/1000 | Loss: 0.00002322
Iteration 294/1000 | Loss: 0.00002322
Iteration 295/1000 | Loss: 0.00002322
Iteration 296/1000 | Loss: 0.00002321
Iteration 297/1000 | Loss: 0.00002321
Iteration 298/1000 | Loss: 0.00002320
Iteration 299/1000 | Loss: 0.00002320
Iteration 300/1000 | Loss: 0.00002320
Iteration 301/1000 | Loss: 0.00002320
Iteration 302/1000 | Loss: 0.00002320
Iteration 303/1000 | Loss: 0.00002319
Iteration 304/1000 | Loss: 0.00002319
Iteration 305/1000 | Loss: 0.00002319
Iteration 306/1000 | Loss: 0.00002318
Iteration 307/1000 | Loss: 0.00002318
Iteration 308/1000 | Loss: 0.00002318
Iteration 309/1000 | Loss: 0.00002317
Iteration 310/1000 | Loss: 0.00002317
Iteration 311/1000 | Loss: 0.00002317
Iteration 312/1000 | Loss: 0.00002316
Iteration 313/1000 | Loss: 0.00002316
Iteration 314/1000 | Loss: 0.00002316
Iteration 315/1000 | Loss: 0.00002316
Iteration 316/1000 | Loss: 0.00002316
Iteration 317/1000 | Loss: 0.00002315
Iteration 318/1000 | Loss: 0.00002315
Iteration 319/1000 | Loss: 0.00002315
Iteration 320/1000 | Loss: 0.00002315
Iteration 321/1000 | Loss: 0.00002315
Iteration 322/1000 | Loss: 0.00002314
Iteration 323/1000 | Loss: 0.00002314
Iteration 324/1000 | Loss: 0.00002314
Iteration 325/1000 | Loss: 0.00002314
Iteration 326/1000 | Loss: 0.00002314
Iteration 327/1000 | Loss: 0.00002313
Iteration 328/1000 | Loss: 0.00002313
Iteration 329/1000 | Loss: 0.00002313
Iteration 330/1000 | Loss: 0.00002313
Iteration 331/1000 | Loss: 0.00002313
Iteration 332/1000 | Loss: 0.00002313
Iteration 333/1000 | Loss: 0.00002313
Iteration 334/1000 | Loss: 0.00002312
Iteration 335/1000 | Loss: 0.00002312
Iteration 336/1000 | Loss: 0.00002312
Iteration 337/1000 | Loss: 0.00002312
Iteration 338/1000 | Loss: 0.00002312
Iteration 339/1000 | Loss: 0.00002311
Iteration 340/1000 | Loss: 0.00002311
Iteration 341/1000 | Loss: 0.00002311
Iteration 342/1000 | Loss: 0.00002311
Iteration 343/1000 | Loss: 0.00002311
Iteration 344/1000 | Loss: 0.00002310
Iteration 345/1000 | Loss: 0.00002310
Iteration 346/1000 | Loss: 0.00002310
Iteration 347/1000 | Loss: 0.00002310
Iteration 348/1000 | Loss: 0.00002310
Iteration 349/1000 | Loss: 0.00002309
Iteration 350/1000 | Loss: 0.00002309
Iteration 351/1000 | Loss: 0.00002309
Iteration 352/1000 | Loss: 0.00002309
Iteration 353/1000 | Loss: 0.00002309
Iteration 354/1000 | Loss: 0.00002309
Iteration 355/1000 | Loss: 0.00002309
Iteration 356/1000 | Loss: 0.00002309
Iteration 357/1000 | Loss: 0.00002309
Iteration 358/1000 | Loss: 0.00002309
Iteration 359/1000 | Loss: 0.00002309
Iteration 360/1000 | Loss: 0.00002309
Iteration 361/1000 | Loss: 0.00002308
Iteration 362/1000 | Loss: 0.00002308
Iteration 363/1000 | Loss: 0.00002308
Iteration 364/1000 | Loss: 0.00002308
Iteration 365/1000 | Loss: 0.00002308
Iteration 366/1000 | Loss: 0.00002308
Iteration 367/1000 | Loss: 0.00002308
Iteration 368/1000 | Loss: 0.00002308
Iteration 369/1000 | Loss: 0.00002307
Iteration 370/1000 | Loss: 0.00002307
Iteration 371/1000 | Loss: 0.00002307
Iteration 372/1000 | Loss: 0.00002307
Iteration 373/1000 | Loss: 0.00002307
Iteration 374/1000 | Loss: 0.00002306
Iteration 375/1000 | Loss: 0.00002306
Iteration 376/1000 | Loss: 0.00002306
Iteration 377/1000 | Loss: 0.00002305
Iteration 378/1000 | Loss: 0.00002305
Iteration 379/1000 | Loss: 0.00002305
Iteration 380/1000 | Loss: 0.00002305
Iteration 381/1000 | Loss: 0.00002305
Iteration 382/1000 | Loss: 0.00002305
Iteration 383/1000 | Loss: 0.00002305
Iteration 384/1000 | Loss: 0.00002304
Iteration 385/1000 | Loss: 0.00002304
Iteration 386/1000 | Loss: 0.00002303
Iteration 387/1000 | Loss: 0.00002303
Iteration 388/1000 | Loss: 0.00002303
Iteration 389/1000 | Loss: 0.00002303
Iteration 390/1000 | Loss: 0.00002302
Iteration 391/1000 | Loss: 0.00002302
Iteration 392/1000 | Loss: 0.00002302
Iteration 393/1000 | Loss: 0.00002301
Iteration 394/1000 | Loss: 0.00002301
Iteration 395/1000 | Loss: 0.00002301
Iteration 396/1000 | Loss: 0.00002301
Iteration 397/1000 | Loss: 0.00002301
Iteration 398/1000 | Loss: 0.00002301
Iteration 399/1000 | Loss: 0.00002301
Iteration 400/1000 | Loss: 0.00002301
Iteration 401/1000 | Loss: 0.00002301
Iteration 402/1000 | Loss: 0.00002301
Iteration 403/1000 | Loss: 0.00002301
Iteration 404/1000 | Loss: 0.00002301
Iteration 405/1000 | Loss: 0.00002300
Iteration 406/1000 | Loss: 0.00002300
Iteration 407/1000 | Loss: 0.00002300
Iteration 408/1000 | Loss: 0.00002300
Iteration 409/1000 | Loss: 0.00002300
Iteration 410/1000 | Loss: 0.00002300
Iteration 411/1000 | Loss: 0.00002300
Iteration 412/1000 | Loss: 0.00002299
Iteration 413/1000 | Loss: 0.00002299
Iteration 414/1000 | Loss: 0.00002299
Iteration 415/1000 | Loss: 0.00002299
Iteration 416/1000 | Loss: 0.00002299
Iteration 417/1000 | Loss: 0.00002299
Iteration 418/1000 | Loss: 0.00002299
Iteration 419/1000 | Loss: 0.00002299
Iteration 420/1000 | Loss: 0.00002299
Iteration 421/1000 | Loss: 0.00002299
Iteration 422/1000 | Loss: 0.00002299
Iteration 423/1000 | Loss: 0.00002299
Iteration 424/1000 | Loss: 0.00002299
Iteration 425/1000 | Loss: 0.00002298
Iteration 426/1000 | Loss: 0.00002298
Iteration 427/1000 | Loss: 0.00002298
Iteration 428/1000 | Loss: 0.00002298
Iteration 429/1000 | Loss: 0.00002298
Iteration 430/1000 | Loss: 0.00002298
Iteration 431/1000 | Loss: 0.00002298
Iteration 432/1000 | Loss: 0.00002298
Iteration 433/1000 | Loss: 0.00002297
Iteration 434/1000 | Loss: 0.00002297
Iteration 435/1000 | Loss: 0.00002297
Iteration 436/1000 | Loss: 0.00002297
Iteration 437/1000 | Loss: 0.00002296
Iteration 438/1000 | Loss: 0.00002296
Iteration 439/1000 | Loss: 0.00002296
Iteration 440/1000 | Loss: 0.00002296
Iteration 441/1000 | Loss: 0.00002296
Iteration 442/1000 | Loss: 0.00002296
Iteration 443/1000 | Loss: 0.00002296
Iteration 444/1000 | Loss: 0.00002295
Iteration 445/1000 | Loss: 0.00002295
Iteration 446/1000 | Loss: 0.00002295
Iteration 447/1000 | Loss: 0.00002295
Iteration 448/1000 | Loss: 0.00002295
Iteration 449/1000 | Loss: 0.00002295
Iteration 450/1000 | Loss: 0.00002295
Iteration 451/1000 | Loss: 0.00002295
Iteration 452/1000 | Loss: 0.00002295
Iteration 453/1000 | Loss: 0.00002295
Iteration 454/1000 | Loss: 0.00002295
Iteration 455/1000 | Loss: 0.00002295
Iteration 456/1000 | Loss: 0.00002295
Iteration 457/1000 | Loss: 0.00002295
Iteration 458/1000 | Loss: 0.00002295
Iteration 459/1000 | Loss: 0.00002295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 459. Stopping optimization.
Last 5 losses: [2.2948108380660415e-05, 2.2948108380660415e-05, 2.2948108380660415e-05, 2.2948108380660415e-05, 2.2948108380660415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2948108380660415e-05

Optimization complete. Final v2v error: 3.785550117492676 mm

Highest mean error: 6.206609725952148 mm for frame 72

Lowest mean error: 2.9670310020446777 mm for frame 200

Saving results

Total time: 408.02284502983093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00683886
Iteration 2/25 | Loss: 0.00135484
Iteration 3/25 | Loss: 0.00124418
Iteration 4/25 | Loss: 0.00123061
Iteration 5/25 | Loss: 0.00122572
Iteration 6/25 | Loss: 0.00122522
Iteration 7/25 | Loss: 0.00122522
Iteration 8/25 | Loss: 0.00122522
Iteration 9/25 | Loss: 0.00122522
Iteration 10/25 | Loss: 0.00122522
Iteration 11/25 | Loss: 0.00122522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001225216081365943, 0.001225216081365943, 0.001225216081365943, 0.001225216081365943, 0.001225216081365943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225216081365943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97563362
Iteration 2/25 | Loss: 0.00110895
Iteration 3/25 | Loss: 0.00110889
Iteration 4/25 | Loss: 0.00110889
Iteration 5/25 | Loss: 0.00110889
Iteration 6/25 | Loss: 0.00110889
Iteration 7/25 | Loss: 0.00110889
Iteration 8/25 | Loss: 0.00110889
Iteration 9/25 | Loss: 0.00110889
Iteration 10/25 | Loss: 0.00110889
Iteration 11/25 | Loss: 0.00110889
Iteration 12/25 | Loss: 0.00110889
Iteration 13/25 | Loss: 0.00110889
Iteration 14/25 | Loss: 0.00110889
Iteration 15/25 | Loss: 0.00110889
Iteration 16/25 | Loss: 0.00110889
Iteration 17/25 | Loss: 0.00110889
Iteration 18/25 | Loss: 0.00110889
Iteration 19/25 | Loss: 0.00110889
Iteration 20/25 | Loss: 0.00110889
Iteration 21/25 | Loss: 0.00110889
Iteration 22/25 | Loss: 0.00110889
Iteration 23/25 | Loss: 0.00110889
Iteration 24/25 | Loss: 0.00110889
Iteration 25/25 | Loss: 0.00110889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110889
Iteration 2/1000 | Loss: 0.00004140
Iteration 3/1000 | Loss: 0.00002573
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00001934
Iteration 6/1000 | Loss: 0.00001830
Iteration 7/1000 | Loss: 0.00001760
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001670
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001590
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001541
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001495
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001494
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001485
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001482
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001481
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.4793068658036646e-05, 1.4793068658036646e-05, 1.4793068658036646e-05, 1.4793068658036646e-05, 1.4793068658036646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4793068658036646e-05

Optimization complete. Final v2v error: 3.3043742179870605 mm

Highest mean error: 3.646493911743164 mm for frame 23

Lowest mean error: 3.0159571170806885 mm for frame 46

Saving results

Total time: 41.26123118400574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496381
Iteration 2/25 | Loss: 0.00133517
Iteration 3/25 | Loss: 0.00125290
Iteration 4/25 | Loss: 0.00123974
Iteration 5/25 | Loss: 0.00123521
Iteration 6/25 | Loss: 0.00123434
Iteration 7/25 | Loss: 0.00123434
Iteration 8/25 | Loss: 0.00123434
Iteration 9/25 | Loss: 0.00123434
Iteration 10/25 | Loss: 0.00123434
Iteration 11/25 | Loss: 0.00123434
Iteration 12/25 | Loss: 0.00123434
Iteration 13/25 | Loss: 0.00123434
Iteration 14/25 | Loss: 0.00123434
Iteration 15/25 | Loss: 0.00123434
Iteration 16/25 | Loss: 0.00123434
Iteration 17/25 | Loss: 0.00123434
Iteration 18/25 | Loss: 0.00123434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001234341529197991, 0.001234341529197991, 0.001234341529197991, 0.001234341529197991, 0.001234341529197991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001234341529197991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.52452564
Iteration 2/25 | Loss: 0.00098729
Iteration 3/25 | Loss: 0.00098725
Iteration 4/25 | Loss: 0.00098725
Iteration 5/25 | Loss: 0.00098725
Iteration 6/25 | Loss: 0.00098725
Iteration 7/25 | Loss: 0.00098725
Iteration 8/25 | Loss: 0.00098725
Iteration 9/25 | Loss: 0.00098725
Iteration 10/25 | Loss: 0.00098725
Iteration 11/25 | Loss: 0.00098725
Iteration 12/25 | Loss: 0.00098725
Iteration 13/25 | Loss: 0.00098725
Iteration 14/25 | Loss: 0.00098725
Iteration 15/25 | Loss: 0.00098725
Iteration 16/25 | Loss: 0.00098725
Iteration 17/25 | Loss: 0.00098725
Iteration 18/25 | Loss: 0.00098725
Iteration 19/25 | Loss: 0.00098725
Iteration 20/25 | Loss: 0.00098725
Iteration 21/25 | Loss: 0.00098725
Iteration 22/25 | Loss: 0.00098725
Iteration 23/25 | Loss: 0.00098725
Iteration 24/25 | Loss: 0.00098725
Iteration 25/25 | Loss: 0.00098725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009872474474832416, 0.0009872474474832416, 0.0009872474474832416, 0.0009872474474832416, 0.0009872474474832416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009872474474832416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098725
Iteration 2/1000 | Loss: 0.00002683
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001756
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001549
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001378
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001351
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001350
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001340
Iteration 24/1000 | Loss: 0.00001340
Iteration 25/1000 | Loss: 0.00001339
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001337
Iteration 28/1000 | Loss: 0.00001337
Iteration 29/1000 | Loss: 0.00001337
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001335
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001328
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001327
Iteration 44/1000 | Loss: 0.00001325
Iteration 45/1000 | Loss: 0.00001324
Iteration 46/1000 | Loss: 0.00001324
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001323
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001322
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001321
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001315
Iteration 71/1000 | Loss: 0.00001315
Iteration 72/1000 | Loss: 0.00001315
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001314
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001303
Iteration 143/1000 | Loss: 0.00001303
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001303
Iteration 146/1000 | Loss: 0.00001303
Iteration 147/1000 | Loss: 0.00001303
Iteration 148/1000 | Loss: 0.00001302
Iteration 149/1000 | Loss: 0.00001302
Iteration 150/1000 | Loss: 0.00001302
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001302
Iteration 153/1000 | Loss: 0.00001302
Iteration 154/1000 | Loss: 0.00001302
Iteration 155/1000 | Loss: 0.00001302
Iteration 156/1000 | Loss: 0.00001302
Iteration 157/1000 | Loss: 0.00001302
Iteration 158/1000 | Loss: 0.00001302
Iteration 159/1000 | Loss: 0.00001302
Iteration 160/1000 | Loss: 0.00001302
Iteration 161/1000 | Loss: 0.00001302
Iteration 162/1000 | Loss: 0.00001302
Iteration 163/1000 | Loss: 0.00001302
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Iteration 168/1000 | Loss: 0.00001301
Iteration 169/1000 | Loss: 0.00001301
Iteration 170/1000 | Loss: 0.00001301
Iteration 171/1000 | Loss: 0.00001301
Iteration 172/1000 | Loss: 0.00001301
Iteration 173/1000 | Loss: 0.00001301
Iteration 174/1000 | Loss: 0.00001301
Iteration 175/1000 | Loss: 0.00001301
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001300
Iteration 179/1000 | Loss: 0.00001300
Iteration 180/1000 | Loss: 0.00001300
Iteration 181/1000 | Loss: 0.00001300
Iteration 182/1000 | Loss: 0.00001300
Iteration 183/1000 | Loss: 0.00001300
Iteration 184/1000 | Loss: 0.00001300
Iteration 185/1000 | Loss: 0.00001300
Iteration 186/1000 | Loss: 0.00001300
Iteration 187/1000 | Loss: 0.00001300
Iteration 188/1000 | Loss: 0.00001300
Iteration 189/1000 | Loss: 0.00001299
Iteration 190/1000 | Loss: 0.00001299
Iteration 191/1000 | Loss: 0.00001299
Iteration 192/1000 | Loss: 0.00001299
Iteration 193/1000 | Loss: 0.00001299
Iteration 194/1000 | Loss: 0.00001299
Iteration 195/1000 | Loss: 0.00001299
Iteration 196/1000 | Loss: 0.00001299
Iteration 197/1000 | Loss: 0.00001299
Iteration 198/1000 | Loss: 0.00001299
Iteration 199/1000 | Loss: 0.00001299
Iteration 200/1000 | Loss: 0.00001299
Iteration 201/1000 | Loss: 0.00001299
Iteration 202/1000 | Loss: 0.00001298
Iteration 203/1000 | Loss: 0.00001298
Iteration 204/1000 | Loss: 0.00001298
Iteration 205/1000 | Loss: 0.00001298
Iteration 206/1000 | Loss: 0.00001298
Iteration 207/1000 | Loss: 0.00001298
Iteration 208/1000 | Loss: 0.00001298
Iteration 209/1000 | Loss: 0.00001298
Iteration 210/1000 | Loss: 0.00001298
Iteration 211/1000 | Loss: 0.00001298
Iteration 212/1000 | Loss: 0.00001298
Iteration 213/1000 | Loss: 0.00001298
Iteration 214/1000 | Loss: 0.00001298
Iteration 215/1000 | Loss: 0.00001298
Iteration 216/1000 | Loss: 0.00001298
Iteration 217/1000 | Loss: 0.00001297
Iteration 218/1000 | Loss: 0.00001297
Iteration 219/1000 | Loss: 0.00001297
Iteration 220/1000 | Loss: 0.00001297
Iteration 221/1000 | Loss: 0.00001297
Iteration 222/1000 | Loss: 0.00001297
Iteration 223/1000 | Loss: 0.00001297
Iteration 224/1000 | Loss: 0.00001297
Iteration 225/1000 | Loss: 0.00001297
Iteration 226/1000 | Loss: 0.00001297
Iteration 227/1000 | Loss: 0.00001297
Iteration 228/1000 | Loss: 0.00001297
Iteration 229/1000 | Loss: 0.00001297
Iteration 230/1000 | Loss: 0.00001297
Iteration 231/1000 | Loss: 0.00001297
Iteration 232/1000 | Loss: 0.00001297
Iteration 233/1000 | Loss: 0.00001296
Iteration 234/1000 | Loss: 0.00001296
Iteration 235/1000 | Loss: 0.00001296
Iteration 236/1000 | Loss: 0.00001296
Iteration 237/1000 | Loss: 0.00001296
Iteration 238/1000 | Loss: 0.00001296
Iteration 239/1000 | Loss: 0.00001296
Iteration 240/1000 | Loss: 0.00001296
Iteration 241/1000 | Loss: 0.00001296
Iteration 242/1000 | Loss: 0.00001296
Iteration 243/1000 | Loss: 0.00001296
Iteration 244/1000 | Loss: 0.00001296
Iteration 245/1000 | Loss: 0.00001296
Iteration 246/1000 | Loss: 0.00001296
Iteration 247/1000 | Loss: 0.00001296
Iteration 248/1000 | Loss: 0.00001296
Iteration 249/1000 | Loss: 0.00001296
Iteration 250/1000 | Loss: 0.00001296
Iteration 251/1000 | Loss: 0.00001296
Iteration 252/1000 | Loss: 0.00001296
Iteration 253/1000 | Loss: 0.00001296
Iteration 254/1000 | Loss: 0.00001296
Iteration 255/1000 | Loss: 0.00001296
Iteration 256/1000 | Loss: 0.00001296
Iteration 257/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.2956368664163165e-05, 1.2956368664163165e-05, 1.2956368664163165e-05, 1.2956368664163165e-05, 1.2956368664163165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2956368664163165e-05

Optimization complete. Final v2v error: 3.0673232078552246 mm

Highest mean error: 3.5612640380859375 mm for frame 120

Lowest mean error: 2.6290154457092285 mm for frame 5

Saving results

Total time: 46.42395091056824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871309
Iteration 2/25 | Loss: 0.00211937
Iteration 3/25 | Loss: 0.00150293
Iteration 4/25 | Loss: 0.00147841
Iteration 5/25 | Loss: 0.00147178
Iteration 6/25 | Loss: 0.00146977
Iteration 7/25 | Loss: 0.00146977
Iteration 8/25 | Loss: 0.00146977
Iteration 9/25 | Loss: 0.00146977
Iteration 10/25 | Loss: 0.00146977
Iteration 11/25 | Loss: 0.00146977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014697725418955088, 0.0014697725418955088, 0.0014697725418955088, 0.0014697725418955088, 0.0014697725418955088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014697725418955088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90674269
Iteration 2/25 | Loss: 0.00105976
Iteration 3/25 | Loss: 0.00105976
Iteration 4/25 | Loss: 0.00105976
Iteration 5/25 | Loss: 0.00105975
Iteration 6/25 | Loss: 0.00105975
Iteration 7/25 | Loss: 0.00105975
Iteration 8/25 | Loss: 0.00105975
Iteration 9/25 | Loss: 0.00105975
Iteration 10/25 | Loss: 0.00105975
Iteration 11/25 | Loss: 0.00105975
Iteration 12/25 | Loss: 0.00105975
Iteration 13/25 | Loss: 0.00105975
Iteration 14/25 | Loss: 0.00105975
Iteration 15/25 | Loss: 0.00105975
Iteration 16/25 | Loss: 0.00105975
Iteration 17/25 | Loss: 0.00105975
Iteration 18/25 | Loss: 0.00105975
Iteration 19/25 | Loss: 0.00105975
Iteration 20/25 | Loss: 0.00105975
Iteration 21/25 | Loss: 0.00105975
Iteration 22/25 | Loss: 0.00105975
Iteration 23/25 | Loss: 0.00105975
Iteration 24/25 | Loss: 0.00105975
Iteration 25/25 | Loss: 0.00105975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105975
Iteration 2/1000 | Loss: 0.00008203
Iteration 3/1000 | Loss: 0.00005739
Iteration 4/1000 | Loss: 0.00005035
Iteration 5/1000 | Loss: 0.00004862
Iteration 6/1000 | Loss: 0.00004699
Iteration 7/1000 | Loss: 0.00004591
Iteration 8/1000 | Loss: 0.00004458
Iteration 9/1000 | Loss: 0.00004371
Iteration 10/1000 | Loss: 0.00004279
Iteration 11/1000 | Loss: 0.00004200
Iteration 12/1000 | Loss: 0.00004145
Iteration 13/1000 | Loss: 0.00004094
Iteration 14/1000 | Loss: 0.00004035
Iteration 15/1000 | Loss: 0.00004004
Iteration 16/1000 | Loss: 0.00003974
Iteration 17/1000 | Loss: 0.00003951
Iteration 18/1000 | Loss: 0.00003926
Iteration 19/1000 | Loss: 0.00003902
Iteration 20/1000 | Loss: 0.00003882
Iteration 21/1000 | Loss: 0.00003864
Iteration 22/1000 | Loss: 0.00003843
Iteration 23/1000 | Loss: 0.00003822
Iteration 24/1000 | Loss: 0.00003802
Iteration 25/1000 | Loss: 0.00003783
Iteration 26/1000 | Loss: 0.00003772
Iteration 27/1000 | Loss: 0.00003766
Iteration 28/1000 | Loss: 0.00003756
Iteration 29/1000 | Loss: 0.00003754
Iteration 30/1000 | Loss: 0.00003747
Iteration 31/1000 | Loss: 0.00003746
Iteration 32/1000 | Loss: 0.00003740
Iteration 33/1000 | Loss: 0.00003738
Iteration 34/1000 | Loss: 0.00003737
Iteration 35/1000 | Loss: 0.00003737
Iteration 36/1000 | Loss: 0.00003736
Iteration 37/1000 | Loss: 0.00003736
Iteration 38/1000 | Loss: 0.00003736
Iteration 39/1000 | Loss: 0.00003735
Iteration 40/1000 | Loss: 0.00003734
Iteration 41/1000 | Loss: 0.00003734
Iteration 42/1000 | Loss: 0.00003733
Iteration 43/1000 | Loss: 0.00003733
Iteration 44/1000 | Loss: 0.00003732
Iteration 45/1000 | Loss: 0.00003732
Iteration 46/1000 | Loss: 0.00003731
Iteration 47/1000 | Loss: 0.00003731
Iteration 48/1000 | Loss: 0.00003731
Iteration 49/1000 | Loss: 0.00003731
Iteration 50/1000 | Loss: 0.00003730
Iteration 51/1000 | Loss: 0.00003729
Iteration 52/1000 | Loss: 0.00003729
Iteration 53/1000 | Loss: 0.00003728
Iteration 54/1000 | Loss: 0.00003728
Iteration 55/1000 | Loss: 0.00003727
Iteration 56/1000 | Loss: 0.00003724
Iteration 57/1000 | Loss: 0.00003724
Iteration 58/1000 | Loss: 0.00003723
Iteration 59/1000 | Loss: 0.00003723
Iteration 60/1000 | Loss: 0.00003722
Iteration 61/1000 | Loss: 0.00003722
Iteration 62/1000 | Loss: 0.00003721
Iteration 63/1000 | Loss: 0.00003720
Iteration 64/1000 | Loss: 0.00003720
Iteration 65/1000 | Loss: 0.00003719
Iteration 66/1000 | Loss: 0.00003719
Iteration 67/1000 | Loss: 0.00003719
Iteration 68/1000 | Loss: 0.00003719
Iteration 69/1000 | Loss: 0.00003719
Iteration 70/1000 | Loss: 0.00003719
Iteration 71/1000 | Loss: 0.00003718
Iteration 72/1000 | Loss: 0.00003718
Iteration 73/1000 | Loss: 0.00003718
Iteration 74/1000 | Loss: 0.00003718
Iteration 75/1000 | Loss: 0.00003718
Iteration 76/1000 | Loss: 0.00003718
Iteration 77/1000 | Loss: 0.00003717
Iteration 78/1000 | Loss: 0.00003717
Iteration 79/1000 | Loss: 0.00003716
Iteration 80/1000 | Loss: 0.00003716
Iteration 81/1000 | Loss: 0.00003716
Iteration 82/1000 | Loss: 0.00003716
Iteration 83/1000 | Loss: 0.00003716
Iteration 84/1000 | Loss: 0.00003716
Iteration 85/1000 | Loss: 0.00003716
Iteration 86/1000 | Loss: 0.00003716
Iteration 87/1000 | Loss: 0.00003716
Iteration 88/1000 | Loss: 0.00003716
Iteration 89/1000 | Loss: 0.00003716
Iteration 90/1000 | Loss: 0.00003716
Iteration 91/1000 | Loss: 0.00003715
Iteration 92/1000 | Loss: 0.00003715
Iteration 93/1000 | Loss: 0.00003715
Iteration 94/1000 | Loss: 0.00003715
Iteration 95/1000 | Loss: 0.00003715
Iteration 96/1000 | Loss: 0.00003715
Iteration 97/1000 | Loss: 0.00003715
Iteration 98/1000 | Loss: 0.00003715
Iteration 99/1000 | Loss: 0.00003715
Iteration 100/1000 | Loss: 0.00003715
Iteration 101/1000 | Loss: 0.00003714
Iteration 102/1000 | Loss: 0.00003713
Iteration 103/1000 | Loss: 0.00003713
Iteration 104/1000 | Loss: 0.00003713
Iteration 105/1000 | Loss: 0.00003713
Iteration 106/1000 | Loss: 0.00003712
Iteration 107/1000 | Loss: 0.00003712
Iteration 108/1000 | Loss: 0.00003712
Iteration 109/1000 | Loss: 0.00003711
Iteration 110/1000 | Loss: 0.00003711
Iteration 111/1000 | Loss: 0.00003711
Iteration 112/1000 | Loss: 0.00003711
Iteration 113/1000 | Loss: 0.00003711
Iteration 114/1000 | Loss: 0.00003711
Iteration 115/1000 | Loss: 0.00003711
Iteration 116/1000 | Loss: 0.00003711
Iteration 117/1000 | Loss: 0.00003711
Iteration 118/1000 | Loss: 0.00003711
Iteration 119/1000 | Loss: 0.00003711
Iteration 120/1000 | Loss: 0.00003710
Iteration 121/1000 | Loss: 0.00003710
Iteration 122/1000 | Loss: 0.00003710
Iteration 123/1000 | Loss: 0.00003710
Iteration 124/1000 | Loss: 0.00003710
Iteration 125/1000 | Loss: 0.00003710
Iteration 126/1000 | Loss: 0.00003710
Iteration 127/1000 | Loss: 0.00003710
Iteration 128/1000 | Loss: 0.00003710
Iteration 129/1000 | Loss: 0.00003710
Iteration 130/1000 | Loss: 0.00003709
Iteration 131/1000 | Loss: 0.00003709
Iteration 132/1000 | Loss: 0.00003709
Iteration 133/1000 | Loss: 0.00003709
Iteration 134/1000 | Loss: 0.00003709
Iteration 135/1000 | Loss: 0.00003709
Iteration 136/1000 | Loss: 0.00003709
Iteration 137/1000 | Loss: 0.00003709
Iteration 138/1000 | Loss: 0.00003709
Iteration 139/1000 | Loss: 0.00003709
Iteration 140/1000 | Loss: 0.00003709
Iteration 141/1000 | Loss: 0.00003709
Iteration 142/1000 | Loss: 0.00003709
Iteration 143/1000 | Loss: 0.00003709
Iteration 144/1000 | Loss: 0.00003709
Iteration 145/1000 | Loss: 0.00003709
Iteration 146/1000 | Loss: 0.00003709
Iteration 147/1000 | Loss: 0.00003709
Iteration 148/1000 | Loss: 0.00003709
Iteration 149/1000 | Loss: 0.00003709
Iteration 150/1000 | Loss: 0.00003709
Iteration 151/1000 | Loss: 0.00003709
Iteration 152/1000 | Loss: 0.00003709
Iteration 153/1000 | Loss: 0.00003709
Iteration 154/1000 | Loss: 0.00003709
Iteration 155/1000 | Loss: 0.00003709
Iteration 156/1000 | Loss: 0.00003709
Iteration 157/1000 | Loss: 0.00003709
Iteration 158/1000 | Loss: 0.00003709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.709417796926573e-05, 3.709417796926573e-05, 3.709417796926573e-05, 3.709417796926573e-05, 3.709417796926573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.709417796926573e-05

Optimization complete. Final v2v error: 5.0983476638793945 mm

Highest mean error: 5.637660026550293 mm for frame 14

Lowest mean error: 4.614997863769531 mm for frame 157

Saving results

Total time: 67.13680744171143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865753
Iteration 2/25 | Loss: 0.00139763
Iteration 3/25 | Loss: 0.00130381
Iteration 4/25 | Loss: 0.00128438
Iteration 5/25 | Loss: 0.00127753
Iteration 6/25 | Loss: 0.00127581
Iteration 7/25 | Loss: 0.00127570
Iteration 8/25 | Loss: 0.00127570
Iteration 9/25 | Loss: 0.00127570
Iteration 10/25 | Loss: 0.00127570
Iteration 11/25 | Loss: 0.00127570
Iteration 12/25 | Loss: 0.00127570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012756962096318603, 0.0012756962096318603, 0.0012756962096318603, 0.0012756962096318603, 0.0012756962096318603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012756962096318603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36701167
Iteration 2/25 | Loss: 0.00097950
Iteration 3/25 | Loss: 0.00097947
Iteration 4/25 | Loss: 0.00097947
Iteration 5/25 | Loss: 0.00097947
Iteration 6/25 | Loss: 0.00097947
Iteration 7/25 | Loss: 0.00097947
Iteration 8/25 | Loss: 0.00097947
Iteration 9/25 | Loss: 0.00097947
Iteration 10/25 | Loss: 0.00097947
Iteration 11/25 | Loss: 0.00097947
Iteration 12/25 | Loss: 0.00097947
Iteration 13/25 | Loss: 0.00097947
Iteration 14/25 | Loss: 0.00097947
Iteration 15/25 | Loss: 0.00097947
Iteration 16/25 | Loss: 0.00097947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009794686920940876, 0.0009794686920940876, 0.0009794686920940876, 0.0009794686920940876, 0.0009794686920940876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009794686920940876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097947
Iteration 2/1000 | Loss: 0.00004557
Iteration 3/1000 | Loss: 0.00003075
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00002417
Iteration 6/1000 | Loss: 0.00002303
Iteration 7/1000 | Loss: 0.00002237
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002129
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002077
Iteration 12/1000 | Loss: 0.00002060
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002042
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002040
Iteration 18/1000 | Loss: 0.00002033
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002018
Iteration 21/1000 | Loss: 0.00002018
Iteration 22/1000 | Loss: 0.00002014
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002007
Iteration 26/1000 | Loss: 0.00002007
Iteration 27/1000 | Loss: 0.00002007
Iteration 28/1000 | Loss: 0.00002006
Iteration 29/1000 | Loss: 0.00002006
Iteration 30/1000 | Loss: 0.00002006
Iteration 31/1000 | Loss: 0.00002005
Iteration 32/1000 | Loss: 0.00002005
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002002
Iteration 40/1000 | Loss: 0.00002002
Iteration 41/1000 | Loss: 0.00002002
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002001
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001998
Iteration 52/1000 | Loss: 0.00001998
Iteration 53/1000 | Loss: 0.00001998
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001998
Iteration 56/1000 | Loss: 0.00001998
Iteration 57/1000 | Loss: 0.00001998
Iteration 58/1000 | Loss: 0.00001998
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001997
Iteration 61/1000 | Loss: 0.00001997
Iteration 62/1000 | Loss: 0.00001996
Iteration 63/1000 | Loss: 0.00001996
Iteration 64/1000 | Loss: 0.00001995
Iteration 65/1000 | Loss: 0.00001995
Iteration 66/1000 | Loss: 0.00001995
Iteration 67/1000 | Loss: 0.00001995
Iteration 68/1000 | Loss: 0.00001994
Iteration 69/1000 | Loss: 0.00001994
Iteration 70/1000 | Loss: 0.00001994
Iteration 71/1000 | Loss: 0.00001994
Iteration 72/1000 | Loss: 0.00001994
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00001992
Iteration 78/1000 | Loss: 0.00001992
Iteration 79/1000 | Loss: 0.00001991
Iteration 80/1000 | Loss: 0.00001991
Iteration 81/1000 | Loss: 0.00001990
Iteration 82/1000 | Loss: 0.00001990
Iteration 83/1000 | Loss: 0.00001990
Iteration 84/1000 | Loss: 0.00001990
Iteration 85/1000 | Loss: 0.00001989
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001987
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001986
Iteration 96/1000 | Loss: 0.00001986
Iteration 97/1000 | Loss: 0.00001986
Iteration 98/1000 | Loss: 0.00001986
Iteration 99/1000 | Loss: 0.00001986
Iteration 100/1000 | Loss: 0.00001985
Iteration 101/1000 | Loss: 0.00001985
Iteration 102/1000 | Loss: 0.00001985
Iteration 103/1000 | Loss: 0.00001985
Iteration 104/1000 | Loss: 0.00001985
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001984
Iteration 109/1000 | Loss: 0.00001984
Iteration 110/1000 | Loss: 0.00001984
Iteration 111/1000 | Loss: 0.00001984
Iteration 112/1000 | Loss: 0.00001984
Iteration 113/1000 | Loss: 0.00001984
Iteration 114/1000 | Loss: 0.00001984
Iteration 115/1000 | Loss: 0.00001984
Iteration 116/1000 | Loss: 0.00001984
Iteration 117/1000 | Loss: 0.00001984
Iteration 118/1000 | Loss: 0.00001984
Iteration 119/1000 | Loss: 0.00001984
Iteration 120/1000 | Loss: 0.00001984
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001984
Iteration 129/1000 | Loss: 0.00001984
Iteration 130/1000 | Loss: 0.00001984
Iteration 131/1000 | Loss: 0.00001984
Iteration 132/1000 | Loss: 0.00001984
Iteration 133/1000 | Loss: 0.00001984
Iteration 134/1000 | Loss: 0.00001984
Iteration 135/1000 | Loss: 0.00001984
Iteration 136/1000 | Loss: 0.00001984
Iteration 137/1000 | Loss: 0.00001984
Iteration 138/1000 | Loss: 0.00001984
Iteration 139/1000 | Loss: 0.00001984
Iteration 140/1000 | Loss: 0.00001984
Iteration 141/1000 | Loss: 0.00001984
Iteration 142/1000 | Loss: 0.00001984
Iteration 143/1000 | Loss: 0.00001984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.9835064449580386e-05, 1.9835064449580386e-05, 1.9835064449580386e-05, 1.9835064449580386e-05, 1.9835064449580386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9835064449580386e-05

Optimization complete. Final v2v error: 3.7413341999053955 mm

Highest mean error: 5.2929301261901855 mm for frame 69

Lowest mean error: 3.1378235816955566 mm for frame 97

Saving results

Total time: 38.75019931793213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710108
Iteration 2/25 | Loss: 0.00146020
Iteration 3/25 | Loss: 0.00135170
Iteration 4/25 | Loss: 0.00134131
Iteration 5/25 | Loss: 0.00133821
Iteration 6/25 | Loss: 0.00133821
Iteration 7/25 | Loss: 0.00133821
Iteration 8/25 | Loss: 0.00133821
Iteration 9/25 | Loss: 0.00133821
Iteration 10/25 | Loss: 0.00133821
Iteration 11/25 | Loss: 0.00133821
Iteration 12/25 | Loss: 0.00133821
Iteration 13/25 | Loss: 0.00133821
Iteration 14/25 | Loss: 0.00133821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013382103061303496, 0.0013382103061303496, 0.0013382103061303496, 0.0013382103061303496, 0.0013382103061303496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013382103061303496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37381911
Iteration 2/25 | Loss: 0.00093795
Iteration 3/25 | Loss: 0.00093795
Iteration 4/25 | Loss: 0.00093795
Iteration 5/25 | Loss: 0.00093795
Iteration 6/25 | Loss: 0.00093795
Iteration 7/25 | Loss: 0.00093795
Iteration 8/25 | Loss: 0.00093795
Iteration 9/25 | Loss: 0.00093795
Iteration 10/25 | Loss: 0.00093795
Iteration 11/25 | Loss: 0.00093795
Iteration 12/25 | Loss: 0.00093795
Iteration 13/25 | Loss: 0.00093795
Iteration 14/25 | Loss: 0.00093795
Iteration 15/25 | Loss: 0.00093795
Iteration 16/25 | Loss: 0.00093795
Iteration 17/25 | Loss: 0.00093795
Iteration 18/25 | Loss: 0.00093795
Iteration 19/25 | Loss: 0.00093795
Iteration 20/25 | Loss: 0.00093795
Iteration 21/25 | Loss: 0.00093795
Iteration 22/25 | Loss: 0.00093795
Iteration 23/25 | Loss: 0.00093795
Iteration 24/25 | Loss: 0.00093795
Iteration 25/25 | Loss: 0.00093795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093795
Iteration 2/1000 | Loss: 0.00004585
Iteration 3/1000 | Loss: 0.00003474
Iteration 4/1000 | Loss: 0.00003031
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002620
Iteration 8/1000 | Loss: 0.00002563
Iteration 9/1000 | Loss: 0.00002532
Iteration 10/1000 | Loss: 0.00002496
Iteration 11/1000 | Loss: 0.00002465
Iteration 12/1000 | Loss: 0.00002441
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002425
Iteration 15/1000 | Loss: 0.00002416
Iteration 16/1000 | Loss: 0.00002410
Iteration 17/1000 | Loss: 0.00002403
Iteration 18/1000 | Loss: 0.00002398
Iteration 19/1000 | Loss: 0.00002388
Iteration 20/1000 | Loss: 0.00002387
Iteration 21/1000 | Loss: 0.00002386
Iteration 22/1000 | Loss: 0.00002386
Iteration 23/1000 | Loss: 0.00002385
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002366
Iteration 26/1000 | Loss: 0.00002365
Iteration 27/1000 | Loss: 0.00002365
Iteration 28/1000 | Loss: 0.00002364
Iteration 29/1000 | Loss: 0.00002360
Iteration 30/1000 | Loss: 0.00002360
Iteration 31/1000 | Loss: 0.00002360
Iteration 32/1000 | Loss: 0.00002360
Iteration 33/1000 | Loss: 0.00002360
Iteration 34/1000 | Loss: 0.00002359
Iteration 35/1000 | Loss: 0.00002359
Iteration 36/1000 | Loss: 0.00002358
Iteration 37/1000 | Loss: 0.00002358
Iteration 38/1000 | Loss: 0.00002358
Iteration 39/1000 | Loss: 0.00002358
Iteration 40/1000 | Loss: 0.00002358
Iteration 41/1000 | Loss: 0.00002358
Iteration 42/1000 | Loss: 0.00002357
Iteration 43/1000 | Loss: 0.00002357
Iteration 44/1000 | Loss: 0.00002357
Iteration 45/1000 | Loss: 0.00002356
Iteration 46/1000 | Loss: 0.00002356
Iteration 47/1000 | Loss: 0.00002356
Iteration 48/1000 | Loss: 0.00002356
Iteration 49/1000 | Loss: 0.00002355
Iteration 50/1000 | Loss: 0.00002355
Iteration 51/1000 | Loss: 0.00002355
Iteration 52/1000 | Loss: 0.00002355
Iteration 53/1000 | Loss: 0.00002354
Iteration 54/1000 | Loss: 0.00002354
Iteration 55/1000 | Loss: 0.00002354
Iteration 56/1000 | Loss: 0.00002354
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002353
Iteration 59/1000 | Loss: 0.00002353
Iteration 60/1000 | Loss: 0.00002353
Iteration 61/1000 | Loss: 0.00002353
Iteration 62/1000 | Loss: 0.00002353
Iteration 63/1000 | Loss: 0.00002352
Iteration 64/1000 | Loss: 0.00002352
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002351
Iteration 68/1000 | Loss: 0.00002351
Iteration 69/1000 | Loss: 0.00002351
Iteration 70/1000 | Loss: 0.00002350
Iteration 71/1000 | Loss: 0.00002350
Iteration 72/1000 | Loss: 0.00002350
Iteration 73/1000 | Loss: 0.00002349
Iteration 74/1000 | Loss: 0.00002349
Iteration 75/1000 | Loss: 0.00002349
Iteration 76/1000 | Loss: 0.00002349
Iteration 77/1000 | Loss: 0.00002348
Iteration 78/1000 | Loss: 0.00002348
Iteration 79/1000 | Loss: 0.00002348
Iteration 80/1000 | Loss: 0.00002347
Iteration 81/1000 | Loss: 0.00002347
Iteration 82/1000 | Loss: 0.00002347
Iteration 83/1000 | Loss: 0.00002346
Iteration 84/1000 | Loss: 0.00002346
Iteration 85/1000 | Loss: 0.00002346
Iteration 86/1000 | Loss: 0.00002346
Iteration 87/1000 | Loss: 0.00002345
Iteration 88/1000 | Loss: 0.00002345
Iteration 89/1000 | Loss: 0.00002345
Iteration 90/1000 | Loss: 0.00002345
Iteration 91/1000 | Loss: 0.00002345
Iteration 92/1000 | Loss: 0.00002345
Iteration 93/1000 | Loss: 0.00002344
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002342
Iteration 99/1000 | Loss: 0.00002342
Iteration 100/1000 | Loss: 0.00002342
Iteration 101/1000 | Loss: 0.00002341
Iteration 102/1000 | Loss: 0.00002341
Iteration 103/1000 | Loss: 0.00002340
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002340
Iteration 107/1000 | Loss: 0.00002339
Iteration 108/1000 | Loss: 0.00002339
Iteration 109/1000 | Loss: 0.00002339
Iteration 110/1000 | Loss: 0.00002338
Iteration 111/1000 | Loss: 0.00002338
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002337
Iteration 116/1000 | Loss: 0.00002336
Iteration 117/1000 | Loss: 0.00002336
Iteration 118/1000 | Loss: 0.00002336
Iteration 119/1000 | Loss: 0.00002336
Iteration 120/1000 | Loss: 0.00002335
Iteration 121/1000 | Loss: 0.00002335
Iteration 122/1000 | Loss: 0.00002335
Iteration 123/1000 | Loss: 0.00002334
Iteration 124/1000 | Loss: 0.00002334
Iteration 125/1000 | Loss: 0.00002334
Iteration 126/1000 | Loss: 0.00002334
Iteration 127/1000 | Loss: 0.00002333
Iteration 128/1000 | Loss: 0.00002333
Iteration 129/1000 | Loss: 0.00002333
Iteration 130/1000 | Loss: 0.00002333
Iteration 131/1000 | Loss: 0.00002333
Iteration 132/1000 | Loss: 0.00002333
Iteration 133/1000 | Loss: 0.00002333
Iteration 134/1000 | Loss: 0.00002332
Iteration 135/1000 | Loss: 0.00002332
Iteration 136/1000 | Loss: 0.00002332
Iteration 137/1000 | Loss: 0.00002332
Iteration 138/1000 | Loss: 0.00002332
Iteration 139/1000 | Loss: 0.00002332
Iteration 140/1000 | Loss: 0.00002332
Iteration 141/1000 | Loss: 0.00002331
Iteration 142/1000 | Loss: 0.00002331
Iteration 143/1000 | Loss: 0.00002331
Iteration 144/1000 | Loss: 0.00002331
Iteration 145/1000 | Loss: 0.00002331
Iteration 146/1000 | Loss: 0.00002331
Iteration 147/1000 | Loss: 0.00002331
Iteration 148/1000 | Loss: 0.00002331
Iteration 149/1000 | Loss: 0.00002330
Iteration 150/1000 | Loss: 0.00002330
Iteration 151/1000 | Loss: 0.00002330
Iteration 152/1000 | Loss: 0.00002330
Iteration 153/1000 | Loss: 0.00002330
Iteration 154/1000 | Loss: 0.00002330
Iteration 155/1000 | Loss: 0.00002330
Iteration 156/1000 | Loss: 0.00002330
Iteration 157/1000 | Loss: 0.00002330
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002330
Iteration 160/1000 | Loss: 0.00002330
Iteration 161/1000 | Loss: 0.00002330
Iteration 162/1000 | Loss: 0.00002329
Iteration 163/1000 | Loss: 0.00002329
Iteration 164/1000 | Loss: 0.00002329
Iteration 165/1000 | Loss: 0.00002329
Iteration 166/1000 | Loss: 0.00002329
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002329
Iteration 169/1000 | Loss: 0.00002329
Iteration 170/1000 | Loss: 0.00002329
Iteration 171/1000 | Loss: 0.00002329
Iteration 172/1000 | Loss: 0.00002329
Iteration 173/1000 | Loss: 0.00002329
Iteration 174/1000 | Loss: 0.00002328
Iteration 175/1000 | Loss: 0.00002328
Iteration 176/1000 | Loss: 0.00002328
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002328
Iteration 180/1000 | Loss: 0.00002328
Iteration 181/1000 | Loss: 0.00002328
Iteration 182/1000 | Loss: 0.00002328
Iteration 183/1000 | Loss: 0.00002328
Iteration 184/1000 | Loss: 0.00002328
Iteration 185/1000 | Loss: 0.00002328
Iteration 186/1000 | Loss: 0.00002328
Iteration 187/1000 | Loss: 0.00002328
Iteration 188/1000 | Loss: 0.00002327
Iteration 189/1000 | Loss: 0.00002327
Iteration 190/1000 | Loss: 0.00002327
Iteration 191/1000 | Loss: 0.00002327
Iteration 192/1000 | Loss: 0.00002327
Iteration 193/1000 | Loss: 0.00002327
Iteration 194/1000 | Loss: 0.00002327
Iteration 195/1000 | Loss: 0.00002327
Iteration 196/1000 | Loss: 0.00002327
Iteration 197/1000 | Loss: 0.00002327
Iteration 198/1000 | Loss: 0.00002327
Iteration 199/1000 | Loss: 0.00002327
Iteration 200/1000 | Loss: 0.00002327
Iteration 201/1000 | Loss: 0.00002327
Iteration 202/1000 | Loss: 0.00002327
Iteration 203/1000 | Loss: 0.00002327
Iteration 204/1000 | Loss: 0.00002327
Iteration 205/1000 | Loss: 0.00002327
Iteration 206/1000 | Loss: 0.00002327
Iteration 207/1000 | Loss: 0.00002327
Iteration 208/1000 | Loss: 0.00002327
Iteration 209/1000 | Loss: 0.00002327
Iteration 210/1000 | Loss: 0.00002327
Iteration 211/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.3274484192370437e-05, 2.3274484192370437e-05, 2.3274484192370437e-05, 2.3274484192370437e-05, 2.3274484192370437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3274484192370437e-05

Optimization complete. Final v2v error: 4.021667957305908 mm

Highest mean error: 5.348330497741699 mm for frame 210

Lowest mean error: 3.5365984439849854 mm for frame 17

Saving results

Total time: 50.382614612579346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936531
Iteration 2/25 | Loss: 0.00301391
Iteration 3/25 | Loss: 0.00242990
Iteration 4/25 | Loss: 0.00213644
Iteration 5/25 | Loss: 0.00195164
Iteration 6/25 | Loss: 0.00192527
Iteration 7/25 | Loss: 0.00194542
Iteration 8/25 | Loss: 0.00185752
Iteration 9/25 | Loss: 0.00176370
Iteration 10/25 | Loss: 0.00170918
Iteration 11/25 | Loss: 0.00167696
Iteration 12/25 | Loss: 0.00163456
Iteration 13/25 | Loss: 0.00161238
Iteration 14/25 | Loss: 0.00159425
Iteration 15/25 | Loss: 0.00159102
Iteration 16/25 | Loss: 0.00157152
Iteration 17/25 | Loss: 0.00156891
Iteration 18/25 | Loss: 0.00156131
Iteration 19/25 | Loss: 0.00155574
Iteration 20/25 | Loss: 0.00155612
Iteration 21/25 | Loss: 0.00155150
Iteration 22/25 | Loss: 0.00155479
Iteration 23/25 | Loss: 0.00155062
Iteration 24/25 | Loss: 0.00156195
Iteration 25/25 | Loss: 0.00156257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32804322
Iteration 2/25 | Loss: 0.00324860
Iteration 3/25 | Loss: 0.00303146
Iteration 4/25 | Loss: 0.00303146
Iteration 5/25 | Loss: 0.00303146
Iteration 6/25 | Loss: 0.00303146
Iteration 7/25 | Loss: 0.00303146
Iteration 8/25 | Loss: 0.00303146
Iteration 9/25 | Loss: 0.00303146
Iteration 10/25 | Loss: 0.00303146
Iteration 11/25 | Loss: 0.00303146
Iteration 12/25 | Loss: 0.00303146
Iteration 13/25 | Loss: 0.00303146
Iteration 14/25 | Loss: 0.00303146
Iteration 15/25 | Loss: 0.00303146
Iteration 16/25 | Loss: 0.00303146
Iteration 17/25 | Loss: 0.00303146
Iteration 18/25 | Loss: 0.00303146
Iteration 19/25 | Loss: 0.00303146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0030314556788653135, 0.0030314556788653135, 0.0030314556788653135, 0.0030314556788653135, 0.0030314556788653135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030314556788653135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00303146
Iteration 2/1000 | Loss: 0.00059640
Iteration 3/1000 | Loss: 0.00037247
Iteration 4/1000 | Loss: 0.00118778
Iteration 5/1000 | Loss: 0.00018763
Iteration 6/1000 | Loss: 0.00064845
Iteration 7/1000 | Loss: 0.00055765
Iteration 8/1000 | Loss: 0.00022806
Iteration 9/1000 | Loss: 0.00016913
Iteration 10/1000 | Loss: 0.00078064
Iteration 11/1000 | Loss: 0.00014158
Iteration 12/1000 | Loss: 0.00027368
Iteration 13/1000 | Loss: 0.00117368
Iteration 14/1000 | Loss: 0.00103236
Iteration 15/1000 | Loss: 0.00012355
Iteration 16/1000 | Loss: 0.00028107
Iteration 17/1000 | Loss: 0.00012066
Iteration 18/1000 | Loss: 0.00028971
Iteration 19/1000 | Loss: 0.00017367
Iteration 20/1000 | Loss: 0.00012177
Iteration 21/1000 | Loss: 0.00014873
Iteration 22/1000 | Loss: 0.00061779
Iteration 23/1000 | Loss: 0.00012305
Iteration 24/1000 | Loss: 0.00015060
Iteration 25/1000 | Loss: 0.00011429
Iteration 26/1000 | Loss: 0.00014007
Iteration 27/1000 | Loss: 0.00011246
Iteration 28/1000 | Loss: 0.00011123
Iteration 29/1000 | Loss: 0.00033088
Iteration 30/1000 | Loss: 0.00010968
Iteration 31/1000 | Loss: 0.00018673
Iteration 32/1000 | Loss: 0.00010856
Iteration 33/1000 | Loss: 0.00010779
Iteration 34/1000 | Loss: 0.00010715
Iteration 35/1000 | Loss: 0.00023549
Iteration 36/1000 | Loss: 0.00011894
Iteration 37/1000 | Loss: 0.00010636
Iteration 38/1000 | Loss: 0.00010598
Iteration 39/1000 | Loss: 0.00030445
Iteration 40/1000 | Loss: 0.00176583
Iteration 41/1000 | Loss: 0.00269179
Iteration 42/1000 | Loss: 0.00049791
Iteration 43/1000 | Loss: 0.00043274
Iteration 44/1000 | Loss: 0.00014604
Iteration 45/1000 | Loss: 0.00021729
Iteration 46/1000 | Loss: 0.00009920
Iteration 47/1000 | Loss: 0.00029336
Iteration 48/1000 | Loss: 0.00029065
Iteration 49/1000 | Loss: 0.00083754
Iteration 50/1000 | Loss: 0.00009416
Iteration 51/1000 | Loss: 0.00007426
Iteration 52/1000 | Loss: 0.00047976
Iteration 53/1000 | Loss: 0.00010967
Iteration 54/1000 | Loss: 0.00006892
Iteration 55/1000 | Loss: 0.00009154
Iteration 56/1000 | Loss: 0.00006313
Iteration 57/1000 | Loss: 0.00005929
Iteration 58/1000 | Loss: 0.00005696
Iteration 59/1000 | Loss: 0.00007181
Iteration 60/1000 | Loss: 0.00005845
Iteration 61/1000 | Loss: 0.00005624
Iteration 62/1000 | Loss: 0.00034726
Iteration 63/1000 | Loss: 0.00023702
Iteration 64/1000 | Loss: 0.00014276
Iteration 65/1000 | Loss: 0.00006206
Iteration 66/1000 | Loss: 0.00005492
Iteration 67/1000 | Loss: 0.00042555
Iteration 68/1000 | Loss: 0.00005243
Iteration 69/1000 | Loss: 0.00025609
Iteration 70/1000 | Loss: 0.00067819
Iteration 71/1000 | Loss: 0.00057900
Iteration 72/1000 | Loss: 0.00058700
Iteration 73/1000 | Loss: 0.00153322
Iteration 74/1000 | Loss: 0.00054689
Iteration 75/1000 | Loss: 0.00005637
Iteration 76/1000 | Loss: 0.00012384
Iteration 77/1000 | Loss: 0.00015082
Iteration 78/1000 | Loss: 0.00004978
Iteration 79/1000 | Loss: 0.00005949
Iteration 80/1000 | Loss: 0.00005014
Iteration 81/1000 | Loss: 0.00004906
Iteration 82/1000 | Loss: 0.00004894
Iteration 83/1000 | Loss: 0.00004893
Iteration 84/1000 | Loss: 0.00004891
Iteration 85/1000 | Loss: 0.00004890
Iteration 86/1000 | Loss: 0.00004890
Iteration 87/1000 | Loss: 0.00004873
Iteration 88/1000 | Loss: 0.00004859
Iteration 89/1000 | Loss: 0.00014716
Iteration 90/1000 | Loss: 0.00011077
Iteration 91/1000 | Loss: 0.00006433
Iteration 92/1000 | Loss: 0.00004886
Iteration 93/1000 | Loss: 0.00007860
Iteration 94/1000 | Loss: 0.00004861
Iteration 95/1000 | Loss: 0.00004856
Iteration 96/1000 | Loss: 0.00009494
Iteration 97/1000 | Loss: 0.00004855
Iteration 98/1000 | Loss: 0.00004846
Iteration 99/1000 | Loss: 0.00004845
Iteration 100/1000 | Loss: 0.00004844
Iteration 101/1000 | Loss: 0.00004844
Iteration 102/1000 | Loss: 0.00004843
Iteration 103/1000 | Loss: 0.00004843
Iteration 104/1000 | Loss: 0.00004843
Iteration 105/1000 | Loss: 0.00004842
Iteration 106/1000 | Loss: 0.00004842
Iteration 107/1000 | Loss: 0.00004841
Iteration 108/1000 | Loss: 0.00004841
Iteration 109/1000 | Loss: 0.00004840
Iteration 110/1000 | Loss: 0.00004840
Iteration 111/1000 | Loss: 0.00004840
Iteration 112/1000 | Loss: 0.00004840
Iteration 113/1000 | Loss: 0.00004840
Iteration 114/1000 | Loss: 0.00004840
Iteration 115/1000 | Loss: 0.00004840
Iteration 116/1000 | Loss: 0.00004840
Iteration 117/1000 | Loss: 0.00004840
Iteration 118/1000 | Loss: 0.00004840
Iteration 119/1000 | Loss: 0.00004840
Iteration 120/1000 | Loss: 0.00004840
Iteration 121/1000 | Loss: 0.00004840
Iteration 122/1000 | Loss: 0.00004840
Iteration 123/1000 | Loss: 0.00004840
Iteration 124/1000 | Loss: 0.00004840
Iteration 125/1000 | Loss: 0.00004840
Iteration 126/1000 | Loss: 0.00004840
Iteration 127/1000 | Loss: 0.00004840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [4.840111796511337e-05, 4.840111796511337e-05, 4.840111796511337e-05, 4.840111796511337e-05, 4.840111796511337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.840111796511337e-05

Optimization complete. Final v2v error: 4.122625827789307 mm

Highest mean error: 10.33043384552002 mm for frame 64

Lowest mean error: 2.9982683658599854 mm for frame 15

Saving results

Total time: 174.55837225914001
