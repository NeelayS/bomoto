Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=284, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15904-15959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104244
Iteration 2/25 | Loss: 0.00178569
Iteration 3/25 | Loss: 0.00118990
Iteration 4/25 | Loss: 0.00113498
Iteration 5/25 | Loss: 0.00110082
Iteration 6/25 | Loss: 0.00111016
Iteration 7/25 | Loss: 0.00109589
Iteration 8/25 | Loss: 0.00107068
Iteration 9/25 | Loss: 0.00105459
Iteration 10/25 | Loss: 0.00104622
Iteration 11/25 | Loss: 0.00103360
Iteration 12/25 | Loss: 0.00102767
Iteration 13/25 | Loss: 0.00102646
Iteration 14/25 | Loss: 0.00102603
Iteration 15/25 | Loss: 0.00102590
Iteration 16/25 | Loss: 0.00102585
Iteration 17/25 | Loss: 0.00102584
Iteration 18/25 | Loss: 0.00102584
Iteration 19/25 | Loss: 0.00102584
Iteration 20/25 | Loss: 0.00102584
Iteration 21/25 | Loss: 0.00102584
Iteration 22/25 | Loss: 0.00102584
Iteration 23/25 | Loss: 0.00102583
Iteration 24/25 | Loss: 0.00102583
Iteration 25/25 | Loss: 0.00102583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66165704
Iteration 2/25 | Loss: 0.00039491
Iteration 3/25 | Loss: 0.00039488
Iteration 4/25 | Loss: 0.00039488
Iteration 5/25 | Loss: 0.00039487
Iteration 6/25 | Loss: 0.00039487
Iteration 7/25 | Loss: 0.00039487
Iteration 8/25 | Loss: 0.00039487
Iteration 9/25 | Loss: 0.00039487
Iteration 10/25 | Loss: 0.00039487
Iteration 11/25 | Loss: 0.00039487
Iteration 12/25 | Loss: 0.00039487
Iteration 13/25 | Loss: 0.00039487
Iteration 14/25 | Loss: 0.00039487
Iteration 15/25 | Loss: 0.00039487
Iteration 16/25 | Loss: 0.00039487
Iteration 17/25 | Loss: 0.00039487
Iteration 18/25 | Loss: 0.00039487
Iteration 19/25 | Loss: 0.00039487
Iteration 20/25 | Loss: 0.00039487
Iteration 21/25 | Loss: 0.00039487
Iteration 22/25 | Loss: 0.00039487
Iteration 23/25 | Loss: 0.00039487
Iteration 24/25 | Loss: 0.00039487
Iteration 25/25 | Loss: 0.00039487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039487
Iteration 2/1000 | Loss: 0.00005149
Iteration 3/1000 | Loss: 0.00010615
Iteration 4/1000 | Loss: 0.00003746
Iteration 5/1000 | Loss: 0.00003568
Iteration 6/1000 | Loss: 0.00003411
Iteration 7/1000 | Loss: 0.00003321
Iteration 8/1000 | Loss: 0.00003220
Iteration 9/1000 | Loss: 0.00003151
Iteration 10/1000 | Loss: 0.00003088
Iteration 11/1000 | Loss: 0.00003035
Iteration 12/1000 | Loss: 0.00002999
Iteration 13/1000 | Loss: 0.00002962
Iteration 14/1000 | Loss: 0.00002936
Iteration 15/1000 | Loss: 0.00002917
Iteration 16/1000 | Loss: 0.00002912
Iteration 17/1000 | Loss: 0.00002910
Iteration 18/1000 | Loss: 0.00002903
Iteration 19/1000 | Loss: 0.00002898
Iteration 20/1000 | Loss: 0.00002886
Iteration 21/1000 | Loss: 0.00002878
Iteration 22/1000 | Loss: 0.00002877
Iteration 23/1000 | Loss: 0.00002872
Iteration 24/1000 | Loss: 0.00002871
Iteration 25/1000 | Loss: 0.00002866
Iteration 26/1000 | Loss: 0.00002866
Iteration 27/1000 | Loss: 0.00002865
Iteration 28/1000 | Loss: 0.00002858
Iteration 29/1000 | Loss: 0.00002849
Iteration 30/1000 | Loss: 0.00002849
Iteration 31/1000 | Loss: 0.00002847
Iteration 32/1000 | Loss: 0.00002846
Iteration 33/1000 | Loss: 0.00002845
Iteration 34/1000 | Loss: 0.00002845
Iteration 35/1000 | Loss: 0.00002844
Iteration 36/1000 | Loss: 0.00002844
Iteration 37/1000 | Loss: 0.00002843
Iteration 38/1000 | Loss: 0.00002843
Iteration 39/1000 | Loss: 0.00002843
Iteration 40/1000 | Loss: 0.00002842
Iteration 41/1000 | Loss: 0.00002840
Iteration 42/1000 | Loss: 0.00002840
Iteration 43/1000 | Loss: 0.00002840
Iteration 44/1000 | Loss: 0.00002840
Iteration 45/1000 | Loss: 0.00002840
Iteration 46/1000 | Loss: 0.00002839
Iteration 47/1000 | Loss: 0.00002839
Iteration 48/1000 | Loss: 0.00002837
Iteration 49/1000 | Loss: 0.00002837
Iteration 50/1000 | Loss: 0.00002836
Iteration 51/1000 | Loss: 0.00002836
Iteration 52/1000 | Loss: 0.00002836
Iteration 53/1000 | Loss: 0.00002836
Iteration 54/1000 | Loss: 0.00002836
Iteration 55/1000 | Loss: 0.00002836
Iteration 56/1000 | Loss: 0.00002836
Iteration 57/1000 | Loss: 0.00002836
Iteration 58/1000 | Loss: 0.00002836
Iteration 59/1000 | Loss: 0.00002836
Iteration 60/1000 | Loss: 0.00002836
Iteration 61/1000 | Loss: 0.00002836
Iteration 62/1000 | Loss: 0.00002835
Iteration 63/1000 | Loss: 0.00002835
Iteration 64/1000 | Loss: 0.00002835
Iteration 65/1000 | Loss: 0.00002835
Iteration 66/1000 | Loss: 0.00002835
Iteration 67/1000 | Loss: 0.00002834
Iteration 68/1000 | Loss: 0.00002833
Iteration 69/1000 | Loss: 0.00002833
Iteration 70/1000 | Loss: 0.00002832
Iteration 71/1000 | Loss: 0.00002832
Iteration 72/1000 | Loss: 0.00002832
Iteration 73/1000 | Loss: 0.00002832
Iteration 74/1000 | Loss: 0.00002832
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002832
Iteration 77/1000 | Loss: 0.00002832
Iteration 78/1000 | Loss: 0.00002832
Iteration 79/1000 | Loss: 0.00002831
Iteration 80/1000 | Loss: 0.00002831
Iteration 81/1000 | Loss: 0.00002830
Iteration 82/1000 | Loss: 0.00002830
Iteration 83/1000 | Loss: 0.00002830
Iteration 84/1000 | Loss: 0.00002830
Iteration 85/1000 | Loss: 0.00002830
Iteration 86/1000 | Loss: 0.00002829
Iteration 87/1000 | Loss: 0.00002829
Iteration 88/1000 | Loss: 0.00002829
Iteration 89/1000 | Loss: 0.00002829
Iteration 90/1000 | Loss: 0.00002829
Iteration 91/1000 | Loss: 0.00002829
Iteration 92/1000 | Loss: 0.00002829
Iteration 93/1000 | Loss: 0.00002829
Iteration 94/1000 | Loss: 0.00002829
Iteration 95/1000 | Loss: 0.00002829
Iteration 96/1000 | Loss: 0.00002829
Iteration 97/1000 | Loss: 0.00002829
Iteration 98/1000 | Loss: 0.00002829
Iteration 99/1000 | Loss: 0.00002829
Iteration 100/1000 | Loss: 0.00002829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.829389450198505e-05, 2.829389450198505e-05, 2.829389450198505e-05, 2.829389450198505e-05, 2.829389450198505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.829389450198505e-05

Optimization complete. Final v2v error: 4.294652462005615 mm

Highest mean error: 5.473841667175293 mm for frame 171

Lowest mean error: 3.3179523944854736 mm for frame 205

Saving results

Total time: 72.10602116584778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003731
Iteration 2/25 | Loss: 0.00351110
Iteration 3/25 | Loss: 0.00214173
Iteration 4/25 | Loss: 0.00184171
Iteration 5/25 | Loss: 0.00159066
Iteration 6/25 | Loss: 0.00147927
Iteration 7/25 | Loss: 0.00144530
Iteration 8/25 | Loss: 0.00136508
Iteration 9/25 | Loss: 0.00134205
Iteration 10/25 | Loss: 0.00132281
Iteration 11/25 | Loss: 0.00130466
Iteration 12/25 | Loss: 0.00128282
Iteration 13/25 | Loss: 0.00124461
Iteration 14/25 | Loss: 0.00124325
Iteration 15/25 | Loss: 0.00122563
Iteration 16/25 | Loss: 0.00121784
Iteration 17/25 | Loss: 0.00120963
Iteration 18/25 | Loss: 0.00119958
Iteration 19/25 | Loss: 0.00119722
Iteration 20/25 | Loss: 0.00117988
Iteration 21/25 | Loss: 0.00117006
Iteration 22/25 | Loss: 0.00117194
Iteration 23/25 | Loss: 0.00123862
Iteration 24/25 | Loss: 0.00114912
Iteration 25/25 | Loss: 0.00111211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38863146
Iteration 2/25 | Loss: 0.00414465
Iteration 3/25 | Loss: 0.00296035
Iteration 4/25 | Loss: 0.00296035
Iteration 5/25 | Loss: 0.00296035
Iteration 6/25 | Loss: 0.00296035
Iteration 7/25 | Loss: 0.00296035
Iteration 8/25 | Loss: 0.00296035
Iteration 9/25 | Loss: 0.00296035
Iteration 10/25 | Loss: 0.00296035
Iteration 11/25 | Loss: 0.00296035
Iteration 12/25 | Loss: 0.00296035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0029603452421724796, 0.0029603452421724796, 0.0029603452421724796, 0.0029603452421724796, 0.0029603452421724796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029603452421724796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296035
Iteration 2/1000 | Loss: 0.00459029
Iteration 3/1000 | Loss: 0.00309095
Iteration 4/1000 | Loss: 0.00088214
Iteration 5/1000 | Loss: 0.00147047
Iteration 6/1000 | Loss: 0.00185657
Iteration 7/1000 | Loss: 0.00095269
Iteration 8/1000 | Loss: 0.00081570
Iteration 9/1000 | Loss: 0.00068110
Iteration 10/1000 | Loss: 0.00035774
Iteration 11/1000 | Loss: 0.00089793
Iteration 12/1000 | Loss: 0.00167471
Iteration 13/1000 | Loss: 0.00018118
Iteration 14/1000 | Loss: 0.00026150
Iteration 15/1000 | Loss: 0.00024863
Iteration 16/1000 | Loss: 0.00017716
Iteration 17/1000 | Loss: 0.00022771
Iteration 18/1000 | Loss: 0.00026162
Iteration 19/1000 | Loss: 0.00013172
Iteration 20/1000 | Loss: 0.00033529
Iteration 21/1000 | Loss: 0.00037830
Iteration 22/1000 | Loss: 0.00039981
Iteration 23/1000 | Loss: 0.00019732
Iteration 24/1000 | Loss: 0.00018079
Iteration 25/1000 | Loss: 0.00009210
Iteration 26/1000 | Loss: 0.00019197
Iteration 27/1000 | Loss: 0.00079392
Iteration 28/1000 | Loss: 0.00014539
Iteration 29/1000 | Loss: 0.00052955
Iteration 30/1000 | Loss: 0.00007698
Iteration 31/1000 | Loss: 0.00007962
Iteration 32/1000 | Loss: 0.00032047
Iteration 33/1000 | Loss: 0.00009162
Iteration 34/1000 | Loss: 0.00008916
Iteration 35/1000 | Loss: 0.00019686
Iteration 36/1000 | Loss: 0.00006098
Iteration 37/1000 | Loss: 0.00020687
Iteration 38/1000 | Loss: 0.00034927
Iteration 39/1000 | Loss: 0.00083246
Iteration 40/1000 | Loss: 0.00124637
Iteration 41/1000 | Loss: 0.00212170
Iteration 42/1000 | Loss: 0.00157197
Iteration 43/1000 | Loss: 0.00204907
Iteration 44/1000 | Loss: 0.00012584
Iteration 45/1000 | Loss: 0.00022789
Iteration 46/1000 | Loss: 0.00037567
Iteration 47/1000 | Loss: 0.00031391
Iteration 48/1000 | Loss: 0.00021374
Iteration 49/1000 | Loss: 0.00098279
Iteration 50/1000 | Loss: 0.00005718
Iteration 51/1000 | Loss: 0.00029833
Iteration 52/1000 | Loss: 0.00041200
Iteration 53/1000 | Loss: 0.00046986
Iteration 54/1000 | Loss: 0.00045320
Iteration 55/1000 | Loss: 0.00013101
Iteration 56/1000 | Loss: 0.00005874
Iteration 57/1000 | Loss: 0.00006370
Iteration 58/1000 | Loss: 0.00067625
Iteration 59/1000 | Loss: 0.00045398
Iteration 60/1000 | Loss: 0.00010730
Iteration 61/1000 | Loss: 0.00006289
Iteration 62/1000 | Loss: 0.00015805
Iteration 63/1000 | Loss: 0.00010321
Iteration 64/1000 | Loss: 0.00038378
Iteration 65/1000 | Loss: 0.00036272
Iteration 66/1000 | Loss: 0.00011718
Iteration 67/1000 | Loss: 0.00005709
Iteration 68/1000 | Loss: 0.00007684
Iteration 69/1000 | Loss: 0.00029835
Iteration 70/1000 | Loss: 0.00041280
Iteration 71/1000 | Loss: 0.00005331
Iteration 72/1000 | Loss: 0.00024317
Iteration 73/1000 | Loss: 0.00005207
Iteration 74/1000 | Loss: 0.00028056
Iteration 75/1000 | Loss: 0.00072119
Iteration 76/1000 | Loss: 0.00070033
Iteration 77/1000 | Loss: 0.00028677
Iteration 78/1000 | Loss: 0.00058421
Iteration 79/1000 | Loss: 0.00072192
Iteration 80/1000 | Loss: 0.00013967
Iteration 81/1000 | Loss: 0.00044856
Iteration 82/1000 | Loss: 0.00028943
Iteration 83/1000 | Loss: 0.00022215
Iteration 84/1000 | Loss: 0.00014567
Iteration 85/1000 | Loss: 0.00050547
Iteration 86/1000 | Loss: 0.00023825
Iteration 87/1000 | Loss: 0.00019265
Iteration 88/1000 | Loss: 0.00026608
Iteration 89/1000 | Loss: 0.00012306
Iteration 90/1000 | Loss: 0.00082007
Iteration 91/1000 | Loss: 0.00028704
Iteration 92/1000 | Loss: 0.00027416
Iteration 93/1000 | Loss: 0.00034978
Iteration 94/1000 | Loss: 0.00017600
Iteration 95/1000 | Loss: 0.00058240
Iteration 96/1000 | Loss: 0.00059072
Iteration 97/1000 | Loss: 0.00037750
Iteration 98/1000 | Loss: 0.00034675
Iteration 99/1000 | Loss: 0.00030988
Iteration 100/1000 | Loss: 0.00034845
Iteration 101/1000 | Loss: 0.00027238
Iteration 102/1000 | Loss: 0.00028085
Iteration 103/1000 | Loss: 0.00035349
Iteration 104/1000 | Loss: 0.00033612
Iteration 105/1000 | Loss: 0.00034299
Iteration 106/1000 | Loss: 0.00028167
Iteration 107/1000 | Loss: 0.00032261
Iteration 108/1000 | Loss: 0.00032907
Iteration 109/1000 | Loss: 0.00030864
Iteration 110/1000 | Loss: 0.00075082
Iteration 111/1000 | Loss: 0.00033045
Iteration 112/1000 | Loss: 0.00043379
Iteration 113/1000 | Loss: 0.00076562
Iteration 114/1000 | Loss: 0.00052884
Iteration 115/1000 | Loss: 0.00008863
Iteration 116/1000 | Loss: 0.00008619
Iteration 117/1000 | Loss: 0.00008773
Iteration 118/1000 | Loss: 0.00004762
Iteration 119/1000 | Loss: 0.00018029
Iteration 120/1000 | Loss: 0.00005914
Iteration 121/1000 | Loss: 0.00004627
Iteration 122/1000 | Loss: 0.00022600
Iteration 123/1000 | Loss: 0.00004317
Iteration 124/1000 | Loss: 0.00011467
Iteration 125/1000 | Loss: 0.00004372
Iteration 126/1000 | Loss: 0.00011333
Iteration 127/1000 | Loss: 0.00010201
Iteration 128/1000 | Loss: 0.00004210
Iteration 129/1000 | Loss: 0.00003748
Iteration 130/1000 | Loss: 0.00009786
Iteration 131/1000 | Loss: 0.00006739
Iteration 132/1000 | Loss: 0.00003645
Iteration 133/1000 | Loss: 0.00003617
Iteration 134/1000 | Loss: 0.00003593
Iteration 135/1000 | Loss: 0.00003585
Iteration 136/1000 | Loss: 0.00020051
Iteration 137/1000 | Loss: 0.00004115
Iteration 138/1000 | Loss: 0.00003728
Iteration 139/1000 | Loss: 0.00003579
Iteration 140/1000 | Loss: 0.00003544
Iteration 141/1000 | Loss: 0.00003528
Iteration 142/1000 | Loss: 0.00003528
Iteration 143/1000 | Loss: 0.00012522
Iteration 144/1000 | Loss: 0.00003759
Iteration 145/1000 | Loss: 0.00023082
Iteration 146/1000 | Loss: 0.00016090
Iteration 147/1000 | Loss: 0.00034481
Iteration 148/1000 | Loss: 0.00021464
Iteration 149/1000 | Loss: 0.00005290
Iteration 150/1000 | Loss: 0.00006666
Iteration 151/1000 | Loss: 0.00003914
Iteration 152/1000 | Loss: 0.00003863
Iteration 153/1000 | Loss: 0.00007593
Iteration 154/1000 | Loss: 0.00003690
Iteration 155/1000 | Loss: 0.00003615
Iteration 156/1000 | Loss: 0.00003611
Iteration 157/1000 | Loss: 0.00023569
Iteration 158/1000 | Loss: 0.00027306
Iteration 159/1000 | Loss: 0.00022028
Iteration 160/1000 | Loss: 0.00012316
Iteration 161/1000 | Loss: 0.00010821
Iteration 162/1000 | Loss: 0.00020653
Iteration 163/1000 | Loss: 0.00006052
Iteration 164/1000 | Loss: 0.00004693
Iteration 165/1000 | Loss: 0.00023287
Iteration 166/1000 | Loss: 0.00005486
Iteration 167/1000 | Loss: 0.00011041
Iteration 168/1000 | Loss: 0.00018750
Iteration 169/1000 | Loss: 0.00008483
Iteration 170/1000 | Loss: 0.00005420
Iteration 171/1000 | Loss: 0.00006531
Iteration 172/1000 | Loss: 0.00014200
Iteration 173/1000 | Loss: 0.00004561
Iteration 174/1000 | Loss: 0.00003625
Iteration 175/1000 | Loss: 0.00006669
Iteration 176/1000 | Loss: 0.00005495
Iteration 177/1000 | Loss: 0.00003481
Iteration 178/1000 | Loss: 0.00004963
Iteration 179/1000 | Loss: 0.00003428
Iteration 180/1000 | Loss: 0.00003695
Iteration 181/1000 | Loss: 0.00010569
Iteration 182/1000 | Loss: 0.00009959
Iteration 183/1000 | Loss: 0.00004079
Iteration 184/1000 | Loss: 0.00003458
Iteration 185/1000 | Loss: 0.00008728
Iteration 186/1000 | Loss: 0.00022572
Iteration 187/1000 | Loss: 0.00009879
Iteration 188/1000 | Loss: 0.00005314
Iteration 189/1000 | Loss: 0.00004216
Iteration 190/1000 | Loss: 0.00003392
Iteration 191/1000 | Loss: 0.00003360
Iteration 192/1000 | Loss: 0.00003338
Iteration 193/1000 | Loss: 0.00006694
Iteration 194/1000 | Loss: 0.00003562
Iteration 195/1000 | Loss: 0.00003438
Iteration 196/1000 | Loss: 0.00003327
Iteration 197/1000 | Loss: 0.00004277
Iteration 198/1000 | Loss: 0.00003673
Iteration 199/1000 | Loss: 0.00003320
Iteration 200/1000 | Loss: 0.00003320
Iteration 201/1000 | Loss: 0.00003320
Iteration 202/1000 | Loss: 0.00003320
Iteration 203/1000 | Loss: 0.00003320
Iteration 204/1000 | Loss: 0.00003319
Iteration 205/1000 | Loss: 0.00003319
Iteration 206/1000 | Loss: 0.00003317
Iteration 207/1000 | Loss: 0.00003317
Iteration 208/1000 | Loss: 0.00003316
Iteration 209/1000 | Loss: 0.00003316
Iteration 210/1000 | Loss: 0.00003316
Iteration 211/1000 | Loss: 0.00003316
Iteration 212/1000 | Loss: 0.00003315
Iteration 213/1000 | Loss: 0.00003315
Iteration 214/1000 | Loss: 0.00003315
Iteration 215/1000 | Loss: 0.00003315
Iteration 216/1000 | Loss: 0.00003314
Iteration 217/1000 | Loss: 0.00003314
Iteration 218/1000 | Loss: 0.00003313
Iteration 219/1000 | Loss: 0.00003313
Iteration 220/1000 | Loss: 0.00003313
Iteration 221/1000 | Loss: 0.00003311
Iteration 222/1000 | Loss: 0.00003311
Iteration 223/1000 | Loss: 0.00003307
Iteration 224/1000 | Loss: 0.00003304
Iteration 225/1000 | Loss: 0.00009375
Iteration 226/1000 | Loss: 0.00006504
Iteration 227/1000 | Loss: 0.00003785
Iteration 228/1000 | Loss: 0.00003306
Iteration 229/1000 | Loss: 0.00003304
Iteration 230/1000 | Loss: 0.00003302
Iteration 231/1000 | Loss: 0.00003302
Iteration 232/1000 | Loss: 0.00003301
Iteration 233/1000 | Loss: 0.00003300
Iteration 234/1000 | Loss: 0.00004615
Iteration 235/1000 | Loss: 0.00003294
Iteration 236/1000 | Loss: 0.00003293
Iteration 237/1000 | Loss: 0.00003293
Iteration 238/1000 | Loss: 0.00003293
Iteration 239/1000 | Loss: 0.00003293
Iteration 240/1000 | Loss: 0.00003293
Iteration 241/1000 | Loss: 0.00003293
Iteration 242/1000 | Loss: 0.00003293
Iteration 243/1000 | Loss: 0.00003292
Iteration 244/1000 | Loss: 0.00003292
Iteration 245/1000 | Loss: 0.00003292
Iteration 246/1000 | Loss: 0.00003292
Iteration 247/1000 | Loss: 0.00003291
Iteration 248/1000 | Loss: 0.00003291
Iteration 249/1000 | Loss: 0.00005289
Iteration 250/1000 | Loss: 0.00008617
Iteration 251/1000 | Loss: 0.00004271
Iteration 252/1000 | Loss: 0.00004501
Iteration 253/1000 | Loss: 0.00005274
Iteration 254/1000 | Loss: 0.00007195
Iteration 255/1000 | Loss: 0.00003293
Iteration 256/1000 | Loss: 0.00003290
Iteration 257/1000 | Loss: 0.00003289
Iteration 258/1000 | Loss: 0.00003285
Iteration 259/1000 | Loss: 0.00003284
Iteration 260/1000 | Loss: 0.00003284
Iteration 261/1000 | Loss: 0.00003283
Iteration 262/1000 | Loss: 0.00003283
Iteration 263/1000 | Loss: 0.00003283
Iteration 264/1000 | Loss: 0.00003282
Iteration 265/1000 | Loss: 0.00003282
Iteration 266/1000 | Loss: 0.00003282
Iteration 267/1000 | Loss: 0.00003282
Iteration 268/1000 | Loss: 0.00003282
Iteration 269/1000 | Loss: 0.00003281
Iteration 270/1000 | Loss: 0.00003281
Iteration 271/1000 | Loss: 0.00003281
Iteration 272/1000 | Loss: 0.00003281
Iteration 273/1000 | Loss: 0.00003281
Iteration 274/1000 | Loss: 0.00003281
Iteration 275/1000 | Loss: 0.00003281
Iteration 276/1000 | Loss: 0.00003281
Iteration 277/1000 | Loss: 0.00003281
Iteration 278/1000 | Loss: 0.00003281
Iteration 279/1000 | Loss: 0.00003281
Iteration 280/1000 | Loss: 0.00003280
Iteration 281/1000 | Loss: 0.00003280
Iteration 282/1000 | Loss: 0.00003280
Iteration 283/1000 | Loss: 0.00003280
Iteration 284/1000 | Loss: 0.00003279
Iteration 285/1000 | Loss: 0.00003279
Iteration 286/1000 | Loss: 0.00003279
Iteration 287/1000 | Loss: 0.00003279
Iteration 288/1000 | Loss: 0.00003279
Iteration 289/1000 | Loss: 0.00003279
Iteration 290/1000 | Loss: 0.00003278
Iteration 291/1000 | Loss: 0.00003278
Iteration 292/1000 | Loss: 0.00003278
Iteration 293/1000 | Loss: 0.00003278
Iteration 294/1000 | Loss: 0.00003277
Iteration 295/1000 | Loss: 0.00003277
Iteration 296/1000 | Loss: 0.00003276
Iteration 297/1000 | Loss: 0.00003276
Iteration 298/1000 | Loss: 0.00003276
Iteration 299/1000 | Loss: 0.00003275
Iteration 300/1000 | Loss: 0.00003275
Iteration 301/1000 | Loss: 0.00003275
Iteration 302/1000 | Loss: 0.00003274
Iteration 303/1000 | Loss: 0.00003274
Iteration 304/1000 | Loss: 0.00003274
Iteration 305/1000 | Loss: 0.00003273
Iteration 306/1000 | Loss: 0.00003273
Iteration 307/1000 | Loss: 0.00003273
Iteration 308/1000 | Loss: 0.00003273
Iteration 309/1000 | Loss: 0.00003273
Iteration 310/1000 | Loss: 0.00007317
Iteration 311/1000 | Loss: 0.00036901
Iteration 312/1000 | Loss: 0.00006581
Iteration 313/1000 | Loss: 0.00004248
Iteration 314/1000 | Loss: 0.00015259
Iteration 315/1000 | Loss: 0.00003630
Iteration 316/1000 | Loss: 0.00008712
Iteration 317/1000 | Loss: 0.00003475
Iteration 318/1000 | Loss: 0.00003405
Iteration 319/1000 | Loss: 0.00003351
Iteration 320/1000 | Loss: 0.00004079
Iteration 321/1000 | Loss: 0.00008254
Iteration 322/1000 | Loss: 0.00003325
Iteration 323/1000 | Loss: 0.00005161
Iteration 324/1000 | Loss: 0.00020522
Iteration 325/1000 | Loss: 0.00012694
Iteration 326/1000 | Loss: 0.00005019
Iteration 327/1000 | Loss: 0.00003349
Iteration 328/1000 | Loss: 0.00003267
Iteration 329/1000 | Loss: 0.00003267
Iteration 330/1000 | Loss: 0.00003266
Iteration 331/1000 | Loss: 0.00003266
Iteration 332/1000 | Loss: 0.00003266
Iteration 333/1000 | Loss: 0.00003266
Iteration 334/1000 | Loss: 0.00003266
Iteration 335/1000 | Loss: 0.00003266
Iteration 336/1000 | Loss: 0.00003266
Iteration 337/1000 | Loss: 0.00003265
Iteration 338/1000 | Loss: 0.00003265
Iteration 339/1000 | Loss: 0.00003265
Iteration 340/1000 | Loss: 0.00003264
Iteration 341/1000 | Loss: 0.00003261
Iteration 342/1000 | Loss: 0.00003258
Iteration 343/1000 | Loss: 0.00003258
Iteration 344/1000 | Loss: 0.00003258
Iteration 345/1000 | Loss: 0.00003257
Iteration 346/1000 | Loss: 0.00003257
Iteration 347/1000 | Loss: 0.00003256
Iteration 348/1000 | Loss: 0.00003253
Iteration 349/1000 | Loss: 0.00003250
Iteration 350/1000 | Loss: 0.00003248
Iteration 351/1000 | Loss: 0.00003246
Iteration 352/1000 | Loss: 0.00003245
Iteration 353/1000 | Loss: 0.00003245
Iteration 354/1000 | Loss: 0.00003244
Iteration 355/1000 | Loss: 0.00003244
Iteration 356/1000 | Loss: 0.00003242
Iteration 357/1000 | Loss: 0.00003241
Iteration 358/1000 | Loss: 0.00003241
Iteration 359/1000 | Loss: 0.00003240
Iteration 360/1000 | Loss: 0.00008957
Iteration 361/1000 | Loss: 0.00003366
Iteration 362/1000 | Loss: 0.00008313
Iteration 363/1000 | Loss: 0.00003293
Iteration 364/1000 | Loss: 0.00003234
Iteration 365/1000 | Loss: 0.00003228
Iteration 366/1000 | Loss: 0.00003228
Iteration 367/1000 | Loss: 0.00003228
Iteration 368/1000 | Loss: 0.00003228
Iteration 369/1000 | Loss: 0.00003228
Iteration 370/1000 | Loss: 0.00003228
Iteration 371/1000 | Loss: 0.00003228
Iteration 372/1000 | Loss: 0.00003228
Iteration 373/1000 | Loss: 0.00003228
Iteration 374/1000 | Loss: 0.00003228
Iteration 375/1000 | Loss: 0.00003228
Iteration 376/1000 | Loss: 0.00003228
Iteration 377/1000 | Loss: 0.00003228
Iteration 378/1000 | Loss: 0.00003227
Iteration 379/1000 | Loss: 0.00003227
Iteration 380/1000 | Loss: 0.00003227
Iteration 381/1000 | Loss: 0.00003227
Iteration 382/1000 | Loss: 0.00003227
Iteration 383/1000 | Loss: 0.00003227
Iteration 384/1000 | Loss: 0.00003227
Iteration 385/1000 | Loss: 0.00003227
Iteration 386/1000 | Loss: 0.00003227
Iteration 387/1000 | Loss: 0.00003227
Iteration 388/1000 | Loss: 0.00003227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 388. Stopping optimization.
Last 5 losses: [3.227400884497911e-05, 3.227400884497911e-05, 3.227400884497911e-05, 3.227400884497911e-05, 3.227400884497911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.227400884497911e-05

Optimization complete. Final v2v error: 3.804366111755371 mm

Highest mean error: 12.387970924377441 mm for frame 80

Lowest mean error: 2.755335807800293 mm for frame 72

Saving results

Total time: 434.63720440864563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817052
Iteration 2/25 | Loss: 0.00137891
Iteration 3/25 | Loss: 0.00091848
Iteration 4/25 | Loss: 0.00087563
Iteration 5/25 | Loss: 0.00087216
Iteration 6/25 | Loss: 0.00087213
Iteration 7/25 | Loss: 0.00087213
Iteration 8/25 | Loss: 0.00087213
Iteration 9/25 | Loss: 0.00087213
Iteration 10/25 | Loss: 0.00087213
Iteration 11/25 | Loss: 0.00087213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008721306221559644, 0.0008721306221559644, 0.0008721306221559644, 0.0008721306221559644, 0.0008721306221559644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008721306221559644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39989781
Iteration 2/25 | Loss: 0.00027768
Iteration 3/25 | Loss: 0.00027768
Iteration 4/25 | Loss: 0.00027767
Iteration 5/25 | Loss: 0.00027767
Iteration 6/25 | Loss: 0.00027767
Iteration 7/25 | Loss: 0.00027767
Iteration 8/25 | Loss: 0.00027767
Iteration 9/25 | Loss: 0.00027767
Iteration 10/25 | Loss: 0.00027767
Iteration 11/25 | Loss: 0.00027767
Iteration 12/25 | Loss: 0.00027767
Iteration 13/25 | Loss: 0.00027767
Iteration 14/25 | Loss: 0.00027767
Iteration 15/25 | Loss: 0.00027767
Iteration 16/25 | Loss: 0.00027767
Iteration 17/25 | Loss: 0.00027767
Iteration 18/25 | Loss: 0.00027767
Iteration 19/25 | Loss: 0.00027767
Iteration 20/25 | Loss: 0.00027767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002776728360913694, 0.0002776728360913694, 0.0002776728360913694, 0.0002776728360913694, 0.0002776728360913694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002776728360913694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027767
Iteration 2/1000 | Loss: 0.00003285
Iteration 3/1000 | Loss: 0.00001846
Iteration 4/1000 | Loss: 0.00001666
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001421
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001287
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001281
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001281
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001275
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001272
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001270
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.2620778761629481e-05, 1.2620778761629481e-05, 1.2620778761629481e-05, 1.2620778761629481e-05, 1.2620778761629481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2620778761629481e-05

Optimization complete. Final v2v error: 2.9496877193450928 mm

Highest mean error: 3.6415793895721436 mm for frame 25

Lowest mean error: 2.6521406173706055 mm for frame 229

Saving results

Total time: 34.19100260734558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478855
Iteration 2/25 | Loss: 0.00095971
Iteration 3/25 | Loss: 0.00085809
Iteration 4/25 | Loss: 0.00084405
Iteration 5/25 | Loss: 0.00083999
Iteration 6/25 | Loss: 0.00083878
Iteration 7/25 | Loss: 0.00083878
Iteration 8/25 | Loss: 0.00083878
Iteration 9/25 | Loss: 0.00083878
Iteration 10/25 | Loss: 0.00083878
Iteration 11/25 | Loss: 0.00083878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008387764682993293, 0.0008387764682993293, 0.0008387764682993293, 0.0008387764682993293, 0.0008387764682993293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008387764682993293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29848814
Iteration 2/25 | Loss: 0.00047676
Iteration 3/25 | Loss: 0.00047672
Iteration 4/25 | Loss: 0.00047672
Iteration 5/25 | Loss: 0.00047672
Iteration 6/25 | Loss: 0.00047672
Iteration 7/25 | Loss: 0.00047672
Iteration 8/25 | Loss: 0.00047672
Iteration 9/25 | Loss: 0.00047672
Iteration 10/25 | Loss: 0.00047672
Iteration 11/25 | Loss: 0.00047672
Iteration 12/25 | Loss: 0.00047672
Iteration 13/25 | Loss: 0.00047672
Iteration 14/25 | Loss: 0.00047672
Iteration 15/25 | Loss: 0.00047672
Iteration 16/25 | Loss: 0.00047672
Iteration 17/25 | Loss: 0.00047672
Iteration 18/25 | Loss: 0.00047672
Iteration 19/25 | Loss: 0.00047672
Iteration 20/25 | Loss: 0.00047672
Iteration 21/25 | Loss: 0.00047672
Iteration 22/25 | Loss: 0.00047672
Iteration 23/25 | Loss: 0.00047672
Iteration 24/25 | Loss: 0.00047672
Iteration 25/25 | Loss: 0.00047672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047672
Iteration 2/1000 | Loss: 0.00002393
Iteration 3/1000 | Loss: 0.00001382
Iteration 4/1000 | Loss: 0.00001171
Iteration 5/1000 | Loss: 0.00001082
Iteration 6/1000 | Loss: 0.00001027
Iteration 7/1000 | Loss: 0.00001011
Iteration 8/1000 | Loss: 0.00000981
Iteration 9/1000 | Loss: 0.00000975
Iteration 10/1000 | Loss: 0.00000963
Iteration 11/1000 | Loss: 0.00000954
Iteration 12/1000 | Loss: 0.00000951
Iteration 13/1000 | Loss: 0.00000950
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000949
Iteration 16/1000 | Loss: 0.00000948
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000945
Iteration 19/1000 | Loss: 0.00000945
Iteration 20/1000 | Loss: 0.00000945
Iteration 21/1000 | Loss: 0.00000945
Iteration 22/1000 | Loss: 0.00000944
Iteration 23/1000 | Loss: 0.00000944
Iteration 24/1000 | Loss: 0.00000944
Iteration 25/1000 | Loss: 0.00000943
Iteration 26/1000 | Loss: 0.00000943
Iteration 27/1000 | Loss: 0.00000943
Iteration 28/1000 | Loss: 0.00000942
Iteration 29/1000 | Loss: 0.00000942
Iteration 30/1000 | Loss: 0.00000941
Iteration 31/1000 | Loss: 0.00000940
Iteration 32/1000 | Loss: 0.00000940
Iteration 33/1000 | Loss: 0.00000940
Iteration 34/1000 | Loss: 0.00000940
Iteration 35/1000 | Loss: 0.00000940
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000937
Iteration 42/1000 | Loss: 0.00000937
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000936
Iteration 48/1000 | Loss: 0.00000936
Iteration 49/1000 | Loss: 0.00000936
Iteration 50/1000 | Loss: 0.00000936
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000935
Iteration 53/1000 | Loss: 0.00000935
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000934
Iteration 56/1000 | Loss: 0.00000934
Iteration 57/1000 | Loss: 0.00000934
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000933
Iteration 60/1000 | Loss: 0.00000933
Iteration 61/1000 | Loss: 0.00000933
Iteration 62/1000 | Loss: 0.00000932
Iteration 63/1000 | Loss: 0.00000932
Iteration 64/1000 | Loss: 0.00000932
Iteration 65/1000 | Loss: 0.00000931
Iteration 66/1000 | Loss: 0.00000931
Iteration 67/1000 | Loss: 0.00000931
Iteration 68/1000 | Loss: 0.00000931
Iteration 69/1000 | Loss: 0.00000930
Iteration 70/1000 | Loss: 0.00000930
Iteration 71/1000 | Loss: 0.00000930
Iteration 72/1000 | Loss: 0.00000930
Iteration 73/1000 | Loss: 0.00000930
Iteration 74/1000 | Loss: 0.00000930
Iteration 75/1000 | Loss: 0.00000930
Iteration 76/1000 | Loss: 0.00000930
Iteration 77/1000 | Loss: 0.00000930
Iteration 78/1000 | Loss: 0.00000929
Iteration 79/1000 | Loss: 0.00000929
Iteration 80/1000 | Loss: 0.00000929
Iteration 81/1000 | Loss: 0.00000928
Iteration 82/1000 | Loss: 0.00000928
Iteration 83/1000 | Loss: 0.00000928
Iteration 84/1000 | Loss: 0.00000926
Iteration 85/1000 | Loss: 0.00000926
Iteration 86/1000 | Loss: 0.00000926
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000926
Iteration 91/1000 | Loss: 0.00000926
Iteration 92/1000 | Loss: 0.00000926
Iteration 93/1000 | Loss: 0.00000926
Iteration 94/1000 | Loss: 0.00000926
Iteration 95/1000 | Loss: 0.00000925
Iteration 96/1000 | Loss: 0.00000925
Iteration 97/1000 | Loss: 0.00000925
Iteration 98/1000 | Loss: 0.00000925
Iteration 99/1000 | Loss: 0.00000925
Iteration 100/1000 | Loss: 0.00000925
Iteration 101/1000 | Loss: 0.00000925
Iteration 102/1000 | Loss: 0.00000924
Iteration 103/1000 | Loss: 0.00000924
Iteration 104/1000 | Loss: 0.00000923
Iteration 105/1000 | Loss: 0.00000923
Iteration 106/1000 | Loss: 0.00000923
Iteration 107/1000 | Loss: 0.00000923
Iteration 108/1000 | Loss: 0.00000922
Iteration 109/1000 | Loss: 0.00000922
Iteration 110/1000 | Loss: 0.00000922
Iteration 111/1000 | Loss: 0.00000922
Iteration 112/1000 | Loss: 0.00000922
Iteration 113/1000 | Loss: 0.00000922
Iteration 114/1000 | Loss: 0.00000922
Iteration 115/1000 | Loss: 0.00000922
Iteration 116/1000 | Loss: 0.00000922
Iteration 117/1000 | Loss: 0.00000922
Iteration 118/1000 | Loss: 0.00000922
Iteration 119/1000 | Loss: 0.00000921
Iteration 120/1000 | Loss: 0.00000921
Iteration 121/1000 | Loss: 0.00000921
Iteration 122/1000 | Loss: 0.00000921
Iteration 123/1000 | Loss: 0.00000921
Iteration 124/1000 | Loss: 0.00000921
Iteration 125/1000 | Loss: 0.00000921
Iteration 126/1000 | Loss: 0.00000921
Iteration 127/1000 | Loss: 0.00000921
Iteration 128/1000 | Loss: 0.00000921
Iteration 129/1000 | Loss: 0.00000920
Iteration 130/1000 | Loss: 0.00000920
Iteration 131/1000 | Loss: 0.00000920
Iteration 132/1000 | Loss: 0.00000920
Iteration 133/1000 | Loss: 0.00000920
Iteration 134/1000 | Loss: 0.00000920
Iteration 135/1000 | Loss: 0.00000920
Iteration 136/1000 | Loss: 0.00000920
Iteration 137/1000 | Loss: 0.00000920
Iteration 138/1000 | Loss: 0.00000920
Iteration 139/1000 | Loss: 0.00000920
Iteration 140/1000 | Loss: 0.00000920
Iteration 141/1000 | Loss: 0.00000920
Iteration 142/1000 | Loss: 0.00000919
Iteration 143/1000 | Loss: 0.00000919
Iteration 144/1000 | Loss: 0.00000919
Iteration 145/1000 | Loss: 0.00000919
Iteration 146/1000 | Loss: 0.00000919
Iteration 147/1000 | Loss: 0.00000919
Iteration 148/1000 | Loss: 0.00000919
Iteration 149/1000 | Loss: 0.00000919
Iteration 150/1000 | Loss: 0.00000919
Iteration 151/1000 | Loss: 0.00000919
Iteration 152/1000 | Loss: 0.00000919
Iteration 153/1000 | Loss: 0.00000918
Iteration 154/1000 | Loss: 0.00000918
Iteration 155/1000 | Loss: 0.00000918
Iteration 156/1000 | Loss: 0.00000918
Iteration 157/1000 | Loss: 0.00000918
Iteration 158/1000 | Loss: 0.00000918
Iteration 159/1000 | Loss: 0.00000918
Iteration 160/1000 | Loss: 0.00000918
Iteration 161/1000 | Loss: 0.00000918
Iteration 162/1000 | Loss: 0.00000918
Iteration 163/1000 | Loss: 0.00000918
Iteration 164/1000 | Loss: 0.00000918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [9.182024768961128e-06, 9.182024768961128e-06, 9.182024768961128e-06, 9.182024768961128e-06, 9.182024768961128e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.182024768961128e-06

Optimization complete. Final v2v error: 2.5084095001220703 mm

Highest mean error: 3.038630485534668 mm for frame 93

Lowest mean error: 2.0407121181488037 mm for frame 132

Saving results

Total time: 34.086509466171265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00314410
Iteration 2/25 | Loss: 0.00097066
Iteration 3/25 | Loss: 0.00081809
Iteration 4/25 | Loss: 0.00079388
Iteration 5/25 | Loss: 0.00078672
Iteration 6/25 | Loss: 0.00078420
Iteration 7/25 | Loss: 0.00078409
Iteration 8/25 | Loss: 0.00078409
Iteration 9/25 | Loss: 0.00078409
Iteration 10/25 | Loss: 0.00078409
Iteration 11/25 | Loss: 0.00078409
Iteration 12/25 | Loss: 0.00078409
Iteration 13/25 | Loss: 0.00078409
Iteration 14/25 | Loss: 0.00078409
Iteration 15/25 | Loss: 0.00078409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007840868202038109, 0.0007840868202038109, 0.0007840868202038109, 0.0007840868202038109, 0.0007840868202038109]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007840868202038109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36142516
Iteration 2/25 | Loss: 0.00048010
Iteration 3/25 | Loss: 0.00048010
Iteration 4/25 | Loss: 0.00048010
Iteration 5/25 | Loss: 0.00048010
Iteration 6/25 | Loss: 0.00048010
Iteration 7/25 | Loss: 0.00048010
Iteration 8/25 | Loss: 0.00048010
Iteration 9/25 | Loss: 0.00048010
Iteration 10/25 | Loss: 0.00048010
Iteration 11/25 | Loss: 0.00048010
Iteration 12/25 | Loss: 0.00048010
Iteration 13/25 | Loss: 0.00048010
Iteration 14/25 | Loss: 0.00048010
Iteration 15/25 | Loss: 0.00048010
Iteration 16/25 | Loss: 0.00048010
Iteration 17/25 | Loss: 0.00048010
Iteration 18/25 | Loss: 0.00048010
Iteration 19/25 | Loss: 0.00048010
Iteration 20/25 | Loss: 0.00048010
Iteration 21/25 | Loss: 0.00048010
Iteration 22/25 | Loss: 0.00048010
Iteration 23/25 | Loss: 0.00048010
Iteration 24/25 | Loss: 0.00048010
Iteration 25/25 | Loss: 0.00048010

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048010
Iteration 2/1000 | Loss: 0.00002084
Iteration 3/1000 | Loss: 0.00001196
Iteration 4/1000 | Loss: 0.00001015
Iteration 5/1000 | Loss: 0.00000938
Iteration 6/1000 | Loss: 0.00000869
Iteration 7/1000 | Loss: 0.00000835
Iteration 8/1000 | Loss: 0.00000819
Iteration 9/1000 | Loss: 0.00000814
Iteration 10/1000 | Loss: 0.00000812
Iteration 11/1000 | Loss: 0.00000811
Iteration 12/1000 | Loss: 0.00000808
Iteration 13/1000 | Loss: 0.00000801
Iteration 14/1000 | Loss: 0.00000795
Iteration 15/1000 | Loss: 0.00000792
Iteration 16/1000 | Loss: 0.00000792
Iteration 17/1000 | Loss: 0.00000791
Iteration 18/1000 | Loss: 0.00000790
Iteration 19/1000 | Loss: 0.00000788
Iteration 20/1000 | Loss: 0.00000787
Iteration 21/1000 | Loss: 0.00000787
Iteration 22/1000 | Loss: 0.00000786
Iteration 23/1000 | Loss: 0.00000786
Iteration 24/1000 | Loss: 0.00000786
Iteration 25/1000 | Loss: 0.00000785
Iteration 26/1000 | Loss: 0.00000785
Iteration 27/1000 | Loss: 0.00000785
Iteration 28/1000 | Loss: 0.00000785
Iteration 29/1000 | Loss: 0.00000785
Iteration 30/1000 | Loss: 0.00000785
Iteration 31/1000 | Loss: 0.00000785
Iteration 32/1000 | Loss: 0.00000785
Iteration 33/1000 | Loss: 0.00000785
Iteration 34/1000 | Loss: 0.00000785
Iteration 35/1000 | Loss: 0.00000785
Iteration 36/1000 | Loss: 0.00000785
Iteration 37/1000 | Loss: 0.00000785
Iteration 38/1000 | Loss: 0.00000785
Iteration 39/1000 | Loss: 0.00000785
Iteration 40/1000 | Loss: 0.00000785
Iteration 41/1000 | Loss: 0.00000785
Iteration 42/1000 | Loss: 0.00000785
Iteration 43/1000 | Loss: 0.00000785
Iteration 44/1000 | Loss: 0.00000785
Iteration 45/1000 | Loss: 0.00000785
Iteration 46/1000 | Loss: 0.00000785
Iteration 47/1000 | Loss: 0.00000785
Iteration 48/1000 | Loss: 0.00000785
Iteration 49/1000 | Loss: 0.00000785
Iteration 50/1000 | Loss: 0.00000785
Iteration 51/1000 | Loss: 0.00000785
Iteration 52/1000 | Loss: 0.00000785
Iteration 53/1000 | Loss: 0.00000785
Iteration 54/1000 | Loss: 0.00000785
Iteration 55/1000 | Loss: 0.00000785
Iteration 56/1000 | Loss: 0.00000785
Iteration 57/1000 | Loss: 0.00000785
Iteration 58/1000 | Loss: 0.00000785
Iteration 59/1000 | Loss: 0.00000785
Iteration 60/1000 | Loss: 0.00000785
Iteration 61/1000 | Loss: 0.00000785
Iteration 62/1000 | Loss: 0.00000785
Iteration 63/1000 | Loss: 0.00000785
Iteration 64/1000 | Loss: 0.00000785
Iteration 65/1000 | Loss: 0.00000785
Iteration 66/1000 | Loss: 0.00000785
Iteration 67/1000 | Loss: 0.00000785
Iteration 68/1000 | Loss: 0.00000785
Iteration 69/1000 | Loss: 0.00000785
Iteration 70/1000 | Loss: 0.00000785
Iteration 71/1000 | Loss: 0.00000785
Iteration 72/1000 | Loss: 0.00000785
Iteration 73/1000 | Loss: 0.00000785
Iteration 74/1000 | Loss: 0.00000785
Iteration 75/1000 | Loss: 0.00000785
Iteration 76/1000 | Loss: 0.00000785
Iteration 77/1000 | Loss: 0.00000785
Iteration 78/1000 | Loss: 0.00000785
Iteration 79/1000 | Loss: 0.00000785
Iteration 80/1000 | Loss: 0.00000785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [7.851920599932782e-06, 7.851920599932782e-06, 7.851920599932782e-06, 7.851920599932782e-06, 7.851920599932782e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.851920599932782e-06

Optimization complete. Final v2v error: 2.3863344192504883 mm

Highest mean error: 2.647754192352295 mm for frame 5

Lowest mean error: 2.1551637649536133 mm for frame 144

Saving results

Total time: 30.150978088378906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818110
Iteration 2/25 | Loss: 0.00118497
Iteration 3/25 | Loss: 0.00095528
Iteration 4/25 | Loss: 0.00091699
Iteration 5/25 | Loss: 0.00090851
Iteration 6/25 | Loss: 0.00090615
Iteration 7/25 | Loss: 0.00090602
Iteration 8/25 | Loss: 0.00090602
Iteration 9/25 | Loss: 0.00090602
Iteration 10/25 | Loss: 0.00090602
Iteration 11/25 | Loss: 0.00090602
Iteration 12/25 | Loss: 0.00090602
Iteration 13/25 | Loss: 0.00090602
Iteration 14/25 | Loss: 0.00090602
Iteration 15/25 | Loss: 0.00090602
Iteration 16/25 | Loss: 0.00090602
Iteration 17/25 | Loss: 0.00090602
Iteration 18/25 | Loss: 0.00090602
Iteration 19/25 | Loss: 0.00090602
Iteration 20/25 | Loss: 0.00090602
Iteration 21/25 | Loss: 0.00090602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009060191223397851, 0.0009060191223397851, 0.0009060191223397851, 0.0009060191223397851, 0.0009060191223397851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009060191223397851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18135941
Iteration 2/25 | Loss: 0.00036167
Iteration 3/25 | Loss: 0.00036167
Iteration 4/25 | Loss: 0.00036167
Iteration 5/25 | Loss: 0.00036167
Iteration 6/25 | Loss: 0.00036167
Iteration 7/25 | Loss: 0.00036167
Iteration 8/25 | Loss: 0.00036167
Iteration 9/25 | Loss: 0.00036167
Iteration 10/25 | Loss: 0.00036167
Iteration 11/25 | Loss: 0.00036167
Iteration 12/25 | Loss: 0.00036167
Iteration 13/25 | Loss: 0.00036167
Iteration 14/25 | Loss: 0.00036167
Iteration 15/25 | Loss: 0.00036167
Iteration 16/25 | Loss: 0.00036167
Iteration 17/25 | Loss: 0.00036167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003616690228227526, 0.0003616690228227526, 0.0003616690228227526, 0.0003616690228227526, 0.0003616690228227526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003616690228227526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036167
Iteration 2/1000 | Loss: 0.00004578
Iteration 3/1000 | Loss: 0.00002744
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002181
Iteration 6/1000 | Loss: 0.00002055
Iteration 7/1000 | Loss: 0.00001985
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001842
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001758
Iteration 14/1000 | Loss: 0.00001757
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001739
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001717
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001707
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001703
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001700
Iteration 35/1000 | Loss: 0.00001700
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001699
Iteration 38/1000 | Loss: 0.00001698
Iteration 39/1000 | Loss: 0.00001698
Iteration 40/1000 | Loss: 0.00001698
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001697
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001695
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001695
Iteration 56/1000 | Loss: 0.00001695
Iteration 57/1000 | Loss: 0.00001695
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00001694
Iteration 60/1000 | Loss: 0.00001694
Iteration 61/1000 | Loss: 0.00001694
Iteration 62/1000 | Loss: 0.00001693
Iteration 63/1000 | Loss: 0.00001693
Iteration 64/1000 | Loss: 0.00001693
Iteration 65/1000 | Loss: 0.00001693
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001692
Iteration 68/1000 | Loss: 0.00001692
Iteration 69/1000 | Loss: 0.00001692
Iteration 70/1000 | Loss: 0.00001692
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.6907202734728344e-05, 1.6907202734728344e-05, 1.6907202734728344e-05, 1.6907202734728344e-05, 1.6907202734728344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6907202734728344e-05

Optimization complete. Final v2v error: 3.3936007022857666 mm

Highest mean error: 4.615710258483887 mm for frame 114

Lowest mean error: 2.4110934734344482 mm for frame 216

Saving results

Total time: 42.51979875564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990665
Iteration 2/25 | Loss: 0.00179040
Iteration 3/25 | Loss: 0.00142297
Iteration 4/25 | Loss: 0.00135495
Iteration 5/25 | Loss: 0.00137336
Iteration 6/25 | Loss: 0.00134544
Iteration 7/25 | Loss: 0.00129265
Iteration 8/25 | Loss: 0.00125556
Iteration 9/25 | Loss: 0.00123519
Iteration 10/25 | Loss: 0.00123050
Iteration 11/25 | Loss: 0.00123188
Iteration 12/25 | Loss: 0.00121779
Iteration 13/25 | Loss: 0.00120872
Iteration 14/25 | Loss: 0.00120907
Iteration 15/25 | Loss: 0.00121074
Iteration 16/25 | Loss: 0.00120299
Iteration 17/25 | Loss: 0.00119846
Iteration 18/25 | Loss: 0.00119688
Iteration 19/25 | Loss: 0.00119648
Iteration 20/25 | Loss: 0.00119638
Iteration 21/25 | Loss: 0.00119638
Iteration 22/25 | Loss: 0.00119638
Iteration 23/25 | Loss: 0.00119638
Iteration 24/25 | Loss: 0.00119638
Iteration 25/25 | Loss: 0.00119638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.18736194
Iteration 2/25 | Loss: 0.00133224
Iteration 3/25 | Loss: 0.00133224
Iteration 4/25 | Loss: 0.00133224
Iteration 5/25 | Loss: 0.00133223
Iteration 6/25 | Loss: 0.00133223
Iteration 7/25 | Loss: 0.00133223
Iteration 8/25 | Loss: 0.00133223
Iteration 9/25 | Loss: 0.00133223
Iteration 10/25 | Loss: 0.00133223
Iteration 11/25 | Loss: 0.00133223
Iteration 12/25 | Loss: 0.00133223
Iteration 13/25 | Loss: 0.00133223
Iteration 14/25 | Loss: 0.00133223
Iteration 15/25 | Loss: 0.00133223
Iteration 16/25 | Loss: 0.00133223
Iteration 17/25 | Loss: 0.00133223
Iteration 18/25 | Loss: 0.00133223
Iteration 19/25 | Loss: 0.00133223
Iteration 20/25 | Loss: 0.00133223
Iteration 21/25 | Loss: 0.00133223
Iteration 22/25 | Loss: 0.00133223
Iteration 23/25 | Loss: 0.00133223
Iteration 24/25 | Loss: 0.00133223
Iteration 25/25 | Loss: 0.00133223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133223
Iteration 2/1000 | Loss: 0.00017501
Iteration 3/1000 | Loss: 0.00012231
Iteration 4/1000 | Loss: 0.00009886
Iteration 5/1000 | Loss: 0.00015247
Iteration 6/1000 | Loss: 0.00008841
Iteration 7/1000 | Loss: 0.00008465
Iteration 8/1000 | Loss: 0.00021715
Iteration 9/1000 | Loss: 0.00025410
Iteration 10/1000 | Loss: 0.00008391
Iteration 11/1000 | Loss: 0.00007638
Iteration 12/1000 | Loss: 0.00007210
Iteration 13/1000 | Loss: 0.00006986
Iteration 14/1000 | Loss: 0.00006873
Iteration 15/1000 | Loss: 0.00006746
Iteration 16/1000 | Loss: 0.00006655
Iteration 17/1000 | Loss: 0.00006569
Iteration 18/1000 | Loss: 0.00006489
Iteration 19/1000 | Loss: 0.00006409
Iteration 20/1000 | Loss: 0.00006355
Iteration 21/1000 | Loss: 0.00006297
Iteration 22/1000 | Loss: 0.00024036
Iteration 23/1000 | Loss: 0.00015228
Iteration 24/1000 | Loss: 0.00038693
Iteration 25/1000 | Loss: 0.00006486
Iteration 26/1000 | Loss: 0.00006019
Iteration 27/1000 | Loss: 0.00005825
Iteration 28/1000 | Loss: 0.00005690
Iteration 29/1000 | Loss: 0.00005596
Iteration 30/1000 | Loss: 0.00080930
Iteration 31/1000 | Loss: 0.00081171
Iteration 32/1000 | Loss: 0.00008181
Iteration 33/1000 | Loss: 0.00006472
Iteration 34/1000 | Loss: 0.00005954
Iteration 35/1000 | Loss: 0.00005598
Iteration 36/1000 | Loss: 0.00005491
Iteration 37/1000 | Loss: 0.00005438
Iteration 38/1000 | Loss: 0.00071068
Iteration 39/1000 | Loss: 0.00014476
Iteration 40/1000 | Loss: 0.00047657
Iteration 41/1000 | Loss: 0.00021956
Iteration 42/1000 | Loss: 0.00026976
Iteration 43/1000 | Loss: 0.00005599
Iteration 44/1000 | Loss: 0.00005410
Iteration 45/1000 | Loss: 0.00005260
Iteration 46/1000 | Loss: 0.00005146
Iteration 47/1000 | Loss: 0.00005046
Iteration 48/1000 | Loss: 0.00005000
Iteration 49/1000 | Loss: 0.00004972
Iteration 50/1000 | Loss: 0.00004963
Iteration 51/1000 | Loss: 0.00004958
Iteration 52/1000 | Loss: 0.00004956
Iteration 53/1000 | Loss: 0.00004952
Iteration 54/1000 | Loss: 0.00004949
Iteration 55/1000 | Loss: 0.00004945
Iteration 56/1000 | Loss: 0.00004944
Iteration 57/1000 | Loss: 0.00004943
Iteration 58/1000 | Loss: 0.00004943
Iteration 59/1000 | Loss: 0.00004943
Iteration 60/1000 | Loss: 0.00004942
Iteration 61/1000 | Loss: 0.00004942
Iteration 62/1000 | Loss: 0.00004942
Iteration 63/1000 | Loss: 0.00004942
Iteration 64/1000 | Loss: 0.00004942
Iteration 65/1000 | Loss: 0.00004941
Iteration 66/1000 | Loss: 0.00004940
Iteration 67/1000 | Loss: 0.00004940
Iteration 68/1000 | Loss: 0.00004939
Iteration 69/1000 | Loss: 0.00004938
Iteration 70/1000 | Loss: 0.00004938
Iteration 71/1000 | Loss: 0.00004938
Iteration 72/1000 | Loss: 0.00004937
Iteration 73/1000 | Loss: 0.00004937
Iteration 74/1000 | Loss: 0.00004937
Iteration 75/1000 | Loss: 0.00004937
Iteration 76/1000 | Loss: 0.00004936
Iteration 77/1000 | Loss: 0.00004936
Iteration 78/1000 | Loss: 0.00004936
Iteration 79/1000 | Loss: 0.00004936
Iteration 80/1000 | Loss: 0.00004935
Iteration 81/1000 | Loss: 0.00004933
Iteration 82/1000 | Loss: 0.00004933
Iteration 83/1000 | Loss: 0.00004933
Iteration 84/1000 | Loss: 0.00004932
Iteration 85/1000 | Loss: 0.00004932
Iteration 86/1000 | Loss: 0.00004932
Iteration 87/1000 | Loss: 0.00004932
Iteration 88/1000 | Loss: 0.00004932
Iteration 89/1000 | Loss: 0.00004931
Iteration 90/1000 | Loss: 0.00004931
Iteration 91/1000 | Loss: 0.00004931
Iteration 92/1000 | Loss: 0.00004930
Iteration 93/1000 | Loss: 0.00004930
Iteration 94/1000 | Loss: 0.00004930
Iteration 95/1000 | Loss: 0.00004929
Iteration 96/1000 | Loss: 0.00004929
Iteration 97/1000 | Loss: 0.00004929
Iteration 98/1000 | Loss: 0.00004926
Iteration 99/1000 | Loss: 0.00004925
Iteration 100/1000 | Loss: 0.00004925
Iteration 101/1000 | Loss: 0.00004924
Iteration 102/1000 | Loss: 0.00004924
Iteration 103/1000 | Loss: 0.00004923
Iteration 104/1000 | Loss: 0.00004923
Iteration 105/1000 | Loss: 0.00004922
Iteration 106/1000 | Loss: 0.00004922
Iteration 107/1000 | Loss: 0.00004922
Iteration 108/1000 | Loss: 0.00004921
Iteration 109/1000 | Loss: 0.00004921
Iteration 110/1000 | Loss: 0.00004921
Iteration 111/1000 | Loss: 0.00004921
Iteration 112/1000 | Loss: 0.00004920
Iteration 113/1000 | Loss: 0.00004920
Iteration 114/1000 | Loss: 0.00004920
Iteration 115/1000 | Loss: 0.00004920
Iteration 116/1000 | Loss: 0.00004920
Iteration 117/1000 | Loss: 0.00004920
Iteration 118/1000 | Loss: 0.00004920
Iteration 119/1000 | Loss: 0.00004920
Iteration 120/1000 | Loss: 0.00004920
Iteration 121/1000 | Loss: 0.00004920
Iteration 122/1000 | Loss: 0.00004919
Iteration 123/1000 | Loss: 0.00004919
Iteration 124/1000 | Loss: 0.00004919
Iteration 125/1000 | Loss: 0.00004918
Iteration 126/1000 | Loss: 0.00004918
Iteration 127/1000 | Loss: 0.00004918
Iteration 128/1000 | Loss: 0.00004918
Iteration 129/1000 | Loss: 0.00004918
Iteration 130/1000 | Loss: 0.00004917
Iteration 131/1000 | Loss: 0.00004917
Iteration 132/1000 | Loss: 0.00004917
Iteration 133/1000 | Loss: 0.00004916
Iteration 134/1000 | Loss: 0.00004916
Iteration 135/1000 | Loss: 0.00004916
Iteration 136/1000 | Loss: 0.00004915
Iteration 137/1000 | Loss: 0.00004915
Iteration 138/1000 | Loss: 0.00004915
Iteration 139/1000 | Loss: 0.00004914
Iteration 140/1000 | Loss: 0.00004914
Iteration 141/1000 | Loss: 0.00004914
Iteration 142/1000 | Loss: 0.00004914
Iteration 143/1000 | Loss: 0.00004913
Iteration 144/1000 | Loss: 0.00004913
Iteration 145/1000 | Loss: 0.00004913
Iteration 146/1000 | Loss: 0.00004913
Iteration 147/1000 | Loss: 0.00004913
Iteration 148/1000 | Loss: 0.00004913
Iteration 149/1000 | Loss: 0.00004912
Iteration 150/1000 | Loss: 0.00004912
Iteration 151/1000 | Loss: 0.00004912
Iteration 152/1000 | Loss: 0.00004912
Iteration 153/1000 | Loss: 0.00004912
Iteration 154/1000 | Loss: 0.00004912
Iteration 155/1000 | Loss: 0.00004912
Iteration 156/1000 | Loss: 0.00004912
Iteration 157/1000 | Loss: 0.00004912
Iteration 158/1000 | Loss: 0.00004912
Iteration 159/1000 | Loss: 0.00004912
Iteration 160/1000 | Loss: 0.00004912
Iteration 161/1000 | Loss: 0.00004912
Iteration 162/1000 | Loss: 0.00004912
Iteration 163/1000 | Loss: 0.00004912
Iteration 164/1000 | Loss: 0.00004912
Iteration 165/1000 | Loss: 0.00004912
Iteration 166/1000 | Loss: 0.00004912
Iteration 167/1000 | Loss: 0.00004912
Iteration 168/1000 | Loss: 0.00004911
Iteration 169/1000 | Loss: 0.00004911
Iteration 170/1000 | Loss: 0.00004911
Iteration 171/1000 | Loss: 0.00004911
Iteration 172/1000 | Loss: 0.00004911
Iteration 173/1000 | Loss: 0.00004911
Iteration 174/1000 | Loss: 0.00004911
Iteration 175/1000 | Loss: 0.00004911
Iteration 176/1000 | Loss: 0.00004911
Iteration 177/1000 | Loss: 0.00004911
Iteration 178/1000 | Loss: 0.00004911
Iteration 179/1000 | Loss: 0.00004911
Iteration 180/1000 | Loss: 0.00004911
Iteration 181/1000 | Loss: 0.00004911
Iteration 182/1000 | Loss: 0.00004911
Iteration 183/1000 | Loss: 0.00004911
Iteration 184/1000 | Loss: 0.00004911
Iteration 185/1000 | Loss: 0.00004911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [4.9113939894596115e-05, 4.9113939894596115e-05, 4.9113939894596115e-05, 4.9113939894596115e-05, 4.9113939894596115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9113939894596115e-05

Optimization complete. Final v2v error: 4.696838855743408 mm

Highest mean error: 13.871920585632324 mm for frame 24

Lowest mean error: 3.585808753967285 mm for frame 83

Saving results

Total time: 115.02845406532288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397645
Iteration 2/25 | Loss: 0.00089720
Iteration 3/25 | Loss: 0.00081034
Iteration 4/25 | Loss: 0.00079858
Iteration 5/25 | Loss: 0.00079416
Iteration 6/25 | Loss: 0.00079265
Iteration 7/25 | Loss: 0.00079265
Iteration 8/25 | Loss: 0.00079265
Iteration 9/25 | Loss: 0.00079265
Iteration 10/25 | Loss: 0.00079265
Iteration 11/25 | Loss: 0.00079265
Iteration 12/25 | Loss: 0.00079265
Iteration 13/25 | Loss: 0.00079265
Iteration 14/25 | Loss: 0.00079265
Iteration 15/25 | Loss: 0.00079265
Iteration 16/25 | Loss: 0.00079265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007926495745778084, 0.0007926495745778084, 0.0007926495745778084, 0.0007926495745778084, 0.0007926495745778084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007926495745778084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88050890
Iteration 2/25 | Loss: 0.00050706
Iteration 3/25 | Loss: 0.00050706
Iteration 4/25 | Loss: 0.00050706
Iteration 5/25 | Loss: 0.00050705
Iteration 6/25 | Loss: 0.00050705
Iteration 7/25 | Loss: 0.00050705
Iteration 8/25 | Loss: 0.00050705
Iteration 9/25 | Loss: 0.00050705
Iteration 10/25 | Loss: 0.00050705
Iteration 11/25 | Loss: 0.00050705
Iteration 12/25 | Loss: 0.00050705
Iteration 13/25 | Loss: 0.00050705
Iteration 14/25 | Loss: 0.00050705
Iteration 15/25 | Loss: 0.00050705
Iteration 16/25 | Loss: 0.00050705
Iteration 17/25 | Loss: 0.00050705
Iteration 18/25 | Loss: 0.00050705
Iteration 19/25 | Loss: 0.00050705
Iteration 20/25 | Loss: 0.00050705
Iteration 21/25 | Loss: 0.00050705
Iteration 22/25 | Loss: 0.00050705
Iteration 23/25 | Loss: 0.00050705
Iteration 24/25 | Loss: 0.00050705
Iteration 25/25 | Loss: 0.00050705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050705
Iteration 2/1000 | Loss: 0.00001260
Iteration 3/1000 | Loss: 0.00000872
Iteration 4/1000 | Loss: 0.00000799
Iteration 5/1000 | Loss: 0.00000762
Iteration 6/1000 | Loss: 0.00000742
Iteration 7/1000 | Loss: 0.00000732
Iteration 8/1000 | Loss: 0.00000732
Iteration 9/1000 | Loss: 0.00000730
Iteration 10/1000 | Loss: 0.00000725
Iteration 11/1000 | Loss: 0.00000723
Iteration 12/1000 | Loss: 0.00000722
Iteration 13/1000 | Loss: 0.00000722
Iteration 14/1000 | Loss: 0.00000722
Iteration 15/1000 | Loss: 0.00000722
Iteration 16/1000 | Loss: 0.00000721
Iteration 17/1000 | Loss: 0.00000721
Iteration 18/1000 | Loss: 0.00000717
Iteration 19/1000 | Loss: 0.00000716
Iteration 20/1000 | Loss: 0.00000715
Iteration 21/1000 | Loss: 0.00000706
Iteration 22/1000 | Loss: 0.00000705
Iteration 23/1000 | Loss: 0.00000705
Iteration 24/1000 | Loss: 0.00000703
Iteration 25/1000 | Loss: 0.00000702
Iteration 26/1000 | Loss: 0.00000702
Iteration 27/1000 | Loss: 0.00000701
Iteration 28/1000 | Loss: 0.00000701
Iteration 29/1000 | Loss: 0.00000701
Iteration 30/1000 | Loss: 0.00000701
Iteration 31/1000 | Loss: 0.00000701
Iteration 32/1000 | Loss: 0.00000700
Iteration 33/1000 | Loss: 0.00000700
Iteration 34/1000 | Loss: 0.00000699
Iteration 35/1000 | Loss: 0.00000699
Iteration 36/1000 | Loss: 0.00000699
Iteration 37/1000 | Loss: 0.00000698
Iteration 38/1000 | Loss: 0.00000698
Iteration 39/1000 | Loss: 0.00000698
Iteration 40/1000 | Loss: 0.00000698
Iteration 41/1000 | Loss: 0.00000698
Iteration 42/1000 | Loss: 0.00000697
Iteration 43/1000 | Loss: 0.00000696
Iteration 44/1000 | Loss: 0.00000696
Iteration 45/1000 | Loss: 0.00000696
Iteration 46/1000 | Loss: 0.00000696
Iteration 47/1000 | Loss: 0.00000696
Iteration 48/1000 | Loss: 0.00000695
Iteration 49/1000 | Loss: 0.00000695
Iteration 50/1000 | Loss: 0.00000695
Iteration 51/1000 | Loss: 0.00000694
Iteration 52/1000 | Loss: 0.00000694
Iteration 53/1000 | Loss: 0.00000694
Iteration 54/1000 | Loss: 0.00000693
Iteration 55/1000 | Loss: 0.00000693
Iteration 56/1000 | Loss: 0.00000692
Iteration 57/1000 | Loss: 0.00000692
Iteration 58/1000 | Loss: 0.00000691
Iteration 59/1000 | Loss: 0.00000691
Iteration 60/1000 | Loss: 0.00000691
Iteration 61/1000 | Loss: 0.00000690
Iteration 62/1000 | Loss: 0.00000690
Iteration 63/1000 | Loss: 0.00000690
Iteration 64/1000 | Loss: 0.00000690
Iteration 65/1000 | Loss: 0.00000690
Iteration 66/1000 | Loss: 0.00000690
Iteration 67/1000 | Loss: 0.00000690
Iteration 68/1000 | Loss: 0.00000690
Iteration 69/1000 | Loss: 0.00000690
Iteration 70/1000 | Loss: 0.00000690
Iteration 71/1000 | Loss: 0.00000690
Iteration 72/1000 | Loss: 0.00000689
Iteration 73/1000 | Loss: 0.00000689
Iteration 74/1000 | Loss: 0.00000688
Iteration 75/1000 | Loss: 0.00000688
Iteration 76/1000 | Loss: 0.00000688
Iteration 77/1000 | Loss: 0.00000688
Iteration 78/1000 | Loss: 0.00000687
Iteration 79/1000 | Loss: 0.00000687
Iteration 80/1000 | Loss: 0.00000687
Iteration 81/1000 | Loss: 0.00000687
Iteration 82/1000 | Loss: 0.00000687
Iteration 83/1000 | Loss: 0.00000687
Iteration 84/1000 | Loss: 0.00000687
Iteration 85/1000 | Loss: 0.00000687
Iteration 86/1000 | Loss: 0.00000687
Iteration 87/1000 | Loss: 0.00000687
Iteration 88/1000 | Loss: 0.00000686
Iteration 89/1000 | Loss: 0.00000686
Iteration 90/1000 | Loss: 0.00000686
Iteration 91/1000 | Loss: 0.00000686
Iteration 92/1000 | Loss: 0.00000685
Iteration 93/1000 | Loss: 0.00000685
Iteration 94/1000 | Loss: 0.00000685
Iteration 95/1000 | Loss: 0.00000685
Iteration 96/1000 | Loss: 0.00000685
Iteration 97/1000 | Loss: 0.00000685
Iteration 98/1000 | Loss: 0.00000685
Iteration 99/1000 | Loss: 0.00000685
Iteration 100/1000 | Loss: 0.00000684
Iteration 101/1000 | Loss: 0.00000684
Iteration 102/1000 | Loss: 0.00000684
Iteration 103/1000 | Loss: 0.00000684
Iteration 104/1000 | Loss: 0.00000684
Iteration 105/1000 | Loss: 0.00000684
Iteration 106/1000 | Loss: 0.00000684
Iteration 107/1000 | Loss: 0.00000684
Iteration 108/1000 | Loss: 0.00000684
Iteration 109/1000 | Loss: 0.00000684
Iteration 110/1000 | Loss: 0.00000683
Iteration 111/1000 | Loss: 0.00000683
Iteration 112/1000 | Loss: 0.00000683
Iteration 113/1000 | Loss: 0.00000683
Iteration 114/1000 | Loss: 0.00000683
Iteration 115/1000 | Loss: 0.00000683
Iteration 116/1000 | Loss: 0.00000683
Iteration 117/1000 | Loss: 0.00000683
Iteration 118/1000 | Loss: 0.00000683
Iteration 119/1000 | Loss: 0.00000683
Iteration 120/1000 | Loss: 0.00000683
Iteration 121/1000 | Loss: 0.00000682
Iteration 122/1000 | Loss: 0.00000682
Iteration 123/1000 | Loss: 0.00000682
Iteration 124/1000 | Loss: 0.00000682
Iteration 125/1000 | Loss: 0.00000682
Iteration 126/1000 | Loss: 0.00000682
Iteration 127/1000 | Loss: 0.00000682
Iteration 128/1000 | Loss: 0.00000681
Iteration 129/1000 | Loss: 0.00000681
Iteration 130/1000 | Loss: 0.00000681
Iteration 131/1000 | Loss: 0.00000681
Iteration 132/1000 | Loss: 0.00000681
Iteration 133/1000 | Loss: 0.00000681
Iteration 134/1000 | Loss: 0.00000681
Iteration 135/1000 | Loss: 0.00000681
Iteration 136/1000 | Loss: 0.00000681
Iteration 137/1000 | Loss: 0.00000681
Iteration 138/1000 | Loss: 0.00000681
Iteration 139/1000 | Loss: 0.00000681
Iteration 140/1000 | Loss: 0.00000681
Iteration 141/1000 | Loss: 0.00000681
Iteration 142/1000 | Loss: 0.00000681
Iteration 143/1000 | Loss: 0.00000681
Iteration 144/1000 | Loss: 0.00000681
Iteration 145/1000 | Loss: 0.00000681
Iteration 146/1000 | Loss: 0.00000681
Iteration 147/1000 | Loss: 0.00000681
Iteration 148/1000 | Loss: 0.00000681
Iteration 149/1000 | Loss: 0.00000681
Iteration 150/1000 | Loss: 0.00000681
Iteration 151/1000 | Loss: 0.00000681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [6.806080364185618e-06, 6.806080364185618e-06, 6.806080364185618e-06, 6.806080364185618e-06, 6.806080364185618e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.806080364185618e-06

Optimization complete. Final v2v error: 2.2413692474365234 mm

Highest mean error: 2.598111629486084 mm for frame 120

Lowest mean error: 2.0368993282318115 mm for frame 34

Saving results

Total time: 31.356480360031128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833605
Iteration 2/25 | Loss: 0.00121406
Iteration 3/25 | Loss: 0.00094511
Iteration 4/25 | Loss: 0.00091992
Iteration 5/25 | Loss: 0.00091650
Iteration 6/25 | Loss: 0.00091602
Iteration 7/25 | Loss: 0.00091602
Iteration 8/25 | Loss: 0.00091602
Iteration 9/25 | Loss: 0.00091602
Iteration 10/25 | Loss: 0.00091602
Iteration 11/25 | Loss: 0.00091602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009160161134786904, 0.0009160161134786904, 0.0009160161134786904, 0.0009160161134786904, 0.0009160161134786904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009160161134786904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92430699
Iteration 2/25 | Loss: 0.00019402
Iteration 3/25 | Loss: 0.00019402
Iteration 4/25 | Loss: 0.00019401
Iteration 5/25 | Loss: 0.00019401
Iteration 6/25 | Loss: 0.00019401
Iteration 7/25 | Loss: 0.00019401
Iteration 8/25 | Loss: 0.00019401
Iteration 9/25 | Loss: 0.00019401
Iteration 10/25 | Loss: 0.00019401
Iteration 11/25 | Loss: 0.00019401
Iteration 12/25 | Loss: 0.00019401
Iteration 13/25 | Loss: 0.00019401
Iteration 14/25 | Loss: 0.00019401
Iteration 15/25 | Loss: 0.00019401
Iteration 16/25 | Loss: 0.00019401
Iteration 17/25 | Loss: 0.00019401
Iteration 18/25 | Loss: 0.00019401
Iteration 19/25 | Loss: 0.00019401
Iteration 20/25 | Loss: 0.00019401
Iteration 21/25 | Loss: 0.00019401
Iteration 22/25 | Loss: 0.00019401
Iteration 23/25 | Loss: 0.00019401
Iteration 24/25 | Loss: 0.00019401
Iteration 25/25 | Loss: 0.00019401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019401
Iteration 2/1000 | Loss: 0.00002740
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001928
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001806
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001751
Iteration 11/1000 | Loss: 0.00001751
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001740
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001738
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001737
Iteration 19/1000 | Loss: 0.00001737
Iteration 20/1000 | Loss: 0.00001736
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001735
Iteration 30/1000 | Loss: 0.00001735
Iteration 31/1000 | Loss: 0.00001735
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001735
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001733
Iteration 36/1000 | Loss: 0.00001733
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001727
Iteration 67/1000 | Loss: 0.00001727
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001726
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.725617767078802e-05, 1.725617767078802e-05, 1.725617767078802e-05, 1.725617767078802e-05, 1.725617767078802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.725617767078802e-05

Optimization complete. Final v2v error: 3.4721853733062744 mm

Highest mean error: 3.5776548385620117 mm for frame 68

Lowest mean error: 3.3414266109466553 mm for frame 149

Saving results

Total time: 25.011005401611328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786711
Iteration 2/25 | Loss: 0.00121339
Iteration 3/25 | Loss: 0.00090556
Iteration 4/25 | Loss: 0.00086444
Iteration 5/25 | Loss: 0.00085796
Iteration 6/25 | Loss: 0.00085637
Iteration 7/25 | Loss: 0.00085637
Iteration 8/25 | Loss: 0.00085637
Iteration 9/25 | Loss: 0.00085637
Iteration 10/25 | Loss: 0.00085637
Iteration 11/25 | Loss: 0.00085637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008563658338971436, 0.0008563658338971436, 0.0008563658338971436, 0.0008563658338971436, 0.0008563658338971436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008563658338971436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35833085
Iteration 2/25 | Loss: 0.00033022
Iteration 3/25 | Loss: 0.00033018
Iteration 4/25 | Loss: 0.00033018
Iteration 5/25 | Loss: 0.00033018
Iteration 6/25 | Loss: 0.00033018
Iteration 7/25 | Loss: 0.00033018
Iteration 8/25 | Loss: 0.00033018
Iteration 9/25 | Loss: 0.00033018
Iteration 10/25 | Loss: 0.00033018
Iteration 11/25 | Loss: 0.00033018
Iteration 12/25 | Loss: 0.00033018
Iteration 13/25 | Loss: 0.00033018
Iteration 14/25 | Loss: 0.00033018
Iteration 15/25 | Loss: 0.00033018
Iteration 16/25 | Loss: 0.00033018
Iteration 17/25 | Loss: 0.00033018
Iteration 18/25 | Loss: 0.00033018
Iteration 19/25 | Loss: 0.00033018
Iteration 20/25 | Loss: 0.00033018
Iteration 21/25 | Loss: 0.00033018
Iteration 22/25 | Loss: 0.00033018
Iteration 23/25 | Loss: 0.00033018
Iteration 24/25 | Loss: 0.00033018
Iteration 25/25 | Loss: 0.00033018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033018
Iteration 2/1000 | Loss: 0.00003243
Iteration 3/1000 | Loss: 0.00001722
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001215
Iteration 7/1000 | Loss: 0.00001167
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001047
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001016
Iteration 13/1000 | Loss: 0.00001016
Iteration 14/1000 | Loss: 0.00001014
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00000995
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000994
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000989
Iteration 23/1000 | Loss: 0.00000988
Iteration 24/1000 | Loss: 0.00000986
Iteration 25/1000 | Loss: 0.00000983
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000979
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000977
Iteration 32/1000 | Loss: 0.00000977
Iteration 33/1000 | Loss: 0.00000977
Iteration 34/1000 | Loss: 0.00000976
Iteration 35/1000 | Loss: 0.00000976
Iteration 36/1000 | Loss: 0.00000975
Iteration 37/1000 | Loss: 0.00000975
Iteration 38/1000 | Loss: 0.00000975
Iteration 39/1000 | Loss: 0.00000974
Iteration 40/1000 | Loss: 0.00000974
Iteration 41/1000 | Loss: 0.00000974
Iteration 42/1000 | Loss: 0.00000973
Iteration 43/1000 | Loss: 0.00000973
Iteration 44/1000 | Loss: 0.00000972
Iteration 45/1000 | Loss: 0.00000972
Iteration 46/1000 | Loss: 0.00000970
Iteration 47/1000 | Loss: 0.00000970
Iteration 48/1000 | Loss: 0.00000970
Iteration 49/1000 | Loss: 0.00000970
Iteration 50/1000 | Loss: 0.00000969
Iteration 51/1000 | Loss: 0.00000969
Iteration 52/1000 | Loss: 0.00000969
Iteration 53/1000 | Loss: 0.00000969
Iteration 54/1000 | Loss: 0.00000969
Iteration 55/1000 | Loss: 0.00000969
Iteration 56/1000 | Loss: 0.00000969
Iteration 57/1000 | Loss: 0.00000969
Iteration 58/1000 | Loss: 0.00000969
Iteration 59/1000 | Loss: 0.00000969
Iteration 60/1000 | Loss: 0.00000969
Iteration 61/1000 | Loss: 0.00000969
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000968
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000967
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000967
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000964
Iteration 80/1000 | Loss: 0.00000964
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000964
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000963
Iteration 88/1000 | Loss: 0.00000963
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000963
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000962
Iteration 101/1000 | Loss: 0.00000962
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000962
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000962
Iteration 112/1000 | Loss: 0.00000962
Iteration 113/1000 | Loss: 0.00000962
Iteration 114/1000 | Loss: 0.00000962
Iteration 115/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [9.621524441172369e-06, 9.621524441172369e-06, 9.621524441172369e-06, 9.621524441172369e-06, 9.621524441172369e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.621524441172369e-06

Optimization complete. Final v2v error: 2.6496055126190186 mm

Highest mean error: 3.3289642333984375 mm for frame 22

Lowest mean error: 2.2383246421813965 mm for frame 76

Saving results

Total time: 39.691763401031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050622
Iteration 2/25 | Loss: 0.00215421
Iteration 3/25 | Loss: 0.00160309
Iteration 4/25 | Loss: 0.00143276
Iteration 5/25 | Loss: 0.00128461
Iteration 6/25 | Loss: 0.00118098
Iteration 7/25 | Loss: 0.00116287
Iteration 8/25 | Loss: 0.00126981
Iteration 9/25 | Loss: 0.00113533
Iteration 10/25 | Loss: 0.00109504
Iteration 11/25 | Loss: 0.00113793
Iteration 12/25 | Loss: 0.00108610
Iteration 13/25 | Loss: 0.00106933
Iteration 14/25 | Loss: 0.00107060
Iteration 15/25 | Loss: 0.00107358
Iteration 16/25 | Loss: 0.00105815
Iteration 17/25 | Loss: 0.00105380
Iteration 18/25 | Loss: 0.00105106
Iteration 19/25 | Loss: 0.00105501
Iteration 20/25 | Loss: 0.00105437
Iteration 21/25 | Loss: 0.00105862
Iteration 22/25 | Loss: 0.00105693
Iteration 23/25 | Loss: 0.00105113
Iteration 24/25 | Loss: 0.00104858
Iteration 25/25 | Loss: 0.00104535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40681243
Iteration 2/25 | Loss: 0.00219106
Iteration 3/25 | Loss: 0.00165637
Iteration 4/25 | Loss: 0.00165637
Iteration 5/25 | Loss: 0.00165637
Iteration 6/25 | Loss: 0.00165637
Iteration 7/25 | Loss: 0.00165637
Iteration 8/25 | Loss: 0.00165637
Iteration 9/25 | Loss: 0.00165637
Iteration 10/25 | Loss: 0.00165637
Iteration 11/25 | Loss: 0.00165637
Iteration 12/25 | Loss: 0.00165637
Iteration 13/25 | Loss: 0.00165637
Iteration 14/25 | Loss: 0.00165637
Iteration 15/25 | Loss: 0.00165637
Iteration 16/25 | Loss: 0.00165637
Iteration 17/25 | Loss: 0.00165637
Iteration 18/25 | Loss: 0.00165637
Iteration 19/25 | Loss: 0.00165637
Iteration 20/25 | Loss: 0.00165637
Iteration 21/25 | Loss: 0.00165637
Iteration 22/25 | Loss: 0.00165637
Iteration 23/25 | Loss: 0.00165637
Iteration 24/25 | Loss: 0.00165637
Iteration 25/25 | Loss: 0.00165637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016563675599172711, 0.0016563675599172711, 0.0016563675599172711, 0.0016563675599172711, 0.0016563675599172711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016563675599172711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165637
Iteration 2/1000 | Loss: 0.00019074
Iteration 3/1000 | Loss: 0.00080385
Iteration 4/1000 | Loss: 0.00012679
Iteration 5/1000 | Loss: 0.00010768
Iteration 6/1000 | Loss: 0.00039455
Iteration 7/1000 | Loss: 0.00030996
Iteration 8/1000 | Loss: 0.00036791
Iteration 9/1000 | Loss: 0.00019907
Iteration 10/1000 | Loss: 0.00028094
Iteration 11/1000 | Loss: 0.00044181
Iteration 12/1000 | Loss: 0.00060743
Iteration 13/1000 | Loss: 0.00042531
Iteration 14/1000 | Loss: 0.00042298
Iteration 15/1000 | Loss: 0.00038695
Iteration 16/1000 | Loss: 0.00035189
Iteration 17/1000 | Loss: 0.00047683
Iteration 18/1000 | Loss: 0.00052849
Iteration 19/1000 | Loss: 0.00054894
Iteration 20/1000 | Loss: 0.00047702
Iteration 21/1000 | Loss: 0.00022536
Iteration 22/1000 | Loss: 0.00010408
Iteration 23/1000 | Loss: 0.00024827
Iteration 24/1000 | Loss: 0.00083012
Iteration 25/1000 | Loss: 0.00148351
Iteration 26/1000 | Loss: 0.00427148
Iteration 27/1000 | Loss: 0.00134974
Iteration 28/1000 | Loss: 0.00068521
Iteration 29/1000 | Loss: 0.00045690
Iteration 30/1000 | Loss: 0.00035911
Iteration 31/1000 | Loss: 0.00017255
Iteration 32/1000 | Loss: 0.00101118
Iteration 33/1000 | Loss: 0.00112138
Iteration 34/1000 | Loss: 0.00051794
Iteration 35/1000 | Loss: 0.00057736
Iteration 36/1000 | Loss: 0.00040864
Iteration 37/1000 | Loss: 0.00050218
Iteration 38/1000 | Loss: 0.00027834
Iteration 39/1000 | Loss: 0.00009961
Iteration 40/1000 | Loss: 0.00043113
Iteration 41/1000 | Loss: 0.00064727
Iteration 42/1000 | Loss: 0.00043836
Iteration 43/1000 | Loss: 0.00017295
Iteration 44/1000 | Loss: 0.00025067
Iteration 45/1000 | Loss: 0.00004725
Iteration 46/1000 | Loss: 0.00003607
Iteration 47/1000 | Loss: 0.00003045
Iteration 48/1000 | Loss: 0.00006746
Iteration 49/1000 | Loss: 0.00007695
Iteration 50/1000 | Loss: 0.00008893
Iteration 51/1000 | Loss: 0.00006476
Iteration 52/1000 | Loss: 0.00003551
Iteration 53/1000 | Loss: 0.00003905
Iteration 54/1000 | Loss: 0.00002307
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00007233
Iteration 63/1000 | Loss: 0.00003900
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00003185
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00004185
Iteration 73/1000 | Loss: 0.00005107
Iteration 74/1000 | Loss: 0.00001785
Iteration 75/1000 | Loss: 0.00002218
Iteration 76/1000 | Loss: 0.00004323
Iteration 77/1000 | Loss: 0.00005623
Iteration 78/1000 | Loss: 0.00003563
Iteration 79/1000 | Loss: 0.00004463
Iteration 80/1000 | Loss: 0.00004200
Iteration 81/1000 | Loss: 0.00004590
Iteration 82/1000 | Loss: 0.00004256
Iteration 83/1000 | Loss: 0.00003777
Iteration 84/1000 | Loss: 0.00004144
Iteration 85/1000 | Loss: 0.00002812
Iteration 86/1000 | Loss: 0.00003920
Iteration 87/1000 | Loss: 0.00003044
Iteration 88/1000 | Loss: 0.00004408
Iteration 89/1000 | Loss: 0.00003024
Iteration 90/1000 | Loss: 0.00001578
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001653
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00004742
Iteration 96/1000 | Loss: 0.00002277
Iteration 97/1000 | Loss: 0.00001954
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00002199
Iteration 100/1000 | Loss: 0.00004114
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00003730
Iteration 104/1000 | Loss: 0.00002283
Iteration 105/1000 | Loss: 0.00001406
Iteration 106/1000 | Loss: 0.00003836
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00002883
Iteration 109/1000 | Loss: 0.00001682
Iteration 110/1000 | Loss: 0.00003443
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00002094
Iteration 113/1000 | Loss: 0.00004046
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00003630
Iteration 117/1000 | Loss: 0.00003215
Iteration 118/1000 | Loss: 0.00004216
Iteration 119/1000 | Loss: 0.00003446
Iteration 120/1000 | Loss: 0.00005183
Iteration 121/1000 | Loss: 0.00003196
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001281
Iteration 129/1000 | Loss: 0.00001281
Iteration 130/1000 | Loss: 0.00001280
Iteration 131/1000 | Loss: 0.00001280
Iteration 132/1000 | Loss: 0.00001280
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001279
Iteration 135/1000 | Loss: 0.00001278
Iteration 136/1000 | Loss: 0.00001278
Iteration 137/1000 | Loss: 0.00001278
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001277
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001276
Iteration 148/1000 | Loss: 0.00001276
Iteration 149/1000 | Loss: 0.00001276
Iteration 150/1000 | Loss: 0.00001276
Iteration 151/1000 | Loss: 0.00001276
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001276
Iteration 155/1000 | Loss: 0.00001276
Iteration 156/1000 | Loss: 0.00001276
Iteration 157/1000 | Loss: 0.00001276
Iteration 158/1000 | Loss: 0.00001276
Iteration 159/1000 | Loss: 0.00001276
Iteration 160/1000 | Loss: 0.00001276
Iteration 161/1000 | Loss: 0.00001275
Iteration 162/1000 | Loss: 0.00001275
Iteration 163/1000 | Loss: 0.00001275
Iteration 164/1000 | Loss: 0.00001275
Iteration 165/1000 | Loss: 0.00001275
Iteration 166/1000 | Loss: 0.00001275
Iteration 167/1000 | Loss: 0.00001275
Iteration 168/1000 | Loss: 0.00001275
Iteration 169/1000 | Loss: 0.00001275
Iteration 170/1000 | Loss: 0.00001275
Iteration 171/1000 | Loss: 0.00001275
Iteration 172/1000 | Loss: 0.00001275
Iteration 173/1000 | Loss: 0.00001275
Iteration 174/1000 | Loss: 0.00001275
Iteration 175/1000 | Loss: 0.00001274
Iteration 176/1000 | Loss: 0.00001274
Iteration 177/1000 | Loss: 0.00001274
Iteration 178/1000 | Loss: 0.00001274
Iteration 179/1000 | Loss: 0.00001273
Iteration 180/1000 | Loss: 0.00001273
Iteration 181/1000 | Loss: 0.00001273
Iteration 182/1000 | Loss: 0.00001273
Iteration 183/1000 | Loss: 0.00001273
Iteration 184/1000 | Loss: 0.00001273
Iteration 185/1000 | Loss: 0.00001273
Iteration 186/1000 | Loss: 0.00001273
Iteration 187/1000 | Loss: 0.00001272
Iteration 188/1000 | Loss: 0.00001272
Iteration 189/1000 | Loss: 0.00001272
Iteration 190/1000 | Loss: 0.00001272
Iteration 191/1000 | Loss: 0.00001272
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001270
Iteration 195/1000 | Loss: 0.00001270
Iteration 196/1000 | Loss: 0.00001270
Iteration 197/1000 | Loss: 0.00001270
Iteration 198/1000 | Loss: 0.00001270
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001269
Iteration 201/1000 | Loss: 0.00001269
Iteration 202/1000 | Loss: 0.00001268
Iteration 203/1000 | Loss: 0.00001268
Iteration 204/1000 | Loss: 0.00001268
Iteration 205/1000 | Loss: 0.00001268
Iteration 206/1000 | Loss: 0.00001268
Iteration 207/1000 | Loss: 0.00001267
Iteration 208/1000 | Loss: 0.00001267
Iteration 209/1000 | Loss: 0.00001267
Iteration 210/1000 | Loss: 0.00001267
Iteration 211/1000 | Loss: 0.00001267
Iteration 212/1000 | Loss: 0.00001266
Iteration 213/1000 | Loss: 0.00001266
Iteration 214/1000 | Loss: 0.00001265
Iteration 215/1000 | Loss: 0.00001265
Iteration 216/1000 | Loss: 0.00001265
Iteration 217/1000 | Loss: 0.00001265
Iteration 218/1000 | Loss: 0.00001264
Iteration 219/1000 | Loss: 0.00001264
Iteration 220/1000 | Loss: 0.00001264
Iteration 221/1000 | Loss: 0.00001264
Iteration 222/1000 | Loss: 0.00001264
Iteration 223/1000 | Loss: 0.00001263
Iteration 224/1000 | Loss: 0.00001263
Iteration 225/1000 | Loss: 0.00001263
Iteration 226/1000 | Loss: 0.00001263
Iteration 227/1000 | Loss: 0.00001263
Iteration 228/1000 | Loss: 0.00001263
Iteration 229/1000 | Loss: 0.00001263
Iteration 230/1000 | Loss: 0.00001263
Iteration 231/1000 | Loss: 0.00001263
Iteration 232/1000 | Loss: 0.00001263
Iteration 233/1000 | Loss: 0.00001263
Iteration 234/1000 | Loss: 0.00001262
Iteration 235/1000 | Loss: 0.00001262
Iteration 236/1000 | Loss: 0.00001262
Iteration 237/1000 | Loss: 0.00001262
Iteration 238/1000 | Loss: 0.00001262
Iteration 239/1000 | Loss: 0.00001262
Iteration 240/1000 | Loss: 0.00001262
Iteration 241/1000 | Loss: 0.00001262
Iteration 242/1000 | Loss: 0.00001262
Iteration 243/1000 | Loss: 0.00001262
Iteration 244/1000 | Loss: 0.00001261
Iteration 245/1000 | Loss: 0.00001261
Iteration 246/1000 | Loss: 0.00001261
Iteration 247/1000 | Loss: 0.00001261
Iteration 248/1000 | Loss: 0.00001261
Iteration 249/1000 | Loss: 0.00001261
Iteration 250/1000 | Loss: 0.00001261
Iteration 251/1000 | Loss: 0.00001261
Iteration 252/1000 | Loss: 0.00001261
Iteration 253/1000 | Loss: 0.00001261
Iteration 254/1000 | Loss: 0.00001261
Iteration 255/1000 | Loss: 0.00001261
Iteration 256/1000 | Loss: 0.00001261
Iteration 257/1000 | Loss: 0.00001260
Iteration 258/1000 | Loss: 0.00001260
Iteration 259/1000 | Loss: 0.00001260
Iteration 260/1000 | Loss: 0.00001260
Iteration 261/1000 | Loss: 0.00001260
Iteration 262/1000 | Loss: 0.00001260
Iteration 263/1000 | Loss: 0.00001260
Iteration 264/1000 | Loss: 0.00001260
Iteration 265/1000 | Loss: 0.00001259
Iteration 266/1000 | Loss: 0.00001259
Iteration 267/1000 | Loss: 0.00001259
Iteration 268/1000 | Loss: 0.00001259
Iteration 269/1000 | Loss: 0.00001259
Iteration 270/1000 | Loss: 0.00001259
Iteration 271/1000 | Loss: 0.00001259
Iteration 272/1000 | Loss: 0.00001259
Iteration 273/1000 | Loss: 0.00001259
Iteration 274/1000 | Loss: 0.00001259
Iteration 275/1000 | Loss: 0.00001259
Iteration 276/1000 | Loss: 0.00001259
Iteration 277/1000 | Loss: 0.00001258
Iteration 278/1000 | Loss: 0.00001258
Iteration 279/1000 | Loss: 0.00001258
Iteration 280/1000 | Loss: 0.00001258
Iteration 281/1000 | Loss: 0.00001258
Iteration 282/1000 | Loss: 0.00001258
Iteration 283/1000 | Loss: 0.00001258
Iteration 284/1000 | Loss: 0.00001258
Iteration 285/1000 | Loss: 0.00001258
Iteration 286/1000 | Loss: 0.00001258
Iteration 287/1000 | Loss: 0.00001258
Iteration 288/1000 | Loss: 0.00001258
Iteration 289/1000 | Loss: 0.00001258
Iteration 290/1000 | Loss: 0.00001258
Iteration 291/1000 | Loss: 0.00001258
Iteration 292/1000 | Loss: 0.00001258
Iteration 293/1000 | Loss: 0.00001258
Iteration 294/1000 | Loss: 0.00001257
Iteration 295/1000 | Loss: 0.00001257
Iteration 296/1000 | Loss: 0.00001257
Iteration 297/1000 | Loss: 0.00001257
Iteration 298/1000 | Loss: 0.00001257
Iteration 299/1000 | Loss: 0.00001257
Iteration 300/1000 | Loss: 0.00001257
Iteration 301/1000 | Loss: 0.00001257
Iteration 302/1000 | Loss: 0.00001257
Iteration 303/1000 | Loss: 0.00001256
Iteration 304/1000 | Loss: 0.00001256
Iteration 305/1000 | Loss: 0.00001256
Iteration 306/1000 | Loss: 0.00001256
Iteration 307/1000 | Loss: 0.00001256
Iteration 308/1000 | Loss: 0.00001256
Iteration 309/1000 | Loss: 0.00001256
Iteration 310/1000 | Loss: 0.00001256
Iteration 311/1000 | Loss: 0.00001256
Iteration 312/1000 | Loss: 0.00001256
Iteration 313/1000 | Loss: 0.00001256
Iteration 314/1000 | Loss: 0.00001256
Iteration 315/1000 | Loss: 0.00001256
Iteration 316/1000 | Loss: 0.00001256
Iteration 317/1000 | Loss: 0.00001256
Iteration 318/1000 | Loss: 0.00001255
Iteration 319/1000 | Loss: 0.00001255
Iteration 320/1000 | Loss: 0.00001255
Iteration 321/1000 | Loss: 0.00001255
Iteration 322/1000 | Loss: 0.00001255
Iteration 323/1000 | Loss: 0.00001255
Iteration 324/1000 | Loss: 0.00001255
Iteration 325/1000 | Loss: 0.00001255
Iteration 326/1000 | Loss: 0.00001255
Iteration 327/1000 | Loss: 0.00001255
Iteration 328/1000 | Loss: 0.00001255
Iteration 329/1000 | Loss: 0.00001255
Iteration 330/1000 | Loss: 0.00001255
Iteration 331/1000 | Loss: 0.00001255
Iteration 332/1000 | Loss: 0.00001255
Iteration 333/1000 | Loss: 0.00001255
Iteration 334/1000 | Loss: 0.00001255
Iteration 335/1000 | Loss: 0.00001255
Iteration 336/1000 | Loss: 0.00001255
Iteration 337/1000 | Loss: 0.00001255
Iteration 338/1000 | Loss: 0.00001255
Iteration 339/1000 | Loss: 0.00001255
Iteration 340/1000 | Loss: 0.00001255
Iteration 341/1000 | Loss: 0.00001255
Iteration 342/1000 | Loss: 0.00001255
Iteration 343/1000 | Loss: 0.00001255
Iteration 344/1000 | Loss: 0.00001255
Iteration 345/1000 | Loss: 0.00001255
Iteration 346/1000 | Loss: 0.00001255
Iteration 347/1000 | Loss: 0.00001255
Iteration 348/1000 | Loss: 0.00001255
Iteration 349/1000 | Loss: 0.00001255
Iteration 350/1000 | Loss: 0.00001255
Iteration 351/1000 | Loss: 0.00001255
Iteration 352/1000 | Loss: 0.00001255
Iteration 353/1000 | Loss: 0.00001255
Iteration 354/1000 | Loss: 0.00001255
Iteration 355/1000 | Loss: 0.00001255
Iteration 356/1000 | Loss: 0.00001255
Iteration 357/1000 | Loss: 0.00001255
Iteration 358/1000 | Loss: 0.00001255
Iteration 359/1000 | Loss: 0.00001255
Iteration 360/1000 | Loss: 0.00001255
Iteration 361/1000 | Loss: 0.00001255
Iteration 362/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [1.2550855899462476e-05, 1.2550855899462476e-05, 1.2550855899462476e-05, 1.2550855899462476e-05, 1.2550855899462476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2550855899462476e-05

Optimization complete. Final v2v error: 2.836768627166748 mm

Highest mean error: 9.407894134521484 mm for frame 62

Lowest mean error: 2.4163448810577393 mm for frame 55

Saving results

Total time: 235.20265984535217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024201
Iteration 2/25 | Loss: 0.00105629
Iteration 3/25 | Loss: 0.00089192
Iteration 4/25 | Loss: 0.00087870
Iteration 5/25 | Loss: 0.00087596
Iteration 6/25 | Loss: 0.00087596
Iteration 7/25 | Loss: 0.00087596
Iteration 8/25 | Loss: 0.00087596
Iteration 9/25 | Loss: 0.00087596
Iteration 10/25 | Loss: 0.00087596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008759628981351852, 0.0008759628981351852, 0.0008759628981351852, 0.0008759628981351852, 0.0008759628981351852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008759628981351852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36743069
Iteration 2/25 | Loss: 0.00038199
Iteration 3/25 | Loss: 0.00038199
Iteration 4/25 | Loss: 0.00038199
Iteration 5/25 | Loss: 0.00038199
Iteration 6/25 | Loss: 0.00038199
Iteration 7/25 | Loss: 0.00038199
Iteration 8/25 | Loss: 0.00038199
Iteration 9/25 | Loss: 0.00038199
Iteration 10/25 | Loss: 0.00038199
Iteration 11/25 | Loss: 0.00038199
Iteration 12/25 | Loss: 0.00038199
Iteration 13/25 | Loss: 0.00038199
Iteration 14/25 | Loss: 0.00038199
Iteration 15/25 | Loss: 0.00038199
Iteration 16/25 | Loss: 0.00038199
Iteration 17/25 | Loss: 0.00038199
Iteration 18/25 | Loss: 0.00038199
Iteration 19/25 | Loss: 0.00038199
Iteration 20/25 | Loss: 0.00038199
Iteration 21/25 | Loss: 0.00038199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003819871926680207, 0.0003819871926680207, 0.0003819871926680207, 0.0003819871926680207, 0.0003819871926680207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003819871926680207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038199
Iteration 2/1000 | Loss: 0.00001709
Iteration 3/1000 | Loss: 0.00001239
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001116
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001056
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001044
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001043
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001039
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001037
Iteration 21/1000 | Loss: 0.00001034
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001030
Iteration 25/1000 | Loss: 0.00001028
Iteration 26/1000 | Loss: 0.00001027
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001026
Iteration 29/1000 | Loss: 0.00001026
Iteration 30/1000 | Loss: 0.00001025
Iteration 31/1000 | Loss: 0.00001025
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001024
Iteration 35/1000 | Loss: 0.00001024
Iteration 36/1000 | Loss: 0.00001024
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001023
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001023
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001021
Iteration 51/1000 | Loss: 0.00001021
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001018
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001018
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001017
Iteration 67/1000 | Loss: 0.00001017
Iteration 68/1000 | Loss: 0.00001017
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001016
Iteration 72/1000 | Loss: 0.00001016
Iteration 73/1000 | Loss: 0.00001016
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001015
Iteration 77/1000 | Loss: 0.00001015
Iteration 78/1000 | Loss: 0.00001015
Iteration 79/1000 | Loss: 0.00001015
Iteration 80/1000 | Loss: 0.00001014
Iteration 81/1000 | Loss: 0.00001014
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001012
Iteration 90/1000 | Loss: 0.00001012
Iteration 91/1000 | Loss: 0.00001012
Iteration 92/1000 | Loss: 0.00001012
Iteration 93/1000 | Loss: 0.00001012
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001011
Iteration 96/1000 | Loss: 0.00001011
Iteration 97/1000 | Loss: 0.00001011
Iteration 98/1000 | Loss: 0.00001011
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Iteration 101/1000 | Loss: 0.00001010
Iteration 102/1000 | Loss: 0.00001010
Iteration 103/1000 | Loss: 0.00001010
Iteration 104/1000 | Loss: 0.00001010
Iteration 105/1000 | Loss: 0.00001010
Iteration 106/1000 | Loss: 0.00001010
Iteration 107/1000 | Loss: 0.00001010
Iteration 108/1000 | Loss: 0.00001010
Iteration 109/1000 | Loss: 0.00001010
Iteration 110/1000 | Loss: 0.00001010
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001009
Iteration 113/1000 | Loss: 0.00001009
Iteration 114/1000 | Loss: 0.00001009
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001008
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001008
Iteration 119/1000 | Loss: 0.00001008
Iteration 120/1000 | Loss: 0.00001008
Iteration 121/1000 | Loss: 0.00001008
Iteration 122/1000 | Loss: 0.00001008
Iteration 123/1000 | Loss: 0.00001008
Iteration 124/1000 | Loss: 0.00001008
Iteration 125/1000 | Loss: 0.00001008
Iteration 126/1000 | Loss: 0.00001008
Iteration 127/1000 | Loss: 0.00001008
Iteration 128/1000 | Loss: 0.00001007
Iteration 129/1000 | Loss: 0.00001007
Iteration 130/1000 | Loss: 0.00001007
Iteration 131/1000 | Loss: 0.00001007
Iteration 132/1000 | Loss: 0.00001007
Iteration 133/1000 | Loss: 0.00001007
Iteration 134/1000 | Loss: 0.00001007
Iteration 135/1000 | Loss: 0.00001007
Iteration 136/1000 | Loss: 0.00001007
Iteration 137/1000 | Loss: 0.00001007
Iteration 138/1000 | Loss: 0.00001007
Iteration 139/1000 | Loss: 0.00001007
Iteration 140/1000 | Loss: 0.00001007
Iteration 141/1000 | Loss: 0.00001007
Iteration 142/1000 | Loss: 0.00001007
Iteration 143/1000 | Loss: 0.00001007
Iteration 144/1000 | Loss: 0.00001007
Iteration 145/1000 | Loss: 0.00001007
Iteration 146/1000 | Loss: 0.00001007
Iteration 147/1000 | Loss: 0.00001007
Iteration 148/1000 | Loss: 0.00001007
Iteration 149/1000 | Loss: 0.00001007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.0068521078210324e-05, 1.0068521078210324e-05, 1.0068521078210324e-05, 1.0068521078210324e-05, 1.0068521078210324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0068521078210324e-05

Optimization complete. Final v2v error: 2.71447491645813 mm

Highest mean error: 2.9480834007263184 mm for frame 182

Lowest mean error: 2.5870261192321777 mm for frame 138

Saving results

Total time: 29.76816153526306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557720
Iteration 2/25 | Loss: 0.00111433
Iteration 3/25 | Loss: 0.00093216
Iteration 4/25 | Loss: 0.00091038
Iteration 5/25 | Loss: 0.00090390
Iteration 6/25 | Loss: 0.00090260
Iteration 7/25 | Loss: 0.00090260
Iteration 8/25 | Loss: 0.00090260
Iteration 9/25 | Loss: 0.00090260
Iteration 10/25 | Loss: 0.00090260
Iteration 11/25 | Loss: 0.00090260
Iteration 12/25 | Loss: 0.00090260
Iteration 13/25 | Loss: 0.00090260
Iteration 14/25 | Loss: 0.00090260
Iteration 15/25 | Loss: 0.00090260
Iteration 16/25 | Loss: 0.00090260
Iteration 17/25 | Loss: 0.00090260
Iteration 18/25 | Loss: 0.00090260
Iteration 19/25 | Loss: 0.00090260
Iteration 20/25 | Loss: 0.00090260
Iteration 21/25 | Loss: 0.00090260
Iteration 22/25 | Loss: 0.00090260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009026039042510092, 0.0009026039042510092, 0.0009026039042510092, 0.0009026039042510092, 0.0009026039042510092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009026039042510092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29852414
Iteration 2/25 | Loss: 0.00057263
Iteration 3/25 | Loss: 0.00057263
Iteration 4/25 | Loss: 0.00057263
Iteration 5/25 | Loss: 0.00057263
Iteration 6/25 | Loss: 0.00057262
Iteration 7/25 | Loss: 0.00057262
Iteration 8/25 | Loss: 0.00057262
Iteration 9/25 | Loss: 0.00057262
Iteration 10/25 | Loss: 0.00057262
Iteration 11/25 | Loss: 0.00057262
Iteration 12/25 | Loss: 0.00057262
Iteration 13/25 | Loss: 0.00057262
Iteration 14/25 | Loss: 0.00057262
Iteration 15/25 | Loss: 0.00057262
Iteration 16/25 | Loss: 0.00057262
Iteration 17/25 | Loss: 0.00057262
Iteration 18/25 | Loss: 0.00057262
Iteration 19/25 | Loss: 0.00057262
Iteration 20/25 | Loss: 0.00057262
Iteration 21/25 | Loss: 0.00057262
Iteration 22/25 | Loss: 0.00057262
Iteration 23/25 | Loss: 0.00057262
Iteration 24/25 | Loss: 0.00057262
Iteration 25/25 | Loss: 0.00057262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057262
Iteration 2/1000 | Loss: 0.00004068
Iteration 3/1000 | Loss: 0.00002043
Iteration 4/1000 | Loss: 0.00001711
Iteration 5/1000 | Loss: 0.00001614
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001454
Iteration 9/1000 | Loss: 0.00001430
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001353
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001348
Iteration 108/1000 | Loss: 0.00001348
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001346
Iteration 123/1000 | Loss: 0.00001346
Iteration 124/1000 | Loss: 0.00001346
Iteration 125/1000 | Loss: 0.00001346
Iteration 126/1000 | Loss: 0.00001346
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001345
Iteration 146/1000 | Loss: 0.00001345
Iteration 147/1000 | Loss: 0.00001345
Iteration 148/1000 | Loss: 0.00001345
Iteration 149/1000 | Loss: 0.00001345
Iteration 150/1000 | Loss: 0.00001345
Iteration 151/1000 | Loss: 0.00001345
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001345
Iteration 155/1000 | Loss: 0.00001345
Iteration 156/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.3446249795379117e-05, 1.3446249795379117e-05, 1.3446249795379117e-05, 1.3446249795379117e-05, 1.3446249795379117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3446249795379117e-05

Optimization complete. Final v2v error: 3.0086071491241455 mm

Highest mean error: 3.9641032218933105 mm for frame 143

Lowest mean error: 2.2852649688720703 mm for frame 98

Saving results

Total time: 41.2601044178009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527223
Iteration 2/25 | Loss: 0.00118689
Iteration 3/25 | Loss: 0.00097821
Iteration 4/25 | Loss: 0.00096101
Iteration 5/25 | Loss: 0.00095813
Iteration 6/25 | Loss: 0.00095734
Iteration 7/25 | Loss: 0.00095734
Iteration 8/25 | Loss: 0.00095734
Iteration 9/25 | Loss: 0.00095734
Iteration 10/25 | Loss: 0.00095734
Iteration 11/25 | Loss: 0.00095734
Iteration 12/25 | Loss: 0.00095734
Iteration 13/25 | Loss: 0.00095734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009573432616889477, 0.0009573432616889477, 0.0009573432616889477, 0.0009573432616889477, 0.0009573432616889477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009573432616889477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50374734
Iteration 2/25 | Loss: 0.00030100
Iteration 3/25 | Loss: 0.00030099
Iteration 4/25 | Loss: 0.00030099
Iteration 5/25 | Loss: 0.00030099
Iteration 6/25 | Loss: 0.00030099
Iteration 7/25 | Loss: 0.00030099
Iteration 8/25 | Loss: 0.00030099
Iteration 9/25 | Loss: 0.00030098
Iteration 10/25 | Loss: 0.00030098
Iteration 11/25 | Loss: 0.00030098
Iteration 12/25 | Loss: 0.00030098
Iteration 13/25 | Loss: 0.00030098
Iteration 14/25 | Loss: 0.00030098
Iteration 15/25 | Loss: 0.00030098
Iteration 16/25 | Loss: 0.00030098
Iteration 17/25 | Loss: 0.00030098
Iteration 18/25 | Loss: 0.00030098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003009845386259258, 0.0003009845386259258, 0.0003009845386259258, 0.0003009845386259258, 0.0003009845386259258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003009845386259258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030098
Iteration 2/1000 | Loss: 0.00004534
Iteration 3/1000 | Loss: 0.00003014
Iteration 4/1000 | Loss: 0.00002396
Iteration 5/1000 | Loss: 0.00002195
Iteration 6/1000 | Loss: 0.00002090
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001971
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001870
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001848
Iteration 17/1000 | Loss: 0.00001847
Iteration 18/1000 | Loss: 0.00001846
Iteration 19/1000 | Loss: 0.00001846
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001840
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001837
Iteration 28/1000 | Loss: 0.00001837
Iteration 29/1000 | Loss: 0.00001836
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001832
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001831
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001830
Iteration 62/1000 | Loss: 0.00001830
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001828
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001828
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001828
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00001827
Iteration 79/1000 | Loss: 0.00001827
Iteration 80/1000 | Loss: 0.00001827
Iteration 81/1000 | Loss: 0.00001827
Iteration 82/1000 | Loss: 0.00001826
Iteration 83/1000 | Loss: 0.00001826
Iteration 84/1000 | Loss: 0.00001826
Iteration 85/1000 | Loss: 0.00001826
Iteration 86/1000 | Loss: 0.00001826
Iteration 87/1000 | Loss: 0.00001826
Iteration 88/1000 | Loss: 0.00001826
Iteration 89/1000 | Loss: 0.00001826
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001824
Iteration 95/1000 | Loss: 0.00001824
Iteration 96/1000 | Loss: 0.00001824
Iteration 97/1000 | Loss: 0.00001824
Iteration 98/1000 | Loss: 0.00001824
Iteration 99/1000 | Loss: 0.00001824
Iteration 100/1000 | Loss: 0.00001824
Iteration 101/1000 | Loss: 0.00001824
Iteration 102/1000 | Loss: 0.00001824
Iteration 103/1000 | Loss: 0.00001824
Iteration 104/1000 | Loss: 0.00001824
Iteration 105/1000 | Loss: 0.00001824
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001824
Iteration 109/1000 | Loss: 0.00001824
Iteration 110/1000 | Loss: 0.00001823
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001823
Iteration 113/1000 | Loss: 0.00001823
Iteration 114/1000 | Loss: 0.00001823
Iteration 115/1000 | Loss: 0.00001823
Iteration 116/1000 | Loss: 0.00001823
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001822
Iteration 121/1000 | Loss: 0.00001822
Iteration 122/1000 | Loss: 0.00001822
Iteration 123/1000 | Loss: 0.00001822
Iteration 124/1000 | Loss: 0.00001822
Iteration 125/1000 | Loss: 0.00001821
Iteration 126/1000 | Loss: 0.00001821
Iteration 127/1000 | Loss: 0.00001821
Iteration 128/1000 | Loss: 0.00001821
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001821
Iteration 134/1000 | Loss: 0.00001821
Iteration 135/1000 | Loss: 0.00001821
Iteration 136/1000 | Loss: 0.00001821
Iteration 137/1000 | Loss: 0.00001821
Iteration 138/1000 | Loss: 0.00001821
Iteration 139/1000 | Loss: 0.00001821
Iteration 140/1000 | Loss: 0.00001821
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00001820
Iteration 146/1000 | Loss: 0.00001820
Iteration 147/1000 | Loss: 0.00001820
Iteration 148/1000 | Loss: 0.00001820
Iteration 149/1000 | Loss: 0.00001820
Iteration 150/1000 | Loss: 0.00001820
Iteration 151/1000 | Loss: 0.00001820
Iteration 152/1000 | Loss: 0.00001820
Iteration 153/1000 | Loss: 0.00001820
Iteration 154/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.8199298210674897e-05, 1.8199298210674897e-05, 1.8199298210674897e-05, 1.8199298210674897e-05, 1.8199298210674897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8199298210674897e-05

Optimization complete. Final v2v error: 3.5644760131835938 mm

Highest mean error: 4.176904201507568 mm for frame 78

Lowest mean error: 3.2279014587402344 mm for frame 13

Saving results

Total time: 37.334906339645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408882
Iteration 2/25 | Loss: 0.00089491
Iteration 3/25 | Loss: 0.00082836
Iteration 4/25 | Loss: 0.00082076
Iteration 5/25 | Loss: 0.00081845
Iteration 6/25 | Loss: 0.00081833
Iteration 7/25 | Loss: 0.00081833
Iteration 8/25 | Loss: 0.00081833
Iteration 9/25 | Loss: 0.00081806
Iteration 10/25 | Loss: 0.00081806
Iteration 11/25 | Loss: 0.00081806
Iteration 12/25 | Loss: 0.00081806
Iteration 13/25 | Loss: 0.00081806
Iteration 14/25 | Loss: 0.00081806
Iteration 15/25 | Loss: 0.00081806
Iteration 16/25 | Loss: 0.00081806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008180613513104618, 0.0008180613513104618, 0.0008180613513104618, 0.0008180613513104618, 0.0008180613513104618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008180613513104618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41323543
Iteration 2/25 | Loss: 0.00037530
Iteration 3/25 | Loss: 0.00037530
Iteration 4/25 | Loss: 0.00037530
Iteration 5/25 | Loss: 0.00037530
Iteration 6/25 | Loss: 0.00037530
Iteration 7/25 | Loss: 0.00037530
Iteration 8/25 | Loss: 0.00037530
Iteration 9/25 | Loss: 0.00037530
Iteration 10/25 | Loss: 0.00037530
Iteration 11/25 | Loss: 0.00037530
Iteration 12/25 | Loss: 0.00037530
Iteration 13/25 | Loss: 0.00037530
Iteration 14/25 | Loss: 0.00037530
Iteration 15/25 | Loss: 0.00037530
Iteration 16/25 | Loss: 0.00037530
Iteration 17/25 | Loss: 0.00037530
Iteration 18/25 | Loss: 0.00037530
Iteration 19/25 | Loss: 0.00037530
Iteration 20/25 | Loss: 0.00037530
Iteration 21/25 | Loss: 0.00037530
Iteration 22/25 | Loss: 0.00037530
Iteration 23/25 | Loss: 0.00037530
Iteration 24/25 | Loss: 0.00037530
Iteration 25/25 | Loss: 0.00037530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037530
Iteration 2/1000 | Loss: 0.00001882
Iteration 3/1000 | Loss: 0.00001265
Iteration 4/1000 | Loss: 0.00001174
Iteration 5/1000 | Loss: 0.00001114
Iteration 6/1000 | Loss: 0.00001075
Iteration 7/1000 | Loss: 0.00001044
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001030
Iteration 10/1000 | Loss: 0.00001027
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001022
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00001001
Iteration 21/1000 | Loss: 0.00001000
Iteration 22/1000 | Loss: 0.00000999
Iteration 23/1000 | Loss: 0.00000998
Iteration 24/1000 | Loss: 0.00000997
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000996
Iteration 27/1000 | Loss: 0.00000996
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000989
Iteration 30/1000 | Loss: 0.00000989
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000989
Iteration 34/1000 | Loss: 0.00000987
Iteration 35/1000 | Loss: 0.00000986
Iteration 36/1000 | Loss: 0.00000986
Iteration 37/1000 | Loss: 0.00000985
Iteration 38/1000 | Loss: 0.00000985
Iteration 39/1000 | Loss: 0.00000984
Iteration 40/1000 | Loss: 0.00000984
Iteration 41/1000 | Loss: 0.00000984
Iteration 42/1000 | Loss: 0.00000984
Iteration 43/1000 | Loss: 0.00000984
Iteration 44/1000 | Loss: 0.00000984
Iteration 45/1000 | Loss: 0.00000982
Iteration 46/1000 | Loss: 0.00000982
Iteration 47/1000 | Loss: 0.00000982
Iteration 48/1000 | Loss: 0.00000981
Iteration 49/1000 | Loss: 0.00000981
Iteration 50/1000 | Loss: 0.00000981
Iteration 51/1000 | Loss: 0.00000980
Iteration 52/1000 | Loss: 0.00000980
Iteration 53/1000 | Loss: 0.00000980
Iteration 54/1000 | Loss: 0.00000980
Iteration 55/1000 | Loss: 0.00000979
Iteration 56/1000 | Loss: 0.00000979
Iteration 57/1000 | Loss: 0.00000978
Iteration 58/1000 | Loss: 0.00000976
Iteration 59/1000 | Loss: 0.00000976
Iteration 60/1000 | Loss: 0.00000976
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000975
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000975
Iteration 67/1000 | Loss: 0.00000975
Iteration 68/1000 | Loss: 0.00000975
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000974
Iteration 72/1000 | Loss: 0.00000974
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000973
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000973
Iteration 77/1000 | Loss: 0.00000973
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000971
Iteration 82/1000 | Loss: 0.00000971
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000970
Iteration 85/1000 | Loss: 0.00000970
Iteration 86/1000 | Loss: 0.00000969
Iteration 87/1000 | Loss: 0.00000969
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000969
Iteration 92/1000 | Loss: 0.00000969
Iteration 93/1000 | Loss: 0.00000968
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000968
Iteration 96/1000 | Loss: 0.00000968
Iteration 97/1000 | Loss: 0.00000968
Iteration 98/1000 | Loss: 0.00000968
Iteration 99/1000 | Loss: 0.00000967
Iteration 100/1000 | Loss: 0.00000967
Iteration 101/1000 | Loss: 0.00000967
Iteration 102/1000 | Loss: 0.00000967
Iteration 103/1000 | Loss: 0.00000967
Iteration 104/1000 | Loss: 0.00000966
Iteration 105/1000 | Loss: 0.00000966
Iteration 106/1000 | Loss: 0.00000965
Iteration 107/1000 | Loss: 0.00000965
Iteration 108/1000 | Loss: 0.00000965
Iteration 109/1000 | Loss: 0.00000965
Iteration 110/1000 | Loss: 0.00000965
Iteration 111/1000 | Loss: 0.00000965
Iteration 112/1000 | Loss: 0.00000964
Iteration 113/1000 | Loss: 0.00000964
Iteration 114/1000 | Loss: 0.00000964
Iteration 115/1000 | Loss: 0.00000964
Iteration 116/1000 | Loss: 0.00000964
Iteration 117/1000 | Loss: 0.00000964
Iteration 118/1000 | Loss: 0.00000964
Iteration 119/1000 | Loss: 0.00000964
Iteration 120/1000 | Loss: 0.00000963
Iteration 121/1000 | Loss: 0.00000963
Iteration 122/1000 | Loss: 0.00000962
Iteration 123/1000 | Loss: 0.00000962
Iteration 124/1000 | Loss: 0.00000962
Iteration 125/1000 | Loss: 0.00000962
Iteration 126/1000 | Loss: 0.00000962
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Iteration 130/1000 | Loss: 0.00000961
Iteration 131/1000 | Loss: 0.00000961
Iteration 132/1000 | Loss: 0.00000961
Iteration 133/1000 | Loss: 0.00000961
Iteration 134/1000 | Loss: 0.00000961
Iteration 135/1000 | Loss: 0.00000961
Iteration 136/1000 | Loss: 0.00000961
Iteration 137/1000 | Loss: 0.00000961
Iteration 138/1000 | Loss: 0.00000960
Iteration 139/1000 | Loss: 0.00000959
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000959
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000959
Iteration 151/1000 | Loss: 0.00000958
Iteration 152/1000 | Loss: 0.00000958
Iteration 153/1000 | Loss: 0.00000958
Iteration 154/1000 | Loss: 0.00000958
Iteration 155/1000 | Loss: 0.00000958
Iteration 156/1000 | Loss: 0.00000958
Iteration 157/1000 | Loss: 0.00000958
Iteration 158/1000 | Loss: 0.00000958
Iteration 159/1000 | Loss: 0.00000957
Iteration 160/1000 | Loss: 0.00000957
Iteration 161/1000 | Loss: 0.00000957
Iteration 162/1000 | Loss: 0.00000957
Iteration 163/1000 | Loss: 0.00000957
Iteration 164/1000 | Loss: 0.00000957
Iteration 165/1000 | Loss: 0.00000957
Iteration 166/1000 | Loss: 0.00000957
Iteration 167/1000 | Loss: 0.00000957
Iteration 168/1000 | Loss: 0.00000957
Iteration 169/1000 | Loss: 0.00000957
Iteration 170/1000 | Loss: 0.00000957
Iteration 171/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [9.57086740527302e-06, 9.57086740527302e-06, 9.57086740527302e-06, 9.57086740527302e-06, 9.57086740527302e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.57086740527302e-06

Optimization complete. Final v2v error: 2.65494966506958 mm

Highest mean error: 2.8858025074005127 mm for frame 7

Lowest mean error: 2.486102342605591 mm for frame 58

Saving results

Total time: 35.328529357910156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052705
Iteration 2/25 | Loss: 0.00166184
Iteration 3/25 | Loss: 0.00127779
Iteration 4/25 | Loss: 0.00119541
Iteration 5/25 | Loss: 0.00112530
Iteration 6/25 | Loss: 0.00109441
Iteration 7/25 | Loss: 0.00105336
Iteration 8/25 | Loss: 0.00104079
Iteration 9/25 | Loss: 0.00101403
Iteration 10/25 | Loss: 0.00101330
Iteration 11/25 | Loss: 0.00100420
Iteration 12/25 | Loss: 0.00099980
Iteration 13/25 | Loss: 0.00099527
Iteration 14/25 | Loss: 0.00099240
Iteration 15/25 | Loss: 0.00099148
Iteration 16/25 | Loss: 0.00099115
Iteration 17/25 | Loss: 0.00099099
Iteration 18/25 | Loss: 0.00099074
Iteration 19/25 | Loss: 0.00099047
Iteration 20/25 | Loss: 0.00099040
Iteration 21/25 | Loss: 0.00099040
Iteration 22/25 | Loss: 0.00099040
Iteration 23/25 | Loss: 0.00099040
Iteration 24/25 | Loss: 0.00099040
Iteration 25/25 | Loss: 0.00099040

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40871298
Iteration 2/25 | Loss: 0.00096602
Iteration 3/25 | Loss: 0.00096602
Iteration 4/25 | Loss: 0.00096602
Iteration 5/25 | Loss: 0.00096602
Iteration 6/25 | Loss: 0.00096602
Iteration 7/25 | Loss: 0.00096602
Iteration 8/25 | Loss: 0.00096602
Iteration 9/25 | Loss: 0.00096602
Iteration 10/25 | Loss: 0.00096601
Iteration 11/25 | Loss: 0.00096601
Iteration 12/25 | Loss: 0.00096601
Iteration 13/25 | Loss: 0.00096601
Iteration 14/25 | Loss: 0.00096601
Iteration 15/25 | Loss: 0.00096601
Iteration 16/25 | Loss: 0.00096601
Iteration 17/25 | Loss: 0.00096601
Iteration 18/25 | Loss: 0.00096601
Iteration 19/25 | Loss: 0.00096601
Iteration 20/25 | Loss: 0.00096601
Iteration 21/25 | Loss: 0.00096601
Iteration 22/25 | Loss: 0.00096601
Iteration 23/25 | Loss: 0.00096601
Iteration 24/25 | Loss: 0.00096601
Iteration 25/25 | Loss: 0.00096601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096601
Iteration 2/1000 | Loss: 0.00014580
Iteration 3/1000 | Loss: 0.00206363
Iteration 4/1000 | Loss: 0.00910931
Iteration 5/1000 | Loss: 0.00624505
Iteration 6/1000 | Loss: 0.00784080
Iteration 7/1000 | Loss: 0.00956056
Iteration 8/1000 | Loss: 0.00876761
Iteration 9/1000 | Loss: 0.00839471
Iteration 10/1000 | Loss: 0.00707106
Iteration 11/1000 | Loss: 0.00764087
Iteration 12/1000 | Loss: 0.00483872
Iteration 13/1000 | Loss: 0.00657303
Iteration 14/1000 | Loss: 0.00938827
Iteration 15/1000 | Loss: 0.00871624
Iteration 16/1000 | Loss: 0.00084954
Iteration 17/1000 | Loss: 0.00218911
Iteration 18/1000 | Loss: 0.00036609
Iteration 19/1000 | Loss: 0.00019434
Iteration 20/1000 | Loss: 0.00063214
Iteration 21/1000 | Loss: 0.00101875
Iteration 22/1000 | Loss: 0.00039545
Iteration 23/1000 | Loss: 0.00009366
Iteration 24/1000 | Loss: 0.00104549
Iteration 25/1000 | Loss: 0.00122264
Iteration 26/1000 | Loss: 0.00076644
Iteration 27/1000 | Loss: 0.00108382
Iteration 28/1000 | Loss: 0.00433189
Iteration 29/1000 | Loss: 0.00155072
Iteration 30/1000 | Loss: 0.00239185
Iteration 31/1000 | Loss: 0.00076169
Iteration 32/1000 | Loss: 0.00042420
Iteration 33/1000 | Loss: 0.00033756
Iteration 34/1000 | Loss: 0.00055526
Iteration 35/1000 | Loss: 0.00005780
Iteration 36/1000 | Loss: 0.00004319
Iteration 37/1000 | Loss: 0.00003677
Iteration 38/1000 | Loss: 0.00003135
Iteration 39/1000 | Loss: 0.00048973
Iteration 40/1000 | Loss: 0.00002904
Iteration 41/1000 | Loss: 0.00002481
Iteration 42/1000 | Loss: 0.00039245
Iteration 43/1000 | Loss: 0.00039379
Iteration 44/1000 | Loss: 0.00002615
Iteration 45/1000 | Loss: 0.00002187
Iteration 46/1000 | Loss: 0.00031723
Iteration 47/1000 | Loss: 0.00026582
Iteration 48/1000 | Loss: 0.00035095
Iteration 49/1000 | Loss: 0.00002240
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001657
Iteration 52/1000 | Loss: 0.00001510
Iteration 53/1000 | Loss: 0.00001453
Iteration 54/1000 | Loss: 0.00001403
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001150
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001145
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001143
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001141
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001140
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001140
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001138
Iteration 118/1000 | Loss: 0.00001138
Iteration 119/1000 | Loss: 0.00001138
Iteration 120/1000 | Loss: 0.00001138
Iteration 121/1000 | Loss: 0.00001138
Iteration 122/1000 | Loss: 0.00001138
Iteration 123/1000 | Loss: 0.00001138
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001137
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001136
Iteration 131/1000 | Loss: 0.00001136
Iteration 132/1000 | Loss: 0.00001136
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001135
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001134
Iteration 140/1000 | Loss: 0.00001133
Iteration 141/1000 | Loss: 0.00001133
Iteration 142/1000 | Loss: 0.00001133
Iteration 143/1000 | Loss: 0.00001133
Iteration 144/1000 | Loss: 0.00001133
Iteration 145/1000 | Loss: 0.00001133
Iteration 146/1000 | Loss: 0.00001133
Iteration 147/1000 | Loss: 0.00001133
Iteration 148/1000 | Loss: 0.00001133
Iteration 149/1000 | Loss: 0.00001133
Iteration 150/1000 | Loss: 0.00001132
Iteration 151/1000 | Loss: 0.00001132
Iteration 152/1000 | Loss: 0.00001132
Iteration 153/1000 | Loss: 0.00001132
Iteration 154/1000 | Loss: 0.00001132
Iteration 155/1000 | Loss: 0.00001132
Iteration 156/1000 | Loss: 0.00001132
Iteration 157/1000 | Loss: 0.00001132
Iteration 158/1000 | Loss: 0.00001132
Iteration 159/1000 | Loss: 0.00001132
Iteration 160/1000 | Loss: 0.00001132
Iteration 161/1000 | Loss: 0.00001132
Iteration 162/1000 | Loss: 0.00001132
Iteration 163/1000 | Loss: 0.00001132
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00001132
Iteration 166/1000 | Loss: 0.00001131
Iteration 167/1000 | Loss: 0.00001131
Iteration 168/1000 | Loss: 0.00001131
Iteration 169/1000 | Loss: 0.00001131
Iteration 170/1000 | Loss: 0.00001131
Iteration 171/1000 | Loss: 0.00001131
Iteration 172/1000 | Loss: 0.00001131
Iteration 173/1000 | Loss: 0.00001131
Iteration 174/1000 | Loss: 0.00001131
Iteration 175/1000 | Loss: 0.00001131
Iteration 176/1000 | Loss: 0.00001131
Iteration 177/1000 | Loss: 0.00001131
Iteration 178/1000 | Loss: 0.00001131
Iteration 179/1000 | Loss: 0.00001131
Iteration 180/1000 | Loss: 0.00001130
Iteration 181/1000 | Loss: 0.00001130
Iteration 182/1000 | Loss: 0.00001130
Iteration 183/1000 | Loss: 0.00001130
Iteration 184/1000 | Loss: 0.00001130
Iteration 185/1000 | Loss: 0.00001130
Iteration 186/1000 | Loss: 0.00001130
Iteration 187/1000 | Loss: 0.00001130
Iteration 188/1000 | Loss: 0.00001130
Iteration 189/1000 | Loss: 0.00001130
Iteration 190/1000 | Loss: 0.00001130
Iteration 191/1000 | Loss: 0.00001130
Iteration 192/1000 | Loss: 0.00001130
Iteration 193/1000 | Loss: 0.00001129
Iteration 194/1000 | Loss: 0.00001129
Iteration 195/1000 | Loss: 0.00001129
Iteration 196/1000 | Loss: 0.00001129
Iteration 197/1000 | Loss: 0.00001129
Iteration 198/1000 | Loss: 0.00001129
Iteration 199/1000 | Loss: 0.00001129
Iteration 200/1000 | Loss: 0.00001129
Iteration 201/1000 | Loss: 0.00001129
Iteration 202/1000 | Loss: 0.00001129
Iteration 203/1000 | Loss: 0.00001129
Iteration 204/1000 | Loss: 0.00001129
Iteration 205/1000 | Loss: 0.00001129
Iteration 206/1000 | Loss: 0.00001129
Iteration 207/1000 | Loss: 0.00001129
Iteration 208/1000 | Loss: 0.00001128
Iteration 209/1000 | Loss: 0.00001128
Iteration 210/1000 | Loss: 0.00001128
Iteration 211/1000 | Loss: 0.00001128
Iteration 212/1000 | Loss: 0.00001128
Iteration 213/1000 | Loss: 0.00001128
Iteration 214/1000 | Loss: 0.00001128
Iteration 215/1000 | Loss: 0.00001128
Iteration 216/1000 | Loss: 0.00001128
Iteration 217/1000 | Loss: 0.00001128
Iteration 218/1000 | Loss: 0.00001128
Iteration 219/1000 | Loss: 0.00001128
Iteration 220/1000 | Loss: 0.00001128
Iteration 221/1000 | Loss: 0.00001128
Iteration 222/1000 | Loss: 0.00001128
Iteration 223/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.1281087608949747e-05, 1.1281087608949747e-05, 1.1281087608949747e-05, 1.1281087608949747e-05, 1.1281087608949747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1281087608949747e-05

Optimization complete. Final v2v error: 2.839576005935669 mm

Highest mean error: 3.811964273452759 mm for frame 151

Lowest mean error: 2.5280089378356934 mm for frame 37

Saving results

Total time: 145.32334065437317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663090
Iteration 2/25 | Loss: 0.00094587
Iteration 3/25 | Loss: 0.00083051
Iteration 4/25 | Loss: 0.00081243
Iteration 5/25 | Loss: 0.00080654
Iteration 6/25 | Loss: 0.00080489
Iteration 7/25 | Loss: 0.00080489
Iteration 8/25 | Loss: 0.00080489
Iteration 9/25 | Loss: 0.00080489
Iteration 10/25 | Loss: 0.00080489
Iteration 11/25 | Loss: 0.00080489
Iteration 12/25 | Loss: 0.00080489
Iteration 13/25 | Loss: 0.00080489
Iteration 14/25 | Loss: 0.00080489
Iteration 15/25 | Loss: 0.00080489
Iteration 16/25 | Loss: 0.00080489
Iteration 17/25 | Loss: 0.00080489
Iteration 18/25 | Loss: 0.00080489
Iteration 19/25 | Loss: 0.00080489
Iteration 20/25 | Loss: 0.00080489
Iteration 21/25 | Loss: 0.00080489
Iteration 22/25 | Loss: 0.00080489
Iteration 23/25 | Loss: 0.00080489
Iteration 24/25 | Loss: 0.00080489
Iteration 25/25 | Loss: 0.00080489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17766809
Iteration 2/25 | Loss: 0.00048198
Iteration 3/25 | Loss: 0.00048198
Iteration 4/25 | Loss: 0.00048198
Iteration 5/25 | Loss: 0.00048198
Iteration 6/25 | Loss: 0.00048198
Iteration 7/25 | Loss: 0.00048198
Iteration 8/25 | Loss: 0.00048198
Iteration 9/25 | Loss: 0.00048198
Iteration 10/25 | Loss: 0.00048198
Iteration 11/25 | Loss: 0.00048198
Iteration 12/25 | Loss: 0.00048198
Iteration 13/25 | Loss: 0.00048198
Iteration 14/25 | Loss: 0.00048198
Iteration 15/25 | Loss: 0.00048198
Iteration 16/25 | Loss: 0.00048198
Iteration 17/25 | Loss: 0.00048198
Iteration 18/25 | Loss: 0.00048198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00048197709838859737, 0.00048197709838859737, 0.00048197709838859737, 0.00048197709838859737, 0.00048197709838859737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048197709838859737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048198
Iteration 2/1000 | Loss: 0.00001439
Iteration 3/1000 | Loss: 0.00000915
Iteration 4/1000 | Loss: 0.00000852
Iteration 5/1000 | Loss: 0.00000819
Iteration 6/1000 | Loss: 0.00000806
Iteration 7/1000 | Loss: 0.00000802
Iteration 8/1000 | Loss: 0.00000779
Iteration 9/1000 | Loss: 0.00000774
Iteration 10/1000 | Loss: 0.00000771
Iteration 11/1000 | Loss: 0.00000771
Iteration 12/1000 | Loss: 0.00000771
Iteration 13/1000 | Loss: 0.00000770
Iteration 14/1000 | Loss: 0.00000770
Iteration 15/1000 | Loss: 0.00000770
Iteration 16/1000 | Loss: 0.00000769
Iteration 17/1000 | Loss: 0.00000769
Iteration 18/1000 | Loss: 0.00000768
Iteration 19/1000 | Loss: 0.00000768
Iteration 20/1000 | Loss: 0.00000767
Iteration 21/1000 | Loss: 0.00000766
Iteration 22/1000 | Loss: 0.00000766
Iteration 23/1000 | Loss: 0.00000765
Iteration 24/1000 | Loss: 0.00000765
Iteration 25/1000 | Loss: 0.00000764
Iteration 26/1000 | Loss: 0.00000764
Iteration 27/1000 | Loss: 0.00000762
Iteration 28/1000 | Loss: 0.00000762
Iteration 29/1000 | Loss: 0.00000759
Iteration 30/1000 | Loss: 0.00000757
Iteration 31/1000 | Loss: 0.00000757
Iteration 32/1000 | Loss: 0.00000754
Iteration 33/1000 | Loss: 0.00000753
Iteration 34/1000 | Loss: 0.00000753
Iteration 35/1000 | Loss: 0.00000752
Iteration 36/1000 | Loss: 0.00000751
Iteration 37/1000 | Loss: 0.00000751
Iteration 38/1000 | Loss: 0.00000751
Iteration 39/1000 | Loss: 0.00000750
Iteration 40/1000 | Loss: 0.00000750
Iteration 41/1000 | Loss: 0.00000749
Iteration 42/1000 | Loss: 0.00000749
Iteration 43/1000 | Loss: 0.00000749
Iteration 44/1000 | Loss: 0.00000749
Iteration 45/1000 | Loss: 0.00000748
Iteration 46/1000 | Loss: 0.00000748
Iteration 47/1000 | Loss: 0.00000747
Iteration 48/1000 | Loss: 0.00000746
Iteration 49/1000 | Loss: 0.00000745
Iteration 50/1000 | Loss: 0.00000744
Iteration 51/1000 | Loss: 0.00000744
Iteration 52/1000 | Loss: 0.00000743
Iteration 53/1000 | Loss: 0.00000743
Iteration 54/1000 | Loss: 0.00000743
Iteration 55/1000 | Loss: 0.00000742
Iteration 56/1000 | Loss: 0.00000742
Iteration 57/1000 | Loss: 0.00000742
Iteration 58/1000 | Loss: 0.00000742
Iteration 59/1000 | Loss: 0.00000742
Iteration 60/1000 | Loss: 0.00000742
Iteration 61/1000 | Loss: 0.00000742
Iteration 62/1000 | Loss: 0.00000742
Iteration 63/1000 | Loss: 0.00000742
Iteration 64/1000 | Loss: 0.00000741
Iteration 65/1000 | Loss: 0.00000741
Iteration 66/1000 | Loss: 0.00000741
Iteration 67/1000 | Loss: 0.00000741
Iteration 68/1000 | Loss: 0.00000741
Iteration 69/1000 | Loss: 0.00000740
Iteration 70/1000 | Loss: 0.00000740
Iteration 71/1000 | Loss: 0.00000740
Iteration 72/1000 | Loss: 0.00000739
Iteration 73/1000 | Loss: 0.00000739
Iteration 74/1000 | Loss: 0.00000738
Iteration 75/1000 | Loss: 0.00000738
Iteration 76/1000 | Loss: 0.00000738
Iteration 77/1000 | Loss: 0.00000738
Iteration 78/1000 | Loss: 0.00000738
Iteration 79/1000 | Loss: 0.00000738
Iteration 80/1000 | Loss: 0.00000738
Iteration 81/1000 | Loss: 0.00000738
Iteration 82/1000 | Loss: 0.00000738
Iteration 83/1000 | Loss: 0.00000738
Iteration 84/1000 | Loss: 0.00000738
Iteration 85/1000 | Loss: 0.00000738
Iteration 86/1000 | Loss: 0.00000738
Iteration 87/1000 | Loss: 0.00000738
Iteration 88/1000 | Loss: 0.00000737
Iteration 89/1000 | Loss: 0.00000737
Iteration 90/1000 | Loss: 0.00000736
Iteration 91/1000 | Loss: 0.00000736
Iteration 92/1000 | Loss: 0.00000736
Iteration 93/1000 | Loss: 0.00000736
Iteration 94/1000 | Loss: 0.00000736
Iteration 95/1000 | Loss: 0.00000736
Iteration 96/1000 | Loss: 0.00000736
Iteration 97/1000 | Loss: 0.00000736
Iteration 98/1000 | Loss: 0.00000736
Iteration 99/1000 | Loss: 0.00000736
Iteration 100/1000 | Loss: 0.00000735
Iteration 101/1000 | Loss: 0.00000735
Iteration 102/1000 | Loss: 0.00000735
Iteration 103/1000 | Loss: 0.00000735
Iteration 104/1000 | Loss: 0.00000735
Iteration 105/1000 | Loss: 0.00000735
Iteration 106/1000 | Loss: 0.00000734
Iteration 107/1000 | Loss: 0.00000734
Iteration 108/1000 | Loss: 0.00000734
Iteration 109/1000 | Loss: 0.00000734
Iteration 110/1000 | Loss: 0.00000734
Iteration 111/1000 | Loss: 0.00000734
Iteration 112/1000 | Loss: 0.00000734
Iteration 113/1000 | Loss: 0.00000733
Iteration 114/1000 | Loss: 0.00000733
Iteration 115/1000 | Loss: 0.00000733
Iteration 116/1000 | Loss: 0.00000733
Iteration 117/1000 | Loss: 0.00000733
Iteration 118/1000 | Loss: 0.00000733
Iteration 119/1000 | Loss: 0.00000733
Iteration 120/1000 | Loss: 0.00000733
Iteration 121/1000 | Loss: 0.00000733
Iteration 122/1000 | Loss: 0.00000733
Iteration 123/1000 | Loss: 0.00000732
Iteration 124/1000 | Loss: 0.00000732
Iteration 125/1000 | Loss: 0.00000732
Iteration 126/1000 | Loss: 0.00000732
Iteration 127/1000 | Loss: 0.00000732
Iteration 128/1000 | Loss: 0.00000731
Iteration 129/1000 | Loss: 0.00000731
Iteration 130/1000 | Loss: 0.00000731
Iteration 131/1000 | Loss: 0.00000731
Iteration 132/1000 | Loss: 0.00000731
Iteration 133/1000 | Loss: 0.00000730
Iteration 134/1000 | Loss: 0.00000730
Iteration 135/1000 | Loss: 0.00000730
Iteration 136/1000 | Loss: 0.00000730
Iteration 137/1000 | Loss: 0.00000730
Iteration 138/1000 | Loss: 0.00000729
Iteration 139/1000 | Loss: 0.00000729
Iteration 140/1000 | Loss: 0.00000729
Iteration 141/1000 | Loss: 0.00000729
Iteration 142/1000 | Loss: 0.00000729
Iteration 143/1000 | Loss: 0.00000729
Iteration 144/1000 | Loss: 0.00000729
Iteration 145/1000 | Loss: 0.00000729
Iteration 146/1000 | Loss: 0.00000729
Iteration 147/1000 | Loss: 0.00000729
Iteration 148/1000 | Loss: 0.00000729
Iteration 149/1000 | Loss: 0.00000729
Iteration 150/1000 | Loss: 0.00000729
Iteration 151/1000 | Loss: 0.00000729
Iteration 152/1000 | Loss: 0.00000728
Iteration 153/1000 | Loss: 0.00000728
Iteration 154/1000 | Loss: 0.00000728
Iteration 155/1000 | Loss: 0.00000728
Iteration 156/1000 | Loss: 0.00000728
Iteration 157/1000 | Loss: 0.00000728
Iteration 158/1000 | Loss: 0.00000727
Iteration 159/1000 | Loss: 0.00000727
Iteration 160/1000 | Loss: 0.00000727
Iteration 161/1000 | Loss: 0.00000727
Iteration 162/1000 | Loss: 0.00000727
Iteration 163/1000 | Loss: 0.00000727
Iteration 164/1000 | Loss: 0.00000727
Iteration 165/1000 | Loss: 0.00000726
Iteration 166/1000 | Loss: 0.00000726
Iteration 167/1000 | Loss: 0.00000726
Iteration 168/1000 | Loss: 0.00000726
Iteration 169/1000 | Loss: 0.00000726
Iteration 170/1000 | Loss: 0.00000726
Iteration 171/1000 | Loss: 0.00000726
Iteration 172/1000 | Loss: 0.00000726
Iteration 173/1000 | Loss: 0.00000726
Iteration 174/1000 | Loss: 0.00000726
Iteration 175/1000 | Loss: 0.00000726
Iteration 176/1000 | Loss: 0.00000725
Iteration 177/1000 | Loss: 0.00000725
Iteration 178/1000 | Loss: 0.00000725
Iteration 179/1000 | Loss: 0.00000725
Iteration 180/1000 | Loss: 0.00000725
Iteration 181/1000 | Loss: 0.00000725
Iteration 182/1000 | Loss: 0.00000725
Iteration 183/1000 | Loss: 0.00000725
Iteration 184/1000 | Loss: 0.00000725
Iteration 185/1000 | Loss: 0.00000725
Iteration 186/1000 | Loss: 0.00000725
Iteration 187/1000 | Loss: 0.00000725
Iteration 188/1000 | Loss: 0.00000725
Iteration 189/1000 | Loss: 0.00000725
Iteration 190/1000 | Loss: 0.00000725
Iteration 191/1000 | Loss: 0.00000724
Iteration 192/1000 | Loss: 0.00000724
Iteration 193/1000 | Loss: 0.00000724
Iteration 194/1000 | Loss: 0.00000724
Iteration 195/1000 | Loss: 0.00000724
Iteration 196/1000 | Loss: 0.00000724
Iteration 197/1000 | Loss: 0.00000724
Iteration 198/1000 | Loss: 0.00000724
Iteration 199/1000 | Loss: 0.00000723
Iteration 200/1000 | Loss: 0.00000723
Iteration 201/1000 | Loss: 0.00000723
Iteration 202/1000 | Loss: 0.00000723
Iteration 203/1000 | Loss: 0.00000723
Iteration 204/1000 | Loss: 0.00000723
Iteration 205/1000 | Loss: 0.00000723
Iteration 206/1000 | Loss: 0.00000723
Iteration 207/1000 | Loss: 0.00000723
Iteration 208/1000 | Loss: 0.00000723
Iteration 209/1000 | Loss: 0.00000723
Iteration 210/1000 | Loss: 0.00000723
Iteration 211/1000 | Loss: 0.00000723
Iteration 212/1000 | Loss: 0.00000723
Iteration 213/1000 | Loss: 0.00000723
Iteration 214/1000 | Loss: 0.00000723
Iteration 215/1000 | Loss: 0.00000723
Iteration 216/1000 | Loss: 0.00000723
Iteration 217/1000 | Loss: 0.00000723
Iteration 218/1000 | Loss: 0.00000723
Iteration 219/1000 | Loss: 0.00000722
Iteration 220/1000 | Loss: 0.00000722
Iteration 221/1000 | Loss: 0.00000722
Iteration 222/1000 | Loss: 0.00000722
Iteration 223/1000 | Loss: 0.00000722
Iteration 224/1000 | Loss: 0.00000722
Iteration 225/1000 | Loss: 0.00000722
Iteration 226/1000 | Loss: 0.00000722
Iteration 227/1000 | Loss: 0.00000722
Iteration 228/1000 | Loss: 0.00000721
Iteration 229/1000 | Loss: 0.00000721
Iteration 230/1000 | Loss: 0.00000721
Iteration 231/1000 | Loss: 0.00000721
Iteration 232/1000 | Loss: 0.00000721
Iteration 233/1000 | Loss: 0.00000721
Iteration 234/1000 | Loss: 0.00000721
Iteration 235/1000 | Loss: 0.00000721
Iteration 236/1000 | Loss: 0.00000721
Iteration 237/1000 | Loss: 0.00000721
Iteration 238/1000 | Loss: 0.00000721
Iteration 239/1000 | Loss: 0.00000721
Iteration 240/1000 | Loss: 0.00000721
Iteration 241/1000 | Loss: 0.00000721
Iteration 242/1000 | Loss: 0.00000721
Iteration 243/1000 | Loss: 0.00000721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [7.210865533124888e-06, 7.210865533124888e-06, 7.210865533124888e-06, 7.210865533124888e-06, 7.210865533124888e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.210865533124888e-06

Optimization complete. Final v2v error: 2.2938435077667236 mm

Highest mean error: 2.6813414096832275 mm for frame 78

Lowest mean error: 2.0868241786956787 mm for frame 2

Saving results

Total time: 36.09505867958069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851348
Iteration 2/25 | Loss: 0.00092132
Iteration 3/25 | Loss: 0.00083354
Iteration 4/25 | Loss: 0.00081826
Iteration 5/25 | Loss: 0.00081365
Iteration 6/25 | Loss: 0.00081290
Iteration 7/25 | Loss: 0.00081290
Iteration 8/25 | Loss: 0.00081290
Iteration 9/25 | Loss: 0.00081290
Iteration 10/25 | Loss: 0.00081290
Iteration 11/25 | Loss: 0.00081290
Iteration 12/25 | Loss: 0.00081290
Iteration 13/25 | Loss: 0.00081290
Iteration 14/25 | Loss: 0.00081290
Iteration 15/25 | Loss: 0.00081290
Iteration 16/25 | Loss: 0.00081290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008129023481160402, 0.0008129023481160402, 0.0008129023481160402, 0.0008129023481160402, 0.0008129023481160402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008129023481160402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40797687
Iteration 2/25 | Loss: 0.00042149
Iteration 3/25 | Loss: 0.00042148
Iteration 4/25 | Loss: 0.00042148
Iteration 5/25 | Loss: 0.00042148
Iteration 6/25 | Loss: 0.00042148
Iteration 7/25 | Loss: 0.00042148
Iteration 8/25 | Loss: 0.00042148
Iteration 9/25 | Loss: 0.00042148
Iteration 10/25 | Loss: 0.00042148
Iteration 11/25 | Loss: 0.00042148
Iteration 12/25 | Loss: 0.00042148
Iteration 13/25 | Loss: 0.00042148
Iteration 14/25 | Loss: 0.00042148
Iteration 15/25 | Loss: 0.00042148
Iteration 16/25 | Loss: 0.00042148
Iteration 17/25 | Loss: 0.00042148
Iteration 18/25 | Loss: 0.00042148
Iteration 19/25 | Loss: 0.00042148
Iteration 20/25 | Loss: 0.00042148
Iteration 21/25 | Loss: 0.00042148
Iteration 22/25 | Loss: 0.00042148
Iteration 23/25 | Loss: 0.00042148
Iteration 24/25 | Loss: 0.00042148
Iteration 25/25 | Loss: 0.00042148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042148
Iteration 2/1000 | Loss: 0.00001657
Iteration 3/1000 | Loss: 0.00001180
Iteration 4/1000 | Loss: 0.00001092
Iteration 5/1000 | Loss: 0.00001031
Iteration 6/1000 | Loss: 0.00001002
Iteration 7/1000 | Loss: 0.00000971
Iteration 8/1000 | Loss: 0.00000962
Iteration 9/1000 | Loss: 0.00000957
Iteration 10/1000 | Loss: 0.00000956
Iteration 11/1000 | Loss: 0.00000956
Iteration 12/1000 | Loss: 0.00000947
Iteration 13/1000 | Loss: 0.00000946
Iteration 14/1000 | Loss: 0.00000940
Iteration 15/1000 | Loss: 0.00000939
Iteration 16/1000 | Loss: 0.00000939
Iteration 17/1000 | Loss: 0.00000938
Iteration 18/1000 | Loss: 0.00000938
Iteration 19/1000 | Loss: 0.00000938
Iteration 20/1000 | Loss: 0.00000938
Iteration 21/1000 | Loss: 0.00000938
Iteration 22/1000 | Loss: 0.00000937
Iteration 23/1000 | Loss: 0.00000936
Iteration 24/1000 | Loss: 0.00000936
Iteration 25/1000 | Loss: 0.00000935
Iteration 26/1000 | Loss: 0.00000935
Iteration 27/1000 | Loss: 0.00000935
Iteration 28/1000 | Loss: 0.00000935
Iteration 29/1000 | Loss: 0.00000934
Iteration 30/1000 | Loss: 0.00000934
Iteration 31/1000 | Loss: 0.00000934
Iteration 32/1000 | Loss: 0.00000934
Iteration 33/1000 | Loss: 0.00000934
Iteration 34/1000 | Loss: 0.00000934
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000934
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000930
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000930
Iteration 46/1000 | Loss: 0.00000930
Iteration 47/1000 | Loss: 0.00000929
Iteration 48/1000 | Loss: 0.00000929
Iteration 49/1000 | Loss: 0.00000928
Iteration 50/1000 | Loss: 0.00000928
Iteration 51/1000 | Loss: 0.00000927
Iteration 52/1000 | Loss: 0.00000927
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000926
Iteration 55/1000 | Loss: 0.00000926
Iteration 56/1000 | Loss: 0.00000926
Iteration 57/1000 | Loss: 0.00000926
Iteration 58/1000 | Loss: 0.00000926
Iteration 59/1000 | Loss: 0.00000926
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000926
Iteration 62/1000 | Loss: 0.00000926
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000926
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000926
Iteration 68/1000 | Loss: 0.00000926
Iteration 69/1000 | Loss: 0.00000926
Iteration 70/1000 | Loss: 0.00000926
Iteration 71/1000 | Loss: 0.00000926
Iteration 72/1000 | Loss: 0.00000926
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000926
Iteration 75/1000 | Loss: 0.00000926
Iteration 76/1000 | Loss: 0.00000926
Iteration 77/1000 | Loss: 0.00000926
Iteration 78/1000 | Loss: 0.00000926
Iteration 79/1000 | Loss: 0.00000926
Iteration 80/1000 | Loss: 0.00000926
Iteration 81/1000 | Loss: 0.00000926
Iteration 82/1000 | Loss: 0.00000926
Iteration 83/1000 | Loss: 0.00000926
Iteration 84/1000 | Loss: 0.00000926
Iteration 85/1000 | Loss: 0.00000926
Iteration 86/1000 | Loss: 0.00000926
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000926
Iteration 91/1000 | Loss: 0.00000926
Iteration 92/1000 | Loss: 0.00000926
Iteration 93/1000 | Loss: 0.00000926
Iteration 94/1000 | Loss: 0.00000926
Iteration 95/1000 | Loss: 0.00000926
Iteration 96/1000 | Loss: 0.00000926
Iteration 97/1000 | Loss: 0.00000926
Iteration 98/1000 | Loss: 0.00000926
Iteration 99/1000 | Loss: 0.00000926
Iteration 100/1000 | Loss: 0.00000926
Iteration 101/1000 | Loss: 0.00000926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [9.25715812627459e-06, 9.25715812627459e-06, 9.25715812627459e-06, 9.25715812627459e-06, 9.25715812627459e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.25715812627459e-06

Optimization complete. Final v2v error: 2.591080665588379 mm

Highest mean error: 3.0808043479919434 mm for frame 96

Lowest mean error: 2.4416565895080566 mm for frame 143

Saving results

Total time: 27.761430025100708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884819
Iteration 2/25 | Loss: 0.00137308
Iteration 3/25 | Loss: 0.00108584
Iteration 4/25 | Loss: 0.00104450
Iteration 5/25 | Loss: 0.00102156
Iteration 6/25 | Loss: 0.00101325
Iteration 7/25 | Loss: 0.00100896
Iteration 8/25 | Loss: 0.00101244
Iteration 9/25 | Loss: 0.00101158
Iteration 10/25 | Loss: 0.00101195
Iteration 11/25 | Loss: 0.00101020
Iteration 12/25 | Loss: 0.00100949
Iteration 13/25 | Loss: 0.00100935
Iteration 14/25 | Loss: 0.00100926
Iteration 15/25 | Loss: 0.00100926
Iteration 16/25 | Loss: 0.00100926
Iteration 17/25 | Loss: 0.00100925
Iteration 18/25 | Loss: 0.00100925
Iteration 19/25 | Loss: 0.00100925
Iteration 20/25 | Loss: 0.00100925
Iteration 21/25 | Loss: 0.00100925
Iteration 22/25 | Loss: 0.00100925
Iteration 23/25 | Loss: 0.00100925
Iteration 24/25 | Loss: 0.00100925
Iteration 25/25 | Loss: 0.00100925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30036211
Iteration 2/25 | Loss: 0.00058172
Iteration 3/25 | Loss: 0.00058172
Iteration 4/25 | Loss: 0.00058172
Iteration 5/25 | Loss: 0.00058171
Iteration 6/25 | Loss: 0.00058171
Iteration 7/25 | Loss: 0.00058171
Iteration 8/25 | Loss: 0.00058171
Iteration 9/25 | Loss: 0.00058171
Iteration 10/25 | Loss: 0.00058171
Iteration 11/25 | Loss: 0.00058171
Iteration 12/25 | Loss: 0.00058171
Iteration 13/25 | Loss: 0.00058171
Iteration 14/25 | Loss: 0.00058171
Iteration 15/25 | Loss: 0.00058171
Iteration 16/25 | Loss: 0.00058171
Iteration 17/25 | Loss: 0.00058171
Iteration 18/25 | Loss: 0.00058171
Iteration 19/25 | Loss: 0.00058171
Iteration 20/25 | Loss: 0.00058171
Iteration 21/25 | Loss: 0.00058171
Iteration 22/25 | Loss: 0.00058171
Iteration 23/25 | Loss: 0.00058171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005817132187075913, 0.0005817132187075913, 0.0005817132187075913, 0.0005817132187075913, 0.0005817132187075913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005817132187075913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058171
Iteration 2/1000 | Loss: 0.00012451
Iteration 3/1000 | Loss: 0.00004939
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00013490
Iteration 6/1000 | Loss: 0.00002941
Iteration 7/1000 | Loss: 0.00002595
Iteration 8/1000 | Loss: 0.00002420
Iteration 9/1000 | Loss: 0.00002339
Iteration 10/1000 | Loss: 0.00002278
Iteration 11/1000 | Loss: 0.00002227
Iteration 12/1000 | Loss: 0.00012283
Iteration 13/1000 | Loss: 0.00003981
Iteration 14/1000 | Loss: 0.00002183
Iteration 15/1000 | Loss: 0.00014882
Iteration 16/1000 | Loss: 0.00003559
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00015702
Iteration 19/1000 | Loss: 0.00004346
Iteration 20/1000 | Loss: 0.00011881
Iteration 21/1000 | Loss: 0.00005622
Iteration 22/1000 | Loss: 0.00010649
Iteration 23/1000 | Loss: 0.00004952
Iteration 24/1000 | Loss: 0.00009475
Iteration 25/1000 | Loss: 0.00003976
Iteration 26/1000 | Loss: 0.00007105
Iteration 27/1000 | Loss: 0.00004296
Iteration 28/1000 | Loss: 0.00017998
Iteration 29/1000 | Loss: 0.00002515
Iteration 30/1000 | Loss: 0.00002207
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002111
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002021
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00001985
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001979
Iteration 50/1000 | Loss: 0.00001979
Iteration 51/1000 | Loss: 0.00001979
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001978
Iteration 54/1000 | Loss: 0.00001977
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001975
Iteration 59/1000 | Loss: 0.00001975
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001973
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001973
Iteration 67/1000 | Loss: 0.00001973
Iteration 68/1000 | Loss: 0.00001973
Iteration 69/1000 | Loss: 0.00001973
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001971
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001970
Iteration 77/1000 | Loss: 0.00001970
Iteration 78/1000 | Loss: 0.00001970
Iteration 79/1000 | Loss: 0.00001970
Iteration 80/1000 | Loss: 0.00001969
Iteration 81/1000 | Loss: 0.00001969
Iteration 82/1000 | Loss: 0.00001969
Iteration 83/1000 | Loss: 0.00001969
Iteration 84/1000 | Loss: 0.00001969
Iteration 85/1000 | Loss: 0.00001969
Iteration 86/1000 | Loss: 0.00001969
Iteration 87/1000 | Loss: 0.00001969
Iteration 88/1000 | Loss: 0.00001969
Iteration 89/1000 | Loss: 0.00001968
Iteration 90/1000 | Loss: 0.00001968
Iteration 91/1000 | Loss: 0.00001968
Iteration 92/1000 | Loss: 0.00001968
Iteration 93/1000 | Loss: 0.00001968
Iteration 94/1000 | Loss: 0.00001968
Iteration 95/1000 | Loss: 0.00001968
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001967
Iteration 101/1000 | Loss: 0.00001967
Iteration 102/1000 | Loss: 0.00001967
Iteration 103/1000 | Loss: 0.00001967
Iteration 104/1000 | Loss: 0.00001967
Iteration 105/1000 | Loss: 0.00001967
Iteration 106/1000 | Loss: 0.00001967
Iteration 107/1000 | Loss: 0.00001967
Iteration 108/1000 | Loss: 0.00001967
Iteration 109/1000 | Loss: 0.00001967
Iteration 110/1000 | Loss: 0.00001966
Iteration 111/1000 | Loss: 0.00001966
Iteration 112/1000 | Loss: 0.00001966
Iteration 113/1000 | Loss: 0.00001966
Iteration 114/1000 | Loss: 0.00001966
Iteration 115/1000 | Loss: 0.00001966
Iteration 116/1000 | Loss: 0.00001966
Iteration 117/1000 | Loss: 0.00001966
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001965
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00001965
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001964
Iteration 125/1000 | Loss: 0.00001964
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001963
Iteration 129/1000 | Loss: 0.00001963
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001961
Iteration 132/1000 | Loss: 0.00001961
Iteration 133/1000 | Loss: 0.00001960
Iteration 134/1000 | Loss: 0.00001960
Iteration 135/1000 | Loss: 0.00001960
Iteration 136/1000 | Loss: 0.00001960
Iteration 137/1000 | Loss: 0.00001959
Iteration 138/1000 | Loss: 0.00001959
Iteration 139/1000 | Loss: 0.00001959
Iteration 140/1000 | Loss: 0.00001959
Iteration 141/1000 | Loss: 0.00001959
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001957
Iteration 148/1000 | Loss: 0.00001957
Iteration 149/1000 | Loss: 0.00001957
Iteration 150/1000 | Loss: 0.00001957
Iteration 151/1000 | Loss: 0.00001957
Iteration 152/1000 | Loss: 0.00001957
Iteration 153/1000 | Loss: 0.00001957
Iteration 154/1000 | Loss: 0.00001957
Iteration 155/1000 | Loss: 0.00001957
Iteration 156/1000 | Loss: 0.00001957
Iteration 157/1000 | Loss: 0.00001956
Iteration 158/1000 | Loss: 0.00001956
Iteration 159/1000 | Loss: 0.00001956
Iteration 160/1000 | Loss: 0.00001956
Iteration 161/1000 | Loss: 0.00001955
Iteration 162/1000 | Loss: 0.00001955
Iteration 163/1000 | Loss: 0.00001955
Iteration 164/1000 | Loss: 0.00001955
Iteration 165/1000 | Loss: 0.00001954
Iteration 166/1000 | Loss: 0.00001954
Iteration 167/1000 | Loss: 0.00001954
Iteration 168/1000 | Loss: 0.00001954
Iteration 169/1000 | Loss: 0.00001954
Iteration 170/1000 | Loss: 0.00001953
Iteration 171/1000 | Loss: 0.00001953
Iteration 172/1000 | Loss: 0.00001953
Iteration 173/1000 | Loss: 0.00001953
Iteration 174/1000 | Loss: 0.00001953
Iteration 175/1000 | Loss: 0.00001953
Iteration 176/1000 | Loss: 0.00001953
Iteration 177/1000 | Loss: 0.00001953
Iteration 178/1000 | Loss: 0.00001953
Iteration 179/1000 | Loss: 0.00001953
Iteration 180/1000 | Loss: 0.00001953
Iteration 181/1000 | Loss: 0.00001953
Iteration 182/1000 | Loss: 0.00001953
Iteration 183/1000 | Loss: 0.00001953
Iteration 184/1000 | Loss: 0.00001952
Iteration 185/1000 | Loss: 0.00001952
Iteration 186/1000 | Loss: 0.00001952
Iteration 187/1000 | Loss: 0.00001952
Iteration 188/1000 | Loss: 0.00001952
Iteration 189/1000 | Loss: 0.00001952
Iteration 190/1000 | Loss: 0.00001952
Iteration 191/1000 | Loss: 0.00001952
Iteration 192/1000 | Loss: 0.00001952
Iteration 193/1000 | Loss: 0.00001952
Iteration 194/1000 | Loss: 0.00001952
Iteration 195/1000 | Loss: 0.00001952
Iteration 196/1000 | Loss: 0.00001952
Iteration 197/1000 | Loss: 0.00001952
Iteration 198/1000 | Loss: 0.00001952
Iteration 199/1000 | Loss: 0.00001952
Iteration 200/1000 | Loss: 0.00001952
Iteration 201/1000 | Loss: 0.00001952
Iteration 202/1000 | Loss: 0.00001951
Iteration 203/1000 | Loss: 0.00001951
Iteration 204/1000 | Loss: 0.00001951
Iteration 205/1000 | Loss: 0.00001951
Iteration 206/1000 | Loss: 0.00001951
Iteration 207/1000 | Loss: 0.00001951
Iteration 208/1000 | Loss: 0.00001951
Iteration 209/1000 | Loss: 0.00001951
Iteration 210/1000 | Loss: 0.00001951
Iteration 211/1000 | Loss: 0.00001951
Iteration 212/1000 | Loss: 0.00001951
Iteration 213/1000 | Loss: 0.00001951
Iteration 214/1000 | Loss: 0.00001951
Iteration 215/1000 | Loss: 0.00001951
Iteration 216/1000 | Loss: 0.00001951
Iteration 217/1000 | Loss: 0.00001951
Iteration 218/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.9512292055878788e-05, 1.9512292055878788e-05, 1.9512292055878788e-05, 1.9512292055878788e-05, 1.9512292055878788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9512292055878788e-05

Optimization complete. Final v2v error: 3.7118587493896484 mm

Highest mean error: 4.942154407501221 mm for frame 162

Lowest mean error: 3.223684072494507 mm for frame 30

Saving results

Total time: 103.88186311721802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429194
Iteration 2/25 | Loss: 0.00093772
Iteration 3/25 | Loss: 0.00085781
Iteration 4/25 | Loss: 0.00085196
Iteration 5/25 | Loss: 0.00085001
Iteration 6/25 | Loss: 0.00084986
Iteration 7/25 | Loss: 0.00084986
Iteration 8/25 | Loss: 0.00084986
Iteration 9/25 | Loss: 0.00084986
Iteration 10/25 | Loss: 0.00084986
Iteration 11/25 | Loss: 0.00084986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000849862874019891, 0.000849862874019891, 0.000849862874019891, 0.000849862874019891, 0.000849862874019891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000849862874019891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02487946
Iteration 2/25 | Loss: 0.00030698
Iteration 3/25 | Loss: 0.00030698
Iteration 4/25 | Loss: 0.00030698
Iteration 5/25 | Loss: 0.00030698
Iteration 6/25 | Loss: 0.00030698
Iteration 7/25 | Loss: 0.00030698
Iteration 8/25 | Loss: 0.00030698
Iteration 9/25 | Loss: 0.00030698
Iteration 10/25 | Loss: 0.00030698
Iteration 11/25 | Loss: 0.00030698
Iteration 12/25 | Loss: 0.00030698
Iteration 13/25 | Loss: 0.00030698
Iteration 14/25 | Loss: 0.00030698
Iteration 15/25 | Loss: 0.00030698
Iteration 16/25 | Loss: 0.00030698
Iteration 17/25 | Loss: 0.00030698
Iteration 18/25 | Loss: 0.00030698
Iteration 19/25 | Loss: 0.00030698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003069755039177835, 0.0003069755039177835, 0.0003069755039177835, 0.0003069755039177835, 0.0003069755039177835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003069755039177835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030698
Iteration 2/1000 | Loss: 0.00003015
Iteration 3/1000 | Loss: 0.00001810
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001196
Iteration 8/1000 | Loss: 0.00001172
Iteration 9/1000 | Loss: 0.00001136
Iteration 10/1000 | Loss: 0.00001134
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001114
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001105
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001096
Iteration 23/1000 | Loss: 0.00001090
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001080
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001080
Iteration 30/1000 | Loss: 0.00001079
Iteration 31/1000 | Loss: 0.00001079
Iteration 32/1000 | Loss: 0.00001079
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001077
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001076
Iteration 52/1000 | Loss: 0.00001076
Iteration 53/1000 | Loss: 0.00001076
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001076
Iteration 56/1000 | Loss: 0.00001075
Iteration 57/1000 | Loss: 0.00001075
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001074
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001074
Iteration 69/1000 | Loss: 0.00001074
Iteration 70/1000 | Loss: 0.00001074
Iteration 71/1000 | Loss: 0.00001074
Iteration 72/1000 | Loss: 0.00001074
Iteration 73/1000 | Loss: 0.00001074
Iteration 74/1000 | Loss: 0.00001074
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.0738751370809041e-05, 1.0738751370809041e-05, 1.0738751370809041e-05, 1.0738751370809041e-05, 1.0738751370809041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0738751370809041e-05

Optimization complete. Final v2v error: 2.7779452800750732 mm

Highest mean error: 2.810518741607666 mm for frame 56

Lowest mean error: 2.7295756340026855 mm for frame 0

Saving results

Total time: 27.208654165267944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827357
Iteration 2/25 | Loss: 0.00120606
Iteration 3/25 | Loss: 0.00091618
Iteration 4/25 | Loss: 0.00088233
Iteration 5/25 | Loss: 0.00087731
Iteration 6/25 | Loss: 0.00087633
Iteration 7/25 | Loss: 0.00087633
Iteration 8/25 | Loss: 0.00087631
Iteration 9/25 | Loss: 0.00087631
Iteration 10/25 | Loss: 0.00087631
Iteration 11/25 | Loss: 0.00087631
Iteration 12/25 | Loss: 0.00087631
Iteration 13/25 | Loss: 0.00087631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008763058576732874, 0.0008763058576732874, 0.0008763058576732874, 0.0008763058576732874, 0.0008763058576732874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008763058576732874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31742978
Iteration 2/25 | Loss: 0.00033708
Iteration 3/25 | Loss: 0.00033707
Iteration 4/25 | Loss: 0.00033707
Iteration 5/25 | Loss: 0.00033707
Iteration 6/25 | Loss: 0.00033707
Iteration 7/25 | Loss: 0.00033707
Iteration 8/25 | Loss: 0.00033707
Iteration 9/25 | Loss: 0.00033707
Iteration 10/25 | Loss: 0.00033707
Iteration 11/25 | Loss: 0.00033707
Iteration 12/25 | Loss: 0.00033707
Iteration 13/25 | Loss: 0.00033707
Iteration 14/25 | Loss: 0.00033707
Iteration 15/25 | Loss: 0.00033707
Iteration 16/25 | Loss: 0.00033707
Iteration 17/25 | Loss: 0.00033707
Iteration 18/25 | Loss: 0.00033707
Iteration 19/25 | Loss: 0.00033707
Iteration 20/25 | Loss: 0.00033707
Iteration 21/25 | Loss: 0.00033707
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00033707066904753447, 0.00033707066904753447, 0.00033707066904753447, 0.00033707066904753447, 0.00033707066904753447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033707066904753447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033707
Iteration 2/1000 | Loss: 0.00003397
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001285
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001138
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001138
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001136
Iteration 39/1000 | Loss: 0.00001136
Iteration 40/1000 | Loss: 0.00001135
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001134
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001130
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001127
Iteration 80/1000 | Loss: 0.00001127
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001126
Iteration 85/1000 | Loss: 0.00001126
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001126
Iteration 89/1000 | Loss: 0.00001126
Iteration 90/1000 | Loss: 0.00001125
Iteration 91/1000 | Loss: 0.00001125
Iteration 92/1000 | Loss: 0.00001125
Iteration 93/1000 | Loss: 0.00001125
Iteration 94/1000 | Loss: 0.00001125
Iteration 95/1000 | Loss: 0.00001125
Iteration 96/1000 | Loss: 0.00001125
Iteration 97/1000 | Loss: 0.00001124
Iteration 98/1000 | Loss: 0.00001124
Iteration 99/1000 | Loss: 0.00001124
Iteration 100/1000 | Loss: 0.00001124
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001123
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001122
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001120
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001120
Iteration 144/1000 | Loss: 0.00001120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1202947462152224e-05, 1.1202947462152224e-05, 1.1202947462152224e-05, 1.1202947462152224e-05, 1.1202947462152224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1202947462152224e-05

Optimization complete. Final v2v error: 2.847360134124756 mm

Highest mean error: 3.6185741424560547 mm for frame 89

Lowest mean error: 2.435983657836914 mm for frame 52

Saving results

Total time: 38.24763798713684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848266
Iteration 2/25 | Loss: 0.00144203
Iteration 3/25 | Loss: 0.00102493
Iteration 4/25 | Loss: 0.00099041
Iteration 5/25 | Loss: 0.00098835
Iteration 6/25 | Loss: 0.00098020
Iteration 7/25 | Loss: 0.00097950
Iteration 8/25 | Loss: 0.00097933
Iteration 9/25 | Loss: 0.00097930
Iteration 10/25 | Loss: 0.00097929
Iteration 11/25 | Loss: 0.00097929
Iteration 12/25 | Loss: 0.00097929
Iteration 13/25 | Loss: 0.00097929
Iteration 14/25 | Loss: 0.00097929
Iteration 15/25 | Loss: 0.00097929
Iteration 16/25 | Loss: 0.00097929
Iteration 17/25 | Loss: 0.00097929
Iteration 18/25 | Loss: 0.00097929
Iteration 19/25 | Loss: 0.00097929
Iteration 20/25 | Loss: 0.00097929
Iteration 21/25 | Loss: 0.00097928
Iteration 22/25 | Loss: 0.00097928
Iteration 23/25 | Loss: 0.00097928
Iteration 24/25 | Loss: 0.00097928
Iteration 25/25 | Loss: 0.00097928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22792363
Iteration 2/25 | Loss: 0.00057522
Iteration 3/25 | Loss: 0.00057522
Iteration 4/25 | Loss: 0.00057522
Iteration 5/25 | Loss: 0.00057522
Iteration 6/25 | Loss: 0.00057522
Iteration 7/25 | Loss: 0.00057522
Iteration 8/25 | Loss: 0.00057522
Iteration 9/25 | Loss: 0.00057521
Iteration 10/25 | Loss: 0.00057521
Iteration 11/25 | Loss: 0.00057521
Iteration 12/25 | Loss: 0.00057521
Iteration 13/25 | Loss: 0.00057521
Iteration 14/25 | Loss: 0.00057521
Iteration 15/25 | Loss: 0.00057521
Iteration 16/25 | Loss: 0.00057521
Iteration 17/25 | Loss: 0.00057521
Iteration 18/25 | Loss: 0.00057521
Iteration 19/25 | Loss: 0.00057521
Iteration 20/25 | Loss: 0.00057521
Iteration 21/25 | Loss: 0.00057521
Iteration 22/25 | Loss: 0.00057521
Iteration 23/25 | Loss: 0.00057521
Iteration 24/25 | Loss: 0.00057521
Iteration 25/25 | Loss: 0.00057521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057521
Iteration 2/1000 | Loss: 0.00002879
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001594
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001462
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001421
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001363
Iteration 28/1000 | Loss: 0.00001363
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001363
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001354
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001354
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001351
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001351
Iteration 111/1000 | Loss: 0.00001351
Iteration 112/1000 | Loss: 0.00001351
Iteration 113/1000 | Loss: 0.00001351
Iteration 114/1000 | Loss: 0.00001351
Iteration 115/1000 | Loss: 0.00001351
Iteration 116/1000 | Loss: 0.00001351
Iteration 117/1000 | Loss: 0.00001351
Iteration 118/1000 | Loss: 0.00001351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.3510817552742083e-05, 1.3510817552742083e-05, 1.3510817552742083e-05, 1.3510817552742083e-05, 1.3510817552742083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3510817552742083e-05

Optimization complete. Final v2v error: 3.128786087036133 mm

Highest mean error: 3.8235549926757812 mm for frame 107

Lowest mean error: 2.7768564224243164 mm for frame 144

Saving results

Total time: 40.5347785949707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789557
Iteration 2/25 | Loss: 0.00139603
Iteration 3/25 | Loss: 0.00102540
Iteration 4/25 | Loss: 0.00098688
Iteration 5/25 | Loss: 0.00098284
Iteration 6/25 | Loss: 0.00098271
Iteration 7/25 | Loss: 0.00098271
Iteration 8/25 | Loss: 0.00098271
Iteration 9/25 | Loss: 0.00098271
Iteration 10/25 | Loss: 0.00098271
Iteration 11/25 | Loss: 0.00098271
Iteration 12/25 | Loss: 0.00098271
Iteration 13/25 | Loss: 0.00098271
Iteration 14/25 | Loss: 0.00098271
Iteration 15/25 | Loss: 0.00098271
Iteration 16/25 | Loss: 0.00098271
Iteration 17/25 | Loss: 0.00098271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009827057365328074, 0.0009827057365328074, 0.0009827057365328074, 0.0009827057365328074, 0.0009827057365328074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009827057365328074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46996152
Iteration 2/25 | Loss: 0.00040335
Iteration 3/25 | Loss: 0.00040333
Iteration 4/25 | Loss: 0.00040333
Iteration 5/25 | Loss: 0.00040333
Iteration 6/25 | Loss: 0.00040333
Iteration 7/25 | Loss: 0.00040333
Iteration 8/25 | Loss: 0.00040333
Iteration 9/25 | Loss: 0.00040333
Iteration 10/25 | Loss: 0.00040333
Iteration 11/25 | Loss: 0.00040333
Iteration 12/25 | Loss: 0.00040333
Iteration 13/25 | Loss: 0.00040333
Iteration 14/25 | Loss: 0.00040333
Iteration 15/25 | Loss: 0.00040333
Iteration 16/25 | Loss: 0.00040333
Iteration 17/25 | Loss: 0.00040333
Iteration 18/25 | Loss: 0.00040333
Iteration 19/25 | Loss: 0.00040333
Iteration 20/25 | Loss: 0.00040333
Iteration 21/25 | Loss: 0.00040333
Iteration 22/25 | Loss: 0.00040333
Iteration 23/25 | Loss: 0.00040333
Iteration 24/25 | Loss: 0.00040333
Iteration 25/25 | Loss: 0.00040333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040333
Iteration 2/1000 | Loss: 0.00002687
Iteration 3/1000 | Loss: 0.00001821
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001598
Iteration 7/1000 | Loss: 0.00001571
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001511
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001499
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001492
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001492
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001487
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001481
Iteration 49/1000 | Loss: 0.00001481
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001472
Iteration 98/1000 | Loss: 0.00001472
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4716858458996285e-05, 1.4716858458996285e-05, 1.4716858458996285e-05, 1.4716858458996285e-05, 1.4716858458996285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4716858458996285e-05

Optimization complete. Final v2v error: 3.2401983737945557 mm

Highest mean error: 3.387829303741455 mm for frame 2

Lowest mean error: 3.0410048961639404 mm for frame 212

Saving results

Total time: 36.00826072692871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841445
Iteration 2/25 | Loss: 0.00143236
Iteration 3/25 | Loss: 0.00108676
Iteration 4/25 | Loss: 0.00102650
Iteration 5/25 | Loss: 0.00101697
Iteration 6/25 | Loss: 0.00102149
Iteration 7/25 | Loss: 0.00103154
Iteration 8/25 | Loss: 0.00099884
Iteration 9/25 | Loss: 0.00097954
Iteration 10/25 | Loss: 0.00097626
Iteration 11/25 | Loss: 0.00097207
Iteration 12/25 | Loss: 0.00096935
Iteration 13/25 | Loss: 0.00096844
Iteration 14/25 | Loss: 0.00096814
Iteration 15/25 | Loss: 0.00096812
Iteration 16/25 | Loss: 0.00096812
Iteration 17/25 | Loss: 0.00096812
Iteration 18/25 | Loss: 0.00096811
Iteration 19/25 | Loss: 0.00096811
Iteration 20/25 | Loss: 0.00096811
Iteration 21/25 | Loss: 0.00096811
Iteration 22/25 | Loss: 0.00096811
Iteration 23/25 | Loss: 0.00096811
Iteration 24/25 | Loss: 0.00096811
Iteration 25/25 | Loss: 0.00096811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29882312
Iteration 2/25 | Loss: 0.00036136
Iteration 3/25 | Loss: 0.00036136
Iteration 4/25 | Loss: 0.00036136
Iteration 5/25 | Loss: 0.00036136
Iteration 6/25 | Loss: 0.00036136
Iteration 7/25 | Loss: 0.00036136
Iteration 8/25 | Loss: 0.00036136
Iteration 9/25 | Loss: 0.00036136
Iteration 10/25 | Loss: 0.00036136
Iteration 11/25 | Loss: 0.00036136
Iteration 12/25 | Loss: 0.00036136
Iteration 13/25 | Loss: 0.00036136
Iteration 14/25 | Loss: 0.00036136
Iteration 15/25 | Loss: 0.00036136
Iteration 16/25 | Loss: 0.00036136
Iteration 17/25 | Loss: 0.00036136
Iteration 18/25 | Loss: 0.00036136
Iteration 19/25 | Loss: 0.00036136
Iteration 20/25 | Loss: 0.00036136
Iteration 21/25 | Loss: 0.00036136
Iteration 22/25 | Loss: 0.00036136
Iteration 23/25 | Loss: 0.00036136
Iteration 24/25 | Loss: 0.00036136
Iteration 25/25 | Loss: 0.00036136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036136
Iteration 2/1000 | Loss: 0.00003941
Iteration 3/1000 | Loss: 0.00002434
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00001970
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001826
Iteration 8/1000 | Loss: 0.00001799
Iteration 9/1000 | Loss: 0.00001777
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001747
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001742
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001740
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00001727
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001722
Iteration 72/1000 | Loss: 0.00001722
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001721
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001716
Iteration 82/1000 | Loss: 0.00001716
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001702
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001702
Iteration 143/1000 | Loss: 0.00001702
Iteration 144/1000 | Loss: 0.00001702
Iteration 145/1000 | Loss: 0.00001702
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001701
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001701
Iteration 151/1000 | Loss: 0.00001701
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001700
Iteration 159/1000 | Loss: 0.00001700
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Iteration 168/1000 | Loss: 0.00001700
Iteration 169/1000 | Loss: 0.00001699
Iteration 170/1000 | Loss: 0.00001699
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001699
Iteration 176/1000 | Loss: 0.00001698
Iteration 177/1000 | Loss: 0.00001698
Iteration 178/1000 | Loss: 0.00001698
Iteration 179/1000 | Loss: 0.00001698
Iteration 180/1000 | Loss: 0.00001698
Iteration 181/1000 | Loss: 0.00001698
Iteration 182/1000 | Loss: 0.00001698
Iteration 183/1000 | Loss: 0.00001698
Iteration 184/1000 | Loss: 0.00001697
Iteration 185/1000 | Loss: 0.00001697
Iteration 186/1000 | Loss: 0.00001697
Iteration 187/1000 | Loss: 0.00001697
Iteration 188/1000 | Loss: 0.00001697
Iteration 189/1000 | Loss: 0.00001696
Iteration 190/1000 | Loss: 0.00001696
Iteration 191/1000 | Loss: 0.00001696
Iteration 192/1000 | Loss: 0.00001695
Iteration 193/1000 | Loss: 0.00001695
Iteration 194/1000 | Loss: 0.00001694
Iteration 195/1000 | Loss: 0.00001694
Iteration 196/1000 | Loss: 0.00001694
Iteration 197/1000 | Loss: 0.00001694
Iteration 198/1000 | Loss: 0.00001694
Iteration 199/1000 | Loss: 0.00001694
Iteration 200/1000 | Loss: 0.00001693
Iteration 201/1000 | Loss: 0.00001693
Iteration 202/1000 | Loss: 0.00001692
Iteration 203/1000 | Loss: 0.00001692
Iteration 204/1000 | Loss: 0.00001692
Iteration 205/1000 | Loss: 0.00001692
Iteration 206/1000 | Loss: 0.00001691
Iteration 207/1000 | Loss: 0.00001691
Iteration 208/1000 | Loss: 0.00001691
Iteration 209/1000 | Loss: 0.00001690
Iteration 210/1000 | Loss: 0.00001690
Iteration 211/1000 | Loss: 0.00001690
Iteration 212/1000 | Loss: 0.00001690
Iteration 213/1000 | Loss: 0.00001690
Iteration 214/1000 | Loss: 0.00001690
Iteration 215/1000 | Loss: 0.00001690
Iteration 216/1000 | Loss: 0.00001690
Iteration 217/1000 | Loss: 0.00001690
Iteration 218/1000 | Loss: 0.00001690
Iteration 219/1000 | Loss: 0.00001690
Iteration 220/1000 | Loss: 0.00001689
Iteration 221/1000 | Loss: 0.00001689
Iteration 222/1000 | Loss: 0.00001689
Iteration 223/1000 | Loss: 0.00001689
Iteration 224/1000 | Loss: 0.00001688
Iteration 225/1000 | Loss: 0.00001688
Iteration 226/1000 | Loss: 0.00001688
Iteration 227/1000 | Loss: 0.00001688
Iteration 228/1000 | Loss: 0.00001688
Iteration 229/1000 | Loss: 0.00001688
Iteration 230/1000 | Loss: 0.00001688
Iteration 231/1000 | Loss: 0.00001688
Iteration 232/1000 | Loss: 0.00001688
Iteration 233/1000 | Loss: 0.00001688
Iteration 234/1000 | Loss: 0.00001688
Iteration 235/1000 | Loss: 0.00001688
Iteration 236/1000 | Loss: 0.00001688
Iteration 237/1000 | Loss: 0.00001688
Iteration 238/1000 | Loss: 0.00001687
Iteration 239/1000 | Loss: 0.00001687
Iteration 240/1000 | Loss: 0.00001687
Iteration 241/1000 | Loss: 0.00001687
Iteration 242/1000 | Loss: 0.00001687
Iteration 243/1000 | Loss: 0.00001687
Iteration 244/1000 | Loss: 0.00001687
Iteration 245/1000 | Loss: 0.00001687
Iteration 246/1000 | Loss: 0.00001687
Iteration 247/1000 | Loss: 0.00001687
Iteration 248/1000 | Loss: 0.00001686
Iteration 249/1000 | Loss: 0.00001686
Iteration 250/1000 | Loss: 0.00001686
Iteration 251/1000 | Loss: 0.00001686
Iteration 252/1000 | Loss: 0.00001686
Iteration 253/1000 | Loss: 0.00001686
Iteration 254/1000 | Loss: 0.00001686
Iteration 255/1000 | Loss: 0.00001686
Iteration 256/1000 | Loss: 0.00001686
Iteration 257/1000 | Loss: 0.00001686
Iteration 258/1000 | Loss: 0.00001686
Iteration 259/1000 | Loss: 0.00001686
Iteration 260/1000 | Loss: 0.00001686
Iteration 261/1000 | Loss: 0.00001686
Iteration 262/1000 | Loss: 0.00001686
Iteration 263/1000 | Loss: 0.00001686
Iteration 264/1000 | Loss: 0.00001686
Iteration 265/1000 | Loss: 0.00001686
Iteration 266/1000 | Loss: 0.00001686
Iteration 267/1000 | Loss: 0.00001686
Iteration 268/1000 | Loss: 0.00001686
Iteration 269/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.6858330127433874e-05, 1.6858330127433874e-05, 1.6858330127433874e-05, 1.6858330127433874e-05, 1.6858330127433874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6858330127433874e-05

Optimization complete. Final v2v error: 3.385728359222412 mm

Highest mean error: 3.993925094604492 mm for frame 64

Lowest mean error: 2.966862201690674 mm for frame 36

Saving results

Total time: 69.84062314033508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525969
Iteration 2/25 | Loss: 0.00112609
Iteration 3/25 | Loss: 0.00093591
Iteration 4/25 | Loss: 0.00087972
Iteration 5/25 | Loss: 0.00086604
Iteration 6/25 | Loss: 0.00086302
Iteration 7/25 | Loss: 0.00087720
Iteration 8/25 | Loss: 0.00084294
Iteration 9/25 | Loss: 0.00083930
Iteration 10/25 | Loss: 0.00083258
Iteration 11/25 | Loss: 0.00083556
Iteration 12/25 | Loss: 0.00083481
Iteration 13/25 | Loss: 0.00083358
Iteration 14/25 | Loss: 0.00082529
Iteration 15/25 | Loss: 0.00082280
Iteration 16/25 | Loss: 0.00082174
Iteration 17/25 | Loss: 0.00082133
Iteration 18/25 | Loss: 0.00082108
Iteration 19/25 | Loss: 0.00082093
Iteration 20/25 | Loss: 0.00082092
Iteration 21/25 | Loss: 0.00082092
Iteration 22/25 | Loss: 0.00082092
Iteration 23/25 | Loss: 0.00082092
Iteration 24/25 | Loss: 0.00082092
Iteration 25/25 | Loss: 0.00082092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44072819
Iteration 2/25 | Loss: 0.00051638
Iteration 3/25 | Loss: 0.00051637
Iteration 4/25 | Loss: 0.00051637
Iteration 5/25 | Loss: 0.00051637
Iteration 6/25 | Loss: 0.00051637
Iteration 7/25 | Loss: 0.00051637
Iteration 8/25 | Loss: 0.00051637
Iteration 9/25 | Loss: 0.00051637
Iteration 10/25 | Loss: 0.00051637
Iteration 11/25 | Loss: 0.00051637
Iteration 12/25 | Loss: 0.00051637
Iteration 13/25 | Loss: 0.00051637
Iteration 14/25 | Loss: 0.00051637
Iteration 15/25 | Loss: 0.00051637
Iteration 16/25 | Loss: 0.00051637
Iteration 17/25 | Loss: 0.00051637
Iteration 18/25 | Loss: 0.00051637
Iteration 19/25 | Loss: 0.00051637
Iteration 20/25 | Loss: 0.00051637
Iteration 21/25 | Loss: 0.00051637
Iteration 22/25 | Loss: 0.00051637
Iteration 23/25 | Loss: 0.00051637
Iteration 24/25 | Loss: 0.00051637
Iteration 25/25 | Loss: 0.00051637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051637
Iteration 2/1000 | Loss: 0.00001788
Iteration 3/1000 | Loss: 0.00001077
Iteration 4/1000 | Loss: 0.00000982
Iteration 5/1000 | Loss: 0.00000916
Iteration 6/1000 | Loss: 0.00000880
Iteration 7/1000 | Loss: 0.00000858
Iteration 8/1000 | Loss: 0.00000842
Iteration 9/1000 | Loss: 0.00000839
Iteration 10/1000 | Loss: 0.00000838
Iteration 11/1000 | Loss: 0.00000838
Iteration 12/1000 | Loss: 0.00000838
Iteration 13/1000 | Loss: 0.00000837
Iteration 14/1000 | Loss: 0.00000837
Iteration 15/1000 | Loss: 0.00000836
Iteration 16/1000 | Loss: 0.00000836
Iteration 17/1000 | Loss: 0.00000836
Iteration 18/1000 | Loss: 0.00000835
Iteration 19/1000 | Loss: 0.00000834
Iteration 20/1000 | Loss: 0.00000833
Iteration 21/1000 | Loss: 0.00000833
Iteration 22/1000 | Loss: 0.00000833
Iteration 23/1000 | Loss: 0.00000832
Iteration 24/1000 | Loss: 0.00000828
Iteration 25/1000 | Loss: 0.00000828
Iteration 26/1000 | Loss: 0.00000827
Iteration 27/1000 | Loss: 0.00000826
Iteration 28/1000 | Loss: 0.00000826
Iteration 29/1000 | Loss: 0.00000822
Iteration 30/1000 | Loss: 0.00000822
Iteration 31/1000 | Loss: 0.00000820
Iteration 32/1000 | Loss: 0.00000819
Iteration 33/1000 | Loss: 0.00000818
Iteration 34/1000 | Loss: 0.00000818
Iteration 35/1000 | Loss: 0.00000817
Iteration 36/1000 | Loss: 0.00000817
Iteration 37/1000 | Loss: 0.00000817
Iteration 38/1000 | Loss: 0.00000816
Iteration 39/1000 | Loss: 0.00000815
Iteration 40/1000 | Loss: 0.00000815
Iteration 41/1000 | Loss: 0.00000815
Iteration 42/1000 | Loss: 0.00000814
Iteration 43/1000 | Loss: 0.00000814
Iteration 44/1000 | Loss: 0.00000813
Iteration 45/1000 | Loss: 0.00000812
Iteration 46/1000 | Loss: 0.00000812
Iteration 47/1000 | Loss: 0.00000812
Iteration 48/1000 | Loss: 0.00000812
Iteration 49/1000 | Loss: 0.00000812
Iteration 50/1000 | Loss: 0.00000812
Iteration 51/1000 | Loss: 0.00000812
Iteration 52/1000 | Loss: 0.00000812
Iteration 53/1000 | Loss: 0.00000812
Iteration 54/1000 | Loss: 0.00000811
Iteration 55/1000 | Loss: 0.00000811
Iteration 56/1000 | Loss: 0.00000811
Iteration 57/1000 | Loss: 0.00000810
Iteration 58/1000 | Loss: 0.00000810
Iteration 59/1000 | Loss: 0.00000809
Iteration 60/1000 | Loss: 0.00000809
Iteration 61/1000 | Loss: 0.00000807
Iteration 62/1000 | Loss: 0.00000807
Iteration 63/1000 | Loss: 0.00000807
Iteration 64/1000 | Loss: 0.00000807
Iteration 65/1000 | Loss: 0.00000807
Iteration 66/1000 | Loss: 0.00000806
Iteration 67/1000 | Loss: 0.00000806
Iteration 68/1000 | Loss: 0.00000805
Iteration 69/1000 | Loss: 0.00000804
Iteration 70/1000 | Loss: 0.00000803
Iteration 71/1000 | Loss: 0.00000803
Iteration 72/1000 | Loss: 0.00000803
Iteration 73/1000 | Loss: 0.00000802
Iteration 74/1000 | Loss: 0.00000802
Iteration 75/1000 | Loss: 0.00000802
Iteration 76/1000 | Loss: 0.00000802
Iteration 77/1000 | Loss: 0.00000802
Iteration 78/1000 | Loss: 0.00000802
Iteration 79/1000 | Loss: 0.00000802
Iteration 80/1000 | Loss: 0.00000802
Iteration 81/1000 | Loss: 0.00000802
Iteration 82/1000 | Loss: 0.00000802
Iteration 83/1000 | Loss: 0.00000801
Iteration 84/1000 | Loss: 0.00000801
Iteration 85/1000 | Loss: 0.00000800
Iteration 86/1000 | Loss: 0.00000800
Iteration 87/1000 | Loss: 0.00000800
Iteration 88/1000 | Loss: 0.00000799
Iteration 89/1000 | Loss: 0.00000799
Iteration 90/1000 | Loss: 0.00000799
Iteration 91/1000 | Loss: 0.00000799
Iteration 92/1000 | Loss: 0.00000798
Iteration 93/1000 | Loss: 0.00000798
Iteration 94/1000 | Loss: 0.00000798
Iteration 95/1000 | Loss: 0.00000798
Iteration 96/1000 | Loss: 0.00000798
Iteration 97/1000 | Loss: 0.00000798
Iteration 98/1000 | Loss: 0.00000797
Iteration 99/1000 | Loss: 0.00000797
Iteration 100/1000 | Loss: 0.00000797
Iteration 101/1000 | Loss: 0.00000797
Iteration 102/1000 | Loss: 0.00000797
Iteration 103/1000 | Loss: 0.00000797
Iteration 104/1000 | Loss: 0.00000796
Iteration 105/1000 | Loss: 0.00000796
Iteration 106/1000 | Loss: 0.00000796
Iteration 107/1000 | Loss: 0.00000796
Iteration 108/1000 | Loss: 0.00000796
Iteration 109/1000 | Loss: 0.00000796
Iteration 110/1000 | Loss: 0.00000796
Iteration 111/1000 | Loss: 0.00000796
Iteration 112/1000 | Loss: 0.00000796
Iteration 113/1000 | Loss: 0.00000796
Iteration 114/1000 | Loss: 0.00000796
Iteration 115/1000 | Loss: 0.00000796
Iteration 116/1000 | Loss: 0.00000795
Iteration 117/1000 | Loss: 0.00000795
Iteration 118/1000 | Loss: 0.00000795
Iteration 119/1000 | Loss: 0.00000795
Iteration 120/1000 | Loss: 0.00000794
Iteration 121/1000 | Loss: 0.00000794
Iteration 122/1000 | Loss: 0.00000794
Iteration 123/1000 | Loss: 0.00000794
Iteration 124/1000 | Loss: 0.00000794
Iteration 125/1000 | Loss: 0.00000794
Iteration 126/1000 | Loss: 0.00000794
Iteration 127/1000 | Loss: 0.00000794
Iteration 128/1000 | Loss: 0.00000794
Iteration 129/1000 | Loss: 0.00000793
Iteration 130/1000 | Loss: 0.00000793
Iteration 131/1000 | Loss: 0.00000793
Iteration 132/1000 | Loss: 0.00000793
Iteration 133/1000 | Loss: 0.00000793
Iteration 134/1000 | Loss: 0.00000793
Iteration 135/1000 | Loss: 0.00000793
Iteration 136/1000 | Loss: 0.00000793
Iteration 137/1000 | Loss: 0.00000793
Iteration 138/1000 | Loss: 0.00000793
Iteration 139/1000 | Loss: 0.00000793
Iteration 140/1000 | Loss: 0.00000793
Iteration 141/1000 | Loss: 0.00000793
Iteration 142/1000 | Loss: 0.00000793
Iteration 143/1000 | Loss: 0.00000792
Iteration 144/1000 | Loss: 0.00000792
Iteration 145/1000 | Loss: 0.00000792
Iteration 146/1000 | Loss: 0.00000792
Iteration 147/1000 | Loss: 0.00000792
Iteration 148/1000 | Loss: 0.00000792
Iteration 149/1000 | Loss: 0.00000792
Iteration 150/1000 | Loss: 0.00000792
Iteration 151/1000 | Loss: 0.00000792
Iteration 152/1000 | Loss: 0.00000791
Iteration 153/1000 | Loss: 0.00000791
Iteration 154/1000 | Loss: 0.00000791
Iteration 155/1000 | Loss: 0.00000791
Iteration 156/1000 | Loss: 0.00000791
Iteration 157/1000 | Loss: 0.00000791
Iteration 158/1000 | Loss: 0.00000791
Iteration 159/1000 | Loss: 0.00000791
Iteration 160/1000 | Loss: 0.00000791
Iteration 161/1000 | Loss: 0.00000791
Iteration 162/1000 | Loss: 0.00000791
Iteration 163/1000 | Loss: 0.00000790
Iteration 164/1000 | Loss: 0.00000790
Iteration 165/1000 | Loss: 0.00000790
Iteration 166/1000 | Loss: 0.00000790
Iteration 167/1000 | Loss: 0.00000790
Iteration 168/1000 | Loss: 0.00000790
Iteration 169/1000 | Loss: 0.00000790
Iteration 170/1000 | Loss: 0.00000789
Iteration 171/1000 | Loss: 0.00000789
Iteration 172/1000 | Loss: 0.00000789
Iteration 173/1000 | Loss: 0.00000789
Iteration 174/1000 | Loss: 0.00000789
Iteration 175/1000 | Loss: 0.00000789
Iteration 176/1000 | Loss: 0.00000789
Iteration 177/1000 | Loss: 0.00000789
Iteration 178/1000 | Loss: 0.00000789
Iteration 179/1000 | Loss: 0.00000789
Iteration 180/1000 | Loss: 0.00000789
Iteration 181/1000 | Loss: 0.00000789
Iteration 182/1000 | Loss: 0.00000789
Iteration 183/1000 | Loss: 0.00000789
Iteration 184/1000 | Loss: 0.00000789
Iteration 185/1000 | Loss: 0.00000789
Iteration 186/1000 | Loss: 0.00000789
Iteration 187/1000 | Loss: 0.00000788
Iteration 188/1000 | Loss: 0.00000788
Iteration 189/1000 | Loss: 0.00000788
Iteration 190/1000 | Loss: 0.00000788
Iteration 191/1000 | Loss: 0.00000788
Iteration 192/1000 | Loss: 0.00000788
Iteration 193/1000 | Loss: 0.00000788
Iteration 194/1000 | Loss: 0.00000788
Iteration 195/1000 | Loss: 0.00000788
Iteration 196/1000 | Loss: 0.00000788
Iteration 197/1000 | Loss: 0.00000788
Iteration 198/1000 | Loss: 0.00000788
Iteration 199/1000 | Loss: 0.00000788
Iteration 200/1000 | Loss: 0.00000788
Iteration 201/1000 | Loss: 0.00000788
Iteration 202/1000 | Loss: 0.00000788
Iteration 203/1000 | Loss: 0.00000788
Iteration 204/1000 | Loss: 0.00000788
Iteration 205/1000 | Loss: 0.00000788
Iteration 206/1000 | Loss: 0.00000788
Iteration 207/1000 | Loss: 0.00000788
Iteration 208/1000 | Loss: 0.00000788
Iteration 209/1000 | Loss: 0.00000788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [7.878322321630549e-06, 7.878322321630549e-06, 7.878322321630549e-06, 7.878322321630549e-06, 7.878322321630549e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.878322321630549e-06

Optimization complete. Final v2v error: 2.3793179988861084 mm

Highest mean error: 3.034693479537964 mm for frame 40

Lowest mean error: 2.086609125137329 mm for frame 125

Saving results

Total time: 61.668700218200684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080924
Iteration 2/25 | Loss: 0.00175625
Iteration 3/25 | Loss: 0.00109429
Iteration 4/25 | Loss: 0.00104448
Iteration 5/25 | Loss: 0.00103406
Iteration 6/25 | Loss: 0.00103040
Iteration 7/25 | Loss: 0.00103009
Iteration 8/25 | Loss: 0.00103009
Iteration 9/25 | Loss: 0.00103009
Iteration 10/25 | Loss: 0.00103009
Iteration 11/25 | Loss: 0.00103009
Iteration 12/25 | Loss: 0.00103009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010300911962985992, 0.0010300911962985992, 0.0010300911962985992, 0.0010300911962985992, 0.0010300911962985992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010300911962985992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40863502
Iteration 2/25 | Loss: 0.00067505
Iteration 3/25 | Loss: 0.00067505
Iteration 4/25 | Loss: 0.00067505
Iteration 5/25 | Loss: 0.00067505
Iteration 6/25 | Loss: 0.00067505
Iteration 7/25 | Loss: 0.00067505
Iteration 8/25 | Loss: 0.00067505
Iteration 9/25 | Loss: 0.00067505
Iteration 10/25 | Loss: 0.00067505
Iteration 11/25 | Loss: 0.00067505
Iteration 12/25 | Loss: 0.00067505
Iteration 13/25 | Loss: 0.00067505
Iteration 14/25 | Loss: 0.00067505
Iteration 15/25 | Loss: 0.00067505
Iteration 16/25 | Loss: 0.00067505
Iteration 17/25 | Loss: 0.00067505
Iteration 18/25 | Loss: 0.00067505
Iteration 19/25 | Loss: 0.00067505
Iteration 20/25 | Loss: 0.00067505
Iteration 21/25 | Loss: 0.00067505
Iteration 22/25 | Loss: 0.00067505
Iteration 23/25 | Loss: 0.00067505
Iteration 24/25 | Loss: 0.00067505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006750489701516926, 0.0006750489701516926, 0.0006750489701516926, 0.0006750489701516926, 0.0006750489701516926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006750489701516926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067505
Iteration 2/1000 | Loss: 0.00005835
Iteration 3/1000 | Loss: 0.00003568
Iteration 4/1000 | Loss: 0.00003154
Iteration 5/1000 | Loss: 0.00003005
Iteration 6/1000 | Loss: 0.00002923
Iteration 7/1000 | Loss: 0.00002862
Iteration 8/1000 | Loss: 0.00002823
Iteration 9/1000 | Loss: 0.00002791
Iteration 10/1000 | Loss: 0.00002768
Iteration 11/1000 | Loss: 0.00002765
Iteration 12/1000 | Loss: 0.00002756
Iteration 13/1000 | Loss: 0.00002754
Iteration 14/1000 | Loss: 0.00002738
Iteration 15/1000 | Loss: 0.00002736
Iteration 16/1000 | Loss: 0.00002735
Iteration 17/1000 | Loss: 0.00002730
Iteration 18/1000 | Loss: 0.00002726
Iteration 19/1000 | Loss: 0.00002725
Iteration 20/1000 | Loss: 0.00002724
Iteration 21/1000 | Loss: 0.00002724
Iteration 22/1000 | Loss: 0.00002724
Iteration 23/1000 | Loss: 0.00002724
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00002724
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00002724
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00002722
Iteration 30/1000 | Loss: 0.00002722
Iteration 31/1000 | Loss: 0.00002721
Iteration 32/1000 | Loss: 0.00002721
Iteration 33/1000 | Loss: 0.00002721
Iteration 34/1000 | Loss: 0.00002720
Iteration 35/1000 | Loss: 0.00002720
Iteration 36/1000 | Loss: 0.00002720
Iteration 37/1000 | Loss: 0.00002720
Iteration 38/1000 | Loss: 0.00002720
Iteration 39/1000 | Loss: 0.00002720
Iteration 40/1000 | Loss: 0.00002720
Iteration 41/1000 | Loss: 0.00002719
Iteration 42/1000 | Loss: 0.00002719
Iteration 43/1000 | Loss: 0.00002719
Iteration 44/1000 | Loss: 0.00002719
Iteration 45/1000 | Loss: 0.00002719
Iteration 46/1000 | Loss: 0.00002719
Iteration 47/1000 | Loss: 0.00002719
Iteration 48/1000 | Loss: 0.00002719
Iteration 49/1000 | Loss: 0.00002719
Iteration 50/1000 | Loss: 0.00002718
Iteration 51/1000 | Loss: 0.00002718
Iteration 52/1000 | Loss: 0.00002718
Iteration 53/1000 | Loss: 0.00002718
Iteration 54/1000 | Loss: 0.00002718
Iteration 55/1000 | Loss: 0.00002718
Iteration 56/1000 | Loss: 0.00002718
Iteration 57/1000 | Loss: 0.00002718
Iteration 58/1000 | Loss: 0.00002717
Iteration 59/1000 | Loss: 0.00002717
Iteration 60/1000 | Loss: 0.00002717
Iteration 61/1000 | Loss: 0.00002717
Iteration 62/1000 | Loss: 0.00002717
Iteration 63/1000 | Loss: 0.00002717
Iteration 64/1000 | Loss: 0.00002717
Iteration 65/1000 | Loss: 0.00002716
Iteration 66/1000 | Loss: 0.00002716
Iteration 67/1000 | Loss: 0.00002716
Iteration 68/1000 | Loss: 0.00002716
Iteration 69/1000 | Loss: 0.00002716
Iteration 70/1000 | Loss: 0.00002716
Iteration 71/1000 | Loss: 0.00002715
Iteration 72/1000 | Loss: 0.00002715
Iteration 73/1000 | Loss: 0.00002715
Iteration 74/1000 | Loss: 0.00002715
Iteration 75/1000 | Loss: 0.00002715
Iteration 76/1000 | Loss: 0.00002714
Iteration 77/1000 | Loss: 0.00002714
Iteration 78/1000 | Loss: 0.00002714
Iteration 79/1000 | Loss: 0.00002714
Iteration 80/1000 | Loss: 0.00002714
Iteration 81/1000 | Loss: 0.00002714
Iteration 82/1000 | Loss: 0.00002713
Iteration 83/1000 | Loss: 0.00002713
Iteration 84/1000 | Loss: 0.00002713
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002712
Iteration 87/1000 | Loss: 0.00002712
Iteration 88/1000 | Loss: 0.00002712
Iteration 89/1000 | Loss: 0.00002712
Iteration 90/1000 | Loss: 0.00002712
Iteration 91/1000 | Loss: 0.00002712
Iteration 92/1000 | Loss: 0.00002712
Iteration 93/1000 | Loss: 0.00002712
Iteration 94/1000 | Loss: 0.00002712
Iteration 95/1000 | Loss: 0.00002712
Iteration 96/1000 | Loss: 0.00002712
Iteration 97/1000 | Loss: 0.00002712
Iteration 98/1000 | Loss: 0.00002712
Iteration 99/1000 | Loss: 0.00002712
Iteration 100/1000 | Loss: 0.00002711
Iteration 101/1000 | Loss: 0.00002711
Iteration 102/1000 | Loss: 0.00002711
Iteration 103/1000 | Loss: 0.00002711
Iteration 104/1000 | Loss: 0.00002711
Iteration 105/1000 | Loss: 0.00002711
Iteration 106/1000 | Loss: 0.00002711
Iteration 107/1000 | Loss: 0.00002711
Iteration 108/1000 | Loss: 0.00002710
Iteration 109/1000 | Loss: 0.00002710
Iteration 110/1000 | Loss: 0.00002710
Iteration 111/1000 | Loss: 0.00002710
Iteration 112/1000 | Loss: 0.00002710
Iteration 113/1000 | Loss: 0.00002710
Iteration 114/1000 | Loss: 0.00002710
Iteration 115/1000 | Loss: 0.00002710
Iteration 116/1000 | Loss: 0.00002710
Iteration 117/1000 | Loss: 0.00002710
Iteration 118/1000 | Loss: 0.00002710
Iteration 119/1000 | Loss: 0.00002710
Iteration 120/1000 | Loss: 0.00002710
Iteration 121/1000 | Loss: 0.00002710
Iteration 122/1000 | Loss: 0.00002710
Iteration 123/1000 | Loss: 0.00002710
Iteration 124/1000 | Loss: 0.00002710
Iteration 125/1000 | Loss: 0.00002710
Iteration 126/1000 | Loss: 0.00002710
Iteration 127/1000 | Loss: 0.00002710
Iteration 128/1000 | Loss: 0.00002710
Iteration 129/1000 | Loss: 0.00002710
Iteration 130/1000 | Loss: 0.00002710
Iteration 131/1000 | Loss: 0.00002709
Iteration 132/1000 | Loss: 0.00002709
Iteration 133/1000 | Loss: 0.00002709
Iteration 134/1000 | Loss: 0.00002709
Iteration 135/1000 | Loss: 0.00002709
Iteration 136/1000 | Loss: 0.00002709
Iteration 137/1000 | Loss: 0.00002709
Iteration 138/1000 | Loss: 0.00002709
Iteration 139/1000 | Loss: 0.00002709
Iteration 140/1000 | Loss: 0.00002709
Iteration 141/1000 | Loss: 0.00002709
Iteration 142/1000 | Loss: 0.00002709
Iteration 143/1000 | Loss: 0.00002709
Iteration 144/1000 | Loss: 0.00002709
Iteration 145/1000 | Loss: 0.00002709
Iteration 146/1000 | Loss: 0.00002709
Iteration 147/1000 | Loss: 0.00002709
Iteration 148/1000 | Loss: 0.00002709
Iteration 149/1000 | Loss: 0.00002709
Iteration 150/1000 | Loss: 0.00002709
Iteration 151/1000 | Loss: 0.00002709
Iteration 152/1000 | Loss: 0.00002709
Iteration 153/1000 | Loss: 0.00002709
Iteration 154/1000 | Loss: 0.00002709
Iteration 155/1000 | Loss: 0.00002709
Iteration 156/1000 | Loss: 0.00002709
Iteration 157/1000 | Loss: 0.00002709
Iteration 158/1000 | Loss: 0.00002709
Iteration 159/1000 | Loss: 0.00002709
Iteration 160/1000 | Loss: 0.00002709
Iteration 161/1000 | Loss: 0.00002709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.7094729375676252e-05, 2.7094729375676252e-05, 2.7094729375676252e-05, 2.7094729375676252e-05, 2.7094729375676252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7094729375676252e-05

Optimization complete. Final v2v error: 4.235095024108887 mm

Highest mean error: 4.671708583831787 mm for frame 46

Lowest mean error: 3.214566707611084 mm for frame 5

Saving results

Total time: 40.59817361831665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385073
Iteration 2/25 | Loss: 0.00107331
Iteration 3/25 | Loss: 0.00089930
Iteration 4/25 | Loss: 0.00085787
Iteration 5/25 | Loss: 0.00084529
Iteration 6/25 | Loss: 0.00083872
Iteration 7/25 | Loss: 0.00083595
Iteration 8/25 | Loss: 0.00083474
Iteration 9/25 | Loss: 0.00084820
Iteration 10/25 | Loss: 0.00082869
Iteration 11/25 | Loss: 0.00082106
Iteration 12/25 | Loss: 0.00081831
Iteration 13/25 | Loss: 0.00081767
Iteration 14/25 | Loss: 0.00081763
Iteration 15/25 | Loss: 0.00081763
Iteration 16/25 | Loss: 0.00081763
Iteration 17/25 | Loss: 0.00081763
Iteration 18/25 | Loss: 0.00081763
Iteration 19/25 | Loss: 0.00081763
Iteration 20/25 | Loss: 0.00081763
Iteration 21/25 | Loss: 0.00081762
Iteration 22/25 | Loss: 0.00081762
Iteration 23/25 | Loss: 0.00081762
Iteration 24/25 | Loss: 0.00081762
Iteration 25/25 | Loss: 0.00081762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37959814
Iteration 2/25 | Loss: 0.00033749
Iteration 3/25 | Loss: 0.00033749
Iteration 4/25 | Loss: 0.00033749
Iteration 5/25 | Loss: 0.00033749
Iteration 6/25 | Loss: 0.00033749
Iteration 7/25 | Loss: 0.00033749
Iteration 8/25 | Loss: 0.00033748
Iteration 9/25 | Loss: 0.00033748
Iteration 10/25 | Loss: 0.00033748
Iteration 11/25 | Loss: 0.00033748
Iteration 12/25 | Loss: 0.00033748
Iteration 13/25 | Loss: 0.00033748
Iteration 14/25 | Loss: 0.00033748
Iteration 15/25 | Loss: 0.00033748
Iteration 16/25 | Loss: 0.00033748
Iteration 17/25 | Loss: 0.00033748
Iteration 18/25 | Loss: 0.00033748
Iteration 19/25 | Loss: 0.00033748
Iteration 20/25 | Loss: 0.00033748
Iteration 21/25 | Loss: 0.00033748
Iteration 22/25 | Loss: 0.00033748
Iteration 23/25 | Loss: 0.00033748
Iteration 24/25 | Loss: 0.00033748
Iteration 25/25 | Loss: 0.00033748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033748
Iteration 2/1000 | Loss: 0.00001777
Iteration 3/1000 | Loss: 0.00001176
Iteration 4/1000 | Loss: 0.00001032
Iteration 5/1000 | Loss: 0.00000977
Iteration 6/1000 | Loss: 0.00000928
Iteration 7/1000 | Loss: 0.00000912
Iteration 8/1000 | Loss: 0.00000888
Iteration 9/1000 | Loss: 0.00000865
Iteration 10/1000 | Loss: 0.00000860
Iteration 11/1000 | Loss: 0.00000858
Iteration 12/1000 | Loss: 0.00000857
Iteration 13/1000 | Loss: 0.00000856
Iteration 14/1000 | Loss: 0.00000850
Iteration 15/1000 | Loss: 0.00000850
Iteration 16/1000 | Loss: 0.00000847
Iteration 17/1000 | Loss: 0.00000846
Iteration 18/1000 | Loss: 0.00000846
Iteration 19/1000 | Loss: 0.00000846
Iteration 20/1000 | Loss: 0.00000845
Iteration 21/1000 | Loss: 0.00000845
Iteration 22/1000 | Loss: 0.00000844
Iteration 23/1000 | Loss: 0.00000842
Iteration 24/1000 | Loss: 0.00000842
Iteration 25/1000 | Loss: 0.00000841
Iteration 26/1000 | Loss: 0.00000839
Iteration 27/1000 | Loss: 0.00000838
Iteration 28/1000 | Loss: 0.00000838
Iteration 29/1000 | Loss: 0.00000838
Iteration 30/1000 | Loss: 0.00000838
Iteration 31/1000 | Loss: 0.00000837
Iteration 32/1000 | Loss: 0.00000837
Iteration 33/1000 | Loss: 0.00000837
Iteration 34/1000 | Loss: 0.00000837
Iteration 35/1000 | Loss: 0.00000837
Iteration 36/1000 | Loss: 0.00000836
Iteration 37/1000 | Loss: 0.00000836
Iteration 38/1000 | Loss: 0.00000836
Iteration 39/1000 | Loss: 0.00000835
Iteration 40/1000 | Loss: 0.00000835
Iteration 41/1000 | Loss: 0.00000835
Iteration 42/1000 | Loss: 0.00000835
Iteration 43/1000 | Loss: 0.00000835
Iteration 44/1000 | Loss: 0.00000835
Iteration 45/1000 | Loss: 0.00000835
Iteration 46/1000 | Loss: 0.00000835
Iteration 47/1000 | Loss: 0.00000835
Iteration 48/1000 | Loss: 0.00000835
Iteration 49/1000 | Loss: 0.00000835
Iteration 50/1000 | Loss: 0.00000835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [8.350722055183724e-06, 8.350722055183724e-06, 8.350722055183724e-06, 8.350722055183724e-06, 8.350722055183724e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.350722055183724e-06

Optimization complete. Final v2v error: 2.4686248302459717 mm

Highest mean error: 2.891667366027832 mm for frame 132

Lowest mean error: 2.226705551147461 mm for frame 242

Saving results

Total time: 47.17079973220825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077511
Iteration 2/25 | Loss: 0.01077510
Iteration 3/25 | Loss: 0.01077510
Iteration 4/25 | Loss: 0.01077510
Iteration 5/25 | Loss: 0.01077510
Iteration 6/25 | Loss: 0.01077509
Iteration 7/25 | Loss: 0.01077509
Iteration 8/25 | Loss: 0.01077509
Iteration 9/25 | Loss: 0.01077509
Iteration 10/25 | Loss: 0.01077509
Iteration 11/25 | Loss: 0.01077508
Iteration 12/25 | Loss: 0.01077508
Iteration 13/25 | Loss: 0.01077508
Iteration 14/25 | Loss: 0.01077508
Iteration 15/25 | Loss: 0.01077508
Iteration 16/25 | Loss: 0.01077508
Iteration 17/25 | Loss: 0.01077507
Iteration 18/25 | Loss: 0.01077507
Iteration 19/25 | Loss: 0.01077507
Iteration 20/25 | Loss: 0.01077507
Iteration 21/25 | Loss: 0.01077507
Iteration 22/25 | Loss: 0.01077506
Iteration 23/25 | Loss: 0.01077506
Iteration 24/25 | Loss: 0.01077506
Iteration 25/25 | Loss: 0.01077506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68359137
Iteration 2/25 | Loss: 0.07474754
Iteration 3/25 | Loss: 0.06337865
Iteration 4/25 | Loss: 0.06334391
Iteration 5/25 | Loss: 0.06330122
Iteration 6/25 | Loss: 0.06249409
Iteration 7/25 | Loss: 0.06242706
Iteration 8/25 | Loss: 0.06242705
Iteration 9/25 | Loss: 0.06242705
Iteration 10/25 | Loss: 0.06242704
Iteration 11/25 | Loss: 0.06242704
Iteration 12/25 | Loss: 0.06242704
Iteration 13/25 | Loss: 0.06242704
Iteration 14/25 | Loss: 0.06242704
Iteration 15/25 | Loss: 0.06242704
Iteration 16/25 | Loss: 0.06242704
Iteration 17/25 | Loss: 0.06242704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.06242704018950462, 0.06242704018950462, 0.06242704018950462, 0.06242704018950462, 0.06242704018950462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06242704018950462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06242704
Iteration 2/1000 | Loss: 0.01018525
Iteration 3/1000 | Loss: 0.00265455
Iteration 4/1000 | Loss: 0.02101725
Iteration 5/1000 | Loss: 0.00087747
Iteration 6/1000 | Loss: 0.00359724
Iteration 7/1000 | Loss: 0.00118262
Iteration 8/1000 | Loss: 0.00081738
Iteration 9/1000 | Loss: 0.00057065
Iteration 10/1000 | Loss: 0.00046010
Iteration 11/1000 | Loss: 0.00040367
Iteration 12/1000 | Loss: 0.00039347
Iteration 13/1000 | Loss: 0.00028465
Iteration 14/1000 | Loss: 0.00040904
Iteration 15/1000 | Loss: 0.00020253
Iteration 16/1000 | Loss: 0.00075874
Iteration 17/1000 | Loss: 0.00170402
Iteration 18/1000 | Loss: 0.00043023
Iteration 19/1000 | Loss: 0.00017539
Iteration 20/1000 | Loss: 0.00053771
Iteration 21/1000 | Loss: 0.00016719
Iteration 22/1000 | Loss: 0.00019959
Iteration 23/1000 | Loss: 0.00027533
Iteration 24/1000 | Loss: 0.00026170
Iteration 25/1000 | Loss: 0.00018653
Iteration 26/1000 | Loss: 0.00027091
Iteration 27/1000 | Loss: 0.00009662
Iteration 28/1000 | Loss: 0.00034588
Iteration 29/1000 | Loss: 0.00041898
Iteration 30/1000 | Loss: 0.00037185
Iteration 31/1000 | Loss: 0.00078616
Iteration 32/1000 | Loss: 0.00008754
Iteration 33/1000 | Loss: 0.00024141
Iteration 34/1000 | Loss: 0.00008612
Iteration 35/1000 | Loss: 0.00018636
Iteration 36/1000 | Loss: 0.00045759
Iteration 37/1000 | Loss: 0.00057330
Iteration 38/1000 | Loss: 0.00043959
Iteration 39/1000 | Loss: 0.00052853
Iteration 40/1000 | Loss: 0.00026895
Iteration 41/1000 | Loss: 0.00008395
Iteration 42/1000 | Loss: 0.00032985
Iteration 43/1000 | Loss: 0.00068249
Iteration 44/1000 | Loss: 0.00028681
Iteration 45/1000 | Loss: 0.00039195
Iteration 46/1000 | Loss: 0.00020465
Iteration 47/1000 | Loss: 0.00058573
Iteration 48/1000 | Loss: 0.00038157
Iteration 49/1000 | Loss: 0.00050968
Iteration 50/1000 | Loss: 0.00018202
Iteration 51/1000 | Loss: 0.00030108
Iteration 52/1000 | Loss: 0.00022603
Iteration 53/1000 | Loss: 0.00045622
Iteration 54/1000 | Loss: 0.00019607
Iteration 55/1000 | Loss: 0.00040380
Iteration 56/1000 | Loss: 0.00014174
Iteration 57/1000 | Loss: 0.00013278
Iteration 58/1000 | Loss: 0.00007082
Iteration 59/1000 | Loss: 0.00005968
Iteration 60/1000 | Loss: 0.00005756
Iteration 61/1000 | Loss: 0.00034544
Iteration 62/1000 | Loss: 0.00073630
Iteration 63/1000 | Loss: 0.00015881
Iteration 64/1000 | Loss: 0.00005780
Iteration 65/1000 | Loss: 0.00012147
Iteration 66/1000 | Loss: 0.00008584
Iteration 67/1000 | Loss: 0.00009308
Iteration 68/1000 | Loss: 0.00005072
Iteration 69/1000 | Loss: 0.00007440
Iteration 70/1000 | Loss: 0.00013126
Iteration 71/1000 | Loss: 0.00021455
Iteration 72/1000 | Loss: 0.00016519
Iteration 73/1000 | Loss: 0.00005971
Iteration 74/1000 | Loss: 0.00004940
Iteration 75/1000 | Loss: 0.00004925
Iteration 76/1000 | Loss: 0.00027794
Iteration 77/1000 | Loss: 0.00120987
Iteration 78/1000 | Loss: 0.00026194
Iteration 79/1000 | Loss: 0.00033886
Iteration 80/1000 | Loss: 0.00028047
Iteration 81/1000 | Loss: 0.00030628
Iteration 82/1000 | Loss: 0.00020384
Iteration 83/1000 | Loss: 0.00009187
Iteration 84/1000 | Loss: 0.00005703
Iteration 85/1000 | Loss: 0.00007057
Iteration 86/1000 | Loss: 0.00006840
Iteration 87/1000 | Loss: 0.00007048
Iteration 88/1000 | Loss: 0.00008753
Iteration 89/1000 | Loss: 0.00005845
Iteration 90/1000 | Loss: 0.00006096
Iteration 91/1000 | Loss: 0.00006049
Iteration 92/1000 | Loss: 0.00005334
Iteration 93/1000 | Loss: 0.00005945
Iteration 94/1000 | Loss: 0.00015768
Iteration 95/1000 | Loss: 0.00005872
Iteration 96/1000 | Loss: 0.00008346
Iteration 97/1000 | Loss: 0.00005509
Iteration 98/1000 | Loss: 0.00015649
Iteration 99/1000 | Loss: 0.00013301
Iteration 100/1000 | Loss: 0.00017068
Iteration 101/1000 | Loss: 0.00058797
Iteration 102/1000 | Loss: 0.00017883
Iteration 103/1000 | Loss: 0.00017020
Iteration 104/1000 | Loss: 0.00015830
Iteration 105/1000 | Loss: 0.00013939
Iteration 106/1000 | Loss: 0.00007033
Iteration 107/1000 | Loss: 0.00007196
Iteration 108/1000 | Loss: 0.00005839
Iteration 109/1000 | Loss: 0.00005410
Iteration 110/1000 | Loss: 0.00004955
Iteration 111/1000 | Loss: 0.00004719
Iteration 112/1000 | Loss: 0.00011194
Iteration 113/1000 | Loss: 0.00004606
Iteration 114/1000 | Loss: 0.00004505
Iteration 115/1000 | Loss: 0.00014192
Iteration 116/1000 | Loss: 0.00004414
Iteration 117/1000 | Loss: 0.00004373
Iteration 118/1000 | Loss: 0.00006034
Iteration 119/1000 | Loss: 0.00004334
Iteration 120/1000 | Loss: 0.00004330
Iteration 121/1000 | Loss: 0.00006834
Iteration 122/1000 | Loss: 0.00004344
Iteration 123/1000 | Loss: 0.00007036
Iteration 124/1000 | Loss: 0.00031994
Iteration 125/1000 | Loss: 0.00048284
Iteration 126/1000 | Loss: 0.00021247
Iteration 127/1000 | Loss: 0.00006146
Iteration 128/1000 | Loss: 0.00006341
Iteration 129/1000 | Loss: 0.00005196
Iteration 130/1000 | Loss: 0.00009633
Iteration 131/1000 | Loss: 0.00007834
Iteration 132/1000 | Loss: 0.00004963
Iteration 133/1000 | Loss: 0.00004851
Iteration 134/1000 | Loss: 0.00009285
Iteration 135/1000 | Loss: 0.00011991
Iteration 136/1000 | Loss: 0.00007976
Iteration 137/1000 | Loss: 0.00013872
Iteration 138/1000 | Loss: 0.00004582
Iteration 139/1000 | Loss: 0.00005796
Iteration 140/1000 | Loss: 0.00004416
Iteration 141/1000 | Loss: 0.00006580
Iteration 142/1000 | Loss: 0.00006137
Iteration 143/1000 | Loss: 0.00004361
Iteration 144/1000 | Loss: 0.00005605
Iteration 145/1000 | Loss: 0.00007158
Iteration 146/1000 | Loss: 0.00004779
Iteration 147/1000 | Loss: 0.00004324
Iteration 148/1000 | Loss: 0.00004324
Iteration 149/1000 | Loss: 0.00004324
Iteration 150/1000 | Loss: 0.00004323
Iteration 151/1000 | Loss: 0.00004323
Iteration 152/1000 | Loss: 0.00008557
Iteration 153/1000 | Loss: 0.00009751
Iteration 154/1000 | Loss: 0.00010354
Iteration 155/1000 | Loss: 0.00004290
Iteration 156/1000 | Loss: 0.00005820
Iteration 157/1000 | Loss: 0.00005820
Iteration 158/1000 | Loss: 0.00005513
Iteration 159/1000 | Loss: 0.00004861
Iteration 160/1000 | Loss: 0.00004572
Iteration 161/1000 | Loss: 0.00004873
Iteration 162/1000 | Loss: 0.00004649
Iteration 163/1000 | Loss: 0.00004281
Iteration 164/1000 | Loss: 0.00004281
Iteration 165/1000 | Loss: 0.00004280
Iteration 166/1000 | Loss: 0.00004279
Iteration 167/1000 | Loss: 0.00004279
Iteration 168/1000 | Loss: 0.00004279
Iteration 169/1000 | Loss: 0.00004278
Iteration 170/1000 | Loss: 0.00004278
Iteration 171/1000 | Loss: 0.00004278
Iteration 172/1000 | Loss: 0.00004483
Iteration 173/1000 | Loss: 0.00004370
Iteration 174/1000 | Loss: 0.00014055
Iteration 175/1000 | Loss: 0.00004326
Iteration 176/1000 | Loss: 0.00004458
Iteration 177/1000 | Loss: 0.00004304
Iteration 178/1000 | Loss: 0.00004336
Iteration 179/1000 | Loss: 0.00004272
Iteration 180/1000 | Loss: 0.00004271
Iteration 181/1000 | Loss: 0.00004271
Iteration 182/1000 | Loss: 0.00004269
Iteration 183/1000 | Loss: 0.00004269
Iteration 184/1000 | Loss: 0.00004269
Iteration 185/1000 | Loss: 0.00004269
Iteration 186/1000 | Loss: 0.00004269
Iteration 187/1000 | Loss: 0.00004269
Iteration 188/1000 | Loss: 0.00004269
Iteration 189/1000 | Loss: 0.00004269
Iteration 190/1000 | Loss: 0.00004269
Iteration 191/1000 | Loss: 0.00004269
Iteration 192/1000 | Loss: 0.00004269
Iteration 193/1000 | Loss: 0.00004428
Iteration 194/1000 | Loss: 0.00004265
Iteration 195/1000 | Loss: 0.00004264
Iteration 196/1000 | Loss: 0.00004264
Iteration 197/1000 | Loss: 0.00004264
Iteration 198/1000 | Loss: 0.00004264
Iteration 199/1000 | Loss: 0.00004264
Iteration 200/1000 | Loss: 0.00004264
Iteration 201/1000 | Loss: 0.00004263
Iteration 202/1000 | Loss: 0.00004263
Iteration 203/1000 | Loss: 0.00004263
Iteration 204/1000 | Loss: 0.00004263
Iteration 205/1000 | Loss: 0.00004262
Iteration 206/1000 | Loss: 0.00004262
Iteration 207/1000 | Loss: 0.00004262
Iteration 208/1000 | Loss: 0.00004261
Iteration 209/1000 | Loss: 0.00004255
Iteration 210/1000 | Loss: 0.00005478
Iteration 211/1000 | Loss: 0.00004255
Iteration 212/1000 | Loss: 0.00004242
Iteration 213/1000 | Loss: 0.00004241
Iteration 214/1000 | Loss: 0.00004241
Iteration 215/1000 | Loss: 0.00004241
Iteration 216/1000 | Loss: 0.00004241
Iteration 217/1000 | Loss: 0.00004240
Iteration 218/1000 | Loss: 0.00004240
Iteration 219/1000 | Loss: 0.00004240
Iteration 220/1000 | Loss: 0.00004240
Iteration 221/1000 | Loss: 0.00004240
Iteration 222/1000 | Loss: 0.00004239
Iteration 223/1000 | Loss: 0.00004239
Iteration 224/1000 | Loss: 0.00004238
Iteration 225/1000 | Loss: 0.00004238
Iteration 226/1000 | Loss: 0.00004237
Iteration 227/1000 | Loss: 0.00004236
Iteration 228/1000 | Loss: 0.00004236
Iteration 229/1000 | Loss: 0.00004236
Iteration 230/1000 | Loss: 0.00004236
Iteration 231/1000 | Loss: 0.00004236
Iteration 232/1000 | Loss: 0.00004236
Iteration 233/1000 | Loss: 0.00009467
Iteration 234/1000 | Loss: 0.00006694
Iteration 235/1000 | Loss: 0.00004287
Iteration 236/1000 | Loss: 0.00006213
Iteration 237/1000 | Loss: 0.00004791
Iteration 238/1000 | Loss: 0.00006444
Iteration 239/1000 | Loss: 0.00004331
Iteration 240/1000 | Loss: 0.00004263
Iteration 241/1000 | Loss: 0.00005216
Iteration 242/1000 | Loss: 0.00004988
Iteration 243/1000 | Loss: 0.00006388
Iteration 244/1000 | Loss: 0.00004356
Iteration 245/1000 | Loss: 0.00004235
Iteration 246/1000 | Loss: 0.00004235
Iteration 247/1000 | Loss: 0.00004234
Iteration 248/1000 | Loss: 0.00004234
Iteration 249/1000 | Loss: 0.00004234
Iteration 250/1000 | Loss: 0.00004234
Iteration 251/1000 | Loss: 0.00004234
Iteration 252/1000 | Loss: 0.00004234
Iteration 253/1000 | Loss: 0.00004234
Iteration 254/1000 | Loss: 0.00004234
Iteration 255/1000 | Loss: 0.00004234
Iteration 256/1000 | Loss: 0.00004234
Iteration 257/1000 | Loss: 0.00004234
Iteration 258/1000 | Loss: 0.00004234
Iteration 259/1000 | Loss: 0.00004234
Iteration 260/1000 | Loss: 0.00004234
Iteration 261/1000 | Loss: 0.00009911
Iteration 262/1000 | Loss: 0.00007132
Iteration 263/1000 | Loss: 0.00004573
Iteration 264/1000 | Loss: 0.00010065
Iteration 265/1000 | Loss: 0.00005825
Iteration 266/1000 | Loss: 0.00004900
Iteration 267/1000 | Loss: 0.00008130
Iteration 268/1000 | Loss: 0.00006844
Iteration 269/1000 | Loss: 0.00008662
Iteration 270/1000 | Loss: 0.00022173
Iteration 271/1000 | Loss: 0.00008431
Iteration 272/1000 | Loss: 0.00035623
Iteration 273/1000 | Loss: 0.00008862
Iteration 274/1000 | Loss: 0.00027936
Iteration 275/1000 | Loss: 0.00008451
Iteration 276/1000 | Loss: 0.00036780
Iteration 277/1000 | Loss: 0.00008647
Iteration 278/1000 | Loss: 0.00021843
Iteration 279/1000 | Loss: 0.00021971
Iteration 280/1000 | Loss: 0.00007383
Iteration 281/1000 | Loss: 0.00008899
Iteration 282/1000 | Loss: 0.00019863
Iteration 283/1000 | Loss: 0.00007875
Iteration 284/1000 | Loss: 0.00005189
Iteration 285/1000 | Loss: 0.00006056
Iteration 286/1000 | Loss: 0.00004247
Iteration 287/1000 | Loss: 0.00004247
Iteration 288/1000 | Loss: 0.00004240
Iteration 289/1000 | Loss: 0.00004237
Iteration 290/1000 | Loss: 0.00004237
Iteration 291/1000 | Loss: 0.00004235
Iteration 292/1000 | Loss: 0.00004235
Iteration 293/1000 | Loss: 0.00004235
Iteration 294/1000 | Loss: 0.00004235
Iteration 295/1000 | Loss: 0.00004235
Iteration 296/1000 | Loss: 0.00004235
Iteration 297/1000 | Loss: 0.00004234
Iteration 298/1000 | Loss: 0.00004234
Iteration 299/1000 | Loss: 0.00004234
Iteration 300/1000 | Loss: 0.00004234
Iteration 301/1000 | Loss: 0.00004233
Iteration 302/1000 | Loss: 0.00004233
Iteration 303/1000 | Loss: 0.00004233
Iteration 304/1000 | Loss: 0.00004232
Iteration 305/1000 | Loss: 0.00004232
Iteration 306/1000 | Loss: 0.00004232
Iteration 307/1000 | Loss: 0.00007807
Iteration 308/1000 | Loss: 0.00004939
Iteration 309/1000 | Loss: 0.00004234
Iteration 310/1000 | Loss: 0.00004230
Iteration 311/1000 | Loss: 0.00005141
Iteration 312/1000 | Loss: 0.00004232
Iteration 313/1000 | Loss: 0.00004799
Iteration 314/1000 | Loss: 0.00005815
Iteration 315/1000 | Loss: 0.00008012
Iteration 316/1000 | Loss: 0.00006979
Iteration 317/1000 | Loss: 0.00004326
Iteration 318/1000 | Loss: 0.00004230
Iteration 319/1000 | Loss: 0.00004230
Iteration 320/1000 | Loss: 0.00004230
Iteration 321/1000 | Loss: 0.00004230
Iteration 322/1000 | Loss: 0.00004229
Iteration 323/1000 | Loss: 0.00004229
Iteration 324/1000 | Loss: 0.00004243
Iteration 325/1000 | Loss: 0.00004229
Iteration 326/1000 | Loss: 0.00004229
Iteration 327/1000 | Loss: 0.00004229
Iteration 328/1000 | Loss: 0.00004229
Iteration 329/1000 | Loss: 0.00004229
Iteration 330/1000 | Loss: 0.00004229
Iteration 331/1000 | Loss: 0.00004229
Iteration 332/1000 | Loss: 0.00004229
Iteration 333/1000 | Loss: 0.00004229
Iteration 334/1000 | Loss: 0.00004229
Iteration 335/1000 | Loss: 0.00004229
Iteration 336/1000 | Loss: 0.00004229
Iteration 337/1000 | Loss: 0.00004229
Iteration 338/1000 | Loss: 0.00004229
Iteration 339/1000 | Loss: 0.00004229
Iteration 340/1000 | Loss: 0.00004229
Iteration 341/1000 | Loss: 0.00004229
Iteration 342/1000 | Loss: 0.00004228
Iteration 343/1000 | Loss: 0.00004228
Iteration 344/1000 | Loss: 0.00004228
Iteration 345/1000 | Loss: 0.00004228
Iteration 346/1000 | Loss: 0.00004228
Iteration 347/1000 | Loss: 0.00004228
Iteration 348/1000 | Loss: 0.00004228
Iteration 349/1000 | Loss: 0.00004228
Iteration 350/1000 | Loss: 0.00004228
Iteration 351/1000 | Loss: 0.00004228
Iteration 352/1000 | Loss: 0.00004228
Iteration 353/1000 | Loss: 0.00004228
Iteration 354/1000 | Loss: 0.00004228
Iteration 355/1000 | Loss: 0.00004228
Iteration 356/1000 | Loss: 0.00004228
Iteration 357/1000 | Loss: 0.00004228
Iteration 358/1000 | Loss: 0.00004228
Iteration 359/1000 | Loss: 0.00004228
Iteration 360/1000 | Loss: 0.00004228
Iteration 361/1000 | Loss: 0.00004228
Iteration 362/1000 | Loss: 0.00004228
Iteration 363/1000 | Loss: 0.00004228
Iteration 364/1000 | Loss: 0.00004228
Iteration 365/1000 | Loss: 0.00004228
Iteration 366/1000 | Loss: 0.00004228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [4.227651879773475e-05, 4.227651879773475e-05, 4.227651879773475e-05, 4.227651879773475e-05, 4.227651879773475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.227651879773475e-05

Optimization complete. Final v2v error: 3.6389873027801514 mm

Highest mean error: 20.24989891052246 mm for frame 150

Lowest mean error: 2.309706926345825 mm for frame 208

Saving results

Total time: 369.4119322299957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401649
Iteration 2/25 | Loss: 0.00115415
Iteration 3/25 | Loss: 0.00086976
Iteration 4/25 | Loss: 0.00084554
Iteration 5/25 | Loss: 0.00083693
Iteration 6/25 | Loss: 0.00083421
Iteration 7/25 | Loss: 0.00083374
Iteration 8/25 | Loss: 0.00083374
Iteration 9/25 | Loss: 0.00083374
Iteration 10/25 | Loss: 0.00083374
Iteration 11/25 | Loss: 0.00083374
Iteration 12/25 | Loss: 0.00083374
Iteration 13/25 | Loss: 0.00083374
Iteration 14/25 | Loss: 0.00083374
Iteration 15/25 | Loss: 0.00083374
Iteration 16/25 | Loss: 0.00083374
Iteration 17/25 | Loss: 0.00083374
Iteration 18/25 | Loss: 0.00083374
Iteration 19/25 | Loss: 0.00083374
Iteration 20/25 | Loss: 0.00083374
Iteration 21/25 | Loss: 0.00083374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008337403414770961, 0.0008337403414770961, 0.0008337403414770961, 0.0008337403414770961, 0.0008337403414770961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008337403414770961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45200706
Iteration 2/25 | Loss: 0.00055978
Iteration 3/25 | Loss: 0.00055978
Iteration 4/25 | Loss: 0.00055978
Iteration 5/25 | Loss: 0.00055978
Iteration 6/25 | Loss: 0.00055978
Iteration 7/25 | Loss: 0.00055978
Iteration 8/25 | Loss: 0.00055978
Iteration 9/25 | Loss: 0.00055978
Iteration 10/25 | Loss: 0.00055978
Iteration 11/25 | Loss: 0.00055978
Iteration 12/25 | Loss: 0.00055978
Iteration 13/25 | Loss: 0.00055978
Iteration 14/25 | Loss: 0.00055978
Iteration 15/25 | Loss: 0.00055978
Iteration 16/25 | Loss: 0.00055978
Iteration 17/25 | Loss: 0.00055978
Iteration 18/25 | Loss: 0.00055978
Iteration 19/25 | Loss: 0.00055978
Iteration 20/25 | Loss: 0.00055978
Iteration 21/25 | Loss: 0.00055978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000559781095944345, 0.000559781095944345, 0.000559781095944345, 0.000559781095944345, 0.000559781095944345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000559781095944345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055978
Iteration 2/1000 | Loss: 0.00002612
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001524
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001382
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001364
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001361
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001336
Iteration 71/1000 | Loss: 0.00001336
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001331
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001327
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001327
Iteration 175/1000 | Loss: 0.00001327
Iteration 176/1000 | Loss: 0.00001327
Iteration 177/1000 | Loss: 0.00001327
Iteration 178/1000 | Loss: 0.00001327
Iteration 179/1000 | Loss: 0.00001327
Iteration 180/1000 | Loss: 0.00001327
Iteration 181/1000 | Loss: 0.00001327
Iteration 182/1000 | Loss: 0.00001327
Iteration 183/1000 | Loss: 0.00001327
Iteration 184/1000 | Loss: 0.00001327
Iteration 185/1000 | Loss: 0.00001327
Iteration 186/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.3267516806081403e-05, 1.3267516806081403e-05, 1.3267516806081403e-05, 1.3267516806081403e-05, 1.3267516806081403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3267516806081403e-05

Optimization complete. Final v2v error: 2.9567484855651855 mm

Highest mean error: 3.5512137413024902 mm for frame 73

Lowest mean error: 2.291628122329712 mm for frame 117

Saving results

Total time: 36.57905602455139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059500
Iteration 2/25 | Loss: 0.01059500
Iteration 3/25 | Loss: 0.01059500
Iteration 4/25 | Loss: 0.01059500
Iteration 5/25 | Loss: 0.01059500
Iteration 6/25 | Loss: 0.01059499
Iteration 7/25 | Loss: 0.01059499
Iteration 8/25 | Loss: 0.01059499
Iteration 9/25 | Loss: 0.01059499
Iteration 10/25 | Loss: 0.01059499
Iteration 11/25 | Loss: 0.01059499
Iteration 12/25 | Loss: 0.01059499
Iteration 13/25 | Loss: 0.01059499
Iteration 14/25 | Loss: 0.01059498
Iteration 15/25 | Loss: 0.01059498
Iteration 16/25 | Loss: 0.01059498
Iteration 17/25 | Loss: 0.01059498
Iteration 18/25 | Loss: 0.01059498
Iteration 19/25 | Loss: 0.01059498
Iteration 20/25 | Loss: 0.01059498
Iteration 21/25 | Loss: 0.01059498
Iteration 22/25 | Loss: 0.01059498
Iteration 23/25 | Loss: 0.01059498
Iteration 24/25 | Loss: 0.01059497
Iteration 25/25 | Loss: 0.01059497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02222061
Iteration 2/25 | Loss: 0.07385198
Iteration 3/25 | Loss: 0.07385194
Iteration 4/25 | Loss: 0.07385194
Iteration 5/25 | Loss: 0.07385194
Iteration 6/25 | Loss: 0.07385194
Iteration 7/25 | Loss: 0.07385194
Iteration 8/25 | Loss: 0.07385193
Iteration 9/25 | Loss: 0.07385193
Iteration 10/25 | Loss: 0.07385193
Iteration 11/25 | Loss: 0.07385193
Iteration 12/25 | Loss: 0.07385193
Iteration 13/25 | Loss: 0.07385193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.07385192811489105, 0.07385192811489105, 0.07385192811489105, 0.07385192811489105, 0.07385192811489105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07385192811489105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07385193
Iteration 2/1000 | Loss: 0.00175319
Iteration 3/1000 | Loss: 0.00100335
Iteration 4/1000 | Loss: 0.00115981
Iteration 5/1000 | Loss: 0.00070599
Iteration 6/1000 | Loss: 0.00035541
Iteration 7/1000 | Loss: 0.00022112
Iteration 8/1000 | Loss: 0.00011822
Iteration 9/1000 | Loss: 0.00009623
Iteration 10/1000 | Loss: 0.00007075
Iteration 11/1000 | Loss: 0.00012189
Iteration 12/1000 | Loss: 0.00012207
Iteration 13/1000 | Loss: 0.00009488
Iteration 14/1000 | Loss: 0.00009208
Iteration 15/1000 | Loss: 0.00010407
Iteration 16/1000 | Loss: 0.00004850
Iteration 17/1000 | Loss: 0.00008331
Iteration 18/1000 | Loss: 0.00010007
Iteration 19/1000 | Loss: 0.00003448
Iteration 20/1000 | Loss: 0.00007895
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00004319
Iteration 23/1000 | Loss: 0.00003473
Iteration 24/1000 | Loss: 0.00004029
Iteration 25/1000 | Loss: 0.00003265
Iteration 26/1000 | Loss: 0.00005867
Iteration 27/1000 | Loss: 0.00003039
Iteration 28/1000 | Loss: 0.00028358
Iteration 29/1000 | Loss: 0.00002988
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00001866
Iteration 32/1000 | Loss: 0.00003361
Iteration 33/1000 | Loss: 0.00003360
Iteration 34/1000 | Loss: 0.00002793
Iteration 35/1000 | Loss: 0.00002894
Iteration 36/1000 | Loss: 0.00002700
Iteration 37/1000 | Loss: 0.00002546
Iteration 38/1000 | Loss: 0.00007846
Iteration 39/1000 | Loss: 0.00002808
Iteration 40/1000 | Loss: 0.00002820
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00002170
Iteration 43/1000 | Loss: 0.00002229
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00018023
Iteration 48/1000 | Loss: 0.00004269
Iteration 49/1000 | Loss: 0.00002289
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00002163
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001582
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00002548
Iteration 57/1000 | Loss: 0.00003813
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00008111
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001531
Iteration 95/1000 | Loss: 0.00001531
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001530
Iteration 98/1000 | Loss: 0.00001530
Iteration 99/1000 | Loss: 0.00001530
Iteration 100/1000 | Loss: 0.00001529
Iteration 101/1000 | Loss: 0.00001529
Iteration 102/1000 | Loss: 0.00001528
Iteration 103/1000 | Loss: 0.00001528
Iteration 104/1000 | Loss: 0.00001528
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001524
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001523
Iteration 117/1000 | Loss: 0.00001523
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00002882
Iteration 124/1000 | Loss: 0.00002034
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Iteration 157/1000 | Loss: 0.00001514
Iteration 158/1000 | Loss: 0.00001514
Iteration 159/1000 | Loss: 0.00001514
Iteration 160/1000 | Loss: 0.00001514
Iteration 161/1000 | Loss: 0.00001514
Iteration 162/1000 | Loss: 0.00001514
Iteration 163/1000 | Loss: 0.00001514
Iteration 164/1000 | Loss: 0.00001514
Iteration 165/1000 | Loss: 0.00001514
Iteration 166/1000 | Loss: 0.00001514
Iteration 167/1000 | Loss: 0.00001514
Iteration 168/1000 | Loss: 0.00001514
Iteration 169/1000 | Loss: 0.00001514
Iteration 170/1000 | Loss: 0.00001514
Iteration 171/1000 | Loss: 0.00001514
Iteration 172/1000 | Loss: 0.00001514
Iteration 173/1000 | Loss: 0.00001514
Iteration 174/1000 | Loss: 0.00001514
Iteration 175/1000 | Loss: 0.00001514
Iteration 176/1000 | Loss: 0.00001514
Iteration 177/1000 | Loss: 0.00001514
Iteration 178/1000 | Loss: 0.00001514
Iteration 179/1000 | Loss: 0.00001514
Iteration 180/1000 | Loss: 0.00001514
Iteration 181/1000 | Loss: 0.00001514
Iteration 182/1000 | Loss: 0.00001514
Iteration 183/1000 | Loss: 0.00001514
Iteration 184/1000 | Loss: 0.00001514
Iteration 185/1000 | Loss: 0.00001514
Iteration 186/1000 | Loss: 0.00001514
Iteration 187/1000 | Loss: 0.00001514
Iteration 188/1000 | Loss: 0.00001514
Iteration 189/1000 | Loss: 0.00001514
Iteration 190/1000 | Loss: 0.00001514
Iteration 191/1000 | Loss: 0.00001514
Iteration 192/1000 | Loss: 0.00001514
Iteration 193/1000 | Loss: 0.00001514
Iteration 194/1000 | Loss: 0.00001514
Iteration 195/1000 | Loss: 0.00001514
Iteration 196/1000 | Loss: 0.00001514
Iteration 197/1000 | Loss: 0.00001514
Iteration 198/1000 | Loss: 0.00001514
Iteration 199/1000 | Loss: 0.00001514
Iteration 200/1000 | Loss: 0.00001514
Iteration 201/1000 | Loss: 0.00001514
Iteration 202/1000 | Loss: 0.00001514
Iteration 203/1000 | Loss: 0.00001514
Iteration 204/1000 | Loss: 0.00001514
Iteration 205/1000 | Loss: 0.00001514
Iteration 206/1000 | Loss: 0.00001514
Iteration 207/1000 | Loss: 0.00001514
Iteration 208/1000 | Loss: 0.00001514
Iteration 209/1000 | Loss: 0.00001514
Iteration 210/1000 | Loss: 0.00001514
Iteration 211/1000 | Loss: 0.00001514
Iteration 212/1000 | Loss: 0.00001514
Iteration 213/1000 | Loss: 0.00001514
Iteration 214/1000 | Loss: 0.00001514
Iteration 215/1000 | Loss: 0.00001514
Iteration 216/1000 | Loss: 0.00001514
Iteration 217/1000 | Loss: 0.00001514
Iteration 218/1000 | Loss: 0.00001514
Iteration 219/1000 | Loss: 0.00001514
Iteration 220/1000 | Loss: 0.00001514
Iteration 221/1000 | Loss: 0.00001514
Iteration 222/1000 | Loss: 0.00001514
Iteration 223/1000 | Loss: 0.00001514
Iteration 224/1000 | Loss: 0.00001514
Iteration 225/1000 | Loss: 0.00001514
Iteration 226/1000 | Loss: 0.00001514
Iteration 227/1000 | Loss: 0.00001514
Iteration 228/1000 | Loss: 0.00001514
Iteration 229/1000 | Loss: 0.00001514
Iteration 230/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.5141134099394549e-05, 1.5141134099394549e-05, 1.5141134099394549e-05, 1.5141134099394549e-05, 1.5141134099394549e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5141134099394549e-05

Optimization complete. Final v2v error: 2.927009344100952 mm

Highest mean error: 19.81293487548828 mm for frame 49

Lowest mean error: 2.219320058822632 mm for frame 70

Saving results

Total time: 113.05060625076294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021813
Iteration 2/25 | Loss: 0.00219914
Iteration 3/25 | Loss: 0.00164494
Iteration 4/25 | Loss: 0.00159792
Iteration 5/25 | Loss: 0.00140572
Iteration 6/25 | Loss: 0.00130911
Iteration 7/25 | Loss: 0.00118065
Iteration 8/25 | Loss: 0.00115024
Iteration 9/25 | Loss: 0.00111614
Iteration 10/25 | Loss: 0.00111167
Iteration 11/25 | Loss: 0.00110922
Iteration 12/25 | Loss: 0.00111092
Iteration 13/25 | Loss: 0.00110557
Iteration 14/25 | Loss: 0.00110077
Iteration 15/25 | Loss: 0.00109877
Iteration 16/25 | Loss: 0.00109810
Iteration 17/25 | Loss: 0.00109745
Iteration 18/25 | Loss: 0.00109664
Iteration 19/25 | Loss: 0.00109905
Iteration 20/25 | Loss: 0.00109572
Iteration 21/25 | Loss: 0.00109379
Iteration 22/25 | Loss: 0.00109597
Iteration 23/25 | Loss: 0.00109295
Iteration 24/25 | Loss: 0.00109111
Iteration 25/25 | Loss: 0.00108996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38435197
Iteration 2/25 | Loss: 0.00259515
Iteration 3/25 | Loss: 0.00259522
Iteration 4/25 | Loss: 0.00257889
Iteration 5/25 | Loss: 0.00257889
Iteration 6/25 | Loss: 0.00257889
Iteration 7/25 | Loss: 0.00257889
Iteration 8/25 | Loss: 0.00257889
Iteration 9/25 | Loss: 0.00257889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.00257889274507761, 0.00257889274507761, 0.00257889274507761, 0.00257889274507761, 0.00257889274507761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00257889274507761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257889
Iteration 2/1000 | Loss: 0.00038687
Iteration 3/1000 | Loss: 0.00063447
Iteration 4/1000 | Loss: 0.00087332
Iteration 5/1000 | Loss: 0.00026303
Iteration 6/1000 | Loss: 0.00057986
Iteration 7/1000 | Loss: 0.00034643
Iteration 8/1000 | Loss: 0.00032925
Iteration 9/1000 | Loss: 0.00025770
Iteration 10/1000 | Loss: 0.00028334
Iteration 11/1000 | Loss: 0.00036947
Iteration 12/1000 | Loss: 0.00022930
Iteration 13/1000 | Loss: 0.00020460
Iteration 14/1000 | Loss: 0.00018295
Iteration 15/1000 | Loss: 0.00051110
Iteration 16/1000 | Loss: 0.00021624
Iteration 17/1000 | Loss: 0.00062057
Iteration 18/1000 | Loss: 0.00203019
Iteration 19/1000 | Loss: 0.00133225
Iteration 20/1000 | Loss: 0.00110584
Iteration 21/1000 | Loss: 0.00014169
Iteration 22/1000 | Loss: 0.00012815
Iteration 23/1000 | Loss: 0.00041362
Iteration 24/1000 | Loss: 0.00018557
Iteration 25/1000 | Loss: 0.00009549
Iteration 26/1000 | Loss: 0.00034038
Iteration 27/1000 | Loss: 0.00012335
Iteration 28/1000 | Loss: 0.00015647
Iteration 29/1000 | Loss: 0.00030407
Iteration 30/1000 | Loss: 0.00014237
Iteration 31/1000 | Loss: 0.00006363
Iteration 32/1000 | Loss: 0.00009065
Iteration 33/1000 | Loss: 0.00006125
Iteration 34/1000 | Loss: 0.00007893
Iteration 35/1000 | Loss: 0.00010993
Iteration 36/1000 | Loss: 0.00002303
Iteration 37/1000 | Loss: 0.00007231
Iteration 38/1000 | Loss: 0.00004185
Iteration 39/1000 | Loss: 0.00004998
Iteration 40/1000 | Loss: 0.00002909
Iteration 41/1000 | Loss: 0.00002504
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00006380
Iteration 44/1000 | Loss: 0.00003412
Iteration 45/1000 | Loss: 0.00002255
Iteration 46/1000 | Loss: 0.00002456
Iteration 47/1000 | Loss: 0.00015168
Iteration 48/1000 | Loss: 0.00006074
Iteration 49/1000 | Loss: 0.00003102
Iteration 50/1000 | Loss: 0.00003066
Iteration 51/1000 | Loss: 0.00002860
Iteration 52/1000 | Loss: 0.00007888
Iteration 53/1000 | Loss: 0.00003165
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00004079
Iteration 57/1000 | Loss: 0.00010384
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00002248
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001601
Iteration 63/1000 | Loss: 0.00001601
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001601
Iteration 67/1000 | Loss: 0.00001601
Iteration 68/1000 | Loss: 0.00001601
Iteration 69/1000 | Loss: 0.00001601
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001574
Iteration 82/1000 | Loss: 0.00003797
Iteration 83/1000 | Loss: 0.00003795
Iteration 84/1000 | Loss: 0.00010683
Iteration 85/1000 | Loss: 0.00004002
Iteration 86/1000 | Loss: 0.00005755
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00008662
Iteration 89/1000 | Loss: 0.00002963
Iteration 90/1000 | Loss: 0.00001592
Iteration 91/1000 | Loss: 0.00005896
Iteration 92/1000 | Loss: 0.00001583
Iteration 93/1000 | Loss: 0.00001544
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00005224
Iteration 97/1000 | Loss: 0.00002975
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00002072
Iteration 101/1000 | Loss: 0.00003996
Iteration 102/1000 | Loss: 0.00003224
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001437
Iteration 106/1000 | Loss: 0.00001437
Iteration 107/1000 | Loss: 0.00001437
Iteration 108/1000 | Loss: 0.00001436
Iteration 109/1000 | Loss: 0.00001436
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00004062
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001432
Iteration 131/1000 | Loss: 0.00001432
Iteration 132/1000 | Loss: 0.00001432
Iteration 133/1000 | Loss: 0.00001432
Iteration 134/1000 | Loss: 0.00001432
Iteration 135/1000 | Loss: 0.00001432
Iteration 136/1000 | Loss: 0.00001432
Iteration 137/1000 | Loss: 0.00001432
Iteration 138/1000 | Loss: 0.00001432
Iteration 139/1000 | Loss: 0.00001431
Iteration 140/1000 | Loss: 0.00001431
Iteration 141/1000 | Loss: 0.00001431
Iteration 142/1000 | Loss: 0.00001431
Iteration 143/1000 | Loss: 0.00001431
Iteration 144/1000 | Loss: 0.00001431
Iteration 145/1000 | Loss: 0.00001431
Iteration 146/1000 | Loss: 0.00001431
Iteration 147/1000 | Loss: 0.00001431
Iteration 148/1000 | Loss: 0.00001431
Iteration 149/1000 | Loss: 0.00001431
Iteration 150/1000 | Loss: 0.00001431
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001430
Iteration 163/1000 | Loss: 0.00001430
Iteration 164/1000 | Loss: 0.00001430
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001430
Iteration 176/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4303966963780113e-05, 1.4303966963780113e-05, 1.4303966963780113e-05, 1.4303966963780113e-05, 1.4303966963780113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4303966963780113e-05

Optimization complete. Final v2v error: 3.164151668548584 mm

Highest mean error: 4.470404148101807 mm for frame 108

Lowest mean error: 3.0090179443359375 mm for frame 9

Saving results

Total time: 190.8751335144043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377353
Iteration 2/25 | Loss: 0.00091591
Iteration 3/25 | Loss: 0.00080668
Iteration 4/25 | Loss: 0.00079089
Iteration 5/25 | Loss: 0.00078580
Iteration 6/25 | Loss: 0.00078468
Iteration 7/25 | Loss: 0.00078429
Iteration 8/25 | Loss: 0.00078429
Iteration 9/25 | Loss: 0.00078429
Iteration 10/25 | Loss: 0.00078429
Iteration 11/25 | Loss: 0.00078429
Iteration 12/25 | Loss: 0.00078429
Iteration 13/25 | Loss: 0.00078429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007842893828637898, 0.0007842893828637898, 0.0007842893828637898, 0.0007842893828637898, 0.0007842893828637898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007842893828637898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35197711
Iteration 2/25 | Loss: 0.00039429
Iteration 3/25 | Loss: 0.00039429
Iteration 4/25 | Loss: 0.00039429
Iteration 5/25 | Loss: 0.00039429
Iteration 6/25 | Loss: 0.00039429
Iteration 7/25 | Loss: 0.00039428
Iteration 8/25 | Loss: 0.00039428
Iteration 9/25 | Loss: 0.00039428
Iteration 10/25 | Loss: 0.00039428
Iteration 11/25 | Loss: 0.00039428
Iteration 12/25 | Loss: 0.00039428
Iteration 13/25 | Loss: 0.00039428
Iteration 14/25 | Loss: 0.00039428
Iteration 15/25 | Loss: 0.00039428
Iteration 16/25 | Loss: 0.00039428
Iteration 17/25 | Loss: 0.00039428
Iteration 18/25 | Loss: 0.00039428
Iteration 19/25 | Loss: 0.00039428
Iteration 20/25 | Loss: 0.00039428
Iteration 21/25 | Loss: 0.00039428
Iteration 22/25 | Loss: 0.00039428
Iteration 23/25 | Loss: 0.00039428
Iteration 24/25 | Loss: 0.00039428
Iteration 25/25 | Loss: 0.00039428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039428
Iteration 2/1000 | Loss: 0.00003354
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001054
Iteration 7/1000 | Loss: 0.00000994
Iteration 8/1000 | Loss: 0.00000962
Iteration 9/1000 | Loss: 0.00000938
Iteration 10/1000 | Loss: 0.00000917
Iteration 11/1000 | Loss: 0.00000905
Iteration 12/1000 | Loss: 0.00000895
Iteration 13/1000 | Loss: 0.00000893
Iteration 14/1000 | Loss: 0.00000885
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000884
Iteration 17/1000 | Loss: 0.00000883
Iteration 18/1000 | Loss: 0.00000882
Iteration 19/1000 | Loss: 0.00000882
Iteration 20/1000 | Loss: 0.00000881
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000880
Iteration 24/1000 | Loss: 0.00000879
Iteration 25/1000 | Loss: 0.00000879
Iteration 26/1000 | Loss: 0.00000879
Iteration 27/1000 | Loss: 0.00000878
Iteration 28/1000 | Loss: 0.00000878
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000875
Iteration 32/1000 | Loss: 0.00000874
Iteration 33/1000 | Loss: 0.00000874
Iteration 34/1000 | Loss: 0.00000874
Iteration 35/1000 | Loss: 0.00000873
Iteration 36/1000 | Loss: 0.00000873
Iteration 37/1000 | Loss: 0.00000872
Iteration 38/1000 | Loss: 0.00000872
Iteration 39/1000 | Loss: 0.00000872
Iteration 40/1000 | Loss: 0.00000871
Iteration 41/1000 | Loss: 0.00000871
Iteration 42/1000 | Loss: 0.00000871
Iteration 43/1000 | Loss: 0.00000870
Iteration 44/1000 | Loss: 0.00000870
Iteration 45/1000 | Loss: 0.00000870
Iteration 46/1000 | Loss: 0.00000869
Iteration 47/1000 | Loss: 0.00000869
Iteration 48/1000 | Loss: 0.00000868
Iteration 49/1000 | Loss: 0.00000868
Iteration 50/1000 | Loss: 0.00000868
Iteration 51/1000 | Loss: 0.00000867
Iteration 52/1000 | Loss: 0.00000867
Iteration 53/1000 | Loss: 0.00000867
Iteration 54/1000 | Loss: 0.00000867
Iteration 55/1000 | Loss: 0.00000867
Iteration 56/1000 | Loss: 0.00000866
Iteration 57/1000 | Loss: 0.00000866
Iteration 58/1000 | Loss: 0.00000866
Iteration 59/1000 | Loss: 0.00000865
Iteration 60/1000 | Loss: 0.00000865
Iteration 61/1000 | Loss: 0.00000864
Iteration 62/1000 | Loss: 0.00000864
Iteration 63/1000 | Loss: 0.00000864
Iteration 64/1000 | Loss: 0.00000863
Iteration 65/1000 | Loss: 0.00000862
Iteration 66/1000 | Loss: 0.00000862
Iteration 67/1000 | Loss: 0.00000862
Iteration 68/1000 | Loss: 0.00000861
Iteration 69/1000 | Loss: 0.00000861
Iteration 70/1000 | Loss: 0.00000861
Iteration 71/1000 | Loss: 0.00000861
Iteration 72/1000 | Loss: 0.00000861
Iteration 73/1000 | Loss: 0.00000860
Iteration 74/1000 | Loss: 0.00000860
Iteration 75/1000 | Loss: 0.00000860
Iteration 76/1000 | Loss: 0.00000860
Iteration 77/1000 | Loss: 0.00000860
Iteration 78/1000 | Loss: 0.00000859
Iteration 79/1000 | Loss: 0.00000859
Iteration 80/1000 | Loss: 0.00000859
Iteration 81/1000 | Loss: 0.00000859
Iteration 82/1000 | Loss: 0.00000859
Iteration 83/1000 | Loss: 0.00000859
Iteration 84/1000 | Loss: 0.00000858
Iteration 85/1000 | Loss: 0.00000858
Iteration 86/1000 | Loss: 0.00000858
Iteration 87/1000 | Loss: 0.00000858
Iteration 88/1000 | Loss: 0.00000858
Iteration 89/1000 | Loss: 0.00000858
Iteration 90/1000 | Loss: 0.00000858
Iteration 91/1000 | Loss: 0.00000858
Iteration 92/1000 | Loss: 0.00000857
Iteration 93/1000 | Loss: 0.00000857
Iteration 94/1000 | Loss: 0.00000857
Iteration 95/1000 | Loss: 0.00000857
Iteration 96/1000 | Loss: 0.00000857
Iteration 97/1000 | Loss: 0.00000856
Iteration 98/1000 | Loss: 0.00000856
Iteration 99/1000 | Loss: 0.00000856
Iteration 100/1000 | Loss: 0.00000856
Iteration 101/1000 | Loss: 0.00000856
Iteration 102/1000 | Loss: 0.00000856
Iteration 103/1000 | Loss: 0.00000856
Iteration 104/1000 | Loss: 0.00000855
Iteration 105/1000 | Loss: 0.00000855
Iteration 106/1000 | Loss: 0.00000855
Iteration 107/1000 | Loss: 0.00000855
Iteration 108/1000 | Loss: 0.00000855
Iteration 109/1000 | Loss: 0.00000855
Iteration 110/1000 | Loss: 0.00000855
Iteration 111/1000 | Loss: 0.00000855
Iteration 112/1000 | Loss: 0.00000855
Iteration 113/1000 | Loss: 0.00000855
Iteration 114/1000 | Loss: 0.00000855
Iteration 115/1000 | Loss: 0.00000855
Iteration 116/1000 | Loss: 0.00000855
Iteration 117/1000 | Loss: 0.00000855
Iteration 118/1000 | Loss: 0.00000855
Iteration 119/1000 | Loss: 0.00000855
Iteration 120/1000 | Loss: 0.00000855
Iteration 121/1000 | Loss: 0.00000855
Iteration 122/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [8.54759036883479e-06, 8.54759036883479e-06, 8.54759036883479e-06, 8.54759036883479e-06, 8.54759036883479e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.54759036883479e-06

Optimization complete. Final v2v error: 2.4690394401550293 mm

Highest mean error: 2.9031782150268555 mm for frame 40

Lowest mean error: 2.1644723415374756 mm for frame 101

Saving results

Total time: 37.440826177597046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00277436
Iteration 2/25 | Loss: 0.00106346
Iteration 3/25 | Loss: 0.00086296
Iteration 4/25 | Loss: 0.00083514
Iteration 5/25 | Loss: 0.00082714
Iteration 6/25 | Loss: 0.00082390
Iteration 7/25 | Loss: 0.00082304
Iteration 8/25 | Loss: 0.00082262
Iteration 9/25 | Loss: 0.00082251
Iteration 10/25 | Loss: 0.00082251
Iteration 11/25 | Loss: 0.00082251
Iteration 12/25 | Loss: 0.00082251
Iteration 13/25 | Loss: 0.00082251
Iteration 14/25 | Loss: 0.00082251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008225090568885207, 0.0008225090568885207, 0.0008225090568885207, 0.0008225090568885207, 0.0008225090568885207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008225090568885207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35252678
Iteration 2/25 | Loss: 0.00051411
Iteration 3/25 | Loss: 0.00051411
Iteration 4/25 | Loss: 0.00051411
Iteration 5/25 | Loss: 0.00051411
Iteration 6/25 | Loss: 0.00051411
Iteration 7/25 | Loss: 0.00051411
Iteration 8/25 | Loss: 0.00051411
Iteration 9/25 | Loss: 0.00051411
Iteration 10/25 | Loss: 0.00051411
Iteration 11/25 | Loss: 0.00051411
Iteration 12/25 | Loss: 0.00051411
Iteration 13/25 | Loss: 0.00051411
Iteration 14/25 | Loss: 0.00051411
Iteration 15/25 | Loss: 0.00051411
Iteration 16/25 | Loss: 0.00051411
Iteration 17/25 | Loss: 0.00051411
Iteration 18/25 | Loss: 0.00051411
Iteration 19/25 | Loss: 0.00051411
Iteration 20/25 | Loss: 0.00051411
Iteration 21/25 | Loss: 0.00051411
Iteration 22/25 | Loss: 0.00051411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005141100846230984, 0.0005141100846230984, 0.0005141100846230984, 0.0005141100846230984, 0.0005141100846230984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005141100846230984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051411
Iteration 2/1000 | Loss: 0.00002989
Iteration 3/1000 | Loss: 0.00001731
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001129
Iteration 7/1000 | Loss: 0.00001091
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001066
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001046
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001025
Iteration 16/1000 | Loss: 0.00001024
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001014
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001011
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001008
Iteration 30/1000 | Loss: 0.00001008
Iteration 31/1000 | Loss: 0.00001007
Iteration 32/1000 | Loss: 0.00001006
Iteration 33/1000 | Loss: 0.00001006
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001005
Iteration 36/1000 | Loss: 0.00001005
Iteration 37/1000 | Loss: 0.00001004
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001003
Iteration 41/1000 | Loss: 0.00001002
Iteration 42/1000 | Loss: 0.00001001
Iteration 43/1000 | Loss: 0.00001000
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000996
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000996
Iteration 54/1000 | Loss: 0.00000995
Iteration 55/1000 | Loss: 0.00000995
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000992
Iteration 59/1000 | Loss: 0.00000992
Iteration 60/1000 | Loss: 0.00000992
Iteration 61/1000 | Loss: 0.00000992
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000991
Iteration 64/1000 | Loss: 0.00000991
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000990
Iteration 67/1000 | Loss: 0.00000990
Iteration 68/1000 | Loss: 0.00000990
Iteration 69/1000 | Loss: 0.00000990
Iteration 70/1000 | Loss: 0.00000989
Iteration 71/1000 | Loss: 0.00000989
Iteration 72/1000 | Loss: 0.00000989
Iteration 73/1000 | Loss: 0.00000989
Iteration 74/1000 | Loss: 0.00000988
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000985
Iteration 93/1000 | Loss: 0.00000985
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000984
Iteration 99/1000 | Loss: 0.00000984
Iteration 100/1000 | Loss: 0.00000984
Iteration 101/1000 | Loss: 0.00000984
Iteration 102/1000 | Loss: 0.00000983
Iteration 103/1000 | Loss: 0.00000983
Iteration 104/1000 | Loss: 0.00000983
Iteration 105/1000 | Loss: 0.00000983
Iteration 106/1000 | Loss: 0.00000983
Iteration 107/1000 | Loss: 0.00000983
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000982
Iteration 110/1000 | Loss: 0.00000982
Iteration 111/1000 | Loss: 0.00000982
Iteration 112/1000 | Loss: 0.00000982
Iteration 113/1000 | Loss: 0.00000982
Iteration 114/1000 | Loss: 0.00000982
Iteration 115/1000 | Loss: 0.00000982
Iteration 116/1000 | Loss: 0.00000982
Iteration 117/1000 | Loss: 0.00000981
Iteration 118/1000 | Loss: 0.00000981
Iteration 119/1000 | Loss: 0.00000981
Iteration 120/1000 | Loss: 0.00000980
Iteration 121/1000 | Loss: 0.00000980
Iteration 122/1000 | Loss: 0.00000980
Iteration 123/1000 | Loss: 0.00000980
Iteration 124/1000 | Loss: 0.00000980
Iteration 125/1000 | Loss: 0.00000980
Iteration 126/1000 | Loss: 0.00000980
Iteration 127/1000 | Loss: 0.00000980
Iteration 128/1000 | Loss: 0.00000980
Iteration 129/1000 | Loss: 0.00000980
Iteration 130/1000 | Loss: 0.00000980
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000979
Iteration 133/1000 | Loss: 0.00000979
Iteration 134/1000 | Loss: 0.00000979
Iteration 135/1000 | Loss: 0.00000979
Iteration 136/1000 | Loss: 0.00000979
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000978
Iteration 143/1000 | Loss: 0.00000978
Iteration 144/1000 | Loss: 0.00000978
Iteration 145/1000 | Loss: 0.00000978
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000977
Iteration 148/1000 | Loss: 0.00000977
Iteration 149/1000 | Loss: 0.00000977
Iteration 150/1000 | Loss: 0.00000977
Iteration 151/1000 | Loss: 0.00000977
Iteration 152/1000 | Loss: 0.00000977
Iteration 153/1000 | Loss: 0.00000977
Iteration 154/1000 | Loss: 0.00000977
Iteration 155/1000 | Loss: 0.00000977
Iteration 156/1000 | Loss: 0.00000977
Iteration 157/1000 | Loss: 0.00000977
Iteration 158/1000 | Loss: 0.00000976
Iteration 159/1000 | Loss: 0.00000976
Iteration 160/1000 | Loss: 0.00000976
Iteration 161/1000 | Loss: 0.00000976
Iteration 162/1000 | Loss: 0.00000976
Iteration 163/1000 | Loss: 0.00000976
Iteration 164/1000 | Loss: 0.00000976
Iteration 165/1000 | Loss: 0.00000976
Iteration 166/1000 | Loss: 0.00000976
Iteration 167/1000 | Loss: 0.00000976
Iteration 168/1000 | Loss: 0.00000976
Iteration 169/1000 | Loss: 0.00000976
Iteration 170/1000 | Loss: 0.00000976
Iteration 171/1000 | Loss: 0.00000976
Iteration 172/1000 | Loss: 0.00000976
Iteration 173/1000 | Loss: 0.00000975
Iteration 174/1000 | Loss: 0.00000975
Iteration 175/1000 | Loss: 0.00000975
Iteration 176/1000 | Loss: 0.00000975
Iteration 177/1000 | Loss: 0.00000975
Iteration 178/1000 | Loss: 0.00000975
Iteration 179/1000 | Loss: 0.00000975
Iteration 180/1000 | Loss: 0.00000975
Iteration 181/1000 | Loss: 0.00000975
Iteration 182/1000 | Loss: 0.00000975
Iteration 183/1000 | Loss: 0.00000975
Iteration 184/1000 | Loss: 0.00000975
Iteration 185/1000 | Loss: 0.00000975
Iteration 186/1000 | Loss: 0.00000975
Iteration 187/1000 | Loss: 0.00000975
Iteration 188/1000 | Loss: 0.00000975
Iteration 189/1000 | Loss: 0.00000975
Iteration 190/1000 | Loss: 0.00000975
Iteration 191/1000 | Loss: 0.00000974
Iteration 192/1000 | Loss: 0.00000974
Iteration 193/1000 | Loss: 0.00000974
Iteration 194/1000 | Loss: 0.00000974
Iteration 195/1000 | Loss: 0.00000974
Iteration 196/1000 | Loss: 0.00000974
Iteration 197/1000 | Loss: 0.00000974
Iteration 198/1000 | Loss: 0.00000974
Iteration 199/1000 | Loss: 0.00000974
Iteration 200/1000 | Loss: 0.00000974
Iteration 201/1000 | Loss: 0.00000974
Iteration 202/1000 | Loss: 0.00000974
Iteration 203/1000 | Loss: 0.00000974
Iteration 204/1000 | Loss: 0.00000974
Iteration 205/1000 | Loss: 0.00000974
Iteration 206/1000 | Loss: 0.00000974
Iteration 207/1000 | Loss: 0.00000974
Iteration 208/1000 | Loss: 0.00000974
Iteration 209/1000 | Loss: 0.00000974
Iteration 210/1000 | Loss: 0.00000974
Iteration 211/1000 | Loss: 0.00000973
Iteration 212/1000 | Loss: 0.00000973
Iteration 213/1000 | Loss: 0.00000973
Iteration 214/1000 | Loss: 0.00000973
Iteration 215/1000 | Loss: 0.00000973
Iteration 216/1000 | Loss: 0.00000973
Iteration 217/1000 | Loss: 0.00000973
Iteration 218/1000 | Loss: 0.00000973
Iteration 219/1000 | Loss: 0.00000973
Iteration 220/1000 | Loss: 0.00000973
Iteration 221/1000 | Loss: 0.00000973
Iteration 222/1000 | Loss: 0.00000973
Iteration 223/1000 | Loss: 0.00000973
Iteration 224/1000 | Loss: 0.00000973
Iteration 225/1000 | Loss: 0.00000973
Iteration 226/1000 | Loss: 0.00000972
Iteration 227/1000 | Loss: 0.00000972
Iteration 228/1000 | Loss: 0.00000972
Iteration 229/1000 | Loss: 0.00000972
Iteration 230/1000 | Loss: 0.00000972
Iteration 231/1000 | Loss: 0.00000972
Iteration 232/1000 | Loss: 0.00000972
Iteration 233/1000 | Loss: 0.00000972
Iteration 234/1000 | Loss: 0.00000972
Iteration 235/1000 | Loss: 0.00000972
Iteration 236/1000 | Loss: 0.00000972
Iteration 237/1000 | Loss: 0.00000972
Iteration 238/1000 | Loss: 0.00000972
Iteration 239/1000 | Loss: 0.00000972
Iteration 240/1000 | Loss: 0.00000972
Iteration 241/1000 | Loss: 0.00000972
Iteration 242/1000 | Loss: 0.00000972
Iteration 243/1000 | Loss: 0.00000972
Iteration 244/1000 | Loss: 0.00000972
Iteration 245/1000 | Loss: 0.00000971
Iteration 246/1000 | Loss: 0.00000971
Iteration 247/1000 | Loss: 0.00000971
Iteration 248/1000 | Loss: 0.00000971
Iteration 249/1000 | Loss: 0.00000971
Iteration 250/1000 | Loss: 0.00000971
Iteration 251/1000 | Loss: 0.00000971
Iteration 252/1000 | Loss: 0.00000971
Iteration 253/1000 | Loss: 0.00000971
Iteration 254/1000 | Loss: 0.00000971
Iteration 255/1000 | Loss: 0.00000971
Iteration 256/1000 | Loss: 0.00000971
Iteration 257/1000 | Loss: 0.00000971
Iteration 258/1000 | Loss: 0.00000970
Iteration 259/1000 | Loss: 0.00000970
Iteration 260/1000 | Loss: 0.00000970
Iteration 261/1000 | Loss: 0.00000970
Iteration 262/1000 | Loss: 0.00000970
Iteration 263/1000 | Loss: 0.00000970
Iteration 264/1000 | Loss: 0.00000970
Iteration 265/1000 | Loss: 0.00000970
Iteration 266/1000 | Loss: 0.00000970
Iteration 267/1000 | Loss: 0.00000970
Iteration 268/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [9.701780982140917e-06, 9.701780982140917e-06, 9.701780982140917e-06, 9.701780982140917e-06, 9.701780982140917e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.701780982140917e-06

Optimization complete. Final v2v error: 2.5941929817199707 mm

Highest mean error: 3.355933666229248 mm for frame 107

Lowest mean error: 2.2101190090179443 mm for frame 13

Saving results

Total time: 44.71953105926514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869839
Iteration 2/25 | Loss: 0.00104683
Iteration 3/25 | Loss: 0.00093181
Iteration 4/25 | Loss: 0.00091298
Iteration 5/25 | Loss: 0.00090482
Iteration 6/25 | Loss: 0.00090290
Iteration 7/25 | Loss: 0.00090290
Iteration 8/25 | Loss: 0.00090290
Iteration 9/25 | Loss: 0.00090290
Iteration 10/25 | Loss: 0.00090290
Iteration 11/25 | Loss: 0.00090290
Iteration 12/25 | Loss: 0.00090290
Iteration 13/25 | Loss: 0.00090290
Iteration 14/25 | Loss: 0.00090290
Iteration 15/25 | Loss: 0.00090290
Iteration 16/25 | Loss: 0.00090290
Iteration 17/25 | Loss: 0.00090290
Iteration 18/25 | Loss: 0.00090290
Iteration 19/25 | Loss: 0.00090290
Iteration 20/25 | Loss: 0.00090290
Iteration 21/25 | Loss: 0.00090290
Iteration 22/25 | Loss: 0.00090290
Iteration 23/25 | Loss: 0.00090290
Iteration 24/25 | Loss: 0.00090290
Iteration 25/25 | Loss: 0.00090290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37246156
Iteration 2/25 | Loss: 0.00062464
Iteration 3/25 | Loss: 0.00062464
Iteration 4/25 | Loss: 0.00062464
Iteration 5/25 | Loss: 0.00062464
Iteration 6/25 | Loss: 0.00062464
Iteration 7/25 | Loss: 0.00062464
Iteration 8/25 | Loss: 0.00062464
Iteration 9/25 | Loss: 0.00062464
Iteration 10/25 | Loss: 0.00062464
Iteration 11/25 | Loss: 0.00062464
Iteration 12/25 | Loss: 0.00062464
Iteration 13/25 | Loss: 0.00062464
Iteration 14/25 | Loss: 0.00062464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006246364791877568, 0.0006246364791877568, 0.0006246364791877568, 0.0006246364791877568, 0.0006246364791877568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006246364791877568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062464
Iteration 2/1000 | Loss: 0.00002438
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001662
Iteration 8/1000 | Loss: 0.00001661
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001609
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001608
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001599
Iteration 17/1000 | Loss: 0.00001597
Iteration 18/1000 | Loss: 0.00001597
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001593
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001587
Iteration 32/1000 | Loss: 0.00001587
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001586
Iteration 36/1000 | Loss: 0.00001586
Iteration 37/1000 | Loss: 0.00001585
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001585
Iteration 41/1000 | Loss: 0.00001585
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001584
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001583
Iteration 47/1000 | Loss: 0.00001583
Iteration 48/1000 | Loss: 0.00001583
Iteration 49/1000 | Loss: 0.00001582
Iteration 50/1000 | Loss: 0.00001582
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001582
Iteration 54/1000 | Loss: 0.00001582
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001582
Iteration 57/1000 | Loss: 0.00001582
Iteration 58/1000 | Loss: 0.00001582
Iteration 59/1000 | Loss: 0.00001582
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001581
Iteration 64/1000 | Loss: 0.00001581
Iteration 65/1000 | Loss: 0.00001581
Iteration 66/1000 | Loss: 0.00001581
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001581
Iteration 73/1000 | Loss: 0.00001581
Iteration 74/1000 | Loss: 0.00001581
Iteration 75/1000 | Loss: 0.00001581
Iteration 76/1000 | Loss: 0.00001581
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001580
Iteration 81/1000 | Loss: 0.00001580
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001580
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001579
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001578
Iteration 93/1000 | Loss: 0.00001578
Iteration 94/1000 | Loss: 0.00001578
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001577
Iteration 97/1000 | Loss: 0.00001577
Iteration 98/1000 | Loss: 0.00001577
Iteration 99/1000 | Loss: 0.00001577
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001577
Iteration 106/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.5772782717249356e-05, 1.5772782717249356e-05, 1.5772782717249356e-05, 1.5772782717249356e-05, 1.5772782717249356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5772782717249356e-05

Optimization complete. Final v2v error: 3.327415943145752 mm

Highest mean error: 3.3749704360961914 mm for frame 192

Lowest mean error: 3.275348424911499 mm for frame 213

Saving results

Total time: 32.88704323768616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063520
Iteration 2/25 | Loss: 0.00190112
Iteration 3/25 | Loss: 0.00161747
Iteration 4/25 | Loss: 0.00171065
Iteration 5/25 | Loss: 0.00115024
Iteration 6/25 | Loss: 0.00111577
Iteration 7/25 | Loss: 0.00094594
Iteration 8/25 | Loss: 0.00088684
Iteration 9/25 | Loss: 0.00085862
Iteration 10/25 | Loss: 0.00084177
Iteration 11/25 | Loss: 0.00083260
Iteration 12/25 | Loss: 0.00083680
Iteration 13/25 | Loss: 0.00083456
Iteration 14/25 | Loss: 0.00083369
Iteration 15/25 | Loss: 0.00083089
Iteration 16/25 | Loss: 0.00083029
Iteration 17/25 | Loss: 0.00082661
Iteration 18/25 | Loss: 0.00082325
Iteration 19/25 | Loss: 0.00082549
Iteration 20/25 | Loss: 0.00082478
Iteration 21/25 | Loss: 0.00082321
Iteration 22/25 | Loss: 0.00082284
Iteration 23/25 | Loss: 0.00082201
Iteration 24/25 | Loss: 0.00081976
Iteration 25/25 | Loss: 0.00081951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42189705
Iteration 2/25 | Loss: 0.00072597
Iteration 3/25 | Loss: 0.00063436
Iteration 4/25 | Loss: 0.00063436
Iteration 5/25 | Loss: 0.00063436
Iteration 6/25 | Loss: 0.00063436
Iteration 7/25 | Loss: 0.00063436
Iteration 8/25 | Loss: 0.00063436
Iteration 9/25 | Loss: 0.00063436
Iteration 10/25 | Loss: 0.00063436
Iteration 11/25 | Loss: 0.00063436
Iteration 12/25 | Loss: 0.00063436
Iteration 13/25 | Loss: 0.00063436
Iteration 14/25 | Loss: 0.00063436
Iteration 15/25 | Loss: 0.00063436
Iteration 16/25 | Loss: 0.00063436
Iteration 17/25 | Loss: 0.00063436
Iteration 18/25 | Loss: 0.00063436
Iteration 19/25 | Loss: 0.00063436
Iteration 20/25 | Loss: 0.00063436
Iteration 21/25 | Loss: 0.00063436
Iteration 22/25 | Loss: 0.00063436
Iteration 23/25 | Loss: 0.00063436
Iteration 24/25 | Loss: 0.00063436
Iteration 25/25 | Loss: 0.00063436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000634360418189317, 0.000634360418189317, 0.000634360418189317, 0.000634360418189317, 0.000634360418189317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000634360418189317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063436
Iteration 2/1000 | Loss: 0.00007094
Iteration 3/1000 | Loss: 0.00036655
Iteration 4/1000 | Loss: 0.00009891
Iteration 5/1000 | Loss: 0.00002931
Iteration 6/1000 | Loss: 0.00002437
Iteration 7/1000 | Loss: 0.00005515
Iteration 8/1000 | Loss: 0.00016997
Iteration 9/1000 | Loss: 0.00007843
Iteration 10/1000 | Loss: 0.00005392
Iteration 11/1000 | Loss: 0.00004685
Iteration 12/1000 | Loss: 0.00004905
Iteration 13/1000 | Loss: 0.00002110
Iteration 14/1000 | Loss: 0.00359382
Iteration 15/1000 | Loss: 0.00280176
Iteration 16/1000 | Loss: 0.00011635
Iteration 17/1000 | Loss: 0.00004208
Iteration 18/1000 | Loss: 0.00004145
Iteration 19/1000 | Loss: 0.00003893
Iteration 20/1000 | Loss: 0.00380207
Iteration 21/1000 | Loss: 0.00026290
Iteration 22/1000 | Loss: 0.00181792
Iteration 23/1000 | Loss: 0.00043207
Iteration 24/1000 | Loss: 0.00058853
Iteration 25/1000 | Loss: 0.00007253
Iteration 26/1000 | Loss: 0.00003226
Iteration 27/1000 | Loss: 0.00002117
Iteration 28/1000 | Loss: 0.00003431
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00002086
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00002010
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001538
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00004015
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001526
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001522
Iteration 56/1000 | Loss: 0.00001522
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00003478
Iteration 89/1000 | Loss: 0.00003377
Iteration 90/1000 | Loss: 0.00001812
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00002722
Iteration 94/1000 | Loss: 0.00002196
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001497
Iteration 102/1000 | Loss: 0.00001497
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001497
Iteration 105/1000 | Loss: 0.00001497
Iteration 106/1000 | Loss: 0.00001497
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Iteration 131/1000 | Loss: 0.00001497
Iteration 132/1000 | Loss: 0.00001497
Iteration 133/1000 | Loss: 0.00001497
Iteration 134/1000 | Loss: 0.00001497
Iteration 135/1000 | Loss: 0.00001497
Iteration 136/1000 | Loss: 0.00001497
Iteration 137/1000 | Loss: 0.00001497
Iteration 138/1000 | Loss: 0.00001497
Iteration 139/1000 | Loss: 0.00001497
Iteration 140/1000 | Loss: 0.00001497
Iteration 141/1000 | Loss: 0.00001497
Iteration 142/1000 | Loss: 0.00001497
Iteration 143/1000 | Loss: 0.00001497
Iteration 144/1000 | Loss: 0.00001497
Iteration 145/1000 | Loss: 0.00001497
Iteration 146/1000 | Loss: 0.00001497
Iteration 147/1000 | Loss: 0.00001497
Iteration 148/1000 | Loss: 0.00001497
Iteration 149/1000 | Loss: 0.00001497
Iteration 150/1000 | Loss: 0.00001497
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001497
Iteration 153/1000 | Loss: 0.00001497
Iteration 154/1000 | Loss: 0.00001497
Iteration 155/1000 | Loss: 0.00001497
Iteration 156/1000 | Loss: 0.00001497
Iteration 157/1000 | Loss: 0.00001497
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001497
Iteration 163/1000 | Loss: 0.00001497
Iteration 164/1000 | Loss: 0.00001497
Iteration 165/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.4973562429076992e-05, 1.4973562429076992e-05, 1.4973562429076992e-05, 1.4973562429076992e-05, 1.4973562429076992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4973562429076992e-05

Optimization complete. Final v2v error: 2.44049072265625 mm

Highest mean error: 20.327342987060547 mm for frame 100

Lowest mean error: 1.935791254043579 mm for frame 124

Saving results

Total time: 115.98300814628601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989137
Iteration 2/25 | Loss: 0.00161380
Iteration 3/25 | Loss: 0.00116271
Iteration 4/25 | Loss: 0.00114400
Iteration 5/25 | Loss: 0.00113976
Iteration 6/25 | Loss: 0.00113846
Iteration 7/25 | Loss: 0.00113840
Iteration 8/25 | Loss: 0.00113840
Iteration 9/25 | Loss: 0.00113840
Iteration 10/25 | Loss: 0.00113840
Iteration 11/25 | Loss: 0.00113840
Iteration 12/25 | Loss: 0.00113840
Iteration 13/25 | Loss: 0.00113840
Iteration 14/25 | Loss: 0.00113840
Iteration 15/25 | Loss: 0.00113840
Iteration 16/25 | Loss: 0.00113840
Iteration 17/25 | Loss: 0.00113840
Iteration 18/25 | Loss: 0.00113840
Iteration 19/25 | Loss: 0.00113840
Iteration 20/25 | Loss: 0.00113840
Iteration 21/25 | Loss: 0.00113840
Iteration 22/25 | Loss: 0.00113840
Iteration 23/25 | Loss: 0.00113840
Iteration 24/25 | Loss: 0.00113840
Iteration 25/25 | Loss: 0.00113840

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24673247
Iteration 2/25 | Loss: 0.00057107
Iteration 3/25 | Loss: 0.00057107
Iteration 4/25 | Loss: 0.00057107
Iteration 5/25 | Loss: 0.00057107
Iteration 6/25 | Loss: 0.00057107
Iteration 7/25 | Loss: 0.00057107
Iteration 8/25 | Loss: 0.00057107
Iteration 9/25 | Loss: 0.00057107
Iteration 10/25 | Loss: 0.00057107
Iteration 11/25 | Loss: 0.00057107
Iteration 12/25 | Loss: 0.00057107
Iteration 13/25 | Loss: 0.00057107
Iteration 14/25 | Loss: 0.00057107
Iteration 15/25 | Loss: 0.00057107
Iteration 16/25 | Loss: 0.00057107
Iteration 17/25 | Loss: 0.00057107
Iteration 18/25 | Loss: 0.00057107
Iteration 19/25 | Loss: 0.00057107
Iteration 20/25 | Loss: 0.00057107
Iteration 21/25 | Loss: 0.00057107
Iteration 22/25 | Loss: 0.00057107
Iteration 23/25 | Loss: 0.00057107
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005710671539418399, 0.0005710671539418399, 0.0005710671539418399, 0.0005710671539418399, 0.0005710671539418399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005710671539418399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057107
Iteration 2/1000 | Loss: 0.00012326
Iteration 3/1000 | Loss: 0.00006229
Iteration 4/1000 | Loss: 0.00004963
Iteration 5/1000 | Loss: 0.00004670
Iteration 6/1000 | Loss: 0.00004459
Iteration 7/1000 | Loss: 0.00004346
Iteration 8/1000 | Loss: 0.00004244
Iteration 9/1000 | Loss: 0.00004125
Iteration 10/1000 | Loss: 0.00004045
Iteration 11/1000 | Loss: 0.00003978
Iteration 12/1000 | Loss: 0.00003914
Iteration 13/1000 | Loss: 0.00003863
Iteration 14/1000 | Loss: 0.00003814
Iteration 15/1000 | Loss: 0.00003783
Iteration 16/1000 | Loss: 0.00003761
Iteration 17/1000 | Loss: 0.00003744
Iteration 18/1000 | Loss: 0.00003742
Iteration 19/1000 | Loss: 0.00003725
Iteration 20/1000 | Loss: 0.00003720
Iteration 21/1000 | Loss: 0.00003700
Iteration 22/1000 | Loss: 0.00003683
Iteration 23/1000 | Loss: 0.00003663
Iteration 24/1000 | Loss: 0.00003652
Iteration 25/1000 | Loss: 0.00003647
Iteration 26/1000 | Loss: 0.00003631
Iteration 27/1000 | Loss: 0.00003630
Iteration 28/1000 | Loss: 0.00003627
Iteration 29/1000 | Loss: 0.00003627
Iteration 30/1000 | Loss: 0.00003626
Iteration 31/1000 | Loss: 0.00003625
Iteration 32/1000 | Loss: 0.00003622
Iteration 33/1000 | Loss: 0.00003622
Iteration 34/1000 | Loss: 0.00003622
Iteration 35/1000 | Loss: 0.00003622
Iteration 36/1000 | Loss: 0.00003622
Iteration 37/1000 | Loss: 0.00003622
Iteration 38/1000 | Loss: 0.00003622
Iteration 39/1000 | Loss: 0.00003622
Iteration 40/1000 | Loss: 0.00003622
Iteration 41/1000 | Loss: 0.00003621
Iteration 42/1000 | Loss: 0.00003621
Iteration 43/1000 | Loss: 0.00003621
Iteration 44/1000 | Loss: 0.00003621
Iteration 45/1000 | Loss: 0.00003620
Iteration 46/1000 | Loss: 0.00003619
Iteration 47/1000 | Loss: 0.00003619
Iteration 48/1000 | Loss: 0.00003619
Iteration 49/1000 | Loss: 0.00003617
Iteration 50/1000 | Loss: 0.00003616
Iteration 51/1000 | Loss: 0.00003616
Iteration 52/1000 | Loss: 0.00003616
Iteration 53/1000 | Loss: 0.00003615
Iteration 54/1000 | Loss: 0.00003614
Iteration 55/1000 | Loss: 0.00003613
Iteration 56/1000 | Loss: 0.00003613
Iteration 57/1000 | Loss: 0.00003613
Iteration 58/1000 | Loss: 0.00003613
Iteration 59/1000 | Loss: 0.00003612
Iteration 60/1000 | Loss: 0.00003612
Iteration 61/1000 | Loss: 0.00003612
Iteration 62/1000 | Loss: 0.00003612
Iteration 63/1000 | Loss: 0.00003611
Iteration 64/1000 | Loss: 0.00003611
Iteration 65/1000 | Loss: 0.00003611
Iteration 66/1000 | Loss: 0.00003611
Iteration 67/1000 | Loss: 0.00003611
Iteration 68/1000 | Loss: 0.00003610
Iteration 69/1000 | Loss: 0.00003610
Iteration 70/1000 | Loss: 0.00003610
Iteration 71/1000 | Loss: 0.00003610
Iteration 72/1000 | Loss: 0.00003609
Iteration 73/1000 | Loss: 0.00003609
Iteration 74/1000 | Loss: 0.00003609
Iteration 75/1000 | Loss: 0.00003609
Iteration 76/1000 | Loss: 0.00003609
Iteration 77/1000 | Loss: 0.00003609
Iteration 78/1000 | Loss: 0.00003608
Iteration 79/1000 | Loss: 0.00003608
Iteration 80/1000 | Loss: 0.00003608
Iteration 81/1000 | Loss: 0.00003608
Iteration 82/1000 | Loss: 0.00003608
Iteration 83/1000 | Loss: 0.00003608
Iteration 84/1000 | Loss: 0.00003608
Iteration 85/1000 | Loss: 0.00003608
Iteration 86/1000 | Loss: 0.00003608
Iteration 87/1000 | Loss: 0.00003608
Iteration 88/1000 | Loss: 0.00003608
Iteration 89/1000 | Loss: 0.00003608
Iteration 90/1000 | Loss: 0.00003608
Iteration 91/1000 | Loss: 0.00003608
Iteration 92/1000 | Loss: 0.00003608
Iteration 93/1000 | Loss: 0.00003608
Iteration 94/1000 | Loss: 0.00003608
Iteration 95/1000 | Loss: 0.00003608
Iteration 96/1000 | Loss: 0.00003608
Iteration 97/1000 | Loss: 0.00003608
Iteration 98/1000 | Loss: 0.00003608
Iteration 99/1000 | Loss: 0.00003608
Iteration 100/1000 | Loss: 0.00003608
Iteration 101/1000 | Loss: 0.00003608
Iteration 102/1000 | Loss: 0.00003608
Iteration 103/1000 | Loss: 0.00003608
Iteration 104/1000 | Loss: 0.00003608
Iteration 105/1000 | Loss: 0.00003608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [3.6080342397326604e-05, 3.6080342397326604e-05, 3.6080342397326604e-05, 3.6080342397326604e-05, 3.6080342397326604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6080342397326604e-05

Optimization complete. Final v2v error: 4.757975101470947 mm

Highest mean error: 5.673557758331299 mm for frame 25

Lowest mean error: 4.091087818145752 mm for frame 36

Saving results

Total time: 49.17251396179199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420664
Iteration 2/25 | Loss: 0.00098374
Iteration 3/25 | Loss: 0.00088103
Iteration 4/25 | Loss: 0.00085884
Iteration 5/25 | Loss: 0.00085251
Iteration 6/25 | Loss: 0.00085127
Iteration 7/25 | Loss: 0.00085127
Iteration 8/25 | Loss: 0.00085127
Iteration 9/25 | Loss: 0.00085127
Iteration 10/25 | Loss: 0.00085127
Iteration 11/25 | Loss: 0.00085127
Iteration 12/25 | Loss: 0.00085127
Iteration 13/25 | Loss: 0.00085127
Iteration 14/25 | Loss: 0.00085127
Iteration 15/25 | Loss: 0.00085127
Iteration 16/25 | Loss: 0.00085127
Iteration 17/25 | Loss: 0.00085127
Iteration 18/25 | Loss: 0.00085127
Iteration 19/25 | Loss: 0.00085127
Iteration 20/25 | Loss: 0.00085127
Iteration 21/25 | Loss: 0.00085127
Iteration 22/25 | Loss: 0.00085127
Iteration 23/25 | Loss: 0.00085127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008512670174241066, 0.0008512670174241066, 0.0008512670174241066, 0.0008512670174241066, 0.0008512670174241066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008512670174241066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36924899
Iteration 2/25 | Loss: 0.00059649
Iteration 3/25 | Loss: 0.00059649
Iteration 4/25 | Loss: 0.00059649
Iteration 5/25 | Loss: 0.00059649
Iteration 6/25 | Loss: 0.00059649
Iteration 7/25 | Loss: 0.00059649
Iteration 8/25 | Loss: 0.00059649
Iteration 9/25 | Loss: 0.00059649
Iteration 10/25 | Loss: 0.00059649
Iteration 11/25 | Loss: 0.00059649
Iteration 12/25 | Loss: 0.00059649
Iteration 13/25 | Loss: 0.00059649
Iteration 14/25 | Loss: 0.00059649
Iteration 15/25 | Loss: 0.00059649
Iteration 16/25 | Loss: 0.00059649
Iteration 17/25 | Loss: 0.00059649
Iteration 18/25 | Loss: 0.00059649
Iteration 19/25 | Loss: 0.00059649
Iteration 20/25 | Loss: 0.00059649
Iteration 21/25 | Loss: 0.00059649
Iteration 22/25 | Loss: 0.00059649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005964868469163775, 0.0005964868469163775, 0.0005964868469163775, 0.0005964868469163775, 0.0005964868469163775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005964868469163775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059649
Iteration 2/1000 | Loss: 0.00003855
Iteration 3/1000 | Loss: 0.00002694
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00002018
Iteration 6/1000 | Loss: 0.00001911
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001762
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001686
Iteration 16/1000 | Loss: 0.00001678
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001670
Iteration 26/1000 | Loss: 0.00001668
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001666
Iteration 31/1000 | Loss: 0.00001666
Iteration 32/1000 | Loss: 0.00001665
Iteration 33/1000 | Loss: 0.00001665
Iteration 34/1000 | Loss: 0.00001664
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001664
Iteration 38/1000 | Loss: 0.00001663
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001662
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001661
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001660
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001660
Iteration 49/1000 | Loss: 0.00001660
Iteration 50/1000 | Loss: 0.00001660
Iteration 51/1000 | Loss: 0.00001660
Iteration 52/1000 | Loss: 0.00001659
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001657
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001653
Iteration 69/1000 | Loss: 0.00001653
Iteration 70/1000 | Loss: 0.00001653
Iteration 71/1000 | Loss: 0.00001652
Iteration 72/1000 | Loss: 0.00001652
Iteration 73/1000 | Loss: 0.00001652
Iteration 74/1000 | Loss: 0.00001652
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001651
Iteration 77/1000 | Loss: 0.00001651
Iteration 78/1000 | Loss: 0.00001651
Iteration 79/1000 | Loss: 0.00001651
Iteration 80/1000 | Loss: 0.00001651
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001650
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001649
Iteration 88/1000 | Loss: 0.00001649
Iteration 89/1000 | Loss: 0.00001648
Iteration 90/1000 | Loss: 0.00001648
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001647
Iteration 99/1000 | Loss: 0.00001647
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001647
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.646557211643085e-05, 1.646557211643085e-05, 1.646557211643085e-05, 1.646557211643085e-05, 1.646557211643085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.646557211643085e-05

Optimization complete. Final v2v error: 3.1720175743103027 mm

Highest mean error: 3.7146196365356445 mm for frame 199

Lowest mean error: 2.4281961917877197 mm for frame 132

Saving results

Total time: 39.678035736083984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941488
Iteration 2/25 | Loss: 0.00202352
Iteration 3/25 | Loss: 0.00114025
Iteration 4/25 | Loss: 0.00111561
Iteration 5/25 | Loss: 0.00110912
Iteration 6/25 | Loss: 0.00110678
Iteration 7/25 | Loss: 0.00110678
Iteration 8/25 | Loss: 0.00110678
Iteration 9/25 | Loss: 0.00110678
Iteration 10/25 | Loss: 0.00110678
Iteration 11/25 | Loss: 0.00110678
Iteration 12/25 | Loss: 0.00110678
Iteration 13/25 | Loss: 0.00110678
Iteration 14/25 | Loss: 0.00110678
Iteration 15/25 | Loss: 0.00110678
Iteration 16/25 | Loss: 0.00110678
Iteration 17/25 | Loss: 0.00110678
Iteration 18/25 | Loss: 0.00110678
Iteration 19/25 | Loss: 0.00110678
Iteration 20/25 | Loss: 0.00110678
Iteration 21/25 | Loss: 0.00110678
Iteration 22/25 | Loss: 0.00110678
Iteration 23/25 | Loss: 0.00110678
Iteration 24/25 | Loss: 0.00110678
Iteration 25/25 | Loss: 0.00110678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54574990
Iteration 2/25 | Loss: 0.00045451
Iteration 3/25 | Loss: 0.00045451
Iteration 4/25 | Loss: 0.00045451
Iteration 5/25 | Loss: 0.00045451
Iteration 6/25 | Loss: 0.00045451
Iteration 7/25 | Loss: 0.00045451
Iteration 8/25 | Loss: 0.00045451
Iteration 9/25 | Loss: 0.00045451
Iteration 10/25 | Loss: 0.00045451
Iteration 11/25 | Loss: 0.00045451
Iteration 12/25 | Loss: 0.00045451
Iteration 13/25 | Loss: 0.00045451
Iteration 14/25 | Loss: 0.00045451
Iteration 15/25 | Loss: 0.00045451
Iteration 16/25 | Loss: 0.00045451
Iteration 17/25 | Loss: 0.00045451
Iteration 18/25 | Loss: 0.00045451
Iteration 19/25 | Loss: 0.00045451
Iteration 20/25 | Loss: 0.00045451
Iteration 21/25 | Loss: 0.00045451
Iteration 22/25 | Loss: 0.00045451
Iteration 23/25 | Loss: 0.00045451
Iteration 24/25 | Loss: 0.00045451
Iteration 25/25 | Loss: 0.00045451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045451
Iteration 2/1000 | Loss: 0.00007762
Iteration 3/1000 | Loss: 0.00005735
Iteration 4/1000 | Loss: 0.00004998
Iteration 5/1000 | Loss: 0.00004764
Iteration 6/1000 | Loss: 0.00004611
Iteration 7/1000 | Loss: 0.00004496
Iteration 8/1000 | Loss: 0.00004380
Iteration 9/1000 | Loss: 0.00004301
Iteration 10/1000 | Loss: 0.00004193
Iteration 11/1000 | Loss: 0.00004133
Iteration 12/1000 | Loss: 0.00004074
Iteration 13/1000 | Loss: 0.00004030
Iteration 14/1000 | Loss: 0.00003990
Iteration 15/1000 | Loss: 0.00003958
Iteration 16/1000 | Loss: 0.00003906
Iteration 17/1000 | Loss: 0.00003871
Iteration 18/1000 | Loss: 0.00003847
Iteration 19/1000 | Loss: 0.00003818
Iteration 20/1000 | Loss: 0.00003797
Iteration 21/1000 | Loss: 0.00003775
Iteration 22/1000 | Loss: 0.00003758
Iteration 23/1000 | Loss: 0.00003743
Iteration 24/1000 | Loss: 0.00003743
Iteration 25/1000 | Loss: 0.00003741
Iteration 26/1000 | Loss: 0.00003741
Iteration 27/1000 | Loss: 0.00003739
Iteration 28/1000 | Loss: 0.00003739
Iteration 29/1000 | Loss: 0.00003738
Iteration 30/1000 | Loss: 0.00003735
Iteration 31/1000 | Loss: 0.00003734
Iteration 32/1000 | Loss: 0.00003734
Iteration 33/1000 | Loss: 0.00003731
Iteration 34/1000 | Loss: 0.00003731
Iteration 35/1000 | Loss: 0.00003731
Iteration 36/1000 | Loss: 0.00003730
Iteration 37/1000 | Loss: 0.00003729
Iteration 38/1000 | Loss: 0.00003727
Iteration 39/1000 | Loss: 0.00003720
Iteration 40/1000 | Loss: 0.00003720
Iteration 41/1000 | Loss: 0.00003715
Iteration 42/1000 | Loss: 0.00003714
Iteration 43/1000 | Loss: 0.00003712
Iteration 44/1000 | Loss: 0.00003711
Iteration 45/1000 | Loss: 0.00003706
Iteration 46/1000 | Loss: 0.00003705
Iteration 47/1000 | Loss: 0.00003705
Iteration 48/1000 | Loss: 0.00003705
Iteration 49/1000 | Loss: 0.00003704
Iteration 50/1000 | Loss: 0.00003704
Iteration 51/1000 | Loss: 0.00003704
Iteration 52/1000 | Loss: 0.00003703
Iteration 53/1000 | Loss: 0.00003703
Iteration 54/1000 | Loss: 0.00003703
Iteration 55/1000 | Loss: 0.00003703
Iteration 56/1000 | Loss: 0.00003703
Iteration 57/1000 | Loss: 0.00003703
Iteration 58/1000 | Loss: 0.00003703
Iteration 59/1000 | Loss: 0.00003703
Iteration 60/1000 | Loss: 0.00003702
Iteration 61/1000 | Loss: 0.00003702
Iteration 62/1000 | Loss: 0.00003702
Iteration 63/1000 | Loss: 0.00003702
Iteration 64/1000 | Loss: 0.00003702
Iteration 65/1000 | Loss: 0.00003702
Iteration 66/1000 | Loss: 0.00003702
Iteration 67/1000 | Loss: 0.00003702
Iteration 68/1000 | Loss: 0.00003702
Iteration 69/1000 | Loss: 0.00003702
Iteration 70/1000 | Loss: 0.00003701
Iteration 71/1000 | Loss: 0.00003701
Iteration 72/1000 | Loss: 0.00003701
Iteration 73/1000 | Loss: 0.00003701
Iteration 74/1000 | Loss: 0.00003701
Iteration 75/1000 | Loss: 0.00003701
Iteration 76/1000 | Loss: 0.00003701
Iteration 77/1000 | Loss: 0.00003701
Iteration 78/1000 | Loss: 0.00003701
Iteration 79/1000 | Loss: 0.00003701
Iteration 80/1000 | Loss: 0.00003701
Iteration 81/1000 | Loss: 0.00003701
Iteration 82/1000 | Loss: 0.00003700
Iteration 83/1000 | Loss: 0.00003700
Iteration 84/1000 | Loss: 0.00003700
Iteration 85/1000 | Loss: 0.00003700
Iteration 86/1000 | Loss: 0.00003699
Iteration 87/1000 | Loss: 0.00003699
Iteration 88/1000 | Loss: 0.00003699
Iteration 89/1000 | Loss: 0.00003699
Iteration 90/1000 | Loss: 0.00003699
Iteration 91/1000 | Loss: 0.00003699
Iteration 92/1000 | Loss: 0.00003698
Iteration 93/1000 | Loss: 0.00003698
Iteration 94/1000 | Loss: 0.00003698
Iteration 95/1000 | Loss: 0.00003698
Iteration 96/1000 | Loss: 0.00003698
Iteration 97/1000 | Loss: 0.00003698
Iteration 98/1000 | Loss: 0.00003698
Iteration 99/1000 | Loss: 0.00003698
Iteration 100/1000 | Loss: 0.00003698
Iteration 101/1000 | Loss: 0.00003698
Iteration 102/1000 | Loss: 0.00003698
Iteration 103/1000 | Loss: 0.00003698
Iteration 104/1000 | Loss: 0.00003698
Iteration 105/1000 | Loss: 0.00003698
Iteration 106/1000 | Loss: 0.00003698
Iteration 107/1000 | Loss: 0.00003698
Iteration 108/1000 | Loss: 0.00003698
Iteration 109/1000 | Loss: 0.00003698
Iteration 110/1000 | Loss: 0.00003698
Iteration 111/1000 | Loss: 0.00003698
Iteration 112/1000 | Loss: 0.00003698
Iteration 113/1000 | Loss: 0.00003698
Iteration 114/1000 | Loss: 0.00003698
Iteration 115/1000 | Loss: 0.00003698
Iteration 116/1000 | Loss: 0.00003698
Iteration 117/1000 | Loss: 0.00003698
Iteration 118/1000 | Loss: 0.00003698
Iteration 119/1000 | Loss: 0.00003698
Iteration 120/1000 | Loss: 0.00003698
Iteration 121/1000 | Loss: 0.00003698
Iteration 122/1000 | Loss: 0.00003698
Iteration 123/1000 | Loss: 0.00003698
Iteration 124/1000 | Loss: 0.00003698
Iteration 125/1000 | Loss: 0.00003698
Iteration 126/1000 | Loss: 0.00003698
Iteration 127/1000 | Loss: 0.00003698
Iteration 128/1000 | Loss: 0.00003698
Iteration 129/1000 | Loss: 0.00003698
Iteration 130/1000 | Loss: 0.00003698
Iteration 131/1000 | Loss: 0.00003698
Iteration 132/1000 | Loss: 0.00003698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.698363434523344e-05, 3.698363434523344e-05, 3.698363434523344e-05, 3.698363434523344e-05, 3.698363434523344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.698363434523344e-05

Optimization complete. Final v2v error: 4.923112392425537 mm

Highest mean error: 5.414438247680664 mm for frame 69

Lowest mean error: 4.553089618682861 mm for frame 228

Saving results

Total time: 59.38376998901367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875172
Iteration 2/25 | Loss: 0.00165346
Iteration 3/25 | Loss: 0.00110277
Iteration 4/25 | Loss: 0.00097482
Iteration 5/25 | Loss: 0.00093705
Iteration 6/25 | Loss: 0.00091666
Iteration 7/25 | Loss: 0.00088223
Iteration 8/25 | Loss: 0.00086679
Iteration 9/25 | Loss: 0.00085587
Iteration 10/25 | Loss: 0.00084884
Iteration 11/25 | Loss: 0.00084653
Iteration 12/25 | Loss: 0.00084562
Iteration 13/25 | Loss: 0.00084510
Iteration 14/25 | Loss: 0.00084479
Iteration 15/25 | Loss: 0.00084480
Iteration 16/25 | Loss: 0.00084479
Iteration 17/25 | Loss: 0.00084480
Iteration 18/25 | Loss: 0.00084475
Iteration 19/25 | Loss: 0.00084478
Iteration 20/25 | Loss: 0.00084477
Iteration 21/25 | Loss: 0.00084469
Iteration 22/25 | Loss: 0.00084478
Iteration 23/25 | Loss: 0.00084483
Iteration 24/25 | Loss: 0.00084458
Iteration 25/25 | Loss: 0.00084455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.88105297
Iteration 2/25 | Loss: 0.00052020
Iteration 3/25 | Loss: 0.00051961
Iteration 4/25 | Loss: 0.00051961
Iteration 5/25 | Loss: 0.00051961
Iteration 6/25 | Loss: 0.00051961
Iteration 7/25 | Loss: 0.00051961
Iteration 8/25 | Loss: 0.00051961
Iteration 9/25 | Loss: 0.00051961
Iteration 10/25 | Loss: 0.00051961
Iteration 11/25 | Loss: 0.00051961
Iteration 12/25 | Loss: 0.00051961
Iteration 13/25 | Loss: 0.00051961
Iteration 14/25 | Loss: 0.00051961
Iteration 15/25 | Loss: 0.00051961
Iteration 16/25 | Loss: 0.00051961
Iteration 17/25 | Loss: 0.00051961
Iteration 18/25 | Loss: 0.00051961
Iteration 19/25 | Loss: 0.00051961
Iteration 20/25 | Loss: 0.00051961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005196065758354962, 0.0005196065758354962, 0.0005196065758354962, 0.0005196065758354962, 0.0005196065758354962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005196065758354962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051961
Iteration 2/1000 | Loss: 0.00002400
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00002256
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00002291
Iteration 9/1000 | Loss: 0.00001557
Iteration 10/1000 | Loss: 0.00001615
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00004102
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001500
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00003474
Iteration 21/1000 | Loss: 0.00004233
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00023713
Iteration 33/1000 | Loss: 0.00011965
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001143
Iteration 36/1000 | Loss: 0.00004566
Iteration 37/1000 | Loss: 0.00011803
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00003725
Iteration 40/1000 | Loss: 0.00001032
Iteration 41/1000 | Loss: 0.00001003
Iteration 42/1000 | Loss: 0.00001002
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000988
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000987
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000987
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000987
Iteration 58/1000 | Loss: 0.00000987
Iteration 59/1000 | Loss: 0.00000986
Iteration 60/1000 | Loss: 0.00000986
Iteration 61/1000 | Loss: 0.00000986
Iteration 62/1000 | Loss: 0.00000986
Iteration 63/1000 | Loss: 0.00000985
Iteration 64/1000 | Loss: 0.00000985
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000981
Iteration 67/1000 | Loss: 0.00000981
Iteration 68/1000 | Loss: 0.00000980
Iteration 69/1000 | Loss: 0.00000980
Iteration 70/1000 | Loss: 0.00001644
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00002665
Iteration 73/1000 | Loss: 0.00009600
Iteration 74/1000 | Loss: 0.00003869
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002490
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000964
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000963
Iteration 84/1000 | Loss: 0.00000963
Iteration 85/1000 | Loss: 0.00000962
Iteration 86/1000 | Loss: 0.00000962
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000961
Iteration 90/1000 | Loss: 0.00000961
Iteration 91/1000 | Loss: 0.00000961
Iteration 92/1000 | Loss: 0.00000961
Iteration 93/1000 | Loss: 0.00000961
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000961
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00002520
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000958
Iteration 120/1000 | Loss: 0.00000958
Iteration 121/1000 | Loss: 0.00000958
Iteration 122/1000 | Loss: 0.00000958
Iteration 123/1000 | Loss: 0.00000958
Iteration 124/1000 | Loss: 0.00000958
Iteration 125/1000 | Loss: 0.00000970
Iteration 126/1000 | Loss: 0.00000970
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000958
Iteration 129/1000 | Loss: 0.00000958
Iteration 130/1000 | Loss: 0.00000958
Iteration 131/1000 | Loss: 0.00000958
Iteration 132/1000 | Loss: 0.00000958
Iteration 133/1000 | Loss: 0.00000958
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000957
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000957
Iteration 142/1000 | Loss: 0.00000957
Iteration 143/1000 | Loss: 0.00000957
Iteration 144/1000 | Loss: 0.00000957
Iteration 145/1000 | Loss: 0.00000957
Iteration 146/1000 | Loss: 0.00000957
Iteration 147/1000 | Loss: 0.00000957
Iteration 148/1000 | Loss: 0.00000957
Iteration 149/1000 | Loss: 0.00000957
Iteration 150/1000 | Loss: 0.00002623
Iteration 151/1000 | Loss: 0.00002452
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00002287
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001100
Iteration 156/1000 | Loss: 0.00000955
Iteration 157/1000 | Loss: 0.00000955
Iteration 158/1000 | Loss: 0.00000954
Iteration 159/1000 | Loss: 0.00000954
Iteration 160/1000 | Loss: 0.00000954
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000954
Iteration 163/1000 | Loss: 0.00000954
Iteration 164/1000 | Loss: 0.00000954
Iteration 165/1000 | Loss: 0.00000954
Iteration 166/1000 | Loss: 0.00000954
Iteration 167/1000 | Loss: 0.00000954
Iteration 168/1000 | Loss: 0.00000954
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000954
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [9.54199094849173e-06, 9.54199094849173e-06, 9.54199094849173e-06, 9.54199094849173e-06, 9.54199094849173e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.54199094849173e-06

Optimization complete. Final v2v error: 2.583347797393799 mm

Highest mean error: 4.145280838012695 mm for frame 72

Lowest mean error: 2.301431894302368 mm for frame 26

Saving results

Total time: 142.78383660316467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826370
Iteration 2/25 | Loss: 0.00122115
Iteration 3/25 | Loss: 0.00088606
Iteration 4/25 | Loss: 0.00085265
Iteration 5/25 | Loss: 0.00084849
Iteration 6/25 | Loss: 0.00084718
Iteration 7/25 | Loss: 0.00084657
Iteration 8/25 | Loss: 0.00084618
Iteration 9/25 | Loss: 0.00084585
Iteration 10/25 | Loss: 0.00084887
Iteration 11/25 | Loss: 0.00085031
Iteration 12/25 | Loss: 0.00085069
Iteration 13/25 | Loss: 0.00084931
Iteration 14/25 | Loss: 0.00084826
Iteration 15/25 | Loss: 0.00084944
Iteration 16/25 | Loss: 0.00085039
Iteration 17/25 | Loss: 0.00084741
Iteration 18/25 | Loss: 0.00084932
Iteration 19/25 | Loss: 0.00084900
Iteration 20/25 | Loss: 0.00084951
Iteration 21/25 | Loss: 0.00084539
Iteration 22/25 | Loss: 0.00084284
Iteration 23/25 | Loss: 0.00084124
Iteration 24/25 | Loss: 0.00084054
Iteration 25/25 | Loss: 0.00084153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39057291
Iteration 2/25 | Loss: 0.00049120
Iteration 3/25 | Loss: 0.00049120
Iteration 4/25 | Loss: 0.00049120
Iteration 5/25 | Loss: 0.00049120
Iteration 6/25 | Loss: 0.00049120
Iteration 7/25 | Loss: 0.00049120
Iteration 8/25 | Loss: 0.00049120
Iteration 9/25 | Loss: 0.00049120
Iteration 10/25 | Loss: 0.00049120
Iteration 11/25 | Loss: 0.00049120
Iteration 12/25 | Loss: 0.00049120
Iteration 13/25 | Loss: 0.00049120
Iteration 14/25 | Loss: 0.00049120
Iteration 15/25 | Loss: 0.00049120
Iteration 16/25 | Loss: 0.00049120
Iteration 17/25 | Loss: 0.00049120
Iteration 18/25 | Loss: 0.00049120
Iteration 19/25 | Loss: 0.00049119
Iteration 20/25 | Loss: 0.00049119
Iteration 21/25 | Loss: 0.00049119
Iteration 22/25 | Loss: 0.00049119
Iteration 23/25 | Loss: 0.00049119
Iteration 24/25 | Loss: 0.00049119
Iteration 25/25 | Loss: 0.00049119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049119
Iteration 2/1000 | Loss: 0.00003842
Iteration 3/1000 | Loss: 0.00001469
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001310
Iteration 6/1000 | Loss: 0.00001025
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00003139
Iteration 10/1000 | Loss: 0.00003231
Iteration 11/1000 | Loss: 0.00002990
Iteration 12/1000 | Loss: 0.00002482
Iteration 13/1000 | Loss: 0.00004639
Iteration 14/1000 | Loss: 0.00002434
Iteration 15/1000 | Loss: 0.00002433
Iteration 16/1000 | Loss: 0.00004511
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001094
Iteration 19/1000 | Loss: 0.00000963
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000859
Iteration 22/1000 | Loss: 0.00000840
Iteration 23/1000 | Loss: 0.00000833
Iteration 24/1000 | Loss: 0.00000833
Iteration 25/1000 | Loss: 0.00000815
Iteration 26/1000 | Loss: 0.00000812
Iteration 27/1000 | Loss: 0.00000807
Iteration 28/1000 | Loss: 0.00000806
Iteration 29/1000 | Loss: 0.00000801
Iteration 30/1000 | Loss: 0.00000800
Iteration 31/1000 | Loss: 0.00000799
Iteration 32/1000 | Loss: 0.00000799
Iteration 33/1000 | Loss: 0.00000798
Iteration 34/1000 | Loss: 0.00000797
Iteration 35/1000 | Loss: 0.00000796
Iteration 36/1000 | Loss: 0.00000795
Iteration 37/1000 | Loss: 0.00000794
Iteration 38/1000 | Loss: 0.00000794
Iteration 39/1000 | Loss: 0.00000794
Iteration 40/1000 | Loss: 0.00000793
Iteration 41/1000 | Loss: 0.00000793
Iteration 42/1000 | Loss: 0.00000793
Iteration 43/1000 | Loss: 0.00000792
Iteration 44/1000 | Loss: 0.00000792
Iteration 45/1000 | Loss: 0.00000792
Iteration 46/1000 | Loss: 0.00000792
Iteration 47/1000 | Loss: 0.00000791
Iteration 48/1000 | Loss: 0.00000791
Iteration 49/1000 | Loss: 0.00000791
Iteration 50/1000 | Loss: 0.00000790
Iteration 51/1000 | Loss: 0.00000790
Iteration 52/1000 | Loss: 0.00000790
Iteration 53/1000 | Loss: 0.00000790
Iteration 54/1000 | Loss: 0.00000790
Iteration 55/1000 | Loss: 0.00000789
Iteration 56/1000 | Loss: 0.00000789
Iteration 57/1000 | Loss: 0.00000788
Iteration 58/1000 | Loss: 0.00000788
Iteration 59/1000 | Loss: 0.00000786
Iteration 60/1000 | Loss: 0.00000785
Iteration 61/1000 | Loss: 0.00000780
Iteration 62/1000 | Loss: 0.00000780
Iteration 63/1000 | Loss: 0.00000780
Iteration 64/1000 | Loss: 0.00000780
Iteration 65/1000 | Loss: 0.00000780
Iteration 66/1000 | Loss: 0.00000780
Iteration 67/1000 | Loss: 0.00000780
Iteration 68/1000 | Loss: 0.00000780
Iteration 69/1000 | Loss: 0.00000780
Iteration 70/1000 | Loss: 0.00000780
Iteration 71/1000 | Loss: 0.00000779
Iteration 72/1000 | Loss: 0.00000779
Iteration 73/1000 | Loss: 0.00000779
Iteration 74/1000 | Loss: 0.00000778
Iteration 75/1000 | Loss: 0.00000778
Iteration 76/1000 | Loss: 0.00000777
Iteration 77/1000 | Loss: 0.00000777
Iteration 78/1000 | Loss: 0.00000777
Iteration 79/1000 | Loss: 0.00000777
Iteration 80/1000 | Loss: 0.00000776
Iteration 81/1000 | Loss: 0.00000776
Iteration 82/1000 | Loss: 0.00000776
Iteration 83/1000 | Loss: 0.00000775
Iteration 84/1000 | Loss: 0.00000775
Iteration 85/1000 | Loss: 0.00000775
Iteration 86/1000 | Loss: 0.00000775
Iteration 87/1000 | Loss: 0.00000774
Iteration 88/1000 | Loss: 0.00000774
Iteration 89/1000 | Loss: 0.00000774
Iteration 90/1000 | Loss: 0.00000774
Iteration 91/1000 | Loss: 0.00000774
Iteration 92/1000 | Loss: 0.00000774
Iteration 93/1000 | Loss: 0.00000774
Iteration 94/1000 | Loss: 0.00000774
Iteration 95/1000 | Loss: 0.00000774
Iteration 96/1000 | Loss: 0.00000774
Iteration 97/1000 | Loss: 0.00000774
Iteration 98/1000 | Loss: 0.00000774
Iteration 99/1000 | Loss: 0.00000774
Iteration 100/1000 | Loss: 0.00000774
Iteration 101/1000 | Loss: 0.00000774
Iteration 102/1000 | Loss: 0.00000774
Iteration 103/1000 | Loss: 0.00000774
Iteration 104/1000 | Loss: 0.00000774
Iteration 105/1000 | Loss: 0.00000774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [7.737360647297464e-06, 7.737360647297464e-06, 7.737360647297464e-06, 7.737360647297464e-06, 7.737360647297464e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.737360647297464e-06

Optimization complete. Final v2v error: 2.385535717010498 mm

Highest mean error: 4.373172283172607 mm for frame 52

Lowest mean error: 2.2817962169647217 mm for frame 44

Saving results

Total time: 86.64967846870422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744781
Iteration 2/25 | Loss: 0.00155423
Iteration 3/25 | Loss: 0.00106848
Iteration 4/25 | Loss: 0.00102152
Iteration 5/25 | Loss: 0.00100777
Iteration 6/25 | Loss: 0.00101314
Iteration 7/25 | Loss: 0.00100619
Iteration 8/25 | Loss: 0.00100179
Iteration 9/25 | Loss: 0.00102803
Iteration 10/25 | Loss: 0.00100605
Iteration 11/25 | Loss: 0.00100228
Iteration 12/25 | Loss: 0.00099494
Iteration 13/25 | Loss: 0.00099199
Iteration 14/25 | Loss: 0.00099102
Iteration 15/25 | Loss: 0.00099055
Iteration 16/25 | Loss: 0.00099053
Iteration 17/25 | Loss: 0.00099050
Iteration 18/25 | Loss: 0.00099050
Iteration 19/25 | Loss: 0.00099050
Iteration 20/25 | Loss: 0.00099050
Iteration 21/25 | Loss: 0.00099050
Iteration 22/25 | Loss: 0.00099050
Iteration 23/25 | Loss: 0.00099050
Iteration 24/25 | Loss: 0.00099050
Iteration 25/25 | Loss: 0.00099050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67941368
Iteration 2/25 | Loss: 0.00057186
Iteration 3/25 | Loss: 0.00057183
Iteration 4/25 | Loss: 0.00057182
Iteration 5/25 | Loss: 0.00057182
Iteration 6/25 | Loss: 0.00057182
Iteration 7/25 | Loss: 0.00057182
Iteration 8/25 | Loss: 0.00057182
Iteration 9/25 | Loss: 0.00057182
Iteration 10/25 | Loss: 0.00057182
Iteration 11/25 | Loss: 0.00057182
Iteration 12/25 | Loss: 0.00057182
Iteration 13/25 | Loss: 0.00057182
Iteration 14/25 | Loss: 0.00057182
Iteration 15/25 | Loss: 0.00057182
Iteration 16/25 | Loss: 0.00057182
Iteration 17/25 | Loss: 0.00057182
Iteration 18/25 | Loss: 0.00057182
Iteration 19/25 | Loss: 0.00057182
Iteration 20/25 | Loss: 0.00057182
Iteration 21/25 | Loss: 0.00057182
Iteration 22/25 | Loss: 0.00057182
Iteration 23/25 | Loss: 0.00057182
Iteration 24/25 | Loss: 0.00057182
Iteration 25/25 | Loss: 0.00057182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057182
Iteration 2/1000 | Loss: 0.00005387
Iteration 3/1000 | Loss: 0.00003361
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002843
Iteration 6/1000 | Loss: 0.00002708
Iteration 7/1000 | Loss: 0.00002666
Iteration 8/1000 | Loss: 0.00002405
Iteration 9/1000 | Loss: 0.00002322
Iteration 10/1000 | Loss: 0.00002447
Iteration 11/1000 | Loss: 0.00002388
Iteration 12/1000 | Loss: 0.00002236
Iteration 13/1000 | Loss: 0.00002210
Iteration 14/1000 | Loss: 0.00002180
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002095
Iteration 17/1000 | Loss: 0.00002070
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002066
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00002062
Iteration 22/1000 | Loss: 0.00002061
Iteration 23/1000 | Loss: 0.00002061
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00002060
Iteration 26/1000 | Loss: 0.00002059
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00002057
Iteration 30/1000 | Loss: 0.00002057
Iteration 31/1000 | Loss: 0.00002056
Iteration 32/1000 | Loss: 0.00002055
Iteration 33/1000 | Loss: 0.00002055
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002054
Iteration 36/1000 | Loss: 0.00002054
Iteration 37/1000 | Loss: 0.00002054
Iteration 38/1000 | Loss: 0.00002054
Iteration 39/1000 | Loss: 0.00002053
Iteration 40/1000 | Loss: 0.00002053
Iteration 41/1000 | Loss: 0.00002053
Iteration 42/1000 | Loss: 0.00002053
Iteration 43/1000 | Loss: 0.00002052
Iteration 44/1000 | Loss: 0.00002052
Iteration 45/1000 | Loss: 0.00002052
Iteration 46/1000 | Loss: 0.00002052
Iteration 47/1000 | Loss: 0.00002051
Iteration 48/1000 | Loss: 0.00002051
Iteration 49/1000 | Loss: 0.00002051
Iteration 50/1000 | Loss: 0.00002050
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00002049
Iteration 55/1000 | Loss: 0.00002049
Iteration 56/1000 | Loss: 0.00002049
Iteration 57/1000 | Loss: 0.00002049
Iteration 58/1000 | Loss: 0.00002049
Iteration 59/1000 | Loss: 0.00002049
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002048
Iteration 63/1000 | Loss: 0.00002048
Iteration 64/1000 | Loss: 0.00002046
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002045
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002044
Iteration 69/1000 | Loss: 0.00002044
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002042
Iteration 75/1000 | Loss: 0.00002042
Iteration 76/1000 | Loss: 0.00002042
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002041
Iteration 79/1000 | Loss: 0.00002041
Iteration 80/1000 | Loss: 0.00002041
Iteration 81/1000 | Loss: 0.00002041
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002041
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002041
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002041
Iteration 90/1000 | Loss: 0.00002041
Iteration 91/1000 | Loss: 0.00002041
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002041
Iteration 94/1000 | Loss: 0.00002041
Iteration 95/1000 | Loss: 0.00002041
Iteration 96/1000 | Loss: 0.00002041
Iteration 97/1000 | Loss: 0.00002041
Iteration 98/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.0408924683579244e-05, 2.0408924683579244e-05, 2.0408924683579244e-05, 2.0408924683579244e-05, 2.0408924683579244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0408924683579244e-05

Optimization complete. Final v2v error: 3.5538387298583984 mm

Highest mean error: 4.763190269470215 mm for frame 138

Lowest mean error: 2.6953392028808594 mm for frame 188

Saving results

Total time: 68.3292589187622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424660
Iteration 2/25 | Loss: 0.00103875
Iteration 3/25 | Loss: 0.00093159
Iteration 4/25 | Loss: 0.00092396
Iteration 5/25 | Loss: 0.00092295
Iteration 6/25 | Loss: 0.00092295
Iteration 7/25 | Loss: 0.00092295
Iteration 8/25 | Loss: 0.00092295
Iteration 9/25 | Loss: 0.00092295
Iteration 10/25 | Loss: 0.00092295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009229503921233118, 0.0009229503921233118, 0.0009229503921233118, 0.0009229503921233118, 0.0009229503921233118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009229503921233118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39391267
Iteration 2/25 | Loss: 0.00033264
Iteration 3/25 | Loss: 0.00033263
Iteration 4/25 | Loss: 0.00033263
Iteration 5/25 | Loss: 0.00033263
Iteration 6/25 | Loss: 0.00033263
Iteration 7/25 | Loss: 0.00033263
Iteration 8/25 | Loss: 0.00033263
Iteration 9/25 | Loss: 0.00033263
Iteration 10/25 | Loss: 0.00033263
Iteration 11/25 | Loss: 0.00033263
Iteration 12/25 | Loss: 0.00033263
Iteration 13/25 | Loss: 0.00033263
Iteration 14/25 | Loss: 0.00033263
Iteration 15/25 | Loss: 0.00033263
Iteration 16/25 | Loss: 0.00033263
Iteration 17/25 | Loss: 0.00033263
Iteration 18/25 | Loss: 0.00033263
Iteration 19/25 | Loss: 0.00033263
Iteration 20/25 | Loss: 0.00033263
Iteration 21/25 | Loss: 0.00033263
Iteration 22/25 | Loss: 0.00033263
Iteration 23/25 | Loss: 0.00033263
Iteration 24/25 | Loss: 0.00033263
Iteration 25/25 | Loss: 0.00033263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033263
Iteration 2/1000 | Loss: 0.00002175
Iteration 3/1000 | Loss: 0.00001547
Iteration 4/1000 | Loss: 0.00001449
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001337
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001290
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001286
Iteration 20/1000 | Loss: 0.00001286
Iteration 21/1000 | Loss: 0.00001285
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001283
Iteration 25/1000 | Loss: 0.00001282
Iteration 26/1000 | Loss: 0.00001282
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001277
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001275
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001274
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001273
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001272
Iteration 54/1000 | Loss: 0.00001272
Iteration 55/1000 | Loss: 0.00001272
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001270
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001269
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001264
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001263
Iteration 86/1000 | Loss: 0.00001263
Iteration 87/1000 | Loss: 0.00001263
Iteration 88/1000 | Loss: 0.00001263
Iteration 89/1000 | Loss: 0.00001263
Iteration 90/1000 | Loss: 0.00001263
Iteration 91/1000 | Loss: 0.00001262
Iteration 92/1000 | Loss: 0.00001262
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001253
Iteration 110/1000 | Loss: 0.00001253
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001253
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001252
Iteration 115/1000 | Loss: 0.00001252
Iteration 116/1000 | Loss: 0.00001252
Iteration 117/1000 | Loss: 0.00001252
Iteration 118/1000 | Loss: 0.00001252
Iteration 119/1000 | Loss: 0.00001252
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001250
Iteration 123/1000 | Loss: 0.00001250
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001249
Iteration 127/1000 | Loss: 0.00001249
Iteration 128/1000 | Loss: 0.00001249
Iteration 129/1000 | Loss: 0.00001249
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Iteration 138/1000 | Loss: 0.00001248
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001248
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001248
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001248
Iteration 160/1000 | Loss: 0.00001248
Iteration 161/1000 | Loss: 0.00001248
Iteration 162/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.2480035366024822e-05, 1.2480035366024822e-05, 1.2480035366024822e-05, 1.2480035366024822e-05, 1.2480035366024822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2480035366024822e-05

Optimization complete. Final v2v error: 3.013131618499756 mm

Highest mean error: 3.0982534885406494 mm for frame 171

Lowest mean error: 2.9881532192230225 mm for frame 14

Saving results

Total time: 31.61806321144104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844147
Iteration 2/25 | Loss: 0.00097081
Iteration 3/25 | Loss: 0.00081469
Iteration 4/25 | Loss: 0.00080144
Iteration 5/25 | Loss: 0.00079922
Iteration 6/25 | Loss: 0.00079883
Iteration 7/25 | Loss: 0.00079883
Iteration 8/25 | Loss: 0.00079883
Iteration 9/25 | Loss: 0.00079883
Iteration 10/25 | Loss: 0.00079883
Iteration 11/25 | Loss: 0.00079883
Iteration 12/25 | Loss: 0.00079883
Iteration 13/25 | Loss: 0.00079883
Iteration 14/25 | Loss: 0.00079883
Iteration 15/25 | Loss: 0.00079883
Iteration 16/25 | Loss: 0.00079883
Iteration 17/25 | Loss: 0.00079883
Iteration 18/25 | Loss: 0.00079883
Iteration 19/25 | Loss: 0.00079883
Iteration 20/25 | Loss: 0.00079883
Iteration 21/25 | Loss: 0.00079883
Iteration 22/25 | Loss: 0.00079883
Iteration 23/25 | Loss: 0.00079883
Iteration 24/25 | Loss: 0.00079883
Iteration 25/25 | Loss: 0.00079883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007988347206264734, 0.0007988347206264734, 0.0007988347206264734, 0.0007988347206264734, 0.0007988347206264734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007988347206264734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38248920
Iteration 2/25 | Loss: 0.00035582
Iteration 3/25 | Loss: 0.00035581
Iteration 4/25 | Loss: 0.00035581
Iteration 5/25 | Loss: 0.00035581
Iteration 6/25 | Loss: 0.00035581
Iteration 7/25 | Loss: 0.00035581
Iteration 8/25 | Loss: 0.00035581
Iteration 9/25 | Loss: 0.00035581
Iteration 10/25 | Loss: 0.00035581
Iteration 11/25 | Loss: 0.00035581
Iteration 12/25 | Loss: 0.00035581
Iteration 13/25 | Loss: 0.00035581
Iteration 14/25 | Loss: 0.00035581
Iteration 15/25 | Loss: 0.00035581
Iteration 16/25 | Loss: 0.00035581
Iteration 17/25 | Loss: 0.00035581
Iteration 18/25 | Loss: 0.00035581
Iteration 19/25 | Loss: 0.00035581
Iteration 20/25 | Loss: 0.00035581
Iteration 21/25 | Loss: 0.00035581
Iteration 22/25 | Loss: 0.00035581
Iteration 23/25 | Loss: 0.00035581
Iteration 24/25 | Loss: 0.00035581
Iteration 25/25 | Loss: 0.00035581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035581
Iteration 2/1000 | Loss: 0.00001795
Iteration 3/1000 | Loss: 0.00001187
Iteration 4/1000 | Loss: 0.00000984
Iteration 5/1000 | Loss: 0.00000934
Iteration 6/1000 | Loss: 0.00000884
Iteration 7/1000 | Loss: 0.00000874
Iteration 8/1000 | Loss: 0.00000848
Iteration 9/1000 | Loss: 0.00000820
Iteration 10/1000 | Loss: 0.00000806
Iteration 11/1000 | Loss: 0.00000805
Iteration 12/1000 | Loss: 0.00000797
Iteration 13/1000 | Loss: 0.00000790
Iteration 14/1000 | Loss: 0.00000789
Iteration 15/1000 | Loss: 0.00000788
Iteration 16/1000 | Loss: 0.00000787
Iteration 17/1000 | Loss: 0.00000787
Iteration 18/1000 | Loss: 0.00000787
Iteration 19/1000 | Loss: 0.00000786
Iteration 20/1000 | Loss: 0.00000785
Iteration 21/1000 | Loss: 0.00000785
Iteration 22/1000 | Loss: 0.00000784
Iteration 23/1000 | Loss: 0.00000784
Iteration 24/1000 | Loss: 0.00000782
Iteration 25/1000 | Loss: 0.00000781
Iteration 26/1000 | Loss: 0.00000781
Iteration 27/1000 | Loss: 0.00000775
Iteration 28/1000 | Loss: 0.00000773
Iteration 29/1000 | Loss: 0.00000773
Iteration 30/1000 | Loss: 0.00000772
Iteration 31/1000 | Loss: 0.00000772
Iteration 32/1000 | Loss: 0.00000771
Iteration 33/1000 | Loss: 0.00000769
Iteration 34/1000 | Loss: 0.00000769
Iteration 35/1000 | Loss: 0.00000768
Iteration 36/1000 | Loss: 0.00000768
Iteration 37/1000 | Loss: 0.00000767
Iteration 38/1000 | Loss: 0.00000767
Iteration 39/1000 | Loss: 0.00000763
Iteration 40/1000 | Loss: 0.00000763
Iteration 41/1000 | Loss: 0.00000761
Iteration 42/1000 | Loss: 0.00000761
Iteration 43/1000 | Loss: 0.00000761
Iteration 44/1000 | Loss: 0.00000760
Iteration 45/1000 | Loss: 0.00000760
Iteration 46/1000 | Loss: 0.00000760
Iteration 47/1000 | Loss: 0.00000760
Iteration 48/1000 | Loss: 0.00000760
Iteration 49/1000 | Loss: 0.00000760
Iteration 50/1000 | Loss: 0.00000760
Iteration 51/1000 | Loss: 0.00000759
Iteration 52/1000 | Loss: 0.00000759
Iteration 53/1000 | Loss: 0.00000759
Iteration 54/1000 | Loss: 0.00000759
Iteration 55/1000 | Loss: 0.00000759
Iteration 56/1000 | Loss: 0.00000759
Iteration 57/1000 | Loss: 0.00000759
Iteration 58/1000 | Loss: 0.00000759
Iteration 59/1000 | Loss: 0.00000759
Iteration 60/1000 | Loss: 0.00000759
Iteration 61/1000 | Loss: 0.00000759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [7.591865141876042e-06, 7.591865141876042e-06, 7.591865141876042e-06, 7.591865141876042e-06, 7.591865141876042e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.591865141876042e-06

Optimization complete. Final v2v error: 2.323580741882324 mm

Highest mean error: 2.5616769790649414 mm for frame 29

Lowest mean error: 2.1978983879089355 mm for frame 82

Saving results

Total time: 28.975317239761353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399963
Iteration 2/25 | Loss: 0.00099754
Iteration 3/25 | Loss: 0.00089565
Iteration 4/25 | Loss: 0.00087719
Iteration 5/25 | Loss: 0.00086933
Iteration 6/25 | Loss: 0.00086765
Iteration 7/25 | Loss: 0.00086765
Iteration 8/25 | Loss: 0.00086765
Iteration 9/25 | Loss: 0.00086765
Iteration 10/25 | Loss: 0.00086765
Iteration 11/25 | Loss: 0.00086765
Iteration 12/25 | Loss: 0.00086765
Iteration 13/25 | Loss: 0.00086765
Iteration 14/25 | Loss: 0.00086765
Iteration 15/25 | Loss: 0.00086765
Iteration 16/25 | Loss: 0.00086765
Iteration 17/25 | Loss: 0.00086765
Iteration 18/25 | Loss: 0.00086765
Iteration 19/25 | Loss: 0.00086765
Iteration 20/25 | Loss: 0.00086765
Iteration 21/25 | Loss: 0.00086765
Iteration 22/25 | Loss: 0.00086765
Iteration 23/25 | Loss: 0.00086765
Iteration 24/25 | Loss: 0.00086765
Iteration 25/25 | Loss: 0.00086765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008676483994349837, 0.0008676483994349837, 0.0008676483994349837, 0.0008676483994349837, 0.0008676483994349837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008676483994349837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37235093
Iteration 2/25 | Loss: 0.00072001
Iteration 3/25 | Loss: 0.00072001
Iteration 4/25 | Loss: 0.00072001
Iteration 5/25 | Loss: 0.00072001
Iteration 6/25 | Loss: 0.00072001
Iteration 7/25 | Loss: 0.00072001
Iteration 8/25 | Loss: 0.00072001
Iteration 9/25 | Loss: 0.00072001
Iteration 10/25 | Loss: 0.00072001
Iteration 11/25 | Loss: 0.00072001
Iteration 12/25 | Loss: 0.00072001
Iteration 13/25 | Loss: 0.00072001
Iteration 14/25 | Loss: 0.00072001
Iteration 15/25 | Loss: 0.00072001
Iteration 16/25 | Loss: 0.00072001
Iteration 17/25 | Loss: 0.00072001
Iteration 18/25 | Loss: 0.00072001
Iteration 19/25 | Loss: 0.00072001
Iteration 20/25 | Loss: 0.00072001
Iteration 21/25 | Loss: 0.00072001
Iteration 22/25 | Loss: 0.00072001
Iteration 23/25 | Loss: 0.00072001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007200066465884447, 0.0007200066465884447, 0.0007200066465884447, 0.0007200066465884447, 0.0007200066465884447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007200066465884447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072001
Iteration 2/1000 | Loss: 0.00004284
Iteration 3/1000 | Loss: 0.00002492
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001576
Iteration 7/1000 | Loss: 0.00001486
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001398
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001359
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001358
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001353
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001351
Iteration 50/1000 | Loss: 0.00001351
Iteration 51/1000 | Loss: 0.00001350
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001350
Iteration 57/1000 | Loss: 0.00001350
Iteration 58/1000 | Loss: 0.00001350
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001349
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001346
Iteration 82/1000 | Loss: 0.00001346
Iteration 83/1000 | Loss: 0.00001346
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001345
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001345
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001345
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001340
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.3401358955889009e-05, 1.3401358955889009e-05, 1.3401358955889009e-05, 1.3401358955889009e-05, 1.3401358955889009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3401358955889009e-05

Optimization complete. Final v2v error: 2.9575655460357666 mm

Highest mean error: 3.4407119750976562 mm for frame 238

Lowest mean error: 2.570418119430542 mm for frame 14

Saving results

Total time: 40.63669490814209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445123
Iteration 2/25 | Loss: 0.00103055
Iteration 3/25 | Loss: 0.00090440
Iteration 4/25 | Loss: 0.00089639
Iteration 5/25 | Loss: 0.00089392
Iteration 6/25 | Loss: 0.00089346
Iteration 7/25 | Loss: 0.00089346
Iteration 8/25 | Loss: 0.00089346
Iteration 9/25 | Loss: 0.00089346
Iteration 10/25 | Loss: 0.00089346
Iteration 11/25 | Loss: 0.00089346
Iteration 12/25 | Loss: 0.00089346
Iteration 13/25 | Loss: 0.00089346
Iteration 14/25 | Loss: 0.00089346
Iteration 15/25 | Loss: 0.00089346
Iteration 16/25 | Loss: 0.00089346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008934590732678771, 0.0008934590732678771, 0.0008934590732678771, 0.0008934590732678771, 0.0008934590732678771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008934590732678771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38981652
Iteration 2/25 | Loss: 0.00048916
Iteration 3/25 | Loss: 0.00048916
Iteration 4/25 | Loss: 0.00048916
Iteration 5/25 | Loss: 0.00048916
Iteration 6/25 | Loss: 0.00048916
Iteration 7/25 | Loss: 0.00048916
Iteration 8/25 | Loss: 0.00048916
Iteration 9/25 | Loss: 0.00048916
Iteration 10/25 | Loss: 0.00048916
Iteration 11/25 | Loss: 0.00048916
Iteration 12/25 | Loss: 0.00048916
Iteration 13/25 | Loss: 0.00048916
Iteration 14/25 | Loss: 0.00048916
Iteration 15/25 | Loss: 0.00048916
Iteration 16/25 | Loss: 0.00048916
Iteration 17/25 | Loss: 0.00048916
Iteration 18/25 | Loss: 0.00048916
Iteration 19/25 | Loss: 0.00048916
Iteration 20/25 | Loss: 0.00048916
Iteration 21/25 | Loss: 0.00048916
Iteration 22/25 | Loss: 0.00048916
Iteration 23/25 | Loss: 0.00048916
Iteration 24/25 | Loss: 0.00048916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004891562275588512, 0.0004891562275588512, 0.0004891562275588512, 0.0004891562275588512, 0.0004891562275588512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004891562275588512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048916
Iteration 2/1000 | Loss: 0.00002082
Iteration 3/1000 | Loss: 0.00001416
Iteration 4/1000 | Loss: 0.00001321
Iteration 5/1000 | Loss: 0.00001272
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00001227
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001195
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001193
Iteration 16/1000 | Loss: 0.00001193
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001162
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001158
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Iteration 152/1000 | Loss: 0.00001158
Iteration 153/1000 | Loss: 0.00001158
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001156
Iteration 174/1000 | Loss: 0.00001156
Iteration 175/1000 | Loss: 0.00001156
Iteration 176/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.1556973731785547e-05, 1.1556973731785547e-05, 1.1556973731785547e-05, 1.1556973731785547e-05, 1.1556973731785547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1556973731785547e-05

Optimization complete. Final v2v error: 2.7824177742004395 mm

Highest mean error: 3.111851692199707 mm for frame 159

Lowest mean error: 2.511167287826538 mm for frame 3

Saving results

Total time: 34.48280167579651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780967
Iteration 2/25 | Loss: 0.00151784
Iteration 3/25 | Loss: 0.00094507
Iteration 4/25 | Loss: 0.00086308
Iteration 5/25 | Loss: 0.00084711
Iteration 6/25 | Loss: 0.00084183
Iteration 7/25 | Loss: 0.00083800
Iteration 8/25 | Loss: 0.00083621
Iteration 9/25 | Loss: 0.00083418
Iteration 10/25 | Loss: 0.00083085
Iteration 11/25 | Loss: 0.00082938
Iteration 12/25 | Loss: 0.00082860
Iteration 13/25 | Loss: 0.00082832
Iteration 14/25 | Loss: 0.00082822
Iteration 15/25 | Loss: 0.00082718
Iteration 16/25 | Loss: 0.00082659
Iteration 17/25 | Loss: 0.00082641
Iteration 18/25 | Loss: 0.00082633
Iteration 19/25 | Loss: 0.00082633
Iteration 20/25 | Loss: 0.00082633
Iteration 21/25 | Loss: 0.00082633
Iteration 22/25 | Loss: 0.00082633
Iteration 23/25 | Loss: 0.00082633
Iteration 24/25 | Loss: 0.00082633
Iteration 25/25 | Loss: 0.00082632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92589605
Iteration 2/25 | Loss: 0.00042214
Iteration 3/25 | Loss: 0.00042214
Iteration 4/25 | Loss: 0.00042214
Iteration 5/25 | Loss: 0.00042214
Iteration 6/25 | Loss: 0.00042214
Iteration 7/25 | Loss: 0.00042214
Iteration 8/25 | Loss: 0.00042214
Iteration 9/25 | Loss: 0.00042214
Iteration 10/25 | Loss: 0.00042214
Iteration 11/25 | Loss: 0.00042214
Iteration 12/25 | Loss: 0.00042214
Iteration 13/25 | Loss: 0.00042214
Iteration 14/25 | Loss: 0.00042214
Iteration 15/25 | Loss: 0.00042214
Iteration 16/25 | Loss: 0.00042214
Iteration 17/25 | Loss: 0.00042214
Iteration 18/25 | Loss: 0.00042214
Iteration 19/25 | Loss: 0.00042214
Iteration 20/25 | Loss: 0.00042214
Iteration 21/25 | Loss: 0.00042214
Iteration 22/25 | Loss: 0.00042214
Iteration 23/25 | Loss: 0.00042214
Iteration 24/25 | Loss: 0.00042214
Iteration 25/25 | Loss: 0.00042214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042214
Iteration 2/1000 | Loss: 0.00002142
Iteration 3/1000 | Loss: 0.00001640
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001378
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00047711
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001092
Iteration 17/1000 | Loss: 0.00001092
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001087
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001087
Iteration 23/1000 | Loss: 0.00001086
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001085
Iteration 26/1000 | Loss: 0.00001085
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001082
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001066
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001062
Iteration 66/1000 | Loss: 0.00001062
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001062
Iteration 69/1000 | Loss: 0.00001062
Iteration 70/1000 | Loss: 0.00001062
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001061
Iteration 73/1000 | Loss: 0.00001061
Iteration 74/1000 | Loss: 0.00001061
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001060
Iteration 77/1000 | Loss: 0.00001060
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001059
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001055
Iteration 93/1000 | Loss: 0.00001055
Iteration 94/1000 | Loss: 0.00001055
Iteration 95/1000 | Loss: 0.00001055
Iteration 96/1000 | Loss: 0.00001055
Iteration 97/1000 | Loss: 0.00001055
Iteration 98/1000 | Loss: 0.00001055
Iteration 99/1000 | Loss: 0.00001054
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001053
Iteration 103/1000 | Loss: 0.00001053
Iteration 104/1000 | Loss: 0.00001053
Iteration 105/1000 | Loss: 0.00001052
Iteration 106/1000 | Loss: 0.00001052
Iteration 107/1000 | Loss: 0.00001052
Iteration 108/1000 | Loss: 0.00001052
Iteration 109/1000 | Loss: 0.00001051
Iteration 110/1000 | Loss: 0.00001051
Iteration 111/1000 | Loss: 0.00001051
Iteration 112/1000 | Loss: 0.00001051
Iteration 113/1000 | Loss: 0.00001050
Iteration 114/1000 | Loss: 0.00001050
Iteration 115/1000 | Loss: 0.00001050
Iteration 116/1000 | Loss: 0.00001050
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001050
Iteration 120/1000 | Loss: 0.00001050
Iteration 121/1000 | Loss: 0.00001050
Iteration 122/1000 | Loss: 0.00001050
Iteration 123/1000 | Loss: 0.00001050
Iteration 124/1000 | Loss: 0.00001050
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001049
Iteration 127/1000 | Loss: 0.00001049
Iteration 128/1000 | Loss: 0.00001049
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001049
Iteration 133/1000 | Loss: 0.00001049
Iteration 134/1000 | Loss: 0.00001049
Iteration 135/1000 | Loss: 0.00001049
Iteration 136/1000 | Loss: 0.00001049
Iteration 137/1000 | Loss: 0.00001049
Iteration 138/1000 | Loss: 0.00001049
Iteration 139/1000 | Loss: 0.00001048
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001048
Iteration 142/1000 | Loss: 0.00001048
Iteration 143/1000 | Loss: 0.00001048
Iteration 144/1000 | Loss: 0.00001048
Iteration 145/1000 | Loss: 0.00001048
Iteration 146/1000 | Loss: 0.00001048
Iteration 147/1000 | Loss: 0.00001048
Iteration 148/1000 | Loss: 0.00001048
Iteration 149/1000 | Loss: 0.00001048
Iteration 150/1000 | Loss: 0.00001048
Iteration 151/1000 | Loss: 0.00001048
Iteration 152/1000 | Loss: 0.00001048
Iteration 153/1000 | Loss: 0.00001048
Iteration 154/1000 | Loss: 0.00001048
Iteration 155/1000 | Loss: 0.00001048
Iteration 156/1000 | Loss: 0.00001048
Iteration 157/1000 | Loss: 0.00001048
Iteration 158/1000 | Loss: 0.00001048
Iteration 159/1000 | Loss: 0.00001048
Iteration 160/1000 | Loss: 0.00001048
Iteration 161/1000 | Loss: 0.00001048
Iteration 162/1000 | Loss: 0.00001047
Iteration 163/1000 | Loss: 0.00001047
Iteration 164/1000 | Loss: 0.00001047
Iteration 165/1000 | Loss: 0.00001047
Iteration 166/1000 | Loss: 0.00001047
Iteration 167/1000 | Loss: 0.00001047
Iteration 168/1000 | Loss: 0.00001047
Iteration 169/1000 | Loss: 0.00001047
Iteration 170/1000 | Loss: 0.00001047
Iteration 171/1000 | Loss: 0.00001047
Iteration 172/1000 | Loss: 0.00001047
Iteration 173/1000 | Loss: 0.00001047
Iteration 174/1000 | Loss: 0.00001047
Iteration 175/1000 | Loss: 0.00001047
Iteration 176/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.0473640031705145e-05, 1.0473640031705145e-05, 1.0473640031705145e-05, 1.0473640031705145e-05, 1.0473640031705145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0473640031705145e-05

Optimization complete. Final v2v error: 2.753807306289673 mm

Highest mean error: 3.301253080368042 mm for frame 170

Lowest mean error: 2.3510332107543945 mm for frame 193

Saving results

Total time: 67.41174483299255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629773
Iteration 2/25 | Loss: 0.00134410
Iteration 3/25 | Loss: 0.00096896
Iteration 4/25 | Loss: 0.00092431
Iteration 5/25 | Loss: 0.00091978
Iteration 6/25 | Loss: 0.00091873
Iteration 7/25 | Loss: 0.00091850
Iteration 8/25 | Loss: 0.00091850
Iteration 9/25 | Loss: 0.00091850
Iteration 10/25 | Loss: 0.00091850
Iteration 11/25 | Loss: 0.00091850
Iteration 12/25 | Loss: 0.00091850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009184995433315635, 0.0009184995433315635, 0.0009184995433315635, 0.0009184995433315635, 0.0009184995433315635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009184995433315635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20135677
Iteration 2/25 | Loss: 0.00027820
Iteration 3/25 | Loss: 0.00027817
Iteration 4/25 | Loss: 0.00027817
Iteration 5/25 | Loss: 0.00027817
Iteration 6/25 | Loss: 0.00027817
Iteration 7/25 | Loss: 0.00027817
Iteration 8/25 | Loss: 0.00027817
Iteration 9/25 | Loss: 0.00027817
Iteration 10/25 | Loss: 0.00027817
Iteration 11/25 | Loss: 0.00027817
Iteration 12/25 | Loss: 0.00027817
Iteration 13/25 | Loss: 0.00027817
Iteration 14/25 | Loss: 0.00027817
Iteration 15/25 | Loss: 0.00027817
Iteration 16/25 | Loss: 0.00027817
Iteration 17/25 | Loss: 0.00027817
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00027817144291475415, 0.00027817144291475415, 0.00027817144291475415, 0.00027817144291475415, 0.00027817144291475415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027817144291475415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027817
Iteration 2/1000 | Loss: 0.00002971
Iteration 3/1000 | Loss: 0.00001881
Iteration 4/1000 | Loss: 0.00001532
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001366
Iteration 7/1000 | Loss: 0.00001322
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001253
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001238
Iteration 38/1000 | Loss: 0.00001237
Iteration 39/1000 | Loss: 0.00001237
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001233
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001218
Iteration 104/1000 | Loss: 0.00001218
Iteration 105/1000 | Loss: 0.00001218
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001218
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.2181239071651362e-05, 1.2181239071651362e-05, 1.2181239071651362e-05, 1.2181239071651362e-05, 1.2181239071651362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2181239071651362e-05

Optimization complete. Final v2v error: 2.9024040699005127 mm

Highest mean error: 3.7002761363983154 mm for frame 55

Lowest mean error: 2.673675775527954 mm for frame 0

Saving results

Total time: 33.79078817367554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846131
Iteration 2/25 | Loss: 0.00134360
Iteration 3/25 | Loss: 0.00095105
Iteration 4/25 | Loss: 0.00090477
Iteration 5/25 | Loss: 0.00089788
Iteration 6/25 | Loss: 0.00089484
Iteration 7/25 | Loss: 0.00089329
Iteration 8/25 | Loss: 0.00089288
Iteration 9/25 | Loss: 0.00089617
Iteration 10/25 | Loss: 0.00089765
Iteration 11/25 | Loss: 0.00089347
Iteration 12/25 | Loss: 0.00089032
Iteration 13/25 | Loss: 0.00088899
Iteration 14/25 | Loss: 0.00089250
Iteration 15/25 | Loss: 0.00088988
Iteration 16/25 | Loss: 0.00088868
Iteration 17/25 | Loss: 0.00089193
Iteration 18/25 | Loss: 0.00089114
Iteration 19/25 | Loss: 0.00088960
Iteration 20/25 | Loss: 0.00088868
Iteration 21/25 | Loss: 0.00088847
Iteration 22/25 | Loss: 0.00089030
Iteration 23/25 | Loss: 0.00088955
Iteration 24/25 | Loss: 0.00088891
Iteration 25/25 | Loss: 0.00088978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32134914
Iteration 2/25 | Loss: 0.00042203
Iteration 3/25 | Loss: 0.00042202
Iteration 4/25 | Loss: 0.00042202
Iteration 5/25 | Loss: 0.00042202
Iteration 6/25 | Loss: 0.00042202
Iteration 7/25 | Loss: 0.00042202
Iteration 8/25 | Loss: 0.00042202
Iteration 9/25 | Loss: 0.00042202
Iteration 10/25 | Loss: 0.00042202
Iteration 11/25 | Loss: 0.00042202
Iteration 12/25 | Loss: 0.00042202
Iteration 13/25 | Loss: 0.00042202
Iteration 14/25 | Loss: 0.00042202
Iteration 15/25 | Loss: 0.00042202
Iteration 16/25 | Loss: 0.00042202
Iteration 17/25 | Loss: 0.00042202
Iteration 18/25 | Loss: 0.00042202
Iteration 19/25 | Loss: 0.00042202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000422018812969327, 0.000422018812969327, 0.000422018812969327, 0.000422018812969327, 0.000422018812969327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000422018812969327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042202
Iteration 2/1000 | Loss: 0.00004816
Iteration 3/1000 | Loss: 0.00019082
Iteration 4/1000 | Loss: 0.00024758
Iteration 5/1000 | Loss: 0.00010596
Iteration 6/1000 | Loss: 0.00004132
Iteration 7/1000 | Loss: 0.00002765
Iteration 8/1000 | Loss: 0.00003053
Iteration 9/1000 | Loss: 0.00002262
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001573
Iteration 12/1000 | Loss: 0.00002419
Iteration 13/1000 | Loss: 0.00002444
Iteration 14/1000 | Loss: 0.00001783
Iteration 15/1000 | Loss: 0.00002512
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00002157
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00002490
Iteration 20/1000 | Loss: 0.00002364
Iteration 21/1000 | Loss: 0.00002315
Iteration 22/1000 | Loss: 0.00001785
Iteration 23/1000 | Loss: 0.00002507
Iteration 24/1000 | Loss: 0.00002413
Iteration 25/1000 | Loss: 0.00002581
Iteration 26/1000 | Loss: 0.00002441
Iteration 27/1000 | Loss: 0.00001964
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002492
Iteration 31/1000 | Loss: 0.00002594
Iteration 32/1000 | Loss: 0.00002061
Iteration 33/1000 | Loss: 0.00002365
Iteration 34/1000 | Loss: 0.00002368
Iteration 35/1000 | Loss: 0.00002468
Iteration 36/1000 | Loss: 0.00002373
Iteration 37/1000 | Loss: 0.00002342
Iteration 38/1000 | Loss: 0.00002351
Iteration 39/1000 | Loss: 0.00002382
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002423
Iteration 42/1000 | Loss: 0.00002545
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00002490
Iteration 45/1000 | Loss: 0.00001830
Iteration 46/1000 | Loss: 0.00002780
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00002261
Iteration 49/1000 | Loss: 0.00002428
Iteration 50/1000 | Loss: 0.00002276
Iteration 51/1000 | Loss: 0.00002625
Iteration 52/1000 | Loss: 0.00002652
Iteration 53/1000 | Loss: 0.00002274
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00003117
Iteration 57/1000 | Loss: 0.00001990
Iteration 58/1000 | Loss: 0.00002410
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00002423
Iteration 61/1000 | Loss: 0.00002975
Iteration 62/1000 | Loss: 0.00002431
Iteration 63/1000 | Loss: 0.00003043
Iteration 64/1000 | Loss: 0.00003191
Iteration 65/1000 | Loss: 0.00002410
Iteration 66/1000 | Loss: 0.00002412
Iteration 67/1000 | Loss: 0.00002820
Iteration 68/1000 | Loss: 0.00002449
Iteration 69/1000 | Loss: 0.00002655
Iteration 70/1000 | Loss: 0.00002419
Iteration 71/1000 | Loss: 0.00002710
Iteration 72/1000 | Loss: 0.00002385
Iteration 73/1000 | Loss: 0.00002451
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001266
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00002368
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00002339
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002445
Iteration 82/1000 | Loss: 0.00002004
Iteration 83/1000 | Loss: 0.00002656
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00002420
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00002393
Iteration 90/1000 | Loss: 0.00002271
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001157
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001149
Iteration 119/1000 | Loss: 0.00001149
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001144
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001141
Iteration 138/1000 | Loss: 0.00001141
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001138
Iteration 143/1000 | Loss: 0.00001135
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001134
Iteration 146/1000 | Loss: 0.00001134
Iteration 147/1000 | Loss: 0.00001134
Iteration 148/1000 | Loss: 0.00001134
Iteration 149/1000 | Loss: 0.00001134
Iteration 150/1000 | Loss: 0.00001133
Iteration 151/1000 | Loss: 0.00001133
Iteration 152/1000 | Loss: 0.00001133
Iteration 153/1000 | Loss: 0.00001133
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001132
Iteration 156/1000 | Loss: 0.00001132
Iteration 157/1000 | Loss: 0.00001132
Iteration 158/1000 | Loss: 0.00001132
Iteration 159/1000 | Loss: 0.00001132
Iteration 160/1000 | Loss: 0.00001132
Iteration 161/1000 | Loss: 0.00001132
Iteration 162/1000 | Loss: 0.00001132
Iteration 163/1000 | Loss: 0.00001132
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00001132
Iteration 166/1000 | Loss: 0.00001132
Iteration 167/1000 | Loss: 0.00001131
Iteration 168/1000 | Loss: 0.00001131
Iteration 169/1000 | Loss: 0.00001131
Iteration 170/1000 | Loss: 0.00001131
Iteration 171/1000 | Loss: 0.00001131
Iteration 172/1000 | Loss: 0.00001131
Iteration 173/1000 | Loss: 0.00001131
Iteration 174/1000 | Loss: 0.00001131
Iteration 175/1000 | Loss: 0.00001131
Iteration 176/1000 | Loss: 0.00001131
Iteration 177/1000 | Loss: 0.00001131
Iteration 178/1000 | Loss: 0.00001131
Iteration 179/1000 | Loss: 0.00001131
Iteration 180/1000 | Loss: 0.00001131
Iteration 181/1000 | Loss: 0.00001131
Iteration 182/1000 | Loss: 0.00001131
Iteration 183/1000 | Loss: 0.00001131
Iteration 184/1000 | Loss: 0.00001131
Iteration 185/1000 | Loss: 0.00001131
Iteration 186/1000 | Loss: 0.00001131
Iteration 187/1000 | Loss: 0.00001131
Iteration 188/1000 | Loss: 0.00001131
Iteration 189/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1310788977425545e-05, 1.1310788977425545e-05, 1.1310788977425545e-05, 1.1310788977425545e-05, 1.1310788977425545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1310788977425545e-05

Optimization complete. Final v2v error: 2.8466200828552246 mm

Highest mean error: 4.383530139923096 mm for frame 51

Lowest mean error: 2.426542043685913 mm for frame 115

Saving results

Total time: 217.91666746139526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837560
Iteration 2/25 | Loss: 0.00119148
Iteration 3/25 | Loss: 0.00101034
Iteration 4/25 | Loss: 0.00098005
Iteration 5/25 | Loss: 0.00097289
Iteration 6/25 | Loss: 0.00097190
Iteration 7/25 | Loss: 0.00097190
Iteration 8/25 | Loss: 0.00097190
Iteration 9/25 | Loss: 0.00097190
Iteration 10/25 | Loss: 0.00097190
Iteration 11/25 | Loss: 0.00097190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009718985529616475, 0.0009718985529616475, 0.0009718985529616475, 0.0009718985529616475, 0.0009718985529616475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009718985529616475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92842931
Iteration 2/25 | Loss: 0.00032952
Iteration 3/25 | Loss: 0.00032952
Iteration 4/25 | Loss: 0.00032952
Iteration 5/25 | Loss: 0.00032952
Iteration 6/25 | Loss: 0.00032952
Iteration 7/25 | Loss: 0.00032952
Iteration 8/25 | Loss: 0.00032952
Iteration 9/25 | Loss: 0.00032952
Iteration 10/25 | Loss: 0.00032952
Iteration 11/25 | Loss: 0.00032951
Iteration 12/25 | Loss: 0.00032951
Iteration 13/25 | Loss: 0.00032951
Iteration 14/25 | Loss: 0.00032951
Iteration 15/25 | Loss: 0.00032951
Iteration 16/25 | Loss: 0.00032951
Iteration 17/25 | Loss: 0.00032951
Iteration 18/25 | Loss: 0.00032951
Iteration 19/25 | Loss: 0.00032951
Iteration 20/25 | Loss: 0.00032951
Iteration 21/25 | Loss: 0.00032951
Iteration 22/25 | Loss: 0.00032951
Iteration 23/25 | Loss: 0.00032951
Iteration 24/25 | Loss: 0.00032951
Iteration 25/25 | Loss: 0.00032951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032951
Iteration 2/1000 | Loss: 0.00003651
Iteration 3/1000 | Loss: 0.00002901
Iteration 4/1000 | Loss: 0.00002634
Iteration 5/1000 | Loss: 0.00002527
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002397
Iteration 8/1000 | Loss: 0.00002354
Iteration 9/1000 | Loss: 0.00002337
Iteration 10/1000 | Loss: 0.00002322
Iteration 11/1000 | Loss: 0.00002322
Iteration 12/1000 | Loss: 0.00002321
Iteration 13/1000 | Loss: 0.00002304
Iteration 14/1000 | Loss: 0.00002303
Iteration 15/1000 | Loss: 0.00002303
Iteration 16/1000 | Loss: 0.00002302
Iteration 17/1000 | Loss: 0.00002300
Iteration 18/1000 | Loss: 0.00002300
Iteration 19/1000 | Loss: 0.00002300
Iteration 20/1000 | Loss: 0.00002299
Iteration 21/1000 | Loss: 0.00002299
Iteration 22/1000 | Loss: 0.00002299
Iteration 23/1000 | Loss: 0.00002299
Iteration 24/1000 | Loss: 0.00002299
Iteration 25/1000 | Loss: 0.00002299
Iteration 26/1000 | Loss: 0.00002299
Iteration 27/1000 | Loss: 0.00002298
Iteration 28/1000 | Loss: 0.00002298
Iteration 29/1000 | Loss: 0.00002298
Iteration 30/1000 | Loss: 0.00002298
Iteration 31/1000 | Loss: 0.00002294
Iteration 32/1000 | Loss: 0.00002294
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002290
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002290
Iteration 40/1000 | Loss: 0.00002289
Iteration 41/1000 | Loss: 0.00002289
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002289
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002287
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002287
Iteration 50/1000 | Loss: 0.00002287
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002287
Iteration 53/1000 | Loss: 0.00002287
Iteration 54/1000 | Loss: 0.00002286
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002286
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002286
Iteration 59/1000 | Loss: 0.00002286
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002285
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002285
Iteration 65/1000 | Loss: 0.00002285
Iteration 66/1000 | Loss: 0.00002285
Iteration 67/1000 | Loss: 0.00002285
Iteration 68/1000 | Loss: 0.00002285
Iteration 69/1000 | Loss: 0.00002284
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002284
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002284
Iteration 76/1000 | Loss: 0.00002284
Iteration 77/1000 | Loss: 0.00002283
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002283
Iteration 80/1000 | Loss: 0.00002283
Iteration 81/1000 | Loss: 0.00002283
Iteration 82/1000 | Loss: 0.00002283
Iteration 83/1000 | Loss: 0.00002283
Iteration 84/1000 | Loss: 0.00002283
Iteration 85/1000 | Loss: 0.00002283
Iteration 86/1000 | Loss: 0.00002283
Iteration 87/1000 | Loss: 0.00002282
Iteration 88/1000 | Loss: 0.00002282
Iteration 89/1000 | Loss: 0.00002282
Iteration 90/1000 | Loss: 0.00002282
Iteration 91/1000 | Loss: 0.00002282
Iteration 92/1000 | Loss: 0.00002282
Iteration 93/1000 | Loss: 0.00002282
Iteration 94/1000 | Loss: 0.00002282
Iteration 95/1000 | Loss: 0.00002282
Iteration 96/1000 | Loss: 0.00002282
Iteration 97/1000 | Loss: 0.00002282
Iteration 98/1000 | Loss: 0.00002281
Iteration 99/1000 | Loss: 0.00002281
Iteration 100/1000 | Loss: 0.00002281
Iteration 101/1000 | Loss: 0.00002281
Iteration 102/1000 | Loss: 0.00002281
Iteration 103/1000 | Loss: 0.00002281
Iteration 104/1000 | Loss: 0.00002281
Iteration 105/1000 | Loss: 0.00002281
Iteration 106/1000 | Loss: 0.00002281
Iteration 107/1000 | Loss: 0.00002281
Iteration 108/1000 | Loss: 0.00002280
Iteration 109/1000 | Loss: 0.00002280
Iteration 110/1000 | Loss: 0.00002280
Iteration 111/1000 | Loss: 0.00002280
Iteration 112/1000 | Loss: 0.00002280
Iteration 113/1000 | Loss: 0.00002280
Iteration 114/1000 | Loss: 0.00002280
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002280
Iteration 118/1000 | Loss: 0.00002280
Iteration 119/1000 | Loss: 0.00002280
Iteration 120/1000 | Loss: 0.00002280
Iteration 121/1000 | Loss: 0.00002280
Iteration 122/1000 | Loss: 0.00002280
Iteration 123/1000 | Loss: 0.00002280
Iteration 124/1000 | Loss: 0.00002280
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002280
Iteration 127/1000 | Loss: 0.00002280
Iteration 128/1000 | Loss: 0.00002280
Iteration 129/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.2800044462201186e-05, 2.2800044462201186e-05, 2.2800044462201186e-05, 2.2800044462201186e-05, 2.2800044462201186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2800044462201186e-05

Optimization complete. Final v2v error: 4.0934553146362305 mm

Highest mean error: 4.397244453430176 mm for frame 29

Lowest mean error: 3.9328677654266357 mm for frame 13

Saving results

Total time: 30.678111791610718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00208877
Iteration 2/25 | Loss: 0.00094000
Iteration 3/25 | Loss: 0.00084477
Iteration 4/25 | Loss: 0.00081999
Iteration 5/25 | Loss: 0.00080937
Iteration 6/25 | Loss: 0.00080633
Iteration 7/25 | Loss: 0.00080519
Iteration 8/25 | Loss: 0.00080505
Iteration 9/25 | Loss: 0.00080505
Iteration 10/25 | Loss: 0.00080505
Iteration 11/25 | Loss: 0.00080505
Iteration 12/25 | Loss: 0.00080505
Iteration 13/25 | Loss: 0.00080505
Iteration 14/25 | Loss: 0.00080505
Iteration 15/25 | Loss: 0.00080505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008050509495660663, 0.0008050509495660663, 0.0008050509495660663, 0.0008050509495660663, 0.0008050509495660663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008050509495660663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35197318
Iteration 2/25 | Loss: 0.00067457
Iteration 3/25 | Loss: 0.00067457
Iteration 4/25 | Loss: 0.00067457
Iteration 5/25 | Loss: 0.00067457
Iteration 6/25 | Loss: 0.00067457
Iteration 7/25 | Loss: 0.00067457
Iteration 8/25 | Loss: 0.00067457
Iteration 9/25 | Loss: 0.00067457
Iteration 10/25 | Loss: 0.00067457
Iteration 11/25 | Loss: 0.00067457
Iteration 12/25 | Loss: 0.00067457
Iteration 13/25 | Loss: 0.00067457
Iteration 14/25 | Loss: 0.00067457
Iteration 15/25 | Loss: 0.00067457
Iteration 16/25 | Loss: 0.00067457
Iteration 17/25 | Loss: 0.00067457
Iteration 18/25 | Loss: 0.00067457
Iteration 19/25 | Loss: 0.00067457
Iteration 20/25 | Loss: 0.00067457
Iteration 21/25 | Loss: 0.00067457
Iteration 22/25 | Loss: 0.00067457
Iteration 23/25 | Loss: 0.00067457
Iteration 24/25 | Loss: 0.00067457
Iteration 25/25 | Loss: 0.00067457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067457
Iteration 2/1000 | Loss: 0.00003033
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001223
Iteration 5/1000 | Loss: 0.00001127
Iteration 6/1000 | Loss: 0.00001054
Iteration 7/1000 | Loss: 0.00001010
Iteration 8/1000 | Loss: 0.00000988
Iteration 9/1000 | Loss: 0.00000970
Iteration 10/1000 | Loss: 0.00000967
Iteration 11/1000 | Loss: 0.00000960
Iteration 12/1000 | Loss: 0.00000958
Iteration 13/1000 | Loss: 0.00000953
Iteration 14/1000 | Loss: 0.00000945
Iteration 15/1000 | Loss: 0.00000944
Iteration 16/1000 | Loss: 0.00000944
Iteration 17/1000 | Loss: 0.00000943
Iteration 18/1000 | Loss: 0.00000941
Iteration 19/1000 | Loss: 0.00000940
Iteration 20/1000 | Loss: 0.00000940
Iteration 21/1000 | Loss: 0.00000940
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000939
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000939
Iteration 28/1000 | Loss: 0.00000939
Iteration 29/1000 | Loss: 0.00000939
Iteration 30/1000 | Loss: 0.00000939
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000936
Iteration 34/1000 | Loss: 0.00000935
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000934
Iteration 37/1000 | Loss: 0.00000934
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000934
Iteration 40/1000 | Loss: 0.00000934
Iteration 41/1000 | Loss: 0.00000933
Iteration 42/1000 | Loss: 0.00000933
Iteration 43/1000 | Loss: 0.00000933
Iteration 44/1000 | Loss: 0.00000933
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000933
Iteration 47/1000 | Loss: 0.00000932
Iteration 48/1000 | Loss: 0.00000932
Iteration 49/1000 | Loss: 0.00000931
Iteration 50/1000 | Loss: 0.00000931
Iteration 51/1000 | Loss: 0.00000931
Iteration 52/1000 | Loss: 0.00000931
Iteration 53/1000 | Loss: 0.00000931
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000930
Iteration 58/1000 | Loss: 0.00000930
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000928
Iteration 63/1000 | Loss: 0.00000928
Iteration 64/1000 | Loss: 0.00000927
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000927
Iteration 67/1000 | Loss: 0.00000927
Iteration 68/1000 | Loss: 0.00000927
Iteration 69/1000 | Loss: 0.00000927
Iteration 70/1000 | Loss: 0.00000926
Iteration 71/1000 | Loss: 0.00000926
Iteration 72/1000 | Loss: 0.00000926
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000926
Iteration 75/1000 | Loss: 0.00000926
Iteration 76/1000 | Loss: 0.00000926
Iteration 77/1000 | Loss: 0.00000926
Iteration 78/1000 | Loss: 0.00000925
Iteration 79/1000 | Loss: 0.00000925
Iteration 80/1000 | Loss: 0.00000925
Iteration 81/1000 | Loss: 0.00000925
Iteration 82/1000 | Loss: 0.00000924
Iteration 83/1000 | Loss: 0.00000924
Iteration 84/1000 | Loss: 0.00000924
Iteration 85/1000 | Loss: 0.00000924
Iteration 86/1000 | Loss: 0.00000924
Iteration 87/1000 | Loss: 0.00000924
Iteration 88/1000 | Loss: 0.00000924
Iteration 89/1000 | Loss: 0.00000924
Iteration 90/1000 | Loss: 0.00000923
Iteration 91/1000 | Loss: 0.00000923
Iteration 92/1000 | Loss: 0.00000923
Iteration 93/1000 | Loss: 0.00000923
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000923
Iteration 96/1000 | Loss: 0.00000923
Iteration 97/1000 | Loss: 0.00000923
Iteration 98/1000 | Loss: 0.00000923
Iteration 99/1000 | Loss: 0.00000922
Iteration 100/1000 | Loss: 0.00000922
Iteration 101/1000 | Loss: 0.00000922
Iteration 102/1000 | Loss: 0.00000922
Iteration 103/1000 | Loss: 0.00000922
Iteration 104/1000 | Loss: 0.00000922
Iteration 105/1000 | Loss: 0.00000922
Iteration 106/1000 | Loss: 0.00000922
Iteration 107/1000 | Loss: 0.00000921
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000921
Iteration 111/1000 | Loss: 0.00000921
Iteration 112/1000 | Loss: 0.00000921
Iteration 113/1000 | Loss: 0.00000921
Iteration 114/1000 | Loss: 0.00000921
Iteration 115/1000 | Loss: 0.00000921
Iteration 116/1000 | Loss: 0.00000921
Iteration 117/1000 | Loss: 0.00000921
Iteration 118/1000 | Loss: 0.00000921
Iteration 119/1000 | Loss: 0.00000921
Iteration 120/1000 | Loss: 0.00000921
Iteration 121/1000 | Loss: 0.00000921
Iteration 122/1000 | Loss: 0.00000920
Iteration 123/1000 | Loss: 0.00000920
Iteration 124/1000 | Loss: 0.00000920
Iteration 125/1000 | Loss: 0.00000919
Iteration 126/1000 | Loss: 0.00000919
Iteration 127/1000 | Loss: 0.00000919
Iteration 128/1000 | Loss: 0.00000919
Iteration 129/1000 | Loss: 0.00000919
Iteration 130/1000 | Loss: 0.00000919
Iteration 131/1000 | Loss: 0.00000919
Iteration 132/1000 | Loss: 0.00000919
Iteration 133/1000 | Loss: 0.00000919
Iteration 134/1000 | Loss: 0.00000919
Iteration 135/1000 | Loss: 0.00000919
Iteration 136/1000 | Loss: 0.00000919
Iteration 137/1000 | Loss: 0.00000919
Iteration 138/1000 | Loss: 0.00000919
Iteration 139/1000 | Loss: 0.00000919
Iteration 140/1000 | Loss: 0.00000919
Iteration 141/1000 | Loss: 0.00000919
Iteration 142/1000 | Loss: 0.00000919
Iteration 143/1000 | Loss: 0.00000919
Iteration 144/1000 | Loss: 0.00000919
Iteration 145/1000 | Loss: 0.00000919
Iteration 146/1000 | Loss: 0.00000919
Iteration 147/1000 | Loss: 0.00000919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [9.190368473355193e-06, 9.190368473355193e-06, 9.190368473355193e-06, 9.190368473355193e-06, 9.190368473355193e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.190368473355193e-06

Optimization complete. Final v2v error: 2.592829465866089 mm

Highest mean error: 3.015244722366333 mm for frame 43

Lowest mean error: 2.343695640563965 mm for frame 188

Saving results

Total time: 38.60014343261719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973328
Iteration 2/25 | Loss: 0.00181449
Iteration 3/25 | Loss: 0.00116379
Iteration 4/25 | Loss: 0.00114506
Iteration 5/25 | Loss: 0.00114214
Iteration 6/25 | Loss: 0.00114119
Iteration 7/25 | Loss: 0.00114119
Iteration 8/25 | Loss: 0.00114119
Iteration 9/25 | Loss: 0.00114119
Iteration 10/25 | Loss: 0.00114119
Iteration 11/25 | Loss: 0.00114119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011411899467930198, 0.0011411899467930198, 0.0011411899467930198, 0.0011411899467930198, 0.0011411899467930198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011411899467930198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23710394
Iteration 2/25 | Loss: 0.00059488
Iteration 3/25 | Loss: 0.00059488
Iteration 4/25 | Loss: 0.00059488
Iteration 5/25 | Loss: 0.00059488
Iteration 6/25 | Loss: 0.00059488
Iteration 7/25 | Loss: 0.00059488
Iteration 8/25 | Loss: 0.00059488
Iteration 9/25 | Loss: 0.00059488
Iteration 10/25 | Loss: 0.00059488
Iteration 11/25 | Loss: 0.00059488
Iteration 12/25 | Loss: 0.00059488
Iteration 13/25 | Loss: 0.00059488
Iteration 14/25 | Loss: 0.00059488
Iteration 15/25 | Loss: 0.00059488
Iteration 16/25 | Loss: 0.00059488
Iteration 17/25 | Loss: 0.00059488
Iteration 18/25 | Loss: 0.00059488
Iteration 19/25 | Loss: 0.00059488
Iteration 20/25 | Loss: 0.00059488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005948758916929364, 0.0005948758916929364, 0.0005948758916929364, 0.0005948758916929364, 0.0005948758916929364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005948758916929364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059488
Iteration 2/1000 | Loss: 0.00007542
Iteration 3/1000 | Loss: 0.00005068
Iteration 4/1000 | Loss: 0.00004326
Iteration 5/1000 | Loss: 0.00004082
Iteration 6/1000 | Loss: 0.00003952
Iteration 7/1000 | Loss: 0.00003870
Iteration 8/1000 | Loss: 0.00003786
Iteration 9/1000 | Loss: 0.00003680
Iteration 10/1000 | Loss: 0.00003616
Iteration 11/1000 | Loss: 0.00003561
Iteration 12/1000 | Loss: 0.00003520
Iteration 13/1000 | Loss: 0.00003476
Iteration 14/1000 | Loss: 0.00003434
Iteration 15/1000 | Loss: 0.00003398
Iteration 16/1000 | Loss: 0.00003375
Iteration 17/1000 | Loss: 0.00003357
Iteration 18/1000 | Loss: 0.00003341
Iteration 19/1000 | Loss: 0.00003326
Iteration 20/1000 | Loss: 0.00003305
Iteration 21/1000 | Loss: 0.00003286
Iteration 22/1000 | Loss: 0.00003270
Iteration 23/1000 | Loss: 0.00003259
Iteration 24/1000 | Loss: 0.00003244
Iteration 25/1000 | Loss: 0.00003234
Iteration 26/1000 | Loss: 0.00003232
Iteration 27/1000 | Loss: 0.00003226
Iteration 28/1000 | Loss: 0.00003224
Iteration 29/1000 | Loss: 0.00003224
Iteration 30/1000 | Loss: 0.00003224
Iteration 31/1000 | Loss: 0.00003223
Iteration 32/1000 | Loss: 0.00003223
Iteration 33/1000 | Loss: 0.00003223
Iteration 34/1000 | Loss: 0.00003223
Iteration 35/1000 | Loss: 0.00003223
Iteration 36/1000 | Loss: 0.00003222
Iteration 37/1000 | Loss: 0.00003221
Iteration 38/1000 | Loss: 0.00003221
Iteration 39/1000 | Loss: 0.00003220
Iteration 40/1000 | Loss: 0.00003220
Iteration 41/1000 | Loss: 0.00003219
Iteration 42/1000 | Loss: 0.00003219
Iteration 43/1000 | Loss: 0.00003219
Iteration 44/1000 | Loss: 0.00003219
Iteration 45/1000 | Loss: 0.00003218
Iteration 46/1000 | Loss: 0.00003218
Iteration 47/1000 | Loss: 0.00003217
Iteration 48/1000 | Loss: 0.00003217
Iteration 49/1000 | Loss: 0.00003217
Iteration 50/1000 | Loss: 0.00003217
Iteration 51/1000 | Loss: 0.00003216
Iteration 52/1000 | Loss: 0.00003216
Iteration 53/1000 | Loss: 0.00003216
Iteration 54/1000 | Loss: 0.00003216
Iteration 55/1000 | Loss: 0.00003216
Iteration 56/1000 | Loss: 0.00003216
Iteration 57/1000 | Loss: 0.00003216
Iteration 58/1000 | Loss: 0.00003216
Iteration 59/1000 | Loss: 0.00003216
Iteration 60/1000 | Loss: 0.00003216
Iteration 61/1000 | Loss: 0.00003216
Iteration 62/1000 | Loss: 0.00003216
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003216
Iteration 65/1000 | Loss: 0.00003216
Iteration 66/1000 | Loss: 0.00003216
Iteration 67/1000 | Loss: 0.00003216
Iteration 68/1000 | Loss: 0.00003216
Iteration 69/1000 | Loss: 0.00003216
Iteration 70/1000 | Loss: 0.00003216
Iteration 71/1000 | Loss: 0.00003216
Iteration 72/1000 | Loss: 0.00003216
Iteration 73/1000 | Loss: 0.00003216
Iteration 74/1000 | Loss: 0.00003216
Iteration 75/1000 | Loss: 0.00003216
Iteration 76/1000 | Loss: 0.00003216
Iteration 77/1000 | Loss: 0.00003216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [3.215857577743009e-05, 3.215857577743009e-05, 3.215857577743009e-05, 3.215857577743009e-05, 3.215857577743009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.215857577743009e-05

Optimization complete. Final v2v error: 4.532856464385986 mm

Highest mean error: 5.248453140258789 mm for frame 132

Lowest mean error: 4.149527549743652 mm for frame 59

Saving results

Total time: 48.86756134033203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799919
Iteration 2/25 | Loss: 0.00110468
Iteration 3/25 | Loss: 0.00086658
Iteration 4/25 | Loss: 0.00084546
Iteration 5/25 | Loss: 0.00084110
Iteration 6/25 | Loss: 0.00084074
Iteration 7/25 | Loss: 0.00084074
Iteration 8/25 | Loss: 0.00084074
Iteration 9/25 | Loss: 0.00084074
Iteration 10/25 | Loss: 0.00084074
Iteration 11/25 | Loss: 0.00084074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008407433051615953, 0.0008407433051615953, 0.0008407433051615953, 0.0008407433051615953, 0.0008407433051615953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008407433051615953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38440228
Iteration 2/25 | Loss: 0.00042076
Iteration 3/25 | Loss: 0.00042076
Iteration 4/25 | Loss: 0.00042076
Iteration 5/25 | Loss: 0.00042076
Iteration 6/25 | Loss: 0.00042076
Iteration 7/25 | Loss: 0.00042076
Iteration 8/25 | Loss: 0.00042076
Iteration 9/25 | Loss: 0.00042076
Iteration 10/25 | Loss: 0.00042076
Iteration 11/25 | Loss: 0.00042076
Iteration 12/25 | Loss: 0.00042076
Iteration 13/25 | Loss: 0.00042076
Iteration 14/25 | Loss: 0.00042076
Iteration 15/25 | Loss: 0.00042076
Iteration 16/25 | Loss: 0.00042076
Iteration 17/25 | Loss: 0.00042076
Iteration 18/25 | Loss: 0.00042076
Iteration 19/25 | Loss: 0.00042076
Iteration 20/25 | Loss: 0.00042076
Iteration 21/25 | Loss: 0.00042076
Iteration 22/25 | Loss: 0.00042076
Iteration 23/25 | Loss: 0.00042076
Iteration 24/25 | Loss: 0.00042076
Iteration 25/25 | Loss: 0.00042076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042076
Iteration 2/1000 | Loss: 0.00002059
Iteration 3/1000 | Loss: 0.00001185
Iteration 4/1000 | Loss: 0.00001054
Iteration 5/1000 | Loss: 0.00000990
Iteration 6/1000 | Loss: 0.00000948
Iteration 7/1000 | Loss: 0.00000922
Iteration 8/1000 | Loss: 0.00000911
Iteration 9/1000 | Loss: 0.00000909
Iteration 10/1000 | Loss: 0.00000891
Iteration 11/1000 | Loss: 0.00000891
Iteration 12/1000 | Loss: 0.00000890
Iteration 13/1000 | Loss: 0.00000886
Iteration 14/1000 | Loss: 0.00000884
Iteration 15/1000 | Loss: 0.00000883
Iteration 16/1000 | Loss: 0.00000883
Iteration 17/1000 | Loss: 0.00000882
Iteration 18/1000 | Loss: 0.00000881
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000880
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000880
Iteration 24/1000 | Loss: 0.00000879
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000878
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000877
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000877
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000875
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000871
Iteration 37/1000 | Loss: 0.00000869
Iteration 38/1000 | Loss: 0.00000868
Iteration 39/1000 | Loss: 0.00000867
Iteration 40/1000 | Loss: 0.00000867
Iteration 41/1000 | Loss: 0.00000867
Iteration 42/1000 | Loss: 0.00000866
Iteration 43/1000 | Loss: 0.00000866
Iteration 44/1000 | Loss: 0.00000866
Iteration 45/1000 | Loss: 0.00000866
Iteration 46/1000 | Loss: 0.00000866
Iteration 47/1000 | Loss: 0.00000866
Iteration 48/1000 | Loss: 0.00000866
Iteration 49/1000 | Loss: 0.00000865
Iteration 50/1000 | Loss: 0.00000865
Iteration 51/1000 | Loss: 0.00000865
Iteration 52/1000 | Loss: 0.00000865
Iteration 53/1000 | Loss: 0.00000865
Iteration 54/1000 | Loss: 0.00000864
Iteration 55/1000 | Loss: 0.00000864
Iteration 56/1000 | Loss: 0.00000864
Iteration 57/1000 | Loss: 0.00000863
Iteration 58/1000 | Loss: 0.00000863
Iteration 59/1000 | Loss: 0.00000863
Iteration 60/1000 | Loss: 0.00000863
Iteration 61/1000 | Loss: 0.00000863
Iteration 62/1000 | Loss: 0.00000863
Iteration 63/1000 | Loss: 0.00000863
Iteration 64/1000 | Loss: 0.00000863
Iteration 65/1000 | Loss: 0.00000863
Iteration 66/1000 | Loss: 0.00000863
Iteration 67/1000 | Loss: 0.00000863
Iteration 68/1000 | Loss: 0.00000862
Iteration 69/1000 | Loss: 0.00000862
Iteration 70/1000 | Loss: 0.00000861
Iteration 71/1000 | Loss: 0.00000861
Iteration 72/1000 | Loss: 0.00000860
Iteration 73/1000 | Loss: 0.00000860
Iteration 74/1000 | Loss: 0.00000860
Iteration 75/1000 | Loss: 0.00000860
Iteration 76/1000 | Loss: 0.00000860
Iteration 77/1000 | Loss: 0.00000860
Iteration 78/1000 | Loss: 0.00000860
Iteration 79/1000 | Loss: 0.00000859
Iteration 80/1000 | Loss: 0.00000859
Iteration 81/1000 | Loss: 0.00000859
Iteration 82/1000 | Loss: 0.00000859
Iteration 83/1000 | Loss: 0.00000859
Iteration 84/1000 | Loss: 0.00000859
Iteration 85/1000 | Loss: 0.00000859
Iteration 86/1000 | Loss: 0.00000859
Iteration 87/1000 | Loss: 0.00000859
Iteration 88/1000 | Loss: 0.00000859
Iteration 89/1000 | Loss: 0.00000859
Iteration 90/1000 | Loss: 0.00000859
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000858
Iteration 93/1000 | Loss: 0.00000858
Iteration 94/1000 | Loss: 0.00000858
Iteration 95/1000 | Loss: 0.00000858
Iteration 96/1000 | Loss: 0.00000858
Iteration 97/1000 | Loss: 0.00000858
Iteration 98/1000 | Loss: 0.00000858
Iteration 99/1000 | Loss: 0.00000858
Iteration 100/1000 | Loss: 0.00000858
Iteration 101/1000 | Loss: 0.00000857
Iteration 102/1000 | Loss: 0.00000857
Iteration 103/1000 | Loss: 0.00000857
Iteration 104/1000 | Loss: 0.00000857
Iteration 105/1000 | Loss: 0.00000857
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00000857
Iteration 108/1000 | Loss: 0.00000857
Iteration 109/1000 | Loss: 0.00000857
Iteration 110/1000 | Loss: 0.00000857
Iteration 111/1000 | Loss: 0.00000857
Iteration 112/1000 | Loss: 0.00000857
Iteration 113/1000 | Loss: 0.00000857
Iteration 114/1000 | Loss: 0.00000857
Iteration 115/1000 | Loss: 0.00000857
Iteration 116/1000 | Loss: 0.00000857
Iteration 117/1000 | Loss: 0.00000857
Iteration 118/1000 | Loss: 0.00000857
Iteration 119/1000 | Loss: 0.00000857
Iteration 120/1000 | Loss: 0.00000857
Iteration 121/1000 | Loss: 0.00000857
Iteration 122/1000 | Loss: 0.00000857
Iteration 123/1000 | Loss: 0.00000857
Iteration 124/1000 | Loss: 0.00000857
Iteration 125/1000 | Loss: 0.00000857
Iteration 126/1000 | Loss: 0.00000857
Iteration 127/1000 | Loss: 0.00000857
Iteration 128/1000 | Loss: 0.00000857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [8.572005754103884e-06, 8.572005754103884e-06, 8.572005754103884e-06, 8.572005754103884e-06, 8.572005754103884e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.572005754103884e-06

Optimization complete. Final v2v error: 2.4505295753479004 mm

Highest mean error: 2.641253709793091 mm for frame 61

Lowest mean error: 2.2444329261779785 mm for frame 235

Saving results

Total time: 33.75265073776245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488385
Iteration 2/25 | Loss: 0.00120774
Iteration 3/25 | Loss: 0.00095904
Iteration 4/25 | Loss: 0.00094793
Iteration 5/25 | Loss: 0.00094601
Iteration 6/25 | Loss: 0.00094584
Iteration 7/25 | Loss: 0.00094584
Iteration 8/25 | Loss: 0.00094584
Iteration 9/25 | Loss: 0.00094584
Iteration 10/25 | Loss: 0.00094584
Iteration 11/25 | Loss: 0.00094584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009458389249630272, 0.0009458389249630272, 0.0009458389249630272, 0.0009458389249630272, 0.0009458389249630272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009458389249630272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39188254
Iteration 2/25 | Loss: 0.00048821
Iteration 3/25 | Loss: 0.00048821
Iteration 4/25 | Loss: 0.00048821
Iteration 5/25 | Loss: 0.00048821
Iteration 6/25 | Loss: 0.00048821
Iteration 7/25 | Loss: 0.00048821
Iteration 8/25 | Loss: 0.00048821
Iteration 9/25 | Loss: 0.00048821
Iteration 10/25 | Loss: 0.00048821
Iteration 11/25 | Loss: 0.00048821
Iteration 12/25 | Loss: 0.00048821
Iteration 13/25 | Loss: 0.00048821
Iteration 14/25 | Loss: 0.00048821
Iteration 15/25 | Loss: 0.00048821
Iteration 16/25 | Loss: 0.00048821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004882055800408125, 0.0004882055800408125, 0.0004882055800408125, 0.0004882055800408125, 0.0004882055800408125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004882055800408125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048821
Iteration 2/1000 | Loss: 0.00003059
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00002097
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001905
Iteration 9/1000 | Loss: 0.00001892
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00001885
Iteration 12/1000 | Loss: 0.00001877
Iteration 13/1000 | Loss: 0.00001869
Iteration 14/1000 | Loss: 0.00001868
Iteration 15/1000 | Loss: 0.00001868
Iteration 16/1000 | Loss: 0.00001868
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001868
Iteration 19/1000 | Loss: 0.00001868
Iteration 20/1000 | Loss: 0.00001868
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001866
Iteration 25/1000 | Loss: 0.00001865
Iteration 26/1000 | Loss: 0.00001865
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001862
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001862
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001861
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001860
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001859
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001857
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001855
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001851
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001848
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001848
Iteration 64/1000 | Loss: 0.00001847
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001839
Iteration 78/1000 | Loss: 0.00001838
Iteration 79/1000 | Loss: 0.00001838
Iteration 80/1000 | Loss: 0.00001837
Iteration 81/1000 | Loss: 0.00001837
Iteration 82/1000 | Loss: 0.00001837
Iteration 83/1000 | Loss: 0.00001837
Iteration 84/1000 | Loss: 0.00001837
Iteration 85/1000 | Loss: 0.00001837
Iteration 86/1000 | Loss: 0.00001837
Iteration 87/1000 | Loss: 0.00001837
Iteration 88/1000 | Loss: 0.00001836
Iteration 89/1000 | Loss: 0.00001836
Iteration 90/1000 | Loss: 0.00001836
Iteration 91/1000 | Loss: 0.00001836
Iteration 92/1000 | Loss: 0.00001836
Iteration 93/1000 | Loss: 0.00001835
Iteration 94/1000 | Loss: 0.00001833
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001833
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001833
Iteration 101/1000 | Loss: 0.00001833
Iteration 102/1000 | Loss: 0.00001833
Iteration 103/1000 | Loss: 0.00001832
Iteration 104/1000 | Loss: 0.00001832
Iteration 105/1000 | Loss: 0.00001831
Iteration 106/1000 | Loss: 0.00001831
Iteration 107/1000 | Loss: 0.00001831
Iteration 108/1000 | Loss: 0.00001831
Iteration 109/1000 | Loss: 0.00001830
Iteration 110/1000 | Loss: 0.00001830
Iteration 111/1000 | Loss: 0.00001830
Iteration 112/1000 | Loss: 0.00001830
Iteration 113/1000 | Loss: 0.00001830
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001830
Iteration 116/1000 | Loss: 0.00001830
Iteration 117/1000 | Loss: 0.00001830
Iteration 118/1000 | Loss: 0.00001830
Iteration 119/1000 | Loss: 0.00001830
Iteration 120/1000 | Loss: 0.00001830
Iteration 121/1000 | Loss: 0.00001829
Iteration 122/1000 | Loss: 0.00001829
Iteration 123/1000 | Loss: 0.00001829
Iteration 124/1000 | Loss: 0.00001829
Iteration 125/1000 | Loss: 0.00001829
Iteration 126/1000 | Loss: 0.00001829
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Iteration 130/1000 | Loss: 0.00001828
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001827
Iteration 133/1000 | Loss: 0.00001827
Iteration 134/1000 | Loss: 0.00001827
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001827
Iteration 138/1000 | Loss: 0.00001827
Iteration 139/1000 | Loss: 0.00001827
Iteration 140/1000 | Loss: 0.00001827
Iteration 141/1000 | Loss: 0.00001827
Iteration 142/1000 | Loss: 0.00001827
Iteration 143/1000 | Loss: 0.00001827
Iteration 144/1000 | Loss: 0.00001827
Iteration 145/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.8272930901730433e-05, 1.8272930901730433e-05, 1.8272930901730433e-05, 1.8272930901730433e-05, 1.8272930901730433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8272930901730433e-05

Optimization complete. Final v2v error: 3.374191999435425 mm

Highest mean error: 3.940507411956787 mm for frame 157

Lowest mean error: 3.002131700515747 mm for frame 5

Saving results

Total time: 38.78128266334534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433189
Iteration 2/25 | Loss: 0.00098852
Iteration 3/25 | Loss: 0.00088185
Iteration 4/25 | Loss: 0.00085928
Iteration 5/25 | Loss: 0.00085274
Iteration 6/25 | Loss: 0.00085140
Iteration 7/25 | Loss: 0.00085136
Iteration 8/25 | Loss: 0.00085136
Iteration 9/25 | Loss: 0.00085136
Iteration 10/25 | Loss: 0.00085136
Iteration 11/25 | Loss: 0.00085136
Iteration 12/25 | Loss: 0.00085136
Iteration 13/25 | Loss: 0.00085136
Iteration 14/25 | Loss: 0.00085136
Iteration 15/25 | Loss: 0.00085136
Iteration 16/25 | Loss: 0.00085136
Iteration 17/25 | Loss: 0.00085136
Iteration 18/25 | Loss: 0.00085136
Iteration 19/25 | Loss: 0.00085136
Iteration 20/25 | Loss: 0.00085136
Iteration 21/25 | Loss: 0.00085136
Iteration 22/25 | Loss: 0.00085136
Iteration 23/25 | Loss: 0.00085136
Iteration 24/25 | Loss: 0.00085136
Iteration 25/25 | Loss: 0.00085136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37503719
Iteration 2/25 | Loss: 0.00040922
Iteration 3/25 | Loss: 0.00040922
Iteration 4/25 | Loss: 0.00040922
Iteration 5/25 | Loss: 0.00040922
Iteration 6/25 | Loss: 0.00040922
Iteration 7/25 | Loss: 0.00040921
Iteration 8/25 | Loss: 0.00040921
Iteration 9/25 | Loss: 0.00040921
Iteration 10/25 | Loss: 0.00040921
Iteration 11/25 | Loss: 0.00040921
Iteration 12/25 | Loss: 0.00040921
Iteration 13/25 | Loss: 0.00040921
Iteration 14/25 | Loss: 0.00040921
Iteration 15/25 | Loss: 0.00040921
Iteration 16/25 | Loss: 0.00040921
Iteration 17/25 | Loss: 0.00040921
Iteration 18/25 | Loss: 0.00040921
Iteration 19/25 | Loss: 0.00040921
Iteration 20/25 | Loss: 0.00040921
Iteration 21/25 | Loss: 0.00040921
Iteration 22/25 | Loss: 0.00040921
Iteration 23/25 | Loss: 0.00040921
Iteration 24/25 | Loss: 0.00040921
Iteration 25/25 | Loss: 0.00040921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040921
Iteration 2/1000 | Loss: 0.00002779
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001633
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001508
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001440
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001406
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001402
Iteration 19/1000 | Loss: 0.00001401
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001398
Iteration 26/1000 | Loss: 0.00001397
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001396
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001380
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001370
Iteration 101/1000 | Loss: 0.00001370
Iteration 102/1000 | Loss: 0.00001370
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001367
Iteration 113/1000 | Loss: 0.00001367
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001365
Iteration 142/1000 | Loss: 0.00001365
Iteration 143/1000 | Loss: 0.00001365
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001364
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001364
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001364
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001364
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001364
Iteration 161/1000 | Loss: 0.00001364
Iteration 162/1000 | Loss: 0.00001364
Iteration 163/1000 | Loss: 0.00001364
Iteration 164/1000 | Loss: 0.00001364
Iteration 165/1000 | Loss: 0.00001364
Iteration 166/1000 | Loss: 0.00001364
Iteration 167/1000 | Loss: 0.00001364
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001362
Iteration 185/1000 | Loss: 0.00001362
Iteration 186/1000 | Loss: 0.00001362
Iteration 187/1000 | Loss: 0.00001362
Iteration 188/1000 | Loss: 0.00001362
Iteration 189/1000 | Loss: 0.00001362
Iteration 190/1000 | Loss: 0.00001361
Iteration 191/1000 | Loss: 0.00001361
Iteration 192/1000 | Loss: 0.00001361
Iteration 193/1000 | Loss: 0.00001361
Iteration 194/1000 | Loss: 0.00001361
Iteration 195/1000 | Loss: 0.00001361
Iteration 196/1000 | Loss: 0.00001361
Iteration 197/1000 | Loss: 0.00001361
Iteration 198/1000 | Loss: 0.00001361
Iteration 199/1000 | Loss: 0.00001361
Iteration 200/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.3611735084850807e-05, 1.3611735084850807e-05, 1.3611735084850807e-05, 1.3611735084850807e-05, 1.3611735084850807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3611735084850807e-05

Optimization complete. Final v2v error: 3.097787618637085 mm

Highest mean error: 3.273164987564087 mm for frame 129

Lowest mean error: 2.8864879608154297 mm for frame 40

Saving results

Total time: 39.397719383239746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035100
Iteration 2/25 | Loss: 0.00218884
Iteration 3/25 | Loss: 0.00152645
Iteration 4/25 | Loss: 0.00125068
Iteration 5/25 | Loss: 0.00119070
Iteration 6/25 | Loss: 0.00113535
Iteration 7/25 | Loss: 0.00109842
Iteration 8/25 | Loss: 0.00107176
Iteration 9/25 | Loss: 0.00104622
Iteration 10/25 | Loss: 0.00104352
Iteration 11/25 | Loss: 0.00103441
Iteration 12/25 | Loss: 0.00103048
Iteration 13/25 | Loss: 0.00102751
Iteration 14/25 | Loss: 0.00103104
Iteration 15/25 | Loss: 0.00103046
Iteration 16/25 | Loss: 0.00103407
Iteration 17/25 | Loss: 0.00102572
Iteration 18/25 | Loss: 0.00101551
Iteration 19/25 | Loss: 0.00100778
Iteration 20/25 | Loss: 0.00100225
Iteration 21/25 | Loss: 0.00098342
Iteration 22/25 | Loss: 0.00097854
Iteration 23/25 | Loss: 0.00097649
Iteration 24/25 | Loss: 0.00097058
Iteration 25/25 | Loss: 0.00096715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38157105
Iteration 2/25 | Loss: 0.00116451
Iteration 3/25 | Loss: 0.00116451
Iteration 4/25 | Loss: 0.00116451
Iteration 5/25 | Loss: 0.00116451
Iteration 6/25 | Loss: 0.00116451
Iteration 7/25 | Loss: 0.00116451
Iteration 8/25 | Loss: 0.00116451
Iteration 9/25 | Loss: 0.00116451
Iteration 10/25 | Loss: 0.00116451
Iteration 11/25 | Loss: 0.00116451
Iteration 12/25 | Loss: 0.00116451
Iteration 13/25 | Loss: 0.00116451
Iteration 14/25 | Loss: 0.00116451
Iteration 15/25 | Loss: 0.00116451
Iteration 16/25 | Loss: 0.00116451
Iteration 17/25 | Loss: 0.00116451
Iteration 18/25 | Loss: 0.00116451
Iteration 19/25 | Loss: 0.00116451
Iteration 20/25 | Loss: 0.00116451
Iteration 21/25 | Loss: 0.00116451
Iteration 22/25 | Loss: 0.00116451
Iteration 23/25 | Loss: 0.00116451
Iteration 24/25 | Loss: 0.00116451
Iteration 25/25 | Loss: 0.00116451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116451
Iteration 2/1000 | Loss: 0.00016540
Iteration 3/1000 | Loss: 0.00013974
Iteration 4/1000 | Loss: 0.00012258
Iteration 5/1000 | Loss: 0.00011452
Iteration 6/1000 | Loss: 0.00009258
Iteration 7/1000 | Loss: 0.00009092
Iteration 8/1000 | Loss: 0.00009089
Iteration 9/1000 | Loss: 0.00009228
Iteration 10/1000 | Loss: 0.00008750
Iteration 11/1000 | Loss: 0.00009787
Iteration 12/1000 | Loss: 0.00008549
Iteration 13/1000 | Loss: 0.00010640
Iteration 14/1000 | Loss: 0.00010208
Iteration 15/1000 | Loss: 0.00007268
Iteration 16/1000 | Loss: 0.00008108
Iteration 17/1000 | Loss: 0.00010034
Iteration 18/1000 | Loss: 0.00030397
Iteration 19/1000 | Loss: 0.00069479
Iteration 20/1000 | Loss: 0.00217369
Iteration 21/1000 | Loss: 0.00253768
Iteration 22/1000 | Loss: 0.00014199
Iteration 23/1000 | Loss: 0.00010920
Iteration 24/1000 | Loss: 0.00011511
Iteration 25/1000 | Loss: 0.00016647
Iteration 26/1000 | Loss: 0.00010207
Iteration 27/1000 | Loss: 0.00003554
Iteration 28/1000 | Loss: 0.00002938
Iteration 29/1000 | Loss: 0.00018515
Iteration 30/1000 | Loss: 0.00252701
Iteration 31/1000 | Loss: 0.00014268
Iteration 32/1000 | Loss: 0.00004548
Iteration 33/1000 | Loss: 0.00030807
Iteration 34/1000 | Loss: 0.00057302
Iteration 35/1000 | Loss: 0.00054322
Iteration 36/1000 | Loss: 0.00002635
Iteration 37/1000 | Loss: 0.00002216
Iteration 38/1000 | Loss: 0.00002023
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00002490
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00002367
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002355
Iteration 47/1000 | Loss: 0.00004266
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001051
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000972
Iteration 54/1000 | Loss: 0.00000969
Iteration 55/1000 | Loss: 0.00000966
Iteration 56/1000 | Loss: 0.00000961
Iteration 57/1000 | Loss: 0.00000958
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00000920
Iteration 60/1000 | Loss: 0.00000909
Iteration 61/1000 | Loss: 0.00000908
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000901
Iteration 66/1000 | Loss: 0.00000900
Iteration 67/1000 | Loss: 0.00000899
Iteration 68/1000 | Loss: 0.00000898
Iteration 69/1000 | Loss: 0.00000898
Iteration 70/1000 | Loss: 0.00000897
Iteration 71/1000 | Loss: 0.00000896
Iteration 72/1000 | Loss: 0.00000896
Iteration 73/1000 | Loss: 0.00000895
Iteration 74/1000 | Loss: 0.00000894
Iteration 75/1000 | Loss: 0.00000894
Iteration 76/1000 | Loss: 0.00000891
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000883
Iteration 81/1000 | Loss: 0.00000883
Iteration 82/1000 | Loss: 0.00000882
Iteration 83/1000 | Loss: 0.00000881
Iteration 84/1000 | Loss: 0.00000881
Iteration 85/1000 | Loss: 0.00000881
Iteration 86/1000 | Loss: 0.00000881
Iteration 87/1000 | Loss: 0.00000881
Iteration 88/1000 | Loss: 0.00000878
Iteration 89/1000 | Loss: 0.00000878
Iteration 90/1000 | Loss: 0.00000876
Iteration 91/1000 | Loss: 0.00000874
Iteration 92/1000 | Loss: 0.00000874
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000873
Iteration 95/1000 | Loss: 0.00000872
Iteration 96/1000 | Loss: 0.00000871
Iteration 97/1000 | Loss: 0.00000869
Iteration 98/1000 | Loss: 0.00000869
Iteration 99/1000 | Loss: 0.00000869
Iteration 100/1000 | Loss: 0.00000869
Iteration 101/1000 | Loss: 0.00000869
Iteration 102/1000 | Loss: 0.00000869
Iteration 103/1000 | Loss: 0.00000869
Iteration 104/1000 | Loss: 0.00000869
Iteration 105/1000 | Loss: 0.00000869
Iteration 106/1000 | Loss: 0.00000868
Iteration 107/1000 | Loss: 0.00000868
Iteration 108/1000 | Loss: 0.00000868
Iteration 109/1000 | Loss: 0.00000868
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000867
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000866
Iteration 115/1000 | Loss: 0.00000865
Iteration 116/1000 | Loss: 0.00000865
Iteration 117/1000 | Loss: 0.00000864
Iteration 118/1000 | Loss: 0.00000864
Iteration 119/1000 | Loss: 0.00000864
Iteration 120/1000 | Loss: 0.00000864
Iteration 121/1000 | Loss: 0.00000863
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000862
Iteration 124/1000 | Loss: 0.00000862
Iteration 125/1000 | Loss: 0.00000862
Iteration 126/1000 | Loss: 0.00000862
Iteration 127/1000 | Loss: 0.00000861
Iteration 128/1000 | Loss: 0.00000861
Iteration 129/1000 | Loss: 0.00000861
Iteration 130/1000 | Loss: 0.00000860
Iteration 131/1000 | Loss: 0.00000860
Iteration 132/1000 | Loss: 0.00000860
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000859
Iteration 135/1000 | Loss: 0.00000859
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000858
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000857
Iteration 150/1000 | Loss: 0.00000857
Iteration 151/1000 | Loss: 0.00000857
Iteration 152/1000 | Loss: 0.00000856
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000856
Iteration 155/1000 | Loss: 0.00000856
Iteration 156/1000 | Loss: 0.00000856
Iteration 157/1000 | Loss: 0.00000856
Iteration 158/1000 | Loss: 0.00000856
Iteration 159/1000 | Loss: 0.00000856
Iteration 160/1000 | Loss: 0.00000856
Iteration 161/1000 | Loss: 0.00000856
Iteration 162/1000 | Loss: 0.00000856
Iteration 163/1000 | Loss: 0.00000856
Iteration 164/1000 | Loss: 0.00000856
Iteration 165/1000 | Loss: 0.00000856
Iteration 166/1000 | Loss: 0.00000856
Iteration 167/1000 | Loss: 0.00000856
Iteration 168/1000 | Loss: 0.00000856
Iteration 169/1000 | Loss: 0.00000856
Iteration 170/1000 | Loss: 0.00000856
Iteration 171/1000 | Loss: 0.00000856
Iteration 172/1000 | Loss: 0.00000856
Iteration 173/1000 | Loss: 0.00000856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [8.558724402973894e-06, 8.558724402973894e-06, 8.558724402973894e-06, 8.558724402973894e-06, 8.558724402973894e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.558724402973894e-06

Optimization complete. Final v2v error: 2.3847103118896484 mm

Highest mean error: 4.6015849113464355 mm for frame 55

Lowest mean error: 2.0718533992767334 mm for frame 107

Saving results

Total time: 133.347829580307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472512
Iteration 2/25 | Loss: 0.00106416
Iteration 3/25 | Loss: 0.00095152
Iteration 4/25 | Loss: 0.00094035
Iteration 5/25 | Loss: 0.00093722
Iteration 6/25 | Loss: 0.00093699
Iteration 7/25 | Loss: 0.00093699
Iteration 8/25 | Loss: 0.00093699
Iteration 9/25 | Loss: 0.00093699
Iteration 10/25 | Loss: 0.00093699
Iteration 11/25 | Loss: 0.00093699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009369857143610716, 0.0009369857143610716, 0.0009369857143610716, 0.0009369857143610716, 0.0009369857143610716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009369857143610716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36912942
Iteration 2/25 | Loss: 0.00056665
Iteration 3/25 | Loss: 0.00056663
Iteration 4/25 | Loss: 0.00056663
Iteration 5/25 | Loss: 0.00056663
Iteration 6/25 | Loss: 0.00056663
Iteration 7/25 | Loss: 0.00056663
Iteration 8/25 | Loss: 0.00056663
Iteration 9/25 | Loss: 0.00056663
Iteration 10/25 | Loss: 0.00056663
Iteration 11/25 | Loss: 0.00056663
Iteration 12/25 | Loss: 0.00056663
Iteration 13/25 | Loss: 0.00056663
Iteration 14/25 | Loss: 0.00056663
Iteration 15/25 | Loss: 0.00056663
Iteration 16/25 | Loss: 0.00056663
Iteration 17/25 | Loss: 0.00056663
Iteration 18/25 | Loss: 0.00056663
Iteration 19/25 | Loss: 0.00056663
Iteration 20/25 | Loss: 0.00056663
Iteration 21/25 | Loss: 0.00056663
Iteration 22/25 | Loss: 0.00056663
Iteration 23/25 | Loss: 0.00056663
Iteration 24/25 | Loss: 0.00056663
Iteration 25/25 | Loss: 0.00056663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056663
Iteration 2/1000 | Loss: 0.00003080
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001608
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001419
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001332
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001282
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001269
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001264
Iteration 25/1000 | Loss: 0.00001263
Iteration 26/1000 | Loss: 0.00001262
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001236
Iteration 101/1000 | Loss: 0.00001236
Iteration 102/1000 | Loss: 0.00001236
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001228
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001226
Iteration 153/1000 | Loss: 0.00001226
Iteration 154/1000 | Loss: 0.00001226
Iteration 155/1000 | Loss: 0.00001226
Iteration 156/1000 | Loss: 0.00001226
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001225
Iteration 164/1000 | Loss: 0.00001225
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001224
Iteration 169/1000 | Loss: 0.00001224
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001224
Iteration 180/1000 | Loss: 0.00001224
Iteration 181/1000 | Loss: 0.00001224
Iteration 182/1000 | Loss: 0.00001224
Iteration 183/1000 | Loss: 0.00001224
Iteration 184/1000 | Loss: 0.00001224
Iteration 185/1000 | Loss: 0.00001224
Iteration 186/1000 | Loss: 0.00001224
Iteration 187/1000 | Loss: 0.00001224
Iteration 188/1000 | Loss: 0.00001224
Iteration 189/1000 | Loss: 0.00001224
Iteration 190/1000 | Loss: 0.00001224
Iteration 191/1000 | Loss: 0.00001224
Iteration 192/1000 | Loss: 0.00001224
Iteration 193/1000 | Loss: 0.00001224
Iteration 194/1000 | Loss: 0.00001224
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001224
Iteration 199/1000 | Loss: 0.00001224
Iteration 200/1000 | Loss: 0.00001224
Iteration 201/1000 | Loss: 0.00001224
Iteration 202/1000 | Loss: 0.00001224
Iteration 203/1000 | Loss: 0.00001224
Iteration 204/1000 | Loss: 0.00001224
Iteration 205/1000 | Loss: 0.00001224
Iteration 206/1000 | Loss: 0.00001224
Iteration 207/1000 | Loss: 0.00001224
Iteration 208/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.223871822730871e-05, 1.223871822730871e-05, 1.223871822730871e-05, 1.223871822730871e-05, 1.223871822730871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.223871822730871e-05

Optimization complete. Final v2v error: 2.9360389709472656 mm

Highest mean error: 3.3370494842529297 mm for frame 91

Lowest mean error: 2.5433247089385986 mm for frame 230

Saving results

Total time: 43.780075550079346
