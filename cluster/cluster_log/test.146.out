Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=146, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8176-8231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898467
Iteration 2/25 | Loss: 0.00240411
Iteration 3/25 | Loss: 0.00210386
Iteration 4/25 | Loss: 0.00205194
Iteration 5/25 | Loss: 0.00201194
Iteration 6/25 | Loss: 0.00200085
Iteration 7/25 | Loss: 0.00199754
Iteration 8/25 | Loss: 0.00199582
Iteration 9/25 | Loss: 0.00199510
Iteration 10/25 | Loss: 0.00199471
Iteration 11/25 | Loss: 0.00199444
Iteration 12/25 | Loss: 0.00199970
Iteration 13/25 | Loss: 0.00198587
Iteration 14/25 | Loss: 0.00196847
Iteration 15/25 | Loss: 0.00195526
Iteration 16/25 | Loss: 0.00195394
Iteration 17/25 | Loss: 0.00195375
Iteration 18/25 | Loss: 0.00195368
Iteration 19/25 | Loss: 0.00195368
Iteration 20/25 | Loss: 0.00195368
Iteration 21/25 | Loss: 0.00195368
Iteration 22/25 | Loss: 0.00195368
Iteration 23/25 | Loss: 0.00195368
Iteration 24/25 | Loss: 0.00195368
Iteration 25/25 | Loss: 0.00195368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40514660
Iteration 2/25 | Loss: 0.00280098
Iteration 3/25 | Loss: 0.00280077
Iteration 4/25 | Loss: 0.00280077
Iteration 5/25 | Loss: 0.00280077
Iteration 6/25 | Loss: 0.00280077
Iteration 7/25 | Loss: 0.00280077
Iteration 8/25 | Loss: 0.00280077
Iteration 9/25 | Loss: 0.00280076
Iteration 10/25 | Loss: 0.00280076
Iteration 11/25 | Loss: 0.00280076
Iteration 12/25 | Loss: 0.00280076
Iteration 13/25 | Loss: 0.00280076
Iteration 14/25 | Loss: 0.00280076
Iteration 15/25 | Loss: 0.00280076
Iteration 16/25 | Loss: 0.00280076
Iteration 17/25 | Loss: 0.00280076
Iteration 18/25 | Loss: 0.00280076
Iteration 19/25 | Loss: 0.00280076
Iteration 20/25 | Loss: 0.00280076
Iteration 21/25 | Loss: 0.00280076
Iteration 22/25 | Loss: 0.00280076
Iteration 23/25 | Loss: 0.00280076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0028007642831653357, 0.0028007642831653357, 0.0028007642831653357, 0.0028007642831653357, 0.0028007642831653357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028007642831653357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280076
Iteration 2/1000 | Loss: 0.00101139
Iteration 3/1000 | Loss: 0.00107445
Iteration 4/1000 | Loss: 0.00011138
Iteration 5/1000 | Loss: 0.00008080
Iteration 6/1000 | Loss: 0.00007219
Iteration 7/1000 | Loss: 0.00006628
Iteration 8/1000 | Loss: 0.00050209
Iteration 9/1000 | Loss: 0.00006555
Iteration 10/1000 | Loss: 0.00005799
Iteration 11/1000 | Loss: 0.00005501
Iteration 12/1000 | Loss: 0.00005265
Iteration 13/1000 | Loss: 0.00005102
Iteration 14/1000 | Loss: 0.00004981
Iteration 15/1000 | Loss: 0.00004916
Iteration 16/1000 | Loss: 0.00004856
Iteration 17/1000 | Loss: 0.00004761
Iteration 18/1000 | Loss: 0.00004706
Iteration 19/1000 | Loss: 0.00004662
Iteration 20/1000 | Loss: 0.00004632
Iteration 21/1000 | Loss: 0.00004607
Iteration 22/1000 | Loss: 0.00004578
Iteration 23/1000 | Loss: 0.00004560
Iteration 24/1000 | Loss: 0.00004546
Iteration 25/1000 | Loss: 0.00004542
Iteration 26/1000 | Loss: 0.00004538
Iteration 27/1000 | Loss: 0.00004538
Iteration 28/1000 | Loss: 0.00004537
Iteration 29/1000 | Loss: 0.00004537
Iteration 30/1000 | Loss: 0.00004532
Iteration 31/1000 | Loss: 0.00004529
Iteration 32/1000 | Loss: 0.00004526
Iteration 33/1000 | Loss: 0.00004524
Iteration 34/1000 | Loss: 0.00004520
Iteration 35/1000 | Loss: 0.00004515
Iteration 36/1000 | Loss: 0.00004514
Iteration 37/1000 | Loss: 0.00004512
Iteration 38/1000 | Loss: 0.00004511
Iteration 39/1000 | Loss: 0.00004510
Iteration 40/1000 | Loss: 0.00004505
Iteration 41/1000 | Loss: 0.00004505
Iteration 42/1000 | Loss: 0.00004504
Iteration 43/1000 | Loss: 0.00004503
Iteration 44/1000 | Loss: 0.00004502
Iteration 45/1000 | Loss: 0.00004501
Iteration 46/1000 | Loss: 0.00004500
Iteration 47/1000 | Loss: 0.00004499
Iteration 48/1000 | Loss: 0.00004499
Iteration 49/1000 | Loss: 0.00004499
Iteration 50/1000 | Loss: 0.00004498
Iteration 51/1000 | Loss: 0.00004498
Iteration 52/1000 | Loss: 0.00004498
Iteration 53/1000 | Loss: 0.00004498
Iteration 54/1000 | Loss: 0.00004498
Iteration 55/1000 | Loss: 0.00004497
Iteration 56/1000 | Loss: 0.00004497
Iteration 57/1000 | Loss: 0.00004497
Iteration 58/1000 | Loss: 0.00004497
Iteration 59/1000 | Loss: 0.00004496
Iteration 60/1000 | Loss: 0.00004496
Iteration 61/1000 | Loss: 0.00004495
Iteration 62/1000 | Loss: 0.00004495
Iteration 63/1000 | Loss: 0.00004495
Iteration 64/1000 | Loss: 0.00004495
Iteration 65/1000 | Loss: 0.00004495
Iteration 66/1000 | Loss: 0.00004494
Iteration 67/1000 | Loss: 0.00004494
Iteration 68/1000 | Loss: 0.00004494
Iteration 69/1000 | Loss: 0.00004493
Iteration 70/1000 | Loss: 0.00004493
Iteration 71/1000 | Loss: 0.00004493
Iteration 72/1000 | Loss: 0.00004493
Iteration 73/1000 | Loss: 0.00004492
Iteration 74/1000 | Loss: 0.00004492
Iteration 75/1000 | Loss: 0.00004492
Iteration 76/1000 | Loss: 0.00004492
Iteration 77/1000 | Loss: 0.00004492
Iteration 78/1000 | Loss: 0.00004491
Iteration 79/1000 | Loss: 0.00004491
Iteration 80/1000 | Loss: 0.00004491
Iteration 81/1000 | Loss: 0.00004491
Iteration 82/1000 | Loss: 0.00004491
Iteration 83/1000 | Loss: 0.00004491
Iteration 84/1000 | Loss: 0.00004491
Iteration 85/1000 | Loss: 0.00004490
Iteration 86/1000 | Loss: 0.00004490
Iteration 87/1000 | Loss: 0.00004490
Iteration 88/1000 | Loss: 0.00004490
Iteration 89/1000 | Loss: 0.00004490
Iteration 90/1000 | Loss: 0.00004490
Iteration 91/1000 | Loss: 0.00004490
Iteration 92/1000 | Loss: 0.00004490
Iteration 93/1000 | Loss: 0.00004490
Iteration 94/1000 | Loss: 0.00004490
Iteration 95/1000 | Loss: 0.00004490
Iteration 96/1000 | Loss: 0.00004490
Iteration 97/1000 | Loss: 0.00004489
Iteration 98/1000 | Loss: 0.00004489
Iteration 99/1000 | Loss: 0.00004489
Iteration 100/1000 | Loss: 0.00004488
Iteration 101/1000 | Loss: 0.00004488
Iteration 102/1000 | Loss: 0.00004488
Iteration 103/1000 | Loss: 0.00004487
Iteration 104/1000 | Loss: 0.00004487
Iteration 105/1000 | Loss: 0.00004486
Iteration 106/1000 | Loss: 0.00004486
Iteration 107/1000 | Loss: 0.00004486
Iteration 108/1000 | Loss: 0.00004486
Iteration 109/1000 | Loss: 0.00004486
Iteration 110/1000 | Loss: 0.00004486
Iteration 111/1000 | Loss: 0.00004486
Iteration 112/1000 | Loss: 0.00004485
Iteration 113/1000 | Loss: 0.00004485
Iteration 114/1000 | Loss: 0.00004485
Iteration 115/1000 | Loss: 0.00004485
Iteration 116/1000 | Loss: 0.00004484
Iteration 117/1000 | Loss: 0.00004484
Iteration 118/1000 | Loss: 0.00004484
Iteration 119/1000 | Loss: 0.00004484
Iteration 120/1000 | Loss: 0.00004484
Iteration 121/1000 | Loss: 0.00004484
Iteration 122/1000 | Loss: 0.00004484
Iteration 123/1000 | Loss: 0.00004484
Iteration 124/1000 | Loss: 0.00004484
Iteration 125/1000 | Loss: 0.00004484
Iteration 126/1000 | Loss: 0.00004484
Iteration 127/1000 | Loss: 0.00004483
Iteration 128/1000 | Loss: 0.00004483
Iteration 129/1000 | Loss: 0.00004483
Iteration 130/1000 | Loss: 0.00004483
Iteration 131/1000 | Loss: 0.00004482
Iteration 132/1000 | Loss: 0.00004482
Iteration 133/1000 | Loss: 0.00004482
Iteration 134/1000 | Loss: 0.00004482
Iteration 135/1000 | Loss: 0.00004482
Iteration 136/1000 | Loss: 0.00004481
Iteration 137/1000 | Loss: 0.00004481
Iteration 138/1000 | Loss: 0.00004481
Iteration 139/1000 | Loss: 0.00004481
Iteration 140/1000 | Loss: 0.00004481
Iteration 141/1000 | Loss: 0.00004481
Iteration 142/1000 | Loss: 0.00004481
Iteration 143/1000 | Loss: 0.00004481
Iteration 144/1000 | Loss: 0.00004481
Iteration 145/1000 | Loss: 0.00004481
Iteration 146/1000 | Loss: 0.00004480
Iteration 147/1000 | Loss: 0.00004480
Iteration 148/1000 | Loss: 0.00004480
Iteration 149/1000 | Loss: 0.00004480
Iteration 150/1000 | Loss: 0.00004480
Iteration 151/1000 | Loss: 0.00004480
Iteration 152/1000 | Loss: 0.00004480
Iteration 153/1000 | Loss: 0.00004480
Iteration 154/1000 | Loss: 0.00004480
Iteration 155/1000 | Loss: 0.00004479
Iteration 156/1000 | Loss: 0.00004479
Iteration 157/1000 | Loss: 0.00004479
Iteration 158/1000 | Loss: 0.00004479
Iteration 159/1000 | Loss: 0.00004479
Iteration 160/1000 | Loss: 0.00004479
Iteration 161/1000 | Loss: 0.00004479
Iteration 162/1000 | Loss: 0.00004479
Iteration 163/1000 | Loss: 0.00004479
Iteration 164/1000 | Loss: 0.00004479
Iteration 165/1000 | Loss: 0.00004478
Iteration 166/1000 | Loss: 0.00004478
Iteration 167/1000 | Loss: 0.00004478
Iteration 168/1000 | Loss: 0.00004478
Iteration 169/1000 | Loss: 0.00004478
Iteration 170/1000 | Loss: 0.00004478
Iteration 171/1000 | Loss: 0.00004478
Iteration 172/1000 | Loss: 0.00004478
Iteration 173/1000 | Loss: 0.00004478
Iteration 174/1000 | Loss: 0.00004478
Iteration 175/1000 | Loss: 0.00004478
Iteration 176/1000 | Loss: 0.00004478
Iteration 177/1000 | Loss: 0.00004478
Iteration 178/1000 | Loss: 0.00004477
Iteration 179/1000 | Loss: 0.00004477
Iteration 180/1000 | Loss: 0.00004477
Iteration 181/1000 | Loss: 0.00004477
Iteration 182/1000 | Loss: 0.00004476
Iteration 183/1000 | Loss: 0.00004476
Iteration 184/1000 | Loss: 0.00004476
Iteration 185/1000 | Loss: 0.00004476
Iteration 186/1000 | Loss: 0.00004476
Iteration 187/1000 | Loss: 0.00004476
Iteration 188/1000 | Loss: 0.00004476
Iteration 189/1000 | Loss: 0.00004476
Iteration 190/1000 | Loss: 0.00004476
Iteration 191/1000 | Loss: 0.00004476
Iteration 192/1000 | Loss: 0.00004475
Iteration 193/1000 | Loss: 0.00004475
Iteration 194/1000 | Loss: 0.00004475
Iteration 195/1000 | Loss: 0.00004475
Iteration 196/1000 | Loss: 0.00004475
Iteration 197/1000 | Loss: 0.00004475
Iteration 198/1000 | Loss: 0.00004475
Iteration 199/1000 | Loss: 0.00004475
Iteration 200/1000 | Loss: 0.00004475
Iteration 201/1000 | Loss: 0.00004475
Iteration 202/1000 | Loss: 0.00004475
Iteration 203/1000 | Loss: 0.00004475
Iteration 204/1000 | Loss: 0.00004475
Iteration 205/1000 | Loss: 0.00004475
Iteration 206/1000 | Loss: 0.00004475
Iteration 207/1000 | Loss: 0.00004474
Iteration 208/1000 | Loss: 0.00004474
Iteration 209/1000 | Loss: 0.00004474
Iteration 210/1000 | Loss: 0.00004474
Iteration 211/1000 | Loss: 0.00004474
Iteration 212/1000 | Loss: 0.00004474
Iteration 213/1000 | Loss: 0.00004474
Iteration 214/1000 | Loss: 0.00004474
Iteration 215/1000 | Loss: 0.00004474
Iteration 216/1000 | Loss: 0.00004474
Iteration 217/1000 | Loss: 0.00004474
Iteration 218/1000 | Loss: 0.00004473
Iteration 219/1000 | Loss: 0.00004473
Iteration 220/1000 | Loss: 0.00004473
Iteration 221/1000 | Loss: 0.00004473
Iteration 222/1000 | Loss: 0.00004473
Iteration 223/1000 | Loss: 0.00004473
Iteration 224/1000 | Loss: 0.00004473
Iteration 225/1000 | Loss: 0.00004473
Iteration 226/1000 | Loss: 0.00004473
Iteration 227/1000 | Loss: 0.00004472
Iteration 228/1000 | Loss: 0.00004472
Iteration 229/1000 | Loss: 0.00004472
Iteration 230/1000 | Loss: 0.00004472
Iteration 231/1000 | Loss: 0.00004472
Iteration 232/1000 | Loss: 0.00004472
Iteration 233/1000 | Loss: 0.00004472
Iteration 234/1000 | Loss: 0.00004472
Iteration 235/1000 | Loss: 0.00004472
Iteration 236/1000 | Loss: 0.00004472
Iteration 237/1000 | Loss: 0.00004471
Iteration 238/1000 | Loss: 0.00004471
Iteration 239/1000 | Loss: 0.00004471
Iteration 240/1000 | Loss: 0.00004471
Iteration 241/1000 | Loss: 0.00004471
Iteration 242/1000 | Loss: 0.00004471
Iteration 243/1000 | Loss: 0.00004471
Iteration 244/1000 | Loss: 0.00004471
Iteration 245/1000 | Loss: 0.00004471
Iteration 246/1000 | Loss: 0.00004471
Iteration 247/1000 | Loss: 0.00004471
Iteration 248/1000 | Loss: 0.00004471
Iteration 249/1000 | Loss: 0.00004471
Iteration 250/1000 | Loss: 0.00004471
Iteration 251/1000 | Loss: 0.00004471
Iteration 252/1000 | Loss: 0.00004471
Iteration 253/1000 | Loss: 0.00004471
Iteration 254/1000 | Loss: 0.00004471
Iteration 255/1000 | Loss: 0.00004470
Iteration 256/1000 | Loss: 0.00004470
Iteration 257/1000 | Loss: 0.00004470
Iteration 258/1000 | Loss: 0.00004470
Iteration 259/1000 | Loss: 0.00004470
Iteration 260/1000 | Loss: 0.00004469
Iteration 261/1000 | Loss: 0.00004469
Iteration 262/1000 | Loss: 0.00004469
Iteration 263/1000 | Loss: 0.00004469
Iteration 264/1000 | Loss: 0.00004469
Iteration 265/1000 | Loss: 0.00004469
Iteration 266/1000 | Loss: 0.00004469
Iteration 267/1000 | Loss: 0.00004469
Iteration 268/1000 | Loss: 0.00004469
Iteration 269/1000 | Loss: 0.00004469
Iteration 270/1000 | Loss: 0.00004469
Iteration 271/1000 | Loss: 0.00004469
Iteration 272/1000 | Loss: 0.00004469
Iteration 273/1000 | Loss: 0.00004469
Iteration 274/1000 | Loss: 0.00004469
Iteration 275/1000 | Loss: 0.00004469
Iteration 276/1000 | Loss: 0.00004469
Iteration 277/1000 | Loss: 0.00004469
Iteration 278/1000 | Loss: 0.00004469
Iteration 279/1000 | Loss: 0.00004469
Iteration 280/1000 | Loss: 0.00004469
Iteration 281/1000 | Loss: 0.00004469
Iteration 282/1000 | Loss: 0.00004469
Iteration 283/1000 | Loss: 0.00004469
Iteration 284/1000 | Loss: 0.00004469
Iteration 285/1000 | Loss: 0.00004469
Iteration 286/1000 | Loss: 0.00004469
Iteration 287/1000 | Loss: 0.00004469
Iteration 288/1000 | Loss: 0.00004469
Iteration 289/1000 | Loss: 0.00004469
Iteration 290/1000 | Loss: 0.00004469
Iteration 291/1000 | Loss: 0.00004469
Iteration 292/1000 | Loss: 0.00004469
Iteration 293/1000 | Loss: 0.00004469
Iteration 294/1000 | Loss: 0.00004469
Iteration 295/1000 | Loss: 0.00004469
Iteration 296/1000 | Loss: 0.00004469
Iteration 297/1000 | Loss: 0.00004469
Iteration 298/1000 | Loss: 0.00004469
Iteration 299/1000 | Loss: 0.00004469
Iteration 300/1000 | Loss: 0.00004469
Iteration 301/1000 | Loss: 0.00004469
Iteration 302/1000 | Loss: 0.00004469
Iteration 303/1000 | Loss: 0.00004469
Iteration 304/1000 | Loss: 0.00004469
Iteration 305/1000 | Loss: 0.00004469
Iteration 306/1000 | Loss: 0.00004469
Iteration 307/1000 | Loss: 0.00004469
Iteration 308/1000 | Loss: 0.00004469
Iteration 309/1000 | Loss: 0.00004469
Iteration 310/1000 | Loss: 0.00004469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [4.4689990318147466e-05, 4.4689990318147466e-05, 4.4689990318147466e-05, 4.4689990318147466e-05, 4.4689990318147466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4689990318147466e-05

Optimization complete. Final v2v error: 5.4571332931518555 mm

Highest mean error: 7.857171058654785 mm for frame 56

Lowest mean error: 4.367766857147217 mm for frame 112

Saving results

Total time: 85.86618041992188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970952
Iteration 2/25 | Loss: 0.00189317
Iteration 3/25 | Loss: 0.00183226
Iteration 4/25 | Loss: 0.00182592
Iteration 5/25 | Loss: 0.00182290
Iteration 6/25 | Loss: 0.00182223
Iteration 7/25 | Loss: 0.00182223
Iteration 8/25 | Loss: 0.00182223
Iteration 9/25 | Loss: 0.00182223
Iteration 10/25 | Loss: 0.00182223
Iteration 11/25 | Loss: 0.00182223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001822232617996633, 0.001822232617996633, 0.001822232617996633, 0.001822232617996633, 0.001822232617996633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001822232617996633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21488297
Iteration 2/25 | Loss: 0.00257561
Iteration 3/25 | Loss: 0.00257561
Iteration 4/25 | Loss: 0.00257561
Iteration 5/25 | Loss: 0.00257561
Iteration 6/25 | Loss: 0.00257561
Iteration 7/25 | Loss: 0.00257561
Iteration 8/25 | Loss: 0.00257561
Iteration 9/25 | Loss: 0.00257561
Iteration 10/25 | Loss: 0.00257561
Iteration 11/25 | Loss: 0.00257561
Iteration 12/25 | Loss: 0.00257561
Iteration 13/25 | Loss: 0.00257561
Iteration 14/25 | Loss: 0.00257561
Iteration 15/25 | Loss: 0.00257561
Iteration 16/25 | Loss: 0.00257561
Iteration 17/25 | Loss: 0.00257561
Iteration 18/25 | Loss: 0.00257561
Iteration 19/25 | Loss: 0.00257561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0025756091345101595, 0.0025756091345101595, 0.0025756091345101595, 0.0025756091345101595, 0.0025756091345101595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025756091345101595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257561
Iteration 2/1000 | Loss: 0.00004794
Iteration 3/1000 | Loss: 0.00003119
Iteration 4/1000 | Loss: 0.00002817
Iteration 5/1000 | Loss: 0.00002640
Iteration 6/1000 | Loss: 0.00002526
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00002420
Iteration 9/1000 | Loss: 0.00002387
Iteration 10/1000 | Loss: 0.00002365
Iteration 11/1000 | Loss: 0.00002338
Iteration 12/1000 | Loss: 0.00002331
Iteration 13/1000 | Loss: 0.00002317
Iteration 14/1000 | Loss: 0.00002311
Iteration 15/1000 | Loss: 0.00002301
Iteration 16/1000 | Loss: 0.00002300
Iteration 17/1000 | Loss: 0.00002296
Iteration 18/1000 | Loss: 0.00002295
Iteration 19/1000 | Loss: 0.00002294
Iteration 20/1000 | Loss: 0.00002294
Iteration 21/1000 | Loss: 0.00002294
Iteration 22/1000 | Loss: 0.00002294
Iteration 23/1000 | Loss: 0.00002294
Iteration 24/1000 | Loss: 0.00002294
Iteration 25/1000 | Loss: 0.00002293
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002292
Iteration 28/1000 | Loss: 0.00002292
Iteration 29/1000 | Loss: 0.00002292
Iteration 30/1000 | Loss: 0.00002292
Iteration 31/1000 | Loss: 0.00002292
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002291
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00002289
Iteration 41/1000 | Loss: 0.00002289
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002288
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002286
Iteration 50/1000 | Loss: 0.00002286
Iteration 51/1000 | Loss: 0.00002286
Iteration 52/1000 | Loss: 0.00002286
Iteration 53/1000 | Loss: 0.00002286
Iteration 54/1000 | Loss: 0.00002285
Iteration 55/1000 | Loss: 0.00002285
Iteration 56/1000 | Loss: 0.00002285
Iteration 57/1000 | Loss: 0.00002285
Iteration 58/1000 | Loss: 0.00002285
Iteration 59/1000 | Loss: 0.00002284
Iteration 60/1000 | Loss: 0.00002284
Iteration 61/1000 | Loss: 0.00002283
Iteration 62/1000 | Loss: 0.00002283
Iteration 63/1000 | Loss: 0.00002283
Iteration 64/1000 | Loss: 0.00002283
Iteration 65/1000 | Loss: 0.00002283
Iteration 66/1000 | Loss: 0.00002283
Iteration 67/1000 | Loss: 0.00002283
Iteration 68/1000 | Loss: 0.00002283
Iteration 69/1000 | Loss: 0.00002283
Iteration 70/1000 | Loss: 0.00002283
Iteration 71/1000 | Loss: 0.00002282
Iteration 72/1000 | Loss: 0.00002282
Iteration 73/1000 | Loss: 0.00002282
Iteration 74/1000 | Loss: 0.00002282
Iteration 75/1000 | Loss: 0.00002282
Iteration 76/1000 | Loss: 0.00002282
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002281
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002281
Iteration 87/1000 | Loss: 0.00002281
Iteration 88/1000 | Loss: 0.00002281
Iteration 89/1000 | Loss: 0.00002281
Iteration 90/1000 | Loss: 0.00002281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.2813592295278795e-05, 2.2813592295278795e-05, 2.2813592295278795e-05, 2.2813592295278795e-05, 2.2813592295278795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2813592295278795e-05

Optimization complete. Final v2v error: 4.098564147949219 mm

Highest mean error: 4.35178804397583 mm for frame 54

Lowest mean error: 3.9099044799804688 mm for frame 40

Saving results

Total time: 30.701001167297363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729143
Iteration 2/25 | Loss: 0.00207206
Iteration 3/25 | Loss: 0.00196503
Iteration 4/25 | Loss: 0.00194239
Iteration 5/25 | Loss: 0.00193574
Iteration 6/25 | Loss: 0.00193464
Iteration 7/25 | Loss: 0.00193464
Iteration 8/25 | Loss: 0.00193464
Iteration 9/25 | Loss: 0.00193464
Iteration 10/25 | Loss: 0.00193464
Iteration 11/25 | Loss: 0.00193464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001934638014063239, 0.001934638014063239, 0.001934638014063239, 0.001934638014063239, 0.001934638014063239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001934638014063239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28342700
Iteration 2/25 | Loss: 0.00262366
Iteration 3/25 | Loss: 0.00262365
Iteration 4/25 | Loss: 0.00262365
Iteration 5/25 | Loss: 0.00262365
Iteration 6/25 | Loss: 0.00262365
Iteration 7/25 | Loss: 0.00262365
Iteration 8/25 | Loss: 0.00262365
Iteration 9/25 | Loss: 0.00262365
Iteration 10/25 | Loss: 0.00262365
Iteration 11/25 | Loss: 0.00262365
Iteration 12/25 | Loss: 0.00262365
Iteration 13/25 | Loss: 0.00262365
Iteration 14/25 | Loss: 0.00262365
Iteration 15/25 | Loss: 0.00262365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026236518751829863, 0.0026236518751829863, 0.0026236518751829863, 0.0026236518751829863, 0.0026236518751829863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026236518751829863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262365
Iteration 2/1000 | Loss: 0.00007918
Iteration 3/1000 | Loss: 0.00005509
Iteration 4/1000 | Loss: 0.00004812
Iteration 5/1000 | Loss: 0.00004407
Iteration 6/1000 | Loss: 0.00004157
Iteration 7/1000 | Loss: 0.00003986
Iteration 8/1000 | Loss: 0.00003899
Iteration 9/1000 | Loss: 0.00003825
Iteration 10/1000 | Loss: 0.00003775
Iteration 11/1000 | Loss: 0.00003732
Iteration 12/1000 | Loss: 0.00003701
Iteration 13/1000 | Loss: 0.00003679
Iteration 14/1000 | Loss: 0.00003662
Iteration 15/1000 | Loss: 0.00003659
Iteration 16/1000 | Loss: 0.00003658
Iteration 17/1000 | Loss: 0.00003655
Iteration 18/1000 | Loss: 0.00003654
Iteration 19/1000 | Loss: 0.00003649
Iteration 20/1000 | Loss: 0.00003646
Iteration 21/1000 | Loss: 0.00003646
Iteration 22/1000 | Loss: 0.00003645
Iteration 23/1000 | Loss: 0.00003644
Iteration 24/1000 | Loss: 0.00003643
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003640
Iteration 27/1000 | Loss: 0.00003638
Iteration 28/1000 | Loss: 0.00003637
Iteration 29/1000 | Loss: 0.00003637
Iteration 30/1000 | Loss: 0.00003637
Iteration 31/1000 | Loss: 0.00003636
Iteration 32/1000 | Loss: 0.00003636
Iteration 33/1000 | Loss: 0.00003635
Iteration 34/1000 | Loss: 0.00003635
Iteration 35/1000 | Loss: 0.00003634
Iteration 36/1000 | Loss: 0.00003634
Iteration 37/1000 | Loss: 0.00003633
Iteration 38/1000 | Loss: 0.00003633
Iteration 39/1000 | Loss: 0.00003632
Iteration 40/1000 | Loss: 0.00003632
Iteration 41/1000 | Loss: 0.00003632
Iteration 42/1000 | Loss: 0.00003631
Iteration 43/1000 | Loss: 0.00003631
Iteration 44/1000 | Loss: 0.00003631
Iteration 45/1000 | Loss: 0.00003631
Iteration 46/1000 | Loss: 0.00003631
Iteration 47/1000 | Loss: 0.00003631
Iteration 48/1000 | Loss: 0.00003631
Iteration 49/1000 | Loss: 0.00003631
Iteration 50/1000 | Loss: 0.00003631
Iteration 51/1000 | Loss: 0.00003631
Iteration 52/1000 | Loss: 0.00003631
Iteration 53/1000 | Loss: 0.00003631
Iteration 54/1000 | Loss: 0.00003631
Iteration 55/1000 | Loss: 0.00003631
Iteration 56/1000 | Loss: 0.00003631
Iteration 57/1000 | Loss: 0.00003631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [3.630580977187492e-05, 3.630580977187492e-05, 3.630580977187492e-05, 3.630580977187492e-05, 3.630580977187492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.630580977187492e-05

Optimization complete. Final v2v error: 5.210404872894287 mm

Highest mean error: 5.7662248611450195 mm for frame 64

Lowest mean error: 4.799736499786377 mm for frame 29

Saving results

Total time: 37.296366691589355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922031
Iteration 2/25 | Loss: 0.00194867
Iteration 3/25 | Loss: 0.00186027
Iteration 4/25 | Loss: 0.00184962
Iteration 5/25 | Loss: 0.00184872
Iteration 6/25 | Loss: 0.00184872
Iteration 7/25 | Loss: 0.00184872
Iteration 8/25 | Loss: 0.00184872
Iteration 9/25 | Loss: 0.00184872
Iteration 10/25 | Loss: 0.00184872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0018487245542928576, 0.0018487245542928576, 0.0018487245542928576, 0.0018487245542928576, 0.0018487245542928576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018487245542928576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21719134
Iteration 2/25 | Loss: 0.00256881
Iteration 3/25 | Loss: 0.00256881
Iteration 4/25 | Loss: 0.00256881
Iteration 5/25 | Loss: 0.00256881
Iteration 6/25 | Loss: 0.00256881
Iteration 7/25 | Loss: 0.00256881
Iteration 8/25 | Loss: 0.00256881
Iteration 9/25 | Loss: 0.00256881
Iteration 10/25 | Loss: 0.00256881
Iteration 11/25 | Loss: 0.00256881
Iteration 12/25 | Loss: 0.00256881
Iteration 13/25 | Loss: 0.00256881
Iteration 14/25 | Loss: 0.00256881
Iteration 15/25 | Loss: 0.00256881
Iteration 16/25 | Loss: 0.00256881
Iteration 17/25 | Loss: 0.00256881
Iteration 18/25 | Loss: 0.00256881
Iteration 19/25 | Loss: 0.00256881
Iteration 20/25 | Loss: 0.00256881
Iteration 21/25 | Loss: 0.00256881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0025688076857477427, 0.0025688076857477427, 0.0025688076857477427, 0.0025688076857477427, 0.0025688076857477427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025688076857477427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256881
Iteration 2/1000 | Loss: 0.00006429
Iteration 3/1000 | Loss: 0.00004345
Iteration 4/1000 | Loss: 0.00003553
Iteration 5/1000 | Loss: 0.00003370
Iteration 6/1000 | Loss: 0.00003144
Iteration 7/1000 | Loss: 0.00003034
Iteration 8/1000 | Loss: 0.00002923
Iteration 9/1000 | Loss: 0.00002859
Iteration 10/1000 | Loss: 0.00002816
Iteration 11/1000 | Loss: 0.00002784
Iteration 12/1000 | Loss: 0.00002770
Iteration 13/1000 | Loss: 0.00002752
Iteration 14/1000 | Loss: 0.00002750
Iteration 15/1000 | Loss: 0.00002736
Iteration 16/1000 | Loss: 0.00002732
Iteration 17/1000 | Loss: 0.00002732
Iteration 18/1000 | Loss: 0.00002732
Iteration 19/1000 | Loss: 0.00002732
Iteration 20/1000 | Loss: 0.00002731
Iteration 21/1000 | Loss: 0.00002730
Iteration 22/1000 | Loss: 0.00002726
Iteration 23/1000 | Loss: 0.00002726
Iteration 24/1000 | Loss: 0.00002726
Iteration 25/1000 | Loss: 0.00002726
Iteration 26/1000 | Loss: 0.00002726
Iteration 27/1000 | Loss: 0.00002726
Iteration 28/1000 | Loss: 0.00002726
Iteration 29/1000 | Loss: 0.00002726
Iteration 30/1000 | Loss: 0.00002724
Iteration 31/1000 | Loss: 0.00002723
Iteration 32/1000 | Loss: 0.00002723
Iteration 33/1000 | Loss: 0.00002722
Iteration 34/1000 | Loss: 0.00002722
Iteration 35/1000 | Loss: 0.00002722
Iteration 36/1000 | Loss: 0.00002722
Iteration 37/1000 | Loss: 0.00002721
Iteration 38/1000 | Loss: 0.00002721
Iteration 39/1000 | Loss: 0.00002721
Iteration 40/1000 | Loss: 0.00002721
Iteration 41/1000 | Loss: 0.00002721
Iteration 42/1000 | Loss: 0.00002720
Iteration 43/1000 | Loss: 0.00002720
Iteration 44/1000 | Loss: 0.00002720
Iteration 45/1000 | Loss: 0.00002720
Iteration 46/1000 | Loss: 0.00002719
Iteration 47/1000 | Loss: 0.00002719
Iteration 48/1000 | Loss: 0.00002719
Iteration 49/1000 | Loss: 0.00002719
Iteration 50/1000 | Loss: 0.00002718
Iteration 51/1000 | Loss: 0.00002718
Iteration 52/1000 | Loss: 0.00002718
Iteration 53/1000 | Loss: 0.00002718
Iteration 54/1000 | Loss: 0.00002718
Iteration 55/1000 | Loss: 0.00002718
Iteration 56/1000 | Loss: 0.00002718
Iteration 57/1000 | Loss: 0.00002717
Iteration 58/1000 | Loss: 0.00002717
Iteration 59/1000 | Loss: 0.00002717
Iteration 60/1000 | Loss: 0.00002717
Iteration 61/1000 | Loss: 0.00002717
Iteration 62/1000 | Loss: 0.00002716
Iteration 63/1000 | Loss: 0.00002716
Iteration 64/1000 | Loss: 0.00002716
Iteration 65/1000 | Loss: 0.00002716
Iteration 66/1000 | Loss: 0.00002716
Iteration 67/1000 | Loss: 0.00002716
Iteration 68/1000 | Loss: 0.00002715
Iteration 69/1000 | Loss: 0.00002715
Iteration 70/1000 | Loss: 0.00002715
Iteration 71/1000 | Loss: 0.00002714
Iteration 72/1000 | Loss: 0.00002714
Iteration 73/1000 | Loss: 0.00002714
Iteration 74/1000 | Loss: 0.00002714
Iteration 75/1000 | Loss: 0.00002714
Iteration 76/1000 | Loss: 0.00002714
Iteration 77/1000 | Loss: 0.00002714
Iteration 78/1000 | Loss: 0.00002713
Iteration 79/1000 | Loss: 0.00002713
Iteration 80/1000 | Loss: 0.00002713
Iteration 81/1000 | Loss: 0.00002713
Iteration 82/1000 | Loss: 0.00002713
Iteration 83/1000 | Loss: 0.00002712
Iteration 84/1000 | Loss: 0.00002712
Iteration 85/1000 | Loss: 0.00002712
Iteration 86/1000 | Loss: 0.00002712
Iteration 87/1000 | Loss: 0.00002712
Iteration 88/1000 | Loss: 0.00002712
Iteration 89/1000 | Loss: 0.00002712
Iteration 90/1000 | Loss: 0.00002712
Iteration 91/1000 | Loss: 0.00002712
Iteration 92/1000 | Loss: 0.00002712
Iteration 93/1000 | Loss: 0.00002711
Iteration 94/1000 | Loss: 0.00002711
Iteration 95/1000 | Loss: 0.00002711
Iteration 96/1000 | Loss: 0.00002711
Iteration 97/1000 | Loss: 0.00002711
Iteration 98/1000 | Loss: 0.00002711
Iteration 99/1000 | Loss: 0.00002711
Iteration 100/1000 | Loss: 0.00002711
Iteration 101/1000 | Loss: 0.00002711
Iteration 102/1000 | Loss: 0.00002711
Iteration 103/1000 | Loss: 0.00002710
Iteration 104/1000 | Loss: 0.00002710
Iteration 105/1000 | Loss: 0.00002710
Iteration 106/1000 | Loss: 0.00002710
Iteration 107/1000 | Loss: 0.00002710
Iteration 108/1000 | Loss: 0.00002710
Iteration 109/1000 | Loss: 0.00002710
Iteration 110/1000 | Loss: 0.00002710
Iteration 111/1000 | Loss: 0.00002710
Iteration 112/1000 | Loss: 0.00002710
Iteration 113/1000 | Loss: 0.00002710
Iteration 114/1000 | Loss: 0.00002710
Iteration 115/1000 | Loss: 0.00002710
Iteration 116/1000 | Loss: 0.00002710
Iteration 117/1000 | Loss: 0.00002710
Iteration 118/1000 | Loss: 0.00002710
Iteration 119/1000 | Loss: 0.00002710
Iteration 120/1000 | Loss: 0.00002710
Iteration 121/1000 | Loss: 0.00002709
Iteration 122/1000 | Loss: 0.00002709
Iteration 123/1000 | Loss: 0.00002709
Iteration 124/1000 | Loss: 0.00002709
Iteration 125/1000 | Loss: 0.00002709
Iteration 126/1000 | Loss: 0.00002709
Iteration 127/1000 | Loss: 0.00002709
Iteration 128/1000 | Loss: 0.00002709
Iteration 129/1000 | Loss: 0.00002709
Iteration 130/1000 | Loss: 0.00002709
Iteration 131/1000 | Loss: 0.00002709
Iteration 132/1000 | Loss: 0.00002709
Iteration 133/1000 | Loss: 0.00002709
Iteration 134/1000 | Loss: 0.00002709
Iteration 135/1000 | Loss: 0.00002709
Iteration 136/1000 | Loss: 0.00002709
Iteration 137/1000 | Loss: 0.00002709
Iteration 138/1000 | Loss: 0.00002709
Iteration 139/1000 | Loss: 0.00002709
Iteration 140/1000 | Loss: 0.00002709
Iteration 141/1000 | Loss: 0.00002709
Iteration 142/1000 | Loss: 0.00002709
Iteration 143/1000 | Loss: 0.00002709
Iteration 144/1000 | Loss: 0.00002708
Iteration 145/1000 | Loss: 0.00002708
Iteration 146/1000 | Loss: 0.00002708
Iteration 147/1000 | Loss: 0.00002708
Iteration 148/1000 | Loss: 0.00002708
Iteration 149/1000 | Loss: 0.00002708
Iteration 150/1000 | Loss: 0.00002708
Iteration 151/1000 | Loss: 0.00002708
Iteration 152/1000 | Loss: 0.00002708
Iteration 153/1000 | Loss: 0.00002708
Iteration 154/1000 | Loss: 0.00002708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.708117062866222e-05, 2.708117062866222e-05, 2.708117062866222e-05, 2.708117062866222e-05, 2.708117062866222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.708117062866222e-05

Optimization complete. Final v2v error: 4.364170551300049 mm

Highest mean error: 5.186286449432373 mm for frame 69

Lowest mean error: 4.100729465484619 mm for frame 5

Saving results

Total time: 35.08416509628296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156988
Iteration 2/25 | Loss: 0.01156988
Iteration 3/25 | Loss: 0.01156988
Iteration 4/25 | Loss: 0.01156988
Iteration 5/25 | Loss: 0.01156988
Iteration 6/25 | Loss: 0.01156987
Iteration 7/25 | Loss: 0.01156987
Iteration 8/25 | Loss: 0.01156987
Iteration 9/25 | Loss: 0.00403568
Iteration 10/25 | Loss: 0.00292632
Iteration 11/25 | Loss: 0.00259811
Iteration 12/25 | Loss: 0.00241700
Iteration 13/25 | Loss: 0.00232678
Iteration 14/25 | Loss: 0.00220488
Iteration 15/25 | Loss: 0.00215376
Iteration 16/25 | Loss: 0.00213347
Iteration 17/25 | Loss: 0.00209877
Iteration 18/25 | Loss: 0.00209385
Iteration 19/25 | Loss: 0.00208169
Iteration 20/25 | Loss: 0.00207617
Iteration 21/25 | Loss: 0.00208576
Iteration 22/25 | Loss: 0.00207675
Iteration 23/25 | Loss: 0.00206623
Iteration 24/25 | Loss: 0.00206020
Iteration 25/25 | Loss: 0.00205478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15859711
Iteration 2/25 | Loss: 0.00526262
Iteration 3/25 | Loss: 0.00526262
Iteration 4/25 | Loss: 0.00484704
Iteration 5/25 | Loss: 0.00484677
Iteration 6/25 | Loss: 0.00484677
Iteration 7/25 | Loss: 0.00484677
Iteration 8/25 | Loss: 0.00484677
Iteration 9/25 | Loss: 0.00484677
Iteration 10/25 | Loss: 0.00484677
Iteration 11/25 | Loss: 0.00484677
Iteration 12/25 | Loss: 0.00484676
Iteration 13/25 | Loss: 0.00484676
Iteration 14/25 | Loss: 0.00484676
Iteration 15/25 | Loss: 0.00484676
Iteration 16/25 | Loss: 0.00484676
Iteration 17/25 | Loss: 0.00484676
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004846764728426933, 0.004846764728426933, 0.004846764728426933, 0.004846764728426933, 0.004846764728426933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004846764728426933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00484676
Iteration 2/1000 | Loss: 0.00113504
Iteration 3/1000 | Loss: 0.00086879
Iteration 4/1000 | Loss: 0.00107214
Iteration 5/1000 | Loss: 0.00119440
Iteration 6/1000 | Loss: 0.00205192
Iteration 7/1000 | Loss: 0.00074431
Iteration 8/1000 | Loss: 0.00037976
Iteration 9/1000 | Loss: 0.00053317
Iteration 10/1000 | Loss: 0.00050031
Iteration 11/1000 | Loss: 0.00132376
Iteration 12/1000 | Loss: 0.00121885
Iteration 13/1000 | Loss: 0.00060917
Iteration 14/1000 | Loss: 0.00070259
Iteration 15/1000 | Loss: 0.00027408
Iteration 16/1000 | Loss: 0.00035296
Iteration 17/1000 | Loss: 0.00035303
Iteration 18/1000 | Loss: 0.00034045
Iteration 19/1000 | Loss: 0.00041486
Iteration 20/1000 | Loss: 0.00095544
Iteration 21/1000 | Loss: 0.00041429
Iteration 22/1000 | Loss: 0.00067363
Iteration 23/1000 | Loss: 0.00083144
Iteration 24/1000 | Loss: 0.00043358
Iteration 25/1000 | Loss: 0.00047871
Iteration 26/1000 | Loss: 0.00105881
Iteration 27/1000 | Loss: 0.00599435
Iteration 28/1000 | Loss: 0.00196627
Iteration 29/1000 | Loss: 0.00099784
Iteration 30/1000 | Loss: 0.00120339
Iteration 31/1000 | Loss: 0.00084744
Iteration 32/1000 | Loss: 0.00029357
Iteration 33/1000 | Loss: 0.00041917
Iteration 34/1000 | Loss: 0.00121280
Iteration 35/1000 | Loss: 0.00029755
Iteration 36/1000 | Loss: 0.00031568
Iteration 37/1000 | Loss: 0.00077755
Iteration 38/1000 | Loss: 0.00023842
Iteration 39/1000 | Loss: 0.00038175
Iteration 40/1000 | Loss: 0.00099221
Iteration 41/1000 | Loss: 0.00189180
Iteration 42/1000 | Loss: 0.00052127
Iteration 43/1000 | Loss: 0.00026631
Iteration 44/1000 | Loss: 0.00024674
Iteration 45/1000 | Loss: 0.00030307
Iteration 46/1000 | Loss: 0.00027846
Iteration 47/1000 | Loss: 0.00033325
Iteration 48/1000 | Loss: 0.00037501
Iteration 49/1000 | Loss: 0.00031130
Iteration 50/1000 | Loss: 0.00024568
Iteration 51/1000 | Loss: 0.00030710
Iteration 52/1000 | Loss: 0.00029572
Iteration 53/1000 | Loss: 0.00030507
Iteration 54/1000 | Loss: 0.00033233
Iteration 55/1000 | Loss: 0.00035401
Iteration 56/1000 | Loss: 0.00027751
Iteration 57/1000 | Loss: 0.00027947
Iteration 58/1000 | Loss: 0.00028755
Iteration 59/1000 | Loss: 0.00046390
Iteration 60/1000 | Loss: 0.00031759
Iteration 61/1000 | Loss: 0.00025865
Iteration 62/1000 | Loss: 0.00046236
Iteration 63/1000 | Loss: 0.00021557
Iteration 64/1000 | Loss: 0.00063845
Iteration 65/1000 | Loss: 0.00036990
Iteration 66/1000 | Loss: 0.00020661
Iteration 67/1000 | Loss: 0.00030249
Iteration 68/1000 | Loss: 0.00028701
Iteration 69/1000 | Loss: 0.00027035
Iteration 70/1000 | Loss: 0.00022112
Iteration 71/1000 | Loss: 0.00024176
Iteration 72/1000 | Loss: 0.00020618
Iteration 73/1000 | Loss: 0.00021088
Iteration 74/1000 | Loss: 0.00043006
Iteration 75/1000 | Loss: 0.00037698
Iteration 76/1000 | Loss: 0.00043240
Iteration 77/1000 | Loss: 0.00026914
Iteration 78/1000 | Loss: 0.00030615
Iteration 79/1000 | Loss: 0.00024569
Iteration 80/1000 | Loss: 0.00028310
Iteration 81/1000 | Loss: 0.00029931
Iteration 82/1000 | Loss: 0.00027984
Iteration 83/1000 | Loss: 0.00028596
Iteration 84/1000 | Loss: 0.00038998
Iteration 85/1000 | Loss: 0.00020106
Iteration 86/1000 | Loss: 0.00019438
Iteration 87/1000 | Loss: 0.00050989
Iteration 88/1000 | Loss: 0.00020105
Iteration 89/1000 | Loss: 0.00019583
Iteration 90/1000 | Loss: 0.00019075
Iteration 91/1000 | Loss: 0.00018989
Iteration 92/1000 | Loss: 0.00018930
Iteration 93/1000 | Loss: 0.00018867
Iteration 94/1000 | Loss: 0.00018807
Iteration 95/1000 | Loss: 0.00018754
Iteration 96/1000 | Loss: 0.00018728
Iteration 97/1000 | Loss: 0.00018938
Iteration 98/1000 | Loss: 0.00020032
Iteration 99/1000 | Loss: 0.00019148
Iteration 100/1000 | Loss: 0.00018685
Iteration 101/1000 | Loss: 0.00018660
Iteration 102/1000 | Loss: 0.00018813
Iteration 103/1000 | Loss: 0.00018644
Iteration 104/1000 | Loss: 0.00018644
Iteration 105/1000 | Loss: 0.00018644
Iteration 106/1000 | Loss: 0.00018644
Iteration 107/1000 | Loss: 0.00018644
Iteration 108/1000 | Loss: 0.00018644
Iteration 109/1000 | Loss: 0.00018643
Iteration 110/1000 | Loss: 0.00018643
Iteration 111/1000 | Loss: 0.00018643
Iteration 112/1000 | Loss: 0.00018643
Iteration 113/1000 | Loss: 0.00018643
Iteration 114/1000 | Loss: 0.00018643
Iteration 115/1000 | Loss: 0.00018643
Iteration 116/1000 | Loss: 0.00018642
Iteration 117/1000 | Loss: 0.00018642
Iteration 118/1000 | Loss: 0.00018642
Iteration 119/1000 | Loss: 0.00018641
Iteration 120/1000 | Loss: 0.00018640
Iteration 121/1000 | Loss: 0.00018640
Iteration 122/1000 | Loss: 0.00018892
Iteration 123/1000 | Loss: 0.00018892
Iteration 124/1000 | Loss: 0.00018694
Iteration 125/1000 | Loss: 0.00018634
Iteration 126/1000 | Loss: 0.00018634
Iteration 127/1000 | Loss: 0.00018634
Iteration 128/1000 | Loss: 0.00018634
Iteration 129/1000 | Loss: 0.00018634
Iteration 130/1000 | Loss: 0.00018633
Iteration 131/1000 | Loss: 0.00018633
Iteration 132/1000 | Loss: 0.00018633
Iteration 133/1000 | Loss: 0.00018633
Iteration 134/1000 | Loss: 0.00018633
Iteration 135/1000 | Loss: 0.00018633
Iteration 136/1000 | Loss: 0.00018645
Iteration 137/1000 | Loss: 0.00018631
Iteration 138/1000 | Loss: 0.00018631
Iteration 139/1000 | Loss: 0.00018631
Iteration 140/1000 | Loss: 0.00018631
Iteration 141/1000 | Loss: 0.00018631
Iteration 142/1000 | Loss: 0.00018631
Iteration 143/1000 | Loss: 0.00018631
Iteration 144/1000 | Loss: 0.00018630
Iteration 145/1000 | Loss: 0.00018630
Iteration 146/1000 | Loss: 0.00018630
Iteration 147/1000 | Loss: 0.00018630
Iteration 148/1000 | Loss: 0.00018630
Iteration 149/1000 | Loss: 0.00018630
Iteration 150/1000 | Loss: 0.00018630
Iteration 151/1000 | Loss: 0.00018634
Iteration 152/1000 | Loss: 0.00018634
Iteration 153/1000 | Loss: 0.00018634
Iteration 154/1000 | Loss: 0.00018633
Iteration 155/1000 | Loss: 0.00018656
Iteration 156/1000 | Loss: 0.00018629
Iteration 157/1000 | Loss: 0.00018628
Iteration 158/1000 | Loss: 0.00018628
Iteration 159/1000 | Loss: 0.00018628
Iteration 160/1000 | Loss: 0.00018628
Iteration 161/1000 | Loss: 0.00018628
Iteration 162/1000 | Loss: 0.00018628
Iteration 163/1000 | Loss: 0.00018628
Iteration 164/1000 | Loss: 0.00018627
Iteration 165/1000 | Loss: 0.00018627
Iteration 166/1000 | Loss: 0.00018627
Iteration 167/1000 | Loss: 0.00018627
Iteration 168/1000 | Loss: 0.00018627
Iteration 169/1000 | Loss: 0.00018627
Iteration 170/1000 | Loss: 0.00018627
Iteration 171/1000 | Loss: 0.00018627
Iteration 172/1000 | Loss: 0.00018627
Iteration 173/1000 | Loss: 0.00018627
Iteration 174/1000 | Loss: 0.00018627
Iteration 175/1000 | Loss: 0.00018627
Iteration 176/1000 | Loss: 0.00018627
Iteration 177/1000 | Loss: 0.00018626
Iteration 178/1000 | Loss: 0.00018626
Iteration 179/1000 | Loss: 0.00018626
Iteration 180/1000 | Loss: 0.00018626
Iteration 181/1000 | Loss: 0.00018626
Iteration 182/1000 | Loss: 0.00018629
Iteration 183/1000 | Loss: 0.00018629
Iteration 184/1000 | Loss: 0.00018628
Iteration 185/1000 | Loss: 0.00018628
Iteration 186/1000 | Loss: 0.00018628
Iteration 187/1000 | Loss: 0.00018628
Iteration 188/1000 | Loss: 0.00018628
Iteration 189/1000 | Loss: 0.00018628
Iteration 190/1000 | Loss: 0.00018628
Iteration 191/1000 | Loss: 0.00018628
Iteration 192/1000 | Loss: 0.00018626
Iteration 193/1000 | Loss: 0.00018626
Iteration 194/1000 | Loss: 0.00018626
Iteration 195/1000 | Loss: 0.00018626
Iteration 196/1000 | Loss: 0.00018626
Iteration 197/1000 | Loss: 0.00018626
Iteration 198/1000 | Loss: 0.00018626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [0.0001862582575995475, 0.0001862582575995475, 0.0001862582575995475, 0.0001862582575995475, 0.0001862582575995475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001862582575995475

Optimization complete. Final v2v error: 7.344240188598633 mm

Highest mean error: 13.804722785949707 mm for frame 67

Lowest mean error: 4.33772087097168 mm for frame 190

Saving results

Total time: 198.39841890335083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429896
Iteration 2/25 | Loss: 0.00203903
Iteration 3/25 | Loss: 0.00194113
Iteration 4/25 | Loss: 0.00192421
Iteration 5/25 | Loss: 0.00191843
Iteration 6/25 | Loss: 0.00191591
Iteration 7/25 | Loss: 0.00191532
Iteration 8/25 | Loss: 0.00191532
Iteration 9/25 | Loss: 0.00191532
Iteration 10/25 | Loss: 0.00191532
Iteration 11/25 | Loss: 0.00191532
Iteration 12/25 | Loss: 0.00191532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0019153167959302664, 0.0019153167959302664, 0.0019153167959302664, 0.0019153167959302664, 0.0019153167959302664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019153167959302664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20906007
Iteration 2/25 | Loss: 0.00308789
Iteration 3/25 | Loss: 0.00308789
Iteration 4/25 | Loss: 0.00308789
Iteration 5/25 | Loss: 0.00308789
Iteration 6/25 | Loss: 0.00308789
Iteration 7/25 | Loss: 0.00308789
Iteration 8/25 | Loss: 0.00308789
Iteration 9/25 | Loss: 0.00308789
Iteration 10/25 | Loss: 0.00308789
Iteration 11/25 | Loss: 0.00308789
Iteration 12/25 | Loss: 0.00308789
Iteration 13/25 | Loss: 0.00308789
Iteration 14/25 | Loss: 0.00308789
Iteration 15/25 | Loss: 0.00308789
Iteration 16/25 | Loss: 0.00308789
Iteration 17/25 | Loss: 0.00308789
Iteration 18/25 | Loss: 0.00308789
Iteration 19/25 | Loss: 0.00308789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003087890800088644, 0.003087890800088644, 0.003087890800088644, 0.003087890800088644, 0.003087890800088644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003087890800088644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308789
Iteration 2/1000 | Loss: 0.00013658
Iteration 3/1000 | Loss: 0.00006667
Iteration 4/1000 | Loss: 0.00004736
Iteration 5/1000 | Loss: 0.00004237
Iteration 6/1000 | Loss: 0.00003965
Iteration 7/1000 | Loss: 0.00003734
Iteration 8/1000 | Loss: 0.00003558
Iteration 9/1000 | Loss: 0.00003459
Iteration 10/1000 | Loss: 0.00003390
Iteration 11/1000 | Loss: 0.00003321
Iteration 12/1000 | Loss: 0.00003265
Iteration 13/1000 | Loss: 0.00003218
Iteration 14/1000 | Loss: 0.00003187
Iteration 15/1000 | Loss: 0.00003158
Iteration 16/1000 | Loss: 0.00003140
Iteration 17/1000 | Loss: 0.00003128
Iteration 18/1000 | Loss: 0.00003125
Iteration 19/1000 | Loss: 0.00003114
Iteration 20/1000 | Loss: 0.00003108
Iteration 21/1000 | Loss: 0.00003108
Iteration 22/1000 | Loss: 0.00003107
Iteration 23/1000 | Loss: 0.00003100
Iteration 24/1000 | Loss: 0.00003099
Iteration 25/1000 | Loss: 0.00003096
Iteration 26/1000 | Loss: 0.00003093
Iteration 27/1000 | Loss: 0.00003093
Iteration 28/1000 | Loss: 0.00003092
Iteration 29/1000 | Loss: 0.00003092
Iteration 30/1000 | Loss: 0.00003091
Iteration 31/1000 | Loss: 0.00003091
Iteration 32/1000 | Loss: 0.00003091
Iteration 33/1000 | Loss: 0.00003090
Iteration 34/1000 | Loss: 0.00003090
Iteration 35/1000 | Loss: 0.00003090
Iteration 36/1000 | Loss: 0.00003089
Iteration 37/1000 | Loss: 0.00003089
Iteration 38/1000 | Loss: 0.00003088
Iteration 39/1000 | Loss: 0.00003088
Iteration 40/1000 | Loss: 0.00003087
Iteration 41/1000 | Loss: 0.00003086
Iteration 42/1000 | Loss: 0.00003086
Iteration 43/1000 | Loss: 0.00003085
Iteration 44/1000 | Loss: 0.00003085
Iteration 45/1000 | Loss: 0.00003085
Iteration 46/1000 | Loss: 0.00003085
Iteration 47/1000 | Loss: 0.00003085
Iteration 48/1000 | Loss: 0.00003084
Iteration 49/1000 | Loss: 0.00003084
Iteration 50/1000 | Loss: 0.00003084
Iteration 51/1000 | Loss: 0.00003084
Iteration 52/1000 | Loss: 0.00003084
Iteration 53/1000 | Loss: 0.00003084
Iteration 54/1000 | Loss: 0.00003084
Iteration 55/1000 | Loss: 0.00003084
Iteration 56/1000 | Loss: 0.00003084
Iteration 57/1000 | Loss: 0.00003084
Iteration 58/1000 | Loss: 0.00003084
Iteration 59/1000 | Loss: 0.00003083
Iteration 60/1000 | Loss: 0.00003083
Iteration 61/1000 | Loss: 0.00003083
Iteration 62/1000 | Loss: 0.00003082
Iteration 63/1000 | Loss: 0.00003082
Iteration 64/1000 | Loss: 0.00003082
Iteration 65/1000 | Loss: 0.00003082
Iteration 66/1000 | Loss: 0.00003082
Iteration 67/1000 | Loss: 0.00003082
Iteration 68/1000 | Loss: 0.00003082
Iteration 69/1000 | Loss: 0.00003082
Iteration 70/1000 | Loss: 0.00003081
Iteration 71/1000 | Loss: 0.00003081
Iteration 72/1000 | Loss: 0.00003081
Iteration 73/1000 | Loss: 0.00003081
Iteration 74/1000 | Loss: 0.00003081
Iteration 75/1000 | Loss: 0.00003081
Iteration 76/1000 | Loss: 0.00003081
Iteration 77/1000 | Loss: 0.00003080
Iteration 78/1000 | Loss: 0.00003080
Iteration 79/1000 | Loss: 0.00003080
Iteration 80/1000 | Loss: 0.00003080
Iteration 81/1000 | Loss: 0.00003080
Iteration 82/1000 | Loss: 0.00003080
Iteration 83/1000 | Loss: 0.00003080
Iteration 84/1000 | Loss: 0.00003080
Iteration 85/1000 | Loss: 0.00003080
Iteration 86/1000 | Loss: 0.00003080
Iteration 87/1000 | Loss: 0.00003080
Iteration 88/1000 | Loss: 0.00003079
Iteration 89/1000 | Loss: 0.00003079
Iteration 90/1000 | Loss: 0.00003079
Iteration 91/1000 | Loss: 0.00003079
Iteration 92/1000 | Loss: 0.00003079
Iteration 93/1000 | Loss: 0.00003079
Iteration 94/1000 | Loss: 0.00003079
Iteration 95/1000 | Loss: 0.00003079
Iteration 96/1000 | Loss: 0.00003079
Iteration 97/1000 | Loss: 0.00003078
Iteration 98/1000 | Loss: 0.00003078
Iteration 99/1000 | Loss: 0.00003078
Iteration 100/1000 | Loss: 0.00003078
Iteration 101/1000 | Loss: 0.00003078
Iteration 102/1000 | Loss: 0.00003078
Iteration 103/1000 | Loss: 0.00003078
Iteration 104/1000 | Loss: 0.00003078
Iteration 105/1000 | Loss: 0.00003078
Iteration 106/1000 | Loss: 0.00003077
Iteration 107/1000 | Loss: 0.00003077
Iteration 108/1000 | Loss: 0.00003077
Iteration 109/1000 | Loss: 0.00003077
Iteration 110/1000 | Loss: 0.00003077
Iteration 111/1000 | Loss: 0.00003077
Iteration 112/1000 | Loss: 0.00003077
Iteration 113/1000 | Loss: 0.00003077
Iteration 114/1000 | Loss: 0.00003077
Iteration 115/1000 | Loss: 0.00003077
Iteration 116/1000 | Loss: 0.00003077
Iteration 117/1000 | Loss: 0.00003077
Iteration 118/1000 | Loss: 0.00003076
Iteration 119/1000 | Loss: 0.00003076
Iteration 120/1000 | Loss: 0.00003076
Iteration 121/1000 | Loss: 0.00003076
Iteration 122/1000 | Loss: 0.00003076
Iteration 123/1000 | Loss: 0.00003076
Iteration 124/1000 | Loss: 0.00003076
Iteration 125/1000 | Loss: 0.00003076
Iteration 126/1000 | Loss: 0.00003076
Iteration 127/1000 | Loss: 0.00003076
Iteration 128/1000 | Loss: 0.00003076
Iteration 129/1000 | Loss: 0.00003075
Iteration 130/1000 | Loss: 0.00003075
Iteration 131/1000 | Loss: 0.00003075
Iteration 132/1000 | Loss: 0.00003075
Iteration 133/1000 | Loss: 0.00003075
Iteration 134/1000 | Loss: 0.00003075
Iteration 135/1000 | Loss: 0.00003075
Iteration 136/1000 | Loss: 0.00003075
Iteration 137/1000 | Loss: 0.00003075
Iteration 138/1000 | Loss: 0.00003075
Iteration 139/1000 | Loss: 0.00003075
Iteration 140/1000 | Loss: 0.00003075
Iteration 141/1000 | Loss: 0.00003074
Iteration 142/1000 | Loss: 0.00003074
Iteration 143/1000 | Loss: 0.00003074
Iteration 144/1000 | Loss: 0.00003074
Iteration 145/1000 | Loss: 0.00003074
Iteration 146/1000 | Loss: 0.00003074
Iteration 147/1000 | Loss: 0.00003074
Iteration 148/1000 | Loss: 0.00003073
Iteration 149/1000 | Loss: 0.00003073
Iteration 150/1000 | Loss: 0.00003073
Iteration 151/1000 | Loss: 0.00003073
Iteration 152/1000 | Loss: 0.00003073
Iteration 153/1000 | Loss: 0.00003073
Iteration 154/1000 | Loss: 0.00003072
Iteration 155/1000 | Loss: 0.00003072
Iteration 156/1000 | Loss: 0.00003072
Iteration 157/1000 | Loss: 0.00003072
Iteration 158/1000 | Loss: 0.00003072
Iteration 159/1000 | Loss: 0.00003072
Iteration 160/1000 | Loss: 0.00003072
Iteration 161/1000 | Loss: 0.00003072
Iteration 162/1000 | Loss: 0.00003072
Iteration 163/1000 | Loss: 0.00003072
Iteration 164/1000 | Loss: 0.00003072
Iteration 165/1000 | Loss: 0.00003072
Iteration 166/1000 | Loss: 0.00003072
Iteration 167/1000 | Loss: 0.00003072
Iteration 168/1000 | Loss: 0.00003072
Iteration 169/1000 | Loss: 0.00003072
Iteration 170/1000 | Loss: 0.00003072
Iteration 171/1000 | Loss: 0.00003072
Iteration 172/1000 | Loss: 0.00003072
Iteration 173/1000 | Loss: 0.00003072
Iteration 174/1000 | Loss: 0.00003072
Iteration 175/1000 | Loss: 0.00003072
Iteration 176/1000 | Loss: 0.00003072
Iteration 177/1000 | Loss: 0.00003072
Iteration 178/1000 | Loss: 0.00003072
Iteration 179/1000 | Loss: 0.00003072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.0719729693373665e-05, 3.0719729693373665e-05, 3.0719729693373665e-05, 3.0719729693373665e-05, 3.0719729693373665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0719729693373665e-05

Optimization complete. Final v2v error: 4.764535903930664 mm

Highest mean error: 5.487589359283447 mm for frame 151

Lowest mean error: 4.219324588775635 mm for frame 0

Saving results

Total time: 46.944881200790405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871951
Iteration 2/25 | Loss: 0.00212629
Iteration 3/25 | Loss: 0.00188811
Iteration 4/25 | Loss: 0.00186941
Iteration 5/25 | Loss: 0.00186663
Iteration 6/25 | Loss: 0.00186663
Iteration 7/25 | Loss: 0.00186663
Iteration 8/25 | Loss: 0.00186663
Iteration 9/25 | Loss: 0.00186663
Iteration 10/25 | Loss: 0.00186663
Iteration 11/25 | Loss: 0.00186663
Iteration 12/25 | Loss: 0.00186663
Iteration 13/25 | Loss: 0.00186663
Iteration 14/25 | Loss: 0.00186663
Iteration 15/25 | Loss: 0.00186663
Iteration 16/25 | Loss: 0.00186663
Iteration 17/25 | Loss: 0.00186663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018666336545720696, 0.0018666336545720696, 0.0018666336545720696, 0.0018666336545720696, 0.0018666336545720696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018666336545720696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17832088
Iteration 2/25 | Loss: 0.00234902
Iteration 3/25 | Loss: 0.00234902
Iteration 4/25 | Loss: 0.00234902
Iteration 5/25 | Loss: 0.00234902
Iteration 6/25 | Loss: 0.00234902
Iteration 7/25 | Loss: 0.00234902
Iteration 8/25 | Loss: 0.00234902
Iteration 9/25 | Loss: 0.00234902
Iteration 10/25 | Loss: 0.00234902
Iteration 11/25 | Loss: 0.00234902
Iteration 12/25 | Loss: 0.00234902
Iteration 13/25 | Loss: 0.00234902
Iteration 14/25 | Loss: 0.00234902
Iteration 15/25 | Loss: 0.00234902
Iteration 16/25 | Loss: 0.00234902
Iteration 17/25 | Loss: 0.00234902
Iteration 18/25 | Loss: 0.00234902
Iteration 19/25 | Loss: 0.00234902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002349015325307846, 0.002349015325307846, 0.002349015325307846, 0.002349015325307846, 0.002349015325307846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002349015325307846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234902
Iteration 2/1000 | Loss: 0.00005785
Iteration 3/1000 | Loss: 0.00004107
Iteration 4/1000 | Loss: 0.00003564
Iteration 5/1000 | Loss: 0.00003280
Iteration 6/1000 | Loss: 0.00003096
Iteration 7/1000 | Loss: 0.00002952
Iteration 8/1000 | Loss: 0.00002872
Iteration 9/1000 | Loss: 0.00002818
Iteration 10/1000 | Loss: 0.00002782
Iteration 11/1000 | Loss: 0.00002746
Iteration 12/1000 | Loss: 0.00002725
Iteration 13/1000 | Loss: 0.00002718
Iteration 14/1000 | Loss: 0.00002699
Iteration 15/1000 | Loss: 0.00002699
Iteration 16/1000 | Loss: 0.00002684
Iteration 17/1000 | Loss: 0.00002677
Iteration 18/1000 | Loss: 0.00002677
Iteration 19/1000 | Loss: 0.00002676
Iteration 20/1000 | Loss: 0.00002675
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00002666
Iteration 23/1000 | Loss: 0.00002666
Iteration 24/1000 | Loss: 0.00002666
Iteration 25/1000 | Loss: 0.00002665
Iteration 26/1000 | Loss: 0.00002665
Iteration 27/1000 | Loss: 0.00002665
Iteration 28/1000 | Loss: 0.00002664
Iteration 29/1000 | Loss: 0.00002664
Iteration 30/1000 | Loss: 0.00002664
Iteration 31/1000 | Loss: 0.00002664
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002663
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002663
Iteration 36/1000 | Loss: 0.00002662
Iteration 37/1000 | Loss: 0.00002662
Iteration 38/1000 | Loss: 0.00002662
Iteration 39/1000 | Loss: 0.00002662
Iteration 40/1000 | Loss: 0.00002662
Iteration 41/1000 | Loss: 0.00002662
Iteration 42/1000 | Loss: 0.00002661
Iteration 43/1000 | Loss: 0.00002659
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00002659
Iteration 46/1000 | Loss: 0.00002658
Iteration 47/1000 | Loss: 0.00002658
Iteration 48/1000 | Loss: 0.00002658
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002658
Iteration 51/1000 | Loss: 0.00002658
Iteration 52/1000 | Loss: 0.00002658
Iteration 53/1000 | Loss: 0.00002657
Iteration 54/1000 | Loss: 0.00002657
Iteration 55/1000 | Loss: 0.00002656
Iteration 56/1000 | Loss: 0.00002656
Iteration 57/1000 | Loss: 0.00002656
Iteration 58/1000 | Loss: 0.00002656
Iteration 59/1000 | Loss: 0.00002656
Iteration 60/1000 | Loss: 0.00002656
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00002656
Iteration 63/1000 | Loss: 0.00002656
Iteration 64/1000 | Loss: 0.00002656
Iteration 65/1000 | Loss: 0.00002656
Iteration 66/1000 | Loss: 0.00002655
Iteration 67/1000 | Loss: 0.00002655
Iteration 68/1000 | Loss: 0.00002655
Iteration 69/1000 | Loss: 0.00002655
Iteration 70/1000 | Loss: 0.00002655
Iteration 71/1000 | Loss: 0.00002655
Iteration 72/1000 | Loss: 0.00002655
Iteration 73/1000 | Loss: 0.00002655
Iteration 74/1000 | Loss: 0.00002655
Iteration 75/1000 | Loss: 0.00002655
Iteration 76/1000 | Loss: 0.00002655
Iteration 77/1000 | Loss: 0.00002655
Iteration 78/1000 | Loss: 0.00002655
Iteration 79/1000 | Loss: 0.00002655
Iteration 80/1000 | Loss: 0.00002655
Iteration 81/1000 | Loss: 0.00002655
Iteration 82/1000 | Loss: 0.00002655
Iteration 83/1000 | Loss: 0.00002655
Iteration 84/1000 | Loss: 0.00002655
Iteration 85/1000 | Loss: 0.00002655
Iteration 86/1000 | Loss: 0.00002655
Iteration 87/1000 | Loss: 0.00002655
Iteration 88/1000 | Loss: 0.00002655
Iteration 89/1000 | Loss: 0.00002655
Iteration 90/1000 | Loss: 0.00002655
Iteration 91/1000 | Loss: 0.00002655
Iteration 92/1000 | Loss: 0.00002655
Iteration 93/1000 | Loss: 0.00002655
Iteration 94/1000 | Loss: 0.00002655
Iteration 95/1000 | Loss: 0.00002655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.6550971597316675e-05, 2.6550971597316675e-05, 2.6550971597316675e-05, 2.6550971597316675e-05, 2.6550971597316675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6550971597316675e-05

Optimization complete. Final v2v error: 4.432966232299805 mm

Highest mean error: 4.631729602813721 mm for frame 39

Lowest mean error: 4.2320661544799805 mm for frame 193

Saving results

Total time: 37.458980083465576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01193556
Iteration 2/25 | Loss: 0.00262475
Iteration 3/25 | Loss: 0.00210507
Iteration 4/25 | Loss: 0.00205070
Iteration 5/25 | Loss: 0.00206141
Iteration 6/25 | Loss: 0.00204562
Iteration 7/25 | Loss: 0.00202165
Iteration 8/25 | Loss: 0.00200052
Iteration 9/25 | Loss: 0.00197983
Iteration 10/25 | Loss: 0.00197868
Iteration 11/25 | Loss: 0.00197453
Iteration 12/25 | Loss: 0.00196838
Iteration 13/25 | Loss: 0.00196659
Iteration 14/25 | Loss: 0.00196873
Iteration 15/25 | Loss: 0.00196815
Iteration 16/25 | Loss: 0.00196653
Iteration 17/25 | Loss: 0.00196557
Iteration 18/25 | Loss: 0.00196557
Iteration 19/25 | Loss: 0.00196557
Iteration 20/25 | Loss: 0.00196557
Iteration 21/25 | Loss: 0.00196557
Iteration 22/25 | Loss: 0.00196557
Iteration 23/25 | Loss: 0.00196557
Iteration 24/25 | Loss: 0.00196556
Iteration 25/25 | Loss: 0.00196556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71563458
Iteration 2/25 | Loss: 0.00234898
Iteration 3/25 | Loss: 0.00234896
Iteration 4/25 | Loss: 0.00234896
Iteration 5/25 | Loss: 0.00234896
Iteration 6/25 | Loss: 0.00234896
Iteration 7/25 | Loss: 0.00234896
Iteration 8/25 | Loss: 0.00234896
Iteration 9/25 | Loss: 0.00234896
Iteration 10/25 | Loss: 0.00234896
Iteration 11/25 | Loss: 0.00234896
Iteration 12/25 | Loss: 0.00234896
Iteration 13/25 | Loss: 0.00234896
Iteration 14/25 | Loss: 0.00234896
Iteration 15/25 | Loss: 0.00234896
Iteration 16/25 | Loss: 0.00234896
Iteration 17/25 | Loss: 0.00234896
Iteration 18/25 | Loss: 0.00234896
Iteration 19/25 | Loss: 0.00234896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002348958747461438, 0.002348958747461438, 0.002348958747461438, 0.002348958747461438, 0.002348958747461438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002348958747461438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234896
Iteration 2/1000 | Loss: 0.00011555
Iteration 3/1000 | Loss: 0.00008599
Iteration 4/1000 | Loss: 0.00007448
Iteration 5/1000 | Loss: 0.00006740
Iteration 6/1000 | Loss: 0.00006366
Iteration 7/1000 | Loss: 0.00006181
Iteration 8/1000 | Loss: 0.00005995
Iteration 9/1000 | Loss: 0.00005869
Iteration 10/1000 | Loss: 0.00010783
Iteration 11/1000 | Loss: 0.00005884
Iteration 12/1000 | Loss: 0.00005659
Iteration 13/1000 | Loss: 0.00005607
Iteration 14/1000 | Loss: 0.00013062
Iteration 15/1000 | Loss: 0.00007275
Iteration 16/1000 | Loss: 0.00007045
Iteration 17/1000 | Loss: 0.00005498
Iteration 18/1000 | Loss: 0.00005466
Iteration 19/1000 | Loss: 0.00010430
Iteration 20/1000 | Loss: 0.00005767
Iteration 21/1000 | Loss: 0.00008653
Iteration 22/1000 | Loss: 0.00005405
Iteration 23/1000 | Loss: 0.00005369
Iteration 24/1000 | Loss: 0.00005343
Iteration 25/1000 | Loss: 0.00008688
Iteration 26/1000 | Loss: 0.00005300
Iteration 27/1000 | Loss: 0.00009979
Iteration 28/1000 | Loss: 0.00005285
Iteration 29/1000 | Loss: 0.00005259
Iteration 30/1000 | Loss: 0.00005258
Iteration 31/1000 | Loss: 0.00005245
Iteration 32/1000 | Loss: 0.00005228
Iteration 33/1000 | Loss: 0.00005217
Iteration 34/1000 | Loss: 0.00005215
Iteration 35/1000 | Loss: 0.00005203
Iteration 36/1000 | Loss: 0.00005201
Iteration 37/1000 | Loss: 0.00005188
Iteration 38/1000 | Loss: 0.00005184
Iteration 39/1000 | Loss: 0.00005184
Iteration 40/1000 | Loss: 0.00005184
Iteration 41/1000 | Loss: 0.00005183
Iteration 42/1000 | Loss: 0.00005183
Iteration 43/1000 | Loss: 0.00005181
Iteration 44/1000 | Loss: 0.00005181
Iteration 45/1000 | Loss: 0.00005181
Iteration 46/1000 | Loss: 0.00005181
Iteration 47/1000 | Loss: 0.00005181
Iteration 48/1000 | Loss: 0.00005181
Iteration 49/1000 | Loss: 0.00005181
Iteration 50/1000 | Loss: 0.00005181
Iteration 51/1000 | Loss: 0.00005180
Iteration 52/1000 | Loss: 0.00005180
Iteration 53/1000 | Loss: 0.00005180
Iteration 54/1000 | Loss: 0.00005180
Iteration 55/1000 | Loss: 0.00005180
Iteration 56/1000 | Loss: 0.00005180
Iteration 57/1000 | Loss: 0.00005180
Iteration 58/1000 | Loss: 0.00005180
Iteration 59/1000 | Loss: 0.00005180
Iteration 60/1000 | Loss: 0.00005179
Iteration 61/1000 | Loss: 0.00005179
Iteration 62/1000 | Loss: 0.00005178
Iteration 63/1000 | Loss: 0.00005178
Iteration 64/1000 | Loss: 0.00005178
Iteration 65/1000 | Loss: 0.00005178
Iteration 66/1000 | Loss: 0.00005178
Iteration 67/1000 | Loss: 0.00005178
Iteration 68/1000 | Loss: 0.00005178
Iteration 69/1000 | Loss: 0.00005178
Iteration 70/1000 | Loss: 0.00005178
Iteration 71/1000 | Loss: 0.00005178
Iteration 72/1000 | Loss: 0.00005178
Iteration 73/1000 | Loss: 0.00005177
Iteration 74/1000 | Loss: 0.00005177
Iteration 75/1000 | Loss: 0.00005177
Iteration 76/1000 | Loss: 0.00005177
Iteration 77/1000 | Loss: 0.00005177
Iteration 78/1000 | Loss: 0.00005176
Iteration 79/1000 | Loss: 0.00005176
Iteration 80/1000 | Loss: 0.00005176
Iteration 81/1000 | Loss: 0.00005176
Iteration 82/1000 | Loss: 0.00005176
Iteration 83/1000 | Loss: 0.00005176
Iteration 84/1000 | Loss: 0.00005176
Iteration 85/1000 | Loss: 0.00005176
Iteration 86/1000 | Loss: 0.00005176
Iteration 87/1000 | Loss: 0.00005176
Iteration 88/1000 | Loss: 0.00005176
Iteration 89/1000 | Loss: 0.00005176
Iteration 90/1000 | Loss: 0.00005175
Iteration 91/1000 | Loss: 0.00005175
Iteration 92/1000 | Loss: 0.00005175
Iteration 93/1000 | Loss: 0.00005175
Iteration 94/1000 | Loss: 0.00005175
Iteration 95/1000 | Loss: 0.00005175
Iteration 96/1000 | Loss: 0.00005174
Iteration 97/1000 | Loss: 0.00005174
Iteration 98/1000 | Loss: 0.00005174
Iteration 99/1000 | Loss: 0.00005174
Iteration 100/1000 | Loss: 0.00005173
Iteration 101/1000 | Loss: 0.00005173
Iteration 102/1000 | Loss: 0.00005173
Iteration 103/1000 | Loss: 0.00005173
Iteration 104/1000 | Loss: 0.00005172
Iteration 105/1000 | Loss: 0.00005172
Iteration 106/1000 | Loss: 0.00005172
Iteration 107/1000 | Loss: 0.00005172
Iteration 108/1000 | Loss: 0.00005172
Iteration 109/1000 | Loss: 0.00005172
Iteration 110/1000 | Loss: 0.00005172
Iteration 111/1000 | Loss: 0.00005171
Iteration 112/1000 | Loss: 0.00005171
Iteration 113/1000 | Loss: 0.00005171
Iteration 114/1000 | Loss: 0.00005171
Iteration 115/1000 | Loss: 0.00005171
Iteration 116/1000 | Loss: 0.00005171
Iteration 117/1000 | Loss: 0.00005171
Iteration 118/1000 | Loss: 0.00005171
Iteration 119/1000 | Loss: 0.00005170
Iteration 120/1000 | Loss: 0.00005170
Iteration 121/1000 | Loss: 0.00005170
Iteration 122/1000 | Loss: 0.00005170
Iteration 123/1000 | Loss: 0.00005170
Iteration 124/1000 | Loss: 0.00005170
Iteration 125/1000 | Loss: 0.00005170
Iteration 126/1000 | Loss: 0.00005170
Iteration 127/1000 | Loss: 0.00005170
Iteration 128/1000 | Loss: 0.00005170
Iteration 129/1000 | Loss: 0.00005170
Iteration 130/1000 | Loss: 0.00005170
Iteration 131/1000 | Loss: 0.00005170
Iteration 132/1000 | Loss: 0.00005170
Iteration 133/1000 | Loss: 0.00005170
Iteration 134/1000 | Loss: 0.00005170
Iteration 135/1000 | Loss: 0.00005170
Iteration 136/1000 | Loss: 0.00005170
Iteration 137/1000 | Loss: 0.00005170
Iteration 138/1000 | Loss: 0.00005170
Iteration 139/1000 | Loss: 0.00005170
Iteration 140/1000 | Loss: 0.00005170
Iteration 141/1000 | Loss: 0.00005170
Iteration 142/1000 | Loss: 0.00005170
Iteration 143/1000 | Loss: 0.00005170
Iteration 144/1000 | Loss: 0.00005170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [5.169637006474659e-05, 5.169637006474659e-05, 5.169637006474659e-05, 5.169637006474659e-05, 5.169637006474659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.169637006474659e-05

Optimization complete. Final v2v error: 5.894033908843994 mm

Highest mean error: 6.898163795471191 mm for frame 170

Lowest mean error: 4.566299915313721 mm for frame 229

Saving results

Total time: 98.99372673034668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771481
Iteration 2/25 | Loss: 0.00200764
Iteration 3/25 | Loss: 0.00190129
Iteration 4/25 | Loss: 0.00184992
Iteration 5/25 | Loss: 0.00184376
Iteration 6/25 | Loss: 0.00183866
Iteration 7/25 | Loss: 0.00183979
Iteration 8/25 | Loss: 0.00183908
Iteration 9/25 | Loss: 0.00183811
Iteration 10/25 | Loss: 0.00183749
Iteration 11/25 | Loss: 0.00183749
Iteration 12/25 | Loss: 0.00183749
Iteration 13/25 | Loss: 0.00183749
Iteration 14/25 | Loss: 0.00183748
Iteration 15/25 | Loss: 0.00183747
Iteration 16/25 | Loss: 0.00183747
Iteration 17/25 | Loss: 0.00183747
Iteration 18/25 | Loss: 0.00183747
Iteration 19/25 | Loss: 0.00183747
Iteration 20/25 | Loss: 0.00183747
Iteration 21/25 | Loss: 0.00183747
Iteration 22/25 | Loss: 0.00183747
Iteration 23/25 | Loss: 0.00183747
Iteration 24/25 | Loss: 0.00183747
Iteration 25/25 | Loss: 0.00183747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17638755
Iteration 2/25 | Loss: 0.00277911
Iteration 3/25 | Loss: 0.00277911
Iteration 4/25 | Loss: 0.00277911
Iteration 5/25 | Loss: 0.00277910
Iteration 6/25 | Loss: 0.00277910
Iteration 7/25 | Loss: 0.00277910
Iteration 8/25 | Loss: 0.00277910
Iteration 9/25 | Loss: 0.00277910
Iteration 10/25 | Loss: 0.00277910
Iteration 11/25 | Loss: 0.00277910
Iteration 12/25 | Loss: 0.00277910
Iteration 13/25 | Loss: 0.00277910
Iteration 14/25 | Loss: 0.00277910
Iteration 15/25 | Loss: 0.00277910
Iteration 16/25 | Loss: 0.00277910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002779102884232998, 0.002779102884232998, 0.002779102884232998, 0.002779102884232998, 0.002779102884232998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002779102884232998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277910
Iteration 2/1000 | Loss: 0.00007211
Iteration 3/1000 | Loss: 0.00005303
Iteration 4/1000 | Loss: 0.00003501
Iteration 5/1000 | Loss: 0.00005388
Iteration 6/1000 | Loss: 0.00003193
Iteration 7/1000 | Loss: 0.00004329
Iteration 8/1000 | Loss: 0.00003021
Iteration 9/1000 | Loss: 0.00002948
Iteration 10/1000 | Loss: 0.00004694
Iteration 11/1000 | Loss: 0.00002872
Iteration 12/1000 | Loss: 0.00002849
Iteration 13/1000 | Loss: 0.00005934
Iteration 14/1000 | Loss: 0.00002799
Iteration 15/1000 | Loss: 0.00002780
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002766
Iteration 18/1000 | Loss: 0.00002761
Iteration 19/1000 | Loss: 0.00002761
Iteration 20/1000 | Loss: 0.00002758
Iteration 21/1000 | Loss: 0.00002758
Iteration 22/1000 | Loss: 0.00002757
Iteration 23/1000 | Loss: 0.00002757
Iteration 24/1000 | Loss: 0.00002756
Iteration 25/1000 | Loss: 0.00002756
Iteration 26/1000 | Loss: 0.00002756
Iteration 27/1000 | Loss: 0.00002755
Iteration 28/1000 | Loss: 0.00002755
Iteration 29/1000 | Loss: 0.00002755
Iteration 30/1000 | Loss: 0.00002755
Iteration 31/1000 | Loss: 0.00002754
Iteration 32/1000 | Loss: 0.00002749
Iteration 33/1000 | Loss: 0.00002749
Iteration 34/1000 | Loss: 0.00004559
Iteration 35/1000 | Loss: 0.00002755
Iteration 36/1000 | Loss: 0.00002746
Iteration 37/1000 | Loss: 0.00002746
Iteration 38/1000 | Loss: 0.00002746
Iteration 39/1000 | Loss: 0.00002746
Iteration 40/1000 | Loss: 0.00002746
Iteration 41/1000 | Loss: 0.00002746
Iteration 42/1000 | Loss: 0.00002746
Iteration 43/1000 | Loss: 0.00002745
Iteration 44/1000 | Loss: 0.00002745
Iteration 45/1000 | Loss: 0.00002745
Iteration 46/1000 | Loss: 0.00002745
Iteration 47/1000 | Loss: 0.00002745
Iteration 48/1000 | Loss: 0.00002745
Iteration 49/1000 | Loss: 0.00002745
Iteration 50/1000 | Loss: 0.00002745
Iteration 51/1000 | Loss: 0.00002745
Iteration 52/1000 | Loss: 0.00002745
Iteration 53/1000 | Loss: 0.00002745
Iteration 54/1000 | Loss: 0.00002745
Iteration 55/1000 | Loss: 0.00002745
Iteration 56/1000 | Loss: 0.00002745
Iteration 57/1000 | Loss: 0.00002745
Iteration 58/1000 | Loss: 0.00002745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.7447857064544223e-05, 2.7447857064544223e-05, 2.7447857064544223e-05, 2.7447857064544223e-05, 2.7447857064544223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7447857064544223e-05

Optimization complete. Final v2v error: 4.430413722991943 mm

Highest mean error: 4.763723373413086 mm for frame 191

Lowest mean error: 4.244074821472168 mm for frame 68

Saving results

Total time: 50.92711091041565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969003
Iteration 2/25 | Loss: 0.00214314
Iteration 3/25 | Loss: 0.00194499
Iteration 4/25 | Loss: 0.00191785
Iteration 5/25 | Loss: 0.00191363
Iteration 6/25 | Loss: 0.00191302
Iteration 7/25 | Loss: 0.00191302
Iteration 8/25 | Loss: 0.00191302
Iteration 9/25 | Loss: 0.00191302
Iteration 10/25 | Loss: 0.00191302
Iteration 11/25 | Loss: 0.00191302
Iteration 12/25 | Loss: 0.00191302
Iteration 13/25 | Loss: 0.00191302
Iteration 14/25 | Loss: 0.00191302
Iteration 15/25 | Loss: 0.00191302
Iteration 16/25 | Loss: 0.00191302
Iteration 17/25 | Loss: 0.00191302
Iteration 18/25 | Loss: 0.00191302
Iteration 19/25 | Loss: 0.00191302
Iteration 20/25 | Loss: 0.00191302
Iteration 21/25 | Loss: 0.00191302
Iteration 22/25 | Loss: 0.00191302
Iteration 23/25 | Loss: 0.00191302
Iteration 24/25 | Loss: 0.00191302
Iteration 25/25 | Loss: 0.00191302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78248781
Iteration 2/25 | Loss: 0.00174924
Iteration 3/25 | Loss: 0.00174924
Iteration 4/25 | Loss: 0.00174924
Iteration 5/25 | Loss: 0.00174924
Iteration 6/25 | Loss: 0.00174924
Iteration 7/25 | Loss: 0.00174924
Iteration 8/25 | Loss: 0.00174924
Iteration 9/25 | Loss: 0.00174924
Iteration 10/25 | Loss: 0.00174924
Iteration 11/25 | Loss: 0.00174924
Iteration 12/25 | Loss: 0.00174924
Iteration 13/25 | Loss: 0.00174924
Iteration 14/25 | Loss: 0.00174924
Iteration 15/25 | Loss: 0.00174924
Iteration 16/25 | Loss: 0.00174924
Iteration 17/25 | Loss: 0.00174924
Iteration 18/25 | Loss: 0.00174924
Iteration 19/25 | Loss: 0.00174924
Iteration 20/25 | Loss: 0.00174924
Iteration 21/25 | Loss: 0.00174924
Iteration 22/25 | Loss: 0.00174924
Iteration 23/25 | Loss: 0.00174924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017492373008280993, 0.0017492373008280993, 0.0017492373008280993, 0.0017492373008280993, 0.0017492373008280993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017492373008280993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174924
Iteration 2/1000 | Loss: 0.00007281
Iteration 3/1000 | Loss: 0.00005929
Iteration 4/1000 | Loss: 0.00005245
Iteration 5/1000 | Loss: 0.00004966
Iteration 6/1000 | Loss: 0.00004813
Iteration 7/1000 | Loss: 0.00004734
Iteration 8/1000 | Loss: 0.00004685
Iteration 9/1000 | Loss: 0.00004648
Iteration 10/1000 | Loss: 0.00004613
Iteration 11/1000 | Loss: 0.00004593
Iteration 12/1000 | Loss: 0.00004583
Iteration 13/1000 | Loss: 0.00004565
Iteration 14/1000 | Loss: 0.00004558
Iteration 15/1000 | Loss: 0.00004558
Iteration 16/1000 | Loss: 0.00004558
Iteration 17/1000 | Loss: 0.00004544
Iteration 18/1000 | Loss: 0.00004544
Iteration 19/1000 | Loss: 0.00004544
Iteration 20/1000 | Loss: 0.00004543
Iteration 21/1000 | Loss: 0.00004543
Iteration 22/1000 | Loss: 0.00004543
Iteration 23/1000 | Loss: 0.00004543
Iteration 24/1000 | Loss: 0.00004543
Iteration 25/1000 | Loss: 0.00004543
Iteration 26/1000 | Loss: 0.00004542
Iteration 27/1000 | Loss: 0.00004535
Iteration 28/1000 | Loss: 0.00004535
Iteration 29/1000 | Loss: 0.00004535
Iteration 30/1000 | Loss: 0.00004534
Iteration 31/1000 | Loss: 0.00004534
Iteration 32/1000 | Loss: 0.00004534
Iteration 33/1000 | Loss: 0.00004534
Iteration 34/1000 | Loss: 0.00004534
Iteration 35/1000 | Loss: 0.00004534
Iteration 36/1000 | Loss: 0.00004534
Iteration 37/1000 | Loss: 0.00004534
Iteration 38/1000 | Loss: 0.00004534
Iteration 39/1000 | Loss: 0.00004533
Iteration 40/1000 | Loss: 0.00004530
Iteration 41/1000 | Loss: 0.00004530
Iteration 42/1000 | Loss: 0.00004530
Iteration 43/1000 | Loss: 0.00004530
Iteration 44/1000 | Loss: 0.00004530
Iteration 45/1000 | Loss: 0.00004530
Iteration 46/1000 | Loss: 0.00004530
Iteration 47/1000 | Loss: 0.00004530
Iteration 48/1000 | Loss: 0.00004530
Iteration 49/1000 | Loss: 0.00004529
Iteration 50/1000 | Loss: 0.00004529
Iteration 51/1000 | Loss: 0.00004529
Iteration 52/1000 | Loss: 0.00004529
Iteration 53/1000 | Loss: 0.00004529
Iteration 54/1000 | Loss: 0.00004529
Iteration 55/1000 | Loss: 0.00004529
Iteration 56/1000 | Loss: 0.00004528
Iteration 57/1000 | Loss: 0.00004527
Iteration 58/1000 | Loss: 0.00004525
Iteration 59/1000 | Loss: 0.00004525
Iteration 60/1000 | Loss: 0.00004524
Iteration 61/1000 | Loss: 0.00004524
Iteration 62/1000 | Loss: 0.00004523
Iteration 63/1000 | Loss: 0.00004521
Iteration 64/1000 | Loss: 0.00004520
Iteration 65/1000 | Loss: 0.00004520
Iteration 66/1000 | Loss: 0.00004520
Iteration 67/1000 | Loss: 0.00004520
Iteration 68/1000 | Loss: 0.00004520
Iteration 69/1000 | Loss: 0.00004520
Iteration 70/1000 | Loss: 0.00004519
Iteration 71/1000 | Loss: 0.00004519
Iteration 72/1000 | Loss: 0.00004519
Iteration 73/1000 | Loss: 0.00004518
Iteration 74/1000 | Loss: 0.00004511
Iteration 75/1000 | Loss: 0.00004511
Iteration 76/1000 | Loss: 0.00004511
Iteration 77/1000 | Loss: 0.00004511
Iteration 78/1000 | Loss: 0.00004510
Iteration 79/1000 | Loss: 0.00004510
Iteration 80/1000 | Loss: 0.00004510
Iteration 81/1000 | Loss: 0.00004510
Iteration 82/1000 | Loss: 0.00004507
Iteration 83/1000 | Loss: 0.00004505
Iteration 84/1000 | Loss: 0.00004505
Iteration 85/1000 | Loss: 0.00004505
Iteration 86/1000 | Loss: 0.00004505
Iteration 87/1000 | Loss: 0.00004505
Iteration 88/1000 | Loss: 0.00004505
Iteration 89/1000 | Loss: 0.00004504
Iteration 90/1000 | Loss: 0.00004502
Iteration 91/1000 | Loss: 0.00004501
Iteration 92/1000 | Loss: 0.00004501
Iteration 93/1000 | Loss: 0.00004501
Iteration 94/1000 | Loss: 0.00004501
Iteration 95/1000 | Loss: 0.00004501
Iteration 96/1000 | Loss: 0.00004501
Iteration 97/1000 | Loss: 0.00004501
Iteration 98/1000 | Loss: 0.00004501
Iteration 99/1000 | Loss: 0.00004501
Iteration 100/1000 | Loss: 0.00004501
Iteration 101/1000 | Loss: 0.00004500
Iteration 102/1000 | Loss: 0.00004500
Iteration 103/1000 | Loss: 0.00004500
Iteration 104/1000 | Loss: 0.00004500
Iteration 105/1000 | Loss: 0.00004499
Iteration 106/1000 | Loss: 0.00004499
Iteration 107/1000 | Loss: 0.00004499
Iteration 108/1000 | Loss: 0.00004499
Iteration 109/1000 | Loss: 0.00004499
Iteration 110/1000 | Loss: 0.00004498
Iteration 111/1000 | Loss: 0.00004498
Iteration 112/1000 | Loss: 0.00004495
Iteration 113/1000 | Loss: 0.00004495
Iteration 114/1000 | Loss: 0.00004490
Iteration 115/1000 | Loss: 0.00004490
Iteration 116/1000 | Loss: 0.00004490
Iteration 117/1000 | Loss: 0.00004490
Iteration 118/1000 | Loss: 0.00004489
Iteration 119/1000 | Loss: 0.00004489
Iteration 120/1000 | Loss: 0.00004489
Iteration 121/1000 | Loss: 0.00004489
Iteration 122/1000 | Loss: 0.00004489
Iteration 123/1000 | Loss: 0.00004489
Iteration 124/1000 | Loss: 0.00004489
Iteration 125/1000 | Loss: 0.00004489
Iteration 126/1000 | Loss: 0.00004489
Iteration 127/1000 | Loss: 0.00004489
Iteration 128/1000 | Loss: 0.00004488
Iteration 129/1000 | Loss: 0.00004480
Iteration 130/1000 | Loss: 0.00004480
Iteration 131/1000 | Loss: 0.00004480
Iteration 132/1000 | Loss: 0.00004480
Iteration 133/1000 | Loss: 0.00004480
Iteration 134/1000 | Loss: 0.00004480
Iteration 135/1000 | Loss: 0.00004479
Iteration 136/1000 | Loss: 0.00004479
Iteration 137/1000 | Loss: 0.00004479
Iteration 138/1000 | Loss: 0.00004479
Iteration 139/1000 | Loss: 0.00004478
Iteration 140/1000 | Loss: 0.00004478
Iteration 141/1000 | Loss: 0.00004477
Iteration 142/1000 | Loss: 0.00004477
Iteration 143/1000 | Loss: 0.00004477
Iteration 144/1000 | Loss: 0.00004477
Iteration 145/1000 | Loss: 0.00004477
Iteration 146/1000 | Loss: 0.00004476
Iteration 147/1000 | Loss: 0.00004476
Iteration 148/1000 | Loss: 0.00004476
Iteration 149/1000 | Loss: 0.00004476
Iteration 150/1000 | Loss: 0.00004476
Iteration 151/1000 | Loss: 0.00004475
Iteration 152/1000 | Loss: 0.00004471
Iteration 153/1000 | Loss: 0.00004471
Iteration 154/1000 | Loss: 0.00004471
Iteration 155/1000 | Loss: 0.00004470
Iteration 156/1000 | Loss: 0.00004470
Iteration 157/1000 | Loss: 0.00004468
Iteration 158/1000 | Loss: 0.00004466
Iteration 159/1000 | Loss: 0.00004465
Iteration 160/1000 | Loss: 0.00004465
Iteration 161/1000 | Loss: 0.00004465
Iteration 162/1000 | Loss: 0.00004465
Iteration 163/1000 | Loss: 0.00004465
Iteration 164/1000 | Loss: 0.00004464
Iteration 165/1000 | Loss: 0.00004464
Iteration 166/1000 | Loss: 0.00004464
Iteration 167/1000 | Loss: 0.00004464
Iteration 168/1000 | Loss: 0.00004464
Iteration 169/1000 | Loss: 0.00004464
Iteration 170/1000 | Loss: 0.00004464
Iteration 171/1000 | Loss: 0.00004464
Iteration 172/1000 | Loss: 0.00004464
Iteration 173/1000 | Loss: 0.00004464
Iteration 174/1000 | Loss: 0.00004464
Iteration 175/1000 | Loss: 0.00004464
Iteration 176/1000 | Loss: 0.00004463
Iteration 177/1000 | Loss: 0.00004463
Iteration 178/1000 | Loss: 0.00004462
Iteration 179/1000 | Loss: 0.00004462
Iteration 180/1000 | Loss: 0.00004461
Iteration 181/1000 | Loss: 0.00004461
Iteration 182/1000 | Loss: 0.00004461
Iteration 183/1000 | Loss: 0.00004461
Iteration 184/1000 | Loss: 0.00004461
Iteration 185/1000 | Loss: 0.00004461
Iteration 186/1000 | Loss: 0.00004461
Iteration 187/1000 | Loss: 0.00004461
Iteration 188/1000 | Loss: 0.00004460
Iteration 189/1000 | Loss: 0.00004460
Iteration 190/1000 | Loss: 0.00004460
Iteration 191/1000 | Loss: 0.00004459
Iteration 192/1000 | Loss: 0.00004459
Iteration 193/1000 | Loss: 0.00004458
Iteration 194/1000 | Loss: 0.00004458
Iteration 195/1000 | Loss: 0.00004456
Iteration 196/1000 | Loss: 0.00004455
Iteration 197/1000 | Loss: 0.00004455
Iteration 198/1000 | Loss: 0.00004455
Iteration 199/1000 | Loss: 0.00004455
Iteration 200/1000 | Loss: 0.00004455
Iteration 201/1000 | Loss: 0.00004455
Iteration 202/1000 | Loss: 0.00004455
Iteration 203/1000 | Loss: 0.00004455
Iteration 204/1000 | Loss: 0.00004455
Iteration 205/1000 | Loss: 0.00004455
Iteration 206/1000 | Loss: 0.00004454
Iteration 207/1000 | Loss: 0.00004454
Iteration 208/1000 | Loss: 0.00004453
Iteration 209/1000 | Loss: 0.00004453
Iteration 210/1000 | Loss: 0.00004453
Iteration 211/1000 | Loss: 0.00004452
Iteration 212/1000 | Loss: 0.00004452
Iteration 213/1000 | Loss: 0.00004452
Iteration 214/1000 | Loss: 0.00004452
Iteration 215/1000 | Loss: 0.00004452
Iteration 216/1000 | Loss: 0.00004452
Iteration 217/1000 | Loss: 0.00004452
Iteration 218/1000 | Loss: 0.00004451
Iteration 219/1000 | Loss: 0.00004451
Iteration 220/1000 | Loss: 0.00004451
Iteration 221/1000 | Loss: 0.00004451
Iteration 222/1000 | Loss: 0.00004451
Iteration 223/1000 | Loss: 0.00004451
Iteration 224/1000 | Loss: 0.00004451
Iteration 225/1000 | Loss: 0.00004451
Iteration 226/1000 | Loss: 0.00004451
Iteration 227/1000 | Loss: 0.00004450
Iteration 228/1000 | Loss: 0.00004450
Iteration 229/1000 | Loss: 0.00004450
Iteration 230/1000 | Loss: 0.00004450
Iteration 231/1000 | Loss: 0.00004450
Iteration 232/1000 | Loss: 0.00004450
Iteration 233/1000 | Loss: 0.00004450
Iteration 234/1000 | Loss: 0.00004450
Iteration 235/1000 | Loss: 0.00004450
Iteration 236/1000 | Loss: 0.00004450
Iteration 237/1000 | Loss: 0.00004450
Iteration 238/1000 | Loss: 0.00004450
Iteration 239/1000 | Loss: 0.00004450
Iteration 240/1000 | Loss: 0.00004449
Iteration 241/1000 | Loss: 0.00004449
Iteration 242/1000 | Loss: 0.00004449
Iteration 243/1000 | Loss: 0.00004449
Iteration 244/1000 | Loss: 0.00004449
Iteration 245/1000 | Loss: 0.00004449
Iteration 246/1000 | Loss: 0.00004449
Iteration 247/1000 | Loss: 0.00004449
Iteration 248/1000 | Loss: 0.00004449
Iteration 249/1000 | Loss: 0.00004448
Iteration 250/1000 | Loss: 0.00004448
Iteration 251/1000 | Loss: 0.00004448
Iteration 252/1000 | Loss: 0.00004448
Iteration 253/1000 | Loss: 0.00004448
Iteration 254/1000 | Loss: 0.00004448
Iteration 255/1000 | Loss: 0.00004448
Iteration 256/1000 | Loss: 0.00004448
Iteration 257/1000 | Loss: 0.00004448
Iteration 258/1000 | Loss: 0.00004448
Iteration 259/1000 | Loss: 0.00004448
Iteration 260/1000 | Loss: 0.00004448
Iteration 261/1000 | Loss: 0.00004448
Iteration 262/1000 | Loss: 0.00004448
Iteration 263/1000 | Loss: 0.00004447
Iteration 264/1000 | Loss: 0.00004447
Iteration 265/1000 | Loss: 0.00004447
Iteration 266/1000 | Loss: 0.00004447
Iteration 267/1000 | Loss: 0.00004447
Iteration 268/1000 | Loss: 0.00004447
Iteration 269/1000 | Loss: 0.00004447
Iteration 270/1000 | Loss: 0.00004447
Iteration 271/1000 | Loss: 0.00004447
Iteration 272/1000 | Loss: 0.00004447
Iteration 273/1000 | Loss: 0.00004447
Iteration 274/1000 | Loss: 0.00004447
Iteration 275/1000 | Loss: 0.00004447
Iteration 276/1000 | Loss: 0.00004447
Iteration 277/1000 | Loss: 0.00004446
Iteration 278/1000 | Loss: 0.00004446
Iteration 279/1000 | Loss: 0.00004446
Iteration 280/1000 | Loss: 0.00004446
Iteration 281/1000 | Loss: 0.00004446
Iteration 282/1000 | Loss: 0.00004446
Iteration 283/1000 | Loss: 0.00004446
Iteration 284/1000 | Loss: 0.00004445
Iteration 285/1000 | Loss: 0.00004445
Iteration 286/1000 | Loss: 0.00004445
Iteration 287/1000 | Loss: 0.00004445
Iteration 288/1000 | Loss: 0.00004445
Iteration 289/1000 | Loss: 0.00004445
Iteration 290/1000 | Loss: 0.00004445
Iteration 291/1000 | Loss: 0.00004445
Iteration 292/1000 | Loss: 0.00004445
Iteration 293/1000 | Loss: 0.00004445
Iteration 294/1000 | Loss: 0.00004445
Iteration 295/1000 | Loss: 0.00004445
Iteration 296/1000 | Loss: 0.00004445
Iteration 297/1000 | Loss: 0.00004445
Iteration 298/1000 | Loss: 0.00004445
Iteration 299/1000 | Loss: 0.00004445
Iteration 300/1000 | Loss: 0.00004445
Iteration 301/1000 | Loss: 0.00004445
Iteration 302/1000 | Loss: 0.00004445
Iteration 303/1000 | Loss: 0.00004445
Iteration 304/1000 | Loss: 0.00004445
Iteration 305/1000 | Loss: 0.00004445
Iteration 306/1000 | Loss: 0.00004445
Iteration 307/1000 | Loss: 0.00004445
Iteration 308/1000 | Loss: 0.00004445
Iteration 309/1000 | Loss: 0.00004445
Iteration 310/1000 | Loss: 0.00004445
Iteration 311/1000 | Loss: 0.00004445
Iteration 312/1000 | Loss: 0.00004445
Iteration 313/1000 | Loss: 0.00004445
Iteration 314/1000 | Loss: 0.00004445
Iteration 315/1000 | Loss: 0.00004444
Iteration 316/1000 | Loss: 0.00004444
Iteration 317/1000 | Loss: 0.00004444
Iteration 318/1000 | Loss: 0.00004444
Iteration 319/1000 | Loss: 0.00004444
Iteration 320/1000 | Loss: 0.00004444
Iteration 321/1000 | Loss: 0.00004444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [4.4444925151765347e-05, 4.4444925151765347e-05, 4.4444925151765347e-05, 4.4444925151765347e-05, 4.4444925151765347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4444925151765347e-05

Optimization complete. Final v2v error: 5.6639275550842285 mm

Highest mean error: 5.780085563659668 mm for frame 142

Lowest mean error: 5.58031702041626 mm for frame 15

Saving results

Total time: 55.81843686103821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067377
Iteration 2/25 | Loss: 0.00248162
Iteration 3/25 | Loss: 0.00185886
Iteration 4/25 | Loss: 0.00169063
Iteration 5/25 | Loss: 0.00166419
Iteration 6/25 | Loss: 0.00154054
Iteration 7/25 | Loss: 0.00148529
Iteration 8/25 | Loss: 0.00146359
Iteration 9/25 | Loss: 0.00145778
Iteration 10/25 | Loss: 0.00145536
Iteration 11/25 | Loss: 0.00145390
Iteration 12/25 | Loss: 0.00145262
Iteration 13/25 | Loss: 0.00145208
Iteration 14/25 | Loss: 0.00145168
Iteration 15/25 | Loss: 0.00145135
Iteration 16/25 | Loss: 0.00145113
Iteration 17/25 | Loss: 0.00145106
Iteration 18/25 | Loss: 0.00145106
Iteration 19/25 | Loss: 0.00145106
Iteration 20/25 | Loss: 0.00145106
Iteration 21/25 | Loss: 0.00145106
Iteration 22/25 | Loss: 0.00145106
Iteration 23/25 | Loss: 0.00145106
Iteration 24/25 | Loss: 0.00145106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014510577311739326, 0.0014510577311739326, 0.0014510577311739326, 0.0014510577311739326, 0.0014510577311739326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014510577311739326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15834153
Iteration 2/25 | Loss: 0.00219398
Iteration 3/25 | Loss: 0.00219398
Iteration 4/25 | Loss: 0.00219398
Iteration 5/25 | Loss: 0.00219398
Iteration 6/25 | Loss: 0.00219398
Iteration 7/25 | Loss: 0.00219398
Iteration 8/25 | Loss: 0.00219398
Iteration 9/25 | Loss: 0.00219398
Iteration 10/25 | Loss: 0.00219398
Iteration 11/25 | Loss: 0.00219398
Iteration 12/25 | Loss: 0.00219398
Iteration 13/25 | Loss: 0.00219398
Iteration 14/25 | Loss: 0.00219398
Iteration 15/25 | Loss: 0.00219398
Iteration 16/25 | Loss: 0.00219398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021939806174486876, 0.0021939806174486876, 0.0021939806174486876, 0.0021939806174486876, 0.0021939806174486876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021939806174486876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219398
Iteration 2/1000 | Loss: 0.00007367
Iteration 3/1000 | Loss: 0.00005069
Iteration 4/1000 | Loss: 0.00004629
Iteration 5/1000 | Loss: 0.00004283
Iteration 6/1000 | Loss: 0.00004068
Iteration 7/1000 | Loss: 0.00003955
Iteration 8/1000 | Loss: 0.00003860
Iteration 9/1000 | Loss: 0.00003799
Iteration 10/1000 | Loss: 0.00003743
Iteration 11/1000 | Loss: 0.00003705
Iteration 12/1000 | Loss: 0.00003683
Iteration 13/1000 | Loss: 0.00003668
Iteration 14/1000 | Loss: 0.00003652
Iteration 15/1000 | Loss: 0.00003644
Iteration 16/1000 | Loss: 0.00003644
Iteration 17/1000 | Loss: 0.00003643
Iteration 18/1000 | Loss: 0.00003642
Iteration 19/1000 | Loss: 0.00003642
Iteration 20/1000 | Loss: 0.00003642
Iteration 21/1000 | Loss: 0.00003642
Iteration 22/1000 | Loss: 0.00003642
Iteration 23/1000 | Loss: 0.00003642
Iteration 24/1000 | Loss: 0.00003642
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003642
Iteration 27/1000 | Loss: 0.00003641
Iteration 28/1000 | Loss: 0.00003641
Iteration 29/1000 | Loss: 0.00003641
Iteration 30/1000 | Loss: 0.00003641
Iteration 31/1000 | Loss: 0.00003641
Iteration 32/1000 | Loss: 0.00003641
Iteration 33/1000 | Loss: 0.00003641
Iteration 34/1000 | Loss: 0.00003640
Iteration 35/1000 | Loss: 0.00003640
Iteration 36/1000 | Loss: 0.00003638
Iteration 37/1000 | Loss: 0.00003637
Iteration 38/1000 | Loss: 0.00003637
Iteration 39/1000 | Loss: 0.00003637
Iteration 40/1000 | Loss: 0.00003637
Iteration 41/1000 | Loss: 0.00003637
Iteration 42/1000 | Loss: 0.00003637
Iteration 43/1000 | Loss: 0.00003637
Iteration 44/1000 | Loss: 0.00003636
Iteration 45/1000 | Loss: 0.00003636
Iteration 46/1000 | Loss: 0.00003635
Iteration 47/1000 | Loss: 0.00003633
Iteration 48/1000 | Loss: 0.00003633
Iteration 49/1000 | Loss: 0.00003633
Iteration 50/1000 | Loss: 0.00003633
Iteration 51/1000 | Loss: 0.00003633
Iteration 52/1000 | Loss: 0.00003632
Iteration 53/1000 | Loss: 0.00003632
Iteration 54/1000 | Loss: 0.00003632
Iteration 55/1000 | Loss: 0.00003632
Iteration 56/1000 | Loss: 0.00003632
Iteration 57/1000 | Loss: 0.00003632
Iteration 58/1000 | Loss: 0.00003632
Iteration 59/1000 | Loss: 0.00003632
Iteration 60/1000 | Loss: 0.00003632
Iteration 61/1000 | Loss: 0.00003632
Iteration 62/1000 | Loss: 0.00003632
Iteration 63/1000 | Loss: 0.00003632
Iteration 64/1000 | Loss: 0.00003632
Iteration 65/1000 | Loss: 0.00003632
Iteration 66/1000 | Loss: 0.00003632
Iteration 67/1000 | Loss: 0.00003632
Iteration 68/1000 | Loss: 0.00003632
Iteration 69/1000 | Loss: 0.00003632
Iteration 70/1000 | Loss: 0.00003632
Iteration 71/1000 | Loss: 0.00003632
Iteration 72/1000 | Loss: 0.00003632
Iteration 73/1000 | Loss: 0.00003632
Iteration 74/1000 | Loss: 0.00003632
Iteration 75/1000 | Loss: 0.00003632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [3.6322613595984876e-05, 3.6322613595984876e-05, 3.6322613595984876e-05, 3.6322613595984876e-05, 3.6322613595984876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6322613595984876e-05

Optimization complete. Final v2v error: 4.829443454742432 mm

Highest mean error: 10.600101470947266 mm for frame 237

Lowest mean error: 4.23148250579834 mm for frame 234

Saving results

Total time: 59.20452928543091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973525
Iteration 2/25 | Loss: 0.00197167
Iteration 3/25 | Loss: 0.00187734
Iteration 4/25 | Loss: 0.00186825
Iteration 5/25 | Loss: 0.00186492
Iteration 6/25 | Loss: 0.00186492
Iteration 7/25 | Loss: 0.00186492
Iteration 8/25 | Loss: 0.00186492
Iteration 9/25 | Loss: 0.00186492
Iteration 10/25 | Loss: 0.00186492
Iteration 11/25 | Loss: 0.00186492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018649152480065823, 0.0018649152480065823, 0.0018649152480065823, 0.0018649152480065823, 0.0018649152480065823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018649152480065823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39948535
Iteration 2/25 | Loss: 0.00253149
Iteration 3/25 | Loss: 0.00253149
Iteration 4/25 | Loss: 0.00253149
Iteration 5/25 | Loss: 0.00253149
Iteration 6/25 | Loss: 0.00253149
Iteration 7/25 | Loss: 0.00253149
Iteration 8/25 | Loss: 0.00253149
Iteration 9/25 | Loss: 0.00253149
Iteration 10/25 | Loss: 0.00253149
Iteration 11/25 | Loss: 0.00253149
Iteration 12/25 | Loss: 0.00253149
Iteration 13/25 | Loss: 0.00253149
Iteration 14/25 | Loss: 0.00253149
Iteration 15/25 | Loss: 0.00253149
Iteration 16/25 | Loss: 0.00253149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025314853992313147, 0.0025314853992313147, 0.0025314853992313147, 0.0025314853992313147, 0.0025314853992313147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025314853992313147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253149
Iteration 2/1000 | Loss: 0.00005312
Iteration 3/1000 | Loss: 0.00003683
Iteration 4/1000 | Loss: 0.00003154
Iteration 5/1000 | Loss: 0.00002970
Iteration 6/1000 | Loss: 0.00002813
Iteration 7/1000 | Loss: 0.00002728
Iteration 8/1000 | Loss: 0.00002658
Iteration 9/1000 | Loss: 0.00002610
Iteration 10/1000 | Loss: 0.00002579
Iteration 11/1000 | Loss: 0.00002549
Iteration 12/1000 | Loss: 0.00002522
Iteration 13/1000 | Loss: 0.00002507
Iteration 14/1000 | Loss: 0.00002502
Iteration 15/1000 | Loss: 0.00002499
Iteration 16/1000 | Loss: 0.00002499
Iteration 17/1000 | Loss: 0.00002499
Iteration 18/1000 | Loss: 0.00002495
Iteration 19/1000 | Loss: 0.00002495
Iteration 20/1000 | Loss: 0.00002495
Iteration 21/1000 | Loss: 0.00002494
Iteration 22/1000 | Loss: 0.00002494
Iteration 23/1000 | Loss: 0.00002494
Iteration 24/1000 | Loss: 0.00002494
Iteration 25/1000 | Loss: 0.00002494
Iteration 26/1000 | Loss: 0.00002494
Iteration 27/1000 | Loss: 0.00002494
Iteration 28/1000 | Loss: 0.00002494
Iteration 29/1000 | Loss: 0.00002493
Iteration 30/1000 | Loss: 0.00002491
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00002491
Iteration 33/1000 | Loss: 0.00002491
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002490
Iteration 36/1000 | Loss: 0.00002489
Iteration 37/1000 | Loss: 0.00002489
Iteration 38/1000 | Loss: 0.00002488
Iteration 39/1000 | Loss: 0.00002488
Iteration 40/1000 | Loss: 0.00002488
Iteration 41/1000 | Loss: 0.00002488
Iteration 42/1000 | Loss: 0.00002488
Iteration 43/1000 | Loss: 0.00002488
Iteration 44/1000 | Loss: 0.00002488
Iteration 45/1000 | Loss: 0.00002488
Iteration 46/1000 | Loss: 0.00002488
Iteration 47/1000 | Loss: 0.00002488
Iteration 48/1000 | Loss: 0.00002488
Iteration 49/1000 | Loss: 0.00002488
Iteration 50/1000 | Loss: 0.00002487
Iteration 51/1000 | Loss: 0.00002487
Iteration 52/1000 | Loss: 0.00002487
Iteration 53/1000 | Loss: 0.00002486
Iteration 54/1000 | Loss: 0.00002486
Iteration 55/1000 | Loss: 0.00002486
Iteration 56/1000 | Loss: 0.00002486
Iteration 57/1000 | Loss: 0.00002486
Iteration 58/1000 | Loss: 0.00002486
Iteration 59/1000 | Loss: 0.00002486
Iteration 60/1000 | Loss: 0.00002486
Iteration 61/1000 | Loss: 0.00002486
Iteration 62/1000 | Loss: 0.00002486
Iteration 63/1000 | Loss: 0.00002485
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002485
Iteration 67/1000 | Loss: 0.00002485
Iteration 68/1000 | Loss: 0.00002485
Iteration 69/1000 | Loss: 0.00002485
Iteration 70/1000 | Loss: 0.00002485
Iteration 71/1000 | Loss: 0.00002485
Iteration 72/1000 | Loss: 0.00002484
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002484
Iteration 80/1000 | Loss: 0.00002484
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002484
Iteration 83/1000 | Loss: 0.00002484
Iteration 84/1000 | Loss: 0.00002484
Iteration 85/1000 | Loss: 0.00002484
Iteration 86/1000 | Loss: 0.00002484
Iteration 87/1000 | Loss: 0.00002484
Iteration 88/1000 | Loss: 0.00002484
Iteration 89/1000 | Loss: 0.00002484
Iteration 90/1000 | Loss: 0.00002484
Iteration 91/1000 | Loss: 0.00002484
Iteration 92/1000 | Loss: 0.00002484
Iteration 93/1000 | Loss: 0.00002484
Iteration 94/1000 | Loss: 0.00002484
Iteration 95/1000 | Loss: 0.00002484
Iteration 96/1000 | Loss: 0.00002484
Iteration 97/1000 | Loss: 0.00002484
Iteration 98/1000 | Loss: 0.00002484
Iteration 99/1000 | Loss: 0.00002484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.4838927856762893e-05, 2.4838927856762893e-05, 2.4838927856762893e-05, 2.4838927856762893e-05, 2.4838927856762893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4838927856762893e-05

Optimization complete. Final v2v error: 4.290735721588135 mm

Highest mean error: 4.618471622467041 mm for frame 88

Lowest mean error: 4.080901145935059 mm for frame 42

Saving results

Total time: 37.325321435928345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121321
Iteration 2/25 | Loss: 0.00200290
Iteration 3/25 | Loss: 0.00184699
Iteration 4/25 | Loss: 0.00183126
Iteration 5/25 | Loss: 0.00182705
Iteration 6/25 | Loss: 0.00182473
Iteration 7/25 | Loss: 0.00182395
Iteration 8/25 | Loss: 0.00183173
Iteration 9/25 | Loss: 0.00182838
Iteration 10/25 | Loss: 0.00182454
Iteration 11/25 | Loss: 0.00182814
Iteration 12/25 | Loss: 0.00182410
Iteration 13/25 | Loss: 0.00182402
Iteration 14/25 | Loss: 0.00182280
Iteration 15/25 | Loss: 0.00182262
Iteration 16/25 | Loss: 0.00182248
Iteration 17/25 | Loss: 0.00182239
Iteration 18/25 | Loss: 0.00182238
Iteration 19/25 | Loss: 0.00182238
Iteration 20/25 | Loss: 0.00182238
Iteration 21/25 | Loss: 0.00182238
Iteration 22/25 | Loss: 0.00182238
Iteration 23/25 | Loss: 0.00182238
Iteration 24/25 | Loss: 0.00182237
Iteration 25/25 | Loss: 0.00182237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.81793785
Iteration 2/25 | Loss: 0.00232596
Iteration 3/25 | Loss: 0.00232595
Iteration 4/25 | Loss: 0.00232595
Iteration 5/25 | Loss: 0.00232595
Iteration 6/25 | Loss: 0.00232595
Iteration 7/25 | Loss: 0.00232595
Iteration 8/25 | Loss: 0.00232595
Iteration 9/25 | Loss: 0.00232595
Iteration 10/25 | Loss: 0.00232595
Iteration 11/25 | Loss: 0.00232595
Iteration 12/25 | Loss: 0.00232595
Iteration 13/25 | Loss: 0.00232595
Iteration 14/25 | Loss: 0.00232595
Iteration 15/25 | Loss: 0.00232595
Iteration 16/25 | Loss: 0.00232595
Iteration 17/25 | Loss: 0.00232595
Iteration 18/25 | Loss: 0.00232595
Iteration 19/25 | Loss: 0.00232595
Iteration 20/25 | Loss: 0.00232595
Iteration 21/25 | Loss: 0.00232595
Iteration 22/25 | Loss: 0.00232595
Iteration 23/25 | Loss: 0.00232595
Iteration 24/25 | Loss: 0.00232595
Iteration 25/25 | Loss: 0.00232595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232595
Iteration 2/1000 | Loss: 0.00006291
Iteration 3/1000 | Loss: 0.00004480
Iteration 4/1000 | Loss: 0.00004016
Iteration 5/1000 | Loss: 0.00003726
Iteration 6/1000 | Loss: 0.00003513
Iteration 7/1000 | Loss: 0.00003415
Iteration 8/1000 | Loss: 0.00003326
Iteration 9/1000 | Loss: 0.00003263
Iteration 10/1000 | Loss: 0.00003222
Iteration 11/1000 | Loss: 0.00003189
Iteration 12/1000 | Loss: 0.00003161
Iteration 13/1000 | Loss: 0.00003142
Iteration 14/1000 | Loss: 0.00003135
Iteration 15/1000 | Loss: 0.00003134
Iteration 16/1000 | Loss: 0.00003131
Iteration 17/1000 | Loss: 0.00003131
Iteration 18/1000 | Loss: 0.00003130
Iteration 19/1000 | Loss: 0.00003124
Iteration 20/1000 | Loss: 0.00003122
Iteration 21/1000 | Loss: 0.00003122
Iteration 22/1000 | Loss: 0.00003122
Iteration 23/1000 | Loss: 0.00003121
Iteration 24/1000 | Loss: 0.00003121
Iteration 25/1000 | Loss: 0.00003115
Iteration 26/1000 | Loss: 0.00003114
Iteration 27/1000 | Loss: 0.00003113
Iteration 28/1000 | Loss: 0.00003113
Iteration 29/1000 | Loss: 0.00003113
Iteration 30/1000 | Loss: 0.00003112
Iteration 31/1000 | Loss: 0.00003112
Iteration 32/1000 | Loss: 0.00003112
Iteration 33/1000 | Loss: 0.00003111
Iteration 34/1000 | Loss: 0.00003111
Iteration 35/1000 | Loss: 0.00003110
Iteration 36/1000 | Loss: 0.00003110
Iteration 37/1000 | Loss: 0.00003110
Iteration 38/1000 | Loss: 0.00003110
Iteration 39/1000 | Loss: 0.00003110
Iteration 40/1000 | Loss: 0.00003110
Iteration 41/1000 | Loss: 0.00003109
Iteration 42/1000 | Loss: 0.00003109
Iteration 43/1000 | Loss: 0.00003109
Iteration 44/1000 | Loss: 0.00003108
Iteration 45/1000 | Loss: 0.00003108
Iteration 46/1000 | Loss: 0.00003108
Iteration 47/1000 | Loss: 0.00003108
Iteration 48/1000 | Loss: 0.00003108
Iteration 49/1000 | Loss: 0.00003108
Iteration 50/1000 | Loss: 0.00003108
Iteration 51/1000 | Loss: 0.00003107
Iteration 52/1000 | Loss: 0.00003107
Iteration 53/1000 | Loss: 0.00003107
Iteration 54/1000 | Loss: 0.00003107
Iteration 55/1000 | Loss: 0.00003107
Iteration 56/1000 | Loss: 0.00003106
Iteration 57/1000 | Loss: 0.00003106
Iteration 58/1000 | Loss: 0.00003106
Iteration 59/1000 | Loss: 0.00003106
Iteration 60/1000 | Loss: 0.00003105
Iteration 61/1000 | Loss: 0.00003105
Iteration 62/1000 | Loss: 0.00003105
Iteration 63/1000 | Loss: 0.00003105
Iteration 64/1000 | Loss: 0.00003105
Iteration 65/1000 | Loss: 0.00003105
Iteration 66/1000 | Loss: 0.00003104
Iteration 67/1000 | Loss: 0.00003104
Iteration 68/1000 | Loss: 0.00003104
Iteration 69/1000 | Loss: 0.00003104
Iteration 70/1000 | Loss: 0.00003104
Iteration 71/1000 | Loss: 0.00003104
Iteration 72/1000 | Loss: 0.00003104
Iteration 73/1000 | Loss: 0.00003104
Iteration 74/1000 | Loss: 0.00003104
Iteration 75/1000 | Loss: 0.00003104
Iteration 76/1000 | Loss: 0.00003103
Iteration 77/1000 | Loss: 0.00003103
Iteration 78/1000 | Loss: 0.00003103
Iteration 79/1000 | Loss: 0.00003103
Iteration 80/1000 | Loss: 0.00003103
Iteration 81/1000 | Loss: 0.00003103
Iteration 82/1000 | Loss: 0.00003103
Iteration 83/1000 | Loss: 0.00003103
Iteration 84/1000 | Loss: 0.00003102
Iteration 85/1000 | Loss: 0.00003102
Iteration 86/1000 | Loss: 0.00003102
Iteration 87/1000 | Loss: 0.00003102
Iteration 88/1000 | Loss: 0.00003102
Iteration 89/1000 | Loss: 0.00003102
Iteration 90/1000 | Loss: 0.00003102
Iteration 91/1000 | Loss: 0.00003101
Iteration 92/1000 | Loss: 0.00003101
Iteration 93/1000 | Loss: 0.00003101
Iteration 94/1000 | Loss: 0.00003101
Iteration 95/1000 | Loss: 0.00003101
Iteration 96/1000 | Loss: 0.00003101
Iteration 97/1000 | Loss: 0.00003101
Iteration 98/1000 | Loss: 0.00003101
Iteration 99/1000 | Loss: 0.00003101
Iteration 100/1000 | Loss: 0.00003101
Iteration 101/1000 | Loss: 0.00003101
Iteration 102/1000 | Loss: 0.00003101
Iteration 103/1000 | Loss: 0.00003101
Iteration 104/1000 | Loss: 0.00003101
Iteration 105/1000 | Loss: 0.00003101
Iteration 106/1000 | Loss: 0.00003100
Iteration 107/1000 | Loss: 0.00003100
Iteration 108/1000 | Loss: 0.00003100
Iteration 109/1000 | Loss: 0.00003100
Iteration 110/1000 | Loss: 0.00003100
Iteration 111/1000 | Loss: 0.00003100
Iteration 112/1000 | Loss: 0.00003100
Iteration 113/1000 | Loss: 0.00003100
Iteration 114/1000 | Loss: 0.00003100
Iteration 115/1000 | Loss: 0.00003099
Iteration 116/1000 | Loss: 0.00003099
Iteration 117/1000 | Loss: 0.00003099
Iteration 118/1000 | Loss: 0.00003099
Iteration 119/1000 | Loss: 0.00003099
Iteration 120/1000 | Loss: 0.00003099
Iteration 121/1000 | Loss: 0.00003099
Iteration 122/1000 | Loss: 0.00003099
Iteration 123/1000 | Loss: 0.00003099
Iteration 124/1000 | Loss: 0.00003099
Iteration 125/1000 | Loss: 0.00003099
Iteration 126/1000 | Loss: 0.00003099
Iteration 127/1000 | Loss: 0.00003099
Iteration 128/1000 | Loss: 0.00003099
Iteration 129/1000 | Loss: 0.00003099
Iteration 130/1000 | Loss: 0.00003099
Iteration 131/1000 | Loss: 0.00003099
Iteration 132/1000 | Loss: 0.00003099
Iteration 133/1000 | Loss: 0.00003098
Iteration 134/1000 | Loss: 0.00003098
Iteration 135/1000 | Loss: 0.00003098
Iteration 136/1000 | Loss: 0.00003098
Iteration 137/1000 | Loss: 0.00003098
Iteration 138/1000 | Loss: 0.00003098
Iteration 139/1000 | Loss: 0.00003098
Iteration 140/1000 | Loss: 0.00003098
Iteration 141/1000 | Loss: 0.00003098
Iteration 142/1000 | Loss: 0.00003098
Iteration 143/1000 | Loss: 0.00003098
Iteration 144/1000 | Loss: 0.00003098
Iteration 145/1000 | Loss: 0.00003098
Iteration 146/1000 | Loss: 0.00003098
Iteration 147/1000 | Loss: 0.00003098
Iteration 148/1000 | Loss: 0.00003098
Iteration 149/1000 | Loss: 0.00003098
Iteration 150/1000 | Loss: 0.00003098
Iteration 151/1000 | Loss: 0.00003098
Iteration 152/1000 | Loss: 0.00003098
Iteration 153/1000 | Loss: 0.00003098
Iteration 154/1000 | Loss: 0.00003098
Iteration 155/1000 | Loss: 0.00003098
Iteration 156/1000 | Loss: 0.00003098
Iteration 157/1000 | Loss: 0.00003098
Iteration 158/1000 | Loss: 0.00003098
Iteration 159/1000 | Loss: 0.00003098
Iteration 160/1000 | Loss: 0.00003098
Iteration 161/1000 | Loss: 0.00003098
Iteration 162/1000 | Loss: 0.00003098
Iteration 163/1000 | Loss: 0.00003098
Iteration 164/1000 | Loss: 0.00003098
Iteration 165/1000 | Loss: 0.00003098
Iteration 166/1000 | Loss: 0.00003098
Iteration 167/1000 | Loss: 0.00003098
Iteration 168/1000 | Loss: 0.00003098
Iteration 169/1000 | Loss: 0.00003098
Iteration 170/1000 | Loss: 0.00003098
Iteration 171/1000 | Loss: 0.00003098
Iteration 172/1000 | Loss: 0.00003098
Iteration 173/1000 | Loss: 0.00003098
Iteration 174/1000 | Loss: 0.00003098
Iteration 175/1000 | Loss: 0.00003098
Iteration 176/1000 | Loss: 0.00003098
Iteration 177/1000 | Loss: 0.00003098
Iteration 178/1000 | Loss: 0.00003098
Iteration 179/1000 | Loss: 0.00003098
Iteration 180/1000 | Loss: 0.00003098
Iteration 181/1000 | Loss: 0.00003098
Iteration 182/1000 | Loss: 0.00003098
Iteration 183/1000 | Loss: 0.00003098
Iteration 184/1000 | Loss: 0.00003098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.0976625566836447e-05, 3.0976625566836447e-05, 3.0976625566836447e-05, 3.0976625566836447e-05, 3.0976625566836447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0976625566836447e-05

Optimization complete. Final v2v error: 4.64456844329834 mm

Highest mean error: 10.211140632629395 mm for frame 145

Lowest mean error: 4.115190029144287 mm for frame 136

Saving results

Total time: 60.65185832977295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01186097
Iteration 2/25 | Loss: 0.01186097
Iteration 3/25 | Loss: 0.00742384
Iteration 4/25 | Loss: 0.00435803
Iteration 5/25 | Loss: 0.00338869
Iteration 6/25 | Loss: 0.00328946
Iteration 7/25 | Loss: 0.00317865
Iteration 8/25 | Loss: 0.00287927
Iteration 9/25 | Loss: 0.00277161
Iteration 10/25 | Loss: 0.00275213
Iteration 11/25 | Loss: 0.00271987
Iteration 12/25 | Loss: 0.00270326
Iteration 13/25 | Loss: 0.00269893
Iteration 14/25 | Loss: 0.00268954
Iteration 15/25 | Loss: 0.00267075
Iteration 16/25 | Loss: 0.00266997
Iteration 17/25 | Loss: 0.00265960
Iteration 18/25 | Loss: 0.00264987
Iteration 19/25 | Loss: 0.00265076
Iteration 20/25 | Loss: 0.00264680
Iteration 21/25 | Loss: 0.00264930
Iteration 22/25 | Loss: 0.00264603
Iteration 23/25 | Loss: 0.00264266
Iteration 24/25 | Loss: 0.00263878
Iteration 25/25 | Loss: 0.00263627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.37001669
Iteration 2/25 | Loss: 0.00823554
Iteration 3/25 | Loss: 0.00819153
Iteration 4/25 | Loss: 0.00819153
Iteration 5/25 | Loss: 0.00819153
Iteration 6/25 | Loss: 0.00819153
Iteration 7/25 | Loss: 0.00819153
Iteration 8/25 | Loss: 0.00819153
Iteration 9/25 | Loss: 0.00819153
Iteration 10/25 | Loss: 0.00819153
Iteration 11/25 | Loss: 0.00819153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.008191525004804134, 0.008191525004804134, 0.008191525004804134, 0.008191525004804134, 0.008191525004804134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008191525004804134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00819153
Iteration 2/1000 | Loss: 0.00131188
Iteration 3/1000 | Loss: 0.00087709
Iteration 4/1000 | Loss: 0.00088782
Iteration 5/1000 | Loss: 0.00094990
Iteration 6/1000 | Loss: 0.00073281
Iteration 7/1000 | Loss: 0.00182414
Iteration 8/1000 | Loss: 0.00126652
Iteration 9/1000 | Loss: 0.00121578
Iteration 10/1000 | Loss: 0.00058293
Iteration 11/1000 | Loss: 0.00078356
Iteration 12/1000 | Loss: 0.00063174
Iteration 13/1000 | Loss: 0.00095160
Iteration 14/1000 | Loss: 0.00056319
Iteration 15/1000 | Loss: 0.00055393
Iteration 16/1000 | Loss: 0.00103136
Iteration 17/1000 | Loss: 0.00078160
Iteration 18/1000 | Loss: 0.00055698
Iteration 19/1000 | Loss: 0.00052564
Iteration 20/1000 | Loss: 0.00067107
Iteration 21/1000 | Loss: 0.00076515
Iteration 22/1000 | Loss: 0.00069831
Iteration 23/1000 | Loss: 0.00074958
Iteration 24/1000 | Loss: 0.00098685
Iteration 25/1000 | Loss: 0.00076086
Iteration 26/1000 | Loss: 0.00160705
Iteration 27/1000 | Loss: 0.00122984
Iteration 28/1000 | Loss: 0.00068465
Iteration 29/1000 | Loss: 0.00098960
Iteration 30/1000 | Loss: 0.00080348
Iteration 31/1000 | Loss: 0.00084344
Iteration 32/1000 | Loss: 0.00055857
Iteration 33/1000 | Loss: 0.00046414
Iteration 34/1000 | Loss: 0.00061758
Iteration 35/1000 | Loss: 0.00049892
Iteration 36/1000 | Loss: 0.00042890
Iteration 37/1000 | Loss: 0.00058730
Iteration 38/1000 | Loss: 0.00096844
Iteration 39/1000 | Loss: 0.00071090
Iteration 40/1000 | Loss: 0.00069087
Iteration 41/1000 | Loss: 0.00056750
Iteration 42/1000 | Loss: 0.00042267
Iteration 43/1000 | Loss: 0.00052764
Iteration 44/1000 | Loss: 0.00050258
Iteration 45/1000 | Loss: 0.00063311
Iteration 46/1000 | Loss: 0.00057676
Iteration 47/1000 | Loss: 0.00047470
Iteration 48/1000 | Loss: 0.00050732
Iteration 49/1000 | Loss: 0.00041576
Iteration 50/1000 | Loss: 0.00066656
Iteration 51/1000 | Loss: 0.00057220
Iteration 52/1000 | Loss: 0.00045930
Iteration 53/1000 | Loss: 0.00069673
Iteration 54/1000 | Loss: 0.00066451
Iteration 55/1000 | Loss: 0.00045985
Iteration 56/1000 | Loss: 0.00042387
Iteration 57/1000 | Loss: 0.00048093
Iteration 58/1000 | Loss: 0.00045541
Iteration 59/1000 | Loss: 0.00042250
Iteration 60/1000 | Loss: 0.00049658
Iteration 61/1000 | Loss: 0.00116680
Iteration 62/1000 | Loss: 0.00052394
Iteration 63/1000 | Loss: 0.00049934
Iteration 64/1000 | Loss: 0.00043905
Iteration 65/1000 | Loss: 0.00045330
Iteration 66/1000 | Loss: 0.00040748
Iteration 67/1000 | Loss: 0.00062513
Iteration 68/1000 | Loss: 0.00128707
Iteration 69/1000 | Loss: 0.00059600
Iteration 70/1000 | Loss: 0.00041201
Iteration 71/1000 | Loss: 0.00040859
Iteration 72/1000 | Loss: 0.00043480
Iteration 73/1000 | Loss: 0.00041780
Iteration 74/1000 | Loss: 0.00041059
Iteration 75/1000 | Loss: 0.00043551
Iteration 76/1000 | Loss: 0.00040180
Iteration 77/1000 | Loss: 0.00063109
Iteration 78/1000 | Loss: 0.00070696
Iteration 79/1000 | Loss: 0.00059872
Iteration 80/1000 | Loss: 0.00059659
Iteration 81/1000 | Loss: 0.00044059
Iteration 82/1000 | Loss: 0.00040737
Iteration 83/1000 | Loss: 0.00057545
Iteration 84/1000 | Loss: 0.00050537
Iteration 85/1000 | Loss: 0.00045138
Iteration 86/1000 | Loss: 0.00042119
Iteration 87/1000 | Loss: 0.00041065
Iteration 88/1000 | Loss: 0.00043830
Iteration 89/1000 | Loss: 0.00056559
Iteration 90/1000 | Loss: 0.00040574
Iteration 91/1000 | Loss: 0.00042691
Iteration 92/1000 | Loss: 0.00071825
Iteration 93/1000 | Loss: 0.00042785
Iteration 94/1000 | Loss: 0.00041187
Iteration 95/1000 | Loss: 0.00041158
Iteration 96/1000 | Loss: 0.00039287
Iteration 97/1000 | Loss: 0.00067288
Iteration 98/1000 | Loss: 0.00048819
Iteration 99/1000 | Loss: 0.00041580
Iteration 100/1000 | Loss: 0.00038952
Iteration 101/1000 | Loss: 0.00038654
Iteration 102/1000 | Loss: 0.00040820
Iteration 103/1000 | Loss: 0.00042194
Iteration 104/1000 | Loss: 0.00039008
Iteration 105/1000 | Loss: 0.00044881
Iteration 106/1000 | Loss: 0.00038611
Iteration 107/1000 | Loss: 0.00038822
Iteration 108/1000 | Loss: 0.00038192
Iteration 109/1000 | Loss: 0.00038072
Iteration 110/1000 | Loss: 0.00063220
Iteration 111/1000 | Loss: 0.00039676
Iteration 112/1000 | Loss: 0.00040832
Iteration 113/1000 | Loss: 0.00038134
Iteration 114/1000 | Loss: 0.00039972
Iteration 115/1000 | Loss: 0.00037930
Iteration 116/1000 | Loss: 0.00037844
Iteration 117/1000 | Loss: 0.00063901
Iteration 118/1000 | Loss: 0.00039736
Iteration 119/1000 | Loss: 0.00038149
Iteration 120/1000 | Loss: 0.00037846
Iteration 121/1000 | Loss: 0.00039709
Iteration 122/1000 | Loss: 0.00037566
Iteration 123/1000 | Loss: 0.00037484
Iteration 124/1000 | Loss: 0.00065121
Iteration 125/1000 | Loss: 0.00039841
Iteration 126/1000 | Loss: 0.00037978
Iteration 127/1000 | Loss: 0.00057191
Iteration 128/1000 | Loss: 0.00043113
Iteration 129/1000 | Loss: 0.00039524
Iteration 130/1000 | Loss: 0.00039080
Iteration 131/1000 | Loss: 0.00037528
Iteration 132/1000 | Loss: 0.00037523
Iteration 133/1000 | Loss: 0.00037158
Iteration 134/1000 | Loss: 0.00037098
Iteration 135/1000 | Loss: 0.00037005
Iteration 136/1000 | Loss: 0.00064390
Iteration 137/1000 | Loss: 0.00059990
Iteration 138/1000 | Loss: 0.00048773
Iteration 139/1000 | Loss: 0.00037388
Iteration 140/1000 | Loss: 0.00037079
Iteration 141/1000 | Loss: 0.00039595
Iteration 142/1000 | Loss: 0.00038325
Iteration 143/1000 | Loss: 0.00036774
Iteration 144/1000 | Loss: 0.00036711
Iteration 145/1000 | Loss: 0.00064167
Iteration 146/1000 | Loss: 0.00038627
Iteration 147/1000 | Loss: 0.00037098
Iteration 148/1000 | Loss: 0.00036794
Iteration 149/1000 | Loss: 0.00036651
Iteration 150/1000 | Loss: 0.00036553
Iteration 151/1000 | Loss: 0.00036493
Iteration 152/1000 | Loss: 0.00064588
Iteration 153/1000 | Loss: 0.00122426
Iteration 154/1000 | Loss: 0.00041816
Iteration 155/1000 | Loss: 0.00121212
Iteration 156/1000 | Loss: 0.00382818
Iteration 157/1000 | Loss: 0.00130984
Iteration 158/1000 | Loss: 0.00083474
Iteration 159/1000 | Loss: 0.00082396
Iteration 160/1000 | Loss: 0.00079190
Iteration 161/1000 | Loss: 0.00055980
Iteration 162/1000 | Loss: 0.00039857
Iteration 163/1000 | Loss: 0.00037321
Iteration 164/1000 | Loss: 0.00109430
Iteration 165/1000 | Loss: 0.00083316
Iteration 166/1000 | Loss: 0.00100718
Iteration 167/1000 | Loss: 0.00095729
Iteration 168/1000 | Loss: 0.00038914
Iteration 169/1000 | Loss: 0.00037635
Iteration 170/1000 | Loss: 0.00035190
Iteration 171/1000 | Loss: 0.00071663
Iteration 172/1000 | Loss: 0.00035727
Iteration 173/1000 | Loss: 0.00034068
Iteration 174/1000 | Loss: 0.00060292
Iteration 175/1000 | Loss: 0.00034369
Iteration 176/1000 | Loss: 0.00060240
Iteration 177/1000 | Loss: 0.00116486
Iteration 178/1000 | Loss: 0.00159877
Iteration 179/1000 | Loss: 0.00039707
Iteration 180/1000 | Loss: 0.00034822
Iteration 181/1000 | Loss: 0.00087247
Iteration 182/1000 | Loss: 0.00062639
Iteration 183/1000 | Loss: 0.00038472
Iteration 184/1000 | Loss: 0.00061408
Iteration 185/1000 | Loss: 0.00074803
Iteration 186/1000 | Loss: 0.00035945
Iteration 187/1000 | Loss: 0.00033484
Iteration 188/1000 | Loss: 0.00060332
Iteration 189/1000 | Loss: 0.00112673
Iteration 190/1000 | Loss: 0.00055621
Iteration 191/1000 | Loss: 0.00037140
Iteration 192/1000 | Loss: 0.00067590
Iteration 193/1000 | Loss: 0.00055073
Iteration 194/1000 | Loss: 0.00111489
Iteration 195/1000 | Loss: 0.00037415
Iteration 196/1000 | Loss: 0.00033704
Iteration 197/1000 | Loss: 0.00071085
Iteration 198/1000 | Loss: 0.00113150
Iteration 199/1000 | Loss: 0.00040042
Iteration 200/1000 | Loss: 0.00060674
Iteration 201/1000 | Loss: 0.00033246
Iteration 202/1000 | Loss: 0.00037703
Iteration 203/1000 | Loss: 0.00152262
Iteration 204/1000 | Loss: 0.00096311
Iteration 205/1000 | Loss: 0.00052346
Iteration 206/1000 | Loss: 0.00119816
Iteration 207/1000 | Loss: 0.00183787
Iteration 208/1000 | Loss: 0.00073913
Iteration 209/1000 | Loss: 0.00042666
Iteration 210/1000 | Loss: 0.00060285
Iteration 211/1000 | Loss: 0.00099053
Iteration 212/1000 | Loss: 0.00062499
Iteration 213/1000 | Loss: 0.00075551
Iteration 214/1000 | Loss: 0.00042550
Iteration 215/1000 | Loss: 0.00032843
Iteration 216/1000 | Loss: 0.00058536
Iteration 217/1000 | Loss: 0.00046585
Iteration 218/1000 | Loss: 0.00032969
Iteration 219/1000 | Loss: 0.00067602
Iteration 220/1000 | Loss: 0.00036826
Iteration 221/1000 | Loss: 0.00110940
Iteration 222/1000 | Loss: 0.00074124
Iteration 223/1000 | Loss: 0.00035250
Iteration 224/1000 | Loss: 0.00050676
Iteration 225/1000 | Loss: 0.00039693
Iteration 226/1000 | Loss: 0.00098110
Iteration 227/1000 | Loss: 0.00040057
Iteration 228/1000 | Loss: 0.00062335
Iteration 229/1000 | Loss: 0.00054348
Iteration 230/1000 | Loss: 0.00092576
Iteration 231/1000 | Loss: 0.00095353
Iteration 232/1000 | Loss: 0.00047477
Iteration 233/1000 | Loss: 0.00036191
Iteration 234/1000 | Loss: 0.00082537
Iteration 235/1000 | Loss: 0.00147473
Iteration 236/1000 | Loss: 0.00217160
Iteration 237/1000 | Loss: 0.00061888
Iteration 238/1000 | Loss: 0.00043243
Iteration 239/1000 | Loss: 0.00052413
Iteration 240/1000 | Loss: 0.00054941
Iteration 241/1000 | Loss: 0.00051009
Iteration 242/1000 | Loss: 0.00288551
Iteration 243/1000 | Loss: 0.00093566
Iteration 244/1000 | Loss: 0.00537129
Iteration 245/1000 | Loss: 0.00069531
Iteration 246/1000 | Loss: 0.00254535
Iteration 247/1000 | Loss: 0.00139724
Iteration 248/1000 | Loss: 0.00081162
Iteration 249/1000 | Loss: 0.00158451
Iteration 250/1000 | Loss: 0.00123965
Iteration 251/1000 | Loss: 0.00119942
Iteration 252/1000 | Loss: 0.00136241
Iteration 253/1000 | Loss: 0.00177209
Iteration 254/1000 | Loss: 0.00203401
Iteration 255/1000 | Loss: 0.00113003
Iteration 256/1000 | Loss: 0.00109941
Iteration 257/1000 | Loss: 0.00173326
Iteration 258/1000 | Loss: 0.00154933
Iteration 259/1000 | Loss: 0.00151611
Iteration 260/1000 | Loss: 0.00077209
Iteration 261/1000 | Loss: 0.00047904
Iteration 262/1000 | Loss: 0.00056390
Iteration 263/1000 | Loss: 0.00047647
Iteration 264/1000 | Loss: 0.00029778
Iteration 265/1000 | Loss: 0.00023503
Iteration 266/1000 | Loss: 0.00060726
Iteration 267/1000 | Loss: 0.00053617
Iteration 268/1000 | Loss: 0.00071309
Iteration 269/1000 | Loss: 0.00049663
Iteration 270/1000 | Loss: 0.00026656
Iteration 271/1000 | Loss: 0.00034831
Iteration 272/1000 | Loss: 0.00091047
Iteration 273/1000 | Loss: 0.00033165
Iteration 274/1000 | Loss: 0.00028257
Iteration 275/1000 | Loss: 0.00059606
Iteration 276/1000 | Loss: 0.00021462
Iteration 277/1000 | Loss: 0.00018545
Iteration 278/1000 | Loss: 0.00016850
Iteration 279/1000 | Loss: 0.00030437
Iteration 280/1000 | Loss: 0.00015743
Iteration 281/1000 | Loss: 0.00068214
Iteration 282/1000 | Loss: 0.00018697
Iteration 283/1000 | Loss: 0.00047051
Iteration 284/1000 | Loss: 0.00030128
Iteration 285/1000 | Loss: 0.00044810
Iteration 286/1000 | Loss: 0.00018917
Iteration 287/1000 | Loss: 0.00015270
Iteration 288/1000 | Loss: 0.00046260
Iteration 289/1000 | Loss: 0.00027896
Iteration 290/1000 | Loss: 0.00014708
Iteration 291/1000 | Loss: 0.00045920
Iteration 292/1000 | Loss: 0.00071697
Iteration 293/1000 | Loss: 0.00052308
Iteration 294/1000 | Loss: 0.00015117
Iteration 295/1000 | Loss: 0.00020143
Iteration 296/1000 | Loss: 0.00014355
Iteration 297/1000 | Loss: 0.00013369
Iteration 298/1000 | Loss: 0.00013073
Iteration 299/1000 | Loss: 0.00012863
Iteration 300/1000 | Loss: 0.00059192
Iteration 301/1000 | Loss: 0.00045621
Iteration 302/1000 | Loss: 0.00022877
Iteration 303/1000 | Loss: 0.00041020
Iteration 304/1000 | Loss: 0.00018122
Iteration 305/1000 | Loss: 0.00029208
Iteration 306/1000 | Loss: 0.00013325
Iteration 307/1000 | Loss: 0.00012723
Iteration 308/1000 | Loss: 0.00012434
Iteration 309/1000 | Loss: 0.00012226
Iteration 310/1000 | Loss: 0.00046123
Iteration 311/1000 | Loss: 0.00013866
Iteration 312/1000 | Loss: 0.00012543
Iteration 313/1000 | Loss: 0.00012186
Iteration 314/1000 | Loss: 0.00012012
Iteration 315/1000 | Loss: 0.00043867
Iteration 316/1000 | Loss: 0.00031540
Iteration 317/1000 | Loss: 0.00024609
Iteration 318/1000 | Loss: 0.00014030
Iteration 319/1000 | Loss: 0.00012688
Iteration 320/1000 | Loss: 0.00012233
Iteration 321/1000 | Loss: 0.00012042
Iteration 322/1000 | Loss: 0.00032430
Iteration 323/1000 | Loss: 0.00022531
Iteration 324/1000 | Loss: 0.00011914
Iteration 325/1000 | Loss: 0.00011806
Iteration 326/1000 | Loss: 0.00036612
Iteration 327/1000 | Loss: 0.00012322
Iteration 328/1000 | Loss: 0.00011909
Iteration 329/1000 | Loss: 0.00011641
Iteration 330/1000 | Loss: 0.00039772
Iteration 331/1000 | Loss: 0.00037562
Iteration 332/1000 | Loss: 0.00011850
Iteration 333/1000 | Loss: 0.00011628
Iteration 334/1000 | Loss: 0.00011550
Iteration 335/1000 | Loss: 0.00011506
Iteration 336/1000 | Loss: 0.00011457
Iteration 337/1000 | Loss: 0.00011434
Iteration 338/1000 | Loss: 0.00011429
Iteration 339/1000 | Loss: 0.00011413
Iteration 340/1000 | Loss: 0.00011405
Iteration 341/1000 | Loss: 0.00011404
Iteration 342/1000 | Loss: 0.00011404
Iteration 343/1000 | Loss: 0.00011397
Iteration 344/1000 | Loss: 0.00011392
Iteration 345/1000 | Loss: 0.00011390
Iteration 346/1000 | Loss: 0.00011386
Iteration 347/1000 | Loss: 0.00011386
Iteration 348/1000 | Loss: 0.00011385
Iteration 349/1000 | Loss: 0.00011385
Iteration 350/1000 | Loss: 0.00011385
Iteration 351/1000 | Loss: 0.00011381
Iteration 352/1000 | Loss: 0.00011369
Iteration 353/1000 | Loss: 0.00011365
Iteration 354/1000 | Loss: 0.00011365
Iteration 355/1000 | Loss: 0.00011365
Iteration 356/1000 | Loss: 0.00011365
Iteration 357/1000 | Loss: 0.00011365
Iteration 358/1000 | Loss: 0.00011365
Iteration 359/1000 | Loss: 0.00011365
Iteration 360/1000 | Loss: 0.00011364
Iteration 361/1000 | Loss: 0.00011364
Iteration 362/1000 | Loss: 0.00011364
Iteration 363/1000 | Loss: 0.00011364
Iteration 364/1000 | Loss: 0.00011364
Iteration 365/1000 | Loss: 0.00011364
Iteration 366/1000 | Loss: 0.00011364
Iteration 367/1000 | Loss: 0.00011364
Iteration 368/1000 | Loss: 0.00011364
Iteration 369/1000 | Loss: 0.00011363
Iteration 370/1000 | Loss: 0.00011363
Iteration 371/1000 | Loss: 0.00011363
Iteration 372/1000 | Loss: 0.00011362
Iteration 373/1000 | Loss: 0.00011362
Iteration 374/1000 | Loss: 0.00011362
Iteration 375/1000 | Loss: 0.00011362
Iteration 376/1000 | Loss: 0.00011362
Iteration 377/1000 | Loss: 0.00011362
Iteration 378/1000 | Loss: 0.00011361
Iteration 379/1000 | Loss: 0.00011361
Iteration 380/1000 | Loss: 0.00011361
Iteration 381/1000 | Loss: 0.00011361
Iteration 382/1000 | Loss: 0.00011361
Iteration 383/1000 | Loss: 0.00011360
Iteration 384/1000 | Loss: 0.00011360
Iteration 385/1000 | Loss: 0.00011359
Iteration 386/1000 | Loss: 0.00011359
Iteration 387/1000 | Loss: 0.00011358
Iteration 388/1000 | Loss: 0.00011358
Iteration 389/1000 | Loss: 0.00011358
Iteration 390/1000 | Loss: 0.00011358
Iteration 391/1000 | Loss: 0.00011358
Iteration 392/1000 | Loss: 0.00011358
Iteration 393/1000 | Loss: 0.00011358
Iteration 394/1000 | Loss: 0.00011358
Iteration 395/1000 | Loss: 0.00011358
Iteration 396/1000 | Loss: 0.00011358
Iteration 397/1000 | Loss: 0.00011358
Iteration 398/1000 | Loss: 0.00011358
Iteration 399/1000 | Loss: 0.00011358
Iteration 400/1000 | Loss: 0.00011358
Iteration 401/1000 | Loss: 0.00011358
Iteration 402/1000 | Loss: 0.00011358
Iteration 403/1000 | Loss: 0.00011358
Iteration 404/1000 | Loss: 0.00011358
Iteration 405/1000 | Loss: 0.00011358
Iteration 406/1000 | Loss: 0.00011358
Iteration 407/1000 | Loss: 0.00011358
Iteration 408/1000 | Loss: 0.00011358
Iteration 409/1000 | Loss: 0.00011358
Iteration 410/1000 | Loss: 0.00011358
Iteration 411/1000 | Loss: 0.00011358
Iteration 412/1000 | Loss: 0.00011358
Iteration 413/1000 | Loss: 0.00011358
Iteration 414/1000 | Loss: 0.00011358
Iteration 415/1000 | Loss: 0.00011358
Iteration 416/1000 | Loss: 0.00011358
Iteration 417/1000 | Loss: 0.00011358
Iteration 418/1000 | Loss: 0.00011358
Iteration 419/1000 | Loss: 0.00011358
Iteration 420/1000 | Loss: 0.00011358
Iteration 421/1000 | Loss: 0.00011358
Iteration 422/1000 | Loss: 0.00011358
Iteration 423/1000 | Loss: 0.00011358
Iteration 424/1000 | Loss: 0.00011358
Iteration 425/1000 | Loss: 0.00011358
Iteration 426/1000 | Loss: 0.00011358
Iteration 427/1000 | Loss: 0.00011358
Iteration 428/1000 | Loss: 0.00011358
Iteration 429/1000 | Loss: 0.00011358
Iteration 430/1000 | Loss: 0.00011358
Iteration 431/1000 | Loss: 0.00011358
Iteration 432/1000 | Loss: 0.00011358
Iteration 433/1000 | Loss: 0.00011358
Iteration 434/1000 | Loss: 0.00011358
Iteration 435/1000 | Loss: 0.00011358
Iteration 436/1000 | Loss: 0.00011358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 436. Stopping optimization.
Last 5 losses: [0.00011357670155121014, 0.00011357670155121014, 0.00011357670155121014, 0.00011357670155121014, 0.00011357670155121014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011357670155121014

Optimization complete. Final v2v error: 7.882391929626465 mm

Highest mean error: 15.651961326599121 mm for frame 123

Lowest mean error: 5.833965301513672 mm for frame 11

Saving results

Total time: 557.2166726589203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460217
Iteration 2/25 | Loss: 0.00191540
Iteration 3/25 | Loss: 0.00183350
Iteration 4/25 | Loss: 0.00182412
Iteration 5/25 | Loss: 0.00181902
Iteration 6/25 | Loss: 0.00181776
Iteration 7/25 | Loss: 0.00181776
Iteration 8/25 | Loss: 0.00181776
Iteration 9/25 | Loss: 0.00181776
Iteration 10/25 | Loss: 0.00181776
Iteration 11/25 | Loss: 0.00181776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018177559832111, 0.0018177559832111, 0.0018177559832111, 0.0018177559832111, 0.0018177559832111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018177559832111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21517634
Iteration 2/25 | Loss: 0.00260147
Iteration 3/25 | Loss: 0.00260147
Iteration 4/25 | Loss: 0.00260147
Iteration 5/25 | Loss: 0.00260147
Iteration 6/25 | Loss: 0.00260147
Iteration 7/25 | Loss: 0.00260147
Iteration 8/25 | Loss: 0.00260147
Iteration 9/25 | Loss: 0.00260147
Iteration 10/25 | Loss: 0.00260147
Iteration 11/25 | Loss: 0.00260147
Iteration 12/25 | Loss: 0.00260147
Iteration 13/25 | Loss: 0.00260147
Iteration 14/25 | Loss: 0.00260147
Iteration 15/25 | Loss: 0.00260147
Iteration 16/25 | Loss: 0.00260147
Iteration 17/25 | Loss: 0.00260147
Iteration 18/25 | Loss: 0.00260147
Iteration 19/25 | Loss: 0.00260147
Iteration 20/25 | Loss: 0.00260147
Iteration 21/25 | Loss: 0.00260147
Iteration 22/25 | Loss: 0.00260147
Iteration 23/25 | Loss: 0.00260147
Iteration 24/25 | Loss: 0.00260147
Iteration 25/25 | Loss: 0.00260147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260147
Iteration 2/1000 | Loss: 0.00004326
Iteration 3/1000 | Loss: 0.00003309
Iteration 4/1000 | Loss: 0.00002939
Iteration 5/1000 | Loss: 0.00002797
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002576
Iteration 8/1000 | Loss: 0.00002496
Iteration 9/1000 | Loss: 0.00002453
Iteration 10/1000 | Loss: 0.00002425
Iteration 11/1000 | Loss: 0.00002400
Iteration 12/1000 | Loss: 0.00002398
Iteration 13/1000 | Loss: 0.00002390
Iteration 14/1000 | Loss: 0.00002374
Iteration 15/1000 | Loss: 0.00002374
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002373
Iteration 18/1000 | Loss: 0.00002373
Iteration 19/1000 | Loss: 0.00002373
Iteration 20/1000 | Loss: 0.00002372
Iteration 21/1000 | Loss: 0.00002369
Iteration 22/1000 | Loss: 0.00002368
Iteration 23/1000 | Loss: 0.00002368
Iteration 24/1000 | Loss: 0.00002368
Iteration 25/1000 | Loss: 0.00002367
Iteration 26/1000 | Loss: 0.00002367
Iteration 27/1000 | Loss: 0.00002367
Iteration 28/1000 | Loss: 0.00002367
Iteration 29/1000 | Loss: 0.00002366
Iteration 30/1000 | Loss: 0.00002365
Iteration 31/1000 | Loss: 0.00002364
Iteration 32/1000 | Loss: 0.00002364
Iteration 33/1000 | Loss: 0.00002364
Iteration 34/1000 | Loss: 0.00002364
Iteration 35/1000 | Loss: 0.00002364
Iteration 36/1000 | Loss: 0.00002364
Iteration 37/1000 | Loss: 0.00002364
Iteration 38/1000 | Loss: 0.00002363
Iteration 39/1000 | Loss: 0.00002363
Iteration 40/1000 | Loss: 0.00002362
Iteration 41/1000 | Loss: 0.00002362
Iteration 42/1000 | Loss: 0.00002361
Iteration 43/1000 | Loss: 0.00002361
Iteration 44/1000 | Loss: 0.00002360
Iteration 45/1000 | Loss: 0.00002360
Iteration 46/1000 | Loss: 0.00002360
Iteration 47/1000 | Loss: 0.00002360
Iteration 48/1000 | Loss: 0.00002360
Iteration 49/1000 | Loss: 0.00002360
Iteration 50/1000 | Loss: 0.00002359
Iteration 51/1000 | Loss: 0.00002359
Iteration 52/1000 | Loss: 0.00002359
Iteration 53/1000 | Loss: 0.00002358
Iteration 54/1000 | Loss: 0.00002358
Iteration 55/1000 | Loss: 0.00002358
Iteration 56/1000 | Loss: 0.00002357
Iteration 57/1000 | Loss: 0.00002357
Iteration 58/1000 | Loss: 0.00002357
Iteration 59/1000 | Loss: 0.00002356
Iteration 60/1000 | Loss: 0.00002356
Iteration 61/1000 | Loss: 0.00002356
Iteration 62/1000 | Loss: 0.00002356
Iteration 63/1000 | Loss: 0.00002356
Iteration 64/1000 | Loss: 0.00002356
Iteration 65/1000 | Loss: 0.00002356
Iteration 66/1000 | Loss: 0.00002356
Iteration 67/1000 | Loss: 0.00002356
Iteration 68/1000 | Loss: 0.00002356
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002355
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002355
Iteration 73/1000 | Loss: 0.00002355
Iteration 74/1000 | Loss: 0.00002355
Iteration 75/1000 | Loss: 0.00002354
Iteration 76/1000 | Loss: 0.00002354
Iteration 77/1000 | Loss: 0.00002354
Iteration 78/1000 | Loss: 0.00002353
Iteration 79/1000 | Loss: 0.00002353
Iteration 80/1000 | Loss: 0.00002353
Iteration 81/1000 | Loss: 0.00002353
Iteration 82/1000 | Loss: 0.00002353
Iteration 83/1000 | Loss: 0.00002353
Iteration 84/1000 | Loss: 0.00002353
Iteration 85/1000 | Loss: 0.00002353
Iteration 86/1000 | Loss: 0.00002353
Iteration 87/1000 | Loss: 0.00002353
Iteration 88/1000 | Loss: 0.00002353
Iteration 89/1000 | Loss: 0.00002352
Iteration 90/1000 | Loss: 0.00002352
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002351
Iteration 93/1000 | Loss: 0.00002351
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002350
Iteration 96/1000 | Loss: 0.00002350
Iteration 97/1000 | Loss: 0.00002350
Iteration 98/1000 | Loss: 0.00002350
Iteration 99/1000 | Loss: 0.00002349
Iteration 100/1000 | Loss: 0.00002349
Iteration 101/1000 | Loss: 0.00002349
Iteration 102/1000 | Loss: 0.00002349
Iteration 103/1000 | Loss: 0.00002349
Iteration 104/1000 | Loss: 0.00002349
Iteration 105/1000 | Loss: 0.00002348
Iteration 106/1000 | Loss: 0.00002348
Iteration 107/1000 | Loss: 0.00002348
Iteration 108/1000 | Loss: 0.00002347
Iteration 109/1000 | Loss: 0.00002347
Iteration 110/1000 | Loss: 0.00002347
Iteration 111/1000 | Loss: 0.00002347
Iteration 112/1000 | Loss: 0.00002347
Iteration 113/1000 | Loss: 0.00002346
Iteration 114/1000 | Loss: 0.00002346
Iteration 115/1000 | Loss: 0.00002346
Iteration 116/1000 | Loss: 0.00002346
Iteration 117/1000 | Loss: 0.00002346
Iteration 118/1000 | Loss: 0.00002346
Iteration 119/1000 | Loss: 0.00002346
Iteration 120/1000 | Loss: 0.00002345
Iteration 121/1000 | Loss: 0.00002345
Iteration 122/1000 | Loss: 0.00002345
Iteration 123/1000 | Loss: 0.00002345
Iteration 124/1000 | Loss: 0.00002345
Iteration 125/1000 | Loss: 0.00002345
Iteration 126/1000 | Loss: 0.00002345
Iteration 127/1000 | Loss: 0.00002345
Iteration 128/1000 | Loss: 0.00002345
Iteration 129/1000 | Loss: 0.00002345
Iteration 130/1000 | Loss: 0.00002345
Iteration 131/1000 | Loss: 0.00002345
Iteration 132/1000 | Loss: 0.00002345
Iteration 133/1000 | Loss: 0.00002345
Iteration 134/1000 | Loss: 0.00002345
Iteration 135/1000 | Loss: 0.00002345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.3451106244465336e-05, 2.3451106244465336e-05, 2.3451106244465336e-05, 2.3451106244465336e-05, 2.3451106244465336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3451106244465336e-05

Optimization complete. Final v2v error: 4.117720127105713 mm

Highest mean error: 4.311612129211426 mm for frame 44

Lowest mean error: 3.9754011631011963 mm for frame 4

Saving results

Total time: 35.87178349494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570921
Iteration 2/25 | Loss: 0.00220267
Iteration 3/25 | Loss: 0.00192535
Iteration 4/25 | Loss: 0.00191025
Iteration 5/25 | Loss: 0.00190750
Iteration 6/25 | Loss: 0.00190750
Iteration 7/25 | Loss: 0.00190750
Iteration 8/25 | Loss: 0.00190750
Iteration 9/25 | Loss: 0.00190750
Iteration 10/25 | Loss: 0.00190750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0019075035816058517, 0.0019075035816058517, 0.0019075035816058517, 0.0019075035816058517, 0.0019075035816058517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019075035816058517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17440009
Iteration 2/25 | Loss: 0.00255758
Iteration 3/25 | Loss: 0.00255757
Iteration 4/25 | Loss: 0.00255757
Iteration 5/25 | Loss: 0.00255757
Iteration 6/25 | Loss: 0.00255757
Iteration 7/25 | Loss: 0.00255757
Iteration 8/25 | Loss: 0.00255757
Iteration 9/25 | Loss: 0.00255757
Iteration 10/25 | Loss: 0.00255757
Iteration 11/25 | Loss: 0.00255757
Iteration 12/25 | Loss: 0.00255757
Iteration 13/25 | Loss: 0.00255757
Iteration 14/25 | Loss: 0.00255757
Iteration 15/25 | Loss: 0.00255757
Iteration 16/25 | Loss: 0.00255757
Iteration 17/25 | Loss: 0.00255757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002557570580393076, 0.002557570580393076, 0.002557570580393076, 0.002557570580393076, 0.002557570580393076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002557570580393076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255757
Iteration 2/1000 | Loss: 0.00010792
Iteration 3/1000 | Loss: 0.00005535
Iteration 4/1000 | Loss: 0.00004052
Iteration 5/1000 | Loss: 0.00003646
Iteration 6/1000 | Loss: 0.00003394
Iteration 7/1000 | Loss: 0.00003206
Iteration 8/1000 | Loss: 0.00003033
Iteration 9/1000 | Loss: 0.00002933
Iteration 10/1000 | Loss: 0.00002841
Iteration 11/1000 | Loss: 0.00002763
Iteration 12/1000 | Loss: 0.00002706
Iteration 13/1000 | Loss: 0.00002659
Iteration 14/1000 | Loss: 0.00002629
Iteration 15/1000 | Loss: 0.00002614
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002593
Iteration 18/1000 | Loss: 0.00002590
Iteration 19/1000 | Loss: 0.00002582
Iteration 20/1000 | Loss: 0.00002577
Iteration 21/1000 | Loss: 0.00002574
Iteration 22/1000 | Loss: 0.00002571
Iteration 23/1000 | Loss: 0.00002571
Iteration 24/1000 | Loss: 0.00002569
Iteration 25/1000 | Loss: 0.00002569
Iteration 26/1000 | Loss: 0.00002569
Iteration 27/1000 | Loss: 0.00002569
Iteration 28/1000 | Loss: 0.00002569
Iteration 29/1000 | Loss: 0.00002569
Iteration 30/1000 | Loss: 0.00002569
Iteration 31/1000 | Loss: 0.00002569
Iteration 32/1000 | Loss: 0.00002569
Iteration 33/1000 | Loss: 0.00002569
Iteration 34/1000 | Loss: 0.00002569
Iteration 35/1000 | Loss: 0.00002568
Iteration 36/1000 | Loss: 0.00002568
Iteration 37/1000 | Loss: 0.00002568
Iteration 38/1000 | Loss: 0.00002568
Iteration 39/1000 | Loss: 0.00002568
Iteration 40/1000 | Loss: 0.00002568
Iteration 41/1000 | Loss: 0.00002568
Iteration 42/1000 | Loss: 0.00002567
Iteration 43/1000 | Loss: 0.00002567
Iteration 44/1000 | Loss: 0.00002567
Iteration 45/1000 | Loss: 0.00002566
Iteration 46/1000 | Loss: 0.00002566
Iteration 47/1000 | Loss: 0.00002566
Iteration 48/1000 | Loss: 0.00002566
Iteration 49/1000 | Loss: 0.00002565
Iteration 50/1000 | Loss: 0.00002565
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002565
Iteration 53/1000 | Loss: 0.00002565
Iteration 54/1000 | Loss: 0.00002565
Iteration 55/1000 | Loss: 0.00002565
Iteration 56/1000 | Loss: 0.00002565
Iteration 57/1000 | Loss: 0.00002565
Iteration 58/1000 | Loss: 0.00002564
Iteration 59/1000 | Loss: 0.00002564
Iteration 60/1000 | Loss: 0.00002564
Iteration 61/1000 | Loss: 0.00002564
Iteration 62/1000 | Loss: 0.00002564
Iteration 63/1000 | Loss: 0.00002564
Iteration 64/1000 | Loss: 0.00002564
Iteration 65/1000 | Loss: 0.00002564
Iteration 66/1000 | Loss: 0.00002564
Iteration 67/1000 | Loss: 0.00002564
Iteration 68/1000 | Loss: 0.00002563
Iteration 69/1000 | Loss: 0.00002563
Iteration 70/1000 | Loss: 0.00002563
Iteration 71/1000 | Loss: 0.00002563
Iteration 72/1000 | Loss: 0.00002563
Iteration 73/1000 | Loss: 0.00002563
Iteration 74/1000 | Loss: 0.00002563
Iteration 75/1000 | Loss: 0.00002563
Iteration 76/1000 | Loss: 0.00002563
Iteration 77/1000 | Loss: 0.00002563
Iteration 78/1000 | Loss: 0.00002563
Iteration 79/1000 | Loss: 0.00002563
Iteration 80/1000 | Loss: 0.00002563
Iteration 81/1000 | Loss: 0.00002562
Iteration 82/1000 | Loss: 0.00002562
Iteration 83/1000 | Loss: 0.00002562
Iteration 84/1000 | Loss: 0.00002562
Iteration 85/1000 | Loss: 0.00002562
Iteration 86/1000 | Loss: 0.00002562
Iteration 87/1000 | Loss: 0.00002562
Iteration 88/1000 | Loss: 0.00002562
Iteration 89/1000 | Loss: 0.00002562
Iteration 90/1000 | Loss: 0.00002562
Iteration 91/1000 | Loss: 0.00002562
Iteration 92/1000 | Loss: 0.00002562
Iteration 93/1000 | Loss: 0.00002561
Iteration 94/1000 | Loss: 0.00002561
Iteration 95/1000 | Loss: 0.00002561
Iteration 96/1000 | Loss: 0.00002561
Iteration 97/1000 | Loss: 0.00002561
Iteration 98/1000 | Loss: 0.00002561
Iteration 99/1000 | Loss: 0.00002561
Iteration 100/1000 | Loss: 0.00002561
Iteration 101/1000 | Loss: 0.00002561
Iteration 102/1000 | Loss: 0.00002561
Iteration 103/1000 | Loss: 0.00002561
Iteration 104/1000 | Loss: 0.00002561
Iteration 105/1000 | Loss: 0.00002561
Iteration 106/1000 | Loss: 0.00002561
Iteration 107/1000 | Loss: 0.00002561
Iteration 108/1000 | Loss: 0.00002561
Iteration 109/1000 | Loss: 0.00002561
Iteration 110/1000 | Loss: 0.00002560
Iteration 111/1000 | Loss: 0.00002560
Iteration 112/1000 | Loss: 0.00002560
Iteration 113/1000 | Loss: 0.00002560
Iteration 114/1000 | Loss: 0.00002560
Iteration 115/1000 | Loss: 0.00002560
Iteration 116/1000 | Loss: 0.00002560
Iteration 117/1000 | Loss: 0.00002560
Iteration 118/1000 | Loss: 0.00002560
Iteration 119/1000 | Loss: 0.00002559
Iteration 120/1000 | Loss: 0.00002559
Iteration 121/1000 | Loss: 0.00002559
Iteration 122/1000 | Loss: 0.00002559
Iteration 123/1000 | Loss: 0.00002559
Iteration 124/1000 | Loss: 0.00002559
Iteration 125/1000 | Loss: 0.00002559
Iteration 126/1000 | Loss: 0.00002559
Iteration 127/1000 | Loss: 0.00002559
Iteration 128/1000 | Loss: 0.00002559
Iteration 129/1000 | Loss: 0.00002559
Iteration 130/1000 | Loss: 0.00002559
Iteration 131/1000 | Loss: 0.00002558
Iteration 132/1000 | Loss: 0.00002558
Iteration 133/1000 | Loss: 0.00002558
Iteration 134/1000 | Loss: 0.00002558
Iteration 135/1000 | Loss: 0.00002557
Iteration 136/1000 | Loss: 0.00002557
Iteration 137/1000 | Loss: 0.00002557
Iteration 138/1000 | Loss: 0.00002557
Iteration 139/1000 | Loss: 0.00002557
Iteration 140/1000 | Loss: 0.00002557
Iteration 141/1000 | Loss: 0.00002557
Iteration 142/1000 | Loss: 0.00002557
Iteration 143/1000 | Loss: 0.00002556
Iteration 144/1000 | Loss: 0.00002556
Iteration 145/1000 | Loss: 0.00002556
Iteration 146/1000 | Loss: 0.00002556
Iteration 147/1000 | Loss: 0.00002556
Iteration 148/1000 | Loss: 0.00002556
Iteration 149/1000 | Loss: 0.00002556
Iteration 150/1000 | Loss: 0.00002556
Iteration 151/1000 | Loss: 0.00002556
Iteration 152/1000 | Loss: 0.00002556
Iteration 153/1000 | Loss: 0.00002556
Iteration 154/1000 | Loss: 0.00002556
Iteration 155/1000 | Loss: 0.00002556
Iteration 156/1000 | Loss: 0.00002556
Iteration 157/1000 | Loss: 0.00002555
Iteration 158/1000 | Loss: 0.00002555
Iteration 159/1000 | Loss: 0.00002555
Iteration 160/1000 | Loss: 0.00002555
Iteration 161/1000 | Loss: 0.00002555
Iteration 162/1000 | Loss: 0.00002555
Iteration 163/1000 | Loss: 0.00002555
Iteration 164/1000 | Loss: 0.00002555
Iteration 165/1000 | Loss: 0.00002555
Iteration 166/1000 | Loss: 0.00002555
Iteration 167/1000 | Loss: 0.00002555
Iteration 168/1000 | Loss: 0.00002555
Iteration 169/1000 | Loss: 0.00002555
Iteration 170/1000 | Loss: 0.00002555
Iteration 171/1000 | Loss: 0.00002555
Iteration 172/1000 | Loss: 0.00002555
Iteration 173/1000 | Loss: 0.00002555
Iteration 174/1000 | Loss: 0.00002555
Iteration 175/1000 | Loss: 0.00002555
Iteration 176/1000 | Loss: 0.00002555
Iteration 177/1000 | Loss: 0.00002555
Iteration 178/1000 | Loss: 0.00002555
Iteration 179/1000 | Loss: 0.00002555
Iteration 180/1000 | Loss: 0.00002555
Iteration 181/1000 | Loss: 0.00002555
Iteration 182/1000 | Loss: 0.00002555
Iteration 183/1000 | Loss: 0.00002555
Iteration 184/1000 | Loss: 0.00002555
Iteration 185/1000 | Loss: 0.00002555
Iteration 186/1000 | Loss: 0.00002555
Iteration 187/1000 | Loss: 0.00002555
Iteration 188/1000 | Loss: 0.00002555
Iteration 189/1000 | Loss: 0.00002555
Iteration 190/1000 | Loss: 0.00002555
Iteration 191/1000 | Loss: 0.00002555
Iteration 192/1000 | Loss: 0.00002555
Iteration 193/1000 | Loss: 0.00002555
Iteration 194/1000 | Loss: 0.00002555
Iteration 195/1000 | Loss: 0.00002555
Iteration 196/1000 | Loss: 0.00002555
Iteration 197/1000 | Loss: 0.00002555
Iteration 198/1000 | Loss: 0.00002555
Iteration 199/1000 | Loss: 0.00002555
Iteration 200/1000 | Loss: 0.00002555
Iteration 201/1000 | Loss: 0.00002555
Iteration 202/1000 | Loss: 0.00002555
Iteration 203/1000 | Loss: 0.00002555
Iteration 204/1000 | Loss: 0.00002555
Iteration 205/1000 | Loss: 0.00002555
Iteration 206/1000 | Loss: 0.00002555
Iteration 207/1000 | Loss: 0.00002555
Iteration 208/1000 | Loss: 0.00002555
Iteration 209/1000 | Loss: 0.00002555
Iteration 210/1000 | Loss: 0.00002555
Iteration 211/1000 | Loss: 0.00002555
Iteration 212/1000 | Loss: 0.00002555
Iteration 213/1000 | Loss: 0.00002555
Iteration 214/1000 | Loss: 0.00002555
Iteration 215/1000 | Loss: 0.00002555
Iteration 216/1000 | Loss: 0.00002555
Iteration 217/1000 | Loss: 0.00002555
Iteration 218/1000 | Loss: 0.00002555
Iteration 219/1000 | Loss: 0.00002555
Iteration 220/1000 | Loss: 0.00002555
Iteration 221/1000 | Loss: 0.00002555
Iteration 222/1000 | Loss: 0.00002555
Iteration 223/1000 | Loss: 0.00002555
Iteration 224/1000 | Loss: 0.00002555
Iteration 225/1000 | Loss: 0.00002555
Iteration 226/1000 | Loss: 0.00002555
Iteration 227/1000 | Loss: 0.00002555
Iteration 228/1000 | Loss: 0.00002555
Iteration 229/1000 | Loss: 0.00002555
Iteration 230/1000 | Loss: 0.00002555
Iteration 231/1000 | Loss: 0.00002555
Iteration 232/1000 | Loss: 0.00002555
Iteration 233/1000 | Loss: 0.00002555
Iteration 234/1000 | Loss: 0.00002555
Iteration 235/1000 | Loss: 0.00002555
Iteration 236/1000 | Loss: 0.00002555
Iteration 237/1000 | Loss: 0.00002555
Iteration 238/1000 | Loss: 0.00002555
Iteration 239/1000 | Loss: 0.00002555
Iteration 240/1000 | Loss: 0.00002555
Iteration 241/1000 | Loss: 0.00002555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.5553359591867775e-05, 2.5553359591867775e-05, 2.5553359591867775e-05, 2.5553359591867775e-05, 2.5553359591867775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5553359591867775e-05

Optimization complete. Final v2v error: 4.308743000030518 mm

Highest mean error: 4.4305949211120605 mm for frame 125

Lowest mean error: 4.157754421234131 mm for frame 93

Saving results

Total time: 42.93266010284424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074563
Iteration 2/25 | Loss: 0.00259835
Iteration 3/25 | Loss: 0.00198652
Iteration 4/25 | Loss: 0.00192355
Iteration 5/25 | Loss: 0.00184450
Iteration 6/25 | Loss: 0.00177688
Iteration 7/25 | Loss: 0.00173757
Iteration 8/25 | Loss: 0.00172347
Iteration 9/25 | Loss: 0.00171067
Iteration 10/25 | Loss: 0.00171131
Iteration 11/25 | Loss: 0.00171117
Iteration 12/25 | Loss: 0.00169936
Iteration 13/25 | Loss: 0.00169509
Iteration 14/25 | Loss: 0.00169422
Iteration 15/25 | Loss: 0.00169421
Iteration 16/25 | Loss: 0.00169360
Iteration 17/25 | Loss: 0.00169247
Iteration 18/25 | Loss: 0.00169262
Iteration 19/25 | Loss: 0.00169306
Iteration 20/25 | Loss: 0.00169218
Iteration 21/25 | Loss: 0.00169137
Iteration 22/25 | Loss: 0.00169181
Iteration 23/25 | Loss: 0.00169101
Iteration 24/25 | Loss: 0.00169181
Iteration 25/25 | Loss: 0.00169227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32352245
Iteration 2/25 | Loss: 0.00212649
Iteration 3/25 | Loss: 0.00212649
Iteration 4/25 | Loss: 0.00212648
Iteration 5/25 | Loss: 0.00211950
Iteration 6/25 | Loss: 0.00211949
Iteration 7/25 | Loss: 0.00211949
Iteration 8/25 | Loss: 0.00211949
Iteration 9/25 | Loss: 0.00211949
Iteration 10/25 | Loss: 0.00211949
Iteration 11/25 | Loss: 0.00211949
Iteration 12/25 | Loss: 0.00211949
Iteration 13/25 | Loss: 0.00211949
Iteration 14/25 | Loss: 0.00211949
Iteration 15/25 | Loss: 0.00211949
Iteration 16/25 | Loss: 0.00211949
Iteration 17/25 | Loss: 0.00211949
Iteration 18/25 | Loss: 0.00211949
Iteration 19/25 | Loss: 0.00211949
Iteration 20/25 | Loss: 0.00211949
Iteration 21/25 | Loss: 0.00211949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002119489014148712, 0.002119489014148712, 0.002119489014148712, 0.002119489014148712, 0.002119489014148712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002119489014148712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211949
Iteration 2/1000 | Loss: 0.00010498
Iteration 3/1000 | Loss: 0.00005351
Iteration 4/1000 | Loss: 0.00004283
Iteration 5/1000 | Loss: 0.00019817
Iteration 6/1000 | Loss: 0.00004812
Iteration 7/1000 | Loss: 0.00029325
Iteration 8/1000 | Loss: 0.00013560
Iteration 9/1000 | Loss: 0.00003465
Iteration 10/1000 | Loss: 0.00003230
Iteration 11/1000 | Loss: 0.00010470
Iteration 12/1000 | Loss: 0.00007101
Iteration 13/1000 | Loss: 0.00006121
Iteration 14/1000 | Loss: 0.00021138
Iteration 15/1000 | Loss: 0.00007980
Iteration 16/1000 | Loss: 0.00015428
Iteration 17/1000 | Loss: 0.00003044
Iteration 18/1000 | Loss: 0.00003149
Iteration 19/1000 | Loss: 0.00003075
Iteration 20/1000 | Loss: 0.00003024
Iteration 21/1000 | Loss: 0.00002947
Iteration 22/1000 | Loss: 0.00012532
Iteration 23/1000 | Loss: 0.00003259
Iteration 24/1000 | Loss: 0.00003002
Iteration 25/1000 | Loss: 0.00002914
Iteration 26/1000 | Loss: 0.00002906
Iteration 27/1000 | Loss: 0.00002905
Iteration 28/1000 | Loss: 0.00002903
Iteration 29/1000 | Loss: 0.00002903
Iteration 30/1000 | Loss: 0.00002902
Iteration 31/1000 | Loss: 0.00002902
Iteration 32/1000 | Loss: 0.00002902
Iteration 33/1000 | Loss: 0.00003065
Iteration 34/1000 | Loss: 0.00004902
Iteration 35/1000 | Loss: 0.00003017
Iteration 36/1000 | Loss: 0.00002996
Iteration 37/1000 | Loss: 0.00002916
Iteration 38/1000 | Loss: 0.00002912
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002910
Iteration 41/1000 | Loss: 0.00002909
Iteration 42/1000 | Loss: 0.00002908
Iteration 43/1000 | Loss: 0.00002908
Iteration 44/1000 | Loss: 0.00002908
Iteration 45/1000 | Loss: 0.00002908
Iteration 46/1000 | Loss: 0.00002907
Iteration 47/1000 | Loss: 0.00002897
Iteration 48/1000 | Loss: 0.00002896
Iteration 49/1000 | Loss: 0.00002895
Iteration 50/1000 | Loss: 0.00002895
Iteration 51/1000 | Loss: 0.00002907
Iteration 52/1000 | Loss: 0.00002907
Iteration 53/1000 | Loss: 0.00002907
Iteration 54/1000 | Loss: 0.00002906
Iteration 55/1000 | Loss: 0.00002892
Iteration 56/1000 | Loss: 0.00002892
Iteration 57/1000 | Loss: 0.00002891
Iteration 58/1000 | Loss: 0.00002891
Iteration 59/1000 | Loss: 0.00002893
Iteration 60/1000 | Loss: 0.00002892
Iteration 61/1000 | Loss: 0.00004630
Iteration 62/1000 | Loss: 0.00002894
Iteration 63/1000 | Loss: 0.00002888
Iteration 64/1000 | Loss: 0.00002888
Iteration 65/1000 | Loss: 0.00002885
Iteration 66/1000 | Loss: 0.00002885
Iteration 67/1000 | Loss: 0.00002880
Iteration 68/1000 | Loss: 0.00002880
Iteration 69/1000 | Loss: 0.00003013
Iteration 70/1000 | Loss: 0.00011550
Iteration 71/1000 | Loss: 0.00003498
Iteration 72/1000 | Loss: 0.00002999
Iteration 73/1000 | Loss: 0.00002910
Iteration 74/1000 | Loss: 0.00002898
Iteration 75/1000 | Loss: 0.00002890
Iteration 76/1000 | Loss: 0.00002890
Iteration 77/1000 | Loss: 0.00002885
Iteration 78/1000 | Loss: 0.00002886
Iteration 79/1000 | Loss: 0.00002885
Iteration 80/1000 | Loss: 0.00002884
Iteration 81/1000 | Loss: 0.00002884
Iteration 82/1000 | Loss: 0.00002884
Iteration 83/1000 | Loss: 0.00002884
Iteration 84/1000 | Loss: 0.00002884
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002884
Iteration 87/1000 | Loss: 0.00002884
Iteration 88/1000 | Loss: 0.00002884
Iteration 89/1000 | Loss: 0.00002884
Iteration 90/1000 | Loss: 0.00002884
Iteration 91/1000 | Loss: 0.00002884
Iteration 92/1000 | Loss: 0.00002884
Iteration 93/1000 | Loss: 0.00002884
Iteration 94/1000 | Loss: 0.00002884
Iteration 95/1000 | Loss: 0.00002884
Iteration 96/1000 | Loss: 0.00002884
Iteration 97/1000 | Loss: 0.00002884
Iteration 98/1000 | Loss: 0.00002884
Iteration 99/1000 | Loss: 0.00002884
Iteration 100/1000 | Loss: 0.00002884
Iteration 101/1000 | Loss: 0.00002884
Iteration 102/1000 | Loss: 0.00002884
Iteration 103/1000 | Loss: 0.00002884
Iteration 104/1000 | Loss: 0.00002884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.884340392483864e-05, 2.884340392483864e-05, 2.884340392483864e-05, 2.884340392483864e-05, 2.884340392483864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.884340392483864e-05

Optimization complete. Final v2v error: 4.231611251831055 mm

Highest mean error: 12.163482666015625 mm for frame 24

Lowest mean error: 3.606051445007324 mm for frame 74

Saving results

Total time: 105.5200629234314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069945
Iteration 2/25 | Loss: 0.00308828
Iteration 3/25 | Loss: 0.00259750
Iteration 4/25 | Loss: 0.00246877
Iteration 5/25 | Loss: 0.00241519
Iteration 6/25 | Loss: 0.00221963
Iteration 7/25 | Loss: 0.00202799
Iteration 8/25 | Loss: 0.00194067
Iteration 9/25 | Loss: 0.00191039
Iteration 10/25 | Loss: 0.00188369
Iteration 11/25 | Loss: 0.00183177
Iteration 12/25 | Loss: 0.00180480
Iteration 13/25 | Loss: 0.00179565
Iteration 14/25 | Loss: 0.00178789
Iteration 15/25 | Loss: 0.00178587
Iteration 16/25 | Loss: 0.00178397
Iteration 17/25 | Loss: 0.00178865
Iteration 18/25 | Loss: 0.00177941
Iteration 19/25 | Loss: 0.00177271
Iteration 20/25 | Loss: 0.00177163
Iteration 21/25 | Loss: 0.00177136
Iteration 22/25 | Loss: 0.00177128
Iteration 23/25 | Loss: 0.00177123
Iteration 24/25 | Loss: 0.00177123
Iteration 25/25 | Loss: 0.00177123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20101047
Iteration 2/25 | Loss: 0.00301881
Iteration 3/25 | Loss: 0.00301881
Iteration 4/25 | Loss: 0.00301881
Iteration 5/25 | Loss: 0.00301881
Iteration 6/25 | Loss: 0.00301880
Iteration 7/25 | Loss: 0.00301880
Iteration 8/25 | Loss: 0.00301880
Iteration 9/25 | Loss: 0.00301880
Iteration 10/25 | Loss: 0.00301880
Iteration 11/25 | Loss: 0.00301880
Iteration 12/25 | Loss: 0.00301880
Iteration 13/25 | Loss: 0.00301880
Iteration 14/25 | Loss: 0.00301880
Iteration 15/25 | Loss: 0.00301880
Iteration 16/25 | Loss: 0.00301880
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0030188029631972313, 0.0030188029631972313, 0.0030188029631972313, 0.0030188029631972313, 0.0030188029631972313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030188029631972313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301880
Iteration 2/1000 | Loss: 0.00014669
Iteration 3/1000 | Loss: 0.00009746
Iteration 4/1000 | Loss: 0.00008219
Iteration 5/1000 | Loss: 0.00007129
Iteration 6/1000 | Loss: 0.00006802
Iteration 7/1000 | Loss: 0.00006596
Iteration 8/1000 | Loss: 0.00006435
Iteration 9/1000 | Loss: 0.00006326
Iteration 10/1000 | Loss: 0.00006263
Iteration 11/1000 | Loss: 0.00006214
Iteration 12/1000 | Loss: 0.00006172
Iteration 13/1000 | Loss: 0.00006114
Iteration 14/1000 | Loss: 0.00006038
Iteration 15/1000 | Loss: 0.00005937
Iteration 16/1000 | Loss: 0.00019169
Iteration 17/1000 | Loss: 0.00052231
Iteration 18/1000 | Loss: 0.00020387
Iteration 19/1000 | Loss: 0.00051003
Iteration 20/1000 | Loss: 0.00016605
Iteration 21/1000 | Loss: 0.00010722
Iteration 22/1000 | Loss: 0.00006976
Iteration 23/1000 | Loss: 0.00006049
Iteration 24/1000 | Loss: 0.00041851
Iteration 25/1000 | Loss: 0.00082423
Iteration 26/1000 | Loss: 0.00044478
Iteration 27/1000 | Loss: 0.00045347
Iteration 28/1000 | Loss: 0.00012172
Iteration 29/1000 | Loss: 0.00007871
Iteration 30/1000 | Loss: 0.00006089
Iteration 31/1000 | Loss: 0.00004864
Iteration 32/1000 | Loss: 0.00004305
Iteration 33/1000 | Loss: 0.00004875
Iteration 34/1000 | Loss: 0.00003705
Iteration 35/1000 | Loss: 0.00003340
Iteration 36/1000 | Loss: 0.00003191
Iteration 37/1000 | Loss: 0.00003097
Iteration 38/1000 | Loss: 0.00003020
Iteration 39/1000 | Loss: 0.00002977
Iteration 40/1000 | Loss: 0.00019157
Iteration 41/1000 | Loss: 0.00004051
Iteration 42/1000 | Loss: 0.00003211
Iteration 43/1000 | Loss: 0.00002954
Iteration 44/1000 | Loss: 0.00002821
Iteration 45/1000 | Loss: 0.00002742
Iteration 46/1000 | Loss: 0.00002692
Iteration 47/1000 | Loss: 0.00002665
Iteration 48/1000 | Loss: 0.00002646
Iteration 49/1000 | Loss: 0.00002634
Iteration 50/1000 | Loss: 0.00002633
Iteration 51/1000 | Loss: 0.00002628
Iteration 52/1000 | Loss: 0.00002628
Iteration 53/1000 | Loss: 0.00002628
Iteration 54/1000 | Loss: 0.00002628
Iteration 55/1000 | Loss: 0.00002628
Iteration 56/1000 | Loss: 0.00002628
Iteration 57/1000 | Loss: 0.00002628
Iteration 58/1000 | Loss: 0.00002627
Iteration 59/1000 | Loss: 0.00002627
Iteration 60/1000 | Loss: 0.00002627
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002625
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002625
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002624
Iteration 69/1000 | Loss: 0.00002623
Iteration 70/1000 | Loss: 0.00002623
Iteration 71/1000 | Loss: 0.00002623
Iteration 72/1000 | Loss: 0.00002623
Iteration 73/1000 | Loss: 0.00002622
Iteration 74/1000 | Loss: 0.00002622
Iteration 75/1000 | Loss: 0.00002622
Iteration 76/1000 | Loss: 0.00002622
Iteration 77/1000 | Loss: 0.00002621
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002621
Iteration 80/1000 | Loss: 0.00002621
Iteration 81/1000 | Loss: 0.00002621
Iteration 82/1000 | Loss: 0.00002621
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002621
Iteration 86/1000 | Loss: 0.00002621
Iteration 87/1000 | Loss: 0.00002621
Iteration 88/1000 | Loss: 0.00002621
Iteration 89/1000 | Loss: 0.00002621
Iteration 90/1000 | Loss: 0.00002621
Iteration 91/1000 | Loss: 0.00002621
Iteration 92/1000 | Loss: 0.00002621
Iteration 93/1000 | Loss: 0.00002621
Iteration 94/1000 | Loss: 0.00002621
Iteration 95/1000 | Loss: 0.00002621
Iteration 96/1000 | Loss: 0.00002621
Iteration 97/1000 | Loss: 0.00002621
Iteration 98/1000 | Loss: 0.00002621
Iteration 99/1000 | Loss: 0.00002621
Iteration 100/1000 | Loss: 0.00002621
Iteration 101/1000 | Loss: 0.00002621
Iteration 102/1000 | Loss: 0.00002621
Iteration 103/1000 | Loss: 0.00002621
Iteration 104/1000 | Loss: 0.00002621
Iteration 105/1000 | Loss: 0.00002621
Iteration 106/1000 | Loss: 0.00002621
Iteration 107/1000 | Loss: 0.00002621
Iteration 108/1000 | Loss: 0.00002621
Iteration 109/1000 | Loss: 0.00002621
Iteration 110/1000 | Loss: 0.00002621
Iteration 111/1000 | Loss: 0.00002621
Iteration 112/1000 | Loss: 0.00002621
Iteration 113/1000 | Loss: 0.00002621
Iteration 114/1000 | Loss: 0.00002621
Iteration 115/1000 | Loss: 0.00002621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.621140993142035e-05, 2.621140993142035e-05, 2.621140993142035e-05, 2.621140993142035e-05, 2.621140993142035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.621140993142035e-05

Optimization complete. Final v2v error: 4.14510440826416 mm

Highest mean error: 11.79004192352295 mm for frame 49

Lowest mean error: 3.8519973754882812 mm for frame 24

Saving results

Total time: 110.21098923683167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547703
Iteration 2/25 | Loss: 0.00191768
Iteration 3/25 | Loss: 0.00184293
Iteration 4/25 | Loss: 0.00183734
Iteration 5/25 | Loss: 0.00183453
Iteration 6/25 | Loss: 0.00183441
Iteration 7/25 | Loss: 0.00183441
Iteration 8/25 | Loss: 0.00183441
Iteration 9/25 | Loss: 0.00183441
Iteration 10/25 | Loss: 0.00183441
Iteration 11/25 | Loss: 0.00183441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018344075651839375, 0.0018344075651839375, 0.0018344075651839375, 0.0018344075651839375, 0.0018344075651839375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018344075651839375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31874633
Iteration 2/25 | Loss: 0.00211875
Iteration 3/25 | Loss: 0.00211874
Iteration 4/25 | Loss: 0.00211874
Iteration 5/25 | Loss: 0.00211874
Iteration 6/25 | Loss: 0.00211874
Iteration 7/25 | Loss: 0.00211874
Iteration 8/25 | Loss: 0.00211874
Iteration 9/25 | Loss: 0.00211874
Iteration 10/25 | Loss: 0.00211874
Iteration 11/25 | Loss: 0.00211874
Iteration 12/25 | Loss: 0.00211874
Iteration 13/25 | Loss: 0.00211874
Iteration 14/25 | Loss: 0.00211874
Iteration 15/25 | Loss: 0.00211874
Iteration 16/25 | Loss: 0.00211874
Iteration 17/25 | Loss: 0.00211874
Iteration 18/25 | Loss: 0.00211874
Iteration 19/25 | Loss: 0.00211874
Iteration 20/25 | Loss: 0.00211874
Iteration 21/25 | Loss: 0.00211874
Iteration 22/25 | Loss: 0.00211874
Iteration 23/25 | Loss: 0.00211874
Iteration 24/25 | Loss: 0.00211874
Iteration 25/25 | Loss: 0.00211874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211874
Iteration 2/1000 | Loss: 0.00006221
Iteration 3/1000 | Loss: 0.00003763
Iteration 4/1000 | Loss: 0.00003331
Iteration 5/1000 | Loss: 0.00003139
Iteration 6/1000 | Loss: 0.00003033
Iteration 7/1000 | Loss: 0.00002961
Iteration 8/1000 | Loss: 0.00002890
Iteration 9/1000 | Loss: 0.00002834
Iteration 10/1000 | Loss: 0.00002805
Iteration 11/1000 | Loss: 0.00002781
Iteration 12/1000 | Loss: 0.00002761
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00002735
Iteration 15/1000 | Loss: 0.00002735
Iteration 16/1000 | Loss: 0.00002733
Iteration 17/1000 | Loss: 0.00002732
Iteration 18/1000 | Loss: 0.00002728
Iteration 19/1000 | Loss: 0.00002727
Iteration 20/1000 | Loss: 0.00002726
Iteration 21/1000 | Loss: 0.00002725
Iteration 22/1000 | Loss: 0.00002725
Iteration 23/1000 | Loss: 0.00002725
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00002724
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00002722
Iteration 28/1000 | Loss: 0.00002721
Iteration 29/1000 | Loss: 0.00002721
Iteration 30/1000 | Loss: 0.00002721
Iteration 31/1000 | Loss: 0.00002721
Iteration 32/1000 | Loss: 0.00002721
Iteration 33/1000 | Loss: 0.00002721
Iteration 34/1000 | Loss: 0.00002721
Iteration 35/1000 | Loss: 0.00002721
Iteration 36/1000 | Loss: 0.00002720
Iteration 37/1000 | Loss: 0.00002720
Iteration 38/1000 | Loss: 0.00002720
Iteration 39/1000 | Loss: 0.00002720
Iteration 40/1000 | Loss: 0.00002720
Iteration 41/1000 | Loss: 0.00002718
Iteration 42/1000 | Loss: 0.00002718
Iteration 43/1000 | Loss: 0.00002718
Iteration 44/1000 | Loss: 0.00002718
Iteration 45/1000 | Loss: 0.00002718
Iteration 46/1000 | Loss: 0.00002718
Iteration 47/1000 | Loss: 0.00002718
Iteration 48/1000 | Loss: 0.00002717
Iteration 49/1000 | Loss: 0.00002717
Iteration 50/1000 | Loss: 0.00002717
Iteration 51/1000 | Loss: 0.00002717
Iteration 52/1000 | Loss: 0.00002717
Iteration 53/1000 | Loss: 0.00002717
Iteration 54/1000 | Loss: 0.00002717
Iteration 55/1000 | Loss: 0.00002717
Iteration 56/1000 | Loss: 0.00002717
Iteration 57/1000 | Loss: 0.00002717
Iteration 58/1000 | Loss: 0.00002716
Iteration 59/1000 | Loss: 0.00002716
Iteration 60/1000 | Loss: 0.00002716
Iteration 61/1000 | Loss: 0.00002716
Iteration 62/1000 | Loss: 0.00002716
Iteration 63/1000 | Loss: 0.00002715
Iteration 64/1000 | Loss: 0.00002715
Iteration 65/1000 | Loss: 0.00002715
Iteration 66/1000 | Loss: 0.00002715
Iteration 67/1000 | Loss: 0.00002714
Iteration 68/1000 | Loss: 0.00002714
Iteration 69/1000 | Loss: 0.00002714
Iteration 70/1000 | Loss: 0.00002714
Iteration 71/1000 | Loss: 0.00002714
Iteration 72/1000 | Loss: 0.00002714
Iteration 73/1000 | Loss: 0.00002714
Iteration 74/1000 | Loss: 0.00002713
Iteration 75/1000 | Loss: 0.00002713
Iteration 76/1000 | Loss: 0.00002713
Iteration 77/1000 | Loss: 0.00002712
Iteration 78/1000 | Loss: 0.00002712
Iteration 79/1000 | Loss: 0.00002712
Iteration 80/1000 | Loss: 0.00002712
Iteration 81/1000 | Loss: 0.00002711
Iteration 82/1000 | Loss: 0.00002711
Iteration 83/1000 | Loss: 0.00002711
Iteration 84/1000 | Loss: 0.00002710
Iteration 85/1000 | Loss: 0.00002710
Iteration 86/1000 | Loss: 0.00002709
Iteration 87/1000 | Loss: 0.00002709
Iteration 88/1000 | Loss: 0.00002709
Iteration 89/1000 | Loss: 0.00002709
Iteration 90/1000 | Loss: 0.00002709
Iteration 91/1000 | Loss: 0.00002709
Iteration 92/1000 | Loss: 0.00002709
Iteration 93/1000 | Loss: 0.00002709
Iteration 94/1000 | Loss: 0.00002708
Iteration 95/1000 | Loss: 0.00002708
Iteration 96/1000 | Loss: 0.00002708
Iteration 97/1000 | Loss: 0.00002708
Iteration 98/1000 | Loss: 0.00002707
Iteration 99/1000 | Loss: 0.00002707
Iteration 100/1000 | Loss: 0.00002707
Iteration 101/1000 | Loss: 0.00002707
Iteration 102/1000 | Loss: 0.00002706
Iteration 103/1000 | Loss: 0.00002706
Iteration 104/1000 | Loss: 0.00002706
Iteration 105/1000 | Loss: 0.00002706
Iteration 106/1000 | Loss: 0.00002706
Iteration 107/1000 | Loss: 0.00002706
Iteration 108/1000 | Loss: 0.00002706
Iteration 109/1000 | Loss: 0.00002706
Iteration 110/1000 | Loss: 0.00002706
Iteration 111/1000 | Loss: 0.00002705
Iteration 112/1000 | Loss: 0.00002705
Iteration 113/1000 | Loss: 0.00002705
Iteration 114/1000 | Loss: 0.00002705
Iteration 115/1000 | Loss: 0.00002705
Iteration 116/1000 | Loss: 0.00002705
Iteration 117/1000 | Loss: 0.00002705
Iteration 118/1000 | Loss: 0.00002704
Iteration 119/1000 | Loss: 0.00002704
Iteration 120/1000 | Loss: 0.00002704
Iteration 121/1000 | Loss: 0.00002704
Iteration 122/1000 | Loss: 0.00002704
Iteration 123/1000 | Loss: 0.00002704
Iteration 124/1000 | Loss: 0.00002704
Iteration 125/1000 | Loss: 0.00002704
Iteration 126/1000 | Loss: 0.00002704
Iteration 127/1000 | Loss: 0.00002704
Iteration 128/1000 | Loss: 0.00002704
Iteration 129/1000 | Loss: 0.00002704
Iteration 130/1000 | Loss: 0.00002704
Iteration 131/1000 | Loss: 0.00002704
Iteration 132/1000 | Loss: 0.00002704
Iteration 133/1000 | Loss: 0.00002704
Iteration 134/1000 | Loss: 0.00002704
Iteration 135/1000 | Loss: 0.00002704
Iteration 136/1000 | Loss: 0.00002704
Iteration 137/1000 | Loss: 0.00002704
Iteration 138/1000 | Loss: 0.00002703
Iteration 139/1000 | Loss: 0.00002703
Iteration 140/1000 | Loss: 0.00002703
Iteration 141/1000 | Loss: 0.00002703
Iteration 142/1000 | Loss: 0.00002703
Iteration 143/1000 | Loss: 0.00002703
Iteration 144/1000 | Loss: 0.00002703
Iteration 145/1000 | Loss: 0.00002703
Iteration 146/1000 | Loss: 0.00002703
Iteration 147/1000 | Loss: 0.00002703
Iteration 148/1000 | Loss: 0.00002703
Iteration 149/1000 | Loss: 0.00002703
Iteration 150/1000 | Loss: 0.00002703
Iteration 151/1000 | Loss: 0.00002703
Iteration 152/1000 | Loss: 0.00002703
Iteration 153/1000 | Loss: 0.00002703
Iteration 154/1000 | Loss: 0.00002703
Iteration 155/1000 | Loss: 0.00002703
Iteration 156/1000 | Loss: 0.00002703
Iteration 157/1000 | Loss: 0.00002703
Iteration 158/1000 | Loss: 0.00002703
Iteration 159/1000 | Loss: 0.00002703
Iteration 160/1000 | Loss: 0.00002703
Iteration 161/1000 | Loss: 0.00002703
Iteration 162/1000 | Loss: 0.00002703
Iteration 163/1000 | Loss: 0.00002703
Iteration 164/1000 | Loss: 0.00002703
Iteration 165/1000 | Loss: 0.00002703
Iteration 166/1000 | Loss: 0.00002703
Iteration 167/1000 | Loss: 0.00002703
Iteration 168/1000 | Loss: 0.00002703
Iteration 169/1000 | Loss: 0.00002703
Iteration 170/1000 | Loss: 0.00002703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.702528036024887e-05, 2.702528036024887e-05, 2.702528036024887e-05, 2.702528036024887e-05, 2.702528036024887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.702528036024887e-05

Optimization complete. Final v2v error: 4.427879333496094 mm

Highest mean error: 4.924740791320801 mm for frame 105

Lowest mean error: 4.007767677307129 mm for frame 14

Saving results

Total time: 43.23695421218872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458970
Iteration 2/25 | Loss: 0.00191259
Iteration 3/25 | Loss: 0.00184719
Iteration 4/25 | Loss: 0.00183953
Iteration 5/25 | Loss: 0.00183540
Iteration 6/25 | Loss: 0.00183461
Iteration 7/25 | Loss: 0.00183461
Iteration 8/25 | Loss: 0.00183461
Iteration 9/25 | Loss: 0.00183461
Iteration 10/25 | Loss: 0.00183461
Iteration 11/25 | Loss: 0.00183461
Iteration 12/25 | Loss: 0.00183461
Iteration 13/25 | Loss: 0.00183461
Iteration 14/25 | Loss: 0.00183461
Iteration 15/25 | Loss: 0.00183461
Iteration 16/25 | Loss: 0.00183461
Iteration 17/25 | Loss: 0.00183461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001834611757658422, 0.001834611757658422, 0.001834611757658422, 0.001834611757658422, 0.001834611757658422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001834611757658422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.21050406
Iteration 2/25 | Loss: 0.00267636
Iteration 3/25 | Loss: 0.00267635
Iteration 4/25 | Loss: 0.00267635
Iteration 5/25 | Loss: 0.00267635
Iteration 6/25 | Loss: 0.00267635
Iteration 7/25 | Loss: 0.00267635
Iteration 8/25 | Loss: 0.00267635
Iteration 9/25 | Loss: 0.00267635
Iteration 10/25 | Loss: 0.00267635
Iteration 11/25 | Loss: 0.00267635
Iteration 12/25 | Loss: 0.00267635
Iteration 13/25 | Loss: 0.00267635
Iteration 14/25 | Loss: 0.00267635
Iteration 15/25 | Loss: 0.00267635
Iteration 16/25 | Loss: 0.00267635
Iteration 17/25 | Loss: 0.00267635
Iteration 18/25 | Loss: 0.00267635
Iteration 19/25 | Loss: 0.00267635
Iteration 20/25 | Loss: 0.00267635
Iteration 21/25 | Loss: 0.00267635
Iteration 22/25 | Loss: 0.00267635
Iteration 23/25 | Loss: 0.00267635
Iteration 24/25 | Loss: 0.00267635
Iteration 25/25 | Loss: 0.00267635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267635
Iteration 2/1000 | Loss: 0.00005885
Iteration 3/1000 | Loss: 0.00003763
Iteration 4/1000 | Loss: 0.00003440
Iteration 5/1000 | Loss: 0.00003260
Iteration 6/1000 | Loss: 0.00003100
Iteration 7/1000 | Loss: 0.00003010
Iteration 8/1000 | Loss: 0.00002942
Iteration 9/1000 | Loss: 0.00002891
Iteration 10/1000 | Loss: 0.00002857
Iteration 11/1000 | Loss: 0.00002827
Iteration 12/1000 | Loss: 0.00002797
Iteration 13/1000 | Loss: 0.00002776
Iteration 14/1000 | Loss: 0.00002772
Iteration 15/1000 | Loss: 0.00002771
Iteration 16/1000 | Loss: 0.00002768
Iteration 17/1000 | Loss: 0.00002766
Iteration 18/1000 | Loss: 0.00002762
Iteration 19/1000 | Loss: 0.00002761
Iteration 20/1000 | Loss: 0.00002760
Iteration 21/1000 | Loss: 0.00002759
Iteration 22/1000 | Loss: 0.00002757
Iteration 23/1000 | Loss: 0.00002757
Iteration 24/1000 | Loss: 0.00002756
Iteration 25/1000 | Loss: 0.00002756
Iteration 26/1000 | Loss: 0.00002756
Iteration 27/1000 | Loss: 0.00002755
Iteration 28/1000 | Loss: 0.00002755
Iteration 29/1000 | Loss: 0.00002754
Iteration 30/1000 | Loss: 0.00002754
Iteration 31/1000 | Loss: 0.00002753
Iteration 32/1000 | Loss: 0.00002753
Iteration 33/1000 | Loss: 0.00002753
Iteration 34/1000 | Loss: 0.00002753
Iteration 35/1000 | Loss: 0.00002752
Iteration 36/1000 | Loss: 0.00002752
Iteration 37/1000 | Loss: 0.00002752
Iteration 38/1000 | Loss: 0.00002752
Iteration 39/1000 | Loss: 0.00002752
Iteration 40/1000 | Loss: 0.00002752
Iteration 41/1000 | Loss: 0.00002752
Iteration 42/1000 | Loss: 0.00002752
Iteration 43/1000 | Loss: 0.00002752
Iteration 44/1000 | Loss: 0.00002752
Iteration 45/1000 | Loss: 0.00002751
Iteration 46/1000 | Loss: 0.00002751
Iteration 47/1000 | Loss: 0.00002751
Iteration 48/1000 | Loss: 0.00002751
Iteration 49/1000 | Loss: 0.00002751
Iteration 50/1000 | Loss: 0.00002750
Iteration 51/1000 | Loss: 0.00002750
Iteration 52/1000 | Loss: 0.00002750
Iteration 53/1000 | Loss: 0.00002750
Iteration 54/1000 | Loss: 0.00002749
Iteration 55/1000 | Loss: 0.00002749
Iteration 56/1000 | Loss: 0.00002749
Iteration 57/1000 | Loss: 0.00002749
Iteration 58/1000 | Loss: 0.00002749
Iteration 59/1000 | Loss: 0.00002748
Iteration 60/1000 | Loss: 0.00002748
Iteration 61/1000 | Loss: 0.00002747
Iteration 62/1000 | Loss: 0.00002747
Iteration 63/1000 | Loss: 0.00002747
Iteration 64/1000 | Loss: 0.00002747
Iteration 65/1000 | Loss: 0.00002747
Iteration 66/1000 | Loss: 0.00002747
Iteration 67/1000 | Loss: 0.00002747
Iteration 68/1000 | Loss: 0.00002747
Iteration 69/1000 | Loss: 0.00002746
Iteration 70/1000 | Loss: 0.00002746
Iteration 71/1000 | Loss: 0.00002746
Iteration 72/1000 | Loss: 0.00002746
Iteration 73/1000 | Loss: 0.00002745
Iteration 74/1000 | Loss: 0.00002745
Iteration 75/1000 | Loss: 0.00002744
Iteration 76/1000 | Loss: 0.00002744
Iteration 77/1000 | Loss: 0.00002744
Iteration 78/1000 | Loss: 0.00002743
Iteration 79/1000 | Loss: 0.00002742
Iteration 80/1000 | Loss: 0.00002742
Iteration 81/1000 | Loss: 0.00002742
Iteration 82/1000 | Loss: 0.00002742
Iteration 83/1000 | Loss: 0.00002741
Iteration 84/1000 | Loss: 0.00002741
Iteration 85/1000 | Loss: 0.00002741
Iteration 86/1000 | Loss: 0.00002741
Iteration 87/1000 | Loss: 0.00002741
Iteration 88/1000 | Loss: 0.00002741
Iteration 89/1000 | Loss: 0.00002741
Iteration 90/1000 | Loss: 0.00002741
Iteration 91/1000 | Loss: 0.00002741
Iteration 92/1000 | Loss: 0.00002741
Iteration 93/1000 | Loss: 0.00002740
Iteration 94/1000 | Loss: 0.00002740
Iteration 95/1000 | Loss: 0.00002740
Iteration 96/1000 | Loss: 0.00002739
Iteration 97/1000 | Loss: 0.00002739
Iteration 98/1000 | Loss: 0.00002739
Iteration 99/1000 | Loss: 0.00002739
Iteration 100/1000 | Loss: 0.00002739
Iteration 101/1000 | Loss: 0.00002739
Iteration 102/1000 | Loss: 0.00002739
Iteration 103/1000 | Loss: 0.00002739
Iteration 104/1000 | Loss: 0.00002738
Iteration 105/1000 | Loss: 0.00002738
Iteration 106/1000 | Loss: 0.00002738
Iteration 107/1000 | Loss: 0.00002738
Iteration 108/1000 | Loss: 0.00002738
Iteration 109/1000 | Loss: 0.00002738
Iteration 110/1000 | Loss: 0.00002738
Iteration 111/1000 | Loss: 0.00002738
Iteration 112/1000 | Loss: 0.00002738
Iteration 113/1000 | Loss: 0.00002737
Iteration 114/1000 | Loss: 0.00002737
Iteration 115/1000 | Loss: 0.00002737
Iteration 116/1000 | Loss: 0.00002737
Iteration 117/1000 | Loss: 0.00002737
Iteration 118/1000 | Loss: 0.00002737
Iteration 119/1000 | Loss: 0.00002737
Iteration 120/1000 | Loss: 0.00002737
Iteration 121/1000 | Loss: 0.00002736
Iteration 122/1000 | Loss: 0.00002736
Iteration 123/1000 | Loss: 0.00002736
Iteration 124/1000 | Loss: 0.00002736
Iteration 125/1000 | Loss: 0.00002736
Iteration 126/1000 | Loss: 0.00002736
Iteration 127/1000 | Loss: 0.00002736
Iteration 128/1000 | Loss: 0.00002736
Iteration 129/1000 | Loss: 0.00002736
Iteration 130/1000 | Loss: 0.00002736
Iteration 131/1000 | Loss: 0.00002736
Iteration 132/1000 | Loss: 0.00002736
Iteration 133/1000 | Loss: 0.00002736
Iteration 134/1000 | Loss: 0.00002735
Iteration 135/1000 | Loss: 0.00002735
Iteration 136/1000 | Loss: 0.00002735
Iteration 137/1000 | Loss: 0.00002735
Iteration 138/1000 | Loss: 0.00002735
Iteration 139/1000 | Loss: 0.00002735
Iteration 140/1000 | Loss: 0.00002735
Iteration 141/1000 | Loss: 0.00002735
Iteration 142/1000 | Loss: 0.00002735
Iteration 143/1000 | Loss: 0.00002735
Iteration 144/1000 | Loss: 0.00002735
Iteration 145/1000 | Loss: 0.00002735
Iteration 146/1000 | Loss: 0.00002735
Iteration 147/1000 | Loss: 0.00002735
Iteration 148/1000 | Loss: 0.00002735
Iteration 149/1000 | Loss: 0.00002735
Iteration 150/1000 | Loss: 0.00002735
Iteration 151/1000 | Loss: 0.00002735
Iteration 152/1000 | Loss: 0.00002734
Iteration 153/1000 | Loss: 0.00002734
Iteration 154/1000 | Loss: 0.00002734
Iteration 155/1000 | Loss: 0.00002734
Iteration 156/1000 | Loss: 0.00002734
Iteration 157/1000 | Loss: 0.00002734
Iteration 158/1000 | Loss: 0.00002734
Iteration 159/1000 | Loss: 0.00002733
Iteration 160/1000 | Loss: 0.00002733
Iteration 161/1000 | Loss: 0.00002733
Iteration 162/1000 | Loss: 0.00002733
Iteration 163/1000 | Loss: 0.00002733
Iteration 164/1000 | Loss: 0.00002733
Iteration 165/1000 | Loss: 0.00002733
Iteration 166/1000 | Loss: 0.00002733
Iteration 167/1000 | Loss: 0.00002733
Iteration 168/1000 | Loss: 0.00002733
Iteration 169/1000 | Loss: 0.00002733
Iteration 170/1000 | Loss: 0.00002733
Iteration 171/1000 | Loss: 0.00002733
Iteration 172/1000 | Loss: 0.00002733
Iteration 173/1000 | Loss: 0.00002733
Iteration 174/1000 | Loss: 0.00002733
Iteration 175/1000 | Loss: 0.00002733
Iteration 176/1000 | Loss: 0.00002733
Iteration 177/1000 | Loss: 0.00002733
Iteration 178/1000 | Loss: 0.00002733
Iteration 179/1000 | Loss: 0.00002733
Iteration 180/1000 | Loss: 0.00002733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.7327454517944716e-05, 2.7327454517944716e-05, 2.7327454517944716e-05, 2.7327454517944716e-05, 2.7327454517944716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7327454517944716e-05

Optimization complete. Final v2v error: 4.430426597595215 mm

Highest mean error: 4.7108612060546875 mm for frame 44

Lowest mean error: 4.214524269104004 mm for frame 13

Saving results

Total time: 44.91599464416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101083
Iteration 2/25 | Loss: 0.01101083
Iteration 3/25 | Loss: 0.01101083
Iteration 4/25 | Loss: 0.01101083
Iteration 5/25 | Loss: 0.01101083
Iteration 6/25 | Loss: 0.01101082
Iteration 7/25 | Loss: 0.01101082
Iteration 8/25 | Loss: 0.01101082
Iteration 9/25 | Loss: 0.01101082
Iteration 10/25 | Loss: 0.01101082
Iteration 11/25 | Loss: 0.01101082
Iteration 12/25 | Loss: 0.01101082
Iteration 13/25 | Loss: 0.01101081
Iteration 14/25 | Loss: 0.01101081
Iteration 15/25 | Loss: 0.01101081
Iteration 16/25 | Loss: 0.01101081
Iteration 17/25 | Loss: 0.01101081
Iteration 18/25 | Loss: 0.01101081
Iteration 19/25 | Loss: 0.01101080
Iteration 20/25 | Loss: 0.01101080
Iteration 21/25 | Loss: 0.01101080
Iteration 22/25 | Loss: 0.01101080
Iteration 23/25 | Loss: 0.01101080
Iteration 24/25 | Loss: 0.01101080
Iteration 25/25 | Loss: 0.01101079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47776246
Iteration 2/25 | Loss: 0.10086506
Iteration 3/25 | Loss: 0.10086456
Iteration 4/25 | Loss: 0.10085490
Iteration 5/25 | Loss: 0.10085487
Iteration 6/25 | Loss: 0.10085487
Iteration 7/25 | Loss: 0.10085486
Iteration 8/25 | Loss: 0.10085486
Iteration 9/25 | Loss: 0.10085486
Iteration 10/25 | Loss: 0.10085486
Iteration 11/25 | Loss: 0.10085486
Iteration 12/25 | Loss: 0.10085486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.10085485875606537, 0.10085485875606537, 0.10085485875606537, 0.10085485875606537, 0.10085485875606537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10085485875606537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10085486
Iteration 2/1000 | Loss: 0.00295444
Iteration 3/1000 | Loss: 0.00157165
Iteration 4/1000 | Loss: 0.00063765
Iteration 5/1000 | Loss: 0.00142264
Iteration 6/1000 | Loss: 0.00046827
Iteration 7/1000 | Loss: 0.00019437
Iteration 8/1000 | Loss: 0.00022742
Iteration 9/1000 | Loss: 0.00052297
Iteration 10/1000 | Loss: 0.00015511
Iteration 11/1000 | Loss: 0.00042669
Iteration 12/1000 | Loss: 0.00010206
Iteration 13/1000 | Loss: 0.00009520
Iteration 14/1000 | Loss: 0.00007978
Iteration 15/1000 | Loss: 0.00006498
Iteration 16/1000 | Loss: 0.00007412
Iteration 17/1000 | Loss: 0.00007781
Iteration 18/1000 | Loss: 0.00023999
Iteration 19/1000 | Loss: 0.00006486
Iteration 20/1000 | Loss: 0.00038382
Iteration 21/1000 | Loss: 0.00019926
Iteration 22/1000 | Loss: 0.00007258
Iteration 23/1000 | Loss: 0.00010992
Iteration 24/1000 | Loss: 0.00007769
Iteration 25/1000 | Loss: 0.00026797
Iteration 26/1000 | Loss: 0.00016319
Iteration 27/1000 | Loss: 0.00008162
Iteration 28/1000 | Loss: 0.00006177
Iteration 29/1000 | Loss: 0.00012673
Iteration 30/1000 | Loss: 0.00017848
Iteration 31/1000 | Loss: 0.00007065
Iteration 32/1000 | Loss: 0.00003886
Iteration 33/1000 | Loss: 0.00009623
Iteration 34/1000 | Loss: 0.00012518
Iteration 35/1000 | Loss: 0.00006091
Iteration 36/1000 | Loss: 0.00006585
Iteration 37/1000 | Loss: 0.00003715
Iteration 38/1000 | Loss: 0.00010456
Iteration 39/1000 | Loss: 0.00024654
Iteration 40/1000 | Loss: 0.00005167
Iteration 41/1000 | Loss: 0.00004034
Iteration 42/1000 | Loss: 0.00003605
Iteration 43/1000 | Loss: 0.00003574
Iteration 44/1000 | Loss: 0.00020538
Iteration 45/1000 | Loss: 0.00006774
Iteration 46/1000 | Loss: 0.00010951
Iteration 47/1000 | Loss: 0.00003550
Iteration 48/1000 | Loss: 0.00003526
Iteration 49/1000 | Loss: 0.00003516
Iteration 50/1000 | Loss: 0.00003501
Iteration 51/1000 | Loss: 0.00003495
Iteration 52/1000 | Loss: 0.00003477
Iteration 53/1000 | Loss: 0.00006175
Iteration 54/1000 | Loss: 0.00003457
Iteration 55/1000 | Loss: 0.00003456
Iteration 56/1000 | Loss: 0.00003456
Iteration 57/1000 | Loss: 0.00003456
Iteration 58/1000 | Loss: 0.00003456
Iteration 59/1000 | Loss: 0.00006679
Iteration 60/1000 | Loss: 0.00030736
Iteration 61/1000 | Loss: 0.00015929
Iteration 62/1000 | Loss: 0.00017211
Iteration 63/1000 | Loss: 0.00019521
Iteration 64/1000 | Loss: 0.00017036
Iteration 65/1000 | Loss: 0.00007332
Iteration 66/1000 | Loss: 0.00019917
Iteration 67/1000 | Loss: 0.00016467
Iteration 68/1000 | Loss: 0.00018409
Iteration 69/1000 | Loss: 0.00010160
Iteration 70/1000 | Loss: 0.00008239
Iteration 71/1000 | Loss: 0.00005505
Iteration 72/1000 | Loss: 0.00005807
Iteration 73/1000 | Loss: 0.00003886
Iteration 74/1000 | Loss: 0.00003660
Iteration 75/1000 | Loss: 0.00003450
Iteration 76/1000 | Loss: 0.00008662
Iteration 77/1000 | Loss: 0.00003287
Iteration 78/1000 | Loss: 0.00009143
Iteration 79/1000 | Loss: 0.00003221
Iteration 80/1000 | Loss: 0.00006005
Iteration 81/1000 | Loss: 0.00004049
Iteration 82/1000 | Loss: 0.00003219
Iteration 83/1000 | Loss: 0.00003186
Iteration 84/1000 | Loss: 0.00006901
Iteration 85/1000 | Loss: 0.00003187
Iteration 86/1000 | Loss: 0.00003648
Iteration 87/1000 | Loss: 0.00003199
Iteration 88/1000 | Loss: 0.00003165
Iteration 89/1000 | Loss: 0.00003164
Iteration 90/1000 | Loss: 0.00003164
Iteration 91/1000 | Loss: 0.00003164
Iteration 92/1000 | Loss: 0.00003164
Iteration 93/1000 | Loss: 0.00003164
Iteration 94/1000 | Loss: 0.00003164
Iteration 95/1000 | Loss: 0.00003164
Iteration 96/1000 | Loss: 0.00003164
Iteration 97/1000 | Loss: 0.00003164
Iteration 98/1000 | Loss: 0.00005775
Iteration 99/1000 | Loss: 0.00009341
Iteration 100/1000 | Loss: 0.00082334
Iteration 101/1000 | Loss: 0.00003358
Iteration 102/1000 | Loss: 0.00003228
Iteration 103/1000 | Loss: 0.00003167
Iteration 104/1000 | Loss: 0.00003162
Iteration 105/1000 | Loss: 0.00003152
Iteration 106/1000 | Loss: 0.00003152
Iteration 107/1000 | Loss: 0.00003151
Iteration 108/1000 | Loss: 0.00003150
Iteration 109/1000 | Loss: 0.00003150
Iteration 110/1000 | Loss: 0.00003149
Iteration 111/1000 | Loss: 0.00003149
Iteration 112/1000 | Loss: 0.00003149
Iteration 113/1000 | Loss: 0.00003149
Iteration 114/1000 | Loss: 0.00003149
Iteration 115/1000 | Loss: 0.00003149
Iteration 116/1000 | Loss: 0.00003149
Iteration 117/1000 | Loss: 0.00003149
Iteration 118/1000 | Loss: 0.00003149
Iteration 119/1000 | Loss: 0.00003149
Iteration 120/1000 | Loss: 0.00003149
Iteration 121/1000 | Loss: 0.00003148
Iteration 122/1000 | Loss: 0.00003148
Iteration 123/1000 | Loss: 0.00003148
Iteration 124/1000 | Loss: 0.00003148
Iteration 125/1000 | Loss: 0.00003148
Iteration 126/1000 | Loss: 0.00003147
Iteration 127/1000 | Loss: 0.00003147
Iteration 128/1000 | Loss: 0.00003147
Iteration 129/1000 | Loss: 0.00003147
Iteration 130/1000 | Loss: 0.00003147
Iteration 131/1000 | Loss: 0.00003147
Iteration 132/1000 | Loss: 0.00003147
Iteration 133/1000 | Loss: 0.00003147
Iteration 134/1000 | Loss: 0.00003147
Iteration 135/1000 | Loss: 0.00003147
Iteration 136/1000 | Loss: 0.00003147
Iteration 137/1000 | Loss: 0.00003147
Iteration 138/1000 | Loss: 0.00003147
Iteration 139/1000 | Loss: 0.00003147
Iteration 140/1000 | Loss: 0.00003147
Iteration 141/1000 | Loss: 0.00003147
Iteration 142/1000 | Loss: 0.00003147
Iteration 143/1000 | Loss: 0.00003147
Iteration 144/1000 | Loss: 0.00003147
Iteration 145/1000 | Loss: 0.00003147
Iteration 146/1000 | Loss: 0.00003147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.146800372633152e-05, 3.146800372633152e-05, 3.146800372633152e-05, 3.146800372633152e-05, 3.146800372633152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.146800372633152e-05

Optimization complete. Final v2v error: 4.547927379608154 mm

Highest mean error: 11.795763969421387 mm for frame 60

Lowest mean error: 3.7704970836639404 mm for frame 153

Saving results

Total time: 149.70893239974976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060291
Iteration 2/25 | Loss: 0.00381267
Iteration 3/25 | Loss: 0.00261476
Iteration 4/25 | Loss: 0.00242959
Iteration 5/25 | Loss: 0.00210724
Iteration 6/25 | Loss: 0.00195086
Iteration 7/25 | Loss: 0.00171260
Iteration 8/25 | Loss: 0.00161796
Iteration 9/25 | Loss: 0.00159623
Iteration 10/25 | Loss: 0.00155087
Iteration 11/25 | Loss: 0.00154419
Iteration 12/25 | Loss: 0.00152365
Iteration 13/25 | Loss: 0.00152666
Iteration 14/25 | Loss: 0.00150956
Iteration 15/25 | Loss: 0.00149809
Iteration 16/25 | Loss: 0.00149430
Iteration 17/25 | Loss: 0.00149420
Iteration 18/25 | Loss: 0.00149600
Iteration 19/25 | Loss: 0.00149511
Iteration 20/25 | Loss: 0.00149294
Iteration 21/25 | Loss: 0.00149471
Iteration 22/25 | Loss: 0.00149281
Iteration 23/25 | Loss: 0.00149281
Iteration 24/25 | Loss: 0.00149281
Iteration 25/25 | Loss: 0.00149281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20545423
Iteration 2/25 | Loss: 0.00298292
Iteration 3/25 | Loss: 0.00285514
Iteration 4/25 | Loss: 0.00285514
Iteration 5/25 | Loss: 0.00285514
Iteration 6/25 | Loss: 0.00285514
Iteration 7/25 | Loss: 0.00285514
Iteration 8/25 | Loss: 0.00285514
Iteration 9/25 | Loss: 0.00285514
Iteration 10/25 | Loss: 0.00285514
Iteration 11/25 | Loss: 0.00285514
Iteration 12/25 | Loss: 0.00285514
Iteration 13/25 | Loss: 0.00285514
Iteration 14/25 | Loss: 0.00285514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002855139784514904, 0.002855139784514904, 0.002855139784514904, 0.002855139784514904, 0.002855139784514904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002855139784514904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285514
Iteration 2/1000 | Loss: 0.00036867
Iteration 3/1000 | Loss: 0.00033816
Iteration 4/1000 | Loss: 0.00027075
Iteration 5/1000 | Loss: 0.00012875
Iteration 6/1000 | Loss: 0.00026900
Iteration 7/1000 | Loss: 0.00014498
Iteration 8/1000 | Loss: 0.00014920
Iteration 9/1000 | Loss: 0.00013714
Iteration 10/1000 | Loss: 0.00013274
Iteration 11/1000 | Loss: 0.00011019
Iteration 12/1000 | Loss: 0.00011245
Iteration 13/1000 | Loss: 0.00013475
Iteration 14/1000 | Loss: 0.00105764
Iteration 15/1000 | Loss: 0.00118094
Iteration 16/1000 | Loss: 0.00122682
Iteration 17/1000 | Loss: 0.00037536
Iteration 18/1000 | Loss: 0.00043035
Iteration 19/1000 | Loss: 0.00030408
Iteration 20/1000 | Loss: 0.00010955
Iteration 21/1000 | Loss: 0.00011684
Iteration 22/1000 | Loss: 0.00021217
Iteration 23/1000 | Loss: 0.00009948
Iteration 24/1000 | Loss: 0.00015842
Iteration 25/1000 | Loss: 0.00010059
Iteration 26/1000 | Loss: 0.00011305
Iteration 27/1000 | Loss: 0.00009475
Iteration 28/1000 | Loss: 0.00010392
Iteration 29/1000 | Loss: 0.00014171
Iteration 30/1000 | Loss: 0.00009877
Iteration 31/1000 | Loss: 0.00009072
Iteration 32/1000 | Loss: 0.00009495
Iteration 33/1000 | Loss: 0.00009605
Iteration 34/1000 | Loss: 0.00009271
Iteration 35/1000 | Loss: 0.00009518
Iteration 36/1000 | Loss: 0.00008947
Iteration 37/1000 | Loss: 0.00010197
Iteration 38/1000 | Loss: 0.00009152
Iteration 39/1000 | Loss: 0.00009586
Iteration 40/1000 | Loss: 0.00008901
Iteration 41/1000 | Loss: 0.00009105
Iteration 42/1000 | Loss: 0.00009095
Iteration 43/1000 | Loss: 0.00010170
Iteration 44/1000 | Loss: 0.00011127
Iteration 45/1000 | Loss: 0.00009250
Iteration 46/1000 | Loss: 0.00008909
Iteration 47/1000 | Loss: 0.00008862
Iteration 48/1000 | Loss: 0.00008862
Iteration 49/1000 | Loss: 0.00008862
Iteration 50/1000 | Loss: 0.00008861
Iteration 51/1000 | Loss: 0.00008861
Iteration 52/1000 | Loss: 0.00008861
Iteration 53/1000 | Loss: 0.00008861
Iteration 54/1000 | Loss: 0.00008861
Iteration 55/1000 | Loss: 0.00008861
Iteration 56/1000 | Loss: 0.00008861
Iteration 57/1000 | Loss: 0.00008861
Iteration 58/1000 | Loss: 0.00008861
Iteration 59/1000 | Loss: 0.00008861
Iteration 60/1000 | Loss: 0.00008861
Iteration 61/1000 | Loss: 0.00008861
Iteration 62/1000 | Loss: 0.00008861
Iteration 63/1000 | Loss: 0.00008861
Iteration 64/1000 | Loss: 0.00008861
Iteration 65/1000 | Loss: 0.00008860
Iteration 66/1000 | Loss: 0.00008860
Iteration 67/1000 | Loss: 0.00008860
Iteration 68/1000 | Loss: 0.00008860
Iteration 69/1000 | Loss: 0.00008860
Iteration 70/1000 | Loss: 0.00008860
Iteration 71/1000 | Loss: 0.00008860
Iteration 72/1000 | Loss: 0.00008860
Iteration 73/1000 | Loss: 0.00008860
Iteration 74/1000 | Loss: 0.00008860
Iteration 75/1000 | Loss: 0.00008860
Iteration 76/1000 | Loss: 0.00008895
Iteration 77/1000 | Loss: 0.00008978
Iteration 78/1000 | Loss: 0.00008854
Iteration 79/1000 | Loss: 0.00008854
Iteration 80/1000 | Loss: 0.00008853
Iteration 81/1000 | Loss: 0.00008852
Iteration 82/1000 | Loss: 0.00008852
Iteration 83/1000 | Loss: 0.00008852
Iteration 84/1000 | Loss: 0.00008852
Iteration 85/1000 | Loss: 0.00008852
Iteration 86/1000 | Loss: 0.00008852
Iteration 87/1000 | Loss: 0.00008852
Iteration 88/1000 | Loss: 0.00008852
Iteration 89/1000 | Loss: 0.00008852
Iteration 90/1000 | Loss: 0.00008852
Iteration 91/1000 | Loss: 0.00008852
Iteration 92/1000 | Loss: 0.00008852
Iteration 93/1000 | Loss: 0.00008852
Iteration 94/1000 | Loss: 0.00008852
Iteration 95/1000 | Loss: 0.00008852
Iteration 96/1000 | Loss: 0.00008851
Iteration 97/1000 | Loss: 0.00008851
Iteration 98/1000 | Loss: 0.00008851
Iteration 99/1000 | Loss: 0.00008851
Iteration 100/1000 | Loss: 0.00008851
Iteration 101/1000 | Loss: 0.00008851
Iteration 102/1000 | Loss: 0.00008851
Iteration 103/1000 | Loss: 0.00008851
Iteration 104/1000 | Loss: 0.00008851
Iteration 105/1000 | Loss: 0.00008851
Iteration 106/1000 | Loss: 0.00008851
Iteration 107/1000 | Loss: 0.00008850
Iteration 108/1000 | Loss: 0.00009093
Iteration 109/1000 | Loss: 0.00009312
Iteration 110/1000 | Loss: 0.00013656
Iteration 111/1000 | Loss: 0.00008916
Iteration 112/1000 | Loss: 0.00009574
Iteration 113/1000 | Loss: 0.00008932
Iteration 114/1000 | Loss: 0.00009418
Iteration 115/1000 | Loss: 0.00008896
Iteration 116/1000 | Loss: 0.00008849
Iteration 117/1000 | Loss: 0.00008849
Iteration 118/1000 | Loss: 0.00008849
Iteration 119/1000 | Loss: 0.00008845
Iteration 120/1000 | Loss: 0.00009392
Iteration 121/1000 | Loss: 0.00008852
Iteration 122/1000 | Loss: 0.00008838
Iteration 123/1000 | Loss: 0.00008837
Iteration 124/1000 | Loss: 0.00008837
Iteration 125/1000 | Loss: 0.00008837
Iteration 126/1000 | Loss: 0.00008837
Iteration 127/1000 | Loss: 0.00008837
Iteration 128/1000 | Loss: 0.00008837
Iteration 129/1000 | Loss: 0.00008837
Iteration 130/1000 | Loss: 0.00008837
Iteration 131/1000 | Loss: 0.00008837
Iteration 132/1000 | Loss: 0.00008837
Iteration 133/1000 | Loss: 0.00008837
Iteration 134/1000 | Loss: 0.00008837
Iteration 135/1000 | Loss: 0.00008837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [8.836823690216988e-05, 8.836823690216988e-05, 8.836823690216988e-05, 8.836823690216988e-05, 8.836823690216988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.836823690216988e-05

Optimization complete. Final v2v error: 4.986898899078369 mm

Highest mean error: 11.21026611328125 mm for frame 25

Lowest mean error: 3.0147132873535156 mm for frame 67

Saving results

Total time: 116.73485493659973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585435
Iteration 2/25 | Loss: 0.00195037
Iteration 3/25 | Loss: 0.00187074
Iteration 4/25 | Loss: 0.00186245
Iteration 5/25 | Loss: 0.00185784
Iteration 6/25 | Loss: 0.00185726
Iteration 7/25 | Loss: 0.00185726
Iteration 8/25 | Loss: 0.00185726
Iteration 9/25 | Loss: 0.00185726
Iteration 10/25 | Loss: 0.00185726
Iteration 11/25 | Loss: 0.00185726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018572554690763354, 0.0018572554690763354, 0.0018572554690763354, 0.0018572554690763354, 0.0018572554690763354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018572554690763354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.07884026
Iteration 2/25 | Loss: 0.00207287
Iteration 3/25 | Loss: 0.00207286
Iteration 4/25 | Loss: 0.00207286
Iteration 5/25 | Loss: 0.00207286
Iteration 6/25 | Loss: 0.00207286
Iteration 7/25 | Loss: 0.00207286
Iteration 8/25 | Loss: 0.00207286
Iteration 9/25 | Loss: 0.00207286
Iteration 10/25 | Loss: 0.00207286
Iteration 11/25 | Loss: 0.00207286
Iteration 12/25 | Loss: 0.00207286
Iteration 13/25 | Loss: 0.00207286
Iteration 14/25 | Loss: 0.00207286
Iteration 15/25 | Loss: 0.00207286
Iteration 16/25 | Loss: 0.00207286
Iteration 17/25 | Loss: 0.00207286
Iteration 18/25 | Loss: 0.00207286
Iteration 19/25 | Loss: 0.00207286
Iteration 20/25 | Loss: 0.00207286
Iteration 21/25 | Loss: 0.00207286
Iteration 22/25 | Loss: 0.00207286
Iteration 23/25 | Loss: 0.00207286
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020728567615151405, 0.0020728567615151405, 0.0020728567615151405, 0.0020728567615151405, 0.0020728567615151405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020728567615151405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207286
Iteration 2/1000 | Loss: 0.00005712
Iteration 3/1000 | Loss: 0.00003655
Iteration 4/1000 | Loss: 0.00003338
Iteration 5/1000 | Loss: 0.00003206
Iteration 6/1000 | Loss: 0.00003103
Iteration 7/1000 | Loss: 0.00003050
Iteration 8/1000 | Loss: 0.00003006
Iteration 9/1000 | Loss: 0.00002965
Iteration 10/1000 | Loss: 0.00002932
Iteration 11/1000 | Loss: 0.00002908
Iteration 12/1000 | Loss: 0.00002906
Iteration 13/1000 | Loss: 0.00002885
Iteration 14/1000 | Loss: 0.00002879
Iteration 15/1000 | Loss: 0.00002866
Iteration 16/1000 | Loss: 0.00002861
Iteration 17/1000 | Loss: 0.00002861
Iteration 18/1000 | Loss: 0.00002860
Iteration 19/1000 | Loss: 0.00002860
Iteration 20/1000 | Loss: 0.00002859
Iteration 21/1000 | Loss: 0.00002859
Iteration 22/1000 | Loss: 0.00002857
Iteration 23/1000 | Loss: 0.00002857
Iteration 24/1000 | Loss: 0.00002856
Iteration 25/1000 | Loss: 0.00002856
Iteration 26/1000 | Loss: 0.00002856
Iteration 27/1000 | Loss: 0.00002855
Iteration 28/1000 | Loss: 0.00002855
Iteration 29/1000 | Loss: 0.00002854
Iteration 30/1000 | Loss: 0.00002851
Iteration 31/1000 | Loss: 0.00002851
Iteration 32/1000 | Loss: 0.00002851
Iteration 33/1000 | Loss: 0.00002851
Iteration 34/1000 | Loss: 0.00002851
Iteration 35/1000 | Loss: 0.00002850
Iteration 36/1000 | Loss: 0.00002850
Iteration 37/1000 | Loss: 0.00002850
Iteration 38/1000 | Loss: 0.00002850
Iteration 39/1000 | Loss: 0.00002850
Iteration 40/1000 | Loss: 0.00002849
Iteration 41/1000 | Loss: 0.00002849
Iteration 42/1000 | Loss: 0.00002848
Iteration 43/1000 | Loss: 0.00002848
Iteration 44/1000 | Loss: 0.00002847
Iteration 45/1000 | Loss: 0.00002847
Iteration 46/1000 | Loss: 0.00002847
Iteration 47/1000 | Loss: 0.00002847
Iteration 48/1000 | Loss: 0.00002847
Iteration 49/1000 | Loss: 0.00002847
Iteration 50/1000 | Loss: 0.00002847
Iteration 51/1000 | Loss: 0.00002847
Iteration 52/1000 | Loss: 0.00002847
Iteration 53/1000 | Loss: 0.00002847
Iteration 54/1000 | Loss: 0.00002847
Iteration 55/1000 | Loss: 0.00002846
Iteration 56/1000 | Loss: 0.00002846
Iteration 57/1000 | Loss: 0.00002846
Iteration 58/1000 | Loss: 0.00002846
Iteration 59/1000 | Loss: 0.00002846
Iteration 60/1000 | Loss: 0.00002846
Iteration 61/1000 | Loss: 0.00002846
Iteration 62/1000 | Loss: 0.00002846
Iteration 63/1000 | Loss: 0.00002845
Iteration 64/1000 | Loss: 0.00002845
Iteration 65/1000 | Loss: 0.00002845
Iteration 66/1000 | Loss: 0.00002845
Iteration 67/1000 | Loss: 0.00002845
Iteration 68/1000 | Loss: 0.00002844
Iteration 69/1000 | Loss: 0.00002844
Iteration 70/1000 | Loss: 0.00002844
Iteration 71/1000 | Loss: 0.00002844
Iteration 72/1000 | Loss: 0.00002844
Iteration 73/1000 | Loss: 0.00002844
Iteration 74/1000 | Loss: 0.00002844
Iteration 75/1000 | Loss: 0.00002843
Iteration 76/1000 | Loss: 0.00002843
Iteration 77/1000 | Loss: 0.00002843
Iteration 78/1000 | Loss: 0.00002843
Iteration 79/1000 | Loss: 0.00002843
Iteration 80/1000 | Loss: 0.00002843
Iteration 81/1000 | Loss: 0.00002843
Iteration 82/1000 | Loss: 0.00002842
Iteration 83/1000 | Loss: 0.00002842
Iteration 84/1000 | Loss: 0.00002842
Iteration 85/1000 | Loss: 0.00002842
Iteration 86/1000 | Loss: 0.00002842
Iteration 87/1000 | Loss: 0.00002842
Iteration 88/1000 | Loss: 0.00002842
Iteration 89/1000 | Loss: 0.00002842
Iteration 90/1000 | Loss: 0.00002841
Iteration 91/1000 | Loss: 0.00002841
Iteration 92/1000 | Loss: 0.00002841
Iteration 93/1000 | Loss: 0.00002841
Iteration 94/1000 | Loss: 0.00002841
Iteration 95/1000 | Loss: 0.00002841
Iteration 96/1000 | Loss: 0.00002841
Iteration 97/1000 | Loss: 0.00002841
Iteration 98/1000 | Loss: 0.00002840
Iteration 99/1000 | Loss: 0.00002840
Iteration 100/1000 | Loss: 0.00002840
Iteration 101/1000 | Loss: 0.00002840
Iteration 102/1000 | Loss: 0.00002840
Iteration 103/1000 | Loss: 0.00002840
Iteration 104/1000 | Loss: 0.00002840
Iteration 105/1000 | Loss: 0.00002840
Iteration 106/1000 | Loss: 0.00002840
Iteration 107/1000 | Loss: 0.00002840
Iteration 108/1000 | Loss: 0.00002840
Iteration 109/1000 | Loss: 0.00002840
Iteration 110/1000 | Loss: 0.00002839
Iteration 111/1000 | Loss: 0.00002839
Iteration 112/1000 | Loss: 0.00002839
Iteration 113/1000 | Loss: 0.00002839
Iteration 114/1000 | Loss: 0.00002839
Iteration 115/1000 | Loss: 0.00002839
Iteration 116/1000 | Loss: 0.00002839
Iteration 117/1000 | Loss: 0.00002839
Iteration 118/1000 | Loss: 0.00002839
Iteration 119/1000 | Loss: 0.00002839
Iteration 120/1000 | Loss: 0.00002839
Iteration 121/1000 | Loss: 0.00002839
Iteration 122/1000 | Loss: 0.00002839
Iteration 123/1000 | Loss: 0.00002839
Iteration 124/1000 | Loss: 0.00002839
Iteration 125/1000 | Loss: 0.00002839
Iteration 126/1000 | Loss: 0.00002839
Iteration 127/1000 | Loss: 0.00002839
Iteration 128/1000 | Loss: 0.00002839
Iteration 129/1000 | Loss: 0.00002839
Iteration 130/1000 | Loss: 0.00002838
Iteration 131/1000 | Loss: 0.00002838
Iteration 132/1000 | Loss: 0.00002838
Iteration 133/1000 | Loss: 0.00002838
Iteration 134/1000 | Loss: 0.00002838
Iteration 135/1000 | Loss: 0.00002838
Iteration 136/1000 | Loss: 0.00002838
Iteration 137/1000 | Loss: 0.00002838
Iteration 138/1000 | Loss: 0.00002838
Iteration 139/1000 | Loss: 0.00002838
Iteration 140/1000 | Loss: 0.00002838
Iteration 141/1000 | Loss: 0.00002838
Iteration 142/1000 | Loss: 0.00002838
Iteration 143/1000 | Loss: 0.00002838
Iteration 144/1000 | Loss: 0.00002838
Iteration 145/1000 | Loss: 0.00002838
Iteration 146/1000 | Loss: 0.00002838
Iteration 147/1000 | Loss: 0.00002838
Iteration 148/1000 | Loss: 0.00002838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.8383645258145407e-05, 2.8383645258145407e-05, 2.8383645258145407e-05, 2.8383645258145407e-05, 2.8383645258145407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8383645258145407e-05

Optimization complete. Final v2v error: 4.4984846115112305 mm

Highest mean error: 4.85124397277832 mm for frame 129

Lowest mean error: 4.188149452209473 mm for frame 20

Saving results

Total time: 39.77637028694153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_2542/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_2542/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116382
Iteration 2/25 | Loss: 0.00265141
Iteration 3/25 | Loss: 0.00234936
Iteration 4/25 | Loss: 0.00228539
Iteration 5/25 | Loss: 0.00224664
Iteration 6/25 | Loss: 0.00223376
Iteration 7/25 | Loss: 0.00222963
Iteration 8/25 | Loss: 0.00222463
Iteration 9/25 | Loss: 0.00222771
Iteration 10/25 | Loss: 0.00222247
Iteration 11/25 | Loss: 0.00221894
Iteration 12/25 | Loss: 0.00221754
Iteration 13/25 | Loss: 0.00221667
Iteration 14/25 | Loss: 0.00221622
Iteration 15/25 | Loss: 0.00221578
Iteration 16/25 | Loss: 0.00221520
Iteration 17/25 | Loss: 0.00221428
Iteration 18/25 | Loss: 0.00221235
Iteration 19/25 | Loss: 0.00221011
Iteration 20/25 | Loss: 0.00220780
Iteration 21/25 | Loss: 0.00220535
Iteration 22/25 | Loss: 0.00220321
Iteration 23/25 | Loss: 0.00220137
Iteration 24/25 | Loss: 0.00219922
Iteration 25/25 | Loss: 0.00220000

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22038639
Iteration 2/25 | Loss: 0.00653039
Iteration 3/25 | Loss: 0.00653039
Iteration 4/25 | Loss: 0.00653039
Iteration 5/25 | Loss: 0.00653039
Iteration 6/25 | Loss: 0.00653039
Iteration 7/25 | Loss: 0.00653039
Iteration 8/25 | Loss: 0.00653039
Iteration 9/25 | Loss: 0.00653039
Iteration 10/25 | Loss: 0.00653039
Iteration 11/25 | Loss: 0.00653039
Iteration 12/25 | Loss: 0.00653039
Iteration 13/25 | Loss: 0.00653039
Iteration 14/25 | Loss: 0.00653039
Iteration 15/25 | Loss: 0.00653039
Iteration 16/25 | Loss: 0.00653039
Iteration 17/25 | Loss: 0.00653039
Iteration 18/25 | Loss: 0.00653039
Iteration 19/25 | Loss: 0.00653039
Iteration 20/25 | Loss: 0.00653039
Iteration 21/25 | Loss: 0.00653039
Iteration 22/25 | Loss: 0.00653039
Iteration 23/25 | Loss: 0.00653039
Iteration 24/25 | Loss: 0.00653039
Iteration 25/25 | Loss: 0.00653039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00653039
Iteration 2/1000 | Loss: 0.00390805
Iteration 3/1000 | Loss: 0.00301881
Iteration 4/1000 | Loss: 0.00172490
Iteration 5/1000 | Loss: 0.00398987
Iteration 6/1000 | Loss: 0.00340821
Iteration 7/1000 | Loss: 0.00280927
Iteration 8/1000 | Loss: 0.00114644
Iteration 9/1000 | Loss: 0.00081244
Iteration 10/1000 | Loss: 0.00152006
Iteration 11/1000 | Loss: 0.00092321
Iteration 12/1000 | Loss: 0.00113585
Iteration 13/1000 | Loss: 0.00035794
Iteration 14/1000 | Loss: 0.00127237
Iteration 15/1000 | Loss: 0.00109243
Iteration 16/1000 | Loss: 0.00107163
Iteration 17/1000 | Loss: 0.00152308
Iteration 18/1000 | Loss: 0.00062918
Iteration 19/1000 | Loss: 0.00022013
Iteration 20/1000 | Loss: 0.00021943
Iteration 21/1000 | Loss: 0.00028486
Iteration 22/1000 | Loss: 0.00012418
Iteration 23/1000 | Loss: 0.00054535
Iteration 24/1000 | Loss: 0.00117461
Iteration 25/1000 | Loss: 0.00068197
Iteration 26/1000 | Loss: 0.00029611
Iteration 27/1000 | Loss: 0.00019034
Iteration 28/1000 | Loss: 0.00035205
Iteration 29/1000 | Loss: 0.00069026
Iteration 30/1000 | Loss: 0.00019643
Iteration 31/1000 | Loss: 0.00017539
Iteration 32/1000 | Loss: 0.00066238
Iteration 33/1000 | Loss: 0.00028334
Iteration 34/1000 | Loss: 0.00037072
Iteration 35/1000 | Loss: 0.00017877
Iteration 36/1000 | Loss: 0.00085097
Iteration 37/1000 | Loss: 0.00058461
Iteration 38/1000 | Loss: 0.00030434
Iteration 39/1000 | Loss: 0.00037464
Iteration 40/1000 | Loss: 0.00029752
Iteration 41/1000 | Loss: 0.00069522
Iteration 42/1000 | Loss: 0.00063304
Iteration 43/1000 | Loss: 0.00024502
Iteration 44/1000 | Loss: 0.00060645
Iteration 45/1000 | Loss: 0.00023531
Iteration 46/1000 | Loss: 0.00041407
Iteration 47/1000 | Loss: 0.00063787
Iteration 48/1000 | Loss: 0.00052424
Iteration 49/1000 | Loss: 0.00022938
Iteration 50/1000 | Loss: 0.00017909
Iteration 51/1000 | Loss: 0.00026637
Iteration 52/1000 | Loss: 0.00007876
Iteration 53/1000 | Loss: 0.00043482
Iteration 54/1000 | Loss: 0.00015034
Iteration 55/1000 | Loss: 0.00012862
Iteration 56/1000 | Loss: 0.00016830
Iteration 57/1000 | Loss: 0.00017260
Iteration 58/1000 | Loss: 0.00036225
Iteration 59/1000 | Loss: 0.00031369
Iteration 60/1000 | Loss: 0.00027592
Iteration 61/1000 | Loss: 0.00058234
Iteration 62/1000 | Loss: 0.00048294
Iteration 63/1000 | Loss: 0.00018201
Iteration 64/1000 | Loss: 0.00007673
Iteration 65/1000 | Loss: 0.00014509
Iteration 66/1000 | Loss: 0.00007780
Iteration 67/1000 | Loss: 0.00016339
Iteration 68/1000 | Loss: 0.00009413
Iteration 69/1000 | Loss: 0.00012114
Iteration 70/1000 | Loss: 0.00013071
Iteration 71/1000 | Loss: 0.00009700
Iteration 72/1000 | Loss: 0.00007927
Iteration 73/1000 | Loss: 0.00005880
Iteration 74/1000 | Loss: 0.00005741
Iteration 75/1000 | Loss: 0.00027685
Iteration 76/1000 | Loss: 0.00030358
Iteration 77/1000 | Loss: 0.00015212
Iteration 78/1000 | Loss: 0.00006374
Iteration 79/1000 | Loss: 0.00005650
Iteration 80/1000 | Loss: 0.00005554
Iteration 81/1000 | Loss: 0.00005496
Iteration 82/1000 | Loss: 0.00072740
Iteration 83/1000 | Loss: 0.00035578
Iteration 84/1000 | Loss: 0.00062563
Iteration 85/1000 | Loss: 0.00046012
Iteration 86/1000 | Loss: 0.00046605
Iteration 87/1000 | Loss: 0.00067990
Iteration 88/1000 | Loss: 0.00037375
Iteration 89/1000 | Loss: 0.00178328
Iteration 90/1000 | Loss: 0.00132729
Iteration 91/1000 | Loss: 0.00081983
Iteration 92/1000 | Loss: 0.00091493
Iteration 93/1000 | Loss: 0.00064514
Iteration 94/1000 | Loss: 0.00029885
Iteration 95/1000 | Loss: 0.00026441
Iteration 96/1000 | Loss: 0.00031967
Iteration 97/1000 | Loss: 0.00034398
Iteration 98/1000 | Loss: 0.00017705
Iteration 99/1000 | Loss: 0.00022459
Iteration 100/1000 | Loss: 0.00023442
Iteration 101/1000 | Loss: 0.00036408
Iteration 102/1000 | Loss: 0.00055949
Iteration 103/1000 | Loss: 0.00063168
Iteration 104/1000 | Loss: 0.00063303
Iteration 105/1000 | Loss: 0.00010047
Iteration 106/1000 | Loss: 0.00027846
Iteration 107/1000 | Loss: 0.00023733
Iteration 108/1000 | Loss: 0.00029579
Iteration 109/1000 | Loss: 0.00009953
Iteration 110/1000 | Loss: 0.00025834
Iteration 111/1000 | Loss: 0.00098324
Iteration 112/1000 | Loss: 0.00049765
Iteration 113/1000 | Loss: 0.00049237
Iteration 114/1000 | Loss: 0.00070624
Iteration 115/1000 | Loss: 0.00067781
Iteration 116/1000 | Loss: 0.00028473
Iteration 117/1000 | Loss: 0.00027671
Iteration 118/1000 | Loss: 0.00005856
Iteration 119/1000 | Loss: 0.00016016
Iteration 120/1000 | Loss: 0.00005778
Iteration 121/1000 | Loss: 0.00006417
Iteration 122/1000 | Loss: 0.00033494
Iteration 123/1000 | Loss: 0.00012065
Iteration 124/1000 | Loss: 0.00005826
Iteration 125/1000 | Loss: 0.00014617
Iteration 126/1000 | Loss: 0.00017875
Iteration 127/1000 | Loss: 0.00029238
Iteration 128/1000 | Loss: 0.00011471
Iteration 129/1000 | Loss: 0.00019627
Iteration 130/1000 | Loss: 0.00032435
Iteration 131/1000 | Loss: 0.00043115
Iteration 132/1000 | Loss: 0.00035826
Iteration 133/1000 | Loss: 0.00006160
Iteration 134/1000 | Loss: 0.00005493
Iteration 135/1000 | Loss: 0.00035230
Iteration 136/1000 | Loss: 0.00017460
Iteration 137/1000 | Loss: 0.00035027
Iteration 138/1000 | Loss: 0.00020593
Iteration 139/1000 | Loss: 0.00004828
Iteration 140/1000 | Loss: 0.00005699
Iteration 141/1000 | Loss: 0.00005178
Iteration 142/1000 | Loss: 0.00005236
Iteration 143/1000 | Loss: 0.00004028
Iteration 144/1000 | Loss: 0.00003897
Iteration 145/1000 | Loss: 0.00003815
Iteration 146/1000 | Loss: 0.00003771
Iteration 147/1000 | Loss: 0.00003726
Iteration 148/1000 | Loss: 0.00003702
Iteration 149/1000 | Loss: 0.00003689
Iteration 150/1000 | Loss: 0.00003687
Iteration 151/1000 | Loss: 0.00003685
Iteration 152/1000 | Loss: 0.00003679
Iteration 153/1000 | Loss: 0.00003664
Iteration 154/1000 | Loss: 0.00003659
Iteration 155/1000 | Loss: 0.00003651
Iteration 156/1000 | Loss: 0.00003650
Iteration 157/1000 | Loss: 0.00003650
Iteration 158/1000 | Loss: 0.00003649
Iteration 159/1000 | Loss: 0.00003648
Iteration 160/1000 | Loss: 0.00003647
Iteration 161/1000 | Loss: 0.00003646
Iteration 162/1000 | Loss: 0.00003645
Iteration 163/1000 | Loss: 0.00003645
Iteration 164/1000 | Loss: 0.00003644
Iteration 165/1000 | Loss: 0.00003643
Iteration 166/1000 | Loss: 0.00003643
Iteration 167/1000 | Loss: 0.00003643
Iteration 168/1000 | Loss: 0.00003642
Iteration 169/1000 | Loss: 0.00003642
Iteration 170/1000 | Loss: 0.00003642
Iteration 171/1000 | Loss: 0.00003642
Iteration 172/1000 | Loss: 0.00003642
Iteration 173/1000 | Loss: 0.00003642
Iteration 174/1000 | Loss: 0.00003642
Iteration 175/1000 | Loss: 0.00003642
Iteration 176/1000 | Loss: 0.00003641
Iteration 177/1000 | Loss: 0.00003641
Iteration 178/1000 | Loss: 0.00003641
Iteration 179/1000 | Loss: 0.00003640
Iteration 180/1000 | Loss: 0.00003640
Iteration 181/1000 | Loss: 0.00003640
Iteration 182/1000 | Loss: 0.00003640
Iteration 183/1000 | Loss: 0.00003639
Iteration 184/1000 | Loss: 0.00003639
Iteration 185/1000 | Loss: 0.00003636
Iteration 186/1000 | Loss: 0.00003636
Iteration 187/1000 | Loss: 0.00003636
Iteration 188/1000 | Loss: 0.00003636
Iteration 189/1000 | Loss: 0.00003636
Iteration 190/1000 | Loss: 0.00003636
Iteration 191/1000 | Loss: 0.00003636
Iteration 192/1000 | Loss: 0.00003635
Iteration 193/1000 | Loss: 0.00003635
Iteration 194/1000 | Loss: 0.00003635
Iteration 195/1000 | Loss: 0.00003634
Iteration 196/1000 | Loss: 0.00003634
Iteration 197/1000 | Loss: 0.00003633
Iteration 198/1000 | Loss: 0.00003633
Iteration 199/1000 | Loss: 0.00003633
Iteration 200/1000 | Loss: 0.00003632
Iteration 201/1000 | Loss: 0.00003632
Iteration 202/1000 | Loss: 0.00003632
Iteration 203/1000 | Loss: 0.00003631
Iteration 204/1000 | Loss: 0.00003631
Iteration 205/1000 | Loss: 0.00003631
Iteration 206/1000 | Loss: 0.00003631
Iteration 207/1000 | Loss: 0.00003631
Iteration 208/1000 | Loss: 0.00003630
Iteration 209/1000 | Loss: 0.00003630
Iteration 210/1000 | Loss: 0.00003630
Iteration 211/1000 | Loss: 0.00003629
Iteration 212/1000 | Loss: 0.00003629
Iteration 213/1000 | Loss: 0.00003629
Iteration 214/1000 | Loss: 0.00003628
Iteration 215/1000 | Loss: 0.00003628
Iteration 216/1000 | Loss: 0.00003628
Iteration 217/1000 | Loss: 0.00003628
Iteration 218/1000 | Loss: 0.00003627
Iteration 219/1000 | Loss: 0.00003627
Iteration 220/1000 | Loss: 0.00003627
Iteration 221/1000 | Loss: 0.00003627
Iteration 222/1000 | Loss: 0.00003627
Iteration 223/1000 | Loss: 0.00003627
Iteration 224/1000 | Loss: 0.00003627
Iteration 225/1000 | Loss: 0.00003627
Iteration 226/1000 | Loss: 0.00003627
Iteration 227/1000 | Loss: 0.00003627
Iteration 228/1000 | Loss: 0.00003627
Iteration 229/1000 | Loss: 0.00003627
Iteration 230/1000 | Loss: 0.00003627
Iteration 231/1000 | Loss: 0.00003626
Iteration 232/1000 | Loss: 0.00003626
Iteration 233/1000 | Loss: 0.00003626
Iteration 234/1000 | Loss: 0.00003626
Iteration 235/1000 | Loss: 0.00003625
Iteration 236/1000 | Loss: 0.00003625
Iteration 237/1000 | Loss: 0.00003625
Iteration 238/1000 | Loss: 0.00003624
Iteration 239/1000 | Loss: 0.00003624
Iteration 240/1000 | Loss: 0.00003624
Iteration 241/1000 | Loss: 0.00003624
Iteration 242/1000 | Loss: 0.00003624
Iteration 243/1000 | Loss: 0.00003623
Iteration 244/1000 | Loss: 0.00003623
Iteration 245/1000 | Loss: 0.00003623
Iteration 246/1000 | Loss: 0.00003623
Iteration 247/1000 | Loss: 0.00003623
Iteration 248/1000 | Loss: 0.00003623
Iteration 249/1000 | Loss: 0.00003623
Iteration 250/1000 | Loss: 0.00003623
Iteration 251/1000 | Loss: 0.00003623
Iteration 252/1000 | Loss: 0.00003623
Iteration 253/1000 | Loss: 0.00003623
Iteration 254/1000 | Loss: 0.00003623
Iteration 255/1000 | Loss: 0.00003623
Iteration 256/1000 | Loss: 0.00003623
Iteration 257/1000 | Loss: 0.00003623
Iteration 258/1000 | Loss: 0.00003622
Iteration 259/1000 | Loss: 0.00003622
Iteration 260/1000 | Loss: 0.00003622
Iteration 261/1000 | Loss: 0.00003622
Iteration 262/1000 | Loss: 0.00003622
Iteration 263/1000 | Loss: 0.00003622
Iteration 264/1000 | Loss: 0.00003622
Iteration 265/1000 | Loss: 0.00003622
Iteration 266/1000 | Loss: 0.00003622
Iteration 267/1000 | Loss: 0.00003622
Iteration 268/1000 | Loss: 0.00003622
Iteration 269/1000 | Loss: 0.00003621
Iteration 270/1000 | Loss: 0.00003621
Iteration 271/1000 | Loss: 0.00003621
Iteration 272/1000 | Loss: 0.00003621
Iteration 273/1000 | Loss: 0.00003621
Iteration 274/1000 | Loss: 0.00003621
Iteration 275/1000 | Loss: 0.00003620
Iteration 276/1000 | Loss: 0.00003620
Iteration 277/1000 | Loss: 0.00003620
Iteration 278/1000 | Loss: 0.00003620
Iteration 279/1000 | Loss: 0.00003620
Iteration 280/1000 | Loss: 0.00003620
Iteration 281/1000 | Loss: 0.00003620
Iteration 282/1000 | Loss: 0.00003620
Iteration 283/1000 | Loss: 0.00003620
Iteration 284/1000 | Loss: 0.00003620
Iteration 285/1000 | Loss: 0.00003620
Iteration 286/1000 | Loss: 0.00003620
Iteration 287/1000 | Loss: 0.00003620
Iteration 288/1000 | Loss: 0.00003620
Iteration 289/1000 | Loss: 0.00003619
Iteration 290/1000 | Loss: 0.00003619
Iteration 291/1000 | Loss: 0.00003619
Iteration 292/1000 | Loss: 0.00003619
Iteration 293/1000 | Loss: 0.00003619
Iteration 294/1000 | Loss: 0.00003619
Iteration 295/1000 | Loss: 0.00003619
Iteration 296/1000 | Loss: 0.00003619
Iteration 297/1000 | Loss: 0.00003619
Iteration 298/1000 | Loss: 0.00003619
Iteration 299/1000 | Loss: 0.00003619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [3.6193509004078805e-05, 3.6193509004078805e-05, 3.6193509004078805e-05, 3.6193509004078805e-05, 3.6193509004078805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6193509004078805e-05

Optimization complete. Final v2v error: 4.876670837402344 mm

Highest mean error: 12.861223220825195 mm for frame 74

Lowest mean error: 4.041859149932861 mm for frame 0

Saving results

Total time: 278.1943430900574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074941
Iteration 2/25 | Loss: 0.01074941
Iteration 3/25 | Loss: 0.01074941
Iteration 4/25 | Loss: 0.00317355
Iteration 5/25 | Loss: 0.00211508
Iteration 6/25 | Loss: 0.00198774
Iteration 7/25 | Loss: 0.00195773
Iteration 8/25 | Loss: 0.00186413
Iteration 9/25 | Loss: 0.00185251
Iteration 10/25 | Loss: 0.00184807
Iteration 11/25 | Loss: 0.00184050
Iteration 12/25 | Loss: 0.00183134
Iteration 13/25 | Loss: 0.00182347
Iteration 14/25 | Loss: 0.00182000
Iteration 15/25 | Loss: 0.00181722
Iteration 16/25 | Loss: 0.00181742
Iteration 17/25 | Loss: 0.00181281
Iteration 18/25 | Loss: 0.00180983
Iteration 19/25 | Loss: 0.00180857
Iteration 20/25 | Loss: 0.00180800
Iteration 21/25 | Loss: 0.00181241
Iteration 22/25 | Loss: 0.00180793
Iteration 23/25 | Loss: 0.00180716
Iteration 24/25 | Loss: 0.00180690
Iteration 25/25 | Loss: 0.00180998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28109753
Iteration 2/25 | Loss: 0.00730218
Iteration 3/25 | Loss: 0.00730218
Iteration 4/25 | Loss: 0.00730218
Iteration 5/25 | Loss: 0.00730218
Iteration 6/25 | Loss: 0.00730218
Iteration 7/25 | Loss: 0.00730218
Iteration 8/25 | Loss: 0.00730218
Iteration 9/25 | Loss: 0.00730218
Iteration 10/25 | Loss: 0.00730218
Iteration 11/25 | Loss: 0.00730218
Iteration 12/25 | Loss: 0.00730218
Iteration 13/25 | Loss: 0.00730218
Iteration 14/25 | Loss: 0.00730218
Iteration 15/25 | Loss: 0.00730218
Iteration 16/25 | Loss: 0.00730218
Iteration 17/25 | Loss: 0.00730218
Iteration 18/25 | Loss: 0.00730218
Iteration 19/25 | Loss: 0.00730218
Iteration 20/25 | Loss: 0.00730218
Iteration 21/25 | Loss: 0.00730218
Iteration 22/25 | Loss: 0.00730218
Iteration 23/25 | Loss: 0.00730218
Iteration 24/25 | Loss: 0.00730218
Iteration 25/25 | Loss: 0.00730218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00730218
Iteration 2/1000 | Loss: 0.00131898
Iteration 3/1000 | Loss: 0.00095312
Iteration 4/1000 | Loss: 0.00075608
Iteration 5/1000 | Loss: 0.00063049
Iteration 6/1000 | Loss: 0.00053914
Iteration 7/1000 | Loss: 0.00049055
Iteration 8/1000 | Loss: 0.00044342
Iteration 9/1000 | Loss: 0.00090328
Iteration 10/1000 | Loss: 0.01086662
Iteration 11/1000 | Loss: 0.01817760
Iteration 12/1000 | Loss: 0.00414130
Iteration 13/1000 | Loss: 0.00187881
Iteration 14/1000 | Loss: 0.00094897
Iteration 15/1000 | Loss: 0.00053387
Iteration 16/1000 | Loss: 0.00050712
Iteration 17/1000 | Loss: 0.00024508
Iteration 18/1000 | Loss: 0.00021141
Iteration 19/1000 | Loss: 0.00013043
Iteration 20/1000 | Loss: 0.00009588
Iteration 21/1000 | Loss: 0.00007649
Iteration 22/1000 | Loss: 0.00006970
Iteration 23/1000 | Loss: 0.00005485
Iteration 24/1000 | Loss: 0.00004963
Iteration 25/1000 | Loss: 0.00004519
Iteration 26/1000 | Loss: 0.00004148
Iteration 27/1000 | Loss: 0.00003760
Iteration 28/1000 | Loss: 0.00003553
Iteration 29/1000 | Loss: 0.00003399
Iteration 30/1000 | Loss: 0.00003305
Iteration 31/1000 | Loss: 0.00003241
Iteration 32/1000 | Loss: 0.00003193
Iteration 33/1000 | Loss: 0.00003153
Iteration 34/1000 | Loss: 0.00003128
Iteration 35/1000 | Loss: 0.00003121
Iteration 36/1000 | Loss: 0.00003112
Iteration 37/1000 | Loss: 0.00003107
Iteration 38/1000 | Loss: 0.00003106
Iteration 39/1000 | Loss: 0.00003106
Iteration 40/1000 | Loss: 0.00003104
Iteration 41/1000 | Loss: 0.00003103
Iteration 42/1000 | Loss: 0.00003103
Iteration 43/1000 | Loss: 0.00003103
Iteration 44/1000 | Loss: 0.00003102
Iteration 45/1000 | Loss: 0.00003102
Iteration 46/1000 | Loss: 0.00003101
Iteration 47/1000 | Loss: 0.00003100
Iteration 48/1000 | Loss: 0.00003100
Iteration 49/1000 | Loss: 0.00003100
Iteration 50/1000 | Loss: 0.00003100
Iteration 51/1000 | Loss: 0.00003099
Iteration 52/1000 | Loss: 0.00003099
Iteration 53/1000 | Loss: 0.00003097
Iteration 54/1000 | Loss: 0.00003097
Iteration 55/1000 | Loss: 0.00003097
Iteration 56/1000 | Loss: 0.00003097
Iteration 57/1000 | Loss: 0.00003097
Iteration 58/1000 | Loss: 0.00003097
Iteration 59/1000 | Loss: 0.00003096
Iteration 60/1000 | Loss: 0.00003096
Iteration 61/1000 | Loss: 0.00003096
Iteration 62/1000 | Loss: 0.00003096
Iteration 63/1000 | Loss: 0.00003096
Iteration 64/1000 | Loss: 0.00003095
Iteration 65/1000 | Loss: 0.00003095
Iteration 66/1000 | Loss: 0.00003095
Iteration 67/1000 | Loss: 0.00003095
Iteration 68/1000 | Loss: 0.00003095
Iteration 69/1000 | Loss: 0.00003095
Iteration 70/1000 | Loss: 0.00003095
Iteration 71/1000 | Loss: 0.00003095
Iteration 72/1000 | Loss: 0.00003095
Iteration 73/1000 | Loss: 0.00003095
Iteration 74/1000 | Loss: 0.00003095
Iteration 75/1000 | Loss: 0.00003094
Iteration 76/1000 | Loss: 0.00003094
Iteration 77/1000 | Loss: 0.00003094
Iteration 78/1000 | Loss: 0.00003094
Iteration 79/1000 | Loss: 0.00003094
Iteration 80/1000 | Loss: 0.00003094
Iteration 81/1000 | Loss: 0.00003094
Iteration 82/1000 | Loss: 0.00003094
Iteration 83/1000 | Loss: 0.00003094
Iteration 84/1000 | Loss: 0.00003094
Iteration 85/1000 | Loss: 0.00003094
Iteration 86/1000 | Loss: 0.00003094
Iteration 87/1000 | Loss: 0.00003094
Iteration 88/1000 | Loss: 0.00003094
Iteration 89/1000 | Loss: 0.00003094
Iteration 90/1000 | Loss: 0.00003094
Iteration 91/1000 | Loss: 0.00003094
Iteration 92/1000 | Loss: 0.00003094
Iteration 93/1000 | Loss: 0.00003093
Iteration 94/1000 | Loss: 0.00003093
Iteration 95/1000 | Loss: 0.00003093
Iteration 96/1000 | Loss: 0.00003093
Iteration 97/1000 | Loss: 0.00003093
Iteration 98/1000 | Loss: 0.00003093
Iteration 99/1000 | Loss: 0.00003093
Iteration 100/1000 | Loss: 0.00003093
Iteration 101/1000 | Loss: 0.00003093
Iteration 102/1000 | Loss: 0.00003093
Iteration 103/1000 | Loss: 0.00003093
Iteration 104/1000 | Loss: 0.00003092
Iteration 105/1000 | Loss: 0.00003092
Iteration 106/1000 | Loss: 0.00003092
Iteration 107/1000 | Loss: 0.00003092
Iteration 108/1000 | Loss: 0.00003092
Iteration 109/1000 | Loss: 0.00003092
Iteration 110/1000 | Loss: 0.00003092
Iteration 111/1000 | Loss: 0.00003092
Iteration 112/1000 | Loss: 0.00003092
Iteration 113/1000 | Loss: 0.00003092
Iteration 114/1000 | Loss: 0.00003092
Iteration 115/1000 | Loss: 0.00003092
Iteration 116/1000 | Loss: 0.00003092
Iteration 117/1000 | Loss: 0.00003092
Iteration 118/1000 | Loss: 0.00003092
Iteration 119/1000 | Loss: 0.00003092
Iteration 120/1000 | Loss: 0.00003092
Iteration 121/1000 | Loss: 0.00003092
Iteration 122/1000 | Loss: 0.00003091
Iteration 123/1000 | Loss: 0.00003091
Iteration 124/1000 | Loss: 0.00003091
Iteration 125/1000 | Loss: 0.00003091
Iteration 126/1000 | Loss: 0.00003091
Iteration 127/1000 | Loss: 0.00003091
Iteration 128/1000 | Loss: 0.00003091
Iteration 129/1000 | Loss: 0.00003091
Iteration 130/1000 | Loss: 0.00003090
Iteration 131/1000 | Loss: 0.00003090
Iteration 132/1000 | Loss: 0.00003090
Iteration 133/1000 | Loss: 0.00003090
Iteration 134/1000 | Loss: 0.00003090
Iteration 135/1000 | Loss: 0.00003090
Iteration 136/1000 | Loss: 0.00003090
Iteration 137/1000 | Loss: 0.00003090
Iteration 138/1000 | Loss: 0.00003090
Iteration 139/1000 | Loss: 0.00003090
Iteration 140/1000 | Loss: 0.00003090
Iteration 141/1000 | Loss: 0.00003090
Iteration 142/1000 | Loss: 0.00003090
Iteration 143/1000 | Loss: 0.00003090
Iteration 144/1000 | Loss: 0.00003090
Iteration 145/1000 | Loss: 0.00003090
Iteration 146/1000 | Loss: 0.00003090
Iteration 147/1000 | Loss: 0.00003090
Iteration 148/1000 | Loss: 0.00003090
Iteration 149/1000 | Loss: 0.00003090
Iteration 150/1000 | Loss: 0.00003090
Iteration 151/1000 | Loss: 0.00003089
Iteration 152/1000 | Loss: 0.00003089
Iteration 153/1000 | Loss: 0.00003089
Iteration 154/1000 | Loss: 0.00003089
Iteration 155/1000 | Loss: 0.00003089
Iteration 156/1000 | Loss: 0.00003089
Iteration 157/1000 | Loss: 0.00003089
Iteration 158/1000 | Loss: 0.00003089
Iteration 159/1000 | Loss: 0.00003089
Iteration 160/1000 | Loss: 0.00003089
Iteration 161/1000 | Loss: 0.00003089
Iteration 162/1000 | Loss: 0.00003089
Iteration 163/1000 | Loss: 0.00003089
Iteration 164/1000 | Loss: 0.00003089
Iteration 165/1000 | Loss: 0.00003089
Iteration 166/1000 | Loss: 0.00003089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.089246456511319e-05, 3.089246456511319e-05, 3.089246456511319e-05, 3.089246456511319e-05, 3.089246456511319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.089246456511319e-05

Optimization complete. Final v2v error: 4.931487083435059 mm

Highest mean error: 5.1141886711120605 mm for frame 224

Lowest mean error: 4.807472229003906 mm for frame 125

Saving results

Total time: 112.53724551200867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896794
Iteration 2/25 | Loss: 0.00145574
Iteration 3/25 | Loss: 0.00112179
Iteration 4/25 | Loss: 0.00108985
Iteration 5/25 | Loss: 0.00108209
Iteration 6/25 | Loss: 0.00108113
Iteration 7/25 | Loss: 0.00108113
Iteration 8/25 | Loss: 0.00108113
Iteration 9/25 | Loss: 0.00108113
Iteration 10/25 | Loss: 0.00108113
Iteration 11/25 | Loss: 0.00108113
Iteration 12/25 | Loss: 0.00108113
Iteration 13/25 | Loss: 0.00108113
Iteration 14/25 | Loss: 0.00108113
Iteration 15/25 | Loss: 0.00108113
Iteration 16/25 | Loss: 0.00108113
Iteration 17/25 | Loss: 0.00108113
Iteration 18/25 | Loss: 0.00108113
Iteration 19/25 | Loss: 0.00108113
Iteration 20/25 | Loss: 0.00108113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010811270913109183, 0.0010811270913109183, 0.0010811270913109183, 0.0010811270913109183, 0.0010811270913109183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010811270913109183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.19669484
Iteration 2/25 | Loss: 0.00061237
Iteration 3/25 | Loss: 0.00061236
Iteration 4/25 | Loss: 0.00061236
Iteration 5/25 | Loss: 0.00061236
Iteration 6/25 | Loss: 0.00061236
Iteration 7/25 | Loss: 0.00061236
Iteration 8/25 | Loss: 0.00061236
Iteration 9/25 | Loss: 0.00061236
Iteration 10/25 | Loss: 0.00061236
Iteration 11/25 | Loss: 0.00061236
Iteration 12/25 | Loss: 0.00061236
Iteration 13/25 | Loss: 0.00061236
Iteration 14/25 | Loss: 0.00061236
Iteration 15/25 | Loss: 0.00061236
Iteration 16/25 | Loss: 0.00061236
Iteration 17/25 | Loss: 0.00061236
Iteration 18/25 | Loss: 0.00061236
Iteration 19/25 | Loss: 0.00061236
Iteration 20/25 | Loss: 0.00061236
Iteration 21/25 | Loss: 0.00061236
Iteration 22/25 | Loss: 0.00061236
Iteration 23/25 | Loss: 0.00061236
Iteration 24/25 | Loss: 0.00061236
Iteration 25/25 | Loss: 0.00061236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061236
Iteration 2/1000 | Loss: 0.00005266
Iteration 3/1000 | Loss: 0.00003519
Iteration 4/1000 | Loss: 0.00002964
Iteration 5/1000 | Loss: 0.00002690
Iteration 6/1000 | Loss: 0.00002551
Iteration 7/1000 | Loss: 0.00002480
Iteration 8/1000 | Loss: 0.00002439
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002377
Iteration 11/1000 | Loss: 0.00002353
Iteration 12/1000 | Loss: 0.00002338
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002323
Iteration 15/1000 | Loss: 0.00002320
Iteration 16/1000 | Loss: 0.00002320
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002318
Iteration 21/1000 | Loss: 0.00002316
Iteration 22/1000 | Loss: 0.00002313
Iteration 23/1000 | Loss: 0.00002312
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002310
Iteration 26/1000 | Loss: 0.00002304
Iteration 27/1000 | Loss: 0.00002301
Iteration 28/1000 | Loss: 0.00002301
Iteration 29/1000 | Loss: 0.00002301
Iteration 30/1000 | Loss: 0.00002300
Iteration 31/1000 | Loss: 0.00002300
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002299
Iteration 34/1000 | Loss: 0.00002299
Iteration 35/1000 | Loss: 0.00002297
Iteration 36/1000 | Loss: 0.00002296
Iteration 37/1000 | Loss: 0.00002295
Iteration 38/1000 | Loss: 0.00002295
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00002291
Iteration 41/1000 | Loss: 0.00002291
Iteration 42/1000 | Loss: 0.00002290
Iteration 43/1000 | Loss: 0.00002290
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002289
Iteration 46/1000 | Loss: 0.00002289
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002288
Iteration 49/1000 | Loss: 0.00002288
Iteration 50/1000 | Loss: 0.00002288
Iteration 51/1000 | Loss: 0.00002288
Iteration 52/1000 | Loss: 0.00002288
Iteration 53/1000 | Loss: 0.00002288
Iteration 54/1000 | Loss: 0.00002288
Iteration 55/1000 | Loss: 0.00002288
Iteration 56/1000 | Loss: 0.00002288
Iteration 57/1000 | Loss: 0.00002287
Iteration 58/1000 | Loss: 0.00002287
Iteration 59/1000 | Loss: 0.00002287
Iteration 60/1000 | Loss: 0.00002287
Iteration 61/1000 | Loss: 0.00002287
Iteration 62/1000 | Loss: 0.00002287
Iteration 63/1000 | Loss: 0.00002287
Iteration 64/1000 | Loss: 0.00002287
Iteration 65/1000 | Loss: 0.00002286
Iteration 66/1000 | Loss: 0.00002286
Iteration 67/1000 | Loss: 0.00002286
Iteration 68/1000 | Loss: 0.00002286
Iteration 69/1000 | Loss: 0.00002286
Iteration 70/1000 | Loss: 0.00002286
Iteration 71/1000 | Loss: 0.00002286
Iteration 72/1000 | Loss: 0.00002286
Iteration 73/1000 | Loss: 0.00002286
Iteration 74/1000 | Loss: 0.00002285
Iteration 75/1000 | Loss: 0.00002285
Iteration 76/1000 | Loss: 0.00002285
Iteration 77/1000 | Loss: 0.00002285
Iteration 78/1000 | Loss: 0.00002285
Iteration 79/1000 | Loss: 0.00002284
Iteration 80/1000 | Loss: 0.00002284
Iteration 81/1000 | Loss: 0.00002284
Iteration 82/1000 | Loss: 0.00002284
Iteration 83/1000 | Loss: 0.00002284
Iteration 84/1000 | Loss: 0.00002284
Iteration 85/1000 | Loss: 0.00002284
Iteration 86/1000 | Loss: 0.00002284
Iteration 87/1000 | Loss: 0.00002284
Iteration 88/1000 | Loss: 0.00002284
Iteration 89/1000 | Loss: 0.00002284
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002284
Iteration 98/1000 | Loss: 0.00002284
Iteration 99/1000 | Loss: 0.00002284
Iteration 100/1000 | Loss: 0.00002284
Iteration 101/1000 | Loss: 0.00002284
Iteration 102/1000 | Loss: 0.00002284
Iteration 103/1000 | Loss: 0.00002284
Iteration 104/1000 | Loss: 0.00002284
Iteration 105/1000 | Loss: 0.00002284
Iteration 106/1000 | Loss: 0.00002284
Iteration 107/1000 | Loss: 0.00002284
Iteration 108/1000 | Loss: 0.00002284
Iteration 109/1000 | Loss: 0.00002284
Iteration 110/1000 | Loss: 0.00002284
Iteration 111/1000 | Loss: 0.00002284
Iteration 112/1000 | Loss: 0.00002284
Iteration 113/1000 | Loss: 0.00002284
Iteration 114/1000 | Loss: 0.00002284
Iteration 115/1000 | Loss: 0.00002284
Iteration 116/1000 | Loss: 0.00002284
Iteration 117/1000 | Loss: 0.00002284
Iteration 118/1000 | Loss: 0.00002284
Iteration 119/1000 | Loss: 0.00002284
Iteration 120/1000 | Loss: 0.00002284
Iteration 121/1000 | Loss: 0.00002284
Iteration 122/1000 | Loss: 0.00002284
Iteration 123/1000 | Loss: 0.00002284
Iteration 124/1000 | Loss: 0.00002284
Iteration 125/1000 | Loss: 0.00002284
Iteration 126/1000 | Loss: 0.00002284
Iteration 127/1000 | Loss: 0.00002284
Iteration 128/1000 | Loss: 0.00002284
Iteration 129/1000 | Loss: 0.00002284
Iteration 130/1000 | Loss: 0.00002284
Iteration 131/1000 | Loss: 0.00002284
Iteration 132/1000 | Loss: 0.00002284
Iteration 133/1000 | Loss: 0.00002284
Iteration 134/1000 | Loss: 0.00002284
Iteration 135/1000 | Loss: 0.00002284
Iteration 136/1000 | Loss: 0.00002284
Iteration 137/1000 | Loss: 0.00002284
Iteration 138/1000 | Loss: 0.00002284
Iteration 139/1000 | Loss: 0.00002284
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002284
Iteration 142/1000 | Loss: 0.00002284
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002284
Iteration 145/1000 | Loss: 0.00002284
Iteration 146/1000 | Loss: 0.00002284
Iteration 147/1000 | Loss: 0.00002284
Iteration 148/1000 | Loss: 0.00002284
Iteration 149/1000 | Loss: 0.00002284
Iteration 150/1000 | Loss: 0.00002284
Iteration 151/1000 | Loss: 0.00002284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.284107540617697e-05, 2.284107540617697e-05, 2.284107540617697e-05, 2.284107540617697e-05, 2.284107540617697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.284107540617697e-05

Optimization complete. Final v2v error: 4.205630779266357 mm

Highest mean error: 4.424032211303711 mm for frame 57

Lowest mean error: 3.981041193008423 mm for frame 212

Saving results

Total time: 41.475250244140625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567214
Iteration 2/25 | Loss: 0.00117296
Iteration 3/25 | Loss: 0.00108123
Iteration 4/25 | Loss: 0.00106543
Iteration 5/25 | Loss: 0.00105945
Iteration 6/25 | Loss: 0.00105785
Iteration 7/25 | Loss: 0.00105728
Iteration 8/25 | Loss: 0.00105728
Iteration 9/25 | Loss: 0.00105728
Iteration 10/25 | Loss: 0.00105728
Iteration 11/25 | Loss: 0.00105728
Iteration 12/25 | Loss: 0.00105728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010572760365903378, 0.0010572760365903378, 0.0010572760365903378, 0.0010572760365903378, 0.0010572760365903378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010572760365903378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62413740
Iteration 2/25 | Loss: 0.00062370
Iteration 3/25 | Loss: 0.00062368
Iteration 4/25 | Loss: 0.00062368
Iteration 5/25 | Loss: 0.00062368
Iteration 6/25 | Loss: 0.00062368
Iteration 7/25 | Loss: 0.00062368
Iteration 8/25 | Loss: 0.00062368
Iteration 9/25 | Loss: 0.00062368
Iteration 10/25 | Loss: 0.00062368
Iteration 11/25 | Loss: 0.00062368
Iteration 12/25 | Loss: 0.00062368
Iteration 13/25 | Loss: 0.00062368
Iteration 14/25 | Loss: 0.00062368
Iteration 15/25 | Loss: 0.00062368
Iteration 16/25 | Loss: 0.00062368
Iteration 17/25 | Loss: 0.00062368
Iteration 18/25 | Loss: 0.00062368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006236758199520409, 0.0006236758199520409, 0.0006236758199520409, 0.0006236758199520409, 0.0006236758199520409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006236758199520409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062368
Iteration 2/1000 | Loss: 0.00003415
Iteration 3/1000 | Loss: 0.00002833
Iteration 4/1000 | Loss: 0.00002631
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002465
Iteration 7/1000 | Loss: 0.00002414
Iteration 8/1000 | Loss: 0.00002398
Iteration 9/1000 | Loss: 0.00002393
Iteration 10/1000 | Loss: 0.00002393
Iteration 11/1000 | Loss: 0.00002391
Iteration 12/1000 | Loss: 0.00002376
Iteration 13/1000 | Loss: 0.00002370
Iteration 14/1000 | Loss: 0.00002363
Iteration 15/1000 | Loss: 0.00002360
Iteration 16/1000 | Loss: 0.00002360
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00002359
Iteration 19/1000 | Loss: 0.00002359
Iteration 20/1000 | Loss: 0.00002359
Iteration 21/1000 | Loss: 0.00002359
Iteration 22/1000 | Loss: 0.00002359
Iteration 23/1000 | Loss: 0.00002359
Iteration 24/1000 | Loss: 0.00002359
Iteration 25/1000 | Loss: 0.00002358
Iteration 26/1000 | Loss: 0.00002358
Iteration 27/1000 | Loss: 0.00002358
Iteration 28/1000 | Loss: 0.00002358
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002358
Iteration 31/1000 | Loss: 0.00002357
Iteration 32/1000 | Loss: 0.00002357
Iteration 33/1000 | Loss: 0.00002357
Iteration 34/1000 | Loss: 0.00002356
Iteration 35/1000 | Loss: 0.00002356
Iteration 36/1000 | Loss: 0.00002356
Iteration 37/1000 | Loss: 0.00002356
Iteration 38/1000 | Loss: 0.00002356
Iteration 39/1000 | Loss: 0.00002355
Iteration 40/1000 | Loss: 0.00002355
Iteration 41/1000 | Loss: 0.00002355
Iteration 42/1000 | Loss: 0.00002355
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002354
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002353
Iteration 48/1000 | Loss: 0.00002353
Iteration 49/1000 | Loss: 0.00002353
Iteration 50/1000 | Loss: 0.00002353
Iteration 51/1000 | Loss: 0.00002352
Iteration 52/1000 | Loss: 0.00002352
Iteration 53/1000 | Loss: 0.00002351
Iteration 54/1000 | Loss: 0.00002351
Iteration 55/1000 | Loss: 0.00002351
Iteration 56/1000 | Loss: 0.00002351
Iteration 57/1000 | Loss: 0.00002351
Iteration 58/1000 | Loss: 0.00002351
Iteration 59/1000 | Loss: 0.00002351
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002351
Iteration 64/1000 | Loss: 0.00002350
Iteration 65/1000 | Loss: 0.00002350
Iteration 66/1000 | Loss: 0.00002350
Iteration 67/1000 | Loss: 0.00002350
Iteration 68/1000 | Loss: 0.00002349
Iteration 69/1000 | Loss: 0.00002349
Iteration 70/1000 | Loss: 0.00002349
Iteration 71/1000 | Loss: 0.00002349
Iteration 72/1000 | Loss: 0.00002349
Iteration 73/1000 | Loss: 0.00002349
Iteration 74/1000 | Loss: 0.00002349
Iteration 75/1000 | Loss: 0.00002349
Iteration 76/1000 | Loss: 0.00002348
Iteration 77/1000 | Loss: 0.00002348
Iteration 78/1000 | Loss: 0.00002348
Iteration 79/1000 | Loss: 0.00002348
Iteration 80/1000 | Loss: 0.00002348
Iteration 81/1000 | Loss: 0.00002348
Iteration 82/1000 | Loss: 0.00002348
Iteration 83/1000 | Loss: 0.00002348
Iteration 84/1000 | Loss: 0.00002348
Iteration 85/1000 | Loss: 0.00002348
Iteration 86/1000 | Loss: 0.00002347
Iteration 87/1000 | Loss: 0.00002347
Iteration 88/1000 | Loss: 0.00002347
Iteration 89/1000 | Loss: 0.00002347
Iteration 90/1000 | Loss: 0.00002346
Iteration 91/1000 | Loss: 0.00002346
Iteration 92/1000 | Loss: 0.00002346
Iteration 93/1000 | Loss: 0.00002346
Iteration 94/1000 | Loss: 0.00002346
Iteration 95/1000 | Loss: 0.00002345
Iteration 96/1000 | Loss: 0.00002345
Iteration 97/1000 | Loss: 0.00002345
Iteration 98/1000 | Loss: 0.00002345
Iteration 99/1000 | Loss: 0.00002345
Iteration 100/1000 | Loss: 0.00002345
Iteration 101/1000 | Loss: 0.00002345
Iteration 102/1000 | Loss: 0.00002345
Iteration 103/1000 | Loss: 0.00002344
Iteration 104/1000 | Loss: 0.00002344
Iteration 105/1000 | Loss: 0.00002344
Iteration 106/1000 | Loss: 0.00002344
Iteration 107/1000 | Loss: 0.00002344
Iteration 108/1000 | Loss: 0.00002344
Iteration 109/1000 | Loss: 0.00002344
Iteration 110/1000 | Loss: 0.00002344
Iteration 111/1000 | Loss: 0.00002344
Iteration 112/1000 | Loss: 0.00002344
Iteration 113/1000 | Loss: 0.00002344
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002344
Iteration 116/1000 | Loss: 0.00002343
Iteration 117/1000 | Loss: 0.00002343
Iteration 118/1000 | Loss: 0.00002343
Iteration 119/1000 | Loss: 0.00002343
Iteration 120/1000 | Loss: 0.00002343
Iteration 121/1000 | Loss: 0.00002343
Iteration 122/1000 | Loss: 0.00002343
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002342
Iteration 126/1000 | Loss: 0.00002342
Iteration 127/1000 | Loss: 0.00002341
Iteration 128/1000 | Loss: 0.00002341
Iteration 129/1000 | Loss: 0.00002341
Iteration 130/1000 | Loss: 0.00002341
Iteration 131/1000 | Loss: 0.00002341
Iteration 132/1000 | Loss: 0.00002341
Iteration 133/1000 | Loss: 0.00002341
Iteration 134/1000 | Loss: 0.00002341
Iteration 135/1000 | Loss: 0.00002341
Iteration 136/1000 | Loss: 0.00002341
Iteration 137/1000 | Loss: 0.00002341
Iteration 138/1000 | Loss: 0.00002340
Iteration 139/1000 | Loss: 0.00002340
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002339
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002338
Iteration 150/1000 | Loss: 0.00002338
Iteration 151/1000 | Loss: 0.00002338
Iteration 152/1000 | Loss: 0.00002338
Iteration 153/1000 | Loss: 0.00002338
Iteration 154/1000 | Loss: 0.00002337
Iteration 155/1000 | Loss: 0.00002337
Iteration 156/1000 | Loss: 0.00002337
Iteration 157/1000 | Loss: 0.00002337
Iteration 158/1000 | Loss: 0.00002337
Iteration 159/1000 | Loss: 0.00002337
Iteration 160/1000 | Loss: 0.00002337
Iteration 161/1000 | Loss: 0.00002337
Iteration 162/1000 | Loss: 0.00002337
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002337
Iteration 165/1000 | Loss: 0.00002337
Iteration 166/1000 | Loss: 0.00002337
Iteration 167/1000 | Loss: 0.00002337
Iteration 168/1000 | Loss: 0.00002337
Iteration 169/1000 | Loss: 0.00002337
Iteration 170/1000 | Loss: 0.00002336
Iteration 171/1000 | Loss: 0.00002336
Iteration 172/1000 | Loss: 0.00002336
Iteration 173/1000 | Loss: 0.00002336
Iteration 174/1000 | Loss: 0.00002336
Iteration 175/1000 | Loss: 0.00002336
Iteration 176/1000 | Loss: 0.00002336
Iteration 177/1000 | Loss: 0.00002336
Iteration 178/1000 | Loss: 0.00002336
Iteration 179/1000 | Loss: 0.00002336
Iteration 180/1000 | Loss: 0.00002336
Iteration 181/1000 | Loss: 0.00002336
Iteration 182/1000 | Loss: 0.00002336
Iteration 183/1000 | Loss: 0.00002336
Iteration 184/1000 | Loss: 0.00002336
Iteration 185/1000 | Loss: 0.00002336
Iteration 186/1000 | Loss: 0.00002336
Iteration 187/1000 | Loss: 0.00002336
Iteration 188/1000 | Loss: 0.00002336
Iteration 189/1000 | Loss: 0.00002336
Iteration 190/1000 | Loss: 0.00002336
Iteration 191/1000 | Loss: 0.00002336
Iteration 192/1000 | Loss: 0.00002336
Iteration 193/1000 | Loss: 0.00002336
Iteration 194/1000 | Loss: 0.00002336
Iteration 195/1000 | Loss: 0.00002336
Iteration 196/1000 | Loss: 0.00002336
Iteration 197/1000 | Loss: 0.00002336
Iteration 198/1000 | Loss: 0.00002336
Iteration 199/1000 | Loss: 0.00002336
Iteration 200/1000 | Loss: 0.00002336
Iteration 201/1000 | Loss: 0.00002336
Iteration 202/1000 | Loss: 0.00002336
Iteration 203/1000 | Loss: 0.00002336
Iteration 204/1000 | Loss: 0.00002336
Iteration 205/1000 | Loss: 0.00002336
Iteration 206/1000 | Loss: 0.00002336
Iteration 207/1000 | Loss: 0.00002336
Iteration 208/1000 | Loss: 0.00002336
Iteration 209/1000 | Loss: 0.00002336
Iteration 210/1000 | Loss: 0.00002336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.3363123546005227e-05, 2.3363123546005227e-05, 2.3363123546005227e-05, 2.3363123546005227e-05, 2.3363123546005227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3363123546005227e-05

Optimization complete. Final v2v error: 4.20637321472168 mm

Highest mean error: 4.617516994476318 mm for frame 102

Lowest mean error: 3.9033985137939453 mm for frame 144

Saving results

Total time: 37.182475328445435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103795
Iteration 2/25 | Loss: 0.00203145
Iteration 3/25 | Loss: 0.00145042
Iteration 4/25 | Loss: 0.00139128
Iteration 5/25 | Loss: 0.00130250
Iteration 6/25 | Loss: 0.00123436
Iteration 7/25 | Loss: 0.00118265
Iteration 8/25 | Loss: 0.00117589
Iteration 9/25 | Loss: 0.00115906
Iteration 10/25 | Loss: 0.00114404
Iteration 11/25 | Loss: 0.00112554
Iteration 12/25 | Loss: 0.00110160
Iteration 13/25 | Loss: 0.00110704
Iteration 14/25 | Loss: 0.00109050
Iteration 15/25 | Loss: 0.00108463
Iteration 16/25 | Loss: 0.00108321
Iteration 17/25 | Loss: 0.00108275
Iteration 18/25 | Loss: 0.00108255
Iteration 19/25 | Loss: 0.00108243
Iteration 20/25 | Loss: 0.00108232
Iteration 21/25 | Loss: 0.00108220
Iteration 22/25 | Loss: 0.00108213
Iteration 23/25 | Loss: 0.00108212
Iteration 24/25 | Loss: 0.00108212
Iteration 25/25 | Loss: 0.00108212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32818472
Iteration 2/25 | Loss: 0.00118260
Iteration 3/25 | Loss: 0.00096858
Iteration 4/25 | Loss: 0.00096858
Iteration 5/25 | Loss: 0.00096858
Iteration 6/25 | Loss: 0.00096858
Iteration 7/25 | Loss: 0.00096858
Iteration 8/25 | Loss: 0.00096858
Iteration 9/25 | Loss: 0.00096858
Iteration 10/25 | Loss: 0.00096858
Iteration 11/25 | Loss: 0.00096858
Iteration 12/25 | Loss: 0.00096858
Iteration 13/25 | Loss: 0.00096858
Iteration 14/25 | Loss: 0.00096858
Iteration 15/25 | Loss: 0.00096858
Iteration 16/25 | Loss: 0.00096858
Iteration 17/25 | Loss: 0.00096858
Iteration 18/25 | Loss: 0.00096858
Iteration 19/25 | Loss: 0.00096858
Iteration 20/25 | Loss: 0.00096858
Iteration 21/25 | Loss: 0.00096858
Iteration 22/25 | Loss: 0.00096858
Iteration 23/25 | Loss: 0.00096858
Iteration 24/25 | Loss: 0.00096858
Iteration 25/25 | Loss: 0.00096858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096858
Iteration 2/1000 | Loss: 0.00101562
Iteration 3/1000 | Loss: 0.00119346
Iteration 4/1000 | Loss: 0.00080181
Iteration 5/1000 | Loss: 0.00102105
Iteration 6/1000 | Loss: 0.00065744
Iteration 7/1000 | Loss: 0.00068307
Iteration 8/1000 | Loss: 0.00053066
Iteration 9/1000 | Loss: 0.00059502
Iteration 10/1000 | Loss: 0.00004216
Iteration 11/1000 | Loss: 0.00003705
Iteration 12/1000 | Loss: 0.00017130
Iteration 13/1000 | Loss: 0.00003354
Iteration 14/1000 | Loss: 0.00003246
Iteration 15/1000 | Loss: 0.00003164
Iteration 16/1000 | Loss: 0.00064759
Iteration 17/1000 | Loss: 0.00039736
Iteration 18/1000 | Loss: 0.00005267
Iteration 19/1000 | Loss: 0.00003516
Iteration 20/1000 | Loss: 0.00003607
Iteration 21/1000 | Loss: 0.00002905
Iteration 22/1000 | Loss: 0.00002799
Iteration 23/1000 | Loss: 0.00002722
Iteration 24/1000 | Loss: 0.00002687
Iteration 25/1000 | Loss: 0.00002669
Iteration 26/1000 | Loss: 0.00002669
Iteration 27/1000 | Loss: 0.00002665
Iteration 28/1000 | Loss: 0.00002664
Iteration 29/1000 | Loss: 0.00002664
Iteration 30/1000 | Loss: 0.00002663
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002660
Iteration 33/1000 | Loss: 0.00002660
Iteration 34/1000 | Loss: 0.00002659
Iteration 35/1000 | Loss: 0.00002659
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002658
Iteration 38/1000 | Loss: 0.00002658
Iteration 39/1000 | Loss: 0.00002657
Iteration 40/1000 | Loss: 0.00002656
Iteration 41/1000 | Loss: 0.00002653
Iteration 42/1000 | Loss: 0.00002652
Iteration 43/1000 | Loss: 0.00002652
Iteration 44/1000 | Loss: 0.00002651
Iteration 45/1000 | Loss: 0.00002651
Iteration 46/1000 | Loss: 0.00002650
Iteration 47/1000 | Loss: 0.00002650
Iteration 48/1000 | Loss: 0.00002650
Iteration 49/1000 | Loss: 0.00002650
Iteration 50/1000 | Loss: 0.00002650
Iteration 51/1000 | Loss: 0.00002650
Iteration 52/1000 | Loss: 0.00002649
Iteration 53/1000 | Loss: 0.00002649
Iteration 54/1000 | Loss: 0.00002649
Iteration 55/1000 | Loss: 0.00002649
Iteration 56/1000 | Loss: 0.00002649
Iteration 57/1000 | Loss: 0.00002649
Iteration 58/1000 | Loss: 0.00002649
Iteration 59/1000 | Loss: 0.00002648
Iteration 60/1000 | Loss: 0.00002648
Iteration 61/1000 | Loss: 0.00002648
Iteration 62/1000 | Loss: 0.00002648
Iteration 63/1000 | Loss: 0.00002648
Iteration 64/1000 | Loss: 0.00002648
Iteration 65/1000 | Loss: 0.00002648
Iteration 66/1000 | Loss: 0.00002648
Iteration 67/1000 | Loss: 0.00002648
Iteration 68/1000 | Loss: 0.00002647
Iteration 69/1000 | Loss: 0.00002647
Iteration 70/1000 | Loss: 0.00002647
Iteration 71/1000 | Loss: 0.00002647
Iteration 72/1000 | Loss: 0.00002647
Iteration 73/1000 | Loss: 0.00002647
Iteration 74/1000 | Loss: 0.00002647
Iteration 75/1000 | Loss: 0.00002646
Iteration 76/1000 | Loss: 0.00002646
Iteration 77/1000 | Loss: 0.00002646
Iteration 78/1000 | Loss: 0.00002646
Iteration 79/1000 | Loss: 0.00002646
Iteration 80/1000 | Loss: 0.00002645
Iteration 81/1000 | Loss: 0.00002645
Iteration 82/1000 | Loss: 0.00002645
Iteration 83/1000 | Loss: 0.00002645
Iteration 84/1000 | Loss: 0.00002645
Iteration 85/1000 | Loss: 0.00002645
Iteration 86/1000 | Loss: 0.00002644
Iteration 87/1000 | Loss: 0.00002644
Iteration 88/1000 | Loss: 0.00002644
Iteration 89/1000 | Loss: 0.00002644
Iteration 90/1000 | Loss: 0.00002644
Iteration 91/1000 | Loss: 0.00002644
Iteration 92/1000 | Loss: 0.00002644
Iteration 93/1000 | Loss: 0.00002644
Iteration 94/1000 | Loss: 0.00002644
Iteration 95/1000 | Loss: 0.00002644
Iteration 96/1000 | Loss: 0.00002644
Iteration 97/1000 | Loss: 0.00002644
Iteration 98/1000 | Loss: 0.00002644
Iteration 99/1000 | Loss: 0.00002644
Iteration 100/1000 | Loss: 0.00002644
Iteration 101/1000 | Loss: 0.00002644
Iteration 102/1000 | Loss: 0.00002644
Iteration 103/1000 | Loss: 0.00002643
Iteration 104/1000 | Loss: 0.00002643
Iteration 105/1000 | Loss: 0.00002643
Iteration 106/1000 | Loss: 0.00002643
Iteration 107/1000 | Loss: 0.00002643
Iteration 108/1000 | Loss: 0.00002643
Iteration 109/1000 | Loss: 0.00002643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.6434603569214232e-05, 2.6434603569214232e-05, 2.6434603569214232e-05, 2.6434603569214232e-05, 2.6434603569214232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6434603569214232e-05

Optimization complete. Final v2v error: 4.520127296447754 mm

Highest mean error: 5.53957462310791 mm for frame 38

Lowest mean error: 3.9680330753326416 mm for frame 7

Saving results

Total time: 79.95214557647705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575121
Iteration 2/25 | Loss: 0.00118588
Iteration 3/25 | Loss: 0.00109301
Iteration 4/25 | Loss: 0.00107431
Iteration 5/25 | Loss: 0.00106703
Iteration 6/25 | Loss: 0.00106534
Iteration 7/25 | Loss: 0.00106468
Iteration 8/25 | Loss: 0.00106468
Iteration 9/25 | Loss: 0.00106468
Iteration 10/25 | Loss: 0.00106466
Iteration 11/25 | Loss: 0.00106466
Iteration 12/25 | Loss: 0.00106466
Iteration 13/25 | Loss: 0.00106466
Iteration 14/25 | Loss: 0.00106466
Iteration 15/25 | Loss: 0.00106466
Iteration 16/25 | Loss: 0.00106466
Iteration 17/25 | Loss: 0.00106466
Iteration 18/25 | Loss: 0.00106466
Iteration 19/25 | Loss: 0.00106466
Iteration 20/25 | Loss: 0.00106466
Iteration 21/25 | Loss: 0.00106466
Iteration 22/25 | Loss: 0.00106466
Iteration 23/25 | Loss: 0.00106466
Iteration 24/25 | Loss: 0.00106466
Iteration 25/25 | Loss: 0.00106466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.02509785
Iteration 2/25 | Loss: 0.00062543
Iteration 3/25 | Loss: 0.00062542
Iteration 4/25 | Loss: 0.00062542
Iteration 5/25 | Loss: 0.00062542
Iteration 6/25 | Loss: 0.00062542
Iteration 7/25 | Loss: 0.00062542
Iteration 8/25 | Loss: 0.00062542
Iteration 9/25 | Loss: 0.00062542
Iteration 10/25 | Loss: 0.00062542
Iteration 11/25 | Loss: 0.00062542
Iteration 12/25 | Loss: 0.00062542
Iteration 13/25 | Loss: 0.00062542
Iteration 14/25 | Loss: 0.00062542
Iteration 15/25 | Loss: 0.00062542
Iteration 16/25 | Loss: 0.00062542
Iteration 17/25 | Loss: 0.00062542
Iteration 18/25 | Loss: 0.00062542
Iteration 19/25 | Loss: 0.00062542
Iteration 20/25 | Loss: 0.00062542
Iteration 21/25 | Loss: 0.00062542
Iteration 22/25 | Loss: 0.00062542
Iteration 23/25 | Loss: 0.00062542
Iteration 24/25 | Loss: 0.00062542
Iteration 25/25 | Loss: 0.00062542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062542
Iteration 2/1000 | Loss: 0.00003307
Iteration 3/1000 | Loss: 0.00002687
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00002351
Iteration 7/1000 | Loss: 0.00002300
Iteration 8/1000 | Loss: 0.00002282
Iteration 9/1000 | Loss: 0.00002278
Iteration 10/1000 | Loss: 0.00002277
Iteration 11/1000 | Loss: 0.00002270
Iteration 12/1000 | Loss: 0.00002263
Iteration 13/1000 | Loss: 0.00002258
Iteration 14/1000 | Loss: 0.00002250
Iteration 15/1000 | Loss: 0.00002250
Iteration 16/1000 | Loss: 0.00002249
Iteration 17/1000 | Loss: 0.00002249
Iteration 18/1000 | Loss: 0.00002249
Iteration 19/1000 | Loss: 0.00002248
Iteration 20/1000 | Loss: 0.00002248
Iteration 21/1000 | Loss: 0.00002247
Iteration 22/1000 | Loss: 0.00002247
Iteration 23/1000 | Loss: 0.00002247
Iteration 24/1000 | Loss: 0.00002246
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002246
Iteration 27/1000 | Loss: 0.00002246
Iteration 28/1000 | Loss: 0.00002246
Iteration 29/1000 | Loss: 0.00002246
Iteration 30/1000 | Loss: 0.00002246
Iteration 31/1000 | Loss: 0.00002246
Iteration 32/1000 | Loss: 0.00002246
Iteration 33/1000 | Loss: 0.00002246
Iteration 34/1000 | Loss: 0.00002246
Iteration 35/1000 | Loss: 0.00002245
Iteration 36/1000 | Loss: 0.00002245
Iteration 37/1000 | Loss: 0.00002245
Iteration 38/1000 | Loss: 0.00002245
Iteration 39/1000 | Loss: 0.00002244
Iteration 40/1000 | Loss: 0.00002244
Iteration 41/1000 | Loss: 0.00002244
Iteration 42/1000 | Loss: 0.00002243
Iteration 43/1000 | Loss: 0.00002243
Iteration 44/1000 | Loss: 0.00002243
Iteration 45/1000 | Loss: 0.00002242
Iteration 46/1000 | Loss: 0.00002242
Iteration 47/1000 | Loss: 0.00002242
Iteration 48/1000 | Loss: 0.00002241
Iteration 49/1000 | Loss: 0.00002241
Iteration 50/1000 | Loss: 0.00002241
Iteration 51/1000 | Loss: 0.00002240
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002240
Iteration 54/1000 | Loss: 0.00002240
Iteration 55/1000 | Loss: 0.00002240
Iteration 56/1000 | Loss: 0.00002240
Iteration 57/1000 | Loss: 0.00002240
Iteration 58/1000 | Loss: 0.00002239
Iteration 59/1000 | Loss: 0.00002239
Iteration 60/1000 | Loss: 0.00002239
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002238
Iteration 63/1000 | Loss: 0.00002238
Iteration 64/1000 | Loss: 0.00002238
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002237
Iteration 70/1000 | Loss: 0.00002237
Iteration 71/1000 | Loss: 0.00002237
Iteration 72/1000 | Loss: 0.00002237
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002235
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002235
Iteration 82/1000 | Loss: 0.00002235
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00002235
Iteration 85/1000 | Loss: 0.00002235
Iteration 86/1000 | Loss: 0.00002235
Iteration 87/1000 | Loss: 0.00002235
Iteration 88/1000 | Loss: 0.00002235
Iteration 89/1000 | Loss: 0.00002234
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002234
Iteration 98/1000 | Loss: 0.00002234
Iteration 99/1000 | Loss: 0.00002234
Iteration 100/1000 | Loss: 0.00002234
Iteration 101/1000 | Loss: 0.00002234
Iteration 102/1000 | Loss: 0.00002234
Iteration 103/1000 | Loss: 0.00002234
Iteration 104/1000 | Loss: 0.00002234
Iteration 105/1000 | Loss: 0.00002234
Iteration 106/1000 | Loss: 0.00002234
Iteration 107/1000 | Loss: 0.00002234
Iteration 108/1000 | Loss: 0.00002234
Iteration 109/1000 | Loss: 0.00002234
Iteration 110/1000 | Loss: 0.00002234
Iteration 111/1000 | Loss: 0.00002234
Iteration 112/1000 | Loss: 0.00002234
Iteration 113/1000 | Loss: 0.00002234
Iteration 114/1000 | Loss: 0.00002234
Iteration 115/1000 | Loss: 0.00002234
Iteration 116/1000 | Loss: 0.00002234
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002234
Iteration 119/1000 | Loss: 0.00002234
Iteration 120/1000 | Loss: 0.00002234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.2337228074320592e-05, 2.2337228074320592e-05, 2.2337228074320592e-05, 2.2337228074320592e-05, 2.2337228074320592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2337228074320592e-05

Optimization complete. Final v2v error: 4.095487117767334 mm

Highest mean error: 4.430732727050781 mm for frame 59

Lowest mean error: 3.8083713054656982 mm for frame 124

Saving results

Total time: 32.37725830078125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965405
Iteration 2/25 | Loss: 0.00176506
Iteration 3/25 | Loss: 0.00134378
Iteration 4/25 | Loss: 0.00125081
Iteration 5/25 | Loss: 0.00125307
Iteration 6/25 | Loss: 0.00123880
Iteration 7/25 | Loss: 0.00122418
Iteration 8/25 | Loss: 0.00121098
Iteration 9/25 | Loss: 0.00119150
Iteration 10/25 | Loss: 0.00117688
Iteration 11/25 | Loss: 0.00117805
Iteration 12/25 | Loss: 0.00117055
Iteration 13/25 | Loss: 0.00116004
Iteration 14/25 | Loss: 0.00114601
Iteration 15/25 | Loss: 0.00114921
Iteration 16/25 | Loss: 0.00114042
Iteration 17/25 | Loss: 0.00113792
Iteration 18/25 | Loss: 0.00113543
Iteration 19/25 | Loss: 0.00112934
Iteration 20/25 | Loss: 0.00112661
Iteration 21/25 | Loss: 0.00112561
Iteration 22/25 | Loss: 0.00112694
Iteration 23/25 | Loss: 0.00112695
Iteration 24/25 | Loss: 0.00112921
Iteration 25/25 | Loss: 0.00112532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28381443
Iteration 2/25 | Loss: 0.00087179
Iteration 3/25 | Loss: 0.00087177
Iteration 4/25 | Loss: 0.00087177
Iteration 5/25 | Loss: 0.00087176
Iteration 6/25 | Loss: 0.00087176
Iteration 7/25 | Loss: 0.00087176
Iteration 8/25 | Loss: 0.00087176
Iteration 9/25 | Loss: 0.00087176
Iteration 10/25 | Loss: 0.00087176
Iteration 11/25 | Loss: 0.00087176
Iteration 12/25 | Loss: 0.00087176
Iteration 13/25 | Loss: 0.00087176
Iteration 14/25 | Loss: 0.00087176
Iteration 15/25 | Loss: 0.00087176
Iteration 16/25 | Loss: 0.00087176
Iteration 17/25 | Loss: 0.00087176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000871763564646244, 0.000871763564646244, 0.000871763564646244, 0.000871763564646244, 0.000871763564646244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000871763564646244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087176
Iteration 2/1000 | Loss: 0.00012001
Iteration 3/1000 | Loss: 0.00011137
Iteration 4/1000 | Loss: 0.00006132
Iteration 5/1000 | Loss: 0.00004874
Iteration 6/1000 | Loss: 0.00004163
Iteration 7/1000 | Loss: 0.00003995
Iteration 8/1000 | Loss: 0.00003889
Iteration 9/1000 | Loss: 0.00003791
Iteration 10/1000 | Loss: 0.00003706
Iteration 11/1000 | Loss: 0.00003644
Iteration 12/1000 | Loss: 0.00003615
Iteration 13/1000 | Loss: 0.00003581
Iteration 14/1000 | Loss: 0.00003563
Iteration 15/1000 | Loss: 0.00003558
Iteration 16/1000 | Loss: 0.00003554
Iteration 17/1000 | Loss: 0.00003550
Iteration 18/1000 | Loss: 0.00003547
Iteration 19/1000 | Loss: 0.00003546
Iteration 20/1000 | Loss: 0.00003542
Iteration 21/1000 | Loss: 0.00003542
Iteration 22/1000 | Loss: 0.00003542
Iteration 23/1000 | Loss: 0.00003541
Iteration 24/1000 | Loss: 0.00003540
Iteration 25/1000 | Loss: 0.00003537
Iteration 26/1000 | Loss: 0.00003536
Iteration 27/1000 | Loss: 0.00003536
Iteration 28/1000 | Loss: 0.00003536
Iteration 29/1000 | Loss: 0.00003535
Iteration 30/1000 | Loss: 0.00003535
Iteration 31/1000 | Loss: 0.00003535
Iteration 32/1000 | Loss: 0.00003534
Iteration 33/1000 | Loss: 0.00003534
Iteration 34/1000 | Loss: 0.00003534
Iteration 35/1000 | Loss: 0.00003534
Iteration 36/1000 | Loss: 0.00003534
Iteration 37/1000 | Loss: 0.00003533
Iteration 38/1000 | Loss: 0.00003533
Iteration 39/1000 | Loss: 0.00003533
Iteration 40/1000 | Loss: 0.00003533
Iteration 41/1000 | Loss: 0.00003533
Iteration 42/1000 | Loss: 0.00003533
Iteration 43/1000 | Loss: 0.00003533
Iteration 44/1000 | Loss: 0.00003533
Iteration 45/1000 | Loss: 0.00003533
Iteration 46/1000 | Loss: 0.00003533
Iteration 47/1000 | Loss: 0.00003533
Iteration 48/1000 | Loss: 0.00003532
Iteration 49/1000 | Loss: 0.00003532
Iteration 50/1000 | Loss: 0.00003532
Iteration 51/1000 | Loss: 0.00003532
Iteration 52/1000 | Loss: 0.00003531
Iteration 53/1000 | Loss: 0.00003531
Iteration 54/1000 | Loss: 0.00003531
Iteration 55/1000 | Loss: 0.00003530
Iteration 56/1000 | Loss: 0.00003530
Iteration 57/1000 | Loss: 0.00003530
Iteration 58/1000 | Loss: 0.00003530
Iteration 59/1000 | Loss: 0.00003529
Iteration 60/1000 | Loss: 0.00003529
Iteration 61/1000 | Loss: 0.00003528
Iteration 62/1000 | Loss: 0.00003528
Iteration 63/1000 | Loss: 0.00003528
Iteration 64/1000 | Loss: 0.00003527
Iteration 65/1000 | Loss: 0.00003527
Iteration 66/1000 | Loss: 0.00003527
Iteration 67/1000 | Loss: 0.00003527
Iteration 68/1000 | Loss: 0.00003526
Iteration 69/1000 | Loss: 0.00003526
Iteration 70/1000 | Loss: 0.00003526
Iteration 71/1000 | Loss: 0.00003526
Iteration 72/1000 | Loss: 0.00003526
Iteration 73/1000 | Loss: 0.00003525
Iteration 74/1000 | Loss: 0.00003525
Iteration 75/1000 | Loss: 0.00003525
Iteration 76/1000 | Loss: 0.00003525
Iteration 77/1000 | Loss: 0.00003525
Iteration 78/1000 | Loss: 0.00003524
Iteration 79/1000 | Loss: 0.00003524
Iteration 80/1000 | Loss: 0.00003524
Iteration 81/1000 | Loss: 0.00003524
Iteration 82/1000 | Loss: 0.00003524
Iteration 83/1000 | Loss: 0.00003524
Iteration 84/1000 | Loss: 0.00003523
Iteration 85/1000 | Loss: 0.00003523
Iteration 86/1000 | Loss: 0.00003523
Iteration 87/1000 | Loss: 0.00003523
Iteration 88/1000 | Loss: 0.00003523
Iteration 89/1000 | Loss: 0.00003523
Iteration 90/1000 | Loss: 0.00003523
Iteration 91/1000 | Loss: 0.00003523
Iteration 92/1000 | Loss: 0.00003523
Iteration 93/1000 | Loss: 0.00003523
Iteration 94/1000 | Loss: 0.00003522
Iteration 95/1000 | Loss: 0.00003522
Iteration 96/1000 | Loss: 0.00003522
Iteration 97/1000 | Loss: 0.00003522
Iteration 98/1000 | Loss: 0.00003522
Iteration 99/1000 | Loss: 0.00003521
Iteration 100/1000 | Loss: 0.00003521
Iteration 101/1000 | Loss: 0.00003521
Iteration 102/1000 | Loss: 0.00003521
Iteration 103/1000 | Loss: 0.00003521
Iteration 104/1000 | Loss: 0.00003521
Iteration 105/1000 | Loss: 0.00003521
Iteration 106/1000 | Loss: 0.00003521
Iteration 107/1000 | Loss: 0.00003521
Iteration 108/1000 | Loss: 0.00003520
Iteration 109/1000 | Loss: 0.00003520
Iteration 110/1000 | Loss: 0.00003520
Iteration 111/1000 | Loss: 0.00003520
Iteration 112/1000 | Loss: 0.00003519
Iteration 113/1000 | Loss: 0.00003519
Iteration 114/1000 | Loss: 0.00003519
Iteration 115/1000 | Loss: 0.00003519
Iteration 116/1000 | Loss: 0.00003519
Iteration 117/1000 | Loss: 0.00003519
Iteration 118/1000 | Loss: 0.00003519
Iteration 119/1000 | Loss: 0.00003519
Iteration 120/1000 | Loss: 0.00003518
Iteration 121/1000 | Loss: 0.00003518
Iteration 122/1000 | Loss: 0.00003518
Iteration 123/1000 | Loss: 0.00003518
Iteration 124/1000 | Loss: 0.00003518
Iteration 125/1000 | Loss: 0.00003518
Iteration 126/1000 | Loss: 0.00003518
Iteration 127/1000 | Loss: 0.00003518
Iteration 128/1000 | Loss: 0.00003518
Iteration 129/1000 | Loss: 0.00003518
Iteration 130/1000 | Loss: 0.00003518
Iteration 131/1000 | Loss: 0.00003518
Iteration 132/1000 | Loss: 0.00003517
Iteration 133/1000 | Loss: 0.00003517
Iteration 134/1000 | Loss: 0.00003517
Iteration 135/1000 | Loss: 0.00003517
Iteration 136/1000 | Loss: 0.00003517
Iteration 137/1000 | Loss: 0.00003517
Iteration 138/1000 | Loss: 0.00003517
Iteration 139/1000 | Loss: 0.00003517
Iteration 140/1000 | Loss: 0.00003517
Iteration 141/1000 | Loss: 0.00003517
Iteration 142/1000 | Loss: 0.00003517
Iteration 143/1000 | Loss: 0.00003517
Iteration 144/1000 | Loss: 0.00003517
Iteration 145/1000 | Loss: 0.00003517
Iteration 146/1000 | Loss: 0.00003517
Iteration 147/1000 | Loss: 0.00003517
Iteration 148/1000 | Loss: 0.00003517
Iteration 149/1000 | Loss: 0.00003516
Iteration 150/1000 | Loss: 0.00003516
Iteration 151/1000 | Loss: 0.00003516
Iteration 152/1000 | Loss: 0.00003516
Iteration 153/1000 | Loss: 0.00003516
Iteration 154/1000 | Loss: 0.00003516
Iteration 155/1000 | Loss: 0.00003516
Iteration 156/1000 | Loss: 0.00003516
Iteration 157/1000 | Loss: 0.00003516
Iteration 158/1000 | Loss: 0.00003516
Iteration 159/1000 | Loss: 0.00003516
Iteration 160/1000 | Loss: 0.00003516
Iteration 161/1000 | Loss: 0.00003516
Iteration 162/1000 | Loss: 0.00003516
Iteration 163/1000 | Loss: 0.00003516
Iteration 164/1000 | Loss: 0.00003516
Iteration 165/1000 | Loss: 0.00003516
Iteration 166/1000 | Loss: 0.00003516
Iteration 167/1000 | Loss: 0.00003516
Iteration 168/1000 | Loss: 0.00003516
Iteration 169/1000 | Loss: 0.00003516
Iteration 170/1000 | Loss: 0.00003516
Iteration 171/1000 | Loss: 0.00003516
Iteration 172/1000 | Loss: 0.00003516
Iteration 173/1000 | Loss: 0.00003516
Iteration 174/1000 | Loss: 0.00003516
Iteration 175/1000 | Loss: 0.00003515
Iteration 176/1000 | Loss: 0.00003515
Iteration 177/1000 | Loss: 0.00003515
Iteration 178/1000 | Loss: 0.00003515
Iteration 179/1000 | Loss: 0.00003515
Iteration 180/1000 | Loss: 0.00003515
Iteration 181/1000 | Loss: 0.00003515
Iteration 182/1000 | Loss: 0.00003515
Iteration 183/1000 | Loss: 0.00003515
Iteration 184/1000 | Loss: 0.00003515
Iteration 185/1000 | Loss: 0.00003515
Iteration 186/1000 | Loss: 0.00003515
Iteration 187/1000 | Loss: 0.00003515
Iteration 188/1000 | Loss: 0.00003515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [3.515411663101986e-05, 3.515411663101986e-05, 3.515411663101986e-05, 3.515411663101986e-05, 3.515411663101986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.515411663101986e-05

Optimization complete. Final v2v error: 5.105226993560791 mm

Highest mean error: 8.531318664550781 mm for frame 94

Lowest mean error: 4.392866611480713 mm for frame 133

Saving results

Total time: 83.9872670173645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082685
Iteration 2/25 | Loss: 0.00266824
Iteration 3/25 | Loss: 0.00211305
Iteration 4/25 | Loss: 0.00206049
Iteration 5/25 | Loss: 0.00174721
Iteration 6/25 | Loss: 0.00159252
Iteration 7/25 | Loss: 0.00144343
Iteration 8/25 | Loss: 0.00126186
Iteration 9/25 | Loss: 0.00120960
Iteration 10/25 | Loss: 0.00120131
Iteration 11/25 | Loss: 0.00119351
Iteration 12/25 | Loss: 0.00118780
Iteration 13/25 | Loss: 0.00118624
Iteration 14/25 | Loss: 0.00118578
Iteration 15/25 | Loss: 0.00118557
Iteration 16/25 | Loss: 0.00118542
Iteration 17/25 | Loss: 0.00118533
Iteration 18/25 | Loss: 0.00118519
Iteration 19/25 | Loss: 0.00118512
Iteration 20/25 | Loss: 0.00118512
Iteration 21/25 | Loss: 0.00118512
Iteration 22/25 | Loss: 0.00118512
Iteration 23/25 | Loss: 0.00118511
Iteration 24/25 | Loss: 0.00118511
Iteration 25/25 | Loss: 0.00118511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30682087
Iteration 2/25 | Loss: 0.00129179
Iteration 3/25 | Loss: 0.00129179
Iteration 4/25 | Loss: 0.00129179
Iteration 5/25 | Loss: 0.00129179
Iteration 6/25 | Loss: 0.00129179
Iteration 7/25 | Loss: 0.00129179
Iteration 8/25 | Loss: 0.00129179
Iteration 9/25 | Loss: 0.00129179
Iteration 10/25 | Loss: 0.00129179
Iteration 11/25 | Loss: 0.00129179
Iteration 12/25 | Loss: 0.00129179
Iteration 13/25 | Loss: 0.00129179
Iteration 14/25 | Loss: 0.00129179
Iteration 15/25 | Loss: 0.00129179
Iteration 16/25 | Loss: 0.00129179
Iteration 17/25 | Loss: 0.00129179
Iteration 18/25 | Loss: 0.00129179
Iteration 19/25 | Loss: 0.00129179
Iteration 20/25 | Loss: 0.00129179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012917900457978249, 0.0012917900457978249, 0.0012917900457978249, 0.0012917900457978249, 0.0012917900457978249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012917900457978249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129179
Iteration 2/1000 | Loss: 0.00014849
Iteration 3/1000 | Loss: 0.00034382
Iteration 4/1000 | Loss: 0.00097349
Iteration 5/1000 | Loss: 0.00066038
Iteration 6/1000 | Loss: 0.00060361
Iteration 7/1000 | Loss: 0.00053327
Iteration 8/1000 | Loss: 0.00043662
Iteration 9/1000 | Loss: 0.00047528
Iteration 10/1000 | Loss: 0.00045285
Iteration 11/1000 | Loss: 0.00046618
Iteration 12/1000 | Loss: 0.00042500
Iteration 13/1000 | Loss: 0.00026686
Iteration 14/1000 | Loss: 0.00008569
Iteration 15/1000 | Loss: 0.00007264
Iteration 16/1000 | Loss: 0.00006750
Iteration 17/1000 | Loss: 0.00073349
Iteration 18/1000 | Loss: 0.00084074
Iteration 19/1000 | Loss: 0.00048611
Iteration 20/1000 | Loss: 0.00009915
Iteration 21/1000 | Loss: 0.00037574
Iteration 22/1000 | Loss: 0.00066428
Iteration 23/1000 | Loss: 0.00037849
Iteration 24/1000 | Loss: 0.00036450
Iteration 25/1000 | Loss: 0.00050430
Iteration 26/1000 | Loss: 0.00040271
Iteration 27/1000 | Loss: 0.00072180
Iteration 28/1000 | Loss: 0.00071258
Iteration 29/1000 | Loss: 0.00051960
Iteration 30/1000 | Loss: 0.00014129
Iteration 31/1000 | Loss: 0.00034221
Iteration 32/1000 | Loss: 0.00007759
Iteration 33/1000 | Loss: 0.00016117
Iteration 34/1000 | Loss: 0.00008490
Iteration 35/1000 | Loss: 0.00035767
Iteration 36/1000 | Loss: 0.00029494
Iteration 37/1000 | Loss: 0.00039946
Iteration 38/1000 | Loss: 0.00026802
Iteration 39/1000 | Loss: 0.00024805
Iteration 40/1000 | Loss: 0.00032214
Iteration 41/1000 | Loss: 0.00052165
Iteration 42/1000 | Loss: 0.00011318
Iteration 43/1000 | Loss: 0.00020996
Iteration 44/1000 | Loss: 0.00015664
Iteration 45/1000 | Loss: 0.00019785
Iteration 46/1000 | Loss: 0.00005160
Iteration 47/1000 | Loss: 0.00035050
Iteration 48/1000 | Loss: 0.00037925
Iteration 49/1000 | Loss: 0.00064622
Iteration 50/1000 | Loss: 0.00034484
Iteration 51/1000 | Loss: 0.00060738
Iteration 52/1000 | Loss: 0.00034705
Iteration 53/1000 | Loss: 0.00048815
Iteration 54/1000 | Loss: 0.00039105
Iteration 55/1000 | Loss: 0.00035097
Iteration 56/1000 | Loss: 0.00069901
Iteration 57/1000 | Loss: 0.00035810
Iteration 58/1000 | Loss: 0.00050359
Iteration 59/1000 | Loss: 0.00030906
Iteration 60/1000 | Loss: 0.00053271
Iteration 61/1000 | Loss: 0.00031421
Iteration 62/1000 | Loss: 0.00052948
Iteration 63/1000 | Loss: 0.00057125
Iteration 64/1000 | Loss: 0.00040445
Iteration 65/1000 | Loss: 0.00057846
Iteration 66/1000 | Loss: 0.00010939
Iteration 67/1000 | Loss: 0.00006710
Iteration 68/1000 | Loss: 0.00005790
Iteration 69/1000 | Loss: 0.00005134
Iteration 70/1000 | Loss: 0.00004614
Iteration 71/1000 | Loss: 0.00023254
Iteration 72/1000 | Loss: 0.00016743
Iteration 73/1000 | Loss: 0.00020086
Iteration 74/1000 | Loss: 0.00024630
Iteration 75/1000 | Loss: 0.00024008
Iteration 76/1000 | Loss: 0.00022834
Iteration 77/1000 | Loss: 0.00012764
Iteration 78/1000 | Loss: 0.00011289
Iteration 79/1000 | Loss: 0.00012430
Iteration 80/1000 | Loss: 0.00012771
Iteration 81/1000 | Loss: 0.00012376
Iteration 82/1000 | Loss: 0.00030926
Iteration 83/1000 | Loss: 0.00020720
Iteration 84/1000 | Loss: 0.00013811
Iteration 85/1000 | Loss: 0.00018031
Iteration 86/1000 | Loss: 0.00021329
Iteration 87/1000 | Loss: 0.00030612
Iteration 88/1000 | Loss: 0.00010108
Iteration 89/1000 | Loss: 0.00025311
Iteration 90/1000 | Loss: 0.00022645
Iteration 91/1000 | Loss: 0.00026724
Iteration 92/1000 | Loss: 0.00028728
Iteration 93/1000 | Loss: 0.00016044
Iteration 94/1000 | Loss: 0.00007572
Iteration 95/1000 | Loss: 0.00005247
Iteration 96/1000 | Loss: 0.00004375
Iteration 97/1000 | Loss: 0.00003884
Iteration 98/1000 | Loss: 0.00003711
Iteration 99/1000 | Loss: 0.00003608
Iteration 100/1000 | Loss: 0.00003533
Iteration 101/1000 | Loss: 0.00003491
Iteration 102/1000 | Loss: 0.00003458
Iteration 103/1000 | Loss: 0.00003437
Iteration 104/1000 | Loss: 0.00003427
Iteration 105/1000 | Loss: 0.00003418
Iteration 106/1000 | Loss: 0.00003417
Iteration 107/1000 | Loss: 0.00003417
Iteration 108/1000 | Loss: 0.00003417
Iteration 109/1000 | Loss: 0.00003416
Iteration 110/1000 | Loss: 0.00003411
Iteration 111/1000 | Loss: 0.00003409
Iteration 112/1000 | Loss: 0.00003408
Iteration 113/1000 | Loss: 0.00003408
Iteration 114/1000 | Loss: 0.00003407
Iteration 115/1000 | Loss: 0.00003407
Iteration 116/1000 | Loss: 0.00003406
Iteration 117/1000 | Loss: 0.00003405
Iteration 118/1000 | Loss: 0.00003405
Iteration 119/1000 | Loss: 0.00003404
Iteration 120/1000 | Loss: 0.00003404
Iteration 121/1000 | Loss: 0.00003403
Iteration 122/1000 | Loss: 0.00003403
Iteration 123/1000 | Loss: 0.00003399
Iteration 124/1000 | Loss: 0.00003397
Iteration 125/1000 | Loss: 0.00003397
Iteration 126/1000 | Loss: 0.00003395
Iteration 127/1000 | Loss: 0.00003394
Iteration 128/1000 | Loss: 0.00003394
Iteration 129/1000 | Loss: 0.00003394
Iteration 130/1000 | Loss: 0.00003394
Iteration 131/1000 | Loss: 0.00003394
Iteration 132/1000 | Loss: 0.00003394
Iteration 133/1000 | Loss: 0.00003394
Iteration 134/1000 | Loss: 0.00003394
Iteration 135/1000 | Loss: 0.00003394
Iteration 136/1000 | Loss: 0.00003394
Iteration 137/1000 | Loss: 0.00003394
Iteration 138/1000 | Loss: 0.00003393
Iteration 139/1000 | Loss: 0.00003393
Iteration 140/1000 | Loss: 0.00003393
Iteration 141/1000 | Loss: 0.00003392
Iteration 142/1000 | Loss: 0.00003392
Iteration 143/1000 | Loss: 0.00003392
Iteration 144/1000 | Loss: 0.00003391
Iteration 145/1000 | Loss: 0.00003391
Iteration 146/1000 | Loss: 0.00003391
Iteration 147/1000 | Loss: 0.00003391
Iteration 148/1000 | Loss: 0.00003391
Iteration 149/1000 | Loss: 0.00003391
Iteration 150/1000 | Loss: 0.00003390
Iteration 151/1000 | Loss: 0.00003390
Iteration 152/1000 | Loss: 0.00003390
Iteration 153/1000 | Loss: 0.00003390
Iteration 154/1000 | Loss: 0.00003390
Iteration 155/1000 | Loss: 0.00003390
Iteration 156/1000 | Loss: 0.00003390
Iteration 157/1000 | Loss: 0.00003390
Iteration 158/1000 | Loss: 0.00003390
Iteration 159/1000 | Loss: 0.00003389
Iteration 160/1000 | Loss: 0.00003389
Iteration 161/1000 | Loss: 0.00003389
Iteration 162/1000 | Loss: 0.00003389
Iteration 163/1000 | Loss: 0.00003389
Iteration 164/1000 | Loss: 0.00003389
Iteration 165/1000 | Loss: 0.00003389
Iteration 166/1000 | Loss: 0.00003389
Iteration 167/1000 | Loss: 0.00003389
Iteration 168/1000 | Loss: 0.00003389
Iteration 169/1000 | Loss: 0.00003389
Iteration 170/1000 | Loss: 0.00003389
Iteration 171/1000 | Loss: 0.00003389
Iteration 172/1000 | Loss: 0.00003389
Iteration 173/1000 | Loss: 0.00003389
Iteration 174/1000 | Loss: 0.00003389
Iteration 175/1000 | Loss: 0.00003389
Iteration 176/1000 | Loss: 0.00003389
Iteration 177/1000 | Loss: 0.00003389
Iteration 178/1000 | Loss: 0.00003389
Iteration 179/1000 | Loss: 0.00003389
Iteration 180/1000 | Loss: 0.00003389
Iteration 181/1000 | Loss: 0.00003389
Iteration 182/1000 | Loss: 0.00003389
Iteration 183/1000 | Loss: 0.00003389
Iteration 184/1000 | Loss: 0.00003389
Iteration 185/1000 | Loss: 0.00003389
Iteration 186/1000 | Loss: 0.00003389
Iteration 187/1000 | Loss: 0.00003389
Iteration 188/1000 | Loss: 0.00003389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [3.3891963539645076e-05, 3.3891963539645076e-05, 3.3891963539645076e-05, 3.3891963539645076e-05, 3.3891963539645076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3891963539645076e-05

Optimization complete. Final v2v error: 4.715014457702637 mm

Highest mean error: 13.176962852478027 mm for frame 9

Lowest mean error: 4.426088809967041 mm for frame 0

Saving results

Total time: 183.02675533294678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541685
Iteration 2/25 | Loss: 0.00148941
Iteration 3/25 | Loss: 0.00118386
Iteration 4/25 | Loss: 0.00112602
Iteration 5/25 | Loss: 0.00110951
Iteration 6/25 | Loss: 0.00110683
Iteration 7/25 | Loss: 0.00110637
Iteration 8/25 | Loss: 0.00110637
Iteration 9/25 | Loss: 0.00110637
Iteration 10/25 | Loss: 0.00110637
Iteration 11/25 | Loss: 0.00110637
Iteration 12/25 | Loss: 0.00110637
Iteration 13/25 | Loss: 0.00110637
Iteration 14/25 | Loss: 0.00110637
Iteration 15/25 | Loss: 0.00110637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001106369192712009, 0.001106369192712009, 0.001106369192712009, 0.001106369192712009, 0.001106369192712009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001106369192712009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69600070
Iteration 2/25 | Loss: 0.00061528
Iteration 3/25 | Loss: 0.00061528
Iteration 4/25 | Loss: 0.00061528
Iteration 5/25 | Loss: 0.00061528
Iteration 6/25 | Loss: 0.00061528
Iteration 7/25 | Loss: 0.00061528
Iteration 8/25 | Loss: 0.00061528
Iteration 9/25 | Loss: 0.00061528
Iteration 10/25 | Loss: 0.00061528
Iteration 11/25 | Loss: 0.00061528
Iteration 12/25 | Loss: 0.00061528
Iteration 13/25 | Loss: 0.00061528
Iteration 14/25 | Loss: 0.00061528
Iteration 15/25 | Loss: 0.00061528
Iteration 16/25 | Loss: 0.00061528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006152795976959169, 0.0006152795976959169, 0.0006152795976959169, 0.0006152795976959169, 0.0006152795976959169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006152795976959169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061528
Iteration 2/1000 | Loss: 0.00004572
Iteration 3/1000 | Loss: 0.00003624
Iteration 4/1000 | Loss: 0.00003289
Iteration 5/1000 | Loss: 0.00003138
Iteration 6/1000 | Loss: 0.00003065
Iteration 7/1000 | Loss: 0.00003008
Iteration 8/1000 | Loss: 0.00002978
Iteration 9/1000 | Loss: 0.00002943
Iteration 10/1000 | Loss: 0.00002925
Iteration 11/1000 | Loss: 0.00002918
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002915
Iteration 14/1000 | Loss: 0.00002915
Iteration 15/1000 | Loss: 0.00002911
Iteration 16/1000 | Loss: 0.00002910
Iteration 17/1000 | Loss: 0.00002909
Iteration 18/1000 | Loss: 0.00002909
Iteration 19/1000 | Loss: 0.00002907
Iteration 20/1000 | Loss: 0.00002907
Iteration 21/1000 | Loss: 0.00002907
Iteration 22/1000 | Loss: 0.00002907
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002906
Iteration 25/1000 | Loss: 0.00002906
Iteration 26/1000 | Loss: 0.00002906
Iteration 27/1000 | Loss: 0.00002905
Iteration 28/1000 | Loss: 0.00002904
Iteration 29/1000 | Loss: 0.00002904
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002904
Iteration 32/1000 | Loss: 0.00002904
Iteration 33/1000 | Loss: 0.00002904
Iteration 34/1000 | Loss: 0.00002904
Iteration 35/1000 | Loss: 0.00002904
Iteration 36/1000 | Loss: 0.00002904
Iteration 37/1000 | Loss: 0.00002904
Iteration 38/1000 | Loss: 0.00002904
Iteration 39/1000 | Loss: 0.00002904
Iteration 40/1000 | Loss: 0.00002903
Iteration 41/1000 | Loss: 0.00002903
Iteration 42/1000 | Loss: 0.00002903
Iteration 43/1000 | Loss: 0.00002903
Iteration 44/1000 | Loss: 0.00002903
Iteration 45/1000 | Loss: 0.00002903
Iteration 46/1000 | Loss: 0.00002903
Iteration 47/1000 | Loss: 0.00002903
Iteration 48/1000 | Loss: 0.00002903
Iteration 49/1000 | Loss: 0.00002902
Iteration 50/1000 | Loss: 0.00002902
Iteration 51/1000 | Loss: 0.00002902
Iteration 52/1000 | Loss: 0.00002902
Iteration 53/1000 | Loss: 0.00002902
Iteration 54/1000 | Loss: 0.00002902
Iteration 55/1000 | Loss: 0.00002902
Iteration 56/1000 | Loss: 0.00002902
Iteration 57/1000 | Loss: 0.00002902
Iteration 58/1000 | Loss: 0.00002902
Iteration 59/1000 | Loss: 0.00002901
Iteration 60/1000 | Loss: 0.00002901
Iteration 61/1000 | Loss: 0.00002901
Iteration 62/1000 | Loss: 0.00002901
Iteration 63/1000 | Loss: 0.00002901
Iteration 64/1000 | Loss: 0.00002901
Iteration 65/1000 | Loss: 0.00002901
Iteration 66/1000 | Loss: 0.00002900
Iteration 67/1000 | Loss: 0.00002900
Iteration 68/1000 | Loss: 0.00002900
Iteration 69/1000 | Loss: 0.00002900
Iteration 70/1000 | Loss: 0.00002900
Iteration 71/1000 | Loss: 0.00002900
Iteration 72/1000 | Loss: 0.00002900
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00002900
Iteration 75/1000 | Loss: 0.00002899
Iteration 76/1000 | Loss: 0.00002899
Iteration 77/1000 | Loss: 0.00002899
Iteration 78/1000 | Loss: 0.00002899
Iteration 79/1000 | Loss: 0.00002899
Iteration 80/1000 | Loss: 0.00002898
Iteration 81/1000 | Loss: 0.00002898
Iteration 82/1000 | Loss: 0.00002898
Iteration 83/1000 | Loss: 0.00002898
Iteration 84/1000 | Loss: 0.00002898
Iteration 85/1000 | Loss: 0.00002898
Iteration 86/1000 | Loss: 0.00002897
Iteration 87/1000 | Loss: 0.00002897
Iteration 88/1000 | Loss: 0.00002897
Iteration 89/1000 | Loss: 0.00002897
Iteration 90/1000 | Loss: 0.00002897
Iteration 91/1000 | Loss: 0.00002897
Iteration 92/1000 | Loss: 0.00002897
Iteration 93/1000 | Loss: 0.00002897
Iteration 94/1000 | Loss: 0.00002897
Iteration 95/1000 | Loss: 0.00002897
Iteration 96/1000 | Loss: 0.00002897
Iteration 97/1000 | Loss: 0.00002897
Iteration 98/1000 | Loss: 0.00002897
Iteration 99/1000 | Loss: 0.00002897
Iteration 100/1000 | Loss: 0.00002896
Iteration 101/1000 | Loss: 0.00002896
Iteration 102/1000 | Loss: 0.00002896
Iteration 103/1000 | Loss: 0.00002896
Iteration 104/1000 | Loss: 0.00002896
Iteration 105/1000 | Loss: 0.00002896
Iteration 106/1000 | Loss: 0.00002896
Iteration 107/1000 | Loss: 0.00002896
Iteration 108/1000 | Loss: 0.00002896
Iteration 109/1000 | Loss: 0.00002896
Iteration 110/1000 | Loss: 0.00002896
Iteration 111/1000 | Loss: 0.00002896
Iteration 112/1000 | Loss: 0.00002896
Iteration 113/1000 | Loss: 0.00002896
Iteration 114/1000 | Loss: 0.00002896
Iteration 115/1000 | Loss: 0.00002896
Iteration 116/1000 | Loss: 0.00002895
Iteration 117/1000 | Loss: 0.00002895
Iteration 118/1000 | Loss: 0.00002895
Iteration 119/1000 | Loss: 0.00002895
Iteration 120/1000 | Loss: 0.00002895
Iteration 121/1000 | Loss: 0.00002895
Iteration 122/1000 | Loss: 0.00002894
Iteration 123/1000 | Loss: 0.00002894
Iteration 124/1000 | Loss: 0.00002894
Iteration 125/1000 | Loss: 0.00002894
Iteration 126/1000 | Loss: 0.00002894
Iteration 127/1000 | Loss: 0.00002894
Iteration 128/1000 | Loss: 0.00002894
Iteration 129/1000 | Loss: 0.00002894
Iteration 130/1000 | Loss: 0.00002894
Iteration 131/1000 | Loss: 0.00002893
Iteration 132/1000 | Loss: 0.00002893
Iteration 133/1000 | Loss: 0.00002893
Iteration 134/1000 | Loss: 0.00002893
Iteration 135/1000 | Loss: 0.00002892
Iteration 136/1000 | Loss: 0.00002892
Iteration 137/1000 | Loss: 0.00002892
Iteration 138/1000 | Loss: 0.00002892
Iteration 139/1000 | Loss: 0.00002891
Iteration 140/1000 | Loss: 0.00002891
Iteration 141/1000 | Loss: 0.00002891
Iteration 142/1000 | Loss: 0.00002891
Iteration 143/1000 | Loss: 0.00002891
Iteration 144/1000 | Loss: 0.00002891
Iteration 145/1000 | Loss: 0.00002891
Iteration 146/1000 | Loss: 0.00002891
Iteration 147/1000 | Loss: 0.00002891
Iteration 148/1000 | Loss: 0.00002891
Iteration 149/1000 | Loss: 0.00002890
Iteration 150/1000 | Loss: 0.00002890
Iteration 151/1000 | Loss: 0.00002890
Iteration 152/1000 | Loss: 0.00002890
Iteration 153/1000 | Loss: 0.00002890
Iteration 154/1000 | Loss: 0.00002890
Iteration 155/1000 | Loss: 0.00002890
Iteration 156/1000 | Loss: 0.00002890
Iteration 157/1000 | Loss: 0.00002889
Iteration 158/1000 | Loss: 0.00002889
Iteration 159/1000 | Loss: 0.00002889
Iteration 160/1000 | Loss: 0.00002889
Iteration 161/1000 | Loss: 0.00002889
Iteration 162/1000 | Loss: 0.00002889
Iteration 163/1000 | Loss: 0.00002889
Iteration 164/1000 | Loss: 0.00002889
Iteration 165/1000 | Loss: 0.00002889
Iteration 166/1000 | Loss: 0.00002889
Iteration 167/1000 | Loss: 0.00002888
Iteration 168/1000 | Loss: 0.00002888
Iteration 169/1000 | Loss: 0.00002888
Iteration 170/1000 | Loss: 0.00002888
Iteration 171/1000 | Loss: 0.00002888
Iteration 172/1000 | Loss: 0.00002888
Iteration 173/1000 | Loss: 0.00002888
Iteration 174/1000 | Loss: 0.00002888
Iteration 175/1000 | Loss: 0.00002888
Iteration 176/1000 | Loss: 0.00002888
Iteration 177/1000 | Loss: 0.00002888
Iteration 178/1000 | Loss: 0.00002888
Iteration 179/1000 | Loss: 0.00002888
Iteration 180/1000 | Loss: 0.00002888
Iteration 181/1000 | Loss: 0.00002888
Iteration 182/1000 | Loss: 0.00002888
Iteration 183/1000 | Loss: 0.00002888
Iteration 184/1000 | Loss: 0.00002888
Iteration 185/1000 | Loss: 0.00002888
Iteration 186/1000 | Loss: 0.00002888
Iteration 187/1000 | Loss: 0.00002888
Iteration 188/1000 | Loss: 0.00002888
Iteration 189/1000 | Loss: 0.00002888
Iteration 190/1000 | Loss: 0.00002888
Iteration 191/1000 | Loss: 0.00002888
Iteration 192/1000 | Loss: 0.00002888
Iteration 193/1000 | Loss: 0.00002888
Iteration 194/1000 | Loss: 0.00002888
Iteration 195/1000 | Loss: 0.00002888
Iteration 196/1000 | Loss: 0.00002888
Iteration 197/1000 | Loss: 0.00002888
Iteration 198/1000 | Loss: 0.00002888
Iteration 199/1000 | Loss: 0.00002888
Iteration 200/1000 | Loss: 0.00002888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.888363269448746e-05, 2.888363269448746e-05, 2.888363269448746e-05, 2.888363269448746e-05, 2.888363269448746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.888363269448746e-05

Optimization complete. Final v2v error: 4.669456958770752 mm

Highest mean error: 4.800450325012207 mm for frame 58

Lowest mean error: 4.514949321746826 mm for frame 35

Saving results

Total time: 42.6481409072876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865748
Iteration 2/25 | Loss: 0.00188052
Iteration 3/25 | Loss: 0.00163183
Iteration 4/25 | Loss: 0.00139359
Iteration 5/25 | Loss: 0.00138277
Iteration 6/25 | Loss: 0.00129659
Iteration 7/25 | Loss: 0.00127710
Iteration 8/25 | Loss: 0.00124301
Iteration 9/25 | Loss: 0.00122787
Iteration 10/25 | Loss: 0.00122769
Iteration 11/25 | Loss: 0.00122669
Iteration 12/25 | Loss: 0.00122782
Iteration 13/25 | Loss: 0.00122095
Iteration 14/25 | Loss: 0.00121146
Iteration 15/25 | Loss: 0.00120619
Iteration 16/25 | Loss: 0.00120475
Iteration 17/25 | Loss: 0.00120429
Iteration 18/25 | Loss: 0.00120410
Iteration 19/25 | Loss: 0.00122847
Iteration 20/25 | Loss: 0.00120751
Iteration 21/25 | Loss: 0.00120121
Iteration 22/25 | Loss: 0.00119902
Iteration 23/25 | Loss: 0.00119809
Iteration 24/25 | Loss: 0.00119741
Iteration 25/25 | Loss: 0.00119721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96534133
Iteration 2/25 | Loss: 0.00083083
Iteration 3/25 | Loss: 0.00083077
Iteration 4/25 | Loss: 0.00083077
Iteration 5/25 | Loss: 0.00083077
Iteration 6/25 | Loss: 0.00083077
Iteration 7/25 | Loss: 0.00083077
Iteration 8/25 | Loss: 0.00083077
Iteration 9/25 | Loss: 0.00083077
Iteration 10/25 | Loss: 0.00083077
Iteration 11/25 | Loss: 0.00083077
Iteration 12/25 | Loss: 0.00083077
Iteration 13/25 | Loss: 0.00083077
Iteration 14/25 | Loss: 0.00083077
Iteration 15/25 | Loss: 0.00083077
Iteration 16/25 | Loss: 0.00083077
Iteration 17/25 | Loss: 0.00083077
Iteration 18/25 | Loss: 0.00083077
Iteration 19/25 | Loss: 0.00083077
Iteration 20/25 | Loss: 0.00083077
Iteration 21/25 | Loss: 0.00083077
Iteration 22/25 | Loss: 0.00083077
Iteration 23/25 | Loss: 0.00083077
Iteration 24/25 | Loss: 0.00083077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008307707030326128, 0.0008307707030326128, 0.0008307707030326128, 0.0008307707030326128, 0.0008307707030326128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008307707030326128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083077
Iteration 2/1000 | Loss: 0.00010397
Iteration 3/1000 | Loss: 0.00006412
Iteration 4/1000 | Loss: 0.00005655
Iteration 5/1000 | Loss: 0.00007202
Iteration 6/1000 | Loss: 0.00005658
Iteration 7/1000 | Loss: 0.00006597
Iteration 8/1000 | Loss: 0.00027924
Iteration 9/1000 | Loss: 0.00005410
Iteration 10/1000 | Loss: 0.00015957
Iteration 11/1000 | Loss: 0.00004203
Iteration 12/1000 | Loss: 0.00015549
Iteration 13/1000 | Loss: 0.00003918
Iteration 14/1000 | Loss: 0.00003768
Iteration 15/1000 | Loss: 0.00019572
Iteration 16/1000 | Loss: 0.00005115
Iteration 17/1000 | Loss: 0.00003766
Iteration 18/1000 | Loss: 0.00003512
Iteration 19/1000 | Loss: 0.00003392
Iteration 20/1000 | Loss: 0.00003308
Iteration 21/1000 | Loss: 0.00003255
Iteration 22/1000 | Loss: 0.00003224
Iteration 23/1000 | Loss: 0.00003207
Iteration 24/1000 | Loss: 0.00003194
Iteration 25/1000 | Loss: 0.00003191
Iteration 26/1000 | Loss: 0.00003188
Iteration 27/1000 | Loss: 0.00003184
Iteration 28/1000 | Loss: 0.00003182
Iteration 29/1000 | Loss: 0.00003177
Iteration 30/1000 | Loss: 0.00003172
Iteration 31/1000 | Loss: 0.00003171
Iteration 32/1000 | Loss: 0.00003168
Iteration 33/1000 | Loss: 0.00003168
Iteration 34/1000 | Loss: 0.00003168
Iteration 35/1000 | Loss: 0.00003166
Iteration 36/1000 | Loss: 0.00003166
Iteration 37/1000 | Loss: 0.00003164
Iteration 38/1000 | Loss: 0.00003163
Iteration 39/1000 | Loss: 0.00003163
Iteration 40/1000 | Loss: 0.00003163
Iteration 41/1000 | Loss: 0.00003163
Iteration 42/1000 | Loss: 0.00003163
Iteration 43/1000 | Loss: 0.00003163
Iteration 44/1000 | Loss: 0.00003163
Iteration 45/1000 | Loss: 0.00003163
Iteration 46/1000 | Loss: 0.00003163
Iteration 47/1000 | Loss: 0.00003163
Iteration 48/1000 | Loss: 0.00003162
Iteration 49/1000 | Loss: 0.00003162
Iteration 50/1000 | Loss: 0.00003162
Iteration 51/1000 | Loss: 0.00003162
Iteration 52/1000 | Loss: 0.00003162
Iteration 53/1000 | Loss: 0.00003162
Iteration 54/1000 | Loss: 0.00003162
Iteration 55/1000 | Loss: 0.00003162
Iteration 56/1000 | Loss: 0.00003162
Iteration 57/1000 | Loss: 0.00003162
Iteration 58/1000 | Loss: 0.00003162
Iteration 59/1000 | Loss: 0.00003161
Iteration 60/1000 | Loss: 0.00003161
Iteration 61/1000 | Loss: 0.00003161
Iteration 62/1000 | Loss: 0.00003160
Iteration 63/1000 | Loss: 0.00003160
Iteration 64/1000 | Loss: 0.00003160
Iteration 65/1000 | Loss: 0.00003159
Iteration 66/1000 | Loss: 0.00003159
Iteration 67/1000 | Loss: 0.00003158
Iteration 68/1000 | Loss: 0.00003158
Iteration 69/1000 | Loss: 0.00003158
Iteration 70/1000 | Loss: 0.00003158
Iteration 71/1000 | Loss: 0.00003158
Iteration 72/1000 | Loss: 0.00003158
Iteration 73/1000 | Loss: 0.00003158
Iteration 74/1000 | Loss: 0.00003157
Iteration 75/1000 | Loss: 0.00003157
Iteration 76/1000 | Loss: 0.00003157
Iteration 77/1000 | Loss: 0.00003157
Iteration 78/1000 | Loss: 0.00003157
Iteration 79/1000 | Loss: 0.00003157
Iteration 80/1000 | Loss: 0.00003157
Iteration 81/1000 | Loss: 0.00003156
Iteration 82/1000 | Loss: 0.00003156
Iteration 83/1000 | Loss: 0.00003156
Iteration 84/1000 | Loss: 0.00003156
Iteration 85/1000 | Loss: 0.00003155
Iteration 86/1000 | Loss: 0.00003155
Iteration 87/1000 | Loss: 0.00003155
Iteration 88/1000 | Loss: 0.00003155
Iteration 89/1000 | Loss: 0.00003155
Iteration 90/1000 | Loss: 0.00003154
Iteration 91/1000 | Loss: 0.00003154
Iteration 92/1000 | Loss: 0.00003153
Iteration 93/1000 | Loss: 0.00003153
Iteration 94/1000 | Loss: 0.00003153
Iteration 95/1000 | Loss: 0.00003153
Iteration 96/1000 | Loss: 0.00003153
Iteration 97/1000 | Loss: 0.00003153
Iteration 98/1000 | Loss: 0.00003153
Iteration 99/1000 | Loss: 0.00003153
Iteration 100/1000 | Loss: 0.00003153
Iteration 101/1000 | Loss: 0.00005336
Iteration 102/1000 | Loss: 0.00004358
Iteration 103/1000 | Loss: 0.00003300
Iteration 104/1000 | Loss: 0.00004594
Iteration 105/1000 | Loss: 0.00003467
Iteration 106/1000 | Loss: 0.00005213
Iteration 107/1000 | Loss: 0.00003355
Iteration 108/1000 | Loss: 0.00005167
Iteration 109/1000 | Loss: 0.00003315
Iteration 110/1000 | Loss: 0.00004440
Iteration 111/1000 | Loss: 0.00003363
Iteration 112/1000 | Loss: 0.00005097
Iteration 113/1000 | Loss: 0.00003524
Iteration 114/1000 | Loss: 0.00005123
Iteration 115/1000 | Loss: 0.00003881
Iteration 116/1000 | Loss: 0.00003246
Iteration 117/1000 | Loss: 0.00003181
Iteration 118/1000 | Loss: 0.00003163
Iteration 119/1000 | Loss: 0.00003160
Iteration 120/1000 | Loss: 0.00003160
Iteration 121/1000 | Loss: 0.00003155
Iteration 122/1000 | Loss: 0.00003153
Iteration 123/1000 | Loss: 0.00003153
Iteration 124/1000 | Loss: 0.00003153
Iteration 125/1000 | Loss: 0.00003153
Iteration 126/1000 | Loss: 0.00003153
Iteration 127/1000 | Loss: 0.00003152
Iteration 128/1000 | Loss: 0.00003152
Iteration 129/1000 | Loss: 0.00003152
Iteration 130/1000 | Loss: 0.00003152
Iteration 131/1000 | Loss: 0.00003152
Iteration 132/1000 | Loss: 0.00003152
Iteration 133/1000 | Loss: 0.00003152
Iteration 134/1000 | Loss: 0.00003152
Iteration 135/1000 | Loss: 0.00003152
Iteration 136/1000 | Loss: 0.00003151
Iteration 137/1000 | Loss: 0.00003151
Iteration 138/1000 | Loss: 0.00003151
Iteration 139/1000 | Loss: 0.00003151
Iteration 140/1000 | Loss: 0.00003151
Iteration 141/1000 | Loss: 0.00003151
Iteration 142/1000 | Loss: 0.00003150
Iteration 143/1000 | Loss: 0.00003150
Iteration 144/1000 | Loss: 0.00003150
Iteration 145/1000 | Loss: 0.00003150
Iteration 146/1000 | Loss: 0.00003149
Iteration 147/1000 | Loss: 0.00003149
Iteration 148/1000 | Loss: 0.00003149
Iteration 149/1000 | Loss: 0.00003149
Iteration 150/1000 | Loss: 0.00003148
Iteration 151/1000 | Loss: 0.00003148
Iteration 152/1000 | Loss: 0.00003148
Iteration 153/1000 | Loss: 0.00003148
Iteration 154/1000 | Loss: 0.00003147
Iteration 155/1000 | Loss: 0.00003147
Iteration 156/1000 | Loss: 0.00003147
Iteration 157/1000 | Loss: 0.00003147
Iteration 158/1000 | Loss: 0.00003147
Iteration 159/1000 | Loss: 0.00003147
Iteration 160/1000 | Loss: 0.00003147
Iteration 161/1000 | Loss: 0.00003147
Iteration 162/1000 | Loss: 0.00003147
Iteration 163/1000 | Loss: 0.00003147
Iteration 164/1000 | Loss: 0.00003146
Iteration 165/1000 | Loss: 0.00003146
Iteration 166/1000 | Loss: 0.00003146
Iteration 167/1000 | Loss: 0.00003146
Iteration 168/1000 | Loss: 0.00003146
Iteration 169/1000 | Loss: 0.00003146
Iteration 170/1000 | Loss: 0.00003146
Iteration 171/1000 | Loss: 0.00003146
Iteration 172/1000 | Loss: 0.00003146
Iteration 173/1000 | Loss: 0.00003146
Iteration 174/1000 | Loss: 0.00003146
Iteration 175/1000 | Loss: 0.00003146
Iteration 176/1000 | Loss: 0.00003146
Iteration 177/1000 | Loss: 0.00003146
Iteration 178/1000 | Loss: 0.00003145
Iteration 179/1000 | Loss: 0.00003145
Iteration 180/1000 | Loss: 0.00003145
Iteration 181/1000 | Loss: 0.00003145
Iteration 182/1000 | Loss: 0.00003145
Iteration 183/1000 | Loss: 0.00003145
Iteration 184/1000 | Loss: 0.00003145
Iteration 185/1000 | Loss: 0.00003145
Iteration 186/1000 | Loss: 0.00003145
Iteration 187/1000 | Loss: 0.00003144
Iteration 188/1000 | Loss: 0.00003144
Iteration 189/1000 | Loss: 0.00003144
Iteration 190/1000 | Loss: 0.00003144
Iteration 191/1000 | Loss: 0.00003144
Iteration 192/1000 | Loss: 0.00003144
Iteration 193/1000 | Loss: 0.00003144
Iteration 194/1000 | Loss: 0.00003144
Iteration 195/1000 | Loss: 0.00003144
Iteration 196/1000 | Loss: 0.00003144
Iteration 197/1000 | Loss: 0.00003144
Iteration 198/1000 | Loss: 0.00003144
Iteration 199/1000 | Loss: 0.00003144
Iteration 200/1000 | Loss: 0.00003144
Iteration 201/1000 | Loss: 0.00003144
Iteration 202/1000 | Loss: 0.00003144
Iteration 203/1000 | Loss: 0.00003143
Iteration 204/1000 | Loss: 0.00003143
Iteration 205/1000 | Loss: 0.00003143
Iteration 206/1000 | Loss: 0.00003143
Iteration 207/1000 | Loss: 0.00003143
Iteration 208/1000 | Loss: 0.00003143
Iteration 209/1000 | Loss: 0.00003143
Iteration 210/1000 | Loss: 0.00003143
Iteration 211/1000 | Loss: 0.00003142
Iteration 212/1000 | Loss: 0.00003142
Iteration 213/1000 | Loss: 0.00003142
Iteration 214/1000 | Loss: 0.00003142
Iteration 215/1000 | Loss: 0.00003142
Iteration 216/1000 | Loss: 0.00003142
Iteration 217/1000 | Loss: 0.00003142
Iteration 218/1000 | Loss: 0.00003142
Iteration 219/1000 | Loss: 0.00003142
Iteration 220/1000 | Loss: 0.00003142
Iteration 221/1000 | Loss: 0.00003142
Iteration 222/1000 | Loss: 0.00003142
Iteration 223/1000 | Loss: 0.00003142
Iteration 224/1000 | Loss: 0.00003142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [3.141969864373095e-05, 3.141969864373095e-05, 3.141969864373095e-05, 3.141969864373095e-05, 3.141969864373095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.141969864373095e-05

Optimization complete. Final v2v error: 4.866108417510986 mm

Highest mean error: 10.705665588378906 mm for frame 192

Lowest mean error: 4.49962854385376 mm for frame 145

Saving results

Total time: 135.68407797813416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144421
Iteration 2/25 | Loss: 0.00181378
Iteration 3/25 | Loss: 0.00125066
Iteration 4/25 | Loss: 0.00120682
Iteration 5/25 | Loss: 0.00119103
Iteration 6/25 | Loss: 0.00118732
Iteration 7/25 | Loss: 0.00118742
Iteration 8/25 | Loss: 0.00118633
Iteration 9/25 | Loss: 0.00118490
Iteration 10/25 | Loss: 0.00118444
Iteration 11/25 | Loss: 0.00118412
Iteration 12/25 | Loss: 0.00118389
Iteration 13/25 | Loss: 0.00118380
Iteration 14/25 | Loss: 0.00118380
Iteration 15/25 | Loss: 0.00118380
Iteration 16/25 | Loss: 0.00118380
Iteration 17/25 | Loss: 0.00118380
Iteration 18/25 | Loss: 0.00118380
Iteration 19/25 | Loss: 0.00118380
Iteration 20/25 | Loss: 0.00118380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011837997008115053, 0.0011837997008115053, 0.0011837997008115053, 0.0011837997008115053, 0.0011837997008115053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011837997008115053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33571041
Iteration 2/25 | Loss: 0.00047186
Iteration 3/25 | Loss: 0.00047186
Iteration 4/25 | Loss: 0.00047186
Iteration 5/25 | Loss: 0.00047186
Iteration 6/25 | Loss: 0.00047186
Iteration 7/25 | Loss: 0.00047186
Iteration 8/25 | Loss: 0.00047186
Iteration 9/25 | Loss: 0.00047186
Iteration 10/25 | Loss: 0.00047186
Iteration 11/25 | Loss: 0.00047186
Iteration 12/25 | Loss: 0.00047186
Iteration 13/25 | Loss: 0.00047186
Iteration 14/25 | Loss: 0.00047186
Iteration 15/25 | Loss: 0.00047186
Iteration 16/25 | Loss: 0.00047186
Iteration 17/25 | Loss: 0.00047186
Iteration 18/25 | Loss: 0.00047186
Iteration 19/25 | Loss: 0.00047186
Iteration 20/25 | Loss: 0.00047186
Iteration 21/25 | Loss: 0.00047186
Iteration 22/25 | Loss: 0.00047186
Iteration 23/25 | Loss: 0.00047186
Iteration 24/25 | Loss: 0.00047186
Iteration 25/25 | Loss: 0.00047186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047186
Iteration 2/1000 | Loss: 0.00006250
Iteration 3/1000 | Loss: 0.00005182
Iteration 4/1000 | Loss: 0.00004790
Iteration 5/1000 | Loss: 0.00004595
Iteration 6/1000 | Loss: 0.00004486
Iteration 7/1000 | Loss: 0.00004419
Iteration 8/1000 | Loss: 0.00004375
Iteration 9/1000 | Loss: 0.00004342
Iteration 10/1000 | Loss: 0.00004314
Iteration 11/1000 | Loss: 0.00004295
Iteration 12/1000 | Loss: 0.00004291
Iteration 13/1000 | Loss: 0.00004291
Iteration 14/1000 | Loss: 0.00004282
Iteration 15/1000 | Loss: 0.00004279
Iteration 16/1000 | Loss: 0.00004278
Iteration 17/1000 | Loss: 0.00004277
Iteration 18/1000 | Loss: 0.00004276
Iteration 19/1000 | Loss: 0.00004276
Iteration 20/1000 | Loss: 0.00004275
Iteration 21/1000 | Loss: 0.00004275
Iteration 22/1000 | Loss: 0.00004274
Iteration 23/1000 | Loss: 0.00004271
Iteration 24/1000 | Loss: 0.00004271
Iteration 25/1000 | Loss: 0.00004271
Iteration 26/1000 | Loss: 0.00004270
Iteration 27/1000 | Loss: 0.00004270
Iteration 28/1000 | Loss: 0.00004269
Iteration 29/1000 | Loss: 0.00004269
Iteration 30/1000 | Loss: 0.00004268
Iteration 31/1000 | Loss: 0.00004268
Iteration 32/1000 | Loss: 0.00004268
Iteration 33/1000 | Loss: 0.00004267
Iteration 34/1000 | Loss: 0.00004267
Iteration 35/1000 | Loss: 0.00004267
Iteration 36/1000 | Loss: 0.00004267
Iteration 37/1000 | Loss: 0.00004267
Iteration 38/1000 | Loss: 0.00004267
Iteration 39/1000 | Loss: 0.00004266
Iteration 40/1000 | Loss: 0.00004266
Iteration 41/1000 | Loss: 0.00004266
Iteration 42/1000 | Loss: 0.00004266
Iteration 43/1000 | Loss: 0.00004266
Iteration 44/1000 | Loss: 0.00004265
Iteration 45/1000 | Loss: 0.00004265
Iteration 46/1000 | Loss: 0.00004265
Iteration 47/1000 | Loss: 0.00004265
Iteration 48/1000 | Loss: 0.00004265
Iteration 49/1000 | Loss: 0.00004265
Iteration 50/1000 | Loss: 0.00004265
Iteration 51/1000 | Loss: 0.00004264
Iteration 52/1000 | Loss: 0.00004264
Iteration 53/1000 | Loss: 0.00004264
Iteration 54/1000 | Loss: 0.00004264
Iteration 55/1000 | Loss: 0.00004264
Iteration 56/1000 | Loss: 0.00004264
Iteration 57/1000 | Loss: 0.00004264
Iteration 58/1000 | Loss: 0.00004264
Iteration 59/1000 | Loss: 0.00004264
Iteration 60/1000 | Loss: 0.00004264
Iteration 61/1000 | Loss: 0.00004263
Iteration 62/1000 | Loss: 0.00004263
Iteration 63/1000 | Loss: 0.00004263
Iteration 64/1000 | Loss: 0.00004263
Iteration 65/1000 | Loss: 0.00004263
Iteration 66/1000 | Loss: 0.00004263
Iteration 67/1000 | Loss: 0.00004263
Iteration 68/1000 | Loss: 0.00004263
Iteration 69/1000 | Loss: 0.00004263
Iteration 70/1000 | Loss: 0.00004263
Iteration 71/1000 | Loss: 0.00004263
Iteration 72/1000 | Loss: 0.00004263
Iteration 73/1000 | Loss: 0.00004263
Iteration 74/1000 | Loss: 0.00004263
Iteration 75/1000 | Loss: 0.00004263
Iteration 76/1000 | Loss: 0.00004263
Iteration 77/1000 | Loss: 0.00004263
Iteration 78/1000 | Loss: 0.00004263
Iteration 79/1000 | Loss: 0.00004263
Iteration 80/1000 | Loss: 0.00004263
Iteration 81/1000 | Loss: 0.00004263
Iteration 82/1000 | Loss: 0.00004263
Iteration 83/1000 | Loss: 0.00004263
Iteration 84/1000 | Loss: 0.00004263
Iteration 85/1000 | Loss: 0.00004263
Iteration 86/1000 | Loss: 0.00004263
Iteration 87/1000 | Loss: 0.00004263
Iteration 88/1000 | Loss: 0.00004263
Iteration 89/1000 | Loss: 0.00004263
Iteration 90/1000 | Loss: 0.00004263
Iteration 91/1000 | Loss: 0.00004263
Iteration 92/1000 | Loss: 0.00004263
Iteration 93/1000 | Loss: 0.00004263
Iteration 94/1000 | Loss: 0.00004263
Iteration 95/1000 | Loss: 0.00004263
Iteration 96/1000 | Loss: 0.00004263
Iteration 97/1000 | Loss: 0.00004263
Iteration 98/1000 | Loss: 0.00004263
Iteration 99/1000 | Loss: 0.00004263
Iteration 100/1000 | Loss: 0.00004263
Iteration 101/1000 | Loss: 0.00004263
Iteration 102/1000 | Loss: 0.00004263
Iteration 103/1000 | Loss: 0.00004263
Iteration 104/1000 | Loss: 0.00004263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [4.2628893424989656e-05, 4.2628893424989656e-05, 4.2628893424989656e-05, 4.2628893424989656e-05, 4.2628893424989656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2628893424989656e-05

Optimization complete. Final v2v error: 5.477534294128418 mm

Highest mean error: 11.351030349731445 mm for frame 3

Lowest mean error: 4.659277439117432 mm for frame 1

Saving results

Total time: 49.21361184120178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925617
Iteration 2/25 | Loss: 0.00120093
Iteration 3/25 | Loss: 0.00106608
Iteration 4/25 | Loss: 0.00104970
Iteration 5/25 | Loss: 0.00104545
Iteration 6/25 | Loss: 0.00104415
Iteration 7/25 | Loss: 0.00104415
Iteration 8/25 | Loss: 0.00104415
Iteration 9/25 | Loss: 0.00104415
Iteration 10/25 | Loss: 0.00104415
Iteration 11/25 | Loss: 0.00104415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010441546328365803, 0.0010441546328365803, 0.0010441546328365803, 0.0010441546328365803, 0.0010441546328365803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010441546328365803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.67709351
Iteration 2/25 | Loss: 0.00066812
Iteration 3/25 | Loss: 0.00066812
Iteration 4/25 | Loss: 0.00066812
Iteration 5/25 | Loss: 0.00066812
Iteration 6/25 | Loss: 0.00066812
Iteration 7/25 | Loss: 0.00066812
Iteration 8/25 | Loss: 0.00066812
Iteration 9/25 | Loss: 0.00066812
Iteration 10/25 | Loss: 0.00066812
Iteration 11/25 | Loss: 0.00066812
Iteration 12/25 | Loss: 0.00066812
Iteration 13/25 | Loss: 0.00066812
Iteration 14/25 | Loss: 0.00066812
Iteration 15/25 | Loss: 0.00066812
Iteration 16/25 | Loss: 0.00066812
Iteration 17/25 | Loss: 0.00066812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006681208615191281, 0.0006681208615191281, 0.0006681208615191281, 0.0006681208615191281, 0.0006681208615191281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006681208615191281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066812
Iteration 2/1000 | Loss: 0.00002699
Iteration 3/1000 | Loss: 0.00002253
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00002033
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001977
Iteration 8/1000 | Loss: 0.00001975
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001946
Iteration 26/1000 | Loss: 0.00001946
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001945
Iteration 34/1000 | Loss: 0.00001945
Iteration 35/1000 | Loss: 0.00001945
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001945
Iteration 39/1000 | Loss: 0.00001944
Iteration 40/1000 | Loss: 0.00001944
Iteration 41/1000 | Loss: 0.00001943
Iteration 42/1000 | Loss: 0.00001943
Iteration 43/1000 | Loss: 0.00001943
Iteration 44/1000 | Loss: 0.00001943
Iteration 45/1000 | Loss: 0.00001943
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001943
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00001943
Iteration 52/1000 | Loss: 0.00001943
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001942
Iteration 55/1000 | Loss: 0.00001942
Iteration 56/1000 | Loss: 0.00001942
Iteration 57/1000 | Loss: 0.00001941
Iteration 58/1000 | Loss: 0.00001941
Iteration 59/1000 | Loss: 0.00001941
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001940
Iteration 62/1000 | Loss: 0.00001940
Iteration 63/1000 | Loss: 0.00001940
Iteration 64/1000 | Loss: 0.00001940
Iteration 65/1000 | Loss: 0.00001940
Iteration 66/1000 | Loss: 0.00001940
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001939
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001938
Iteration 80/1000 | Loss: 0.00001938
Iteration 81/1000 | Loss: 0.00001938
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001936
Iteration 100/1000 | Loss: 0.00001936
Iteration 101/1000 | Loss: 0.00001936
Iteration 102/1000 | Loss: 0.00001936
Iteration 103/1000 | Loss: 0.00001936
Iteration 104/1000 | Loss: 0.00001935
Iteration 105/1000 | Loss: 0.00001935
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001935
Iteration 108/1000 | Loss: 0.00001935
Iteration 109/1000 | Loss: 0.00001935
Iteration 110/1000 | Loss: 0.00001935
Iteration 111/1000 | Loss: 0.00001935
Iteration 112/1000 | Loss: 0.00001935
Iteration 113/1000 | Loss: 0.00001935
Iteration 114/1000 | Loss: 0.00001935
Iteration 115/1000 | Loss: 0.00001935
Iteration 116/1000 | Loss: 0.00001935
Iteration 117/1000 | Loss: 0.00001935
Iteration 118/1000 | Loss: 0.00001935
Iteration 119/1000 | Loss: 0.00001935
Iteration 120/1000 | Loss: 0.00001935
Iteration 121/1000 | Loss: 0.00001935
Iteration 122/1000 | Loss: 0.00001935
Iteration 123/1000 | Loss: 0.00001935
Iteration 124/1000 | Loss: 0.00001935
Iteration 125/1000 | Loss: 0.00001934
Iteration 126/1000 | Loss: 0.00001934
Iteration 127/1000 | Loss: 0.00001934
Iteration 128/1000 | Loss: 0.00001934
Iteration 129/1000 | Loss: 0.00001933
Iteration 130/1000 | Loss: 0.00001933
Iteration 131/1000 | Loss: 0.00001933
Iteration 132/1000 | Loss: 0.00001933
Iteration 133/1000 | Loss: 0.00001933
Iteration 134/1000 | Loss: 0.00001933
Iteration 135/1000 | Loss: 0.00001932
Iteration 136/1000 | Loss: 0.00001932
Iteration 137/1000 | Loss: 0.00001932
Iteration 138/1000 | Loss: 0.00001932
Iteration 139/1000 | Loss: 0.00001932
Iteration 140/1000 | Loss: 0.00001932
Iteration 141/1000 | Loss: 0.00001932
Iteration 142/1000 | Loss: 0.00001932
Iteration 143/1000 | Loss: 0.00001931
Iteration 144/1000 | Loss: 0.00001931
Iteration 145/1000 | Loss: 0.00001931
Iteration 146/1000 | Loss: 0.00001931
Iteration 147/1000 | Loss: 0.00001930
Iteration 148/1000 | Loss: 0.00001930
Iteration 149/1000 | Loss: 0.00001930
Iteration 150/1000 | Loss: 0.00001930
Iteration 151/1000 | Loss: 0.00001930
Iteration 152/1000 | Loss: 0.00001930
Iteration 153/1000 | Loss: 0.00001930
Iteration 154/1000 | Loss: 0.00001930
Iteration 155/1000 | Loss: 0.00001930
Iteration 156/1000 | Loss: 0.00001930
Iteration 157/1000 | Loss: 0.00001930
Iteration 158/1000 | Loss: 0.00001930
Iteration 159/1000 | Loss: 0.00001930
Iteration 160/1000 | Loss: 0.00001930
Iteration 161/1000 | Loss: 0.00001930
Iteration 162/1000 | Loss: 0.00001930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.9298520783195272e-05, 1.9298520783195272e-05, 1.9298520783195272e-05, 1.9298520783195272e-05, 1.9298520783195272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9298520783195272e-05

Optimization complete. Final v2v error: 3.8483383655548096 mm

Highest mean error: 4.385798454284668 mm for frame 239

Lowest mean error: 3.5593488216400146 mm for frame 128

Saving results

Total time: 35.00870990753174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828820
Iteration 2/25 | Loss: 0.00133705
Iteration 3/25 | Loss: 0.00118123
Iteration 4/25 | Loss: 0.00115742
Iteration 5/25 | Loss: 0.00114953
Iteration 6/25 | Loss: 0.00114802
Iteration 7/25 | Loss: 0.00114802
Iteration 8/25 | Loss: 0.00114802
Iteration 9/25 | Loss: 0.00114802
Iteration 10/25 | Loss: 0.00114802
Iteration 11/25 | Loss: 0.00114802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001148021430708468, 0.001148021430708468, 0.001148021430708468, 0.001148021430708468, 0.001148021430708468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001148021430708468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26170945
Iteration 2/25 | Loss: 0.00084027
Iteration 3/25 | Loss: 0.00084026
Iteration 4/25 | Loss: 0.00084026
Iteration 5/25 | Loss: 0.00084026
Iteration 6/25 | Loss: 0.00084026
Iteration 7/25 | Loss: 0.00084025
Iteration 8/25 | Loss: 0.00084025
Iteration 9/25 | Loss: 0.00084025
Iteration 10/25 | Loss: 0.00084025
Iteration 11/25 | Loss: 0.00084025
Iteration 12/25 | Loss: 0.00084025
Iteration 13/25 | Loss: 0.00084025
Iteration 14/25 | Loss: 0.00084025
Iteration 15/25 | Loss: 0.00084025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008402543026022613, 0.0008402543026022613, 0.0008402543026022613, 0.0008402543026022613, 0.0008402543026022613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008402543026022613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084025
Iteration 2/1000 | Loss: 0.00004792
Iteration 3/1000 | Loss: 0.00003586
Iteration 4/1000 | Loss: 0.00003249
Iteration 5/1000 | Loss: 0.00003118
Iteration 6/1000 | Loss: 0.00002998
Iteration 7/1000 | Loss: 0.00002939
Iteration 8/1000 | Loss: 0.00002887
Iteration 9/1000 | Loss: 0.00002854
Iteration 10/1000 | Loss: 0.00002825
Iteration 11/1000 | Loss: 0.00002818
Iteration 12/1000 | Loss: 0.00002794
Iteration 13/1000 | Loss: 0.00002792
Iteration 14/1000 | Loss: 0.00002789
Iteration 15/1000 | Loss: 0.00002774
Iteration 16/1000 | Loss: 0.00002774
Iteration 17/1000 | Loss: 0.00002773
Iteration 18/1000 | Loss: 0.00002773
Iteration 19/1000 | Loss: 0.00002773
Iteration 20/1000 | Loss: 0.00002772
Iteration 21/1000 | Loss: 0.00002772
Iteration 22/1000 | Loss: 0.00002770
Iteration 23/1000 | Loss: 0.00002769
Iteration 24/1000 | Loss: 0.00002769
Iteration 25/1000 | Loss: 0.00002768
Iteration 26/1000 | Loss: 0.00002768
Iteration 27/1000 | Loss: 0.00002767
Iteration 28/1000 | Loss: 0.00002766
Iteration 29/1000 | Loss: 0.00002766
Iteration 30/1000 | Loss: 0.00002765
Iteration 31/1000 | Loss: 0.00002765
Iteration 32/1000 | Loss: 0.00002765
Iteration 33/1000 | Loss: 0.00002764
Iteration 34/1000 | Loss: 0.00002764
Iteration 35/1000 | Loss: 0.00002763
Iteration 36/1000 | Loss: 0.00002763
Iteration 37/1000 | Loss: 0.00002762
Iteration 38/1000 | Loss: 0.00002762
Iteration 39/1000 | Loss: 0.00002760
Iteration 40/1000 | Loss: 0.00002760
Iteration 41/1000 | Loss: 0.00002760
Iteration 42/1000 | Loss: 0.00002760
Iteration 43/1000 | Loss: 0.00002760
Iteration 44/1000 | Loss: 0.00002760
Iteration 45/1000 | Loss: 0.00002759
Iteration 46/1000 | Loss: 0.00002758
Iteration 47/1000 | Loss: 0.00002757
Iteration 48/1000 | Loss: 0.00002757
Iteration 49/1000 | Loss: 0.00002756
Iteration 50/1000 | Loss: 0.00002756
Iteration 51/1000 | Loss: 0.00002756
Iteration 52/1000 | Loss: 0.00002756
Iteration 53/1000 | Loss: 0.00002755
Iteration 54/1000 | Loss: 0.00002755
Iteration 55/1000 | Loss: 0.00002755
Iteration 56/1000 | Loss: 0.00002755
Iteration 57/1000 | Loss: 0.00002755
Iteration 58/1000 | Loss: 0.00002755
Iteration 59/1000 | Loss: 0.00002755
Iteration 60/1000 | Loss: 0.00002754
Iteration 61/1000 | Loss: 0.00002754
Iteration 62/1000 | Loss: 0.00002754
Iteration 63/1000 | Loss: 0.00002754
Iteration 64/1000 | Loss: 0.00002754
Iteration 65/1000 | Loss: 0.00002754
Iteration 66/1000 | Loss: 0.00002754
Iteration 67/1000 | Loss: 0.00002753
Iteration 68/1000 | Loss: 0.00002753
Iteration 69/1000 | Loss: 0.00002753
Iteration 70/1000 | Loss: 0.00002753
Iteration 71/1000 | Loss: 0.00002752
Iteration 72/1000 | Loss: 0.00002752
Iteration 73/1000 | Loss: 0.00002752
Iteration 74/1000 | Loss: 0.00002751
Iteration 75/1000 | Loss: 0.00002751
Iteration 76/1000 | Loss: 0.00002751
Iteration 77/1000 | Loss: 0.00002751
Iteration 78/1000 | Loss: 0.00002751
Iteration 79/1000 | Loss: 0.00002751
Iteration 80/1000 | Loss: 0.00002751
Iteration 81/1000 | Loss: 0.00002751
Iteration 82/1000 | Loss: 0.00002751
Iteration 83/1000 | Loss: 0.00002751
Iteration 84/1000 | Loss: 0.00002751
Iteration 85/1000 | Loss: 0.00002751
Iteration 86/1000 | Loss: 0.00002751
Iteration 87/1000 | Loss: 0.00002750
Iteration 88/1000 | Loss: 0.00002750
Iteration 89/1000 | Loss: 0.00002749
Iteration 90/1000 | Loss: 0.00002749
Iteration 91/1000 | Loss: 0.00002748
Iteration 92/1000 | Loss: 0.00002748
Iteration 93/1000 | Loss: 0.00002747
Iteration 94/1000 | Loss: 0.00002747
Iteration 95/1000 | Loss: 0.00002747
Iteration 96/1000 | Loss: 0.00002747
Iteration 97/1000 | Loss: 0.00002746
Iteration 98/1000 | Loss: 0.00002746
Iteration 99/1000 | Loss: 0.00002746
Iteration 100/1000 | Loss: 0.00002746
Iteration 101/1000 | Loss: 0.00002745
Iteration 102/1000 | Loss: 0.00002745
Iteration 103/1000 | Loss: 0.00002745
Iteration 104/1000 | Loss: 0.00002745
Iteration 105/1000 | Loss: 0.00002744
Iteration 106/1000 | Loss: 0.00002744
Iteration 107/1000 | Loss: 0.00002744
Iteration 108/1000 | Loss: 0.00002744
Iteration 109/1000 | Loss: 0.00002744
Iteration 110/1000 | Loss: 0.00002744
Iteration 111/1000 | Loss: 0.00002744
Iteration 112/1000 | Loss: 0.00002744
Iteration 113/1000 | Loss: 0.00002744
Iteration 114/1000 | Loss: 0.00002744
Iteration 115/1000 | Loss: 0.00002744
Iteration 116/1000 | Loss: 0.00002744
Iteration 117/1000 | Loss: 0.00002744
Iteration 118/1000 | Loss: 0.00002744
Iteration 119/1000 | Loss: 0.00002744
Iteration 120/1000 | Loss: 0.00002744
Iteration 121/1000 | Loss: 0.00002744
Iteration 122/1000 | Loss: 0.00002744
Iteration 123/1000 | Loss: 0.00002744
Iteration 124/1000 | Loss: 0.00002744
Iteration 125/1000 | Loss: 0.00002744
Iteration 126/1000 | Loss: 0.00002744
Iteration 127/1000 | Loss: 0.00002744
Iteration 128/1000 | Loss: 0.00002744
Iteration 129/1000 | Loss: 0.00002744
Iteration 130/1000 | Loss: 0.00002744
Iteration 131/1000 | Loss: 0.00002744
Iteration 132/1000 | Loss: 0.00002744
Iteration 133/1000 | Loss: 0.00002744
Iteration 134/1000 | Loss: 0.00002744
Iteration 135/1000 | Loss: 0.00002744
Iteration 136/1000 | Loss: 0.00002744
Iteration 137/1000 | Loss: 0.00002744
Iteration 138/1000 | Loss: 0.00002744
Iteration 139/1000 | Loss: 0.00002744
Iteration 140/1000 | Loss: 0.00002744
Iteration 141/1000 | Loss: 0.00002744
Iteration 142/1000 | Loss: 0.00002744
Iteration 143/1000 | Loss: 0.00002744
Iteration 144/1000 | Loss: 0.00002744
Iteration 145/1000 | Loss: 0.00002744
Iteration 146/1000 | Loss: 0.00002744
Iteration 147/1000 | Loss: 0.00002744
Iteration 148/1000 | Loss: 0.00002744
Iteration 149/1000 | Loss: 0.00002744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.7435562515165657e-05, 2.7435562515165657e-05, 2.7435562515165657e-05, 2.7435562515165657e-05, 2.7435562515165657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7435562515165657e-05

Optimization complete. Final v2v error: 4.668764114379883 mm

Highest mean error: 5.11154317855835 mm for frame 1

Lowest mean error: 4.400009632110596 mm for frame 137

Saving results

Total time: 39.92837858200073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01166081
Iteration 2/25 | Loss: 0.00221751
Iteration 3/25 | Loss: 0.00155429
Iteration 4/25 | Loss: 0.00138415
Iteration 5/25 | Loss: 0.00135194
Iteration 6/25 | Loss: 0.00132332
Iteration 7/25 | Loss: 0.00126383
Iteration 8/25 | Loss: 0.00122280
Iteration 9/25 | Loss: 0.00120026
Iteration 10/25 | Loss: 0.00118531
Iteration 11/25 | Loss: 0.00118811
Iteration 12/25 | Loss: 0.00119199
Iteration 13/25 | Loss: 0.00118042
Iteration 14/25 | Loss: 0.00118699
Iteration 15/25 | Loss: 0.00117741
Iteration 16/25 | Loss: 0.00116365
Iteration 17/25 | Loss: 0.00116827
Iteration 18/25 | Loss: 0.00116849
Iteration 19/25 | Loss: 0.00116015
Iteration 20/25 | Loss: 0.00115270
Iteration 21/25 | Loss: 0.00114634
Iteration 22/25 | Loss: 0.00114038
Iteration 23/25 | Loss: 0.00113821
Iteration 24/25 | Loss: 0.00113742
Iteration 25/25 | Loss: 0.00113803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30584657
Iteration 2/25 | Loss: 0.00073388
Iteration 3/25 | Loss: 0.00073388
Iteration 4/25 | Loss: 0.00073388
Iteration 5/25 | Loss: 0.00073388
Iteration 6/25 | Loss: 0.00073388
Iteration 7/25 | Loss: 0.00073388
Iteration 8/25 | Loss: 0.00073388
Iteration 9/25 | Loss: 0.00073388
Iteration 10/25 | Loss: 0.00073388
Iteration 11/25 | Loss: 0.00073388
Iteration 12/25 | Loss: 0.00073388
Iteration 13/25 | Loss: 0.00073388
Iteration 14/25 | Loss: 0.00073388
Iteration 15/25 | Loss: 0.00073388
Iteration 16/25 | Loss: 0.00073388
Iteration 17/25 | Loss: 0.00073388
Iteration 18/25 | Loss: 0.00073388
Iteration 19/25 | Loss: 0.00073388
Iteration 20/25 | Loss: 0.00073388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007338803261518478, 0.0007338803261518478, 0.0007338803261518478, 0.0007338803261518478, 0.0007338803261518478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007338803261518478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073388
Iteration 2/1000 | Loss: 0.00028177
Iteration 3/1000 | Loss: 0.00027637
Iteration 4/1000 | Loss: 0.00027559
Iteration 5/1000 | Loss: 0.00035868
Iteration 6/1000 | Loss: 0.00016726
Iteration 7/1000 | Loss: 0.00007681
Iteration 8/1000 | Loss: 0.00009142
Iteration 9/1000 | Loss: 0.00006517
Iteration 10/1000 | Loss: 0.00008395
Iteration 11/1000 | Loss: 0.00034564
Iteration 12/1000 | Loss: 0.00026362
Iteration 13/1000 | Loss: 0.00009082
Iteration 14/1000 | Loss: 0.00046531
Iteration 15/1000 | Loss: 0.00034378
Iteration 16/1000 | Loss: 0.00044515
Iteration 17/1000 | Loss: 0.00028113
Iteration 18/1000 | Loss: 0.00054003
Iteration 19/1000 | Loss: 0.00043602
Iteration 20/1000 | Loss: 0.00031081
Iteration 21/1000 | Loss: 0.00032617
Iteration 22/1000 | Loss: 0.00043166
Iteration 23/1000 | Loss: 0.00026946
Iteration 24/1000 | Loss: 0.00018554
Iteration 25/1000 | Loss: 0.00119116
Iteration 26/1000 | Loss: 0.00027179
Iteration 27/1000 | Loss: 0.00024915
Iteration 28/1000 | Loss: 0.00023989
Iteration 29/1000 | Loss: 0.00030334
Iteration 30/1000 | Loss: 0.00029399
Iteration 31/1000 | Loss: 0.00007456
Iteration 32/1000 | Loss: 0.00009557
Iteration 33/1000 | Loss: 0.00009568
Iteration 34/1000 | Loss: 0.00055938
Iteration 35/1000 | Loss: 0.00020862
Iteration 36/1000 | Loss: 0.00024716
Iteration 37/1000 | Loss: 0.00022142
Iteration 38/1000 | Loss: 0.00010868
Iteration 39/1000 | Loss: 0.00014785
Iteration 40/1000 | Loss: 0.00022604
Iteration 41/1000 | Loss: 0.00012233
Iteration 42/1000 | Loss: 0.00009924
Iteration 43/1000 | Loss: 0.00008402
Iteration 44/1000 | Loss: 0.00007983
Iteration 45/1000 | Loss: 0.00010345
Iteration 46/1000 | Loss: 0.00006460
Iteration 47/1000 | Loss: 0.00005437
Iteration 48/1000 | Loss: 0.00030633
Iteration 49/1000 | Loss: 0.00020033
Iteration 50/1000 | Loss: 0.00005356
Iteration 51/1000 | Loss: 0.00004751
Iteration 52/1000 | Loss: 0.00004471
Iteration 53/1000 | Loss: 0.00058674
Iteration 54/1000 | Loss: 0.00044952
Iteration 55/1000 | Loss: 0.00038530
Iteration 56/1000 | Loss: 0.00022952
Iteration 57/1000 | Loss: 0.00035603
Iteration 58/1000 | Loss: 0.00025263
Iteration 59/1000 | Loss: 0.00035053
Iteration 60/1000 | Loss: 0.00010960
Iteration 61/1000 | Loss: 0.00038400
Iteration 62/1000 | Loss: 0.00046344
Iteration 63/1000 | Loss: 0.00012425
Iteration 64/1000 | Loss: 0.00007561
Iteration 65/1000 | Loss: 0.00021991
Iteration 66/1000 | Loss: 0.00005751
Iteration 67/1000 | Loss: 0.00013149
Iteration 68/1000 | Loss: 0.00009137
Iteration 69/1000 | Loss: 0.00020595
Iteration 70/1000 | Loss: 0.00021078
Iteration 71/1000 | Loss: 0.00013008
Iteration 72/1000 | Loss: 0.00004372
Iteration 73/1000 | Loss: 0.00021650
Iteration 74/1000 | Loss: 0.00034005
Iteration 75/1000 | Loss: 0.00021820
Iteration 76/1000 | Loss: 0.00006469
Iteration 77/1000 | Loss: 0.00008234
Iteration 78/1000 | Loss: 0.00054593
Iteration 79/1000 | Loss: 0.00039540
Iteration 80/1000 | Loss: 0.00018825
Iteration 81/1000 | Loss: 0.00026546
Iteration 82/1000 | Loss: 0.00009328
Iteration 83/1000 | Loss: 0.00005630
Iteration 84/1000 | Loss: 0.00013987
Iteration 85/1000 | Loss: 0.00030796
Iteration 86/1000 | Loss: 0.00038336
Iteration 87/1000 | Loss: 0.00078115
Iteration 88/1000 | Loss: 0.00073714
Iteration 89/1000 | Loss: 0.00089739
Iteration 90/1000 | Loss: 0.00063529
Iteration 91/1000 | Loss: 0.00060504
Iteration 92/1000 | Loss: 0.00059817
Iteration 93/1000 | Loss: 0.00071128
Iteration 94/1000 | Loss: 0.00046338
Iteration 95/1000 | Loss: 0.00027329
Iteration 96/1000 | Loss: 0.00011548
Iteration 97/1000 | Loss: 0.00006272
Iteration 98/1000 | Loss: 0.00005529
Iteration 99/1000 | Loss: 0.00102976
Iteration 100/1000 | Loss: 0.00032146
Iteration 101/1000 | Loss: 0.00031088
Iteration 102/1000 | Loss: 0.00013521
Iteration 103/1000 | Loss: 0.00007155
Iteration 104/1000 | Loss: 0.00017428
Iteration 105/1000 | Loss: 0.00005989
Iteration 106/1000 | Loss: 0.00007134
Iteration 107/1000 | Loss: 0.00006719
Iteration 108/1000 | Loss: 0.00004452
Iteration 109/1000 | Loss: 0.00004800
Iteration 110/1000 | Loss: 0.00004015
Iteration 111/1000 | Loss: 0.00003904
Iteration 112/1000 | Loss: 0.00003823
Iteration 113/1000 | Loss: 0.00003758
Iteration 114/1000 | Loss: 0.00027606
Iteration 115/1000 | Loss: 0.00020905
Iteration 116/1000 | Loss: 0.00005035
Iteration 117/1000 | Loss: 0.00004248
Iteration 118/1000 | Loss: 0.00003949
Iteration 119/1000 | Loss: 0.00003774
Iteration 120/1000 | Loss: 0.00003707
Iteration 121/1000 | Loss: 0.00003669
Iteration 122/1000 | Loss: 0.00003623
Iteration 123/1000 | Loss: 0.00003592
Iteration 124/1000 | Loss: 0.00003541
Iteration 125/1000 | Loss: 0.00003513
Iteration 126/1000 | Loss: 0.00003503
Iteration 127/1000 | Loss: 0.00003487
Iteration 128/1000 | Loss: 0.00003471
Iteration 129/1000 | Loss: 0.00003470
Iteration 130/1000 | Loss: 0.00003466
Iteration 131/1000 | Loss: 0.00003466
Iteration 132/1000 | Loss: 0.00003465
Iteration 133/1000 | Loss: 0.00003465
Iteration 134/1000 | Loss: 0.00003464
Iteration 135/1000 | Loss: 0.00003464
Iteration 136/1000 | Loss: 0.00003458
Iteration 137/1000 | Loss: 0.00003457
Iteration 138/1000 | Loss: 0.00003457
Iteration 139/1000 | Loss: 0.00003456
Iteration 140/1000 | Loss: 0.00003452
Iteration 141/1000 | Loss: 0.00003451
Iteration 142/1000 | Loss: 0.00003451
Iteration 143/1000 | Loss: 0.00003451
Iteration 144/1000 | Loss: 0.00003450
Iteration 145/1000 | Loss: 0.00003450
Iteration 146/1000 | Loss: 0.00003450
Iteration 147/1000 | Loss: 0.00003450
Iteration 148/1000 | Loss: 0.00003450
Iteration 149/1000 | Loss: 0.00003450
Iteration 150/1000 | Loss: 0.00003450
Iteration 151/1000 | Loss: 0.00003450
Iteration 152/1000 | Loss: 0.00003450
Iteration 153/1000 | Loss: 0.00003449
Iteration 154/1000 | Loss: 0.00003449
Iteration 155/1000 | Loss: 0.00003448
Iteration 156/1000 | Loss: 0.00003448
Iteration 157/1000 | Loss: 0.00003448
Iteration 158/1000 | Loss: 0.00003447
Iteration 159/1000 | Loss: 0.00003447
Iteration 160/1000 | Loss: 0.00003447
Iteration 161/1000 | Loss: 0.00003447
Iteration 162/1000 | Loss: 0.00003446
Iteration 163/1000 | Loss: 0.00003446
Iteration 164/1000 | Loss: 0.00003446
Iteration 165/1000 | Loss: 0.00003445
Iteration 166/1000 | Loss: 0.00003445
Iteration 167/1000 | Loss: 0.00003445
Iteration 168/1000 | Loss: 0.00003444
Iteration 169/1000 | Loss: 0.00003444
Iteration 170/1000 | Loss: 0.00003444
Iteration 171/1000 | Loss: 0.00003443
Iteration 172/1000 | Loss: 0.00003443
Iteration 173/1000 | Loss: 0.00003443
Iteration 174/1000 | Loss: 0.00003442
Iteration 175/1000 | Loss: 0.00003442
Iteration 176/1000 | Loss: 0.00003442
Iteration 177/1000 | Loss: 0.00003441
Iteration 178/1000 | Loss: 0.00003441
Iteration 179/1000 | Loss: 0.00003441
Iteration 180/1000 | Loss: 0.00003440
Iteration 181/1000 | Loss: 0.00003440
Iteration 182/1000 | Loss: 0.00003440
Iteration 183/1000 | Loss: 0.00003440
Iteration 184/1000 | Loss: 0.00003440
Iteration 185/1000 | Loss: 0.00003440
Iteration 186/1000 | Loss: 0.00003440
Iteration 187/1000 | Loss: 0.00003440
Iteration 188/1000 | Loss: 0.00003440
Iteration 189/1000 | Loss: 0.00003440
Iteration 190/1000 | Loss: 0.00003440
Iteration 191/1000 | Loss: 0.00003440
Iteration 192/1000 | Loss: 0.00003440
Iteration 193/1000 | Loss: 0.00003439
Iteration 194/1000 | Loss: 0.00003439
Iteration 195/1000 | Loss: 0.00003439
Iteration 196/1000 | Loss: 0.00003439
Iteration 197/1000 | Loss: 0.00003439
Iteration 198/1000 | Loss: 0.00003439
Iteration 199/1000 | Loss: 0.00003439
Iteration 200/1000 | Loss: 0.00003439
Iteration 201/1000 | Loss: 0.00003439
Iteration 202/1000 | Loss: 0.00003438
Iteration 203/1000 | Loss: 0.00003438
Iteration 204/1000 | Loss: 0.00003438
Iteration 205/1000 | Loss: 0.00003438
Iteration 206/1000 | Loss: 0.00003438
Iteration 207/1000 | Loss: 0.00003438
Iteration 208/1000 | Loss: 0.00003438
Iteration 209/1000 | Loss: 0.00003438
Iteration 210/1000 | Loss: 0.00003438
Iteration 211/1000 | Loss: 0.00003438
Iteration 212/1000 | Loss: 0.00003438
Iteration 213/1000 | Loss: 0.00003438
Iteration 214/1000 | Loss: 0.00003438
Iteration 215/1000 | Loss: 0.00003438
Iteration 216/1000 | Loss: 0.00003438
Iteration 217/1000 | Loss: 0.00003438
Iteration 218/1000 | Loss: 0.00003438
Iteration 219/1000 | Loss: 0.00003438
Iteration 220/1000 | Loss: 0.00003438
Iteration 221/1000 | Loss: 0.00003438
Iteration 222/1000 | Loss: 0.00003438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [3.4379441785858944e-05, 3.4379441785858944e-05, 3.4379441785858944e-05, 3.4379441785858944e-05, 3.4379441785858944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4379441785858944e-05

Optimization complete. Final v2v error: 4.668970108032227 mm

Highest mean error: 20.87578773498535 mm for frame 151

Lowest mean error: 4.152266025543213 mm for frame 184

Saving results

Total time: 241.21453714370728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120108
Iteration 2/25 | Loss: 0.01120108
Iteration 3/25 | Loss: 0.01120108
Iteration 4/25 | Loss: 0.01120108
Iteration 5/25 | Loss: 0.01120108
Iteration 6/25 | Loss: 0.01120108
Iteration 7/25 | Loss: 0.01120108
Iteration 8/25 | Loss: 0.01120107
Iteration 9/25 | Loss: 0.01120107
Iteration 10/25 | Loss: 0.01120107
Iteration 11/25 | Loss: 0.01120107
Iteration 12/25 | Loss: 0.01120107
Iteration 13/25 | Loss: 0.01120107
Iteration 14/25 | Loss: 0.01120107
Iteration 15/25 | Loss: 0.01120107
Iteration 16/25 | Loss: 0.01120107
Iteration 17/25 | Loss: 0.01120106
Iteration 18/25 | Loss: 0.01120106
Iteration 19/25 | Loss: 0.01120106
Iteration 20/25 | Loss: 0.01120106
Iteration 21/25 | Loss: 0.01120106
Iteration 22/25 | Loss: 0.01120106
Iteration 23/25 | Loss: 0.01120106
Iteration 24/25 | Loss: 0.01120106
Iteration 25/25 | Loss: 0.01120106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48057926
Iteration 2/25 | Loss: 0.15893890
Iteration 3/25 | Loss: 0.15991917
Iteration 4/25 | Loss: 0.15781200
Iteration 5/25 | Loss: 0.15658025
Iteration 6/25 | Loss: 0.15576027
Iteration 7/25 | Loss: 0.15576026
Iteration 8/25 | Loss: 0.15576024
Iteration 9/25 | Loss: 0.15576023
Iteration 10/25 | Loss: 0.15576023
Iteration 11/25 | Loss: 0.15576021
Iteration 12/25 | Loss: 0.15576021
Iteration 13/25 | Loss: 0.15576021
Iteration 14/25 | Loss: 0.15576021
Iteration 15/25 | Loss: 0.15576021
Iteration 16/25 | Loss: 0.15576021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.15576021373271942, 0.15576021373271942, 0.15576021373271942, 0.15576021373271942, 0.15576021373271942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15576021373271942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15576021
Iteration 2/1000 | Loss: 0.01681470
Iteration 3/1000 | Loss: 0.00158054
Iteration 4/1000 | Loss: 0.00098479
Iteration 5/1000 | Loss: 0.00069538
Iteration 6/1000 | Loss: 0.00043818
Iteration 7/1000 | Loss: 0.00027322
Iteration 8/1000 | Loss: 0.00014043
Iteration 9/1000 | Loss: 0.00022577
Iteration 10/1000 | Loss: 0.00063490
Iteration 11/1000 | Loss: 0.00029430
Iteration 12/1000 | Loss: 0.00007546
Iteration 13/1000 | Loss: 0.00008439
Iteration 14/1000 | Loss: 0.00052053
Iteration 15/1000 | Loss: 0.00076443
Iteration 16/1000 | Loss: 0.00009431
Iteration 17/1000 | Loss: 0.00005287
Iteration 18/1000 | Loss: 0.00011397
Iteration 19/1000 | Loss: 0.00004578
Iteration 20/1000 | Loss: 0.00012033
Iteration 21/1000 | Loss: 0.00098017
Iteration 22/1000 | Loss: 0.00068573
Iteration 23/1000 | Loss: 0.00006728
Iteration 24/1000 | Loss: 0.00004272
Iteration 25/1000 | Loss: 0.00004156
Iteration 26/1000 | Loss: 0.00045829
Iteration 27/1000 | Loss: 0.00004760
Iteration 28/1000 | Loss: 0.00005719
Iteration 29/1000 | Loss: 0.00003783
Iteration 30/1000 | Loss: 0.00039019
Iteration 31/1000 | Loss: 0.00005998
Iteration 32/1000 | Loss: 0.00005734
Iteration 33/1000 | Loss: 0.00006211
Iteration 34/1000 | Loss: 0.00003693
Iteration 35/1000 | Loss: 0.00005223
Iteration 36/1000 | Loss: 0.00003625
Iteration 37/1000 | Loss: 0.00003572
Iteration 38/1000 | Loss: 0.00003530
Iteration 39/1000 | Loss: 0.00003502
Iteration 40/1000 | Loss: 0.00012960
Iteration 41/1000 | Loss: 0.00004417
Iteration 42/1000 | Loss: 0.00003713
Iteration 43/1000 | Loss: 0.00003483
Iteration 44/1000 | Loss: 0.00003468
Iteration 45/1000 | Loss: 0.00003458
Iteration 46/1000 | Loss: 0.00003457
Iteration 47/1000 | Loss: 0.00003457
Iteration 48/1000 | Loss: 0.00003453
Iteration 49/1000 | Loss: 0.00003452
Iteration 50/1000 | Loss: 0.00003451
Iteration 51/1000 | Loss: 0.00003451
Iteration 52/1000 | Loss: 0.00003445
Iteration 53/1000 | Loss: 0.00003445
Iteration 54/1000 | Loss: 0.00003445
Iteration 55/1000 | Loss: 0.00003445
Iteration 56/1000 | Loss: 0.00003445
Iteration 57/1000 | Loss: 0.00003444
Iteration 58/1000 | Loss: 0.00003444
Iteration 59/1000 | Loss: 0.00003444
Iteration 60/1000 | Loss: 0.00003444
Iteration 61/1000 | Loss: 0.00003444
Iteration 62/1000 | Loss: 0.00003443
Iteration 63/1000 | Loss: 0.00003442
Iteration 64/1000 | Loss: 0.00003441
Iteration 65/1000 | Loss: 0.00003440
Iteration 66/1000 | Loss: 0.00003440
Iteration 67/1000 | Loss: 0.00013024
Iteration 68/1000 | Loss: 0.00003462
Iteration 69/1000 | Loss: 0.00003437
Iteration 70/1000 | Loss: 0.00003437
Iteration 71/1000 | Loss: 0.00003436
Iteration 72/1000 | Loss: 0.00003435
Iteration 73/1000 | Loss: 0.00003435
Iteration 74/1000 | Loss: 0.00003435
Iteration 75/1000 | Loss: 0.00003435
Iteration 76/1000 | Loss: 0.00003435
Iteration 77/1000 | Loss: 0.00003435
Iteration 78/1000 | Loss: 0.00003435
Iteration 79/1000 | Loss: 0.00003435
Iteration 80/1000 | Loss: 0.00003435
Iteration 81/1000 | Loss: 0.00003435
Iteration 82/1000 | Loss: 0.00003434
Iteration 83/1000 | Loss: 0.00003434
Iteration 84/1000 | Loss: 0.00003434
Iteration 85/1000 | Loss: 0.00003434
Iteration 86/1000 | Loss: 0.00003434
Iteration 87/1000 | Loss: 0.00003434
Iteration 88/1000 | Loss: 0.00003434
Iteration 89/1000 | Loss: 0.00003434
Iteration 90/1000 | Loss: 0.00003434
Iteration 91/1000 | Loss: 0.00003434
Iteration 92/1000 | Loss: 0.00003433
Iteration 93/1000 | Loss: 0.00003433
Iteration 94/1000 | Loss: 0.00003433
Iteration 95/1000 | Loss: 0.00003433
Iteration 96/1000 | Loss: 0.00003433
Iteration 97/1000 | Loss: 0.00003433
Iteration 98/1000 | Loss: 0.00003433
Iteration 99/1000 | Loss: 0.00003433
Iteration 100/1000 | Loss: 0.00003433
Iteration 101/1000 | Loss: 0.00003432
Iteration 102/1000 | Loss: 0.00003432
Iteration 103/1000 | Loss: 0.00003432
Iteration 104/1000 | Loss: 0.00003432
Iteration 105/1000 | Loss: 0.00003432
Iteration 106/1000 | Loss: 0.00003432
Iteration 107/1000 | Loss: 0.00003432
Iteration 108/1000 | Loss: 0.00003432
Iteration 109/1000 | Loss: 0.00003431
Iteration 110/1000 | Loss: 0.00003431
Iteration 111/1000 | Loss: 0.00003431
Iteration 112/1000 | Loss: 0.00003431
Iteration 113/1000 | Loss: 0.00003431
Iteration 114/1000 | Loss: 0.00003431
Iteration 115/1000 | Loss: 0.00003430
Iteration 116/1000 | Loss: 0.00003430
Iteration 117/1000 | Loss: 0.00003430
Iteration 118/1000 | Loss: 0.00003430
Iteration 119/1000 | Loss: 0.00003430
Iteration 120/1000 | Loss: 0.00003430
Iteration 121/1000 | Loss: 0.00003429
Iteration 122/1000 | Loss: 0.00003429
Iteration 123/1000 | Loss: 0.00003428
Iteration 124/1000 | Loss: 0.00003428
Iteration 125/1000 | Loss: 0.00003428
Iteration 126/1000 | Loss: 0.00003428
Iteration 127/1000 | Loss: 0.00003428
Iteration 128/1000 | Loss: 0.00003428
Iteration 129/1000 | Loss: 0.00003428
Iteration 130/1000 | Loss: 0.00003427
Iteration 131/1000 | Loss: 0.00003427
Iteration 132/1000 | Loss: 0.00003427
Iteration 133/1000 | Loss: 0.00003427
Iteration 134/1000 | Loss: 0.00003427
Iteration 135/1000 | Loss: 0.00003427
Iteration 136/1000 | Loss: 0.00003427
Iteration 137/1000 | Loss: 0.00003426
Iteration 138/1000 | Loss: 0.00003426
Iteration 139/1000 | Loss: 0.00003426
Iteration 140/1000 | Loss: 0.00003426
Iteration 141/1000 | Loss: 0.00003426
Iteration 142/1000 | Loss: 0.00003426
Iteration 143/1000 | Loss: 0.00003426
Iteration 144/1000 | Loss: 0.00003426
Iteration 145/1000 | Loss: 0.00003426
Iteration 146/1000 | Loss: 0.00003426
Iteration 147/1000 | Loss: 0.00003426
Iteration 148/1000 | Loss: 0.00003426
Iteration 149/1000 | Loss: 0.00003426
Iteration 150/1000 | Loss: 0.00003426
Iteration 151/1000 | Loss: 0.00003426
Iteration 152/1000 | Loss: 0.00003426
Iteration 153/1000 | Loss: 0.00003426
Iteration 154/1000 | Loss: 0.00003425
Iteration 155/1000 | Loss: 0.00003425
Iteration 156/1000 | Loss: 0.00003425
Iteration 157/1000 | Loss: 0.00003425
Iteration 158/1000 | Loss: 0.00003425
Iteration 159/1000 | Loss: 0.00003425
Iteration 160/1000 | Loss: 0.00003425
Iteration 161/1000 | Loss: 0.00003425
Iteration 162/1000 | Loss: 0.00003425
Iteration 163/1000 | Loss: 0.00003425
Iteration 164/1000 | Loss: 0.00003424
Iteration 165/1000 | Loss: 0.00003424
Iteration 166/1000 | Loss: 0.00003424
Iteration 167/1000 | Loss: 0.00003424
Iteration 168/1000 | Loss: 0.00003424
Iteration 169/1000 | Loss: 0.00003424
Iteration 170/1000 | Loss: 0.00003424
Iteration 171/1000 | Loss: 0.00003424
Iteration 172/1000 | Loss: 0.00003424
Iteration 173/1000 | Loss: 0.00003424
Iteration 174/1000 | Loss: 0.00003424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.424295937293209e-05, 3.424295937293209e-05, 3.424295937293209e-05, 3.424295937293209e-05, 3.424295937293209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.424295937293209e-05

Optimization complete. Final v2v error: 4.893825054168701 mm

Highest mean error: 21.1236629486084 mm for frame 227

Lowest mean error: 4.218596458435059 mm for frame 211

Saving results

Total time: 96.34410190582275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990888
Iteration 2/25 | Loss: 0.00161570
Iteration 3/25 | Loss: 0.00112729
Iteration 4/25 | Loss: 0.00108571
Iteration 5/25 | Loss: 0.00107358
Iteration 6/25 | Loss: 0.00106953
Iteration 7/25 | Loss: 0.00106907
Iteration 8/25 | Loss: 0.00106907
Iteration 9/25 | Loss: 0.00106907
Iteration 10/25 | Loss: 0.00106907
Iteration 11/25 | Loss: 0.00106907
Iteration 12/25 | Loss: 0.00106907
Iteration 13/25 | Loss: 0.00106907
Iteration 14/25 | Loss: 0.00106907
Iteration 15/25 | Loss: 0.00106907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010690703056752682, 0.0010690703056752682, 0.0010690703056752682, 0.0010690703056752682, 0.0010690703056752682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010690703056752682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21337807
Iteration 2/25 | Loss: 0.00050183
Iteration 3/25 | Loss: 0.00050182
Iteration 4/25 | Loss: 0.00050182
Iteration 5/25 | Loss: 0.00050182
Iteration 6/25 | Loss: 0.00050182
Iteration 7/25 | Loss: 0.00050182
Iteration 8/25 | Loss: 0.00050182
Iteration 9/25 | Loss: 0.00050182
Iteration 10/25 | Loss: 0.00050182
Iteration 11/25 | Loss: 0.00050182
Iteration 12/25 | Loss: 0.00050182
Iteration 13/25 | Loss: 0.00050182
Iteration 14/25 | Loss: 0.00050182
Iteration 15/25 | Loss: 0.00050182
Iteration 16/25 | Loss: 0.00050182
Iteration 17/25 | Loss: 0.00050182
Iteration 18/25 | Loss: 0.00050182
Iteration 19/25 | Loss: 0.00050182
Iteration 20/25 | Loss: 0.00050182
Iteration 21/25 | Loss: 0.00050182
Iteration 22/25 | Loss: 0.00050182
Iteration 23/25 | Loss: 0.00050182
Iteration 24/25 | Loss: 0.00050182
Iteration 25/25 | Loss: 0.00050182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050182
Iteration 2/1000 | Loss: 0.00004706
Iteration 3/1000 | Loss: 0.00003462
Iteration 4/1000 | Loss: 0.00003161
Iteration 5/1000 | Loss: 0.00003020
Iteration 6/1000 | Loss: 0.00002941
Iteration 7/1000 | Loss: 0.00002883
Iteration 8/1000 | Loss: 0.00002837
Iteration 9/1000 | Loss: 0.00002809
Iteration 10/1000 | Loss: 0.00002788
Iteration 11/1000 | Loss: 0.00002770
Iteration 12/1000 | Loss: 0.00002762
Iteration 13/1000 | Loss: 0.00002756
Iteration 14/1000 | Loss: 0.00002755
Iteration 15/1000 | Loss: 0.00002754
Iteration 16/1000 | Loss: 0.00002753
Iteration 17/1000 | Loss: 0.00002753
Iteration 18/1000 | Loss: 0.00002751
Iteration 19/1000 | Loss: 0.00002748
Iteration 20/1000 | Loss: 0.00002748
Iteration 21/1000 | Loss: 0.00002748
Iteration 22/1000 | Loss: 0.00002747
Iteration 23/1000 | Loss: 0.00002745
Iteration 24/1000 | Loss: 0.00002745
Iteration 25/1000 | Loss: 0.00002745
Iteration 26/1000 | Loss: 0.00002745
Iteration 27/1000 | Loss: 0.00002745
Iteration 28/1000 | Loss: 0.00002745
Iteration 29/1000 | Loss: 0.00002745
Iteration 30/1000 | Loss: 0.00002745
Iteration 31/1000 | Loss: 0.00002744
Iteration 32/1000 | Loss: 0.00002744
Iteration 33/1000 | Loss: 0.00002744
Iteration 34/1000 | Loss: 0.00002744
Iteration 35/1000 | Loss: 0.00002744
Iteration 36/1000 | Loss: 0.00002744
Iteration 37/1000 | Loss: 0.00002743
Iteration 38/1000 | Loss: 0.00002742
Iteration 39/1000 | Loss: 0.00002742
Iteration 40/1000 | Loss: 0.00002742
Iteration 41/1000 | Loss: 0.00002742
Iteration 42/1000 | Loss: 0.00002741
Iteration 43/1000 | Loss: 0.00002741
Iteration 44/1000 | Loss: 0.00002741
Iteration 45/1000 | Loss: 0.00002741
Iteration 46/1000 | Loss: 0.00002741
Iteration 47/1000 | Loss: 0.00002740
Iteration 48/1000 | Loss: 0.00002740
Iteration 49/1000 | Loss: 0.00002739
Iteration 50/1000 | Loss: 0.00002739
Iteration 51/1000 | Loss: 0.00002739
Iteration 52/1000 | Loss: 0.00002739
Iteration 53/1000 | Loss: 0.00002739
Iteration 54/1000 | Loss: 0.00002739
Iteration 55/1000 | Loss: 0.00002739
Iteration 56/1000 | Loss: 0.00002739
Iteration 57/1000 | Loss: 0.00002739
Iteration 58/1000 | Loss: 0.00002738
Iteration 59/1000 | Loss: 0.00002738
Iteration 60/1000 | Loss: 0.00002738
Iteration 61/1000 | Loss: 0.00002738
Iteration 62/1000 | Loss: 0.00002738
Iteration 63/1000 | Loss: 0.00002738
Iteration 64/1000 | Loss: 0.00002738
Iteration 65/1000 | Loss: 0.00002738
Iteration 66/1000 | Loss: 0.00002738
Iteration 67/1000 | Loss: 0.00002738
Iteration 68/1000 | Loss: 0.00002738
Iteration 69/1000 | Loss: 0.00002738
Iteration 70/1000 | Loss: 0.00002738
Iteration 71/1000 | Loss: 0.00002738
Iteration 72/1000 | Loss: 0.00002738
Iteration 73/1000 | Loss: 0.00002737
Iteration 74/1000 | Loss: 0.00002737
Iteration 75/1000 | Loss: 0.00002737
Iteration 76/1000 | Loss: 0.00002737
Iteration 77/1000 | Loss: 0.00002737
Iteration 78/1000 | Loss: 0.00002737
Iteration 79/1000 | Loss: 0.00002737
Iteration 80/1000 | Loss: 0.00002737
Iteration 81/1000 | Loss: 0.00002737
Iteration 82/1000 | Loss: 0.00002736
Iteration 83/1000 | Loss: 0.00002736
Iteration 84/1000 | Loss: 0.00002736
Iteration 85/1000 | Loss: 0.00002736
Iteration 86/1000 | Loss: 0.00002735
Iteration 87/1000 | Loss: 0.00002735
Iteration 88/1000 | Loss: 0.00002735
Iteration 89/1000 | Loss: 0.00002735
Iteration 90/1000 | Loss: 0.00002735
Iteration 91/1000 | Loss: 0.00002735
Iteration 92/1000 | Loss: 0.00002734
Iteration 93/1000 | Loss: 0.00002734
Iteration 94/1000 | Loss: 0.00002734
Iteration 95/1000 | Loss: 0.00002733
Iteration 96/1000 | Loss: 0.00002733
Iteration 97/1000 | Loss: 0.00002733
Iteration 98/1000 | Loss: 0.00002733
Iteration 99/1000 | Loss: 0.00002733
Iteration 100/1000 | Loss: 0.00002733
Iteration 101/1000 | Loss: 0.00002733
Iteration 102/1000 | Loss: 0.00002733
Iteration 103/1000 | Loss: 0.00002733
Iteration 104/1000 | Loss: 0.00002733
Iteration 105/1000 | Loss: 0.00002733
Iteration 106/1000 | Loss: 0.00002732
Iteration 107/1000 | Loss: 0.00002732
Iteration 108/1000 | Loss: 0.00002732
Iteration 109/1000 | Loss: 0.00002732
Iteration 110/1000 | Loss: 0.00002732
Iteration 111/1000 | Loss: 0.00002732
Iteration 112/1000 | Loss: 0.00002732
Iteration 113/1000 | Loss: 0.00002732
Iteration 114/1000 | Loss: 0.00002732
Iteration 115/1000 | Loss: 0.00002732
Iteration 116/1000 | Loss: 0.00002732
Iteration 117/1000 | Loss: 0.00002732
Iteration 118/1000 | Loss: 0.00002732
Iteration 119/1000 | Loss: 0.00002732
Iteration 120/1000 | Loss: 0.00002732
Iteration 121/1000 | Loss: 0.00002732
Iteration 122/1000 | Loss: 0.00002732
Iteration 123/1000 | Loss: 0.00002732
Iteration 124/1000 | Loss: 0.00002732
Iteration 125/1000 | Loss: 0.00002732
Iteration 126/1000 | Loss: 0.00002732
Iteration 127/1000 | Loss: 0.00002732
Iteration 128/1000 | Loss: 0.00002732
Iteration 129/1000 | Loss: 0.00002732
Iteration 130/1000 | Loss: 0.00002732
Iteration 131/1000 | Loss: 0.00002732
Iteration 132/1000 | Loss: 0.00002732
Iteration 133/1000 | Loss: 0.00002732
Iteration 134/1000 | Loss: 0.00002732
Iteration 135/1000 | Loss: 0.00002732
Iteration 136/1000 | Loss: 0.00002732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.7321872039465234e-05, 2.7321872039465234e-05, 2.7321872039465234e-05, 2.7321872039465234e-05, 2.7321872039465234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7321872039465234e-05

Optimization complete. Final v2v error: 4.449008464813232 mm

Highest mean error: 5.651158809661865 mm for frame 134

Lowest mean error: 3.6963751316070557 mm for frame 0

Saving results

Total time: 36.579087018966675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850136
Iteration 2/25 | Loss: 0.00141398
Iteration 3/25 | Loss: 0.00113243
Iteration 4/25 | Loss: 0.00111077
Iteration 5/25 | Loss: 0.00110487
Iteration 6/25 | Loss: 0.00110277
Iteration 7/25 | Loss: 0.00110277
Iteration 8/25 | Loss: 0.00110277
Iteration 9/25 | Loss: 0.00110277
Iteration 10/25 | Loss: 0.00110277
Iteration 11/25 | Loss: 0.00110277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011027655564248562, 0.0011027655564248562, 0.0011027655564248562, 0.0011027655564248562, 0.0011027655564248562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011027655564248562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34406340
Iteration 2/25 | Loss: 0.00056226
Iteration 3/25 | Loss: 0.00056225
Iteration 4/25 | Loss: 0.00056225
Iteration 5/25 | Loss: 0.00056225
Iteration 6/25 | Loss: 0.00056225
Iteration 7/25 | Loss: 0.00056225
Iteration 8/25 | Loss: 0.00056225
Iteration 9/25 | Loss: 0.00056225
Iteration 10/25 | Loss: 0.00056224
Iteration 11/25 | Loss: 0.00056224
Iteration 12/25 | Loss: 0.00056224
Iteration 13/25 | Loss: 0.00056224
Iteration 14/25 | Loss: 0.00056224
Iteration 15/25 | Loss: 0.00056224
Iteration 16/25 | Loss: 0.00056224
Iteration 17/25 | Loss: 0.00056224
Iteration 18/25 | Loss: 0.00056224
Iteration 19/25 | Loss: 0.00056224
Iteration 20/25 | Loss: 0.00056224
Iteration 21/25 | Loss: 0.00056224
Iteration 22/25 | Loss: 0.00056224
Iteration 23/25 | Loss: 0.00056224
Iteration 24/25 | Loss: 0.00056224
Iteration 25/25 | Loss: 0.00056224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056224
Iteration 2/1000 | Loss: 0.00004799
Iteration 3/1000 | Loss: 0.00003812
Iteration 4/1000 | Loss: 0.00003448
Iteration 5/1000 | Loss: 0.00003303
Iteration 6/1000 | Loss: 0.00003231
Iteration 7/1000 | Loss: 0.00003175
Iteration 8/1000 | Loss: 0.00003143
Iteration 9/1000 | Loss: 0.00003122
Iteration 10/1000 | Loss: 0.00003104
Iteration 11/1000 | Loss: 0.00003095
Iteration 12/1000 | Loss: 0.00003087
Iteration 13/1000 | Loss: 0.00003087
Iteration 14/1000 | Loss: 0.00003083
Iteration 15/1000 | Loss: 0.00003082
Iteration 16/1000 | Loss: 0.00003082
Iteration 17/1000 | Loss: 0.00003082
Iteration 18/1000 | Loss: 0.00003081
Iteration 19/1000 | Loss: 0.00003081
Iteration 20/1000 | Loss: 0.00003081
Iteration 21/1000 | Loss: 0.00003081
Iteration 22/1000 | Loss: 0.00003080
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003080
Iteration 25/1000 | Loss: 0.00003080
Iteration 26/1000 | Loss: 0.00003079
Iteration 27/1000 | Loss: 0.00003079
Iteration 28/1000 | Loss: 0.00003079
Iteration 29/1000 | Loss: 0.00003079
Iteration 30/1000 | Loss: 0.00003079
Iteration 31/1000 | Loss: 0.00003079
Iteration 32/1000 | Loss: 0.00003078
Iteration 33/1000 | Loss: 0.00003078
Iteration 34/1000 | Loss: 0.00003077
Iteration 35/1000 | Loss: 0.00003077
Iteration 36/1000 | Loss: 0.00003077
Iteration 37/1000 | Loss: 0.00003076
Iteration 38/1000 | Loss: 0.00003076
Iteration 39/1000 | Loss: 0.00003076
Iteration 40/1000 | Loss: 0.00003076
Iteration 41/1000 | Loss: 0.00003076
Iteration 42/1000 | Loss: 0.00003075
Iteration 43/1000 | Loss: 0.00003075
Iteration 44/1000 | Loss: 0.00003075
Iteration 45/1000 | Loss: 0.00003075
Iteration 46/1000 | Loss: 0.00003075
Iteration 47/1000 | Loss: 0.00003074
Iteration 48/1000 | Loss: 0.00003074
Iteration 49/1000 | Loss: 0.00003074
Iteration 50/1000 | Loss: 0.00003074
Iteration 51/1000 | Loss: 0.00003074
Iteration 52/1000 | Loss: 0.00003073
Iteration 53/1000 | Loss: 0.00003073
Iteration 54/1000 | Loss: 0.00003073
Iteration 55/1000 | Loss: 0.00003073
Iteration 56/1000 | Loss: 0.00003073
Iteration 57/1000 | Loss: 0.00003073
Iteration 58/1000 | Loss: 0.00003072
Iteration 59/1000 | Loss: 0.00003072
Iteration 60/1000 | Loss: 0.00003072
Iteration 61/1000 | Loss: 0.00003072
Iteration 62/1000 | Loss: 0.00003072
Iteration 63/1000 | Loss: 0.00003072
Iteration 64/1000 | Loss: 0.00003072
Iteration 65/1000 | Loss: 0.00003072
Iteration 66/1000 | Loss: 0.00003072
Iteration 67/1000 | Loss: 0.00003072
Iteration 68/1000 | Loss: 0.00003072
Iteration 69/1000 | Loss: 0.00003071
Iteration 70/1000 | Loss: 0.00003071
Iteration 71/1000 | Loss: 0.00003071
Iteration 72/1000 | Loss: 0.00003070
Iteration 73/1000 | Loss: 0.00003070
Iteration 74/1000 | Loss: 0.00003070
Iteration 75/1000 | Loss: 0.00003070
Iteration 76/1000 | Loss: 0.00003070
Iteration 77/1000 | Loss: 0.00003070
Iteration 78/1000 | Loss: 0.00003070
Iteration 79/1000 | Loss: 0.00003070
Iteration 80/1000 | Loss: 0.00003070
Iteration 81/1000 | Loss: 0.00003070
Iteration 82/1000 | Loss: 0.00003070
Iteration 83/1000 | Loss: 0.00003070
Iteration 84/1000 | Loss: 0.00003070
Iteration 85/1000 | Loss: 0.00003070
Iteration 86/1000 | Loss: 0.00003070
Iteration 87/1000 | Loss: 0.00003069
Iteration 88/1000 | Loss: 0.00003069
Iteration 89/1000 | Loss: 0.00003069
Iteration 90/1000 | Loss: 0.00003069
Iteration 91/1000 | Loss: 0.00003069
Iteration 92/1000 | Loss: 0.00003069
Iteration 93/1000 | Loss: 0.00003069
Iteration 94/1000 | Loss: 0.00003069
Iteration 95/1000 | Loss: 0.00003069
Iteration 96/1000 | Loss: 0.00003069
Iteration 97/1000 | Loss: 0.00003069
Iteration 98/1000 | Loss: 0.00003069
Iteration 99/1000 | Loss: 0.00003069
Iteration 100/1000 | Loss: 0.00003069
Iteration 101/1000 | Loss: 0.00003069
Iteration 102/1000 | Loss: 0.00003069
Iteration 103/1000 | Loss: 0.00003069
Iteration 104/1000 | Loss: 0.00003069
Iteration 105/1000 | Loss: 0.00003069
Iteration 106/1000 | Loss: 0.00003069
Iteration 107/1000 | Loss: 0.00003069
Iteration 108/1000 | Loss: 0.00003069
Iteration 109/1000 | Loss: 0.00003069
Iteration 110/1000 | Loss: 0.00003068
Iteration 111/1000 | Loss: 0.00003068
Iteration 112/1000 | Loss: 0.00003068
Iteration 113/1000 | Loss: 0.00003068
Iteration 114/1000 | Loss: 0.00003068
Iteration 115/1000 | Loss: 0.00003068
Iteration 116/1000 | Loss: 0.00003068
Iteration 117/1000 | Loss: 0.00003068
Iteration 118/1000 | Loss: 0.00003068
Iteration 119/1000 | Loss: 0.00003068
Iteration 120/1000 | Loss: 0.00003068
Iteration 121/1000 | Loss: 0.00003068
Iteration 122/1000 | Loss: 0.00003068
Iteration 123/1000 | Loss: 0.00003068
Iteration 124/1000 | Loss: 0.00003068
Iteration 125/1000 | Loss: 0.00003068
Iteration 126/1000 | Loss: 0.00003068
Iteration 127/1000 | Loss: 0.00003068
Iteration 128/1000 | Loss: 0.00003068
Iteration 129/1000 | Loss: 0.00003068
Iteration 130/1000 | Loss: 0.00003068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [3.068477235501632e-05, 3.068477235501632e-05, 3.068477235501632e-05, 3.068477235501632e-05, 3.068477235501632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.068477235501632e-05

Optimization complete. Final v2v error: 4.803772449493408 mm

Highest mean error: 5.235411643981934 mm for frame 164

Lowest mean error: 4.443920612335205 mm for frame 121

Saving results

Total time: 37.46151256561279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831386
Iteration 2/25 | Loss: 0.00155817
Iteration 3/25 | Loss: 0.00129322
Iteration 4/25 | Loss: 0.00121085
Iteration 5/25 | Loss: 0.00119049
Iteration 6/25 | Loss: 0.00118563
Iteration 7/25 | Loss: 0.00118377
Iteration 8/25 | Loss: 0.00118320
Iteration 9/25 | Loss: 0.00118301
Iteration 10/25 | Loss: 0.00118287
Iteration 11/25 | Loss: 0.00118274
Iteration 12/25 | Loss: 0.00118264
Iteration 13/25 | Loss: 0.00118259
Iteration 14/25 | Loss: 0.00118259
Iteration 15/25 | Loss: 0.00118259
Iteration 16/25 | Loss: 0.00118259
Iteration 17/25 | Loss: 0.00118259
Iteration 18/25 | Loss: 0.00118259
Iteration 19/25 | Loss: 0.00118259
Iteration 20/25 | Loss: 0.00118259
Iteration 21/25 | Loss: 0.00118259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011825894471257925, 0.0011825894471257925, 0.0011825894471257925, 0.0011825894471257925, 0.0011825894471257925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011825894471257925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84970069
Iteration 2/25 | Loss: 0.00065628
Iteration 3/25 | Loss: 0.00065622
Iteration 4/25 | Loss: 0.00065622
Iteration 5/25 | Loss: 0.00065622
Iteration 6/25 | Loss: 0.00065622
Iteration 7/25 | Loss: 0.00065622
Iteration 8/25 | Loss: 0.00065622
Iteration 9/25 | Loss: 0.00065622
Iteration 10/25 | Loss: 0.00065622
Iteration 11/25 | Loss: 0.00065622
Iteration 12/25 | Loss: 0.00065622
Iteration 13/25 | Loss: 0.00065622
Iteration 14/25 | Loss: 0.00065622
Iteration 15/25 | Loss: 0.00065622
Iteration 16/25 | Loss: 0.00065622
Iteration 17/25 | Loss: 0.00065622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006562185590155423, 0.0006562185590155423, 0.0006562185590155423, 0.0006562185590155423, 0.0006562185590155423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006562185590155423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065622
Iteration 2/1000 | Loss: 0.00007410
Iteration 3/1000 | Loss: 0.00005388
Iteration 4/1000 | Loss: 0.00004747
Iteration 5/1000 | Loss: 0.00004309
Iteration 6/1000 | Loss: 0.00027721
Iteration 7/1000 | Loss: 0.00038817
Iteration 8/1000 | Loss: 0.00027303
Iteration 9/1000 | Loss: 0.00032997
Iteration 10/1000 | Loss: 0.00025512
Iteration 11/1000 | Loss: 0.00003931
Iteration 12/1000 | Loss: 0.00003744
Iteration 13/1000 | Loss: 0.00003615
Iteration 14/1000 | Loss: 0.00003531
Iteration 15/1000 | Loss: 0.00003472
Iteration 16/1000 | Loss: 0.00003432
Iteration 17/1000 | Loss: 0.00003397
Iteration 18/1000 | Loss: 0.00003372
Iteration 19/1000 | Loss: 0.00003354
Iteration 20/1000 | Loss: 0.00003346
Iteration 21/1000 | Loss: 0.00003338
Iteration 22/1000 | Loss: 0.00003328
Iteration 23/1000 | Loss: 0.00003325
Iteration 24/1000 | Loss: 0.00003324
Iteration 25/1000 | Loss: 0.00003324
Iteration 26/1000 | Loss: 0.00003322
Iteration 27/1000 | Loss: 0.00003322
Iteration 28/1000 | Loss: 0.00003322
Iteration 29/1000 | Loss: 0.00003321
Iteration 30/1000 | Loss: 0.00003321
Iteration 31/1000 | Loss: 0.00003320
Iteration 32/1000 | Loss: 0.00003320
Iteration 33/1000 | Loss: 0.00003319
Iteration 34/1000 | Loss: 0.00003319
Iteration 35/1000 | Loss: 0.00003318
Iteration 36/1000 | Loss: 0.00003307
Iteration 37/1000 | Loss: 0.00003303
Iteration 38/1000 | Loss: 0.00003303
Iteration 39/1000 | Loss: 0.00003302
Iteration 40/1000 | Loss: 0.00003302
Iteration 41/1000 | Loss: 0.00003302
Iteration 42/1000 | Loss: 0.00003301
Iteration 43/1000 | Loss: 0.00003301
Iteration 44/1000 | Loss: 0.00003300
Iteration 45/1000 | Loss: 0.00003300
Iteration 46/1000 | Loss: 0.00003300
Iteration 47/1000 | Loss: 0.00003300
Iteration 48/1000 | Loss: 0.00003299
Iteration 49/1000 | Loss: 0.00003299
Iteration 50/1000 | Loss: 0.00003299
Iteration 51/1000 | Loss: 0.00003298
Iteration 52/1000 | Loss: 0.00003298
Iteration 53/1000 | Loss: 0.00003298
Iteration 54/1000 | Loss: 0.00003297
Iteration 55/1000 | Loss: 0.00003297
Iteration 56/1000 | Loss: 0.00003297
Iteration 57/1000 | Loss: 0.00003296
Iteration 58/1000 | Loss: 0.00003296
Iteration 59/1000 | Loss: 0.00003296
Iteration 60/1000 | Loss: 0.00003295
Iteration 61/1000 | Loss: 0.00003295
Iteration 62/1000 | Loss: 0.00003295
Iteration 63/1000 | Loss: 0.00003294
Iteration 64/1000 | Loss: 0.00003294
Iteration 65/1000 | Loss: 0.00003294
Iteration 66/1000 | Loss: 0.00003293
Iteration 67/1000 | Loss: 0.00003293
Iteration 68/1000 | Loss: 0.00003293
Iteration 69/1000 | Loss: 0.00003292
Iteration 70/1000 | Loss: 0.00003292
Iteration 71/1000 | Loss: 0.00003292
Iteration 72/1000 | Loss: 0.00003292
Iteration 73/1000 | Loss: 0.00003292
Iteration 74/1000 | Loss: 0.00003292
Iteration 75/1000 | Loss: 0.00003291
Iteration 76/1000 | Loss: 0.00003291
Iteration 77/1000 | Loss: 0.00003291
Iteration 78/1000 | Loss: 0.00003291
Iteration 79/1000 | Loss: 0.00003291
Iteration 80/1000 | Loss: 0.00003291
Iteration 81/1000 | Loss: 0.00003290
Iteration 82/1000 | Loss: 0.00003290
Iteration 83/1000 | Loss: 0.00003290
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003289
Iteration 86/1000 | Loss: 0.00003289
Iteration 87/1000 | Loss: 0.00003289
Iteration 88/1000 | Loss: 0.00003289
Iteration 89/1000 | Loss: 0.00003289
Iteration 90/1000 | Loss: 0.00003289
Iteration 91/1000 | Loss: 0.00003288
Iteration 92/1000 | Loss: 0.00003288
Iteration 93/1000 | Loss: 0.00003288
Iteration 94/1000 | Loss: 0.00003288
Iteration 95/1000 | Loss: 0.00003288
Iteration 96/1000 | Loss: 0.00003288
Iteration 97/1000 | Loss: 0.00003288
Iteration 98/1000 | Loss: 0.00003287
Iteration 99/1000 | Loss: 0.00003287
Iteration 100/1000 | Loss: 0.00003287
Iteration 101/1000 | Loss: 0.00003287
Iteration 102/1000 | Loss: 0.00003287
Iteration 103/1000 | Loss: 0.00003287
Iteration 104/1000 | Loss: 0.00003287
Iteration 105/1000 | Loss: 0.00003286
Iteration 106/1000 | Loss: 0.00003286
Iteration 107/1000 | Loss: 0.00003286
Iteration 108/1000 | Loss: 0.00003286
Iteration 109/1000 | Loss: 0.00003286
Iteration 110/1000 | Loss: 0.00003285
Iteration 111/1000 | Loss: 0.00003285
Iteration 112/1000 | Loss: 0.00003285
Iteration 113/1000 | Loss: 0.00003285
Iteration 114/1000 | Loss: 0.00003284
Iteration 115/1000 | Loss: 0.00003284
Iteration 116/1000 | Loss: 0.00003284
Iteration 117/1000 | Loss: 0.00003284
Iteration 118/1000 | Loss: 0.00003284
Iteration 119/1000 | Loss: 0.00003283
Iteration 120/1000 | Loss: 0.00003283
Iteration 121/1000 | Loss: 0.00003283
Iteration 122/1000 | Loss: 0.00003283
Iteration 123/1000 | Loss: 0.00003283
Iteration 124/1000 | Loss: 0.00003283
Iteration 125/1000 | Loss: 0.00003282
Iteration 126/1000 | Loss: 0.00003282
Iteration 127/1000 | Loss: 0.00003282
Iteration 128/1000 | Loss: 0.00003282
Iteration 129/1000 | Loss: 0.00003282
Iteration 130/1000 | Loss: 0.00003282
Iteration 131/1000 | Loss: 0.00003282
Iteration 132/1000 | Loss: 0.00003282
Iteration 133/1000 | Loss: 0.00003282
Iteration 134/1000 | Loss: 0.00003282
Iteration 135/1000 | Loss: 0.00003282
Iteration 136/1000 | Loss: 0.00003282
Iteration 137/1000 | Loss: 0.00003282
Iteration 138/1000 | Loss: 0.00003282
Iteration 139/1000 | Loss: 0.00003282
Iteration 140/1000 | Loss: 0.00003282
Iteration 141/1000 | Loss: 0.00003282
Iteration 142/1000 | Loss: 0.00003282
Iteration 143/1000 | Loss: 0.00003282
Iteration 144/1000 | Loss: 0.00003282
Iteration 145/1000 | Loss: 0.00003282
Iteration 146/1000 | Loss: 0.00003282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.281771205365658e-05, 3.281771205365658e-05, 3.281771205365658e-05, 3.281771205365658e-05, 3.281771205365658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.281771205365658e-05

Optimization complete. Final v2v error: 5.0417633056640625 mm

Highest mean error: 6.729074478149414 mm for frame 106

Lowest mean error: 4.084204196929932 mm for frame 81

Saving results

Total time: 68.92478489875793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109742
Iteration 2/25 | Loss: 0.00200090
Iteration 3/25 | Loss: 0.00133427
Iteration 4/25 | Loss: 0.00126140
Iteration 5/25 | Loss: 0.00121358
Iteration 6/25 | Loss: 0.00118996
Iteration 7/25 | Loss: 0.00118041
Iteration 8/25 | Loss: 0.00117639
Iteration 9/25 | Loss: 0.00117508
Iteration 10/25 | Loss: 0.00117456
Iteration 11/25 | Loss: 0.00117447
Iteration 12/25 | Loss: 0.00117447
Iteration 13/25 | Loss: 0.00117447
Iteration 14/25 | Loss: 0.00117447
Iteration 15/25 | Loss: 0.00117443
Iteration 16/25 | Loss: 0.00117443
Iteration 17/25 | Loss: 0.00117443
Iteration 18/25 | Loss: 0.00117443
Iteration 19/25 | Loss: 0.00117442
Iteration 20/25 | Loss: 0.00117442
Iteration 21/25 | Loss: 0.00117442
Iteration 22/25 | Loss: 0.00117442
Iteration 23/25 | Loss: 0.00117442
Iteration 24/25 | Loss: 0.00117442
Iteration 25/25 | Loss: 0.00117442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95331925
Iteration 2/25 | Loss: 0.00057018
Iteration 3/25 | Loss: 0.00057018
Iteration 4/25 | Loss: 0.00057018
Iteration 5/25 | Loss: 0.00057018
Iteration 6/25 | Loss: 0.00057017
Iteration 7/25 | Loss: 0.00057017
Iteration 8/25 | Loss: 0.00057017
Iteration 9/25 | Loss: 0.00057017
Iteration 10/25 | Loss: 0.00057017
Iteration 11/25 | Loss: 0.00057017
Iteration 12/25 | Loss: 0.00057017
Iteration 13/25 | Loss: 0.00057017
Iteration 14/25 | Loss: 0.00057017
Iteration 15/25 | Loss: 0.00057017
Iteration 16/25 | Loss: 0.00057017
Iteration 17/25 | Loss: 0.00057017
Iteration 18/25 | Loss: 0.00057017
Iteration 19/25 | Loss: 0.00057017
Iteration 20/25 | Loss: 0.00057017
Iteration 21/25 | Loss: 0.00057017
Iteration 22/25 | Loss: 0.00057017
Iteration 23/25 | Loss: 0.00057017
Iteration 24/25 | Loss: 0.00057017
Iteration 25/25 | Loss: 0.00057017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057017
Iteration 2/1000 | Loss: 0.00005780
Iteration 3/1000 | Loss: 0.00004241
Iteration 4/1000 | Loss: 0.00003791
Iteration 5/1000 | Loss: 0.00003561
Iteration 6/1000 | Loss: 0.00003441
Iteration 7/1000 | Loss: 0.00003378
Iteration 8/1000 | Loss: 0.00003308
Iteration 9/1000 | Loss: 0.00003254
Iteration 10/1000 | Loss: 0.00003218
Iteration 11/1000 | Loss: 0.00003185
Iteration 12/1000 | Loss: 0.00003161
Iteration 13/1000 | Loss: 0.00003144
Iteration 14/1000 | Loss: 0.00003138
Iteration 15/1000 | Loss: 0.00003136
Iteration 16/1000 | Loss: 0.00003135
Iteration 17/1000 | Loss: 0.00003132
Iteration 18/1000 | Loss: 0.00003132
Iteration 19/1000 | Loss: 0.00003132
Iteration 20/1000 | Loss: 0.00003132
Iteration 21/1000 | Loss: 0.00003132
Iteration 22/1000 | Loss: 0.00003131
Iteration 23/1000 | Loss: 0.00003131
Iteration 24/1000 | Loss: 0.00003131
Iteration 25/1000 | Loss: 0.00003131
Iteration 26/1000 | Loss: 0.00003130
Iteration 27/1000 | Loss: 0.00003128
Iteration 28/1000 | Loss: 0.00003128
Iteration 29/1000 | Loss: 0.00003127
Iteration 30/1000 | Loss: 0.00003127
Iteration 31/1000 | Loss: 0.00003127
Iteration 32/1000 | Loss: 0.00003126
Iteration 33/1000 | Loss: 0.00003125
Iteration 34/1000 | Loss: 0.00003125
Iteration 35/1000 | Loss: 0.00003124
Iteration 36/1000 | Loss: 0.00003124
Iteration 37/1000 | Loss: 0.00003124
Iteration 38/1000 | Loss: 0.00003123
Iteration 39/1000 | Loss: 0.00003123
Iteration 40/1000 | Loss: 0.00003123
Iteration 41/1000 | Loss: 0.00003123
Iteration 42/1000 | Loss: 0.00003123
Iteration 43/1000 | Loss: 0.00003123
Iteration 44/1000 | Loss: 0.00003123
Iteration 45/1000 | Loss: 0.00003122
Iteration 46/1000 | Loss: 0.00003122
Iteration 47/1000 | Loss: 0.00003122
Iteration 48/1000 | Loss: 0.00003122
Iteration 49/1000 | Loss: 0.00003122
Iteration 50/1000 | Loss: 0.00003121
Iteration 51/1000 | Loss: 0.00003121
Iteration 52/1000 | Loss: 0.00003121
Iteration 53/1000 | Loss: 0.00003120
Iteration 54/1000 | Loss: 0.00003120
Iteration 55/1000 | Loss: 0.00003119
Iteration 56/1000 | Loss: 0.00003119
Iteration 57/1000 | Loss: 0.00003119
Iteration 58/1000 | Loss: 0.00003118
Iteration 59/1000 | Loss: 0.00003118
Iteration 60/1000 | Loss: 0.00003118
Iteration 61/1000 | Loss: 0.00003117
Iteration 62/1000 | Loss: 0.00003116
Iteration 63/1000 | Loss: 0.00003116
Iteration 64/1000 | Loss: 0.00003116
Iteration 65/1000 | Loss: 0.00003116
Iteration 66/1000 | Loss: 0.00003116
Iteration 67/1000 | Loss: 0.00003115
Iteration 68/1000 | Loss: 0.00003115
Iteration 69/1000 | Loss: 0.00003115
Iteration 70/1000 | Loss: 0.00003115
Iteration 71/1000 | Loss: 0.00003114
Iteration 72/1000 | Loss: 0.00003114
Iteration 73/1000 | Loss: 0.00003114
Iteration 74/1000 | Loss: 0.00003114
Iteration 75/1000 | Loss: 0.00003113
Iteration 76/1000 | Loss: 0.00003113
Iteration 77/1000 | Loss: 0.00003113
Iteration 78/1000 | Loss: 0.00003113
Iteration 79/1000 | Loss: 0.00003113
Iteration 80/1000 | Loss: 0.00003113
Iteration 81/1000 | Loss: 0.00003112
Iteration 82/1000 | Loss: 0.00003112
Iteration 83/1000 | Loss: 0.00003112
Iteration 84/1000 | Loss: 0.00003111
Iteration 85/1000 | Loss: 0.00003111
Iteration 86/1000 | Loss: 0.00003111
Iteration 87/1000 | Loss: 0.00003111
Iteration 88/1000 | Loss: 0.00003111
Iteration 89/1000 | Loss: 0.00003110
Iteration 90/1000 | Loss: 0.00003110
Iteration 91/1000 | Loss: 0.00003110
Iteration 92/1000 | Loss: 0.00003110
Iteration 93/1000 | Loss: 0.00003110
Iteration 94/1000 | Loss: 0.00003110
Iteration 95/1000 | Loss: 0.00003109
Iteration 96/1000 | Loss: 0.00003109
Iteration 97/1000 | Loss: 0.00003109
Iteration 98/1000 | Loss: 0.00003109
Iteration 99/1000 | Loss: 0.00003109
Iteration 100/1000 | Loss: 0.00003109
Iteration 101/1000 | Loss: 0.00003109
Iteration 102/1000 | Loss: 0.00003109
Iteration 103/1000 | Loss: 0.00003109
Iteration 104/1000 | Loss: 0.00003109
Iteration 105/1000 | Loss: 0.00003109
Iteration 106/1000 | Loss: 0.00003108
Iteration 107/1000 | Loss: 0.00003108
Iteration 108/1000 | Loss: 0.00003108
Iteration 109/1000 | Loss: 0.00003108
Iteration 110/1000 | Loss: 0.00003108
Iteration 111/1000 | Loss: 0.00003108
Iteration 112/1000 | Loss: 0.00003108
Iteration 113/1000 | Loss: 0.00003108
Iteration 114/1000 | Loss: 0.00003108
Iteration 115/1000 | Loss: 0.00003108
Iteration 116/1000 | Loss: 0.00003108
Iteration 117/1000 | Loss: 0.00003108
Iteration 118/1000 | Loss: 0.00003108
Iteration 119/1000 | Loss: 0.00003108
Iteration 120/1000 | Loss: 0.00003107
Iteration 121/1000 | Loss: 0.00003107
Iteration 122/1000 | Loss: 0.00003107
Iteration 123/1000 | Loss: 0.00003107
Iteration 124/1000 | Loss: 0.00003107
Iteration 125/1000 | Loss: 0.00003107
Iteration 126/1000 | Loss: 0.00003107
Iteration 127/1000 | Loss: 0.00003107
Iteration 128/1000 | Loss: 0.00003107
Iteration 129/1000 | Loss: 0.00003106
Iteration 130/1000 | Loss: 0.00003106
Iteration 131/1000 | Loss: 0.00003106
Iteration 132/1000 | Loss: 0.00003106
Iteration 133/1000 | Loss: 0.00003106
Iteration 134/1000 | Loss: 0.00003106
Iteration 135/1000 | Loss: 0.00003106
Iteration 136/1000 | Loss: 0.00003106
Iteration 137/1000 | Loss: 0.00003106
Iteration 138/1000 | Loss: 0.00003106
Iteration 139/1000 | Loss: 0.00003106
Iteration 140/1000 | Loss: 0.00003106
Iteration 141/1000 | Loss: 0.00003106
Iteration 142/1000 | Loss: 0.00003106
Iteration 143/1000 | Loss: 0.00003106
Iteration 144/1000 | Loss: 0.00003106
Iteration 145/1000 | Loss: 0.00003106
Iteration 146/1000 | Loss: 0.00003106
Iteration 147/1000 | Loss: 0.00003106
Iteration 148/1000 | Loss: 0.00003106
Iteration 149/1000 | Loss: 0.00003106
Iteration 150/1000 | Loss: 0.00003106
Iteration 151/1000 | Loss: 0.00003106
Iteration 152/1000 | Loss: 0.00003106
Iteration 153/1000 | Loss: 0.00003106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.1061950721777976e-05, 3.1061950721777976e-05, 3.1061950721777976e-05, 3.1061950721777976e-05, 3.1061950721777976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1061950721777976e-05

Optimization complete. Final v2v error: 4.708649635314941 mm

Highest mean error: 5.641296863555908 mm for frame 78

Lowest mean error: 4.076314449310303 mm for frame 58

Saving results

Total time: 48.856727838516235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467668
Iteration 2/25 | Loss: 0.00122623
Iteration 3/25 | Loss: 0.00105739
Iteration 4/25 | Loss: 0.00103887
Iteration 5/25 | Loss: 0.00103214
Iteration 6/25 | Loss: 0.00103084
Iteration 7/25 | Loss: 0.00103083
Iteration 8/25 | Loss: 0.00103083
Iteration 9/25 | Loss: 0.00103083
Iteration 10/25 | Loss: 0.00103083
Iteration 11/25 | Loss: 0.00103083
Iteration 12/25 | Loss: 0.00103083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010308342752978206, 0.0010308342752978206, 0.0010308342752978206, 0.0010308342752978206, 0.0010308342752978206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010308342752978206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.71816015
Iteration 2/25 | Loss: 0.00059537
Iteration 3/25 | Loss: 0.00059532
Iteration 4/25 | Loss: 0.00059531
Iteration 5/25 | Loss: 0.00059531
Iteration 6/25 | Loss: 0.00059531
Iteration 7/25 | Loss: 0.00059531
Iteration 8/25 | Loss: 0.00059531
Iteration 9/25 | Loss: 0.00059531
Iteration 10/25 | Loss: 0.00059531
Iteration 11/25 | Loss: 0.00059531
Iteration 12/25 | Loss: 0.00059531
Iteration 13/25 | Loss: 0.00059531
Iteration 14/25 | Loss: 0.00059531
Iteration 15/25 | Loss: 0.00059531
Iteration 16/25 | Loss: 0.00059531
Iteration 17/25 | Loss: 0.00059531
Iteration 18/25 | Loss: 0.00059531
Iteration 19/25 | Loss: 0.00059531
Iteration 20/25 | Loss: 0.00059531
Iteration 21/25 | Loss: 0.00059531
Iteration 22/25 | Loss: 0.00059531
Iteration 23/25 | Loss: 0.00059531
Iteration 24/25 | Loss: 0.00059531
Iteration 25/25 | Loss: 0.00059531

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059531
Iteration 2/1000 | Loss: 0.00003590
Iteration 3/1000 | Loss: 0.00003056
Iteration 4/1000 | Loss: 0.00002877
Iteration 5/1000 | Loss: 0.00002762
Iteration 6/1000 | Loss: 0.00002697
Iteration 7/1000 | Loss: 0.00002664
Iteration 8/1000 | Loss: 0.00002638
Iteration 9/1000 | Loss: 0.00002621
Iteration 10/1000 | Loss: 0.00002598
Iteration 11/1000 | Loss: 0.00002593
Iteration 12/1000 | Loss: 0.00002590
Iteration 13/1000 | Loss: 0.00002587
Iteration 14/1000 | Loss: 0.00002587
Iteration 15/1000 | Loss: 0.00002586
Iteration 16/1000 | Loss: 0.00002586
Iteration 17/1000 | Loss: 0.00002585
Iteration 18/1000 | Loss: 0.00002585
Iteration 19/1000 | Loss: 0.00002585
Iteration 20/1000 | Loss: 0.00002585
Iteration 21/1000 | Loss: 0.00002584
Iteration 22/1000 | Loss: 0.00002584
Iteration 23/1000 | Loss: 0.00002584
Iteration 24/1000 | Loss: 0.00002584
Iteration 25/1000 | Loss: 0.00002584
Iteration 26/1000 | Loss: 0.00002583
Iteration 27/1000 | Loss: 0.00002583
Iteration 28/1000 | Loss: 0.00002583
Iteration 29/1000 | Loss: 0.00002583
Iteration 30/1000 | Loss: 0.00002583
Iteration 31/1000 | Loss: 0.00002583
Iteration 32/1000 | Loss: 0.00002583
Iteration 33/1000 | Loss: 0.00002583
Iteration 34/1000 | Loss: 0.00002583
Iteration 35/1000 | Loss: 0.00002582
Iteration 36/1000 | Loss: 0.00002582
Iteration 37/1000 | Loss: 0.00002582
Iteration 38/1000 | Loss: 0.00002582
Iteration 39/1000 | Loss: 0.00002582
Iteration 40/1000 | Loss: 0.00002582
Iteration 41/1000 | Loss: 0.00002582
Iteration 42/1000 | Loss: 0.00002582
Iteration 43/1000 | Loss: 0.00002581
Iteration 44/1000 | Loss: 0.00002581
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002580
Iteration 51/1000 | Loss: 0.00002580
Iteration 52/1000 | Loss: 0.00002580
Iteration 53/1000 | Loss: 0.00002580
Iteration 54/1000 | Loss: 0.00002580
Iteration 55/1000 | Loss: 0.00002580
Iteration 56/1000 | Loss: 0.00002580
Iteration 57/1000 | Loss: 0.00002580
Iteration 58/1000 | Loss: 0.00002579
Iteration 59/1000 | Loss: 0.00002579
Iteration 60/1000 | Loss: 0.00002579
Iteration 61/1000 | Loss: 0.00002578
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00002578
Iteration 64/1000 | Loss: 0.00002578
Iteration 65/1000 | Loss: 0.00002578
Iteration 66/1000 | Loss: 0.00002577
Iteration 67/1000 | Loss: 0.00002577
Iteration 68/1000 | Loss: 0.00002577
Iteration 69/1000 | Loss: 0.00002577
Iteration 70/1000 | Loss: 0.00002576
Iteration 71/1000 | Loss: 0.00002576
Iteration 72/1000 | Loss: 0.00002576
Iteration 73/1000 | Loss: 0.00002576
Iteration 74/1000 | Loss: 0.00002576
Iteration 75/1000 | Loss: 0.00002575
Iteration 76/1000 | Loss: 0.00002575
Iteration 77/1000 | Loss: 0.00002575
Iteration 78/1000 | Loss: 0.00002574
Iteration 79/1000 | Loss: 0.00002574
Iteration 80/1000 | Loss: 0.00002574
Iteration 81/1000 | Loss: 0.00002574
Iteration 82/1000 | Loss: 0.00002574
Iteration 83/1000 | Loss: 0.00002574
Iteration 84/1000 | Loss: 0.00002573
Iteration 85/1000 | Loss: 0.00002573
Iteration 86/1000 | Loss: 0.00002573
Iteration 87/1000 | Loss: 0.00002573
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002572
Iteration 91/1000 | Loss: 0.00002572
Iteration 92/1000 | Loss: 0.00002572
Iteration 93/1000 | Loss: 0.00002572
Iteration 94/1000 | Loss: 0.00002572
Iteration 95/1000 | Loss: 0.00002572
Iteration 96/1000 | Loss: 0.00002571
Iteration 97/1000 | Loss: 0.00002571
Iteration 98/1000 | Loss: 0.00002570
Iteration 99/1000 | Loss: 0.00002570
Iteration 100/1000 | Loss: 0.00002570
Iteration 101/1000 | Loss: 0.00002570
Iteration 102/1000 | Loss: 0.00002569
Iteration 103/1000 | Loss: 0.00002569
Iteration 104/1000 | Loss: 0.00002569
Iteration 105/1000 | Loss: 0.00002568
Iteration 106/1000 | Loss: 0.00002568
Iteration 107/1000 | Loss: 0.00002568
Iteration 108/1000 | Loss: 0.00002568
Iteration 109/1000 | Loss: 0.00002567
Iteration 110/1000 | Loss: 0.00002567
Iteration 111/1000 | Loss: 0.00002567
Iteration 112/1000 | Loss: 0.00002567
Iteration 113/1000 | Loss: 0.00002567
Iteration 114/1000 | Loss: 0.00002567
Iteration 115/1000 | Loss: 0.00002567
Iteration 116/1000 | Loss: 0.00002566
Iteration 117/1000 | Loss: 0.00002566
Iteration 118/1000 | Loss: 0.00002566
Iteration 119/1000 | Loss: 0.00002566
Iteration 120/1000 | Loss: 0.00002566
Iteration 121/1000 | Loss: 0.00002566
Iteration 122/1000 | Loss: 0.00002566
Iteration 123/1000 | Loss: 0.00002566
Iteration 124/1000 | Loss: 0.00002565
Iteration 125/1000 | Loss: 0.00002565
Iteration 126/1000 | Loss: 0.00002565
Iteration 127/1000 | Loss: 0.00002564
Iteration 128/1000 | Loss: 0.00002564
Iteration 129/1000 | Loss: 0.00002564
Iteration 130/1000 | Loss: 0.00002564
Iteration 131/1000 | Loss: 0.00002564
Iteration 132/1000 | Loss: 0.00002563
Iteration 133/1000 | Loss: 0.00002563
Iteration 134/1000 | Loss: 0.00002563
Iteration 135/1000 | Loss: 0.00002563
Iteration 136/1000 | Loss: 0.00002563
Iteration 137/1000 | Loss: 0.00002563
Iteration 138/1000 | Loss: 0.00002563
Iteration 139/1000 | Loss: 0.00002563
Iteration 140/1000 | Loss: 0.00002563
Iteration 141/1000 | Loss: 0.00002563
Iteration 142/1000 | Loss: 0.00002563
Iteration 143/1000 | Loss: 0.00002563
Iteration 144/1000 | Loss: 0.00002563
Iteration 145/1000 | Loss: 0.00002563
Iteration 146/1000 | Loss: 0.00002563
Iteration 147/1000 | Loss: 0.00002563
Iteration 148/1000 | Loss: 0.00002563
Iteration 149/1000 | Loss: 0.00002563
Iteration 150/1000 | Loss: 0.00002563
Iteration 151/1000 | Loss: 0.00002563
Iteration 152/1000 | Loss: 0.00002563
Iteration 153/1000 | Loss: 0.00002563
Iteration 154/1000 | Loss: 0.00002563
Iteration 155/1000 | Loss: 0.00002563
Iteration 156/1000 | Loss: 0.00002563
Iteration 157/1000 | Loss: 0.00002563
Iteration 158/1000 | Loss: 0.00002563
Iteration 159/1000 | Loss: 0.00002563
Iteration 160/1000 | Loss: 0.00002563
Iteration 161/1000 | Loss: 0.00002563
Iteration 162/1000 | Loss: 0.00002563
Iteration 163/1000 | Loss: 0.00002563
Iteration 164/1000 | Loss: 0.00002563
Iteration 165/1000 | Loss: 0.00002563
Iteration 166/1000 | Loss: 0.00002563
Iteration 167/1000 | Loss: 0.00002563
Iteration 168/1000 | Loss: 0.00002563
Iteration 169/1000 | Loss: 0.00002563
Iteration 170/1000 | Loss: 0.00002563
Iteration 171/1000 | Loss: 0.00002563
Iteration 172/1000 | Loss: 0.00002563
Iteration 173/1000 | Loss: 0.00002563
Iteration 174/1000 | Loss: 0.00002563
Iteration 175/1000 | Loss: 0.00002563
Iteration 176/1000 | Loss: 0.00002563
Iteration 177/1000 | Loss: 0.00002563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.5627854483900592e-05, 2.5627854483900592e-05, 2.5627854483900592e-05, 2.5627854483900592e-05, 2.5627854483900592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5627854483900592e-05

Optimization complete. Final v2v error: 4.424710750579834 mm

Highest mean error: 4.7523417472839355 mm for frame 75

Lowest mean error: 4.0978851318359375 mm for frame 4

Saving results

Total time: 33.393261671066284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835571
Iteration 2/25 | Loss: 0.00133202
Iteration 3/25 | Loss: 0.00113096
Iteration 4/25 | Loss: 0.00109666
Iteration 5/25 | Loss: 0.00108520
Iteration 6/25 | Loss: 0.00108328
Iteration 7/25 | Loss: 0.00108328
Iteration 8/25 | Loss: 0.00108328
Iteration 9/25 | Loss: 0.00108328
Iteration 10/25 | Loss: 0.00108328
Iteration 11/25 | Loss: 0.00108328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001083283219486475, 0.001083283219486475, 0.001083283219486475, 0.001083283219486475, 0.001083283219486475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083283219486475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38744521
Iteration 2/25 | Loss: 0.00060686
Iteration 3/25 | Loss: 0.00060686
Iteration 4/25 | Loss: 0.00060686
Iteration 5/25 | Loss: 0.00060686
Iteration 6/25 | Loss: 0.00060686
Iteration 7/25 | Loss: 0.00060685
Iteration 8/25 | Loss: 0.00060685
Iteration 9/25 | Loss: 0.00060685
Iteration 10/25 | Loss: 0.00060685
Iteration 11/25 | Loss: 0.00060685
Iteration 12/25 | Loss: 0.00060685
Iteration 13/25 | Loss: 0.00060685
Iteration 14/25 | Loss: 0.00060685
Iteration 15/25 | Loss: 0.00060685
Iteration 16/25 | Loss: 0.00060685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000606854388024658, 0.000606854388024658, 0.000606854388024658, 0.000606854388024658, 0.000606854388024658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000606854388024658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060685
Iteration 2/1000 | Loss: 0.00004851
Iteration 3/1000 | Loss: 0.00004004
Iteration 4/1000 | Loss: 0.00003738
Iteration 5/1000 | Loss: 0.00003541
Iteration 6/1000 | Loss: 0.00003433
Iteration 7/1000 | Loss: 0.00003372
Iteration 8/1000 | Loss: 0.00003320
Iteration 9/1000 | Loss: 0.00003285
Iteration 10/1000 | Loss: 0.00003252
Iteration 11/1000 | Loss: 0.00003225
Iteration 12/1000 | Loss: 0.00003207
Iteration 13/1000 | Loss: 0.00003203
Iteration 14/1000 | Loss: 0.00003202
Iteration 15/1000 | Loss: 0.00003201
Iteration 16/1000 | Loss: 0.00003199
Iteration 17/1000 | Loss: 0.00003198
Iteration 18/1000 | Loss: 0.00003198
Iteration 19/1000 | Loss: 0.00003197
Iteration 20/1000 | Loss: 0.00003197
Iteration 21/1000 | Loss: 0.00003197
Iteration 22/1000 | Loss: 0.00003196
Iteration 23/1000 | Loss: 0.00003196
Iteration 24/1000 | Loss: 0.00003196
Iteration 25/1000 | Loss: 0.00003195
Iteration 26/1000 | Loss: 0.00003195
Iteration 27/1000 | Loss: 0.00003195
Iteration 28/1000 | Loss: 0.00003195
Iteration 29/1000 | Loss: 0.00003195
Iteration 30/1000 | Loss: 0.00003194
Iteration 31/1000 | Loss: 0.00003194
Iteration 32/1000 | Loss: 0.00003194
Iteration 33/1000 | Loss: 0.00003194
Iteration 34/1000 | Loss: 0.00003194
Iteration 35/1000 | Loss: 0.00003194
Iteration 36/1000 | Loss: 0.00003194
Iteration 37/1000 | Loss: 0.00003194
Iteration 38/1000 | Loss: 0.00003194
Iteration 39/1000 | Loss: 0.00003193
Iteration 40/1000 | Loss: 0.00003193
Iteration 41/1000 | Loss: 0.00003193
Iteration 42/1000 | Loss: 0.00003192
Iteration 43/1000 | Loss: 0.00003192
Iteration 44/1000 | Loss: 0.00003192
Iteration 45/1000 | Loss: 0.00003192
Iteration 46/1000 | Loss: 0.00003192
Iteration 47/1000 | Loss: 0.00003191
Iteration 48/1000 | Loss: 0.00003191
Iteration 49/1000 | Loss: 0.00003191
Iteration 50/1000 | Loss: 0.00003191
Iteration 51/1000 | Loss: 0.00003190
Iteration 52/1000 | Loss: 0.00003190
Iteration 53/1000 | Loss: 0.00003190
Iteration 54/1000 | Loss: 0.00003189
Iteration 55/1000 | Loss: 0.00003189
Iteration 56/1000 | Loss: 0.00003189
Iteration 57/1000 | Loss: 0.00003188
Iteration 58/1000 | Loss: 0.00003188
Iteration 59/1000 | Loss: 0.00003188
Iteration 60/1000 | Loss: 0.00003188
Iteration 61/1000 | Loss: 0.00003188
Iteration 62/1000 | Loss: 0.00003187
Iteration 63/1000 | Loss: 0.00003187
Iteration 64/1000 | Loss: 0.00003187
Iteration 65/1000 | Loss: 0.00003187
Iteration 66/1000 | Loss: 0.00003187
Iteration 67/1000 | Loss: 0.00003187
Iteration 68/1000 | Loss: 0.00003186
Iteration 69/1000 | Loss: 0.00003186
Iteration 70/1000 | Loss: 0.00003186
Iteration 71/1000 | Loss: 0.00003186
Iteration 72/1000 | Loss: 0.00003186
Iteration 73/1000 | Loss: 0.00003186
Iteration 74/1000 | Loss: 0.00003186
Iteration 75/1000 | Loss: 0.00003186
Iteration 76/1000 | Loss: 0.00003186
Iteration 77/1000 | Loss: 0.00003186
Iteration 78/1000 | Loss: 0.00003185
Iteration 79/1000 | Loss: 0.00003185
Iteration 80/1000 | Loss: 0.00003185
Iteration 81/1000 | Loss: 0.00003185
Iteration 82/1000 | Loss: 0.00003185
Iteration 83/1000 | Loss: 0.00003185
Iteration 84/1000 | Loss: 0.00003185
Iteration 85/1000 | Loss: 0.00003185
Iteration 86/1000 | Loss: 0.00003185
Iteration 87/1000 | Loss: 0.00003185
Iteration 88/1000 | Loss: 0.00003185
Iteration 89/1000 | Loss: 0.00003185
Iteration 90/1000 | Loss: 0.00003184
Iteration 91/1000 | Loss: 0.00003184
Iteration 92/1000 | Loss: 0.00003184
Iteration 93/1000 | Loss: 0.00003184
Iteration 94/1000 | Loss: 0.00003184
Iteration 95/1000 | Loss: 0.00003184
Iteration 96/1000 | Loss: 0.00003183
Iteration 97/1000 | Loss: 0.00003183
Iteration 98/1000 | Loss: 0.00003183
Iteration 99/1000 | Loss: 0.00003183
Iteration 100/1000 | Loss: 0.00003183
Iteration 101/1000 | Loss: 0.00003182
Iteration 102/1000 | Loss: 0.00003182
Iteration 103/1000 | Loss: 0.00003182
Iteration 104/1000 | Loss: 0.00003182
Iteration 105/1000 | Loss: 0.00003182
Iteration 106/1000 | Loss: 0.00003182
Iteration 107/1000 | Loss: 0.00003181
Iteration 108/1000 | Loss: 0.00003181
Iteration 109/1000 | Loss: 0.00003181
Iteration 110/1000 | Loss: 0.00003181
Iteration 111/1000 | Loss: 0.00003181
Iteration 112/1000 | Loss: 0.00003181
Iteration 113/1000 | Loss: 0.00003181
Iteration 114/1000 | Loss: 0.00003181
Iteration 115/1000 | Loss: 0.00003180
Iteration 116/1000 | Loss: 0.00003180
Iteration 117/1000 | Loss: 0.00003180
Iteration 118/1000 | Loss: 0.00003180
Iteration 119/1000 | Loss: 0.00003180
Iteration 120/1000 | Loss: 0.00003179
Iteration 121/1000 | Loss: 0.00003179
Iteration 122/1000 | Loss: 0.00003179
Iteration 123/1000 | Loss: 0.00003179
Iteration 124/1000 | Loss: 0.00003178
Iteration 125/1000 | Loss: 0.00003178
Iteration 126/1000 | Loss: 0.00003178
Iteration 127/1000 | Loss: 0.00003178
Iteration 128/1000 | Loss: 0.00003178
Iteration 129/1000 | Loss: 0.00003178
Iteration 130/1000 | Loss: 0.00003178
Iteration 131/1000 | Loss: 0.00003178
Iteration 132/1000 | Loss: 0.00003178
Iteration 133/1000 | Loss: 0.00003177
Iteration 134/1000 | Loss: 0.00003177
Iteration 135/1000 | Loss: 0.00003177
Iteration 136/1000 | Loss: 0.00003177
Iteration 137/1000 | Loss: 0.00003177
Iteration 138/1000 | Loss: 0.00003177
Iteration 139/1000 | Loss: 0.00003177
Iteration 140/1000 | Loss: 0.00003177
Iteration 141/1000 | Loss: 0.00003177
Iteration 142/1000 | Loss: 0.00003177
Iteration 143/1000 | Loss: 0.00003177
Iteration 144/1000 | Loss: 0.00003177
Iteration 145/1000 | Loss: 0.00003177
Iteration 146/1000 | Loss: 0.00003177
Iteration 147/1000 | Loss: 0.00003177
Iteration 148/1000 | Loss: 0.00003177
Iteration 149/1000 | Loss: 0.00003177
Iteration 150/1000 | Loss: 0.00003177
Iteration 151/1000 | Loss: 0.00003177
Iteration 152/1000 | Loss: 0.00003176
Iteration 153/1000 | Loss: 0.00003176
Iteration 154/1000 | Loss: 0.00003176
Iteration 155/1000 | Loss: 0.00003176
Iteration 156/1000 | Loss: 0.00003176
Iteration 157/1000 | Loss: 0.00003176
Iteration 158/1000 | Loss: 0.00003176
Iteration 159/1000 | Loss: 0.00003176
Iteration 160/1000 | Loss: 0.00003176
Iteration 161/1000 | Loss: 0.00003176
Iteration 162/1000 | Loss: 0.00003176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.176346217514947e-05, 3.176346217514947e-05, 3.176346217514947e-05, 3.176346217514947e-05, 3.176346217514947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.176346217514947e-05

Optimization complete. Final v2v error: 4.959845542907715 mm

Highest mean error: 5.458553791046143 mm for frame 25

Lowest mean error: 4.594451904296875 mm for frame 63

Saving results

Total time: 40.08733081817627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374244
Iteration 2/25 | Loss: 0.00115625
Iteration 3/25 | Loss: 0.00104350
Iteration 4/25 | Loss: 0.00102635
Iteration 5/25 | Loss: 0.00102238
Iteration 6/25 | Loss: 0.00102190
Iteration 7/25 | Loss: 0.00102190
Iteration 8/25 | Loss: 0.00102190
Iteration 9/25 | Loss: 0.00102190
Iteration 10/25 | Loss: 0.00102190
Iteration 11/25 | Loss: 0.00102190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001021899632178247, 0.001021899632178247, 0.001021899632178247, 0.001021899632178247, 0.001021899632178247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001021899632178247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56516695
Iteration 2/25 | Loss: 0.00060763
Iteration 3/25 | Loss: 0.00060762
Iteration 4/25 | Loss: 0.00060762
Iteration 5/25 | Loss: 0.00060762
Iteration 6/25 | Loss: 0.00060762
Iteration 7/25 | Loss: 0.00060762
Iteration 8/25 | Loss: 0.00060762
Iteration 9/25 | Loss: 0.00060762
Iteration 10/25 | Loss: 0.00060762
Iteration 11/25 | Loss: 0.00060762
Iteration 12/25 | Loss: 0.00060762
Iteration 13/25 | Loss: 0.00060762
Iteration 14/25 | Loss: 0.00060762
Iteration 15/25 | Loss: 0.00060762
Iteration 16/25 | Loss: 0.00060762
Iteration 17/25 | Loss: 0.00060762
Iteration 18/25 | Loss: 0.00060762
Iteration 19/25 | Loss: 0.00060762
Iteration 20/25 | Loss: 0.00060762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006076223799027503, 0.0006076223799027503, 0.0006076223799027503, 0.0006076223799027503, 0.0006076223799027503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006076223799027503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060762
Iteration 2/1000 | Loss: 0.00003342
Iteration 3/1000 | Loss: 0.00002575
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002216
Iteration 7/1000 | Loss: 0.00002200
Iteration 8/1000 | Loss: 0.00002179
Iteration 9/1000 | Loss: 0.00002173
Iteration 10/1000 | Loss: 0.00002166
Iteration 11/1000 | Loss: 0.00002163
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002161
Iteration 14/1000 | Loss: 0.00002161
Iteration 15/1000 | Loss: 0.00002161
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002160
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002158
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002157
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00002156
Iteration 27/1000 | Loss: 0.00002156
Iteration 28/1000 | Loss: 0.00002156
Iteration 29/1000 | Loss: 0.00002156
Iteration 30/1000 | Loss: 0.00002156
Iteration 31/1000 | Loss: 0.00002156
Iteration 32/1000 | Loss: 0.00002156
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002156
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002156
Iteration 37/1000 | Loss: 0.00002155
Iteration 38/1000 | Loss: 0.00002155
Iteration 39/1000 | Loss: 0.00002154
Iteration 40/1000 | Loss: 0.00002154
Iteration 41/1000 | Loss: 0.00002154
Iteration 42/1000 | Loss: 0.00002154
Iteration 43/1000 | Loss: 0.00002154
Iteration 44/1000 | Loss: 0.00002154
Iteration 45/1000 | Loss: 0.00002154
Iteration 46/1000 | Loss: 0.00002154
Iteration 47/1000 | Loss: 0.00002154
Iteration 48/1000 | Loss: 0.00002154
Iteration 49/1000 | Loss: 0.00002154
Iteration 50/1000 | Loss: 0.00002154
Iteration 51/1000 | Loss: 0.00002154
Iteration 52/1000 | Loss: 0.00002154
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002154
Iteration 55/1000 | Loss: 0.00002154
Iteration 56/1000 | Loss: 0.00002154
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [2.1537958673434332e-05, 2.1537958673434332e-05, 2.1537958673434332e-05, 2.1537958673434332e-05, 2.1537958673434332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1537958673434332e-05

Optimization complete. Final v2v error: 4.062322616577148 mm

Highest mean error: 4.247525691986084 mm for frame 38

Lowest mean error: 3.8451805114746094 mm for frame 139

Saving results

Total time: 27.36951994895935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416062
Iteration 2/25 | Loss: 0.00128406
Iteration 3/25 | Loss: 0.00113972
Iteration 4/25 | Loss: 0.00111509
Iteration 5/25 | Loss: 0.00110773
Iteration 6/25 | Loss: 0.00110575
Iteration 7/25 | Loss: 0.00110497
Iteration 8/25 | Loss: 0.00110497
Iteration 9/25 | Loss: 0.00110497
Iteration 10/25 | Loss: 0.00110497
Iteration 11/25 | Loss: 0.00110497
Iteration 12/25 | Loss: 0.00110497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001104969996958971, 0.001104969996958971, 0.001104969996958971, 0.001104969996958971, 0.001104969996958971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001104969996958971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28104854
Iteration 2/25 | Loss: 0.00071686
Iteration 3/25 | Loss: 0.00071686
Iteration 4/25 | Loss: 0.00071686
Iteration 5/25 | Loss: 0.00071686
Iteration 6/25 | Loss: 0.00071686
Iteration 7/25 | Loss: 0.00071686
Iteration 8/25 | Loss: 0.00071686
Iteration 9/25 | Loss: 0.00071686
Iteration 10/25 | Loss: 0.00071686
Iteration 11/25 | Loss: 0.00071686
Iteration 12/25 | Loss: 0.00071686
Iteration 13/25 | Loss: 0.00071686
Iteration 14/25 | Loss: 0.00071686
Iteration 15/25 | Loss: 0.00071686
Iteration 16/25 | Loss: 0.00071686
Iteration 17/25 | Loss: 0.00071686
Iteration 18/25 | Loss: 0.00071686
Iteration 19/25 | Loss: 0.00071686
Iteration 20/25 | Loss: 0.00071686
Iteration 21/25 | Loss: 0.00071686
Iteration 22/25 | Loss: 0.00071686
Iteration 23/25 | Loss: 0.00071686
Iteration 24/25 | Loss: 0.00071686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000716861744876951, 0.000716861744876951, 0.000716861744876951, 0.000716861744876951, 0.000716861744876951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000716861744876951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071686
Iteration 2/1000 | Loss: 0.00007680
Iteration 3/1000 | Loss: 0.00005341
Iteration 4/1000 | Loss: 0.00004513
Iteration 5/1000 | Loss: 0.00003989
Iteration 6/1000 | Loss: 0.00003749
Iteration 7/1000 | Loss: 0.00003593
Iteration 8/1000 | Loss: 0.00003468
Iteration 9/1000 | Loss: 0.00003377
Iteration 10/1000 | Loss: 0.00003314
Iteration 11/1000 | Loss: 0.00003276
Iteration 12/1000 | Loss: 0.00003245
Iteration 13/1000 | Loss: 0.00003218
Iteration 14/1000 | Loss: 0.00003199
Iteration 15/1000 | Loss: 0.00003199
Iteration 16/1000 | Loss: 0.00003193
Iteration 17/1000 | Loss: 0.00003188
Iteration 18/1000 | Loss: 0.00003185
Iteration 19/1000 | Loss: 0.00003184
Iteration 20/1000 | Loss: 0.00003183
Iteration 21/1000 | Loss: 0.00003180
Iteration 22/1000 | Loss: 0.00003178
Iteration 23/1000 | Loss: 0.00003178
Iteration 24/1000 | Loss: 0.00003178
Iteration 25/1000 | Loss: 0.00003177
Iteration 26/1000 | Loss: 0.00003177
Iteration 27/1000 | Loss: 0.00003177
Iteration 28/1000 | Loss: 0.00003177
Iteration 29/1000 | Loss: 0.00003176
Iteration 30/1000 | Loss: 0.00003176
Iteration 31/1000 | Loss: 0.00003176
Iteration 32/1000 | Loss: 0.00003176
Iteration 33/1000 | Loss: 0.00003175
Iteration 34/1000 | Loss: 0.00003175
Iteration 35/1000 | Loss: 0.00003175
Iteration 36/1000 | Loss: 0.00003175
Iteration 37/1000 | Loss: 0.00003174
Iteration 38/1000 | Loss: 0.00003174
Iteration 39/1000 | Loss: 0.00003174
Iteration 40/1000 | Loss: 0.00003174
Iteration 41/1000 | Loss: 0.00003174
Iteration 42/1000 | Loss: 0.00003173
Iteration 43/1000 | Loss: 0.00003173
Iteration 44/1000 | Loss: 0.00003173
Iteration 45/1000 | Loss: 0.00003172
Iteration 46/1000 | Loss: 0.00003172
Iteration 47/1000 | Loss: 0.00003172
Iteration 48/1000 | Loss: 0.00003172
Iteration 49/1000 | Loss: 0.00003172
Iteration 50/1000 | Loss: 0.00003172
Iteration 51/1000 | Loss: 0.00003172
Iteration 52/1000 | Loss: 0.00003171
Iteration 53/1000 | Loss: 0.00003171
Iteration 54/1000 | Loss: 0.00003171
Iteration 55/1000 | Loss: 0.00003171
Iteration 56/1000 | Loss: 0.00003171
Iteration 57/1000 | Loss: 0.00003171
Iteration 58/1000 | Loss: 0.00003171
Iteration 59/1000 | Loss: 0.00003171
Iteration 60/1000 | Loss: 0.00003171
Iteration 61/1000 | Loss: 0.00003171
Iteration 62/1000 | Loss: 0.00003170
Iteration 63/1000 | Loss: 0.00003170
Iteration 64/1000 | Loss: 0.00003170
Iteration 65/1000 | Loss: 0.00003170
Iteration 66/1000 | Loss: 0.00003170
Iteration 67/1000 | Loss: 0.00003170
Iteration 68/1000 | Loss: 0.00003170
Iteration 69/1000 | Loss: 0.00003170
Iteration 70/1000 | Loss: 0.00003169
Iteration 71/1000 | Loss: 0.00003169
Iteration 72/1000 | Loss: 0.00003169
Iteration 73/1000 | Loss: 0.00003169
Iteration 74/1000 | Loss: 0.00003169
Iteration 75/1000 | Loss: 0.00003169
Iteration 76/1000 | Loss: 0.00003168
Iteration 77/1000 | Loss: 0.00003168
Iteration 78/1000 | Loss: 0.00003168
Iteration 79/1000 | Loss: 0.00003168
Iteration 80/1000 | Loss: 0.00003168
Iteration 81/1000 | Loss: 0.00003168
Iteration 82/1000 | Loss: 0.00003168
Iteration 83/1000 | Loss: 0.00003168
Iteration 84/1000 | Loss: 0.00003168
Iteration 85/1000 | Loss: 0.00003167
Iteration 86/1000 | Loss: 0.00003167
Iteration 87/1000 | Loss: 0.00003167
Iteration 88/1000 | Loss: 0.00003167
Iteration 89/1000 | Loss: 0.00003167
Iteration 90/1000 | Loss: 0.00003167
Iteration 91/1000 | Loss: 0.00003167
Iteration 92/1000 | Loss: 0.00003167
Iteration 93/1000 | Loss: 0.00003167
Iteration 94/1000 | Loss: 0.00003167
Iteration 95/1000 | Loss: 0.00003167
Iteration 96/1000 | Loss: 0.00003167
Iteration 97/1000 | Loss: 0.00003166
Iteration 98/1000 | Loss: 0.00003166
Iteration 99/1000 | Loss: 0.00003166
Iteration 100/1000 | Loss: 0.00003166
Iteration 101/1000 | Loss: 0.00003166
Iteration 102/1000 | Loss: 0.00003166
Iteration 103/1000 | Loss: 0.00003166
Iteration 104/1000 | Loss: 0.00003166
Iteration 105/1000 | Loss: 0.00003166
Iteration 106/1000 | Loss: 0.00003166
Iteration 107/1000 | Loss: 0.00003166
Iteration 108/1000 | Loss: 0.00003166
Iteration 109/1000 | Loss: 0.00003166
Iteration 110/1000 | Loss: 0.00003166
Iteration 111/1000 | Loss: 0.00003166
Iteration 112/1000 | Loss: 0.00003166
Iteration 113/1000 | Loss: 0.00003166
Iteration 114/1000 | Loss: 0.00003166
Iteration 115/1000 | Loss: 0.00003166
Iteration 116/1000 | Loss: 0.00003166
Iteration 117/1000 | Loss: 0.00003166
Iteration 118/1000 | Loss: 0.00003166
Iteration 119/1000 | Loss: 0.00003166
Iteration 120/1000 | Loss: 0.00003166
Iteration 121/1000 | Loss: 0.00003166
Iteration 122/1000 | Loss: 0.00003166
Iteration 123/1000 | Loss: 0.00003166
Iteration 124/1000 | Loss: 0.00003166
Iteration 125/1000 | Loss: 0.00003166
Iteration 126/1000 | Loss: 0.00003166
Iteration 127/1000 | Loss: 0.00003166
Iteration 128/1000 | Loss: 0.00003166
Iteration 129/1000 | Loss: 0.00003166
Iteration 130/1000 | Loss: 0.00003166
Iteration 131/1000 | Loss: 0.00003166
Iteration 132/1000 | Loss: 0.00003166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.165952512063086e-05, 3.165952512063086e-05, 3.165952512063086e-05, 3.165952512063086e-05, 3.165952512063086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.165952512063086e-05

Optimization complete. Final v2v error: 4.812039375305176 mm

Highest mean error: 5.230081558227539 mm for frame 5

Lowest mean error: 4.415245056152344 mm for frame 69

Saving results

Total time: 37.96222448348999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872366
Iteration 2/25 | Loss: 0.00148574
Iteration 3/25 | Loss: 0.00123908
Iteration 4/25 | Loss: 0.00121057
Iteration 5/25 | Loss: 0.00120628
Iteration 6/25 | Loss: 0.00120485
Iteration 7/25 | Loss: 0.00120439
Iteration 8/25 | Loss: 0.00120439
Iteration 9/25 | Loss: 0.00120439
Iteration 10/25 | Loss: 0.00120439
Iteration 11/25 | Loss: 0.00120439
Iteration 12/25 | Loss: 0.00120439
Iteration 13/25 | Loss: 0.00120439
Iteration 14/25 | Loss: 0.00120439
Iteration 15/25 | Loss: 0.00120439
Iteration 16/25 | Loss: 0.00120439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012043905444443226, 0.0012043905444443226, 0.0012043905444443226, 0.0012043905444443226, 0.0012043905444443226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012043905444443226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68058324
Iteration 2/25 | Loss: 0.00056791
Iteration 3/25 | Loss: 0.00056787
Iteration 4/25 | Loss: 0.00056787
Iteration 5/25 | Loss: 0.00056787
Iteration 6/25 | Loss: 0.00056787
Iteration 7/25 | Loss: 0.00056787
Iteration 8/25 | Loss: 0.00056787
Iteration 9/25 | Loss: 0.00056787
Iteration 10/25 | Loss: 0.00056787
Iteration 11/25 | Loss: 0.00056787
Iteration 12/25 | Loss: 0.00056787
Iteration 13/25 | Loss: 0.00056787
Iteration 14/25 | Loss: 0.00056787
Iteration 15/25 | Loss: 0.00056787
Iteration 16/25 | Loss: 0.00056787
Iteration 17/25 | Loss: 0.00056787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005678706802427769, 0.0005678706802427769, 0.0005678706802427769, 0.0005678706802427769, 0.0005678706802427769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005678706802427769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056787
Iteration 2/1000 | Loss: 0.00009160
Iteration 3/1000 | Loss: 0.00006490
Iteration 4/1000 | Loss: 0.00005498
Iteration 5/1000 | Loss: 0.00004988
Iteration 6/1000 | Loss: 0.00004740
Iteration 7/1000 | Loss: 0.00004562
Iteration 8/1000 | Loss: 0.00004443
Iteration 9/1000 | Loss: 0.00004352
Iteration 10/1000 | Loss: 0.00004267
Iteration 11/1000 | Loss: 0.00004204
Iteration 12/1000 | Loss: 0.00004139
Iteration 13/1000 | Loss: 0.00004081
Iteration 14/1000 | Loss: 0.00004042
Iteration 15/1000 | Loss: 0.00004015
Iteration 16/1000 | Loss: 0.00004011
Iteration 17/1000 | Loss: 0.00003993
Iteration 18/1000 | Loss: 0.00003980
Iteration 19/1000 | Loss: 0.00003979
Iteration 20/1000 | Loss: 0.00003975
Iteration 21/1000 | Loss: 0.00003975
Iteration 22/1000 | Loss: 0.00003975
Iteration 23/1000 | Loss: 0.00003975
Iteration 24/1000 | Loss: 0.00003975
Iteration 25/1000 | Loss: 0.00003975
Iteration 26/1000 | Loss: 0.00003975
Iteration 27/1000 | Loss: 0.00003974
Iteration 28/1000 | Loss: 0.00003974
Iteration 29/1000 | Loss: 0.00003972
Iteration 30/1000 | Loss: 0.00003972
Iteration 31/1000 | Loss: 0.00003970
Iteration 32/1000 | Loss: 0.00003969
Iteration 33/1000 | Loss: 0.00003969
Iteration 34/1000 | Loss: 0.00003968
Iteration 35/1000 | Loss: 0.00003967
Iteration 36/1000 | Loss: 0.00003967
Iteration 37/1000 | Loss: 0.00003964
Iteration 38/1000 | Loss: 0.00003964
Iteration 39/1000 | Loss: 0.00003963
Iteration 40/1000 | Loss: 0.00003963
Iteration 41/1000 | Loss: 0.00003963
Iteration 42/1000 | Loss: 0.00003962
Iteration 43/1000 | Loss: 0.00003962
Iteration 44/1000 | Loss: 0.00003962
Iteration 45/1000 | Loss: 0.00003961
Iteration 46/1000 | Loss: 0.00003961
Iteration 47/1000 | Loss: 0.00003961
Iteration 48/1000 | Loss: 0.00003961
Iteration 49/1000 | Loss: 0.00003961
Iteration 50/1000 | Loss: 0.00003960
Iteration 51/1000 | Loss: 0.00003960
Iteration 52/1000 | Loss: 0.00003960
Iteration 53/1000 | Loss: 0.00003960
Iteration 54/1000 | Loss: 0.00003960
Iteration 55/1000 | Loss: 0.00003960
Iteration 56/1000 | Loss: 0.00003959
Iteration 57/1000 | Loss: 0.00003959
Iteration 58/1000 | Loss: 0.00003959
Iteration 59/1000 | Loss: 0.00003958
Iteration 60/1000 | Loss: 0.00003958
Iteration 61/1000 | Loss: 0.00003958
Iteration 62/1000 | Loss: 0.00003958
Iteration 63/1000 | Loss: 0.00003958
Iteration 64/1000 | Loss: 0.00003958
Iteration 65/1000 | Loss: 0.00003957
Iteration 66/1000 | Loss: 0.00003957
Iteration 67/1000 | Loss: 0.00003957
Iteration 68/1000 | Loss: 0.00003957
Iteration 69/1000 | Loss: 0.00003957
Iteration 70/1000 | Loss: 0.00003957
Iteration 71/1000 | Loss: 0.00003957
Iteration 72/1000 | Loss: 0.00003956
Iteration 73/1000 | Loss: 0.00003956
Iteration 74/1000 | Loss: 0.00003956
Iteration 75/1000 | Loss: 0.00003956
Iteration 76/1000 | Loss: 0.00003956
Iteration 77/1000 | Loss: 0.00003956
Iteration 78/1000 | Loss: 0.00003956
Iteration 79/1000 | Loss: 0.00003955
Iteration 80/1000 | Loss: 0.00003955
Iteration 81/1000 | Loss: 0.00003955
Iteration 82/1000 | Loss: 0.00003955
Iteration 83/1000 | Loss: 0.00003955
Iteration 84/1000 | Loss: 0.00003955
Iteration 85/1000 | Loss: 0.00003955
Iteration 86/1000 | Loss: 0.00003955
Iteration 87/1000 | Loss: 0.00003954
Iteration 88/1000 | Loss: 0.00003954
Iteration 89/1000 | Loss: 0.00003954
Iteration 90/1000 | Loss: 0.00003954
Iteration 91/1000 | Loss: 0.00003953
Iteration 92/1000 | Loss: 0.00003953
Iteration 93/1000 | Loss: 0.00003953
Iteration 94/1000 | Loss: 0.00003953
Iteration 95/1000 | Loss: 0.00003953
Iteration 96/1000 | Loss: 0.00003953
Iteration 97/1000 | Loss: 0.00003953
Iteration 98/1000 | Loss: 0.00003952
Iteration 99/1000 | Loss: 0.00003952
Iteration 100/1000 | Loss: 0.00003952
Iteration 101/1000 | Loss: 0.00003952
Iteration 102/1000 | Loss: 0.00003952
Iteration 103/1000 | Loss: 0.00003952
Iteration 104/1000 | Loss: 0.00003952
Iteration 105/1000 | Loss: 0.00003952
Iteration 106/1000 | Loss: 0.00003952
Iteration 107/1000 | Loss: 0.00003952
Iteration 108/1000 | Loss: 0.00003952
Iteration 109/1000 | Loss: 0.00003951
Iteration 110/1000 | Loss: 0.00003951
Iteration 111/1000 | Loss: 0.00003951
Iteration 112/1000 | Loss: 0.00003951
Iteration 113/1000 | Loss: 0.00003951
Iteration 114/1000 | Loss: 0.00003951
Iteration 115/1000 | Loss: 0.00003951
Iteration 116/1000 | Loss: 0.00003951
Iteration 117/1000 | Loss: 0.00003951
Iteration 118/1000 | Loss: 0.00003951
Iteration 119/1000 | Loss: 0.00003951
Iteration 120/1000 | Loss: 0.00003951
Iteration 121/1000 | Loss: 0.00003951
Iteration 122/1000 | Loss: 0.00003951
Iteration 123/1000 | Loss: 0.00003950
Iteration 124/1000 | Loss: 0.00003950
Iteration 125/1000 | Loss: 0.00003950
Iteration 126/1000 | Loss: 0.00003950
Iteration 127/1000 | Loss: 0.00003950
Iteration 128/1000 | Loss: 0.00003950
Iteration 129/1000 | Loss: 0.00003950
Iteration 130/1000 | Loss: 0.00003950
Iteration 131/1000 | Loss: 0.00003950
Iteration 132/1000 | Loss: 0.00003950
Iteration 133/1000 | Loss: 0.00003950
Iteration 134/1000 | Loss: 0.00003950
Iteration 135/1000 | Loss: 0.00003950
Iteration 136/1000 | Loss: 0.00003950
Iteration 137/1000 | Loss: 0.00003949
Iteration 138/1000 | Loss: 0.00003949
Iteration 139/1000 | Loss: 0.00003949
Iteration 140/1000 | Loss: 0.00003949
Iteration 141/1000 | Loss: 0.00003949
Iteration 142/1000 | Loss: 0.00003949
Iteration 143/1000 | Loss: 0.00003949
Iteration 144/1000 | Loss: 0.00003949
Iteration 145/1000 | Loss: 0.00003949
Iteration 146/1000 | Loss: 0.00003948
Iteration 147/1000 | Loss: 0.00003948
Iteration 148/1000 | Loss: 0.00003948
Iteration 149/1000 | Loss: 0.00003948
Iteration 150/1000 | Loss: 0.00003948
Iteration 151/1000 | Loss: 0.00003948
Iteration 152/1000 | Loss: 0.00003948
Iteration 153/1000 | Loss: 0.00003948
Iteration 154/1000 | Loss: 0.00003948
Iteration 155/1000 | Loss: 0.00003948
Iteration 156/1000 | Loss: 0.00003948
Iteration 157/1000 | Loss: 0.00003947
Iteration 158/1000 | Loss: 0.00003947
Iteration 159/1000 | Loss: 0.00003947
Iteration 160/1000 | Loss: 0.00003947
Iteration 161/1000 | Loss: 0.00003947
Iteration 162/1000 | Loss: 0.00003947
Iteration 163/1000 | Loss: 0.00003947
Iteration 164/1000 | Loss: 0.00003947
Iteration 165/1000 | Loss: 0.00003947
Iteration 166/1000 | Loss: 0.00003947
Iteration 167/1000 | Loss: 0.00003947
Iteration 168/1000 | Loss: 0.00003947
Iteration 169/1000 | Loss: 0.00003946
Iteration 170/1000 | Loss: 0.00003946
Iteration 171/1000 | Loss: 0.00003946
Iteration 172/1000 | Loss: 0.00003946
Iteration 173/1000 | Loss: 0.00003946
Iteration 174/1000 | Loss: 0.00003946
Iteration 175/1000 | Loss: 0.00003946
Iteration 176/1000 | Loss: 0.00003946
Iteration 177/1000 | Loss: 0.00003946
Iteration 178/1000 | Loss: 0.00003946
Iteration 179/1000 | Loss: 0.00003946
Iteration 180/1000 | Loss: 0.00003946
Iteration 181/1000 | Loss: 0.00003946
Iteration 182/1000 | Loss: 0.00003946
Iteration 183/1000 | Loss: 0.00003945
Iteration 184/1000 | Loss: 0.00003945
Iteration 185/1000 | Loss: 0.00003945
Iteration 186/1000 | Loss: 0.00003945
Iteration 187/1000 | Loss: 0.00003945
Iteration 188/1000 | Loss: 0.00003945
Iteration 189/1000 | Loss: 0.00003945
Iteration 190/1000 | Loss: 0.00003945
Iteration 191/1000 | Loss: 0.00003945
Iteration 192/1000 | Loss: 0.00003945
Iteration 193/1000 | Loss: 0.00003945
Iteration 194/1000 | Loss: 0.00003945
Iteration 195/1000 | Loss: 0.00003945
Iteration 196/1000 | Loss: 0.00003945
Iteration 197/1000 | Loss: 0.00003945
Iteration 198/1000 | Loss: 0.00003945
Iteration 199/1000 | Loss: 0.00003945
Iteration 200/1000 | Loss: 0.00003945
Iteration 201/1000 | Loss: 0.00003944
Iteration 202/1000 | Loss: 0.00003944
Iteration 203/1000 | Loss: 0.00003944
Iteration 204/1000 | Loss: 0.00003944
Iteration 205/1000 | Loss: 0.00003944
Iteration 206/1000 | Loss: 0.00003944
Iteration 207/1000 | Loss: 0.00003944
Iteration 208/1000 | Loss: 0.00003944
Iteration 209/1000 | Loss: 0.00003944
Iteration 210/1000 | Loss: 0.00003944
Iteration 211/1000 | Loss: 0.00003944
Iteration 212/1000 | Loss: 0.00003944
Iteration 213/1000 | Loss: 0.00003944
Iteration 214/1000 | Loss: 0.00003944
Iteration 215/1000 | Loss: 0.00003944
Iteration 216/1000 | Loss: 0.00003944
Iteration 217/1000 | Loss: 0.00003944
Iteration 218/1000 | Loss: 0.00003944
Iteration 219/1000 | Loss: 0.00003944
Iteration 220/1000 | Loss: 0.00003943
Iteration 221/1000 | Loss: 0.00003943
Iteration 222/1000 | Loss: 0.00003943
Iteration 223/1000 | Loss: 0.00003943
Iteration 224/1000 | Loss: 0.00003943
Iteration 225/1000 | Loss: 0.00003943
Iteration 226/1000 | Loss: 0.00003943
Iteration 227/1000 | Loss: 0.00003943
Iteration 228/1000 | Loss: 0.00003943
Iteration 229/1000 | Loss: 0.00003943
Iteration 230/1000 | Loss: 0.00003943
Iteration 231/1000 | Loss: 0.00003943
Iteration 232/1000 | Loss: 0.00003943
Iteration 233/1000 | Loss: 0.00003943
Iteration 234/1000 | Loss: 0.00003943
Iteration 235/1000 | Loss: 0.00003943
Iteration 236/1000 | Loss: 0.00003943
Iteration 237/1000 | Loss: 0.00003943
Iteration 238/1000 | Loss: 0.00003943
Iteration 239/1000 | Loss: 0.00003943
Iteration 240/1000 | Loss: 0.00003943
Iteration 241/1000 | Loss: 0.00003943
Iteration 242/1000 | Loss: 0.00003943
Iteration 243/1000 | Loss: 0.00003943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [3.942646071664058e-05, 3.942646071664058e-05, 3.942646071664058e-05, 3.942646071664058e-05, 3.942646071664058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.942646071664058e-05

Optimization complete. Final v2v error: 5.214505195617676 mm

Highest mean error: 6.6711649894714355 mm for frame 28

Lowest mean error: 3.980897903442383 mm for frame 102

Saving results

Total time: 48.7936532497406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908541
Iteration 2/25 | Loss: 0.00120646
Iteration 3/25 | Loss: 0.00105490
Iteration 4/25 | Loss: 0.00103376
Iteration 5/25 | Loss: 0.00102952
Iteration 6/25 | Loss: 0.00102805
Iteration 7/25 | Loss: 0.00102805
Iteration 8/25 | Loss: 0.00102805
Iteration 9/25 | Loss: 0.00102805
Iteration 10/25 | Loss: 0.00102805
Iteration 11/25 | Loss: 0.00102805
Iteration 12/25 | Loss: 0.00102805
Iteration 13/25 | Loss: 0.00102805
Iteration 14/25 | Loss: 0.00102805
Iteration 15/25 | Loss: 0.00102805
Iteration 16/25 | Loss: 0.00102805
Iteration 17/25 | Loss: 0.00102805
Iteration 18/25 | Loss: 0.00102805
Iteration 19/25 | Loss: 0.00102805
Iteration 20/25 | Loss: 0.00102805
Iteration 21/25 | Loss: 0.00102805
Iteration 22/25 | Loss: 0.00102805
Iteration 23/25 | Loss: 0.00102805
Iteration 24/25 | Loss: 0.00102805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001028054510243237, 0.001028054510243237, 0.001028054510243237, 0.001028054510243237, 0.001028054510243237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001028054510243237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31624746
Iteration 2/25 | Loss: 0.00065845
Iteration 3/25 | Loss: 0.00065845
Iteration 4/25 | Loss: 0.00065845
Iteration 5/25 | Loss: 0.00065845
Iteration 6/25 | Loss: 0.00065845
Iteration 7/25 | Loss: 0.00065845
Iteration 8/25 | Loss: 0.00065845
Iteration 9/25 | Loss: 0.00065845
Iteration 10/25 | Loss: 0.00065845
Iteration 11/25 | Loss: 0.00065845
Iteration 12/25 | Loss: 0.00065845
Iteration 13/25 | Loss: 0.00065845
Iteration 14/25 | Loss: 0.00065845
Iteration 15/25 | Loss: 0.00065845
Iteration 16/25 | Loss: 0.00065845
Iteration 17/25 | Loss: 0.00065845
Iteration 18/25 | Loss: 0.00065845
Iteration 19/25 | Loss: 0.00065845
Iteration 20/25 | Loss: 0.00065845
Iteration 21/25 | Loss: 0.00065845
Iteration 22/25 | Loss: 0.00065845
Iteration 23/25 | Loss: 0.00065845
Iteration 24/25 | Loss: 0.00065845
Iteration 25/25 | Loss: 0.00065845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065845
Iteration 2/1000 | Loss: 0.00003048
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00002041
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00002028
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00002007
Iteration 18/1000 | Loss: 0.00002004
Iteration 19/1000 | Loss: 0.00002004
Iteration 20/1000 | Loss: 0.00002004
Iteration 21/1000 | Loss: 0.00002004
Iteration 22/1000 | Loss: 0.00002004
Iteration 23/1000 | Loss: 0.00002004
Iteration 24/1000 | Loss: 0.00002004
Iteration 25/1000 | Loss: 0.00002004
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002001
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00002001
Iteration 35/1000 | Loss: 0.00002001
Iteration 36/1000 | Loss: 0.00002001
Iteration 37/1000 | Loss: 0.00002001
Iteration 38/1000 | Loss: 0.00002001
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002001
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002001
Iteration 44/1000 | Loss: 0.00002001
Iteration 45/1000 | Loss: 0.00002001
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00002001
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00002001
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002001
Iteration 53/1000 | Loss: 0.00002001
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [2.000865788431838e-05, 2.000865788431838e-05, 2.000865788431838e-05, 2.000865788431838e-05, 2.000865788431838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.000865788431838e-05

Optimization complete. Final v2v error: 3.9101462364196777 mm

Highest mean error: 4.132071018218994 mm for frame 104

Lowest mean error: 3.7172458171844482 mm for frame 230

Saving results

Total time: 28.671049118041992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_5280/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_5280/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907350
Iteration 2/25 | Loss: 0.00196435
Iteration 3/25 | Loss: 0.00145263
Iteration 4/25 | Loss: 0.00140858
Iteration 5/25 | Loss: 0.00138467
Iteration 6/25 | Loss: 0.00137920
Iteration 7/25 | Loss: 0.00132908
Iteration 8/25 | Loss: 0.00128905
Iteration 9/25 | Loss: 0.00126977
Iteration 10/25 | Loss: 0.00123894
Iteration 11/25 | Loss: 0.00124002
Iteration 12/25 | Loss: 0.00123914
Iteration 13/25 | Loss: 0.00123130
Iteration 14/25 | Loss: 0.00122931
Iteration 15/25 | Loss: 0.00122730
Iteration 16/25 | Loss: 0.00123025
Iteration 17/25 | Loss: 0.00123065
Iteration 18/25 | Loss: 0.00122706
Iteration 19/25 | Loss: 0.00122228
Iteration 20/25 | Loss: 0.00122518
Iteration 21/25 | Loss: 0.00122404
Iteration 22/25 | Loss: 0.00122673
Iteration 23/25 | Loss: 0.00122852
Iteration 24/25 | Loss: 0.00122269
Iteration 25/25 | Loss: 0.00121956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32350004
Iteration 2/25 | Loss: 0.00077671
Iteration 3/25 | Loss: 0.00077670
Iteration 4/25 | Loss: 0.00077670
Iteration 5/25 | Loss: 0.00077670
Iteration 6/25 | Loss: 0.00077670
Iteration 7/25 | Loss: 0.00077670
Iteration 8/25 | Loss: 0.00077670
Iteration 9/25 | Loss: 0.00077670
Iteration 10/25 | Loss: 0.00077670
Iteration 11/25 | Loss: 0.00077670
Iteration 12/25 | Loss: 0.00077670
Iteration 13/25 | Loss: 0.00077670
Iteration 14/25 | Loss: 0.00077670
Iteration 15/25 | Loss: 0.00077670
Iteration 16/25 | Loss: 0.00077670
Iteration 17/25 | Loss: 0.00077670
Iteration 18/25 | Loss: 0.00077670
Iteration 19/25 | Loss: 0.00077670
Iteration 20/25 | Loss: 0.00077670
Iteration 21/25 | Loss: 0.00077670
Iteration 22/25 | Loss: 0.00077670
Iteration 23/25 | Loss: 0.00077670
Iteration 24/25 | Loss: 0.00077670
Iteration 25/25 | Loss: 0.00077670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077670
Iteration 2/1000 | Loss: 0.00171097
Iteration 3/1000 | Loss: 0.00123019
Iteration 4/1000 | Loss: 0.00119991
Iteration 5/1000 | Loss: 0.00022174
Iteration 6/1000 | Loss: 0.00019224
Iteration 7/1000 | Loss: 0.00016960
Iteration 8/1000 | Loss: 0.00013042
Iteration 9/1000 | Loss: 0.00081828
Iteration 10/1000 | Loss: 0.00075925
Iteration 11/1000 | Loss: 0.00035756
Iteration 12/1000 | Loss: 0.00061142
Iteration 13/1000 | Loss: 0.00060510
Iteration 14/1000 | Loss: 0.00050863
Iteration 15/1000 | Loss: 0.00099137
Iteration 16/1000 | Loss: 0.00025233
Iteration 17/1000 | Loss: 0.00020057
Iteration 18/1000 | Loss: 0.00026875
Iteration 19/1000 | Loss: 0.00035987
Iteration 20/1000 | Loss: 0.00040378
Iteration 21/1000 | Loss: 0.00035385
Iteration 22/1000 | Loss: 0.00028372
Iteration 23/1000 | Loss: 0.00015538
Iteration 24/1000 | Loss: 0.00027886
Iteration 25/1000 | Loss: 0.00035145
Iteration 26/1000 | Loss: 0.00033840
Iteration 27/1000 | Loss: 0.00008064
Iteration 28/1000 | Loss: 0.00049101
Iteration 29/1000 | Loss: 0.00026623
Iteration 30/1000 | Loss: 0.00053692
Iteration 31/1000 | Loss: 0.00033431
Iteration 32/1000 | Loss: 0.00009848
Iteration 33/1000 | Loss: 0.00008546
Iteration 34/1000 | Loss: 0.00007875
Iteration 35/1000 | Loss: 0.00007329
Iteration 36/1000 | Loss: 0.00007679
Iteration 37/1000 | Loss: 0.00007296
Iteration 38/1000 | Loss: 0.00005374
Iteration 39/1000 | Loss: 0.00005014
Iteration 40/1000 | Loss: 0.00006183
Iteration 41/1000 | Loss: 0.00005671
Iteration 42/1000 | Loss: 0.00004858
Iteration 43/1000 | Loss: 0.00005485
Iteration 44/1000 | Loss: 0.00004761
Iteration 45/1000 | Loss: 0.00005894
Iteration 46/1000 | Loss: 0.00005873
Iteration 47/1000 | Loss: 0.00006967
Iteration 48/1000 | Loss: 0.00005769
Iteration 49/1000 | Loss: 0.00005558
Iteration 50/1000 | Loss: 0.00006861
Iteration 51/1000 | Loss: 0.00008514
Iteration 52/1000 | Loss: 0.00007124
Iteration 53/1000 | Loss: 0.00006636
Iteration 54/1000 | Loss: 0.00004893
Iteration 55/1000 | Loss: 0.00004929
Iteration 56/1000 | Loss: 0.00005485
Iteration 57/1000 | Loss: 0.00005273
Iteration 58/1000 | Loss: 0.00004946
Iteration 59/1000 | Loss: 0.00005233
Iteration 60/1000 | Loss: 0.00005393
Iteration 61/1000 | Loss: 0.00005432
Iteration 62/1000 | Loss: 0.00006450
Iteration 63/1000 | Loss: 0.00005237
Iteration 64/1000 | Loss: 0.00005445
Iteration 65/1000 | Loss: 0.00005182
Iteration 66/1000 | Loss: 0.00004098
Iteration 67/1000 | Loss: 0.00005490
Iteration 68/1000 | Loss: 0.00005077
Iteration 69/1000 | Loss: 0.00005094
Iteration 70/1000 | Loss: 0.00005301
Iteration 71/1000 | Loss: 0.00005145
Iteration 72/1000 | Loss: 0.00006419
Iteration 73/1000 | Loss: 0.00005277
Iteration 74/1000 | Loss: 0.00005243
Iteration 75/1000 | Loss: 0.00005578
Iteration 76/1000 | Loss: 0.00004670
Iteration 77/1000 | Loss: 0.00005745
Iteration 78/1000 | Loss: 0.00004480
Iteration 79/1000 | Loss: 0.00003982
Iteration 80/1000 | Loss: 0.00005770
Iteration 81/1000 | Loss: 0.00005673
Iteration 82/1000 | Loss: 0.00005437
Iteration 83/1000 | Loss: 0.00005548
Iteration 84/1000 | Loss: 0.00004468
Iteration 85/1000 | Loss: 0.00004209
Iteration 86/1000 | Loss: 0.00004957
Iteration 87/1000 | Loss: 0.00004272
Iteration 88/1000 | Loss: 0.00005662
Iteration 89/1000 | Loss: 0.00005007
Iteration 90/1000 | Loss: 0.00005886
Iteration 91/1000 | Loss: 0.00005308
Iteration 92/1000 | Loss: 0.00005042
Iteration 93/1000 | Loss: 0.00005607
Iteration 94/1000 | Loss: 0.00005842
Iteration 95/1000 | Loss: 0.00005702
Iteration 96/1000 | Loss: 0.00004156
Iteration 97/1000 | Loss: 0.00004977
Iteration 98/1000 | Loss: 0.00005503
Iteration 99/1000 | Loss: 0.00005237
Iteration 100/1000 | Loss: 0.00006105
Iteration 101/1000 | Loss: 0.00005296
Iteration 102/1000 | Loss: 0.00004346
Iteration 103/1000 | Loss: 0.00005906
Iteration 104/1000 | Loss: 0.00004999
Iteration 105/1000 | Loss: 0.00005146
Iteration 106/1000 | Loss: 0.00005646
Iteration 107/1000 | Loss: 0.00005404
Iteration 108/1000 | Loss: 0.00005567
Iteration 109/1000 | Loss: 0.00005386
Iteration 110/1000 | Loss: 0.00005649
Iteration 111/1000 | Loss: 0.00005638
Iteration 112/1000 | Loss: 0.00005625
Iteration 113/1000 | Loss: 0.00004972
Iteration 114/1000 | Loss: 0.00004156
Iteration 115/1000 | Loss: 0.00003966
Iteration 116/1000 | Loss: 0.00003900
Iteration 117/1000 | Loss: 0.00004237
Iteration 118/1000 | Loss: 0.00005727
Iteration 119/1000 | Loss: 0.00004949
Iteration 120/1000 | Loss: 0.00005211
Iteration 121/1000 | Loss: 0.00004309
Iteration 122/1000 | Loss: 0.00004042
Iteration 123/1000 | Loss: 0.00004221
Iteration 124/1000 | Loss: 0.00005030
Iteration 125/1000 | Loss: 0.00005412
Iteration 126/1000 | Loss: 0.00004963
Iteration 127/1000 | Loss: 0.00005168
Iteration 128/1000 | Loss: 0.00005228
Iteration 129/1000 | Loss: 0.00004824
Iteration 130/1000 | Loss: 0.00005575
Iteration 131/1000 | Loss: 0.00005413
Iteration 132/1000 | Loss: 0.00005334
Iteration 133/1000 | Loss: 0.00005240
Iteration 134/1000 | Loss: 0.00005434
Iteration 135/1000 | Loss: 0.00004923
Iteration 136/1000 | Loss: 0.00005701
Iteration 137/1000 | Loss: 0.00004422
Iteration 138/1000 | Loss: 0.00004645
Iteration 139/1000 | Loss: 0.00004840
Iteration 140/1000 | Loss: 0.00004916
Iteration 141/1000 | Loss: 0.00005525
Iteration 142/1000 | Loss: 0.00005769
Iteration 143/1000 | Loss: 0.00005093
Iteration 144/1000 | Loss: 0.00005362
Iteration 145/1000 | Loss: 0.00005195
Iteration 146/1000 | Loss: 0.00005316
Iteration 147/1000 | Loss: 0.00004210
Iteration 148/1000 | Loss: 0.00005660
Iteration 149/1000 | Loss: 0.00005383
Iteration 150/1000 | Loss: 0.00004876
Iteration 151/1000 | Loss: 0.00005612
Iteration 152/1000 | Loss: 0.00004699
Iteration 153/1000 | Loss: 0.00004872
Iteration 154/1000 | Loss: 0.00004697
Iteration 155/1000 | Loss: 0.00004990
Iteration 156/1000 | Loss: 0.00004629
Iteration 157/1000 | Loss: 0.00005016
Iteration 158/1000 | Loss: 0.00004881
Iteration 159/1000 | Loss: 0.00004894
Iteration 160/1000 | Loss: 0.00004738
Iteration 161/1000 | Loss: 0.00004949
Iteration 162/1000 | Loss: 0.00004911
Iteration 163/1000 | Loss: 0.00004891
Iteration 164/1000 | Loss: 0.00004918
Iteration 165/1000 | Loss: 0.00005163
Iteration 166/1000 | Loss: 0.00005305
Iteration 167/1000 | Loss: 0.00005131
Iteration 168/1000 | Loss: 0.00005469
Iteration 169/1000 | Loss: 0.00004999
Iteration 170/1000 | Loss: 0.00005194
Iteration 171/1000 | Loss: 0.00004815
Iteration 172/1000 | Loss: 0.00005327
Iteration 173/1000 | Loss: 0.00004873
Iteration 174/1000 | Loss: 0.00005145
Iteration 175/1000 | Loss: 0.00004956
Iteration 176/1000 | Loss: 0.00005219
Iteration 177/1000 | Loss: 0.00005105
Iteration 178/1000 | Loss: 0.00005371
Iteration 179/1000 | Loss: 0.00005059
Iteration 180/1000 | Loss: 0.00004270
Iteration 181/1000 | Loss: 0.00004905
Iteration 182/1000 | Loss: 0.00005204
Iteration 183/1000 | Loss: 0.00005231
Iteration 184/1000 | Loss: 0.00005101
Iteration 185/1000 | Loss: 0.00004950
Iteration 186/1000 | Loss: 0.00006000
Iteration 187/1000 | Loss: 0.00005548
Iteration 188/1000 | Loss: 0.00005489
Iteration 189/1000 | Loss: 0.00005003
Iteration 190/1000 | Loss: 0.00005389
Iteration 191/1000 | Loss: 0.00005786
Iteration 192/1000 | Loss: 0.00006965
Iteration 193/1000 | Loss: 0.00004691
Iteration 194/1000 | Loss: 0.00005175
Iteration 195/1000 | Loss: 0.00004851
Iteration 196/1000 | Loss: 0.00005040
Iteration 197/1000 | Loss: 0.00005313
Iteration 198/1000 | Loss: 0.00005883
Iteration 199/1000 | Loss: 0.00005625
Iteration 200/1000 | Loss: 0.00005545
Iteration 201/1000 | Loss: 0.00005137
Iteration 202/1000 | Loss: 0.00005228
Iteration 203/1000 | Loss: 0.00004921
Iteration 204/1000 | Loss: 0.00005194
Iteration 205/1000 | Loss: 0.00004569
Iteration 206/1000 | Loss: 0.00004385
Iteration 207/1000 | Loss: 0.00005447
Iteration 208/1000 | Loss: 0.00004615
Iteration 209/1000 | Loss: 0.00005203
Iteration 210/1000 | Loss: 0.00005128
Iteration 211/1000 | Loss: 0.00005320
Iteration 212/1000 | Loss: 0.00006689
Iteration 213/1000 | Loss: 0.00005256
Iteration 214/1000 | Loss: 0.00005028
Iteration 215/1000 | Loss: 0.00003940
Iteration 216/1000 | Loss: 0.00006068
Iteration 217/1000 | Loss: 0.00006714
Iteration 218/1000 | Loss: 0.00005057
Iteration 219/1000 | Loss: 0.00005314
Iteration 220/1000 | Loss: 0.00005044
Iteration 221/1000 | Loss: 0.00005477
Iteration 222/1000 | Loss: 0.00004038
Iteration 223/1000 | Loss: 0.00005051
Iteration 224/1000 | Loss: 0.00006321
Iteration 225/1000 | Loss: 0.00005058
Iteration 226/1000 | Loss: 0.00005062
Iteration 227/1000 | Loss: 0.00005002
Iteration 228/1000 | Loss: 0.00005009
Iteration 229/1000 | Loss: 0.00005377
Iteration 230/1000 | Loss: 0.00006601
Iteration 231/1000 | Loss: 0.00005212
Iteration 232/1000 | Loss: 0.00005312
Iteration 233/1000 | Loss: 0.00004841
Iteration 234/1000 | Loss: 0.00005405
Iteration 235/1000 | Loss: 0.00005516
Iteration 236/1000 | Loss: 0.00004597
Iteration 237/1000 | Loss: 0.00004961
Iteration 238/1000 | Loss: 0.00004182
Iteration 239/1000 | Loss: 0.00006461
Iteration 240/1000 | Loss: 0.00006113
Iteration 241/1000 | Loss: 0.00004990
Iteration 242/1000 | Loss: 0.00005299
Iteration 243/1000 | Loss: 0.00004968
Iteration 244/1000 | Loss: 0.00005206
Iteration 245/1000 | Loss: 0.00006386
Iteration 246/1000 | Loss: 0.00005111
Iteration 247/1000 | Loss: 0.00003971
Iteration 248/1000 | Loss: 0.00003845
Iteration 249/1000 | Loss: 0.00003790
Iteration 250/1000 | Loss: 0.00003773
Iteration 251/1000 | Loss: 0.00003772
Iteration 252/1000 | Loss: 0.00003770
Iteration 253/1000 | Loss: 0.00003768
Iteration 254/1000 | Loss: 0.00003768
Iteration 255/1000 | Loss: 0.00003767
Iteration 256/1000 | Loss: 0.00003767
Iteration 257/1000 | Loss: 0.00003767
Iteration 258/1000 | Loss: 0.00003766
Iteration 259/1000 | Loss: 0.00003766
Iteration 260/1000 | Loss: 0.00003765
Iteration 261/1000 | Loss: 0.00003760
Iteration 262/1000 | Loss: 0.00003760
Iteration 263/1000 | Loss: 0.00003760
Iteration 264/1000 | Loss: 0.00003760
Iteration 265/1000 | Loss: 0.00003759
Iteration 266/1000 | Loss: 0.00003759
Iteration 267/1000 | Loss: 0.00003759
Iteration 268/1000 | Loss: 0.00003759
Iteration 269/1000 | Loss: 0.00003759
Iteration 270/1000 | Loss: 0.00003759
Iteration 271/1000 | Loss: 0.00003759
Iteration 272/1000 | Loss: 0.00003759
Iteration 273/1000 | Loss: 0.00003759
Iteration 274/1000 | Loss: 0.00003759
Iteration 275/1000 | Loss: 0.00003758
Iteration 276/1000 | Loss: 0.00003758
Iteration 277/1000 | Loss: 0.00003756
Iteration 278/1000 | Loss: 0.00003752
Iteration 279/1000 | Loss: 0.00003752
Iteration 280/1000 | Loss: 0.00003752
Iteration 281/1000 | Loss: 0.00003752
Iteration 282/1000 | Loss: 0.00003751
Iteration 283/1000 | Loss: 0.00003751
Iteration 284/1000 | Loss: 0.00003751
Iteration 285/1000 | Loss: 0.00003751
Iteration 286/1000 | Loss: 0.00003750
Iteration 287/1000 | Loss: 0.00003750
Iteration 288/1000 | Loss: 0.00003750
Iteration 289/1000 | Loss: 0.00003750
Iteration 290/1000 | Loss: 0.00003749
Iteration 291/1000 | Loss: 0.00003749
Iteration 292/1000 | Loss: 0.00003749
Iteration 293/1000 | Loss: 0.00003749
Iteration 294/1000 | Loss: 0.00003749
Iteration 295/1000 | Loss: 0.00003749
Iteration 296/1000 | Loss: 0.00003749
Iteration 297/1000 | Loss: 0.00003748
Iteration 298/1000 | Loss: 0.00003748
Iteration 299/1000 | Loss: 0.00003747
Iteration 300/1000 | Loss: 0.00003747
Iteration 301/1000 | Loss: 0.00003746
Iteration 302/1000 | Loss: 0.00003745
Iteration 303/1000 | Loss: 0.00003745
Iteration 304/1000 | Loss: 0.00003745
Iteration 305/1000 | Loss: 0.00003744
Iteration 306/1000 | Loss: 0.00003744
Iteration 307/1000 | Loss: 0.00003744
Iteration 308/1000 | Loss: 0.00003744
Iteration 309/1000 | Loss: 0.00003744
Iteration 310/1000 | Loss: 0.00003744
Iteration 311/1000 | Loss: 0.00003743
Iteration 312/1000 | Loss: 0.00003743
Iteration 313/1000 | Loss: 0.00003743
Iteration 314/1000 | Loss: 0.00003742
Iteration 315/1000 | Loss: 0.00003742
Iteration 316/1000 | Loss: 0.00003741
Iteration 317/1000 | Loss: 0.00003741
Iteration 318/1000 | Loss: 0.00003741
Iteration 319/1000 | Loss: 0.00003741
Iteration 320/1000 | Loss: 0.00003741
Iteration 321/1000 | Loss: 0.00003741
Iteration 322/1000 | Loss: 0.00003741
Iteration 323/1000 | Loss: 0.00003741
Iteration 324/1000 | Loss: 0.00003741
Iteration 325/1000 | Loss: 0.00003741
Iteration 326/1000 | Loss: 0.00003740
Iteration 327/1000 | Loss: 0.00003740
Iteration 328/1000 | Loss: 0.00003740
Iteration 329/1000 | Loss: 0.00003740
Iteration 330/1000 | Loss: 0.00003739
Iteration 331/1000 | Loss: 0.00003739
Iteration 332/1000 | Loss: 0.00003739
Iteration 333/1000 | Loss: 0.00003739
Iteration 334/1000 | Loss: 0.00003739
Iteration 335/1000 | Loss: 0.00003739
Iteration 336/1000 | Loss: 0.00003739
Iteration 337/1000 | Loss: 0.00003739
Iteration 338/1000 | Loss: 0.00003738
Iteration 339/1000 | Loss: 0.00003738
Iteration 340/1000 | Loss: 0.00003738
Iteration 341/1000 | Loss: 0.00003738
Iteration 342/1000 | Loss: 0.00003738
Iteration 343/1000 | Loss: 0.00003738
Iteration 344/1000 | Loss: 0.00003738
Iteration 345/1000 | Loss: 0.00003738
Iteration 346/1000 | Loss: 0.00003738
Iteration 347/1000 | Loss: 0.00003738
Iteration 348/1000 | Loss: 0.00003737
Iteration 349/1000 | Loss: 0.00003737
Iteration 350/1000 | Loss: 0.00003737
Iteration 351/1000 | Loss: 0.00003737
Iteration 352/1000 | Loss: 0.00003737
Iteration 353/1000 | Loss: 0.00003737
Iteration 354/1000 | Loss: 0.00003736
Iteration 355/1000 | Loss: 0.00003736
Iteration 356/1000 | Loss: 0.00003736
Iteration 357/1000 | Loss: 0.00003736
Iteration 358/1000 | Loss: 0.00003736
Iteration 359/1000 | Loss: 0.00003736
Iteration 360/1000 | Loss: 0.00003736
Iteration 361/1000 | Loss: 0.00003736
Iteration 362/1000 | Loss: 0.00003736
Iteration 363/1000 | Loss: 0.00003736
Iteration 364/1000 | Loss: 0.00003736
Iteration 365/1000 | Loss: 0.00003736
Iteration 366/1000 | Loss: 0.00003736
Iteration 367/1000 | Loss: 0.00003736
Iteration 368/1000 | Loss: 0.00003735
Iteration 369/1000 | Loss: 0.00003735
Iteration 370/1000 | Loss: 0.00003735
Iteration 371/1000 | Loss: 0.00003735
Iteration 372/1000 | Loss: 0.00003735
Iteration 373/1000 | Loss: 0.00003735
Iteration 374/1000 | Loss: 0.00003735
Iteration 375/1000 | Loss: 0.00003735
Iteration 376/1000 | Loss: 0.00003735
Iteration 377/1000 | Loss: 0.00003735
Iteration 378/1000 | Loss: 0.00003735
Iteration 379/1000 | Loss: 0.00003734
Iteration 380/1000 | Loss: 0.00003734
Iteration 381/1000 | Loss: 0.00003734
Iteration 382/1000 | Loss: 0.00003734
Iteration 383/1000 | Loss: 0.00003734
Iteration 384/1000 | Loss: 0.00003734
Iteration 385/1000 | Loss: 0.00003734
Iteration 386/1000 | Loss: 0.00003734
Iteration 387/1000 | Loss: 0.00003734
Iteration 388/1000 | Loss: 0.00003734
Iteration 389/1000 | Loss: 0.00003734
Iteration 390/1000 | Loss: 0.00003734
Iteration 391/1000 | Loss: 0.00003734
Iteration 392/1000 | Loss: 0.00003734
Iteration 393/1000 | Loss: 0.00003734
Iteration 394/1000 | Loss: 0.00003734
Iteration 395/1000 | Loss: 0.00003734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 395. Stopping optimization.
Last 5 losses: [3.734325582627207e-05, 3.734325582627207e-05, 3.734325582627207e-05, 3.734325582627207e-05, 3.734325582627207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.734325582627207e-05

Optimization complete. Final v2v error: 5.331996917724609 mm

Highest mean error: 7.586310386657715 mm for frame 150

Lowest mean error: 4.7789082527160645 mm for frame 92

Saving results

Total time: 461.0554304122925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065885
Iteration 2/25 | Loss: 0.00196400
Iteration 3/25 | Loss: 0.00137801
Iteration 4/25 | Loss: 0.00125638
Iteration 5/25 | Loss: 0.00124636
Iteration 6/25 | Loss: 0.00124538
Iteration 7/25 | Loss: 0.00125540
Iteration 8/25 | Loss: 0.00122377
Iteration 9/25 | Loss: 0.00119975
Iteration 10/25 | Loss: 0.00119223
Iteration 11/25 | Loss: 0.00118726
Iteration 12/25 | Loss: 0.00118131
Iteration 13/25 | Loss: 0.00117839
Iteration 14/25 | Loss: 0.00117964
Iteration 15/25 | Loss: 0.00117839
Iteration 16/25 | Loss: 0.00117732
Iteration 17/25 | Loss: 0.00117603
Iteration 18/25 | Loss: 0.00117563
Iteration 19/25 | Loss: 0.00117555
Iteration 20/25 | Loss: 0.00117547
Iteration 21/25 | Loss: 0.00117545
Iteration 22/25 | Loss: 0.00117543
Iteration 23/25 | Loss: 0.00117543
Iteration 24/25 | Loss: 0.00117543
Iteration 25/25 | Loss: 0.00117543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47192514
Iteration 2/25 | Loss: 0.00050954
Iteration 3/25 | Loss: 0.00050954
Iteration 4/25 | Loss: 0.00050954
Iteration 5/25 | Loss: 0.00050954
Iteration 6/25 | Loss: 0.00050954
Iteration 7/25 | Loss: 0.00050954
Iteration 8/25 | Loss: 0.00050954
Iteration 9/25 | Loss: 0.00050954
Iteration 10/25 | Loss: 0.00050953
Iteration 11/25 | Loss: 0.00050953
Iteration 12/25 | Loss: 0.00050953
Iteration 13/25 | Loss: 0.00050953
Iteration 14/25 | Loss: 0.00050953
Iteration 15/25 | Loss: 0.00050953
Iteration 16/25 | Loss: 0.00050953
Iteration 17/25 | Loss: 0.00050953
Iteration 18/25 | Loss: 0.00050953
Iteration 19/25 | Loss: 0.00050953
Iteration 20/25 | Loss: 0.00050953
Iteration 21/25 | Loss: 0.00050953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005095349624752998, 0.0005095349624752998, 0.0005095349624752998, 0.0005095349624752998, 0.0005095349624752998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005095349624752998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050954
Iteration 2/1000 | Loss: 0.00013863
Iteration 3/1000 | Loss: 0.00011276
Iteration 4/1000 | Loss: 0.00002556
Iteration 5/1000 | Loss: 0.00003246
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00005483
Iteration 8/1000 | Loss: 0.00003111
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00004822
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00002086
Iteration 13/1000 | Loss: 0.00002039
Iteration 14/1000 | Loss: 0.00001994
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001942
Iteration 20/1000 | Loss: 0.00002031
Iteration 21/1000 | Loss: 0.00001970
Iteration 22/1000 | Loss: 0.00002073
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00002208
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00002295
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002486
Iteration 29/1000 | Loss: 0.00002104
Iteration 30/1000 | Loss: 0.00002614
Iteration 31/1000 | Loss: 0.00002062
Iteration 32/1000 | Loss: 0.00002613
Iteration 33/1000 | Loss: 0.00002102
Iteration 34/1000 | Loss: 0.00002498
Iteration 35/1000 | Loss: 0.00002093
Iteration 36/1000 | Loss: 0.00002712
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001776
Iteration 47/1000 | Loss: 0.00001776
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001813
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001773
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001772
Iteration 69/1000 | Loss: 0.00001772
Iteration 70/1000 | Loss: 0.00001772
Iteration 71/1000 | Loss: 0.00001772
Iteration 72/1000 | Loss: 0.00001772
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001771
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001770
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001769
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001769
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001769
Iteration 91/1000 | Loss: 0.00001769
Iteration 92/1000 | Loss: 0.00001769
Iteration 93/1000 | Loss: 0.00001769
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001768
Iteration 98/1000 | Loss: 0.00001768
Iteration 99/1000 | Loss: 0.00001768
Iteration 100/1000 | Loss: 0.00001768
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.7683403711998835e-05, 1.7683403711998835e-05, 1.7683403711998835e-05, 1.7683403711998835e-05, 1.7683403711998835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7683403711998835e-05

Optimization complete. Final v2v error: 3.509615421295166 mm

Highest mean error: 11.970246315002441 mm for frame 187

Lowest mean error: 3.1322035789489746 mm for frame 236

Saving results

Total time: 102.77609968185425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084955
Iteration 2/25 | Loss: 0.00289368
Iteration 3/25 | Loss: 0.00229466
Iteration 4/25 | Loss: 0.00194600
Iteration 5/25 | Loss: 0.00189846
Iteration 6/25 | Loss: 0.00194494
Iteration 7/25 | Loss: 0.00190984
Iteration 8/25 | Loss: 0.00181581
Iteration 9/25 | Loss: 0.00174435
Iteration 10/25 | Loss: 0.00162485
Iteration 11/25 | Loss: 0.00155646
Iteration 12/25 | Loss: 0.00151781
Iteration 13/25 | Loss: 0.00148821
Iteration 14/25 | Loss: 0.00141870
Iteration 15/25 | Loss: 0.00142998
Iteration 16/25 | Loss: 0.00142779
Iteration 17/25 | Loss: 0.00137554
Iteration 18/25 | Loss: 0.00137071
Iteration 19/25 | Loss: 0.00136810
Iteration 20/25 | Loss: 0.00135777
Iteration 21/25 | Loss: 0.00135148
Iteration 22/25 | Loss: 0.00135545
Iteration 23/25 | Loss: 0.00135223
Iteration 24/25 | Loss: 0.00135469
Iteration 25/25 | Loss: 0.00135795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49012125
Iteration 2/25 | Loss: 0.00469942
Iteration 3/25 | Loss: 0.00294599
Iteration 4/25 | Loss: 0.00294598
Iteration 5/25 | Loss: 0.00294598
Iteration 6/25 | Loss: 0.00294598
Iteration 7/25 | Loss: 0.00294598
Iteration 8/25 | Loss: 0.00294598
Iteration 9/25 | Loss: 0.00294598
Iteration 10/25 | Loss: 0.00294598
Iteration 11/25 | Loss: 0.00294598
Iteration 12/25 | Loss: 0.00294598
Iteration 13/25 | Loss: 0.00294598
Iteration 14/25 | Loss: 0.00294598
Iteration 15/25 | Loss: 0.00294598
Iteration 16/25 | Loss: 0.00294598
Iteration 17/25 | Loss: 0.00294598
Iteration 18/25 | Loss: 0.00294598
Iteration 19/25 | Loss: 0.00294598
Iteration 20/25 | Loss: 0.00294598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002945981454104185, 0.002945981454104185, 0.002945981454104185, 0.002945981454104185, 0.002945981454104185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002945981454104185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294598
Iteration 2/1000 | Loss: 0.00608816
Iteration 3/1000 | Loss: 0.00061609
Iteration 4/1000 | Loss: 0.00254756
Iteration 5/1000 | Loss: 0.00459890
Iteration 6/1000 | Loss: 0.00268297
Iteration 7/1000 | Loss: 0.00390602
Iteration 8/1000 | Loss: 0.00041140
Iteration 9/1000 | Loss: 0.00377410
Iteration 10/1000 | Loss: 0.00303667
Iteration 11/1000 | Loss: 0.00147719
Iteration 12/1000 | Loss: 0.00026479
Iteration 13/1000 | Loss: 0.00071281
Iteration 14/1000 | Loss: 0.00112568
Iteration 15/1000 | Loss: 0.00030953
Iteration 16/1000 | Loss: 0.00018222
Iteration 17/1000 | Loss: 0.00028714
Iteration 18/1000 | Loss: 0.00111955
Iteration 19/1000 | Loss: 0.00016370
Iteration 20/1000 | Loss: 0.00013630
Iteration 21/1000 | Loss: 0.00369086
Iteration 22/1000 | Loss: 0.00075095
Iteration 23/1000 | Loss: 0.00013170
Iteration 24/1000 | Loss: 0.00673517
Iteration 25/1000 | Loss: 0.00140264
Iteration 26/1000 | Loss: 0.00038360
Iteration 27/1000 | Loss: 0.00516328
Iteration 28/1000 | Loss: 0.00113751
Iteration 29/1000 | Loss: 0.00103768
Iteration 30/1000 | Loss: 0.00020803
Iteration 31/1000 | Loss: 0.00134248
Iteration 32/1000 | Loss: 0.00069342
Iteration 33/1000 | Loss: 0.00069310
Iteration 34/1000 | Loss: 0.00519107
Iteration 35/1000 | Loss: 0.00160123
Iteration 36/1000 | Loss: 0.00096134
Iteration 37/1000 | Loss: 0.00048865
Iteration 38/1000 | Loss: 0.00100908
Iteration 39/1000 | Loss: 0.00061796
Iteration 40/1000 | Loss: 0.00066448
Iteration 41/1000 | Loss: 0.00348020
Iteration 42/1000 | Loss: 0.00108928
Iteration 43/1000 | Loss: 0.00297080
Iteration 44/1000 | Loss: 0.00048938
Iteration 45/1000 | Loss: 0.00033724
Iteration 46/1000 | Loss: 0.00090211
Iteration 47/1000 | Loss: 0.00257695
Iteration 48/1000 | Loss: 0.00365614
Iteration 49/1000 | Loss: 0.00319526
Iteration 50/1000 | Loss: 0.00047815
Iteration 51/1000 | Loss: 0.00358379
Iteration 52/1000 | Loss: 0.00275598
Iteration 53/1000 | Loss: 0.00142359
Iteration 54/1000 | Loss: 0.00266103
Iteration 55/1000 | Loss: 0.00250977
Iteration 56/1000 | Loss: 0.00065373
Iteration 57/1000 | Loss: 0.00208900
Iteration 58/1000 | Loss: 0.00118024
Iteration 59/1000 | Loss: 0.00115856
Iteration 60/1000 | Loss: 0.00152882
Iteration 61/1000 | Loss: 0.00178507
Iteration 62/1000 | Loss: 0.00227053
Iteration 63/1000 | Loss: 0.00199519
Iteration 64/1000 | Loss: 0.00171271
Iteration 65/1000 | Loss: 0.00087683
Iteration 66/1000 | Loss: 0.00101675
Iteration 67/1000 | Loss: 0.00459743
Iteration 68/1000 | Loss: 0.00224596
Iteration 69/1000 | Loss: 0.00015929
Iteration 70/1000 | Loss: 0.00042311
Iteration 71/1000 | Loss: 0.00076527
Iteration 72/1000 | Loss: 0.00027434
Iteration 73/1000 | Loss: 0.00009535
Iteration 74/1000 | Loss: 0.00008388
Iteration 75/1000 | Loss: 0.00065113
Iteration 76/1000 | Loss: 0.00408441
Iteration 77/1000 | Loss: 0.00084177
Iteration 78/1000 | Loss: 0.00014948
Iteration 79/1000 | Loss: 0.00007551
Iteration 80/1000 | Loss: 0.00006505
Iteration 81/1000 | Loss: 0.00085285
Iteration 82/1000 | Loss: 0.00125210
Iteration 83/1000 | Loss: 0.00296251
Iteration 84/1000 | Loss: 0.00282044
Iteration 85/1000 | Loss: 0.00285402
Iteration 86/1000 | Loss: 0.00132340
Iteration 87/1000 | Loss: 0.00496421
Iteration 88/1000 | Loss: 0.00348648
Iteration 89/1000 | Loss: 0.00430085
Iteration 90/1000 | Loss: 0.00316958
Iteration 91/1000 | Loss: 0.00112586
Iteration 92/1000 | Loss: 0.00150740
Iteration 93/1000 | Loss: 0.00317268
Iteration 94/1000 | Loss: 0.00138187
Iteration 95/1000 | Loss: 0.00123546
Iteration 96/1000 | Loss: 0.00154282
Iteration 97/1000 | Loss: 0.00057235
Iteration 98/1000 | Loss: 0.00077831
Iteration 99/1000 | Loss: 0.00065695
Iteration 100/1000 | Loss: 0.00523639
Iteration 101/1000 | Loss: 0.00092995
Iteration 102/1000 | Loss: 0.00059766
Iteration 103/1000 | Loss: 0.00018507
Iteration 104/1000 | Loss: 0.00134312
Iteration 105/1000 | Loss: 0.00084146
Iteration 106/1000 | Loss: 0.00176074
Iteration 107/1000 | Loss: 0.00121476
Iteration 108/1000 | Loss: 0.00235353
Iteration 109/1000 | Loss: 0.00174558
Iteration 110/1000 | Loss: 0.00172645
Iteration 111/1000 | Loss: 0.00231109
Iteration 112/1000 | Loss: 0.00144286
Iteration 113/1000 | Loss: 0.00177151
Iteration 114/1000 | Loss: 0.00247619
Iteration 115/1000 | Loss: 0.00137474
Iteration 116/1000 | Loss: 0.00201440
Iteration 117/1000 | Loss: 0.00170501
Iteration 118/1000 | Loss: 0.00155423
Iteration 119/1000 | Loss: 0.00214162
Iteration 120/1000 | Loss: 0.00187345
Iteration 121/1000 | Loss: 0.00359533
Iteration 122/1000 | Loss: 0.00076994
Iteration 123/1000 | Loss: 0.00418567
Iteration 124/1000 | Loss: 0.00181635
Iteration 125/1000 | Loss: 0.00115324
Iteration 126/1000 | Loss: 0.00460178
Iteration 127/1000 | Loss: 0.00118099
Iteration 128/1000 | Loss: 0.00236686
Iteration 129/1000 | Loss: 0.00308057
Iteration 130/1000 | Loss: 0.00231073
Iteration 131/1000 | Loss: 0.00254490
Iteration 132/1000 | Loss: 0.00278736
Iteration 133/1000 | Loss: 0.00230879
Iteration 134/1000 | Loss: 0.00201830
Iteration 135/1000 | Loss: 0.00258119
Iteration 136/1000 | Loss: 0.00233079
Iteration 137/1000 | Loss: 0.00312628
Iteration 138/1000 | Loss: 0.00115227
Iteration 139/1000 | Loss: 0.00323229
Iteration 140/1000 | Loss: 0.00171395
Iteration 141/1000 | Loss: 0.00126282
Iteration 142/1000 | Loss: 0.00175918
Iteration 143/1000 | Loss: 0.00120997
Iteration 144/1000 | Loss: 0.00176116
Iteration 145/1000 | Loss: 0.00505764
Iteration 146/1000 | Loss: 0.00320252
Iteration 147/1000 | Loss: 0.00089161
Iteration 148/1000 | Loss: 0.00165430
Iteration 149/1000 | Loss: 0.00103763
Iteration 150/1000 | Loss: 0.00248951
Iteration 151/1000 | Loss: 0.00155982
Iteration 152/1000 | Loss: 0.00223997
Iteration 153/1000 | Loss: 0.00256087
Iteration 154/1000 | Loss: 0.00325878
Iteration 155/1000 | Loss: 0.00316697
Iteration 156/1000 | Loss: 0.00224168
Iteration 157/1000 | Loss: 0.00256448
Iteration 158/1000 | Loss: 0.00165015
Iteration 159/1000 | Loss: 0.00186370
Iteration 160/1000 | Loss: 0.00097455
Iteration 161/1000 | Loss: 0.00059234
Iteration 162/1000 | Loss: 0.00094683
Iteration 163/1000 | Loss: 0.00106977
Iteration 164/1000 | Loss: 0.00086409
Iteration 165/1000 | Loss: 0.00137667
Iteration 166/1000 | Loss: 0.00116204
Iteration 167/1000 | Loss: 0.00116669
Iteration 168/1000 | Loss: 0.00155439
Iteration 169/1000 | Loss: 0.00146616
Iteration 170/1000 | Loss: 0.00205246
Iteration 171/1000 | Loss: 0.00149061
Iteration 172/1000 | Loss: 0.00154826
Iteration 173/1000 | Loss: 0.00290832
Iteration 174/1000 | Loss: 0.00059092
Iteration 175/1000 | Loss: 0.00028112
Iteration 176/1000 | Loss: 0.00162416
Iteration 177/1000 | Loss: 0.00138528
Iteration 178/1000 | Loss: 0.00160150
Iteration 179/1000 | Loss: 0.00206830
Iteration 180/1000 | Loss: 0.00320857
Iteration 181/1000 | Loss: 0.00167333
Iteration 182/1000 | Loss: 0.00182581
Iteration 183/1000 | Loss: 0.00133401
Iteration 184/1000 | Loss: 0.00145000
Iteration 185/1000 | Loss: 0.00114304
Iteration 186/1000 | Loss: 0.00143530
Iteration 187/1000 | Loss: 0.00070410
Iteration 188/1000 | Loss: 0.00178303
Iteration 189/1000 | Loss: 0.00118296
Iteration 190/1000 | Loss: 0.00046264
Iteration 191/1000 | Loss: 0.00076808
Iteration 192/1000 | Loss: 0.00120684
Iteration 193/1000 | Loss: 0.00169091
Iteration 194/1000 | Loss: 0.00102575
Iteration 195/1000 | Loss: 0.00384896
Iteration 196/1000 | Loss: 0.00078976
Iteration 197/1000 | Loss: 0.00157066
Iteration 198/1000 | Loss: 0.00120845
Iteration 199/1000 | Loss: 0.00164019
Iteration 200/1000 | Loss: 0.00089331
Iteration 201/1000 | Loss: 0.00006457
Iteration 202/1000 | Loss: 0.00005213
Iteration 203/1000 | Loss: 0.00310739
Iteration 204/1000 | Loss: 0.00155312
Iteration 205/1000 | Loss: 0.00068498
Iteration 206/1000 | Loss: 0.00178336
Iteration 207/1000 | Loss: 0.00176442
Iteration 208/1000 | Loss: 0.00141566
Iteration 209/1000 | Loss: 0.00259539
Iteration 210/1000 | Loss: 0.00308291
Iteration 211/1000 | Loss: 0.00159484
Iteration 212/1000 | Loss: 0.00136250
Iteration 213/1000 | Loss: 0.00519078
Iteration 214/1000 | Loss: 0.00692213
Iteration 215/1000 | Loss: 0.00022640
Iteration 216/1000 | Loss: 0.00022669
Iteration 217/1000 | Loss: 0.00015851
Iteration 218/1000 | Loss: 0.00026715
Iteration 219/1000 | Loss: 0.00004449
Iteration 220/1000 | Loss: 0.00008213
Iteration 221/1000 | Loss: 0.00003353
Iteration 222/1000 | Loss: 0.00003177
Iteration 223/1000 | Loss: 0.00040500
Iteration 224/1000 | Loss: 0.00017262
Iteration 225/1000 | Loss: 0.00003008
Iteration 226/1000 | Loss: 0.00002846
Iteration 227/1000 | Loss: 0.00011860
Iteration 228/1000 | Loss: 0.00025616
Iteration 229/1000 | Loss: 0.00002745
Iteration 230/1000 | Loss: 0.00018877
Iteration 231/1000 | Loss: 0.00002901
Iteration 232/1000 | Loss: 0.00002685
Iteration 233/1000 | Loss: 0.00002651
Iteration 234/1000 | Loss: 0.00002629
Iteration 235/1000 | Loss: 0.00014768
Iteration 236/1000 | Loss: 0.00002641
Iteration 237/1000 | Loss: 0.00002603
Iteration 238/1000 | Loss: 0.00002601
Iteration 239/1000 | Loss: 0.00002596
Iteration 240/1000 | Loss: 0.00002596
Iteration 241/1000 | Loss: 0.00002596
Iteration 242/1000 | Loss: 0.00002596
Iteration 243/1000 | Loss: 0.00002593
Iteration 244/1000 | Loss: 0.00002591
Iteration 245/1000 | Loss: 0.00002591
Iteration 246/1000 | Loss: 0.00002590
Iteration 247/1000 | Loss: 0.00002589
Iteration 248/1000 | Loss: 0.00002589
Iteration 249/1000 | Loss: 0.00002587
Iteration 250/1000 | Loss: 0.00002583
Iteration 251/1000 | Loss: 0.00002583
Iteration 252/1000 | Loss: 0.00002582
Iteration 253/1000 | Loss: 0.00002582
Iteration 254/1000 | Loss: 0.00002579
Iteration 255/1000 | Loss: 0.00015535
Iteration 256/1000 | Loss: 0.00003205
Iteration 257/1000 | Loss: 0.00010188
Iteration 258/1000 | Loss: 0.00002583
Iteration 259/1000 | Loss: 0.00002571
Iteration 260/1000 | Loss: 0.00002571
Iteration 261/1000 | Loss: 0.00002571
Iteration 262/1000 | Loss: 0.00002570
Iteration 263/1000 | Loss: 0.00002570
Iteration 264/1000 | Loss: 0.00002570
Iteration 265/1000 | Loss: 0.00002569
Iteration 266/1000 | Loss: 0.00002569
Iteration 267/1000 | Loss: 0.00002569
Iteration 268/1000 | Loss: 0.00002569
Iteration 269/1000 | Loss: 0.00002568
Iteration 270/1000 | Loss: 0.00002568
Iteration 271/1000 | Loss: 0.00002568
Iteration 272/1000 | Loss: 0.00002568
Iteration 273/1000 | Loss: 0.00002568
Iteration 274/1000 | Loss: 0.00002568
Iteration 275/1000 | Loss: 0.00002568
Iteration 276/1000 | Loss: 0.00002568
Iteration 277/1000 | Loss: 0.00002568
Iteration 278/1000 | Loss: 0.00002568
Iteration 279/1000 | Loss: 0.00002567
Iteration 280/1000 | Loss: 0.00002567
Iteration 281/1000 | Loss: 0.00002567
Iteration 282/1000 | Loss: 0.00002567
Iteration 283/1000 | Loss: 0.00002567
Iteration 284/1000 | Loss: 0.00002567
Iteration 285/1000 | Loss: 0.00002567
Iteration 286/1000 | Loss: 0.00002567
Iteration 287/1000 | Loss: 0.00002567
Iteration 288/1000 | Loss: 0.00002567
Iteration 289/1000 | Loss: 0.00002567
Iteration 290/1000 | Loss: 0.00002567
Iteration 291/1000 | Loss: 0.00002567
Iteration 292/1000 | Loss: 0.00002566
Iteration 293/1000 | Loss: 0.00002566
Iteration 294/1000 | Loss: 0.00002566
Iteration 295/1000 | Loss: 0.00002566
Iteration 296/1000 | Loss: 0.00002566
Iteration 297/1000 | Loss: 0.00002566
Iteration 298/1000 | Loss: 0.00002566
Iteration 299/1000 | Loss: 0.00002566
Iteration 300/1000 | Loss: 0.00002566
Iteration 301/1000 | Loss: 0.00002566
Iteration 302/1000 | Loss: 0.00002566
Iteration 303/1000 | Loss: 0.00002566
Iteration 304/1000 | Loss: 0.00002566
Iteration 305/1000 | Loss: 0.00002566
Iteration 306/1000 | Loss: 0.00002566
Iteration 307/1000 | Loss: 0.00002566
Iteration 308/1000 | Loss: 0.00002566
Iteration 309/1000 | Loss: 0.00002566
Iteration 310/1000 | Loss: 0.00002566
Iteration 311/1000 | Loss: 0.00002566
Iteration 312/1000 | Loss: 0.00002566
Iteration 313/1000 | Loss: 0.00002566
Iteration 314/1000 | Loss: 0.00002566
Iteration 315/1000 | Loss: 0.00002566
Iteration 316/1000 | Loss: 0.00002566
Iteration 317/1000 | Loss: 0.00002566
Iteration 318/1000 | Loss: 0.00002566
Iteration 319/1000 | Loss: 0.00002566
Iteration 320/1000 | Loss: 0.00002566
Iteration 321/1000 | Loss: 0.00002566
Iteration 322/1000 | Loss: 0.00002566
Iteration 323/1000 | Loss: 0.00002566
Iteration 324/1000 | Loss: 0.00002566
Iteration 325/1000 | Loss: 0.00002566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [2.5660388928372413e-05, 2.5660388928372413e-05, 2.5660388928372413e-05, 2.5660388928372413e-05, 2.5660388928372413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5660388928372413e-05

Optimization complete. Final v2v error: 3.3833630084991455 mm

Highest mean error: 13.038074493408203 mm for frame 68

Lowest mean error: 2.697796106338501 mm for frame 9

Saving results

Total time: 387.1827964782715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421847
Iteration 2/25 | Loss: 0.00121673
Iteration 3/25 | Loss: 0.00113682
Iteration 4/25 | Loss: 0.00112527
Iteration 5/25 | Loss: 0.00112075
Iteration 6/25 | Loss: 0.00111958
Iteration 7/25 | Loss: 0.00111952
Iteration 8/25 | Loss: 0.00111952
Iteration 9/25 | Loss: 0.00111952
Iteration 10/25 | Loss: 0.00111952
Iteration 11/25 | Loss: 0.00111952
Iteration 12/25 | Loss: 0.00111952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001119521097280085, 0.001119521097280085, 0.001119521097280085, 0.001119521097280085, 0.001119521097280085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001119521097280085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98143339
Iteration 2/25 | Loss: 0.00051144
Iteration 3/25 | Loss: 0.00051144
Iteration 4/25 | Loss: 0.00051143
Iteration 5/25 | Loss: 0.00051143
Iteration 6/25 | Loss: 0.00051143
Iteration 7/25 | Loss: 0.00051143
Iteration 8/25 | Loss: 0.00051143
Iteration 9/25 | Loss: 0.00051143
Iteration 10/25 | Loss: 0.00051143
Iteration 11/25 | Loss: 0.00051143
Iteration 12/25 | Loss: 0.00051143
Iteration 13/25 | Loss: 0.00051143
Iteration 14/25 | Loss: 0.00051143
Iteration 15/25 | Loss: 0.00051143
Iteration 16/25 | Loss: 0.00051143
Iteration 17/25 | Loss: 0.00051143
Iteration 18/25 | Loss: 0.00051143
Iteration 19/25 | Loss: 0.00051143
Iteration 20/25 | Loss: 0.00051143
Iteration 21/25 | Loss: 0.00051143
Iteration 22/25 | Loss: 0.00051143
Iteration 23/25 | Loss: 0.00051143
Iteration 24/25 | Loss: 0.00051143
Iteration 25/25 | Loss: 0.00051143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051143
Iteration 2/1000 | Loss: 0.00003513
Iteration 3/1000 | Loss: 0.00002208
Iteration 4/1000 | Loss: 0.00001818
Iteration 5/1000 | Loss: 0.00001731
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001609
Iteration 9/1000 | Loss: 0.00001608
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001587
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001560
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001560
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001556
Iteration 30/1000 | Loss: 0.00001556
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001555
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001550
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001550
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.5445406461367384e-05, 1.5445406461367384e-05, 1.5445406461367384e-05, 1.5445406461367384e-05, 1.5445406461367384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5445406461367384e-05

Optimization complete. Final v2v error: 3.3873088359832764 mm

Highest mean error: 3.9753568172454834 mm for frame 74

Lowest mean error: 2.9331183433532715 mm for frame 100

Saving results

Total time: 29.924490213394165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903050
Iteration 2/25 | Loss: 0.00138676
Iteration 3/25 | Loss: 0.00118635
Iteration 4/25 | Loss: 0.00116680
Iteration 5/25 | Loss: 0.00116359
Iteration 6/25 | Loss: 0.00116336
Iteration 7/25 | Loss: 0.00116336
Iteration 8/25 | Loss: 0.00116336
Iteration 9/25 | Loss: 0.00116336
Iteration 10/25 | Loss: 0.00116336
Iteration 11/25 | Loss: 0.00116336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011633617104962468, 0.0011633617104962468, 0.0011633617104962468, 0.0011633617104962468, 0.0011633617104962468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011633617104962468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44481063
Iteration 2/25 | Loss: 0.00062223
Iteration 3/25 | Loss: 0.00062219
Iteration 4/25 | Loss: 0.00062219
Iteration 5/25 | Loss: 0.00062219
Iteration 6/25 | Loss: 0.00062219
Iteration 7/25 | Loss: 0.00062219
Iteration 8/25 | Loss: 0.00062218
Iteration 9/25 | Loss: 0.00062218
Iteration 10/25 | Loss: 0.00062218
Iteration 11/25 | Loss: 0.00062218
Iteration 12/25 | Loss: 0.00062218
Iteration 13/25 | Loss: 0.00062218
Iteration 14/25 | Loss: 0.00062218
Iteration 15/25 | Loss: 0.00062218
Iteration 16/25 | Loss: 0.00062218
Iteration 17/25 | Loss: 0.00062218
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006221840740181506, 0.0006221840740181506, 0.0006221840740181506, 0.0006221840740181506, 0.0006221840740181506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006221840740181506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062218
Iteration 2/1000 | Loss: 0.00002846
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001744
Iteration 6/1000 | Loss: 0.00001646
Iteration 7/1000 | Loss: 0.00001589
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001484
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001471
Iteration 16/1000 | Loss: 0.00001470
Iteration 17/1000 | Loss: 0.00001470
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001465
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001464
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001462
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001461
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001455
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001455
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001455
Iteration 91/1000 | Loss: 0.00001455
Iteration 92/1000 | Loss: 0.00001455
Iteration 93/1000 | Loss: 0.00001455
Iteration 94/1000 | Loss: 0.00001455
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001454
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.4544565601681825e-05, 1.4544565601681825e-05, 1.4544565601681825e-05, 1.4544565601681825e-05, 1.4544565601681825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4544565601681825e-05

Optimization complete. Final v2v error: 3.2958314418792725 mm

Highest mean error: 4.056246280670166 mm for frame 43

Lowest mean error: 2.8734514713287354 mm for frame 93

Saving results

Total time: 32.50186610221863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542325
Iteration 2/25 | Loss: 0.00146371
Iteration 3/25 | Loss: 0.00121933
Iteration 4/25 | Loss: 0.00120308
Iteration 5/25 | Loss: 0.00119877
Iteration 6/25 | Loss: 0.00119710
Iteration 7/25 | Loss: 0.00119698
Iteration 8/25 | Loss: 0.00119698
Iteration 9/25 | Loss: 0.00119698
Iteration 10/25 | Loss: 0.00119698
Iteration 11/25 | Loss: 0.00119698
Iteration 12/25 | Loss: 0.00119698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001196981524117291, 0.001196981524117291, 0.001196981524117291, 0.001196981524117291, 0.001196981524117291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196981524117291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47240341
Iteration 2/25 | Loss: 0.00069946
Iteration 3/25 | Loss: 0.00069945
Iteration 4/25 | Loss: 0.00069945
Iteration 5/25 | Loss: 0.00069945
Iteration 6/25 | Loss: 0.00069945
Iteration 7/25 | Loss: 0.00069945
Iteration 8/25 | Loss: 0.00069945
Iteration 9/25 | Loss: 0.00069945
Iteration 10/25 | Loss: 0.00069945
Iteration 11/25 | Loss: 0.00069945
Iteration 12/25 | Loss: 0.00069945
Iteration 13/25 | Loss: 0.00069945
Iteration 14/25 | Loss: 0.00069945
Iteration 15/25 | Loss: 0.00069945
Iteration 16/25 | Loss: 0.00069945
Iteration 17/25 | Loss: 0.00069945
Iteration 18/25 | Loss: 0.00069945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006994471768848598, 0.0006994471768848598, 0.0006994471768848598, 0.0006994471768848598, 0.0006994471768848598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006994471768848598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069945
Iteration 2/1000 | Loss: 0.00005882
Iteration 3/1000 | Loss: 0.00004296
Iteration 4/1000 | Loss: 0.00004009
Iteration 5/1000 | Loss: 0.00003864
Iteration 6/1000 | Loss: 0.00003803
Iteration 7/1000 | Loss: 0.00003762
Iteration 8/1000 | Loss: 0.00003725
Iteration 9/1000 | Loss: 0.00003693
Iteration 10/1000 | Loss: 0.00003662
Iteration 11/1000 | Loss: 0.00003644
Iteration 12/1000 | Loss: 0.00003628
Iteration 13/1000 | Loss: 0.00003624
Iteration 14/1000 | Loss: 0.00003615
Iteration 15/1000 | Loss: 0.00003611
Iteration 16/1000 | Loss: 0.00003611
Iteration 17/1000 | Loss: 0.00003610
Iteration 18/1000 | Loss: 0.00003610
Iteration 19/1000 | Loss: 0.00003609
Iteration 20/1000 | Loss: 0.00003609
Iteration 21/1000 | Loss: 0.00003606
Iteration 22/1000 | Loss: 0.00003601
Iteration 23/1000 | Loss: 0.00003601
Iteration 24/1000 | Loss: 0.00003599
Iteration 25/1000 | Loss: 0.00003598
Iteration 26/1000 | Loss: 0.00003598
Iteration 27/1000 | Loss: 0.00003598
Iteration 28/1000 | Loss: 0.00003598
Iteration 29/1000 | Loss: 0.00003598
Iteration 30/1000 | Loss: 0.00003598
Iteration 31/1000 | Loss: 0.00003598
Iteration 32/1000 | Loss: 0.00003597
Iteration 33/1000 | Loss: 0.00003597
Iteration 34/1000 | Loss: 0.00003597
Iteration 35/1000 | Loss: 0.00003597
Iteration 36/1000 | Loss: 0.00003596
Iteration 37/1000 | Loss: 0.00003596
Iteration 38/1000 | Loss: 0.00003596
Iteration 39/1000 | Loss: 0.00003595
Iteration 40/1000 | Loss: 0.00003595
Iteration 41/1000 | Loss: 0.00003595
Iteration 42/1000 | Loss: 0.00003595
Iteration 43/1000 | Loss: 0.00003594
Iteration 44/1000 | Loss: 0.00003594
Iteration 45/1000 | Loss: 0.00003594
Iteration 46/1000 | Loss: 0.00003594
Iteration 47/1000 | Loss: 0.00003593
Iteration 48/1000 | Loss: 0.00003593
Iteration 49/1000 | Loss: 0.00003593
Iteration 50/1000 | Loss: 0.00003592
Iteration 51/1000 | Loss: 0.00003592
Iteration 52/1000 | Loss: 0.00003592
Iteration 53/1000 | Loss: 0.00003591
Iteration 54/1000 | Loss: 0.00003591
Iteration 55/1000 | Loss: 0.00003589
Iteration 56/1000 | Loss: 0.00003589
Iteration 57/1000 | Loss: 0.00003589
Iteration 58/1000 | Loss: 0.00003589
Iteration 59/1000 | Loss: 0.00003589
Iteration 60/1000 | Loss: 0.00003589
Iteration 61/1000 | Loss: 0.00003588
Iteration 62/1000 | Loss: 0.00003588
Iteration 63/1000 | Loss: 0.00003588
Iteration 64/1000 | Loss: 0.00003588
Iteration 65/1000 | Loss: 0.00003588
Iteration 66/1000 | Loss: 0.00003588
Iteration 67/1000 | Loss: 0.00003587
Iteration 68/1000 | Loss: 0.00003587
Iteration 69/1000 | Loss: 0.00003587
Iteration 70/1000 | Loss: 0.00003587
Iteration 71/1000 | Loss: 0.00003587
Iteration 72/1000 | Loss: 0.00003587
Iteration 73/1000 | Loss: 0.00003587
Iteration 74/1000 | Loss: 0.00003587
Iteration 75/1000 | Loss: 0.00003586
Iteration 76/1000 | Loss: 0.00003586
Iteration 77/1000 | Loss: 0.00003586
Iteration 78/1000 | Loss: 0.00003585
Iteration 79/1000 | Loss: 0.00003585
Iteration 80/1000 | Loss: 0.00003585
Iteration 81/1000 | Loss: 0.00003584
Iteration 82/1000 | Loss: 0.00003584
Iteration 83/1000 | Loss: 0.00003584
Iteration 84/1000 | Loss: 0.00003584
Iteration 85/1000 | Loss: 0.00003584
Iteration 86/1000 | Loss: 0.00003584
Iteration 87/1000 | Loss: 0.00003583
Iteration 88/1000 | Loss: 0.00003583
Iteration 89/1000 | Loss: 0.00003583
Iteration 90/1000 | Loss: 0.00003583
Iteration 91/1000 | Loss: 0.00003583
Iteration 92/1000 | Loss: 0.00003582
Iteration 93/1000 | Loss: 0.00003582
Iteration 94/1000 | Loss: 0.00003582
Iteration 95/1000 | Loss: 0.00003582
Iteration 96/1000 | Loss: 0.00003582
Iteration 97/1000 | Loss: 0.00003582
Iteration 98/1000 | Loss: 0.00003582
Iteration 99/1000 | Loss: 0.00003582
Iteration 100/1000 | Loss: 0.00003582
Iteration 101/1000 | Loss: 0.00003582
Iteration 102/1000 | Loss: 0.00003581
Iteration 103/1000 | Loss: 0.00003581
Iteration 104/1000 | Loss: 0.00003581
Iteration 105/1000 | Loss: 0.00003581
Iteration 106/1000 | Loss: 0.00003581
Iteration 107/1000 | Loss: 0.00003581
Iteration 108/1000 | Loss: 0.00003580
Iteration 109/1000 | Loss: 0.00003580
Iteration 110/1000 | Loss: 0.00003580
Iteration 111/1000 | Loss: 0.00003580
Iteration 112/1000 | Loss: 0.00003580
Iteration 113/1000 | Loss: 0.00003580
Iteration 114/1000 | Loss: 0.00003580
Iteration 115/1000 | Loss: 0.00003580
Iteration 116/1000 | Loss: 0.00003580
Iteration 117/1000 | Loss: 0.00003580
Iteration 118/1000 | Loss: 0.00003580
Iteration 119/1000 | Loss: 0.00003580
Iteration 120/1000 | Loss: 0.00003579
Iteration 121/1000 | Loss: 0.00003579
Iteration 122/1000 | Loss: 0.00003579
Iteration 123/1000 | Loss: 0.00003579
Iteration 124/1000 | Loss: 0.00003579
Iteration 125/1000 | Loss: 0.00003579
Iteration 126/1000 | Loss: 0.00003579
Iteration 127/1000 | Loss: 0.00003579
Iteration 128/1000 | Loss: 0.00003579
Iteration 129/1000 | Loss: 0.00003579
Iteration 130/1000 | Loss: 0.00003579
Iteration 131/1000 | Loss: 0.00003579
Iteration 132/1000 | Loss: 0.00003579
Iteration 133/1000 | Loss: 0.00003579
Iteration 134/1000 | Loss: 0.00003579
Iteration 135/1000 | Loss: 0.00003579
Iteration 136/1000 | Loss: 0.00003579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [3.578809628379531e-05, 3.578809628379531e-05, 3.578809628379531e-05, 3.578809628379531e-05, 3.578809628379531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.578809628379531e-05

Optimization complete. Final v2v error: 4.577454090118408 mm

Highest mean error: 5.576915264129639 mm for frame 175

Lowest mean error: 3.092353105545044 mm for frame 60

Saving results

Total time: 39.272990226745605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895329
Iteration 2/25 | Loss: 0.00133051
Iteration 3/25 | Loss: 0.00114416
Iteration 4/25 | Loss: 0.00112520
Iteration 5/25 | Loss: 0.00112182
Iteration 6/25 | Loss: 0.00112052
Iteration 7/25 | Loss: 0.00112052
Iteration 8/25 | Loss: 0.00112052
Iteration 9/25 | Loss: 0.00112052
Iteration 10/25 | Loss: 0.00112052
Iteration 11/25 | Loss: 0.00112052
Iteration 12/25 | Loss: 0.00112052
Iteration 13/25 | Loss: 0.00112052
Iteration 14/25 | Loss: 0.00112052
Iteration 15/25 | Loss: 0.00112052
Iteration 16/25 | Loss: 0.00112052
Iteration 17/25 | Loss: 0.00112052
Iteration 18/25 | Loss: 0.00112052
Iteration 19/25 | Loss: 0.00112052
Iteration 20/25 | Loss: 0.00112052
Iteration 21/25 | Loss: 0.00112052
Iteration 22/25 | Loss: 0.00112052
Iteration 23/25 | Loss: 0.00112052
Iteration 24/25 | Loss: 0.00112052
Iteration 25/25 | Loss: 0.00112052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46847212
Iteration 2/25 | Loss: 0.00056195
Iteration 3/25 | Loss: 0.00056194
Iteration 4/25 | Loss: 0.00056194
Iteration 5/25 | Loss: 0.00056194
Iteration 6/25 | Loss: 0.00056194
Iteration 7/25 | Loss: 0.00056194
Iteration 8/25 | Loss: 0.00056194
Iteration 9/25 | Loss: 0.00056194
Iteration 10/25 | Loss: 0.00056194
Iteration 11/25 | Loss: 0.00056194
Iteration 12/25 | Loss: 0.00056194
Iteration 13/25 | Loss: 0.00056194
Iteration 14/25 | Loss: 0.00056194
Iteration 15/25 | Loss: 0.00056194
Iteration 16/25 | Loss: 0.00056194
Iteration 17/25 | Loss: 0.00056194
Iteration 18/25 | Loss: 0.00056194
Iteration 19/25 | Loss: 0.00056194
Iteration 20/25 | Loss: 0.00056194
Iteration 21/25 | Loss: 0.00056194
Iteration 22/25 | Loss: 0.00056194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005619386211037636, 0.0005619386211037636, 0.0005619386211037636, 0.0005619386211037636, 0.0005619386211037636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005619386211037636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056194
Iteration 2/1000 | Loss: 0.00002859
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001553
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001496
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001466
Iteration 12/1000 | Loss: 0.00001463
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001462
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001457
Iteration 17/1000 | Loss: 0.00001456
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001454
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001443
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001437
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001436
Iteration 69/1000 | Loss: 0.00001436
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001434
Iteration 83/1000 | Loss: 0.00001434
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001434
Iteration 88/1000 | Loss: 0.00001434
Iteration 89/1000 | Loss: 0.00001434
Iteration 90/1000 | Loss: 0.00001434
Iteration 91/1000 | Loss: 0.00001434
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001433
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001433
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001433
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001432
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001431
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001430
Iteration 143/1000 | Loss: 0.00001430
Iteration 144/1000 | Loss: 0.00001430
Iteration 145/1000 | Loss: 0.00001430
Iteration 146/1000 | Loss: 0.00001430
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001429
Iteration 152/1000 | Loss: 0.00001429
Iteration 153/1000 | Loss: 0.00001429
Iteration 154/1000 | Loss: 0.00001429
Iteration 155/1000 | Loss: 0.00001429
Iteration 156/1000 | Loss: 0.00001429
Iteration 157/1000 | Loss: 0.00001429
Iteration 158/1000 | Loss: 0.00001429
Iteration 159/1000 | Loss: 0.00001429
Iteration 160/1000 | Loss: 0.00001429
Iteration 161/1000 | Loss: 0.00001429
Iteration 162/1000 | Loss: 0.00001429
Iteration 163/1000 | Loss: 0.00001429
Iteration 164/1000 | Loss: 0.00001429
Iteration 165/1000 | Loss: 0.00001428
Iteration 166/1000 | Loss: 0.00001428
Iteration 167/1000 | Loss: 0.00001428
Iteration 168/1000 | Loss: 0.00001428
Iteration 169/1000 | Loss: 0.00001428
Iteration 170/1000 | Loss: 0.00001428
Iteration 171/1000 | Loss: 0.00001428
Iteration 172/1000 | Loss: 0.00001428
Iteration 173/1000 | Loss: 0.00001428
Iteration 174/1000 | Loss: 0.00001428
Iteration 175/1000 | Loss: 0.00001428
Iteration 176/1000 | Loss: 0.00001428
Iteration 177/1000 | Loss: 0.00001428
Iteration 178/1000 | Loss: 0.00001428
Iteration 179/1000 | Loss: 0.00001428
Iteration 180/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.4281319636211265e-05, 1.4281319636211265e-05, 1.4281319636211265e-05, 1.4281319636211265e-05, 1.4281319636211265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4281319636211265e-05

Optimization complete. Final v2v error: 3.2430028915405273 mm

Highest mean error: 3.8743393421173096 mm for frame 65

Lowest mean error: 2.8263561725616455 mm for frame 151

Saving results

Total time: 35.5108757019043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107842
Iteration 2/25 | Loss: 0.00200224
Iteration 3/25 | Loss: 0.00152280
Iteration 4/25 | Loss: 0.00144451
Iteration 5/25 | Loss: 0.00148205
Iteration 6/25 | Loss: 0.00147262
Iteration 7/25 | Loss: 0.00142795
Iteration 8/25 | Loss: 0.00135253
Iteration 9/25 | Loss: 0.00133142
Iteration 10/25 | Loss: 0.00132187
Iteration 11/25 | Loss: 0.00130860
Iteration 12/25 | Loss: 0.00130300
Iteration 13/25 | Loss: 0.00130559
Iteration 14/25 | Loss: 0.00130551
Iteration 15/25 | Loss: 0.00129936
Iteration 16/25 | Loss: 0.00129903
Iteration 17/25 | Loss: 0.00129883
Iteration 18/25 | Loss: 0.00129864
Iteration 19/25 | Loss: 0.00129834
Iteration 20/25 | Loss: 0.00129814
Iteration 21/25 | Loss: 0.00129808
Iteration 22/25 | Loss: 0.00129807
Iteration 23/25 | Loss: 0.00129807
Iteration 24/25 | Loss: 0.00129807
Iteration 25/25 | Loss: 0.00129807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91191196
Iteration 2/25 | Loss: 0.00124269
Iteration 3/25 | Loss: 0.00113963
Iteration 4/25 | Loss: 0.00113963
Iteration 5/25 | Loss: 0.00113963
Iteration 6/25 | Loss: 0.00113963
Iteration 7/25 | Loss: 0.00113963
Iteration 8/25 | Loss: 0.00113963
Iteration 9/25 | Loss: 0.00113963
Iteration 10/25 | Loss: 0.00113963
Iteration 11/25 | Loss: 0.00113963
Iteration 12/25 | Loss: 0.00113963
Iteration 13/25 | Loss: 0.00113963
Iteration 14/25 | Loss: 0.00113963
Iteration 15/25 | Loss: 0.00113963
Iteration 16/25 | Loss: 0.00113963
Iteration 17/25 | Loss: 0.00113963
Iteration 18/25 | Loss: 0.00113963
Iteration 19/25 | Loss: 0.00113963
Iteration 20/25 | Loss: 0.00113963
Iteration 21/25 | Loss: 0.00113963
Iteration 22/25 | Loss: 0.00113963
Iteration 23/25 | Loss: 0.00113963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011396335903555155, 0.0011396335903555155, 0.0011396335903555155, 0.0011396335903555155, 0.0011396335903555155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011396335903555155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113963
Iteration 2/1000 | Loss: 0.00013192
Iteration 3/1000 | Loss: 0.00007523
Iteration 4/1000 | Loss: 0.00006548
Iteration 5/1000 | Loss: 0.00062723
Iteration 6/1000 | Loss: 0.00300534
Iteration 7/1000 | Loss: 0.00275939
Iteration 8/1000 | Loss: 0.00034707
Iteration 9/1000 | Loss: 0.00026755
Iteration 10/1000 | Loss: 0.00053744
Iteration 11/1000 | Loss: 0.00232057
Iteration 12/1000 | Loss: 0.00135273
Iteration 13/1000 | Loss: 0.00218273
Iteration 14/1000 | Loss: 0.00102104
Iteration 15/1000 | Loss: 0.00020004
Iteration 16/1000 | Loss: 0.00041047
Iteration 17/1000 | Loss: 0.00008348
Iteration 18/1000 | Loss: 0.00012141
Iteration 19/1000 | Loss: 0.00006725
Iteration 20/1000 | Loss: 0.00004456
Iteration 21/1000 | Loss: 0.00041673
Iteration 22/1000 | Loss: 0.00042336
Iteration 23/1000 | Loss: 0.00006546
Iteration 24/1000 | Loss: 0.00004824
Iteration 25/1000 | Loss: 0.00009771
Iteration 26/1000 | Loss: 0.00024805
Iteration 27/1000 | Loss: 0.00029864
Iteration 28/1000 | Loss: 0.00031026
Iteration 29/1000 | Loss: 0.00004098
Iteration 30/1000 | Loss: 0.00020852
Iteration 31/1000 | Loss: 0.00043882
Iteration 32/1000 | Loss: 0.00055895
Iteration 33/1000 | Loss: 0.00061962
Iteration 34/1000 | Loss: 0.00029406
Iteration 35/1000 | Loss: 0.00038753
Iteration 36/1000 | Loss: 0.00053318
Iteration 37/1000 | Loss: 0.00021407
Iteration 38/1000 | Loss: 0.00018571
Iteration 39/1000 | Loss: 0.00016591
Iteration 40/1000 | Loss: 0.00014805
Iteration 41/1000 | Loss: 0.00028927
Iteration 42/1000 | Loss: 0.00018957
Iteration 43/1000 | Loss: 0.00116918
Iteration 44/1000 | Loss: 0.00074710
Iteration 45/1000 | Loss: 0.00026766
Iteration 46/1000 | Loss: 0.00005643
Iteration 47/1000 | Loss: 0.00008779
Iteration 48/1000 | Loss: 0.00019276
Iteration 49/1000 | Loss: 0.00056802
Iteration 50/1000 | Loss: 0.00024623
Iteration 51/1000 | Loss: 0.00010789
Iteration 52/1000 | Loss: 0.00004070
Iteration 53/1000 | Loss: 0.00016278
Iteration 54/1000 | Loss: 0.00027774
Iteration 55/1000 | Loss: 0.00022453
Iteration 56/1000 | Loss: 0.00025784
Iteration 57/1000 | Loss: 0.00027007
Iteration 58/1000 | Loss: 0.00087562
Iteration 59/1000 | Loss: 0.00052758
Iteration 60/1000 | Loss: 0.00006939
Iteration 61/1000 | Loss: 0.00010966
Iteration 62/1000 | Loss: 0.00004038
Iteration 63/1000 | Loss: 0.00026271
Iteration 64/1000 | Loss: 0.00003737
Iteration 65/1000 | Loss: 0.00003700
Iteration 66/1000 | Loss: 0.00003668
Iteration 67/1000 | Loss: 0.00003640
Iteration 68/1000 | Loss: 0.00003617
Iteration 69/1000 | Loss: 0.00003601
Iteration 70/1000 | Loss: 0.00003595
Iteration 71/1000 | Loss: 0.00003594
Iteration 72/1000 | Loss: 0.00003594
Iteration 73/1000 | Loss: 0.00003594
Iteration 74/1000 | Loss: 0.00003593
Iteration 75/1000 | Loss: 0.00003593
Iteration 76/1000 | Loss: 0.00003592
Iteration 77/1000 | Loss: 0.00003592
Iteration 78/1000 | Loss: 0.00003591
Iteration 79/1000 | Loss: 0.00003588
Iteration 80/1000 | Loss: 0.00003588
Iteration 81/1000 | Loss: 0.00003584
Iteration 82/1000 | Loss: 0.00003584
Iteration 83/1000 | Loss: 0.00003581
Iteration 84/1000 | Loss: 0.00003580
Iteration 85/1000 | Loss: 0.00003580
Iteration 86/1000 | Loss: 0.00003578
Iteration 87/1000 | Loss: 0.00003577
Iteration 88/1000 | Loss: 0.00003576
Iteration 89/1000 | Loss: 0.00003575
Iteration 90/1000 | Loss: 0.00003575
Iteration 91/1000 | Loss: 0.00003575
Iteration 92/1000 | Loss: 0.00003574
Iteration 93/1000 | Loss: 0.00003574
Iteration 94/1000 | Loss: 0.00014538
Iteration 95/1000 | Loss: 0.00018790
Iteration 96/1000 | Loss: 0.00005552
Iteration 97/1000 | Loss: 0.00003551
Iteration 98/1000 | Loss: 0.00005400
Iteration 99/1000 | Loss: 0.00013687
Iteration 100/1000 | Loss: 0.00003379
Iteration 101/1000 | Loss: 0.00004749
Iteration 102/1000 | Loss: 0.00003239
Iteration 103/1000 | Loss: 0.00003220
Iteration 104/1000 | Loss: 0.00003203
Iteration 105/1000 | Loss: 0.00003201
Iteration 106/1000 | Loss: 0.00008440
Iteration 107/1000 | Loss: 0.00003200
Iteration 108/1000 | Loss: 0.00003194
Iteration 109/1000 | Loss: 0.00003194
Iteration 110/1000 | Loss: 0.00003194
Iteration 111/1000 | Loss: 0.00003194
Iteration 112/1000 | Loss: 0.00003194
Iteration 113/1000 | Loss: 0.00003193
Iteration 114/1000 | Loss: 0.00003193
Iteration 115/1000 | Loss: 0.00003193
Iteration 116/1000 | Loss: 0.00003193
Iteration 117/1000 | Loss: 0.00003193
Iteration 118/1000 | Loss: 0.00003193
Iteration 119/1000 | Loss: 0.00003193
Iteration 120/1000 | Loss: 0.00003193
Iteration 121/1000 | Loss: 0.00003193
Iteration 122/1000 | Loss: 0.00003193
Iteration 123/1000 | Loss: 0.00003192
Iteration 124/1000 | Loss: 0.00003192
Iteration 125/1000 | Loss: 0.00003192
Iteration 126/1000 | Loss: 0.00003192
Iteration 127/1000 | Loss: 0.00005354
Iteration 128/1000 | Loss: 0.00004182
Iteration 129/1000 | Loss: 0.00004094
Iteration 130/1000 | Loss: 0.00003192
Iteration 131/1000 | Loss: 0.00003192
Iteration 132/1000 | Loss: 0.00003192
Iteration 133/1000 | Loss: 0.00003192
Iteration 134/1000 | Loss: 0.00003192
Iteration 135/1000 | Loss: 0.00003191
Iteration 136/1000 | Loss: 0.00003191
Iteration 137/1000 | Loss: 0.00003191
Iteration 138/1000 | Loss: 0.00003191
Iteration 139/1000 | Loss: 0.00003191
Iteration 140/1000 | Loss: 0.00003191
Iteration 141/1000 | Loss: 0.00003191
Iteration 142/1000 | Loss: 0.00003191
Iteration 143/1000 | Loss: 0.00003191
Iteration 144/1000 | Loss: 0.00003191
Iteration 145/1000 | Loss: 0.00003191
Iteration 146/1000 | Loss: 0.00003191
Iteration 147/1000 | Loss: 0.00003191
Iteration 148/1000 | Loss: 0.00003191
Iteration 149/1000 | Loss: 0.00003191
Iteration 150/1000 | Loss: 0.00003191
Iteration 151/1000 | Loss: 0.00003191
Iteration 152/1000 | Loss: 0.00003191
Iteration 153/1000 | Loss: 0.00003191
Iteration 154/1000 | Loss: 0.00003190
Iteration 155/1000 | Loss: 0.00003190
Iteration 156/1000 | Loss: 0.00003190
Iteration 157/1000 | Loss: 0.00003190
Iteration 158/1000 | Loss: 0.00003190
Iteration 159/1000 | Loss: 0.00003189
Iteration 160/1000 | Loss: 0.00003189
Iteration 161/1000 | Loss: 0.00003189
Iteration 162/1000 | Loss: 0.00003189
Iteration 163/1000 | Loss: 0.00003189
Iteration 164/1000 | Loss: 0.00003189
Iteration 165/1000 | Loss: 0.00003189
Iteration 166/1000 | Loss: 0.00003188
Iteration 167/1000 | Loss: 0.00003188
Iteration 168/1000 | Loss: 0.00003188
Iteration 169/1000 | Loss: 0.00003188
Iteration 170/1000 | Loss: 0.00003188
Iteration 171/1000 | Loss: 0.00003188
Iteration 172/1000 | Loss: 0.00003188
Iteration 173/1000 | Loss: 0.00003188
Iteration 174/1000 | Loss: 0.00003187
Iteration 175/1000 | Loss: 0.00003187
Iteration 176/1000 | Loss: 0.00003187
Iteration 177/1000 | Loss: 0.00003186
Iteration 178/1000 | Loss: 0.00003186
Iteration 179/1000 | Loss: 0.00003185
Iteration 180/1000 | Loss: 0.00003185
Iteration 181/1000 | Loss: 0.00003185
Iteration 182/1000 | Loss: 0.00003185
Iteration 183/1000 | Loss: 0.00003185
Iteration 184/1000 | Loss: 0.00003184
Iteration 185/1000 | Loss: 0.00003184
Iteration 186/1000 | Loss: 0.00003184
Iteration 187/1000 | Loss: 0.00003184
Iteration 188/1000 | Loss: 0.00003183
Iteration 189/1000 | Loss: 0.00003183
Iteration 190/1000 | Loss: 0.00003183
Iteration 191/1000 | Loss: 0.00003183
Iteration 192/1000 | Loss: 0.00003183
Iteration 193/1000 | Loss: 0.00003183
Iteration 194/1000 | Loss: 0.00003182
Iteration 195/1000 | Loss: 0.00003182
Iteration 196/1000 | Loss: 0.00003182
Iteration 197/1000 | Loss: 0.00003182
Iteration 198/1000 | Loss: 0.00003182
Iteration 199/1000 | Loss: 0.00003182
Iteration 200/1000 | Loss: 0.00003182
Iteration 201/1000 | Loss: 0.00003182
Iteration 202/1000 | Loss: 0.00003182
Iteration 203/1000 | Loss: 0.00003182
Iteration 204/1000 | Loss: 0.00003181
Iteration 205/1000 | Loss: 0.00003181
Iteration 206/1000 | Loss: 0.00003181
Iteration 207/1000 | Loss: 0.00003181
Iteration 208/1000 | Loss: 0.00003181
Iteration 209/1000 | Loss: 0.00003181
Iteration 210/1000 | Loss: 0.00003181
Iteration 211/1000 | Loss: 0.00003181
Iteration 212/1000 | Loss: 0.00003181
Iteration 213/1000 | Loss: 0.00003181
Iteration 214/1000 | Loss: 0.00003180
Iteration 215/1000 | Loss: 0.00003180
Iteration 216/1000 | Loss: 0.00003180
Iteration 217/1000 | Loss: 0.00003180
Iteration 218/1000 | Loss: 0.00003180
Iteration 219/1000 | Loss: 0.00003180
Iteration 220/1000 | Loss: 0.00003180
Iteration 221/1000 | Loss: 0.00003180
Iteration 222/1000 | Loss: 0.00003180
Iteration 223/1000 | Loss: 0.00003180
Iteration 224/1000 | Loss: 0.00003180
Iteration 225/1000 | Loss: 0.00003180
Iteration 226/1000 | Loss: 0.00003180
Iteration 227/1000 | Loss: 0.00003180
Iteration 228/1000 | Loss: 0.00003180
Iteration 229/1000 | Loss: 0.00003180
Iteration 230/1000 | Loss: 0.00003180
Iteration 231/1000 | Loss: 0.00003180
Iteration 232/1000 | Loss: 0.00003180
Iteration 233/1000 | Loss: 0.00003180
Iteration 234/1000 | Loss: 0.00003180
Iteration 235/1000 | Loss: 0.00003180
Iteration 236/1000 | Loss: 0.00003180
Iteration 237/1000 | Loss: 0.00003180
Iteration 238/1000 | Loss: 0.00003180
Iteration 239/1000 | Loss: 0.00003180
Iteration 240/1000 | Loss: 0.00003180
Iteration 241/1000 | Loss: 0.00003180
Iteration 242/1000 | Loss: 0.00003180
Iteration 243/1000 | Loss: 0.00003180
Iteration 244/1000 | Loss: 0.00003180
Iteration 245/1000 | Loss: 0.00003180
Iteration 246/1000 | Loss: 0.00003180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [3.179898703820072e-05, 3.179898703820072e-05, 3.179898703820072e-05, 3.179898703820072e-05, 3.179898703820072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.179898703820072e-05

Optimization complete. Final v2v error: 4.397058486938477 mm

Highest mean error: 10.821707725524902 mm for frame 117

Lowest mean error: 3.459136486053467 mm for frame 8

Saving results

Total time: 165.60616660118103
