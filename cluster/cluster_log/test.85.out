Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=85, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4760-4815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008232
Iteration 2/25 | Loss: 0.00154961
Iteration 3/25 | Loss: 0.00135980
Iteration 4/25 | Loss: 0.00133452
Iteration 5/25 | Loss: 0.00132642
Iteration 6/25 | Loss: 0.00132431
Iteration 7/25 | Loss: 0.00132425
Iteration 8/25 | Loss: 0.00132425
Iteration 9/25 | Loss: 0.00132425
Iteration 10/25 | Loss: 0.00132425
Iteration 11/25 | Loss: 0.00132425
Iteration 12/25 | Loss: 0.00132425
Iteration 13/25 | Loss: 0.00132425
Iteration 14/25 | Loss: 0.00132425
Iteration 15/25 | Loss: 0.00132425
Iteration 16/25 | Loss: 0.00132425
Iteration 17/25 | Loss: 0.00132425
Iteration 18/25 | Loss: 0.00132425
Iteration 19/25 | Loss: 0.00132425
Iteration 20/25 | Loss: 0.00132425
Iteration 21/25 | Loss: 0.00132425
Iteration 22/25 | Loss: 0.00132425
Iteration 23/25 | Loss: 0.00132425
Iteration 24/25 | Loss: 0.00132425
Iteration 25/25 | Loss: 0.00132425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69781226
Iteration 2/25 | Loss: 0.00144143
Iteration 3/25 | Loss: 0.00144139
Iteration 4/25 | Loss: 0.00144139
Iteration 5/25 | Loss: 0.00144139
Iteration 6/25 | Loss: 0.00144139
Iteration 7/25 | Loss: 0.00144139
Iteration 8/25 | Loss: 0.00144139
Iteration 9/25 | Loss: 0.00144139
Iteration 10/25 | Loss: 0.00144139
Iteration 11/25 | Loss: 0.00144139
Iteration 12/25 | Loss: 0.00144139
Iteration 13/25 | Loss: 0.00144139
Iteration 14/25 | Loss: 0.00144139
Iteration 15/25 | Loss: 0.00144139
Iteration 16/25 | Loss: 0.00144139
Iteration 17/25 | Loss: 0.00144139
Iteration 18/25 | Loss: 0.00144139
Iteration 19/25 | Loss: 0.00144139
Iteration 20/25 | Loss: 0.00144139
Iteration 21/25 | Loss: 0.00144139
Iteration 22/25 | Loss: 0.00144139
Iteration 23/25 | Loss: 0.00144139
Iteration 24/25 | Loss: 0.00144139
Iteration 25/25 | Loss: 0.00144139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144139
Iteration 2/1000 | Loss: 0.00006016
Iteration 3/1000 | Loss: 0.00004415
Iteration 4/1000 | Loss: 0.00003484
Iteration 5/1000 | Loss: 0.00003267
Iteration 6/1000 | Loss: 0.00003126
Iteration 7/1000 | Loss: 0.00003029
Iteration 8/1000 | Loss: 0.00002930
Iteration 9/1000 | Loss: 0.00002861
Iteration 10/1000 | Loss: 0.00002813
Iteration 11/1000 | Loss: 0.00002781
Iteration 12/1000 | Loss: 0.00002743
Iteration 13/1000 | Loss: 0.00002718
Iteration 14/1000 | Loss: 0.00002694
Iteration 15/1000 | Loss: 0.00002676
Iteration 16/1000 | Loss: 0.00002671
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00002657
Iteration 19/1000 | Loss: 0.00002657
Iteration 20/1000 | Loss: 0.00002653
Iteration 21/1000 | Loss: 0.00002652
Iteration 22/1000 | Loss: 0.00002652
Iteration 23/1000 | Loss: 0.00002652
Iteration 24/1000 | Loss: 0.00002648
Iteration 25/1000 | Loss: 0.00002648
Iteration 26/1000 | Loss: 0.00002648
Iteration 27/1000 | Loss: 0.00002647
Iteration 28/1000 | Loss: 0.00002646
Iteration 29/1000 | Loss: 0.00002646
Iteration 30/1000 | Loss: 0.00002646
Iteration 31/1000 | Loss: 0.00002646
Iteration 32/1000 | Loss: 0.00002646
Iteration 33/1000 | Loss: 0.00002646
Iteration 34/1000 | Loss: 0.00002646
Iteration 35/1000 | Loss: 0.00002646
Iteration 36/1000 | Loss: 0.00002646
Iteration 37/1000 | Loss: 0.00002646
Iteration 38/1000 | Loss: 0.00002645
Iteration 39/1000 | Loss: 0.00002645
Iteration 40/1000 | Loss: 0.00002645
Iteration 41/1000 | Loss: 0.00002645
Iteration 42/1000 | Loss: 0.00002644
Iteration 43/1000 | Loss: 0.00002644
Iteration 44/1000 | Loss: 0.00002644
Iteration 45/1000 | Loss: 0.00002644
Iteration 46/1000 | Loss: 0.00002644
Iteration 47/1000 | Loss: 0.00002644
Iteration 48/1000 | Loss: 0.00002644
Iteration 49/1000 | Loss: 0.00002644
Iteration 50/1000 | Loss: 0.00002644
Iteration 51/1000 | Loss: 0.00002644
Iteration 52/1000 | Loss: 0.00002644
Iteration 53/1000 | Loss: 0.00002643
Iteration 54/1000 | Loss: 0.00002643
Iteration 55/1000 | Loss: 0.00002643
Iteration 56/1000 | Loss: 0.00002643
Iteration 57/1000 | Loss: 0.00002643
Iteration 58/1000 | Loss: 0.00002643
Iteration 59/1000 | Loss: 0.00002643
Iteration 60/1000 | Loss: 0.00002643
Iteration 61/1000 | Loss: 0.00002643
Iteration 62/1000 | Loss: 0.00002643
Iteration 63/1000 | Loss: 0.00002643
Iteration 64/1000 | Loss: 0.00002643
Iteration 65/1000 | Loss: 0.00002643
Iteration 66/1000 | Loss: 0.00002643
Iteration 67/1000 | Loss: 0.00002643
Iteration 68/1000 | Loss: 0.00002643
Iteration 69/1000 | Loss: 0.00002643
Iteration 70/1000 | Loss: 0.00002643
Iteration 71/1000 | Loss: 0.00002643
Iteration 72/1000 | Loss: 0.00002643
Iteration 73/1000 | Loss: 0.00002643
Iteration 74/1000 | Loss: 0.00002643
Iteration 75/1000 | Loss: 0.00002643
Iteration 76/1000 | Loss: 0.00002643
Iteration 77/1000 | Loss: 0.00002643
Iteration 78/1000 | Loss: 0.00002643
Iteration 79/1000 | Loss: 0.00002643
Iteration 80/1000 | Loss: 0.00002643
Iteration 81/1000 | Loss: 0.00002643
Iteration 82/1000 | Loss: 0.00002643
Iteration 83/1000 | Loss: 0.00002643
Iteration 84/1000 | Loss: 0.00002643
Iteration 85/1000 | Loss: 0.00002643
Iteration 86/1000 | Loss: 0.00002643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.6426385375089012e-05, 2.6426385375089012e-05, 2.6426385375089012e-05, 2.6426385375089012e-05, 2.6426385375089012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6426385375089012e-05

Optimization complete. Final v2v error: 4.357244968414307 mm

Highest mean error: 4.8994340896606445 mm for frame 70

Lowest mean error: 4.025496006011963 mm for frame 56

Saving results

Total time: 39.38123869895935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959807
Iteration 2/25 | Loss: 0.00180051
Iteration 3/25 | Loss: 0.00147250
Iteration 4/25 | Loss: 0.00143064
Iteration 5/25 | Loss: 0.00142019
Iteration 6/25 | Loss: 0.00141721
Iteration 7/25 | Loss: 0.00141636
Iteration 8/25 | Loss: 0.00141629
Iteration 9/25 | Loss: 0.00141629
Iteration 10/25 | Loss: 0.00141629
Iteration 11/25 | Loss: 0.00141629
Iteration 12/25 | Loss: 0.00141629
Iteration 13/25 | Loss: 0.00141629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014162920415401459, 0.0014162920415401459, 0.0014162920415401459, 0.0014162920415401459, 0.0014162920415401459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014162920415401459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26466107
Iteration 2/25 | Loss: 0.00137948
Iteration 3/25 | Loss: 0.00137945
Iteration 4/25 | Loss: 0.00137945
Iteration 5/25 | Loss: 0.00137945
Iteration 6/25 | Loss: 0.00137944
Iteration 7/25 | Loss: 0.00137944
Iteration 8/25 | Loss: 0.00137944
Iteration 9/25 | Loss: 0.00137944
Iteration 10/25 | Loss: 0.00137944
Iteration 11/25 | Loss: 0.00137944
Iteration 12/25 | Loss: 0.00137944
Iteration 13/25 | Loss: 0.00137944
Iteration 14/25 | Loss: 0.00137944
Iteration 15/25 | Loss: 0.00137944
Iteration 16/25 | Loss: 0.00137944
Iteration 17/25 | Loss: 0.00137944
Iteration 18/25 | Loss: 0.00137944
Iteration 19/25 | Loss: 0.00137944
Iteration 20/25 | Loss: 0.00137944
Iteration 21/25 | Loss: 0.00137944
Iteration 22/25 | Loss: 0.00137944
Iteration 23/25 | Loss: 0.00137944
Iteration 24/25 | Loss: 0.00137944
Iteration 25/25 | Loss: 0.00137944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137944
Iteration 2/1000 | Loss: 0.00008127
Iteration 3/1000 | Loss: 0.00005156
Iteration 4/1000 | Loss: 0.00004478
Iteration 5/1000 | Loss: 0.00004254
Iteration 6/1000 | Loss: 0.00004118
Iteration 7/1000 | Loss: 0.00003999
Iteration 8/1000 | Loss: 0.00003919
Iteration 9/1000 | Loss: 0.00003862
Iteration 10/1000 | Loss: 0.00003821
Iteration 11/1000 | Loss: 0.00003783
Iteration 12/1000 | Loss: 0.00003752
Iteration 13/1000 | Loss: 0.00003723
Iteration 14/1000 | Loss: 0.00003704
Iteration 15/1000 | Loss: 0.00003689
Iteration 16/1000 | Loss: 0.00003688
Iteration 17/1000 | Loss: 0.00003682
Iteration 18/1000 | Loss: 0.00003675
Iteration 19/1000 | Loss: 0.00003671
Iteration 20/1000 | Loss: 0.00003670
Iteration 21/1000 | Loss: 0.00003670
Iteration 22/1000 | Loss: 0.00003666
Iteration 23/1000 | Loss: 0.00003658
Iteration 24/1000 | Loss: 0.00003658
Iteration 25/1000 | Loss: 0.00003656
Iteration 26/1000 | Loss: 0.00003653
Iteration 27/1000 | Loss: 0.00003651
Iteration 28/1000 | Loss: 0.00003650
Iteration 29/1000 | Loss: 0.00003650
Iteration 30/1000 | Loss: 0.00003649
Iteration 31/1000 | Loss: 0.00003649
Iteration 32/1000 | Loss: 0.00003648
Iteration 33/1000 | Loss: 0.00003646
Iteration 34/1000 | Loss: 0.00003646
Iteration 35/1000 | Loss: 0.00003646
Iteration 36/1000 | Loss: 0.00003645
Iteration 37/1000 | Loss: 0.00003644
Iteration 38/1000 | Loss: 0.00003644
Iteration 39/1000 | Loss: 0.00003643
Iteration 40/1000 | Loss: 0.00003643
Iteration 41/1000 | Loss: 0.00003642
Iteration 42/1000 | Loss: 0.00003642
Iteration 43/1000 | Loss: 0.00003641
Iteration 44/1000 | Loss: 0.00003640
Iteration 45/1000 | Loss: 0.00003639
Iteration 46/1000 | Loss: 0.00003638
Iteration 47/1000 | Loss: 0.00003637
Iteration 48/1000 | Loss: 0.00003637
Iteration 49/1000 | Loss: 0.00003634
Iteration 50/1000 | Loss: 0.00003634
Iteration 51/1000 | Loss: 0.00003633
Iteration 52/1000 | Loss: 0.00003632
Iteration 53/1000 | Loss: 0.00003632
Iteration 54/1000 | Loss: 0.00003632
Iteration 55/1000 | Loss: 0.00003632
Iteration 56/1000 | Loss: 0.00003631
Iteration 57/1000 | Loss: 0.00003630
Iteration 58/1000 | Loss: 0.00003629
Iteration 59/1000 | Loss: 0.00003628
Iteration 60/1000 | Loss: 0.00003628
Iteration 61/1000 | Loss: 0.00003628
Iteration 62/1000 | Loss: 0.00003627
Iteration 63/1000 | Loss: 0.00003627
Iteration 64/1000 | Loss: 0.00003626
Iteration 65/1000 | Loss: 0.00003626
Iteration 66/1000 | Loss: 0.00003625
Iteration 67/1000 | Loss: 0.00003625
Iteration 68/1000 | Loss: 0.00003625
Iteration 69/1000 | Loss: 0.00003625
Iteration 70/1000 | Loss: 0.00003624
Iteration 71/1000 | Loss: 0.00003624
Iteration 72/1000 | Loss: 0.00003624
Iteration 73/1000 | Loss: 0.00003624
Iteration 74/1000 | Loss: 0.00003623
Iteration 75/1000 | Loss: 0.00003623
Iteration 76/1000 | Loss: 0.00003623
Iteration 77/1000 | Loss: 0.00003622
Iteration 78/1000 | Loss: 0.00003622
Iteration 79/1000 | Loss: 0.00003622
Iteration 80/1000 | Loss: 0.00003621
Iteration 81/1000 | Loss: 0.00003621
Iteration 82/1000 | Loss: 0.00003621
Iteration 83/1000 | Loss: 0.00003620
Iteration 84/1000 | Loss: 0.00003620
Iteration 85/1000 | Loss: 0.00003620
Iteration 86/1000 | Loss: 0.00003620
Iteration 87/1000 | Loss: 0.00003619
Iteration 88/1000 | Loss: 0.00003619
Iteration 89/1000 | Loss: 0.00003619
Iteration 90/1000 | Loss: 0.00003619
Iteration 91/1000 | Loss: 0.00003618
Iteration 92/1000 | Loss: 0.00003618
Iteration 93/1000 | Loss: 0.00003617
Iteration 94/1000 | Loss: 0.00003617
Iteration 95/1000 | Loss: 0.00003617
Iteration 96/1000 | Loss: 0.00003617
Iteration 97/1000 | Loss: 0.00003617
Iteration 98/1000 | Loss: 0.00003617
Iteration 99/1000 | Loss: 0.00003617
Iteration 100/1000 | Loss: 0.00003616
Iteration 101/1000 | Loss: 0.00003616
Iteration 102/1000 | Loss: 0.00003616
Iteration 103/1000 | Loss: 0.00003616
Iteration 104/1000 | Loss: 0.00003616
Iteration 105/1000 | Loss: 0.00003615
Iteration 106/1000 | Loss: 0.00003615
Iteration 107/1000 | Loss: 0.00003615
Iteration 108/1000 | Loss: 0.00003615
Iteration 109/1000 | Loss: 0.00003615
Iteration 110/1000 | Loss: 0.00003615
Iteration 111/1000 | Loss: 0.00003615
Iteration 112/1000 | Loss: 0.00003615
Iteration 113/1000 | Loss: 0.00003615
Iteration 114/1000 | Loss: 0.00003615
Iteration 115/1000 | Loss: 0.00003615
Iteration 116/1000 | Loss: 0.00003615
Iteration 117/1000 | Loss: 0.00003615
Iteration 118/1000 | Loss: 0.00003614
Iteration 119/1000 | Loss: 0.00003614
Iteration 120/1000 | Loss: 0.00003614
Iteration 121/1000 | Loss: 0.00003614
Iteration 122/1000 | Loss: 0.00003614
Iteration 123/1000 | Loss: 0.00003614
Iteration 124/1000 | Loss: 0.00003614
Iteration 125/1000 | Loss: 0.00003614
Iteration 126/1000 | Loss: 0.00003614
Iteration 127/1000 | Loss: 0.00003614
Iteration 128/1000 | Loss: 0.00003614
Iteration 129/1000 | Loss: 0.00003614
Iteration 130/1000 | Loss: 0.00003614
Iteration 131/1000 | Loss: 0.00003614
Iteration 132/1000 | Loss: 0.00003614
Iteration 133/1000 | Loss: 0.00003614
Iteration 134/1000 | Loss: 0.00003614
Iteration 135/1000 | Loss: 0.00003614
Iteration 136/1000 | Loss: 0.00003614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [3.6137844290351495e-05, 3.6137844290351495e-05, 3.6137844290351495e-05, 3.6137844290351495e-05, 3.6137844290351495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6137844290351495e-05

Optimization complete. Final v2v error: 4.8166937828063965 mm

Highest mean error: 5.954472064971924 mm for frame 123

Lowest mean error: 3.8449621200561523 mm for frame 78

Saving results

Total time: 53.92316293716431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037489
Iteration 2/25 | Loss: 0.00180377
Iteration 3/25 | Loss: 0.00144391
Iteration 4/25 | Loss: 0.00139407
Iteration 5/25 | Loss: 0.00138487
Iteration 6/25 | Loss: 0.00138285
Iteration 7/25 | Loss: 0.00138285
Iteration 8/25 | Loss: 0.00138285
Iteration 9/25 | Loss: 0.00138285
Iteration 10/25 | Loss: 0.00138285
Iteration 11/25 | Loss: 0.00138285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013828491792082787, 0.0013828491792082787, 0.0013828491792082787, 0.0013828491792082787, 0.0013828491792082787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013828491792082787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09752345
Iteration 2/25 | Loss: 0.00116112
Iteration 3/25 | Loss: 0.00116110
Iteration 4/25 | Loss: 0.00116110
Iteration 5/25 | Loss: 0.00116110
Iteration 6/25 | Loss: 0.00116110
Iteration 7/25 | Loss: 0.00116110
Iteration 8/25 | Loss: 0.00116110
Iteration 9/25 | Loss: 0.00116110
Iteration 10/25 | Loss: 0.00116110
Iteration 11/25 | Loss: 0.00116110
Iteration 12/25 | Loss: 0.00116110
Iteration 13/25 | Loss: 0.00116110
Iteration 14/25 | Loss: 0.00116110
Iteration 15/25 | Loss: 0.00116110
Iteration 16/25 | Loss: 0.00116110
Iteration 17/25 | Loss: 0.00116110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011610956862568855, 0.0011610956862568855, 0.0011610956862568855, 0.0011610956862568855, 0.0011610956862568855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011610956862568855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116110
Iteration 2/1000 | Loss: 0.00006904
Iteration 3/1000 | Loss: 0.00004146
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003474
Iteration 6/1000 | Loss: 0.00003311
Iteration 7/1000 | Loss: 0.00003210
Iteration 8/1000 | Loss: 0.00003149
Iteration 9/1000 | Loss: 0.00003106
Iteration 10/1000 | Loss: 0.00003068
Iteration 11/1000 | Loss: 0.00003030
Iteration 12/1000 | Loss: 0.00003006
Iteration 13/1000 | Loss: 0.00002981
Iteration 14/1000 | Loss: 0.00002962
Iteration 15/1000 | Loss: 0.00002951
Iteration 16/1000 | Loss: 0.00002931
Iteration 17/1000 | Loss: 0.00002929
Iteration 18/1000 | Loss: 0.00002929
Iteration 19/1000 | Loss: 0.00002919
Iteration 20/1000 | Loss: 0.00002918
Iteration 21/1000 | Loss: 0.00002918
Iteration 22/1000 | Loss: 0.00002918
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002917
Iteration 26/1000 | Loss: 0.00002916
Iteration 27/1000 | Loss: 0.00002915
Iteration 28/1000 | Loss: 0.00002915
Iteration 29/1000 | Loss: 0.00002915
Iteration 30/1000 | Loss: 0.00002914
Iteration 31/1000 | Loss: 0.00002914
Iteration 32/1000 | Loss: 0.00002913
Iteration 33/1000 | Loss: 0.00002913
Iteration 34/1000 | Loss: 0.00002913
Iteration 35/1000 | Loss: 0.00002913
Iteration 36/1000 | Loss: 0.00002912
Iteration 37/1000 | Loss: 0.00002912
Iteration 38/1000 | Loss: 0.00002911
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002911
Iteration 41/1000 | Loss: 0.00002910
Iteration 42/1000 | Loss: 0.00002910
Iteration 43/1000 | Loss: 0.00002910
Iteration 44/1000 | Loss: 0.00002910
Iteration 45/1000 | Loss: 0.00002909
Iteration 46/1000 | Loss: 0.00002909
Iteration 47/1000 | Loss: 0.00002907
Iteration 48/1000 | Loss: 0.00002906
Iteration 49/1000 | Loss: 0.00002906
Iteration 50/1000 | Loss: 0.00002906
Iteration 51/1000 | Loss: 0.00002905
Iteration 52/1000 | Loss: 0.00002905
Iteration 53/1000 | Loss: 0.00002902
Iteration 54/1000 | Loss: 0.00002902
Iteration 55/1000 | Loss: 0.00002902
Iteration 56/1000 | Loss: 0.00002901
Iteration 57/1000 | Loss: 0.00002901
Iteration 58/1000 | Loss: 0.00002901
Iteration 59/1000 | Loss: 0.00002899
Iteration 60/1000 | Loss: 0.00002898
Iteration 61/1000 | Loss: 0.00002898
Iteration 62/1000 | Loss: 0.00002897
Iteration 63/1000 | Loss: 0.00002897
Iteration 64/1000 | Loss: 0.00002897
Iteration 65/1000 | Loss: 0.00002896
Iteration 66/1000 | Loss: 0.00002896
Iteration 67/1000 | Loss: 0.00002896
Iteration 68/1000 | Loss: 0.00002895
Iteration 69/1000 | Loss: 0.00002895
Iteration 70/1000 | Loss: 0.00002895
Iteration 71/1000 | Loss: 0.00002894
Iteration 72/1000 | Loss: 0.00002894
Iteration 73/1000 | Loss: 0.00002894
Iteration 74/1000 | Loss: 0.00002894
Iteration 75/1000 | Loss: 0.00002894
Iteration 76/1000 | Loss: 0.00002894
Iteration 77/1000 | Loss: 0.00002893
Iteration 78/1000 | Loss: 0.00002893
Iteration 79/1000 | Loss: 0.00002893
Iteration 80/1000 | Loss: 0.00002893
Iteration 81/1000 | Loss: 0.00002892
Iteration 82/1000 | Loss: 0.00002892
Iteration 83/1000 | Loss: 0.00002891
Iteration 84/1000 | Loss: 0.00002891
Iteration 85/1000 | Loss: 0.00002891
Iteration 86/1000 | Loss: 0.00002891
Iteration 87/1000 | Loss: 0.00002891
Iteration 88/1000 | Loss: 0.00002891
Iteration 89/1000 | Loss: 0.00002890
Iteration 90/1000 | Loss: 0.00002890
Iteration 91/1000 | Loss: 0.00002890
Iteration 92/1000 | Loss: 0.00002889
Iteration 93/1000 | Loss: 0.00002889
Iteration 94/1000 | Loss: 0.00002889
Iteration 95/1000 | Loss: 0.00002889
Iteration 96/1000 | Loss: 0.00002889
Iteration 97/1000 | Loss: 0.00002889
Iteration 98/1000 | Loss: 0.00002889
Iteration 99/1000 | Loss: 0.00002889
Iteration 100/1000 | Loss: 0.00002889
Iteration 101/1000 | Loss: 0.00002889
Iteration 102/1000 | Loss: 0.00002889
Iteration 103/1000 | Loss: 0.00002889
Iteration 104/1000 | Loss: 0.00002889
Iteration 105/1000 | Loss: 0.00002889
Iteration 106/1000 | Loss: 0.00002889
Iteration 107/1000 | Loss: 0.00002889
Iteration 108/1000 | Loss: 0.00002889
Iteration 109/1000 | Loss: 0.00002889
Iteration 110/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.8887568987556733e-05, 2.8887568987556733e-05, 2.8887568987556733e-05, 2.8887568987556733e-05, 2.8887568987556733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8887568987556733e-05

Optimization complete. Final v2v error: 4.476494312286377 mm

Highest mean error: 5.475167751312256 mm for frame 158

Lowest mean error: 3.885037660598755 mm for frame 232

Saving results

Total time: 45.319058418273926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027766
Iteration 2/25 | Loss: 0.00180377
Iteration 3/25 | Loss: 0.00217559
Iteration 4/25 | Loss: 0.00137832
Iteration 5/25 | Loss: 0.00132544
Iteration 6/25 | Loss: 0.00128196
Iteration 7/25 | Loss: 0.00125370
Iteration 8/25 | Loss: 0.00125321
Iteration 9/25 | Loss: 0.00122208
Iteration 10/25 | Loss: 0.00121914
Iteration 11/25 | Loss: 0.00120774
Iteration 12/25 | Loss: 0.00120328
Iteration 13/25 | Loss: 0.00120141
Iteration 14/25 | Loss: 0.00120093
Iteration 15/25 | Loss: 0.00120060
Iteration 16/25 | Loss: 0.00120046
Iteration 17/25 | Loss: 0.00120216
Iteration 18/25 | Loss: 0.00119887
Iteration 19/25 | Loss: 0.00119796
Iteration 20/25 | Loss: 0.00119775
Iteration 21/25 | Loss: 0.00119771
Iteration 22/25 | Loss: 0.00119770
Iteration 23/25 | Loss: 0.00119770
Iteration 24/25 | Loss: 0.00119769
Iteration 25/25 | Loss: 0.00119769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34121907
Iteration 2/25 | Loss: 0.00163048
Iteration 3/25 | Loss: 0.00133755
Iteration 4/25 | Loss: 0.00133754
Iteration 5/25 | Loss: 0.00133754
Iteration 6/25 | Loss: 0.00133754
Iteration 7/25 | Loss: 0.00133754
Iteration 8/25 | Loss: 0.00133754
Iteration 9/25 | Loss: 0.00133754
Iteration 10/25 | Loss: 0.00133754
Iteration 11/25 | Loss: 0.00133754
Iteration 12/25 | Loss: 0.00133754
Iteration 13/25 | Loss: 0.00133754
Iteration 14/25 | Loss: 0.00133754
Iteration 15/25 | Loss: 0.00133754
Iteration 16/25 | Loss: 0.00133754
Iteration 17/25 | Loss: 0.00133754
Iteration 18/25 | Loss: 0.00133754
Iteration 19/25 | Loss: 0.00133754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013375350972637534, 0.0013375350972637534, 0.0013375350972637534, 0.0013375350972637534, 0.0013375350972637534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013375350972637534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133754
Iteration 2/1000 | Loss: 0.00024758
Iteration 3/1000 | Loss: 0.00058551
Iteration 4/1000 | Loss: 0.00003007
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001453
Iteration 7/1000 | Loss: 0.00039305
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00013045
Iteration 10/1000 | Loss: 0.00002214
Iteration 11/1000 | Loss: 0.00004696
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00016074
Iteration 14/1000 | Loss: 0.00001202
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001102
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001093
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001092
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001089
Iteration 52/1000 | Loss: 0.00001089
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001083
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001081
Iteration 58/1000 | Loss: 0.00001081
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001079
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001079
Iteration 65/1000 | Loss: 0.00001079
Iteration 66/1000 | Loss: 0.00001079
Iteration 67/1000 | Loss: 0.00001078
Iteration 68/1000 | Loss: 0.00001078
Iteration 69/1000 | Loss: 0.00001078
Iteration 70/1000 | Loss: 0.00001078
Iteration 71/1000 | Loss: 0.00001078
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001077
Iteration 74/1000 | Loss: 0.00001077
Iteration 75/1000 | Loss: 0.00001077
Iteration 76/1000 | Loss: 0.00001077
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001076
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001075
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001074
Iteration 99/1000 | Loss: 0.00001074
Iteration 100/1000 | Loss: 0.00001074
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001074
Iteration 104/1000 | Loss: 0.00001074
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001073
Iteration 110/1000 | Loss: 0.00001073
Iteration 111/1000 | Loss: 0.00001073
Iteration 112/1000 | Loss: 0.00001073
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001072
Iteration 126/1000 | Loss: 0.00001072
Iteration 127/1000 | Loss: 0.00001072
Iteration 128/1000 | Loss: 0.00001072
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001072
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001071
Iteration 142/1000 | Loss: 0.00001071
Iteration 143/1000 | Loss: 0.00001071
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001070
Iteration 147/1000 | Loss: 0.00001070
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001070
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001070
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.0701852261263411e-05, 1.0701852261263411e-05, 1.0701852261263411e-05, 1.0701852261263411e-05, 1.0701852261263411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0701852261263411e-05

Optimization complete. Final v2v error: 2.8015639781951904 mm

Highest mean error: 3.5934393405914307 mm for frame 71

Lowest mean error: 2.544156789779663 mm for frame 44

Saving results

Total time: 72.29021120071411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028964
Iteration 2/25 | Loss: 0.00189263
Iteration 3/25 | Loss: 0.00158496
Iteration 4/25 | Loss: 0.00163251
Iteration 5/25 | Loss: 0.00152387
Iteration 6/25 | Loss: 0.00151769
Iteration 7/25 | Loss: 0.00144568
Iteration 8/25 | Loss: 0.00143630
Iteration 9/25 | Loss: 0.00142183
Iteration 10/25 | Loss: 0.00141752
Iteration 11/25 | Loss: 0.00140175
Iteration 12/25 | Loss: 0.00138849
Iteration 13/25 | Loss: 0.00137976
Iteration 14/25 | Loss: 0.00137960
Iteration 15/25 | Loss: 0.00137758
Iteration 16/25 | Loss: 0.00137679
Iteration 17/25 | Loss: 0.00137321
Iteration 18/25 | Loss: 0.00137151
Iteration 19/25 | Loss: 0.00137038
Iteration 20/25 | Loss: 0.00136869
Iteration 21/25 | Loss: 0.00136861
Iteration 22/25 | Loss: 0.00136823
Iteration 23/25 | Loss: 0.00138493
Iteration 24/25 | Loss: 0.00136674
Iteration 25/25 | Loss: 0.00135772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60429525
Iteration 2/25 | Loss: 0.00157848
Iteration 3/25 | Loss: 0.00157847
Iteration 4/25 | Loss: 0.00157847
Iteration 5/25 | Loss: 0.00157847
Iteration 6/25 | Loss: 0.00157847
Iteration 7/25 | Loss: 0.00157847
Iteration 8/25 | Loss: 0.00157847
Iteration 9/25 | Loss: 0.00157847
Iteration 10/25 | Loss: 0.00157847
Iteration 11/25 | Loss: 0.00157847
Iteration 12/25 | Loss: 0.00157847
Iteration 13/25 | Loss: 0.00157847
Iteration 14/25 | Loss: 0.00157847
Iteration 15/25 | Loss: 0.00157847
Iteration 16/25 | Loss: 0.00157847
Iteration 17/25 | Loss: 0.00157847
Iteration 18/25 | Loss: 0.00157847
Iteration 19/25 | Loss: 0.00157847
Iteration 20/25 | Loss: 0.00147838
Iteration 21/25 | Loss: 0.00147838
Iteration 22/25 | Loss: 0.00147838
Iteration 23/25 | Loss: 0.00147838
Iteration 24/25 | Loss: 0.00147838
Iteration 25/25 | Loss: 0.00147838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147838
Iteration 2/1000 | Loss: 0.00181908
Iteration 3/1000 | Loss: 0.00006119
Iteration 4/1000 | Loss: 0.00003657
Iteration 5/1000 | Loss: 0.00004187
Iteration 6/1000 | Loss: 0.00002700
Iteration 7/1000 | Loss: 0.00015056
Iteration 8/1000 | Loss: 0.00015055
Iteration 9/1000 | Loss: 0.00050673
Iteration 10/1000 | Loss: 0.00005267
Iteration 11/1000 | Loss: 0.00002393
Iteration 12/1000 | Loss: 0.00014661
Iteration 13/1000 | Loss: 0.00019756
Iteration 14/1000 | Loss: 0.00008788
Iteration 15/1000 | Loss: 0.00003249
Iteration 16/1000 | Loss: 0.00002372
Iteration 17/1000 | Loss: 0.00002228
Iteration 18/1000 | Loss: 0.00010423
Iteration 19/1000 | Loss: 0.00002267
Iteration 20/1000 | Loss: 0.00002164
Iteration 21/1000 | Loss: 0.00002141
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002075
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002055
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002050
Iteration 29/1000 | Loss: 0.00002050
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002047
Iteration 32/1000 | Loss: 0.00002046
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002039
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00029466
Iteration 40/1000 | Loss: 0.00002339
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002035
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001943
Iteration 45/1000 | Loss: 0.00001930
Iteration 46/1000 | Loss: 0.00001918
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001911
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00001911
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00001899
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00014942
Iteration 70/1000 | Loss: 0.00003154
Iteration 71/1000 | Loss: 0.00001913
Iteration 72/1000 | Loss: 0.00005145
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001891
Iteration 77/1000 | Loss: 0.00001890
Iteration 78/1000 | Loss: 0.00001889
Iteration 79/1000 | Loss: 0.00001889
Iteration 80/1000 | Loss: 0.00001889
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00001888
Iteration 83/1000 | Loss: 0.00001888
Iteration 84/1000 | Loss: 0.00001884
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001878
Iteration 95/1000 | Loss: 0.00001878
Iteration 96/1000 | Loss: 0.00001878
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001877
Iteration 104/1000 | Loss: 0.00001877
Iteration 105/1000 | Loss: 0.00001877
Iteration 106/1000 | Loss: 0.00001877
Iteration 107/1000 | Loss: 0.00001877
Iteration 108/1000 | Loss: 0.00001877
Iteration 109/1000 | Loss: 0.00001877
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001875
Iteration 114/1000 | Loss: 0.00001875
Iteration 115/1000 | Loss: 0.00001875
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001875
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001875
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001874
Iteration 125/1000 | Loss: 0.00001874
Iteration 126/1000 | Loss: 0.00001874
Iteration 127/1000 | Loss: 0.00001874
Iteration 128/1000 | Loss: 0.00001874
Iteration 129/1000 | Loss: 0.00001874
Iteration 130/1000 | Loss: 0.00001873
Iteration 131/1000 | Loss: 0.00001873
Iteration 132/1000 | Loss: 0.00001873
Iteration 133/1000 | Loss: 0.00001872
Iteration 134/1000 | Loss: 0.00001872
Iteration 135/1000 | Loss: 0.00001872
Iteration 136/1000 | Loss: 0.00001872
Iteration 137/1000 | Loss: 0.00001872
Iteration 138/1000 | Loss: 0.00001872
Iteration 139/1000 | Loss: 0.00001872
Iteration 140/1000 | Loss: 0.00001872
Iteration 141/1000 | Loss: 0.00001871
Iteration 142/1000 | Loss: 0.00001871
Iteration 143/1000 | Loss: 0.00001871
Iteration 144/1000 | Loss: 0.00001871
Iteration 145/1000 | Loss: 0.00001871
Iteration 146/1000 | Loss: 0.00001871
Iteration 147/1000 | Loss: 0.00001871
Iteration 148/1000 | Loss: 0.00001871
Iteration 149/1000 | Loss: 0.00001871
Iteration 150/1000 | Loss: 0.00001871
Iteration 151/1000 | Loss: 0.00001871
Iteration 152/1000 | Loss: 0.00001871
Iteration 153/1000 | Loss: 0.00001871
Iteration 154/1000 | Loss: 0.00001871
Iteration 155/1000 | Loss: 0.00001871
Iteration 156/1000 | Loss: 0.00001871
Iteration 157/1000 | Loss: 0.00001871
Iteration 158/1000 | Loss: 0.00001871
Iteration 159/1000 | Loss: 0.00001870
Iteration 160/1000 | Loss: 0.00001870
Iteration 161/1000 | Loss: 0.00001870
Iteration 162/1000 | Loss: 0.00001870
Iteration 163/1000 | Loss: 0.00001870
Iteration 164/1000 | Loss: 0.00001870
Iteration 165/1000 | Loss: 0.00001870
Iteration 166/1000 | Loss: 0.00001869
Iteration 167/1000 | Loss: 0.00001869
Iteration 168/1000 | Loss: 0.00001869
Iteration 169/1000 | Loss: 0.00001869
Iteration 170/1000 | Loss: 0.00001869
Iteration 171/1000 | Loss: 0.00001869
Iteration 172/1000 | Loss: 0.00001869
Iteration 173/1000 | Loss: 0.00001869
Iteration 174/1000 | Loss: 0.00001869
Iteration 175/1000 | Loss: 0.00001869
Iteration 176/1000 | Loss: 0.00001869
Iteration 177/1000 | Loss: 0.00001869
Iteration 178/1000 | Loss: 0.00001869
Iteration 179/1000 | Loss: 0.00001869
Iteration 180/1000 | Loss: 0.00001869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.869243169494439e-05, 1.869243169494439e-05, 1.869243169494439e-05, 1.869243169494439e-05, 1.869243169494439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.869243169494439e-05

Optimization complete. Final v2v error: 3.717207670211792 mm

Highest mean error: 4.542927265167236 mm for frame 210

Lowest mean error: 3.4537858963012695 mm for frame 63

Saving results

Total time: 130.47394633293152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782929
Iteration 2/25 | Loss: 0.00146058
Iteration 3/25 | Loss: 0.00126982
Iteration 4/25 | Loss: 0.00124746
Iteration 5/25 | Loss: 0.00124410
Iteration 6/25 | Loss: 0.00124375
Iteration 7/25 | Loss: 0.00124375
Iteration 8/25 | Loss: 0.00124368
Iteration 9/25 | Loss: 0.00124368
Iteration 10/25 | Loss: 0.00124368
Iteration 11/25 | Loss: 0.00124368
Iteration 12/25 | Loss: 0.00124368
Iteration 13/25 | Loss: 0.00124368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012436762917786837, 0.0012436762917786837, 0.0012436762917786837, 0.0012436762917786837, 0.0012436762917786837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012436762917786837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28500450
Iteration 2/25 | Loss: 0.00108660
Iteration 3/25 | Loss: 0.00108660
Iteration 4/25 | Loss: 0.00108660
Iteration 5/25 | Loss: 0.00108660
Iteration 6/25 | Loss: 0.00108660
Iteration 7/25 | Loss: 0.00108660
Iteration 8/25 | Loss: 0.00108660
Iteration 9/25 | Loss: 0.00108660
Iteration 10/25 | Loss: 0.00108660
Iteration 11/25 | Loss: 0.00108660
Iteration 12/25 | Loss: 0.00108660
Iteration 13/25 | Loss: 0.00108660
Iteration 14/25 | Loss: 0.00108660
Iteration 15/25 | Loss: 0.00108660
Iteration 16/25 | Loss: 0.00108660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010865979129448533, 0.0010865979129448533, 0.0010865979129448533, 0.0010865979129448533, 0.0010865979129448533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010865979129448533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108660
Iteration 2/1000 | Loss: 0.00003168
Iteration 3/1000 | Loss: 0.00002213
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001853
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001678
Iteration 12/1000 | Loss: 0.00001659
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001654
Iteration 15/1000 | Loss: 0.00001653
Iteration 16/1000 | Loss: 0.00001649
Iteration 17/1000 | Loss: 0.00001648
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001633
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001630
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001629
Iteration 31/1000 | Loss: 0.00001629
Iteration 32/1000 | Loss: 0.00001629
Iteration 33/1000 | Loss: 0.00001629
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001628
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001617
Iteration 65/1000 | Loss: 0.00001617
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001616
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001615
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001614
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001611
Iteration 100/1000 | Loss: 0.00001611
Iteration 101/1000 | Loss: 0.00001611
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001611
Iteration 105/1000 | Loss: 0.00001611
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.610585422895383e-05, 1.610585422895383e-05, 1.610585422895383e-05, 1.610585422895383e-05, 1.610585422895383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.610585422895383e-05

Optimization complete. Final v2v error: 3.4094884395599365 mm

Highest mean error: 4.22412109375 mm for frame 177

Lowest mean error: 2.9788613319396973 mm for frame 136

Saving results

Total time: 40.40944480895996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043642
Iteration 2/25 | Loss: 0.00280936
Iteration 3/25 | Loss: 0.00203189
Iteration 4/25 | Loss: 0.00176662
Iteration 5/25 | Loss: 0.00164165
Iteration 6/25 | Loss: 0.00150317
Iteration 7/25 | Loss: 0.00129507
Iteration 8/25 | Loss: 0.00121681
Iteration 9/25 | Loss: 0.00120341
Iteration 10/25 | Loss: 0.00120076
Iteration 11/25 | Loss: 0.00120013
Iteration 12/25 | Loss: 0.00119996
Iteration 13/25 | Loss: 0.00119992
Iteration 14/25 | Loss: 0.00119992
Iteration 15/25 | Loss: 0.00119992
Iteration 16/25 | Loss: 0.00119992
Iteration 17/25 | Loss: 0.00119992
Iteration 18/25 | Loss: 0.00119992
Iteration 19/25 | Loss: 0.00119992
Iteration 20/25 | Loss: 0.00119992
Iteration 21/25 | Loss: 0.00119992
Iteration 22/25 | Loss: 0.00119992
Iteration 23/25 | Loss: 0.00119992
Iteration 24/25 | Loss: 0.00119992
Iteration 25/25 | Loss: 0.00119992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28288794
Iteration 2/25 | Loss: 0.00123748
Iteration 3/25 | Loss: 0.00123748
Iteration 4/25 | Loss: 0.00123748
Iteration 5/25 | Loss: 0.00123748
Iteration 6/25 | Loss: 0.00123748
Iteration 7/25 | Loss: 0.00123748
Iteration 8/25 | Loss: 0.00123748
Iteration 9/25 | Loss: 0.00123748
Iteration 10/25 | Loss: 0.00123748
Iteration 11/25 | Loss: 0.00123748
Iteration 12/25 | Loss: 0.00123748
Iteration 13/25 | Loss: 0.00123748
Iteration 14/25 | Loss: 0.00123748
Iteration 15/25 | Loss: 0.00123748
Iteration 16/25 | Loss: 0.00123748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012374765938147902, 0.0012374765938147902, 0.0012374765938147902, 0.0012374765938147902, 0.0012374765938147902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012374765938147902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123748
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00001967
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001688
Iteration 9/1000 | Loss: 0.00001646
Iteration 10/1000 | Loss: 0.00043333
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001449
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001396
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001353
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001339
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001338
Iteration 39/1000 | Loss: 0.00001338
Iteration 40/1000 | Loss: 0.00001338
Iteration 41/1000 | Loss: 0.00001337
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001333
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001324
Iteration 77/1000 | Loss: 0.00001324
Iteration 78/1000 | Loss: 0.00001324
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001323
Iteration 85/1000 | Loss: 0.00001323
Iteration 86/1000 | Loss: 0.00001323
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001322
Iteration 89/1000 | Loss: 0.00001322
Iteration 90/1000 | Loss: 0.00001322
Iteration 91/1000 | Loss: 0.00001322
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001322
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001320
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001318
Iteration 124/1000 | Loss: 0.00001318
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001316
Iteration 145/1000 | Loss: 0.00001316
Iteration 146/1000 | Loss: 0.00001316
Iteration 147/1000 | Loss: 0.00001316
Iteration 148/1000 | Loss: 0.00001316
Iteration 149/1000 | Loss: 0.00001316
Iteration 150/1000 | Loss: 0.00001316
Iteration 151/1000 | Loss: 0.00001316
Iteration 152/1000 | Loss: 0.00001316
Iteration 153/1000 | Loss: 0.00001316
Iteration 154/1000 | Loss: 0.00001316
Iteration 155/1000 | Loss: 0.00001316
Iteration 156/1000 | Loss: 0.00001316
Iteration 157/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.3159453374100849e-05, 1.3159453374100849e-05, 1.3159453374100849e-05, 1.3159453374100849e-05, 1.3159453374100849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3159453374100849e-05

Optimization complete. Final v2v error: 3.105337142944336 mm

Highest mean error: 3.950902223587036 mm for frame 159

Lowest mean error: 2.915606737136841 mm for frame 7

Saving results

Total time: 61.208261251449585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839176
Iteration 2/25 | Loss: 0.00126376
Iteration 3/25 | Loss: 0.00119955
Iteration 4/25 | Loss: 0.00119288
Iteration 5/25 | Loss: 0.00119060
Iteration 6/25 | Loss: 0.00119014
Iteration 7/25 | Loss: 0.00119014
Iteration 8/25 | Loss: 0.00119014
Iteration 9/25 | Loss: 0.00119014
Iteration 10/25 | Loss: 0.00119014
Iteration 11/25 | Loss: 0.00119014
Iteration 12/25 | Loss: 0.00119014
Iteration 13/25 | Loss: 0.00119014
Iteration 14/25 | Loss: 0.00119014
Iteration 15/25 | Loss: 0.00119014
Iteration 16/25 | Loss: 0.00119014
Iteration 17/25 | Loss: 0.00119014
Iteration 18/25 | Loss: 0.00119014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011901359539479017, 0.0011901359539479017, 0.0011901359539479017, 0.0011901359539479017, 0.0011901359539479017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011901359539479017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63785791
Iteration 2/25 | Loss: 0.00122580
Iteration 3/25 | Loss: 0.00122579
Iteration 4/25 | Loss: 0.00122579
Iteration 5/25 | Loss: 0.00122579
Iteration 6/25 | Loss: 0.00122579
Iteration 7/25 | Loss: 0.00122579
Iteration 8/25 | Loss: 0.00122579
Iteration 9/25 | Loss: 0.00122579
Iteration 10/25 | Loss: 0.00122579
Iteration 11/25 | Loss: 0.00122579
Iteration 12/25 | Loss: 0.00122579
Iteration 13/25 | Loss: 0.00122579
Iteration 14/25 | Loss: 0.00122579
Iteration 15/25 | Loss: 0.00122579
Iteration 16/25 | Loss: 0.00122579
Iteration 17/25 | Loss: 0.00122579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012257915223017335, 0.0012257915223017335, 0.0012257915223017335, 0.0012257915223017335, 0.0012257915223017335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012257915223017335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122579
Iteration 2/1000 | Loss: 0.00002103
Iteration 3/1000 | Loss: 0.00001387
Iteration 4/1000 | Loss: 0.00001243
Iteration 5/1000 | Loss: 0.00001161
Iteration 6/1000 | Loss: 0.00001104
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001014
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000986
Iteration 13/1000 | Loss: 0.00000986
Iteration 14/1000 | Loss: 0.00000985
Iteration 15/1000 | Loss: 0.00000985
Iteration 16/1000 | Loss: 0.00000980
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000971
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000968
Iteration 22/1000 | Loss: 0.00000967
Iteration 23/1000 | Loss: 0.00000966
Iteration 24/1000 | Loss: 0.00000965
Iteration 25/1000 | Loss: 0.00000963
Iteration 26/1000 | Loss: 0.00000963
Iteration 27/1000 | Loss: 0.00000963
Iteration 28/1000 | Loss: 0.00000962
Iteration 29/1000 | Loss: 0.00000962
Iteration 30/1000 | Loss: 0.00000962
Iteration 31/1000 | Loss: 0.00000961
Iteration 32/1000 | Loss: 0.00000961
Iteration 33/1000 | Loss: 0.00000959
Iteration 34/1000 | Loss: 0.00000959
Iteration 35/1000 | Loss: 0.00000959
Iteration 36/1000 | Loss: 0.00000959
Iteration 37/1000 | Loss: 0.00000958
Iteration 38/1000 | Loss: 0.00000958
Iteration 39/1000 | Loss: 0.00000958
Iteration 40/1000 | Loss: 0.00000957
Iteration 41/1000 | Loss: 0.00000957
Iteration 42/1000 | Loss: 0.00000956
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000955
Iteration 45/1000 | Loss: 0.00000955
Iteration 46/1000 | Loss: 0.00000954
Iteration 47/1000 | Loss: 0.00000954
Iteration 48/1000 | Loss: 0.00000953
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00000952
Iteration 52/1000 | Loss: 0.00000952
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000951
Iteration 55/1000 | Loss: 0.00000951
Iteration 56/1000 | Loss: 0.00000950
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000948
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000948
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000946
Iteration 70/1000 | Loss: 0.00000945
Iteration 71/1000 | Loss: 0.00000945
Iteration 72/1000 | Loss: 0.00000945
Iteration 73/1000 | Loss: 0.00000945
Iteration 74/1000 | Loss: 0.00000945
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000944
Iteration 77/1000 | Loss: 0.00000944
Iteration 78/1000 | Loss: 0.00000944
Iteration 79/1000 | Loss: 0.00000944
Iteration 80/1000 | Loss: 0.00000944
Iteration 81/1000 | Loss: 0.00000944
Iteration 82/1000 | Loss: 0.00000944
Iteration 83/1000 | Loss: 0.00000944
Iteration 84/1000 | Loss: 0.00000944
Iteration 85/1000 | Loss: 0.00000944
Iteration 86/1000 | Loss: 0.00000943
Iteration 87/1000 | Loss: 0.00000943
Iteration 88/1000 | Loss: 0.00000943
Iteration 89/1000 | Loss: 0.00000943
Iteration 90/1000 | Loss: 0.00000943
Iteration 91/1000 | Loss: 0.00000942
Iteration 92/1000 | Loss: 0.00000942
Iteration 93/1000 | Loss: 0.00000942
Iteration 94/1000 | Loss: 0.00000942
Iteration 95/1000 | Loss: 0.00000942
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000941
Iteration 99/1000 | Loss: 0.00000941
Iteration 100/1000 | Loss: 0.00000941
Iteration 101/1000 | Loss: 0.00000939
Iteration 102/1000 | Loss: 0.00000939
Iteration 103/1000 | Loss: 0.00000939
Iteration 104/1000 | Loss: 0.00000938
Iteration 105/1000 | Loss: 0.00000938
Iteration 106/1000 | Loss: 0.00000938
Iteration 107/1000 | Loss: 0.00000938
Iteration 108/1000 | Loss: 0.00000938
Iteration 109/1000 | Loss: 0.00000938
Iteration 110/1000 | Loss: 0.00000937
Iteration 111/1000 | Loss: 0.00000937
Iteration 112/1000 | Loss: 0.00000936
Iteration 113/1000 | Loss: 0.00000936
Iteration 114/1000 | Loss: 0.00000936
Iteration 115/1000 | Loss: 0.00000935
Iteration 116/1000 | Loss: 0.00000935
Iteration 117/1000 | Loss: 0.00000935
Iteration 118/1000 | Loss: 0.00000934
Iteration 119/1000 | Loss: 0.00000934
Iteration 120/1000 | Loss: 0.00000934
Iteration 121/1000 | Loss: 0.00000933
Iteration 122/1000 | Loss: 0.00000933
Iteration 123/1000 | Loss: 0.00000933
Iteration 124/1000 | Loss: 0.00000933
Iteration 125/1000 | Loss: 0.00000933
Iteration 126/1000 | Loss: 0.00000933
Iteration 127/1000 | Loss: 0.00000933
Iteration 128/1000 | Loss: 0.00000933
Iteration 129/1000 | Loss: 0.00000932
Iteration 130/1000 | Loss: 0.00000932
Iteration 131/1000 | Loss: 0.00000932
Iteration 132/1000 | Loss: 0.00000932
Iteration 133/1000 | Loss: 0.00000932
Iteration 134/1000 | Loss: 0.00000932
Iteration 135/1000 | Loss: 0.00000932
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000931
Iteration 139/1000 | Loss: 0.00000931
Iteration 140/1000 | Loss: 0.00000931
Iteration 141/1000 | Loss: 0.00000931
Iteration 142/1000 | Loss: 0.00000931
Iteration 143/1000 | Loss: 0.00000931
Iteration 144/1000 | Loss: 0.00000931
Iteration 145/1000 | Loss: 0.00000931
Iteration 146/1000 | Loss: 0.00000931
Iteration 147/1000 | Loss: 0.00000931
Iteration 148/1000 | Loss: 0.00000931
Iteration 149/1000 | Loss: 0.00000931
Iteration 150/1000 | Loss: 0.00000931
Iteration 151/1000 | Loss: 0.00000931
Iteration 152/1000 | Loss: 0.00000931
Iteration 153/1000 | Loss: 0.00000931
Iteration 154/1000 | Loss: 0.00000931
Iteration 155/1000 | Loss: 0.00000931
Iteration 156/1000 | Loss: 0.00000931
Iteration 157/1000 | Loss: 0.00000931
Iteration 158/1000 | Loss: 0.00000931
Iteration 159/1000 | Loss: 0.00000931
Iteration 160/1000 | Loss: 0.00000931
Iteration 161/1000 | Loss: 0.00000930
Iteration 162/1000 | Loss: 0.00000930
Iteration 163/1000 | Loss: 0.00000930
Iteration 164/1000 | Loss: 0.00000930
Iteration 165/1000 | Loss: 0.00000930
Iteration 166/1000 | Loss: 0.00000930
Iteration 167/1000 | Loss: 0.00000930
Iteration 168/1000 | Loss: 0.00000930
Iteration 169/1000 | Loss: 0.00000930
Iteration 170/1000 | Loss: 0.00000930
Iteration 171/1000 | Loss: 0.00000930
Iteration 172/1000 | Loss: 0.00000930
Iteration 173/1000 | Loss: 0.00000930
Iteration 174/1000 | Loss: 0.00000930
Iteration 175/1000 | Loss: 0.00000930
Iteration 176/1000 | Loss: 0.00000930
Iteration 177/1000 | Loss: 0.00000930
Iteration 178/1000 | Loss: 0.00000930
Iteration 179/1000 | Loss: 0.00000930
Iteration 180/1000 | Loss: 0.00000930
Iteration 181/1000 | Loss: 0.00000930
Iteration 182/1000 | Loss: 0.00000930
Iteration 183/1000 | Loss: 0.00000930
Iteration 184/1000 | Loss: 0.00000930
Iteration 185/1000 | Loss: 0.00000930
Iteration 186/1000 | Loss: 0.00000930
Iteration 187/1000 | Loss: 0.00000930
Iteration 188/1000 | Loss: 0.00000930
Iteration 189/1000 | Loss: 0.00000930
Iteration 190/1000 | Loss: 0.00000930
Iteration 191/1000 | Loss: 0.00000930
Iteration 192/1000 | Loss: 0.00000930
Iteration 193/1000 | Loss: 0.00000930
Iteration 194/1000 | Loss: 0.00000930
Iteration 195/1000 | Loss: 0.00000930
Iteration 196/1000 | Loss: 0.00000930
Iteration 197/1000 | Loss: 0.00000930
Iteration 198/1000 | Loss: 0.00000930
Iteration 199/1000 | Loss: 0.00000930
Iteration 200/1000 | Loss: 0.00000930
Iteration 201/1000 | Loss: 0.00000930
Iteration 202/1000 | Loss: 0.00000930
Iteration 203/1000 | Loss: 0.00000930
Iteration 204/1000 | Loss: 0.00000930
Iteration 205/1000 | Loss: 0.00000930
Iteration 206/1000 | Loss: 0.00000930
Iteration 207/1000 | Loss: 0.00000930
Iteration 208/1000 | Loss: 0.00000930
Iteration 209/1000 | Loss: 0.00000930
Iteration 210/1000 | Loss: 0.00000930
Iteration 211/1000 | Loss: 0.00000930
Iteration 212/1000 | Loss: 0.00000930
Iteration 213/1000 | Loss: 0.00000930
Iteration 214/1000 | Loss: 0.00000930
Iteration 215/1000 | Loss: 0.00000930
Iteration 216/1000 | Loss: 0.00000930
Iteration 217/1000 | Loss: 0.00000930
Iteration 218/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [9.295972631662153e-06, 9.295972631662153e-06, 9.295972631662153e-06, 9.295972631662153e-06, 9.295972631662153e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.295972631662153e-06

Optimization complete. Final v2v error: 2.6280927658081055 mm

Highest mean error: 3.011625051498413 mm for frame 56

Lowest mean error: 2.421137809753418 mm for frame 128

Saving results

Total time: 38.80225610733032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822497
Iteration 2/25 | Loss: 0.00148585
Iteration 3/25 | Loss: 0.00131338
Iteration 4/25 | Loss: 0.00128983
Iteration 5/25 | Loss: 0.00128401
Iteration 6/25 | Loss: 0.00128329
Iteration 7/25 | Loss: 0.00128329
Iteration 8/25 | Loss: 0.00128329
Iteration 9/25 | Loss: 0.00128329
Iteration 10/25 | Loss: 0.00128329
Iteration 11/25 | Loss: 0.00128329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012832891661673784, 0.0012832891661673784, 0.0012832891661673784, 0.0012832891661673784, 0.0012832891661673784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012832891661673784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28982615
Iteration 2/25 | Loss: 0.00160971
Iteration 3/25 | Loss: 0.00160965
Iteration 4/25 | Loss: 0.00160965
Iteration 5/25 | Loss: 0.00160965
Iteration 6/25 | Loss: 0.00160965
Iteration 7/25 | Loss: 0.00160965
Iteration 8/25 | Loss: 0.00160965
Iteration 9/25 | Loss: 0.00160965
Iteration 10/25 | Loss: 0.00160965
Iteration 11/25 | Loss: 0.00160965
Iteration 12/25 | Loss: 0.00160965
Iteration 13/25 | Loss: 0.00160965
Iteration 14/25 | Loss: 0.00160965
Iteration 15/25 | Loss: 0.00160965
Iteration 16/25 | Loss: 0.00160965
Iteration 17/25 | Loss: 0.00160965
Iteration 18/25 | Loss: 0.00160965
Iteration 19/25 | Loss: 0.00160965
Iteration 20/25 | Loss: 0.00160965
Iteration 21/25 | Loss: 0.00160965
Iteration 22/25 | Loss: 0.00160965
Iteration 23/25 | Loss: 0.00160965
Iteration 24/25 | Loss: 0.00160965
Iteration 25/25 | Loss: 0.00160965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160965
Iteration 2/1000 | Loss: 0.00005049
Iteration 3/1000 | Loss: 0.00003256
Iteration 4/1000 | Loss: 0.00002638
Iteration 5/1000 | Loss: 0.00002415
Iteration 6/1000 | Loss: 0.00002258
Iteration 7/1000 | Loss: 0.00002152
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001966
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001906
Iteration 13/1000 | Loss: 0.00001887
Iteration 14/1000 | Loss: 0.00001880
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001855
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001839
Iteration 21/1000 | Loss: 0.00001838
Iteration 22/1000 | Loss: 0.00001838
Iteration 23/1000 | Loss: 0.00001836
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001820
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001819
Iteration 33/1000 | Loss: 0.00001816
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001811
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001806
Iteration 48/1000 | Loss: 0.00001806
Iteration 49/1000 | Loss: 0.00001805
Iteration 50/1000 | Loss: 0.00001805
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001804
Iteration 53/1000 | Loss: 0.00001803
Iteration 54/1000 | Loss: 0.00001803
Iteration 55/1000 | Loss: 0.00001802
Iteration 56/1000 | Loss: 0.00001802
Iteration 57/1000 | Loss: 0.00001801
Iteration 58/1000 | Loss: 0.00001801
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001800
Iteration 61/1000 | Loss: 0.00001800
Iteration 62/1000 | Loss: 0.00001799
Iteration 63/1000 | Loss: 0.00001799
Iteration 64/1000 | Loss: 0.00001798
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001794
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00001794
Iteration 85/1000 | Loss: 0.00001794
Iteration 86/1000 | Loss: 0.00001794
Iteration 87/1000 | Loss: 0.00001794
Iteration 88/1000 | Loss: 0.00001794
Iteration 89/1000 | Loss: 0.00001794
Iteration 90/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.7941736587090418e-05, 1.7941736587090418e-05, 1.7941736587090418e-05, 1.7941736587090418e-05, 1.7941736587090418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7941736587090418e-05

Optimization complete. Final v2v error: 3.527386426925659 mm

Highest mean error: 4.0494842529296875 mm for frame 221

Lowest mean error: 3.2298624515533447 mm for frame 10

Saving results

Total time: 44.884981870651245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028598
Iteration 2/25 | Loss: 0.00151347
Iteration 3/25 | Loss: 0.00133406
Iteration 4/25 | Loss: 0.00131142
Iteration 5/25 | Loss: 0.00130375
Iteration 6/25 | Loss: 0.00130222
Iteration 7/25 | Loss: 0.00130222
Iteration 8/25 | Loss: 0.00130222
Iteration 9/25 | Loss: 0.00130222
Iteration 10/25 | Loss: 0.00130222
Iteration 11/25 | Loss: 0.00130222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013022164348512888, 0.0013022164348512888, 0.0013022164348512888, 0.0013022164348512888, 0.0013022164348512888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013022164348512888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73794359
Iteration 2/25 | Loss: 0.00147835
Iteration 3/25 | Loss: 0.00147826
Iteration 4/25 | Loss: 0.00147826
Iteration 5/25 | Loss: 0.00147826
Iteration 6/25 | Loss: 0.00147826
Iteration 7/25 | Loss: 0.00147826
Iteration 8/25 | Loss: 0.00147826
Iteration 9/25 | Loss: 0.00147826
Iteration 10/25 | Loss: 0.00147826
Iteration 11/25 | Loss: 0.00147826
Iteration 12/25 | Loss: 0.00147826
Iteration 13/25 | Loss: 0.00147826
Iteration 14/25 | Loss: 0.00147826
Iteration 15/25 | Loss: 0.00147826
Iteration 16/25 | Loss: 0.00147826
Iteration 17/25 | Loss: 0.00147826
Iteration 18/25 | Loss: 0.00147826
Iteration 19/25 | Loss: 0.00147826
Iteration 20/25 | Loss: 0.00147826
Iteration 21/25 | Loss: 0.00147826
Iteration 22/25 | Loss: 0.00147826
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014782593352720141, 0.0014782593352720141, 0.0014782593352720141, 0.0014782593352720141, 0.0014782593352720141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014782593352720141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147826
Iteration 2/1000 | Loss: 0.00007559
Iteration 3/1000 | Loss: 0.00004049
Iteration 4/1000 | Loss: 0.00003517
Iteration 5/1000 | Loss: 0.00003271
Iteration 6/1000 | Loss: 0.00003133
Iteration 7/1000 | Loss: 0.00003022
Iteration 8/1000 | Loss: 0.00002949
Iteration 9/1000 | Loss: 0.00002893
Iteration 10/1000 | Loss: 0.00002847
Iteration 11/1000 | Loss: 0.00002813
Iteration 12/1000 | Loss: 0.00002789
Iteration 13/1000 | Loss: 0.00002771
Iteration 14/1000 | Loss: 0.00002763
Iteration 15/1000 | Loss: 0.00002759
Iteration 16/1000 | Loss: 0.00002752
Iteration 17/1000 | Loss: 0.00002739
Iteration 18/1000 | Loss: 0.00002738
Iteration 19/1000 | Loss: 0.00002737
Iteration 20/1000 | Loss: 0.00002726
Iteration 21/1000 | Loss: 0.00002720
Iteration 22/1000 | Loss: 0.00002715
Iteration 23/1000 | Loss: 0.00002707
Iteration 24/1000 | Loss: 0.00002702
Iteration 25/1000 | Loss: 0.00002700
Iteration 26/1000 | Loss: 0.00002699
Iteration 27/1000 | Loss: 0.00002697
Iteration 28/1000 | Loss: 0.00002694
Iteration 29/1000 | Loss: 0.00002694
Iteration 30/1000 | Loss: 0.00002692
Iteration 31/1000 | Loss: 0.00002691
Iteration 32/1000 | Loss: 0.00002691
Iteration 33/1000 | Loss: 0.00002690
Iteration 34/1000 | Loss: 0.00002690
Iteration 35/1000 | Loss: 0.00002690
Iteration 36/1000 | Loss: 0.00002689
Iteration 37/1000 | Loss: 0.00002688
Iteration 38/1000 | Loss: 0.00002685
Iteration 39/1000 | Loss: 0.00002684
Iteration 40/1000 | Loss: 0.00002684
Iteration 41/1000 | Loss: 0.00002682
Iteration 42/1000 | Loss: 0.00002681
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002679
Iteration 45/1000 | Loss: 0.00002679
Iteration 46/1000 | Loss: 0.00002679
Iteration 47/1000 | Loss: 0.00002679
Iteration 48/1000 | Loss: 0.00002678
Iteration 49/1000 | Loss: 0.00002677
Iteration 50/1000 | Loss: 0.00002676
Iteration 51/1000 | Loss: 0.00002673
Iteration 52/1000 | Loss: 0.00002673
Iteration 53/1000 | Loss: 0.00002670
Iteration 54/1000 | Loss: 0.00002670
Iteration 55/1000 | Loss: 0.00002669
Iteration 56/1000 | Loss: 0.00002669
Iteration 57/1000 | Loss: 0.00002669
Iteration 58/1000 | Loss: 0.00002669
Iteration 59/1000 | Loss: 0.00002669
Iteration 60/1000 | Loss: 0.00002669
Iteration 61/1000 | Loss: 0.00002669
Iteration 62/1000 | Loss: 0.00002668
Iteration 63/1000 | Loss: 0.00002668
Iteration 64/1000 | Loss: 0.00002666
Iteration 65/1000 | Loss: 0.00002666
Iteration 66/1000 | Loss: 0.00002666
Iteration 67/1000 | Loss: 0.00002665
Iteration 68/1000 | Loss: 0.00002665
Iteration 69/1000 | Loss: 0.00002665
Iteration 70/1000 | Loss: 0.00002665
Iteration 71/1000 | Loss: 0.00002665
Iteration 72/1000 | Loss: 0.00002665
Iteration 73/1000 | Loss: 0.00002665
Iteration 74/1000 | Loss: 0.00002665
Iteration 75/1000 | Loss: 0.00002665
Iteration 76/1000 | Loss: 0.00002664
Iteration 77/1000 | Loss: 0.00002664
Iteration 78/1000 | Loss: 0.00002663
Iteration 79/1000 | Loss: 0.00002662
Iteration 80/1000 | Loss: 0.00002662
Iteration 81/1000 | Loss: 0.00002662
Iteration 82/1000 | Loss: 0.00002661
Iteration 83/1000 | Loss: 0.00002660
Iteration 84/1000 | Loss: 0.00002660
Iteration 85/1000 | Loss: 0.00002660
Iteration 86/1000 | Loss: 0.00002660
Iteration 87/1000 | Loss: 0.00002660
Iteration 88/1000 | Loss: 0.00002659
Iteration 89/1000 | Loss: 0.00002659
Iteration 90/1000 | Loss: 0.00002659
Iteration 91/1000 | Loss: 0.00002658
Iteration 92/1000 | Loss: 0.00002658
Iteration 93/1000 | Loss: 0.00002658
Iteration 94/1000 | Loss: 0.00002657
Iteration 95/1000 | Loss: 0.00002657
Iteration 96/1000 | Loss: 0.00002656
Iteration 97/1000 | Loss: 0.00002656
Iteration 98/1000 | Loss: 0.00002656
Iteration 99/1000 | Loss: 0.00002655
Iteration 100/1000 | Loss: 0.00002655
Iteration 101/1000 | Loss: 0.00002655
Iteration 102/1000 | Loss: 0.00002654
Iteration 103/1000 | Loss: 0.00002654
Iteration 104/1000 | Loss: 0.00002654
Iteration 105/1000 | Loss: 0.00002653
Iteration 106/1000 | Loss: 0.00002653
Iteration 107/1000 | Loss: 0.00002653
Iteration 108/1000 | Loss: 0.00002653
Iteration 109/1000 | Loss: 0.00002653
Iteration 110/1000 | Loss: 0.00002652
Iteration 111/1000 | Loss: 0.00002652
Iteration 112/1000 | Loss: 0.00002652
Iteration 113/1000 | Loss: 0.00002652
Iteration 114/1000 | Loss: 0.00002651
Iteration 115/1000 | Loss: 0.00002651
Iteration 116/1000 | Loss: 0.00002651
Iteration 117/1000 | Loss: 0.00002651
Iteration 118/1000 | Loss: 0.00002651
Iteration 119/1000 | Loss: 0.00002650
Iteration 120/1000 | Loss: 0.00002650
Iteration 121/1000 | Loss: 0.00002649
Iteration 122/1000 | Loss: 0.00002649
Iteration 123/1000 | Loss: 0.00002649
Iteration 124/1000 | Loss: 0.00002649
Iteration 125/1000 | Loss: 0.00002648
Iteration 126/1000 | Loss: 0.00002648
Iteration 127/1000 | Loss: 0.00002648
Iteration 128/1000 | Loss: 0.00002647
Iteration 129/1000 | Loss: 0.00002647
Iteration 130/1000 | Loss: 0.00002647
Iteration 131/1000 | Loss: 0.00002647
Iteration 132/1000 | Loss: 0.00002647
Iteration 133/1000 | Loss: 0.00002647
Iteration 134/1000 | Loss: 0.00002647
Iteration 135/1000 | Loss: 0.00002646
Iteration 136/1000 | Loss: 0.00002646
Iteration 137/1000 | Loss: 0.00002646
Iteration 138/1000 | Loss: 0.00002646
Iteration 139/1000 | Loss: 0.00002646
Iteration 140/1000 | Loss: 0.00002646
Iteration 141/1000 | Loss: 0.00002646
Iteration 142/1000 | Loss: 0.00002646
Iteration 143/1000 | Loss: 0.00002646
Iteration 144/1000 | Loss: 0.00002646
Iteration 145/1000 | Loss: 0.00002645
Iteration 146/1000 | Loss: 0.00002645
Iteration 147/1000 | Loss: 0.00002645
Iteration 148/1000 | Loss: 0.00002645
Iteration 149/1000 | Loss: 0.00002645
Iteration 150/1000 | Loss: 0.00002645
Iteration 151/1000 | Loss: 0.00002645
Iteration 152/1000 | Loss: 0.00002644
Iteration 153/1000 | Loss: 0.00002644
Iteration 154/1000 | Loss: 0.00002644
Iteration 155/1000 | Loss: 0.00002644
Iteration 156/1000 | Loss: 0.00002644
Iteration 157/1000 | Loss: 0.00002644
Iteration 158/1000 | Loss: 0.00002644
Iteration 159/1000 | Loss: 0.00002644
Iteration 160/1000 | Loss: 0.00002644
Iteration 161/1000 | Loss: 0.00002644
Iteration 162/1000 | Loss: 0.00002644
Iteration 163/1000 | Loss: 0.00002644
Iteration 164/1000 | Loss: 0.00002644
Iteration 165/1000 | Loss: 0.00002644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.644494452397339e-05, 2.644494452397339e-05, 2.644494452397339e-05, 2.644494452397339e-05, 2.644494452397339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.644494452397339e-05

Optimization complete. Final v2v error: 4.3032612800598145 mm

Highest mean error: 5.421933174133301 mm for frame 112

Lowest mean error: 3.6394712924957275 mm for frame 68

Saving results

Total time: 53.949586391448975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971713
Iteration 2/25 | Loss: 0.00288732
Iteration 3/25 | Loss: 0.00174676
Iteration 4/25 | Loss: 0.00164691
Iteration 5/25 | Loss: 0.00161065
Iteration 6/25 | Loss: 0.00152857
Iteration 7/25 | Loss: 0.00150498
Iteration 8/25 | Loss: 0.00132631
Iteration 9/25 | Loss: 0.00128398
Iteration 10/25 | Loss: 0.00127724
Iteration 11/25 | Loss: 0.00123664
Iteration 12/25 | Loss: 0.00122517
Iteration 13/25 | Loss: 0.00121357
Iteration 14/25 | Loss: 0.00120895
Iteration 15/25 | Loss: 0.00121077
Iteration 16/25 | Loss: 0.00120831
Iteration 17/25 | Loss: 0.00120812
Iteration 18/25 | Loss: 0.00120812
Iteration 19/25 | Loss: 0.00120812
Iteration 20/25 | Loss: 0.00120812
Iteration 21/25 | Loss: 0.00120812
Iteration 22/25 | Loss: 0.00120812
Iteration 23/25 | Loss: 0.00120812
Iteration 24/25 | Loss: 0.00120811
Iteration 25/25 | Loss: 0.00120811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31883049
Iteration 2/25 | Loss: 0.00145727
Iteration 3/25 | Loss: 0.00149193
Iteration 4/25 | Loss: 0.00235556
Iteration 5/25 | Loss: 0.00210580
Iteration 6/25 | Loss: 0.00121737
Iteration 7/25 | Loss: 0.00112178
Iteration 8/25 | Loss: 0.00112177
Iteration 9/25 | Loss: 0.00112177
Iteration 10/25 | Loss: 0.00112177
Iteration 11/25 | Loss: 0.00112177
Iteration 12/25 | Loss: 0.00112177
Iteration 13/25 | Loss: 0.00112177
Iteration 14/25 | Loss: 0.00112177
Iteration 15/25 | Loss: 0.00112177
Iteration 16/25 | Loss: 0.00112177
Iteration 17/25 | Loss: 0.00112177
Iteration 18/25 | Loss: 0.00112177
Iteration 19/25 | Loss: 0.00112177
Iteration 20/25 | Loss: 0.00112177
Iteration 21/25 | Loss: 0.00112177
Iteration 22/25 | Loss: 0.00112177
Iteration 23/25 | Loss: 0.00112177
Iteration 24/25 | Loss: 0.00112177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011217663995921612, 0.0011217663995921612, 0.0011217663995921612, 0.0011217663995921612, 0.0011217663995921612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011217663995921612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112177
Iteration 2/1000 | Loss: 0.00012274
Iteration 3/1000 | Loss: 0.00052665
Iteration 4/1000 | Loss: 0.00004505
Iteration 5/1000 | Loss: 0.00004882
Iteration 6/1000 | Loss: 0.00012626
Iteration 7/1000 | Loss: 0.00004499
Iteration 8/1000 | Loss: 0.00004853
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00006831
Iteration 11/1000 | Loss: 0.00017450
Iteration 12/1000 | Loss: 0.00083507
Iteration 13/1000 | Loss: 0.00027668
Iteration 14/1000 | Loss: 0.00014536
Iteration 15/1000 | Loss: 0.00016723
Iteration 16/1000 | Loss: 0.00004141
Iteration 17/1000 | Loss: 0.00004137
Iteration 18/1000 | Loss: 0.00002427
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00007812
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001442
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001440
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001436
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00002306
Iteration 93/1000 | Loss: 0.00012788
Iteration 94/1000 | Loss: 0.00041216
Iteration 95/1000 | Loss: 0.00007029
Iteration 96/1000 | Loss: 0.00026519
Iteration 97/1000 | Loss: 0.00002771
Iteration 98/1000 | Loss: 0.00013099
Iteration 99/1000 | Loss: 0.00003301
Iteration 100/1000 | Loss: 0.00017265
Iteration 101/1000 | Loss: 0.00004301
Iteration 102/1000 | Loss: 0.00001915
Iteration 103/1000 | Loss: 0.00016643
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00002428
Iteration 106/1000 | Loss: 0.00016924
Iteration 107/1000 | Loss: 0.00019639
Iteration 108/1000 | Loss: 0.00007596
Iteration 109/1000 | Loss: 0.00036693
Iteration 110/1000 | Loss: 0.00003863
Iteration 111/1000 | Loss: 0.00007255
Iteration 112/1000 | Loss: 0.00023715
Iteration 113/1000 | Loss: 0.00038404
Iteration 114/1000 | Loss: 0.00018462
Iteration 115/1000 | Loss: 0.00032418
Iteration 116/1000 | Loss: 0.00021951
Iteration 117/1000 | Loss: 0.00019045
Iteration 118/1000 | Loss: 0.00049360
Iteration 119/1000 | Loss: 0.00017373
Iteration 120/1000 | Loss: 0.00012270
Iteration 121/1000 | Loss: 0.00009995
Iteration 122/1000 | Loss: 0.00015367
Iteration 123/1000 | Loss: 0.00004604
Iteration 124/1000 | Loss: 0.00004585
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00008255
Iteration 127/1000 | Loss: 0.00004218
Iteration 128/1000 | Loss: 0.00002486
Iteration 129/1000 | Loss: 0.00001614
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001492
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00006691
Iteration 136/1000 | Loss: 0.00001773
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00002509
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001488
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001415
Iteration 148/1000 | Loss: 0.00001415
Iteration 149/1000 | Loss: 0.00001415
Iteration 150/1000 | Loss: 0.00001415
Iteration 151/1000 | Loss: 0.00001415
Iteration 152/1000 | Loss: 0.00001415
Iteration 153/1000 | Loss: 0.00001415
Iteration 154/1000 | Loss: 0.00001415
Iteration 155/1000 | Loss: 0.00001414
Iteration 156/1000 | Loss: 0.00001414
Iteration 157/1000 | Loss: 0.00001414
Iteration 158/1000 | Loss: 0.00001414
Iteration 159/1000 | Loss: 0.00001414
Iteration 160/1000 | Loss: 0.00001414
Iteration 161/1000 | Loss: 0.00001414
Iteration 162/1000 | Loss: 0.00001414
Iteration 163/1000 | Loss: 0.00001414
Iteration 164/1000 | Loss: 0.00001414
Iteration 165/1000 | Loss: 0.00001414
Iteration 166/1000 | Loss: 0.00001414
Iteration 167/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.414324560755631e-05, 1.414324560755631e-05, 1.414324560755631e-05, 1.414324560755631e-05, 1.414324560755631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.414324560755631e-05

Optimization complete. Final v2v error: 3.1706759929656982 mm

Highest mean error: 3.949906587600708 mm for frame 79

Lowest mean error: 2.8206260204315186 mm for frame 143

Saving results

Total time: 139.97924423217773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055199
Iteration 2/25 | Loss: 0.00343944
Iteration 3/25 | Loss: 0.00238402
Iteration 4/25 | Loss: 0.00168791
Iteration 5/25 | Loss: 0.00159270
Iteration 6/25 | Loss: 0.00177206
Iteration 7/25 | Loss: 0.00153512
Iteration 8/25 | Loss: 0.00147637
Iteration 9/25 | Loss: 0.00150772
Iteration 10/25 | Loss: 0.00150246
Iteration 11/25 | Loss: 0.00144335
Iteration 12/25 | Loss: 0.00141644
Iteration 13/25 | Loss: 0.00141078
Iteration 14/25 | Loss: 0.00140991
Iteration 15/25 | Loss: 0.00140716
Iteration 16/25 | Loss: 0.00140750
Iteration 17/25 | Loss: 0.00140503
Iteration 18/25 | Loss: 0.00140199
Iteration 19/25 | Loss: 0.00140131
Iteration 20/25 | Loss: 0.00140187
Iteration 21/25 | Loss: 0.00140160
Iteration 22/25 | Loss: 0.00140194
Iteration 23/25 | Loss: 0.00140204
Iteration 24/25 | Loss: 0.00140166
Iteration 25/25 | Loss: 0.00140257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92499363
Iteration 2/25 | Loss: 0.00149073
Iteration 3/25 | Loss: 0.00149073
Iteration 4/25 | Loss: 0.00149073
Iteration 5/25 | Loss: 0.00149073
Iteration 6/25 | Loss: 0.00149073
Iteration 7/25 | Loss: 0.00149073
Iteration 8/25 | Loss: 0.00149073
Iteration 9/25 | Loss: 0.00149073
Iteration 10/25 | Loss: 0.00149073
Iteration 11/25 | Loss: 0.00149073
Iteration 12/25 | Loss: 0.00149072
Iteration 13/25 | Loss: 0.00149072
Iteration 14/25 | Loss: 0.00149072
Iteration 15/25 | Loss: 0.00149072
Iteration 16/25 | Loss: 0.00149072
Iteration 17/25 | Loss: 0.00149072
Iteration 18/25 | Loss: 0.00149072
Iteration 19/25 | Loss: 0.00149072
Iteration 20/25 | Loss: 0.00149072
Iteration 21/25 | Loss: 0.00149072
Iteration 22/25 | Loss: 0.00149072
Iteration 23/25 | Loss: 0.00149072
Iteration 24/25 | Loss: 0.00149072
Iteration 25/25 | Loss: 0.00149072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149072
Iteration 2/1000 | Loss: 0.00010109
Iteration 3/1000 | Loss: 0.00007043
Iteration 4/1000 | Loss: 0.00006443
Iteration 5/1000 | Loss: 0.00006395
Iteration 6/1000 | Loss: 0.00004938
Iteration 7/1000 | Loss: 0.00005446
Iteration 8/1000 | Loss: 0.00004393
Iteration 9/1000 | Loss: 0.00004264
Iteration 10/1000 | Loss: 0.00005159
Iteration 11/1000 | Loss: 0.00004512
Iteration 12/1000 | Loss: 0.00004650
Iteration 13/1000 | Loss: 0.00004494
Iteration 14/1000 | Loss: 0.00004727
Iteration 15/1000 | Loss: 0.00005348
Iteration 16/1000 | Loss: 0.00005574
Iteration 17/1000 | Loss: 0.00004636
Iteration 18/1000 | Loss: 0.00004601
Iteration 19/1000 | Loss: 0.00041561
Iteration 20/1000 | Loss: 0.00017837
Iteration 21/1000 | Loss: 0.00005996
Iteration 22/1000 | Loss: 0.00004751
Iteration 23/1000 | Loss: 0.00005396
Iteration 24/1000 | Loss: 0.00005383
Iteration 25/1000 | Loss: 0.00005259
Iteration 26/1000 | Loss: 0.00004823
Iteration 27/1000 | Loss: 0.00005348
Iteration 28/1000 | Loss: 0.00145811
Iteration 29/1000 | Loss: 0.00061912
Iteration 30/1000 | Loss: 0.00005998
Iteration 31/1000 | Loss: 0.00004459
Iteration 32/1000 | Loss: 0.00003650
Iteration 33/1000 | Loss: 0.00004507
Iteration 34/1000 | Loss: 0.00003642
Iteration 35/1000 | Loss: 0.00004356
Iteration 36/1000 | Loss: 0.00003778
Iteration 37/1000 | Loss: 0.00004455
Iteration 38/1000 | Loss: 0.00003954
Iteration 39/1000 | Loss: 0.00003580
Iteration 40/1000 | Loss: 0.00003554
Iteration 41/1000 | Loss: 0.00004061
Iteration 42/1000 | Loss: 0.00003496
Iteration 43/1000 | Loss: 0.00003653
Iteration 44/1000 | Loss: 0.00003802
Iteration 45/1000 | Loss: 0.00004416
Iteration 46/1000 | Loss: 0.00004022
Iteration 47/1000 | Loss: 0.00003901
Iteration 48/1000 | Loss: 0.00003181
Iteration 49/1000 | Loss: 0.00003803
Iteration 50/1000 | Loss: 0.00003222
Iteration 51/1000 | Loss: 0.00003965
Iteration 52/1000 | Loss: 0.00003023
Iteration 53/1000 | Loss: 0.00004196
Iteration 54/1000 | Loss: 0.00003555
Iteration 55/1000 | Loss: 0.00003873
Iteration 56/1000 | Loss: 0.00004779
Iteration 57/1000 | Loss: 0.00003045
Iteration 58/1000 | Loss: 0.00002804
Iteration 59/1000 | Loss: 0.00002662
Iteration 60/1000 | Loss: 0.00002598
Iteration 61/1000 | Loss: 0.00002532
Iteration 62/1000 | Loss: 0.00002481
Iteration 63/1000 | Loss: 0.00002437
Iteration 64/1000 | Loss: 0.00002416
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002396
Iteration 67/1000 | Loss: 0.00002392
Iteration 68/1000 | Loss: 0.00002391
Iteration 69/1000 | Loss: 0.00002378
Iteration 70/1000 | Loss: 0.00002378
Iteration 71/1000 | Loss: 0.00002369
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002366
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002364
Iteration 76/1000 | Loss: 0.00002363
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002360
Iteration 81/1000 | Loss: 0.00002360
Iteration 82/1000 | Loss: 0.00002360
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002359
Iteration 93/1000 | Loss: 0.00002359
Iteration 94/1000 | Loss: 0.00002358
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002358
Iteration 97/1000 | Loss: 0.00002358
Iteration 98/1000 | Loss: 0.00002358
Iteration 99/1000 | Loss: 0.00002358
Iteration 100/1000 | Loss: 0.00002358
Iteration 101/1000 | Loss: 0.00002358
Iteration 102/1000 | Loss: 0.00002358
Iteration 103/1000 | Loss: 0.00002358
Iteration 104/1000 | Loss: 0.00002358
Iteration 105/1000 | Loss: 0.00002358
Iteration 106/1000 | Loss: 0.00002358
Iteration 107/1000 | Loss: 0.00002358
Iteration 108/1000 | Loss: 0.00002358
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002357
Iteration 111/1000 | Loss: 0.00002357
Iteration 112/1000 | Loss: 0.00002357
Iteration 113/1000 | Loss: 0.00002357
Iteration 114/1000 | Loss: 0.00002357
Iteration 115/1000 | Loss: 0.00002357
Iteration 116/1000 | Loss: 0.00002357
Iteration 117/1000 | Loss: 0.00002357
Iteration 118/1000 | Loss: 0.00002357
Iteration 119/1000 | Loss: 0.00002356
Iteration 120/1000 | Loss: 0.00002356
Iteration 121/1000 | Loss: 0.00002356
Iteration 122/1000 | Loss: 0.00002356
Iteration 123/1000 | Loss: 0.00002356
Iteration 124/1000 | Loss: 0.00002356
Iteration 125/1000 | Loss: 0.00002356
Iteration 126/1000 | Loss: 0.00002356
Iteration 127/1000 | Loss: 0.00002356
Iteration 128/1000 | Loss: 0.00002355
Iteration 129/1000 | Loss: 0.00002355
Iteration 130/1000 | Loss: 0.00002355
Iteration 131/1000 | Loss: 0.00002355
Iteration 132/1000 | Loss: 0.00002355
Iteration 133/1000 | Loss: 0.00002355
Iteration 134/1000 | Loss: 0.00002355
Iteration 135/1000 | Loss: 0.00002355
Iteration 136/1000 | Loss: 0.00002355
Iteration 137/1000 | Loss: 0.00002355
Iteration 138/1000 | Loss: 0.00002354
Iteration 139/1000 | Loss: 0.00002354
Iteration 140/1000 | Loss: 0.00002353
Iteration 141/1000 | Loss: 0.00002353
Iteration 142/1000 | Loss: 0.00002353
Iteration 143/1000 | Loss: 0.00002353
Iteration 144/1000 | Loss: 0.00002353
Iteration 145/1000 | Loss: 0.00002352
Iteration 146/1000 | Loss: 0.00002352
Iteration 147/1000 | Loss: 0.00002352
Iteration 148/1000 | Loss: 0.00002352
Iteration 149/1000 | Loss: 0.00002352
Iteration 150/1000 | Loss: 0.00002352
Iteration 151/1000 | Loss: 0.00002352
Iteration 152/1000 | Loss: 0.00002352
Iteration 153/1000 | Loss: 0.00002352
Iteration 154/1000 | Loss: 0.00002352
Iteration 155/1000 | Loss: 0.00002352
Iteration 156/1000 | Loss: 0.00002351
Iteration 157/1000 | Loss: 0.00002351
Iteration 158/1000 | Loss: 0.00002351
Iteration 159/1000 | Loss: 0.00002351
Iteration 160/1000 | Loss: 0.00002351
Iteration 161/1000 | Loss: 0.00002351
Iteration 162/1000 | Loss: 0.00002351
Iteration 163/1000 | Loss: 0.00002351
Iteration 164/1000 | Loss: 0.00002351
Iteration 165/1000 | Loss: 0.00002351
Iteration 166/1000 | Loss: 0.00002351
Iteration 167/1000 | Loss: 0.00002351
Iteration 168/1000 | Loss: 0.00002351
Iteration 169/1000 | Loss: 0.00002350
Iteration 170/1000 | Loss: 0.00002350
Iteration 171/1000 | Loss: 0.00002350
Iteration 172/1000 | Loss: 0.00002350
Iteration 173/1000 | Loss: 0.00002350
Iteration 174/1000 | Loss: 0.00002350
Iteration 175/1000 | Loss: 0.00002350
Iteration 176/1000 | Loss: 0.00002350
Iteration 177/1000 | Loss: 0.00002350
Iteration 178/1000 | Loss: 0.00002350
Iteration 179/1000 | Loss: 0.00002350
Iteration 180/1000 | Loss: 0.00002350
Iteration 181/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.350287468289025e-05, 2.350287468289025e-05, 2.350287468289025e-05, 2.350287468289025e-05, 2.350287468289025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.350287468289025e-05

Optimization complete. Final v2v error: 4.019964218139648 mm

Highest mean error: 4.879435062408447 mm for frame 139

Lowest mean error: 3.3526594638824463 mm for frame 26

Saving results

Total time: 155.0888602733612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365517
Iteration 2/25 | Loss: 0.00121998
Iteration 3/25 | Loss: 0.00116272
Iteration 4/25 | Loss: 0.00115380
Iteration 5/25 | Loss: 0.00115064
Iteration 6/25 | Loss: 0.00114990
Iteration 7/25 | Loss: 0.00114990
Iteration 8/25 | Loss: 0.00114990
Iteration 9/25 | Loss: 0.00114990
Iteration 10/25 | Loss: 0.00114990
Iteration 11/25 | Loss: 0.00114990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011499017709866166, 0.0011499017709866166, 0.0011499017709866166, 0.0011499017709866166, 0.0011499017709866166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011499017709866166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41324973
Iteration 2/25 | Loss: 0.00118233
Iteration 3/25 | Loss: 0.00118233
Iteration 4/25 | Loss: 0.00118233
Iteration 5/25 | Loss: 0.00118233
Iteration 6/25 | Loss: 0.00118233
Iteration 7/25 | Loss: 0.00118233
Iteration 8/25 | Loss: 0.00118233
Iteration 9/25 | Loss: 0.00118233
Iteration 10/25 | Loss: 0.00118233
Iteration 11/25 | Loss: 0.00118233
Iteration 12/25 | Loss: 0.00118233
Iteration 13/25 | Loss: 0.00118233
Iteration 14/25 | Loss: 0.00118233
Iteration 15/25 | Loss: 0.00118233
Iteration 16/25 | Loss: 0.00118233
Iteration 17/25 | Loss: 0.00118233
Iteration 18/25 | Loss: 0.00118233
Iteration 19/25 | Loss: 0.00118233
Iteration 20/25 | Loss: 0.00118233
Iteration 21/25 | Loss: 0.00118233
Iteration 22/25 | Loss: 0.00118233
Iteration 23/25 | Loss: 0.00118233
Iteration 24/25 | Loss: 0.00118233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011823257664218545, 0.0011823257664218545, 0.0011823257664218545, 0.0011823257664218545, 0.0011823257664218545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011823257664218545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118233
Iteration 2/1000 | Loss: 0.00001699
Iteration 3/1000 | Loss: 0.00001221
Iteration 4/1000 | Loss: 0.00001109
Iteration 5/1000 | Loss: 0.00001043
Iteration 6/1000 | Loss: 0.00000986
Iteration 7/1000 | Loss: 0.00000950
Iteration 8/1000 | Loss: 0.00000945
Iteration 9/1000 | Loss: 0.00000944
Iteration 10/1000 | Loss: 0.00000943
Iteration 11/1000 | Loss: 0.00000919
Iteration 12/1000 | Loss: 0.00000908
Iteration 13/1000 | Loss: 0.00000897
Iteration 14/1000 | Loss: 0.00000896
Iteration 15/1000 | Loss: 0.00000895
Iteration 16/1000 | Loss: 0.00000894
Iteration 17/1000 | Loss: 0.00000893
Iteration 18/1000 | Loss: 0.00000888
Iteration 19/1000 | Loss: 0.00000886
Iteration 20/1000 | Loss: 0.00000879
Iteration 21/1000 | Loss: 0.00000879
Iteration 22/1000 | Loss: 0.00000877
Iteration 23/1000 | Loss: 0.00000873
Iteration 24/1000 | Loss: 0.00000872
Iteration 25/1000 | Loss: 0.00000872
Iteration 26/1000 | Loss: 0.00000871
Iteration 27/1000 | Loss: 0.00000870
Iteration 28/1000 | Loss: 0.00000870
Iteration 29/1000 | Loss: 0.00000869
Iteration 30/1000 | Loss: 0.00000869
Iteration 31/1000 | Loss: 0.00000868
Iteration 32/1000 | Loss: 0.00000868
Iteration 33/1000 | Loss: 0.00000868
Iteration 34/1000 | Loss: 0.00000867
Iteration 35/1000 | Loss: 0.00000867
Iteration 36/1000 | Loss: 0.00000867
Iteration 37/1000 | Loss: 0.00000867
Iteration 38/1000 | Loss: 0.00000866
Iteration 39/1000 | Loss: 0.00000865
Iteration 40/1000 | Loss: 0.00000865
Iteration 41/1000 | Loss: 0.00000865
Iteration 42/1000 | Loss: 0.00000864
Iteration 43/1000 | Loss: 0.00000864
Iteration 44/1000 | Loss: 0.00000864
Iteration 45/1000 | Loss: 0.00000864
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000863
Iteration 49/1000 | Loss: 0.00000863
Iteration 50/1000 | Loss: 0.00000863
Iteration 51/1000 | Loss: 0.00000863
Iteration 52/1000 | Loss: 0.00000863
Iteration 53/1000 | Loss: 0.00000863
Iteration 54/1000 | Loss: 0.00000863
Iteration 55/1000 | Loss: 0.00000862
Iteration 56/1000 | Loss: 0.00000862
Iteration 57/1000 | Loss: 0.00000862
Iteration 58/1000 | Loss: 0.00000861
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000859
Iteration 61/1000 | Loss: 0.00000858
Iteration 62/1000 | Loss: 0.00000858
Iteration 63/1000 | Loss: 0.00000857
Iteration 64/1000 | Loss: 0.00000857
Iteration 65/1000 | Loss: 0.00000856
Iteration 66/1000 | Loss: 0.00000854
Iteration 67/1000 | Loss: 0.00000854
Iteration 68/1000 | Loss: 0.00000854
Iteration 69/1000 | Loss: 0.00000854
Iteration 70/1000 | Loss: 0.00000853
Iteration 71/1000 | Loss: 0.00000853
Iteration 72/1000 | Loss: 0.00000853
Iteration 73/1000 | Loss: 0.00000853
Iteration 74/1000 | Loss: 0.00000853
Iteration 75/1000 | Loss: 0.00000853
Iteration 76/1000 | Loss: 0.00000853
Iteration 77/1000 | Loss: 0.00000853
Iteration 78/1000 | Loss: 0.00000852
Iteration 79/1000 | Loss: 0.00000852
Iteration 80/1000 | Loss: 0.00000850
Iteration 81/1000 | Loss: 0.00000850
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000848
Iteration 84/1000 | Loss: 0.00000848
Iteration 85/1000 | Loss: 0.00000848
Iteration 86/1000 | Loss: 0.00000848
Iteration 87/1000 | Loss: 0.00000848
Iteration 88/1000 | Loss: 0.00000848
Iteration 89/1000 | Loss: 0.00000848
Iteration 90/1000 | Loss: 0.00000848
Iteration 91/1000 | Loss: 0.00000848
Iteration 92/1000 | Loss: 0.00000847
Iteration 93/1000 | Loss: 0.00000846
Iteration 94/1000 | Loss: 0.00000845
Iteration 95/1000 | Loss: 0.00000845
Iteration 96/1000 | Loss: 0.00000845
Iteration 97/1000 | Loss: 0.00000845
Iteration 98/1000 | Loss: 0.00000845
Iteration 99/1000 | Loss: 0.00000845
Iteration 100/1000 | Loss: 0.00000845
Iteration 101/1000 | Loss: 0.00000845
Iteration 102/1000 | Loss: 0.00000845
Iteration 103/1000 | Loss: 0.00000844
Iteration 104/1000 | Loss: 0.00000844
Iteration 105/1000 | Loss: 0.00000844
Iteration 106/1000 | Loss: 0.00000843
Iteration 107/1000 | Loss: 0.00000843
Iteration 108/1000 | Loss: 0.00000843
Iteration 109/1000 | Loss: 0.00000842
Iteration 110/1000 | Loss: 0.00000841
Iteration 111/1000 | Loss: 0.00000841
Iteration 112/1000 | Loss: 0.00000841
Iteration 113/1000 | Loss: 0.00000841
Iteration 114/1000 | Loss: 0.00000841
Iteration 115/1000 | Loss: 0.00000840
Iteration 116/1000 | Loss: 0.00000840
Iteration 117/1000 | Loss: 0.00000840
Iteration 118/1000 | Loss: 0.00000839
Iteration 119/1000 | Loss: 0.00000839
Iteration 120/1000 | Loss: 0.00000839
Iteration 121/1000 | Loss: 0.00000839
Iteration 122/1000 | Loss: 0.00000839
Iteration 123/1000 | Loss: 0.00000838
Iteration 124/1000 | Loss: 0.00000838
Iteration 125/1000 | Loss: 0.00000838
Iteration 126/1000 | Loss: 0.00000838
Iteration 127/1000 | Loss: 0.00000837
Iteration 128/1000 | Loss: 0.00000837
Iteration 129/1000 | Loss: 0.00000837
Iteration 130/1000 | Loss: 0.00000837
Iteration 131/1000 | Loss: 0.00000837
Iteration 132/1000 | Loss: 0.00000837
Iteration 133/1000 | Loss: 0.00000837
Iteration 134/1000 | Loss: 0.00000836
Iteration 135/1000 | Loss: 0.00000836
Iteration 136/1000 | Loss: 0.00000836
Iteration 137/1000 | Loss: 0.00000836
Iteration 138/1000 | Loss: 0.00000836
Iteration 139/1000 | Loss: 0.00000836
Iteration 140/1000 | Loss: 0.00000836
Iteration 141/1000 | Loss: 0.00000836
Iteration 142/1000 | Loss: 0.00000836
Iteration 143/1000 | Loss: 0.00000836
Iteration 144/1000 | Loss: 0.00000835
Iteration 145/1000 | Loss: 0.00000835
Iteration 146/1000 | Loss: 0.00000835
Iteration 147/1000 | Loss: 0.00000835
Iteration 148/1000 | Loss: 0.00000834
Iteration 149/1000 | Loss: 0.00000834
Iteration 150/1000 | Loss: 0.00000834
Iteration 151/1000 | Loss: 0.00000833
Iteration 152/1000 | Loss: 0.00000833
Iteration 153/1000 | Loss: 0.00000833
Iteration 154/1000 | Loss: 0.00000833
Iteration 155/1000 | Loss: 0.00000833
Iteration 156/1000 | Loss: 0.00000833
Iteration 157/1000 | Loss: 0.00000833
Iteration 158/1000 | Loss: 0.00000833
Iteration 159/1000 | Loss: 0.00000833
Iteration 160/1000 | Loss: 0.00000833
Iteration 161/1000 | Loss: 0.00000832
Iteration 162/1000 | Loss: 0.00000832
Iteration 163/1000 | Loss: 0.00000832
Iteration 164/1000 | Loss: 0.00000832
Iteration 165/1000 | Loss: 0.00000832
Iteration 166/1000 | Loss: 0.00000832
Iteration 167/1000 | Loss: 0.00000831
Iteration 168/1000 | Loss: 0.00000831
Iteration 169/1000 | Loss: 0.00000831
Iteration 170/1000 | Loss: 0.00000831
Iteration 171/1000 | Loss: 0.00000831
Iteration 172/1000 | Loss: 0.00000831
Iteration 173/1000 | Loss: 0.00000831
Iteration 174/1000 | Loss: 0.00000831
Iteration 175/1000 | Loss: 0.00000831
Iteration 176/1000 | Loss: 0.00000831
Iteration 177/1000 | Loss: 0.00000831
Iteration 178/1000 | Loss: 0.00000831
Iteration 179/1000 | Loss: 0.00000831
Iteration 180/1000 | Loss: 0.00000831
Iteration 181/1000 | Loss: 0.00000831
Iteration 182/1000 | Loss: 0.00000831
Iteration 183/1000 | Loss: 0.00000831
Iteration 184/1000 | Loss: 0.00000831
Iteration 185/1000 | Loss: 0.00000831
Iteration 186/1000 | Loss: 0.00000831
Iteration 187/1000 | Loss: 0.00000830
Iteration 188/1000 | Loss: 0.00000830
Iteration 189/1000 | Loss: 0.00000830
Iteration 190/1000 | Loss: 0.00000830
Iteration 191/1000 | Loss: 0.00000830
Iteration 192/1000 | Loss: 0.00000830
Iteration 193/1000 | Loss: 0.00000830
Iteration 194/1000 | Loss: 0.00000830
Iteration 195/1000 | Loss: 0.00000830
Iteration 196/1000 | Loss: 0.00000830
Iteration 197/1000 | Loss: 0.00000830
Iteration 198/1000 | Loss: 0.00000830
Iteration 199/1000 | Loss: 0.00000830
Iteration 200/1000 | Loss: 0.00000830
Iteration 201/1000 | Loss: 0.00000830
Iteration 202/1000 | Loss: 0.00000830
Iteration 203/1000 | Loss: 0.00000830
Iteration 204/1000 | Loss: 0.00000829
Iteration 205/1000 | Loss: 0.00000829
Iteration 206/1000 | Loss: 0.00000829
Iteration 207/1000 | Loss: 0.00000829
Iteration 208/1000 | Loss: 0.00000829
Iteration 209/1000 | Loss: 0.00000829
Iteration 210/1000 | Loss: 0.00000829
Iteration 211/1000 | Loss: 0.00000829
Iteration 212/1000 | Loss: 0.00000829
Iteration 213/1000 | Loss: 0.00000829
Iteration 214/1000 | Loss: 0.00000829
Iteration 215/1000 | Loss: 0.00000829
Iteration 216/1000 | Loss: 0.00000829
Iteration 217/1000 | Loss: 0.00000829
Iteration 218/1000 | Loss: 0.00000829
Iteration 219/1000 | Loss: 0.00000829
Iteration 220/1000 | Loss: 0.00000829
Iteration 221/1000 | Loss: 0.00000829
Iteration 222/1000 | Loss: 0.00000829
Iteration 223/1000 | Loss: 0.00000829
Iteration 224/1000 | Loss: 0.00000829
Iteration 225/1000 | Loss: 0.00000829
Iteration 226/1000 | Loss: 0.00000829
Iteration 227/1000 | Loss: 0.00000829
Iteration 228/1000 | Loss: 0.00000828
Iteration 229/1000 | Loss: 0.00000828
Iteration 230/1000 | Loss: 0.00000828
Iteration 231/1000 | Loss: 0.00000828
Iteration 232/1000 | Loss: 0.00000828
Iteration 233/1000 | Loss: 0.00000828
Iteration 234/1000 | Loss: 0.00000828
Iteration 235/1000 | Loss: 0.00000828
Iteration 236/1000 | Loss: 0.00000828
Iteration 237/1000 | Loss: 0.00000828
Iteration 238/1000 | Loss: 0.00000828
Iteration 239/1000 | Loss: 0.00000828
Iteration 240/1000 | Loss: 0.00000827
Iteration 241/1000 | Loss: 0.00000827
Iteration 242/1000 | Loss: 0.00000827
Iteration 243/1000 | Loss: 0.00000827
Iteration 244/1000 | Loss: 0.00000827
Iteration 245/1000 | Loss: 0.00000827
Iteration 246/1000 | Loss: 0.00000827
Iteration 247/1000 | Loss: 0.00000827
Iteration 248/1000 | Loss: 0.00000827
Iteration 249/1000 | Loss: 0.00000827
Iteration 250/1000 | Loss: 0.00000827
Iteration 251/1000 | Loss: 0.00000827
Iteration 252/1000 | Loss: 0.00000827
Iteration 253/1000 | Loss: 0.00000827
Iteration 254/1000 | Loss: 0.00000827
Iteration 255/1000 | Loss: 0.00000827
Iteration 256/1000 | Loss: 0.00000826
Iteration 257/1000 | Loss: 0.00000826
Iteration 258/1000 | Loss: 0.00000826
Iteration 259/1000 | Loss: 0.00000826
Iteration 260/1000 | Loss: 0.00000826
Iteration 261/1000 | Loss: 0.00000826
Iteration 262/1000 | Loss: 0.00000826
Iteration 263/1000 | Loss: 0.00000826
Iteration 264/1000 | Loss: 0.00000825
Iteration 265/1000 | Loss: 0.00000825
Iteration 266/1000 | Loss: 0.00000825
Iteration 267/1000 | Loss: 0.00000825
Iteration 268/1000 | Loss: 0.00000825
Iteration 269/1000 | Loss: 0.00000825
Iteration 270/1000 | Loss: 0.00000825
Iteration 271/1000 | Loss: 0.00000825
Iteration 272/1000 | Loss: 0.00000825
Iteration 273/1000 | Loss: 0.00000825
Iteration 274/1000 | Loss: 0.00000825
Iteration 275/1000 | Loss: 0.00000825
Iteration 276/1000 | Loss: 0.00000824
Iteration 277/1000 | Loss: 0.00000824
Iteration 278/1000 | Loss: 0.00000824
Iteration 279/1000 | Loss: 0.00000824
Iteration 280/1000 | Loss: 0.00000824
Iteration 281/1000 | Loss: 0.00000824
Iteration 282/1000 | Loss: 0.00000824
Iteration 283/1000 | Loss: 0.00000824
Iteration 284/1000 | Loss: 0.00000824
Iteration 285/1000 | Loss: 0.00000824
Iteration 286/1000 | Loss: 0.00000823
Iteration 287/1000 | Loss: 0.00000823
Iteration 288/1000 | Loss: 0.00000823
Iteration 289/1000 | Loss: 0.00000823
Iteration 290/1000 | Loss: 0.00000823
Iteration 291/1000 | Loss: 0.00000823
Iteration 292/1000 | Loss: 0.00000823
Iteration 293/1000 | Loss: 0.00000823
Iteration 294/1000 | Loss: 0.00000823
Iteration 295/1000 | Loss: 0.00000823
Iteration 296/1000 | Loss: 0.00000823
Iteration 297/1000 | Loss: 0.00000823
Iteration 298/1000 | Loss: 0.00000823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [8.233801054302603e-06, 8.233801054302603e-06, 8.233801054302603e-06, 8.233801054302603e-06, 8.233801054302603e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.233801054302603e-06

Optimization complete. Final v2v error: 2.4859724044799805 mm

Highest mean error: 2.648845911026001 mm for frame 109

Lowest mean error: 2.4319889545440674 mm for frame 32

Saving results

Total time: 43.43526029586792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406235
Iteration 2/25 | Loss: 0.00137747
Iteration 3/25 | Loss: 0.00122510
Iteration 4/25 | Loss: 0.00121208
Iteration 5/25 | Loss: 0.00121021
Iteration 6/25 | Loss: 0.00120970
Iteration 7/25 | Loss: 0.00120970
Iteration 8/25 | Loss: 0.00120970
Iteration 9/25 | Loss: 0.00120970
Iteration 10/25 | Loss: 0.00120970
Iteration 11/25 | Loss: 0.00120970
Iteration 12/25 | Loss: 0.00120970
Iteration 13/25 | Loss: 0.00120970
Iteration 14/25 | Loss: 0.00120970
Iteration 15/25 | Loss: 0.00120970
Iteration 16/25 | Loss: 0.00120970
Iteration 17/25 | Loss: 0.00120970
Iteration 18/25 | Loss: 0.00120970
Iteration 19/25 | Loss: 0.00120970
Iteration 20/25 | Loss: 0.00120970
Iteration 21/25 | Loss: 0.00120970
Iteration 22/25 | Loss: 0.00120970
Iteration 23/25 | Loss: 0.00120970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012096958234906197, 0.0012096958234906197, 0.0012096958234906197, 0.0012096958234906197, 0.0012096958234906197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012096958234906197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31566060
Iteration 2/25 | Loss: 0.00094676
Iteration 3/25 | Loss: 0.00094676
Iteration 4/25 | Loss: 0.00094676
Iteration 5/25 | Loss: 0.00094676
Iteration 6/25 | Loss: 0.00094676
Iteration 7/25 | Loss: 0.00094676
Iteration 8/25 | Loss: 0.00094676
Iteration 9/25 | Loss: 0.00094676
Iteration 10/25 | Loss: 0.00094676
Iteration 11/25 | Loss: 0.00094676
Iteration 12/25 | Loss: 0.00094676
Iteration 13/25 | Loss: 0.00094676
Iteration 14/25 | Loss: 0.00094676
Iteration 15/25 | Loss: 0.00094676
Iteration 16/25 | Loss: 0.00094676
Iteration 17/25 | Loss: 0.00094676
Iteration 18/25 | Loss: 0.00094676
Iteration 19/25 | Loss: 0.00094676
Iteration 20/25 | Loss: 0.00094676
Iteration 21/25 | Loss: 0.00094676
Iteration 22/25 | Loss: 0.00094676
Iteration 23/25 | Loss: 0.00094676
Iteration 24/25 | Loss: 0.00094676
Iteration 25/25 | Loss: 0.00094676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094676
Iteration 2/1000 | Loss: 0.00002424
Iteration 3/1000 | Loss: 0.00001784
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00001520
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001368
Iteration 9/1000 | Loss: 0.00001339
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001305
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001272
Iteration 30/1000 | Loss: 0.00001272
Iteration 31/1000 | Loss: 0.00001272
Iteration 32/1000 | Loss: 0.00001271
Iteration 33/1000 | Loss: 0.00001271
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001266
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001260
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001255
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001251
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001246
Iteration 88/1000 | Loss: 0.00001246
Iteration 89/1000 | Loss: 0.00001245
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001225
Iteration 114/1000 | Loss: 0.00001225
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001224
Iteration 119/1000 | Loss: 0.00001224
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001223
Iteration 128/1000 | Loss: 0.00001223
Iteration 129/1000 | Loss: 0.00001223
Iteration 130/1000 | Loss: 0.00001223
Iteration 131/1000 | Loss: 0.00001223
Iteration 132/1000 | Loss: 0.00001223
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001223
Iteration 136/1000 | Loss: 0.00001223
Iteration 137/1000 | Loss: 0.00001223
Iteration 138/1000 | Loss: 0.00001223
Iteration 139/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.223109029524494e-05, 1.223109029524494e-05, 1.223109029524494e-05, 1.223109029524494e-05, 1.223109029524494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.223109029524494e-05

Optimization complete. Final v2v error: 3.019758462905884 mm

Highest mean error: 3.1100926399230957 mm for frame 24

Lowest mean error: 2.914640188217163 mm for frame 120

Saving results

Total time: 37.81848883628845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065628
Iteration 2/25 | Loss: 0.00315025
Iteration 3/25 | Loss: 0.00234961
Iteration 4/25 | Loss: 0.00215996
Iteration 5/25 | Loss: 0.00210720
Iteration 6/25 | Loss: 0.00166851
Iteration 7/25 | Loss: 0.00156910
Iteration 8/25 | Loss: 0.00167232
Iteration 9/25 | Loss: 0.00143056
Iteration 10/25 | Loss: 0.00138265
Iteration 11/25 | Loss: 0.00130799
Iteration 12/25 | Loss: 0.00129826
Iteration 13/25 | Loss: 0.00133269
Iteration 14/25 | Loss: 0.00128035
Iteration 15/25 | Loss: 0.00128198
Iteration 16/25 | Loss: 0.00127765
Iteration 17/25 | Loss: 0.00127521
Iteration 18/25 | Loss: 0.00127447
Iteration 19/25 | Loss: 0.00127429
Iteration 20/25 | Loss: 0.00127426
Iteration 21/25 | Loss: 0.00127426
Iteration 22/25 | Loss: 0.00127426
Iteration 23/25 | Loss: 0.00127426
Iteration 24/25 | Loss: 0.00127426
Iteration 25/25 | Loss: 0.00127426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45059025
Iteration 2/25 | Loss: 0.00134138
Iteration 3/25 | Loss: 0.00134137
Iteration 4/25 | Loss: 0.00134137
Iteration 5/25 | Loss: 0.00134137
Iteration 6/25 | Loss: 0.00134137
Iteration 7/25 | Loss: 0.00134137
Iteration 8/25 | Loss: 0.00134137
Iteration 9/25 | Loss: 0.00134137
Iteration 10/25 | Loss: 0.00134137
Iteration 11/25 | Loss: 0.00134137
Iteration 12/25 | Loss: 0.00134137
Iteration 13/25 | Loss: 0.00134137
Iteration 14/25 | Loss: 0.00134137
Iteration 15/25 | Loss: 0.00134137
Iteration 16/25 | Loss: 0.00134137
Iteration 17/25 | Loss: 0.00134137
Iteration 18/25 | Loss: 0.00134137
Iteration 19/25 | Loss: 0.00134137
Iteration 20/25 | Loss: 0.00134137
Iteration 21/25 | Loss: 0.00134137
Iteration 22/25 | Loss: 0.00134137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013413698179647326, 0.0013413698179647326, 0.0013413698179647326, 0.0013413698179647326, 0.0013413698179647326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013413698179647326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134137
Iteration 2/1000 | Loss: 0.00049317
Iteration 3/1000 | Loss: 0.00485756
Iteration 4/1000 | Loss: 0.00315392
Iteration 5/1000 | Loss: 0.00322330
Iteration 6/1000 | Loss: 0.00410884
Iteration 7/1000 | Loss: 0.00007541
Iteration 8/1000 | Loss: 0.00394611
Iteration 9/1000 | Loss: 0.00170833
Iteration 10/1000 | Loss: 0.00128519
Iteration 11/1000 | Loss: 0.00028423
Iteration 12/1000 | Loss: 0.00027803
Iteration 13/1000 | Loss: 0.00025821
Iteration 14/1000 | Loss: 0.00017629
Iteration 15/1000 | Loss: 0.00017110
Iteration 16/1000 | Loss: 0.00017354
Iteration 17/1000 | Loss: 0.00004570
Iteration 18/1000 | Loss: 0.00003889
Iteration 19/1000 | Loss: 0.00003533
Iteration 20/1000 | Loss: 0.00022173
Iteration 21/1000 | Loss: 0.00092960
Iteration 22/1000 | Loss: 0.00116260
Iteration 23/1000 | Loss: 0.00134590
Iteration 24/1000 | Loss: 0.00112506
Iteration 25/1000 | Loss: 0.00089494
Iteration 26/1000 | Loss: 0.00119867
Iteration 27/1000 | Loss: 0.00118172
Iteration 28/1000 | Loss: 0.00140984
Iteration 29/1000 | Loss: 0.00059191
Iteration 30/1000 | Loss: 0.00064056
Iteration 31/1000 | Loss: 0.00109930
Iteration 32/1000 | Loss: 0.00078462
Iteration 33/1000 | Loss: 0.00057105
Iteration 34/1000 | Loss: 0.00117673
Iteration 35/1000 | Loss: 0.00036147
Iteration 36/1000 | Loss: 0.00106919
Iteration 37/1000 | Loss: 0.00154275
Iteration 38/1000 | Loss: 0.00108047
Iteration 39/1000 | Loss: 0.00006484
Iteration 40/1000 | Loss: 0.00119431
Iteration 41/1000 | Loss: 0.00174984
Iteration 42/1000 | Loss: 0.00138875
Iteration 43/1000 | Loss: 0.00212623
Iteration 44/1000 | Loss: 0.00032045
Iteration 45/1000 | Loss: 0.00004008
Iteration 46/1000 | Loss: 0.00003551
Iteration 47/1000 | Loss: 0.00118238
Iteration 48/1000 | Loss: 0.00060031
Iteration 49/1000 | Loss: 0.00049722
Iteration 50/1000 | Loss: 0.00082747
Iteration 51/1000 | Loss: 0.00112563
Iteration 52/1000 | Loss: 0.00003854
Iteration 53/1000 | Loss: 0.00054209
Iteration 54/1000 | Loss: 0.00145160
Iteration 55/1000 | Loss: 0.00115742
Iteration 56/1000 | Loss: 0.00067225
Iteration 57/1000 | Loss: 0.00082971
Iteration 58/1000 | Loss: 0.00156795
Iteration 59/1000 | Loss: 0.00060451
Iteration 60/1000 | Loss: 0.00068712
Iteration 61/1000 | Loss: 0.00051298
Iteration 62/1000 | Loss: 0.00108057
Iteration 63/1000 | Loss: 0.00060570
Iteration 64/1000 | Loss: 0.00082169
Iteration 65/1000 | Loss: 0.00082434
Iteration 66/1000 | Loss: 0.00083906
Iteration 67/1000 | Loss: 0.00091450
Iteration 68/1000 | Loss: 0.00134208
Iteration 69/1000 | Loss: 0.00091299
Iteration 70/1000 | Loss: 0.00095616
Iteration 71/1000 | Loss: 0.00003615
Iteration 72/1000 | Loss: 0.00003180
Iteration 73/1000 | Loss: 0.00003087
Iteration 74/1000 | Loss: 0.00003045
Iteration 75/1000 | Loss: 0.00002982
Iteration 76/1000 | Loss: 0.00081439
Iteration 77/1000 | Loss: 0.00042555
Iteration 78/1000 | Loss: 0.00122170
Iteration 79/1000 | Loss: 0.00006234
Iteration 80/1000 | Loss: 0.00004633
Iteration 81/1000 | Loss: 0.00003354
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002516
Iteration 84/1000 | Loss: 0.00002401
Iteration 85/1000 | Loss: 0.00002291
Iteration 86/1000 | Loss: 0.00033013
Iteration 87/1000 | Loss: 0.00019206
Iteration 88/1000 | Loss: 0.00011296
Iteration 89/1000 | Loss: 0.00002224
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00001997
Iteration 93/1000 | Loss: 0.00020168
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001776
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001737
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001707
Iteration 113/1000 | Loss: 0.00001707
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001701
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001700
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001699
Iteration 156/1000 | Loss: 0.00001699
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001698
Iteration 160/1000 | Loss: 0.00001698
Iteration 161/1000 | Loss: 0.00001698
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001698
Iteration 164/1000 | Loss: 0.00001698
Iteration 165/1000 | Loss: 0.00001698
Iteration 166/1000 | Loss: 0.00001698
Iteration 167/1000 | Loss: 0.00001698
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001698
Iteration 170/1000 | Loss: 0.00001698
Iteration 171/1000 | Loss: 0.00001698
Iteration 172/1000 | Loss: 0.00001698
Iteration 173/1000 | Loss: 0.00001698
Iteration 174/1000 | Loss: 0.00001698
Iteration 175/1000 | Loss: 0.00001698
Iteration 176/1000 | Loss: 0.00001697
Iteration 177/1000 | Loss: 0.00001697
Iteration 178/1000 | Loss: 0.00001697
Iteration 179/1000 | Loss: 0.00001697
Iteration 180/1000 | Loss: 0.00001697
Iteration 181/1000 | Loss: 0.00001696
Iteration 182/1000 | Loss: 0.00001696
Iteration 183/1000 | Loss: 0.00001695
Iteration 184/1000 | Loss: 0.00001695
Iteration 185/1000 | Loss: 0.00001695
Iteration 186/1000 | Loss: 0.00001694
Iteration 187/1000 | Loss: 0.00001694
Iteration 188/1000 | Loss: 0.00001694
Iteration 189/1000 | Loss: 0.00001694
Iteration 190/1000 | Loss: 0.00001694
Iteration 191/1000 | Loss: 0.00001693
Iteration 192/1000 | Loss: 0.00001693
Iteration 193/1000 | Loss: 0.00001692
Iteration 194/1000 | Loss: 0.00001692
Iteration 195/1000 | Loss: 0.00001692
Iteration 196/1000 | Loss: 0.00001692
Iteration 197/1000 | Loss: 0.00001692
Iteration 198/1000 | Loss: 0.00001692
Iteration 199/1000 | Loss: 0.00001692
Iteration 200/1000 | Loss: 0.00001692
Iteration 201/1000 | Loss: 0.00001692
Iteration 202/1000 | Loss: 0.00001692
Iteration 203/1000 | Loss: 0.00001692
Iteration 204/1000 | Loss: 0.00001692
Iteration 205/1000 | Loss: 0.00001692
Iteration 206/1000 | Loss: 0.00001692
Iteration 207/1000 | Loss: 0.00001692
Iteration 208/1000 | Loss: 0.00001692
Iteration 209/1000 | Loss: 0.00001692
Iteration 210/1000 | Loss: 0.00001692
Iteration 211/1000 | Loss: 0.00001692
Iteration 212/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.6915544620133005e-05, 1.6915544620133005e-05, 1.6915544620133005e-05, 1.6915544620133005e-05, 1.6915544620133005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6915544620133005e-05

Optimization complete. Final v2v error: 3.418726921081543 mm

Highest mean error: 8.534966468811035 mm for frame 159

Lowest mean error: 2.8677926063537598 mm for frame 158

Saving results

Total time: 188.40409922599792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525556
Iteration 2/25 | Loss: 0.00131322
Iteration 3/25 | Loss: 0.00125307
Iteration 4/25 | Loss: 0.00124712
Iteration 5/25 | Loss: 0.00124582
Iteration 6/25 | Loss: 0.00124582
Iteration 7/25 | Loss: 0.00124582
Iteration 8/25 | Loss: 0.00124582
Iteration 9/25 | Loss: 0.00124582
Iteration 10/25 | Loss: 0.00124582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012458241544663906, 0.0012458241544663906, 0.0012458241544663906, 0.0012458241544663906, 0.0012458241544663906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012458241544663906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.22346783
Iteration 2/25 | Loss: 0.00108185
Iteration 3/25 | Loss: 0.00108184
Iteration 4/25 | Loss: 0.00108184
Iteration 5/25 | Loss: 0.00108184
Iteration 6/25 | Loss: 0.00108184
Iteration 7/25 | Loss: 0.00108184
Iteration 8/25 | Loss: 0.00108184
Iteration 9/25 | Loss: 0.00108183
Iteration 10/25 | Loss: 0.00108183
Iteration 11/25 | Loss: 0.00108183
Iteration 12/25 | Loss: 0.00108183
Iteration 13/25 | Loss: 0.00108183
Iteration 14/25 | Loss: 0.00108183
Iteration 15/25 | Loss: 0.00108183
Iteration 16/25 | Loss: 0.00108183
Iteration 17/25 | Loss: 0.00108183
Iteration 18/25 | Loss: 0.00108183
Iteration 19/25 | Loss: 0.00108183
Iteration 20/25 | Loss: 0.00108183
Iteration 21/25 | Loss: 0.00108183
Iteration 22/25 | Loss: 0.00108183
Iteration 23/25 | Loss: 0.00108183
Iteration 24/25 | Loss: 0.00108183
Iteration 25/25 | Loss: 0.00108183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108183
Iteration 2/1000 | Loss: 0.00002712
Iteration 3/1000 | Loss: 0.00001749
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001466
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001386
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001327
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001319
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001293
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001287
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001281
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001271
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001271
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001247
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001245
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001245
Iteration 92/1000 | Loss: 0.00001245
Iteration 93/1000 | Loss: 0.00001245
Iteration 94/1000 | Loss: 0.00001245
Iteration 95/1000 | Loss: 0.00001244
Iteration 96/1000 | Loss: 0.00001244
Iteration 97/1000 | Loss: 0.00001244
Iteration 98/1000 | Loss: 0.00001244
Iteration 99/1000 | Loss: 0.00001244
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001243
Iteration 103/1000 | Loss: 0.00001243
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001242
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001240
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001240
Iteration 119/1000 | Loss: 0.00001240
Iteration 120/1000 | Loss: 0.00001240
Iteration 121/1000 | Loss: 0.00001240
Iteration 122/1000 | Loss: 0.00001239
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001238
Iteration 130/1000 | Loss: 0.00001238
Iteration 131/1000 | Loss: 0.00001238
Iteration 132/1000 | Loss: 0.00001238
Iteration 133/1000 | Loss: 0.00001238
Iteration 134/1000 | Loss: 0.00001238
Iteration 135/1000 | Loss: 0.00001238
Iteration 136/1000 | Loss: 0.00001238
Iteration 137/1000 | Loss: 0.00001238
Iteration 138/1000 | Loss: 0.00001238
Iteration 139/1000 | Loss: 0.00001238
Iteration 140/1000 | Loss: 0.00001238
Iteration 141/1000 | Loss: 0.00001238
Iteration 142/1000 | Loss: 0.00001237
Iteration 143/1000 | Loss: 0.00001237
Iteration 144/1000 | Loss: 0.00001237
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001237
Iteration 147/1000 | Loss: 0.00001237
Iteration 148/1000 | Loss: 0.00001237
Iteration 149/1000 | Loss: 0.00001237
Iteration 150/1000 | Loss: 0.00001237
Iteration 151/1000 | Loss: 0.00001237
Iteration 152/1000 | Loss: 0.00001237
Iteration 153/1000 | Loss: 0.00001237
Iteration 154/1000 | Loss: 0.00001237
Iteration 155/1000 | Loss: 0.00001237
Iteration 156/1000 | Loss: 0.00001237
Iteration 157/1000 | Loss: 0.00001237
Iteration 158/1000 | Loss: 0.00001237
Iteration 159/1000 | Loss: 0.00001237
Iteration 160/1000 | Loss: 0.00001237
Iteration 161/1000 | Loss: 0.00001237
Iteration 162/1000 | Loss: 0.00001237
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001237
Iteration 165/1000 | Loss: 0.00001237
Iteration 166/1000 | Loss: 0.00001237
Iteration 167/1000 | Loss: 0.00001237
Iteration 168/1000 | Loss: 0.00001237
Iteration 169/1000 | Loss: 0.00001237
Iteration 170/1000 | Loss: 0.00001237
Iteration 171/1000 | Loss: 0.00001237
Iteration 172/1000 | Loss: 0.00001237
Iteration 173/1000 | Loss: 0.00001237
Iteration 174/1000 | Loss: 0.00001237
Iteration 175/1000 | Loss: 0.00001237
Iteration 176/1000 | Loss: 0.00001237
Iteration 177/1000 | Loss: 0.00001237
Iteration 178/1000 | Loss: 0.00001237
Iteration 179/1000 | Loss: 0.00001237
Iteration 180/1000 | Loss: 0.00001237
Iteration 181/1000 | Loss: 0.00001237
Iteration 182/1000 | Loss: 0.00001237
Iteration 183/1000 | Loss: 0.00001237
Iteration 184/1000 | Loss: 0.00001237
Iteration 185/1000 | Loss: 0.00001237
Iteration 186/1000 | Loss: 0.00001237
Iteration 187/1000 | Loss: 0.00001237
Iteration 188/1000 | Loss: 0.00001237
Iteration 189/1000 | Loss: 0.00001237
Iteration 190/1000 | Loss: 0.00001237
Iteration 191/1000 | Loss: 0.00001237
Iteration 192/1000 | Loss: 0.00001237
Iteration 193/1000 | Loss: 0.00001237
Iteration 194/1000 | Loss: 0.00001237
Iteration 195/1000 | Loss: 0.00001237
Iteration 196/1000 | Loss: 0.00001237
Iteration 197/1000 | Loss: 0.00001237
Iteration 198/1000 | Loss: 0.00001237
Iteration 199/1000 | Loss: 0.00001237
Iteration 200/1000 | Loss: 0.00001237
Iteration 201/1000 | Loss: 0.00001237
Iteration 202/1000 | Loss: 0.00001237
Iteration 203/1000 | Loss: 0.00001237
Iteration 204/1000 | Loss: 0.00001237
Iteration 205/1000 | Loss: 0.00001237
Iteration 206/1000 | Loss: 0.00001237
Iteration 207/1000 | Loss: 0.00001237
Iteration 208/1000 | Loss: 0.00001237
Iteration 209/1000 | Loss: 0.00001237
Iteration 210/1000 | Loss: 0.00001237
Iteration 211/1000 | Loss: 0.00001237
Iteration 212/1000 | Loss: 0.00001237
Iteration 213/1000 | Loss: 0.00001237
Iteration 214/1000 | Loss: 0.00001237
Iteration 215/1000 | Loss: 0.00001237
Iteration 216/1000 | Loss: 0.00001237
Iteration 217/1000 | Loss: 0.00001237
Iteration 218/1000 | Loss: 0.00001237
Iteration 219/1000 | Loss: 0.00001237
Iteration 220/1000 | Loss: 0.00001237
Iteration 221/1000 | Loss: 0.00001237
Iteration 222/1000 | Loss: 0.00001237
Iteration 223/1000 | Loss: 0.00001237
Iteration 224/1000 | Loss: 0.00001237
Iteration 225/1000 | Loss: 0.00001237
Iteration 226/1000 | Loss: 0.00001237
Iteration 227/1000 | Loss: 0.00001237
Iteration 228/1000 | Loss: 0.00001237
Iteration 229/1000 | Loss: 0.00001237
Iteration 230/1000 | Loss: 0.00001237
Iteration 231/1000 | Loss: 0.00001237
Iteration 232/1000 | Loss: 0.00001237
Iteration 233/1000 | Loss: 0.00001237
Iteration 234/1000 | Loss: 0.00001237
Iteration 235/1000 | Loss: 0.00001237
Iteration 236/1000 | Loss: 0.00001237
Iteration 237/1000 | Loss: 0.00001237
Iteration 238/1000 | Loss: 0.00001237
Iteration 239/1000 | Loss: 0.00001237
Iteration 240/1000 | Loss: 0.00001237
Iteration 241/1000 | Loss: 0.00001237
Iteration 242/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.2371176126180217e-05, 1.2371176126180217e-05, 1.2371176126180217e-05, 1.2371176126180217e-05, 1.2371176126180217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2371176126180217e-05

Optimization complete. Final v2v error: 2.9936046600341797 mm

Highest mean error: 3.1692287921905518 mm for frame 41

Lowest mean error: 2.7980804443359375 mm for frame 31

Saving results

Total time: 46.38750433921814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013055
Iteration 2/25 | Loss: 0.00259715
Iteration 3/25 | Loss: 0.00218188
Iteration 4/25 | Loss: 0.00209620
Iteration 5/25 | Loss: 0.00170132
Iteration 6/25 | Loss: 0.00145716
Iteration 7/25 | Loss: 0.00139302
Iteration 8/25 | Loss: 0.00136324
Iteration 9/25 | Loss: 0.00135449
Iteration 10/25 | Loss: 0.00134877
Iteration 11/25 | Loss: 0.00134550
Iteration 12/25 | Loss: 0.00134403
Iteration 13/25 | Loss: 0.00134544
Iteration 14/25 | Loss: 0.00134344
Iteration 15/25 | Loss: 0.00134268
Iteration 16/25 | Loss: 0.00134529
Iteration 17/25 | Loss: 0.00134404
Iteration 18/25 | Loss: 0.00134218
Iteration 19/25 | Loss: 0.00134164
Iteration 20/25 | Loss: 0.00134131
Iteration 21/25 | Loss: 0.00134117
Iteration 22/25 | Loss: 0.00134115
Iteration 23/25 | Loss: 0.00134115
Iteration 24/25 | Loss: 0.00134114
Iteration 25/25 | Loss: 0.00134114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36173463
Iteration 2/25 | Loss: 0.00098541
Iteration 3/25 | Loss: 0.00098541
Iteration 4/25 | Loss: 0.00098541
Iteration 5/25 | Loss: 0.00098541
Iteration 6/25 | Loss: 0.00098541
Iteration 7/25 | Loss: 0.00098541
Iteration 8/25 | Loss: 0.00098541
Iteration 9/25 | Loss: 0.00098541
Iteration 10/25 | Loss: 0.00098541
Iteration 11/25 | Loss: 0.00098541
Iteration 12/25 | Loss: 0.00098541
Iteration 13/25 | Loss: 0.00098541
Iteration 14/25 | Loss: 0.00098541
Iteration 15/25 | Loss: 0.00098541
Iteration 16/25 | Loss: 0.00098541
Iteration 17/25 | Loss: 0.00098541
Iteration 18/25 | Loss: 0.00098541
Iteration 19/25 | Loss: 0.00098541
Iteration 20/25 | Loss: 0.00098541
Iteration 21/25 | Loss: 0.00098541
Iteration 22/25 | Loss: 0.00098541
Iteration 23/25 | Loss: 0.00098541
Iteration 24/25 | Loss: 0.00098541
Iteration 25/25 | Loss: 0.00098541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098541
Iteration 2/1000 | Loss: 0.00004918
Iteration 3/1000 | Loss: 0.00003452
Iteration 4/1000 | Loss: 0.00003028
Iteration 5/1000 | Loss: 0.00002864
Iteration 6/1000 | Loss: 0.00022322
Iteration 7/1000 | Loss: 0.00003622
Iteration 8/1000 | Loss: 0.00003185
Iteration 9/1000 | Loss: 0.00010128
Iteration 10/1000 | Loss: 0.00019563
Iteration 11/1000 | Loss: 0.00017047
Iteration 12/1000 | Loss: 0.00019496
Iteration 13/1000 | Loss: 0.00003759
Iteration 14/1000 | Loss: 0.00003205
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00003201
Iteration 17/1000 | Loss: 0.00002936
Iteration 18/1000 | Loss: 0.00002797
Iteration 19/1000 | Loss: 0.00024051
Iteration 20/1000 | Loss: 0.00003611
Iteration 21/1000 | Loss: 0.00012921
Iteration 22/1000 | Loss: 0.00023869
Iteration 23/1000 | Loss: 0.00023095
Iteration 24/1000 | Loss: 0.00015642
Iteration 25/1000 | Loss: 0.00021374
Iteration 26/1000 | Loss: 0.00014556
Iteration 27/1000 | Loss: 0.00023215
Iteration 28/1000 | Loss: 0.00016185
Iteration 29/1000 | Loss: 0.00014015
Iteration 30/1000 | Loss: 0.00027555
Iteration 31/1000 | Loss: 0.00015870
Iteration 32/1000 | Loss: 0.00009376
Iteration 33/1000 | Loss: 0.00012891
Iteration 34/1000 | Loss: 0.00003570
Iteration 35/1000 | Loss: 0.00003058
Iteration 36/1000 | Loss: 0.00002899
Iteration 37/1000 | Loss: 0.00025338
Iteration 38/1000 | Loss: 0.00003325
Iteration 39/1000 | Loss: 0.00002870
Iteration 40/1000 | Loss: 0.00002744
Iteration 41/1000 | Loss: 0.00002662
Iteration 42/1000 | Loss: 0.00002589
Iteration 43/1000 | Loss: 0.00002528
Iteration 44/1000 | Loss: 0.00002497
Iteration 45/1000 | Loss: 0.00002492
Iteration 46/1000 | Loss: 0.00002471
Iteration 47/1000 | Loss: 0.00002467
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002459
Iteration 51/1000 | Loss: 0.00002457
Iteration 52/1000 | Loss: 0.00002456
Iteration 53/1000 | Loss: 0.00002456
Iteration 54/1000 | Loss: 0.00002454
Iteration 55/1000 | Loss: 0.00002454
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00002454
Iteration 58/1000 | Loss: 0.00002454
Iteration 59/1000 | Loss: 0.00002454
Iteration 60/1000 | Loss: 0.00002454
Iteration 61/1000 | Loss: 0.00002453
Iteration 62/1000 | Loss: 0.00002453
Iteration 63/1000 | Loss: 0.00002453
Iteration 64/1000 | Loss: 0.00002452
Iteration 65/1000 | Loss: 0.00002452
Iteration 66/1000 | Loss: 0.00002452
Iteration 67/1000 | Loss: 0.00002452
Iteration 68/1000 | Loss: 0.00002452
Iteration 69/1000 | Loss: 0.00002452
Iteration 70/1000 | Loss: 0.00002452
Iteration 71/1000 | Loss: 0.00002452
Iteration 72/1000 | Loss: 0.00002452
Iteration 73/1000 | Loss: 0.00002452
Iteration 74/1000 | Loss: 0.00002451
Iteration 75/1000 | Loss: 0.00002451
Iteration 76/1000 | Loss: 0.00002451
Iteration 77/1000 | Loss: 0.00002451
Iteration 78/1000 | Loss: 0.00002451
Iteration 79/1000 | Loss: 0.00002451
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002451
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002451
Iteration 85/1000 | Loss: 0.00002451
Iteration 86/1000 | Loss: 0.00002451
Iteration 87/1000 | Loss: 0.00002451
Iteration 88/1000 | Loss: 0.00002451
Iteration 89/1000 | Loss: 0.00002451
Iteration 90/1000 | Loss: 0.00002451
Iteration 91/1000 | Loss: 0.00002451
Iteration 92/1000 | Loss: 0.00002451
Iteration 93/1000 | Loss: 0.00002451
Iteration 94/1000 | Loss: 0.00002451
Iteration 95/1000 | Loss: 0.00002451
Iteration 96/1000 | Loss: 0.00002451
Iteration 97/1000 | Loss: 0.00002451
Iteration 98/1000 | Loss: 0.00002451
Iteration 99/1000 | Loss: 0.00002451
Iteration 100/1000 | Loss: 0.00002451
Iteration 101/1000 | Loss: 0.00002451
Iteration 102/1000 | Loss: 0.00002451
Iteration 103/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.450968349876348e-05, 2.450968349876348e-05, 2.450968349876348e-05, 2.450968349876348e-05, 2.450968349876348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.450968349876348e-05

Optimization complete. Final v2v error: 4.2897491455078125 mm

Highest mean error: 4.872066020965576 mm for frame 3

Lowest mean error: 3.753518581390381 mm for frame 6

Saving results

Total time: 104.39326548576355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930675
Iteration 2/25 | Loss: 0.00210700
Iteration 3/25 | Loss: 0.00155498
Iteration 4/25 | Loss: 0.00151848
Iteration 5/25 | Loss: 0.00149389
Iteration 6/25 | Loss: 0.00148546
Iteration 7/25 | Loss: 0.00150833
Iteration 8/25 | Loss: 0.00139523
Iteration 9/25 | Loss: 0.00136326
Iteration 10/25 | Loss: 0.00134116
Iteration 11/25 | Loss: 0.00132727
Iteration 12/25 | Loss: 0.00132229
Iteration 13/25 | Loss: 0.00131696
Iteration 14/25 | Loss: 0.00131383
Iteration 15/25 | Loss: 0.00131237
Iteration 16/25 | Loss: 0.00131317
Iteration 17/25 | Loss: 0.00132081
Iteration 18/25 | Loss: 0.00133283
Iteration 19/25 | Loss: 0.00131115
Iteration 20/25 | Loss: 0.00131149
Iteration 21/25 | Loss: 0.00131095
Iteration 22/25 | Loss: 0.00130993
Iteration 23/25 | Loss: 0.00130323
Iteration 24/25 | Loss: 0.00129745
Iteration 25/25 | Loss: 0.00130090

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40610588
Iteration 2/25 | Loss: 0.00162479
Iteration 3/25 | Loss: 0.00136791
Iteration 4/25 | Loss: 0.00136788
Iteration 5/25 | Loss: 0.00136788
Iteration 6/25 | Loss: 0.00136788
Iteration 7/25 | Loss: 0.00136788
Iteration 8/25 | Loss: 0.00136787
Iteration 9/25 | Loss: 0.00136787
Iteration 10/25 | Loss: 0.00136787
Iteration 11/25 | Loss: 0.00136787
Iteration 12/25 | Loss: 0.00136787
Iteration 13/25 | Loss: 0.00136787
Iteration 14/25 | Loss: 0.00136787
Iteration 15/25 | Loss: 0.00136787
Iteration 16/25 | Loss: 0.00136787
Iteration 17/25 | Loss: 0.00136787
Iteration 18/25 | Loss: 0.00136787
Iteration 19/25 | Loss: 0.00136787
Iteration 20/25 | Loss: 0.00136787
Iteration 21/25 | Loss: 0.00136787
Iteration 22/25 | Loss: 0.00136787
Iteration 23/25 | Loss: 0.00136787
Iteration 24/25 | Loss: 0.00136787
Iteration 25/25 | Loss: 0.00136787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136787
Iteration 2/1000 | Loss: 0.00019353
Iteration 3/1000 | Loss: 0.00012726
Iteration 4/1000 | Loss: 0.00012138
Iteration 5/1000 | Loss: 0.00015428
Iteration 6/1000 | Loss: 0.00017869
Iteration 7/1000 | Loss: 0.00031783
Iteration 8/1000 | Loss: 0.00063400
Iteration 9/1000 | Loss: 0.00025781
Iteration 10/1000 | Loss: 0.00017303
Iteration 11/1000 | Loss: 0.00044105
Iteration 12/1000 | Loss: 0.00026807
Iteration 13/1000 | Loss: 0.00019061
Iteration 14/1000 | Loss: 0.00047186
Iteration 15/1000 | Loss: 0.00039064
Iteration 16/1000 | Loss: 0.00024104
Iteration 17/1000 | Loss: 0.00027543
Iteration 18/1000 | Loss: 0.00023192
Iteration 19/1000 | Loss: 0.00057278
Iteration 20/1000 | Loss: 0.00022729
Iteration 21/1000 | Loss: 0.00022320
Iteration 22/1000 | Loss: 0.00021440
Iteration 23/1000 | Loss: 0.00019109
Iteration 24/1000 | Loss: 0.00021552
Iteration 25/1000 | Loss: 0.00030240
Iteration 26/1000 | Loss: 0.00029151
Iteration 27/1000 | Loss: 0.00029774
Iteration 28/1000 | Loss: 0.00048629
Iteration 29/1000 | Loss: 0.00033116
Iteration 30/1000 | Loss: 0.00027618
Iteration 31/1000 | Loss: 0.00029821
Iteration 32/1000 | Loss: 0.00037438
Iteration 33/1000 | Loss: 0.00027611
Iteration 34/1000 | Loss: 0.00039265
Iteration 35/1000 | Loss: 0.00061728
Iteration 36/1000 | Loss: 0.00039531
Iteration 37/1000 | Loss: 0.00031561
Iteration 38/1000 | Loss: 0.00020750
Iteration 39/1000 | Loss: 0.00027231
Iteration 40/1000 | Loss: 0.00034291
Iteration 41/1000 | Loss: 0.00061866
Iteration 42/1000 | Loss: 0.00021705
Iteration 43/1000 | Loss: 0.00025476
Iteration 44/1000 | Loss: 0.00026219
Iteration 45/1000 | Loss: 0.00036818
Iteration 46/1000 | Loss: 0.00032347
Iteration 47/1000 | Loss: 0.00057018
Iteration 48/1000 | Loss: 0.00029584
Iteration 49/1000 | Loss: 0.00023715
Iteration 50/1000 | Loss: 0.00025584
Iteration 51/1000 | Loss: 0.00016488
Iteration 52/1000 | Loss: 0.00025565
Iteration 53/1000 | Loss: 0.00024703
Iteration 54/1000 | Loss: 0.00024428
Iteration 55/1000 | Loss: 0.00025069
Iteration 56/1000 | Loss: 0.00029500
Iteration 57/1000 | Loss: 0.00025980
Iteration 58/1000 | Loss: 0.00032765
Iteration 59/1000 | Loss: 0.00027357
Iteration 60/1000 | Loss: 0.00028137
Iteration 61/1000 | Loss: 0.00029749
Iteration 62/1000 | Loss: 0.00025919
Iteration 63/1000 | Loss: 0.00014560
Iteration 64/1000 | Loss: 0.00043952
Iteration 65/1000 | Loss: 0.00022086
Iteration 66/1000 | Loss: 0.00017068
Iteration 67/1000 | Loss: 0.00019155
Iteration 68/1000 | Loss: 0.00029676
Iteration 69/1000 | Loss: 0.00035868
Iteration 70/1000 | Loss: 0.00019833
Iteration 71/1000 | Loss: 0.00028859
Iteration 72/1000 | Loss: 0.00022769
Iteration 73/1000 | Loss: 0.00025068
Iteration 74/1000 | Loss: 0.00015835
Iteration 75/1000 | Loss: 0.00022127
Iteration 76/1000 | Loss: 0.00021220
Iteration 77/1000 | Loss: 0.00022152
Iteration 78/1000 | Loss: 0.00021214
Iteration 79/1000 | Loss: 0.00034675
Iteration 80/1000 | Loss: 0.00018212
Iteration 81/1000 | Loss: 0.00038203
Iteration 82/1000 | Loss: 0.00026194
Iteration 83/1000 | Loss: 0.00024350
Iteration 84/1000 | Loss: 0.00024732
Iteration 85/1000 | Loss: 0.00028107
Iteration 86/1000 | Loss: 0.00031555
Iteration 87/1000 | Loss: 0.00030161
Iteration 88/1000 | Loss: 0.00029431
Iteration 89/1000 | Loss: 0.00027598
Iteration 90/1000 | Loss: 0.00040164
Iteration 91/1000 | Loss: 0.00077213
Iteration 92/1000 | Loss: 0.00031111
Iteration 93/1000 | Loss: 0.00026571
Iteration 94/1000 | Loss: 0.00026073
Iteration 95/1000 | Loss: 0.00021199
Iteration 96/1000 | Loss: 0.00026609
Iteration 97/1000 | Loss: 0.00019934
Iteration 98/1000 | Loss: 0.00037134
Iteration 99/1000 | Loss: 0.00019606
Iteration 100/1000 | Loss: 0.00024860
Iteration 101/1000 | Loss: 0.00020849
Iteration 102/1000 | Loss: 0.00026356
Iteration 103/1000 | Loss: 0.00020876
Iteration 104/1000 | Loss: 0.00022479
Iteration 105/1000 | Loss: 0.00024773
Iteration 106/1000 | Loss: 0.00025499
Iteration 107/1000 | Loss: 0.00022719
Iteration 108/1000 | Loss: 0.00037928
Iteration 109/1000 | Loss: 0.00020596
Iteration 110/1000 | Loss: 0.00019956
Iteration 111/1000 | Loss: 0.00020601
Iteration 112/1000 | Loss: 0.00024881
Iteration 113/1000 | Loss: 0.00031563
Iteration 114/1000 | Loss: 0.00025817
Iteration 115/1000 | Loss: 0.00018485
Iteration 116/1000 | Loss: 0.00034268
Iteration 117/1000 | Loss: 0.00022117
Iteration 118/1000 | Loss: 0.00021957
Iteration 119/1000 | Loss: 0.00021844
Iteration 120/1000 | Loss: 0.00025041
Iteration 121/1000 | Loss: 0.00022487
Iteration 122/1000 | Loss: 0.00064117
Iteration 123/1000 | Loss: 0.00021299
Iteration 124/1000 | Loss: 0.00021859
Iteration 125/1000 | Loss: 0.00022500
Iteration 126/1000 | Loss: 0.00022238
Iteration 127/1000 | Loss: 0.00072633
Iteration 128/1000 | Loss: 0.00021260
Iteration 129/1000 | Loss: 0.00019788
Iteration 130/1000 | Loss: 0.00020440
Iteration 131/1000 | Loss: 0.00019066
Iteration 132/1000 | Loss: 0.00028693
Iteration 133/1000 | Loss: 0.00021005
Iteration 134/1000 | Loss: 0.00029695
Iteration 135/1000 | Loss: 0.00025365
Iteration 136/1000 | Loss: 0.00038074
Iteration 137/1000 | Loss: 0.00025101
Iteration 138/1000 | Loss: 0.00031335
Iteration 139/1000 | Loss: 0.00025871
Iteration 140/1000 | Loss: 0.00026302
Iteration 141/1000 | Loss: 0.00055423
Iteration 142/1000 | Loss: 0.00041575
Iteration 143/1000 | Loss: 0.00015897
Iteration 144/1000 | Loss: 0.00013843
Iteration 145/1000 | Loss: 0.00025539
Iteration 146/1000 | Loss: 0.00009775
Iteration 147/1000 | Loss: 0.00036600
Iteration 148/1000 | Loss: 0.00006800
Iteration 149/1000 | Loss: 0.00006739
Iteration 150/1000 | Loss: 0.00036875
Iteration 151/1000 | Loss: 0.00040044
Iteration 152/1000 | Loss: 0.00038157
Iteration 153/1000 | Loss: 0.00042564
Iteration 154/1000 | Loss: 0.00030420
Iteration 155/1000 | Loss: 0.00006119
Iteration 156/1000 | Loss: 0.00103633
Iteration 157/1000 | Loss: 0.00023951
Iteration 158/1000 | Loss: 0.00003315
Iteration 159/1000 | Loss: 0.00002605
Iteration 160/1000 | Loss: 0.00004314
Iteration 161/1000 | Loss: 0.00003806
Iteration 162/1000 | Loss: 0.00003651
Iteration 163/1000 | Loss: 0.00003222
Iteration 164/1000 | Loss: 0.00021787
Iteration 165/1000 | Loss: 0.00005057
Iteration 166/1000 | Loss: 0.00032266
Iteration 167/1000 | Loss: 0.00004507
Iteration 168/1000 | Loss: 0.00002024
Iteration 169/1000 | Loss: 0.00004181
Iteration 170/1000 | Loss: 0.00003256
Iteration 171/1000 | Loss: 0.00002975
Iteration 172/1000 | Loss: 0.00002328
Iteration 173/1000 | Loss: 0.00009889
Iteration 174/1000 | Loss: 0.00002245
Iteration 175/1000 | Loss: 0.00004971
Iteration 176/1000 | Loss: 0.00003423
Iteration 177/1000 | Loss: 0.00003759
Iteration 178/1000 | Loss: 0.00003289
Iteration 179/1000 | Loss: 0.00003229
Iteration 180/1000 | Loss: 0.00003021
Iteration 181/1000 | Loss: 0.00003537
Iteration 182/1000 | Loss: 0.00002009
Iteration 183/1000 | Loss: 0.00003700
Iteration 184/1000 | Loss: 0.00003524
Iteration 185/1000 | Loss: 0.00003658
Iteration 186/1000 | Loss: 0.00004179
Iteration 187/1000 | Loss: 0.00003798
Iteration 188/1000 | Loss: 0.00014308
Iteration 189/1000 | Loss: 0.00002888
Iteration 190/1000 | Loss: 0.00001966
Iteration 191/1000 | Loss: 0.00001782
Iteration 192/1000 | Loss: 0.00001698
Iteration 193/1000 | Loss: 0.00004981
Iteration 194/1000 | Loss: 0.00002998
Iteration 195/1000 | Loss: 0.00001622
Iteration 196/1000 | Loss: 0.00001579
Iteration 197/1000 | Loss: 0.00021179
Iteration 198/1000 | Loss: 0.00023829
Iteration 199/1000 | Loss: 0.00002960
Iteration 200/1000 | Loss: 0.00001538
Iteration 201/1000 | Loss: 0.00001523
Iteration 202/1000 | Loss: 0.00001514
Iteration 203/1000 | Loss: 0.00001510
Iteration 204/1000 | Loss: 0.00001509
Iteration 205/1000 | Loss: 0.00001508
Iteration 206/1000 | Loss: 0.00008028
Iteration 207/1000 | Loss: 0.00001517
Iteration 208/1000 | Loss: 0.00001495
Iteration 209/1000 | Loss: 0.00001493
Iteration 210/1000 | Loss: 0.00001492
Iteration 211/1000 | Loss: 0.00001487
Iteration 212/1000 | Loss: 0.00001485
Iteration 213/1000 | Loss: 0.00001485
Iteration 214/1000 | Loss: 0.00017973
Iteration 215/1000 | Loss: 0.00001555
Iteration 216/1000 | Loss: 0.00001475
Iteration 217/1000 | Loss: 0.00001472
Iteration 218/1000 | Loss: 0.00001471
Iteration 219/1000 | Loss: 0.00001469
Iteration 220/1000 | Loss: 0.00001465
Iteration 221/1000 | Loss: 0.00001465
Iteration 222/1000 | Loss: 0.00001465
Iteration 223/1000 | Loss: 0.00001463
Iteration 224/1000 | Loss: 0.00001463
Iteration 225/1000 | Loss: 0.00001463
Iteration 226/1000 | Loss: 0.00001462
Iteration 227/1000 | Loss: 0.00001462
Iteration 228/1000 | Loss: 0.00001461
Iteration 229/1000 | Loss: 0.00001461
Iteration 230/1000 | Loss: 0.00001461
Iteration 231/1000 | Loss: 0.00001461
Iteration 232/1000 | Loss: 0.00001461
Iteration 233/1000 | Loss: 0.00001461
Iteration 234/1000 | Loss: 0.00001460
Iteration 235/1000 | Loss: 0.00001460
Iteration 236/1000 | Loss: 0.00001460
Iteration 237/1000 | Loss: 0.00001460
Iteration 238/1000 | Loss: 0.00001460
Iteration 239/1000 | Loss: 0.00001460
Iteration 240/1000 | Loss: 0.00001460
Iteration 241/1000 | Loss: 0.00001460
Iteration 242/1000 | Loss: 0.00001460
Iteration 243/1000 | Loss: 0.00001460
Iteration 244/1000 | Loss: 0.00001459
Iteration 245/1000 | Loss: 0.00001459
Iteration 246/1000 | Loss: 0.00001459
Iteration 247/1000 | Loss: 0.00001458
Iteration 248/1000 | Loss: 0.00001458
Iteration 249/1000 | Loss: 0.00001458
Iteration 250/1000 | Loss: 0.00001457
Iteration 251/1000 | Loss: 0.00001457
Iteration 252/1000 | Loss: 0.00001456
Iteration 253/1000 | Loss: 0.00001456
Iteration 254/1000 | Loss: 0.00001456
Iteration 255/1000 | Loss: 0.00001455
Iteration 256/1000 | Loss: 0.00001455
Iteration 257/1000 | Loss: 0.00001455
Iteration 258/1000 | Loss: 0.00001454
Iteration 259/1000 | Loss: 0.00001454
Iteration 260/1000 | Loss: 0.00001454
Iteration 261/1000 | Loss: 0.00001454
Iteration 262/1000 | Loss: 0.00001454
Iteration 263/1000 | Loss: 0.00001454
Iteration 264/1000 | Loss: 0.00001453
Iteration 265/1000 | Loss: 0.00001453
Iteration 266/1000 | Loss: 0.00001453
Iteration 267/1000 | Loss: 0.00001453
Iteration 268/1000 | Loss: 0.00001453
Iteration 269/1000 | Loss: 0.00001453
Iteration 270/1000 | Loss: 0.00001453
Iteration 271/1000 | Loss: 0.00001453
Iteration 272/1000 | Loss: 0.00001453
Iteration 273/1000 | Loss: 0.00001453
Iteration 274/1000 | Loss: 0.00001453
Iteration 275/1000 | Loss: 0.00001452
Iteration 276/1000 | Loss: 0.00001452
Iteration 277/1000 | Loss: 0.00001451
Iteration 278/1000 | Loss: 0.00001451
Iteration 279/1000 | Loss: 0.00001451
Iteration 280/1000 | Loss: 0.00001450
Iteration 281/1000 | Loss: 0.00001450
Iteration 282/1000 | Loss: 0.00001450
Iteration 283/1000 | Loss: 0.00001449
Iteration 284/1000 | Loss: 0.00001449
Iteration 285/1000 | Loss: 0.00001449
Iteration 286/1000 | Loss: 0.00001448
Iteration 287/1000 | Loss: 0.00001448
Iteration 288/1000 | Loss: 0.00001447
Iteration 289/1000 | Loss: 0.00001447
Iteration 290/1000 | Loss: 0.00001447
Iteration 291/1000 | Loss: 0.00001447
Iteration 292/1000 | Loss: 0.00001447
Iteration 293/1000 | Loss: 0.00001447
Iteration 294/1000 | Loss: 0.00001447
Iteration 295/1000 | Loss: 0.00001447
Iteration 296/1000 | Loss: 0.00001446
Iteration 297/1000 | Loss: 0.00001446
Iteration 298/1000 | Loss: 0.00001446
Iteration 299/1000 | Loss: 0.00001446
Iteration 300/1000 | Loss: 0.00001446
Iteration 301/1000 | Loss: 0.00001446
Iteration 302/1000 | Loss: 0.00001446
Iteration 303/1000 | Loss: 0.00001446
Iteration 304/1000 | Loss: 0.00001446
Iteration 305/1000 | Loss: 0.00001446
Iteration 306/1000 | Loss: 0.00001446
Iteration 307/1000 | Loss: 0.00001445
Iteration 308/1000 | Loss: 0.00001445
Iteration 309/1000 | Loss: 0.00001445
Iteration 310/1000 | Loss: 0.00001445
Iteration 311/1000 | Loss: 0.00001445
Iteration 312/1000 | Loss: 0.00001444
Iteration 313/1000 | Loss: 0.00001444
Iteration 314/1000 | Loss: 0.00001444
Iteration 315/1000 | Loss: 0.00001444
Iteration 316/1000 | Loss: 0.00001444
Iteration 317/1000 | Loss: 0.00001444
Iteration 318/1000 | Loss: 0.00001443
Iteration 319/1000 | Loss: 0.00001443
Iteration 320/1000 | Loss: 0.00001443
Iteration 321/1000 | Loss: 0.00001443
Iteration 322/1000 | Loss: 0.00001443
Iteration 323/1000 | Loss: 0.00001442
Iteration 324/1000 | Loss: 0.00001442
Iteration 325/1000 | Loss: 0.00001441
Iteration 326/1000 | Loss: 0.00001441
Iteration 327/1000 | Loss: 0.00001441
Iteration 328/1000 | Loss: 0.00001440
Iteration 329/1000 | Loss: 0.00015878
Iteration 330/1000 | Loss: 0.00006891
Iteration 331/1000 | Loss: 0.00001472
Iteration 332/1000 | Loss: 0.00019542
Iteration 333/1000 | Loss: 0.00020727
Iteration 334/1000 | Loss: 0.00002905
Iteration 335/1000 | Loss: 0.00001548
Iteration 336/1000 | Loss: 0.00001471
Iteration 337/1000 | Loss: 0.00003359
Iteration 338/1000 | Loss: 0.00001459
Iteration 339/1000 | Loss: 0.00001448
Iteration 340/1000 | Loss: 0.00001444
Iteration 341/1000 | Loss: 0.00001444
Iteration 342/1000 | Loss: 0.00001444
Iteration 343/1000 | Loss: 0.00014944
Iteration 344/1000 | Loss: 0.00001466
Iteration 345/1000 | Loss: 0.00001446
Iteration 346/1000 | Loss: 0.00001444
Iteration 347/1000 | Loss: 0.00001441
Iteration 348/1000 | Loss: 0.00001441
Iteration 349/1000 | Loss: 0.00001441
Iteration 350/1000 | Loss: 0.00001441
Iteration 351/1000 | Loss: 0.00018628
Iteration 352/1000 | Loss: 0.00039618
Iteration 353/1000 | Loss: 0.00002913
Iteration 354/1000 | Loss: 0.00001477
Iteration 355/1000 | Loss: 0.00001455
Iteration 356/1000 | Loss: 0.00019053
Iteration 357/1000 | Loss: 0.00046040
Iteration 358/1000 | Loss: 0.00038762
Iteration 359/1000 | Loss: 0.00005812
Iteration 360/1000 | Loss: 0.00001934
Iteration 361/1000 | Loss: 0.00005046
Iteration 362/1000 | Loss: 0.00002417
Iteration 363/1000 | Loss: 0.00007062
Iteration 364/1000 | Loss: 0.00003010
Iteration 365/1000 | Loss: 0.00001575
Iteration 366/1000 | Loss: 0.00001516
Iteration 367/1000 | Loss: 0.00008210
Iteration 368/1000 | Loss: 0.00001470
Iteration 369/1000 | Loss: 0.00007752
Iteration 370/1000 | Loss: 0.00001467
Iteration 371/1000 | Loss: 0.00001404
Iteration 372/1000 | Loss: 0.00001364
Iteration 373/1000 | Loss: 0.00024503
Iteration 374/1000 | Loss: 0.00033105
Iteration 375/1000 | Loss: 0.00001490
Iteration 376/1000 | Loss: 0.00033573
Iteration 377/1000 | Loss: 0.00001391
Iteration 378/1000 | Loss: 0.00001286
Iteration 379/1000 | Loss: 0.00001275
Iteration 380/1000 | Loss: 0.00001271
Iteration 381/1000 | Loss: 0.00001271
Iteration 382/1000 | Loss: 0.00001270
Iteration 383/1000 | Loss: 0.00001270
Iteration 384/1000 | Loss: 0.00001269
Iteration 385/1000 | Loss: 0.00001268
Iteration 386/1000 | Loss: 0.00001264
Iteration 387/1000 | Loss: 0.00009820
Iteration 388/1000 | Loss: 0.00004128
Iteration 389/1000 | Loss: 0.00001257
Iteration 390/1000 | Loss: 0.00003926
Iteration 391/1000 | Loss: 0.00001253
Iteration 392/1000 | Loss: 0.00001253
Iteration 393/1000 | Loss: 0.00001253
Iteration 394/1000 | Loss: 0.00001252
Iteration 395/1000 | Loss: 0.00001252
Iteration 396/1000 | Loss: 0.00001251
Iteration 397/1000 | Loss: 0.00001251
Iteration 398/1000 | Loss: 0.00001251
Iteration 399/1000 | Loss: 0.00001251
Iteration 400/1000 | Loss: 0.00001251
Iteration 401/1000 | Loss: 0.00001250
Iteration 402/1000 | Loss: 0.00001250
Iteration 403/1000 | Loss: 0.00001250
Iteration 404/1000 | Loss: 0.00001250
Iteration 405/1000 | Loss: 0.00001250
Iteration 406/1000 | Loss: 0.00001249
Iteration 407/1000 | Loss: 0.00002705
Iteration 408/1000 | Loss: 0.00001249
Iteration 409/1000 | Loss: 0.00001247
Iteration 410/1000 | Loss: 0.00001246
Iteration 411/1000 | Loss: 0.00001246
Iteration 412/1000 | Loss: 0.00001246
Iteration 413/1000 | Loss: 0.00001246
Iteration 414/1000 | Loss: 0.00001245
Iteration 415/1000 | Loss: 0.00001245
Iteration 416/1000 | Loss: 0.00001245
Iteration 417/1000 | Loss: 0.00001245
Iteration 418/1000 | Loss: 0.00001244
Iteration 419/1000 | Loss: 0.00001244
Iteration 420/1000 | Loss: 0.00001244
Iteration 421/1000 | Loss: 0.00001244
Iteration 422/1000 | Loss: 0.00001244
Iteration 423/1000 | Loss: 0.00001244
Iteration 424/1000 | Loss: 0.00001244
Iteration 425/1000 | Loss: 0.00001243
Iteration 426/1000 | Loss: 0.00001243
Iteration 427/1000 | Loss: 0.00001243
Iteration 428/1000 | Loss: 0.00001243
Iteration 429/1000 | Loss: 0.00001243
Iteration 430/1000 | Loss: 0.00001242
Iteration 431/1000 | Loss: 0.00001242
Iteration 432/1000 | Loss: 0.00001242
Iteration 433/1000 | Loss: 0.00001242
Iteration 434/1000 | Loss: 0.00001242
Iteration 435/1000 | Loss: 0.00001242
Iteration 436/1000 | Loss: 0.00001242
Iteration 437/1000 | Loss: 0.00001241
Iteration 438/1000 | Loss: 0.00001241
Iteration 439/1000 | Loss: 0.00001241
Iteration 440/1000 | Loss: 0.00001241
Iteration 441/1000 | Loss: 0.00001241
Iteration 442/1000 | Loss: 0.00001240
Iteration 443/1000 | Loss: 0.00001240
Iteration 444/1000 | Loss: 0.00001240
Iteration 445/1000 | Loss: 0.00001240
Iteration 446/1000 | Loss: 0.00001239
Iteration 447/1000 | Loss: 0.00001239
Iteration 448/1000 | Loss: 0.00001239
Iteration 449/1000 | Loss: 0.00001239
Iteration 450/1000 | Loss: 0.00001239
Iteration 451/1000 | Loss: 0.00001239
Iteration 452/1000 | Loss: 0.00001239
Iteration 453/1000 | Loss: 0.00001239
Iteration 454/1000 | Loss: 0.00001239
Iteration 455/1000 | Loss: 0.00001239
Iteration 456/1000 | Loss: 0.00001238
Iteration 457/1000 | Loss: 0.00001238
Iteration 458/1000 | Loss: 0.00001238
Iteration 459/1000 | Loss: 0.00001238
Iteration 460/1000 | Loss: 0.00001238
Iteration 461/1000 | Loss: 0.00001238
Iteration 462/1000 | Loss: 0.00001238
Iteration 463/1000 | Loss: 0.00001238
Iteration 464/1000 | Loss: 0.00001238
Iteration 465/1000 | Loss: 0.00001238
Iteration 466/1000 | Loss: 0.00001238
Iteration 467/1000 | Loss: 0.00001238
Iteration 468/1000 | Loss: 0.00001238
Iteration 469/1000 | Loss: 0.00001238
Iteration 470/1000 | Loss: 0.00001238
Iteration 471/1000 | Loss: 0.00001238
Iteration 472/1000 | Loss: 0.00001238
Iteration 473/1000 | Loss: 0.00001238
Iteration 474/1000 | Loss: 0.00001238
Iteration 475/1000 | Loss: 0.00001238
Iteration 476/1000 | Loss: 0.00001238
Iteration 477/1000 | Loss: 0.00001238
Iteration 478/1000 | Loss: 0.00001237
Iteration 479/1000 | Loss: 0.00001237
Iteration 480/1000 | Loss: 0.00001237
Iteration 481/1000 | Loss: 0.00001237
Iteration 482/1000 | Loss: 0.00001237
Iteration 483/1000 | Loss: 0.00001237
Iteration 484/1000 | Loss: 0.00001237
Iteration 485/1000 | Loss: 0.00001237
Iteration 486/1000 | Loss: 0.00001237
Iteration 487/1000 | Loss: 0.00001237
Iteration 488/1000 | Loss: 0.00001237
Iteration 489/1000 | Loss: 0.00001237
Iteration 490/1000 | Loss: 0.00001237
Iteration 491/1000 | Loss: 0.00001237
Iteration 492/1000 | Loss: 0.00001237
Iteration 493/1000 | Loss: 0.00001237
Iteration 494/1000 | Loss: 0.00001237
Iteration 495/1000 | Loss: 0.00001237
Iteration 496/1000 | Loss: 0.00001237
Iteration 497/1000 | Loss: 0.00001237
Iteration 498/1000 | Loss: 0.00001237
Iteration 499/1000 | Loss: 0.00001237
Iteration 500/1000 | Loss: 0.00001237
Iteration 501/1000 | Loss: 0.00001237
Iteration 502/1000 | Loss: 0.00001237
Iteration 503/1000 | Loss: 0.00001237
Iteration 504/1000 | Loss: 0.00001237
Iteration 505/1000 | Loss: 0.00001237
Iteration 506/1000 | Loss: 0.00001237
Iteration 507/1000 | Loss: 0.00001237
Iteration 508/1000 | Loss: 0.00001237
Iteration 509/1000 | Loss: 0.00001237
Iteration 510/1000 | Loss: 0.00001237
Iteration 511/1000 | Loss: 0.00001237
Iteration 512/1000 | Loss: 0.00001237
Iteration 513/1000 | Loss: 0.00001237
Iteration 514/1000 | Loss: 0.00001237
Iteration 515/1000 | Loss: 0.00001237
Iteration 516/1000 | Loss: 0.00001237
Iteration 517/1000 | Loss: 0.00001237
Iteration 518/1000 | Loss: 0.00001237
Iteration 519/1000 | Loss: 0.00001237
Iteration 520/1000 | Loss: 0.00001237
Iteration 521/1000 | Loss: 0.00001237
Iteration 522/1000 | Loss: 0.00001237
Iteration 523/1000 | Loss: 0.00001237
Iteration 524/1000 | Loss: 0.00001237
Iteration 525/1000 | Loss: 0.00001237
Iteration 526/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 526. Stopping optimization.
Last 5 losses: [1.2374776815704536e-05, 1.2374776815704536e-05, 1.2374776815704536e-05, 1.2374776815704536e-05, 1.2374776815704536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2374776815704536e-05

Optimization complete. Final v2v error: 3.0179128646850586 mm

Highest mean error: 4.361302852630615 mm for frame 8

Lowest mean error: 2.656355619430542 mm for frame 58

Saving results

Total time: 422.3430950641632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052075
Iteration 2/25 | Loss: 0.00178682
Iteration 3/25 | Loss: 0.00143715
Iteration 4/25 | Loss: 0.00136359
Iteration 5/25 | Loss: 0.00135983
Iteration 6/25 | Loss: 0.00133447
Iteration 7/25 | Loss: 0.00131078
Iteration 8/25 | Loss: 0.00131012
Iteration 9/25 | Loss: 0.00130486
Iteration 10/25 | Loss: 0.00130407
Iteration 11/25 | Loss: 0.00130681
Iteration 12/25 | Loss: 0.00130388
Iteration 13/25 | Loss: 0.00130375
Iteration 14/25 | Loss: 0.00130375
Iteration 15/25 | Loss: 0.00130375
Iteration 16/25 | Loss: 0.00130375
Iteration 17/25 | Loss: 0.00130375
Iteration 18/25 | Loss: 0.00130375
Iteration 19/25 | Loss: 0.00130374
Iteration 20/25 | Loss: 0.00130374
Iteration 21/25 | Loss: 0.00130374
Iteration 22/25 | Loss: 0.00130374
Iteration 23/25 | Loss: 0.00130374
Iteration 24/25 | Loss: 0.00130374
Iteration 25/25 | Loss: 0.00130374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.03873825
Iteration 2/25 | Loss: 0.00161736
Iteration 3/25 | Loss: 0.00161663
Iteration 4/25 | Loss: 0.00161663
Iteration 5/25 | Loss: 0.00161663
Iteration 6/25 | Loss: 0.00161663
Iteration 7/25 | Loss: 0.00161663
Iteration 8/25 | Loss: 0.00161663
Iteration 9/25 | Loss: 0.00161663
Iteration 10/25 | Loss: 0.00161663
Iteration 11/25 | Loss: 0.00161663
Iteration 12/25 | Loss: 0.00161663
Iteration 13/25 | Loss: 0.00161663
Iteration 14/25 | Loss: 0.00161663
Iteration 15/25 | Loss: 0.00161663
Iteration 16/25 | Loss: 0.00161663
Iteration 17/25 | Loss: 0.00161663
Iteration 18/25 | Loss: 0.00161663
Iteration 19/25 | Loss: 0.00161663
Iteration 20/25 | Loss: 0.00161663
Iteration 21/25 | Loss: 0.00161663
Iteration 22/25 | Loss: 0.00161663
Iteration 23/25 | Loss: 0.00161663
Iteration 24/25 | Loss: 0.00161663
Iteration 25/25 | Loss: 0.00161663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161663
Iteration 2/1000 | Loss: 0.00009882
Iteration 3/1000 | Loss: 0.00006868
Iteration 4/1000 | Loss: 0.00004922
Iteration 5/1000 | Loss: 0.00004516
Iteration 6/1000 | Loss: 0.00004290
Iteration 7/1000 | Loss: 0.00004105
Iteration 8/1000 | Loss: 0.00003986
Iteration 9/1000 | Loss: 0.00003893
Iteration 10/1000 | Loss: 0.00003828
Iteration 11/1000 | Loss: 0.00003777
Iteration 12/1000 | Loss: 0.00003742
Iteration 13/1000 | Loss: 0.00003711
Iteration 14/1000 | Loss: 0.00003684
Iteration 15/1000 | Loss: 0.00003659
Iteration 16/1000 | Loss: 0.00003639
Iteration 17/1000 | Loss: 0.00003636
Iteration 18/1000 | Loss: 0.00003618
Iteration 19/1000 | Loss: 0.00003601
Iteration 20/1000 | Loss: 0.00003598
Iteration 21/1000 | Loss: 0.00003587
Iteration 22/1000 | Loss: 0.00003585
Iteration 23/1000 | Loss: 0.00003580
Iteration 24/1000 | Loss: 0.00003579
Iteration 25/1000 | Loss: 0.00003574
Iteration 26/1000 | Loss: 0.00003569
Iteration 27/1000 | Loss: 0.00003568
Iteration 28/1000 | Loss: 0.00003565
Iteration 29/1000 | Loss: 0.00003564
Iteration 30/1000 | Loss: 0.00003564
Iteration 31/1000 | Loss: 0.00003563
Iteration 32/1000 | Loss: 0.00003563
Iteration 33/1000 | Loss: 0.00003562
Iteration 34/1000 | Loss: 0.00003561
Iteration 35/1000 | Loss: 0.00003559
Iteration 36/1000 | Loss: 0.00003559
Iteration 37/1000 | Loss: 0.00003558
Iteration 38/1000 | Loss: 0.00003558
Iteration 39/1000 | Loss: 0.00003558
Iteration 40/1000 | Loss: 0.00003555
Iteration 41/1000 | Loss: 0.00003555
Iteration 42/1000 | Loss: 0.00003554
Iteration 43/1000 | Loss: 0.00003554
Iteration 44/1000 | Loss: 0.00003553
Iteration 45/1000 | Loss: 0.00003553
Iteration 46/1000 | Loss: 0.00003553
Iteration 47/1000 | Loss: 0.00003552
Iteration 48/1000 | Loss: 0.00003552
Iteration 49/1000 | Loss: 0.00003552
Iteration 50/1000 | Loss: 0.00003551
Iteration 51/1000 | Loss: 0.00003551
Iteration 52/1000 | Loss: 0.00003550
Iteration 53/1000 | Loss: 0.00003549
Iteration 54/1000 | Loss: 0.00003548
Iteration 55/1000 | Loss: 0.00003548
Iteration 56/1000 | Loss: 0.00003548
Iteration 57/1000 | Loss: 0.00003548
Iteration 58/1000 | Loss: 0.00003548
Iteration 59/1000 | Loss: 0.00003548
Iteration 60/1000 | Loss: 0.00003548
Iteration 61/1000 | Loss: 0.00003548
Iteration 62/1000 | Loss: 0.00003548
Iteration 63/1000 | Loss: 0.00003547
Iteration 64/1000 | Loss: 0.00003546
Iteration 65/1000 | Loss: 0.00003546
Iteration 66/1000 | Loss: 0.00003545
Iteration 67/1000 | Loss: 0.00003545
Iteration 68/1000 | Loss: 0.00003545
Iteration 69/1000 | Loss: 0.00003545
Iteration 70/1000 | Loss: 0.00003545
Iteration 71/1000 | Loss: 0.00003545
Iteration 72/1000 | Loss: 0.00003545
Iteration 73/1000 | Loss: 0.00003545
Iteration 74/1000 | Loss: 0.00003545
Iteration 75/1000 | Loss: 0.00003544
Iteration 76/1000 | Loss: 0.00003544
Iteration 77/1000 | Loss: 0.00003544
Iteration 78/1000 | Loss: 0.00003544
Iteration 79/1000 | Loss: 0.00003544
Iteration 80/1000 | Loss: 0.00003544
Iteration 81/1000 | Loss: 0.00003543
Iteration 82/1000 | Loss: 0.00003542
Iteration 83/1000 | Loss: 0.00003542
Iteration 84/1000 | Loss: 0.00003542
Iteration 85/1000 | Loss: 0.00003541
Iteration 86/1000 | Loss: 0.00003541
Iteration 87/1000 | Loss: 0.00003541
Iteration 88/1000 | Loss: 0.00003540
Iteration 89/1000 | Loss: 0.00003540
Iteration 90/1000 | Loss: 0.00003539
Iteration 91/1000 | Loss: 0.00003539
Iteration 92/1000 | Loss: 0.00003539
Iteration 93/1000 | Loss: 0.00003539
Iteration 94/1000 | Loss: 0.00003538
Iteration 95/1000 | Loss: 0.00003538
Iteration 96/1000 | Loss: 0.00003537
Iteration 97/1000 | Loss: 0.00003537
Iteration 98/1000 | Loss: 0.00003537
Iteration 99/1000 | Loss: 0.00003537
Iteration 100/1000 | Loss: 0.00003537
Iteration 101/1000 | Loss: 0.00003537
Iteration 102/1000 | Loss: 0.00003536
Iteration 103/1000 | Loss: 0.00003536
Iteration 104/1000 | Loss: 0.00003536
Iteration 105/1000 | Loss: 0.00003536
Iteration 106/1000 | Loss: 0.00003536
Iteration 107/1000 | Loss: 0.00003536
Iteration 108/1000 | Loss: 0.00003536
Iteration 109/1000 | Loss: 0.00003536
Iteration 110/1000 | Loss: 0.00003535
Iteration 111/1000 | Loss: 0.00003535
Iteration 112/1000 | Loss: 0.00003534
Iteration 113/1000 | Loss: 0.00003534
Iteration 114/1000 | Loss: 0.00003534
Iteration 115/1000 | Loss: 0.00003534
Iteration 116/1000 | Loss: 0.00003533
Iteration 117/1000 | Loss: 0.00003533
Iteration 118/1000 | Loss: 0.00003533
Iteration 119/1000 | Loss: 0.00003532
Iteration 120/1000 | Loss: 0.00003532
Iteration 121/1000 | Loss: 0.00003532
Iteration 122/1000 | Loss: 0.00003532
Iteration 123/1000 | Loss: 0.00003532
Iteration 124/1000 | Loss: 0.00003532
Iteration 125/1000 | Loss: 0.00003532
Iteration 126/1000 | Loss: 0.00003531
Iteration 127/1000 | Loss: 0.00003531
Iteration 128/1000 | Loss: 0.00003531
Iteration 129/1000 | Loss: 0.00003530
Iteration 130/1000 | Loss: 0.00003530
Iteration 131/1000 | Loss: 0.00003530
Iteration 132/1000 | Loss: 0.00003530
Iteration 133/1000 | Loss: 0.00003529
Iteration 134/1000 | Loss: 0.00003529
Iteration 135/1000 | Loss: 0.00003529
Iteration 136/1000 | Loss: 0.00003528
Iteration 137/1000 | Loss: 0.00003528
Iteration 138/1000 | Loss: 0.00003528
Iteration 139/1000 | Loss: 0.00003528
Iteration 140/1000 | Loss: 0.00003528
Iteration 141/1000 | Loss: 0.00003528
Iteration 142/1000 | Loss: 0.00003528
Iteration 143/1000 | Loss: 0.00003528
Iteration 144/1000 | Loss: 0.00003527
Iteration 145/1000 | Loss: 0.00003527
Iteration 146/1000 | Loss: 0.00003527
Iteration 147/1000 | Loss: 0.00003527
Iteration 148/1000 | Loss: 0.00003527
Iteration 149/1000 | Loss: 0.00003527
Iteration 150/1000 | Loss: 0.00003526
Iteration 151/1000 | Loss: 0.00003526
Iteration 152/1000 | Loss: 0.00003526
Iteration 153/1000 | Loss: 0.00003525
Iteration 154/1000 | Loss: 0.00003525
Iteration 155/1000 | Loss: 0.00003525
Iteration 156/1000 | Loss: 0.00003525
Iteration 157/1000 | Loss: 0.00003525
Iteration 158/1000 | Loss: 0.00003525
Iteration 159/1000 | Loss: 0.00003525
Iteration 160/1000 | Loss: 0.00003525
Iteration 161/1000 | Loss: 0.00003525
Iteration 162/1000 | Loss: 0.00003525
Iteration 163/1000 | Loss: 0.00003525
Iteration 164/1000 | Loss: 0.00003525
Iteration 165/1000 | Loss: 0.00003524
Iteration 166/1000 | Loss: 0.00003524
Iteration 167/1000 | Loss: 0.00003524
Iteration 168/1000 | Loss: 0.00003524
Iteration 169/1000 | Loss: 0.00003524
Iteration 170/1000 | Loss: 0.00003524
Iteration 171/1000 | Loss: 0.00003524
Iteration 172/1000 | Loss: 0.00003524
Iteration 173/1000 | Loss: 0.00003524
Iteration 174/1000 | Loss: 0.00003524
Iteration 175/1000 | Loss: 0.00003524
Iteration 176/1000 | Loss: 0.00003523
Iteration 177/1000 | Loss: 0.00003523
Iteration 178/1000 | Loss: 0.00003523
Iteration 179/1000 | Loss: 0.00003523
Iteration 180/1000 | Loss: 0.00003523
Iteration 181/1000 | Loss: 0.00003523
Iteration 182/1000 | Loss: 0.00003523
Iteration 183/1000 | Loss: 0.00003523
Iteration 184/1000 | Loss: 0.00003523
Iteration 185/1000 | Loss: 0.00003523
Iteration 186/1000 | Loss: 0.00003523
Iteration 187/1000 | Loss: 0.00003523
Iteration 188/1000 | Loss: 0.00003523
Iteration 189/1000 | Loss: 0.00003523
Iteration 190/1000 | Loss: 0.00003523
Iteration 191/1000 | Loss: 0.00003523
Iteration 192/1000 | Loss: 0.00003523
Iteration 193/1000 | Loss: 0.00003523
Iteration 194/1000 | Loss: 0.00003522
Iteration 195/1000 | Loss: 0.00003522
Iteration 196/1000 | Loss: 0.00003522
Iteration 197/1000 | Loss: 0.00003522
Iteration 198/1000 | Loss: 0.00003522
Iteration 199/1000 | Loss: 0.00003522
Iteration 200/1000 | Loss: 0.00003522
Iteration 201/1000 | Loss: 0.00003522
Iteration 202/1000 | Loss: 0.00003522
Iteration 203/1000 | Loss: 0.00003522
Iteration 204/1000 | Loss: 0.00003522
Iteration 205/1000 | Loss: 0.00003522
Iteration 206/1000 | Loss: 0.00003522
Iteration 207/1000 | Loss: 0.00003522
Iteration 208/1000 | Loss: 0.00003522
Iteration 209/1000 | Loss: 0.00003522
Iteration 210/1000 | Loss: 0.00003522
Iteration 211/1000 | Loss: 0.00003522
Iteration 212/1000 | Loss: 0.00003522
Iteration 213/1000 | Loss: 0.00003522
Iteration 214/1000 | Loss: 0.00003522
Iteration 215/1000 | Loss: 0.00003522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [3.5223947634221986e-05, 3.5223947634221986e-05, 3.5223947634221986e-05, 3.5223947634221986e-05, 3.5223947634221986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5223947634221986e-05

Optimization complete. Final v2v error: 4.743478298187256 mm

Highest mean error: 7.336419105529785 mm for frame 99

Lowest mean error: 3.210794687271118 mm for frame 140

Saving results

Total time: 64.05539464950562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030811
Iteration 2/25 | Loss: 0.01030811
Iteration 3/25 | Loss: 0.01030811
Iteration 4/25 | Loss: 0.01030811
Iteration 5/25 | Loss: 0.01030811
Iteration 6/25 | Loss: 0.01030811
Iteration 7/25 | Loss: 0.01030811
Iteration 8/25 | Loss: 0.01030811
Iteration 9/25 | Loss: 0.01030810
Iteration 10/25 | Loss: 0.01030810
Iteration 11/25 | Loss: 0.01030810
Iteration 12/25 | Loss: 0.01030810
Iteration 13/25 | Loss: 0.01030810
Iteration 14/25 | Loss: 0.01030810
Iteration 15/25 | Loss: 0.01030810
Iteration 16/25 | Loss: 0.01030810
Iteration 17/25 | Loss: 0.01030810
Iteration 18/25 | Loss: 0.01030810
Iteration 19/25 | Loss: 0.01030810
Iteration 20/25 | Loss: 0.01030810
Iteration 21/25 | Loss: 0.01030809
Iteration 22/25 | Loss: 0.01030809
Iteration 23/25 | Loss: 0.01030809
Iteration 24/25 | Loss: 0.01030809
Iteration 25/25 | Loss: 0.01030809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78542662
Iteration 2/25 | Loss: 0.09433879
Iteration 3/25 | Loss: 0.06482122
Iteration 4/25 | Loss: 0.06392141
Iteration 5/25 | Loss: 0.06392138
Iteration 6/25 | Loss: 0.06392138
Iteration 7/25 | Loss: 0.06392138
Iteration 8/25 | Loss: 0.06392138
Iteration 9/25 | Loss: 0.06392138
Iteration 10/25 | Loss: 0.06392138
Iteration 11/25 | Loss: 0.06392138
Iteration 12/25 | Loss: 0.06392138
Iteration 13/25 | Loss: 0.06392138
Iteration 14/25 | Loss: 0.06392138
Iteration 15/25 | Loss: 0.06392138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.06392137706279755, 0.06392137706279755, 0.06392137706279755, 0.06392137706279755, 0.06392137706279755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06392137706279755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06392138
Iteration 2/1000 | Loss: 0.00477593
Iteration 3/1000 | Loss: 0.00092762
Iteration 4/1000 | Loss: 0.00032700
Iteration 5/1000 | Loss: 0.00016990
Iteration 6/1000 | Loss: 0.00011631
Iteration 7/1000 | Loss: 0.00008501
Iteration 8/1000 | Loss: 0.00006659
Iteration 9/1000 | Loss: 0.00005629
Iteration 10/1000 | Loss: 0.00004879
Iteration 11/1000 | Loss: 0.00004355
Iteration 12/1000 | Loss: 0.00003853
Iteration 13/1000 | Loss: 0.00003534
Iteration 14/1000 | Loss: 0.00003240
Iteration 15/1000 | Loss: 0.00003015
Iteration 16/1000 | Loss: 0.00002838
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00002496
Iteration 19/1000 | Loss: 0.00002395
Iteration 20/1000 | Loss: 0.00002315
Iteration 21/1000 | Loss: 0.00002237
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002062
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001879
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001847
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001842
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001820
Iteration 46/1000 | Loss: 0.00001819
Iteration 47/1000 | Loss: 0.00001819
Iteration 48/1000 | Loss: 0.00001815
Iteration 49/1000 | Loss: 0.00001815
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001809
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001802
Iteration 65/1000 | Loss: 0.00001801
Iteration 66/1000 | Loss: 0.00001801
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001801
Iteration 70/1000 | Loss: 0.00001801
Iteration 71/1000 | Loss: 0.00001801
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001801
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001800
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001800
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001800
Iteration 82/1000 | Loss: 0.00001800
Iteration 83/1000 | Loss: 0.00001800
Iteration 84/1000 | Loss: 0.00001800
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001798
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001797
Iteration 94/1000 | Loss: 0.00001797
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001792
Iteration 116/1000 | Loss: 0.00001792
Iteration 117/1000 | Loss: 0.00001792
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001792
Iteration 121/1000 | Loss: 0.00001792
Iteration 122/1000 | Loss: 0.00001792
Iteration 123/1000 | Loss: 0.00001792
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001791
Iteration 127/1000 | Loss: 0.00001791
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001791
Iteration 130/1000 | Loss: 0.00001791
Iteration 131/1000 | Loss: 0.00001791
Iteration 132/1000 | Loss: 0.00001791
Iteration 133/1000 | Loss: 0.00001791
Iteration 134/1000 | Loss: 0.00001790
Iteration 135/1000 | Loss: 0.00001790
Iteration 136/1000 | Loss: 0.00001790
Iteration 137/1000 | Loss: 0.00001790
Iteration 138/1000 | Loss: 0.00001790
Iteration 139/1000 | Loss: 0.00001790
Iteration 140/1000 | Loss: 0.00001790
Iteration 141/1000 | Loss: 0.00001790
Iteration 142/1000 | Loss: 0.00001790
Iteration 143/1000 | Loss: 0.00001790
Iteration 144/1000 | Loss: 0.00001789
Iteration 145/1000 | Loss: 0.00001789
Iteration 146/1000 | Loss: 0.00001789
Iteration 147/1000 | Loss: 0.00001789
Iteration 148/1000 | Loss: 0.00001789
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001789
Iteration 152/1000 | Loss: 0.00001789
Iteration 153/1000 | Loss: 0.00001789
Iteration 154/1000 | Loss: 0.00001789
Iteration 155/1000 | Loss: 0.00001789
Iteration 156/1000 | Loss: 0.00001789
Iteration 157/1000 | Loss: 0.00001788
Iteration 158/1000 | Loss: 0.00001788
Iteration 159/1000 | Loss: 0.00001788
Iteration 160/1000 | Loss: 0.00001788
Iteration 161/1000 | Loss: 0.00001788
Iteration 162/1000 | Loss: 0.00001788
Iteration 163/1000 | Loss: 0.00001788
Iteration 164/1000 | Loss: 0.00001788
Iteration 165/1000 | Loss: 0.00001788
Iteration 166/1000 | Loss: 0.00001788
Iteration 167/1000 | Loss: 0.00001788
Iteration 168/1000 | Loss: 0.00001788
Iteration 169/1000 | Loss: 0.00001788
Iteration 170/1000 | Loss: 0.00001788
Iteration 171/1000 | Loss: 0.00001788
Iteration 172/1000 | Loss: 0.00001788
Iteration 173/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.7879499864648096e-05, 1.7879499864648096e-05, 1.7879499864648096e-05, 1.7879499864648096e-05, 1.7879499864648096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7879499864648096e-05

Optimization complete. Final v2v error: 3.41115140914917 mm

Highest mean error: 5.3409833908081055 mm for frame 53

Lowest mean error: 2.773514986038208 mm for frame 188

Saving results

Total time: 71.05412030220032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038822
Iteration 2/25 | Loss: 0.00207280
Iteration 3/25 | Loss: 0.00160234
Iteration 4/25 | Loss: 0.00153509
Iteration 5/25 | Loss: 0.00149191
Iteration 6/25 | Loss: 0.00146554
Iteration 7/25 | Loss: 0.00147050
Iteration 8/25 | Loss: 0.00142606
Iteration 9/25 | Loss: 0.00138952
Iteration 10/25 | Loss: 0.00138139
Iteration 11/25 | Loss: 0.00137909
Iteration 12/25 | Loss: 0.00137835
Iteration 13/25 | Loss: 0.00138584
Iteration 14/25 | Loss: 0.00140408
Iteration 15/25 | Loss: 0.00140903
Iteration 16/25 | Loss: 0.00137726
Iteration 17/25 | Loss: 0.00136855
Iteration 18/25 | Loss: 0.00136784
Iteration 19/25 | Loss: 0.00136943
Iteration 20/25 | Loss: 0.00136876
Iteration 21/25 | Loss: 0.00136769
Iteration 22/25 | Loss: 0.00136722
Iteration 23/25 | Loss: 0.00136634
Iteration 24/25 | Loss: 0.00136589
Iteration 25/25 | Loss: 0.00136567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19519150
Iteration 2/25 | Loss: 0.00106064
Iteration 3/25 | Loss: 0.00106063
Iteration 4/25 | Loss: 0.00106063
Iteration 5/25 | Loss: 0.00106063
Iteration 6/25 | Loss: 0.00106063
Iteration 7/25 | Loss: 0.00106063
Iteration 8/25 | Loss: 0.00106063
Iteration 9/25 | Loss: 0.00106063
Iteration 10/25 | Loss: 0.00106063
Iteration 11/25 | Loss: 0.00106063
Iteration 12/25 | Loss: 0.00106063
Iteration 13/25 | Loss: 0.00106063
Iteration 14/25 | Loss: 0.00106063
Iteration 15/25 | Loss: 0.00106063
Iteration 16/25 | Loss: 0.00106063
Iteration 17/25 | Loss: 0.00106063
Iteration 18/25 | Loss: 0.00106063
Iteration 19/25 | Loss: 0.00106063
Iteration 20/25 | Loss: 0.00106063
Iteration 21/25 | Loss: 0.00106063
Iteration 22/25 | Loss: 0.00106063
Iteration 23/25 | Loss: 0.00106063
Iteration 24/25 | Loss: 0.00106063
Iteration 25/25 | Loss: 0.00106063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106063
Iteration 2/1000 | Loss: 0.00006043
Iteration 3/1000 | Loss: 0.00003918
Iteration 4/1000 | Loss: 0.00003546
Iteration 5/1000 | Loss: 0.00003343
Iteration 6/1000 | Loss: 0.00003204
Iteration 7/1000 | Loss: 0.00003111
Iteration 8/1000 | Loss: 0.00074665
Iteration 9/1000 | Loss: 0.00003775
Iteration 10/1000 | Loss: 0.00002960
Iteration 11/1000 | Loss: 0.00002798
Iteration 12/1000 | Loss: 0.00002700
Iteration 13/1000 | Loss: 0.00002627
Iteration 14/1000 | Loss: 0.00002579
Iteration 15/1000 | Loss: 0.00002545
Iteration 16/1000 | Loss: 0.00002517
Iteration 17/1000 | Loss: 0.00002496
Iteration 18/1000 | Loss: 0.00002471
Iteration 19/1000 | Loss: 0.00002460
Iteration 20/1000 | Loss: 0.00002459
Iteration 21/1000 | Loss: 0.00002457
Iteration 22/1000 | Loss: 0.00002447
Iteration 23/1000 | Loss: 0.00002447
Iteration 24/1000 | Loss: 0.00002446
Iteration 25/1000 | Loss: 0.00002445
Iteration 26/1000 | Loss: 0.00002445
Iteration 27/1000 | Loss: 0.00002444
Iteration 28/1000 | Loss: 0.00002444
Iteration 29/1000 | Loss: 0.00002444
Iteration 30/1000 | Loss: 0.00002444
Iteration 31/1000 | Loss: 0.00002444
Iteration 32/1000 | Loss: 0.00002444
Iteration 33/1000 | Loss: 0.00002444
Iteration 34/1000 | Loss: 0.00002443
Iteration 35/1000 | Loss: 0.00002443
Iteration 36/1000 | Loss: 0.00002443
Iteration 37/1000 | Loss: 0.00002442
Iteration 38/1000 | Loss: 0.00002442
Iteration 39/1000 | Loss: 0.00002442
Iteration 40/1000 | Loss: 0.00002442
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002441
Iteration 43/1000 | Loss: 0.00002441
Iteration 44/1000 | Loss: 0.00002441
Iteration 45/1000 | Loss: 0.00002441
Iteration 46/1000 | Loss: 0.00002441
Iteration 47/1000 | Loss: 0.00002440
Iteration 48/1000 | Loss: 0.00002440
Iteration 49/1000 | Loss: 0.00002440
Iteration 50/1000 | Loss: 0.00002440
Iteration 51/1000 | Loss: 0.00002440
Iteration 52/1000 | Loss: 0.00002440
Iteration 53/1000 | Loss: 0.00002439
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002438
Iteration 56/1000 | Loss: 0.00002437
Iteration 57/1000 | Loss: 0.00002437
Iteration 58/1000 | Loss: 0.00002436
Iteration 59/1000 | Loss: 0.00002436
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002435
Iteration 62/1000 | Loss: 0.00002435
Iteration 63/1000 | Loss: 0.00002435
Iteration 64/1000 | Loss: 0.00002434
Iteration 65/1000 | Loss: 0.00002433
Iteration 66/1000 | Loss: 0.00002433
Iteration 67/1000 | Loss: 0.00002433
Iteration 68/1000 | Loss: 0.00002432
Iteration 69/1000 | Loss: 0.00002432
Iteration 70/1000 | Loss: 0.00002431
Iteration 71/1000 | Loss: 0.00002431
Iteration 72/1000 | Loss: 0.00002429
Iteration 73/1000 | Loss: 0.00002428
Iteration 74/1000 | Loss: 0.00002428
Iteration 75/1000 | Loss: 0.00002427
Iteration 76/1000 | Loss: 0.00002425
Iteration 77/1000 | Loss: 0.00002425
Iteration 78/1000 | Loss: 0.00002425
Iteration 79/1000 | Loss: 0.00002425
Iteration 80/1000 | Loss: 0.00002425
Iteration 81/1000 | Loss: 0.00002425
Iteration 82/1000 | Loss: 0.00002425
Iteration 83/1000 | Loss: 0.00002424
Iteration 84/1000 | Loss: 0.00002424
Iteration 85/1000 | Loss: 0.00002424
Iteration 86/1000 | Loss: 0.00002424
Iteration 87/1000 | Loss: 0.00002424
Iteration 88/1000 | Loss: 0.00002423
Iteration 89/1000 | Loss: 0.00002423
Iteration 90/1000 | Loss: 0.00002423
Iteration 91/1000 | Loss: 0.00002423
Iteration 92/1000 | Loss: 0.00002423
Iteration 93/1000 | Loss: 0.00002423
Iteration 94/1000 | Loss: 0.00002423
Iteration 95/1000 | Loss: 0.00002423
Iteration 96/1000 | Loss: 0.00002422
Iteration 97/1000 | Loss: 0.00002421
Iteration 98/1000 | Loss: 0.00002421
Iteration 99/1000 | Loss: 0.00002421
Iteration 100/1000 | Loss: 0.00002421
Iteration 101/1000 | Loss: 0.00002421
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002420
Iteration 104/1000 | Loss: 0.00002420
Iteration 105/1000 | Loss: 0.00002420
Iteration 106/1000 | Loss: 0.00002420
Iteration 107/1000 | Loss: 0.00002419
Iteration 108/1000 | Loss: 0.00002419
Iteration 109/1000 | Loss: 0.00002419
Iteration 110/1000 | Loss: 0.00002419
Iteration 111/1000 | Loss: 0.00002419
Iteration 112/1000 | Loss: 0.00002419
Iteration 113/1000 | Loss: 0.00002419
Iteration 114/1000 | Loss: 0.00002419
Iteration 115/1000 | Loss: 0.00002419
Iteration 116/1000 | Loss: 0.00002419
Iteration 117/1000 | Loss: 0.00002418
Iteration 118/1000 | Loss: 0.00002418
Iteration 119/1000 | Loss: 0.00002418
Iteration 120/1000 | Loss: 0.00002418
Iteration 121/1000 | Loss: 0.00002418
Iteration 122/1000 | Loss: 0.00002418
Iteration 123/1000 | Loss: 0.00002418
Iteration 124/1000 | Loss: 0.00002417
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002417
Iteration 130/1000 | Loss: 0.00002417
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00002417
Iteration 133/1000 | Loss: 0.00002417
Iteration 134/1000 | Loss: 0.00002416
Iteration 135/1000 | Loss: 0.00002416
Iteration 136/1000 | Loss: 0.00002416
Iteration 137/1000 | Loss: 0.00002416
Iteration 138/1000 | Loss: 0.00002416
Iteration 139/1000 | Loss: 0.00002416
Iteration 140/1000 | Loss: 0.00002416
Iteration 141/1000 | Loss: 0.00002416
Iteration 142/1000 | Loss: 0.00002416
Iteration 143/1000 | Loss: 0.00002416
Iteration 144/1000 | Loss: 0.00002416
Iteration 145/1000 | Loss: 0.00002416
Iteration 146/1000 | Loss: 0.00002416
Iteration 147/1000 | Loss: 0.00002416
Iteration 148/1000 | Loss: 0.00002415
Iteration 149/1000 | Loss: 0.00002415
Iteration 150/1000 | Loss: 0.00002415
Iteration 151/1000 | Loss: 0.00002415
Iteration 152/1000 | Loss: 0.00002415
Iteration 153/1000 | Loss: 0.00002415
Iteration 154/1000 | Loss: 0.00002415
Iteration 155/1000 | Loss: 0.00002415
Iteration 156/1000 | Loss: 0.00002415
Iteration 157/1000 | Loss: 0.00002415
Iteration 158/1000 | Loss: 0.00002415
Iteration 159/1000 | Loss: 0.00002415
Iteration 160/1000 | Loss: 0.00002415
Iteration 161/1000 | Loss: 0.00002415
Iteration 162/1000 | Loss: 0.00002415
Iteration 163/1000 | Loss: 0.00002415
Iteration 164/1000 | Loss: 0.00002415
Iteration 165/1000 | Loss: 0.00002415
Iteration 166/1000 | Loss: 0.00002415
Iteration 167/1000 | Loss: 0.00002415
Iteration 168/1000 | Loss: 0.00002415
Iteration 169/1000 | Loss: 0.00002415
Iteration 170/1000 | Loss: 0.00002415
Iteration 171/1000 | Loss: 0.00002415
Iteration 172/1000 | Loss: 0.00002415
Iteration 173/1000 | Loss: 0.00002415
Iteration 174/1000 | Loss: 0.00002415
Iteration 175/1000 | Loss: 0.00002415
Iteration 176/1000 | Loss: 0.00002415
Iteration 177/1000 | Loss: 0.00002415
Iteration 178/1000 | Loss: 0.00002415
Iteration 179/1000 | Loss: 0.00002415
Iteration 180/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.4146624127752148e-05, 2.4146624127752148e-05, 2.4146624127752148e-05, 2.4146624127752148e-05, 2.4146624127752148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4146624127752148e-05

Optimization complete. Final v2v error: 3.9843530654907227 mm

Highest mean error: 5.371088981628418 mm for frame 230

Lowest mean error: 3.4859514236450195 mm for frame 53

Saving results

Total time: 91.32153010368347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509231
Iteration 2/25 | Loss: 0.00132740
Iteration 3/25 | Loss: 0.00123079
Iteration 4/25 | Loss: 0.00121548
Iteration 5/25 | Loss: 0.00121323
Iteration 6/25 | Loss: 0.00121130
Iteration 7/25 | Loss: 0.00121003
Iteration 8/25 | Loss: 0.00121161
Iteration 9/25 | Loss: 0.00120976
Iteration 10/25 | Loss: 0.00120974
Iteration 11/25 | Loss: 0.00120974
Iteration 12/25 | Loss: 0.00120974
Iteration 13/25 | Loss: 0.00120974
Iteration 14/25 | Loss: 0.00120974
Iteration 15/25 | Loss: 0.00120974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012097387807443738, 0.0012097387807443738, 0.0012097387807443738, 0.0012097387807443738, 0.0012097387807443738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012097387807443738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.79323387
Iteration 2/25 | Loss: 0.00117483
Iteration 3/25 | Loss: 0.00117482
Iteration 4/25 | Loss: 0.00117481
Iteration 5/25 | Loss: 0.00117481
Iteration 6/25 | Loss: 0.00117481
Iteration 7/25 | Loss: 0.00117481
Iteration 8/25 | Loss: 0.00117481
Iteration 9/25 | Loss: 0.00117481
Iteration 10/25 | Loss: 0.00117481
Iteration 11/25 | Loss: 0.00117481
Iteration 12/25 | Loss: 0.00117481
Iteration 13/25 | Loss: 0.00117481
Iteration 14/25 | Loss: 0.00117481
Iteration 15/25 | Loss: 0.00117481
Iteration 16/25 | Loss: 0.00117481
Iteration 17/25 | Loss: 0.00117481
Iteration 18/25 | Loss: 0.00117481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011748127872124314, 0.0011748127872124314, 0.0011748127872124314, 0.0011748127872124314, 0.0011748127872124314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011748127872124314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117481
Iteration 2/1000 | Loss: 0.00002235
Iteration 3/1000 | Loss: 0.00003111
Iteration 4/1000 | Loss: 0.00007956
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00008048
Iteration 9/1000 | Loss: 0.00002766
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001351
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00003861
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00004971
Iteration 18/1000 | Loss: 0.00003950
Iteration 19/1000 | Loss: 0.00002907
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001274
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00002667
Iteration 27/1000 | Loss: 0.00002667
Iteration 28/1000 | Loss: 0.00014698
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001617
Iteration 44/1000 | Loss: 0.00001254
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001253
Iteration 51/1000 | Loss: 0.00001253
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001252
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001638
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001241
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001891
Iteration 73/1000 | Loss: 0.00011272
Iteration 74/1000 | Loss: 0.00003290
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001238
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001229
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001228
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Iteration 135/1000 | Loss: 0.00001228
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001228
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001226
Iteration 156/1000 | Loss: 0.00001226
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2263965800229926e-05, 1.2263965800229926e-05, 1.2263965800229926e-05, 1.2263965800229926e-05, 1.2263965800229926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2263965800229926e-05

Optimization complete. Final v2v error: 2.964841604232788 mm

Highest mean error: 3.7467594146728516 mm for frame 201

Lowest mean error: 2.736999750137329 mm for frame 181

Saving results

Total time: 78.44077634811401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495471
Iteration 2/25 | Loss: 0.00147800
Iteration 3/25 | Loss: 0.00129749
Iteration 4/25 | Loss: 0.00127809
Iteration 5/25 | Loss: 0.00127228
Iteration 6/25 | Loss: 0.00127021
Iteration 7/25 | Loss: 0.00126954
Iteration 8/25 | Loss: 0.00126930
Iteration 9/25 | Loss: 0.00126930
Iteration 10/25 | Loss: 0.00126930
Iteration 11/25 | Loss: 0.00126930
Iteration 12/25 | Loss: 0.00126930
Iteration 13/25 | Loss: 0.00126930
Iteration 14/25 | Loss: 0.00126930
Iteration 15/25 | Loss: 0.00126930
Iteration 16/25 | Loss: 0.00126930
Iteration 17/25 | Loss: 0.00126930
Iteration 18/25 | Loss: 0.00126930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012693003518506885, 0.0012693003518506885, 0.0012693003518506885, 0.0012693003518506885, 0.0012693003518506885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012693003518506885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32916260
Iteration 2/25 | Loss: 0.00156216
Iteration 3/25 | Loss: 0.00156215
Iteration 4/25 | Loss: 0.00156215
Iteration 5/25 | Loss: 0.00156215
Iteration 6/25 | Loss: 0.00156215
Iteration 7/25 | Loss: 0.00156215
Iteration 8/25 | Loss: 0.00156215
Iteration 9/25 | Loss: 0.00156215
Iteration 10/25 | Loss: 0.00156215
Iteration 11/25 | Loss: 0.00156215
Iteration 12/25 | Loss: 0.00156215
Iteration 13/25 | Loss: 0.00156215
Iteration 14/25 | Loss: 0.00156215
Iteration 15/25 | Loss: 0.00156215
Iteration 16/25 | Loss: 0.00156215
Iteration 17/25 | Loss: 0.00156214
Iteration 18/25 | Loss: 0.00156215
Iteration 19/25 | Loss: 0.00156215
Iteration 20/25 | Loss: 0.00156215
Iteration 21/25 | Loss: 0.00156215
Iteration 22/25 | Loss: 0.00156215
Iteration 23/25 | Loss: 0.00156214
Iteration 24/25 | Loss: 0.00156215
Iteration 25/25 | Loss: 0.00156214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156214
Iteration 2/1000 | Loss: 0.00006496
Iteration 3/1000 | Loss: 0.00004080
Iteration 4/1000 | Loss: 0.00003333
Iteration 5/1000 | Loss: 0.00003096
Iteration 6/1000 | Loss: 0.00002944
Iteration 7/1000 | Loss: 0.00002836
Iteration 8/1000 | Loss: 0.00002758
Iteration 9/1000 | Loss: 0.00002675
Iteration 10/1000 | Loss: 0.00002621
Iteration 11/1000 | Loss: 0.00002577
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002492
Iteration 15/1000 | Loss: 0.00002475
Iteration 16/1000 | Loss: 0.00002460
Iteration 17/1000 | Loss: 0.00002446
Iteration 18/1000 | Loss: 0.00002442
Iteration 19/1000 | Loss: 0.00002441
Iteration 20/1000 | Loss: 0.00002440
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002438
Iteration 23/1000 | Loss: 0.00002437
Iteration 24/1000 | Loss: 0.00002437
Iteration 25/1000 | Loss: 0.00002436
Iteration 26/1000 | Loss: 0.00002435
Iteration 27/1000 | Loss: 0.00002434
Iteration 28/1000 | Loss: 0.00002434
Iteration 29/1000 | Loss: 0.00002433
Iteration 30/1000 | Loss: 0.00002433
Iteration 31/1000 | Loss: 0.00002433
Iteration 32/1000 | Loss: 0.00002432
Iteration 33/1000 | Loss: 0.00002431
Iteration 34/1000 | Loss: 0.00002431
Iteration 35/1000 | Loss: 0.00002431
Iteration 36/1000 | Loss: 0.00002430
Iteration 37/1000 | Loss: 0.00002429
Iteration 38/1000 | Loss: 0.00002428
Iteration 39/1000 | Loss: 0.00002426
Iteration 40/1000 | Loss: 0.00002425
Iteration 41/1000 | Loss: 0.00002425
Iteration 42/1000 | Loss: 0.00002425
Iteration 43/1000 | Loss: 0.00002424
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002422
Iteration 49/1000 | Loss: 0.00002422
Iteration 50/1000 | Loss: 0.00002422
Iteration 51/1000 | Loss: 0.00002421
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002420
Iteration 54/1000 | Loss: 0.00002420
Iteration 55/1000 | Loss: 0.00002419
Iteration 56/1000 | Loss: 0.00002419
Iteration 57/1000 | Loss: 0.00002419
Iteration 58/1000 | Loss: 0.00002418
Iteration 59/1000 | Loss: 0.00002418
Iteration 60/1000 | Loss: 0.00002418
Iteration 61/1000 | Loss: 0.00002418
Iteration 62/1000 | Loss: 0.00002417
Iteration 63/1000 | Loss: 0.00002417
Iteration 64/1000 | Loss: 0.00002417
Iteration 65/1000 | Loss: 0.00002416
Iteration 66/1000 | Loss: 0.00002416
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002415
Iteration 69/1000 | Loss: 0.00002415
Iteration 70/1000 | Loss: 0.00002415
Iteration 71/1000 | Loss: 0.00002415
Iteration 72/1000 | Loss: 0.00002415
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002413
Iteration 77/1000 | Loss: 0.00002413
Iteration 78/1000 | Loss: 0.00002413
Iteration 79/1000 | Loss: 0.00002413
Iteration 80/1000 | Loss: 0.00002412
Iteration 81/1000 | Loss: 0.00002412
Iteration 82/1000 | Loss: 0.00002412
Iteration 83/1000 | Loss: 0.00002412
Iteration 84/1000 | Loss: 0.00002412
Iteration 85/1000 | Loss: 0.00002412
Iteration 86/1000 | Loss: 0.00002412
Iteration 87/1000 | Loss: 0.00002411
Iteration 88/1000 | Loss: 0.00002411
Iteration 89/1000 | Loss: 0.00002411
Iteration 90/1000 | Loss: 0.00002411
Iteration 91/1000 | Loss: 0.00002410
Iteration 92/1000 | Loss: 0.00002410
Iteration 93/1000 | Loss: 0.00002410
Iteration 94/1000 | Loss: 0.00002410
Iteration 95/1000 | Loss: 0.00002410
Iteration 96/1000 | Loss: 0.00002409
Iteration 97/1000 | Loss: 0.00002409
Iteration 98/1000 | Loss: 0.00002409
Iteration 99/1000 | Loss: 0.00002409
Iteration 100/1000 | Loss: 0.00002409
Iteration 101/1000 | Loss: 0.00002409
Iteration 102/1000 | Loss: 0.00002408
Iteration 103/1000 | Loss: 0.00002408
Iteration 104/1000 | Loss: 0.00002408
Iteration 105/1000 | Loss: 0.00002407
Iteration 106/1000 | Loss: 0.00002407
Iteration 107/1000 | Loss: 0.00002407
Iteration 108/1000 | Loss: 0.00002407
Iteration 109/1000 | Loss: 0.00002407
Iteration 110/1000 | Loss: 0.00002407
Iteration 111/1000 | Loss: 0.00002407
Iteration 112/1000 | Loss: 0.00002406
Iteration 113/1000 | Loss: 0.00002406
Iteration 114/1000 | Loss: 0.00002406
Iteration 115/1000 | Loss: 0.00002406
Iteration 116/1000 | Loss: 0.00002405
Iteration 117/1000 | Loss: 0.00002405
Iteration 118/1000 | Loss: 0.00002405
Iteration 119/1000 | Loss: 0.00002405
Iteration 120/1000 | Loss: 0.00002405
Iteration 121/1000 | Loss: 0.00002405
Iteration 122/1000 | Loss: 0.00002405
Iteration 123/1000 | Loss: 0.00002405
Iteration 124/1000 | Loss: 0.00002405
Iteration 125/1000 | Loss: 0.00002405
Iteration 126/1000 | Loss: 0.00002405
Iteration 127/1000 | Loss: 0.00002405
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002404
Iteration 130/1000 | Loss: 0.00002404
Iteration 131/1000 | Loss: 0.00002404
Iteration 132/1000 | Loss: 0.00002404
Iteration 133/1000 | Loss: 0.00002404
Iteration 134/1000 | Loss: 0.00002404
Iteration 135/1000 | Loss: 0.00002404
Iteration 136/1000 | Loss: 0.00002403
Iteration 137/1000 | Loss: 0.00002403
Iteration 138/1000 | Loss: 0.00002403
Iteration 139/1000 | Loss: 0.00002403
Iteration 140/1000 | Loss: 0.00002403
Iteration 141/1000 | Loss: 0.00002403
Iteration 142/1000 | Loss: 0.00002403
Iteration 143/1000 | Loss: 0.00002403
Iteration 144/1000 | Loss: 0.00002403
Iteration 145/1000 | Loss: 0.00002402
Iteration 146/1000 | Loss: 0.00002402
Iteration 147/1000 | Loss: 0.00002402
Iteration 148/1000 | Loss: 0.00002402
Iteration 149/1000 | Loss: 0.00002402
Iteration 150/1000 | Loss: 0.00002402
Iteration 151/1000 | Loss: 0.00002402
Iteration 152/1000 | Loss: 0.00002402
Iteration 153/1000 | Loss: 0.00002402
Iteration 154/1000 | Loss: 0.00002402
Iteration 155/1000 | Loss: 0.00002402
Iteration 156/1000 | Loss: 0.00002401
Iteration 157/1000 | Loss: 0.00002401
Iteration 158/1000 | Loss: 0.00002401
Iteration 159/1000 | Loss: 0.00002401
Iteration 160/1000 | Loss: 0.00002401
Iteration 161/1000 | Loss: 0.00002401
Iteration 162/1000 | Loss: 0.00002401
Iteration 163/1000 | Loss: 0.00002401
Iteration 164/1000 | Loss: 0.00002401
Iteration 165/1000 | Loss: 0.00002401
Iteration 166/1000 | Loss: 0.00002401
Iteration 167/1000 | Loss: 0.00002401
Iteration 168/1000 | Loss: 0.00002401
Iteration 169/1000 | Loss: 0.00002401
Iteration 170/1000 | Loss: 0.00002401
Iteration 171/1000 | Loss: 0.00002401
Iteration 172/1000 | Loss: 0.00002401
Iteration 173/1000 | Loss: 0.00002401
Iteration 174/1000 | Loss: 0.00002401
Iteration 175/1000 | Loss: 0.00002401
Iteration 176/1000 | Loss: 0.00002401
Iteration 177/1000 | Loss: 0.00002401
Iteration 178/1000 | Loss: 0.00002401
Iteration 179/1000 | Loss: 0.00002401
Iteration 180/1000 | Loss: 0.00002401
Iteration 181/1000 | Loss: 0.00002401
Iteration 182/1000 | Loss: 0.00002401
Iteration 183/1000 | Loss: 0.00002400
Iteration 184/1000 | Loss: 0.00002400
Iteration 185/1000 | Loss: 0.00002400
Iteration 186/1000 | Loss: 0.00002400
Iteration 187/1000 | Loss: 0.00002400
Iteration 188/1000 | Loss: 0.00002400
Iteration 189/1000 | Loss: 0.00002400
Iteration 190/1000 | Loss: 0.00002400
Iteration 191/1000 | Loss: 0.00002400
Iteration 192/1000 | Loss: 0.00002400
Iteration 193/1000 | Loss: 0.00002400
Iteration 194/1000 | Loss: 0.00002400
Iteration 195/1000 | Loss: 0.00002400
Iteration 196/1000 | Loss: 0.00002400
Iteration 197/1000 | Loss: 0.00002400
Iteration 198/1000 | Loss: 0.00002400
Iteration 199/1000 | Loss: 0.00002400
Iteration 200/1000 | Loss: 0.00002400
Iteration 201/1000 | Loss: 0.00002400
Iteration 202/1000 | Loss: 0.00002400
Iteration 203/1000 | Loss: 0.00002400
Iteration 204/1000 | Loss: 0.00002400
Iteration 205/1000 | Loss: 0.00002400
Iteration 206/1000 | Loss: 0.00002400
Iteration 207/1000 | Loss: 0.00002400
Iteration 208/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.4004277292988263e-05, 2.4004277292988263e-05, 2.4004277292988263e-05, 2.4004277292988263e-05, 2.4004277292988263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4004277292988263e-05

Optimization complete. Final v2v error: 4.006880760192871 mm

Highest mean error: 5.243155479431152 mm for frame 106

Lowest mean error: 2.869128465652466 mm for frame 75

Saving results

Total time: 48.52208161354065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402157
Iteration 2/25 | Loss: 0.00123752
Iteration 3/25 | Loss: 0.00118106
Iteration 4/25 | Loss: 0.00117258
Iteration 5/25 | Loss: 0.00116974
Iteration 6/25 | Loss: 0.00116939
Iteration 7/25 | Loss: 0.00116939
Iteration 8/25 | Loss: 0.00116939
Iteration 9/25 | Loss: 0.00116939
Iteration 10/25 | Loss: 0.00116939
Iteration 11/25 | Loss: 0.00116939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011693936539813876, 0.0011693936539813876, 0.0011693936539813876, 0.0011693936539813876, 0.0011693936539813876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011693936539813876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84311533
Iteration 2/25 | Loss: 0.00107570
Iteration 3/25 | Loss: 0.00107568
Iteration 4/25 | Loss: 0.00107568
Iteration 5/25 | Loss: 0.00107568
Iteration 6/25 | Loss: 0.00107568
Iteration 7/25 | Loss: 0.00107568
Iteration 8/25 | Loss: 0.00107568
Iteration 9/25 | Loss: 0.00107568
Iteration 10/25 | Loss: 0.00107568
Iteration 11/25 | Loss: 0.00107568
Iteration 12/25 | Loss: 0.00107568
Iteration 13/25 | Loss: 0.00107568
Iteration 14/25 | Loss: 0.00107568
Iteration 15/25 | Loss: 0.00107568
Iteration 16/25 | Loss: 0.00107568
Iteration 17/25 | Loss: 0.00107568
Iteration 18/25 | Loss: 0.00107568
Iteration 19/25 | Loss: 0.00107568
Iteration 20/25 | Loss: 0.00107568
Iteration 21/25 | Loss: 0.00107568
Iteration 22/25 | Loss: 0.00107568
Iteration 23/25 | Loss: 0.00107568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010756796691566706, 0.0010756796691566706, 0.0010756796691566706, 0.0010756796691566706, 0.0010756796691566706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010756796691566706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107568
Iteration 2/1000 | Loss: 0.00001832
Iteration 3/1000 | Loss: 0.00001443
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001247
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001170
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001087
Iteration 12/1000 | Loss: 0.00001080
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001061
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001058
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001046
Iteration 20/1000 | Loss: 0.00001044
Iteration 21/1000 | Loss: 0.00001043
Iteration 22/1000 | Loss: 0.00001043
Iteration 23/1000 | Loss: 0.00001042
Iteration 24/1000 | Loss: 0.00001042
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001040
Iteration 27/1000 | Loss: 0.00001039
Iteration 28/1000 | Loss: 0.00001038
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001031
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001029
Iteration 41/1000 | Loss: 0.00001029
Iteration 42/1000 | Loss: 0.00001028
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001027
Iteration 45/1000 | Loss: 0.00001025
Iteration 46/1000 | Loss: 0.00001025
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001024
Iteration 50/1000 | Loss: 0.00001022
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001022
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001020
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001016
Iteration 61/1000 | Loss: 0.00001016
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001014
Iteration 65/1000 | Loss: 0.00001013
Iteration 66/1000 | Loss: 0.00001013
Iteration 67/1000 | Loss: 0.00001013
Iteration 68/1000 | Loss: 0.00001012
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001011
Iteration 72/1000 | Loss: 0.00001011
Iteration 73/1000 | Loss: 0.00001011
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001010
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001009
Iteration 83/1000 | Loss: 0.00001009
Iteration 84/1000 | Loss: 0.00001009
Iteration 85/1000 | Loss: 0.00001009
Iteration 86/1000 | Loss: 0.00001009
Iteration 87/1000 | Loss: 0.00001009
Iteration 88/1000 | Loss: 0.00001008
Iteration 89/1000 | Loss: 0.00001008
Iteration 90/1000 | Loss: 0.00001008
Iteration 91/1000 | Loss: 0.00001008
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001008
Iteration 95/1000 | Loss: 0.00001008
Iteration 96/1000 | Loss: 0.00001008
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001007
Iteration 102/1000 | Loss: 0.00001007
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001006
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001005
Iteration 107/1000 | Loss: 0.00001004
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001004
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001002
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001000
Iteration 125/1000 | Loss: 0.00001000
Iteration 126/1000 | Loss: 0.00001000
Iteration 127/1000 | Loss: 0.00001000
Iteration 128/1000 | Loss: 0.00000999
Iteration 129/1000 | Loss: 0.00000999
Iteration 130/1000 | Loss: 0.00000999
Iteration 131/1000 | Loss: 0.00000999
Iteration 132/1000 | Loss: 0.00000999
Iteration 133/1000 | Loss: 0.00000999
Iteration 134/1000 | Loss: 0.00000999
Iteration 135/1000 | Loss: 0.00000999
Iteration 136/1000 | Loss: 0.00000998
Iteration 137/1000 | Loss: 0.00000998
Iteration 138/1000 | Loss: 0.00000998
Iteration 139/1000 | Loss: 0.00000998
Iteration 140/1000 | Loss: 0.00000998
Iteration 141/1000 | Loss: 0.00000998
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000996
Iteration 150/1000 | Loss: 0.00000996
Iteration 151/1000 | Loss: 0.00000996
Iteration 152/1000 | Loss: 0.00000996
Iteration 153/1000 | Loss: 0.00000996
Iteration 154/1000 | Loss: 0.00000995
Iteration 155/1000 | Loss: 0.00000995
Iteration 156/1000 | Loss: 0.00000995
Iteration 157/1000 | Loss: 0.00000995
Iteration 158/1000 | Loss: 0.00000995
Iteration 159/1000 | Loss: 0.00000995
Iteration 160/1000 | Loss: 0.00000995
Iteration 161/1000 | Loss: 0.00000995
Iteration 162/1000 | Loss: 0.00000995
Iteration 163/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [9.952842447091825e-06, 9.952842447091825e-06, 9.952842447091825e-06, 9.952842447091825e-06, 9.952842447091825e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.952842447091825e-06

Optimization complete. Final v2v error: 2.7309181690216064 mm

Highest mean error: 2.947108507156372 mm for frame 120

Lowest mean error: 2.6444435119628906 mm for frame 182

Saving results

Total time: 38.51360034942627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00723040
Iteration 2/25 | Loss: 0.00138985
Iteration 3/25 | Loss: 0.00126437
Iteration 4/25 | Loss: 0.00124735
Iteration 5/25 | Loss: 0.00124288
Iteration 6/25 | Loss: 0.00124216
Iteration 7/25 | Loss: 0.00124216
Iteration 8/25 | Loss: 0.00124216
Iteration 9/25 | Loss: 0.00124216
Iteration 10/25 | Loss: 0.00124216
Iteration 11/25 | Loss: 0.00124216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012421617284417152, 0.0012421617284417152, 0.0012421617284417152, 0.0012421617284417152, 0.0012421617284417152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012421617284417152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.97571182
Iteration 2/25 | Loss: 0.00111392
Iteration 3/25 | Loss: 0.00111381
Iteration 4/25 | Loss: 0.00111381
Iteration 5/25 | Loss: 0.00111381
Iteration 6/25 | Loss: 0.00111381
Iteration 7/25 | Loss: 0.00111381
Iteration 8/25 | Loss: 0.00111381
Iteration 9/25 | Loss: 0.00111381
Iteration 10/25 | Loss: 0.00111381
Iteration 11/25 | Loss: 0.00111381
Iteration 12/25 | Loss: 0.00111381
Iteration 13/25 | Loss: 0.00111381
Iteration 14/25 | Loss: 0.00111381
Iteration 15/25 | Loss: 0.00111381
Iteration 16/25 | Loss: 0.00111381
Iteration 17/25 | Loss: 0.00111381
Iteration 18/25 | Loss: 0.00111381
Iteration 19/25 | Loss: 0.00111381
Iteration 20/25 | Loss: 0.00111381
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011138069676235318, 0.0011138069676235318, 0.0011138069676235318, 0.0011138069676235318, 0.0011138069676235318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011138069676235318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111381
Iteration 2/1000 | Loss: 0.00004408
Iteration 3/1000 | Loss: 0.00002896
Iteration 4/1000 | Loss: 0.00002519
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002212
Iteration 7/1000 | Loss: 0.00002120
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00001994
Iteration 11/1000 | Loss: 0.00001965
Iteration 12/1000 | Loss: 0.00001955
Iteration 13/1000 | Loss: 0.00001947
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001929
Iteration 16/1000 | Loss: 0.00001928
Iteration 17/1000 | Loss: 0.00001913
Iteration 18/1000 | Loss: 0.00001913
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001903
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001895
Iteration 31/1000 | Loss: 0.00001895
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001893
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001892
Iteration 39/1000 | Loss: 0.00001892
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001889
Iteration 45/1000 | Loss: 0.00001889
Iteration 46/1000 | Loss: 0.00001889
Iteration 47/1000 | Loss: 0.00001888
Iteration 48/1000 | Loss: 0.00001888
Iteration 49/1000 | Loss: 0.00001887
Iteration 50/1000 | Loss: 0.00001887
Iteration 51/1000 | Loss: 0.00001887
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001887
Iteration 54/1000 | Loss: 0.00001887
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001885
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001884
Iteration 65/1000 | Loss: 0.00001884
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00001883
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001882
Iteration 71/1000 | Loss: 0.00001882
Iteration 72/1000 | Loss: 0.00001882
Iteration 73/1000 | Loss: 0.00001882
Iteration 74/1000 | Loss: 0.00001882
Iteration 75/1000 | Loss: 0.00001881
Iteration 76/1000 | Loss: 0.00001881
Iteration 77/1000 | Loss: 0.00001881
Iteration 78/1000 | Loss: 0.00001881
Iteration 79/1000 | Loss: 0.00001881
Iteration 80/1000 | Loss: 0.00001880
Iteration 81/1000 | Loss: 0.00001880
Iteration 82/1000 | Loss: 0.00001880
Iteration 83/1000 | Loss: 0.00001880
Iteration 84/1000 | Loss: 0.00001879
Iteration 85/1000 | Loss: 0.00001879
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001879
Iteration 88/1000 | Loss: 0.00001879
Iteration 89/1000 | Loss: 0.00001878
Iteration 90/1000 | Loss: 0.00001878
Iteration 91/1000 | Loss: 0.00001878
Iteration 92/1000 | Loss: 0.00001878
Iteration 93/1000 | Loss: 0.00001878
Iteration 94/1000 | Loss: 0.00001877
Iteration 95/1000 | Loss: 0.00001877
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001877
Iteration 98/1000 | Loss: 0.00001876
Iteration 99/1000 | Loss: 0.00001876
Iteration 100/1000 | Loss: 0.00001876
Iteration 101/1000 | Loss: 0.00001875
Iteration 102/1000 | Loss: 0.00001875
Iteration 103/1000 | Loss: 0.00001875
Iteration 104/1000 | Loss: 0.00001874
Iteration 105/1000 | Loss: 0.00001874
Iteration 106/1000 | Loss: 0.00001874
Iteration 107/1000 | Loss: 0.00001874
Iteration 108/1000 | Loss: 0.00001873
Iteration 109/1000 | Loss: 0.00001873
Iteration 110/1000 | Loss: 0.00001873
Iteration 111/1000 | Loss: 0.00001873
Iteration 112/1000 | Loss: 0.00001873
Iteration 113/1000 | Loss: 0.00001873
Iteration 114/1000 | Loss: 0.00001872
Iteration 115/1000 | Loss: 0.00001872
Iteration 116/1000 | Loss: 0.00001872
Iteration 117/1000 | Loss: 0.00001872
Iteration 118/1000 | Loss: 0.00001872
Iteration 119/1000 | Loss: 0.00001872
Iteration 120/1000 | Loss: 0.00001872
Iteration 121/1000 | Loss: 0.00001872
Iteration 122/1000 | Loss: 0.00001872
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001871
Iteration 125/1000 | Loss: 0.00001871
Iteration 126/1000 | Loss: 0.00001871
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001869
Iteration 134/1000 | Loss: 0.00001869
Iteration 135/1000 | Loss: 0.00001869
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001869
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001869
Iteration 148/1000 | Loss: 0.00001869
Iteration 149/1000 | Loss: 0.00001869
Iteration 150/1000 | Loss: 0.00001869
Iteration 151/1000 | Loss: 0.00001869
Iteration 152/1000 | Loss: 0.00001869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.8688735508476384e-05, 1.8688735508476384e-05, 1.8688735508476384e-05, 1.8688735508476384e-05, 1.8688735508476384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8688735508476384e-05

Optimization complete. Final v2v error: 3.6806561946868896 mm

Highest mean error: 4.450788974761963 mm for frame 177

Lowest mean error: 2.9934678077697754 mm for frame 225

Saving results

Total time: 45.60519504547119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827595
Iteration 2/25 | Loss: 0.00153144
Iteration 3/25 | Loss: 0.00126300
Iteration 4/25 | Loss: 0.00122576
Iteration 5/25 | Loss: 0.00122056
Iteration 6/25 | Loss: 0.00122031
Iteration 7/25 | Loss: 0.00122031
Iteration 8/25 | Loss: 0.00122031
Iteration 9/25 | Loss: 0.00122031
Iteration 10/25 | Loss: 0.00122031
Iteration 11/25 | Loss: 0.00122031
Iteration 12/25 | Loss: 0.00122031
Iteration 13/25 | Loss: 0.00122031
Iteration 14/25 | Loss: 0.00122031
Iteration 15/25 | Loss: 0.00122031
Iteration 16/25 | Loss: 0.00122031
Iteration 17/25 | Loss: 0.00122031
Iteration 18/25 | Loss: 0.00122031
Iteration 19/25 | Loss: 0.00122031
Iteration 20/25 | Loss: 0.00122031
Iteration 21/25 | Loss: 0.00122031
Iteration 22/25 | Loss: 0.00122031
Iteration 23/25 | Loss: 0.00122031
Iteration 24/25 | Loss: 0.00122031
Iteration 25/25 | Loss: 0.00122031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32424080
Iteration 2/25 | Loss: 0.00114594
Iteration 3/25 | Loss: 0.00114594
Iteration 4/25 | Loss: 0.00114594
Iteration 5/25 | Loss: 0.00114594
Iteration 6/25 | Loss: 0.00114594
Iteration 7/25 | Loss: 0.00114594
Iteration 8/25 | Loss: 0.00114594
Iteration 9/25 | Loss: 0.00114594
Iteration 10/25 | Loss: 0.00114594
Iteration 11/25 | Loss: 0.00114594
Iteration 12/25 | Loss: 0.00114594
Iteration 13/25 | Loss: 0.00114594
Iteration 14/25 | Loss: 0.00114594
Iteration 15/25 | Loss: 0.00114594
Iteration 16/25 | Loss: 0.00114594
Iteration 17/25 | Loss: 0.00114594
Iteration 18/25 | Loss: 0.00114594
Iteration 19/25 | Loss: 0.00114594
Iteration 20/25 | Loss: 0.00114594
Iteration 21/25 | Loss: 0.00114594
Iteration 22/25 | Loss: 0.00114594
Iteration 23/25 | Loss: 0.00114594
Iteration 24/25 | Loss: 0.00114594
Iteration 25/25 | Loss: 0.00114594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114594
Iteration 2/1000 | Loss: 0.00003301
Iteration 3/1000 | Loss: 0.00001984
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001314
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001266
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001234
Iteration 76/1000 | Loss: 0.00001234
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001233
Iteration 79/1000 | Loss: 0.00001233
Iteration 80/1000 | Loss: 0.00001233
Iteration 81/1000 | Loss: 0.00001233
Iteration 82/1000 | Loss: 0.00001233
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001226
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00001225
Iteration 102/1000 | Loss: 0.00001225
Iteration 103/1000 | Loss: 0.00001225
Iteration 104/1000 | Loss: 0.00001225
Iteration 105/1000 | Loss: 0.00001224
Iteration 106/1000 | Loss: 0.00001224
Iteration 107/1000 | Loss: 0.00001224
Iteration 108/1000 | Loss: 0.00001224
Iteration 109/1000 | Loss: 0.00001223
Iteration 110/1000 | Loss: 0.00001223
Iteration 111/1000 | Loss: 0.00001223
Iteration 112/1000 | Loss: 0.00001223
Iteration 113/1000 | Loss: 0.00001223
Iteration 114/1000 | Loss: 0.00001223
Iteration 115/1000 | Loss: 0.00001223
Iteration 116/1000 | Loss: 0.00001223
Iteration 117/1000 | Loss: 0.00001223
Iteration 118/1000 | Loss: 0.00001223
Iteration 119/1000 | Loss: 0.00001223
Iteration 120/1000 | Loss: 0.00001222
Iteration 121/1000 | Loss: 0.00001222
Iteration 122/1000 | Loss: 0.00001222
Iteration 123/1000 | Loss: 0.00001222
Iteration 124/1000 | Loss: 0.00001222
Iteration 125/1000 | Loss: 0.00001221
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001218
Iteration 134/1000 | Loss: 0.00001218
Iteration 135/1000 | Loss: 0.00001218
Iteration 136/1000 | Loss: 0.00001218
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001217
Iteration 140/1000 | Loss: 0.00001217
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001215
Iteration 145/1000 | Loss: 0.00001215
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001214
Iteration 148/1000 | Loss: 0.00001214
Iteration 149/1000 | Loss: 0.00001214
Iteration 150/1000 | Loss: 0.00001214
Iteration 151/1000 | Loss: 0.00001213
Iteration 152/1000 | Loss: 0.00001213
Iteration 153/1000 | Loss: 0.00001213
Iteration 154/1000 | Loss: 0.00001212
Iteration 155/1000 | Loss: 0.00001212
Iteration 156/1000 | Loss: 0.00001212
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001211
Iteration 159/1000 | Loss: 0.00001211
Iteration 160/1000 | Loss: 0.00001211
Iteration 161/1000 | Loss: 0.00001211
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001209
Iteration 168/1000 | Loss: 0.00001209
Iteration 169/1000 | Loss: 0.00001208
Iteration 170/1000 | Loss: 0.00001208
Iteration 171/1000 | Loss: 0.00001208
Iteration 172/1000 | Loss: 0.00001207
Iteration 173/1000 | Loss: 0.00001207
Iteration 174/1000 | Loss: 0.00001207
Iteration 175/1000 | Loss: 0.00001207
Iteration 176/1000 | Loss: 0.00001207
Iteration 177/1000 | Loss: 0.00001206
Iteration 178/1000 | Loss: 0.00001206
Iteration 179/1000 | Loss: 0.00001206
Iteration 180/1000 | Loss: 0.00001206
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001205
Iteration 184/1000 | Loss: 0.00001204
Iteration 185/1000 | Loss: 0.00001204
Iteration 186/1000 | Loss: 0.00001204
Iteration 187/1000 | Loss: 0.00001204
Iteration 188/1000 | Loss: 0.00001204
Iteration 189/1000 | Loss: 0.00001204
Iteration 190/1000 | Loss: 0.00001204
Iteration 191/1000 | Loss: 0.00001204
Iteration 192/1000 | Loss: 0.00001204
Iteration 193/1000 | Loss: 0.00001204
Iteration 194/1000 | Loss: 0.00001204
Iteration 195/1000 | Loss: 0.00001203
Iteration 196/1000 | Loss: 0.00001203
Iteration 197/1000 | Loss: 0.00001203
Iteration 198/1000 | Loss: 0.00001203
Iteration 199/1000 | Loss: 0.00001203
Iteration 200/1000 | Loss: 0.00001203
Iteration 201/1000 | Loss: 0.00001203
Iteration 202/1000 | Loss: 0.00001203
Iteration 203/1000 | Loss: 0.00001203
Iteration 204/1000 | Loss: 0.00001203
Iteration 205/1000 | Loss: 0.00001203
Iteration 206/1000 | Loss: 0.00001203
Iteration 207/1000 | Loss: 0.00001203
Iteration 208/1000 | Loss: 0.00001203
Iteration 209/1000 | Loss: 0.00001203
Iteration 210/1000 | Loss: 0.00001203
Iteration 211/1000 | Loss: 0.00001203
Iteration 212/1000 | Loss: 0.00001203
Iteration 213/1000 | Loss: 0.00001203
Iteration 214/1000 | Loss: 0.00001203
Iteration 215/1000 | Loss: 0.00001203
Iteration 216/1000 | Loss: 0.00001203
Iteration 217/1000 | Loss: 0.00001203
Iteration 218/1000 | Loss: 0.00001203
Iteration 219/1000 | Loss: 0.00001203
Iteration 220/1000 | Loss: 0.00001203
Iteration 221/1000 | Loss: 0.00001203
Iteration 222/1000 | Loss: 0.00001203
Iteration 223/1000 | Loss: 0.00001203
Iteration 224/1000 | Loss: 0.00001203
Iteration 225/1000 | Loss: 0.00001203
Iteration 226/1000 | Loss: 0.00001203
Iteration 227/1000 | Loss: 0.00001203
Iteration 228/1000 | Loss: 0.00001203
Iteration 229/1000 | Loss: 0.00001203
Iteration 230/1000 | Loss: 0.00001203
Iteration 231/1000 | Loss: 0.00001203
Iteration 232/1000 | Loss: 0.00001203
Iteration 233/1000 | Loss: 0.00001203
Iteration 234/1000 | Loss: 0.00001203
Iteration 235/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2027641787426546e-05, 1.2027641787426546e-05, 1.2027641787426546e-05, 1.2027641787426546e-05, 1.2027641787426546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2027641787426546e-05

Optimization complete. Final v2v error: 2.951896905899048 mm

Highest mean error: 3.7469873428344727 mm for frame 97

Lowest mean error: 2.557227373123169 mm for frame 32

Saving results

Total time: 48.392163038253784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818262
Iteration 2/25 | Loss: 0.00170289
Iteration 3/25 | Loss: 0.00136984
Iteration 4/25 | Loss: 0.00134806
Iteration 5/25 | Loss: 0.00134437
Iteration 6/25 | Loss: 0.00133709
Iteration 7/25 | Loss: 0.00133295
Iteration 8/25 | Loss: 0.00133269
Iteration 9/25 | Loss: 0.00133257
Iteration 10/25 | Loss: 0.00133257
Iteration 11/25 | Loss: 0.00133257
Iteration 12/25 | Loss: 0.00133257
Iteration 13/25 | Loss: 0.00133257
Iteration 14/25 | Loss: 0.00133257
Iteration 15/25 | Loss: 0.00133257
Iteration 16/25 | Loss: 0.00133257
Iteration 17/25 | Loss: 0.00133257
Iteration 18/25 | Loss: 0.00133257
Iteration 19/25 | Loss: 0.00133256
Iteration 20/25 | Loss: 0.00133256
Iteration 21/25 | Loss: 0.00133256
Iteration 22/25 | Loss: 0.00133256
Iteration 23/25 | Loss: 0.00133256
Iteration 24/25 | Loss: 0.00133256
Iteration 25/25 | Loss: 0.00133256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22781658
Iteration 2/25 | Loss: 0.00116463
Iteration 3/25 | Loss: 0.00116463
Iteration 4/25 | Loss: 0.00116463
Iteration 5/25 | Loss: 0.00116463
Iteration 6/25 | Loss: 0.00116462
Iteration 7/25 | Loss: 0.00116462
Iteration 8/25 | Loss: 0.00116462
Iteration 9/25 | Loss: 0.00116462
Iteration 10/25 | Loss: 0.00116462
Iteration 11/25 | Loss: 0.00116462
Iteration 12/25 | Loss: 0.00116462
Iteration 13/25 | Loss: 0.00116462
Iteration 14/25 | Loss: 0.00116462
Iteration 15/25 | Loss: 0.00116462
Iteration 16/25 | Loss: 0.00116462
Iteration 17/25 | Loss: 0.00116462
Iteration 18/25 | Loss: 0.00116462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011646240018308163, 0.0011646240018308163, 0.0011646240018308163, 0.0011646240018308163, 0.0011646240018308163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011646240018308163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116462
Iteration 2/1000 | Loss: 0.00003272
Iteration 3/1000 | Loss: 0.00002336
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00002000
Iteration 6/1000 | Loss: 0.00001936
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001815
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001748
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001740
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001738
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001733
Iteration 28/1000 | Loss: 0.00001733
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001732
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001731
Iteration 36/1000 | Loss: 0.00001731
Iteration 37/1000 | Loss: 0.00001731
Iteration 38/1000 | Loss: 0.00001731
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001729
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001729
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00001728
Iteration 48/1000 | Loss: 0.00001728
Iteration 49/1000 | Loss: 0.00001728
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001727
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001726
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001725
Iteration 61/1000 | Loss: 0.00001725
Iteration 62/1000 | Loss: 0.00001725
Iteration 63/1000 | Loss: 0.00001725
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001724
Iteration 66/1000 | Loss: 0.00001724
Iteration 67/1000 | Loss: 0.00001724
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001719
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001718
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001713
Iteration 131/1000 | Loss: 0.00001713
Iteration 132/1000 | Loss: 0.00001713
Iteration 133/1000 | Loss: 0.00001713
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001713
Iteration 152/1000 | Loss: 0.00001713
Iteration 153/1000 | Loss: 0.00001713
Iteration 154/1000 | Loss: 0.00001713
Iteration 155/1000 | Loss: 0.00001713
Iteration 156/1000 | Loss: 0.00001713
Iteration 157/1000 | Loss: 0.00001713
Iteration 158/1000 | Loss: 0.00001713
Iteration 159/1000 | Loss: 0.00001713
Iteration 160/1000 | Loss: 0.00001713
Iteration 161/1000 | Loss: 0.00001713
Iteration 162/1000 | Loss: 0.00001713
Iteration 163/1000 | Loss: 0.00001713
Iteration 164/1000 | Loss: 0.00001713
Iteration 165/1000 | Loss: 0.00001713
Iteration 166/1000 | Loss: 0.00001713
Iteration 167/1000 | Loss: 0.00001713
Iteration 168/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.7129954358097166e-05, 1.7129954358097166e-05, 1.7129954358097166e-05, 1.7129954358097166e-05, 1.7129954358097166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7129954358097166e-05

Optimization complete. Final v2v error: 3.499415636062622 mm

Highest mean error: 4.170736789703369 mm for frame 195

Lowest mean error: 3.152744770050049 mm for frame 233

Saving results

Total time: 48.94048833847046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821587
Iteration 2/25 | Loss: 0.00151574
Iteration 3/25 | Loss: 0.00129969
Iteration 4/25 | Loss: 0.00128157
Iteration 5/25 | Loss: 0.00127985
Iteration 6/25 | Loss: 0.00127985
Iteration 7/25 | Loss: 0.00127985
Iteration 8/25 | Loss: 0.00127985
Iteration 9/25 | Loss: 0.00127985
Iteration 10/25 | Loss: 0.00127985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012798527022823691, 0.0012798527022823691, 0.0012798527022823691, 0.0012798527022823691, 0.0012798527022823691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012798527022823691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94882423
Iteration 2/25 | Loss: 0.00065293
Iteration 3/25 | Loss: 0.00065293
Iteration 4/25 | Loss: 0.00065293
Iteration 5/25 | Loss: 0.00065293
Iteration 6/25 | Loss: 0.00065293
Iteration 7/25 | Loss: 0.00065293
Iteration 8/25 | Loss: 0.00065293
Iteration 9/25 | Loss: 0.00065293
Iteration 10/25 | Loss: 0.00065293
Iteration 11/25 | Loss: 0.00065293
Iteration 12/25 | Loss: 0.00065293
Iteration 13/25 | Loss: 0.00065292
Iteration 14/25 | Loss: 0.00065292
Iteration 15/25 | Loss: 0.00065292
Iteration 16/25 | Loss: 0.00065292
Iteration 17/25 | Loss: 0.00065292
Iteration 18/25 | Loss: 0.00065292
Iteration 19/25 | Loss: 0.00065292
Iteration 20/25 | Loss: 0.00065292
Iteration 21/25 | Loss: 0.00065292
Iteration 22/25 | Loss: 0.00065292
Iteration 23/25 | Loss: 0.00065292
Iteration 24/25 | Loss: 0.00065292
Iteration 25/25 | Loss: 0.00065292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065292
Iteration 2/1000 | Loss: 0.00003457
Iteration 3/1000 | Loss: 0.00002694
Iteration 4/1000 | Loss: 0.00002458
Iteration 5/1000 | Loss: 0.00002360
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002251
Iteration 8/1000 | Loss: 0.00002213
Iteration 9/1000 | Loss: 0.00002181
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002132
Iteration 12/1000 | Loss: 0.00002129
Iteration 13/1000 | Loss: 0.00002126
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00002126
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002118
Iteration 19/1000 | Loss: 0.00002112
Iteration 20/1000 | Loss: 0.00002098
Iteration 21/1000 | Loss: 0.00002096
Iteration 22/1000 | Loss: 0.00002092
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002091
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002090
Iteration 32/1000 | Loss: 0.00002089
Iteration 33/1000 | Loss: 0.00002087
Iteration 34/1000 | Loss: 0.00002085
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002081
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002080
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00002078
Iteration 50/1000 | Loss: 0.00002078
Iteration 51/1000 | Loss: 0.00002077
Iteration 52/1000 | Loss: 0.00002077
Iteration 53/1000 | Loss: 0.00002077
Iteration 54/1000 | Loss: 0.00002077
Iteration 55/1000 | Loss: 0.00002077
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002076
Iteration 59/1000 | Loss: 0.00002076
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002071
Iteration 62/1000 | Loss: 0.00002071
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002070
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002069
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002067
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002063
Iteration 87/1000 | Loss: 0.00002063
Iteration 88/1000 | Loss: 0.00002063
Iteration 89/1000 | Loss: 0.00002062
Iteration 90/1000 | Loss: 0.00002062
Iteration 91/1000 | Loss: 0.00002062
Iteration 92/1000 | Loss: 0.00002062
Iteration 93/1000 | Loss: 0.00002062
Iteration 94/1000 | Loss: 0.00002062
Iteration 95/1000 | Loss: 0.00002061
Iteration 96/1000 | Loss: 0.00002061
Iteration 97/1000 | Loss: 0.00002061
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002057
Iteration 107/1000 | Loss: 0.00002057
Iteration 108/1000 | Loss: 0.00002057
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002057
Iteration 112/1000 | Loss: 0.00002057
Iteration 113/1000 | Loss: 0.00002057
Iteration 114/1000 | Loss: 0.00002057
Iteration 115/1000 | Loss: 0.00002057
Iteration 116/1000 | Loss: 0.00002057
Iteration 117/1000 | Loss: 0.00002057
Iteration 118/1000 | Loss: 0.00002057
Iteration 119/1000 | Loss: 0.00002057
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002057
Iteration 122/1000 | Loss: 0.00002057
Iteration 123/1000 | Loss: 0.00002057
Iteration 124/1000 | Loss: 0.00002057
Iteration 125/1000 | Loss: 0.00002057
Iteration 126/1000 | Loss: 0.00002057
Iteration 127/1000 | Loss: 0.00002057
Iteration 128/1000 | Loss: 0.00002057
Iteration 129/1000 | Loss: 0.00002057
Iteration 130/1000 | Loss: 0.00002057
Iteration 131/1000 | Loss: 0.00002057
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Iteration 138/1000 | Loss: 0.00002057
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002056
Iteration 141/1000 | Loss: 0.00002056
Iteration 142/1000 | Loss: 0.00002056
Iteration 143/1000 | Loss: 0.00002056
Iteration 144/1000 | Loss: 0.00002056
Iteration 145/1000 | Loss: 0.00002056
Iteration 146/1000 | Loss: 0.00002056
Iteration 147/1000 | Loss: 0.00002056
Iteration 148/1000 | Loss: 0.00002056
Iteration 149/1000 | Loss: 0.00002056
Iteration 150/1000 | Loss: 0.00002056
Iteration 151/1000 | Loss: 0.00002056
Iteration 152/1000 | Loss: 0.00002056
Iteration 153/1000 | Loss: 0.00002056
Iteration 154/1000 | Loss: 0.00002056
Iteration 155/1000 | Loss: 0.00002056
Iteration 156/1000 | Loss: 0.00002056
Iteration 157/1000 | Loss: 0.00002056
Iteration 158/1000 | Loss: 0.00002056
Iteration 159/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.0563838916132227e-05, 2.0563838916132227e-05, 2.0563838916132227e-05, 2.0563838916132227e-05, 2.0563838916132227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0563838916132227e-05

Optimization complete. Final v2v error: 3.8218841552734375 mm

Highest mean error: 4.146328926086426 mm for frame 64

Lowest mean error: 3.608875274658203 mm for frame 125

Saving results

Total time: 35.95693922042847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399455
Iteration 2/25 | Loss: 0.00126800
Iteration 3/25 | Loss: 0.00119529
Iteration 4/25 | Loss: 0.00118364
Iteration 5/25 | Loss: 0.00117932
Iteration 6/25 | Loss: 0.00117839
Iteration 7/25 | Loss: 0.00117839
Iteration 8/25 | Loss: 0.00117839
Iteration 9/25 | Loss: 0.00117839
Iteration 10/25 | Loss: 0.00117839
Iteration 11/25 | Loss: 0.00117839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011783870868384838, 0.0011783870868384838, 0.0011783870868384838, 0.0011783870868384838, 0.0011783870868384838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011783870868384838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85351014
Iteration 2/25 | Loss: 0.00119805
Iteration 3/25 | Loss: 0.00119804
Iteration 4/25 | Loss: 0.00119804
Iteration 5/25 | Loss: 0.00119804
Iteration 6/25 | Loss: 0.00119804
Iteration 7/25 | Loss: 0.00119804
Iteration 8/25 | Loss: 0.00119804
Iteration 9/25 | Loss: 0.00119804
Iteration 10/25 | Loss: 0.00119804
Iteration 11/25 | Loss: 0.00119804
Iteration 12/25 | Loss: 0.00119804
Iteration 13/25 | Loss: 0.00119804
Iteration 14/25 | Loss: 0.00119804
Iteration 15/25 | Loss: 0.00119804
Iteration 16/25 | Loss: 0.00119804
Iteration 17/25 | Loss: 0.00119804
Iteration 18/25 | Loss: 0.00119804
Iteration 19/25 | Loss: 0.00119804
Iteration 20/25 | Loss: 0.00119804
Iteration 21/25 | Loss: 0.00119804
Iteration 22/25 | Loss: 0.00119804
Iteration 23/25 | Loss: 0.00119804
Iteration 24/25 | Loss: 0.00119804
Iteration 25/25 | Loss: 0.00119804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119804
Iteration 2/1000 | Loss: 0.00002686
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001379
Iteration 6/1000 | Loss: 0.00001285
Iteration 7/1000 | Loss: 0.00001227
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001088
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001075
Iteration 15/1000 | Loss: 0.00001070
Iteration 16/1000 | Loss: 0.00001069
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001060
Iteration 20/1000 | Loss: 0.00001059
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001057
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001052
Iteration 26/1000 | Loss: 0.00001051
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001047
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001043
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001035
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001034
Iteration 60/1000 | Loss: 0.00001034
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001034
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001033
Iteration 67/1000 | Loss: 0.00001033
Iteration 68/1000 | Loss: 0.00001032
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001031
Iteration 71/1000 | Loss: 0.00001031
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001026
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001024
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001023
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001022
Iteration 96/1000 | Loss: 0.00001022
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001022
Iteration 99/1000 | Loss: 0.00001022
Iteration 100/1000 | Loss: 0.00001021
Iteration 101/1000 | Loss: 0.00001021
Iteration 102/1000 | Loss: 0.00001021
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001018
Iteration 108/1000 | Loss: 0.00001018
Iteration 109/1000 | Loss: 0.00001018
Iteration 110/1000 | Loss: 0.00001017
Iteration 111/1000 | Loss: 0.00001017
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001016
Iteration 114/1000 | Loss: 0.00001015
Iteration 115/1000 | Loss: 0.00001015
Iteration 116/1000 | Loss: 0.00001015
Iteration 117/1000 | Loss: 0.00001015
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001015
Iteration 121/1000 | Loss: 0.00001014
Iteration 122/1000 | Loss: 0.00001014
Iteration 123/1000 | Loss: 0.00001013
Iteration 124/1000 | Loss: 0.00001013
Iteration 125/1000 | Loss: 0.00001013
Iteration 126/1000 | Loss: 0.00001012
Iteration 127/1000 | Loss: 0.00001012
Iteration 128/1000 | Loss: 0.00001012
Iteration 129/1000 | Loss: 0.00001012
Iteration 130/1000 | Loss: 0.00001012
Iteration 131/1000 | Loss: 0.00001012
Iteration 132/1000 | Loss: 0.00001012
Iteration 133/1000 | Loss: 0.00001011
Iteration 134/1000 | Loss: 0.00001011
Iteration 135/1000 | Loss: 0.00001011
Iteration 136/1000 | Loss: 0.00001011
Iteration 137/1000 | Loss: 0.00001011
Iteration 138/1000 | Loss: 0.00001011
Iteration 139/1000 | Loss: 0.00001011
Iteration 140/1000 | Loss: 0.00001011
Iteration 141/1000 | Loss: 0.00001011
Iteration 142/1000 | Loss: 0.00001010
Iteration 143/1000 | Loss: 0.00001010
Iteration 144/1000 | Loss: 0.00001010
Iteration 145/1000 | Loss: 0.00001010
Iteration 146/1000 | Loss: 0.00001010
Iteration 147/1000 | Loss: 0.00001010
Iteration 148/1000 | Loss: 0.00001010
Iteration 149/1000 | Loss: 0.00001009
Iteration 150/1000 | Loss: 0.00001009
Iteration 151/1000 | Loss: 0.00001009
Iteration 152/1000 | Loss: 0.00001009
Iteration 153/1000 | Loss: 0.00001009
Iteration 154/1000 | Loss: 0.00001008
Iteration 155/1000 | Loss: 0.00001008
Iteration 156/1000 | Loss: 0.00001008
Iteration 157/1000 | Loss: 0.00001008
Iteration 158/1000 | Loss: 0.00001008
Iteration 159/1000 | Loss: 0.00001008
Iteration 160/1000 | Loss: 0.00001007
Iteration 161/1000 | Loss: 0.00001007
Iteration 162/1000 | Loss: 0.00001007
Iteration 163/1000 | Loss: 0.00001006
Iteration 164/1000 | Loss: 0.00001006
Iteration 165/1000 | Loss: 0.00001006
Iteration 166/1000 | Loss: 0.00001006
Iteration 167/1000 | Loss: 0.00001006
Iteration 168/1000 | Loss: 0.00001006
Iteration 169/1000 | Loss: 0.00001005
Iteration 170/1000 | Loss: 0.00001005
Iteration 171/1000 | Loss: 0.00001005
Iteration 172/1000 | Loss: 0.00001005
Iteration 173/1000 | Loss: 0.00001005
Iteration 174/1000 | Loss: 0.00001004
Iteration 175/1000 | Loss: 0.00001004
Iteration 176/1000 | Loss: 0.00001004
Iteration 177/1000 | Loss: 0.00001004
Iteration 178/1000 | Loss: 0.00001004
Iteration 179/1000 | Loss: 0.00001004
Iteration 180/1000 | Loss: 0.00001003
Iteration 181/1000 | Loss: 0.00001003
Iteration 182/1000 | Loss: 0.00001003
Iteration 183/1000 | Loss: 0.00001003
Iteration 184/1000 | Loss: 0.00001003
Iteration 185/1000 | Loss: 0.00001003
Iteration 186/1000 | Loss: 0.00001003
Iteration 187/1000 | Loss: 0.00001003
Iteration 188/1000 | Loss: 0.00001003
Iteration 189/1000 | Loss: 0.00001003
Iteration 190/1000 | Loss: 0.00001003
Iteration 191/1000 | Loss: 0.00001003
Iteration 192/1000 | Loss: 0.00001003
Iteration 193/1000 | Loss: 0.00001003
Iteration 194/1000 | Loss: 0.00001003
Iteration 195/1000 | Loss: 0.00001003
Iteration 196/1000 | Loss: 0.00001003
Iteration 197/1000 | Loss: 0.00001003
Iteration 198/1000 | Loss: 0.00001003
Iteration 199/1000 | Loss: 0.00001003
Iteration 200/1000 | Loss: 0.00001003
Iteration 201/1000 | Loss: 0.00001003
Iteration 202/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.0027774806076195e-05, 1.0027774806076195e-05, 1.0027774806076195e-05, 1.0027774806076195e-05, 1.0027774806076195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0027774806076195e-05

Optimization complete. Final v2v error: 2.723569393157959 mm

Highest mean error: 3.9882302284240723 mm for frame 77

Lowest mean error: 2.5287177562713623 mm for frame 99

Saving results

Total time: 42.64497995376587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790315
Iteration 2/25 | Loss: 0.00126190
Iteration 3/25 | Loss: 0.00118998
Iteration 4/25 | Loss: 0.00118055
Iteration 5/25 | Loss: 0.00117855
Iteration 6/25 | Loss: 0.00117855
Iteration 7/25 | Loss: 0.00117855
Iteration 8/25 | Loss: 0.00117855
Iteration 9/25 | Loss: 0.00117855
Iteration 10/25 | Loss: 0.00117855
Iteration 11/25 | Loss: 0.00117855
Iteration 12/25 | Loss: 0.00117855
Iteration 13/25 | Loss: 0.00117855
Iteration 14/25 | Loss: 0.00117855
Iteration 15/25 | Loss: 0.00117855
Iteration 16/25 | Loss: 0.00117855
Iteration 17/25 | Loss: 0.00117855
Iteration 18/25 | Loss: 0.00117855
Iteration 19/25 | Loss: 0.00117855
Iteration 20/25 | Loss: 0.00117855
Iteration 21/25 | Loss: 0.00117855
Iteration 22/25 | Loss: 0.00117855
Iteration 23/25 | Loss: 0.00117855
Iteration 24/25 | Loss: 0.00117855
Iteration 25/25 | Loss: 0.00117855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32654655
Iteration 2/25 | Loss: 0.00115530
Iteration 3/25 | Loss: 0.00115530
Iteration 4/25 | Loss: 0.00115530
Iteration 5/25 | Loss: 0.00115530
Iteration 6/25 | Loss: 0.00115530
Iteration 7/25 | Loss: 0.00115530
Iteration 8/25 | Loss: 0.00115530
Iteration 9/25 | Loss: 0.00115530
Iteration 10/25 | Loss: 0.00115530
Iteration 11/25 | Loss: 0.00115530
Iteration 12/25 | Loss: 0.00115530
Iteration 13/25 | Loss: 0.00115530
Iteration 14/25 | Loss: 0.00115530
Iteration 15/25 | Loss: 0.00115530
Iteration 16/25 | Loss: 0.00115530
Iteration 17/25 | Loss: 0.00115530
Iteration 18/25 | Loss: 0.00115530
Iteration 19/25 | Loss: 0.00115530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011552952928468585, 0.0011552952928468585, 0.0011552952928468585, 0.0011552952928468585, 0.0011552952928468585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011552952928468585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115530
Iteration 2/1000 | Loss: 0.00002074
Iteration 3/1000 | Loss: 0.00001417
Iteration 4/1000 | Loss: 0.00001255
Iteration 5/1000 | Loss: 0.00001181
Iteration 6/1000 | Loss: 0.00001123
Iteration 7/1000 | Loss: 0.00001079
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001033
Iteration 10/1000 | Loss: 0.00001015
Iteration 11/1000 | Loss: 0.00001012
Iteration 12/1000 | Loss: 0.00001010
Iteration 13/1000 | Loss: 0.00001006
Iteration 14/1000 | Loss: 0.00001005
Iteration 15/1000 | Loss: 0.00001005
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000996
Iteration 19/1000 | Loss: 0.00000995
Iteration 20/1000 | Loss: 0.00000990
Iteration 21/1000 | Loss: 0.00000988
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000982
Iteration 24/1000 | Loss: 0.00000982
Iteration 25/1000 | Loss: 0.00000982
Iteration 26/1000 | Loss: 0.00000981
Iteration 27/1000 | Loss: 0.00000981
Iteration 28/1000 | Loss: 0.00000979
Iteration 29/1000 | Loss: 0.00000979
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000978
Iteration 32/1000 | Loss: 0.00000978
Iteration 33/1000 | Loss: 0.00000978
Iteration 34/1000 | Loss: 0.00000978
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000977
Iteration 37/1000 | Loss: 0.00000977
Iteration 38/1000 | Loss: 0.00000977
Iteration 39/1000 | Loss: 0.00000976
Iteration 40/1000 | Loss: 0.00000975
Iteration 41/1000 | Loss: 0.00000975
Iteration 42/1000 | Loss: 0.00000974
Iteration 43/1000 | Loss: 0.00000974
Iteration 44/1000 | Loss: 0.00000973
Iteration 45/1000 | Loss: 0.00000970
Iteration 46/1000 | Loss: 0.00000969
Iteration 47/1000 | Loss: 0.00000969
Iteration 48/1000 | Loss: 0.00000969
Iteration 49/1000 | Loss: 0.00000969
Iteration 50/1000 | Loss: 0.00000968
Iteration 51/1000 | Loss: 0.00000968
Iteration 52/1000 | Loss: 0.00000968
Iteration 53/1000 | Loss: 0.00000967
Iteration 54/1000 | Loss: 0.00000967
Iteration 55/1000 | Loss: 0.00000966
Iteration 56/1000 | Loss: 0.00000966
Iteration 57/1000 | Loss: 0.00000966
Iteration 58/1000 | Loss: 0.00000965
Iteration 59/1000 | Loss: 0.00000965
Iteration 60/1000 | Loss: 0.00000964
Iteration 61/1000 | Loss: 0.00000962
Iteration 62/1000 | Loss: 0.00000962
Iteration 63/1000 | Loss: 0.00000961
Iteration 64/1000 | Loss: 0.00000961
Iteration 65/1000 | Loss: 0.00000960
Iteration 66/1000 | Loss: 0.00000960
Iteration 67/1000 | Loss: 0.00000959
Iteration 68/1000 | Loss: 0.00000959
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000958
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000957
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000956
Iteration 76/1000 | Loss: 0.00000956
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000954
Iteration 80/1000 | Loss: 0.00000954
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000953
Iteration 83/1000 | Loss: 0.00000953
Iteration 84/1000 | Loss: 0.00000953
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000952
Iteration 88/1000 | Loss: 0.00000952
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000949
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000949
Iteration 102/1000 | Loss: 0.00000949
Iteration 103/1000 | Loss: 0.00000948
Iteration 104/1000 | Loss: 0.00000948
Iteration 105/1000 | Loss: 0.00000947
Iteration 106/1000 | Loss: 0.00000947
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000946
Iteration 109/1000 | Loss: 0.00000946
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000945
Iteration 112/1000 | Loss: 0.00000945
Iteration 113/1000 | Loss: 0.00000945
Iteration 114/1000 | Loss: 0.00000945
Iteration 115/1000 | Loss: 0.00000945
Iteration 116/1000 | Loss: 0.00000945
Iteration 117/1000 | Loss: 0.00000945
Iteration 118/1000 | Loss: 0.00000945
Iteration 119/1000 | Loss: 0.00000945
Iteration 120/1000 | Loss: 0.00000945
Iteration 121/1000 | Loss: 0.00000945
Iteration 122/1000 | Loss: 0.00000945
Iteration 123/1000 | Loss: 0.00000945
Iteration 124/1000 | Loss: 0.00000945
Iteration 125/1000 | Loss: 0.00000945
Iteration 126/1000 | Loss: 0.00000945
Iteration 127/1000 | Loss: 0.00000945
Iteration 128/1000 | Loss: 0.00000945
Iteration 129/1000 | Loss: 0.00000945
Iteration 130/1000 | Loss: 0.00000945
Iteration 131/1000 | Loss: 0.00000945
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000945
Iteration 139/1000 | Loss: 0.00000945
Iteration 140/1000 | Loss: 0.00000945
Iteration 141/1000 | Loss: 0.00000945
Iteration 142/1000 | Loss: 0.00000945
Iteration 143/1000 | Loss: 0.00000945
Iteration 144/1000 | Loss: 0.00000945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [9.447871889278758e-06, 9.447871889278758e-06, 9.447871889278758e-06, 9.447871889278758e-06, 9.447871889278758e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.447871889278758e-06

Optimization complete. Final v2v error: 2.633218288421631 mm

Highest mean error: 2.8226943016052246 mm for frame 42

Lowest mean error: 2.491865873336792 mm for frame 29

Saving results

Total time: 34.56283092498779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457846
Iteration 2/25 | Loss: 0.00128648
Iteration 3/25 | Loss: 0.00121633
Iteration 4/25 | Loss: 0.00120939
Iteration 5/25 | Loss: 0.00120769
Iteration 6/25 | Loss: 0.00120748
Iteration 7/25 | Loss: 0.00120748
Iteration 8/25 | Loss: 0.00120748
Iteration 9/25 | Loss: 0.00120748
Iteration 10/25 | Loss: 0.00120748
Iteration 11/25 | Loss: 0.00120748
Iteration 12/25 | Loss: 0.00120748
Iteration 13/25 | Loss: 0.00120748
Iteration 14/25 | Loss: 0.00120748
Iteration 15/25 | Loss: 0.00120748
Iteration 16/25 | Loss: 0.00120748
Iteration 17/25 | Loss: 0.00120748
Iteration 18/25 | Loss: 0.00120748
Iteration 19/25 | Loss: 0.00120748
Iteration 20/25 | Loss: 0.00120748
Iteration 21/25 | Loss: 0.00120748
Iteration 22/25 | Loss: 0.00120748
Iteration 23/25 | Loss: 0.00120748
Iteration 24/25 | Loss: 0.00120748
Iteration 25/25 | Loss: 0.00120748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33241200
Iteration 2/25 | Loss: 0.00133907
Iteration 3/25 | Loss: 0.00133906
Iteration 4/25 | Loss: 0.00133906
Iteration 5/25 | Loss: 0.00133906
Iteration 6/25 | Loss: 0.00133906
Iteration 7/25 | Loss: 0.00133906
Iteration 8/25 | Loss: 0.00133906
Iteration 9/25 | Loss: 0.00133906
Iteration 10/25 | Loss: 0.00133906
Iteration 11/25 | Loss: 0.00133906
Iteration 12/25 | Loss: 0.00133906
Iteration 13/25 | Loss: 0.00133906
Iteration 14/25 | Loss: 0.00133906
Iteration 15/25 | Loss: 0.00133906
Iteration 16/25 | Loss: 0.00133906
Iteration 17/25 | Loss: 0.00133906
Iteration 18/25 | Loss: 0.00133906
Iteration 19/25 | Loss: 0.00133906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013390554813668132, 0.0013390554813668132, 0.0013390554813668132, 0.0013390554813668132, 0.0013390554813668132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013390554813668132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133906
Iteration 2/1000 | Loss: 0.00002920
Iteration 3/1000 | Loss: 0.00001608
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001119
Iteration 8/1000 | Loss: 0.00001083
Iteration 9/1000 | Loss: 0.00001071
Iteration 10/1000 | Loss: 0.00001068
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001064
Iteration 13/1000 | Loss: 0.00001064
Iteration 14/1000 | Loss: 0.00001063
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001053
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001043
Iteration 19/1000 | Loss: 0.00001041
Iteration 20/1000 | Loss: 0.00001039
Iteration 21/1000 | Loss: 0.00001033
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001030
Iteration 24/1000 | Loss: 0.00001029
Iteration 25/1000 | Loss: 0.00001026
Iteration 26/1000 | Loss: 0.00001025
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001023
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001023
Iteration 31/1000 | Loss: 0.00001023
Iteration 32/1000 | Loss: 0.00001023
Iteration 33/1000 | Loss: 0.00001023
Iteration 34/1000 | Loss: 0.00001022
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001022
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001018
Iteration 39/1000 | Loss: 0.00001018
Iteration 40/1000 | Loss: 0.00001017
Iteration 41/1000 | Loss: 0.00001017
Iteration 42/1000 | Loss: 0.00001016
Iteration 43/1000 | Loss: 0.00001016
Iteration 44/1000 | Loss: 0.00001015
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001014
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001014
Iteration 51/1000 | Loss: 0.00001014
Iteration 52/1000 | Loss: 0.00001014
Iteration 53/1000 | Loss: 0.00001014
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001013
Iteration 59/1000 | Loss: 0.00001013
Iteration 60/1000 | Loss: 0.00001013
Iteration 61/1000 | Loss: 0.00001013
Iteration 62/1000 | Loss: 0.00001013
Iteration 63/1000 | Loss: 0.00001012
Iteration 64/1000 | Loss: 0.00001012
Iteration 65/1000 | Loss: 0.00001011
Iteration 66/1000 | Loss: 0.00001011
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001010
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001008
Iteration 76/1000 | Loss: 0.00001008
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001007
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001007
Iteration 82/1000 | Loss: 0.00001007
Iteration 83/1000 | Loss: 0.00001007
Iteration 84/1000 | Loss: 0.00001007
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00001006
Iteration 87/1000 | Loss: 0.00001004
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001004
Iteration 90/1000 | Loss: 0.00001004
Iteration 91/1000 | Loss: 0.00001004
Iteration 92/1000 | Loss: 0.00001004
Iteration 93/1000 | Loss: 0.00001004
Iteration 94/1000 | Loss: 0.00001004
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001003
Iteration 97/1000 | Loss: 0.00001003
Iteration 98/1000 | Loss: 0.00001003
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001000
Iteration 104/1000 | Loss: 0.00001000
Iteration 105/1000 | Loss: 0.00001000
Iteration 106/1000 | Loss: 0.00001000
Iteration 107/1000 | Loss: 0.00001000
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00000999
Iteration 112/1000 | Loss: 0.00000999
Iteration 113/1000 | Loss: 0.00000999
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000998
Iteration 121/1000 | Loss: 0.00000998
Iteration 122/1000 | Loss: 0.00000997
Iteration 123/1000 | Loss: 0.00000997
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000997
Iteration 130/1000 | Loss: 0.00000997
Iteration 131/1000 | Loss: 0.00000997
Iteration 132/1000 | Loss: 0.00000997
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000995
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000994
Iteration 143/1000 | Loss: 0.00000994
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000994
Iteration 147/1000 | Loss: 0.00000993
Iteration 148/1000 | Loss: 0.00000993
Iteration 149/1000 | Loss: 0.00000993
Iteration 150/1000 | Loss: 0.00000993
Iteration 151/1000 | Loss: 0.00000993
Iteration 152/1000 | Loss: 0.00000993
Iteration 153/1000 | Loss: 0.00000992
Iteration 154/1000 | Loss: 0.00000992
Iteration 155/1000 | Loss: 0.00000991
Iteration 156/1000 | Loss: 0.00000991
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000989
Iteration 161/1000 | Loss: 0.00000989
Iteration 162/1000 | Loss: 0.00000989
Iteration 163/1000 | Loss: 0.00000988
Iteration 164/1000 | Loss: 0.00000988
Iteration 165/1000 | Loss: 0.00000988
Iteration 166/1000 | Loss: 0.00000988
Iteration 167/1000 | Loss: 0.00000987
Iteration 168/1000 | Loss: 0.00000987
Iteration 169/1000 | Loss: 0.00000987
Iteration 170/1000 | Loss: 0.00000987
Iteration 171/1000 | Loss: 0.00000986
Iteration 172/1000 | Loss: 0.00000986
Iteration 173/1000 | Loss: 0.00000986
Iteration 174/1000 | Loss: 0.00000986
Iteration 175/1000 | Loss: 0.00000986
Iteration 176/1000 | Loss: 0.00000985
Iteration 177/1000 | Loss: 0.00000985
Iteration 178/1000 | Loss: 0.00000985
Iteration 179/1000 | Loss: 0.00000985
Iteration 180/1000 | Loss: 0.00000985
Iteration 181/1000 | Loss: 0.00000985
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000984
Iteration 187/1000 | Loss: 0.00000984
Iteration 188/1000 | Loss: 0.00000984
Iteration 189/1000 | Loss: 0.00000984
Iteration 190/1000 | Loss: 0.00000984
Iteration 191/1000 | Loss: 0.00000984
Iteration 192/1000 | Loss: 0.00000984
Iteration 193/1000 | Loss: 0.00000984
Iteration 194/1000 | Loss: 0.00000984
Iteration 195/1000 | Loss: 0.00000984
Iteration 196/1000 | Loss: 0.00000983
Iteration 197/1000 | Loss: 0.00000983
Iteration 198/1000 | Loss: 0.00000983
Iteration 199/1000 | Loss: 0.00000983
Iteration 200/1000 | Loss: 0.00000983
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000982
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000981
Iteration 210/1000 | Loss: 0.00000981
Iteration 211/1000 | Loss: 0.00000981
Iteration 212/1000 | Loss: 0.00000981
Iteration 213/1000 | Loss: 0.00000981
Iteration 214/1000 | Loss: 0.00000981
Iteration 215/1000 | Loss: 0.00000981
Iteration 216/1000 | Loss: 0.00000980
Iteration 217/1000 | Loss: 0.00000980
Iteration 218/1000 | Loss: 0.00000980
Iteration 219/1000 | Loss: 0.00000980
Iteration 220/1000 | Loss: 0.00000980
Iteration 221/1000 | Loss: 0.00000980
Iteration 222/1000 | Loss: 0.00000980
Iteration 223/1000 | Loss: 0.00000980
Iteration 224/1000 | Loss: 0.00000980
Iteration 225/1000 | Loss: 0.00000980
Iteration 226/1000 | Loss: 0.00000980
Iteration 227/1000 | Loss: 0.00000980
Iteration 228/1000 | Loss: 0.00000980
Iteration 229/1000 | Loss: 0.00000980
Iteration 230/1000 | Loss: 0.00000980
Iteration 231/1000 | Loss: 0.00000980
Iteration 232/1000 | Loss: 0.00000980
Iteration 233/1000 | Loss: 0.00000979
Iteration 234/1000 | Loss: 0.00000979
Iteration 235/1000 | Loss: 0.00000979
Iteration 236/1000 | Loss: 0.00000979
Iteration 237/1000 | Loss: 0.00000979
Iteration 238/1000 | Loss: 0.00000979
Iteration 239/1000 | Loss: 0.00000979
Iteration 240/1000 | Loss: 0.00000979
Iteration 241/1000 | Loss: 0.00000979
Iteration 242/1000 | Loss: 0.00000979
Iteration 243/1000 | Loss: 0.00000979
Iteration 244/1000 | Loss: 0.00000978
Iteration 245/1000 | Loss: 0.00000978
Iteration 246/1000 | Loss: 0.00000978
Iteration 247/1000 | Loss: 0.00000978
Iteration 248/1000 | Loss: 0.00000977
Iteration 249/1000 | Loss: 0.00000977
Iteration 250/1000 | Loss: 0.00000977
Iteration 251/1000 | Loss: 0.00000977
Iteration 252/1000 | Loss: 0.00000977
Iteration 253/1000 | Loss: 0.00000977
Iteration 254/1000 | Loss: 0.00000977
Iteration 255/1000 | Loss: 0.00000977
Iteration 256/1000 | Loss: 0.00000977
Iteration 257/1000 | Loss: 0.00000977
Iteration 258/1000 | Loss: 0.00000977
Iteration 259/1000 | Loss: 0.00000977
Iteration 260/1000 | Loss: 0.00000977
Iteration 261/1000 | Loss: 0.00000977
Iteration 262/1000 | Loss: 0.00000977
Iteration 263/1000 | Loss: 0.00000977
Iteration 264/1000 | Loss: 0.00000976
Iteration 265/1000 | Loss: 0.00000976
Iteration 266/1000 | Loss: 0.00000976
Iteration 267/1000 | Loss: 0.00000976
Iteration 268/1000 | Loss: 0.00000976
Iteration 269/1000 | Loss: 0.00000976
Iteration 270/1000 | Loss: 0.00000976
Iteration 271/1000 | Loss: 0.00000976
Iteration 272/1000 | Loss: 0.00000975
Iteration 273/1000 | Loss: 0.00000975
Iteration 274/1000 | Loss: 0.00000975
Iteration 275/1000 | Loss: 0.00000975
Iteration 276/1000 | Loss: 0.00000975
Iteration 277/1000 | Loss: 0.00000975
Iteration 278/1000 | Loss: 0.00000975
Iteration 279/1000 | Loss: 0.00000975
Iteration 280/1000 | Loss: 0.00000975
Iteration 281/1000 | Loss: 0.00000975
Iteration 282/1000 | Loss: 0.00000975
Iteration 283/1000 | Loss: 0.00000975
Iteration 284/1000 | Loss: 0.00000974
Iteration 285/1000 | Loss: 0.00000974
Iteration 286/1000 | Loss: 0.00000974
Iteration 287/1000 | Loss: 0.00000974
Iteration 288/1000 | Loss: 0.00000973
Iteration 289/1000 | Loss: 0.00000973
Iteration 290/1000 | Loss: 0.00000973
Iteration 291/1000 | Loss: 0.00000973
Iteration 292/1000 | Loss: 0.00000973
Iteration 293/1000 | Loss: 0.00000973
Iteration 294/1000 | Loss: 0.00000973
Iteration 295/1000 | Loss: 0.00000973
Iteration 296/1000 | Loss: 0.00000973
Iteration 297/1000 | Loss: 0.00000973
Iteration 298/1000 | Loss: 0.00000973
Iteration 299/1000 | Loss: 0.00000973
Iteration 300/1000 | Loss: 0.00000972
Iteration 301/1000 | Loss: 0.00000972
Iteration 302/1000 | Loss: 0.00000972
Iteration 303/1000 | Loss: 0.00000972
Iteration 304/1000 | Loss: 0.00000972
Iteration 305/1000 | Loss: 0.00000971
Iteration 306/1000 | Loss: 0.00000971
Iteration 307/1000 | Loss: 0.00000971
Iteration 308/1000 | Loss: 0.00000971
Iteration 309/1000 | Loss: 0.00000971
Iteration 310/1000 | Loss: 0.00000971
Iteration 311/1000 | Loss: 0.00000971
Iteration 312/1000 | Loss: 0.00000971
Iteration 313/1000 | Loss: 0.00000970
Iteration 314/1000 | Loss: 0.00000970
Iteration 315/1000 | Loss: 0.00000970
Iteration 316/1000 | Loss: 0.00000970
Iteration 317/1000 | Loss: 0.00000970
Iteration 318/1000 | Loss: 0.00000970
Iteration 319/1000 | Loss: 0.00000970
Iteration 320/1000 | Loss: 0.00000970
Iteration 321/1000 | Loss: 0.00000970
Iteration 322/1000 | Loss: 0.00000970
Iteration 323/1000 | Loss: 0.00000970
Iteration 324/1000 | Loss: 0.00000970
Iteration 325/1000 | Loss: 0.00000970
Iteration 326/1000 | Loss: 0.00000970
Iteration 327/1000 | Loss: 0.00000970
Iteration 328/1000 | Loss: 0.00000970
Iteration 329/1000 | Loss: 0.00000969
Iteration 330/1000 | Loss: 0.00000969
Iteration 331/1000 | Loss: 0.00000969
Iteration 332/1000 | Loss: 0.00000969
Iteration 333/1000 | Loss: 0.00000969
Iteration 334/1000 | Loss: 0.00000969
Iteration 335/1000 | Loss: 0.00000969
Iteration 336/1000 | Loss: 0.00000969
Iteration 337/1000 | Loss: 0.00000969
Iteration 338/1000 | Loss: 0.00000969
Iteration 339/1000 | Loss: 0.00000969
Iteration 340/1000 | Loss: 0.00000969
Iteration 341/1000 | Loss: 0.00000969
Iteration 342/1000 | Loss: 0.00000969
Iteration 343/1000 | Loss: 0.00000969
Iteration 344/1000 | Loss: 0.00000969
Iteration 345/1000 | Loss: 0.00000969
Iteration 346/1000 | Loss: 0.00000969
Iteration 347/1000 | Loss: 0.00000969
Iteration 348/1000 | Loss: 0.00000969
Iteration 349/1000 | Loss: 0.00000968
Iteration 350/1000 | Loss: 0.00000968
Iteration 351/1000 | Loss: 0.00000968
Iteration 352/1000 | Loss: 0.00000968
Iteration 353/1000 | Loss: 0.00000968
Iteration 354/1000 | Loss: 0.00000968
Iteration 355/1000 | Loss: 0.00000968
Iteration 356/1000 | Loss: 0.00000967
Iteration 357/1000 | Loss: 0.00000967
Iteration 358/1000 | Loss: 0.00000967
Iteration 359/1000 | Loss: 0.00000967
Iteration 360/1000 | Loss: 0.00000967
Iteration 361/1000 | Loss: 0.00000967
Iteration 362/1000 | Loss: 0.00000967
Iteration 363/1000 | Loss: 0.00000967
Iteration 364/1000 | Loss: 0.00000967
Iteration 365/1000 | Loss: 0.00000967
Iteration 366/1000 | Loss: 0.00000967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [9.669577593740541e-06, 9.669577593740541e-06, 9.669577593740541e-06, 9.669577593740541e-06, 9.669577593740541e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.669577593740541e-06

Optimization complete. Final v2v error: 2.588240146636963 mm

Highest mean error: 2.9776387214660645 mm for frame 59

Lowest mean error: 2.4291951656341553 mm for frame 93

Saving results

Total time: 46.85918831825256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044011
Iteration 2/25 | Loss: 0.01044011
Iteration 3/25 | Loss: 0.01044011
Iteration 4/25 | Loss: 0.01044011
Iteration 5/25 | Loss: 0.01044011
Iteration 6/25 | Loss: 0.01044010
Iteration 7/25 | Loss: 0.01044010
Iteration 8/25 | Loss: 0.01044010
Iteration 9/25 | Loss: 0.01044009
Iteration 10/25 | Loss: 0.01044009
Iteration 11/25 | Loss: 0.01044009
Iteration 12/25 | Loss: 0.01044009
Iteration 13/25 | Loss: 0.01044009
Iteration 14/25 | Loss: 0.01044008
Iteration 15/25 | Loss: 0.01044008
Iteration 16/25 | Loss: 0.01044008
Iteration 17/25 | Loss: 0.01044007
Iteration 18/25 | Loss: 0.01044007
Iteration 19/25 | Loss: 0.01044007
Iteration 20/25 | Loss: 0.01044007
Iteration 21/25 | Loss: 0.01044007
Iteration 22/25 | Loss: 0.01044007
Iteration 23/25 | Loss: 0.01044006
Iteration 24/25 | Loss: 0.01044006
Iteration 25/25 | Loss: 0.01044005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96645498
Iteration 2/25 | Loss: 0.09112743
Iteration 3/25 | Loss: 0.09058128
Iteration 4/25 | Loss: 0.08897310
Iteration 5/25 | Loss: 0.08896710
Iteration 6/25 | Loss: 0.08896710
Iteration 7/25 | Loss: 0.08896709
Iteration 8/25 | Loss: 0.08897255
Iteration 9/25 | Loss: 0.08897255
Iteration 10/25 | Loss: 0.08897255
Iteration 11/25 | Loss: 0.08896710
Iteration 12/25 | Loss: 0.08896710
Iteration 13/25 | Loss: 0.08896710
Iteration 14/25 | Loss: 0.08896709
Iteration 15/25 | Loss: 0.08896709
Iteration 16/25 | Loss: 0.08896708
Iteration 17/25 | Loss: 0.08896708
Iteration 18/25 | Loss: 0.08896708
Iteration 19/25 | Loss: 0.08896708
Iteration 20/25 | Loss: 0.08896708
Iteration 21/25 | Loss: 0.08896708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.08896708488464355, 0.08896708488464355, 0.08896708488464355, 0.08896708488464355, 0.08896708488464355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08896708488464355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08896708
Iteration 2/1000 | Loss: 0.00157144
Iteration 3/1000 | Loss: 0.00240718
Iteration 4/1000 | Loss: 0.00363603
Iteration 5/1000 | Loss: 0.00201765
Iteration 6/1000 | Loss: 0.00259869
Iteration 7/1000 | Loss: 0.00170924
Iteration 8/1000 | Loss: 0.00109149
Iteration 9/1000 | Loss: 0.00187274
Iteration 10/1000 | Loss: 0.00083051
Iteration 11/1000 | Loss: 0.00116249
Iteration 12/1000 | Loss: 0.00024726
Iteration 13/1000 | Loss: 0.00034787
Iteration 14/1000 | Loss: 0.00018592
Iteration 15/1000 | Loss: 0.00007810
Iteration 16/1000 | Loss: 0.00007936
Iteration 17/1000 | Loss: 0.00056672
Iteration 18/1000 | Loss: 0.00007040
Iteration 19/1000 | Loss: 0.00060202
Iteration 20/1000 | Loss: 0.00030632
Iteration 21/1000 | Loss: 0.00008336
Iteration 22/1000 | Loss: 0.00004118
Iteration 23/1000 | Loss: 0.00009769
Iteration 24/1000 | Loss: 0.00038964
Iteration 25/1000 | Loss: 0.00011362
Iteration 26/1000 | Loss: 0.00013109
Iteration 27/1000 | Loss: 0.00005079
Iteration 28/1000 | Loss: 0.00004563
Iteration 29/1000 | Loss: 0.00055491
Iteration 30/1000 | Loss: 0.00003383
Iteration 31/1000 | Loss: 0.00003610
Iteration 32/1000 | Loss: 0.00003245
Iteration 33/1000 | Loss: 0.00018518
Iteration 34/1000 | Loss: 0.00003153
Iteration 35/1000 | Loss: 0.00035905
Iteration 36/1000 | Loss: 0.00002694
Iteration 37/1000 | Loss: 0.00010947
Iteration 38/1000 | Loss: 0.00002438
Iteration 39/1000 | Loss: 0.00003730
Iteration 40/1000 | Loss: 0.00029121
Iteration 41/1000 | Loss: 0.00007672
Iteration 42/1000 | Loss: 0.00027153
Iteration 43/1000 | Loss: 0.00004549
Iteration 44/1000 | Loss: 0.00009729
Iteration 45/1000 | Loss: 0.00003221
Iteration 46/1000 | Loss: 0.00002421
Iteration 47/1000 | Loss: 0.00009506
Iteration 48/1000 | Loss: 0.00008725
Iteration 49/1000 | Loss: 0.00003692
Iteration 50/1000 | Loss: 0.00002454
Iteration 51/1000 | Loss: 0.00003987
Iteration 52/1000 | Loss: 0.00004315
Iteration 53/1000 | Loss: 0.00003386
Iteration 54/1000 | Loss: 0.00003051
Iteration 55/1000 | Loss: 0.00005516
Iteration 56/1000 | Loss: 0.00004385
Iteration 57/1000 | Loss: 0.00006566
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00007501
Iteration 60/1000 | Loss: 0.00002294
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00008249
Iteration 63/1000 | Loss: 0.00012043
Iteration 64/1000 | Loss: 0.00006418
Iteration 65/1000 | Loss: 0.00003395
Iteration 66/1000 | Loss: 0.00003821
Iteration 67/1000 | Loss: 0.00087770
Iteration 68/1000 | Loss: 0.00004766
Iteration 69/1000 | Loss: 0.00003852
Iteration 70/1000 | Loss: 0.00025826
Iteration 71/1000 | Loss: 0.00002353
Iteration 72/1000 | Loss: 0.00004473
Iteration 73/1000 | Loss: 0.00006058
Iteration 74/1000 | Loss: 0.00002706
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00010098
Iteration 77/1000 | Loss: 0.00002183
Iteration 78/1000 | Loss: 0.00003910
Iteration 79/1000 | Loss: 0.00003530
Iteration 80/1000 | Loss: 0.00011960
Iteration 81/1000 | Loss: 0.00002902
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00004097
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00007134
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00002107
Iteration 89/1000 | Loss: 0.00006404
Iteration 90/1000 | Loss: 0.00011683
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00007343
Iteration 93/1000 | Loss: 0.00015976
Iteration 94/1000 | Loss: 0.00047887
Iteration 95/1000 | Loss: 0.00117485
Iteration 96/1000 | Loss: 0.00002869
Iteration 97/1000 | Loss: 0.00003063
Iteration 98/1000 | Loss: 0.00008274
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001895
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00026687
Iteration 103/1000 | Loss: 0.00002951
Iteration 104/1000 | Loss: 0.00001953
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001875
Iteration 111/1000 | Loss: 0.00001875
Iteration 112/1000 | Loss: 0.00001875
Iteration 113/1000 | Loss: 0.00001875
Iteration 114/1000 | Loss: 0.00001875
Iteration 115/1000 | Loss: 0.00001875
Iteration 116/1000 | Loss: 0.00001874
Iteration 117/1000 | Loss: 0.00002428
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001870
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001868
Iteration 143/1000 | Loss: 0.00001868
Iteration 144/1000 | Loss: 0.00001868
Iteration 145/1000 | Loss: 0.00001868
Iteration 146/1000 | Loss: 0.00001868
Iteration 147/1000 | Loss: 0.00001868
Iteration 148/1000 | Loss: 0.00001868
Iteration 149/1000 | Loss: 0.00001868
Iteration 150/1000 | Loss: 0.00001867
Iteration 151/1000 | Loss: 0.00001867
Iteration 152/1000 | Loss: 0.00001867
Iteration 153/1000 | Loss: 0.00001867
Iteration 154/1000 | Loss: 0.00001867
Iteration 155/1000 | Loss: 0.00001867
Iteration 156/1000 | Loss: 0.00001867
Iteration 157/1000 | Loss: 0.00001867
Iteration 158/1000 | Loss: 0.00001867
Iteration 159/1000 | Loss: 0.00001867
Iteration 160/1000 | Loss: 0.00001866
Iteration 161/1000 | Loss: 0.00001866
Iteration 162/1000 | Loss: 0.00001866
Iteration 163/1000 | Loss: 0.00001866
Iteration 164/1000 | Loss: 0.00001865
Iteration 165/1000 | Loss: 0.00001865
Iteration 166/1000 | Loss: 0.00001865
Iteration 167/1000 | Loss: 0.00001865
Iteration 168/1000 | Loss: 0.00001865
Iteration 169/1000 | Loss: 0.00001865
Iteration 170/1000 | Loss: 0.00001865
Iteration 171/1000 | Loss: 0.00001865
Iteration 172/1000 | Loss: 0.00001864
Iteration 173/1000 | Loss: 0.00001864
Iteration 174/1000 | Loss: 0.00001864
Iteration 175/1000 | Loss: 0.00001864
Iteration 176/1000 | Loss: 0.00001864
Iteration 177/1000 | Loss: 0.00001864
Iteration 178/1000 | Loss: 0.00001864
Iteration 179/1000 | Loss: 0.00002219
Iteration 180/1000 | Loss: 0.00002917
Iteration 181/1000 | Loss: 0.00005628
Iteration 182/1000 | Loss: 0.00001983
Iteration 183/1000 | Loss: 0.00001861
Iteration 184/1000 | Loss: 0.00001861
Iteration 185/1000 | Loss: 0.00001861
Iteration 186/1000 | Loss: 0.00001861
Iteration 187/1000 | Loss: 0.00001861
Iteration 188/1000 | Loss: 0.00001861
Iteration 189/1000 | Loss: 0.00001861
Iteration 190/1000 | Loss: 0.00001861
Iteration 191/1000 | Loss: 0.00001861
Iteration 192/1000 | Loss: 0.00001861
Iteration 193/1000 | Loss: 0.00001860
Iteration 194/1000 | Loss: 0.00001865
Iteration 195/1000 | Loss: 0.00001865
Iteration 196/1000 | Loss: 0.00001860
Iteration 197/1000 | Loss: 0.00001859
Iteration 198/1000 | Loss: 0.00001859
Iteration 199/1000 | Loss: 0.00001859
Iteration 200/1000 | Loss: 0.00001859
Iteration 201/1000 | Loss: 0.00001859
Iteration 202/1000 | Loss: 0.00001859
Iteration 203/1000 | Loss: 0.00001859
Iteration 204/1000 | Loss: 0.00001859
Iteration 205/1000 | Loss: 0.00001859
Iteration 206/1000 | Loss: 0.00001859
Iteration 207/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.8590682884678245e-05, 1.8590682884678245e-05, 1.8590682884678245e-05, 1.8590682884678245e-05, 1.8590682884678245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8590682884678245e-05

Optimization complete. Final v2v error: 3.6805224418640137 mm

Highest mean error: 5.06422233581543 mm for frame 53

Lowest mean error: 2.8482699394226074 mm for frame 134

Saving results

Total time: 188.85613203048706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565620
Iteration 2/25 | Loss: 0.00137901
Iteration 3/25 | Loss: 0.00127519
Iteration 4/25 | Loss: 0.00125612
Iteration 5/25 | Loss: 0.00125000
Iteration 6/25 | Loss: 0.00124849
Iteration 7/25 | Loss: 0.00124832
Iteration 8/25 | Loss: 0.00124832
Iteration 9/25 | Loss: 0.00124832
Iteration 10/25 | Loss: 0.00124832
Iteration 11/25 | Loss: 0.00124832
Iteration 12/25 | Loss: 0.00124832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012483242899179459, 0.0012483242899179459, 0.0012483242899179459, 0.0012483242899179459, 0.0012483242899179459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012483242899179459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.76387453
Iteration 2/25 | Loss: 0.00139731
Iteration 3/25 | Loss: 0.00139731
Iteration 4/25 | Loss: 0.00139730
Iteration 5/25 | Loss: 0.00139730
Iteration 6/25 | Loss: 0.00139730
Iteration 7/25 | Loss: 0.00139730
Iteration 8/25 | Loss: 0.00139730
Iteration 9/25 | Loss: 0.00139730
Iteration 10/25 | Loss: 0.00139730
Iteration 11/25 | Loss: 0.00139730
Iteration 12/25 | Loss: 0.00139730
Iteration 13/25 | Loss: 0.00139730
Iteration 14/25 | Loss: 0.00139730
Iteration 15/25 | Loss: 0.00139730
Iteration 16/25 | Loss: 0.00139730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013973028399050236, 0.0013973028399050236, 0.0013973028399050236, 0.0013973028399050236, 0.0013973028399050236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013973028399050236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139730
Iteration 2/1000 | Loss: 0.00003939
Iteration 3/1000 | Loss: 0.00002611
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00002001
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001843
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001817
Iteration 13/1000 | Loss: 0.00001805
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001791
Iteration 16/1000 | Loss: 0.00001778
Iteration 17/1000 | Loss: 0.00001773
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001759
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001758
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001755
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001753
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001747
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001743
Iteration 89/1000 | Loss: 0.00001743
Iteration 90/1000 | Loss: 0.00001743
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001742
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001739
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001739
Iteration 100/1000 | Loss: 0.00001738
Iteration 101/1000 | Loss: 0.00001738
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001736
Iteration 105/1000 | Loss: 0.00001736
Iteration 106/1000 | Loss: 0.00001736
Iteration 107/1000 | Loss: 0.00001736
Iteration 108/1000 | Loss: 0.00001736
Iteration 109/1000 | Loss: 0.00001736
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001735
Iteration 112/1000 | Loss: 0.00001735
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001734
Iteration 116/1000 | Loss: 0.00001734
Iteration 117/1000 | Loss: 0.00001734
Iteration 118/1000 | Loss: 0.00001733
Iteration 119/1000 | Loss: 0.00001733
Iteration 120/1000 | Loss: 0.00001733
Iteration 121/1000 | Loss: 0.00001733
Iteration 122/1000 | Loss: 0.00001733
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001732
Iteration 127/1000 | Loss: 0.00001732
Iteration 128/1000 | Loss: 0.00001732
Iteration 129/1000 | Loss: 0.00001732
Iteration 130/1000 | Loss: 0.00001732
Iteration 131/1000 | Loss: 0.00001732
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001731
Iteration 136/1000 | Loss: 0.00001731
Iteration 137/1000 | Loss: 0.00001731
Iteration 138/1000 | Loss: 0.00001730
Iteration 139/1000 | Loss: 0.00001730
Iteration 140/1000 | Loss: 0.00001730
Iteration 141/1000 | Loss: 0.00001730
Iteration 142/1000 | Loss: 0.00001730
Iteration 143/1000 | Loss: 0.00001730
Iteration 144/1000 | Loss: 0.00001730
Iteration 145/1000 | Loss: 0.00001729
Iteration 146/1000 | Loss: 0.00001729
Iteration 147/1000 | Loss: 0.00001729
Iteration 148/1000 | Loss: 0.00001729
Iteration 149/1000 | Loss: 0.00001729
Iteration 150/1000 | Loss: 0.00001729
Iteration 151/1000 | Loss: 0.00001729
Iteration 152/1000 | Loss: 0.00001728
Iteration 153/1000 | Loss: 0.00001728
Iteration 154/1000 | Loss: 0.00001728
Iteration 155/1000 | Loss: 0.00001728
Iteration 156/1000 | Loss: 0.00001728
Iteration 157/1000 | Loss: 0.00001727
Iteration 158/1000 | Loss: 0.00001727
Iteration 159/1000 | Loss: 0.00001727
Iteration 160/1000 | Loss: 0.00001727
Iteration 161/1000 | Loss: 0.00001727
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001726
Iteration 164/1000 | Loss: 0.00001726
Iteration 165/1000 | Loss: 0.00001726
Iteration 166/1000 | Loss: 0.00001726
Iteration 167/1000 | Loss: 0.00001726
Iteration 168/1000 | Loss: 0.00001726
Iteration 169/1000 | Loss: 0.00001726
Iteration 170/1000 | Loss: 0.00001726
Iteration 171/1000 | Loss: 0.00001726
Iteration 172/1000 | Loss: 0.00001726
Iteration 173/1000 | Loss: 0.00001726
Iteration 174/1000 | Loss: 0.00001726
Iteration 175/1000 | Loss: 0.00001726
Iteration 176/1000 | Loss: 0.00001726
Iteration 177/1000 | Loss: 0.00001726
Iteration 178/1000 | Loss: 0.00001726
Iteration 179/1000 | Loss: 0.00001726
Iteration 180/1000 | Loss: 0.00001725
Iteration 181/1000 | Loss: 0.00001725
Iteration 182/1000 | Loss: 0.00001725
Iteration 183/1000 | Loss: 0.00001725
Iteration 184/1000 | Loss: 0.00001725
Iteration 185/1000 | Loss: 0.00001725
Iteration 186/1000 | Loss: 0.00001725
Iteration 187/1000 | Loss: 0.00001724
Iteration 188/1000 | Loss: 0.00001724
Iteration 189/1000 | Loss: 0.00001724
Iteration 190/1000 | Loss: 0.00001724
Iteration 191/1000 | Loss: 0.00001724
Iteration 192/1000 | Loss: 0.00001724
Iteration 193/1000 | Loss: 0.00001724
Iteration 194/1000 | Loss: 0.00001724
Iteration 195/1000 | Loss: 0.00001724
Iteration 196/1000 | Loss: 0.00001724
Iteration 197/1000 | Loss: 0.00001724
Iteration 198/1000 | Loss: 0.00001724
Iteration 199/1000 | Loss: 0.00001724
Iteration 200/1000 | Loss: 0.00001724
Iteration 201/1000 | Loss: 0.00001723
Iteration 202/1000 | Loss: 0.00001723
Iteration 203/1000 | Loss: 0.00001723
Iteration 204/1000 | Loss: 0.00001723
Iteration 205/1000 | Loss: 0.00001723
Iteration 206/1000 | Loss: 0.00001723
Iteration 207/1000 | Loss: 0.00001723
Iteration 208/1000 | Loss: 0.00001723
Iteration 209/1000 | Loss: 0.00001723
Iteration 210/1000 | Loss: 0.00001723
Iteration 211/1000 | Loss: 0.00001723
Iteration 212/1000 | Loss: 0.00001723
Iteration 213/1000 | Loss: 0.00001723
Iteration 214/1000 | Loss: 0.00001723
Iteration 215/1000 | Loss: 0.00001723
Iteration 216/1000 | Loss: 0.00001723
Iteration 217/1000 | Loss: 0.00001723
Iteration 218/1000 | Loss: 0.00001723
Iteration 219/1000 | Loss: 0.00001723
Iteration 220/1000 | Loss: 0.00001723
Iteration 221/1000 | Loss: 0.00001723
Iteration 222/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.7234260667464696e-05, 1.7234260667464696e-05, 1.7234260667464696e-05, 1.7234260667464696e-05, 1.7234260667464696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7234260667464696e-05

Optimization complete. Final v2v error: 3.459928274154663 mm

Highest mean error: 4.934113025665283 mm for frame 97

Lowest mean error: 2.8265655040740967 mm for frame 0

Saving results

Total time: 43.00134754180908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893078
Iteration 2/25 | Loss: 0.00164740
Iteration 3/25 | Loss: 0.00141405
Iteration 4/25 | Loss: 0.00139896
Iteration 5/25 | Loss: 0.00138038
Iteration 6/25 | Loss: 0.00137461
Iteration 7/25 | Loss: 0.00136938
Iteration 8/25 | Loss: 0.00136660
Iteration 9/25 | Loss: 0.00135577
Iteration 10/25 | Loss: 0.00134603
Iteration 11/25 | Loss: 0.00135016
Iteration 12/25 | Loss: 0.00134285
Iteration 13/25 | Loss: 0.00134125
Iteration 14/25 | Loss: 0.00134082
Iteration 15/25 | Loss: 0.00134144
Iteration 16/25 | Loss: 0.00133966
Iteration 17/25 | Loss: 0.00133907
Iteration 18/25 | Loss: 0.00133856
Iteration 19/25 | Loss: 0.00134148
Iteration 20/25 | Loss: 0.00134226
Iteration 21/25 | Loss: 0.00133915
Iteration 22/25 | Loss: 0.00133767
Iteration 23/25 | Loss: 0.00133696
Iteration 24/25 | Loss: 0.00133625
Iteration 25/25 | Loss: 0.00133608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60906661
Iteration 2/25 | Loss: 0.00106102
Iteration 3/25 | Loss: 0.00106101
Iteration 4/25 | Loss: 0.00106101
Iteration 5/25 | Loss: 0.00106101
Iteration 6/25 | Loss: 0.00106101
Iteration 7/25 | Loss: 0.00106101
Iteration 8/25 | Loss: 0.00106101
Iteration 9/25 | Loss: 0.00106101
Iteration 10/25 | Loss: 0.00106101
Iteration 11/25 | Loss: 0.00106101
Iteration 12/25 | Loss: 0.00106101
Iteration 13/25 | Loss: 0.00106101
Iteration 14/25 | Loss: 0.00106101
Iteration 15/25 | Loss: 0.00106101
Iteration 16/25 | Loss: 0.00106101
Iteration 17/25 | Loss: 0.00106101
Iteration 18/25 | Loss: 0.00106101
Iteration 19/25 | Loss: 0.00106101
Iteration 20/25 | Loss: 0.00106101
Iteration 21/25 | Loss: 0.00106101
Iteration 22/25 | Loss: 0.00106101
Iteration 23/25 | Loss: 0.00106101
Iteration 24/25 | Loss: 0.00106101
Iteration 25/25 | Loss: 0.00106101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106101
Iteration 2/1000 | Loss: 0.00008744
Iteration 3/1000 | Loss: 0.00006104
Iteration 4/1000 | Loss: 0.00005075
Iteration 5/1000 | Loss: 0.00004708
Iteration 6/1000 | Loss: 0.00004485
Iteration 7/1000 | Loss: 0.00004315
Iteration 8/1000 | Loss: 0.00039311
Iteration 9/1000 | Loss: 0.00004171
Iteration 10/1000 | Loss: 0.00003980
Iteration 11/1000 | Loss: 0.00003822
Iteration 12/1000 | Loss: 0.00026340
Iteration 13/1000 | Loss: 0.00003700
Iteration 14/1000 | Loss: 0.00013364
Iteration 15/1000 | Loss: 0.00151636
Iteration 16/1000 | Loss: 0.00016637
Iteration 17/1000 | Loss: 0.00015857
Iteration 18/1000 | Loss: 0.00003640
Iteration 19/1000 | Loss: 0.00003561
Iteration 20/1000 | Loss: 0.00145200
Iteration 21/1000 | Loss: 0.00273119
Iteration 22/1000 | Loss: 0.00078841
Iteration 23/1000 | Loss: 0.00007781
Iteration 24/1000 | Loss: 0.00005481
Iteration 25/1000 | Loss: 0.00004332
Iteration 26/1000 | Loss: 0.00003903
Iteration 27/1000 | Loss: 0.00003712
Iteration 28/1000 | Loss: 0.00003500
Iteration 29/1000 | Loss: 0.00045363
Iteration 30/1000 | Loss: 0.00032885
Iteration 31/1000 | Loss: 0.00017919
Iteration 32/1000 | Loss: 0.00034204
Iteration 33/1000 | Loss: 0.00099581
Iteration 34/1000 | Loss: 0.00056457
Iteration 35/1000 | Loss: 0.00004508
Iteration 36/1000 | Loss: 0.00003484
Iteration 37/1000 | Loss: 0.00003244
Iteration 38/1000 | Loss: 0.00003114
Iteration 39/1000 | Loss: 0.00002993
Iteration 40/1000 | Loss: 0.00018830
Iteration 41/1000 | Loss: 0.00003606
Iteration 42/1000 | Loss: 0.00003248
Iteration 43/1000 | Loss: 0.00002945
Iteration 44/1000 | Loss: 0.00002890
Iteration 45/1000 | Loss: 0.00002874
Iteration 46/1000 | Loss: 0.00002852
Iteration 47/1000 | Loss: 0.00002830
Iteration 48/1000 | Loss: 0.00002816
Iteration 49/1000 | Loss: 0.00002807
Iteration 50/1000 | Loss: 0.00002807
Iteration 51/1000 | Loss: 0.00002806
Iteration 52/1000 | Loss: 0.00002806
Iteration 53/1000 | Loss: 0.00002805
Iteration 54/1000 | Loss: 0.00002805
Iteration 55/1000 | Loss: 0.00002804
Iteration 56/1000 | Loss: 0.00002803
Iteration 57/1000 | Loss: 0.00002803
Iteration 58/1000 | Loss: 0.00002802
Iteration 59/1000 | Loss: 0.00002801
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002800
Iteration 62/1000 | Loss: 0.00002800
Iteration 63/1000 | Loss: 0.00002800
Iteration 64/1000 | Loss: 0.00002800
Iteration 65/1000 | Loss: 0.00002799
Iteration 66/1000 | Loss: 0.00002799
Iteration 67/1000 | Loss: 0.00002798
Iteration 68/1000 | Loss: 0.00002798
Iteration 69/1000 | Loss: 0.00002798
Iteration 70/1000 | Loss: 0.00002798
Iteration 71/1000 | Loss: 0.00002798
Iteration 72/1000 | Loss: 0.00002797
Iteration 73/1000 | Loss: 0.00002797
Iteration 74/1000 | Loss: 0.00002797
Iteration 75/1000 | Loss: 0.00002797
Iteration 76/1000 | Loss: 0.00002796
Iteration 77/1000 | Loss: 0.00002796
Iteration 78/1000 | Loss: 0.00002796
Iteration 79/1000 | Loss: 0.00002796
Iteration 80/1000 | Loss: 0.00002796
Iteration 81/1000 | Loss: 0.00002796
Iteration 82/1000 | Loss: 0.00002796
Iteration 83/1000 | Loss: 0.00002795
Iteration 84/1000 | Loss: 0.00002794
Iteration 85/1000 | Loss: 0.00002793
Iteration 86/1000 | Loss: 0.00002792
Iteration 87/1000 | Loss: 0.00002790
Iteration 88/1000 | Loss: 0.00002790
Iteration 89/1000 | Loss: 0.00002790
Iteration 90/1000 | Loss: 0.00002790
Iteration 91/1000 | Loss: 0.00002790
Iteration 92/1000 | Loss: 0.00002790
Iteration 93/1000 | Loss: 0.00002790
Iteration 94/1000 | Loss: 0.00002790
Iteration 95/1000 | Loss: 0.00002790
Iteration 96/1000 | Loss: 0.00002789
Iteration 97/1000 | Loss: 0.00002789
Iteration 98/1000 | Loss: 0.00002789
Iteration 99/1000 | Loss: 0.00002789
Iteration 100/1000 | Loss: 0.00002789
Iteration 101/1000 | Loss: 0.00002789
Iteration 102/1000 | Loss: 0.00002789
Iteration 103/1000 | Loss: 0.00002789
Iteration 104/1000 | Loss: 0.00002789
Iteration 105/1000 | Loss: 0.00002789
Iteration 106/1000 | Loss: 0.00002788
Iteration 107/1000 | Loss: 0.00002787
Iteration 108/1000 | Loss: 0.00002787
Iteration 109/1000 | Loss: 0.00002787
Iteration 110/1000 | Loss: 0.00002786
Iteration 111/1000 | Loss: 0.00002786
Iteration 112/1000 | Loss: 0.00002786
Iteration 113/1000 | Loss: 0.00002785
Iteration 114/1000 | Loss: 0.00002785
Iteration 115/1000 | Loss: 0.00002784
Iteration 116/1000 | Loss: 0.00002784
Iteration 117/1000 | Loss: 0.00002783
Iteration 118/1000 | Loss: 0.00002783
Iteration 119/1000 | Loss: 0.00002782
Iteration 120/1000 | Loss: 0.00002782
Iteration 121/1000 | Loss: 0.00002782
Iteration 122/1000 | Loss: 0.00002782
Iteration 123/1000 | Loss: 0.00002781
Iteration 124/1000 | Loss: 0.00002781
Iteration 125/1000 | Loss: 0.00002780
Iteration 126/1000 | Loss: 0.00002780
Iteration 127/1000 | Loss: 0.00002780
Iteration 128/1000 | Loss: 0.00002780
Iteration 129/1000 | Loss: 0.00002780
Iteration 130/1000 | Loss: 0.00002780
Iteration 131/1000 | Loss: 0.00002780
Iteration 132/1000 | Loss: 0.00002779
Iteration 133/1000 | Loss: 0.00002779
Iteration 134/1000 | Loss: 0.00002779
Iteration 135/1000 | Loss: 0.00002779
Iteration 136/1000 | Loss: 0.00002778
Iteration 137/1000 | Loss: 0.00002778
Iteration 138/1000 | Loss: 0.00002778
Iteration 139/1000 | Loss: 0.00002778
Iteration 140/1000 | Loss: 0.00002777
Iteration 141/1000 | Loss: 0.00002777
Iteration 142/1000 | Loss: 0.00002776
Iteration 143/1000 | Loss: 0.00002776
Iteration 144/1000 | Loss: 0.00002776
Iteration 145/1000 | Loss: 0.00002775
Iteration 146/1000 | Loss: 0.00002775
Iteration 147/1000 | Loss: 0.00002774
Iteration 148/1000 | Loss: 0.00002774
Iteration 149/1000 | Loss: 0.00002774
Iteration 150/1000 | Loss: 0.00002773
Iteration 151/1000 | Loss: 0.00002773
Iteration 152/1000 | Loss: 0.00002773
Iteration 153/1000 | Loss: 0.00002773
Iteration 154/1000 | Loss: 0.00002773
Iteration 155/1000 | Loss: 0.00002773
Iteration 156/1000 | Loss: 0.00002772
Iteration 157/1000 | Loss: 0.00002772
Iteration 158/1000 | Loss: 0.00002772
Iteration 159/1000 | Loss: 0.00002772
Iteration 160/1000 | Loss: 0.00002772
Iteration 161/1000 | Loss: 0.00002772
Iteration 162/1000 | Loss: 0.00002772
Iteration 163/1000 | Loss: 0.00002772
Iteration 164/1000 | Loss: 0.00002771
Iteration 165/1000 | Loss: 0.00002771
Iteration 166/1000 | Loss: 0.00002771
Iteration 167/1000 | Loss: 0.00002771
Iteration 168/1000 | Loss: 0.00002771
Iteration 169/1000 | Loss: 0.00002771
Iteration 170/1000 | Loss: 0.00002771
Iteration 171/1000 | Loss: 0.00002770
Iteration 172/1000 | Loss: 0.00002770
Iteration 173/1000 | Loss: 0.00002770
Iteration 174/1000 | Loss: 0.00002770
Iteration 175/1000 | Loss: 0.00002770
Iteration 176/1000 | Loss: 0.00002770
Iteration 177/1000 | Loss: 0.00002769
Iteration 178/1000 | Loss: 0.00002769
Iteration 179/1000 | Loss: 0.00002769
Iteration 180/1000 | Loss: 0.00002768
Iteration 181/1000 | Loss: 0.00002768
Iteration 182/1000 | Loss: 0.00002768
Iteration 183/1000 | Loss: 0.00002768
Iteration 184/1000 | Loss: 0.00002768
Iteration 185/1000 | Loss: 0.00002768
Iteration 186/1000 | Loss: 0.00002768
Iteration 187/1000 | Loss: 0.00002768
Iteration 188/1000 | Loss: 0.00002767
Iteration 189/1000 | Loss: 0.00002767
Iteration 190/1000 | Loss: 0.00002767
Iteration 191/1000 | Loss: 0.00002767
Iteration 192/1000 | Loss: 0.00002767
Iteration 193/1000 | Loss: 0.00002767
Iteration 194/1000 | Loss: 0.00002767
Iteration 195/1000 | Loss: 0.00002767
Iteration 196/1000 | Loss: 0.00002767
Iteration 197/1000 | Loss: 0.00002767
Iteration 198/1000 | Loss: 0.00002767
Iteration 199/1000 | Loss: 0.00002767
Iteration 200/1000 | Loss: 0.00002767
Iteration 201/1000 | Loss: 0.00002767
Iteration 202/1000 | Loss: 0.00002767
Iteration 203/1000 | Loss: 0.00002767
Iteration 204/1000 | Loss: 0.00002767
Iteration 205/1000 | Loss: 0.00002767
Iteration 206/1000 | Loss: 0.00002767
Iteration 207/1000 | Loss: 0.00002766
Iteration 208/1000 | Loss: 0.00002766
Iteration 209/1000 | Loss: 0.00002766
Iteration 210/1000 | Loss: 0.00002766
Iteration 211/1000 | Loss: 0.00002766
Iteration 212/1000 | Loss: 0.00002766
Iteration 213/1000 | Loss: 0.00002766
Iteration 214/1000 | Loss: 0.00002766
Iteration 215/1000 | Loss: 0.00002766
Iteration 216/1000 | Loss: 0.00002766
Iteration 217/1000 | Loss: 0.00002766
Iteration 218/1000 | Loss: 0.00002766
Iteration 219/1000 | Loss: 0.00002766
Iteration 220/1000 | Loss: 0.00002766
Iteration 221/1000 | Loss: 0.00002766
Iteration 222/1000 | Loss: 0.00002766
Iteration 223/1000 | Loss: 0.00002766
Iteration 224/1000 | Loss: 0.00002766
Iteration 225/1000 | Loss: 0.00002766
Iteration 226/1000 | Loss: 0.00002766
Iteration 227/1000 | Loss: 0.00002766
Iteration 228/1000 | Loss: 0.00002766
Iteration 229/1000 | Loss: 0.00002766
Iteration 230/1000 | Loss: 0.00002766
Iteration 231/1000 | Loss: 0.00002766
Iteration 232/1000 | Loss: 0.00002766
Iteration 233/1000 | Loss: 0.00002766
Iteration 234/1000 | Loss: 0.00002766
Iteration 235/1000 | Loss: 0.00002766
Iteration 236/1000 | Loss: 0.00002766
Iteration 237/1000 | Loss: 0.00002766
Iteration 238/1000 | Loss: 0.00002766
Iteration 239/1000 | Loss: 0.00002766
Iteration 240/1000 | Loss: 0.00002766
Iteration 241/1000 | Loss: 0.00002766
Iteration 242/1000 | Loss: 0.00002766
Iteration 243/1000 | Loss: 0.00002766
Iteration 244/1000 | Loss: 0.00002766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.76635582849849e-05, 2.76635582849849e-05, 2.76635582849849e-05, 2.76635582849849e-05, 2.76635582849849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.76635582849849e-05

Optimization complete. Final v2v error: 3.7958686351776123 mm

Highest mean error: 12.659348487854004 mm for frame 40

Lowest mean error: 3.005345106124878 mm for frame 141

Saving results

Total time: 141.5539574623108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453259
Iteration 2/25 | Loss: 0.00129314
Iteration 3/25 | Loss: 0.00123367
Iteration 4/25 | Loss: 0.00122451
Iteration 5/25 | Loss: 0.00122143
Iteration 6/25 | Loss: 0.00122122
Iteration 7/25 | Loss: 0.00122122
Iteration 8/25 | Loss: 0.00122122
Iteration 9/25 | Loss: 0.00122122
Iteration 10/25 | Loss: 0.00122122
Iteration 11/25 | Loss: 0.00122122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001221224432811141, 0.001221224432811141, 0.001221224432811141, 0.001221224432811141, 0.001221224432811141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001221224432811141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31535804
Iteration 2/25 | Loss: 0.00110312
Iteration 3/25 | Loss: 0.00110311
Iteration 4/25 | Loss: 0.00110311
Iteration 5/25 | Loss: 0.00110311
Iteration 6/25 | Loss: 0.00110311
Iteration 7/25 | Loss: 0.00110311
Iteration 8/25 | Loss: 0.00110311
Iteration 9/25 | Loss: 0.00110311
Iteration 10/25 | Loss: 0.00110311
Iteration 11/25 | Loss: 0.00110311
Iteration 12/25 | Loss: 0.00110311
Iteration 13/25 | Loss: 0.00110311
Iteration 14/25 | Loss: 0.00110311
Iteration 15/25 | Loss: 0.00110311
Iteration 16/25 | Loss: 0.00110311
Iteration 17/25 | Loss: 0.00110311
Iteration 18/25 | Loss: 0.00110311
Iteration 19/25 | Loss: 0.00110311
Iteration 20/25 | Loss: 0.00110311
Iteration 21/25 | Loss: 0.00110311
Iteration 22/25 | Loss: 0.00110311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011031079338863492, 0.0011031079338863492, 0.0011031079338863492, 0.0011031079338863492, 0.0011031079338863492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011031079338863492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110311
Iteration 2/1000 | Loss: 0.00003032
Iteration 3/1000 | Loss: 0.00002349
Iteration 4/1000 | Loss: 0.00002146
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001963
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001842
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001730
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001723
Iteration 15/1000 | Loss: 0.00001705
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001694
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001694
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001693
Iteration 26/1000 | Loss: 0.00001693
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001692
Iteration 30/1000 | Loss: 0.00001692
Iteration 31/1000 | Loss: 0.00001689
Iteration 32/1000 | Loss: 0.00001688
Iteration 33/1000 | Loss: 0.00001688
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001666
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001661
Iteration 54/1000 | Loss: 0.00001660
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001658
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001657
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001656
Iteration 68/1000 | Loss: 0.00001656
Iteration 69/1000 | Loss: 0.00001656
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001655
Iteration 72/1000 | Loss: 0.00001655
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001650
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001649
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001648
Iteration 92/1000 | Loss: 0.00001648
Iteration 93/1000 | Loss: 0.00001648
Iteration 94/1000 | Loss: 0.00001648
Iteration 95/1000 | Loss: 0.00001648
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001647
Iteration 99/1000 | Loss: 0.00001647
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001647
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001646
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001646
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001645
Iteration 111/1000 | Loss: 0.00001645
Iteration 112/1000 | Loss: 0.00001645
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001643
Iteration 117/1000 | Loss: 0.00001643
Iteration 118/1000 | Loss: 0.00001643
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001642
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001641
Iteration 129/1000 | Loss: 0.00001641
Iteration 130/1000 | Loss: 0.00001641
Iteration 131/1000 | Loss: 0.00001641
Iteration 132/1000 | Loss: 0.00001641
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001638
Iteration 150/1000 | Loss: 0.00001638
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001638
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Iteration 170/1000 | Loss: 0.00001637
Iteration 171/1000 | Loss: 0.00001637
Iteration 172/1000 | Loss: 0.00001637
Iteration 173/1000 | Loss: 0.00001637
Iteration 174/1000 | Loss: 0.00001637
Iteration 175/1000 | Loss: 0.00001637
Iteration 176/1000 | Loss: 0.00001637
Iteration 177/1000 | Loss: 0.00001636
Iteration 178/1000 | Loss: 0.00001636
Iteration 179/1000 | Loss: 0.00001636
Iteration 180/1000 | Loss: 0.00001636
Iteration 181/1000 | Loss: 0.00001636
Iteration 182/1000 | Loss: 0.00001636
Iteration 183/1000 | Loss: 0.00001636
Iteration 184/1000 | Loss: 0.00001636
Iteration 185/1000 | Loss: 0.00001636
Iteration 186/1000 | Loss: 0.00001636
Iteration 187/1000 | Loss: 0.00001636
Iteration 188/1000 | Loss: 0.00001636
Iteration 189/1000 | Loss: 0.00001636
Iteration 190/1000 | Loss: 0.00001636
Iteration 191/1000 | Loss: 0.00001636
Iteration 192/1000 | Loss: 0.00001636
Iteration 193/1000 | Loss: 0.00001636
Iteration 194/1000 | Loss: 0.00001636
Iteration 195/1000 | Loss: 0.00001636
Iteration 196/1000 | Loss: 0.00001636
Iteration 197/1000 | Loss: 0.00001636
Iteration 198/1000 | Loss: 0.00001636
Iteration 199/1000 | Loss: 0.00001635
Iteration 200/1000 | Loss: 0.00001635
Iteration 201/1000 | Loss: 0.00001635
Iteration 202/1000 | Loss: 0.00001635
Iteration 203/1000 | Loss: 0.00001635
Iteration 204/1000 | Loss: 0.00001635
Iteration 205/1000 | Loss: 0.00001635
Iteration 206/1000 | Loss: 0.00001635
Iteration 207/1000 | Loss: 0.00001635
Iteration 208/1000 | Loss: 0.00001635
Iteration 209/1000 | Loss: 0.00001635
Iteration 210/1000 | Loss: 0.00001635
Iteration 211/1000 | Loss: 0.00001635
Iteration 212/1000 | Loss: 0.00001635
Iteration 213/1000 | Loss: 0.00001635
Iteration 214/1000 | Loss: 0.00001635
Iteration 215/1000 | Loss: 0.00001635
Iteration 216/1000 | Loss: 0.00001635
Iteration 217/1000 | Loss: 0.00001635
Iteration 218/1000 | Loss: 0.00001635
Iteration 219/1000 | Loss: 0.00001635
Iteration 220/1000 | Loss: 0.00001635
Iteration 221/1000 | Loss: 0.00001635
Iteration 222/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.6350384612451307e-05, 1.6350384612451307e-05, 1.6350384612451307e-05, 1.6350384612451307e-05, 1.6350384612451307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6350384612451307e-05

Optimization complete. Final v2v error: 3.4542455673217773 mm

Highest mean error: 3.5847389698028564 mm for frame 123

Lowest mean error: 3.2925562858581543 mm for frame 42

Saving results

Total time: 42.48049831390381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866703
Iteration 2/25 | Loss: 0.00136070
Iteration 3/25 | Loss: 0.00123331
Iteration 4/25 | Loss: 0.00122038
Iteration 5/25 | Loss: 0.00121725
Iteration 6/25 | Loss: 0.00121667
Iteration 7/25 | Loss: 0.00121667
Iteration 8/25 | Loss: 0.00121667
Iteration 9/25 | Loss: 0.00121667
Iteration 10/25 | Loss: 0.00121667
Iteration 11/25 | Loss: 0.00121667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001216666423715651, 0.001216666423715651, 0.001216666423715651, 0.001216666423715651, 0.001216666423715651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001216666423715651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40461576
Iteration 2/25 | Loss: 0.00123975
Iteration 3/25 | Loss: 0.00123974
Iteration 4/25 | Loss: 0.00123974
Iteration 5/25 | Loss: 0.00123974
Iteration 6/25 | Loss: 0.00123974
Iteration 7/25 | Loss: 0.00123974
Iteration 8/25 | Loss: 0.00123974
Iteration 9/25 | Loss: 0.00123974
Iteration 10/25 | Loss: 0.00123973
Iteration 11/25 | Loss: 0.00123973
Iteration 12/25 | Loss: 0.00123973
Iteration 13/25 | Loss: 0.00123973
Iteration 14/25 | Loss: 0.00123973
Iteration 15/25 | Loss: 0.00123973
Iteration 16/25 | Loss: 0.00123973
Iteration 17/25 | Loss: 0.00123973
Iteration 18/25 | Loss: 0.00123973
Iteration 19/25 | Loss: 0.00123973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00123973423615098, 0.00123973423615098, 0.00123973423615098, 0.00123973423615098, 0.00123973423615098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00123973423615098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123973
Iteration 2/1000 | Loss: 0.00002243
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001422
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001150
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001121
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001121
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001096
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001087
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001087
Iteration 107/1000 | Loss: 0.00001086
Iteration 108/1000 | Loss: 0.00001086
Iteration 109/1000 | Loss: 0.00001086
Iteration 110/1000 | Loss: 0.00001086
Iteration 111/1000 | Loss: 0.00001086
Iteration 112/1000 | Loss: 0.00001086
Iteration 113/1000 | Loss: 0.00001086
Iteration 114/1000 | Loss: 0.00001085
Iteration 115/1000 | Loss: 0.00001085
Iteration 116/1000 | Loss: 0.00001085
Iteration 117/1000 | Loss: 0.00001085
Iteration 118/1000 | Loss: 0.00001085
Iteration 119/1000 | Loss: 0.00001085
Iteration 120/1000 | Loss: 0.00001085
Iteration 121/1000 | Loss: 0.00001085
Iteration 122/1000 | Loss: 0.00001085
Iteration 123/1000 | Loss: 0.00001084
Iteration 124/1000 | Loss: 0.00001084
Iteration 125/1000 | Loss: 0.00001084
Iteration 126/1000 | Loss: 0.00001084
Iteration 127/1000 | Loss: 0.00001084
Iteration 128/1000 | Loss: 0.00001084
Iteration 129/1000 | Loss: 0.00001084
Iteration 130/1000 | Loss: 0.00001084
Iteration 131/1000 | Loss: 0.00001084
Iteration 132/1000 | Loss: 0.00001084
Iteration 133/1000 | Loss: 0.00001084
Iteration 134/1000 | Loss: 0.00001084
Iteration 135/1000 | Loss: 0.00001084
Iteration 136/1000 | Loss: 0.00001084
Iteration 137/1000 | Loss: 0.00001084
Iteration 138/1000 | Loss: 0.00001084
Iteration 139/1000 | Loss: 0.00001084
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001084
Iteration 142/1000 | Loss: 0.00001084
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0835858120117337e-05, 1.0835858120117337e-05, 1.0835858120117337e-05, 1.0835858120117337e-05, 1.0835858120117337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0835858120117337e-05

Optimization complete. Final v2v error: 2.7762396335601807 mm

Highest mean error: 3.587899923324585 mm for frame 58

Lowest mean error: 2.5561795234680176 mm for frame 138

Saving results

Total time: 37.969557762145996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409654
Iteration 2/25 | Loss: 0.00150306
Iteration 3/25 | Loss: 0.00124629
Iteration 4/25 | Loss: 0.00122320
Iteration 5/25 | Loss: 0.00122008
Iteration 6/25 | Loss: 0.00121946
Iteration 7/25 | Loss: 0.00121946
Iteration 8/25 | Loss: 0.00121946
Iteration 9/25 | Loss: 0.00121946
Iteration 10/25 | Loss: 0.00121946
Iteration 11/25 | Loss: 0.00121946
Iteration 12/25 | Loss: 0.00121946
Iteration 13/25 | Loss: 0.00121946
Iteration 14/25 | Loss: 0.00121946
Iteration 15/25 | Loss: 0.00121946
Iteration 16/25 | Loss: 0.00121946
Iteration 17/25 | Loss: 0.00121946
Iteration 18/25 | Loss: 0.00121946
Iteration 19/25 | Loss: 0.00121946
Iteration 20/25 | Loss: 0.00121946
Iteration 21/25 | Loss: 0.00121946
Iteration 22/25 | Loss: 0.00121946
Iteration 23/25 | Loss: 0.00121946
Iteration 24/25 | Loss: 0.00121946
Iteration 25/25 | Loss: 0.00121946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32403302
Iteration 2/25 | Loss: 0.00118347
Iteration 3/25 | Loss: 0.00118347
Iteration 4/25 | Loss: 0.00118347
Iteration 5/25 | Loss: 0.00118347
Iteration 6/25 | Loss: 0.00118347
Iteration 7/25 | Loss: 0.00118347
Iteration 8/25 | Loss: 0.00118347
Iteration 9/25 | Loss: 0.00118347
Iteration 10/25 | Loss: 0.00118347
Iteration 11/25 | Loss: 0.00118347
Iteration 12/25 | Loss: 0.00118347
Iteration 13/25 | Loss: 0.00118347
Iteration 14/25 | Loss: 0.00118347
Iteration 15/25 | Loss: 0.00118347
Iteration 16/25 | Loss: 0.00118347
Iteration 17/25 | Loss: 0.00118347
Iteration 18/25 | Loss: 0.00118347
Iteration 19/25 | Loss: 0.00118347
Iteration 20/25 | Loss: 0.00118347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011834679171442986, 0.0011834679171442986, 0.0011834679171442986, 0.0011834679171442986, 0.0011834679171442986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011834679171442986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118347
Iteration 2/1000 | Loss: 0.00003208
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001234
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001158
Iteration 23/1000 | Loss: 0.00001157
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001154
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001149
Iteration 35/1000 | Loss: 0.00001149
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001145
Iteration 41/1000 | Loss: 0.00001145
Iteration 42/1000 | Loss: 0.00001145
Iteration 43/1000 | Loss: 0.00001145
Iteration 44/1000 | Loss: 0.00001145
Iteration 45/1000 | Loss: 0.00001145
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001144
Iteration 48/1000 | Loss: 0.00001144
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001144
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001136
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001135
Iteration 75/1000 | Loss: 0.00001134
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001134
Iteration 78/1000 | Loss: 0.00001134
Iteration 79/1000 | Loss: 0.00001133
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001129
Iteration 90/1000 | Loss: 0.00001129
Iteration 91/1000 | Loss: 0.00001129
Iteration 92/1000 | Loss: 0.00001129
Iteration 93/1000 | Loss: 0.00001129
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001128
Iteration 97/1000 | Loss: 0.00001128
Iteration 98/1000 | Loss: 0.00001128
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001124
Iteration 106/1000 | Loss: 0.00001124
Iteration 107/1000 | Loss: 0.00001124
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001123
Iteration 110/1000 | Loss: 0.00001123
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001120
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001118
Iteration 135/1000 | Loss: 0.00001118
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001117
Iteration 138/1000 | Loss: 0.00001117
Iteration 139/1000 | Loss: 0.00001117
Iteration 140/1000 | Loss: 0.00001116
Iteration 141/1000 | Loss: 0.00001116
Iteration 142/1000 | Loss: 0.00001116
Iteration 143/1000 | Loss: 0.00001115
Iteration 144/1000 | Loss: 0.00001115
Iteration 145/1000 | Loss: 0.00001115
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001115
Iteration 148/1000 | Loss: 0.00001115
Iteration 149/1000 | Loss: 0.00001115
Iteration 150/1000 | Loss: 0.00001115
Iteration 151/1000 | Loss: 0.00001115
Iteration 152/1000 | Loss: 0.00001115
Iteration 153/1000 | Loss: 0.00001115
Iteration 154/1000 | Loss: 0.00001115
Iteration 155/1000 | Loss: 0.00001114
Iteration 156/1000 | Loss: 0.00001114
Iteration 157/1000 | Loss: 0.00001114
Iteration 158/1000 | Loss: 0.00001114
Iteration 159/1000 | Loss: 0.00001114
Iteration 160/1000 | Loss: 0.00001114
Iteration 161/1000 | Loss: 0.00001114
Iteration 162/1000 | Loss: 0.00001113
Iteration 163/1000 | Loss: 0.00001113
Iteration 164/1000 | Loss: 0.00001113
Iteration 165/1000 | Loss: 0.00001112
Iteration 166/1000 | Loss: 0.00001112
Iteration 167/1000 | Loss: 0.00001111
Iteration 168/1000 | Loss: 0.00001111
Iteration 169/1000 | Loss: 0.00001111
Iteration 170/1000 | Loss: 0.00001111
Iteration 171/1000 | Loss: 0.00001111
Iteration 172/1000 | Loss: 0.00001111
Iteration 173/1000 | Loss: 0.00001110
Iteration 174/1000 | Loss: 0.00001110
Iteration 175/1000 | Loss: 0.00001110
Iteration 176/1000 | Loss: 0.00001110
Iteration 177/1000 | Loss: 0.00001109
Iteration 178/1000 | Loss: 0.00001109
Iteration 179/1000 | Loss: 0.00001108
Iteration 180/1000 | Loss: 0.00001108
Iteration 181/1000 | Loss: 0.00001108
Iteration 182/1000 | Loss: 0.00001108
Iteration 183/1000 | Loss: 0.00001108
Iteration 184/1000 | Loss: 0.00001108
Iteration 185/1000 | Loss: 0.00001108
Iteration 186/1000 | Loss: 0.00001108
Iteration 187/1000 | Loss: 0.00001108
Iteration 188/1000 | Loss: 0.00001108
Iteration 189/1000 | Loss: 0.00001108
Iteration 190/1000 | Loss: 0.00001108
Iteration 191/1000 | Loss: 0.00001108
Iteration 192/1000 | Loss: 0.00001107
Iteration 193/1000 | Loss: 0.00001107
Iteration 194/1000 | Loss: 0.00001107
Iteration 195/1000 | Loss: 0.00001107
Iteration 196/1000 | Loss: 0.00001107
Iteration 197/1000 | Loss: 0.00001107
Iteration 198/1000 | Loss: 0.00001107
Iteration 199/1000 | Loss: 0.00001106
Iteration 200/1000 | Loss: 0.00001106
Iteration 201/1000 | Loss: 0.00001106
Iteration 202/1000 | Loss: 0.00001106
Iteration 203/1000 | Loss: 0.00001106
Iteration 204/1000 | Loss: 0.00001105
Iteration 205/1000 | Loss: 0.00001105
Iteration 206/1000 | Loss: 0.00001105
Iteration 207/1000 | Loss: 0.00001105
Iteration 208/1000 | Loss: 0.00001105
Iteration 209/1000 | Loss: 0.00001104
Iteration 210/1000 | Loss: 0.00001104
Iteration 211/1000 | Loss: 0.00001104
Iteration 212/1000 | Loss: 0.00001104
Iteration 213/1000 | Loss: 0.00001104
Iteration 214/1000 | Loss: 0.00001104
Iteration 215/1000 | Loss: 0.00001104
Iteration 216/1000 | Loss: 0.00001104
Iteration 217/1000 | Loss: 0.00001104
Iteration 218/1000 | Loss: 0.00001104
Iteration 219/1000 | Loss: 0.00001103
Iteration 220/1000 | Loss: 0.00001103
Iteration 221/1000 | Loss: 0.00001103
Iteration 222/1000 | Loss: 0.00001103
Iteration 223/1000 | Loss: 0.00001103
Iteration 224/1000 | Loss: 0.00001103
Iteration 225/1000 | Loss: 0.00001103
Iteration 226/1000 | Loss: 0.00001102
Iteration 227/1000 | Loss: 0.00001102
Iteration 228/1000 | Loss: 0.00001102
Iteration 229/1000 | Loss: 0.00001101
Iteration 230/1000 | Loss: 0.00001101
Iteration 231/1000 | Loss: 0.00001101
Iteration 232/1000 | Loss: 0.00001101
Iteration 233/1000 | Loss: 0.00001101
Iteration 234/1000 | Loss: 0.00001101
Iteration 235/1000 | Loss: 0.00001101
Iteration 236/1000 | Loss: 0.00001100
Iteration 237/1000 | Loss: 0.00001100
Iteration 238/1000 | Loss: 0.00001100
Iteration 239/1000 | Loss: 0.00001100
Iteration 240/1000 | Loss: 0.00001100
Iteration 241/1000 | Loss: 0.00001100
Iteration 242/1000 | Loss: 0.00001100
Iteration 243/1000 | Loss: 0.00001099
Iteration 244/1000 | Loss: 0.00001099
Iteration 245/1000 | Loss: 0.00001099
Iteration 246/1000 | Loss: 0.00001099
Iteration 247/1000 | Loss: 0.00001099
Iteration 248/1000 | Loss: 0.00001099
Iteration 249/1000 | Loss: 0.00001099
Iteration 250/1000 | Loss: 0.00001098
Iteration 251/1000 | Loss: 0.00001098
Iteration 252/1000 | Loss: 0.00001098
Iteration 253/1000 | Loss: 0.00001098
Iteration 254/1000 | Loss: 0.00001098
Iteration 255/1000 | Loss: 0.00001098
Iteration 256/1000 | Loss: 0.00001098
Iteration 257/1000 | Loss: 0.00001098
Iteration 258/1000 | Loss: 0.00001098
Iteration 259/1000 | Loss: 0.00001098
Iteration 260/1000 | Loss: 0.00001098
Iteration 261/1000 | Loss: 0.00001098
Iteration 262/1000 | Loss: 0.00001098
Iteration 263/1000 | Loss: 0.00001098
Iteration 264/1000 | Loss: 0.00001098
Iteration 265/1000 | Loss: 0.00001097
Iteration 266/1000 | Loss: 0.00001097
Iteration 267/1000 | Loss: 0.00001097
Iteration 268/1000 | Loss: 0.00001097
Iteration 269/1000 | Loss: 0.00001097
Iteration 270/1000 | Loss: 0.00001097
Iteration 271/1000 | Loss: 0.00001097
Iteration 272/1000 | Loss: 0.00001097
Iteration 273/1000 | Loss: 0.00001097
Iteration 274/1000 | Loss: 0.00001097
Iteration 275/1000 | Loss: 0.00001097
Iteration 276/1000 | Loss: 0.00001097
Iteration 277/1000 | Loss: 0.00001097
Iteration 278/1000 | Loss: 0.00001097
Iteration 279/1000 | Loss: 0.00001096
Iteration 280/1000 | Loss: 0.00001096
Iteration 281/1000 | Loss: 0.00001096
Iteration 282/1000 | Loss: 0.00001096
Iteration 283/1000 | Loss: 0.00001096
Iteration 284/1000 | Loss: 0.00001096
Iteration 285/1000 | Loss: 0.00001095
Iteration 286/1000 | Loss: 0.00001095
Iteration 287/1000 | Loss: 0.00001095
Iteration 288/1000 | Loss: 0.00001095
Iteration 289/1000 | Loss: 0.00001095
Iteration 290/1000 | Loss: 0.00001095
Iteration 291/1000 | Loss: 0.00001095
Iteration 292/1000 | Loss: 0.00001095
Iteration 293/1000 | Loss: 0.00001095
Iteration 294/1000 | Loss: 0.00001095
Iteration 295/1000 | Loss: 0.00001095
Iteration 296/1000 | Loss: 0.00001095
Iteration 297/1000 | Loss: 0.00001095
Iteration 298/1000 | Loss: 0.00001095
Iteration 299/1000 | Loss: 0.00001095
Iteration 300/1000 | Loss: 0.00001095
Iteration 301/1000 | Loss: 0.00001095
Iteration 302/1000 | Loss: 0.00001095
Iteration 303/1000 | Loss: 0.00001095
Iteration 304/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [1.0945713256660383e-05, 1.0945713256660383e-05, 1.0945713256660383e-05, 1.0945713256660383e-05, 1.0945713256660383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0945713256660383e-05

Optimization complete. Final v2v error: 2.8419158458709717 mm

Highest mean error: 3.4826834201812744 mm for frame 75

Lowest mean error: 2.6287167072296143 mm for frame 148

Saving results

Total time: 46.79590702056885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810766
Iteration 2/25 | Loss: 0.00130500
Iteration 3/25 | Loss: 0.00119858
Iteration 4/25 | Loss: 0.00118724
Iteration 5/25 | Loss: 0.00118536
Iteration 6/25 | Loss: 0.00118507
Iteration 7/25 | Loss: 0.00118507
Iteration 8/25 | Loss: 0.00118507
Iteration 9/25 | Loss: 0.00118507
Iteration 10/25 | Loss: 0.00118507
Iteration 11/25 | Loss: 0.00118507
Iteration 12/25 | Loss: 0.00118507
Iteration 13/25 | Loss: 0.00118507
Iteration 14/25 | Loss: 0.00118507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011850736336782575, 0.0011850736336782575, 0.0011850736336782575, 0.0011850736336782575, 0.0011850736336782575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011850736336782575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32197988
Iteration 2/25 | Loss: 0.00102048
Iteration 3/25 | Loss: 0.00102047
Iteration 4/25 | Loss: 0.00102047
Iteration 5/25 | Loss: 0.00102047
Iteration 6/25 | Loss: 0.00102047
Iteration 7/25 | Loss: 0.00102046
Iteration 8/25 | Loss: 0.00102046
Iteration 9/25 | Loss: 0.00102046
Iteration 10/25 | Loss: 0.00102046
Iteration 11/25 | Loss: 0.00102046
Iteration 12/25 | Loss: 0.00102046
Iteration 13/25 | Loss: 0.00102046
Iteration 14/25 | Loss: 0.00102046
Iteration 15/25 | Loss: 0.00102046
Iteration 16/25 | Loss: 0.00102046
Iteration 17/25 | Loss: 0.00102046
Iteration 18/25 | Loss: 0.00102046
Iteration 19/25 | Loss: 0.00102046
Iteration 20/25 | Loss: 0.00102046
Iteration 21/25 | Loss: 0.00102046
Iteration 22/25 | Loss: 0.00102046
Iteration 23/25 | Loss: 0.00102046
Iteration 24/25 | Loss: 0.00102046
Iteration 25/25 | Loss: 0.00102046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010204627178609371, 0.0010204627178609371, 0.0010204627178609371, 0.0010204627178609371, 0.0010204627178609371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010204627178609371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102046
Iteration 2/1000 | Loss: 0.00001875
Iteration 3/1000 | Loss: 0.00001354
Iteration 4/1000 | Loss: 0.00001196
Iteration 5/1000 | Loss: 0.00001078
Iteration 6/1000 | Loss: 0.00001012
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000940
Iteration 9/1000 | Loss: 0.00000923
Iteration 10/1000 | Loss: 0.00000908
Iteration 11/1000 | Loss: 0.00000902
Iteration 12/1000 | Loss: 0.00000899
Iteration 13/1000 | Loss: 0.00000898
Iteration 14/1000 | Loss: 0.00000896
Iteration 15/1000 | Loss: 0.00000895
Iteration 16/1000 | Loss: 0.00000895
Iteration 17/1000 | Loss: 0.00000892
Iteration 18/1000 | Loss: 0.00000890
Iteration 19/1000 | Loss: 0.00000889
Iteration 20/1000 | Loss: 0.00000888
Iteration 21/1000 | Loss: 0.00000888
Iteration 22/1000 | Loss: 0.00000884
Iteration 23/1000 | Loss: 0.00000882
Iteration 24/1000 | Loss: 0.00000881
Iteration 25/1000 | Loss: 0.00000881
Iteration 26/1000 | Loss: 0.00000880
Iteration 27/1000 | Loss: 0.00000879
Iteration 28/1000 | Loss: 0.00000879
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000875
Iteration 32/1000 | Loss: 0.00000873
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000867
Iteration 35/1000 | Loss: 0.00000866
Iteration 36/1000 | Loss: 0.00000866
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000864
Iteration 40/1000 | Loss: 0.00000864
Iteration 41/1000 | Loss: 0.00000864
Iteration 42/1000 | Loss: 0.00000864
Iteration 43/1000 | Loss: 0.00000864
Iteration 44/1000 | Loss: 0.00000864
Iteration 45/1000 | Loss: 0.00000864
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000862
Iteration 48/1000 | Loss: 0.00000862
Iteration 49/1000 | Loss: 0.00000861
Iteration 50/1000 | Loss: 0.00000861
Iteration 51/1000 | Loss: 0.00000861
Iteration 52/1000 | Loss: 0.00000861
Iteration 53/1000 | Loss: 0.00000861
Iteration 54/1000 | Loss: 0.00000861
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00000861
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000859
Iteration 66/1000 | Loss: 0.00000859
Iteration 67/1000 | Loss: 0.00000858
Iteration 68/1000 | Loss: 0.00000858
Iteration 69/1000 | Loss: 0.00000858
Iteration 70/1000 | Loss: 0.00000858
Iteration 71/1000 | Loss: 0.00000858
Iteration 72/1000 | Loss: 0.00000858
Iteration 73/1000 | Loss: 0.00000858
Iteration 74/1000 | Loss: 0.00000858
Iteration 75/1000 | Loss: 0.00000858
Iteration 76/1000 | Loss: 0.00000858
Iteration 77/1000 | Loss: 0.00000858
Iteration 78/1000 | Loss: 0.00000858
Iteration 79/1000 | Loss: 0.00000858
Iteration 80/1000 | Loss: 0.00000858
Iteration 81/1000 | Loss: 0.00000857
Iteration 82/1000 | Loss: 0.00000857
Iteration 83/1000 | Loss: 0.00000857
Iteration 84/1000 | Loss: 0.00000857
Iteration 85/1000 | Loss: 0.00000856
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000855
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000854
Iteration 93/1000 | Loss: 0.00000854
Iteration 94/1000 | Loss: 0.00000854
Iteration 95/1000 | Loss: 0.00000853
Iteration 96/1000 | Loss: 0.00000853
Iteration 97/1000 | Loss: 0.00000853
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000852
Iteration 102/1000 | Loss: 0.00000852
Iteration 103/1000 | Loss: 0.00000851
Iteration 104/1000 | Loss: 0.00000851
Iteration 105/1000 | Loss: 0.00000851
Iteration 106/1000 | Loss: 0.00000851
Iteration 107/1000 | Loss: 0.00000851
Iteration 108/1000 | Loss: 0.00000851
Iteration 109/1000 | Loss: 0.00000851
Iteration 110/1000 | Loss: 0.00000850
Iteration 111/1000 | Loss: 0.00000850
Iteration 112/1000 | Loss: 0.00000850
Iteration 113/1000 | Loss: 0.00000850
Iteration 114/1000 | Loss: 0.00000849
Iteration 115/1000 | Loss: 0.00000849
Iteration 116/1000 | Loss: 0.00000848
Iteration 117/1000 | Loss: 0.00000848
Iteration 118/1000 | Loss: 0.00000847
Iteration 119/1000 | Loss: 0.00000847
Iteration 120/1000 | Loss: 0.00000847
Iteration 121/1000 | Loss: 0.00000847
Iteration 122/1000 | Loss: 0.00000847
Iteration 123/1000 | Loss: 0.00000846
Iteration 124/1000 | Loss: 0.00000846
Iteration 125/1000 | Loss: 0.00000846
Iteration 126/1000 | Loss: 0.00000846
Iteration 127/1000 | Loss: 0.00000845
Iteration 128/1000 | Loss: 0.00000845
Iteration 129/1000 | Loss: 0.00000844
Iteration 130/1000 | Loss: 0.00000844
Iteration 131/1000 | Loss: 0.00000844
Iteration 132/1000 | Loss: 0.00000843
Iteration 133/1000 | Loss: 0.00000843
Iteration 134/1000 | Loss: 0.00000843
Iteration 135/1000 | Loss: 0.00000843
Iteration 136/1000 | Loss: 0.00000843
Iteration 137/1000 | Loss: 0.00000843
Iteration 138/1000 | Loss: 0.00000842
Iteration 139/1000 | Loss: 0.00000842
Iteration 140/1000 | Loss: 0.00000842
Iteration 141/1000 | Loss: 0.00000842
Iteration 142/1000 | Loss: 0.00000842
Iteration 143/1000 | Loss: 0.00000842
Iteration 144/1000 | Loss: 0.00000842
Iteration 145/1000 | Loss: 0.00000842
Iteration 146/1000 | Loss: 0.00000842
Iteration 147/1000 | Loss: 0.00000841
Iteration 148/1000 | Loss: 0.00000841
Iteration 149/1000 | Loss: 0.00000840
Iteration 150/1000 | Loss: 0.00000840
Iteration 151/1000 | Loss: 0.00000840
Iteration 152/1000 | Loss: 0.00000840
Iteration 153/1000 | Loss: 0.00000840
Iteration 154/1000 | Loss: 0.00000840
Iteration 155/1000 | Loss: 0.00000840
Iteration 156/1000 | Loss: 0.00000840
Iteration 157/1000 | Loss: 0.00000840
Iteration 158/1000 | Loss: 0.00000840
Iteration 159/1000 | Loss: 0.00000840
Iteration 160/1000 | Loss: 0.00000840
Iteration 161/1000 | Loss: 0.00000839
Iteration 162/1000 | Loss: 0.00000839
Iteration 163/1000 | Loss: 0.00000839
Iteration 164/1000 | Loss: 0.00000839
Iteration 165/1000 | Loss: 0.00000839
Iteration 166/1000 | Loss: 0.00000839
Iteration 167/1000 | Loss: 0.00000839
Iteration 168/1000 | Loss: 0.00000838
Iteration 169/1000 | Loss: 0.00000838
Iteration 170/1000 | Loss: 0.00000838
Iteration 171/1000 | Loss: 0.00000838
Iteration 172/1000 | Loss: 0.00000838
Iteration 173/1000 | Loss: 0.00000838
Iteration 174/1000 | Loss: 0.00000838
Iteration 175/1000 | Loss: 0.00000837
Iteration 176/1000 | Loss: 0.00000837
Iteration 177/1000 | Loss: 0.00000837
Iteration 178/1000 | Loss: 0.00000837
Iteration 179/1000 | Loss: 0.00000837
Iteration 180/1000 | Loss: 0.00000837
Iteration 181/1000 | Loss: 0.00000837
Iteration 182/1000 | Loss: 0.00000837
Iteration 183/1000 | Loss: 0.00000837
Iteration 184/1000 | Loss: 0.00000837
Iteration 185/1000 | Loss: 0.00000837
Iteration 186/1000 | Loss: 0.00000837
Iteration 187/1000 | Loss: 0.00000837
Iteration 188/1000 | Loss: 0.00000837
Iteration 189/1000 | Loss: 0.00000837
Iteration 190/1000 | Loss: 0.00000837
Iteration 191/1000 | Loss: 0.00000837
Iteration 192/1000 | Loss: 0.00000837
Iteration 193/1000 | Loss: 0.00000837
Iteration 194/1000 | Loss: 0.00000837
Iteration 195/1000 | Loss: 0.00000837
Iteration 196/1000 | Loss: 0.00000837
Iteration 197/1000 | Loss: 0.00000837
Iteration 198/1000 | Loss: 0.00000837
Iteration 199/1000 | Loss: 0.00000837
Iteration 200/1000 | Loss: 0.00000837
Iteration 201/1000 | Loss: 0.00000837
Iteration 202/1000 | Loss: 0.00000837
Iteration 203/1000 | Loss: 0.00000837
Iteration 204/1000 | Loss: 0.00000837
Iteration 205/1000 | Loss: 0.00000837
Iteration 206/1000 | Loss: 0.00000837
Iteration 207/1000 | Loss: 0.00000837
Iteration 208/1000 | Loss: 0.00000837
Iteration 209/1000 | Loss: 0.00000837
Iteration 210/1000 | Loss: 0.00000837
Iteration 211/1000 | Loss: 0.00000837
Iteration 212/1000 | Loss: 0.00000837
Iteration 213/1000 | Loss: 0.00000837
Iteration 214/1000 | Loss: 0.00000837
Iteration 215/1000 | Loss: 0.00000837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [8.371303010790143e-06, 8.371303010790143e-06, 8.371303010790143e-06, 8.371303010790143e-06, 8.371303010790143e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.371303010790143e-06

Optimization complete. Final v2v error: 2.504687786102295 mm

Highest mean error: 2.6552157402038574 mm for frame 3

Lowest mean error: 2.4001684188842773 mm for frame 100

Saving results

Total time: 37.970959186553955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997926
Iteration 2/25 | Loss: 0.00192503
Iteration 3/25 | Loss: 0.00141935
Iteration 4/25 | Loss: 0.00135759
Iteration 5/25 | Loss: 0.00131913
Iteration 6/25 | Loss: 0.00131646
Iteration 7/25 | Loss: 0.00130415
Iteration 8/25 | Loss: 0.00129869
Iteration 9/25 | Loss: 0.00129709
Iteration 10/25 | Loss: 0.00128889
Iteration 11/25 | Loss: 0.00128828
Iteration 12/25 | Loss: 0.00128103
Iteration 13/25 | Loss: 0.00128449
Iteration 14/25 | Loss: 0.00128787
Iteration 15/25 | Loss: 0.00128859
Iteration 16/25 | Loss: 0.00126610
Iteration 17/25 | Loss: 0.00125564
Iteration 18/25 | Loss: 0.00124884
Iteration 19/25 | Loss: 0.00124770
Iteration 20/25 | Loss: 0.00124769
Iteration 21/25 | Loss: 0.00124419
Iteration 22/25 | Loss: 0.00124137
Iteration 23/25 | Loss: 0.00124115
Iteration 24/25 | Loss: 0.00124148
Iteration 25/25 | Loss: 0.00124013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27683818
Iteration 2/25 | Loss: 0.00160605
Iteration 3/25 | Loss: 0.00160604
Iteration 4/25 | Loss: 0.00160604
Iteration 5/25 | Loss: 0.00160604
Iteration 6/25 | Loss: 0.00160604
Iteration 7/25 | Loss: 0.00160604
Iteration 8/25 | Loss: 0.00160604
Iteration 9/25 | Loss: 0.00160604
Iteration 10/25 | Loss: 0.00160604
Iteration 11/25 | Loss: 0.00160604
Iteration 12/25 | Loss: 0.00160604
Iteration 13/25 | Loss: 0.00160604
Iteration 14/25 | Loss: 0.00160604
Iteration 15/25 | Loss: 0.00160604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016060417983680964, 0.0016060417983680964, 0.0016060417983680964, 0.0016060417983680964, 0.0016060417983680964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016060417983680964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160604
Iteration 2/1000 | Loss: 0.00006355
Iteration 3/1000 | Loss: 0.00015325
Iteration 4/1000 | Loss: 0.00031248
Iteration 5/1000 | Loss: 0.00006853
Iteration 6/1000 | Loss: 0.00003160
Iteration 7/1000 | Loss: 0.00029465
Iteration 8/1000 | Loss: 0.00019034
Iteration 9/1000 | Loss: 0.00032594
Iteration 10/1000 | Loss: 0.00028845
Iteration 11/1000 | Loss: 0.00011566
Iteration 12/1000 | Loss: 0.00028811
Iteration 13/1000 | Loss: 0.00008227
Iteration 14/1000 | Loss: 0.00004012
Iteration 15/1000 | Loss: 0.00005498
Iteration 16/1000 | Loss: 0.00002999
Iteration 17/1000 | Loss: 0.00009248
Iteration 18/1000 | Loss: 0.00057874
Iteration 19/1000 | Loss: 0.00005673
Iteration 20/1000 | Loss: 0.00002920
Iteration 21/1000 | Loss: 0.00003068
Iteration 22/1000 | Loss: 0.00002957
Iteration 23/1000 | Loss: 0.00012683
Iteration 24/1000 | Loss: 0.00008195
Iteration 25/1000 | Loss: 0.00004026
Iteration 26/1000 | Loss: 0.00002736
Iteration 27/1000 | Loss: 0.00002951
Iteration 28/1000 | Loss: 0.00003114
Iteration 29/1000 | Loss: 0.00002939
Iteration 30/1000 | Loss: 0.00003666
Iteration 31/1000 | Loss: 0.00007462
Iteration 32/1000 | Loss: 0.00002384
Iteration 33/1000 | Loss: 0.00002466
Iteration 34/1000 | Loss: 0.00005609
Iteration 35/1000 | Loss: 0.00002415
Iteration 36/1000 | Loss: 0.00017490
Iteration 37/1000 | Loss: 0.00002800
Iteration 38/1000 | Loss: 0.00010370
Iteration 39/1000 | Loss: 0.00004071
Iteration 40/1000 | Loss: 0.00003193
Iteration 41/1000 | Loss: 0.00002423
Iteration 42/1000 | Loss: 0.00012179
Iteration 43/1000 | Loss: 0.00002106
Iteration 44/1000 | Loss: 0.00007126
Iteration 45/1000 | Loss: 0.00003079
Iteration 46/1000 | Loss: 0.00010084
Iteration 47/1000 | Loss: 0.00006575
Iteration 48/1000 | Loss: 0.00002117
Iteration 49/1000 | Loss: 0.00002296
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00007553
Iteration 52/1000 | Loss: 0.00017724
Iteration 53/1000 | Loss: 0.00005900
Iteration 54/1000 | Loss: 0.00003609
Iteration 55/1000 | Loss: 0.00003332
Iteration 56/1000 | Loss: 0.00003335
Iteration 57/1000 | Loss: 0.00002734
Iteration 58/1000 | Loss: 0.00010308
Iteration 59/1000 | Loss: 0.00006823
Iteration 60/1000 | Loss: 0.00002925
Iteration 61/1000 | Loss: 0.00009121
Iteration 62/1000 | Loss: 0.00004815
Iteration 63/1000 | Loss: 0.00002987
Iteration 64/1000 | Loss: 0.00006467
Iteration 65/1000 | Loss: 0.00002906
Iteration 66/1000 | Loss: 0.00005827
Iteration 67/1000 | Loss: 0.00002958
Iteration 68/1000 | Loss: 0.00003706
Iteration 69/1000 | Loss: 0.00002627
Iteration 70/1000 | Loss: 0.00002803
Iteration 71/1000 | Loss: 0.00005425
Iteration 72/1000 | Loss: 0.00004345
Iteration 73/1000 | Loss: 0.00003462
Iteration 74/1000 | Loss: 0.00003366
Iteration 75/1000 | Loss: 0.00004932
Iteration 76/1000 | Loss: 0.00004001
Iteration 77/1000 | Loss: 0.00004959
Iteration 78/1000 | Loss: 0.00003333
Iteration 79/1000 | Loss: 0.00003063
Iteration 80/1000 | Loss: 0.00002638
Iteration 81/1000 | Loss: 0.00002757
Iteration 82/1000 | Loss: 0.00006894
Iteration 83/1000 | Loss: 0.00008957
Iteration 84/1000 | Loss: 0.00004122
Iteration 85/1000 | Loss: 0.00002471
Iteration 86/1000 | Loss: 0.00009286
Iteration 87/1000 | Loss: 0.00004470
Iteration 88/1000 | Loss: 0.00002653
Iteration 89/1000 | Loss: 0.00002318
Iteration 90/1000 | Loss: 0.00027011
Iteration 91/1000 | Loss: 0.00002805
Iteration 92/1000 | Loss: 0.00002042
Iteration 93/1000 | Loss: 0.00001894
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001646
Iteration 96/1000 | Loss: 0.00022551
Iteration 97/1000 | Loss: 0.00001699
Iteration 98/1000 | Loss: 0.00001609
Iteration 99/1000 | Loss: 0.00001592
Iteration 100/1000 | Loss: 0.00001589
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001583
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001582
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001580
Iteration 124/1000 | Loss: 0.00001580
Iteration 125/1000 | Loss: 0.00001580
Iteration 126/1000 | Loss: 0.00001580
Iteration 127/1000 | Loss: 0.00001579
Iteration 128/1000 | Loss: 0.00001579
Iteration 129/1000 | Loss: 0.00001579
Iteration 130/1000 | Loss: 0.00001578
Iteration 131/1000 | Loss: 0.00001578
Iteration 132/1000 | Loss: 0.00001578
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001577
Iteration 135/1000 | Loss: 0.00001577
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Iteration 150/1000 | Loss: 0.00001573
Iteration 151/1000 | Loss: 0.00001573
Iteration 152/1000 | Loss: 0.00001573
Iteration 153/1000 | Loss: 0.00001573
Iteration 154/1000 | Loss: 0.00001573
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001573
Iteration 159/1000 | Loss: 0.00001573
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001573
Iteration 165/1000 | Loss: 0.00001573
Iteration 166/1000 | Loss: 0.00001573
Iteration 167/1000 | Loss: 0.00001573
Iteration 168/1000 | Loss: 0.00001573
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00001573
Iteration 171/1000 | Loss: 0.00001573
Iteration 172/1000 | Loss: 0.00001573
Iteration 173/1000 | Loss: 0.00001573
Iteration 174/1000 | Loss: 0.00001573
Iteration 175/1000 | Loss: 0.00001573
Iteration 176/1000 | Loss: 0.00001573
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001573
Iteration 182/1000 | Loss: 0.00001573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.572971996210981e-05, 1.572971996210981e-05, 1.572971996210981e-05, 1.572971996210981e-05, 1.572971996210981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.572971996210981e-05

Optimization complete. Final v2v error: 3.3578407764434814 mm

Highest mean error: 5.415860176086426 mm for frame 48

Lowest mean error: 2.9362435340881348 mm for frame 2

Saving results

Total time: 211.11433005332947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071775
Iteration 2/25 | Loss: 0.01071775
Iteration 3/25 | Loss: 0.00338192
Iteration 4/25 | Loss: 0.00182706
Iteration 5/25 | Loss: 0.00172871
Iteration 6/25 | Loss: 0.00192870
Iteration 7/25 | Loss: 0.00166957
Iteration 8/25 | Loss: 0.00158937
Iteration 9/25 | Loss: 0.00150446
Iteration 10/25 | Loss: 0.00147740
Iteration 11/25 | Loss: 0.00148757
Iteration 12/25 | Loss: 0.00152643
Iteration 13/25 | Loss: 0.00142114
Iteration 14/25 | Loss: 0.00137409
Iteration 15/25 | Loss: 0.00133442
Iteration 16/25 | Loss: 0.00132305
Iteration 17/25 | Loss: 0.00131482
Iteration 18/25 | Loss: 0.00131230
Iteration 19/25 | Loss: 0.00131134
Iteration 20/25 | Loss: 0.00131143
Iteration 21/25 | Loss: 0.00131229
Iteration 22/25 | Loss: 0.00131072
Iteration 23/25 | Loss: 0.00130965
Iteration 24/25 | Loss: 0.00130992
Iteration 25/25 | Loss: 0.00130992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53140557
Iteration 2/25 | Loss: 0.00099316
Iteration 3/25 | Loss: 0.00088086
Iteration 4/25 | Loss: 0.00088086
Iteration 5/25 | Loss: 0.00088086
Iteration 6/25 | Loss: 0.00088086
Iteration 7/25 | Loss: 0.00088086
Iteration 8/25 | Loss: 0.00088086
Iteration 9/25 | Loss: 0.00088086
Iteration 10/25 | Loss: 0.00088086
Iteration 11/25 | Loss: 0.00088086
Iteration 12/25 | Loss: 0.00088086
Iteration 13/25 | Loss: 0.00088086
Iteration 14/25 | Loss: 0.00088086
Iteration 15/25 | Loss: 0.00088086
Iteration 16/25 | Loss: 0.00088086
Iteration 17/25 | Loss: 0.00088086
Iteration 18/25 | Loss: 0.00088086
Iteration 19/25 | Loss: 0.00088086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008808608399704099, 0.0008808608399704099, 0.0008808608399704099, 0.0008808608399704099, 0.0008808608399704099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008808608399704099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088086
Iteration 2/1000 | Loss: 0.00020799
Iteration 3/1000 | Loss: 0.00089209
Iteration 4/1000 | Loss: 0.00018280
Iteration 5/1000 | Loss: 0.00072513
Iteration 6/1000 | Loss: 0.00008692
Iteration 7/1000 | Loss: 0.00008078
Iteration 8/1000 | Loss: 0.00005633
Iteration 9/1000 | Loss: 0.00008684
Iteration 10/1000 | Loss: 0.00001992
Iteration 11/1000 | Loss: 0.00002821
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001954
Iteration 15/1000 | Loss: 0.00004979
Iteration 16/1000 | Loss: 0.00002017
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00002005
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00002628
Iteration 22/1000 | Loss: 0.00002455
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001727
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001645
Iteration 29/1000 | Loss: 0.00001645
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001645
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001829
Iteration 53/1000 | Loss: 0.00002028
Iteration 54/1000 | Loss: 0.00003465
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001641
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00003086
Iteration 63/1000 | Loss: 0.00002298
Iteration 64/1000 | Loss: 0.00001684
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001714
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001614
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001614
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001614
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6133342796820216e-05, 1.6133342796820216e-05, 1.6133342796820216e-05, 1.6133342796820216e-05, 1.6133342796820216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6133342796820216e-05

Optimization complete. Final v2v error: 3.452504873275757 mm

Highest mean error: 3.653914451599121 mm for frame 104

Lowest mean error: 3.3127222061157227 mm for frame 72

Saving results

Total time: 105.07703828811646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01148928
Iteration 2/25 | Loss: 0.01148928
Iteration 3/25 | Loss: 0.01148928
Iteration 4/25 | Loss: 0.01148928
Iteration 5/25 | Loss: 0.01148928
Iteration 6/25 | Loss: 0.01148928
Iteration 7/25 | Loss: 0.01148928
Iteration 8/25 | Loss: 0.01148928
Iteration 9/25 | Loss: 0.01148928
Iteration 10/25 | Loss: 0.01148927
Iteration 11/25 | Loss: 0.01148927
Iteration 12/25 | Loss: 0.01148927
Iteration 13/25 | Loss: 0.01148927
Iteration 14/25 | Loss: 0.01148927
Iteration 15/25 | Loss: 0.01148927
Iteration 16/25 | Loss: 0.01148927
Iteration 17/25 | Loss: 0.01148927
Iteration 18/25 | Loss: 0.01148927
Iteration 19/25 | Loss: 0.01148927
Iteration 20/25 | Loss: 0.01148927
Iteration 21/25 | Loss: 0.01148926
Iteration 22/25 | Loss: 0.01148926
Iteration 23/25 | Loss: 0.01148926
Iteration 24/25 | Loss: 0.01148926
Iteration 25/25 | Loss: 0.01148926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93531454
Iteration 2/25 | Loss: 0.05062830
Iteration 3/25 | Loss: 0.05062122
Iteration 4/25 | Loss: 0.05062121
Iteration 5/25 | Loss: 0.05062121
Iteration 6/25 | Loss: 0.05062121
Iteration 7/25 | Loss: 0.05062121
Iteration 8/25 | Loss: 0.05062121
Iteration 9/25 | Loss: 0.05062121
Iteration 10/25 | Loss: 0.05062121
Iteration 11/25 | Loss: 0.05062121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.05062120780348778, 0.05062120780348778, 0.05062120780348778, 0.05062120780348778, 0.05062120780348778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05062120780348778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05062121
Iteration 2/1000 | Loss: 0.00619482
Iteration 3/1000 | Loss: 0.00300552
Iteration 4/1000 | Loss: 0.00062398
Iteration 5/1000 | Loss: 0.00077036
Iteration 6/1000 | Loss: 0.00020478
Iteration 7/1000 | Loss: 0.00026660
Iteration 8/1000 | Loss: 0.00006331
Iteration 9/1000 | Loss: 0.00027253
Iteration 10/1000 | Loss: 0.00031740
Iteration 11/1000 | Loss: 0.00014177
Iteration 12/1000 | Loss: 0.00022073
Iteration 13/1000 | Loss: 0.00007959
Iteration 14/1000 | Loss: 0.00029119
Iteration 15/1000 | Loss: 0.00048063
Iteration 16/1000 | Loss: 0.00003982
Iteration 17/1000 | Loss: 0.00024498
Iteration 18/1000 | Loss: 0.00015484
Iteration 19/1000 | Loss: 0.00084036
Iteration 20/1000 | Loss: 0.00026703
Iteration 21/1000 | Loss: 0.00018834
Iteration 22/1000 | Loss: 0.00004922
Iteration 23/1000 | Loss: 0.00032671
Iteration 24/1000 | Loss: 0.00003099
Iteration 25/1000 | Loss: 0.00010631
Iteration 26/1000 | Loss: 0.00009834
Iteration 27/1000 | Loss: 0.00006551
Iteration 28/1000 | Loss: 0.00034355
Iteration 29/1000 | Loss: 0.00011691
Iteration 30/1000 | Loss: 0.00002540
Iteration 31/1000 | Loss: 0.00002449
Iteration 32/1000 | Loss: 0.00016509
Iteration 33/1000 | Loss: 0.00002380
Iteration 34/1000 | Loss: 0.00013875
Iteration 35/1000 | Loss: 0.00054238
Iteration 36/1000 | Loss: 0.00042217
Iteration 37/1000 | Loss: 0.00010297
Iteration 38/1000 | Loss: 0.00009766
Iteration 39/1000 | Loss: 0.00007236
Iteration 40/1000 | Loss: 0.00002273
Iteration 41/1000 | Loss: 0.00002235
Iteration 42/1000 | Loss: 0.00023852
Iteration 43/1000 | Loss: 0.00002254
Iteration 44/1000 | Loss: 0.00011323
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00015565
Iteration 47/1000 | Loss: 0.00008189
Iteration 48/1000 | Loss: 0.00028679
Iteration 49/1000 | Loss: 0.00010501
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00006870
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00017712
Iteration 54/1000 | Loss: 0.00004653
Iteration 55/1000 | Loss: 0.00002359
Iteration 56/1000 | Loss: 0.00005399
Iteration 57/1000 | Loss: 0.00006918
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00003916
Iteration 60/1000 | Loss: 0.00002106
Iteration 61/1000 | Loss: 0.00007977
Iteration 62/1000 | Loss: 0.00010377
Iteration 63/1000 | Loss: 0.00046832
Iteration 64/1000 | Loss: 0.00024669
Iteration 65/1000 | Loss: 0.00002336
Iteration 66/1000 | Loss: 0.00026555
Iteration 67/1000 | Loss: 0.00053099
Iteration 68/1000 | Loss: 0.00002163
Iteration 69/1000 | Loss: 0.00002088
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002085
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002084
Iteration 78/1000 | Loss: 0.00002084
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00026123
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002082
Iteration 83/1000 | Loss: 0.00002080
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002076
Iteration 87/1000 | Loss: 0.00002076
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00002075
Iteration 90/1000 | Loss: 0.00002075
Iteration 91/1000 | Loss: 0.00002075
Iteration 92/1000 | Loss: 0.00002075
Iteration 93/1000 | Loss: 0.00002075
Iteration 94/1000 | Loss: 0.00002075
Iteration 95/1000 | Loss: 0.00002074
Iteration 96/1000 | Loss: 0.00002074
Iteration 97/1000 | Loss: 0.00009880
Iteration 98/1000 | Loss: 0.00013691
Iteration 99/1000 | Loss: 0.00002605
Iteration 100/1000 | Loss: 0.00007748
Iteration 101/1000 | Loss: 0.00002106
Iteration 102/1000 | Loss: 0.00002083
Iteration 103/1000 | Loss: 0.00002079
Iteration 104/1000 | Loss: 0.00002078
Iteration 105/1000 | Loss: 0.00020385
Iteration 106/1000 | Loss: 0.00008588
Iteration 107/1000 | Loss: 0.00051764
Iteration 108/1000 | Loss: 0.00002952
Iteration 109/1000 | Loss: 0.00002121
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00013051
Iteration 112/1000 | Loss: 0.00003787
Iteration 113/1000 | Loss: 0.00004901
Iteration 114/1000 | Loss: 0.00002087
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00003573
Iteration 117/1000 | Loss: 0.00002080
Iteration 118/1000 | Loss: 0.00002363
Iteration 119/1000 | Loss: 0.00002074
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002073
Iteration 122/1000 | Loss: 0.00002073
Iteration 123/1000 | Loss: 0.00002073
Iteration 124/1000 | Loss: 0.00002073
Iteration 125/1000 | Loss: 0.00002073
Iteration 126/1000 | Loss: 0.00002072
Iteration 127/1000 | Loss: 0.00002072
Iteration 128/1000 | Loss: 0.00002072
Iteration 129/1000 | Loss: 0.00002072
Iteration 130/1000 | Loss: 0.00002071
Iteration 131/1000 | Loss: 0.00002071
Iteration 132/1000 | Loss: 0.00002071
Iteration 133/1000 | Loss: 0.00002071
Iteration 134/1000 | Loss: 0.00002070
Iteration 135/1000 | Loss: 0.00002070
Iteration 136/1000 | Loss: 0.00002070
Iteration 137/1000 | Loss: 0.00002070
Iteration 138/1000 | Loss: 0.00002070
Iteration 139/1000 | Loss: 0.00002069
Iteration 140/1000 | Loss: 0.00002069
Iteration 141/1000 | Loss: 0.00002069
Iteration 142/1000 | Loss: 0.00002069
Iteration 143/1000 | Loss: 0.00002069
Iteration 144/1000 | Loss: 0.00002068
Iteration 145/1000 | Loss: 0.00002068
Iteration 146/1000 | Loss: 0.00006021
Iteration 147/1000 | Loss: 0.00002096
Iteration 148/1000 | Loss: 0.00002070
Iteration 149/1000 | Loss: 0.00009637
Iteration 150/1000 | Loss: 0.00003105
Iteration 151/1000 | Loss: 0.00002069
Iteration 152/1000 | Loss: 0.00002069
Iteration 153/1000 | Loss: 0.00002068
Iteration 154/1000 | Loss: 0.00002068
Iteration 155/1000 | Loss: 0.00002067
Iteration 156/1000 | Loss: 0.00002067
Iteration 157/1000 | Loss: 0.00002067
Iteration 158/1000 | Loss: 0.00002066
Iteration 159/1000 | Loss: 0.00002066
Iteration 160/1000 | Loss: 0.00002066
Iteration 161/1000 | Loss: 0.00002066
Iteration 162/1000 | Loss: 0.00002065
Iteration 163/1000 | Loss: 0.00002065
Iteration 164/1000 | Loss: 0.00002065
Iteration 165/1000 | Loss: 0.00002065
Iteration 166/1000 | Loss: 0.00002064
Iteration 167/1000 | Loss: 0.00002064
Iteration 168/1000 | Loss: 0.00002064
Iteration 169/1000 | Loss: 0.00002064
Iteration 170/1000 | Loss: 0.00002064
Iteration 171/1000 | Loss: 0.00002064
Iteration 172/1000 | Loss: 0.00002064
Iteration 173/1000 | Loss: 0.00002064
Iteration 174/1000 | Loss: 0.00002064
Iteration 175/1000 | Loss: 0.00002064
Iteration 176/1000 | Loss: 0.00002063
Iteration 177/1000 | Loss: 0.00002063
Iteration 178/1000 | Loss: 0.00002063
Iteration 179/1000 | Loss: 0.00002063
Iteration 180/1000 | Loss: 0.00002063
Iteration 181/1000 | Loss: 0.00002063
Iteration 182/1000 | Loss: 0.00002062
Iteration 183/1000 | Loss: 0.00002062
Iteration 184/1000 | Loss: 0.00002062
Iteration 185/1000 | Loss: 0.00002062
Iteration 186/1000 | Loss: 0.00002062
Iteration 187/1000 | Loss: 0.00002062
Iteration 188/1000 | Loss: 0.00002062
Iteration 189/1000 | Loss: 0.00002062
Iteration 190/1000 | Loss: 0.00002062
Iteration 191/1000 | Loss: 0.00002062
Iteration 192/1000 | Loss: 0.00002062
Iteration 193/1000 | Loss: 0.00002062
Iteration 194/1000 | Loss: 0.00002062
Iteration 195/1000 | Loss: 0.00002061
Iteration 196/1000 | Loss: 0.00002061
Iteration 197/1000 | Loss: 0.00002061
Iteration 198/1000 | Loss: 0.00002061
Iteration 199/1000 | Loss: 0.00002061
Iteration 200/1000 | Loss: 0.00002061
Iteration 201/1000 | Loss: 0.00002061
Iteration 202/1000 | Loss: 0.00002061
Iteration 203/1000 | Loss: 0.00002061
Iteration 204/1000 | Loss: 0.00002061
Iteration 205/1000 | Loss: 0.00002061
Iteration 206/1000 | Loss: 0.00002061
Iteration 207/1000 | Loss: 0.00002061
Iteration 208/1000 | Loss: 0.00002061
Iteration 209/1000 | Loss: 0.00002061
Iteration 210/1000 | Loss: 0.00002061
Iteration 211/1000 | Loss: 0.00002061
Iteration 212/1000 | Loss: 0.00002061
Iteration 213/1000 | Loss: 0.00002061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.060623410216067e-05, 2.060623410216067e-05, 2.060623410216067e-05, 2.060623410216067e-05, 2.060623410216067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.060623410216067e-05

Optimization complete. Final v2v error: 3.808626174926758 mm

Highest mean error: 10.551164627075195 mm for frame 223

Lowest mean error: 3.348233461380005 mm for frame 105

Saving results

Total time: 164.29266357421875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080134
Iteration 2/25 | Loss: 0.00237276
Iteration 3/25 | Loss: 0.00165683
Iteration 4/25 | Loss: 0.00142462
Iteration 5/25 | Loss: 0.00146030
Iteration 6/25 | Loss: 0.00154854
Iteration 7/25 | Loss: 0.00126335
Iteration 8/25 | Loss: 0.00113048
Iteration 9/25 | Loss: 0.00109637
Iteration 10/25 | Loss: 0.00105592
Iteration 11/25 | Loss: 0.00101079
Iteration 12/25 | Loss: 0.00100317
Iteration 13/25 | Loss: 0.00099333
Iteration 14/25 | Loss: 0.00099141
Iteration 15/25 | Loss: 0.00104068
Iteration 16/25 | Loss: 0.00102651
Iteration 17/25 | Loss: 0.00100510
Iteration 18/25 | Loss: 0.00098782
Iteration 19/25 | Loss: 0.00097485
Iteration 20/25 | Loss: 0.00097259
Iteration 21/25 | Loss: 0.00097662
Iteration 22/25 | Loss: 0.00100338
Iteration 23/25 | Loss: 0.00101344
Iteration 24/25 | Loss: 0.00101053
Iteration 25/25 | Loss: 0.00100964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45704079
Iteration 2/25 | Loss: 0.00141640
Iteration 3/25 | Loss: 0.00128203
Iteration 4/25 | Loss: 0.00128202
Iteration 5/25 | Loss: 0.00128202
Iteration 6/25 | Loss: 0.00128202
Iteration 7/25 | Loss: 0.00128202
Iteration 8/25 | Loss: 0.00128202
Iteration 9/25 | Loss: 0.00128202
Iteration 10/25 | Loss: 0.00128202
Iteration 11/25 | Loss: 0.00128202
Iteration 12/25 | Loss: 0.00128202
Iteration 13/25 | Loss: 0.00128202
Iteration 14/25 | Loss: 0.00128202
Iteration 15/25 | Loss: 0.00128202
Iteration 16/25 | Loss: 0.00128202
Iteration 17/25 | Loss: 0.00128202
Iteration 18/25 | Loss: 0.00128202
Iteration 19/25 | Loss: 0.00128202
Iteration 20/25 | Loss: 0.00128202
Iteration 21/25 | Loss: 0.00128202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012820231495425105, 0.0012820231495425105, 0.0012820231495425105, 0.0012820231495425105, 0.0012820231495425105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012820231495425105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128202
Iteration 2/1000 | Loss: 0.00142432
Iteration 3/1000 | Loss: 0.00078278
Iteration 4/1000 | Loss: 0.00127339
Iteration 5/1000 | Loss: 0.00153696
Iteration 6/1000 | Loss: 0.00275368
Iteration 7/1000 | Loss: 0.00132335
Iteration 8/1000 | Loss: 0.00308431
Iteration 9/1000 | Loss: 0.00510946
Iteration 10/1000 | Loss: 0.00190496
Iteration 11/1000 | Loss: 0.00052254
Iteration 12/1000 | Loss: 0.00013772
Iteration 13/1000 | Loss: 0.00011528
Iteration 14/1000 | Loss: 0.00043599
Iteration 15/1000 | Loss: 0.00021616
Iteration 16/1000 | Loss: 0.00008618
Iteration 17/1000 | Loss: 0.00007945
Iteration 18/1000 | Loss: 0.00008623
Iteration 19/1000 | Loss: 0.00270496
Iteration 20/1000 | Loss: 0.00083979
Iteration 21/1000 | Loss: 0.00009882
Iteration 22/1000 | Loss: 0.00006993
Iteration 23/1000 | Loss: 0.00019707
Iteration 24/1000 | Loss: 0.00003953
Iteration 25/1000 | Loss: 0.00003173
Iteration 26/1000 | Loss: 0.00002913
Iteration 27/1000 | Loss: 0.00002759
Iteration 28/1000 | Loss: 0.00002613
Iteration 29/1000 | Loss: 0.00002502
Iteration 30/1000 | Loss: 0.00002413
Iteration 31/1000 | Loss: 0.00002350
Iteration 32/1000 | Loss: 0.00002307
Iteration 33/1000 | Loss: 0.00002272
Iteration 34/1000 | Loss: 0.00002242
Iteration 35/1000 | Loss: 0.00002220
Iteration 36/1000 | Loss: 0.00002210
Iteration 37/1000 | Loss: 0.00002206
Iteration 38/1000 | Loss: 0.00002205
Iteration 39/1000 | Loss: 0.00002204
Iteration 40/1000 | Loss: 0.00002201
Iteration 41/1000 | Loss: 0.00002200
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002198
Iteration 44/1000 | Loss: 0.00002196
Iteration 45/1000 | Loss: 0.00002193
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00002189
Iteration 48/1000 | Loss: 0.00002189
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002186
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002184
Iteration 58/1000 | Loss: 0.00002183
Iteration 59/1000 | Loss: 0.00002183
Iteration 60/1000 | Loss: 0.00002183
Iteration 61/1000 | Loss: 0.00002183
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002182
Iteration 64/1000 | Loss: 0.00002182
Iteration 65/1000 | Loss: 0.00002182
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002180
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002179
Iteration 74/1000 | Loss: 0.00002179
Iteration 75/1000 | Loss: 0.00002179
Iteration 76/1000 | Loss: 0.00002179
Iteration 77/1000 | Loss: 0.00002179
Iteration 78/1000 | Loss: 0.00002179
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002179
Iteration 81/1000 | Loss: 0.00002179
Iteration 82/1000 | Loss: 0.00002179
Iteration 83/1000 | Loss: 0.00002179
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00030553
Iteration 86/1000 | Loss: 0.00024276
Iteration 87/1000 | Loss: 0.00003770
Iteration 88/1000 | Loss: 0.00003046
Iteration 89/1000 | Loss: 0.00002748
Iteration 90/1000 | Loss: 0.00002491
Iteration 91/1000 | Loss: 0.00002285
Iteration 92/1000 | Loss: 0.00002199
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002107
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002101
Iteration 99/1000 | Loss: 0.00002087
Iteration 100/1000 | Loss: 0.00002085
Iteration 101/1000 | Loss: 0.00002083
Iteration 102/1000 | Loss: 0.00002083
Iteration 103/1000 | Loss: 0.00002082
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002079
Iteration 107/1000 | Loss: 0.00002079
Iteration 108/1000 | Loss: 0.00002079
Iteration 109/1000 | Loss: 0.00002079
Iteration 110/1000 | Loss: 0.00002079
Iteration 111/1000 | Loss: 0.00002079
Iteration 112/1000 | Loss: 0.00002079
Iteration 113/1000 | Loss: 0.00002078
Iteration 114/1000 | Loss: 0.00002078
Iteration 115/1000 | Loss: 0.00002078
Iteration 116/1000 | Loss: 0.00002078
Iteration 117/1000 | Loss: 0.00002078
Iteration 118/1000 | Loss: 0.00002078
Iteration 119/1000 | Loss: 0.00002077
Iteration 120/1000 | Loss: 0.00002077
Iteration 121/1000 | Loss: 0.00002076
Iteration 122/1000 | Loss: 0.00002076
Iteration 123/1000 | Loss: 0.00002076
Iteration 124/1000 | Loss: 0.00002076
Iteration 125/1000 | Loss: 0.00002076
Iteration 126/1000 | Loss: 0.00002076
Iteration 127/1000 | Loss: 0.00002075
Iteration 128/1000 | Loss: 0.00002075
Iteration 129/1000 | Loss: 0.00002075
Iteration 130/1000 | Loss: 0.00002075
Iteration 131/1000 | Loss: 0.00002074
Iteration 132/1000 | Loss: 0.00002074
Iteration 133/1000 | Loss: 0.00002074
Iteration 134/1000 | Loss: 0.00002073
Iteration 135/1000 | Loss: 0.00002073
Iteration 136/1000 | Loss: 0.00002072
Iteration 137/1000 | Loss: 0.00002072
Iteration 138/1000 | Loss: 0.00002072
Iteration 139/1000 | Loss: 0.00002071
Iteration 140/1000 | Loss: 0.00002071
Iteration 141/1000 | Loss: 0.00002070
Iteration 142/1000 | Loss: 0.00002070
Iteration 143/1000 | Loss: 0.00002070
Iteration 144/1000 | Loss: 0.00002069
Iteration 145/1000 | Loss: 0.00002069
Iteration 146/1000 | Loss: 0.00002069
Iteration 147/1000 | Loss: 0.00002069
Iteration 148/1000 | Loss: 0.00002069
Iteration 149/1000 | Loss: 0.00002068
Iteration 150/1000 | Loss: 0.00002068
Iteration 151/1000 | Loss: 0.00002068
Iteration 152/1000 | Loss: 0.00002067
Iteration 153/1000 | Loss: 0.00002067
Iteration 154/1000 | Loss: 0.00002067
Iteration 155/1000 | Loss: 0.00002067
Iteration 156/1000 | Loss: 0.00002066
Iteration 157/1000 | Loss: 0.00002066
Iteration 158/1000 | Loss: 0.00002066
Iteration 159/1000 | Loss: 0.00002065
Iteration 160/1000 | Loss: 0.00002065
Iteration 161/1000 | Loss: 0.00002065
Iteration 162/1000 | Loss: 0.00002064
Iteration 163/1000 | Loss: 0.00002064
Iteration 164/1000 | Loss: 0.00002064
Iteration 165/1000 | Loss: 0.00002064
Iteration 166/1000 | Loss: 0.00002063
Iteration 167/1000 | Loss: 0.00002063
Iteration 168/1000 | Loss: 0.00002063
Iteration 169/1000 | Loss: 0.00002063
Iteration 170/1000 | Loss: 0.00002063
Iteration 171/1000 | Loss: 0.00002063
Iteration 172/1000 | Loss: 0.00002063
Iteration 173/1000 | Loss: 0.00002063
Iteration 174/1000 | Loss: 0.00002063
Iteration 175/1000 | Loss: 0.00002063
Iteration 176/1000 | Loss: 0.00002063
Iteration 177/1000 | Loss: 0.00002063
Iteration 178/1000 | Loss: 0.00002063
Iteration 179/1000 | Loss: 0.00002063
Iteration 180/1000 | Loss: 0.00002063
Iteration 181/1000 | Loss: 0.00002063
Iteration 182/1000 | Loss: 0.00002063
Iteration 183/1000 | Loss: 0.00002063
Iteration 184/1000 | Loss: 0.00002063
Iteration 185/1000 | Loss: 0.00002063
Iteration 186/1000 | Loss: 0.00002063
Iteration 187/1000 | Loss: 0.00002063
Iteration 188/1000 | Loss: 0.00002063
Iteration 189/1000 | Loss: 0.00002063
Iteration 190/1000 | Loss: 0.00002063
Iteration 191/1000 | Loss: 0.00002063
Iteration 192/1000 | Loss: 0.00002063
Iteration 193/1000 | Loss: 0.00002063
Iteration 194/1000 | Loss: 0.00002063
Iteration 195/1000 | Loss: 0.00002063
Iteration 196/1000 | Loss: 0.00002063
Iteration 197/1000 | Loss: 0.00002063
Iteration 198/1000 | Loss: 0.00002063
Iteration 199/1000 | Loss: 0.00002063
Iteration 200/1000 | Loss: 0.00002063
Iteration 201/1000 | Loss: 0.00002063
Iteration 202/1000 | Loss: 0.00002063
Iteration 203/1000 | Loss: 0.00002063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.06266158784274e-05, 2.06266158784274e-05, 2.06266158784274e-05, 2.06266158784274e-05, 2.06266158784274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.06266158784274e-05

Optimization complete. Final v2v error: 3.786281108856201 mm

Highest mean error: 4.378029823303223 mm for frame 71

Lowest mean error: 3.2606759071350098 mm for frame 147

Saving results

Total time: 124.27062773704529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401945
Iteration 2/25 | Loss: 0.00093586
Iteration 3/25 | Loss: 0.00080355
Iteration 4/25 | Loss: 0.00078620
Iteration 5/25 | Loss: 0.00078198
Iteration 6/25 | Loss: 0.00078045
Iteration 7/25 | Loss: 0.00078012
Iteration 8/25 | Loss: 0.00078012
Iteration 9/25 | Loss: 0.00078012
Iteration 10/25 | Loss: 0.00078012
Iteration 11/25 | Loss: 0.00078012
Iteration 12/25 | Loss: 0.00078012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007801158353686333, 0.0007801158353686333, 0.0007801158353686333, 0.0007801158353686333, 0.0007801158353686333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007801158353686333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44365084
Iteration 2/25 | Loss: 0.00058061
Iteration 3/25 | Loss: 0.00058061
Iteration 4/25 | Loss: 0.00058060
Iteration 5/25 | Loss: 0.00058060
Iteration 6/25 | Loss: 0.00058060
Iteration 7/25 | Loss: 0.00058060
Iteration 8/25 | Loss: 0.00058060
Iteration 9/25 | Loss: 0.00058060
Iteration 10/25 | Loss: 0.00058060
Iteration 11/25 | Loss: 0.00058060
Iteration 12/25 | Loss: 0.00058060
Iteration 13/25 | Loss: 0.00058060
Iteration 14/25 | Loss: 0.00058060
Iteration 15/25 | Loss: 0.00058060
Iteration 16/25 | Loss: 0.00058060
Iteration 17/25 | Loss: 0.00058060
Iteration 18/25 | Loss: 0.00058060
Iteration 19/25 | Loss: 0.00058060
Iteration 20/25 | Loss: 0.00058060
Iteration 21/25 | Loss: 0.00058060
Iteration 22/25 | Loss: 0.00058060
Iteration 23/25 | Loss: 0.00058060
Iteration 24/25 | Loss: 0.00058060
Iteration 25/25 | Loss: 0.00058060

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058060
Iteration 2/1000 | Loss: 0.00003148
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001404
Iteration 6/1000 | Loss: 0.00001331
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001269
Iteration 9/1000 | Loss: 0.00001257
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001236
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001228
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001228
Iteration 54/1000 | Loss: 0.00001228
Iteration 55/1000 | Loss: 0.00001228
Iteration 56/1000 | Loss: 0.00001228
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001227
Iteration 61/1000 | Loss: 0.00001227
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001226
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001226
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001224
Iteration 102/1000 | Loss: 0.00001224
Iteration 103/1000 | Loss: 0.00001224
Iteration 104/1000 | Loss: 0.00001224
Iteration 105/1000 | Loss: 0.00001224
Iteration 106/1000 | Loss: 0.00001224
Iteration 107/1000 | Loss: 0.00001224
Iteration 108/1000 | Loss: 0.00001224
Iteration 109/1000 | Loss: 0.00001224
Iteration 110/1000 | Loss: 0.00001224
Iteration 111/1000 | Loss: 0.00001224
Iteration 112/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.2235629583301488e-05, 1.2235629583301488e-05, 1.2235629583301488e-05, 1.2235629583301488e-05, 1.2235629583301488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2235629583301488e-05

Optimization complete. Final v2v error: 2.9270410537719727 mm

Highest mean error: 3.550999164581299 mm for frame 72

Lowest mean error: 2.5694305896759033 mm for frame 157

Saving results

Total time: 29.739595413208008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125320
Iteration 2/25 | Loss: 0.00181639
Iteration 3/25 | Loss: 0.00143498
Iteration 4/25 | Loss: 0.00120203
Iteration 5/25 | Loss: 0.00135093
Iteration 6/25 | Loss: 0.00130729
Iteration 7/25 | Loss: 0.00101005
Iteration 8/25 | Loss: 0.00114116
Iteration 9/25 | Loss: 0.00107269
Iteration 10/25 | Loss: 0.00102297
Iteration 11/25 | Loss: 0.00101263
Iteration 12/25 | Loss: 0.00096409
Iteration 13/25 | Loss: 0.00094482
Iteration 14/25 | Loss: 0.00094471
Iteration 15/25 | Loss: 0.00093695
Iteration 16/25 | Loss: 0.00092943
Iteration 17/25 | Loss: 0.00092678
Iteration 18/25 | Loss: 0.00092493
Iteration 19/25 | Loss: 0.00092515
Iteration 20/25 | Loss: 0.00092711
Iteration 21/25 | Loss: 0.00091904
Iteration 22/25 | Loss: 0.00092240
Iteration 23/25 | Loss: 0.00092395
Iteration 24/25 | Loss: 0.00091861
Iteration 25/25 | Loss: 0.00092029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50028753
Iteration 2/25 | Loss: 0.00489796
Iteration 3/25 | Loss: 0.00193119
Iteration 4/25 | Loss: 0.00193119
Iteration 5/25 | Loss: 0.00193119
Iteration 6/25 | Loss: 0.00193119
Iteration 7/25 | Loss: 0.00193119
Iteration 8/25 | Loss: 0.00193119
Iteration 9/25 | Loss: 0.00193119
Iteration 10/25 | Loss: 0.00193119
Iteration 11/25 | Loss: 0.00193119
Iteration 12/25 | Loss: 0.00193119
Iteration 13/25 | Loss: 0.00193119
Iteration 14/25 | Loss: 0.00193119
Iteration 15/25 | Loss: 0.00193119
Iteration 16/25 | Loss: 0.00193119
Iteration 17/25 | Loss: 0.00193119
Iteration 18/25 | Loss: 0.00193119
Iteration 19/25 | Loss: 0.00193119
Iteration 20/25 | Loss: 0.00193119
Iteration 21/25 | Loss: 0.00193119
Iteration 22/25 | Loss: 0.00193119
Iteration 23/25 | Loss: 0.00193119
Iteration 24/25 | Loss: 0.00193119
Iteration 25/25 | Loss: 0.00193119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193119
Iteration 2/1000 | Loss: 0.00090678
Iteration 3/1000 | Loss: 0.00075639
Iteration 4/1000 | Loss: 0.00061540
Iteration 5/1000 | Loss: 0.00079446
Iteration 6/1000 | Loss: 0.00195492
Iteration 7/1000 | Loss: 0.00125642
Iteration 8/1000 | Loss: 0.00085544
Iteration 9/1000 | Loss: 0.00160667
Iteration 10/1000 | Loss: 0.00035959
Iteration 11/1000 | Loss: 0.00012775
Iteration 12/1000 | Loss: 0.00030798
Iteration 13/1000 | Loss: 0.00010382
Iteration 14/1000 | Loss: 0.00008494
Iteration 15/1000 | Loss: 0.00016051
Iteration 16/1000 | Loss: 0.00023310
Iteration 17/1000 | Loss: 0.00016793
Iteration 18/1000 | Loss: 0.00022367
Iteration 19/1000 | Loss: 0.00022852
Iteration 20/1000 | Loss: 0.00034204
Iteration 21/1000 | Loss: 0.00020577
Iteration 22/1000 | Loss: 0.00022239
Iteration 23/1000 | Loss: 0.00026281
Iteration 24/1000 | Loss: 0.00014622
Iteration 25/1000 | Loss: 0.00020044
Iteration 26/1000 | Loss: 0.00019496
Iteration 27/1000 | Loss: 0.00019471
Iteration 28/1000 | Loss: 0.00024654
Iteration 29/1000 | Loss: 0.00018153
Iteration 30/1000 | Loss: 0.00020151
Iteration 31/1000 | Loss: 0.00019577
Iteration 32/1000 | Loss: 0.00024463
Iteration 33/1000 | Loss: 0.00022631
Iteration 34/1000 | Loss: 0.00018617
Iteration 35/1000 | Loss: 0.00018594
Iteration 36/1000 | Loss: 0.00012689
Iteration 37/1000 | Loss: 0.00019674
Iteration 38/1000 | Loss: 0.00025293
Iteration 39/1000 | Loss: 0.00012578
Iteration 40/1000 | Loss: 0.00017425
Iteration 41/1000 | Loss: 0.00024848
Iteration 42/1000 | Loss: 0.00017373
Iteration 43/1000 | Loss: 0.00023641
Iteration 44/1000 | Loss: 0.00017868
Iteration 45/1000 | Loss: 0.00099954
Iteration 46/1000 | Loss: 0.00009954
Iteration 47/1000 | Loss: 0.00004982
Iteration 48/1000 | Loss: 0.00003764
Iteration 49/1000 | Loss: 0.00003472
Iteration 50/1000 | Loss: 0.00003282
Iteration 51/1000 | Loss: 0.00003190
Iteration 52/1000 | Loss: 0.00003128
Iteration 53/1000 | Loss: 0.00003082
Iteration 54/1000 | Loss: 0.00003050
Iteration 55/1000 | Loss: 0.00003036
Iteration 56/1000 | Loss: 0.00003025
Iteration 57/1000 | Loss: 0.00003033
Iteration 58/1000 | Loss: 0.00003020
Iteration 59/1000 | Loss: 0.00003020
Iteration 60/1000 | Loss: 0.00003017
Iteration 61/1000 | Loss: 0.00003016
Iteration 62/1000 | Loss: 0.00003009
Iteration 63/1000 | Loss: 0.00003009
Iteration 64/1000 | Loss: 0.00003008
Iteration 65/1000 | Loss: 0.00003008
Iteration 66/1000 | Loss: 0.00003008
Iteration 67/1000 | Loss: 0.00003003
Iteration 68/1000 | Loss: 0.00003000
Iteration 69/1000 | Loss: 0.00002997
Iteration 70/1000 | Loss: 0.00002996
Iteration 71/1000 | Loss: 0.00002990
Iteration 72/1000 | Loss: 0.00002988
Iteration 73/1000 | Loss: 0.00002987
Iteration 74/1000 | Loss: 0.00002986
Iteration 75/1000 | Loss: 0.00002986
Iteration 76/1000 | Loss: 0.00002984
Iteration 77/1000 | Loss: 0.00002983
Iteration 78/1000 | Loss: 0.00002983
Iteration 79/1000 | Loss: 0.00002983
Iteration 80/1000 | Loss: 0.00002983
Iteration 81/1000 | Loss: 0.00002983
Iteration 82/1000 | Loss: 0.00002983
Iteration 83/1000 | Loss: 0.00002983
Iteration 84/1000 | Loss: 0.00002983
Iteration 85/1000 | Loss: 0.00002983
Iteration 86/1000 | Loss: 0.00002983
Iteration 87/1000 | Loss: 0.00002982
Iteration 88/1000 | Loss: 0.00002982
Iteration 89/1000 | Loss: 0.00002982
Iteration 90/1000 | Loss: 0.00002982
Iteration 91/1000 | Loss: 0.00002981
Iteration 92/1000 | Loss: 0.00002980
Iteration 93/1000 | Loss: 0.00002980
Iteration 94/1000 | Loss: 0.00002979
Iteration 95/1000 | Loss: 0.00002979
Iteration 96/1000 | Loss: 0.00002979
Iteration 97/1000 | Loss: 0.00002978
Iteration 98/1000 | Loss: 0.00002978
Iteration 99/1000 | Loss: 0.00002977
Iteration 100/1000 | Loss: 0.00002977
Iteration 101/1000 | Loss: 0.00002977
Iteration 102/1000 | Loss: 0.00002977
Iteration 103/1000 | Loss: 0.00002976
Iteration 104/1000 | Loss: 0.00002976
Iteration 105/1000 | Loss: 0.00002976
Iteration 106/1000 | Loss: 0.00002975
Iteration 107/1000 | Loss: 0.00002975
Iteration 108/1000 | Loss: 0.00002975
Iteration 109/1000 | Loss: 0.00002974
Iteration 110/1000 | Loss: 0.00002973
Iteration 111/1000 | Loss: 0.00002973
Iteration 112/1000 | Loss: 0.00002972
Iteration 113/1000 | Loss: 0.00002972
Iteration 114/1000 | Loss: 0.00002971
Iteration 115/1000 | Loss: 0.00002971
Iteration 116/1000 | Loss: 0.00002970
Iteration 117/1000 | Loss: 0.00002969
Iteration 118/1000 | Loss: 0.00002969
Iteration 119/1000 | Loss: 0.00002969
Iteration 120/1000 | Loss: 0.00002968
Iteration 121/1000 | Loss: 0.00002968
Iteration 122/1000 | Loss: 0.00002968
Iteration 123/1000 | Loss: 0.00002968
Iteration 124/1000 | Loss: 0.00002968
Iteration 125/1000 | Loss: 0.00002968
Iteration 126/1000 | Loss: 0.00002968
Iteration 127/1000 | Loss: 0.00002968
Iteration 128/1000 | Loss: 0.00002968
Iteration 129/1000 | Loss: 0.00002968
Iteration 130/1000 | Loss: 0.00002968
Iteration 131/1000 | Loss: 0.00002968
Iteration 132/1000 | Loss: 0.00002967
Iteration 133/1000 | Loss: 0.00002966
Iteration 134/1000 | Loss: 0.00002966
Iteration 135/1000 | Loss: 0.00002966
Iteration 136/1000 | Loss: 0.00002966
Iteration 137/1000 | Loss: 0.00002966
Iteration 138/1000 | Loss: 0.00002965
Iteration 139/1000 | Loss: 0.00002965
Iteration 140/1000 | Loss: 0.00002965
Iteration 141/1000 | Loss: 0.00002965
Iteration 142/1000 | Loss: 0.00002965
Iteration 143/1000 | Loss: 0.00002964
Iteration 144/1000 | Loss: 0.00002964
Iteration 145/1000 | Loss: 0.00002964
Iteration 146/1000 | Loss: 0.00002964
Iteration 147/1000 | Loss: 0.00002964
Iteration 148/1000 | Loss: 0.00002964
Iteration 149/1000 | Loss: 0.00002964
Iteration 150/1000 | Loss: 0.00002964
Iteration 151/1000 | Loss: 0.00002964
Iteration 152/1000 | Loss: 0.00002964
Iteration 153/1000 | Loss: 0.00002963
Iteration 154/1000 | Loss: 0.00002963
Iteration 155/1000 | Loss: 0.00002963
Iteration 156/1000 | Loss: 0.00002963
Iteration 157/1000 | Loss: 0.00002963
Iteration 158/1000 | Loss: 0.00002962
Iteration 159/1000 | Loss: 0.00002962
Iteration 160/1000 | Loss: 0.00002962
Iteration 161/1000 | Loss: 0.00002962
Iteration 162/1000 | Loss: 0.00002962
Iteration 163/1000 | Loss: 0.00002962
Iteration 164/1000 | Loss: 0.00002962
Iteration 165/1000 | Loss: 0.00002962
Iteration 166/1000 | Loss: 0.00002962
Iteration 167/1000 | Loss: 0.00002961
Iteration 168/1000 | Loss: 0.00002961
Iteration 169/1000 | Loss: 0.00002961
Iteration 170/1000 | Loss: 0.00002961
Iteration 171/1000 | Loss: 0.00002961
Iteration 172/1000 | Loss: 0.00002961
Iteration 173/1000 | Loss: 0.00002961
Iteration 174/1000 | Loss: 0.00002960
Iteration 175/1000 | Loss: 0.00002960
Iteration 176/1000 | Loss: 0.00002960
Iteration 177/1000 | Loss: 0.00002960
Iteration 178/1000 | Loss: 0.00002960
Iteration 179/1000 | Loss: 0.00002959
Iteration 180/1000 | Loss: 0.00002959
Iteration 181/1000 | Loss: 0.00002959
Iteration 182/1000 | Loss: 0.00002959
Iteration 183/1000 | Loss: 0.00002959
Iteration 184/1000 | Loss: 0.00002959
Iteration 185/1000 | Loss: 0.00002959
Iteration 186/1000 | Loss: 0.00002959
Iteration 187/1000 | Loss: 0.00002959
Iteration 188/1000 | Loss: 0.00002958
Iteration 189/1000 | Loss: 0.00002958
Iteration 190/1000 | Loss: 0.00002958
Iteration 191/1000 | Loss: 0.00002958
Iteration 192/1000 | Loss: 0.00002958
Iteration 193/1000 | Loss: 0.00002958
Iteration 194/1000 | Loss: 0.00002958
Iteration 195/1000 | Loss: 0.00002958
Iteration 196/1000 | Loss: 0.00002958
Iteration 197/1000 | Loss: 0.00002958
Iteration 198/1000 | Loss: 0.00002958
Iteration 199/1000 | Loss: 0.00002958
Iteration 200/1000 | Loss: 0.00002958
Iteration 201/1000 | Loss: 0.00002957
Iteration 202/1000 | Loss: 0.00002957
Iteration 203/1000 | Loss: 0.00002957
Iteration 204/1000 | Loss: 0.00002957
Iteration 205/1000 | Loss: 0.00002957
Iteration 206/1000 | Loss: 0.00002957
Iteration 207/1000 | Loss: 0.00002957
Iteration 208/1000 | Loss: 0.00002957
Iteration 209/1000 | Loss: 0.00002957
Iteration 210/1000 | Loss: 0.00002957
Iteration 211/1000 | Loss: 0.00002957
Iteration 212/1000 | Loss: 0.00002957
Iteration 213/1000 | Loss: 0.00002957
Iteration 214/1000 | Loss: 0.00002957
Iteration 215/1000 | Loss: 0.00002957
Iteration 216/1000 | Loss: 0.00002957
Iteration 217/1000 | Loss: 0.00002957
Iteration 218/1000 | Loss: 0.00002957
Iteration 219/1000 | Loss: 0.00002957
Iteration 220/1000 | Loss: 0.00002957
Iteration 221/1000 | Loss: 0.00002957
Iteration 222/1000 | Loss: 0.00002957
Iteration 223/1000 | Loss: 0.00002957
Iteration 224/1000 | Loss: 0.00002957
Iteration 225/1000 | Loss: 0.00002957
Iteration 226/1000 | Loss: 0.00002957
Iteration 227/1000 | Loss: 0.00002957
Iteration 228/1000 | Loss: 0.00002957
Iteration 229/1000 | Loss: 0.00002957
Iteration 230/1000 | Loss: 0.00002957
Iteration 231/1000 | Loss: 0.00002957
Iteration 232/1000 | Loss: 0.00002957
Iteration 233/1000 | Loss: 0.00002957
Iteration 234/1000 | Loss: 0.00002957
Iteration 235/1000 | Loss: 0.00002957
Iteration 236/1000 | Loss: 0.00002957
Iteration 237/1000 | Loss: 0.00002957
Iteration 238/1000 | Loss: 0.00002957
Iteration 239/1000 | Loss: 0.00002957
Iteration 240/1000 | Loss: 0.00002957
Iteration 241/1000 | Loss: 0.00002957
Iteration 242/1000 | Loss: 0.00002957
Iteration 243/1000 | Loss: 0.00002957
Iteration 244/1000 | Loss: 0.00002957
Iteration 245/1000 | Loss: 0.00002957
Iteration 246/1000 | Loss: 0.00002957
Iteration 247/1000 | Loss: 0.00002957
Iteration 248/1000 | Loss: 0.00002957
Iteration 249/1000 | Loss: 0.00002957
Iteration 250/1000 | Loss: 0.00002957
Iteration 251/1000 | Loss: 0.00002957
Iteration 252/1000 | Loss: 0.00002957
Iteration 253/1000 | Loss: 0.00002957
Iteration 254/1000 | Loss: 0.00002957
Iteration 255/1000 | Loss: 0.00002957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.957089600386098e-05, 2.957089600386098e-05, 2.957089600386098e-05, 2.957089600386098e-05, 2.957089600386098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.957089600386098e-05

Optimization complete. Final v2v error: 4.058760166168213 mm

Highest mean error: 21.090129852294922 mm for frame 13

Lowest mean error: 3.4157989025115967 mm for frame 129

Saving results

Total time: 136.92307567596436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508835
Iteration 2/25 | Loss: 0.00109931
Iteration 3/25 | Loss: 0.00098784
Iteration 4/25 | Loss: 0.00095226
Iteration 5/25 | Loss: 0.00093711
Iteration 6/25 | Loss: 0.00093405
Iteration 7/25 | Loss: 0.00093271
Iteration 8/25 | Loss: 0.00093248
Iteration 9/25 | Loss: 0.00093248
Iteration 10/25 | Loss: 0.00093248
Iteration 11/25 | Loss: 0.00093248
Iteration 12/25 | Loss: 0.00093248
Iteration 13/25 | Loss: 0.00093248
Iteration 14/25 | Loss: 0.00093248
Iteration 15/25 | Loss: 0.00093248
Iteration 16/25 | Loss: 0.00093248
Iteration 17/25 | Loss: 0.00093248
Iteration 18/25 | Loss: 0.00093248
Iteration 19/25 | Loss: 0.00093248
Iteration 20/25 | Loss: 0.00093248
Iteration 21/25 | Loss: 0.00093248
Iteration 22/25 | Loss: 0.00093248
Iteration 23/25 | Loss: 0.00093248
Iteration 24/25 | Loss: 0.00093248
Iteration 25/25 | Loss: 0.00093248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38384438
Iteration 2/25 | Loss: 0.00075791
Iteration 3/25 | Loss: 0.00075789
Iteration 4/25 | Loss: 0.00075789
Iteration 5/25 | Loss: 0.00075789
Iteration 6/25 | Loss: 0.00075789
Iteration 7/25 | Loss: 0.00075789
Iteration 8/25 | Loss: 0.00075789
Iteration 9/25 | Loss: 0.00075789
Iteration 10/25 | Loss: 0.00075789
Iteration 11/25 | Loss: 0.00075789
Iteration 12/25 | Loss: 0.00075789
Iteration 13/25 | Loss: 0.00075789
Iteration 14/25 | Loss: 0.00075789
Iteration 15/25 | Loss: 0.00075789
Iteration 16/25 | Loss: 0.00075789
Iteration 17/25 | Loss: 0.00075789
Iteration 18/25 | Loss: 0.00075789
Iteration 19/25 | Loss: 0.00075789
Iteration 20/25 | Loss: 0.00075789
Iteration 21/25 | Loss: 0.00075789
Iteration 22/25 | Loss: 0.00075789
Iteration 23/25 | Loss: 0.00075789
Iteration 24/25 | Loss: 0.00075789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007578915101476014, 0.0007578915101476014, 0.0007578915101476014, 0.0007578915101476014, 0.0007578915101476014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007578915101476014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075789
Iteration 2/1000 | Loss: 0.00006310
Iteration 3/1000 | Loss: 0.00004168
Iteration 4/1000 | Loss: 0.00003754
Iteration 5/1000 | Loss: 0.00003566
Iteration 6/1000 | Loss: 0.00003442
Iteration 7/1000 | Loss: 0.00003350
Iteration 8/1000 | Loss: 0.00003272
Iteration 9/1000 | Loss: 0.00003223
Iteration 10/1000 | Loss: 0.00003192
Iteration 11/1000 | Loss: 0.00003186
Iteration 12/1000 | Loss: 0.00003179
Iteration 13/1000 | Loss: 0.00003174
Iteration 14/1000 | Loss: 0.00003173
Iteration 15/1000 | Loss: 0.00003172
Iteration 16/1000 | Loss: 0.00003171
Iteration 17/1000 | Loss: 0.00003167
Iteration 18/1000 | Loss: 0.00003165
Iteration 19/1000 | Loss: 0.00003165
Iteration 20/1000 | Loss: 0.00003161
Iteration 21/1000 | Loss: 0.00003159
Iteration 22/1000 | Loss: 0.00003158
Iteration 23/1000 | Loss: 0.00003158
Iteration 24/1000 | Loss: 0.00003157
Iteration 25/1000 | Loss: 0.00003157
Iteration 26/1000 | Loss: 0.00003157
Iteration 27/1000 | Loss: 0.00003157
Iteration 28/1000 | Loss: 0.00003156
Iteration 29/1000 | Loss: 0.00003156
Iteration 30/1000 | Loss: 0.00003156
Iteration 31/1000 | Loss: 0.00003156
Iteration 32/1000 | Loss: 0.00003156
Iteration 33/1000 | Loss: 0.00003156
Iteration 34/1000 | Loss: 0.00003156
Iteration 35/1000 | Loss: 0.00003156
Iteration 36/1000 | Loss: 0.00003156
Iteration 37/1000 | Loss: 0.00003156
Iteration 38/1000 | Loss: 0.00003156
Iteration 39/1000 | Loss: 0.00003155
Iteration 40/1000 | Loss: 0.00003155
Iteration 41/1000 | Loss: 0.00003155
Iteration 42/1000 | Loss: 0.00003155
Iteration 43/1000 | Loss: 0.00003153
Iteration 44/1000 | Loss: 0.00003153
Iteration 45/1000 | Loss: 0.00003153
Iteration 46/1000 | Loss: 0.00003152
Iteration 47/1000 | Loss: 0.00003152
Iteration 48/1000 | Loss: 0.00003151
Iteration 49/1000 | Loss: 0.00003150
Iteration 50/1000 | Loss: 0.00003149
Iteration 51/1000 | Loss: 0.00003149
Iteration 52/1000 | Loss: 0.00003149
Iteration 53/1000 | Loss: 0.00003149
Iteration 54/1000 | Loss: 0.00003148
Iteration 55/1000 | Loss: 0.00003148
Iteration 56/1000 | Loss: 0.00003148
Iteration 57/1000 | Loss: 0.00003147
Iteration 58/1000 | Loss: 0.00003147
Iteration 59/1000 | Loss: 0.00003147
Iteration 60/1000 | Loss: 0.00003147
Iteration 61/1000 | Loss: 0.00003146
Iteration 62/1000 | Loss: 0.00003146
Iteration 63/1000 | Loss: 0.00003146
Iteration 64/1000 | Loss: 0.00003146
Iteration 65/1000 | Loss: 0.00003145
Iteration 66/1000 | Loss: 0.00003145
Iteration 67/1000 | Loss: 0.00003145
Iteration 68/1000 | Loss: 0.00003145
Iteration 69/1000 | Loss: 0.00003145
Iteration 70/1000 | Loss: 0.00003144
Iteration 71/1000 | Loss: 0.00003144
Iteration 72/1000 | Loss: 0.00003144
Iteration 73/1000 | Loss: 0.00003144
Iteration 74/1000 | Loss: 0.00003143
Iteration 75/1000 | Loss: 0.00003143
Iteration 76/1000 | Loss: 0.00003143
Iteration 77/1000 | Loss: 0.00003143
Iteration 78/1000 | Loss: 0.00003143
Iteration 79/1000 | Loss: 0.00003142
Iteration 80/1000 | Loss: 0.00003142
Iteration 81/1000 | Loss: 0.00003142
Iteration 82/1000 | Loss: 0.00003142
Iteration 83/1000 | Loss: 0.00003141
Iteration 84/1000 | Loss: 0.00003141
Iteration 85/1000 | Loss: 0.00003141
Iteration 86/1000 | Loss: 0.00003141
Iteration 87/1000 | Loss: 0.00003140
Iteration 88/1000 | Loss: 0.00003140
Iteration 89/1000 | Loss: 0.00003140
Iteration 90/1000 | Loss: 0.00003140
Iteration 91/1000 | Loss: 0.00003140
Iteration 92/1000 | Loss: 0.00003140
Iteration 93/1000 | Loss: 0.00003139
Iteration 94/1000 | Loss: 0.00003139
Iteration 95/1000 | Loss: 0.00003139
Iteration 96/1000 | Loss: 0.00003139
Iteration 97/1000 | Loss: 0.00003139
Iteration 98/1000 | Loss: 0.00003139
Iteration 99/1000 | Loss: 0.00003139
Iteration 100/1000 | Loss: 0.00003139
Iteration 101/1000 | Loss: 0.00003139
Iteration 102/1000 | Loss: 0.00003139
Iteration 103/1000 | Loss: 0.00003138
Iteration 104/1000 | Loss: 0.00003138
Iteration 105/1000 | Loss: 0.00003138
Iteration 106/1000 | Loss: 0.00003138
Iteration 107/1000 | Loss: 0.00003138
Iteration 108/1000 | Loss: 0.00003138
Iteration 109/1000 | Loss: 0.00003138
Iteration 110/1000 | Loss: 0.00003138
Iteration 111/1000 | Loss: 0.00003137
Iteration 112/1000 | Loss: 0.00003137
Iteration 113/1000 | Loss: 0.00003137
Iteration 114/1000 | Loss: 0.00003137
Iteration 115/1000 | Loss: 0.00003137
Iteration 116/1000 | Loss: 0.00003136
Iteration 117/1000 | Loss: 0.00003136
Iteration 118/1000 | Loss: 0.00003136
Iteration 119/1000 | Loss: 0.00003136
Iteration 120/1000 | Loss: 0.00003136
Iteration 121/1000 | Loss: 0.00003136
Iteration 122/1000 | Loss: 0.00003136
Iteration 123/1000 | Loss: 0.00003136
Iteration 124/1000 | Loss: 0.00003136
Iteration 125/1000 | Loss: 0.00003136
Iteration 126/1000 | Loss: 0.00003136
Iteration 127/1000 | Loss: 0.00003136
Iteration 128/1000 | Loss: 0.00003136
Iteration 129/1000 | Loss: 0.00003136
Iteration 130/1000 | Loss: 0.00003136
Iteration 131/1000 | Loss: 0.00003135
Iteration 132/1000 | Loss: 0.00003135
Iteration 133/1000 | Loss: 0.00003135
Iteration 134/1000 | Loss: 0.00003135
Iteration 135/1000 | Loss: 0.00003135
Iteration 136/1000 | Loss: 0.00003135
Iteration 137/1000 | Loss: 0.00003135
Iteration 138/1000 | Loss: 0.00003135
Iteration 139/1000 | Loss: 0.00003135
Iteration 140/1000 | Loss: 0.00003135
Iteration 141/1000 | Loss: 0.00003135
Iteration 142/1000 | Loss: 0.00003135
Iteration 143/1000 | Loss: 0.00003135
Iteration 144/1000 | Loss: 0.00003135
Iteration 145/1000 | Loss: 0.00003135
Iteration 146/1000 | Loss: 0.00003135
Iteration 147/1000 | Loss: 0.00003135
Iteration 148/1000 | Loss: 0.00003135
Iteration 149/1000 | Loss: 0.00003135
Iteration 150/1000 | Loss: 0.00003135
Iteration 151/1000 | Loss: 0.00003135
Iteration 152/1000 | Loss: 0.00003135
Iteration 153/1000 | Loss: 0.00003135
Iteration 154/1000 | Loss: 0.00003135
Iteration 155/1000 | Loss: 0.00003135
Iteration 156/1000 | Loss: 0.00003135
Iteration 157/1000 | Loss: 0.00003135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.1346393370768055e-05, 3.1346393370768055e-05, 3.1346393370768055e-05, 3.1346393370768055e-05, 3.1346393370768055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1346393370768055e-05

Optimization complete. Final v2v error: 4.697429180145264 mm

Highest mean error: 5.419136047363281 mm for frame 18

Lowest mean error: 3.834101915359497 mm for frame 4

Saving results

Total time: 39.25977349281311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100274
Iteration 2/25 | Loss: 0.00362012
Iteration 3/25 | Loss: 0.00212230
Iteration 4/25 | Loss: 0.00186795
Iteration 5/25 | Loss: 0.00185854
Iteration 6/25 | Loss: 0.00177574
Iteration 7/25 | Loss: 0.00141311
Iteration 8/25 | Loss: 0.00104474
Iteration 9/25 | Loss: 0.00095165
Iteration 10/25 | Loss: 0.00086715
Iteration 11/25 | Loss: 0.00085373
Iteration 12/25 | Loss: 0.00084842
Iteration 13/25 | Loss: 0.00084842
Iteration 14/25 | Loss: 0.00084895
Iteration 15/25 | Loss: 0.00084612
Iteration 16/25 | Loss: 0.00084445
Iteration 17/25 | Loss: 0.00084750
Iteration 18/25 | Loss: 0.00084317
Iteration 19/25 | Loss: 0.00084461
Iteration 20/25 | Loss: 0.00084718
Iteration 21/25 | Loss: 0.00084273
Iteration 22/25 | Loss: 0.00083793
Iteration 23/25 | Loss: 0.00083850
Iteration 24/25 | Loss: 0.00083713
Iteration 25/25 | Loss: 0.00083830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45212853
Iteration 2/25 | Loss: 0.00074612
Iteration 3/25 | Loss: 0.00074612
Iteration 4/25 | Loss: 0.00074612
Iteration 5/25 | Loss: 0.00074612
Iteration 6/25 | Loss: 0.00074612
Iteration 7/25 | Loss: 0.00074612
Iteration 8/25 | Loss: 0.00074612
Iteration 9/25 | Loss: 0.00074612
Iteration 10/25 | Loss: 0.00074612
Iteration 11/25 | Loss: 0.00074612
Iteration 12/25 | Loss: 0.00074612
Iteration 13/25 | Loss: 0.00074612
Iteration 14/25 | Loss: 0.00074612
Iteration 15/25 | Loss: 0.00074612
Iteration 16/25 | Loss: 0.00074612
Iteration 17/25 | Loss: 0.00074612
Iteration 18/25 | Loss: 0.00074612
Iteration 19/25 | Loss: 0.00074612
Iteration 20/25 | Loss: 0.00074612
Iteration 21/25 | Loss: 0.00074612
Iteration 22/25 | Loss: 0.00074612
Iteration 23/25 | Loss: 0.00074612
Iteration 24/25 | Loss: 0.00074612
Iteration 25/25 | Loss: 0.00074612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074612
Iteration 2/1000 | Loss: 0.00019556
Iteration 3/1000 | Loss: 0.00015319
Iteration 4/1000 | Loss: 0.00009674
Iteration 5/1000 | Loss: 0.00004070
Iteration 6/1000 | Loss: 0.00018953
Iteration 7/1000 | Loss: 0.00017723
Iteration 8/1000 | Loss: 0.00017184
Iteration 9/1000 | Loss: 0.00004014
Iteration 10/1000 | Loss: 0.00026335
Iteration 11/1000 | Loss: 0.00028969
Iteration 12/1000 | Loss: 0.00022582
Iteration 13/1000 | Loss: 0.00052096
Iteration 14/1000 | Loss: 0.00015741
Iteration 15/1000 | Loss: 0.00026110
Iteration 16/1000 | Loss: 0.00014343
Iteration 17/1000 | Loss: 0.00016364
Iteration 18/1000 | Loss: 0.00016311
Iteration 19/1000 | Loss: 0.00005608
Iteration 20/1000 | Loss: 0.00011305
Iteration 21/1000 | Loss: 0.00012029
Iteration 22/1000 | Loss: 0.00021977
Iteration 23/1000 | Loss: 0.00013258
Iteration 24/1000 | Loss: 0.00011939
Iteration 25/1000 | Loss: 0.00013861
Iteration 26/1000 | Loss: 0.00040955
Iteration 27/1000 | Loss: 0.00019634
Iteration 28/1000 | Loss: 0.00013621
Iteration 29/1000 | Loss: 0.00010202
Iteration 30/1000 | Loss: 0.00024039
Iteration 31/1000 | Loss: 0.00010498
Iteration 32/1000 | Loss: 0.00011201
Iteration 33/1000 | Loss: 0.00027077
Iteration 34/1000 | Loss: 0.00011984
Iteration 35/1000 | Loss: 0.00004362
Iteration 36/1000 | Loss: 0.00017071
Iteration 37/1000 | Loss: 0.00012087
Iteration 38/1000 | Loss: 0.00004782
Iteration 39/1000 | Loss: 0.00014278
Iteration 40/1000 | Loss: 0.00010862
Iteration 41/1000 | Loss: 0.00007473
Iteration 42/1000 | Loss: 0.00136510
Iteration 43/1000 | Loss: 0.00102005
Iteration 44/1000 | Loss: 0.00019863
Iteration 45/1000 | Loss: 0.00023679
Iteration 46/1000 | Loss: 0.00040055
Iteration 47/1000 | Loss: 0.00015743
Iteration 48/1000 | Loss: 0.00009017
Iteration 49/1000 | Loss: 0.00055428
Iteration 50/1000 | Loss: 0.00036130
Iteration 51/1000 | Loss: 0.00008017
Iteration 52/1000 | Loss: 0.00004388
Iteration 53/1000 | Loss: 0.00007260
Iteration 54/1000 | Loss: 0.00004405
Iteration 55/1000 | Loss: 0.00003385
Iteration 56/1000 | Loss: 0.00003064
Iteration 57/1000 | Loss: 0.00002916
Iteration 58/1000 | Loss: 0.00002787
Iteration 59/1000 | Loss: 0.00002695
Iteration 60/1000 | Loss: 0.00053834
Iteration 61/1000 | Loss: 0.00057457
Iteration 62/1000 | Loss: 0.00003616
Iteration 63/1000 | Loss: 0.00002607
Iteration 64/1000 | Loss: 0.00002387
Iteration 65/1000 | Loss: 0.00002261
Iteration 66/1000 | Loss: 0.00002184
Iteration 67/1000 | Loss: 0.00002131
Iteration 68/1000 | Loss: 0.00002097
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002078
Iteration 71/1000 | Loss: 0.00002077
Iteration 72/1000 | Loss: 0.00002076
Iteration 73/1000 | Loss: 0.00002076
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002069
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002067
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002064
Iteration 88/1000 | Loss: 0.00002064
Iteration 89/1000 | Loss: 0.00002064
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002063
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002063
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002062
Iteration 102/1000 | Loss: 0.00002062
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002061
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002060
Iteration 109/1000 | Loss: 0.00002060
Iteration 110/1000 | Loss: 0.00002060
Iteration 111/1000 | Loss: 0.00002060
Iteration 112/1000 | Loss: 0.00002059
Iteration 113/1000 | Loss: 0.00002059
Iteration 114/1000 | Loss: 0.00002059
Iteration 115/1000 | Loss: 0.00002058
Iteration 116/1000 | Loss: 0.00002058
Iteration 117/1000 | Loss: 0.00002056
Iteration 118/1000 | Loss: 0.00002056
Iteration 119/1000 | Loss: 0.00002056
Iteration 120/1000 | Loss: 0.00002055
Iteration 121/1000 | Loss: 0.00002055
Iteration 122/1000 | Loss: 0.00002055
Iteration 123/1000 | Loss: 0.00002055
Iteration 124/1000 | Loss: 0.00002054
Iteration 125/1000 | Loss: 0.00002054
Iteration 126/1000 | Loss: 0.00002054
Iteration 127/1000 | Loss: 0.00002054
Iteration 128/1000 | Loss: 0.00002054
Iteration 129/1000 | Loss: 0.00002054
Iteration 130/1000 | Loss: 0.00002054
Iteration 131/1000 | Loss: 0.00002054
Iteration 132/1000 | Loss: 0.00002053
Iteration 133/1000 | Loss: 0.00002053
Iteration 134/1000 | Loss: 0.00002053
Iteration 135/1000 | Loss: 0.00002053
Iteration 136/1000 | Loss: 0.00002053
Iteration 137/1000 | Loss: 0.00002053
Iteration 138/1000 | Loss: 0.00002053
Iteration 139/1000 | Loss: 0.00002053
Iteration 140/1000 | Loss: 0.00002053
Iteration 141/1000 | Loss: 0.00002053
Iteration 142/1000 | Loss: 0.00002053
Iteration 143/1000 | Loss: 0.00002052
Iteration 144/1000 | Loss: 0.00002052
Iteration 145/1000 | Loss: 0.00002052
Iteration 146/1000 | Loss: 0.00002052
Iteration 147/1000 | Loss: 0.00002052
Iteration 148/1000 | Loss: 0.00002051
Iteration 149/1000 | Loss: 0.00002051
Iteration 150/1000 | Loss: 0.00002051
Iteration 151/1000 | Loss: 0.00002051
Iteration 152/1000 | Loss: 0.00002051
Iteration 153/1000 | Loss: 0.00002051
Iteration 154/1000 | Loss: 0.00002051
Iteration 155/1000 | Loss: 0.00002050
Iteration 156/1000 | Loss: 0.00002050
Iteration 157/1000 | Loss: 0.00002050
Iteration 158/1000 | Loss: 0.00002050
Iteration 159/1000 | Loss: 0.00002050
Iteration 160/1000 | Loss: 0.00002050
Iteration 161/1000 | Loss: 0.00002050
Iteration 162/1000 | Loss: 0.00002050
Iteration 163/1000 | Loss: 0.00002050
Iteration 164/1000 | Loss: 0.00002050
Iteration 165/1000 | Loss: 0.00002050
Iteration 166/1000 | Loss: 0.00002050
Iteration 167/1000 | Loss: 0.00002050
Iteration 168/1000 | Loss: 0.00002050
Iteration 169/1000 | Loss: 0.00002050
Iteration 170/1000 | Loss: 0.00002050
Iteration 171/1000 | Loss: 0.00002050
Iteration 172/1000 | Loss: 0.00002050
Iteration 173/1000 | Loss: 0.00002050
Iteration 174/1000 | Loss: 0.00002050
Iteration 175/1000 | Loss: 0.00002050
Iteration 176/1000 | Loss: 0.00002050
Iteration 177/1000 | Loss: 0.00002050
Iteration 178/1000 | Loss: 0.00002050
Iteration 179/1000 | Loss: 0.00002049
Iteration 180/1000 | Loss: 0.00002049
Iteration 181/1000 | Loss: 0.00002049
Iteration 182/1000 | Loss: 0.00002049
Iteration 183/1000 | Loss: 0.00002049
Iteration 184/1000 | Loss: 0.00002049
Iteration 185/1000 | Loss: 0.00002049
Iteration 186/1000 | Loss: 0.00002049
Iteration 187/1000 | Loss: 0.00002049
Iteration 188/1000 | Loss: 0.00002049
Iteration 189/1000 | Loss: 0.00002049
Iteration 190/1000 | Loss: 0.00002049
Iteration 191/1000 | Loss: 0.00002049
Iteration 192/1000 | Loss: 0.00002048
Iteration 193/1000 | Loss: 0.00002048
Iteration 194/1000 | Loss: 0.00002048
Iteration 195/1000 | Loss: 0.00002048
Iteration 196/1000 | Loss: 0.00002048
Iteration 197/1000 | Loss: 0.00002048
Iteration 198/1000 | Loss: 0.00002048
Iteration 199/1000 | Loss: 0.00002048
Iteration 200/1000 | Loss: 0.00002048
Iteration 201/1000 | Loss: 0.00002048
Iteration 202/1000 | Loss: 0.00002048
Iteration 203/1000 | Loss: 0.00002048
Iteration 204/1000 | Loss: 0.00002048
Iteration 205/1000 | Loss: 0.00002048
Iteration 206/1000 | Loss: 0.00002048
Iteration 207/1000 | Loss: 0.00002048
Iteration 208/1000 | Loss: 0.00002048
Iteration 209/1000 | Loss: 0.00002047
Iteration 210/1000 | Loss: 0.00002047
Iteration 211/1000 | Loss: 0.00002047
Iteration 212/1000 | Loss: 0.00002047
Iteration 213/1000 | Loss: 0.00002047
Iteration 214/1000 | Loss: 0.00002047
Iteration 215/1000 | Loss: 0.00002047
Iteration 216/1000 | Loss: 0.00002047
Iteration 217/1000 | Loss: 0.00002047
Iteration 218/1000 | Loss: 0.00002047
Iteration 219/1000 | Loss: 0.00002047
Iteration 220/1000 | Loss: 0.00002047
Iteration 221/1000 | Loss: 0.00002047
Iteration 222/1000 | Loss: 0.00002047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.0467048670980148e-05, 2.0467048670980148e-05, 2.0467048670980148e-05, 2.0467048670980148e-05, 2.0467048670980148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0467048670980148e-05

Optimization complete. Final v2v error: 3.757234573364258 mm

Highest mean error: 9.131622314453125 mm for frame 131

Lowest mean error: 3.439695358276367 mm for frame 126

Saving results

Total time: 149.32662844657898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906713
Iteration 2/25 | Loss: 0.00104130
Iteration 3/25 | Loss: 0.00088600
Iteration 4/25 | Loss: 0.00086367
Iteration 5/25 | Loss: 0.00085676
Iteration 6/25 | Loss: 0.00085451
Iteration 7/25 | Loss: 0.00085419
Iteration 8/25 | Loss: 0.00085419
Iteration 9/25 | Loss: 0.00085419
Iteration 10/25 | Loss: 0.00085419
Iteration 11/25 | Loss: 0.00085419
Iteration 12/25 | Loss: 0.00085419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008541860152035952, 0.0008541860152035952, 0.0008541860152035952, 0.0008541860152035952, 0.0008541860152035952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008541860152035952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44392347
Iteration 2/25 | Loss: 0.00070496
Iteration 3/25 | Loss: 0.00070493
Iteration 4/25 | Loss: 0.00070493
Iteration 5/25 | Loss: 0.00070493
Iteration 6/25 | Loss: 0.00070493
Iteration 7/25 | Loss: 0.00070493
Iteration 8/25 | Loss: 0.00070493
Iteration 9/25 | Loss: 0.00070493
Iteration 10/25 | Loss: 0.00070493
Iteration 11/25 | Loss: 0.00070493
Iteration 12/25 | Loss: 0.00070493
Iteration 13/25 | Loss: 0.00070493
Iteration 14/25 | Loss: 0.00070493
Iteration 15/25 | Loss: 0.00070493
Iteration 16/25 | Loss: 0.00070493
Iteration 17/25 | Loss: 0.00070493
Iteration 18/25 | Loss: 0.00070493
Iteration 19/25 | Loss: 0.00070493
Iteration 20/25 | Loss: 0.00070493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007049322011880577, 0.0007049322011880577, 0.0007049322011880577, 0.0007049322011880577, 0.0007049322011880577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007049322011880577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070493
Iteration 2/1000 | Loss: 0.00004811
Iteration 3/1000 | Loss: 0.00003352
Iteration 4/1000 | Loss: 0.00002726
Iteration 5/1000 | Loss: 0.00002504
Iteration 6/1000 | Loss: 0.00002275
Iteration 7/1000 | Loss: 0.00002156
Iteration 8/1000 | Loss: 0.00002084
Iteration 9/1000 | Loss: 0.00002051
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00002011
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00002010
Iteration 16/1000 | Loss: 0.00002009
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002006
Iteration 19/1000 | Loss: 0.00002006
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00002005
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00002005
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002004
Iteration 29/1000 | Loss: 0.00002003
Iteration 30/1000 | Loss: 0.00002003
Iteration 31/1000 | Loss: 0.00002003
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002002
Iteration 40/1000 | Loss: 0.00002002
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002001
Iteration 44/1000 | Loss: 0.00002001
Iteration 45/1000 | Loss: 0.00002001
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00002000
Iteration 48/1000 | Loss: 0.00002000
Iteration 49/1000 | Loss: 0.00002000
Iteration 50/1000 | Loss: 0.00002000
Iteration 51/1000 | Loss: 0.00002000
Iteration 52/1000 | Loss: 0.00002000
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00002000
Iteration 55/1000 | Loss: 0.00002000
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00002000
Iteration 60/1000 | Loss: 0.00002000
Iteration 61/1000 | Loss: 0.00002000
Iteration 62/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.999865344259888e-05, 1.999865344259888e-05, 1.999865344259888e-05, 1.999865344259888e-05, 1.999865344259888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.999865344259888e-05

Optimization complete. Final v2v error: 3.835866928100586 mm

Highest mean error: 4.063628196716309 mm for frame 84

Lowest mean error: 3.495272397994995 mm for frame 1

Saving results

Total time: 28.727259874343872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819316
Iteration 2/25 | Loss: 0.00112433
Iteration 3/25 | Loss: 0.00094224
Iteration 4/25 | Loss: 0.00090821
Iteration 5/25 | Loss: 0.00089482
Iteration 6/25 | Loss: 0.00089101
Iteration 7/25 | Loss: 0.00088983
Iteration 8/25 | Loss: 0.00088967
Iteration 9/25 | Loss: 0.00088967
Iteration 10/25 | Loss: 0.00088967
Iteration 11/25 | Loss: 0.00088967
Iteration 12/25 | Loss: 0.00088967
Iteration 13/25 | Loss: 0.00088967
Iteration 14/25 | Loss: 0.00088967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008896705694496632, 0.0008896705694496632, 0.0008896705694496632, 0.0008896705694496632, 0.0008896705694496632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008896705694496632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70063400
Iteration 2/25 | Loss: 0.00076947
Iteration 3/25 | Loss: 0.00076945
Iteration 4/25 | Loss: 0.00076944
Iteration 5/25 | Loss: 0.00076944
Iteration 6/25 | Loss: 0.00076944
Iteration 7/25 | Loss: 0.00076944
Iteration 8/25 | Loss: 0.00076944
Iteration 9/25 | Loss: 0.00076944
Iteration 10/25 | Loss: 0.00076944
Iteration 11/25 | Loss: 0.00076944
Iteration 12/25 | Loss: 0.00076944
Iteration 13/25 | Loss: 0.00076944
Iteration 14/25 | Loss: 0.00076944
Iteration 15/25 | Loss: 0.00076944
Iteration 16/25 | Loss: 0.00076944
Iteration 17/25 | Loss: 0.00076944
Iteration 18/25 | Loss: 0.00076944
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007694427622482181, 0.0007694427622482181, 0.0007694427622482181, 0.0007694427622482181, 0.0007694427622482181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007694427622482181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076944
Iteration 2/1000 | Loss: 0.00004594
Iteration 3/1000 | Loss: 0.00003057
Iteration 4/1000 | Loss: 0.00002647
Iteration 5/1000 | Loss: 0.00002458
Iteration 6/1000 | Loss: 0.00002354
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002237
Iteration 9/1000 | Loss: 0.00002209
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002197
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002192
Iteration 17/1000 | Loss: 0.00002192
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002190
Iteration 21/1000 | Loss: 0.00002189
Iteration 22/1000 | Loss: 0.00002184
Iteration 23/1000 | Loss: 0.00002181
Iteration 24/1000 | Loss: 0.00002181
Iteration 25/1000 | Loss: 0.00002178
Iteration 26/1000 | Loss: 0.00002177
Iteration 27/1000 | Loss: 0.00002177
Iteration 28/1000 | Loss: 0.00002177
Iteration 29/1000 | Loss: 0.00002176
Iteration 30/1000 | Loss: 0.00002176
Iteration 31/1000 | Loss: 0.00002176
Iteration 32/1000 | Loss: 0.00002175
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002173
Iteration 37/1000 | Loss: 0.00002173
Iteration 38/1000 | Loss: 0.00002172
Iteration 39/1000 | Loss: 0.00002172
Iteration 40/1000 | Loss: 0.00002172
Iteration 41/1000 | Loss: 0.00002172
Iteration 42/1000 | Loss: 0.00002171
Iteration 43/1000 | Loss: 0.00002171
Iteration 44/1000 | Loss: 0.00002170
Iteration 45/1000 | Loss: 0.00002170
Iteration 46/1000 | Loss: 0.00002170
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002169
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002169
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002168
Iteration 58/1000 | Loss: 0.00002168
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002167
Iteration 61/1000 | Loss: 0.00002167
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00002166
Iteration 64/1000 | Loss: 0.00002166
Iteration 65/1000 | Loss: 0.00002166
Iteration 66/1000 | Loss: 0.00002166
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002165
Iteration 70/1000 | Loss: 0.00002165
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002164
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002163
Iteration 84/1000 | Loss: 0.00002163
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002163
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002162
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002162
Iteration 93/1000 | Loss: 0.00002161
Iteration 94/1000 | Loss: 0.00002161
Iteration 95/1000 | Loss: 0.00002161
Iteration 96/1000 | Loss: 0.00002161
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002161
Iteration 99/1000 | Loss: 0.00002161
Iteration 100/1000 | Loss: 0.00002160
Iteration 101/1000 | Loss: 0.00002160
Iteration 102/1000 | Loss: 0.00002160
Iteration 103/1000 | Loss: 0.00002160
Iteration 104/1000 | Loss: 0.00002160
Iteration 105/1000 | Loss: 0.00002160
Iteration 106/1000 | Loss: 0.00002159
Iteration 107/1000 | Loss: 0.00002159
Iteration 108/1000 | Loss: 0.00002159
Iteration 109/1000 | Loss: 0.00002159
Iteration 110/1000 | Loss: 0.00002159
Iteration 111/1000 | Loss: 0.00002159
Iteration 112/1000 | Loss: 0.00002159
Iteration 113/1000 | Loss: 0.00002159
Iteration 114/1000 | Loss: 0.00002159
Iteration 115/1000 | Loss: 0.00002159
Iteration 116/1000 | Loss: 0.00002159
Iteration 117/1000 | Loss: 0.00002159
Iteration 118/1000 | Loss: 0.00002159
Iteration 119/1000 | Loss: 0.00002159
Iteration 120/1000 | Loss: 0.00002159
Iteration 121/1000 | Loss: 0.00002158
Iteration 122/1000 | Loss: 0.00002158
Iteration 123/1000 | Loss: 0.00002158
Iteration 124/1000 | Loss: 0.00002158
Iteration 125/1000 | Loss: 0.00002158
Iteration 126/1000 | Loss: 0.00002158
Iteration 127/1000 | Loss: 0.00002158
Iteration 128/1000 | Loss: 0.00002158
Iteration 129/1000 | Loss: 0.00002158
Iteration 130/1000 | Loss: 0.00002158
Iteration 131/1000 | Loss: 0.00002157
Iteration 132/1000 | Loss: 0.00002157
Iteration 133/1000 | Loss: 0.00002157
Iteration 134/1000 | Loss: 0.00002157
Iteration 135/1000 | Loss: 0.00002157
Iteration 136/1000 | Loss: 0.00002157
Iteration 137/1000 | Loss: 0.00002157
Iteration 138/1000 | Loss: 0.00002157
Iteration 139/1000 | Loss: 0.00002157
Iteration 140/1000 | Loss: 0.00002157
Iteration 141/1000 | Loss: 0.00002157
Iteration 142/1000 | Loss: 0.00002157
Iteration 143/1000 | Loss: 0.00002157
Iteration 144/1000 | Loss: 0.00002157
Iteration 145/1000 | Loss: 0.00002157
Iteration 146/1000 | Loss: 0.00002157
Iteration 147/1000 | Loss: 0.00002157
Iteration 148/1000 | Loss: 0.00002157
Iteration 149/1000 | Loss: 0.00002156
Iteration 150/1000 | Loss: 0.00002156
Iteration 151/1000 | Loss: 0.00002156
Iteration 152/1000 | Loss: 0.00002156
Iteration 153/1000 | Loss: 0.00002156
Iteration 154/1000 | Loss: 0.00002156
Iteration 155/1000 | Loss: 0.00002155
Iteration 156/1000 | Loss: 0.00002155
Iteration 157/1000 | Loss: 0.00002155
Iteration 158/1000 | Loss: 0.00002155
Iteration 159/1000 | Loss: 0.00002155
Iteration 160/1000 | Loss: 0.00002155
Iteration 161/1000 | Loss: 0.00002155
Iteration 162/1000 | Loss: 0.00002155
Iteration 163/1000 | Loss: 0.00002155
Iteration 164/1000 | Loss: 0.00002155
Iteration 165/1000 | Loss: 0.00002155
Iteration 166/1000 | Loss: 0.00002155
Iteration 167/1000 | Loss: 0.00002155
Iteration 168/1000 | Loss: 0.00002155
Iteration 169/1000 | Loss: 0.00002155
Iteration 170/1000 | Loss: 0.00002155
Iteration 171/1000 | Loss: 0.00002155
Iteration 172/1000 | Loss: 0.00002155
Iteration 173/1000 | Loss: 0.00002155
Iteration 174/1000 | Loss: 0.00002155
Iteration 175/1000 | Loss: 0.00002155
Iteration 176/1000 | Loss: 0.00002155
Iteration 177/1000 | Loss: 0.00002155
Iteration 178/1000 | Loss: 0.00002155
Iteration 179/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.1551733880187385e-05, 2.1551733880187385e-05, 2.1551733880187385e-05, 2.1551733880187385e-05, 2.1551733880187385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1551733880187385e-05

Optimization complete. Final v2v error: 3.9446401596069336 mm

Highest mean error: 4.387807846069336 mm for frame 48

Lowest mean error: 3.6104087829589844 mm for frame 4

Saving results

Total time: 36.479710817337036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500070
Iteration 2/25 | Loss: 0.00099377
Iteration 3/25 | Loss: 0.00088741
Iteration 4/25 | Loss: 0.00086416
Iteration 5/25 | Loss: 0.00085675
Iteration 6/25 | Loss: 0.00085451
Iteration 7/25 | Loss: 0.00085406
Iteration 8/25 | Loss: 0.00085406
Iteration 9/25 | Loss: 0.00085406
Iteration 10/25 | Loss: 0.00085406
Iteration 11/25 | Loss: 0.00085406
Iteration 12/25 | Loss: 0.00085406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008540591225028038, 0.0008540591225028038, 0.0008540591225028038, 0.0008540591225028038, 0.0008540591225028038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008540591225028038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.36454868
Iteration 2/25 | Loss: 0.00069867
Iteration 3/25 | Loss: 0.00069867
Iteration 4/25 | Loss: 0.00069867
Iteration 5/25 | Loss: 0.00069867
Iteration 6/25 | Loss: 0.00069867
Iteration 7/25 | Loss: 0.00069867
Iteration 8/25 | Loss: 0.00069867
Iteration 9/25 | Loss: 0.00069867
Iteration 10/25 | Loss: 0.00069867
Iteration 11/25 | Loss: 0.00069867
Iteration 12/25 | Loss: 0.00069867
Iteration 13/25 | Loss: 0.00069867
Iteration 14/25 | Loss: 0.00069867
Iteration 15/25 | Loss: 0.00069867
Iteration 16/25 | Loss: 0.00069867
Iteration 17/25 | Loss: 0.00069867
Iteration 18/25 | Loss: 0.00069867
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006986715598031878, 0.0006986715598031878, 0.0006986715598031878, 0.0006986715598031878, 0.0006986715598031878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006986715598031878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069867
Iteration 2/1000 | Loss: 0.00005059
Iteration 3/1000 | Loss: 0.00003116
Iteration 4/1000 | Loss: 0.00002789
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002463
Iteration 7/1000 | Loss: 0.00002385
Iteration 8/1000 | Loss: 0.00002335
Iteration 9/1000 | Loss: 0.00002314
Iteration 10/1000 | Loss: 0.00002295
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002292
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002290
Iteration 15/1000 | Loss: 0.00002289
Iteration 16/1000 | Loss: 0.00002288
Iteration 17/1000 | Loss: 0.00002288
Iteration 18/1000 | Loss: 0.00002288
Iteration 19/1000 | Loss: 0.00002287
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002287
Iteration 22/1000 | Loss: 0.00002287
Iteration 23/1000 | Loss: 0.00002286
Iteration 24/1000 | Loss: 0.00002285
Iteration 25/1000 | Loss: 0.00002285
Iteration 26/1000 | Loss: 0.00002284
Iteration 27/1000 | Loss: 0.00002284
Iteration 28/1000 | Loss: 0.00002283
Iteration 29/1000 | Loss: 0.00002283
Iteration 30/1000 | Loss: 0.00002283
Iteration 31/1000 | Loss: 0.00002283
Iteration 32/1000 | Loss: 0.00002282
Iteration 33/1000 | Loss: 0.00002282
Iteration 34/1000 | Loss: 0.00002281
Iteration 35/1000 | Loss: 0.00002281
Iteration 36/1000 | Loss: 0.00002280
Iteration 37/1000 | Loss: 0.00002280
Iteration 38/1000 | Loss: 0.00002280
Iteration 39/1000 | Loss: 0.00002279
Iteration 40/1000 | Loss: 0.00002279
Iteration 41/1000 | Loss: 0.00002279
Iteration 42/1000 | Loss: 0.00002278
Iteration 43/1000 | Loss: 0.00002278
Iteration 44/1000 | Loss: 0.00002278
Iteration 45/1000 | Loss: 0.00002278
Iteration 46/1000 | Loss: 0.00002277
Iteration 47/1000 | Loss: 0.00002277
Iteration 48/1000 | Loss: 0.00002276
Iteration 49/1000 | Loss: 0.00002276
Iteration 50/1000 | Loss: 0.00002276
Iteration 51/1000 | Loss: 0.00002275
Iteration 52/1000 | Loss: 0.00002275
Iteration 53/1000 | Loss: 0.00002275
Iteration 54/1000 | Loss: 0.00002275
Iteration 55/1000 | Loss: 0.00002274
Iteration 56/1000 | Loss: 0.00002274
Iteration 57/1000 | Loss: 0.00002274
Iteration 58/1000 | Loss: 0.00002274
Iteration 59/1000 | Loss: 0.00002274
Iteration 60/1000 | Loss: 0.00002273
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002273
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002273
Iteration 65/1000 | Loss: 0.00002272
Iteration 66/1000 | Loss: 0.00002272
Iteration 67/1000 | Loss: 0.00002272
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002272
Iteration 71/1000 | Loss: 0.00002272
Iteration 72/1000 | Loss: 0.00002272
Iteration 73/1000 | Loss: 0.00002272
Iteration 74/1000 | Loss: 0.00002272
Iteration 75/1000 | Loss: 0.00002272
Iteration 76/1000 | Loss: 0.00002272
Iteration 77/1000 | Loss: 0.00002272
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002271
Iteration 80/1000 | Loss: 0.00002271
Iteration 81/1000 | Loss: 0.00002271
Iteration 82/1000 | Loss: 0.00002271
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002270
Iteration 85/1000 | Loss: 0.00002270
Iteration 86/1000 | Loss: 0.00002270
Iteration 87/1000 | Loss: 0.00002269
Iteration 88/1000 | Loss: 0.00002269
Iteration 89/1000 | Loss: 0.00002269
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.269222568429541e-05, 2.269222568429541e-05, 2.269222568429541e-05, 2.269222568429541e-05, 2.269222568429541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.269222568429541e-05

Optimization complete. Final v2v error: 4.067226886749268 mm

Highest mean error: 4.398741245269775 mm for frame 110

Lowest mean error: 3.820643424987793 mm for frame 7

Saving results

Total time: 30.036951065063477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00667653
Iteration 2/25 | Loss: 0.00105498
Iteration 3/25 | Loss: 0.00091413
Iteration 4/25 | Loss: 0.00087634
Iteration 5/25 | Loss: 0.00086691
Iteration 6/25 | Loss: 0.00086561
Iteration 7/25 | Loss: 0.00086551
Iteration 8/25 | Loss: 0.00086551
Iteration 9/25 | Loss: 0.00086551
Iteration 10/25 | Loss: 0.00086551
Iteration 11/25 | Loss: 0.00086551
Iteration 12/25 | Loss: 0.00086551
Iteration 13/25 | Loss: 0.00086551
Iteration 14/25 | Loss: 0.00086551
Iteration 15/25 | Loss: 0.00086551
Iteration 16/25 | Loss: 0.00086551
Iteration 17/25 | Loss: 0.00086551
Iteration 18/25 | Loss: 0.00086551
Iteration 19/25 | Loss: 0.00086551
Iteration 20/25 | Loss: 0.00086551
Iteration 21/25 | Loss: 0.00086551
Iteration 22/25 | Loss: 0.00086551
Iteration 23/25 | Loss: 0.00086551
Iteration 24/25 | Loss: 0.00086551
Iteration 25/25 | Loss: 0.00086551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61675477
Iteration 2/25 | Loss: 0.00051427
Iteration 3/25 | Loss: 0.00051427
Iteration 4/25 | Loss: 0.00051427
Iteration 5/25 | Loss: 0.00051427
Iteration 6/25 | Loss: 0.00051427
Iteration 7/25 | Loss: 0.00051427
Iteration 8/25 | Loss: 0.00051427
Iteration 9/25 | Loss: 0.00051427
Iteration 10/25 | Loss: 0.00051427
Iteration 11/25 | Loss: 0.00051427
Iteration 12/25 | Loss: 0.00051427
Iteration 13/25 | Loss: 0.00051427
Iteration 14/25 | Loss: 0.00051427
Iteration 15/25 | Loss: 0.00051427
Iteration 16/25 | Loss: 0.00051427
Iteration 17/25 | Loss: 0.00051427
Iteration 18/25 | Loss: 0.00051427
Iteration 19/25 | Loss: 0.00051427
Iteration 20/25 | Loss: 0.00051427
Iteration 21/25 | Loss: 0.00051427
Iteration 22/25 | Loss: 0.00051427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005142703303135931, 0.0005142703303135931, 0.0005142703303135931, 0.0005142703303135931, 0.0005142703303135931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005142703303135931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051427
Iteration 2/1000 | Loss: 0.00003793
Iteration 3/1000 | Loss: 0.00002836
Iteration 4/1000 | Loss: 0.00002638
Iteration 5/1000 | Loss: 0.00002486
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002339
Iteration 9/1000 | Loss: 0.00002318
Iteration 10/1000 | Loss: 0.00002317
Iteration 11/1000 | Loss: 0.00002317
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002316
Iteration 14/1000 | Loss: 0.00002314
Iteration 15/1000 | Loss: 0.00002310
Iteration 16/1000 | Loss: 0.00002310
Iteration 17/1000 | Loss: 0.00002310
Iteration 18/1000 | Loss: 0.00002309
Iteration 19/1000 | Loss: 0.00002309
Iteration 20/1000 | Loss: 0.00002309
Iteration 21/1000 | Loss: 0.00002309
Iteration 22/1000 | Loss: 0.00002309
Iteration 23/1000 | Loss: 0.00002309
Iteration 24/1000 | Loss: 0.00002309
Iteration 25/1000 | Loss: 0.00002308
Iteration 26/1000 | Loss: 0.00002308
Iteration 27/1000 | Loss: 0.00002308
Iteration 28/1000 | Loss: 0.00002308
Iteration 29/1000 | Loss: 0.00002308
Iteration 30/1000 | Loss: 0.00002308
Iteration 31/1000 | Loss: 0.00002308
Iteration 32/1000 | Loss: 0.00002308
Iteration 33/1000 | Loss: 0.00002308
Iteration 34/1000 | Loss: 0.00002308
Iteration 35/1000 | Loss: 0.00002308
Iteration 36/1000 | Loss: 0.00002308
Iteration 37/1000 | Loss: 0.00002308
Iteration 38/1000 | Loss: 0.00002308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [2.308235525561031e-05, 2.308235525561031e-05, 2.308235525561031e-05, 2.308235525561031e-05, 2.308235525561031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.308235525561031e-05

Optimization complete. Final v2v error: 4.0508832931518555 mm

Highest mean error: 4.365241050720215 mm for frame 89

Lowest mean error: 3.6942174434661865 mm for frame 114

Saving results

Total time: 23.274787664413452
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137677
Iteration 2/25 | Loss: 0.00185984
Iteration 3/25 | Loss: 0.00121317
Iteration 4/25 | Loss: 0.00110857
Iteration 5/25 | Loss: 0.00108745
Iteration 6/25 | Loss: 0.00105667
Iteration 7/25 | Loss: 0.00104016
Iteration 8/25 | Loss: 0.00102802
Iteration 9/25 | Loss: 0.00102118
Iteration 10/25 | Loss: 0.00101696
Iteration 11/25 | Loss: 0.00101414
Iteration 12/25 | Loss: 0.00101357
Iteration 13/25 | Loss: 0.00101328
Iteration 14/25 | Loss: 0.00101309
Iteration 15/25 | Loss: 0.00101287
Iteration 16/25 | Loss: 0.00101260
Iteration 17/25 | Loss: 0.00101170
Iteration 18/25 | Loss: 0.00101085
Iteration 19/25 | Loss: 0.00101074
Iteration 20/25 | Loss: 0.00101071
Iteration 21/25 | Loss: 0.00101071
Iteration 22/25 | Loss: 0.00101070
Iteration 23/25 | Loss: 0.00101070
Iteration 24/25 | Loss: 0.00101070
Iteration 25/25 | Loss: 0.00101070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95832729
Iteration 2/25 | Loss: 0.00129685
Iteration 3/25 | Loss: 0.00129685
Iteration 4/25 | Loss: 0.00129685
Iteration 5/25 | Loss: 0.00129685
Iteration 6/25 | Loss: 0.00129685
Iteration 7/25 | Loss: 0.00129685
Iteration 8/25 | Loss: 0.00129685
Iteration 9/25 | Loss: 0.00129685
Iteration 10/25 | Loss: 0.00129685
Iteration 11/25 | Loss: 0.00129685
Iteration 12/25 | Loss: 0.00129685
Iteration 13/25 | Loss: 0.00129685
Iteration 14/25 | Loss: 0.00129685
Iteration 15/25 | Loss: 0.00129685
Iteration 16/25 | Loss: 0.00129685
Iteration 17/25 | Loss: 0.00129685
Iteration 18/25 | Loss: 0.00129685
Iteration 19/25 | Loss: 0.00129685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001296849688515067, 0.001296849688515067, 0.001296849688515067, 0.001296849688515067, 0.001296849688515067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001296849688515067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129685
Iteration 2/1000 | Loss: 0.00010051
Iteration 3/1000 | Loss: 0.00004784
Iteration 4/1000 | Loss: 0.00006508
Iteration 5/1000 | Loss: 0.00006255
Iteration 6/1000 | Loss: 0.00005817
Iteration 7/1000 | Loss: 0.00004684
Iteration 8/1000 | Loss: 0.00005253
Iteration 9/1000 | Loss: 0.00005798
Iteration 10/1000 | Loss: 0.00004612
Iteration 11/1000 | Loss: 0.00004764
Iteration 12/1000 | Loss: 0.00005800
Iteration 13/1000 | Loss: 0.00005088
Iteration 14/1000 | Loss: 0.00006986
Iteration 15/1000 | Loss: 0.00006216
Iteration 16/1000 | Loss: 0.00005603
Iteration 17/1000 | Loss: 0.00006776
Iteration 18/1000 | Loss: 0.00006291
Iteration 19/1000 | Loss: 0.00005984
Iteration 20/1000 | Loss: 0.00005460
Iteration 21/1000 | Loss: 0.00005888
Iteration 22/1000 | Loss: 0.00007465
Iteration 23/1000 | Loss: 0.00007222
Iteration 24/1000 | Loss: 0.00004043
Iteration 25/1000 | Loss: 0.00005469
Iteration 26/1000 | Loss: 0.00005333
Iteration 27/1000 | Loss: 0.00005825
Iteration 28/1000 | Loss: 0.00008595
Iteration 29/1000 | Loss: 0.00007090
Iteration 30/1000 | Loss: 0.00006925
Iteration 31/1000 | Loss: 0.00006016
Iteration 32/1000 | Loss: 0.00005776
Iteration 33/1000 | Loss: 0.00006643
Iteration 34/1000 | Loss: 0.00006151
Iteration 35/1000 | Loss: 0.00005576
Iteration 36/1000 | Loss: 0.00004187
Iteration 37/1000 | Loss: 0.00005966
Iteration 38/1000 | Loss: 0.00007368
Iteration 39/1000 | Loss: 0.00006016
Iteration 40/1000 | Loss: 0.00006997
Iteration 41/1000 | Loss: 0.00006525
Iteration 42/1000 | Loss: 0.00006890
Iteration 43/1000 | Loss: 0.00006307
Iteration 44/1000 | Loss: 0.00006566
Iteration 45/1000 | Loss: 0.00004512
Iteration 46/1000 | Loss: 0.00007381
Iteration 47/1000 | Loss: 0.00005759
Iteration 48/1000 | Loss: 0.00006961
Iteration 49/1000 | Loss: 0.00007048
Iteration 50/1000 | Loss: 0.00006959
Iteration 51/1000 | Loss: 0.00006592
Iteration 52/1000 | Loss: 0.00006659
Iteration 53/1000 | Loss: 0.00005995
Iteration 54/1000 | Loss: 0.00005849
Iteration 55/1000 | Loss: 0.00006068
Iteration 56/1000 | Loss: 0.00005817
Iteration 57/1000 | Loss: 0.00009771
Iteration 58/1000 | Loss: 0.00005554
Iteration 59/1000 | Loss: 0.00003855
Iteration 60/1000 | Loss: 0.00014795
Iteration 61/1000 | Loss: 0.00007963
Iteration 62/1000 | Loss: 0.00003319
Iteration 63/1000 | Loss: 0.00003199
Iteration 64/1000 | Loss: 0.00003124
Iteration 65/1000 | Loss: 0.00003055
Iteration 66/1000 | Loss: 0.00003008
Iteration 67/1000 | Loss: 0.00002965
Iteration 68/1000 | Loss: 0.00002927
Iteration 69/1000 | Loss: 0.00002899
Iteration 70/1000 | Loss: 0.00002878
Iteration 71/1000 | Loss: 0.00002865
Iteration 72/1000 | Loss: 0.00002858
Iteration 73/1000 | Loss: 0.00002858
Iteration 74/1000 | Loss: 0.00002858
Iteration 75/1000 | Loss: 0.00002858
Iteration 76/1000 | Loss: 0.00002858
Iteration 77/1000 | Loss: 0.00002857
Iteration 78/1000 | Loss: 0.00002857
Iteration 79/1000 | Loss: 0.00002855
Iteration 80/1000 | Loss: 0.00002855
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002855
Iteration 83/1000 | Loss: 0.00002855
Iteration 84/1000 | Loss: 0.00002854
Iteration 85/1000 | Loss: 0.00002854
Iteration 86/1000 | Loss: 0.00002854
Iteration 87/1000 | Loss: 0.00002854
Iteration 88/1000 | Loss: 0.00002854
Iteration 89/1000 | Loss: 0.00002854
Iteration 90/1000 | Loss: 0.00002853
Iteration 91/1000 | Loss: 0.00002853
Iteration 92/1000 | Loss: 0.00002853
Iteration 93/1000 | Loss: 0.00002853
Iteration 94/1000 | Loss: 0.00002852
Iteration 95/1000 | Loss: 0.00002852
Iteration 96/1000 | Loss: 0.00002852
Iteration 97/1000 | Loss: 0.00002852
Iteration 98/1000 | Loss: 0.00002852
Iteration 99/1000 | Loss: 0.00002852
Iteration 100/1000 | Loss: 0.00002852
Iteration 101/1000 | Loss: 0.00002852
Iteration 102/1000 | Loss: 0.00002852
Iteration 103/1000 | Loss: 0.00002852
Iteration 104/1000 | Loss: 0.00002851
Iteration 105/1000 | Loss: 0.00002851
Iteration 106/1000 | Loss: 0.00002851
Iteration 107/1000 | Loss: 0.00002851
Iteration 108/1000 | Loss: 0.00002851
Iteration 109/1000 | Loss: 0.00002851
Iteration 110/1000 | Loss: 0.00002851
Iteration 111/1000 | Loss: 0.00002851
Iteration 112/1000 | Loss: 0.00002851
Iteration 113/1000 | Loss: 0.00002851
Iteration 114/1000 | Loss: 0.00002850
Iteration 115/1000 | Loss: 0.00002850
Iteration 116/1000 | Loss: 0.00002850
Iteration 117/1000 | Loss: 0.00002850
Iteration 118/1000 | Loss: 0.00002850
Iteration 119/1000 | Loss: 0.00002850
Iteration 120/1000 | Loss: 0.00002850
Iteration 121/1000 | Loss: 0.00002850
Iteration 122/1000 | Loss: 0.00002850
Iteration 123/1000 | Loss: 0.00002850
Iteration 124/1000 | Loss: 0.00002850
Iteration 125/1000 | Loss: 0.00002850
Iteration 126/1000 | Loss: 0.00002850
Iteration 127/1000 | Loss: 0.00002850
Iteration 128/1000 | Loss: 0.00002850
Iteration 129/1000 | Loss: 0.00002850
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00002850
Iteration 132/1000 | Loss: 0.00002850
Iteration 133/1000 | Loss: 0.00002850
Iteration 134/1000 | Loss: 0.00002850
Iteration 135/1000 | Loss: 0.00002850
Iteration 136/1000 | Loss: 0.00002850
Iteration 137/1000 | Loss: 0.00002850
Iteration 138/1000 | Loss: 0.00002850
Iteration 139/1000 | Loss: 0.00002850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.8499585823738016e-05, 2.8499585823738016e-05, 2.8499585823738016e-05, 2.8499585823738016e-05, 2.8499585823738016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8499585823738016e-05

Optimization complete. Final v2v error: 4.471768379211426 mm

Highest mean error: 5.788628578186035 mm for frame 49

Lowest mean error: 3.975781202316284 mm for frame 151

Saving results

Total time: 152.43359303474426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982433
Iteration 2/25 | Loss: 0.00153070
Iteration 3/25 | Loss: 0.00105309
Iteration 4/25 | Loss: 0.00095412
Iteration 5/25 | Loss: 0.00094619
Iteration 6/25 | Loss: 0.00094474
Iteration 7/25 | Loss: 0.00094462
Iteration 8/25 | Loss: 0.00094462
Iteration 9/25 | Loss: 0.00094462
Iteration 10/25 | Loss: 0.00094462
Iteration 11/25 | Loss: 0.00094462
Iteration 12/25 | Loss: 0.00094462
Iteration 13/25 | Loss: 0.00094462
Iteration 14/25 | Loss: 0.00094462
Iteration 15/25 | Loss: 0.00094462
Iteration 16/25 | Loss: 0.00094462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000944623607210815, 0.000944623607210815, 0.000944623607210815, 0.000944623607210815, 0.000944623607210815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000944623607210815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46909165
Iteration 2/25 | Loss: 0.00084732
Iteration 3/25 | Loss: 0.00084729
Iteration 4/25 | Loss: 0.00084729
Iteration 5/25 | Loss: 0.00084729
Iteration 6/25 | Loss: 0.00084729
Iteration 7/25 | Loss: 0.00084729
Iteration 8/25 | Loss: 0.00084729
Iteration 9/25 | Loss: 0.00084729
Iteration 10/25 | Loss: 0.00084729
Iteration 11/25 | Loss: 0.00084729
Iteration 12/25 | Loss: 0.00084729
Iteration 13/25 | Loss: 0.00084729
Iteration 14/25 | Loss: 0.00084729
Iteration 15/25 | Loss: 0.00084729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008472856134176254, 0.0008472856134176254, 0.0008472856134176254, 0.0008472856134176254, 0.0008472856134176254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008472856134176254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084729
Iteration 2/1000 | Loss: 0.00005138
Iteration 3/1000 | Loss: 0.00003743
Iteration 4/1000 | Loss: 0.00002855
Iteration 5/1000 | Loss: 0.00002501
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002148
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002049
Iteration 11/1000 | Loss: 0.00002019
Iteration 12/1000 | Loss: 0.00002001
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001915
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001902
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001894
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001889
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001885
Iteration 27/1000 | Loss: 0.00001885
Iteration 28/1000 | Loss: 0.00001885
Iteration 29/1000 | Loss: 0.00001885
Iteration 30/1000 | Loss: 0.00001885
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001879
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00001874
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001873
Iteration 44/1000 | Loss: 0.00001873
Iteration 45/1000 | Loss: 0.00001873
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001873
Iteration 49/1000 | Loss: 0.00001871
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001863
Iteration 62/1000 | Loss: 0.00001863
Iteration 63/1000 | Loss: 0.00001863
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001862
Iteration 72/1000 | Loss: 0.00001862
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001862
Iteration 75/1000 | Loss: 0.00001862
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001859
Iteration 85/1000 | Loss: 0.00001859
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001858
Iteration 88/1000 | Loss: 0.00001858
Iteration 89/1000 | Loss: 0.00001858
Iteration 90/1000 | Loss: 0.00001857
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001857
Iteration 93/1000 | Loss: 0.00001857
Iteration 94/1000 | Loss: 0.00001857
Iteration 95/1000 | Loss: 0.00001857
Iteration 96/1000 | Loss: 0.00001857
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Iteration 100/1000 | Loss: 0.00001856
Iteration 101/1000 | Loss: 0.00001856
Iteration 102/1000 | Loss: 0.00001856
Iteration 103/1000 | Loss: 0.00001856
Iteration 104/1000 | Loss: 0.00001856
Iteration 105/1000 | Loss: 0.00001856
Iteration 106/1000 | Loss: 0.00001856
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001856
Iteration 117/1000 | Loss: 0.00001856
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001856
Iteration 120/1000 | Loss: 0.00001856
Iteration 121/1000 | Loss: 0.00001856
Iteration 122/1000 | Loss: 0.00001856
Iteration 123/1000 | Loss: 0.00001856
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001856
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001856
Iteration 133/1000 | Loss: 0.00001856
Iteration 134/1000 | Loss: 0.00001856
Iteration 135/1000 | Loss: 0.00001856
Iteration 136/1000 | Loss: 0.00001856
Iteration 137/1000 | Loss: 0.00001856
Iteration 138/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.8561648175818846e-05, 1.8561648175818846e-05, 1.8561648175818846e-05, 1.8561648175818846e-05, 1.8561648175818846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8561648175818846e-05

Optimization complete. Final v2v error: 3.6089258193969727 mm

Highest mean error: 3.9132437705993652 mm for frame 8

Lowest mean error: 3.4465103149414062 mm for frame 71

Saving results

Total time: 40.06484007835388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416560
Iteration 2/25 | Loss: 0.00114725
Iteration 3/25 | Loss: 0.00086123
Iteration 4/25 | Loss: 0.00082629
Iteration 5/25 | Loss: 0.00081911
Iteration 6/25 | Loss: 0.00081677
Iteration 7/25 | Loss: 0.00081644
Iteration 8/25 | Loss: 0.00081644
Iteration 9/25 | Loss: 0.00081644
Iteration 10/25 | Loss: 0.00081644
Iteration 11/25 | Loss: 0.00081644
Iteration 12/25 | Loss: 0.00081644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008164445753209293, 0.0008164445753209293, 0.0008164445753209293, 0.0008164445753209293, 0.0008164445753209293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008164445753209293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48401904
Iteration 2/25 | Loss: 0.00058150
Iteration 3/25 | Loss: 0.00058150
Iteration 4/25 | Loss: 0.00058150
Iteration 5/25 | Loss: 0.00058150
Iteration 6/25 | Loss: 0.00058150
Iteration 7/25 | Loss: 0.00058150
Iteration 8/25 | Loss: 0.00058150
Iteration 9/25 | Loss: 0.00058150
Iteration 10/25 | Loss: 0.00058150
Iteration 11/25 | Loss: 0.00058150
Iteration 12/25 | Loss: 0.00058150
Iteration 13/25 | Loss: 0.00058150
Iteration 14/25 | Loss: 0.00058150
Iteration 15/25 | Loss: 0.00058150
Iteration 16/25 | Loss: 0.00058150
Iteration 17/25 | Loss: 0.00058150
Iteration 18/25 | Loss: 0.00058150
Iteration 19/25 | Loss: 0.00058150
Iteration 20/25 | Loss: 0.00058150
Iteration 21/25 | Loss: 0.00058150
Iteration 22/25 | Loss: 0.00058150
Iteration 23/25 | Loss: 0.00058150
Iteration 24/25 | Loss: 0.00058150
Iteration 25/25 | Loss: 0.00058150

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058150
Iteration 2/1000 | Loss: 0.00003651
Iteration 3/1000 | Loss: 0.00002410
Iteration 4/1000 | Loss: 0.00002245
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00002004
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00002002
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00002002
Iteration 17/1000 | Loss: 0.00002002
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001998
Iteration 22/1000 | Loss: 0.00001998
Iteration 23/1000 | Loss: 0.00001997
Iteration 24/1000 | Loss: 0.00001997
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001997
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001997
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001995
Iteration 38/1000 | Loss: 0.00001995
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00001995
Iteration 46/1000 | Loss: 0.00001994
Iteration 47/1000 | Loss: 0.00001994
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001992
Iteration 54/1000 | Loss: 0.00001992
Iteration 55/1000 | Loss: 0.00001991
Iteration 56/1000 | Loss: 0.00001991
Iteration 57/1000 | Loss: 0.00001990
Iteration 58/1000 | Loss: 0.00001990
Iteration 59/1000 | Loss: 0.00001990
Iteration 60/1000 | Loss: 0.00001990
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001987
Iteration 76/1000 | Loss: 0.00001986
Iteration 77/1000 | Loss: 0.00001986
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001984
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001984
Iteration 88/1000 | Loss: 0.00001984
Iteration 89/1000 | Loss: 0.00001984
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001982
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001981
Iteration 107/1000 | Loss: 0.00001981
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001981
Iteration 110/1000 | Loss: 0.00001981
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001980
Iteration 130/1000 | Loss: 0.00001980
Iteration 131/1000 | Loss: 0.00001980
Iteration 132/1000 | Loss: 0.00001980
Iteration 133/1000 | Loss: 0.00001980
Iteration 134/1000 | Loss: 0.00001980
Iteration 135/1000 | Loss: 0.00001980
Iteration 136/1000 | Loss: 0.00001980
Iteration 137/1000 | Loss: 0.00001980
Iteration 138/1000 | Loss: 0.00001980
Iteration 139/1000 | Loss: 0.00001980
Iteration 140/1000 | Loss: 0.00001980
Iteration 141/1000 | Loss: 0.00001980
Iteration 142/1000 | Loss: 0.00001980
Iteration 143/1000 | Loss: 0.00001980
Iteration 144/1000 | Loss: 0.00001980
Iteration 145/1000 | Loss: 0.00001980
Iteration 146/1000 | Loss: 0.00001980
Iteration 147/1000 | Loss: 0.00001980
Iteration 148/1000 | Loss: 0.00001980
Iteration 149/1000 | Loss: 0.00001980
Iteration 150/1000 | Loss: 0.00001980
Iteration 151/1000 | Loss: 0.00001980
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.9803277609753422e-05, 1.9803277609753422e-05, 1.9803277609753422e-05, 1.9803277609753422e-05, 1.9803277609753422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9803277609753422e-05

Optimization complete. Final v2v error: 3.771826982498169 mm

Highest mean error: 4.300233364105225 mm for frame 55

Lowest mean error: 3.1516013145446777 mm for frame 195

Saving results

Total time: 31.82625102996826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840164
Iteration 2/25 | Loss: 0.00127541
Iteration 3/25 | Loss: 0.00097621
Iteration 4/25 | Loss: 0.00091472
Iteration 5/25 | Loss: 0.00089604
Iteration 6/25 | Loss: 0.00088987
Iteration 7/25 | Loss: 0.00088764
Iteration 8/25 | Loss: 0.00088707
Iteration 9/25 | Loss: 0.00088707
Iteration 10/25 | Loss: 0.00088707
Iteration 11/25 | Loss: 0.00088707
Iteration 12/25 | Loss: 0.00088707
Iteration 13/25 | Loss: 0.00088707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008870723540894687, 0.0008870723540894687, 0.0008870723540894687, 0.0008870723540894687, 0.0008870723540894687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008870723540894687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65945756
Iteration 2/25 | Loss: 0.00083071
Iteration 3/25 | Loss: 0.00083070
Iteration 4/25 | Loss: 0.00083070
Iteration 5/25 | Loss: 0.00083070
Iteration 6/25 | Loss: 0.00083070
Iteration 7/25 | Loss: 0.00083070
Iteration 8/25 | Loss: 0.00083070
Iteration 9/25 | Loss: 0.00083070
Iteration 10/25 | Loss: 0.00083070
Iteration 11/25 | Loss: 0.00083070
Iteration 12/25 | Loss: 0.00083070
Iteration 13/25 | Loss: 0.00083070
Iteration 14/25 | Loss: 0.00083070
Iteration 15/25 | Loss: 0.00083070
Iteration 16/25 | Loss: 0.00083070
Iteration 17/25 | Loss: 0.00083070
Iteration 18/25 | Loss: 0.00083070
Iteration 19/25 | Loss: 0.00083070
Iteration 20/25 | Loss: 0.00083070
Iteration 21/25 | Loss: 0.00083070
Iteration 22/25 | Loss: 0.00083070
Iteration 23/25 | Loss: 0.00083070
Iteration 24/25 | Loss: 0.00083070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000830701959785074, 0.000830701959785074, 0.000830701959785074, 0.000830701959785074, 0.000830701959785074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000830701959785074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083070
Iteration 2/1000 | Loss: 0.00005899
Iteration 3/1000 | Loss: 0.00003505
Iteration 4/1000 | Loss: 0.00002660
Iteration 5/1000 | Loss: 0.00002407
Iteration 6/1000 | Loss: 0.00002298
Iteration 7/1000 | Loss: 0.00002206
Iteration 8/1000 | Loss: 0.00002153
Iteration 9/1000 | Loss: 0.00002085
Iteration 10/1000 | Loss: 0.00002045
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00002003
Iteration 16/1000 | Loss: 0.00002002
Iteration 17/1000 | Loss: 0.00001996
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001988
Iteration 27/1000 | Loss: 0.00001986
Iteration 28/1000 | Loss: 0.00001985
Iteration 29/1000 | Loss: 0.00001985
Iteration 30/1000 | Loss: 0.00001984
Iteration 31/1000 | Loss: 0.00001984
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001980
Iteration 34/1000 | Loss: 0.00001979
Iteration 35/1000 | Loss: 0.00001979
Iteration 36/1000 | Loss: 0.00001978
Iteration 37/1000 | Loss: 0.00001978
Iteration 38/1000 | Loss: 0.00001972
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001970
Iteration 45/1000 | Loss: 0.00001970
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001968
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001963
Iteration 76/1000 | Loss: 0.00001963
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001962
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001959
Iteration 107/1000 | Loss: 0.00001959
Iteration 108/1000 | Loss: 0.00001959
Iteration 109/1000 | Loss: 0.00001959
Iteration 110/1000 | Loss: 0.00001959
Iteration 111/1000 | Loss: 0.00001959
Iteration 112/1000 | Loss: 0.00001959
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001959
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001958
Iteration 118/1000 | Loss: 0.00001958
Iteration 119/1000 | Loss: 0.00001958
Iteration 120/1000 | Loss: 0.00001958
Iteration 121/1000 | Loss: 0.00001958
Iteration 122/1000 | Loss: 0.00001958
Iteration 123/1000 | Loss: 0.00001958
Iteration 124/1000 | Loss: 0.00001958
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001958
Iteration 132/1000 | Loss: 0.00001958
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001958
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001958
Iteration 137/1000 | Loss: 0.00001958
Iteration 138/1000 | Loss: 0.00001958
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001958
Iteration 148/1000 | Loss: 0.00001958
Iteration 149/1000 | Loss: 0.00001958
Iteration 150/1000 | Loss: 0.00001958
Iteration 151/1000 | Loss: 0.00001958
Iteration 152/1000 | Loss: 0.00001958
Iteration 153/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.9579276340664364e-05, 1.9579276340664364e-05, 1.9579276340664364e-05, 1.9579276340664364e-05, 1.9579276340664364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9579276340664364e-05

Optimization complete. Final v2v error: 3.731936454772949 mm

Highest mean error: 5.279246807098389 mm for frame 148

Lowest mean error: 2.977604389190674 mm for frame 129

Saving results

Total time: 46.38019061088562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063086
Iteration 2/25 | Loss: 0.00141317
Iteration 3/25 | Loss: 0.00102576
Iteration 4/25 | Loss: 0.00090333
Iteration 5/25 | Loss: 0.00088024
Iteration 6/25 | Loss: 0.00087133
Iteration 7/25 | Loss: 0.00087549
Iteration 8/25 | Loss: 0.00086998
Iteration 9/25 | Loss: 0.00086182
Iteration 10/25 | Loss: 0.00085984
Iteration 11/25 | Loss: 0.00085863
Iteration 12/25 | Loss: 0.00085822
Iteration 13/25 | Loss: 0.00085775
Iteration 14/25 | Loss: 0.00085743
Iteration 15/25 | Loss: 0.00085726
Iteration 16/25 | Loss: 0.00085722
Iteration 17/25 | Loss: 0.00085722
Iteration 18/25 | Loss: 0.00085722
Iteration 19/25 | Loss: 0.00085722
Iteration 20/25 | Loss: 0.00085721
Iteration 21/25 | Loss: 0.00085721
Iteration 22/25 | Loss: 0.00085721
Iteration 23/25 | Loss: 0.00085721
Iteration 24/25 | Loss: 0.00085721
Iteration 25/25 | Loss: 0.00085721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56355250
Iteration 2/25 | Loss: 0.00061993
Iteration 3/25 | Loss: 0.00061992
Iteration 4/25 | Loss: 0.00061992
Iteration 5/25 | Loss: 0.00061992
Iteration 6/25 | Loss: 0.00061992
Iteration 7/25 | Loss: 0.00061992
Iteration 8/25 | Loss: 0.00061992
Iteration 9/25 | Loss: 0.00061992
Iteration 10/25 | Loss: 0.00061992
Iteration 11/25 | Loss: 0.00061992
Iteration 12/25 | Loss: 0.00061992
Iteration 13/25 | Loss: 0.00061992
Iteration 14/25 | Loss: 0.00061992
Iteration 15/25 | Loss: 0.00061992
Iteration 16/25 | Loss: 0.00061992
Iteration 17/25 | Loss: 0.00061992
Iteration 18/25 | Loss: 0.00061992
Iteration 19/25 | Loss: 0.00061992
Iteration 20/25 | Loss: 0.00061992
Iteration 21/25 | Loss: 0.00061992
Iteration 22/25 | Loss: 0.00061992
Iteration 23/25 | Loss: 0.00061992
Iteration 24/25 | Loss: 0.00061992
Iteration 25/25 | Loss: 0.00061992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006199179915711284, 0.0006199179915711284, 0.0006199179915711284, 0.0006199179915711284, 0.0006199179915711284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006199179915711284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061992
Iteration 2/1000 | Loss: 0.00003483
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00002043
Iteration 8/1000 | Loss: 0.00012078
Iteration 9/1000 | Loss: 0.00002434
Iteration 10/1000 | Loss: 0.00002118
Iteration 11/1000 | Loss: 0.00001988
Iteration 12/1000 | Loss: 0.00001898
Iteration 13/1000 | Loss: 0.00001822
Iteration 14/1000 | Loss: 0.00001791
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001779
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001768
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001766
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001764
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001762
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001755
Iteration 33/1000 | Loss: 0.00001755
Iteration 34/1000 | Loss: 0.00001754
Iteration 35/1000 | Loss: 0.00001754
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001749
Iteration 45/1000 | Loss: 0.00001749
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001744
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00001742
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001740
Iteration 66/1000 | Loss: 0.00001740
Iteration 67/1000 | Loss: 0.00001740
Iteration 68/1000 | Loss: 0.00001740
Iteration 69/1000 | Loss: 0.00001739
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001739
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001737
Iteration 78/1000 | Loss: 0.00001737
Iteration 79/1000 | Loss: 0.00001736
Iteration 80/1000 | Loss: 0.00001736
Iteration 81/1000 | Loss: 0.00001736
Iteration 82/1000 | Loss: 0.00001736
Iteration 83/1000 | Loss: 0.00001736
Iteration 84/1000 | Loss: 0.00001735
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001735
Iteration 88/1000 | Loss: 0.00001735
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001733
Iteration 93/1000 | Loss: 0.00001733
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001732
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001732
Iteration 109/1000 | Loss: 0.00001732
Iteration 110/1000 | Loss: 0.00001731
Iteration 111/1000 | Loss: 0.00001731
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001731
Iteration 118/1000 | Loss: 0.00001731
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001731
Iteration 121/1000 | Loss: 0.00001731
Iteration 122/1000 | Loss: 0.00001731
Iteration 123/1000 | Loss: 0.00001731
Iteration 124/1000 | Loss: 0.00001731
Iteration 125/1000 | Loss: 0.00001731
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001730
Iteration 130/1000 | Loss: 0.00001730
Iteration 131/1000 | Loss: 0.00001730
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001730
Iteration 137/1000 | Loss: 0.00001730
Iteration 138/1000 | Loss: 0.00001730
Iteration 139/1000 | Loss: 0.00001730
Iteration 140/1000 | Loss: 0.00001730
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001729
Iteration 144/1000 | Loss: 0.00001729
Iteration 145/1000 | Loss: 0.00001729
Iteration 146/1000 | Loss: 0.00001729
Iteration 147/1000 | Loss: 0.00001729
Iteration 148/1000 | Loss: 0.00001729
Iteration 149/1000 | Loss: 0.00001729
Iteration 150/1000 | Loss: 0.00001729
Iteration 151/1000 | Loss: 0.00001729
Iteration 152/1000 | Loss: 0.00001729
Iteration 153/1000 | Loss: 0.00001729
Iteration 154/1000 | Loss: 0.00001729
Iteration 155/1000 | Loss: 0.00001729
Iteration 156/1000 | Loss: 0.00001729
Iteration 157/1000 | Loss: 0.00001729
Iteration 158/1000 | Loss: 0.00001729
Iteration 159/1000 | Loss: 0.00001729
Iteration 160/1000 | Loss: 0.00001729
Iteration 161/1000 | Loss: 0.00001729
Iteration 162/1000 | Loss: 0.00001729
Iteration 163/1000 | Loss: 0.00001729
Iteration 164/1000 | Loss: 0.00001729
Iteration 165/1000 | Loss: 0.00001729
Iteration 166/1000 | Loss: 0.00001729
Iteration 167/1000 | Loss: 0.00001729
Iteration 168/1000 | Loss: 0.00001729
Iteration 169/1000 | Loss: 0.00001729
Iteration 170/1000 | Loss: 0.00001729
Iteration 171/1000 | Loss: 0.00001729
Iteration 172/1000 | Loss: 0.00001729
Iteration 173/1000 | Loss: 0.00001729
Iteration 174/1000 | Loss: 0.00001729
Iteration 175/1000 | Loss: 0.00001729
Iteration 176/1000 | Loss: 0.00001729
Iteration 177/1000 | Loss: 0.00001729
Iteration 178/1000 | Loss: 0.00001729
Iteration 179/1000 | Loss: 0.00001729
Iteration 180/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.7285534340771846e-05, 1.7285534340771846e-05, 1.7285534340771846e-05, 1.7285534340771846e-05, 1.7285534340771846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7285534340771846e-05

Optimization complete. Final v2v error: 3.61501407623291 mm

Highest mean error: 4.313333034515381 mm for frame 198

Lowest mean error: 3.1547329425811768 mm for frame 122

Saving results

Total time: 67.96101880073547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00672271
Iteration 2/25 | Loss: 0.00095128
Iteration 3/25 | Loss: 0.00083121
Iteration 4/25 | Loss: 0.00081176
Iteration 5/25 | Loss: 0.00080421
Iteration 6/25 | Loss: 0.00080231
Iteration 7/25 | Loss: 0.00080172
Iteration 8/25 | Loss: 0.00080172
Iteration 9/25 | Loss: 0.00080172
Iteration 10/25 | Loss: 0.00080172
Iteration 11/25 | Loss: 0.00080172
Iteration 12/25 | Loss: 0.00080172
Iteration 13/25 | Loss: 0.00080172
Iteration 14/25 | Loss: 0.00080172
Iteration 15/25 | Loss: 0.00080172
Iteration 16/25 | Loss: 0.00080172
Iteration 17/25 | Loss: 0.00080172
Iteration 18/25 | Loss: 0.00080172
Iteration 19/25 | Loss: 0.00080172
Iteration 20/25 | Loss: 0.00080172
Iteration 21/25 | Loss: 0.00080172
Iteration 22/25 | Loss: 0.00080172
Iteration 23/25 | Loss: 0.00080172
Iteration 24/25 | Loss: 0.00080172
Iteration 25/25 | Loss: 0.00080172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47395086
Iteration 2/25 | Loss: 0.00053656
Iteration 3/25 | Loss: 0.00053656
Iteration 4/25 | Loss: 0.00053656
Iteration 5/25 | Loss: 0.00053656
Iteration 6/25 | Loss: 0.00053656
Iteration 7/25 | Loss: 0.00053656
Iteration 8/25 | Loss: 0.00053656
Iteration 9/25 | Loss: 0.00053656
Iteration 10/25 | Loss: 0.00053656
Iteration 11/25 | Loss: 0.00053656
Iteration 12/25 | Loss: 0.00053656
Iteration 13/25 | Loss: 0.00053656
Iteration 14/25 | Loss: 0.00053656
Iteration 15/25 | Loss: 0.00053656
Iteration 16/25 | Loss: 0.00053656
Iteration 17/25 | Loss: 0.00053656
Iteration 18/25 | Loss: 0.00053656
Iteration 19/25 | Loss: 0.00053656
Iteration 20/25 | Loss: 0.00053656
Iteration 21/25 | Loss: 0.00053656
Iteration 22/25 | Loss: 0.00053656
Iteration 23/25 | Loss: 0.00053656
Iteration 24/25 | Loss: 0.00053656
Iteration 25/25 | Loss: 0.00053656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053656
Iteration 2/1000 | Loss: 0.00003240
Iteration 3/1000 | Loss: 0.00002345
Iteration 4/1000 | Loss: 0.00002140
Iteration 5/1000 | Loss: 0.00002010
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001892
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001876
Iteration 11/1000 | Loss: 0.00001875
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001875
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001873
Iteration 17/1000 | Loss: 0.00001873
Iteration 18/1000 | Loss: 0.00001873
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001873
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001870
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001868
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001867
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001867
Iteration 44/1000 | Loss: 0.00001867
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001863
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001862
Iteration 72/1000 | Loss: 0.00001862
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001862
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001859
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001859
Iteration 107/1000 | Loss: 0.00001859
Iteration 108/1000 | Loss: 0.00001859
Iteration 109/1000 | Loss: 0.00001859
Iteration 110/1000 | Loss: 0.00001859
Iteration 111/1000 | Loss: 0.00001859
Iteration 112/1000 | Loss: 0.00001859
Iteration 113/1000 | Loss: 0.00001859
Iteration 114/1000 | Loss: 0.00001859
Iteration 115/1000 | Loss: 0.00001859
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001858
Iteration 120/1000 | Loss: 0.00001858
Iteration 121/1000 | Loss: 0.00001858
Iteration 122/1000 | Loss: 0.00001858
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001858
Iteration 129/1000 | Loss: 0.00001858
Iteration 130/1000 | Loss: 0.00001858
Iteration 131/1000 | Loss: 0.00001858
Iteration 132/1000 | Loss: 0.00001858
Iteration 133/1000 | Loss: 0.00001857
Iteration 134/1000 | Loss: 0.00001857
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001857
Iteration 137/1000 | Loss: 0.00001857
Iteration 138/1000 | Loss: 0.00001857
Iteration 139/1000 | Loss: 0.00001857
Iteration 140/1000 | Loss: 0.00001857
Iteration 141/1000 | Loss: 0.00001856
Iteration 142/1000 | Loss: 0.00001856
Iteration 143/1000 | Loss: 0.00001856
Iteration 144/1000 | Loss: 0.00001856
Iteration 145/1000 | Loss: 0.00001856
Iteration 146/1000 | Loss: 0.00001856
Iteration 147/1000 | Loss: 0.00001856
Iteration 148/1000 | Loss: 0.00001856
Iteration 149/1000 | Loss: 0.00001856
Iteration 150/1000 | Loss: 0.00001856
Iteration 151/1000 | Loss: 0.00001856
Iteration 152/1000 | Loss: 0.00001856
Iteration 153/1000 | Loss: 0.00001856
Iteration 154/1000 | Loss: 0.00001856
Iteration 155/1000 | Loss: 0.00001856
Iteration 156/1000 | Loss: 0.00001856
Iteration 157/1000 | Loss: 0.00001856
Iteration 158/1000 | Loss: 0.00001856
Iteration 159/1000 | Loss: 0.00001856
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.8562246623332612e-05, 1.8562246623332612e-05, 1.8562246623332612e-05, 1.8562246623332612e-05, 1.8562246623332612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8562246623332612e-05

Optimization complete. Final v2v error: 3.6845059394836426 mm

Highest mean error: 4.062590599060059 mm for frame 3

Lowest mean error: 3.3459877967834473 mm for frame 42

Saving results

Total time: 31.718225717544556
