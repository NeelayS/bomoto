Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=254, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14224-14279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626336
Iteration 2/25 | Loss: 0.00296224
Iteration 3/25 | Loss: 0.00283285
Iteration 4/25 | Loss: 0.00282591
Iteration 5/25 | Loss: 0.00282473
Iteration 6/25 | Loss: 0.00282473
Iteration 7/25 | Loss: 0.00282473
Iteration 8/25 | Loss: 0.00282473
Iteration 9/25 | Loss: 0.00282473
Iteration 10/25 | Loss: 0.00282473
Iteration 11/25 | Loss: 0.00282473
Iteration 12/25 | Loss: 0.00282473
Iteration 13/25 | Loss: 0.00282473
Iteration 14/25 | Loss: 0.00282473
Iteration 15/25 | Loss: 0.00282473
Iteration 16/25 | Loss: 0.00282473
Iteration 17/25 | Loss: 0.00282473
Iteration 18/25 | Loss: 0.00282473
Iteration 19/25 | Loss: 0.00282473
Iteration 20/25 | Loss: 0.00282473
Iteration 21/25 | Loss: 0.00282473
Iteration 22/25 | Loss: 0.00282473
Iteration 23/25 | Loss: 0.00282473
Iteration 24/25 | Loss: 0.00282473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0028247316367924213, 0.0028247316367924213, 0.0028247316367924213, 0.0028247316367924213, 0.0028247316367924213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028247316367924213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46597040
Iteration 2/25 | Loss: 0.00393302
Iteration 3/25 | Loss: 0.00393301
Iteration 4/25 | Loss: 0.00393301
Iteration 5/25 | Loss: 0.00393301
Iteration 6/25 | Loss: 0.00393301
Iteration 7/25 | Loss: 0.00393301
Iteration 8/25 | Loss: 0.00393301
Iteration 9/25 | Loss: 0.00393301
Iteration 10/25 | Loss: 0.00393301
Iteration 11/25 | Loss: 0.00393301
Iteration 12/25 | Loss: 0.00393301
Iteration 13/25 | Loss: 0.00393301
Iteration 14/25 | Loss: 0.00393301
Iteration 15/25 | Loss: 0.00393301
Iteration 16/25 | Loss: 0.00393301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00393300736322999, 0.00393300736322999, 0.00393300736322999, 0.00393300736322999, 0.00393300736322999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00393300736322999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00393301
Iteration 2/1000 | Loss: 0.00018331
Iteration 3/1000 | Loss: 0.00011685
Iteration 4/1000 | Loss: 0.00010340
Iteration 5/1000 | Loss: 0.00009688
Iteration 6/1000 | Loss: 0.00009292
Iteration 7/1000 | Loss: 0.00008960
Iteration 8/1000 | Loss: 0.00008744
Iteration 9/1000 | Loss: 0.00008555
Iteration 10/1000 | Loss: 0.00008463
Iteration 11/1000 | Loss: 0.00008372
Iteration 12/1000 | Loss: 0.00008328
Iteration 13/1000 | Loss: 0.00008298
Iteration 14/1000 | Loss: 0.00008275
Iteration 15/1000 | Loss: 0.00008266
Iteration 16/1000 | Loss: 0.00008260
Iteration 17/1000 | Loss: 0.00008256
Iteration 18/1000 | Loss: 0.00008255
Iteration 19/1000 | Loss: 0.00008254
Iteration 20/1000 | Loss: 0.00008253
Iteration 21/1000 | Loss: 0.00008252
Iteration 22/1000 | Loss: 0.00008252
Iteration 23/1000 | Loss: 0.00008252
Iteration 24/1000 | Loss: 0.00008251
Iteration 25/1000 | Loss: 0.00008250
Iteration 26/1000 | Loss: 0.00008250
Iteration 27/1000 | Loss: 0.00008249
Iteration 28/1000 | Loss: 0.00008249
Iteration 29/1000 | Loss: 0.00008249
Iteration 30/1000 | Loss: 0.00008248
Iteration 31/1000 | Loss: 0.00008248
Iteration 32/1000 | Loss: 0.00008247
Iteration 33/1000 | Loss: 0.00008247
Iteration 34/1000 | Loss: 0.00008247
Iteration 35/1000 | Loss: 0.00008247
Iteration 36/1000 | Loss: 0.00008246
Iteration 37/1000 | Loss: 0.00008246
Iteration 38/1000 | Loss: 0.00008246
Iteration 39/1000 | Loss: 0.00008246
Iteration 40/1000 | Loss: 0.00008245
Iteration 41/1000 | Loss: 0.00008245
Iteration 42/1000 | Loss: 0.00008245
Iteration 43/1000 | Loss: 0.00008245
Iteration 44/1000 | Loss: 0.00008245
Iteration 45/1000 | Loss: 0.00008244
Iteration 46/1000 | Loss: 0.00008244
Iteration 47/1000 | Loss: 0.00008244
Iteration 48/1000 | Loss: 0.00008243
Iteration 49/1000 | Loss: 0.00008243
Iteration 50/1000 | Loss: 0.00008243
Iteration 51/1000 | Loss: 0.00008242
Iteration 52/1000 | Loss: 0.00008242
Iteration 53/1000 | Loss: 0.00008242
Iteration 54/1000 | Loss: 0.00008241
Iteration 55/1000 | Loss: 0.00008241
Iteration 56/1000 | Loss: 0.00008241
Iteration 57/1000 | Loss: 0.00008241
Iteration 58/1000 | Loss: 0.00008240
Iteration 59/1000 | Loss: 0.00008240
Iteration 60/1000 | Loss: 0.00008240
Iteration 61/1000 | Loss: 0.00008240
Iteration 62/1000 | Loss: 0.00008239
Iteration 63/1000 | Loss: 0.00008239
Iteration 64/1000 | Loss: 0.00008239
Iteration 65/1000 | Loss: 0.00008238
Iteration 66/1000 | Loss: 0.00008238
Iteration 67/1000 | Loss: 0.00008238
Iteration 68/1000 | Loss: 0.00008238
Iteration 69/1000 | Loss: 0.00008237
Iteration 70/1000 | Loss: 0.00008237
Iteration 71/1000 | Loss: 0.00008237
Iteration 72/1000 | Loss: 0.00008237
Iteration 73/1000 | Loss: 0.00008237
Iteration 74/1000 | Loss: 0.00008237
Iteration 75/1000 | Loss: 0.00008237
Iteration 76/1000 | Loss: 0.00008237
Iteration 77/1000 | Loss: 0.00008237
Iteration 78/1000 | Loss: 0.00008237
Iteration 79/1000 | Loss: 0.00008237
Iteration 80/1000 | Loss: 0.00008237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [8.236966823460534e-05, 8.236966823460534e-05, 8.236966823460534e-05, 8.236966823460534e-05, 8.236966823460534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.236966823460534e-05

Optimization complete. Final v2v error: 8.089624404907227 mm

Highest mean error: 8.340442657470703 mm for frame 211

Lowest mean error: 7.845067501068115 mm for frame 68

Saving results

Total time: 39.99867868423462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623882
Iteration 2/25 | Loss: 0.00137178
Iteration 3/25 | Loss: 0.00129788
Iteration 4/25 | Loss: 0.00128878
Iteration 5/25 | Loss: 0.00128540
Iteration 6/25 | Loss: 0.00128470
Iteration 7/25 | Loss: 0.00128470
Iteration 8/25 | Loss: 0.00128470
Iteration 9/25 | Loss: 0.00128470
Iteration 10/25 | Loss: 0.00128470
Iteration 11/25 | Loss: 0.00128470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012846984900534153, 0.0012846984900534153, 0.0012846984900534153, 0.0012846984900534153, 0.0012846984900534153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012846984900534153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33873940
Iteration 2/25 | Loss: 0.00134070
Iteration 3/25 | Loss: 0.00134070
Iteration 4/25 | Loss: 0.00134070
Iteration 5/25 | Loss: 0.00134070
Iteration 6/25 | Loss: 0.00134070
Iteration 7/25 | Loss: 0.00134070
Iteration 8/25 | Loss: 0.00134070
Iteration 9/25 | Loss: 0.00134070
Iteration 10/25 | Loss: 0.00134070
Iteration 11/25 | Loss: 0.00134070
Iteration 12/25 | Loss: 0.00134070
Iteration 13/25 | Loss: 0.00134070
Iteration 14/25 | Loss: 0.00134070
Iteration 15/25 | Loss: 0.00134070
Iteration 16/25 | Loss: 0.00134070
Iteration 17/25 | Loss: 0.00134070
Iteration 18/25 | Loss: 0.00134070
Iteration 19/25 | Loss: 0.00134070
Iteration 20/25 | Loss: 0.00134070
Iteration 21/25 | Loss: 0.00134070
Iteration 22/25 | Loss: 0.00134070
Iteration 23/25 | Loss: 0.00134070
Iteration 24/25 | Loss: 0.00134070
Iteration 25/25 | Loss: 0.00134070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013406979851424694, 0.0013406979851424694, 0.0013406979851424694, 0.0013406979851424694, 0.0013406979851424694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013406979851424694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134070
Iteration 2/1000 | Loss: 0.00002195
Iteration 3/1000 | Loss: 0.00001538
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001254
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001177
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001120
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001100
Iteration 13/1000 | Loss: 0.00001092
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001069
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001065
Iteration 19/1000 | Loss: 0.00001064
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001060
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001058
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001055
Iteration 35/1000 | Loss: 0.00001055
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001055
Iteration 38/1000 | Loss: 0.00001055
Iteration 39/1000 | Loss: 0.00001055
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001051
Iteration 50/1000 | Loss: 0.00001051
Iteration 51/1000 | Loss: 0.00001051
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001047
Iteration 57/1000 | Loss: 0.00001047
Iteration 58/1000 | Loss: 0.00001046
Iteration 59/1000 | Loss: 0.00001046
Iteration 60/1000 | Loss: 0.00001046
Iteration 61/1000 | Loss: 0.00001046
Iteration 62/1000 | Loss: 0.00001046
Iteration 63/1000 | Loss: 0.00001046
Iteration 64/1000 | Loss: 0.00001045
Iteration 65/1000 | Loss: 0.00001045
Iteration 66/1000 | Loss: 0.00001044
Iteration 67/1000 | Loss: 0.00001044
Iteration 68/1000 | Loss: 0.00001044
Iteration 69/1000 | Loss: 0.00001043
Iteration 70/1000 | Loss: 0.00001043
Iteration 71/1000 | Loss: 0.00001043
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001042
Iteration 74/1000 | Loss: 0.00001042
Iteration 75/1000 | Loss: 0.00001042
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001042
Iteration 78/1000 | Loss: 0.00001042
Iteration 79/1000 | Loss: 0.00001042
Iteration 80/1000 | Loss: 0.00001042
Iteration 81/1000 | Loss: 0.00001042
Iteration 82/1000 | Loss: 0.00001042
Iteration 83/1000 | Loss: 0.00001042
Iteration 84/1000 | Loss: 0.00001042
Iteration 85/1000 | Loss: 0.00001041
Iteration 86/1000 | Loss: 0.00001041
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001039
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001039
Iteration 96/1000 | Loss: 0.00001038
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001038
Iteration 102/1000 | Loss: 0.00001038
Iteration 103/1000 | Loss: 0.00001037
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001037
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001035
Iteration 128/1000 | Loss: 0.00001035
Iteration 129/1000 | Loss: 0.00001035
Iteration 130/1000 | Loss: 0.00001035
Iteration 131/1000 | Loss: 0.00001035
Iteration 132/1000 | Loss: 0.00001035
Iteration 133/1000 | Loss: 0.00001035
Iteration 134/1000 | Loss: 0.00001035
Iteration 135/1000 | Loss: 0.00001035
Iteration 136/1000 | Loss: 0.00001034
Iteration 137/1000 | Loss: 0.00001034
Iteration 138/1000 | Loss: 0.00001034
Iteration 139/1000 | Loss: 0.00001034
Iteration 140/1000 | Loss: 0.00001034
Iteration 141/1000 | Loss: 0.00001034
Iteration 142/1000 | Loss: 0.00001034
Iteration 143/1000 | Loss: 0.00001034
Iteration 144/1000 | Loss: 0.00001034
Iteration 145/1000 | Loss: 0.00001034
Iteration 146/1000 | Loss: 0.00001034
Iteration 147/1000 | Loss: 0.00001034
Iteration 148/1000 | Loss: 0.00001034
Iteration 149/1000 | Loss: 0.00001034
Iteration 150/1000 | Loss: 0.00001034
Iteration 151/1000 | Loss: 0.00001034
Iteration 152/1000 | Loss: 0.00001034
Iteration 153/1000 | Loss: 0.00001034
Iteration 154/1000 | Loss: 0.00001034
Iteration 155/1000 | Loss: 0.00001034
Iteration 156/1000 | Loss: 0.00001034
Iteration 157/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.0338143511035014e-05, 1.0338143511035014e-05, 1.0338143511035014e-05, 1.0338143511035014e-05, 1.0338143511035014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0338143511035014e-05

Optimization complete. Final v2v error: 2.783507823944092 mm

Highest mean error: 3.43635892868042 mm for frame 80

Lowest mean error: 2.6028735637664795 mm for frame 146

Saving results

Total time: 37.908135652542114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995598
Iteration 2/25 | Loss: 0.00311282
Iteration 3/25 | Loss: 0.00244863
Iteration 4/25 | Loss: 0.00192320
Iteration 5/25 | Loss: 0.00200243
Iteration 6/25 | Loss: 0.00171814
Iteration 7/25 | Loss: 0.00168577
Iteration 8/25 | Loss: 0.00162988
Iteration 9/25 | Loss: 0.00156063
Iteration 10/25 | Loss: 0.00150855
Iteration 11/25 | Loss: 0.00147730
Iteration 12/25 | Loss: 0.00146937
Iteration 13/25 | Loss: 0.00144530
Iteration 14/25 | Loss: 0.00143709
Iteration 15/25 | Loss: 0.00145055
Iteration 16/25 | Loss: 0.00147400
Iteration 17/25 | Loss: 0.00144276
Iteration 18/25 | Loss: 0.00142831
Iteration 19/25 | Loss: 0.00143688
Iteration 20/25 | Loss: 0.00143076
Iteration 21/25 | Loss: 0.00143603
Iteration 22/25 | Loss: 0.00141644
Iteration 23/25 | Loss: 0.00141498
Iteration 24/25 | Loss: 0.00141447
Iteration 25/25 | Loss: 0.00141425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37762105
Iteration 2/25 | Loss: 0.00227541
Iteration 3/25 | Loss: 0.00157558
Iteration 4/25 | Loss: 0.00157557
Iteration 5/25 | Loss: 0.00157557
Iteration 6/25 | Loss: 0.00157557
Iteration 7/25 | Loss: 0.00157557
Iteration 8/25 | Loss: 0.00157557
Iteration 9/25 | Loss: 0.00157557
Iteration 10/25 | Loss: 0.00157557
Iteration 11/25 | Loss: 0.00157557
Iteration 12/25 | Loss: 0.00157557
Iteration 13/25 | Loss: 0.00157557
Iteration 14/25 | Loss: 0.00157557
Iteration 15/25 | Loss: 0.00157557
Iteration 16/25 | Loss: 0.00157557
Iteration 17/25 | Loss: 0.00157557
Iteration 18/25 | Loss: 0.00157557
Iteration 19/25 | Loss: 0.00157557
Iteration 20/25 | Loss: 0.00157557
Iteration 21/25 | Loss: 0.00157557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015755676431581378, 0.0015755676431581378, 0.0015755676431581378, 0.0015755676431581378, 0.0015755676431581378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015755676431581378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157557
Iteration 2/1000 | Loss: 0.00042075
Iteration 3/1000 | Loss: 0.00098088
Iteration 4/1000 | Loss: 0.00141331
Iteration 5/1000 | Loss: 0.00009035
Iteration 6/1000 | Loss: 0.00034038
Iteration 7/1000 | Loss: 0.00050394
Iteration 8/1000 | Loss: 0.00070932
Iteration 9/1000 | Loss: 0.00009058
Iteration 10/1000 | Loss: 0.00007757
Iteration 11/1000 | Loss: 0.00009152
Iteration 12/1000 | Loss: 0.00005487
Iteration 13/1000 | Loss: 0.00008302
Iteration 14/1000 | Loss: 0.00023774
Iteration 15/1000 | Loss: 0.00005231
Iteration 16/1000 | Loss: 0.00005012
Iteration 17/1000 | Loss: 0.00012901
Iteration 18/1000 | Loss: 0.00010300
Iteration 19/1000 | Loss: 0.00074412
Iteration 20/1000 | Loss: 0.00022173
Iteration 21/1000 | Loss: 0.00023009
Iteration 22/1000 | Loss: 0.00004753
Iteration 23/1000 | Loss: 0.00028280
Iteration 24/1000 | Loss: 0.00052997
Iteration 25/1000 | Loss: 0.00135373
Iteration 26/1000 | Loss: 0.00141832
Iteration 27/1000 | Loss: 0.00034354
Iteration 28/1000 | Loss: 0.00058540
Iteration 29/1000 | Loss: 0.00052536
Iteration 30/1000 | Loss: 0.00027477
Iteration 31/1000 | Loss: 0.00020802
Iteration 32/1000 | Loss: 0.00014380
Iteration 33/1000 | Loss: 0.00004818
Iteration 34/1000 | Loss: 0.00008004
Iteration 35/1000 | Loss: 0.00029112
Iteration 36/1000 | Loss: 0.00008673
Iteration 37/1000 | Loss: 0.00008659
Iteration 38/1000 | Loss: 0.00013998
Iteration 39/1000 | Loss: 0.00023479
Iteration 40/1000 | Loss: 0.00004275
Iteration 41/1000 | Loss: 0.00008004
Iteration 42/1000 | Loss: 0.00002779
Iteration 43/1000 | Loss: 0.00002684
Iteration 44/1000 | Loss: 0.00002609
Iteration 45/1000 | Loss: 0.00002554
Iteration 46/1000 | Loss: 0.00002516
Iteration 47/1000 | Loss: 0.00002479
Iteration 48/1000 | Loss: 0.00002461
Iteration 49/1000 | Loss: 0.00006365
Iteration 50/1000 | Loss: 0.00002456
Iteration 51/1000 | Loss: 0.00002443
Iteration 52/1000 | Loss: 0.00009205
Iteration 53/1000 | Loss: 0.00002446
Iteration 54/1000 | Loss: 0.00005052
Iteration 55/1000 | Loss: 0.00005558
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00004323
Iteration 58/1000 | Loss: 0.00002428
Iteration 59/1000 | Loss: 0.00002427
Iteration 60/1000 | Loss: 0.00002426
Iteration 61/1000 | Loss: 0.00002426
Iteration 62/1000 | Loss: 0.00002426
Iteration 63/1000 | Loss: 0.00002426
Iteration 64/1000 | Loss: 0.00002426
Iteration 65/1000 | Loss: 0.00002425
Iteration 66/1000 | Loss: 0.00002425
Iteration 67/1000 | Loss: 0.00002425
Iteration 68/1000 | Loss: 0.00002425
Iteration 69/1000 | Loss: 0.00002425
Iteration 70/1000 | Loss: 0.00002425
Iteration 71/1000 | Loss: 0.00002425
Iteration 72/1000 | Loss: 0.00002425
Iteration 73/1000 | Loss: 0.00002424
Iteration 74/1000 | Loss: 0.00002424
Iteration 75/1000 | Loss: 0.00002424
Iteration 76/1000 | Loss: 0.00002424
Iteration 77/1000 | Loss: 0.00002424
Iteration 78/1000 | Loss: 0.00002424
Iteration 79/1000 | Loss: 0.00002424
Iteration 80/1000 | Loss: 0.00002424
Iteration 81/1000 | Loss: 0.00002424
Iteration 82/1000 | Loss: 0.00002423
Iteration 83/1000 | Loss: 0.00002423
Iteration 84/1000 | Loss: 0.00002423
Iteration 85/1000 | Loss: 0.00002423
Iteration 86/1000 | Loss: 0.00002422
Iteration 87/1000 | Loss: 0.00002422
Iteration 88/1000 | Loss: 0.00002422
Iteration 89/1000 | Loss: 0.00002422
Iteration 90/1000 | Loss: 0.00002422
Iteration 91/1000 | Loss: 0.00002421
Iteration 92/1000 | Loss: 0.00002421
Iteration 93/1000 | Loss: 0.00002420
Iteration 94/1000 | Loss: 0.00002420
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00002420
Iteration 97/1000 | Loss: 0.00002420
Iteration 98/1000 | Loss: 0.00002420
Iteration 99/1000 | Loss: 0.00002420
Iteration 100/1000 | Loss: 0.00002420
Iteration 101/1000 | Loss: 0.00002420
Iteration 102/1000 | Loss: 0.00002420
Iteration 103/1000 | Loss: 0.00002420
Iteration 104/1000 | Loss: 0.00002420
Iteration 105/1000 | Loss: 0.00002420
Iteration 106/1000 | Loss: 0.00002420
Iteration 107/1000 | Loss: 0.00002419
Iteration 108/1000 | Loss: 0.00002419
Iteration 109/1000 | Loss: 0.00002419
Iteration 110/1000 | Loss: 0.00002419
Iteration 111/1000 | Loss: 0.00002419
Iteration 112/1000 | Loss: 0.00002419
Iteration 113/1000 | Loss: 0.00002419
Iteration 114/1000 | Loss: 0.00002419
Iteration 115/1000 | Loss: 0.00002419
Iteration 116/1000 | Loss: 0.00002419
Iteration 117/1000 | Loss: 0.00002419
Iteration 118/1000 | Loss: 0.00002418
Iteration 119/1000 | Loss: 0.00002418
Iteration 120/1000 | Loss: 0.00002418
Iteration 121/1000 | Loss: 0.00002418
Iteration 122/1000 | Loss: 0.00002418
Iteration 123/1000 | Loss: 0.00002417
Iteration 124/1000 | Loss: 0.00002417
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002416
Iteration 130/1000 | Loss: 0.00002416
Iteration 131/1000 | Loss: 0.00002416
Iteration 132/1000 | Loss: 0.00006405
Iteration 133/1000 | Loss: 0.00002843
Iteration 134/1000 | Loss: 0.00003238
Iteration 135/1000 | Loss: 0.00002420
Iteration 136/1000 | Loss: 0.00002419
Iteration 137/1000 | Loss: 0.00002419
Iteration 138/1000 | Loss: 0.00002419
Iteration 139/1000 | Loss: 0.00002419
Iteration 140/1000 | Loss: 0.00002419
Iteration 141/1000 | Loss: 0.00002419
Iteration 142/1000 | Loss: 0.00002419
Iteration 143/1000 | Loss: 0.00002419
Iteration 144/1000 | Loss: 0.00002419
Iteration 145/1000 | Loss: 0.00002419
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00002418
Iteration 148/1000 | Loss: 0.00002418
Iteration 149/1000 | Loss: 0.00002417
Iteration 150/1000 | Loss: 0.00002417
Iteration 151/1000 | Loss: 0.00002417
Iteration 152/1000 | Loss: 0.00002417
Iteration 153/1000 | Loss: 0.00002417
Iteration 154/1000 | Loss: 0.00002417
Iteration 155/1000 | Loss: 0.00002416
Iteration 156/1000 | Loss: 0.00002416
Iteration 157/1000 | Loss: 0.00002416
Iteration 158/1000 | Loss: 0.00002416
Iteration 159/1000 | Loss: 0.00002416
Iteration 160/1000 | Loss: 0.00002416
Iteration 161/1000 | Loss: 0.00002416
Iteration 162/1000 | Loss: 0.00002416
Iteration 163/1000 | Loss: 0.00002416
Iteration 164/1000 | Loss: 0.00002416
Iteration 165/1000 | Loss: 0.00002416
Iteration 166/1000 | Loss: 0.00002416
Iteration 167/1000 | Loss: 0.00002415
Iteration 168/1000 | Loss: 0.00003054
Iteration 169/1000 | Loss: 0.00002416
Iteration 170/1000 | Loss: 0.00002414
Iteration 171/1000 | Loss: 0.00002414
Iteration 172/1000 | Loss: 0.00002414
Iteration 173/1000 | Loss: 0.00002414
Iteration 174/1000 | Loss: 0.00002413
Iteration 175/1000 | Loss: 0.00002413
Iteration 176/1000 | Loss: 0.00002413
Iteration 177/1000 | Loss: 0.00002413
Iteration 178/1000 | Loss: 0.00002413
Iteration 179/1000 | Loss: 0.00002413
Iteration 180/1000 | Loss: 0.00002957
Iteration 181/1000 | Loss: 0.00002413
Iteration 182/1000 | Loss: 0.00002413
Iteration 183/1000 | Loss: 0.00002412
Iteration 184/1000 | Loss: 0.00002412
Iteration 185/1000 | Loss: 0.00002412
Iteration 186/1000 | Loss: 0.00002412
Iteration 187/1000 | Loss: 0.00002412
Iteration 188/1000 | Loss: 0.00002412
Iteration 189/1000 | Loss: 0.00002412
Iteration 190/1000 | Loss: 0.00002412
Iteration 191/1000 | Loss: 0.00002412
Iteration 192/1000 | Loss: 0.00002412
Iteration 193/1000 | Loss: 0.00002412
Iteration 194/1000 | Loss: 0.00002412
Iteration 195/1000 | Loss: 0.00002412
Iteration 196/1000 | Loss: 0.00002412
Iteration 197/1000 | Loss: 0.00002412
Iteration 198/1000 | Loss: 0.00002412
Iteration 199/1000 | Loss: 0.00002412
Iteration 200/1000 | Loss: 0.00002412
Iteration 201/1000 | Loss: 0.00002412
Iteration 202/1000 | Loss: 0.00002412
Iteration 203/1000 | Loss: 0.00002412
Iteration 204/1000 | Loss: 0.00002412
Iteration 205/1000 | Loss: 0.00002412
Iteration 206/1000 | Loss: 0.00002412
Iteration 207/1000 | Loss: 0.00002412
Iteration 208/1000 | Loss: 0.00002412
Iteration 209/1000 | Loss: 0.00002412
Iteration 210/1000 | Loss: 0.00002412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.411713830952067e-05, 2.411713830952067e-05, 2.411713830952067e-05, 2.411713830952067e-05, 2.411713830952067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.411713830952067e-05

Optimization complete. Final v2v error: 3.9066500663757324 mm

Highest mean error: 11.276433944702148 mm for frame 199

Lowest mean error: 3.428950786590576 mm for frame 137

Saving results

Total time: 162.10397672653198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524986
Iteration 2/25 | Loss: 0.00164850
Iteration 3/25 | Loss: 0.00142228
Iteration 4/25 | Loss: 0.00139107
Iteration 5/25 | Loss: 0.00138426
Iteration 6/25 | Loss: 0.00138218
Iteration 7/25 | Loss: 0.00138216
Iteration 8/25 | Loss: 0.00138216
Iteration 9/25 | Loss: 0.00138216
Iteration 10/25 | Loss: 0.00138216
Iteration 11/25 | Loss: 0.00138216
Iteration 12/25 | Loss: 0.00138216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013821553438901901, 0.0013821553438901901, 0.0013821553438901901, 0.0013821553438901901, 0.0013821553438901901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013821553438901901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66378355
Iteration 2/25 | Loss: 0.00114621
Iteration 3/25 | Loss: 0.00114621
Iteration 4/25 | Loss: 0.00114621
Iteration 5/25 | Loss: 0.00114621
Iteration 6/25 | Loss: 0.00114621
Iteration 7/25 | Loss: 0.00114621
Iteration 8/25 | Loss: 0.00114621
Iteration 9/25 | Loss: 0.00114621
Iteration 10/25 | Loss: 0.00114621
Iteration 11/25 | Loss: 0.00114621
Iteration 12/25 | Loss: 0.00114621
Iteration 13/25 | Loss: 0.00114621
Iteration 14/25 | Loss: 0.00114621
Iteration 15/25 | Loss: 0.00114621
Iteration 16/25 | Loss: 0.00114621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011462069815024734, 0.0011462069815024734, 0.0011462069815024734, 0.0011462069815024734, 0.0011462069815024734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011462069815024734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114621
Iteration 2/1000 | Loss: 0.00004925
Iteration 3/1000 | Loss: 0.00003122
Iteration 4/1000 | Loss: 0.00002893
Iteration 5/1000 | Loss: 0.00002713
Iteration 6/1000 | Loss: 0.00002595
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00002492
Iteration 9/1000 | Loss: 0.00002456
Iteration 10/1000 | Loss: 0.00002434
Iteration 11/1000 | Loss: 0.00002433
Iteration 12/1000 | Loss: 0.00002431
Iteration 13/1000 | Loss: 0.00002426
Iteration 14/1000 | Loss: 0.00002420
Iteration 15/1000 | Loss: 0.00002420
Iteration 16/1000 | Loss: 0.00002419
Iteration 17/1000 | Loss: 0.00002409
Iteration 18/1000 | Loss: 0.00002405
Iteration 19/1000 | Loss: 0.00002405
Iteration 20/1000 | Loss: 0.00002405
Iteration 21/1000 | Loss: 0.00002405
Iteration 22/1000 | Loss: 0.00002402
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002398
Iteration 25/1000 | Loss: 0.00002398
Iteration 26/1000 | Loss: 0.00002395
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002394
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002392
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002387
Iteration 34/1000 | Loss: 0.00002385
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002385
Iteration 38/1000 | Loss: 0.00002385
Iteration 39/1000 | Loss: 0.00002385
Iteration 40/1000 | Loss: 0.00002385
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002384
Iteration 45/1000 | Loss: 0.00002384
Iteration 46/1000 | Loss: 0.00002383
Iteration 47/1000 | Loss: 0.00002383
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002383
Iteration 50/1000 | Loss: 0.00002383
Iteration 51/1000 | Loss: 0.00002383
Iteration 52/1000 | Loss: 0.00002383
Iteration 53/1000 | Loss: 0.00002383
Iteration 54/1000 | Loss: 0.00002383
Iteration 55/1000 | Loss: 0.00002383
Iteration 56/1000 | Loss: 0.00002383
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002382
Iteration 59/1000 | Loss: 0.00002382
Iteration 60/1000 | Loss: 0.00002382
Iteration 61/1000 | Loss: 0.00002382
Iteration 62/1000 | Loss: 0.00002382
Iteration 63/1000 | Loss: 0.00002382
Iteration 64/1000 | Loss: 0.00002382
Iteration 65/1000 | Loss: 0.00002382
Iteration 66/1000 | Loss: 0.00002381
Iteration 67/1000 | Loss: 0.00002381
Iteration 68/1000 | Loss: 0.00002381
Iteration 69/1000 | Loss: 0.00002381
Iteration 70/1000 | Loss: 0.00002381
Iteration 71/1000 | Loss: 0.00002381
Iteration 72/1000 | Loss: 0.00002381
Iteration 73/1000 | Loss: 0.00002381
Iteration 74/1000 | Loss: 0.00002381
Iteration 75/1000 | Loss: 0.00002381
Iteration 76/1000 | Loss: 0.00002381
Iteration 77/1000 | Loss: 0.00002380
Iteration 78/1000 | Loss: 0.00002380
Iteration 79/1000 | Loss: 0.00002380
Iteration 80/1000 | Loss: 0.00002380
Iteration 81/1000 | Loss: 0.00002380
Iteration 82/1000 | Loss: 0.00002380
Iteration 83/1000 | Loss: 0.00002380
Iteration 84/1000 | Loss: 0.00002380
Iteration 85/1000 | Loss: 0.00002380
Iteration 86/1000 | Loss: 0.00002380
Iteration 87/1000 | Loss: 0.00002380
Iteration 88/1000 | Loss: 0.00002380
Iteration 89/1000 | Loss: 0.00002380
Iteration 90/1000 | Loss: 0.00002380
Iteration 91/1000 | Loss: 0.00002380
Iteration 92/1000 | Loss: 0.00002380
Iteration 93/1000 | Loss: 0.00002380
Iteration 94/1000 | Loss: 0.00002380
Iteration 95/1000 | Loss: 0.00002380
Iteration 96/1000 | Loss: 0.00002379
Iteration 97/1000 | Loss: 0.00002379
Iteration 98/1000 | Loss: 0.00002379
Iteration 99/1000 | Loss: 0.00002379
Iteration 100/1000 | Loss: 0.00002379
Iteration 101/1000 | Loss: 0.00002379
Iteration 102/1000 | Loss: 0.00002379
Iteration 103/1000 | Loss: 0.00002379
Iteration 104/1000 | Loss: 0.00002379
Iteration 105/1000 | Loss: 0.00002379
Iteration 106/1000 | Loss: 0.00002379
Iteration 107/1000 | Loss: 0.00002379
Iteration 108/1000 | Loss: 0.00002379
Iteration 109/1000 | Loss: 0.00002379
Iteration 110/1000 | Loss: 0.00002379
Iteration 111/1000 | Loss: 0.00002379
Iteration 112/1000 | Loss: 0.00002379
Iteration 113/1000 | Loss: 0.00002378
Iteration 114/1000 | Loss: 0.00002378
Iteration 115/1000 | Loss: 0.00002378
Iteration 116/1000 | Loss: 0.00002378
Iteration 117/1000 | Loss: 0.00002378
Iteration 118/1000 | Loss: 0.00002378
Iteration 119/1000 | Loss: 0.00002378
Iteration 120/1000 | Loss: 0.00002378
Iteration 121/1000 | Loss: 0.00002378
Iteration 122/1000 | Loss: 0.00002378
Iteration 123/1000 | Loss: 0.00002378
Iteration 124/1000 | Loss: 0.00002377
Iteration 125/1000 | Loss: 0.00002377
Iteration 126/1000 | Loss: 0.00002377
Iteration 127/1000 | Loss: 0.00002377
Iteration 128/1000 | Loss: 0.00002377
Iteration 129/1000 | Loss: 0.00002377
Iteration 130/1000 | Loss: 0.00002377
Iteration 131/1000 | Loss: 0.00002377
Iteration 132/1000 | Loss: 0.00002377
Iteration 133/1000 | Loss: 0.00002377
Iteration 134/1000 | Loss: 0.00002377
Iteration 135/1000 | Loss: 0.00002377
Iteration 136/1000 | Loss: 0.00002377
Iteration 137/1000 | Loss: 0.00002377
Iteration 138/1000 | Loss: 0.00002377
Iteration 139/1000 | Loss: 0.00002377
Iteration 140/1000 | Loss: 0.00002377
Iteration 141/1000 | Loss: 0.00002377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3770724510541186e-05, 2.3770724510541186e-05, 2.3770724510541186e-05, 2.3770724510541186e-05, 2.3770724510541186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3770724510541186e-05

Optimization complete. Final v2v error: 4.147461414337158 mm

Highest mean error: 4.42245626449585 mm for frame 16

Lowest mean error: 3.8582887649536133 mm for frame 34

Saving results

Total time: 34.254767656326294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598259
Iteration 2/25 | Loss: 0.00167243
Iteration 3/25 | Loss: 0.00146907
Iteration 4/25 | Loss: 0.00145157
Iteration 5/25 | Loss: 0.00145041
Iteration 6/25 | Loss: 0.00145041
Iteration 7/25 | Loss: 0.00145041
Iteration 8/25 | Loss: 0.00145041
Iteration 9/25 | Loss: 0.00145041
Iteration 10/25 | Loss: 0.00145041
Iteration 11/25 | Loss: 0.00145041
Iteration 12/25 | Loss: 0.00145041
Iteration 13/25 | Loss: 0.00145041
Iteration 14/25 | Loss: 0.00145041
Iteration 15/25 | Loss: 0.00145041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014504084829241037, 0.0014504084829241037, 0.0014504084829241037, 0.0014504084829241037, 0.0014504084829241037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014504084829241037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94776827
Iteration 2/25 | Loss: 0.00109885
Iteration 3/25 | Loss: 0.00109884
Iteration 4/25 | Loss: 0.00109884
Iteration 5/25 | Loss: 0.00109884
Iteration 6/25 | Loss: 0.00109884
Iteration 7/25 | Loss: 0.00109884
Iteration 8/25 | Loss: 0.00109884
Iteration 9/25 | Loss: 0.00109884
Iteration 10/25 | Loss: 0.00109884
Iteration 11/25 | Loss: 0.00109884
Iteration 12/25 | Loss: 0.00109884
Iteration 13/25 | Loss: 0.00109884
Iteration 14/25 | Loss: 0.00109884
Iteration 15/25 | Loss: 0.00109884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010988414287567139, 0.0010988414287567139, 0.0010988414287567139, 0.0010988414287567139, 0.0010988414287567139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010988414287567139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109884
Iteration 2/1000 | Loss: 0.00003955
Iteration 3/1000 | Loss: 0.00003062
Iteration 4/1000 | Loss: 0.00002889
Iteration 5/1000 | Loss: 0.00002758
Iteration 6/1000 | Loss: 0.00002688
Iteration 7/1000 | Loss: 0.00002620
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002525
Iteration 10/1000 | Loss: 0.00002502
Iteration 11/1000 | Loss: 0.00002463
Iteration 12/1000 | Loss: 0.00002425
Iteration 13/1000 | Loss: 0.00002404
Iteration 14/1000 | Loss: 0.00002382
Iteration 15/1000 | Loss: 0.00002361
Iteration 16/1000 | Loss: 0.00002360
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00002359
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002355
Iteration 21/1000 | Loss: 0.00002348
Iteration 22/1000 | Loss: 0.00002346
Iteration 23/1000 | Loss: 0.00002346
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002344
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002343
Iteration 28/1000 | Loss: 0.00002343
Iteration 29/1000 | Loss: 0.00002343
Iteration 30/1000 | Loss: 0.00002343
Iteration 31/1000 | Loss: 0.00002342
Iteration 32/1000 | Loss: 0.00002342
Iteration 33/1000 | Loss: 0.00002337
Iteration 34/1000 | Loss: 0.00002337
Iteration 35/1000 | Loss: 0.00002335
Iteration 36/1000 | Loss: 0.00002335
Iteration 37/1000 | Loss: 0.00002335
Iteration 38/1000 | Loss: 0.00002334
Iteration 39/1000 | Loss: 0.00002333
Iteration 40/1000 | Loss: 0.00002333
Iteration 41/1000 | Loss: 0.00002333
Iteration 42/1000 | Loss: 0.00002333
Iteration 43/1000 | Loss: 0.00002333
Iteration 44/1000 | Loss: 0.00002333
Iteration 45/1000 | Loss: 0.00002333
Iteration 46/1000 | Loss: 0.00002333
Iteration 47/1000 | Loss: 0.00002333
Iteration 48/1000 | Loss: 0.00002333
Iteration 49/1000 | Loss: 0.00002330
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002329
Iteration 52/1000 | Loss: 0.00002329
Iteration 53/1000 | Loss: 0.00002328
Iteration 54/1000 | Loss: 0.00002328
Iteration 55/1000 | Loss: 0.00002328
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002328
Iteration 60/1000 | Loss: 0.00002328
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002328
Iteration 63/1000 | Loss: 0.00002328
Iteration 64/1000 | Loss: 0.00002328
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002326
Iteration 70/1000 | Loss: 0.00002326
Iteration 71/1000 | Loss: 0.00002326
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002325
Iteration 74/1000 | Loss: 0.00002325
Iteration 75/1000 | Loss: 0.00002324
Iteration 76/1000 | Loss: 0.00002324
Iteration 77/1000 | Loss: 0.00002324
Iteration 78/1000 | Loss: 0.00002324
Iteration 79/1000 | Loss: 0.00002324
Iteration 80/1000 | Loss: 0.00002323
Iteration 81/1000 | Loss: 0.00002323
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002323
Iteration 84/1000 | Loss: 0.00002323
Iteration 85/1000 | Loss: 0.00002323
Iteration 86/1000 | Loss: 0.00002323
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002322
Iteration 95/1000 | Loss: 0.00002322
Iteration 96/1000 | Loss: 0.00002322
Iteration 97/1000 | Loss: 0.00002322
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00002322
Iteration 100/1000 | Loss: 0.00002322
Iteration 101/1000 | Loss: 0.00002322
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00002322
Iteration 107/1000 | Loss: 0.00002322
Iteration 108/1000 | Loss: 0.00002322
Iteration 109/1000 | Loss: 0.00002322
Iteration 110/1000 | Loss: 0.00002322
Iteration 111/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.3219034119392745e-05, 2.3219034119392745e-05, 2.3219034119392745e-05, 2.3219034119392745e-05, 2.3219034119392745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3219034119392745e-05

Optimization complete. Final v2v error: 4.095740795135498 mm

Highest mean error: 4.162282466888428 mm for frame 55

Lowest mean error: 4.045121669769287 mm for frame 194

Saving results

Total time: 41.359326124191284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821682
Iteration 2/25 | Loss: 0.00145571
Iteration 3/25 | Loss: 0.00131925
Iteration 4/25 | Loss: 0.00128737
Iteration 5/25 | Loss: 0.00128642
Iteration 6/25 | Loss: 0.00128053
Iteration 7/25 | Loss: 0.00128086
Iteration 8/25 | Loss: 0.00128034
Iteration 9/25 | Loss: 0.00127941
Iteration 10/25 | Loss: 0.00127934
Iteration 11/25 | Loss: 0.00127933
Iteration 12/25 | Loss: 0.00127931
Iteration 13/25 | Loss: 0.00127930
Iteration 14/25 | Loss: 0.00127930
Iteration 15/25 | Loss: 0.00127930
Iteration 16/25 | Loss: 0.00127930
Iteration 17/25 | Loss: 0.00127930
Iteration 18/25 | Loss: 0.00127930
Iteration 19/25 | Loss: 0.00127930
Iteration 20/25 | Loss: 0.00127930
Iteration 21/25 | Loss: 0.00127930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012793017085641623, 0.0012793017085641623, 0.0012793017085641623, 0.0012793017085641623, 0.0012793017085641623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012793017085641623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67359662
Iteration 2/25 | Loss: 0.00137614
Iteration 3/25 | Loss: 0.00136687
Iteration 4/25 | Loss: 0.00136687
Iteration 5/25 | Loss: 0.00136687
Iteration 6/25 | Loss: 0.00136686
Iteration 7/25 | Loss: 0.00136686
Iteration 8/25 | Loss: 0.00136686
Iteration 9/25 | Loss: 0.00136686
Iteration 10/25 | Loss: 0.00136686
Iteration 11/25 | Loss: 0.00136686
Iteration 12/25 | Loss: 0.00136686
Iteration 13/25 | Loss: 0.00136686
Iteration 14/25 | Loss: 0.00136686
Iteration 15/25 | Loss: 0.00136686
Iteration 16/25 | Loss: 0.00136686
Iteration 17/25 | Loss: 0.00136686
Iteration 18/25 | Loss: 0.00136686
Iteration 19/25 | Loss: 0.00136686
Iteration 20/25 | Loss: 0.00136686
Iteration 21/25 | Loss: 0.00136686
Iteration 22/25 | Loss: 0.00136686
Iteration 23/25 | Loss: 0.00136686
Iteration 24/25 | Loss: 0.00136686
Iteration 25/25 | Loss: 0.00136686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136686
Iteration 2/1000 | Loss: 0.00004972
Iteration 3/1000 | Loss: 0.00002273
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00002408
Iteration 13/1000 | Loss: 0.00007181
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001180
Iteration 16/1000 | Loss: 0.00001180
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001180
Iteration 19/1000 | Loss: 0.00001171
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001157
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001151
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001140
Iteration 39/1000 | Loss: 0.00001140
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001132
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001132
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001132
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.1317153621348552e-05, 1.1317153621348552e-05, 1.1317153621348552e-05, 1.1317153621348552e-05, 1.1317153621348552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1317153621348552e-05

Optimization complete. Final v2v error: 2.9268176555633545 mm

Highest mean error: 3.2787606716156006 mm for frame 39

Lowest mean error: 2.7012245655059814 mm for frame 8

Saving results

Total time: 52.437347173690796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419702
Iteration 2/25 | Loss: 0.00142744
Iteration 3/25 | Loss: 0.00134304
Iteration 4/25 | Loss: 0.00132984
Iteration 5/25 | Loss: 0.00132561
Iteration 6/25 | Loss: 0.00132511
Iteration 7/25 | Loss: 0.00132511
Iteration 8/25 | Loss: 0.00132511
Iteration 9/25 | Loss: 0.00132511
Iteration 10/25 | Loss: 0.00132511
Iteration 11/25 | Loss: 0.00132511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013251138152554631, 0.0013251138152554631, 0.0013251138152554631, 0.0013251138152554631, 0.0013251138152554631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013251138152554631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34021950
Iteration 2/25 | Loss: 0.00139093
Iteration 3/25 | Loss: 0.00139093
Iteration 4/25 | Loss: 0.00139093
Iteration 5/25 | Loss: 0.00139093
Iteration 6/25 | Loss: 0.00139093
Iteration 7/25 | Loss: 0.00139093
Iteration 8/25 | Loss: 0.00139093
Iteration 9/25 | Loss: 0.00139093
Iteration 10/25 | Loss: 0.00139093
Iteration 11/25 | Loss: 0.00139093
Iteration 12/25 | Loss: 0.00139093
Iteration 13/25 | Loss: 0.00139093
Iteration 14/25 | Loss: 0.00139093
Iteration 15/25 | Loss: 0.00139093
Iteration 16/25 | Loss: 0.00139093
Iteration 17/25 | Loss: 0.00139093
Iteration 18/25 | Loss: 0.00139093
Iteration 19/25 | Loss: 0.00139093
Iteration 20/25 | Loss: 0.00139093
Iteration 21/25 | Loss: 0.00139093
Iteration 22/25 | Loss: 0.00139093
Iteration 23/25 | Loss: 0.00139093
Iteration 24/25 | Loss: 0.00139093
Iteration 25/25 | Loss: 0.00139093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139093
Iteration 2/1000 | Loss: 0.00003378
Iteration 3/1000 | Loss: 0.00002634
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002158
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001926
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001789
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001782
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001781
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001780
Iteration 23/1000 | Loss: 0.00001780
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001771
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001747
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001744
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001740
Iteration 87/1000 | Loss: 0.00001740
Iteration 88/1000 | Loss: 0.00001740
Iteration 89/1000 | Loss: 0.00001740
Iteration 90/1000 | Loss: 0.00001740
Iteration 91/1000 | Loss: 0.00001739
Iteration 92/1000 | Loss: 0.00001739
Iteration 93/1000 | Loss: 0.00001739
Iteration 94/1000 | Loss: 0.00001739
Iteration 95/1000 | Loss: 0.00001739
Iteration 96/1000 | Loss: 0.00001739
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001738
Iteration 99/1000 | Loss: 0.00001738
Iteration 100/1000 | Loss: 0.00001738
Iteration 101/1000 | Loss: 0.00001738
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001737
Iteration 106/1000 | Loss: 0.00001737
Iteration 107/1000 | Loss: 0.00001737
Iteration 108/1000 | Loss: 0.00001737
Iteration 109/1000 | Loss: 0.00001737
Iteration 110/1000 | Loss: 0.00001737
Iteration 111/1000 | Loss: 0.00001737
Iteration 112/1000 | Loss: 0.00001737
Iteration 113/1000 | Loss: 0.00001737
Iteration 114/1000 | Loss: 0.00001737
Iteration 115/1000 | Loss: 0.00001737
Iteration 116/1000 | Loss: 0.00001737
Iteration 117/1000 | Loss: 0.00001737
Iteration 118/1000 | Loss: 0.00001737
Iteration 119/1000 | Loss: 0.00001736
Iteration 120/1000 | Loss: 0.00001736
Iteration 121/1000 | Loss: 0.00001736
Iteration 122/1000 | Loss: 0.00001736
Iteration 123/1000 | Loss: 0.00001736
Iteration 124/1000 | Loss: 0.00001736
Iteration 125/1000 | Loss: 0.00001736
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001736
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001736
Iteration 139/1000 | Loss: 0.00001736
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001735
Iteration 148/1000 | Loss: 0.00001735
Iteration 149/1000 | Loss: 0.00001735
Iteration 150/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.7353382645524107e-05, 1.7353382645524107e-05, 1.7353382645524107e-05, 1.7353382645524107e-05, 1.7353382645524107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7353382645524107e-05

Optimization complete. Final v2v error: 3.577376365661621 mm

Highest mean error: 4.0048418045043945 mm for frame 14

Lowest mean error: 3.3836286067962646 mm for frame 0

Saving results

Total time: 41.1943883895874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819182
Iteration 2/25 | Loss: 0.00160999
Iteration 3/25 | Loss: 0.00134706
Iteration 4/25 | Loss: 0.00133177
Iteration 5/25 | Loss: 0.00133011
Iteration 6/25 | Loss: 0.00133011
Iteration 7/25 | Loss: 0.00133011
Iteration 8/25 | Loss: 0.00133011
Iteration 9/25 | Loss: 0.00133011
Iteration 10/25 | Loss: 0.00133011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00133011385332793, 0.00133011385332793, 0.00133011385332793, 0.00133011385332793, 0.00133011385332793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133011385332793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29925942
Iteration 2/25 | Loss: 0.00115860
Iteration 3/25 | Loss: 0.00115858
Iteration 4/25 | Loss: 0.00115858
Iteration 5/25 | Loss: 0.00115858
Iteration 6/25 | Loss: 0.00115858
Iteration 7/25 | Loss: 0.00115858
Iteration 8/25 | Loss: 0.00115858
Iteration 9/25 | Loss: 0.00115858
Iteration 10/25 | Loss: 0.00115858
Iteration 11/25 | Loss: 0.00115858
Iteration 12/25 | Loss: 0.00115858
Iteration 13/25 | Loss: 0.00115858
Iteration 14/25 | Loss: 0.00115858
Iteration 15/25 | Loss: 0.00115858
Iteration 16/25 | Loss: 0.00115858
Iteration 17/25 | Loss: 0.00115858
Iteration 18/25 | Loss: 0.00115858
Iteration 19/25 | Loss: 0.00115858
Iteration 20/25 | Loss: 0.00115858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011585801839828491, 0.0011585801839828491, 0.0011585801839828491, 0.0011585801839828491, 0.0011585801839828491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011585801839828491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115858
Iteration 2/1000 | Loss: 0.00002303
Iteration 3/1000 | Loss: 0.00001763
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001549
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001448
Iteration 9/1000 | Loss: 0.00001423
Iteration 10/1000 | Loss: 0.00001415
Iteration 11/1000 | Loss: 0.00001411
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001380
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001363
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001362
Iteration 32/1000 | Loss: 0.00001359
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001356
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001345
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001341
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001330
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001315
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Iteration 166/1000 | Loss: 0.00001313
Iteration 167/1000 | Loss: 0.00001313
Iteration 168/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.312757649429841e-05, 1.312757649429841e-05, 1.312757649429841e-05, 1.312757649429841e-05, 1.312757649429841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.312757649429841e-05

Optimization complete. Final v2v error: 3.125262498855591 mm

Highest mean error: 3.4688899517059326 mm for frame 115

Lowest mean error: 2.6605377197265625 mm for frame 19

Saving results

Total time: 43.93153214454651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408099
Iteration 2/25 | Loss: 0.00140940
Iteration 3/25 | Loss: 0.00133392
Iteration 4/25 | Loss: 0.00132563
Iteration 5/25 | Loss: 0.00132312
Iteration 6/25 | Loss: 0.00132243
Iteration 7/25 | Loss: 0.00132234
Iteration 8/25 | Loss: 0.00132234
Iteration 9/25 | Loss: 0.00132234
Iteration 10/25 | Loss: 0.00132234
Iteration 11/25 | Loss: 0.00132234
Iteration 12/25 | Loss: 0.00132234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013223405694589019, 0.0013223405694589019, 0.0013223405694589019, 0.0013223405694589019, 0.0013223405694589019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013223405694589019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39355981
Iteration 2/25 | Loss: 0.00143862
Iteration 3/25 | Loss: 0.00143862
Iteration 4/25 | Loss: 0.00143862
Iteration 5/25 | Loss: 0.00143862
Iteration 6/25 | Loss: 0.00143862
Iteration 7/25 | Loss: 0.00143862
Iteration 8/25 | Loss: 0.00143862
Iteration 9/25 | Loss: 0.00143862
Iteration 10/25 | Loss: 0.00143862
Iteration 11/25 | Loss: 0.00143862
Iteration 12/25 | Loss: 0.00143862
Iteration 13/25 | Loss: 0.00143862
Iteration 14/25 | Loss: 0.00143862
Iteration 15/25 | Loss: 0.00143862
Iteration 16/25 | Loss: 0.00143862
Iteration 17/25 | Loss: 0.00143862
Iteration 18/25 | Loss: 0.00143862
Iteration 19/25 | Loss: 0.00143862
Iteration 20/25 | Loss: 0.00143862
Iteration 21/25 | Loss: 0.00143862
Iteration 22/25 | Loss: 0.00143862
Iteration 23/25 | Loss: 0.00143862
Iteration 24/25 | Loss: 0.00143862
Iteration 25/25 | Loss: 0.00143862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143862
Iteration 2/1000 | Loss: 0.00002818
Iteration 3/1000 | Loss: 0.00001877
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001595
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001403
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001361
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001359
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001358
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001355
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001354
Iteration 46/1000 | Loss: 0.00001354
Iteration 47/1000 | Loss: 0.00001353
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001351
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001350
Iteration 57/1000 | Loss: 0.00001350
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001345
Iteration 78/1000 | Loss: 0.00001345
Iteration 79/1000 | Loss: 0.00001345
Iteration 80/1000 | Loss: 0.00001345
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001344
Iteration 85/1000 | Loss: 0.00001344
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001342
Iteration 96/1000 | Loss: 0.00001342
Iteration 97/1000 | Loss: 0.00001342
Iteration 98/1000 | Loss: 0.00001342
Iteration 99/1000 | Loss: 0.00001341
Iteration 100/1000 | Loss: 0.00001341
Iteration 101/1000 | Loss: 0.00001341
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001340
Iteration 104/1000 | Loss: 0.00001340
Iteration 105/1000 | Loss: 0.00001340
Iteration 106/1000 | Loss: 0.00001339
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001337
Iteration 132/1000 | Loss: 0.00001337
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001336
Iteration 136/1000 | Loss: 0.00001336
Iteration 137/1000 | Loss: 0.00001336
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001335
Iteration 143/1000 | Loss: 0.00001335
Iteration 144/1000 | Loss: 0.00001335
Iteration 145/1000 | Loss: 0.00001335
Iteration 146/1000 | Loss: 0.00001335
Iteration 147/1000 | Loss: 0.00001335
Iteration 148/1000 | Loss: 0.00001335
Iteration 149/1000 | Loss: 0.00001335
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001335
Iteration 152/1000 | Loss: 0.00001335
Iteration 153/1000 | Loss: 0.00001335
Iteration 154/1000 | Loss: 0.00001335
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001335
Iteration 157/1000 | Loss: 0.00001335
Iteration 158/1000 | Loss: 0.00001334
Iteration 159/1000 | Loss: 0.00001334
Iteration 160/1000 | Loss: 0.00001334
Iteration 161/1000 | Loss: 0.00001334
Iteration 162/1000 | Loss: 0.00001334
Iteration 163/1000 | Loss: 0.00001334
Iteration 164/1000 | Loss: 0.00001334
Iteration 165/1000 | Loss: 0.00001334
Iteration 166/1000 | Loss: 0.00001334
Iteration 167/1000 | Loss: 0.00001334
Iteration 168/1000 | Loss: 0.00001334
Iteration 169/1000 | Loss: 0.00001334
Iteration 170/1000 | Loss: 0.00001334
Iteration 171/1000 | Loss: 0.00001333
Iteration 172/1000 | Loss: 0.00001333
Iteration 173/1000 | Loss: 0.00001333
Iteration 174/1000 | Loss: 0.00001333
Iteration 175/1000 | Loss: 0.00001333
Iteration 176/1000 | Loss: 0.00001333
Iteration 177/1000 | Loss: 0.00001333
Iteration 178/1000 | Loss: 0.00001333
Iteration 179/1000 | Loss: 0.00001333
Iteration 180/1000 | Loss: 0.00001333
Iteration 181/1000 | Loss: 0.00001333
Iteration 182/1000 | Loss: 0.00001333
Iteration 183/1000 | Loss: 0.00001333
Iteration 184/1000 | Loss: 0.00001333
Iteration 185/1000 | Loss: 0.00001333
Iteration 186/1000 | Loss: 0.00001333
Iteration 187/1000 | Loss: 0.00001333
Iteration 188/1000 | Loss: 0.00001333
Iteration 189/1000 | Loss: 0.00001333
Iteration 190/1000 | Loss: 0.00001332
Iteration 191/1000 | Loss: 0.00001332
Iteration 192/1000 | Loss: 0.00001332
Iteration 193/1000 | Loss: 0.00001332
Iteration 194/1000 | Loss: 0.00001332
Iteration 195/1000 | Loss: 0.00001332
Iteration 196/1000 | Loss: 0.00001332
Iteration 197/1000 | Loss: 0.00001332
Iteration 198/1000 | Loss: 0.00001332
Iteration 199/1000 | Loss: 0.00001332
Iteration 200/1000 | Loss: 0.00001332
Iteration 201/1000 | Loss: 0.00001331
Iteration 202/1000 | Loss: 0.00001331
Iteration 203/1000 | Loss: 0.00001331
Iteration 204/1000 | Loss: 0.00001331
Iteration 205/1000 | Loss: 0.00001331
Iteration 206/1000 | Loss: 0.00001331
Iteration 207/1000 | Loss: 0.00001331
Iteration 208/1000 | Loss: 0.00001331
Iteration 209/1000 | Loss: 0.00001331
Iteration 210/1000 | Loss: 0.00001331
Iteration 211/1000 | Loss: 0.00001331
Iteration 212/1000 | Loss: 0.00001331
Iteration 213/1000 | Loss: 0.00001331
Iteration 214/1000 | Loss: 0.00001331
Iteration 215/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.3313345334609039e-05, 1.3313345334609039e-05, 1.3313345334609039e-05, 1.3313345334609039e-05, 1.3313345334609039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3313345334609039e-05

Optimization complete. Final v2v error: 3.11102032661438 mm

Highest mean error: 3.9677200317382812 mm for frame 44

Lowest mean error: 2.851640462875366 mm for frame 100

Saving results

Total time: 43.088446378707886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010057
Iteration 2/25 | Loss: 0.01010057
Iteration 3/25 | Loss: 0.01010056
Iteration 4/25 | Loss: 0.01010056
Iteration 5/25 | Loss: 0.01010056
Iteration 6/25 | Loss: 0.01010056
Iteration 7/25 | Loss: 0.01010056
Iteration 8/25 | Loss: 0.01010055
Iteration 9/25 | Loss: 0.01010055
Iteration 10/25 | Loss: 0.01010055
Iteration 11/25 | Loss: 0.01010055
Iteration 12/25 | Loss: 0.01010055
Iteration 13/25 | Loss: 0.00195050
Iteration 14/25 | Loss: 0.00163703
Iteration 15/25 | Loss: 0.00148673
Iteration 16/25 | Loss: 0.00145660
Iteration 17/25 | Loss: 0.00141583
Iteration 18/25 | Loss: 0.00141480
Iteration 19/25 | Loss: 0.00140558
Iteration 20/25 | Loss: 0.00139907
Iteration 21/25 | Loss: 0.00139909
Iteration 22/25 | Loss: 0.00141240
Iteration 23/25 | Loss: 0.00139248
Iteration 24/25 | Loss: 0.00139419
Iteration 25/25 | Loss: 0.00137758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31147945
Iteration 2/25 | Loss: 0.00222931
Iteration 3/25 | Loss: 0.00189767
Iteration 4/25 | Loss: 0.00189767
Iteration 5/25 | Loss: 0.00189766
Iteration 6/25 | Loss: 0.00189766
Iteration 7/25 | Loss: 0.00189766
Iteration 8/25 | Loss: 0.00189766
Iteration 9/25 | Loss: 0.00189766
Iteration 10/25 | Loss: 0.00189766
Iteration 11/25 | Loss: 0.00189766
Iteration 12/25 | Loss: 0.00189766
Iteration 13/25 | Loss: 0.00189766
Iteration 14/25 | Loss: 0.00189766
Iteration 15/25 | Loss: 0.00189766
Iteration 16/25 | Loss: 0.00189766
Iteration 17/25 | Loss: 0.00189766
Iteration 18/25 | Loss: 0.00189766
Iteration 19/25 | Loss: 0.00189766
Iteration 20/25 | Loss: 0.00189766
Iteration 21/25 | Loss: 0.00189766
Iteration 22/25 | Loss: 0.00189766
Iteration 23/25 | Loss: 0.00189766
Iteration 24/25 | Loss: 0.00189766
Iteration 25/25 | Loss: 0.00189766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189766
Iteration 2/1000 | Loss: 0.00072795
Iteration 3/1000 | Loss: 0.00041067
Iteration 4/1000 | Loss: 0.00023059
Iteration 5/1000 | Loss: 0.00040671
Iteration 6/1000 | Loss: 0.00010580
Iteration 7/1000 | Loss: 0.00069120
Iteration 8/1000 | Loss: 0.00038100
Iteration 9/1000 | Loss: 0.00021626
Iteration 10/1000 | Loss: 0.00006882
Iteration 11/1000 | Loss: 0.00022539
Iteration 12/1000 | Loss: 0.00012700
Iteration 13/1000 | Loss: 0.00014245
Iteration 14/1000 | Loss: 0.00016981
Iteration 15/1000 | Loss: 0.00016224
Iteration 16/1000 | Loss: 0.00014747
Iteration 17/1000 | Loss: 0.00021845
Iteration 18/1000 | Loss: 0.00015529
Iteration 19/1000 | Loss: 0.00013500
Iteration 20/1000 | Loss: 0.00015015
Iteration 21/1000 | Loss: 0.00012691
Iteration 22/1000 | Loss: 0.00007803
Iteration 23/1000 | Loss: 0.00034282
Iteration 24/1000 | Loss: 0.00028799
Iteration 25/1000 | Loss: 0.00021142
Iteration 26/1000 | Loss: 0.00023843
Iteration 27/1000 | Loss: 0.00019188
Iteration 28/1000 | Loss: 0.00021508
Iteration 29/1000 | Loss: 0.00008711
Iteration 30/1000 | Loss: 0.00017630
Iteration 31/1000 | Loss: 0.00006270
Iteration 32/1000 | Loss: 0.00006363
Iteration 33/1000 | Loss: 0.00004589
Iteration 34/1000 | Loss: 0.00007797
Iteration 35/1000 | Loss: 0.00013486
Iteration 36/1000 | Loss: 0.00018540
Iteration 37/1000 | Loss: 0.00004819
Iteration 38/1000 | Loss: 0.00008408
Iteration 39/1000 | Loss: 0.00007775
Iteration 40/1000 | Loss: 0.00004550
Iteration 41/1000 | Loss: 0.00004351
Iteration 42/1000 | Loss: 0.00007825
Iteration 43/1000 | Loss: 0.00005316
Iteration 44/1000 | Loss: 0.00013322
Iteration 45/1000 | Loss: 0.00006540
Iteration 46/1000 | Loss: 0.00022505
Iteration 47/1000 | Loss: 0.00027878
Iteration 48/1000 | Loss: 0.00022394
Iteration 49/1000 | Loss: 0.00020299
Iteration 50/1000 | Loss: 0.00021995
Iteration 51/1000 | Loss: 0.00006310
Iteration 52/1000 | Loss: 0.00007443
Iteration 53/1000 | Loss: 0.00005098
Iteration 54/1000 | Loss: 0.00012964
Iteration 55/1000 | Loss: 0.00005225
Iteration 56/1000 | Loss: 0.00005181
Iteration 57/1000 | Loss: 0.00005765
Iteration 58/1000 | Loss: 0.00004165
Iteration 59/1000 | Loss: 0.00004856
Iteration 60/1000 | Loss: 0.00003969
Iteration 61/1000 | Loss: 0.00004919
Iteration 62/1000 | Loss: 0.00009220
Iteration 63/1000 | Loss: 0.00010784
Iteration 64/1000 | Loss: 0.00008709
Iteration 65/1000 | Loss: 0.00005882
Iteration 66/1000 | Loss: 0.00007066
Iteration 67/1000 | Loss: 0.00003911
Iteration 68/1000 | Loss: 0.00003898
Iteration 69/1000 | Loss: 0.00003895
Iteration 70/1000 | Loss: 0.00003891
Iteration 71/1000 | Loss: 0.00003891
Iteration 72/1000 | Loss: 0.00005582
Iteration 73/1000 | Loss: 0.00004030
Iteration 74/1000 | Loss: 0.00005387
Iteration 75/1000 | Loss: 0.00010242
Iteration 76/1000 | Loss: 0.00007954
Iteration 77/1000 | Loss: 0.00007825
Iteration 78/1000 | Loss: 0.00004026
Iteration 79/1000 | Loss: 0.00003887
Iteration 80/1000 | Loss: 0.00004026
Iteration 81/1000 | Loss: 0.00004088
Iteration 82/1000 | Loss: 0.00003873
Iteration 83/1000 | Loss: 0.00003873
Iteration 84/1000 | Loss: 0.00003873
Iteration 85/1000 | Loss: 0.00003873
Iteration 86/1000 | Loss: 0.00004176
Iteration 87/1000 | Loss: 0.00003869
Iteration 88/1000 | Loss: 0.00003869
Iteration 89/1000 | Loss: 0.00003867
Iteration 90/1000 | Loss: 0.00003867
Iteration 91/1000 | Loss: 0.00004203
Iteration 92/1000 | Loss: 0.00003857
Iteration 93/1000 | Loss: 0.00003856
Iteration 94/1000 | Loss: 0.00003856
Iteration 95/1000 | Loss: 0.00003855
Iteration 96/1000 | Loss: 0.00003855
Iteration 97/1000 | Loss: 0.00003855
Iteration 98/1000 | Loss: 0.00003852
Iteration 99/1000 | Loss: 0.00003852
Iteration 100/1000 | Loss: 0.00003852
Iteration 101/1000 | Loss: 0.00005327
Iteration 102/1000 | Loss: 0.00003920
Iteration 103/1000 | Loss: 0.00005459
Iteration 104/1000 | Loss: 0.00003850
Iteration 105/1000 | Loss: 0.00003838
Iteration 106/1000 | Loss: 0.00007152
Iteration 107/1000 | Loss: 0.00026812
Iteration 108/1000 | Loss: 0.00065559
Iteration 109/1000 | Loss: 0.00050087
Iteration 110/1000 | Loss: 0.00014585
Iteration 111/1000 | Loss: 0.00025664
Iteration 112/1000 | Loss: 0.00013670
Iteration 113/1000 | Loss: 0.00007351
Iteration 114/1000 | Loss: 0.00014346
Iteration 115/1000 | Loss: 0.00008150
Iteration 116/1000 | Loss: 0.00007829
Iteration 117/1000 | Loss: 0.00011946
Iteration 118/1000 | Loss: 0.00005458
Iteration 119/1000 | Loss: 0.00005520
Iteration 120/1000 | Loss: 0.00005156
Iteration 121/1000 | Loss: 0.00003898
Iteration 122/1000 | Loss: 0.00003803
Iteration 123/1000 | Loss: 0.00003497
Iteration 124/1000 | Loss: 0.00003462
Iteration 125/1000 | Loss: 0.00004691
Iteration 126/1000 | Loss: 0.00003406
Iteration 127/1000 | Loss: 0.00003398
Iteration 128/1000 | Loss: 0.00004142
Iteration 129/1000 | Loss: 0.00007382
Iteration 130/1000 | Loss: 0.00003364
Iteration 131/1000 | Loss: 0.00003360
Iteration 132/1000 | Loss: 0.00003359
Iteration 133/1000 | Loss: 0.00003357
Iteration 134/1000 | Loss: 0.00003357
Iteration 135/1000 | Loss: 0.00003356
Iteration 136/1000 | Loss: 0.00003353
Iteration 137/1000 | Loss: 0.00003351
Iteration 138/1000 | Loss: 0.00003347
Iteration 139/1000 | Loss: 0.00004960
Iteration 140/1000 | Loss: 0.00003339
Iteration 141/1000 | Loss: 0.00003336
Iteration 142/1000 | Loss: 0.00003333
Iteration 143/1000 | Loss: 0.00003332
Iteration 144/1000 | Loss: 0.00003331
Iteration 145/1000 | Loss: 0.00003330
Iteration 146/1000 | Loss: 0.00003330
Iteration 147/1000 | Loss: 0.00003329
Iteration 148/1000 | Loss: 0.00003329
Iteration 149/1000 | Loss: 0.00003328
Iteration 150/1000 | Loss: 0.00003328
Iteration 151/1000 | Loss: 0.00003327
Iteration 152/1000 | Loss: 0.00003327
Iteration 153/1000 | Loss: 0.00003327
Iteration 154/1000 | Loss: 0.00003327
Iteration 155/1000 | Loss: 0.00003327
Iteration 156/1000 | Loss: 0.00003327
Iteration 157/1000 | Loss: 0.00003327
Iteration 158/1000 | Loss: 0.00003327
Iteration 159/1000 | Loss: 0.00003326
Iteration 160/1000 | Loss: 0.00003326
Iteration 161/1000 | Loss: 0.00003326
Iteration 162/1000 | Loss: 0.00003325
Iteration 163/1000 | Loss: 0.00003325
Iteration 164/1000 | Loss: 0.00003324
Iteration 165/1000 | Loss: 0.00003324
Iteration 166/1000 | Loss: 0.00003324
Iteration 167/1000 | Loss: 0.00003324
Iteration 168/1000 | Loss: 0.00003323
Iteration 169/1000 | Loss: 0.00003323
Iteration 170/1000 | Loss: 0.00003323
Iteration 171/1000 | Loss: 0.00003323
Iteration 172/1000 | Loss: 0.00003323
Iteration 173/1000 | Loss: 0.00003322
Iteration 174/1000 | Loss: 0.00003322
Iteration 175/1000 | Loss: 0.00003322
Iteration 176/1000 | Loss: 0.00003321
Iteration 177/1000 | Loss: 0.00003321
Iteration 178/1000 | Loss: 0.00003320
Iteration 179/1000 | Loss: 0.00003320
Iteration 180/1000 | Loss: 0.00003320
Iteration 181/1000 | Loss: 0.00003320
Iteration 182/1000 | Loss: 0.00003320
Iteration 183/1000 | Loss: 0.00003320
Iteration 184/1000 | Loss: 0.00003319
Iteration 185/1000 | Loss: 0.00003319
Iteration 186/1000 | Loss: 0.00003319
Iteration 187/1000 | Loss: 0.00003319
Iteration 188/1000 | Loss: 0.00003318
Iteration 189/1000 | Loss: 0.00003318
Iteration 190/1000 | Loss: 0.00003318
Iteration 191/1000 | Loss: 0.00003318
Iteration 192/1000 | Loss: 0.00003318
Iteration 193/1000 | Loss: 0.00003318
Iteration 194/1000 | Loss: 0.00003318
Iteration 195/1000 | Loss: 0.00003317
Iteration 196/1000 | Loss: 0.00003317
Iteration 197/1000 | Loss: 0.00003317
Iteration 198/1000 | Loss: 0.00003317
Iteration 199/1000 | Loss: 0.00003317
Iteration 200/1000 | Loss: 0.00003316
Iteration 201/1000 | Loss: 0.00003316
Iteration 202/1000 | Loss: 0.00003316
Iteration 203/1000 | Loss: 0.00004465
Iteration 204/1000 | Loss: 0.00004465
Iteration 205/1000 | Loss: 0.00003816
Iteration 206/1000 | Loss: 0.00003314
Iteration 207/1000 | Loss: 0.00003313
Iteration 208/1000 | Loss: 0.00003313
Iteration 209/1000 | Loss: 0.00003313
Iteration 210/1000 | Loss: 0.00003313
Iteration 211/1000 | Loss: 0.00003313
Iteration 212/1000 | Loss: 0.00003313
Iteration 213/1000 | Loss: 0.00003313
Iteration 214/1000 | Loss: 0.00003313
Iteration 215/1000 | Loss: 0.00003313
Iteration 216/1000 | Loss: 0.00003312
Iteration 217/1000 | Loss: 0.00003312
Iteration 218/1000 | Loss: 0.00003312
Iteration 219/1000 | Loss: 0.00003312
Iteration 220/1000 | Loss: 0.00003312
Iteration 221/1000 | Loss: 0.00003312
Iteration 222/1000 | Loss: 0.00003312
Iteration 223/1000 | Loss: 0.00003312
Iteration 224/1000 | Loss: 0.00003312
Iteration 225/1000 | Loss: 0.00003312
Iteration 226/1000 | Loss: 0.00003312
Iteration 227/1000 | Loss: 0.00003312
Iteration 228/1000 | Loss: 0.00003312
Iteration 229/1000 | Loss: 0.00003311
Iteration 230/1000 | Loss: 0.00003311
Iteration 231/1000 | Loss: 0.00003311
Iteration 232/1000 | Loss: 0.00003311
Iteration 233/1000 | Loss: 0.00003311
Iteration 234/1000 | Loss: 0.00003311
Iteration 235/1000 | Loss: 0.00003311
Iteration 236/1000 | Loss: 0.00003311
Iteration 237/1000 | Loss: 0.00003311
Iteration 238/1000 | Loss: 0.00003311
Iteration 239/1000 | Loss: 0.00003311
Iteration 240/1000 | Loss: 0.00003311
Iteration 241/1000 | Loss: 0.00003311
Iteration 242/1000 | Loss: 0.00003311
Iteration 243/1000 | Loss: 0.00003311
Iteration 244/1000 | Loss: 0.00003311
Iteration 245/1000 | Loss: 0.00003311
Iteration 246/1000 | Loss: 0.00003311
Iteration 247/1000 | Loss: 0.00003311
Iteration 248/1000 | Loss: 0.00003311
Iteration 249/1000 | Loss: 0.00003310
Iteration 250/1000 | Loss: 0.00003310
Iteration 251/1000 | Loss: 0.00003310
Iteration 252/1000 | Loss: 0.00003310
Iteration 253/1000 | Loss: 0.00003310
Iteration 254/1000 | Loss: 0.00003310
Iteration 255/1000 | Loss: 0.00003310
Iteration 256/1000 | Loss: 0.00003310
Iteration 257/1000 | Loss: 0.00003310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [3.310405736556277e-05, 3.310405736556277e-05, 3.310405736556277e-05, 3.310405736556277e-05, 3.310405736556277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.310405736556277e-05

Optimization complete. Final v2v error: 3.516066551208496 mm

Highest mean error: 11.035714149475098 mm for frame 208

Lowest mean error: 2.841583490371704 mm for frame 33

Saving results

Total time: 225.20817184448242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919350
Iteration 2/25 | Loss: 0.00294109
Iteration 3/25 | Loss: 0.00193049
Iteration 4/25 | Loss: 0.00172764
Iteration 5/25 | Loss: 0.00175540
Iteration 6/25 | Loss: 0.00176313
Iteration 7/25 | Loss: 0.00169639
Iteration 8/25 | Loss: 0.00169607
Iteration 9/25 | Loss: 0.00170128
Iteration 10/25 | Loss: 0.00169661
Iteration 11/25 | Loss: 0.00170521
Iteration 12/25 | Loss: 0.00170125
Iteration 13/25 | Loss: 0.00167902
Iteration 14/25 | Loss: 0.00163119
Iteration 15/25 | Loss: 0.00161816
Iteration 16/25 | Loss: 0.00161074
Iteration 17/25 | Loss: 0.00160208
Iteration 18/25 | Loss: 0.00158155
Iteration 19/25 | Loss: 0.00157727
Iteration 20/25 | Loss: 0.00157931
Iteration 21/25 | Loss: 0.00158215
Iteration 22/25 | Loss: 0.00157123
Iteration 23/25 | Loss: 0.00155288
Iteration 24/25 | Loss: 0.00155367
Iteration 25/25 | Loss: 0.00154983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65778327
Iteration 2/25 | Loss: 0.00357577
Iteration 3/25 | Loss: 0.00357576
Iteration 4/25 | Loss: 0.00357576
Iteration 5/25 | Loss: 0.00357576
Iteration 6/25 | Loss: 0.00357576
Iteration 7/25 | Loss: 0.00357576
Iteration 8/25 | Loss: 0.00357576
Iteration 9/25 | Loss: 0.00357576
Iteration 10/25 | Loss: 0.00357576
Iteration 11/25 | Loss: 0.00357576
Iteration 12/25 | Loss: 0.00357576
Iteration 13/25 | Loss: 0.00357576
Iteration 14/25 | Loss: 0.00357576
Iteration 15/25 | Loss: 0.00357576
Iteration 16/25 | Loss: 0.00357576
Iteration 17/25 | Loss: 0.00357576
Iteration 18/25 | Loss: 0.00357576
Iteration 19/25 | Loss: 0.00357576
Iteration 20/25 | Loss: 0.00357576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003575759008526802, 0.003575759008526802, 0.003575759008526802, 0.003575759008526802, 0.003575759008526802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003575759008526802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00357576
Iteration 2/1000 | Loss: 0.00038216
Iteration 3/1000 | Loss: 0.00048275
Iteration 4/1000 | Loss: 0.00031844
Iteration 5/1000 | Loss: 0.00043780
Iteration 6/1000 | Loss: 0.00070335
Iteration 7/1000 | Loss: 0.00202407
Iteration 8/1000 | Loss: 0.00078119
Iteration 9/1000 | Loss: 0.00022669
Iteration 10/1000 | Loss: 0.00182096
Iteration 11/1000 | Loss: 0.00107900
Iteration 12/1000 | Loss: 0.00061366
Iteration 13/1000 | Loss: 0.00018705
Iteration 14/1000 | Loss: 0.00025221
Iteration 15/1000 | Loss: 0.00100321
Iteration 16/1000 | Loss: 0.00017767
Iteration 17/1000 | Loss: 0.00066911
Iteration 18/1000 | Loss: 0.00074267
Iteration 19/1000 | Loss: 0.00088946
Iteration 20/1000 | Loss: 0.00056080
Iteration 21/1000 | Loss: 0.00013826
Iteration 22/1000 | Loss: 0.00307735
Iteration 23/1000 | Loss: 0.00336959
Iteration 24/1000 | Loss: 0.00076281
Iteration 25/1000 | Loss: 0.00091287
Iteration 26/1000 | Loss: 0.00081953
Iteration 27/1000 | Loss: 0.00052823
Iteration 28/1000 | Loss: 0.00021999
Iteration 29/1000 | Loss: 0.00024889
Iteration 30/1000 | Loss: 0.00022171
Iteration 31/1000 | Loss: 0.00017519
Iteration 32/1000 | Loss: 0.00071375
Iteration 33/1000 | Loss: 0.00062411
Iteration 34/1000 | Loss: 0.00180133
Iteration 35/1000 | Loss: 0.00077038
Iteration 36/1000 | Loss: 0.00167101
Iteration 37/1000 | Loss: 0.00144501
Iteration 38/1000 | Loss: 0.00060766
Iteration 39/1000 | Loss: 0.00049767
Iteration 40/1000 | Loss: 0.00129879
Iteration 41/1000 | Loss: 0.00059559
Iteration 42/1000 | Loss: 0.00048630
Iteration 43/1000 | Loss: 0.00112476
Iteration 44/1000 | Loss: 0.00050679
Iteration 45/1000 | Loss: 0.00047738
Iteration 46/1000 | Loss: 0.00059423
Iteration 47/1000 | Loss: 0.00014026
Iteration 48/1000 | Loss: 0.00012767
Iteration 49/1000 | Loss: 0.00011009
Iteration 50/1000 | Loss: 0.00011302
Iteration 51/1000 | Loss: 0.00099047
Iteration 52/1000 | Loss: 0.00043785
Iteration 53/1000 | Loss: 0.00096967
Iteration 54/1000 | Loss: 0.00145077
Iteration 55/1000 | Loss: 0.00096135
Iteration 56/1000 | Loss: 0.00066538
Iteration 57/1000 | Loss: 0.00064436
Iteration 58/1000 | Loss: 0.00059478
Iteration 59/1000 | Loss: 0.00021731
Iteration 60/1000 | Loss: 0.00015902
Iteration 61/1000 | Loss: 0.00015711
Iteration 62/1000 | Loss: 0.00014196
Iteration 63/1000 | Loss: 0.00012577
Iteration 64/1000 | Loss: 0.00015655
Iteration 65/1000 | Loss: 0.00011834
Iteration 66/1000 | Loss: 0.00009316
Iteration 67/1000 | Loss: 0.00060558
Iteration 68/1000 | Loss: 0.00032149
Iteration 69/1000 | Loss: 0.00039075
Iteration 70/1000 | Loss: 0.00065636
Iteration 71/1000 | Loss: 0.00032738
Iteration 72/1000 | Loss: 0.00040815
Iteration 73/1000 | Loss: 0.00033026
Iteration 74/1000 | Loss: 0.00008684
Iteration 75/1000 | Loss: 0.00053167
Iteration 76/1000 | Loss: 0.00144744
Iteration 77/1000 | Loss: 0.00079783
Iteration 78/1000 | Loss: 0.00032382
Iteration 79/1000 | Loss: 0.00048913
Iteration 80/1000 | Loss: 0.00387486
Iteration 81/1000 | Loss: 0.00126537
Iteration 82/1000 | Loss: 0.00388500
Iteration 83/1000 | Loss: 0.00360912
Iteration 84/1000 | Loss: 0.00118131
Iteration 85/1000 | Loss: 0.00079789
Iteration 86/1000 | Loss: 0.00053696
Iteration 87/1000 | Loss: 0.00096174
Iteration 88/1000 | Loss: 0.00062939
Iteration 89/1000 | Loss: 0.00009867
Iteration 90/1000 | Loss: 0.00008265
Iteration 91/1000 | Loss: 0.00062661
Iteration 92/1000 | Loss: 0.00326800
Iteration 93/1000 | Loss: 0.00532556
Iteration 94/1000 | Loss: 0.00329653
Iteration 95/1000 | Loss: 0.00103040
Iteration 96/1000 | Loss: 0.00023003
Iteration 97/1000 | Loss: 0.00032544
Iteration 98/1000 | Loss: 0.00011148
Iteration 99/1000 | Loss: 0.00007850
Iteration 100/1000 | Loss: 0.00055201
Iteration 101/1000 | Loss: 0.00121691
Iteration 102/1000 | Loss: 0.00101528
Iteration 103/1000 | Loss: 0.00147531
Iteration 104/1000 | Loss: 0.00041717
Iteration 105/1000 | Loss: 0.00036715
Iteration 106/1000 | Loss: 0.00025399
Iteration 107/1000 | Loss: 0.00030368
Iteration 108/1000 | Loss: 0.00025425
Iteration 109/1000 | Loss: 0.00080150
Iteration 110/1000 | Loss: 0.00039690
Iteration 111/1000 | Loss: 0.00102286
Iteration 112/1000 | Loss: 0.00058921
Iteration 113/1000 | Loss: 0.00040542
Iteration 114/1000 | Loss: 0.00049559
Iteration 115/1000 | Loss: 0.00056655
Iteration 116/1000 | Loss: 0.00032490
Iteration 117/1000 | Loss: 0.00013372
Iteration 118/1000 | Loss: 0.00043792
Iteration 119/1000 | Loss: 0.00050891
Iteration 120/1000 | Loss: 0.00043028
Iteration 121/1000 | Loss: 0.00018834
Iteration 122/1000 | Loss: 0.00056670
Iteration 123/1000 | Loss: 0.00050562
Iteration 124/1000 | Loss: 0.00053040
Iteration 125/1000 | Loss: 0.00092243
Iteration 126/1000 | Loss: 0.00083957
Iteration 127/1000 | Loss: 0.00025271
Iteration 128/1000 | Loss: 0.00046330
Iteration 129/1000 | Loss: 0.00079948
Iteration 130/1000 | Loss: 0.00070470
Iteration 131/1000 | Loss: 0.00055445
Iteration 132/1000 | Loss: 0.00025647
Iteration 133/1000 | Loss: 0.00019445
Iteration 134/1000 | Loss: 0.00070868
Iteration 135/1000 | Loss: 0.00023218
Iteration 136/1000 | Loss: 0.00077621
Iteration 137/1000 | Loss: 0.00029148
Iteration 138/1000 | Loss: 0.00044964
Iteration 139/1000 | Loss: 0.00050024
Iteration 140/1000 | Loss: 0.00039754
Iteration 141/1000 | Loss: 0.00029394
Iteration 142/1000 | Loss: 0.00027592
Iteration 143/1000 | Loss: 0.00034045
Iteration 144/1000 | Loss: 0.00038443
Iteration 145/1000 | Loss: 0.00044332
Iteration 146/1000 | Loss: 0.00016521
Iteration 147/1000 | Loss: 0.00039720
Iteration 148/1000 | Loss: 0.00075077
Iteration 149/1000 | Loss: 0.00056363
Iteration 150/1000 | Loss: 0.00037306
Iteration 151/1000 | Loss: 0.00047145
Iteration 152/1000 | Loss: 0.00006734
Iteration 153/1000 | Loss: 0.00035342
Iteration 154/1000 | Loss: 0.00028895
Iteration 155/1000 | Loss: 0.00045557
Iteration 156/1000 | Loss: 0.00020539
Iteration 157/1000 | Loss: 0.00043835
Iteration 158/1000 | Loss: 0.00021987
Iteration 159/1000 | Loss: 0.00009466
Iteration 160/1000 | Loss: 0.00005267
Iteration 161/1000 | Loss: 0.00014808
Iteration 162/1000 | Loss: 0.00010317
Iteration 163/1000 | Loss: 0.00020569
Iteration 164/1000 | Loss: 0.00041657
Iteration 165/1000 | Loss: 0.00006024
Iteration 166/1000 | Loss: 0.00015991
Iteration 167/1000 | Loss: 0.00044015
Iteration 168/1000 | Loss: 0.00028332
Iteration 169/1000 | Loss: 0.00027259
Iteration 170/1000 | Loss: 0.00024558
Iteration 171/1000 | Loss: 0.00019038
Iteration 172/1000 | Loss: 0.00007282
Iteration 173/1000 | Loss: 0.00005273
Iteration 174/1000 | Loss: 0.00006180
Iteration 175/1000 | Loss: 0.00014324
Iteration 176/1000 | Loss: 0.00011076
Iteration 177/1000 | Loss: 0.00003838
Iteration 178/1000 | Loss: 0.00004040
Iteration 179/1000 | Loss: 0.00003978
Iteration 180/1000 | Loss: 0.00004134
Iteration 181/1000 | Loss: 0.00003825
Iteration 182/1000 | Loss: 0.00003154
Iteration 183/1000 | Loss: 0.00004004
Iteration 184/1000 | Loss: 0.00004150
Iteration 185/1000 | Loss: 0.00004187
Iteration 186/1000 | Loss: 0.00004978
Iteration 187/1000 | Loss: 0.00004582
Iteration 188/1000 | Loss: 0.00003137
Iteration 189/1000 | Loss: 0.00004976
Iteration 190/1000 | Loss: 0.00004022
Iteration 191/1000 | Loss: 0.00004049
Iteration 192/1000 | Loss: 0.00004089
Iteration 193/1000 | Loss: 0.00004030
Iteration 194/1000 | Loss: 0.00004033
Iteration 195/1000 | Loss: 0.00003909
Iteration 196/1000 | Loss: 0.00003964
Iteration 197/1000 | Loss: 0.00005450
Iteration 198/1000 | Loss: 0.00022150
Iteration 199/1000 | Loss: 0.00007220
Iteration 200/1000 | Loss: 0.00005459
Iteration 201/1000 | Loss: 0.00012993
Iteration 202/1000 | Loss: 0.00015887
Iteration 203/1000 | Loss: 0.00015908
Iteration 204/1000 | Loss: 0.00008158
Iteration 205/1000 | Loss: 0.00006066
Iteration 206/1000 | Loss: 0.00004494
Iteration 207/1000 | Loss: 0.00021845
Iteration 208/1000 | Loss: 0.00009342
Iteration 209/1000 | Loss: 0.00002630
Iteration 210/1000 | Loss: 0.00002511
Iteration 211/1000 | Loss: 0.00020446
Iteration 212/1000 | Loss: 0.00008260
Iteration 213/1000 | Loss: 0.00002496
Iteration 214/1000 | Loss: 0.00020090
Iteration 215/1000 | Loss: 0.00022671
Iteration 216/1000 | Loss: 0.00017157
Iteration 217/1000 | Loss: 0.00024219
Iteration 218/1000 | Loss: 0.00016178
Iteration 219/1000 | Loss: 0.00017924
Iteration 220/1000 | Loss: 0.00011589
Iteration 221/1000 | Loss: 0.00047549
Iteration 222/1000 | Loss: 0.00031303
Iteration 223/1000 | Loss: 0.00032312
Iteration 224/1000 | Loss: 0.00053863
Iteration 225/1000 | Loss: 0.00020718
Iteration 226/1000 | Loss: 0.00016782
Iteration 227/1000 | Loss: 0.00014319
Iteration 228/1000 | Loss: 0.00012774
Iteration 229/1000 | Loss: 0.00013643
Iteration 230/1000 | Loss: 0.00011747
Iteration 231/1000 | Loss: 0.00011592
Iteration 232/1000 | Loss: 0.00037688
Iteration 233/1000 | Loss: 0.00022217
Iteration 234/1000 | Loss: 0.00003579
Iteration 235/1000 | Loss: 0.00016405
Iteration 236/1000 | Loss: 0.00020288
Iteration 237/1000 | Loss: 0.00018338
Iteration 238/1000 | Loss: 0.00003574
Iteration 239/1000 | Loss: 0.00002781
Iteration 240/1000 | Loss: 0.00002606
Iteration 241/1000 | Loss: 0.00002514
Iteration 242/1000 | Loss: 0.00002380
Iteration 243/1000 | Loss: 0.00003631
Iteration 244/1000 | Loss: 0.00002766
Iteration 245/1000 | Loss: 0.00009232
Iteration 246/1000 | Loss: 0.00002581
Iteration 247/1000 | Loss: 0.00002296
Iteration 248/1000 | Loss: 0.00002172
Iteration 249/1000 | Loss: 0.00002120
Iteration 250/1000 | Loss: 0.00002653
Iteration 251/1000 | Loss: 0.00002056
Iteration 252/1000 | Loss: 0.00002001
Iteration 253/1000 | Loss: 0.00001961
Iteration 254/1000 | Loss: 0.00001926
Iteration 255/1000 | Loss: 0.00001897
Iteration 256/1000 | Loss: 0.00001876
Iteration 257/1000 | Loss: 0.00001863
Iteration 258/1000 | Loss: 0.00001860
Iteration 259/1000 | Loss: 0.00001858
Iteration 260/1000 | Loss: 0.00001857
Iteration 261/1000 | Loss: 0.00001846
Iteration 262/1000 | Loss: 0.00001845
Iteration 263/1000 | Loss: 0.00001845
Iteration 264/1000 | Loss: 0.00001844
Iteration 265/1000 | Loss: 0.00001843
Iteration 266/1000 | Loss: 0.00001843
Iteration 267/1000 | Loss: 0.00001843
Iteration 268/1000 | Loss: 0.00001842
Iteration 269/1000 | Loss: 0.00001841
Iteration 270/1000 | Loss: 0.00001841
Iteration 271/1000 | Loss: 0.00001841
Iteration 272/1000 | Loss: 0.00001840
Iteration 273/1000 | Loss: 0.00001840
Iteration 274/1000 | Loss: 0.00001840
Iteration 275/1000 | Loss: 0.00001840
Iteration 276/1000 | Loss: 0.00001840
Iteration 277/1000 | Loss: 0.00001839
Iteration 278/1000 | Loss: 0.00001839
Iteration 279/1000 | Loss: 0.00001838
Iteration 280/1000 | Loss: 0.00001838
Iteration 281/1000 | Loss: 0.00001837
Iteration 282/1000 | Loss: 0.00001836
Iteration 283/1000 | Loss: 0.00001836
Iteration 284/1000 | Loss: 0.00001835
Iteration 285/1000 | Loss: 0.00001835
Iteration 286/1000 | Loss: 0.00001833
Iteration 287/1000 | Loss: 0.00001833
Iteration 288/1000 | Loss: 0.00001833
Iteration 289/1000 | Loss: 0.00001833
Iteration 290/1000 | Loss: 0.00001833
Iteration 291/1000 | Loss: 0.00001833
Iteration 292/1000 | Loss: 0.00001833
Iteration 293/1000 | Loss: 0.00001833
Iteration 294/1000 | Loss: 0.00001833
Iteration 295/1000 | Loss: 0.00001833
Iteration 296/1000 | Loss: 0.00001832
Iteration 297/1000 | Loss: 0.00001832
Iteration 298/1000 | Loss: 0.00001832
Iteration 299/1000 | Loss: 0.00001832
Iteration 300/1000 | Loss: 0.00001832
Iteration 301/1000 | Loss: 0.00001832
Iteration 302/1000 | Loss: 0.00001832
Iteration 303/1000 | Loss: 0.00001832
Iteration 304/1000 | Loss: 0.00001830
Iteration 305/1000 | Loss: 0.00001830
Iteration 306/1000 | Loss: 0.00001829
Iteration 307/1000 | Loss: 0.00001829
Iteration 308/1000 | Loss: 0.00001829
Iteration 309/1000 | Loss: 0.00001829
Iteration 310/1000 | Loss: 0.00001828
Iteration 311/1000 | Loss: 0.00001828
Iteration 312/1000 | Loss: 0.00001828
Iteration 313/1000 | Loss: 0.00001827
Iteration 314/1000 | Loss: 0.00001827
Iteration 315/1000 | Loss: 0.00001826
Iteration 316/1000 | Loss: 0.00001826
Iteration 317/1000 | Loss: 0.00001825
Iteration 318/1000 | Loss: 0.00001825
Iteration 319/1000 | Loss: 0.00001824
Iteration 320/1000 | Loss: 0.00001824
Iteration 321/1000 | Loss: 0.00001824
Iteration 322/1000 | Loss: 0.00001823
Iteration 323/1000 | Loss: 0.00001823
Iteration 324/1000 | Loss: 0.00001823
Iteration 325/1000 | Loss: 0.00001823
Iteration 326/1000 | Loss: 0.00001823
Iteration 327/1000 | Loss: 0.00001822
Iteration 328/1000 | Loss: 0.00001822
Iteration 329/1000 | Loss: 0.00001821
Iteration 330/1000 | Loss: 0.00001821
Iteration 331/1000 | Loss: 0.00001820
Iteration 332/1000 | Loss: 0.00001820
Iteration 333/1000 | Loss: 0.00001820
Iteration 334/1000 | Loss: 0.00001820
Iteration 335/1000 | Loss: 0.00001820
Iteration 336/1000 | Loss: 0.00001820
Iteration 337/1000 | Loss: 0.00001820
Iteration 338/1000 | Loss: 0.00001820
Iteration 339/1000 | Loss: 0.00001820
Iteration 340/1000 | Loss: 0.00001819
Iteration 341/1000 | Loss: 0.00001819
Iteration 342/1000 | Loss: 0.00001819
Iteration 343/1000 | Loss: 0.00001819
Iteration 344/1000 | Loss: 0.00001819
Iteration 345/1000 | Loss: 0.00001819
Iteration 346/1000 | Loss: 0.00001818
Iteration 347/1000 | Loss: 0.00001818
Iteration 348/1000 | Loss: 0.00001818
Iteration 349/1000 | Loss: 0.00001818
Iteration 350/1000 | Loss: 0.00001818
Iteration 351/1000 | Loss: 0.00001818
Iteration 352/1000 | Loss: 0.00001818
Iteration 353/1000 | Loss: 0.00001817
Iteration 354/1000 | Loss: 0.00001817
Iteration 355/1000 | Loss: 0.00001817
Iteration 356/1000 | Loss: 0.00001817
Iteration 357/1000 | Loss: 0.00001817
Iteration 358/1000 | Loss: 0.00001817
Iteration 359/1000 | Loss: 0.00001817
Iteration 360/1000 | Loss: 0.00001817
Iteration 361/1000 | Loss: 0.00001817
Iteration 362/1000 | Loss: 0.00001817
Iteration 363/1000 | Loss: 0.00001817
Iteration 364/1000 | Loss: 0.00001816
Iteration 365/1000 | Loss: 0.00001816
Iteration 366/1000 | Loss: 0.00001816
Iteration 367/1000 | Loss: 0.00001816
Iteration 368/1000 | Loss: 0.00001816
Iteration 369/1000 | Loss: 0.00001816
Iteration 370/1000 | Loss: 0.00001816
Iteration 371/1000 | Loss: 0.00001816
Iteration 372/1000 | Loss: 0.00001816
Iteration 373/1000 | Loss: 0.00001815
Iteration 374/1000 | Loss: 0.00001815
Iteration 375/1000 | Loss: 0.00001815
Iteration 376/1000 | Loss: 0.00001815
Iteration 377/1000 | Loss: 0.00001815
Iteration 378/1000 | Loss: 0.00001815
Iteration 379/1000 | Loss: 0.00001815
Iteration 380/1000 | Loss: 0.00001815
Iteration 381/1000 | Loss: 0.00001815
Iteration 382/1000 | Loss: 0.00001814
Iteration 383/1000 | Loss: 0.00001814
Iteration 384/1000 | Loss: 0.00001814
Iteration 385/1000 | Loss: 0.00001814
Iteration 386/1000 | Loss: 0.00001814
Iteration 387/1000 | Loss: 0.00001814
Iteration 388/1000 | Loss: 0.00001814
Iteration 389/1000 | Loss: 0.00001814
Iteration 390/1000 | Loss: 0.00001813
Iteration 391/1000 | Loss: 0.00001813
Iteration 392/1000 | Loss: 0.00001813
Iteration 393/1000 | Loss: 0.00001813
Iteration 394/1000 | Loss: 0.00001813
Iteration 395/1000 | Loss: 0.00001813
Iteration 396/1000 | Loss: 0.00001813
Iteration 397/1000 | Loss: 0.00001812
Iteration 398/1000 | Loss: 0.00001812
Iteration 399/1000 | Loss: 0.00001812
Iteration 400/1000 | Loss: 0.00001812
Iteration 401/1000 | Loss: 0.00001811
Iteration 402/1000 | Loss: 0.00001811
Iteration 403/1000 | Loss: 0.00001811
Iteration 404/1000 | Loss: 0.00001811
Iteration 405/1000 | Loss: 0.00001811
Iteration 406/1000 | Loss: 0.00001811
Iteration 407/1000 | Loss: 0.00001811
Iteration 408/1000 | Loss: 0.00001811
Iteration 409/1000 | Loss: 0.00001811
Iteration 410/1000 | Loss: 0.00001811
Iteration 411/1000 | Loss: 0.00001811
Iteration 412/1000 | Loss: 0.00001811
Iteration 413/1000 | Loss: 0.00001811
Iteration 414/1000 | Loss: 0.00001811
Iteration 415/1000 | Loss: 0.00001811
Iteration 416/1000 | Loss: 0.00001811
Iteration 417/1000 | Loss: 0.00001811
Iteration 418/1000 | Loss: 0.00001811
Iteration 419/1000 | Loss: 0.00001811
Iteration 420/1000 | Loss: 0.00001811
Iteration 421/1000 | Loss: 0.00001810
Iteration 422/1000 | Loss: 0.00001810
Iteration 423/1000 | Loss: 0.00001810
Iteration 424/1000 | Loss: 0.00001810
Iteration 425/1000 | Loss: 0.00001810
Iteration 426/1000 | Loss: 0.00001810
Iteration 427/1000 | Loss: 0.00001810
Iteration 428/1000 | Loss: 0.00001810
Iteration 429/1000 | Loss: 0.00001810
Iteration 430/1000 | Loss: 0.00001810
Iteration 431/1000 | Loss: 0.00001810
Iteration 432/1000 | Loss: 0.00001810
Iteration 433/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 433. Stopping optimization.
Last 5 losses: [1.810200046747923e-05, 1.810200046747923e-05, 1.810200046747923e-05, 1.810200046747923e-05, 1.810200046747923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.810200046747923e-05

Optimization complete. Final v2v error: 3.5115535259246826 mm

Highest mean error: 4.917836666107178 mm for frame 52

Lowest mean error: 2.932918071746826 mm for frame 160

Saving results

Total time: 433.8567461967468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788665
Iteration 2/25 | Loss: 0.00151120
Iteration 3/25 | Loss: 0.00136497
Iteration 4/25 | Loss: 0.00135325
Iteration 5/25 | Loss: 0.00135038
Iteration 6/25 | Loss: 0.00135033
Iteration 7/25 | Loss: 0.00135033
Iteration 8/25 | Loss: 0.00135033
Iteration 9/25 | Loss: 0.00135033
Iteration 10/25 | Loss: 0.00135033
Iteration 11/25 | Loss: 0.00135033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001350333564914763, 0.001350333564914763, 0.001350333564914763, 0.001350333564914763, 0.001350333564914763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001350333564914763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26147079
Iteration 2/25 | Loss: 0.00123056
Iteration 3/25 | Loss: 0.00123052
Iteration 4/25 | Loss: 0.00123052
Iteration 5/25 | Loss: 0.00123052
Iteration 6/25 | Loss: 0.00123052
Iteration 7/25 | Loss: 0.00123052
Iteration 8/25 | Loss: 0.00123052
Iteration 9/25 | Loss: 0.00123052
Iteration 10/25 | Loss: 0.00123052
Iteration 11/25 | Loss: 0.00123052
Iteration 12/25 | Loss: 0.00123052
Iteration 13/25 | Loss: 0.00123052
Iteration 14/25 | Loss: 0.00123052
Iteration 15/25 | Loss: 0.00123052
Iteration 16/25 | Loss: 0.00123052
Iteration 17/25 | Loss: 0.00123052
Iteration 18/25 | Loss: 0.00123052
Iteration 19/25 | Loss: 0.00123052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012305182171985507, 0.0012305182171985507, 0.0012305182171985507, 0.0012305182171985507, 0.0012305182171985507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012305182171985507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123052
Iteration 2/1000 | Loss: 0.00003641
Iteration 3/1000 | Loss: 0.00002497
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002121
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00001974
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001823
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001791
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001774
Iteration 21/1000 | Loss: 0.00001773
Iteration 22/1000 | Loss: 0.00001772
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001764
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001757
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001748
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001744
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001741
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001740
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001739
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001739
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001738
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001738
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001736
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001735
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001733
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001728
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Iteration 99/1000 | Loss: 0.00001727
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001726
Iteration 103/1000 | Loss: 0.00001726
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001720
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001719
Iteration 132/1000 | Loss: 0.00001719
Iteration 133/1000 | Loss: 0.00001719
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001718
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001717
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001717
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001714
Iteration 157/1000 | Loss: 0.00001714
Iteration 158/1000 | Loss: 0.00001714
Iteration 159/1000 | Loss: 0.00001714
Iteration 160/1000 | Loss: 0.00001714
Iteration 161/1000 | Loss: 0.00001714
Iteration 162/1000 | Loss: 0.00001713
Iteration 163/1000 | Loss: 0.00001713
Iteration 164/1000 | Loss: 0.00001713
Iteration 165/1000 | Loss: 0.00001713
Iteration 166/1000 | Loss: 0.00001713
Iteration 167/1000 | Loss: 0.00001713
Iteration 168/1000 | Loss: 0.00001713
Iteration 169/1000 | Loss: 0.00001713
Iteration 170/1000 | Loss: 0.00001713
Iteration 171/1000 | Loss: 0.00001713
Iteration 172/1000 | Loss: 0.00001712
Iteration 173/1000 | Loss: 0.00001712
Iteration 174/1000 | Loss: 0.00001712
Iteration 175/1000 | Loss: 0.00001712
Iteration 176/1000 | Loss: 0.00001712
Iteration 177/1000 | Loss: 0.00001712
Iteration 178/1000 | Loss: 0.00001712
Iteration 179/1000 | Loss: 0.00001712
Iteration 180/1000 | Loss: 0.00001712
Iteration 181/1000 | Loss: 0.00001712
Iteration 182/1000 | Loss: 0.00001711
Iteration 183/1000 | Loss: 0.00001711
Iteration 184/1000 | Loss: 0.00001711
Iteration 185/1000 | Loss: 0.00001711
Iteration 186/1000 | Loss: 0.00001711
Iteration 187/1000 | Loss: 0.00001711
Iteration 188/1000 | Loss: 0.00001711
Iteration 189/1000 | Loss: 0.00001711
Iteration 190/1000 | Loss: 0.00001711
Iteration 191/1000 | Loss: 0.00001711
Iteration 192/1000 | Loss: 0.00001711
Iteration 193/1000 | Loss: 0.00001711
Iteration 194/1000 | Loss: 0.00001711
Iteration 195/1000 | Loss: 0.00001711
Iteration 196/1000 | Loss: 0.00001711
Iteration 197/1000 | Loss: 0.00001711
Iteration 198/1000 | Loss: 0.00001711
Iteration 199/1000 | Loss: 0.00001711
Iteration 200/1000 | Loss: 0.00001711
Iteration 201/1000 | Loss: 0.00001711
Iteration 202/1000 | Loss: 0.00001711
Iteration 203/1000 | Loss: 0.00001710
Iteration 204/1000 | Loss: 0.00001710
Iteration 205/1000 | Loss: 0.00001710
Iteration 206/1000 | Loss: 0.00001710
Iteration 207/1000 | Loss: 0.00001710
Iteration 208/1000 | Loss: 0.00001710
Iteration 209/1000 | Loss: 0.00001710
Iteration 210/1000 | Loss: 0.00001710
Iteration 211/1000 | Loss: 0.00001710
Iteration 212/1000 | Loss: 0.00001710
Iteration 213/1000 | Loss: 0.00001710
Iteration 214/1000 | Loss: 0.00001710
Iteration 215/1000 | Loss: 0.00001710
Iteration 216/1000 | Loss: 0.00001710
Iteration 217/1000 | Loss: 0.00001710
Iteration 218/1000 | Loss: 0.00001710
Iteration 219/1000 | Loss: 0.00001710
Iteration 220/1000 | Loss: 0.00001710
Iteration 221/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.7103298887377605e-05, 1.7103298887377605e-05, 1.7103298887377605e-05, 1.7103298887377605e-05, 1.7103298887377605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7103298887377605e-05

Optimization complete. Final v2v error: 3.4668233394622803 mm

Highest mean error: 4.80815315246582 mm for frame 23

Lowest mean error: 2.925074577331543 mm for frame 206

Saving results

Total time: 51.7300591468811
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733967
Iteration 2/25 | Loss: 0.00187318
Iteration 3/25 | Loss: 0.00144996
Iteration 4/25 | Loss: 0.00138104
Iteration 5/25 | Loss: 0.00137205
Iteration 6/25 | Loss: 0.00137162
Iteration 7/25 | Loss: 0.00137162
Iteration 8/25 | Loss: 0.00137162
Iteration 9/25 | Loss: 0.00137162
Iteration 10/25 | Loss: 0.00137162
Iteration 11/25 | Loss: 0.00137162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013716189423575997, 0.0013716189423575997, 0.0013716189423575997, 0.0013716189423575997, 0.0013716189423575997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013716189423575997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65073037
Iteration 2/25 | Loss: 0.00119312
Iteration 3/25 | Loss: 0.00119308
Iteration 4/25 | Loss: 0.00119308
Iteration 5/25 | Loss: 0.00119308
Iteration 6/25 | Loss: 0.00119308
Iteration 7/25 | Loss: 0.00119307
Iteration 8/25 | Loss: 0.00119307
Iteration 9/25 | Loss: 0.00119307
Iteration 10/25 | Loss: 0.00119307
Iteration 11/25 | Loss: 0.00119307
Iteration 12/25 | Loss: 0.00119307
Iteration 13/25 | Loss: 0.00119307
Iteration 14/25 | Loss: 0.00119307
Iteration 15/25 | Loss: 0.00119307
Iteration 16/25 | Loss: 0.00119307
Iteration 17/25 | Loss: 0.00119307
Iteration 18/25 | Loss: 0.00119307
Iteration 19/25 | Loss: 0.00119307
Iteration 20/25 | Loss: 0.00119307
Iteration 21/25 | Loss: 0.00119307
Iteration 22/25 | Loss: 0.00119307
Iteration 23/25 | Loss: 0.00119307
Iteration 24/25 | Loss: 0.00119307
Iteration 25/25 | Loss: 0.00119307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119307
Iteration 2/1000 | Loss: 0.00003804
Iteration 3/1000 | Loss: 0.00002819
Iteration 4/1000 | Loss: 0.00002661
Iteration 5/1000 | Loss: 0.00002545
Iteration 6/1000 | Loss: 0.00002468
Iteration 7/1000 | Loss: 0.00002429
Iteration 8/1000 | Loss: 0.00002380
Iteration 9/1000 | Loss: 0.00002342
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002278
Iteration 12/1000 | Loss: 0.00002248
Iteration 13/1000 | Loss: 0.00002237
Iteration 14/1000 | Loss: 0.00002230
Iteration 15/1000 | Loss: 0.00002227
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002214
Iteration 18/1000 | Loss: 0.00002210
Iteration 19/1000 | Loss: 0.00002210
Iteration 20/1000 | Loss: 0.00002210
Iteration 21/1000 | Loss: 0.00002209
Iteration 22/1000 | Loss: 0.00002209
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002208
Iteration 25/1000 | Loss: 0.00002207
Iteration 26/1000 | Loss: 0.00002207
Iteration 27/1000 | Loss: 0.00002206
Iteration 28/1000 | Loss: 0.00002206
Iteration 29/1000 | Loss: 0.00002206
Iteration 30/1000 | Loss: 0.00002206
Iteration 31/1000 | Loss: 0.00002205
Iteration 32/1000 | Loss: 0.00002205
Iteration 33/1000 | Loss: 0.00002205
Iteration 34/1000 | Loss: 0.00002205
Iteration 35/1000 | Loss: 0.00002204
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002203
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002202
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00002201
Iteration 43/1000 | Loss: 0.00002201
Iteration 44/1000 | Loss: 0.00002199
Iteration 45/1000 | Loss: 0.00002199
Iteration 46/1000 | Loss: 0.00002199
Iteration 47/1000 | Loss: 0.00002198
Iteration 48/1000 | Loss: 0.00002198
Iteration 49/1000 | Loss: 0.00002197
Iteration 50/1000 | Loss: 0.00002197
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002196
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002194
Iteration 58/1000 | Loss: 0.00002194
Iteration 59/1000 | Loss: 0.00002194
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002192
Iteration 64/1000 | Loss: 0.00002192
Iteration 65/1000 | Loss: 0.00002192
Iteration 66/1000 | Loss: 0.00002192
Iteration 67/1000 | Loss: 0.00002191
Iteration 68/1000 | Loss: 0.00002191
Iteration 69/1000 | Loss: 0.00002191
Iteration 70/1000 | Loss: 0.00002191
Iteration 71/1000 | Loss: 0.00002191
Iteration 72/1000 | Loss: 0.00002191
Iteration 73/1000 | Loss: 0.00002191
Iteration 74/1000 | Loss: 0.00002191
Iteration 75/1000 | Loss: 0.00002191
Iteration 76/1000 | Loss: 0.00002191
Iteration 77/1000 | Loss: 0.00002191
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002190
Iteration 80/1000 | Loss: 0.00002189
Iteration 81/1000 | Loss: 0.00002189
Iteration 82/1000 | Loss: 0.00002189
Iteration 83/1000 | Loss: 0.00002188
Iteration 84/1000 | Loss: 0.00002188
Iteration 85/1000 | Loss: 0.00002188
Iteration 86/1000 | Loss: 0.00002187
Iteration 87/1000 | Loss: 0.00002187
Iteration 88/1000 | Loss: 0.00002187
Iteration 89/1000 | Loss: 0.00002187
Iteration 90/1000 | Loss: 0.00002187
Iteration 91/1000 | Loss: 0.00002187
Iteration 92/1000 | Loss: 0.00002187
Iteration 93/1000 | Loss: 0.00002187
Iteration 94/1000 | Loss: 0.00002187
Iteration 95/1000 | Loss: 0.00002187
Iteration 96/1000 | Loss: 0.00002186
Iteration 97/1000 | Loss: 0.00002186
Iteration 98/1000 | Loss: 0.00002186
Iteration 99/1000 | Loss: 0.00002186
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002186
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002184
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00002184
Iteration 111/1000 | Loss: 0.00002183
Iteration 112/1000 | Loss: 0.00002183
Iteration 113/1000 | Loss: 0.00002183
Iteration 114/1000 | Loss: 0.00002183
Iteration 115/1000 | Loss: 0.00002183
Iteration 116/1000 | Loss: 0.00002183
Iteration 117/1000 | Loss: 0.00002183
Iteration 118/1000 | Loss: 0.00002183
Iteration 119/1000 | Loss: 0.00002183
Iteration 120/1000 | Loss: 0.00002183
Iteration 121/1000 | Loss: 0.00002183
Iteration 122/1000 | Loss: 0.00002183
Iteration 123/1000 | Loss: 0.00002183
Iteration 124/1000 | Loss: 0.00002183
Iteration 125/1000 | Loss: 0.00002183
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002182
Iteration 130/1000 | Loss: 0.00002182
Iteration 131/1000 | Loss: 0.00002182
Iteration 132/1000 | Loss: 0.00002182
Iteration 133/1000 | Loss: 0.00002182
Iteration 134/1000 | Loss: 0.00002182
Iteration 135/1000 | Loss: 0.00002182
Iteration 136/1000 | Loss: 0.00002182
Iteration 137/1000 | Loss: 0.00002181
Iteration 138/1000 | Loss: 0.00002181
Iteration 139/1000 | Loss: 0.00002181
Iteration 140/1000 | Loss: 0.00002181
Iteration 141/1000 | Loss: 0.00002181
Iteration 142/1000 | Loss: 0.00002181
Iteration 143/1000 | Loss: 0.00002181
Iteration 144/1000 | Loss: 0.00002181
Iteration 145/1000 | Loss: 0.00002181
Iteration 146/1000 | Loss: 0.00002181
Iteration 147/1000 | Loss: 0.00002181
Iteration 148/1000 | Loss: 0.00002181
Iteration 149/1000 | Loss: 0.00002181
Iteration 150/1000 | Loss: 0.00002181
Iteration 151/1000 | Loss: 0.00002181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.1808189558214508e-05, 2.1808189558214508e-05, 2.1808189558214508e-05, 2.1808189558214508e-05, 2.1808189558214508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1808189558214508e-05

Optimization complete. Final v2v error: 3.8888931274414062 mm

Highest mean error: 4.205655574798584 mm for frame 235

Lowest mean error: 3.6399543285369873 mm for frame 91

Saving results

Total time: 44.31130790710449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797101
Iteration 2/25 | Loss: 0.00177937
Iteration 3/25 | Loss: 0.00146234
Iteration 4/25 | Loss: 0.00142482
Iteration 5/25 | Loss: 0.00141897
Iteration 6/25 | Loss: 0.00141723
Iteration 7/25 | Loss: 0.00141689
Iteration 8/25 | Loss: 0.00141689
Iteration 9/25 | Loss: 0.00141689
Iteration 10/25 | Loss: 0.00141689
Iteration 11/25 | Loss: 0.00141689
Iteration 12/25 | Loss: 0.00141689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014168930938467383, 0.0014168930938467383, 0.0014168930938467383, 0.0014168930938467383, 0.0014168930938467383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014168930938467383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35627234
Iteration 2/25 | Loss: 0.00112951
Iteration 3/25 | Loss: 0.00112951
Iteration 4/25 | Loss: 0.00112951
Iteration 5/25 | Loss: 0.00112951
Iteration 6/25 | Loss: 0.00112951
Iteration 7/25 | Loss: 0.00112951
Iteration 8/25 | Loss: 0.00112951
Iteration 9/25 | Loss: 0.00112951
Iteration 10/25 | Loss: 0.00112951
Iteration 11/25 | Loss: 0.00112951
Iteration 12/25 | Loss: 0.00112951
Iteration 13/25 | Loss: 0.00112951
Iteration 14/25 | Loss: 0.00112951
Iteration 15/25 | Loss: 0.00112951
Iteration 16/25 | Loss: 0.00112951
Iteration 17/25 | Loss: 0.00112951
Iteration 18/25 | Loss: 0.00112951
Iteration 19/25 | Loss: 0.00112951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011295088334009051, 0.0011295088334009051, 0.0011295088334009051, 0.0011295088334009051, 0.0011295088334009051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011295088334009051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112951
Iteration 2/1000 | Loss: 0.00006220
Iteration 3/1000 | Loss: 0.00004287
Iteration 4/1000 | Loss: 0.00003555
Iteration 5/1000 | Loss: 0.00003280
Iteration 6/1000 | Loss: 0.00003158
Iteration 7/1000 | Loss: 0.00003055
Iteration 8/1000 | Loss: 0.00002974
Iteration 9/1000 | Loss: 0.00002921
Iteration 10/1000 | Loss: 0.00002876
Iteration 11/1000 | Loss: 0.00002842
Iteration 12/1000 | Loss: 0.00002825
Iteration 13/1000 | Loss: 0.00002813
Iteration 14/1000 | Loss: 0.00002798
Iteration 15/1000 | Loss: 0.00002794
Iteration 16/1000 | Loss: 0.00002791
Iteration 17/1000 | Loss: 0.00002790
Iteration 18/1000 | Loss: 0.00002789
Iteration 19/1000 | Loss: 0.00002784
Iteration 20/1000 | Loss: 0.00002783
Iteration 21/1000 | Loss: 0.00002776
Iteration 22/1000 | Loss: 0.00002775
Iteration 23/1000 | Loss: 0.00002775
Iteration 24/1000 | Loss: 0.00002774
Iteration 25/1000 | Loss: 0.00002773
Iteration 26/1000 | Loss: 0.00002772
Iteration 27/1000 | Loss: 0.00002770
Iteration 28/1000 | Loss: 0.00002767
Iteration 29/1000 | Loss: 0.00002763
Iteration 30/1000 | Loss: 0.00002763
Iteration 31/1000 | Loss: 0.00002763
Iteration 32/1000 | Loss: 0.00002763
Iteration 33/1000 | Loss: 0.00002763
Iteration 34/1000 | Loss: 0.00002763
Iteration 35/1000 | Loss: 0.00002763
Iteration 36/1000 | Loss: 0.00002762
Iteration 37/1000 | Loss: 0.00002762
Iteration 38/1000 | Loss: 0.00002762
Iteration 39/1000 | Loss: 0.00002762
Iteration 40/1000 | Loss: 0.00002762
Iteration 41/1000 | Loss: 0.00002762
Iteration 42/1000 | Loss: 0.00002762
Iteration 43/1000 | Loss: 0.00002762
Iteration 44/1000 | Loss: 0.00002762
Iteration 45/1000 | Loss: 0.00002762
Iteration 46/1000 | Loss: 0.00002761
Iteration 47/1000 | Loss: 0.00002761
Iteration 48/1000 | Loss: 0.00002761
Iteration 49/1000 | Loss: 0.00002761
Iteration 50/1000 | Loss: 0.00002761
Iteration 51/1000 | Loss: 0.00002761
Iteration 52/1000 | Loss: 0.00002761
Iteration 53/1000 | Loss: 0.00002761
Iteration 54/1000 | Loss: 0.00002760
Iteration 55/1000 | Loss: 0.00002760
Iteration 56/1000 | Loss: 0.00002760
Iteration 57/1000 | Loss: 0.00002759
Iteration 58/1000 | Loss: 0.00002759
Iteration 59/1000 | Loss: 0.00002758
Iteration 60/1000 | Loss: 0.00002758
Iteration 61/1000 | Loss: 0.00002758
Iteration 62/1000 | Loss: 0.00002758
Iteration 63/1000 | Loss: 0.00002758
Iteration 64/1000 | Loss: 0.00002757
Iteration 65/1000 | Loss: 0.00002757
Iteration 66/1000 | Loss: 0.00002757
Iteration 67/1000 | Loss: 0.00002757
Iteration 68/1000 | Loss: 0.00002757
Iteration 69/1000 | Loss: 0.00002757
Iteration 70/1000 | Loss: 0.00002757
Iteration 71/1000 | Loss: 0.00002757
Iteration 72/1000 | Loss: 0.00002757
Iteration 73/1000 | Loss: 0.00002756
Iteration 74/1000 | Loss: 0.00002756
Iteration 75/1000 | Loss: 0.00002756
Iteration 76/1000 | Loss: 0.00002756
Iteration 77/1000 | Loss: 0.00002756
Iteration 78/1000 | Loss: 0.00002756
Iteration 79/1000 | Loss: 0.00002756
Iteration 80/1000 | Loss: 0.00002756
Iteration 81/1000 | Loss: 0.00002756
Iteration 82/1000 | Loss: 0.00002755
Iteration 83/1000 | Loss: 0.00002755
Iteration 84/1000 | Loss: 0.00002754
Iteration 85/1000 | Loss: 0.00002754
Iteration 86/1000 | Loss: 0.00002753
Iteration 87/1000 | Loss: 0.00002753
Iteration 88/1000 | Loss: 0.00002753
Iteration 89/1000 | Loss: 0.00002753
Iteration 90/1000 | Loss: 0.00002752
Iteration 91/1000 | Loss: 0.00002752
Iteration 92/1000 | Loss: 0.00002752
Iteration 93/1000 | Loss: 0.00002751
Iteration 94/1000 | Loss: 0.00002751
Iteration 95/1000 | Loss: 0.00002751
Iteration 96/1000 | Loss: 0.00002751
Iteration 97/1000 | Loss: 0.00002750
Iteration 98/1000 | Loss: 0.00002750
Iteration 99/1000 | Loss: 0.00002750
Iteration 100/1000 | Loss: 0.00002750
Iteration 101/1000 | Loss: 0.00002750
Iteration 102/1000 | Loss: 0.00002750
Iteration 103/1000 | Loss: 0.00002750
Iteration 104/1000 | Loss: 0.00002750
Iteration 105/1000 | Loss: 0.00002749
Iteration 106/1000 | Loss: 0.00002749
Iteration 107/1000 | Loss: 0.00002749
Iteration 108/1000 | Loss: 0.00002749
Iteration 109/1000 | Loss: 0.00002749
Iteration 110/1000 | Loss: 0.00002749
Iteration 111/1000 | Loss: 0.00002748
Iteration 112/1000 | Loss: 0.00002748
Iteration 113/1000 | Loss: 0.00002748
Iteration 114/1000 | Loss: 0.00002748
Iteration 115/1000 | Loss: 0.00002748
Iteration 116/1000 | Loss: 0.00002748
Iteration 117/1000 | Loss: 0.00002748
Iteration 118/1000 | Loss: 0.00002748
Iteration 119/1000 | Loss: 0.00002748
Iteration 120/1000 | Loss: 0.00002748
Iteration 121/1000 | Loss: 0.00002748
Iteration 122/1000 | Loss: 0.00002747
Iteration 123/1000 | Loss: 0.00002747
Iteration 124/1000 | Loss: 0.00002746
Iteration 125/1000 | Loss: 0.00002746
Iteration 126/1000 | Loss: 0.00002746
Iteration 127/1000 | Loss: 0.00002746
Iteration 128/1000 | Loss: 0.00002746
Iteration 129/1000 | Loss: 0.00002746
Iteration 130/1000 | Loss: 0.00002746
Iteration 131/1000 | Loss: 0.00002746
Iteration 132/1000 | Loss: 0.00002746
Iteration 133/1000 | Loss: 0.00002746
Iteration 134/1000 | Loss: 0.00002745
Iteration 135/1000 | Loss: 0.00002745
Iteration 136/1000 | Loss: 0.00002745
Iteration 137/1000 | Loss: 0.00002745
Iteration 138/1000 | Loss: 0.00002744
Iteration 139/1000 | Loss: 0.00002744
Iteration 140/1000 | Loss: 0.00002744
Iteration 141/1000 | Loss: 0.00002744
Iteration 142/1000 | Loss: 0.00002744
Iteration 143/1000 | Loss: 0.00002744
Iteration 144/1000 | Loss: 0.00002744
Iteration 145/1000 | Loss: 0.00002744
Iteration 146/1000 | Loss: 0.00002744
Iteration 147/1000 | Loss: 0.00002743
Iteration 148/1000 | Loss: 0.00002743
Iteration 149/1000 | Loss: 0.00002743
Iteration 150/1000 | Loss: 0.00002742
Iteration 151/1000 | Loss: 0.00002742
Iteration 152/1000 | Loss: 0.00002742
Iteration 153/1000 | Loss: 0.00002742
Iteration 154/1000 | Loss: 0.00002742
Iteration 155/1000 | Loss: 0.00002742
Iteration 156/1000 | Loss: 0.00002742
Iteration 157/1000 | Loss: 0.00002741
Iteration 158/1000 | Loss: 0.00002741
Iteration 159/1000 | Loss: 0.00002740
Iteration 160/1000 | Loss: 0.00002740
Iteration 161/1000 | Loss: 0.00002740
Iteration 162/1000 | Loss: 0.00002740
Iteration 163/1000 | Loss: 0.00002740
Iteration 164/1000 | Loss: 0.00002740
Iteration 165/1000 | Loss: 0.00002740
Iteration 166/1000 | Loss: 0.00002740
Iteration 167/1000 | Loss: 0.00002740
Iteration 168/1000 | Loss: 0.00002739
Iteration 169/1000 | Loss: 0.00002739
Iteration 170/1000 | Loss: 0.00002739
Iteration 171/1000 | Loss: 0.00002739
Iteration 172/1000 | Loss: 0.00002739
Iteration 173/1000 | Loss: 0.00002739
Iteration 174/1000 | Loss: 0.00002739
Iteration 175/1000 | Loss: 0.00002739
Iteration 176/1000 | Loss: 0.00002739
Iteration 177/1000 | Loss: 0.00002738
Iteration 178/1000 | Loss: 0.00002738
Iteration 179/1000 | Loss: 0.00002738
Iteration 180/1000 | Loss: 0.00002738
Iteration 181/1000 | Loss: 0.00002738
Iteration 182/1000 | Loss: 0.00002737
Iteration 183/1000 | Loss: 0.00002737
Iteration 184/1000 | Loss: 0.00002737
Iteration 185/1000 | Loss: 0.00002737
Iteration 186/1000 | Loss: 0.00002737
Iteration 187/1000 | Loss: 0.00002737
Iteration 188/1000 | Loss: 0.00002737
Iteration 189/1000 | Loss: 0.00002737
Iteration 190/1000 | Loss: 0.00002737
Iteration 191/1000 | Loss: 0.00002737
Iteration 192/1000 | Loss: 0.00002736
Iteration 193/1000 | Loss: 0.00002736
Iteration 194/1000 | Loss: 0.00002736
Iteration 195/1000 | Loss: 0.00002736
Iteration 196/1000 | Loss: 0.00002736
Iteration 197/1000 | Loss: 0.00002736
Iteration 198/1000 | Loss: 0.00002736
Iteration 199/1000 | Loss: 0.00002735
Iteration 200/1000 | Loss: 0.00002735
Iteration 201/1000 | Loss: 0.00002735
Iteration 202/1000 | Loss: 0.00002735
Iteration 203/1000 | Loss: 0.00002735
Iteration 204/1000 | Loss: 0.00002735
Iteration 205/1000 | Loss: 0.00002735
Iteration 206/1000 | Loss: 0.00002735
Iteration 207/1000 | Loss: 0.00002735
Iteration 208/1000 | Loss: 0.00002735
Iteration 209/1000 | Loss: 0.00002735
Iteration 210/1000 | Loss: 0.00002735
Iteration 211/1000 | Loss: 0.00002735
Iteration 212/1000 | Loss: 0.00002735
Iteration 213/1000 | Loss: 0.00002735
Iteration 214/1000 | Loss: 0.00002735
Iteration 215/1000 | Loss: 0.00002735
Iteration 216/1000 | Loss: 0.00002735
Iteration 217/1000 | Loss: 0.00002735
Iteration 218/1000 | Loss: 0.00002735
Iteration 219/1000 | Loss: 0.00002735
Iteration 220/1000 | Loss: 0.00002735
Iteration 221/1000 | Loss: 0.00002735
Iteration 222/1000 | Loss: 0.00002735
Iteration 223/1000 | Loss: 0.00002735
Iteration 224/1000 | Loss: 0.00002735
Iteration 225/1000 | Loss: 0.00002735
Iteration 226/1000 | Loss: 0.00002735
Iteration 227/1000 | Loss: 0.00002735
Iteration 228/1000 | Loss: 0.00002735
Iteration 229/1000 | Loss: 0.00002735
Iteration 230/1000 | Loss: 0.00002735
Iteration 231/1000 | Loss: 0.00002735
Iteration 232/1000 | Loss: 0.00002735
Iteration 233/1000 | Loss: 0.00002735
Iteration 234/1000 | Loss: 0.00002735
Iteration 235/1000 | Loss: 0.00002735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.7351712560630403e-05, 2.7351712560630403e-05, 2.7351712560630403e-05, 2.7351712560630403e-05, 2.7351712560630403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7351712560630403e-05

Optimization complete. Final v2v error: 4.413211345672607 mm

Highest mean error: 5.186029434204102 mm for frame 36

Lowest mean error: 3.9383304119110107 mm for frame 14

Saving results

Total time: 44.107093334198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535341
Iteration 2/25 | Loss: 0.00153728
Iteration 3/25 | Loss: 0.00137973
Iteration 4/25 | Loss: 0.00136664
Iteration 5/25 | Loss: 0.00136547
Iteration 6/25 | Loss: 0.00136547
Iteration 7/25 | Loss: 0.00136547
Iteration 8/25 | Loss: 0.00136547
Iteration 9/25 | Loss: 0.00136547
Iteration 10/25 | Loss: 0.00136547
Iteration 11/25 | Loss: 0.00136547
Iteration 12/25 | Loss: 0.00136547
Iteration 13/25 | Loss: 0.00136547
Iteration 14/25 | Loss: 0.00136547
Iteration 15/25 | Loss: 0.00136547
Iteration 16/25 | Loss: 0.00136547
Iteration 17/25 | Loss: 0.00136547
Iteration 18/25 | Loss: 0.00136547
Iteration 19/25 | Loss: 0.00136547
Iteration 20/25 | Loss: 0.00136547
Iteration 21/25 | Loss: 0.00136547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013654738431796432, 0.0013654738431796432, 0.0013654738431796432, 0.0013654738431796432, 0.0013654738431796432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013654738431796432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82038528
Iteration 2/25 | Loss: 0.00120482
Iteration 3/25 | Loss: 0.00120482
Iteration 4/25 | Loss: 0.00120482
Iteration 5/25 | Loss: 0.00120482
Iteration 6/25 | Loss: 0.00120482
Iteration 7/25 | Loss: 0.00120482
Iteration 8/25 | Loss: 0.00120482
Iteration 9/25 | Loss: 0.00120481
Iteration 10/25 | Loss: 0.00120481
Iteration 11/25 | Loss: 0.00120481
Iteration 12/25 | Loss: 0.00120481
Iteration 13/25 | Loss: 0.00120481
Iteration 14/25 | Loss: 0.00120481
Iteration 15/25 | Loss: 0.00120481
Iteration 16/25 | Loss: 0.00120481
Iteration 17/25 | Loss: 0.00120481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012048145290464163, 0.0012048145290464163, 0.0012048145290464163, 0.0012048145290464163, 0.0012048145290464163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012048145290464163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120481
Iteration 2/1000 | Loss: 0.00003369
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002059
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001754
Iteration 11/1000 | Loss: 0.00001724
Iteration 12/1000 | Loss: 0.00001698
Iteration 13/1000 | Loss: 0.00001672
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001637
Iteration 16/1000 | Loss: 0.00001634
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001585
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001569
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001562
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001561
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001559
Iteration 37/1000 | Loss: 0.00001558
Iteration 38/1000 | Loss: 0.00001558
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001549
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001547
Iteration 105/1000 | Loss: 0.00001547
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001546
Iteration 113/1000 | Loss: 0.00001546
Iteration 114/1000 | Loss: 0.00001546
Iteration 115/1000 | Loss: 0.00001546
Iteration 116/1000 | Loss: 0.00001546
Iteration 117/1000 | Loss: 0.00001546
Iteration 118/1000 | Loss: 0.00001546
Iteration 119/1000 | Loss: 0.00001546
Iteration 120/1000 | Loss: 0.00001546
Iteration 121/1000 | Loss: 0.00001546
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001546
Iteration 124/1000 | Loss: 0.00001545
Iteration 125/1000 | Loss: 0.00001545
Iteration 126/1000 | Loss: 0.00001545
Iteration 127/1000 | Loss: 0.00001545
Iteration 128/1000 | Loss: 0.00001545
Iteration 129/1000 | Loss: 0.00001545
Iteration 130/1000 | Loss: 0.00001545
Iteration 131/1000 | Loss: 0.00001545
Iteration 132/1000 | Loss: 0.00001545
Iteration 133/1000 | Loss: 0.00001545
Iteration 134/1000 | Loss: 0.00001545
Iteration 135/1000 | Loss: 0.00001544
Iteration 136/1000 | Loss: 0.00001544
Iteration 137/1000 | Loss: 0.00001544
Iteration 138/1000 | Loss: 0.00001543
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001543
Iteration 145/1000 | Loss: 0.00001543
Iteration 146/1000 | Loss: 0.00001543
Iteration 147/1000 | Loss: 0.00001542
Iteration 148/1000 | Loss: 0.00001542
Iteration 149/1000 | Loss: 0.00001542
Iteration 150/1000 | Loss: 0.00001542
Iteration 151/1000 | Loss: 0.00001542
Iteration 152/1000 | Loss: 0.00001542
Iteration 153/1000 | Loss: 0.00001542
Iteration 154/1000 | Loss: 0.00001542
Iteration 155/1000 | Loss: 0.00001542
Iteration 156/1000 | Loss: 0.00001542
Iteration 157/1000 | Loss: 0.00001542
Iteration 158/1000 | Loss: 0.00001541
Iteration 159/1000 | Loss: 0.00001541
Iteration 160/1000 | Loss: 0.00001541
Iteration 161/1000 | Loss: 0.00001541
Iteration 162/1000 | Loss: 0.00001541
Iteration 163/1000 | Loss: 0.00001541
Iteration 164/1000 | Loss: 0.00001541
Iteration 165/1000 | Loss: 0.00001541
Iteration 166/1000 | Loss: 0.00001541
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001540
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001539
Iteration 177/1000 | Loss: 0.00001539
Iteration 178/1000 | Loss: 0.00001539
Iteration 179/1000 | Loss: 0.00001539
Iteration 180/1000 | Loss: 0.00001539
Iteration 181/1000 | Loss: 0.00001539
Iteration 182/1000 | Loss: 0.00001539
Iteration 183/1000 | Loss: 0.00001539
Iteration 184/1000 | Loss: 0.00001539
Iteration 185/1000 | Loss: 0.00001539
Iteration 186/1000 | Loss: 0.00001538
Iteration 187/1000 | Loss: 0.00001538
Iteration 188/1000 | Loss: 0.00001538
Iteration 189/1000 | Loss: 0.00001538
Iteration 190/1000 | Loss: 0.00001538
Iteration 191/1000 | Loss: 0.00001538
Iteration 192/1000 | Loss: 0.00001538
Iteration 193/1000 | Loss: 0.00001538
Iteration 194/1000 | Loss: 0.00001538
Iteration 195/1000 | Loss: 0.00001538
Iteration 196/1000 | Loss: 0.00001538
Iteration 197/1000 | Loss: 0.00001538
Iteration 198/1000 | Loss: 0.00001538
Iteration 199/1000 | Loss: 0.00001538
Iteration 200/1000 | Loss: 0.00001538
Iteration 201/1000 | Loss: 0.00001537
Iteration 202/1000 | Loss: 0.00001537
Iteration 203/1000 | Loss: 0.00001537
Iteration 204/1000 | Loss: 0.00001537
Iteration 205/1000 | Loss: 0.00001537
Iteration 206/1000 | Loss: 0.00001537
Iteration 207/1000 | Loss: 0.00001537
Iteration 208/1000 | Loss: 0.00001537
Iteration 209/1000 | Loss: 0.00001537
Iteration 210/1000 | Loss: 0.00001537
Iteration 211/1000 | Loss: 0.00001537
Iteration 212/1000 | Loss: 0.00001537
Iteration 213/1000 | Loss: 0.00001537
Iteration 214/1000 | Loss: 0.00001537
Iteration 215/1000 | Loss: 0.00001537
Iteration 216/1000 | Loss: 0.00001537
Iteration 217/1000 | Loss: 0.00001537
Iteration 218/1000 | Loss: 0.00001537
Iteration 219/1000 | Loss: 0.00001537
Iteration 220/1000 | Loss: 0.00001537
Iteration 221/1000 | Loss: 0.00001537
Iteration 222/1000 | Loss: 0.00001537
Iteration 223/1000 | Loss: 0.00001537
Iteration 224/1000 | Loss: 0.00001537
Iteration 225/1000 | Loss: 0.00001537
Iteration 226/1000 | Loss: 0.00001537
Iteration 227/1000 | Loss: 0.00001537
Iteration 228/1000 | Loss: 0.00001537
Iteration 229/1000 | Loss: 0.00001537
Iteration 230/1000 | Loss: 0.00001537
Iteration 231/1000 | Loss: 0.00001537
Iteration 232/1000 | Loss: 0.00001537
Iteration 233/1000 | Loss: 0.00001537
Iteration 234/1000 | Loss: 0.00001537
Iteration 235/1000 | Loss: 0.00001537
Iteration 236/1000 | Loss: 0.00001537
Iteration 237/1000 | Loss: 0.00001537
Iteration 238/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.5370162145700306e-05, 1.5370162145700306e-05, 1.5370162145700306e-05, 1.5370162145700306e-05, 1.5370162145700306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5370162145700306e-05

Optimization complete. Final v2v error: 3.306447982788086 mm

Highest mean error: 3.58004093170166 mm for frame 57

Lowest mean error: 2.9654884338378906 mm for frame 217

Saving results

Total time: 53.87923192977905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036662
Iteration 2/25 | Loss: 0.00173761
Iteration 3/25 | Loss: 0.00144829
Iteration 4/25 | Loss: 0.00141561
Iteration 5/25 | Loss: 0.00141731
Iteration 6/25 | Loss: 0.00142017
Iteration 7/25 | Loss: 0.00140935
Iteration 8/25 | Loss: 0.00140140
Iteration 9/25 | Loss: 0.00139652
Iteration 10/25 | Loss: 0.00139439
Iteration 11/25 | Loss: 0.00139355
Iteration 12/25 | Loss: 0.00139426
Iteration 13/25 | Loss: 0.00139292
Iteration 14/25 | Loss: 0.00139207
Iteration 15/25 | Loss: 0.00139190
Iteration 16/25 | Loss: 0.00139181
Iteration 17/25 | Loss: 0.00139181
Iteration 18/25 | Loss: 0.00139181
Iteration 19/25 | Loss: 0.00139181
Iteration 20/25 | Loss: 0.00139181
Iteration 21/25 | Loss: 0.00139181
Iteration 22/25 | Loss: 0.00139181
Iteration 23/25 | Loss: 0.00139181
Iteration 24/25 | Loss: 0.00139181
Iteration 25/25 | Loss: 0.00139181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08092928
Iteration 2/25 | Loss: 0.00161709
Iteration 3/25 | Loss: 0.00161709
Iteration 4/25 | Loss: 0.00161709
Iteration 5/25 | Loss: 0.00161709
Iteration 6/25 | Loss: 0.00161709
Iteration 7/25 | Loss: 0.00161709
Iteration 8/25 | Loss: 0.00161709
Iteration 9/25 | Loss: 0.00161709
Iteration 10/25 | Loss: 0.00161709
Iteration 11/25 | Loss: 0.00161709
Iteration 12/25 | Loss: 0.00161709
Iteration 13/25 | Loss: 0.00161709
Iteration 14/25 | Loss: 0.00161709
Iteration 15/25 | Loss: 0.00161709
Iteration 16/25 | Loss: 0.00161709
Iteration 17/25 | Loss: 0.00161709
Iteration 18/25 | Loss: 0.00161709
Iteration 19/25 | Loss: 0.00161709
Iteration 20/25 | Loss: 0.00161709
Iteration 21/25 | Loss: 0.00161709
Iteration 22/25 | Loss: 0.00161709
Iteration 23/25 | Loss: 0.00161709
Iteration 24/25 | Loss: 0.00161709
Iteration 25/25 | Loss: 0.00161709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001617087284103036, 0.001617087284103036, 0.001617087284103036, 0.001617087284103036, 0.001617087284103036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001617087284103036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161709
Iteration 2/1000 | Loss: 0.00006208
Iteration 3/1000 | Loss: 0.00003689
Iteration 4/1000 | Loss: 0.00003013
Iteration 5/1000 | Loss: 0.00002831
Iteration 6/1000 | Loss: 0.00002679
Iteration 7/1000 | Loss: 0.00002592
Iteration 8/1000 | Loss: 0.00002517
Iteration 9/1000 | Loss: 0.00002474
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002410
Iteration 12/1000 | Loss: 0.00002391
Iteration 13/1000 | Loss: 0.00002386
Iteration 14/1000 | Loss: 0.00002367
Iteration 15/1000 | Loss: 0.00002346
Iteration 16/1000 | Loss: 0.00002333
Iteration 17/1000 | Loss: 0.00002324
Iteration 18/1000 | Loss: 0.00002309
Iteration 19/1000 | Loss: 0.00002299
Iteration 20/1000 | Loss: 0.00002293
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002290
Iteration 23/1000 | Loss: 0.00002290
Iteration 24/1000 | Loss: 0.00002289
Iteration 25/1000 | Loss: 0.00002288
Iteration 26/1000 | Loss: 0.00002288
Iteration 27/1000 | Loss: 0.00002287
Iteration 28/1000 | Loss: 0.00002287
Iteration 29/1000 | Loss: 0.00002287
Iteration 30/1000 | Loss: 0.00002287
Iteration 31/1000 | Loss: 0.00002286
Iteration 32/1000 | Loss: 0.00002282
Iteration 33/1000 | Loss: 0.00002280
Iteration 34/1000 | Loss: 0.00002280
Iteration 35/1000 | Loss: 0.00002279
Iteration 36/1000 | Loss: 0.00002279
Iteration 37/1000 | Loss: 0.00002279
Iteration 38/1000 | Loss: 0.00002278
Iteration 39/1000 | Loss: 0.00002278
Iteration 40/1000 | Loss: 0.00002278
Iteration 41/1000 | Loss: 0.00002278
Iteration 42/1000 | Loss: 0.00002278
Iteration 43/1000 | Loss: 0.00002277
Iteration 44/1000 | Loss: 0.00002277
Iteration 45/1000 | Loss: 0.00002277
Iteration 46/1000 | Loss: 0.00002277
Iteration 47/1000 | Loss: 0.00002277
Iteration 48/1000 | Loss: 0.00002276
Iteration 49/1000 | Loss: 0.00002275
Iteration 50/1000 | Loss: 0.00002274
Iteration 51/1000 | Loss: 0.00002274
Iteration 52/1000 | Loss: 0.00002274
Iteration 53/1000 | Loss: 0.00002274
Iteration 54/1000 | Loss: 0.00002273
Iteration 55/1000 | Loss: 0.00002272
Iteration 56/1000 | Loss: 0.00002272
Iteration 57/1000 | Loss: 0.00002272
Iteration 58/1000 | Loss: 0.00002271
Iteration 59/1000 | Loss: 0.00002271
Iteration 60/1000 | Loss: 0.00002271
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002270
Iteration 63/1000 | Loss: 0.00002269
Iteration 64/1000 | Loss: 0.00002268
Iteration 65/1000 | Loss: 0.00002268
Iteration 66/1000 | Loss: 0.00002268
Iteration 67/1000 | Loss: 0.00002268
Iteration 68/1000 | Loss: 0.00002268
Iteration 69/1000 | Loss: 0.00002267
Iteration 70/1000 | Loss: 0.00002267
Iteration 71/1000 | Loss: 0.00002267
Iteration 72/1000 | Loss: 0.00002266
Iteration 73/1000 | Loss: 0.00002266
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002266
Iteration 76/1000 | Loss: 0.00002265
Iteration 77/1000 | Loss: 0.00002265
Iteration 78/1000 | Loss: 0.00002265
Iteration 79/1000 | Loss: 0.00002265
Iteration 80/1000 | Loss: 0.00002265
Iteration 81/1000 | Loss: 0.00002264
Iteration 82/1000 | Loss: 0.00002264
Iteration 83/1000 | Loss: 0.00002263
Iteration 84/1000 | Loss: 0.00002263
Iteration 85/1000 | Loss: 0.00002263
Iteration 86/1000 | Loss: 0.00002263
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002261
Iteration 93/1000 | Loss: 0.00002261
Iteration 94/1000 | Loss: 0.00002261
Iteration 95/1000 | Loss: 0.00002261
Iteration 96/1000 | Loss: 0.00002261
Iteration 97/1000 | Loss: 0.00002260
Iteration 98/1000 | Loss: 0.00002260
Iteration 99/1000 | Loss: 0.00002260
Iteration 100/1000 | Loss: 0.00002260
Iteration 101/1000 | Loss: 0.00002259
Iteration 102/1000 | Loss: 0.00002259
Iteration 103/1000 | Loss: 0.00002259
Iteration 104/1000 | Loss: 0.00002259
Iteration 105/1000 | Loss: 0.00002259
Iteration 106/1000 | Loss: 0.00002258
Iteration 107/1000 | Loss: 0.00002258
Iteration 108/1000 | Loss: 0.00002258
Iteration 109/1000 | Loss: 0.00002258
Iteration 110/1000 | Loss: 0.00002258
Iteration 111/1000 | Loss: 0.00002258
Iteration 112/1000 | Loss: 0.00002258
Iteration 113/1000 | Loss: 0.00002258
Iteration 114/1000 | Loss: 0.00002258
Iteration 115/1000 | Loss: 0.00002258
Iteration 116/1000 | Loss: 0.00002257
Iteration 117/1000 | Loss: 0.00002257
Iteration 118/1000 | Loss: 0.00002257
Iteration 119/1000 | Loss: 0.00002257
Iteration 120/1000 | Loss: 0.00002257
Iteration 121/1000 | Loss: 0.00002257
Iteration 122/1000 | Loss: 0.00002257
Iteration 123/1000 | Loss: 0.00002257
Iteration 124/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.257106280012522e-05, 2.257106280012522e-05, 2.257106280012522e-05, 2.257106280012522e-05, 2.257106280012522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.257106280012522e-05

Optimization complete. Final v2v error: 3.9441347122192383 mm

Highest mean error: 5.32926082611084 mm for frame 47

Lowest mean error: 3.495973825454712 mm for frame 150

Saving results

Total time: 65.40903091430664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360499
Iteration 2/25 | Loss: 0.00147426
Iteration 3/25 | Loss: 0.00131896
Iteration 4/25 | Loss: 0.00130103
Iteration 5/25 | Loss: 0.00129726
Iteration 6/25 | Loss: 0.00129631
Iteration 7/25 | Loss: 0.00129631
Iteration 8/25 | Loss: 0.00129631
Iteration 9/25 | Loss: 0.00129631
Iteration 10/25 | Loss: 0.00129631
Iteration 11/25 | Loss: 0.00129631
Iteration 12/25 | Loss: 0.00129631
Iteration 13/25 | Loss: 0.00129631
Iteration 14/25 | Loss: 0.00129631
Iteration 15/25 | Loss: 0.00129631
Iteration 16/25 | Loss: 0.00129631
Iteration 17/25 | Loss: 0.00129631
Iteration 18/25 | Loss: 0.00129631
Iteration 19/25 | Loss: 0.00129631
Iteration 20/25 | Loss: 0.00129631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012963126646354795, 0.0012963126646354795, 0.0012963126646354795, 0.0012963126646354795, 0.0012963126646354795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012963126646354795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32918835
Iteration 2/25 | Loss: 0.00160552
Iteration 3/25 | Loss: 0.00160552
Iteration 4/25 | Loss: 0.00160552
Iteration 5/25 | Loss: 0.00160552
Iteration 6/25 | Loss: 0.00160552
Iteration 7/25 | Loss: 0.00160552
Iteration 8/25 | Loss: 0.00160552
Iteration 9/25 | Loss: 0.00160552
Iteration 10/25 | Loss: 0.00160552
Iteration 11/25 | Loss: 0.00160552
Iteration 12/25 | Loss: 0.00160552
Iteration 13/25 | Loss: 0.00160552
Iteration 14/25 | Loss: 0.00160552
Iteration 15/25 | Loss: 0.00160551
Iteration 16/25 | Loss: 0.00160551
Iteration 17/25 | Loss: 0.00160551
Iteration 18/25 | Loss: 0.00160551
Iteration 19/25 | Loss: 0.00160551
Iteration 20/25 | Loss: 0.00160551
Iteration 21/25 | Loss: 0.00160551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016055147862061858, 0.0016055147862061858, 0.0016055147862061858, 0.0016055147862061858, 0.0016055147862061858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016055147862061858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160551
Iteration 2/1000 | Loss: 0.00003432
Iteration 3/1000 | Loss: 0.00002019
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001638
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001516
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001392
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001358
Iteration 15/1000 | Loss: 0.00001355
Iteration 16/1000 | Loss: 0.00001351
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001347
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001336
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001325
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001319
Iteration 150/1000 | Loss: 0.00001319
Iteration 151/1000 | Loss: 0.00001319
Iteration 152/1000 | Loss: 0.00001319
Iteration 153/1000 | Loss: 0.00001318
Iteration 154/1000 | Loss: 0.00001318
Iteration 155/1000 | Loss: 0.00001318
Iteration 156/1000 | Loss: 0.00001318
Iteration 157/1000 | Loss: 0.00001317
Iteration 158/1000 | Loss: 0.00001317
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Iteration 161/1000 | Loss: 0.00001315
Iteration 162/1000 | Loss: 0.00001315
Iteration 163/1000 | Loss: 0.00001315
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001313
Iteration 169/1000 | Loss: 0.00001313
Iteration 170/1000 | Loss: 0.00001312
Iteration 171/1000 | Loss: 0.00001312
Iteration 172/1000 | Loss: 0.00001312
Iteration 173/1000 | Loss: 0.00001312
Iteration 174/1000 | Loss: 0.00001311
Iteration 175/1000 | Loss: 0.00001311
Iteration 176/1000 | Loss: 0.00001311
Iteration 177/1000 | Loss: 0.00001311
Iteration 178/1000 | Loss: 0.00001310
Iteration 179/1000 | Loss: 0.00001310
Iteration 180/1000 | Loss: 0.00001310
Iteration 181/1000 | Loss: 0.00001310
Iteration 182/1000 | Loss: 0.00001310
Iteration 183/1000 | Loss: 0.00001310
Iteration 184/1000 | Loss: 0.00001310
Iteration 185/1000 | Loss: 0.00001310
Iteration 186/1000 | Loss: 0.00001310
Iteration 187/1000 | Loss: 0.00001309
Iteration 188/1000 | Loss: 0.00001309
Iteration 189/1000 | Loss: 0.00001309
Iteration 190/1000 | Loss: 0.00001309
Iteration 191/1000 | Loss: 0.00001309
Iteration 192/1000 | Loss: 0.00001309
Iteration 193/1000 | Loss: 0.00001309
Iteration 194/1000 | Loss: 0.00001309
Iteration 195/1000 | Loss: 0.00001308
Iteration 196/1000 | Loss: 0.00001308
Iteration 197/1000 | Loss: 0.00001308
Iteration 198/1000 | Loss: 0.00001308
Iteration 199/1000 | Loss: 0.00001308
Iteration 200/1000 | Loss: 0.00001308
Iteration 201/1000 | Loss: 0.00001308
Iteration 202/1000 | Loss: 0.00001308
Iteration 203/1000 | Loss: 0.00001308
Iteration 204/1000 | Loss: 0.00001308
Iteration 205/1000 | Loss: 0.00001308
Iteration 206/1000 | Loss: 0.00001308
Iteration 207/1000 | Loss: 0.00001308
Iteration 208/1000 | Loss: 0.00001308
Iteration 209/1000 | Loss: 0.00001308
Iteration 210/1000 | Loss: 0.00001308
Iteration 211/1000 | Loss: 0.00001308
Iteration 212/1000 | Loss: 0.00001308
Iteration 213/1000 | Loss: 0.00001307
Iteration 214/1000 | Loss: 0.00001307
Iteration 215/1000 | Loss: 0.00001307
Iteration 216/1000 | Loss: 0.00001307
Iteration 217/1000 | Loss: 0.00001307
Iteration 218/1000 | Loss: 0.00001307
Iteration 219/1000 | Loss: 0.00001307
Iteration 220/1000 | Loss: 0.00001307
Iteration 221/1000 | Loss: 0.00001307
Iteration 222/1000 | Loss: 0.00001307
Iteration 223/1000 | Loss: 0.00001307
Iteration 224/1000 | Loss: 0.00001307
Iteration 225/1000 | Loss: 0.00001307
Iteration 226/1000 | Loss: 0.00001307
Iteration 227/1000 | Loss: 0.00001306
Iteration 228/1000 | Loss: 0.00001306
Iteration 229/1000 | Loss: 0.00001306
Iteration 230/1000 | Loss: 0.00001306
Iteration 231/1000 | Loss: 0.00001306
Iteration 232/1000 | Loss: 0.00001306
Iteration 233/1000 | Loss: 0.00001306
Iteration 234/1000 | Loss: 0.00001306
Iteration 235/1000 | Loss: 0.00001305
Iteration 236/1000 | Loss: 0.00001305
Iteration 237/1000 | Loss: 0.00001305
Iteration 238/1000 | Loss: 0.00001305
Iteration 239/1000 | Loss: 0.00001305
Iteration 240/1000 | Loss: 0.00001305
Iteration 241/1000 | Loss: 0.00001305
Iteration 242/1000 | Loss: 0.00001305
Iteration 243/1000 | Loss: 0.00001305
Iteration 244/1000 | Loss: 0.00001305
Iteration 245/1000 | Loss: 0.00001305
Iteration 246/1000 | Loss: 0.00001305
Iteration 247/1000 | Loss: 0.00001305
Iteration 248/1000 | Loss: 0.00001305
Iteration 249/1000 | Loss: 0.00001305
Iteration 250/1000 | Loss: 0.00001305
Iteration 251/1000 | Loss: 0.00001305
Iteration 252/1000 | Loss: 0.00001305
Iteration 253/1000 | Loss: 0.00001305
Iteration 254/1000 | Loss: 0.00001305
Iteration 255/1000 | Loss: 0.00001305
Iteration 256/1000 | Loss: 0.00001305
Iteration 257/1000 | Loss: 0.00001305
Iteration 258/1000 | Loss: 0.00001305
Iteration 259/1000 | Loss: 0.00001305
Iteration 260/1000 | Loss: 0.00001305
Iteration 261/1000 | Loss: 0.00001305
Iteration 262/1000 | Loss: 0.00001305
Iteration 263/1000 | Loss: 0.00001305
Iteration 264/1000 | Loss: 0.00001305
Iteration 265/1000 | Loss: 0.00001305
Iteration 266/1000 | Loss: 0.00001305
Iteration 267/1000 | Loss: 0.00001305
Iteration 268/1000 | Loss: 0.00001305
Iteration 269/1000 | Loss: 0.00001305
Iteration 270/1000 | Loss: 0.00001305
Iteration 271/1000 | Loss: 0.00001305
Iteration 272/1000 | Loss: 0.00001305
Iteration 273/1000 | Loss: 0.00001305
Iteration 274/1000 | Loss: 0.00001305
Iteration 275/1000 | Loss: 0.00001305
Iteration 276/1000 | Loss: 0.00001305
Iteration 277/1000 | Loss: 0.00001305
Iteration 278/1000 | Loss: 0.00001305
Iteration 279/1000 | Loss: 0.00001305
Iteration 280/1000 | Loss: 0.00001305
Iteration 281/1000 | Loss: 0.00001305
Iteration 282/1000 | Loss: 0.00001305
Iteration 283/1000 | Loss: 0.00001305
Iteration 284/1000 | Loss: 0.00001305
Iteration 285/1000 | Loss: 0.00001305
Iteration 286/1000 | Loss: 0.00001305
Iteration 287/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.3046608728473075e-05, 1.3046608728473075e-05, 1.3046608728473075e-05, 1.3046608728473075e-05, 1.3046608728473075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3046608728473075e-05

Optimization complete. Final v2v error: 3.090930938720703 mm

Highest mean error: 3.408940553665161 mm for frame 14

Lowest mean error: 2.8909592628479004 mm for frame 65

Saving results

Total time: 45.70380520820618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009012
Iteration 2/25 | Loss: 0.01009012
Iteration 3/25 | Loss: 0.01009012
Iteration 4/25 | Loss: 0.01009011
Iteration 5/25 | Loss: 0.01009011
Iteration 6/25 | Loss: 0.01009011
Iteration 7/25 | Loss: 0.01009011
Iteration 8/25 | Loss: 0.01009010
Iteration 9/25 | Loss: 0.01009010
Iteration 10/25 | Loss: 0.01009010
Iteration 11/25 | Loss: 0.01009010
Iteration 12/25 | Loss: 0.01009009
Iteration 13/25 | Loss: 0.01009009
Iteration 14/25 | Loss: 0.01009009
Iteration 15/25 | Loss: 0.01009009
Iteration 16/25 | Loss: 0.01009009
Iteration 17/25 | Loss: 0.01009008
Iteration 18/25 | Loss: 0.01009008
Iteration 19/25 | Loss: 0.01009008
Iteration 20/25 | Loss: 0.01009008
Iteration 21/25 | Loss: 0.01009008
Iteration 22/25 | Loss: 0.01009008
Iteration 23/25 | Loss: 0.01009008
Iteration 24/25 | Loss: 0.01009007
Iteration 25/25 | Loss: 0.01009007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42264283
Iteration 2/25 | Loss: 0.14904584
Iteration 3/25 | Loss: 0.14835989
Iteration 4/25 | Loss: 0.14824618
Iteration 5/25 | Loss: 0.14824618
Iteration 6/25 | Loss: 0.14824617
Iteration 7/25 | Loss: 0.14824617
Iteration 8/25 | Loss: 0.14824617
Iteration 9/25 | Loss: 0.14824617
Iteration 10/25 | Loss: 0.14824617
Iteration 11/25 | Loss: 0.14824615
Iteration 12/25 | Loss: 0.14824615
Iteration 13/25 | Loss: 0.14824615
Iteration 14/25 | Loss: 0.14824615
Iteration 15/25 | Loss: 0.14824615
Iteration 16/25 | Loss: 0.14824615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.1482461541891098, 0.1482461541891098, 0.1482461541891098, 0.1482461541891098, 0.1482461541891098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1482461541891098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14824615
Iteration 2/1000 | Loss: 0.00192396
Iteration 3/1000 | Loss: 0.00053848
Iteration 4/1000 | Loss: 0.00028604
Iteration 5/1000 | Loss: 0.00051568
Iteration 6/1000 | Loss: 0.00048546
Iteration 7/1000 | Loss: 0.00012910
Iteration 8/1000 | Loss: 0.00145474
Iteration 9/1000 | Loss: 0.00005705
Iteration 10/1000 | Loss: 0.00034050
Iteration 11/1000 | Loss: 0.00014817
Iteration 12/1000 | Loss: 0.00004354
Iteration 13/1000 | Loss: 0.00040080
Iteration 14/1000 | Loss: 0.00003835
Iteration 15/1000 | Loss: 0.00004528
Iteration 16/1000 | Loss: 0.00013987
Iteration 17/1000 | Loss: 0.00011825
Iteration 18/1000 | Loss: 0.00002741
Iteration 19/1000 | Loss: 0.00007460
Iteration 20/1000 | Loss: 0.00068828
Iteration 21/1000 | Loss: 0.00016611
Iteration 22/1000 | Loss: 0.00006152
Iteration 23/1000 | Loss: 0.00066754
Iteration 24/1000 | Loss: 0.00326308
Iteration 25/1000 | Loss: 0.00164552
Iteration 26/1000 | Loss: 0.00016158
Iteration 27/1000 | Loss: 0.00184850
Iteration 28/1000 | Loss: 0.00017378
Iteration 29/1000 | Loss: 0.00012200
Iteration 30/1000 | Loss: 0.00011467
Iteration 31/1000 | Loss: 0.00027818
Iteration 32/1000 | Loss: 0.00005098
Iteration 33/1000 | Loss: 0.00013109
Iteration 34/1000 | Loss: 0.00002935
Iteration 35/1000 | Loss: 0.00003581
Iteration 36/1000 | Loss: 0.00009064
Iteration 37/1000 | Loss: 0.00002279
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00004699
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00007680
Iteration 42/1000 | Loss: 0.00002949
Iteration 43/1000 | Loss: 0.00007306
Iteration 44/1000 | Loss: 0.00001974
Iteration 45/1000 | Loss: 0.00005217
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00006725
Iteration 48/1000 | Loss: 0.00003422
Iteration 49/1000 | Loss: 0.00004738
Iteration 50/1000 | Loss: 0.00003399
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001707
Iteration 53/1000 | Loss: 0.00021600
Iteration 54/1000 | Loss: 0.00004994
Iteration 55/1000 | Loss: 0.00002607
Iteration 56/1000 | Loss: 0.00008954
Iteration 57/1000 | Loss: 0.00005183
Iteration 58/1000 | Loss: 0.00003642
Iteration 59/1000 | Loss: 0.00004226
Iteration 60/1000 | Loss: 0.00002750
Iteration 61/1000 | Loss: 0.00001995
Iteration 62/1000 | Loss: 0.00015184
Iteration 63/1000 | Loss: 0.00001845
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00002337
Iteration 66/1000 | Loss: 0.00002142
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001472
Iteration 70/1000 | Loss: 0.00002251
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00002870
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001676
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.4007136087457184e-05, 1.4007136087457184e-05, 1.4007136087457184e-05, 1.4007136087457184e-05, 1.4007136087457184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4007136087457184e-05

Optimization complete. Final v2v error: 3.200697660446167 mm

Highest mean error: 3.9942893981933594 mm for frame 2

Lowest mean error: 2.8409626483917236 mm for frame 39

Saving results

Total time: 129.257093667984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503779
Iteration 2/25 | Loss: 0.00138716
Iteration 3/25 | Loss: 0.00132977
Iteration 4/25 | Loss: 0.00131817
Iteration 5/25 | Loss: 0.00131487
Iteration 6/25 | Loss: 0.00131406
Iteration 7/25 | Loss: 0.00131406
Iteration 8/25 | Loss: 0.00131406
Iteration 9/25 | Loss: 0.00131406
Iteration 10/25 | Loss: 0.00131406
Iteration 11/25 | Loss: 0.00131406
Iteration 12/25 | Loss: 0.00131406
Iteration 13/25 | Loss: 0.00131406
Iteration 14/25 | Loss: 0.00131406
Iteration 15/25 | Loss: 0.00131406
Iteration 16/25 | Loss: 0.00131406
Iteration 17/25 | Loss: 0.00131406
Iteration 18/25 | Loss: 0.00131406
Iteration 19/25 | Loss: 0.00131406
Iteration 20/25 | Loss: 0.00131406
Iteration 21/25 | Loss: 0.00131406
Iteration 22/25 | Loss: 0.00131406
Iteration 23/25 | Loss: 0.00131406
Iteration 24/25 | Loss: 0.00131406
Iteration 25/25 | Loss: 0.00131406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63532305
Iteration 2/25 | Loss: 0.00147752
Iteration 3/25 | Loss: 0.00147752
Iteration 4/25 | Loss: 0.00147752
Iteration 5/25 | Loss: 0.00147752
Iteration 6/25 | Loss: 0.00147752
Iteration 7/25 | Loss: 0.00147752
Iteration 8/25 | Loss: 0.00147752
Iteration 9/25 | Loss: 0.00147752
Iteration 10/25 | Loss: 0.00147752
Iteration 11/25 | Loss: 0.00147752
Iteration 12/25 | Loss: 0.00147752
Iteration 13/25 | Loss: 0.00147752
Iteration 14/25 | Loss: 0.00147752
Iteration 15/25 | Loss: 0.00147752
Iteration 16/25 | Loss: 0.00147752
Iteration 17/25 | Loss: 0.00147752
Iteration 18/25 | Loss: 0.00147752
Iteration 19/25 | Loss: 0.00147752
Iteration 20/25 | Loss: 0.00147752
Iteration 21/25 | Loss: 0.00147752
Iteration 22/25 | Loss: 0.00147752
Iteration 23/25 | Loss: 0.00147752
Iteration 24/25 | Loss: 0.00147752
Iteration 25/25 | Loss: 0.00147752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147752
Iteration 2/1000 | Loss: 0.00004032
Iteration 3/1000 | Loss: 0.00003000
Iteration 4/1000 | Loss: 0.00002577
Iteration 5/1000 | Loss: 0.00002417
Iteration 6/1000 | Loss: 0.00002307
Iteration 7/1000 | Loss: 0.00002237
Iteration 8/1000 | Loss: 0.00002169
Iteration 9/1000 | Loss: 0.00002122
Iteration 10/1000 | Loss: 0.00002082
Iteration 11/1000 | Loss: 0.00002044
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00001989
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001947
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001923
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001918
Iteration 40/1000 | Loss: 0.00001918
Iteration 41/1000 | Loss: 0.00001917
Iteration 42/1000 | Loss: 0.00001917
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001913
Iteration 53/1000 | Loss: 0.00001913
Iteration 54/1000 | Loss: 0.00001913
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001907
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001906
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001903
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001902
Iteration 84/1000 | Loss: 0.00001902
Iteration 85/1000 | Loss: 0.00001902
Iteration 86/1000 | Loss: 0.00001901
Iteration 87/1000 | Loss: 0.00001901
Iteration 88/1000 | Loss: 0.00001901
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001898
Iteration 100/1000 | Loss: 0.00001898
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001897
Iteration 104/1000 | Loss: 0.00001897
Iteration 105/1000 | Loss: 0.00001897
Iteration 106/1000 | Loss: 0.00001897
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001896
Iteration 111/1000 | Loss: 0.00001896
Iteration 112/1000 | Loss: 0.00001896
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001895
Iteration 117/1000 | Loss: 0.00001895
Iteration 118/1000 | Loss: 0.00001895
Iteration 119/1000 | Loss: 0.00001895
Iteration 120/1000 | Loss: 0.00001895
Iteration 121/1000 | Loss: 0.00001895
Iteration 122/1000 | Loss: 0.00001895
Iteration 123/1000 | Loss: 0.00001895
Iteration 124/1000 | Loss: 0.00001895
Iteration 125/1000 | Loss: 0.00001895
Iteration 126/1000 | Loss: 0.00001895
Iteration 127/1000 | Loss: 0.00001894
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001894
Iteration 130/1000 | Loss: 0.00001894
Iteration 131/1000 | Loss: 0.00001894
Iteration 132/1000 | Loss: 0.00001894
Iteration 133/1000 | Loss: 0.00001894
Iteration 134/1000 | Loss: 0.00001894
Iteration 135/1000 | Loss: 0.00001894
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001893
Iteration 143/1000 | Loss: 0.00001893
Iteration 144/1000 | Loss: 0.00001893
Iteration 145/1000 | Loss: 0.00001893
Iteration 146/1000 | Loss: 0.00001893
Iteration 147/1000 | Loss: 0.00001893
Iteration 148/1000 | Loss: 0.00001893
Iteration 149/1000 | Loss: 0.00001893
Iteration 150/1000 | Loss: 0.00001893
Iteration 151/1000 | Loss: 0.00001893
Iteration 152/1000 | Loss: 0.00001893
Iteration 153/1000 | Loss: 0.00001893
Iteration 154/1000 | Loss: 0.00001893
Iteration 155/1000 | Loss: 0.00001893
Iteration 156/1000 | Loss: 0.00001893
Iteration 157/1000 | Loss: 0.00001893
Iteration 158/1000 | Loss: 0.00001893
Iteration 159/1000 | Loss: 0.00001893
Iteration 160/1000 | Loss: 0.00001893
Iteration 161/1000 | Loss: 0.00001893
Iteration 162/1000 | Loss: 0.00001893
Iteration 163/1000 | Loss: 0.00001893
Iteration 164/1000 | Loss: 0.00001893
Iteration 165/1000 | Loss: 0.00001893
Iteration 166/1000 | Loss: 0.00001893
Iteration 167/1000 | Loss: 0.00001893
Iteration 168/1000 | Loss: 0.00001893
Iteration 169/1000 | Loss: 0.00001893
Iteration 170/1000 | Loss: 0.00001893
Iteration 171/1000 | Loss: 0.00001893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.8928600184153765e-05, 1.8928600184153765e-05, 1.8928600184153765e-05, 1.8928600184153765e-05, 1.8928600184153765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8928600184153765e-05

Optimization complete. Final v2v error: 3.7120163440704346 mm

Highest mean error: 4.025031566619873 mm for frame 51

Lowest mean error: 3.3341050148010254 mm for frame 36

Saving results

Total time: 43.6672580242157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754494
Iteration 2/25 | Loss: 0.00165125
Iteration 3/25 | Loss: 0.00143592
Iteration 4/25 | Loss: 0.00141654
Iteration 5/25 | Loss: 0.00141261
Iteration 6/25 | Loss: 0.00141261
Iteration 7/25 | Loss: 0.00141261
Iteration 8/25 | Loss: 0.00141261
Iteration 9/25 | Loss: 0.00141261
Iteration 10/25 | Loss: 0.00141261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014126134337857366, 0.0014126134337857366, 0.0014126134337857366, 0.0014126134337857366, 0.0014126134337857366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014126134337857366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17791724
Iteration 2/25 | Loss: 0.00113277
Iteration 3/25 | Loss: 0.00113274
Iteration 4/25 | Loss: 0.00113274
Iteration 5/25 | Loss: 0.00113274
Iteration 6/25 | Loss: 0.00113274
Iteration 7/25 | Loss: 0.00113274
Iteration 8/25 | Loss: 0.00113274
Iteration 9/25 | Loss: 0.00113274
Iteration 10/25 | Loss: 0.00113274
Iteration 11/25 | Loss: 0.00113274
Iteration 12/25 | Loss: 0.00113274
Iteration 13/25 | Loss: 0.00113273
Iteration 14/25 | Loss: 0.00113273
Iteration 15/25 | Loss: 0.00113274
Iteration 16/25 | Loss: 0.00113273
Iteration 17/25 | Loss: 0.00113273
Iteration 18/25 | Loss: 0.00113273
Iteration 19/25 | Loss: 0.00113273
Iteration 20/25 | Loss: 0.00113273
Iteration 21/25 | Loss: 0.00113273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011327349347993731, 0.0011327349347993731, 0.0011327349347993731, 0.0011327349347993731, 0.0011327349347993731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011327349347993731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113273
Iteration 2/1000 | Loss: 0.00004202
Iteration 3/1000 | Loss: 0.00002826
Iteration 4/1000 | Loss: 0.00002538
Iteration 5/1000 | Loss: 0.00002362
Iteration 6/1000 | Loss: 0.00002276
Iteration 7/1000 | Loss: 0.00002191
Iteration 8/1000 | Loss: 0.00002149
Iteration 9/1000 | Loss: 0.00002107
Iteration 10/1000 | Loss: 0.00002074
Iteration 11/1000 | Loss: 0.00002045
Iteration 12/1000 | Loss: 0.00002039
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002000
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001986
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001981
Iteration 19/1000 | Loss: 0.00001968
Iteration 20/1000 | Loss: 0.00001958
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001934
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001933
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001927
Iteration 36/1000 | Loss: 0.00001927
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001925
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001923
Iteration 49/1000 | Loss: 0.00001923
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001920
Iteration 55/1000 | Loss: 0.00001920
Iteration 56/1000 | Loss: 0.00001920
Iteration 57/1000 | Loss: 0.00001920
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001918
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001918
Iteration 66/1000 | Loss: 0.00001917
Iteration 67/1000 | Loss: 0.00001917
Iteration 68/1000 | Loss: 0.00001917
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001915
Iteration 77/1000 | Loss: 0.00001915
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001915
Iteration 89/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.915201028168667e-05, 1.915201028168667e-05, 1.915201028168667e-05, 1.915201028168667e-05, 1.915201028168667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.915201028168667e-05

Optimization complete. Final v2v error: 3.6018970012664795 mm

Highest mean error: 4.623669624328613 mm for frame 221

Lowest mean error: 3.0934255123138428 mm for frame 101

Saving results

Total time: 44.077470779418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952127
Iteration 2/25 | Loss: 0.00952127
Iteration 3/25 | Loss: 0.00952126
Iteration 4/25 | Loss: 0.00381191
Iteration 5/25 | Loss: 0.00225508
Iteration 6/25 | Loss: 0.00207041
Iteration 7/25 | Loss: 0.00196416
Iteration 8/25 | Loss: 0.00176437
Iteration 9/25 | Loss: 0.00167061
Iteration 10/25 | Loss: 0.00158335
Iteration 11/25 | Loss: 0.00153496
Iteration 12/25 | Loss: 0.00151745
Iteration 13/25 | Loss: 0.00152780
Iteration 14/25 | Loss: 0.00148730
Iteration 15/25 | Loss: 0.00148081
Iteration 16/25 | Loss: 0.00147063
Iteration 17/25 | Loss: 0.00146227
Iteration 18/25 | Loss: 0.00146270
Iteration 19/25 | Loss: 0.00144864
Iteration 20/25 | Loss: 0.00144790
Iteration 21/25 | Loss: 0.00144332
Iteration 22/25 | Loss: 0.00144495
Iteration 23/25 | Loss: 0.00144068
Iteration 24/25 | Loss: 0.00143931
Iteration 25/25 | Loss: 0.00144055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26531994
Iteration 2/25 | Loss: 0.00177226
Iteration 3/25 | Loss: 0.00176310
Iteration 4/25 | Loss: 0.00176334
Iteration 5/25 | Loss: 0.00176248
Iteration 6/25 | Loss: 0.00176319
Iteration 7/25 | Loss: 0.00176311
Iteration 8/25 | Loss: 0.00176274
Iteration 9/25 | Loss: 0.00175948
Iteration 10/25 | Loss: 0.00175947
Iteration 11/25 | Loss: 0.00175947
Iteration 12/25 | Loss: 0.00175947
Iteration 13/25 | Loss: 0.00175947
Iteration 14/25 | Loss: 0.00176002
Iteration 15/25 | Loss: 0.00175947
Iteration 16/25 | Loss: 0.00175947
Iteration 17/25 | Loss: 0.00175947
Iteration 18/25 | Loss: 0.00175947
Iteration 19/25 | Loss: 0.00175947
Iteration 20/25 | Loss: 0.00175947
Iteration 21/25 | Loss: 0.00175947
Iteration 22/25 | Loss: 0.00175947
Iteration 23/25 | Loss: 0.00175947
Iteration 24/25 | Loss: 0.00175947
Iteration 25/25 | Loss: 0.00175947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175947
Iteration 2/1000 | Loss: 0.00013749
Iteration 3/1000 | Loss: 0.00022895
Iteration 4/1000 | Loss: 0.00008536
Iteration 5/1000 | Loss: 0.00022141
Iteration 6/1000 | Loss: 0.00007015
Iteration 7/1000 | Loss: 0.00006164
Iteration 8/1000 | Loss: 0.00005544
Iteration 9/1000 | Loss: 0.00006854
Iteration 10/1000 | Loss: 0.00005367
Iteration 11/1000 | Loss: 0.00007104
Iteration 12/1000 | Loss: 0.00013739
Iteration 13/1000 | Loss: 0.00004764
Iteration 14/1000 | Loss: 0.00006721
Iteration 15/1000 | Loss: 0.00030420
Iteration 16/1000 | Loss: 0.00050995
Iteration 17/1000 | Loss: 0.00005274
Iteration 18/1000 | Loss: 0.00004584
Iteration 19/1000 | Loss: 0.00006772
Iteration 20/1000 | Loss: 0.00006671
Iteration 21/1000 | Loss: 0.00004235
Iteration 22/1000 | Loss: 0.00004922
Iteration 23/1000 | Loss: 0.00005480
Iteration 24/1000 | Loss: 0.00005418
Iteration 25/1000 | Loss: 0.00004002
Iteration 26/1000 | Loss: 0.00003922
Iteration 27/1000 | Loss: 0.00003870
Iteration 28/1000 | Loss: 0.00004652
Iteration 29/1000 | Loss: 0.00004130
Iteration 30/1000 | Loss: 0.00003792
Iteration 31/1000 | Loss: 0.00004249
Iteration 32/1000 | Loss: 0.00003818
Iteration 33/1000 | Loss: 0.00004119
Iteration 34/1000 | Loss: 0.00004075
Iteration 35/1000 | Loss: 0.00003711
Iteration 36/1000 | Loss: 0.00003723
Iteration 37/1000 | Loss: 0.00003789
Iteration 38/1000 | Loss: 0.00004453
Iteration 39/1000 | Loss: 0.00003684
Iteration 40/1000 | Loss: 0.00003780
Iteration 41/1000 | Loss: 0.00004132
Iteration 42/1000 | Loss: 0.00003761
Iteration 43/1000 | Loss: 0.00003688
Iteration 44/1000 | Loss: 0.00003670
Iteration 45/1000 | Loss: 0.00003670
Iteration 46/1000 | Loss: 0.00003670
Iteration 47/1000 | Loss: 0.00003670
Iteration 48/1000 | Loss: 0.00003670
Iteration 49/1000 | Loss: 0.00003670
Iteration 50/1000 | Loss: 0.00003670
Iteration 51/1000 | Loss: 0.00003670
Iteration 52/1000 | Loss: 0.00003670
Iteration 53/1000 | Loss: 0.00003670
Iteration 54/1000 | Loss: 0.00003670
Iteration 55/1000 | Loss: 0.00003670
Iteration 56/1000 | Loss: 0.00003670
Iteration 57/1000 | Loss: 0.00003670
Iteration 58/1000 | Loss: 0.00003670
Iteration 59/1000 | Loss: 0.00003670
Iteration 60/1000 | Loss: 0.00003670
Iteration 61/1000 | Loss: 0.00003670
Iteration 62/1000 | Loss: 0.00003670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [3.6696645111078396e-05, 3.6696645111078396e-05, 3.6696645111078396e-05, 3.6696645111078396e-05, 3.6696645111078396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6696645111078396e-05

Optimization complete. Final v2v error: 4.321505546569824 mm

Highest mean error: 11.683231353759766 mm for frame 110

Lowest mean error: 3.6770949363708496 mm for frame 53

Saving results

Total time: 121.3268551826477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00638838
Iteration 2/25 | Loss: 0.00156718
Iteration 3/25 | Loss: 0.00142751
Iteration 4/25 | Loss: 0.00141549
Iteration 5/25 | Loss: 0.00141217
Iteration 6/25 | Loss: 0.00141192
Iteration 7/25 | Loss: 0.00141192
Iteration 8/25 | Loss: 0.00141192
Iteration 9/25 | Loss: 0.00141192
Iteration 10/25 | Loss: 0.00141192
Iteration 11/25 | Loss: 0.00141192
Iteration 12/25 | Loss: 0.00141192
Iteration 13/25 | Loss: 0.00141192
Iteration 14/25 | Loss: 0.00141192
Iteration 15/25 | Loss: 0.00141192
Iteration 16/25 | Loss: 0.00141192
Iteration 17/25 | Loss: 0.00141192
Iteration 18/25 | Loss: 0.00141192
Iteration 19/25 | Loss: 0.00141192
Iteration 20/25 | Loss: 0.00141192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001411917619407177, 0.001411917619407177, 0.001411917619407177, 0.001411917619407177, 0.001411917619407177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001411917619407177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51792395
Iteration 2/25 | Loss: 0.00154012
Iteration 3/25 | Loss: 0.00154012
Iteration 4/25 | Loss: 0.00154012
Iteration 5/25 | Loss: 0.00154012
Iteration 6/25 | Loss: 0.00154012
Iteration 7/25 | Loss: 0.00154012
Iteration 8/25 | Loss: 0.00154012
Iteration 9/25 | Loss: 0.00154012
Iteration 10/25 | Loss: 0.00154012
Iteration 11/25 | Loss: 0.00154012
Iteration 12/25 | Loss: 0.00154012
Iteration 13/25 | Loss: 0.00154012
Iteration 14/25 | Loss: 0.00154012
Iteration 15/25 | Loss: 0.00154012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001540121273137629, 0.001540121273137629, 0.001540121273137629, 0.001540121273137629, 0.001540121273137629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001540121273137629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154012
Iteration 2/1000 | Loss: 0.00006860
Iteration 3/1000 | Loss: 0.00004133
Iteration 4/1000 | Loss: 0.00003242
Iteration 5/1000 | Loss: 0.00002857
Iteration 6/1000 | Loss: 0.00002652
Iteration 7/1000 | Loss: 0.00002525
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002299
Iteration 12/1000 | Loss: 0.00002269
Iteration 13/1000 | Loss: 0.00002243
Iteration 14/1000 | Loss: 0.00002224
Iteration 15/1000 | Loss: 0.00002205
Iteration 16/1000 | Loss: 0.00002202
Iteration 17/1000 | Loss: 0.00002200
Iteration 18/1000 | Loss: 0.00002192
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002187
Iteration 21/1000 | Loss: 0.00002186
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002176
Iteration 24/1000 | Loss: 0.00002171
Iteration 25/1000 | Loss: 0.00002171
Iteration 26/1000 | Loss: 0.00002170
Iteration 27/1000 | Loss: 0.00002170
Iteration 28/1000 | Loss: 0.00002168
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002162
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002161
Iteration 35/1000 | Loss: 0.00002161
Iteration 36/1000 | Loss: 0.00002160
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002158
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002157
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00002150
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00002148
Iteration 45/1000 | Loss: 0.00002145
Iteration 46/1000 | Loss: 0.00002144
Iteration 47/1000 | Loss: 0.00002143
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00002141
Iteration 50/1000 | Loss: 0.00002140
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002139
Iteration 53/1000 | Loss: 0.00002139
Iteration 54/1000 | Loss: 0.00002138
Iteration 55/1000 | Loss: 0.00002138
Iteration 56/1000 | Loss: 0.00002138
Iteration 57/1000 | Loss: 0.00002138
Iteration 58/1000 | Loss: 0.00002138
Iteration 59/1000 | Loss: 0.00002137
Iteration 60/1000 | Loss: 0.00002137
Iteration 61/1000 | Loss: 0.00002137
Iteration 62/1000 | Loss: 0.00002136
Iteration 63/1000 | Loss: 0.00002135
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002125
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002122
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002120
Iteration 78/1000 | Loss: 0.00002119
Iteration 79/1000 | Loss: 0.00002119
Iteration 80/1000 | Loss: 0.00002119
Iteration 81/1000 | Loss: 0.00002118
Iteration 82/1000 | Loss: 0.00002118
Iteration 83/1000 | Loss: 0.00002117
Iteration 84/1000 | Loss: 0.00002117
Iteration 85/1000 | Loss: 0.00002116
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00002116
Iteration 89/1000 | Loss: 0.00002115
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002114
Iteration 93/1000 | Loss: 0.00002114
Iteration 94/1000 | Loss: 0.00002114
Iteration 95/1000 | Loss: 0.00002113
Iteration 96/1000 | Loss: 0.00002113
Iteration 97/1000 | Loss: 0.00002112
Iteration 98/1000 | Loss: 0.00002112
Iteration 99/1000 | Loss: 0.00002112
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002110
Iteration 106/1000 | Loss: 0.00002110
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002109
Iteration 109/1000 | Loss: 0.00002109
Iteration 110/1000 | Loss: 0.00002109
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00002107
Iteration 115/1000 | Loss: 0.00002107
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002106
Iteration 121/1000 | Loss: 0.00002106
Iteration 122/1000 | Loss: 0.00002106
Iteration 123/1000 | Loss: 0.00002105
Iteration 124/1000 | Loss: 0.00002105
Iteration 125/1000 | Loss: 0.00002105
Iteration 126/1000 | Loss: 0.00002105
Iteration 127/1000 | Loss: 0.00002105
Iteration 128/1000 | Loss: 0.00002104
Iteration 129/1000 | Loss: 0.00002104
Iteration 130/1000 | Loss: 0.00002104
Iteration 131/1000 | Loss: 0.00002104
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002103
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002101
Iteration 143/1000 | Loss: 0.00002101
Iteration 144/1000 | Loss: 0.00002101
Iteration 145/1000 | Loss: 0.00002101
Iteration 146/1000 | Loss: 0.00002101
Iteration 147/1000 | Loss: 0.00002101
Iteration 148/1000 | Loss: 0.00002101
Iteration 149/1000 | Loss: 0.00002100
Iteration 150/1000 | Loss: 0.00002100
Iteration 151/1000 | Loss: 0.00002100
Iteration 152/1000 | Loss: 0.00002100
Iteration 153/1000 | Loss: 0.00002099
Iteration 154/1000 | Loss: 0.00002099
Iteration 155/1000 | Loss: 0.00002099
Iteration 156/1000 | Loss: 0.00002099
Iteration 157/1000 | Loss: 0.00002099
Iteration 158/1000 | Loss: 0.00002098
Iteration 159/1000 | Loss: 0.00002098
Iteration 160/1000 | Loss: 0.00002098
Iteration 161/1000 | Loss: 0.00002098
Iteration 162/1000 | Loss: 0.00002098
Iteration 163/1000 | Loss: 0.00002098
Iteration 164/1000 | Loss: 0.00002098
Iteration 165/1000 | Loss: 0.00002098
Iteration 166/1000 | Loss: 0.00002098
Iteration 167/1000 | Loss: 0.00002097
Iteration 168/1000 | Loss: 0.00002097
Iteration 169/1000 | Loss: 0.00002097
Iteration 170/1000 | Loss: 0.00002097
Iteration 171/1000 | Loss: 0.00002097
Iteration 172/1000 | Loss: 0.00002097
Iteration 173/1000 | Loss: 0.00002097
Iteration 174/1000 | Loss: 0.00002096
Iteration 175/1000 | Loss: 0.00002096
Iteration 176/1000 | Loss: 0.00002096
Iteration 177/1000 | Loss: 0.00002096
Iteration 178/1000 | Loss: 0.00002096
Iteration 179/1000 | Loss: 0.00002096
Iteration 180/1000 | Loss: 0.00002096
Iteration 181/1000 | Loss: 0.00002095
Iteration 182/1000 | Loss: 0.00002095
Iteration 183/1000 | Loss: 0.00002095
Iteration 184/1000 | Loss: 0.00002095
Iteration 185/1000 | Loss: 0.00002095
Iteration 186/1000 | Loss: 0.00002095
Iteration 187/1000 | Loss: 0.00002095
Iteration 188/1000 | Loss: 0.00002095
Iteration 189/1000 | Loss: 0.00002094
Iteration 190/1000 | Loss: 0.00002094
Iteration 191/1000 | Loss: 0.00002094
Iteration 192/1000 | Loss: 0.00002094
Iteration 193/1000 | Loss: 0.00002094
Iteration 194/1000 | Loss: 0.00002094
Iteration 195/1000 | Loss: 0.00002094
Iteration 196/1000 | Loss: 0.00002094
Iteration 197/1000 | Loss: 0.00002094
Iteration 198/1000 | Loss: 0.00002094
Iteration 199/1000 | Loss: 0.00002094
Iteration 200/1000 | Loss: 0.00002094
Iteration 201/1000 | Loss: 0.00002094
Iteration 202/1000 | Loss: 0.00002093
Iteration 203/1000 | Loss: 0.00002093
Iteration 204/1000 | Loss: 0.00002093
Iteration 205/1000 | Loss: 0.00002093
Iteration 206/1000 | Loss: 0.00002093
Iteration 207/1000 | Loss: 0.00002093
Iteration 208/1000 | Loss: 0.00002093
Iteration 209/1000 | Loss: 0.00002093
Iteration 210/1000 | Loss: 0.00002093
Iteration 211/1000 | Loss: 0.00002093
Iteration 212/1000 | Loss: 0.00002093
Iteration 213/1000 | Loss: 0.00002093
Iteration 214/1000 | Loss: 0.00002093
Iteration 215/1000 | Loss: 0.00002093
Iteration 216/1000 | Loss: 0.00002093
Iteration 217/1000 | Loss: 0.00002093
Iteration 218/1000 | Loss: 0.00002093
Iteration 219/1000 | Loss: 0.00002093
Iteration 220/1000 | Loss: 0.00002093
Iteration 221/1000 | Loss: 0.00002093
Iteration 222/1000 | Loss: 0.00002093
Iteration 223/1000 | Loss: 0.00002093
Iteration 224/1000 | Loss: 0.00002093
Iteration 225/1000 | Loss: 0.00002093
Iteration 226/1000 | Loss: 0.00002093
Iteration 227/1000 | Loss: 0.00002093
Iteration 228/1000 | Loss: 0.00002093
Iteration 229/1000 | Loss: 0.00002093
Iteration 230/1000 | Loss: 0.00002093
Iteration 231/1000 | Loss: 0.00002093
Iteration 232/1000 | Loss: 0.00002093
Iteration 233/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.092935574182775e-05, 2.092935574182775e-05, 2.092935574182775e-05, 2.092935574182775e-05, 2.092935574182775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.092935574182775e-05

Optimization complete. Final v2v error: 3.7648072242736816 mm

Highest mean error: 4.825340270996094 mm for frame 82

Lowest mean error: 3.1515777111053467 mm for frame 38

Saving results

Total time: 59.39137649536133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475521
Iteration 2/25 | Loss: 0.00146044
Iteration 3/25 | Loss: 0.00136493
Iteration 4/25 | Loss: 0.00134788
Iteration 5/25 | Loss: 0.00134378
Iteration 6/25 | Loss: 0.00134365
Iteration 7/25 | Loss: 0.00134365
Iteration 8/25 | Loss: 0.00134365
Iteration 9/25 | Loss: 0.00134365
Iteration 10/25 | Loss: 0.00134365
Iteration 11/25 | Loss: 0.00134365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001343649928458035, 0.001343649928458035, 0.001343649928458035, 0.001343649928458035, 0.001343649928458035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001343649928458035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29242313
Iteration 2/25 | Loss: 0.00136033
Iteration 3/25 | Loss: 0.00136033
Iteration 4/25 | Loss: 0.00136033
Iteration 5/25 | Loss: 0.00136033
Iteration 6/25 | Loss: 0.00136033
Iteration 7/25 | Loss: 0.00136033
Iteration 8/25 | Loss: 0.00136033
Iteration 9/25 | Loss: 0.00136033
Iteration 10/25 | Loss: 0.00136033
Iteration 11/25 | Loss: 0.00136033
Iteration 12/25 | Loss: 0.00136033
Iteration 13/25 | Loss: 0.00136033
Iteration 14/25 | Loss: 0.00136033
Iteration 15/25 | Loss: 0.00136033
Iteration 16/25 | Loss: 0.00136033
Iteration 17/25 | Loss: 0.00136033
Iteration 18/25 | Loss: 0.00136033
Iteration 19/25 | Loss: 0.00136033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013603264233097434, 0.0013603264233097434, 0.0013603264233097434, 0.0013603264233097434, 0.0013603264233097434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013603264233097434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136033
Iteration 2/1000 | Loss: 0.00002445
Iteration 3/1000 | Loss: 0.00001944
Iteration 4/1000 | Loss: 0.00001829
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001607
Iteration 13/1000 | Loss: 0.00001593
Iteration 14/1000 | Loss: 0.00001589
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001556
Iteration 24/1000 | Loss: 0.00001556
Iteration 25/1000 | Loss: 0.00001556
Iteration 26/1000 | Loss: 0.00001555
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001544
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001543
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001536
Iteration 69/1000 | Loss: 0.00001536
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001533
Iteration 77/1000 | Loss: 0.00001533
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001532
Iteration 80/1000 | Loss: 0.00001532
Iteration 81/1000 | Loss: 0.00001532
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001531
Iteration 84/1000 | Loss: 0.00001531
Iteration 85/1000 | Loss: 0.00001531
Iteration 86/1000 | Loss: 0.00001531
Iteration 87/1000 | Loss: 0.00001531
Iteration 88/1000 | Loss: 0.00001531
Iteration 89/1000 | Loss: 0.00001531
Iteration 90/1000 | Loss: 0.00001531
Iteration 91/1000 | Loss: 0.00001531
Iteration 92/1000 | Loss: 0.00001530
Iteration 93/1000 | Loss: 0.00001530
Iteration 94/1000 | Loss: 0.00001530
Iteration 95/1000 | Loss: 0.00001530
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001530
Iteration 98/1000 | Loss: 0.00001530
Iteration 99/1000 | Loss: 0.00001529
Iteration 100/1000 | Loss: 0.00001529
Iteration 101/1000 | Loss: 0.00001529
Iteration 102/1000 | Loss: 0.00001529
Iteration 103/1000 | Loss: 0.00001529
Iteration 104/1000 | Loss: 0.00001529
Iteration 105/1000 | Loss: 0.00001529
Iteration 106/1000 | Loss: 0.00001529
Iteration 107/1000 | Loss: 0.00001529
Iteration 108/1000 | Loss: 0.00001529
Iteration 109/1000 | Loss: 0.00001529
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001528
Iteration 112/1000 | Loss: 0.00001528
Iteration 113/1000 | Loss: 0.00001528
Iteration 114/1000 | Loss: 0.00001528
Iteration 115/1000 | Loss: 0.00001528
Iteration 116/1000 | Loss: 0.00001528
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001528
Iteration 119/1000 | Loss: 0.00001527
Iteration 120/1000 | Loss: 0.00001527
Iteration 121/1000 | Loss: 0.00001527
Iteration 122/1000 | Loss: 0.00001527
Iteration 123/1000 | Loss: 0.00001526
Iteration 124/1000 | Loss: 0.00001526
Iteration 125/1000 | Loss: 0.00001526
Iteration 126/1000 | Loss: 0.00001526
Iteration 127/1000 | Loss: 0.00001526
Iteration 128/1000 | Loss: 0.00001526
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001526
Iteration 133/1000 | Loss: 0.00001526
Iteration 134/1000 | Loss: 0.00001526
Iteration 135/1000 | Loss: 0.00001526
Iteration 136/1000 | Loss: 0.00001526
Iteration 137/1000 | Loss: 0.00001526
Iteration 138/1000 | Loss: 0.00001526
Iteration 139/1000 | Loss: 0.00001526
Iteration 140/1000 | Loss: 0.00001526
Iteration 141/1000 | Loss: 0.00001526
Iteration 142/1000 | Loss: 0.00001526
Iteration 143/1000 | Loss: 0.00001526
Iteration 144/1000 | Loss: 0.00001526
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001526
Iteration 149/1000 | Loss: 0.00001526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.5258027815434616e-05, 1.5258027815434616e-05, 1.5258027815434616e-05, 1.5258027815434616e-05, 1.5258027815434616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5258027815434616e-05

Optimization complete. Final v2v error: 3.278914213180542 mm

Highest mean error: 3.635340690612793 mm for frame 147

Lowest mean error: 3.0739166736602783 mm for frame 124

Saving results

Total time: 42.14129900932312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453445
Iteration 2/25 | Loss: 0.00142350
Iteration 3/25 | Loss: 0.00134163
Iteration 4/25 | Loss: 0.00133097
Iteration 5/25 | Loss: 0.00132689
Iteration 6/25 | Loss: 0.00132689
Iteration 7/25 | Loss: 0.00132689
Iteration 8/25 | Loss: 0.00132689
Iteration 9/25 | Loss: 0.00132689
Iteration 10/25 | Loss: 0.00132689
Iteration 11/25 | Loss: 0.00132689
Iteration 12/25 | Loss: 0.00132689
Iteration 13/25 | Loss: 0.00132689
Iteration 14/25 | Loss: 0.00132689
Iteration 15/25 | Loss: 0.00132689
Iteration 16/25 | Loss: 0.00132689
Iteration 17/25 | Loss: 0.00132689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013268882175907493, 0.0013268882175907493, 0.0013268882175907493, 0.0013268882175907493, 0.0013268882175907493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013268882175907493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31085885
Iteration 2/25 | Loss: 0.00157266
Iteration 3/25 | Loss: 0.00157266
Iteration 4/25 | Loss: 0.00157266
Iteration 5/25 | Loss: 0.00157266
Iteration 6/25 | Loss: 0.00157265
Iteration 7/25 | Loss: 0.00157265
Iteration 8/25 | Loss: 0.00157265
Iteration 9/25 | Loss: 0.00157265
Iteration 10/25 | Loss: 0.00157265
Iteration 11/25 | Loss: 0.00157265
Iteration 12/25 | Loss: 0.00157265
Iteration 13/25 | Loss: 0.00157265
Iteration 14/25 | Loss: 0.00157265
Iteration 15/25 | Loss: 0.00157265
Iteration 16/25 | Loss: 0.00157265
Iteration 17/25 | Loss: 0.00157265
Iteration 18/25 | Loss: 0.00157265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015726533019915223, 0.0015726533019915223, 0.0015726533019915223, 0.0015726533019915223, 0.0015726533019915223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015726533019915223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157265
Iteration 2/1000 | Loss: 0.00002705
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001584
Iteration 7/1000 | Loss: 0.00001543
Iteration 8/1000 | Loss: 0.00001516
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001409
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001394
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001384
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001382
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001375
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001362
Iteration 70/1000 | Loss: 0.00001362
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001361
Iteration 75/1000 | Loss: 0.00001361
Iteration 76/1000 | Loss: 0.00001361
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001360
Iteration 83/1000 | Loss: 0.00001360
Iteration 84/1000 | Loss: 0.00001360
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001359
Iteration 87/1000 | Loss: 0.00001359
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001359
Iteration 96/1000 | Loss: 0.00001359
Iteration 97/1000 | Loss: 0.00001359
Iteration 98/1000 | Loss: 0.00001359
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.3590338312496897e-05, 1.3590338312496897e-05, 1.3590338312496897e-05, 1.3590338312496897e-05, 1.3590338312496897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3590338312496897e-05

Optimization complete. Final v2v error: 3.100639581680298 mm

Highest mean error: 3.5465915203094482 mm for frame 77

Lowest mean error: 2.835526943206787 mm for frame 206

Saving results

Total time: 40.318543910980225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890063
Iteration 2/25 | Loss: 0.00166589
Iteration 3/25 | Loss: 0.00143649
Iteration 4/25 | Loss: 0.00141323
Iteration 5/25 | Loss: 0.00140605
Iteration 6/25 | Loss: 0.00140403
Iteration 7/25 | Loss: 0.00140396
Iteration 8/25 | Loss: 0.00140396
Iteration 9/25 | Loss: 0.00140396
Iteration 10/25 | Loss: 0.00140396
Iteration 11/25 | Loss: 0.00140396
Iteration 12/25 | Loss: 0.00140396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014039608649909496, 0.0014039608649909496, 0.0014039608649909496, 0.0014039608649909496, 0.0014039608649909496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014039608649909496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05505860
Iteration 2/25 | Loss: 0.00121405
Iteration 3/25 | Loss: 0.00121403
Iteration 4/25 | Loss: 0.00121403
Iteration 5/25 | Loss: 0.00121403
Iteration 6/25 | Loss: 0.00121403
Iteration 7/25 | Loss: 0.00121403
Iteration 8/25 | Loss: 0.00121403
Iteration 9/25 | Loss: 0.00121403
Iteration 10/25 | Loss: 0.00121403
Iteration 11/25 | Loss: 0.00121403
Iteration 12/25 | Loss: 0.00121403
Iteration 13/25 | Loss: 0.00121403
Iteration 14/25 | Loss: 0.00121403
Iteration 15/25 | Loss: 0.00121403
Iteration 16/25 | Loss: 0.00121403
Iteration 17/25 | Loss: 0.00121403
Iteration 18/25 | Loss: 0.00121403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00121403019875288, 0.00121403019875288, 0.00121403019875288, 0.00121403019875288, 0.00121403019875288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00121403019875288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121403
Iteration 2/1000 | Loss: 0.00005740
Iteration 3/1000 | Loss: 0.00003972
Iteration 4/1000 | Loss: 0.00003220
Iteration 5/1000 | Loss: 0.00002935
Iteration 6/1000 | Loss: 0.00002802
Iteration 7/1000 | Loss: 0.00002730
Iteration 8/1000 | Loss: 0.00002639
Iteration 9/1000 | Loss: 0.00002579
Iteration 10/1000 | Loss: 0.00002540
Iteration 11/1000 | Loss: 0.00002507
Iteration 12/1000 | Loss: 0.00002481
Iteration 13/1000 | Loss: 0.00002457
Iteration 14/1000 | Loss: 0.00002434
Iteration 15/1000 | Loss: 0.00002415
Iteration 16/1000 | Loss: 0.00002398
Iteration 17/1000 | Loss: 0.00002380
Iteration 18/1000 | Loss: 0.00002371
Iteration 19/1000 | Loss: 0.00002363
Iteration 20/1000 | Loss: 0.00002357
Iteration 21/1000 | Loss: 0.00002353
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002346
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002343
Iteration 26/1000 | Loss: 0.00002341
Iteration 27/1000 | Loss: 0.00002339
Iteration 28/1000 | Loss: 0.00002338
Iteration 29/1000 | Loss: 0.00002330
Iteration 30/1000 | Loss: 0.00002330
Iteration 31/1000 | Loss: 0.00002330
Iteration 32/1000 | Loss: 0.00002330
Iteration 33/1000 | Loss: 0.00002330
Iteration 34/1000 | Loss: 0.00002330
Iteration 35/1000 | Loss: 0.00002330
Iteration 36/1000 | Loss: 0.00002330
Iteration 37/1000 | Loss: 0.00002330
Iteration 38/1000 | Loss: 0.00002330
Iteration 39/1000 | Loss: 0.00002329
Iteration 40/1000 | Loss: 0.00002329
Iteration 41/1000 | Loss: 0.00002329
Iteration 42/1000 | Loss: 0.00002329
Iteration 43/1000 | Loss: 0.00002329
Iteration 44/1000 | Loss: 0.00002329
Iteration 45/1000 | Loss: 0.00002329
Iteration 46/1000 | Loss: 0.00002329
Iteration 47/1000 | Loss: 0.00002329
Iteration 48/1000 | Loss: 0.00002329
Iteration 49/1000 | Loss: 0.00002329
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002329
Iteration 52/1000 | Loss: 0.00002329
Iteration 53/1000 | Loss: 0.00002329
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00002329
Iteration 56/1000 | Loss: 0.00002329
Iteration 57/1000 | Loss: 0.00002329
Iteration 58/1000 | Loss: 0.00002329
Iteration 59/1000 | Loss: 0.00002329
Iteration 60/1000 | Loss: 0.00002328
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002327
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002327
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002327
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002324
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002324
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002322
Iteration 96/1000 | Loss: 0.00002322
Iteration 97/1000 | Loss: 0.00002322
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00002322
Iteration 100/1000 | Loss: 0.00002322
Iteration 101/1000 | Loss: 0.00002322
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002321
Iteration 106/1000 | Loss: 0.00002321
Iteration 107/1000 | Loss: 0.00002321
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002320
Iteration 112/1000 | Loss: 0.00002320
Iteration 113/1000 | Loss: 0.00002320
Iteration 114/1000 | Loss: 0.00002320
Iteration 115/1000 | Loss: 0.00002320
Iteration 116/1000 | Loss: 0.00002320
Iteration 117/1000 | Loss: 0.00002320
Iteration 118/1000 | Loss: 0.00002319
Iteration 119/1000 | Loss: 0.00002319
Iteration 120/1000 | Loss: 0.00002319
Iteration 121/1000 | Loss: 0.00002319
Iteration 122/1000 | Loss: 0.00002319
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002318
Iteration 128/1000 | Loss: 0.00002318
Iteration 129/1000 | Loss: 0.00002318
Iteration 130/1000 | Loss: 0.00002318
Iteration 131/1000 | Loss: 0.00002318
Iteration 132/1000 | Loss: 0.00002318
Iteration 133/1000 | Loss: 0.00002318
Iteration 134/1000 | Loss: 0.00002318
Iteration 135/1000 | Loss: 0.00002318
Iteration 136/1000 | Loss: 0.00002318
Iteration 137/1000 | Loss: 0.00002317
Iteration 138/1000 | Loss: 0.00002317
Iteration 139/1000 | Loss: 0.00002317
Iteration 140/1000 | Loss: 0.00002317
Iteration 141/1000 | Loss: 0.00002317
Iteration 142/1000 | Loss: 0.00002317
Iteration 143/1000 | Loss: 0.00002317
Iteration 144/1000 | Loss: 0.00002317
Iteration 145/1000 | Loss: 0.00002317
Iteration 146/1000 | Loss: 0.00002317
Iteration 147/1000 | Loss: 0.00002317
Iteration 148/1000 | Loss: 0.00002317
Iteration 149/1000 | Loss: 0.00002316
Iteration 150/1000 | Loss: 0.00002316
Iteration 151/1000 | Loss: 0.00002316
Iteration 152/1000 | Loss: 0.00002316
Iteration 153/1000 | Loss: 0.00002316
Iteration 154/1000 | Loss: 0.00002316
Iteration 155/1000 | Loss: 0.00002316
Iteration 156/1000 | Loss: 0.00002316
Iteration 157/1000 | Loss: 0.00002316
Iteration 158/1000 | Loss: 0.00002316
Iteration 159/1000 | Loss: 0.00002316
Iteration 160/1000 | Loss: 0.00002315
Iteration 161/1000 | Loss: 0.00002315
Iteration 162/1000 | Loss: 0.00002315
Iteration 163/1000 | Loss: 0.00002315
Iteration 164/1000 | Loss: 0.00002315
Iteration 165/1000 | Loss: 0.00002314
Iteration 166/1000 | Loss: 0.00002314
Iteration 167/1000 | Loss: 0.00002314
Iteration 168/1000 | Loss: 0.00002314
Iteration 169/1000 | Loss: 0.00002314
Iteration 170/1000 | Loss: 0.00002314
Iteration 171/1000 | Loss: 0.00002314
Iteration 172/1000 | Loss: 0.00002314
Iteration 173/1000 | Loss: 0.00002314
Iteration 174/1000 | Loss: 0.00002314
Iteration 175/1000 | Loss: 0.00002314
Iteration 176/1000 | Loss: 0.00002314
Iteration 177/1000 | Loss: 0.00002314
Iteration 178/1000 | Loss: 0.00002314
Iteration 179/1000 | Loss: 0.00002314
Iteration 180/1000 | Loss: 0.00002314
Iteration 181/1000 | Loss: 0.00002314
Iteration 182/1000 | Loss: 0.00002314
Iteration 183/1000 | Loss: 0.00002314
Iteration 184/1000 | Loss: 0.00002314
Iteration 185/1000 | Loss: 0.00002314
Iteration 186/1000 | Loss: 0.00002314
Iteration 187/1000 | Loss: 0.00002314
Iteration 188/1000 | Loss: 0.00002314
Iteration 189/1000 | Loss: 0.00002314
Iteration 190/1000 | Loss: 0.00002314
Iteration 191/1000 | Loss: 0.00002314
Iteration 192/1000 | Loss: 0.00002314
Iteration 193/1000 | Loss: 0.00002314
Iteration 194/1000 | Loss: 0.00002314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.3139597033150494e-05, 2.3139597033150494e-05, 2.3139597033150494e-05, 2.3139597033150494e-05, 2.3139597033150494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3139597033150494e-05

Optimization complete. Final v2v error: 4.041402339935303 mm

Highest mean error: 5.2956767082214355 mm for frame 103

Lowest mean error: 3.3032774925231934 mm for frame 120

Saving results

Total time: 49.49620866775513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662917
Iteration 2/25 | Loss: 0.00162184
Iteration 3/25 | Loss: 0.00142238
Iteration 4/25 | Loss: 0.00140829
Iteration 5/25 | Loss: 0.00140897
Iteration 6/25 | Loss: 0.00140572
Iteration 7/25 | Loss: 0.00140407
Iteration 8/25 | Loss: 0.00140387
Iteration 9/25 | Loss: 0.00140383
Iteration 10/25 | Loss: 0.00140382
Iteration 11/25 | Loss: 0.00140382
Iteration 12/25 | Loss: 0.00140382
Iteration 13/25 | Loss: 0.00140382
Iteration 14/25 | Loss: 0.00140382
Iteration 15/25 | Loss: 0.00140382
Iteration 16/25 | Loss: 0.00140382
Iteration 17/25 | Loss: 0.00140382
Iteration 18/25 | Loss: 0.00140382
Iteration 19/25 | Loss: 0.00140382
Iteration 20/25 | Loss: 0.00140382
Iteration 21/25 | Loss: 0.00140382
Iteration 22/25 | Loss: 0.00140382
Iteration 23/25 | Loss: 0.00140382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014038155786693096, 0.0014038155786693096, 0.0014038155786693096, 0.0014038155786693096, 0.0014038155786693096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014038155786693096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.66325140
Iteration 2/25 | Loss: 0.00145866
Iteration 3/25 | Loss: 0.00145857
Iteration 4/25 | Loss: 0.00145857
Iteration 5/25 | Loss: 0.00145857
Iteration 6/25 | Loss: 0.00145857
Iteration 7/25 | Loss: 0.00145857
Iteration 8/25 | Loss: 0.00145857
Iteration 9/25 | Loss: 0.00145857
Iteration 10/25 | Loss: 0.00145857
Iteration 11/25 | Loss: 0.00145857
Iteration 12/25 | Loss: 0.00145857
Iteration 13/25 | Loss: 0.00145857
Iteration 14/25 | Loss: 0.00145857
Iteration 15/25 | Loss: 0.00145857
Iteration 16/25 | Loss: 0.00145857
Iteration 17/25 | Loss: 0.00145857
Iteration 18/25 | Loss: 0.00145857
Iteration 19/25 | Loss: 0.00145857
Iteration 20/25 | Loss: 0.00145857
Iteration 21/25 | Loss: 0.00145857
Iteration 22/25 | Loss: 0.00145857
Iteration 23/25 | Loss: 0.00145857
Iteration 24/25 | Loss: 0.00145857
Iteration 25/25 | Loss: 0.00145857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145857
Iteration 2/1000 | Loss: 0.00003410
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002325
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002081
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00001992
Iteration 9/1000 | Loss: 0.00001955
Iteration 10/1000 | Loss: 0.00001922
Iteration 11/1000 | Loss: 0.00001895
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001871
Iteration 15/1000 | Loss: 0.00001870
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001858
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001846
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001834
Iteration 24/1000 | Loss: 0.00001829
Iteration 25/1000 | Loss: 0.00001827
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001817
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001814
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001814
Iteration 38/1000 | Loss: 0.00001814
Iteration 39/1000 | Loss: 0.00001814
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001814
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001813
Iteration 49/1000 | Loss: 0.00001813
Iteration 50/1000 | Loss: 0.00001813
Iteration 51/1000 | Loss: 0.00001813
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001812
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001812
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001811
Iteration 59/1000 | Loss: 0.00001811
Iteration 60/1000 | Loss: 0.00001810
Iteration 61/1000 | Loss: 0.00001810
Iteration 62/1000 | Loss: 0.00001810
Iteration 63/1000 | Loss: 0.00001810
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00001810
Iteration 67/1000 | Loss: 0.00001810
Iteration 68/1000 | Loss: 0.00001810
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001810
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.8097642168868333e-05, 1.8097642168868333e-05, 1.8097642168868333e-05, 1.8097642168868333e-05, 1.8097642168868333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8097642168868333e-05

Optimization complete. Final v2v error: 3.5233099460601807 mm

Highest mean error: 4.008886337280273 mm for frame 119

Lowest mean error: 3.0781617164611816 mm for frame 43

Saving results

Total time: 47.265610456466675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411156
Iteration 2/25 | Loss: 0.00150708
Iteration 3/25 | Loss: 0.00131927
Iteration 4/25 | Loss: 0.00130473
Iteration 5/25 | Loss: 0.00130273
Iteration 6/25 | Loss: 0.00130229
Iteration 7/25 | Loss: 0.00130229
Iteration 8/25 | Loss: 0.00130229
Iteration 9/25 | Loss: 0.00130229
Iteration 10/25 | Loss: 0.00130229
Iteration 11/25 | Loss: 0.00130229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001302288263104856, 0.001302288263104856, 0.001302288263104856, 0.001302288263104856, 0.001302288263104856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302288263104856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29613674
Iteration 2/25 | Loss: 0.00113848
Iteration 3/25 | Loss: 0.00113847
Iteration 4/25 | Loss: 0.00113847
Iteration 5/25 | Loss: 0.00113847
Iteration 6/25 | Loss: 0.00113847
Iteration 7/25 | Loss: 0.00113847
Iteration 8/25 | Loss: 0.00113847
Iteration 9/25 | Loss: 0.00113847
Iteration 10/25 | Loss: 0.00113847
Iteration 11/25 | Loss: 0.00113847
Iteration 12/25 | Loss: 0.00113847
Iteration 13/25 | Loss: 0.00113847
Iteration 14/25 | Loss: 0.00113847
Iteration 15/25 | Loss: 0.00113847
Iteration 16/25 | Loss: 0.00113847
Iteration 17/25 | Loss: 0.00113847
Iteration 18/25 | Loss: 0.00113847
Iteration 19/25 | Loss: 0.00113847
Iteration 20/25 | Loss: 0.00113847
Iteration 21/25 | Loss: 0.00113847
Iteration 22/25 | Loss: 0.00113847
Iteration 23/25 | Loss: 0.00113847
Iteration 24/25 | Loss: 0.00113847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011384666431695223, 0.0011384666431695223, 0.0011384666431695223, 0.0011384666431695223, 0.0011384666431695223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011384666431695223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113847
Iteration 2/1000 | Loss: 0.00002849
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001527
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001440
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001426
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001422
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001417
Iteration 31/1000 | Loss: 0.00001417
Iteration 32/1000 | Loss: 0.00001416
Iteration 33/1000 | Loss: 0.00001416
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001396
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001391
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001376
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001375
Iteration 97/1000 | Loss: 0.00001375
Iteration 98/1000 | Loss: 0.00001375
Iteration 99/1000 | Loss: 0.00001375
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001375
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001374
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001372
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001368
Iteration 148/1000 | Loss: 0.00001368
Iteration 149/1000 | Loss: 0.00001367
Iteration 150/1000 | Loss: 0.00001367
Iteration 151/1000 | Loss: 0.00001367
Iteration 152/1000 | Loss: 0.00001366
Iteration 153/1000 | Loss: 0.00001366
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001364
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001364
Iteration 161/1000 | Loss: 0.00001364
Iteration 162/1000 | Loss: 0.00001364
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Iteration 167/1000 | Loss: 0.00001363
Iteration 168/1000 | Loss: 0.00001362
Iteration 169/1000 | Loss: 0.00001362
Iteration 170/1000 | Loss: 0.00001362
Iteration 171/1000 | Loss: 0.00001362
Iteration 172/1000 | Loss: 0.00001362
Iteration 173/1000 | Loss: 0.00001362
Iteration 174/1000 | Loss: 0.00001362
Iteration 175/1000 | Loss: 0.00001362
Iteration 176/1000 | Loss: 0.00001361
Iteration 177/1000 | Loss: 0.00001361
Iteration 178/1000 | Loss: 0.00001361
Iteration 179/1000 | Loss: 0.00001361
Iteration 180/1000 | Loss: 0.00001361
Iteration 181/1000 | Loss: 0.00001361
Iteration 182/1000 | Loss: 0.00001361
Iteration 183/1000 | Loss: 0.00001361
Iteration 184/1000 | Loss: 0.00001361
Iteration 185/1000 | Loss: 0.00001361
Iteration 186/1000 | Loss: 0.00001360
Iteration 187/1000 | Loss: 0.00001360
Iteration 188/1000 | Loss: 0.00001360
Iteration 189/1000 | Loss: 0.00001360
Iteration 190/1000 | Loss: 0.00001360
Iteration 191/1000 | Loss: 0.00001360
Iteration 192/1000 | Loss: 0.00001360
Iteration 193/1000 | Loss: 0.00001360
Iteration 194/1000 | Loss: 0.00001360
Iteration 195/1000 | Loss: 0.00001360
Iteration 196/1000 | Loss: 0.00001359
Iteration 197/1000 | Loss: 0.00001359
Iteration 198/1000 | Loss: 0.00001359
Iteration 199/1000 | Loss: 0.00001359
Iteration 200/1000 | Loss: 0.00001359
Iteration 201/1000 | Loss: 0.00001359
Iteration 202/1000 | Loss: 0.00001359
Iteration 203/1000 | Loss: 0.00001359
Iteration 204/1000 | Loss: 0.00001359
Iteration 205/1000 | Loss: 0.00001359
Iteration 206/1000 | Loss: 0.00001359
Iteration 207/1000 | Loss: 0.00001359
Iteration 208/1000 | Loss: 0.00001359
Iteration 209/1000 | Loss: 0.00001359
Iteration 210/1000 | Loss: 0.00001359
Iteration 211/1000 | Loss: 0.00001359
Iteration 212/1000 | Loss: 0.00001358
Iteration 213/1000 | Loss: 0.00001358
Iteration 214/1000 | Loss: 0.00001358
Iteration 215/1000 | Loss: 0.00001358
Iteration 216/1000 | Loss: 0.00001358
Iteration 217/1000 | Loss: 0.00001358
Iteration 218/1000 | Loss: 0.00001358
Iteration 219/1000 | Loss: 0.00001357
Iteration 220/1000 | Loss: 0.00001357
Iteration 221/1000 | Loss: 0.00001357
Iteration 222/1000 | Loss: 0.00001357
Iteration 223/1000 | Loss: 0.00001357
Iteration 224/1000 | Loss: 0.00001357
Iteration 225/1000 | Loss: 0.00001357
Iteration 226/1000 | Loss: 0.00001356
Iteration 227/1000 | Loss: 0.00001356
Iteration 228/1000 | Loss: 0.00001356
Iteration 229/1000 | Loss: 0.00001356
Iteration 230/1000 | Loss: 0.00001356
Iteration 231/1000 | Loss: 0.00001356
Iteration 232/1000 | Loss: 0.00001356
Iteration 233/1000 | Loss: 0.00001356
Iteration 234/1000 | Loss: 0.00001356
Iteration 235/1000 | Loss: 0.00001356
Iteration 236/1000 | Loss: 0.00001356
Iteration 237/1000 | Loss: 0.00001355
Iteration 238/1000 | Loss: 0.00001355
Iteration 239/1000 | Loss: 0.00001355
Iteration 240/1000 | Loss: 0.00001355
Iteration 241/1000 | Loss: 0.00001355
Iteration 242/1000 | Loss: 0.00001355
Iteration 243/1000 | Loss: 0.00001355
Iteration 244/1000 | Loss: 0.00001355
Iteration 245/1000 | Loss: 0.00001355
Iteration 246/1000 | Loss: 0.00001354
Iteration 247/1000 | Loss: 0.00001354
Iteration 248/1000 | Loss: 0.00001354
Iteration 249/1000 | Loss: 0.00001354
Iteration 250/1000 | Loss: 0.00001354
Iteration 251/1000 | Loss: 0.00001354
Iteration 252/1000 | Loss: 0.00001354
Iteration 253/1000 | Loss: 0.00001353
Iteration 254/1000 | Loss: 0.00001353
Iteration 255/1000 | Loss: 0.00001353
Iteration 256/1000 | Loss: 0.00001353
Iteration 257/1000 | Loss: 0.00001353
Iteration 258/1000 | Loss: 0.00001353
Iteration 259/1000 | Loss: 0.00001353
Iteration 260/1000 | Loss: 0.00001353
Iteration 261/1000 | Loss: 0.00001352
Iteration 262/1000 | Loss: 0.00001352
Iteration 263/1000 | Loss: 0.00001352
Iteration 264/1000 | Loss: 0.00001352
Iteration 265/1000 | Loss: 0.00001352
Iteration 266/1000 | Loss: 0.00001352
Iteration 267/1000 | Loss: 0.00001352
Iteration 268/1000 | Loss: 0.00001351
Iteration 269/1000 | Loss: 0.00001351
Iteration 270/1000 | Loss: 0.00001351
Iteration 271/1000 | Loss: 0.00001351
Iteration 272/1000 | Loss: 0.00001350
Iteration 273/1000 | Loss: 0.00001350
Iteration 274/1000 | Loss: 0.00001350
Iteration 275/1000 | Loss: 0.00001350
Iteration 276/1000 | Loss: 0.00001350
Iteration 277/1000 | Loss: 0.00001350
Iteration 278/1000 | Loss: 0.00001350
Iteration 279/1000 | Loss: 0.00001349
Iteration 280/1000 | Loss: 0.00001349
Iteration 281/1000 | Loss: 0.00001349
Iteration 282/1000 | Loss: 0.00001349
Iteration 283/1000 | Loss: 0.00001349
Iteration 284/1000 | Loss: 0.00001349
Iteration 285/1000 | Loss: 0.00001349
Iteration 286/1000 | Loss: 0.00001349
Iteration 287/1000 | Loss: 0.00001349
Iteration 288/1000 | Loss: 0.00001349
Iteration 289/1000 | Loss: 0.00001349
Iteration 290/1000 | Loss: 0.00001349
Iteration 291/1000 | Loss: 0.00001349
Iteration 292/1000 | Loss: 0.00001349
Iteration 293/1000 | Loss: 0.00001349
Iteration 294/1000 | Loss: 0.00001349
Iteration 295/1000 | Loss: 0.00001349
Iteration 296/1000 | Loss: 0.00001348
Iteration 297/1000 | Loss: 0.00001348
Iteration 298/1000 | Loss: 0.00001348
Iteration 299/1000 | Loss: 0.00001348
Iteration 300/1000 | Loss: 0.00001348
Iteration 301/1000 | Loss: 0.00001348
Iteration 302/1000 | Loss: 0.00001348
Iteration 303/1000 | Loss: 0.00001348
Iteration 304/1000 | Loss: 0.00001348
Iteration 305/1000 | Loss: 0.00001348
Iteration 306/1000 | Loss: 0.00001348
Iteration 307/1000 | Loss: 0.00001348
Iteration 308/1000 | Loss: 0.00001348
Iteration 309/1000 | Loss: 0.00001348
Iteration 310/1000 | Loss: 0.00001348
Iteration 311/1000 | Loss: 0.00001348
Iteration 312/1000 | Loss: 0.00001348
Iteration 313/1000 | Loss: 0.00001348
Iteration 314/1000 | Loss: 0.00001348
Iteration 315/1000 | Loss: 0.00001347
Iteration 316/1000 | Loss: 0.00001347
Iteration 317/1000 | Loss: 0.00001347
Iteration 318/1000 | Loss: 0.00001347
Iteration 319/1000 | Loss: 0.00001347
Iteration 320/1000 | Loss: 0.00001347
Iteration 321/1000 | Loss: 0.00001347
Iteration 322/1000 | Loss: 0.00001347
Iteration 323/1000 | Loss: 0.00001347
Iteration 324/1000 | Loss: 0.00001347
Iteration 325/1000 | Loss: 0.00001347
Iteration 326/1000 | Loss: 0.00001347
Iteration 327/1000 | Loss: 0.00001347
Iteration 328/1000 | Loss: 0.00001347
Iteration 329/1000 | Loss: 0.00001347
Iteration 330/1000 | Loss: 0.00001347
Iteration 331/1000 | Loss: 0.00001347
Iteration 332/1000 | Loss: 0.00001346
Iteration 333/1000 | Loss: 0.00001346
Iteration 334/1000 | Loss: 0.00001346
Iteration 335/1000 | Loss: 0.00001346
Iteration 336/1000 | Loss: 0.00001346
Iteration 337/1000 | Loss: 0.00001346
Iteration 338/1000 | Loss: 0.00001346
Iteration 339/1000 | Loss: 0.00001346
Iteration 340/1000 | Loss: 0.00001346
Iteration 341/1000 | Loss: 0.00001346
Iteration 342/1000 | Loss: 0.00001346
Iteration 343/1000 | Loss: 0.00001346
Iteration 344/1000 | Loss: 0.00001346
Iteration 345/1000 | Loss: 0.00001346
Iteration 346/1000 | Loss: 0.00001346
Iteration 347/1000 | Loss: 0.00001346
Iteration 348/1000 | Loss: 0.00001346
Iteration 349/1000 | Loss: 0.00001346
Iteration 350/1000 | Loss: 0.00001346
Iteration 351/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [1.3464158655551728e-05, 1.3464158655551728e-05, 1.3464158655551728e-05, 1.3464158655551728e-05, 1.3464158655551728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3464158655551728e-05

Optimization complete. Final v2v error: 3.1495301723480225 mm

Highest mean error: 3.2584662437438965 mm for frame 91

Lowest mean error: 3.0442869663238525 mm for frame 120

Saving results

Total time: 51.97132897377014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531372
Iteration 2/25 | Loss: 0.00158045
Iteration 3/25 | Loss: 0.00141144
Iteration 4/25 | Loss: 0.00139739
Iteration 5/25 | Loss: 0.00139544
Iteration 6/25 | Loss: 0.00139544
Iteration 7/25 | Loss: 0.00139544
Iteration 8/25 | Loss: 0.00139544
Iteration 9/25 | Loss: 0.00139544
Iteration 10/25 | Loss: 0.00139544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013954420574009418, 0.0013954420574009418, 0.0013954420574009418, 0.0013954420574009418, 0.0013954420574009418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013954420574009418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42186761
Iteration 2/25 | Loss: 0.00103452
Iteration 3/25 | Loss: 0.00103451
Iteration 4/25 | Loss: 0.00103451
Iteration 5/25 | Loss: 0.00103451
Iteration 6/25 | Loss: 0.00103451
Iteration 7/25 | Loss: 0.00103451
Iteration 8/25 | Loss: 0.00103451
Iteration 9/25 | Loss: 0.00103451
Iteration 10/25 | Loss: 0.00103451
Iteration 11/25 | Loss: 0.00103451
Iteration 12/25 | Loss: 0.00103451
Iteration 13/25 | Loss: 0.00103451
Iteration 14/25 | Loss: 0.00103451
Iteration 15/25 | Loss: 0.00103451
Iteration 16/25 | Loss: 0.00103451
Iteration 17/25 | Loss: 0.00103451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001034511486068368, 0.001034511486068368, 0.001034511486068368, 0.001034511486068368, 0.001034511486068368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001034511486068368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103451
Iteration 2/1000 | Loss: 0.00005085
Iteration 3/1000 | Loss: 0.00003790
Iteration 4/1000 | Loss: 0.00003110
Iteration 5/1000 | Loss: 0.00002872
Iteration 6/1000 | Loss: 0.00002768
Iteration 7/1000 | Loss: 0.00002668
Iteration 8/1000 | Loss: 0.00002605
Iteration 9/1000 | Loss: 0.00002549
Iteration 10/1000 | Loss: 0.00002501
Iteration 11/1000 | Loss: 0.00002456
Iteration 12/1000 | Loss: 0.00002430
Iteration 13/1000 | Loss: 0.00002416
Iteration 14/1000 | Loss: 0.00002393
Iteration 15/1000 | Loss: 0.00002369
Iteration 16/1000 | Loss: 0.00002361
Iteration 17/1000 | Loss: 0.00002360
Iteration 18/1000 | Loss: 0.00002359
Iteration 19/1000 | Loss: 0.00002354
Iteration 20/1000 | Loss: 0.00002350
Iteration 21/1000 | Loss: 0.00002344
Iteration 22/1000 | Loss: 0.00002342
Iteration 23/1000 | Loss: 0.00002342
Iteration 24/1000 | Loss: 0.00002342
Iteration 25/1000 | Loss: 0.00002342
Iteration 26/1000 | Loss: 0.00002342
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002341
Iteration 29/1000 | Loss: 0.00002341
Iteration 30/1000 | Loss: 0.00002341
Iteration 31/1000 | Loss: 0.00002341
Iteration 32/1000 | Loss: 0.00002341
Iteration 33/1000 | Loss: 0.00002340
Iteration 34/1000 | Loss: 0.00002339
Iteration 35/1000 | Loss: 0.00002339
Iteration 36/1000 | Loss: 0.00002338
Iteration 37/1000 | Loss: 0.00002338
Iteration 38/1000 | Loss: 0.00002338
Iteration 39/1000 | Loss: 0.00002337
Iteration 40/1000 | Loss: 0.00002336
Iteration 41/1000 | Loss: 0.00002336
Iteration 42/1000 | Loss: 0.00002335
Iteration 43/1000 | Loss: 0.00002335
Iteration 44/1000 | Loss: 0.00002335
Iteration 45/1000 | Loss: 0.00002334
Iteration 46/1000 | Loss: 0.00002334
Iteration 47/1000 | Loss: 0.00002334
Iteration 48/1000 | Loss: 0.00002333
Iteration 49/1000 | Loss: 0.00002333
Iteration 50/1000 | Loss: 0.00002333
Iteration 51/1000 | Loss: 0.00002332
Iteration 52/1000 | Loss: 0.00002332
Iteration 53/1000 | Loss: 0.00002332
Iteration 54/1000 | Loss: 0.00002331
Iteration 55/1000 | Loss: 0.00002331
Iteration 56/1000 | Loss: 0.00002331
Iteration 57/1000 | Loss: 0.00002331
Iteration 58/1000 | Loss: 0.00002330
Iteration 59/1000 | Loss: 0.00002330
Iteration 60/1000 | Loss: 0.00002330
Iteration 61/1000 | Loss: 0.00002330
Iteration 62/1000 | Loss: 0.00002330
Iteration 63/1000 | Loss: 0.00002330
Iteration 64/1000 | Loss: 0.00002330
Iteration 65/1000 | Loss: 0.00002330
Iteration 66/1000 | Loss: 0.00002329
Iteration 67/1000 | Loss: 0.00002329
Iteration 68/1000 | Loss: 0.00002329
Iteration 69/1000 | Loss: 0.00002329
Iteration 70/1000 | Loss: 0.00002329
Iteration 71/1000 | Loss: 0.00002328
Iteration 72/1000 | Loss: 0.00002328
Iteration 73/1000 | Loss: 0.00002328
Iteration 74/1000 | Loss: 0.00002328
Iteration 75/1000 | Loss: 0.00002328
Iteration 76/1000 | Loss: 0.00002327
Iteration 77/1000 | Loss: 0.00002327
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002326
Iteration 82/1000 | Loss: 0.00002326
Iteration 83/1000 | Loss: 0.00002326
Iteration 84/1000 | Loss: 0.00002326
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002325
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002322
Iteration 95/1000 | Loss: 0.00002322
Iteration 96/1000 | Loss: 0.00002321
Iteration 97/1000 | Loss: 0.00002321
Iteration 98/1000 | Loss: 0.00002321
Iteration 99/1000 | Loss: 0.00002321
Iteration 100/1000 | Loss: 0.00002321
Iteration 101/1000 | Loss: 0.00002320
Iteration 102/1000 | Loss: 0.00002320
Iteration 103/1000 | Loss: 0.00002320
Iteration 104/1000 | Loss: 0.00002320
Iteration 105/1000 | Loss: 0.00002320
Iteration 106/1000 | Loss: 0.00002320
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002319
Iteration 111/1000 | Loss: 0.00002319
Iteration 112/1000 | Loss: 0.00002319
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002318
Iteration 115/1000 | Loss: 0.00002318
Iteration 116/1000 | Loss: 0.00002318
Iteration 117/1000 | Loss: 0.00002318
Iteration 118/1000 | Loss: 0.00002318
Iteration 119/1000 | Loss: 0.00002318
Iteration 120/1000 | Loss: 0.00002318
Iteration 121/1000 | Loss: 0.00002318
Iteration 122/1000 | Loss: 0.00002318
Iteration 123/1000 | Loss: 0.00002318
Iteration 124/1000 | Loss: 0.00002318
Iteration 125/1000 | Loss: 0.00002318
Iteration 126/1000 | Loss: 0.00002318
Iteration 127/1000 | Loss: 0.00002318
Iteration 128/1000 | Loss: 0.00002318
Iteration 129/1000 | Loss: 0.00002318
Iteration 130/1000 | Loss: 0.00002318
Iteration 131/1000 | Loss: 0.00002318
Iteration 132/1000 | Loss: 0.00002318
Iteration 133/1000 | Loss: 0.00002318
Iteration 134/1000 | Loss: 0.00002318
Iteration 135/1000 | Loss: 0.00002318
Iteration 136/1000 | Loss: 0.00002318
Iteration 137/1000 | Loss: 0.00002318
Iteration 138/1000 | Loss: 0.00002318
Iteration 139/1000 | Loss: 0.00002318
Iteration 140/1000 | Loss: 0.00002318
Iteration 141/1000 | Loss: 0.00002318
Iteration 142/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.3177221009973437e-05, 2.3177221009973437e-05, 2.3177221009973437e-05, 2.3177221009973437e-05, 2.3177221009973437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3177221009973437e-05

Optimization complete. Final v2v error: 4.028061866760254 mm

Highest mean error: 4.412229537963867 mm for frame 78

Lowest mean error: 3.7468204498291016 mm for frame 13

Saving results

Total time: 39.648622274398804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369790
Iteration 2/25 | Loss: 0.00149300
Iteration 3/25 | Loss: 0.00134385
Iteration 4/25 | Loss: 0.00132478
Iteration 5/25 | Loss: 0.00131998
Iteration 6/25 | Loss: 0.00131803
Iteration 7/25 | Loss: 0.00131787
Iteration 8/25 | Loss: 0.00131787
Iteration 9/25 | Loss: 0.00131787
Iteration 10/25 | Loss: 0.00131787
Iteration 11/25 | Loss: 0.00131787
Iteration 12/25 | Loss: 0.00131787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013178710360080004, 0.0013178710360080004, 0.0013178710360080004, 0.0013178710360080004, 0.0013178710360080004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013178710360080004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29564881
Iteration 2/25 | Loss: 0.00182694
Iteration 3/25 | Loss: 0.00182694
Iteration 4/25 | Loss: 0.00182694
Iteration 5/25 | Loss: 0.00182694
Iteration 6/25 | Loss: 0.00182694
Iteration 7/25 | Loss: 0.00182694
Iteration 8/25 | Loss: 0.00182694
Iteration 9/25 | Loss: 0.00182694
Iteration 10/25 | Loss: 0.00182694
Iteration 11/25 | Loss: 0.00182694
Iteration 12/25 | Loss: 0.00182694
Iteration 13/25 | Loss: 0.00182694
Iteration 14/25 | Loss: 0.00182694
Iteration 15/25 | Loss: 0.00182694
Iteration 16/25 | Loss: 0.00182694
Iteration 17/25 | Loss: 0.00182694
Iteration 18/25 | Loss: 0.00182694
Iteration 19/25 | Loss: 0.00182694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018269410356879234, 0.0018269410356879234, 0.0018269410356879234, 0.0018269410356879234, 0.0018269410356879234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018269410356879234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182694
Iteration 2/1000 | Loss: 0.00004198
Iteration 3/1000 | Loss: 0.00002510
Iteration 4/1000 | Loss: 0.00001767
Iteration 5/1000 | Loss: 0.00001621
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001512
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001402
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001400
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001366
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001356
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001346
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001345
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001345
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.3433999811240938e-05, 1.3433999811240938e-05, 1.3433999811240938e-05, 1.3433999811240938e-05, 1.3433999811240938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3433999811240938e-05

Optimization complete. Final v2v error: 3.16745662689209 mm

Highest mean error: 3.3927695751190186 mm for frame 93

Lowest mean error: 2.870882749557495 mm for frame 7

Saving results

Total time: 35.104910135269165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755161
Iteration 2/25 | Loss: 0.00226811
Iteration 3/25 | Loss: 0.00160749
Iteration 4/25 | Loss: 0.00146935
Iteration 5/25 | Loss: 0.00143703
Iteration 6/25 | Loss: 0.00142430
Iteration 7/25 | Loss: 0.00142337
Iteration 8/25 | Loss: 0.00143800
Iteration 9/25 | Loss: 0.00144342
Iteration 10/25 | Loss: 0.00142245
Iteration 11/25 | Loss: 0.00140460
Iteration 12/25 | Loss: 0.00139877
Iteration 13/25 | Loss: 0.00139525
Iteration 14/25 | Loss: 0.00139114
Iteration 15/25 | Loss: 0.00139188
Iteration 16/25 | Loss: 0.00138739
Iteration 17/25 | Loss: 0.00138986
Iteration 18/25 | Loss: 0.00139129
Iteration 19/25 | Loss: 0.00138770
Iteration 20/25 | Loss: 0.00138827
Iteration 21/25 | Loss: 0.00138789
Iteration 22/25 | Loss: 0.00138617
Iteration 23/25 | Loss: 0.00138334
Iteration 24/25 | Loss: 0.00138501
Iteration 25/25 | Loss: 0.00138485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30962229
Iteration 2/25 | Loss: 0.00086466
Iteration 3/25 | Loss: 0.00086466
Iteration 4/25 | Loss: 0.00086466
Iteration 5/25 | Loss: 0.00083345
Iteration 6/25 | Loss: 0.00083344
Iteration 7/25 | Loss: 0.00083344
Iteration 8/25 | Loss: 0.00083344
Iteration 9/25 | Loss: 0.00083344
Iteration 10/25 | Loss: 0.00083344
Iteration 11/25 | Loss: 0.00083344
Iteration 12/25 | Loss: 0.00083344
Iteration 13/25 | Loss: 0.00083344
Iteration 14/25 | Loss: 0.00083344
Iteration 15/25 | Loss: 0.00083344
Iteration 16/25 | Loss: 0.00083344
Iteration 17/25 | Loss: 0.00083344
Iteration 18/25 | Loss: 0.00083344
Iteration 19/25 | Loss: 0.00083344
Iteration 20/25 | Loss: 0.00083344
Iteration 21/25 | Loss: 0.00083344
Iteration 22/25 | Loss: 0.00083344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008334393496625125, 0.0008334393496625125, 0.0008334393496625125, 0.0008334393496625125, 0.0008334393496625125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008334393496625125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083344
Iteration 2/1000 | Loss: 0.00010812
Iteration 3/1000 | Loss: 0.00004594
Iteration 4/1000 | Loss: 0.00005877
Iteration 5/1000 | Loss: 0.00004895
Iteration 6/1000 | Loss: 0.00004364
Iteration 7/1000 | Loss: 0.00004720
Iteration 8/1000 | Loss: 0.00005834
Iteration 9/1000 | Loss: 0.00004027
Iteration 10/1000 | Loss: 0.00003237
Iteration 11/1000 | Loss: 0.00005590
Iteration 12/1000 | Loss: 0.00003127
Iteration 13/1000 | Loss: 0.00003660
Iteration 14/1000 | Loss: 0.00002959
Iteration 15/1000 | Loss: 0.00002918
Iteration 16/1000 | Loss: 0.00002884
Iteration 17/1000 | Loss: 0.00002855
Iteration 18/1000 | Loss: 0.00002833
Iteration 19/1000 | Loss: 0.00010201
Iteration 20/1000 | Loss: 0.00002811
Iteration 21/1000 | Loss: 0.00002804
Iteration 22/1000 | Loss: 0.00002804
Iteration 23/1000 | Loss: 0.00002799
Iteration 24/1000 | Loss: 0.00002796
Iteration 25/1000 | Loss: 0.00002796
Iteration 26/1000 | Loss: 0.00002795
Iteration 27/1000 | Loss: 0.00002795
Iteration 28/1000 | Loss: 0.00002794
Iteration 29/1000 | Loss: 0.00002792
Iteration 30/1000 | Loss: 0.00002791
Iteration 31/1000 | Loss: 0.00007868
Iteration 32/1000 | Loss: 0.00013769
Iteration 33/1000 | Loss: 0.00012919
Iteration 34/1000 | Loss: 0.00012487
Iteration 35/1000 | Loss: 0.00004201
Iteration 36/1000 | Loss: 0.00003220
Iteration 37/1000 | Loss: 0.00002781
Iteration 38/1000 | Loss: 0.00002777
Iteration 39/1000 | Loss: 0.00002774
Iteration 40/1000 | Loss: 0.00002772
Iteration 41/1000 | Loss: 0.00002772
Iteration 42/1000 | Loss: 0.00002772
Iteration 43/1000 | Loss: 0.00002772
Iteration 44/1000 | Loss: 0.00002771
Iteration 45/1000 | Loss: 0.00002771
Iteration 46/1000 | Loss: 0.00002771
Iteration 47/1000 | Loss: 0.00002771
Iteration 48/1000 | Loss: 0.00002771
Iteration 49/1000 | Loss: 0.00002771
Iteration 50/1000 | Loss: 0.00003959
Iteration 51/1000 | Loss: 0.00002777
Iteration 52/1000 | Loss: 0.00002769
Iteration 53/1000 | Loss: 0.00002769
Iteration 54/1000 | Loss: 0.00002769
Iteration 55/1000 | Loss: 0.00002769
Iteration 56/1000 | Loss: 0.00002768
Iteration 57/1000 | Loss: 0.00002768
Iteration 58/1000 | Loss: 0.00002768
Iteration 59/1000 | Loss: 0.00002768
Iteration 60/1000 | Loss: 0.00002768
Iteration 61/1000 | Loss: 0.00002768
Iteration 62/1000 | Loss: 0.00002768
Iteration 63/1000 | Loss: 0.00002768
Iteration 64/1000 | Loss: 0.00002768
Iteration 65/1000 | Loss: 0.00002768
Iteration 66/1000 | Loss: 0.00002768
Iteration 67/1000 | Loss: 0.00002768
Iteration 68/1000 | Loss: 0.00002768
Iteration 69/1000 | Loss: 0.00002767
Iteration 70/1000 | Loss: 0.00002767
Iteration 71/1000 | Loss: 0.00002767
Iteration 72/1000 | Loss: 0.00002766
Iteration 73/1000 | Loss: 0.00002766
Iteration 74/1000 | Loss: 0.00002764
Iteration 75/1000 | Loss: 0.00002764
Iteration 76/1000 | Loss: 0.00002764
Iteration 77/1000 | Loss: 0.00002764
Iteration 78/1000 | Loss: 0.00002764
Iteration 79/1000 | Loss: 0.00002763
Iteration 80/1000 | Loss: 0.00002763
Iteration 81/1000 | Loss: 0.00002763
Iteration 82/1000 | Loss: 0.00002763
Iteration 83/1000 | Loss: 0.00002763
Iteration 84/1000 | Loss: 0.00002762
Iteration 85/1000 | Loss: 0.00002762
Iteration 86/1000 | Loss: 0.00002762
Iteration 87/1000 | Loss: 0.00002762
Iteration 88/1000 | Loss: 0.00002762
Iteration 89/1000 | Loss: 0.00002762
Iteration 90/1000 | Loss: 0.00002762
Iteration 91/1000 | Loss: 0.00002761
Iteration 92/1000 | Loss: 0.00002761
Iteration 93/1000 | Loss: 0.00002761
Iteration 94/1000 | Loss: 0.00002761
Iteration 95/1000 | Loss: 0.00002761
Iteration 96/1000 | Loss: 0.00002761
Iteration 97/1000 | Loss: 0.00002761
Iteration 98/1000 | Loss: 0.00002761
Iteration 99/1000 | Loss: 0.00002761
Iteration 100/1000 | Loss: 0.00002761
Iteration 101/1000 | Loss: 0.00002760
Iteration 102/1000 | Loss: 0.00002760
Iteration 103/1000 | Loss: 0.00002760
Iteration 104/1000 | Loss: 0.00002760
Iteration 105/1000 | Loss: 0.00002760
Iteration 106/1000 | Loss: 0.00002760
Iteration 107/1000 | Loss: 0.00002760
Iteration 108/1000 | Loss: 0.00002760
Iteration 109/1000 | Loss: 0.00002760
Iteration 110/1000 | Loss: 0.00002760
Iteration 111/1000 | Loss: 0.00002760
Iteration 112/1000 | Loss: 0.00002760
Iteration 113/1000 | Loss: 0.00002760
Iteration 114/1000 | Loss: 0.00002760
Iteration 115/1000 | Loss: 0.00002760
Iteration 116/1000 | Loss: 0.00002760
Iteration 117/1000 | Loss: 0.00002760
Iteration 118/1000 | Loss: 0.00002760
Iteration 119/1000 | Loss: 0.00002760
Iteration 120/1000 | Loss: 0.00002760
Iteration 121/1000 | Loss: 0.00002760
Iteration 122/1000 | Loss: 0.00002760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.7603504349826835e-05, 2.7603504349826835e-05, 2.7603504349826835e-05, 2.7603504349826835e-05, 2.7603504349826835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7603504349826835e-05

Optimization complete. Final v2v error: 4.348304748535156 mm

Highest mean error: 6.90639591217041 mm for frame 96

Lowest mean error: 3.8750901222229004 mm for frame 183

Saving results

Total time: 107.50736880302429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474812
Iteration 2/25 | Loss: 0.00137544
Iteration 3/25 | Loss: 0.00129998
Iteration 4/25 | Loss: 0.00129095
Iteration 5/25 | Loss: 0.00128842
Iteration 6/25 | Loss: 0.00128837
Iteration 7/25 | Loss: 0.00128837
Iteration 8/25 | Loss: 0.00128837
Iteration 9/25 | Loss: 0.00128837
Iteration 10/25 | Loss: 0.00128837
Iteration 11/25 | Loss: 0.00128837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012883655726909637, 0.0012883655726909637, 0.0012883655726909637, 0.0012883655726909637, 0.0012883655726909637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012883655726909637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.05003023
Iteration 2/25 | Loss: 0.00132363
Iteration 3/25 | Loss: 0.00132363
Iteration 4/25 | Loss: 0.00132362
Iteration 5/25 | Loss: 0.00132362
Iteration 6/25 | Loss: 0.00132362
Iteration 7/25 | Loss: 0.00132362
Iteration 8/25 | Loss: 0.00132362
Iteration 9/25 | Loss: 0.00132362
Iteration 10/25 | Loss: 0.00132362
Iteration 11/25 | Loss: 0.00132362
Iteration 12/25 | Loss: 0.00132362
Iteration 13/25 | Loss: 0.00132362
Iteration 14/25 | Loss: 0.00132362
Iteration 15/25 | Loss: 0.00132362
Iteration 16/25 | Loss: 0.00132362
Iteration 17/25 | Loss: 0.00132362
Iteration 18/25 | Loss: 0.00132362
Iteration 19/25 | Loss: 0.00132362
Iteration 20/25 | Loss: 0.00132362
Iteration 21/25 | Loss: 0.00132362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013236209051683545, 0.0013236209051683545, 0.0013236209051683545, 0.0013236209051683545, 0.0013236209051683545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013236209051683545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132362
Iteration 2/1000 | Loss: 0.00002710
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001524
Iteration 7/1000 | Loss: 0.00001452
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001328
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001260
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001207
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001204
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Iteration 101/1000 | Loss: 0.00001188
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001185
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001180
Iteration 150/1000 | Loss: 0.00001180
Iteration 151/1000 | Loss: 0.00001180
Iteration 152/1000 | Loss: 0.00001180
Iteration 153/1000 | Loss: 0.00001180
Iteration 154/1000 | Loss: 0.00001180
Iteration 155/1000 | Loss: 0.00001180
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001180
Iteration 158/1000 | Loss: 0.00001180
Iteration 159/1000 | Loss: 0.00001180
Iteration 160/1000 | Loss: 0.00001180
Iteration 161/1000 | Loss: 0.00001180
Iteration 162/1000 | Loss: 0.00001180
Iteration 163/1000 | Loss: 0.00001180
Iteration 164/1000 | Loss: 0.00001180
Iteration 165/1000 | Loss: 0.00001180
Iteration 166/1000 | Loss: 0.00001180
Iteration 167/1000 | Loss: 0.00001180
Iteration 168/1000 | Loss: 0.00001180
Iteration 169/1000 | Loss: 0.00001180
Iteration 170/1000 | Loss: 0.00001180
Iteration 171/1000 | Loss: 0.00001180
Iteration 172/1000 | Loss: 0.00001180
Iteration 173/1000 | Loss: 0.00001180
Iteration 174/1000 | Loss: 0.00001180
Iteration 175/1000 | Loss: 0.00001180
Iteration 176/1000 | Loss: 0.00001180
Iteration 177/1000 | Loss: 0.00001180
Iteration 178/1000 | Loss: 0.00001180
Iteration 179/1000 | Loss: 0.00001180
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001180
Iteration 182/1000 | Loss: 0.00001180
Iteration 183/1000 | Loss: 0.00001180
Iteration 184/1000 | Loss: 0.00001180
Iteration 185/1000 | Loss: 0.00001180
Iteration 186/1000 | Loss: 0.00001180
Iteration 187/1000 | Loss: 0.00001180
Iteration 188/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1802388144133147e-05, 1.1802388144133147e-05, 1.1802388144133147e-05, 1.1802388144133147e-05, 1.1802388144133147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1802388144133147e-05

Optimization complete. Final v2v error: 2.9666507244110107 mm

Highest mean error: 3.363718032836914 mm for frame 75

Lowest mean error: 2.771273374557495 mm for frame 30

Saving results

Total time: 43.42679142951965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890829
Iteration 2/25 | Loss: 0.00187415
Iteration 3/25 | Loss: 0.00155900
Iteration 4/25 | Loss: 0.00151619
Iteration 5/25 | Loss: 0.00149904
Iteration 6/25 | Loss: 0.00149426
Iteration 7/25 | Loss: 0.00148859
Iteration 8/25 | Loss: 0.00149369
Iteration 9/25 | Loss: 0.00148697
Iteration 10/25 | Loss: 0.00149505
Iteration 11/25 | Loss: 0.00149745
Iteration 12/25 | Loss: 0.00147248
Iteration 13/25 | Loss: 0.00146377
Iteration 14/25 | Loss: 0.00148652
Iteration 15/25 | Loss: 0.00145647
Iteration 16/25 | Loss: 0.00144800
Iteration 17/25 | Loss: 0.00144681
Iteration 18/25 | Loss: 0.00144683
Iteration 19/25 | Loss: 0.00144640
Iteration 20/25 | Loss: 0.00144489
Iteration 21/25 | Loss: 0.00144810
Iteration 22/25 | Loss: 0.00144467
Iteration 23/25 | Loss: 0.00144549
Iteration 24/25 | Loss: 0.00144507
Iteration 25/25 | Loss: 0.00144477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27478290
Iteration 2/25 | Loss: 0.00192045
Iteration 3/25 | Loss: 0.00192011
Iteration 4/25 | Loss: 0.00192011
Iteration 5/25 | Loss: 0.00192011
Iteration 6/25 | Loss: 0.00192011
Iteration 7/25 | Loss: 0.00192011
Iteration 8/25 | Loss: 0.00192011
Iteration 9/25 | Loss: 0.00192011
Iteration 10/25 | Loss: 0.00192011
Iteration 11/25 | Loss: 0.00192011
Iteration 12/25 | Loss: 0.00192011
Iteration 13/25 | Loss: 0.00192011
Iteration 14/25 | Loss: 0.00192011
Iteration 15/25 | Loss: 0.00192011
Iteration 16/25 | Loss: 0.00192011
Iteration 17/25 | Loss: 0.00192011
Iteration 18/25 | Loss: 0.00192011
Iteration 19/25 | Loss: 0.00192011
Iteration 20/25 | Loss: 0.00192011
Iteration 21/25 | Loss: 0.00192011
Iteration 22/25 | Loss: 0.00192011
Iteration 23/25 | Loss: 0.00192011
Iteration 24/25 | Loss: 0.00192011
Iteration 25/25 | Loss: 0.00192011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192011
Iteration 2/1000 | Loss: 0.00332236
Iteration 3/1000 | Loss: 0.00033244
Iteration 4/1000 | Loss: 0.00219888
Iteration 5/1000 | Loss: 0.00084937
Iteration 6/1000 | Loss: 0.00020777
Iteration 7/1000 | Loss: 0.00035348
Iteration 8/1000 | Loss: 0.00038790
Iteration 9/1000 | Loss: 0.00018485
Iteration 10/1000 | Loss: 0.00076507
Iteration 11/1000 | Loss: 0.00034387
Iteration 12/1000 | Loss: 0.00027826
Iteration 13/1000 | Loss: 0.00022746
Iteration 14/1000 | Loss: 0.00006886
Iteration 15/1000 | Loss: 0.00025260
Iteration 16/1000 | Loss: 0.00093594
Iteration 17/1000 | Loss: 0.00038788
Iteration 18/1000 | Loss: 0.00102161
Iteration 19/1000 | Loss: 0.00014057
Iteration 20/1000 | Loss: 0.00006750
Iteration 21/1000 | Loss: 0.00018325
Iteration 22/1000 | Loss: 0.00013827
Iteration 23/1000 | Loss: 0.00018009
Iteration 24/1000 | Loss: 0.00011591
Iteration 25/1000 | Loss: 0.00005575
Iteration 26/1000 | Loss: 0.00005681
Iteration 27/1000 | Loss: 0.00006041
Iteration 28/1000 | Loss: 0.00005875
Iteration 29/1000 | Loss: 0.00005917
Iteration 30/1000 | Loss: 0.00006881
Iteration 31/1000 | Loss: 0.00075029
Iteration 32/1000 | Loss: 0.00056179
Iteration 33/1000 | Loss: 0.00030546
Iteration 34/1000 | Loss: 0.00009501
Iteration 35/1000 | Loss: 0.00005665
Iteration 36/1000 | Loss: 0.00018272
Iteration 37/1000 | Loss: 0.00018731
Iteration 38/1000 | Loss: 0.00016609
Iteration 39/1000 | Loss: 0.00016585
Iteration 40/1000 | Loss: 0.00005704
Iteration 41/1000 | Loss: 0.00014787
Iteration 42/1000 | Loss: 0.00014613
Iteration 43/1000 | Loss: 0.00014466
Iteration 44/1000 | Loss: 0.00025401
Iteration 45/1000 | Loss: 0.00028342
Iteration 46/1000 | Loss: 0.00016007
Iteration 47/1000 | Loss: 0.00044620
Iteration 48/1000 | Loss: 0.00025527
Iteration 49/1000 | Loss: 0.00013402
Iteration 50/1000 | Loss: 0.00015391
Iteration 51/1000 | Loss: 0.00007820
Iteration 52/1000 | Loss: 0.00021619
Iteration 53/1000 | Loss: 0.00019777
Iteration 54/1000 | Loss: 0.00020601
Iteration 55/1000 | Loss: 0.00014373
Iteration 56/1000 | Loss: 0.00019107
Iteration 57/1000 | Loss: 0.00019722
Iteration 58/1000 | Loss: 0.00018031
Iteration 59/1000 | Loss: 0.00015373
Iteration 60/1000 | Loss: 0.00014282
Iteration 61/1000 | Loss: 0.00018667
Iteration 62/1000 | Loss: 0.00023343
Iteration 63/1000 | Loss: 0.00023866
Iteration 64/1000 | Loss: 0.00004830
Iteration 65/1000 | Loss: 0.00003907
Iteration 66/1000 | Loss: 0.00028176
Iteration 67/1000 | Loss: 0.00024117
Iteration 68/1000 | Loss: 0.00010789
Iteration 69/1000 | Loss: 0.00028047
Iteration 70/1000 | Loss: 0.00017377
Iteration 71/1000 | Loss: 0.00029574
Iteration 72/1000 | Loss: 0.00012232
Iteration 73/1000 | Loss: 0.00004257
Iteration 74/1000 | Loss: 0.00003734
Iteration 75/1000 | Loss: 0.00003535
Iteration 76/1000 | Loss: 0.00003375
Iteration 77/1000 | Loss: 0.00006062
Iteration 78/1000 | Loss: 0.00003263
Iteration 79/1000 | Loss: 0.00003097
Iteration 80/1000 | Loss: 0.00002953
Iteration 81/1000 | Loss: 0.00002877
Iteration 82/1000 | Loss: 0.00005208
Iteration 83/1000 | Loss: 0.00004370
Iteration 84/1000 | Loss: 0.00003298
Iteration 85/1000 | Loss: 0.00003119
Iteration 86/1000 | Loss: 0.00003000
Iteration 87/1000 | Loss: 0.00002874
Iteration 88/1000 | Loss: 0.00002783
Iteration 89/1000 | Loss: 0.00002712
Iteration 90/1000 | Loss: 0.00002678
Iteration 91/1000 | Loss: 0.00002652
Iteration 92/1000 | Loss: 0.00002630
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002611
Iteration 95/1000 | Loss: 0.00002610
Iteration 96/1000 | Loss: 0.00002610
Iteration 97/1000 | Loss: 0.00002605
Iteration 98/1000 | Loss: 0.00002601
Iteration 99/1000 | Loss: 0.00002601
Iteration 100/1000 | Loss: 0.00002595
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002590
Iteration 103/1000 | Loss: 0.00002588
Iteration 104/1000 | Loss: 0.00002588
Iteration 105/1000 | Loss: 0.00002588
Iteration 106/1000 | Loss: 0.00002587
Iteration 107/1000 | Loss: 0.00002587
Iteration 108/1000 | Loss: 0.00002587
Iteration 109/1000 | Loss: 0.00002586
Iteration 110/1000 | Loss: 0.00002584
Iteration 111/1000 | Loss: 0.00002584
Iteration 112/1000 | Loss: 0.00002584
Iteration 113/1000 | Loss: 0.00002583
Iteration 114/1000 | Loss: 0.00002583
Iteration 115/1000 | Loss: 0.00002582
Iteration 116/1000 | Loss: 0.00002582
Iteration 117/1000 | Loss: 0.00002582
Iteration 118/1000 | Loss: 0.00002581
Iteration 119/1000 | Loss: 0.00002581
Iteration 120/1000 | Loss: 0.00002581
Iteration 121/1000 | Loss: 0.00002581
Iteration 122/1000 | Loss: 0.00002580
Iteration 123/1000 | Loss: 0.00002580
Iteration 124/1000 | Loss: 0.00002580
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002580
Iteration 127/1000 | Loss: 0.00002580
Iteration 128/1000 | Loss: 0.00002580
Iteration 129/1000 | Loss: 0.00002580
Iteration 130/1000 | Loss: 0.00002580
Iteration 131/1000 | Loss: 0.00002580
Iteration 132/1000 | Loss: 0.00002579
Iteration 133/1000 | Loss: 0.00002579
Iteration 134/1000 | Loss: 0.00002579
Iteration 135/1000 | Loss: 0.00002578
Iteration 136/1000 | Loss: 0.00002578
Iteration 137/1000 | Loss: 0.00002577
Iteration 138/1000 | Loss: 0.00002577
Iteration 139/1000 | Loss: 0.00002577
Iteration 140/1000 | Loss: 0.00002577
Iteration 141/1000 | Loss: 0.00002577
Iteration 142/1000 | Loss: 0.00002577
Iteration 143/1000 | Loss: 0.00002577
Iteration 144/1000 | Loss: 0.00002577
Iteration 145/1000 | Loss: 0.00002572
Iteration 146/1000 | Loss: 0.00002569
Iteration 147/1000 | Loss: 0.00002569
Iteration 148/1000 | Loss: 0.00002569
Iteration 149/1000 | Loss: 0.00002568
Iteration 150/1000 | Loss: 0.00002568
Iteration 151/1000 | Loss: 0.00002568
Iteration 152/1000 | Loss: 0.00002568
Iteration 153/1000 | Loss: 0.00002568
Iteration 154/1000 | Loss: 0.00002568
Iteration 155/1000 | Loss: 0.00002568
Iteration 156/1000 | Loss: 0.00002567
Iteration 157/1000 | Loss: 0.00002566
Iteration 158/1000 | Loss: 0.00002566
Iteration 159/1000 | Loss: 0.00002566
Iteration 160/1000 | Loss: 0.00002566
Iteration 161/1000 | Loss: 0.00002565
Iteration 162/1000 | Loss: 0.00002565
Iteration 163/1000 | Loss: 0.00002564
Iteration 164/1000 | Loss: 0.00002563
Iteration 165/1000 | Loss: 0.00002562
Iteration 166/1000 | Loss: 0.00002562
Iteration 167/1000 | Loss: 0.00002561
Iteration 168/1000 | Loss: 0.00002561
Iteration 169/1000 | Loss: 0.00002560
Iteration 170/1000 | Loss: 0.00002557
Iteration 171/1000 | Loss: 0.00002554
Iteration 172/1000 | Loss: 0.00002552
Iteration 173/1000 | Loss: 0.00002551
Iteration 174/1000 | Loss: 0.00002551
Iteration 175/1000 | Loss: 0.00002550
Iteration 176/1000 | Loss: 0.00002544
Iteration 177/1000 | Loss: 0.00002543
Iteration 178/1000 | Loss: 0.00002543
Iteration 179/1000 | Loss: 0.00002542
Iteration 180/1000 | Loss: 0.00002542
Iteration 181/1000 | Loss: 0.00002535
Iteration 182/1000 | Loss: 0.00002527
Iteration 183/1000 | Loss: 0.00002527
Iteration 184/1000 | Loss: 0.00002526
Iteration 185/1000 | Loss: 0.00002526
Iteration 186/1000 | Loss: 0.00002525
Iteration 187/1000 | Loss: 0.00002525
Iteration 188/1000 | Loss: 0.00002524
Iteration 189/1000 | Loss: 0.00002524
Iteration 190/1000 | Loss: 0.00002523
Iteration 191/1000 | Loss: 0.00002523
Iteration 192/1000 | Loss: 0.00002523
Iteration 193/1000 | Loss: 0.00002520
Iteration 194/1000 | Loss: 0.00002519
Iteration 195/1000 | Loss: 0.00002519
Iteration 196/1000 | Loss: 0.00002519
Iteration 197/1000 | Loss: 0.00002518
Iteration 198/1000 | Loss: 0.00002518
Iteration 199/1000 | Loss: 0.00002517
Iteration 200/1000 | Loss: 0.00002517
Iteration 201/1000 | Loss: 0.00002517
Iteration 202/1000 | Loss: 0.00002517
Iteration 203/1000 | Loss: 0.00002516
Iteration 204/1000 | Loss: 0.00002516
Iteration 205/1000 | Loss: 0.00002516
Iteration 206/1000 | Loss: 0.00002516
Iteration 207/1000 | Loss: 0.00002516
Iteration 208/1000 | Loss: 0.00002516
Iteration 209/1000 | Loss: 0.00002516
Iteration 210/1000 | Loss: 0.00002515
Iteration 211/1000 | Loss: 0.00002515
Iteration 212/1000 | Loss: 0.00002514
Iteration 213/1000 | Loss: 0.00002513
Iteration 214/1000 | Loss: 0.00002513
Iteration 215/1000 | Loss: 0.00002512
Iteration 216/1000 | Loss: 0.00002511
Iteration 217/1000 | Loss: 0.00002510
Iteration 218/1000 | Loss: 0.00002507
Iteration 219/1000 | Loss: 0.00002507
Iteration 220/1000 | Loss: 0.00002507
Iteration 221/1000 | Loss: 0.00002504
Iteration 222/1000 | Loss: 0.00002503
Iteration 223/1000 | Loss: 0.00002503
Iteration 224/1000 | Loss: 0.00002501
Iteration 225/1000 | Loss: 0.00002500
Iteration 226/1000 | Loss: 0.00002500
Iteration 227/1000 | Loss: 0.00002499
Iteration 228/1000 | Loss: 0.00002498
Iteration 229/1000 | Loss: 0.00002498
Iteration 230/1000 | Loss: 0.00002498
Iteration 231/1000 | Loss: 0.00002497
Iteration 232/1000 | Loss: 0.00002496
Iteration 233/1000 | Loss: 0.00002496
Iteration 234/1000 | Loss: 0.00002496
Iteration 235/1000 | Loss: 0.00002496
Iteration 236/1000 | Loss: 0.00002496
Iteration 237/1000 | Loss: 0.00002495
Iteration 238/1000 | Loss: 0.00002495
Iteration 239/1000 | Loss: 0.00002495
Iteration 240/1000 | Loss: 0.00002495
Iteration 241/1000 | Loss: 0.00002495
Iteration 242/1000 | Loss: 0.00002495
Iteration 243/1000 | Loss: 0.00002495
Iteration 244/1000 | Loss: 0.00002495
Iteration 245/1000 | Loss: 0.00002495
Iteration 246/1000 | Loss: 0.00002495
Iteration 247/1000 | Loss: 0.00002494
Iteration 248/1000 | Loss: 0.00002494
Iteration 249/1000 | Loss: 0.00002494
Iteration 250/1000 | Loss: 0.00002494
Iteration 251/1000 | Loss: 0.00002494
Iteration 252/1000 | Loss: 0.00002494
Iteration 253/1000 | Loss: 0.00002494
Iteration 254/1000 | Loss: 0.00002494
Iteration 255/1000 | Loss: 0.00002494
Iteration 256/1000 | Loss: 0.00002494
Iteration 257/1000 | Loss: 0.00002494
Iteration 258/1000 | Loss: 0.00002493
Iteration 259/1000 | Loss: 0.00002493
Iteration 260/1000 | Loss: 0.00002493
Iteration 261/1000 | Loss: 0.00002493
Iteration 262/1000 | Loss: 0.00002493
Iteration 263/1000 | Loss: 0.00002493
Iteration 264/1000 | Loss: 0.00002492
Iteration 265/1000 | Loss: 0.00002492
Iteration 266/1000 | Loss: 0.00002492
Iteration 267/1000 | Loss: 0.00002492
Iteration 268/1000 | Loss: 0.00002492
Iteration 269/1000 | Loss: 0.00002492
Iteration 270/1000 | Loss: 0.00002492
Iteration 271/1000 | Loss: 0.00002492
Iteration 272/1000 | Loss: 0.00002492
Iteration 273/1000 | Loss: 0.00002492
Iteration 274/1000 | Loss: 0.00002492
Iteration 275/1000 | Loss: 0.00002492
Iteration 276/1000 | Loss: 0.00002492
Iteration 277/1000 | Loss: 0.00002492
Iteration 278/1000 | Loss: 0.00002492
Iteration 279/1000 | Loss: 0.00002492
Iteration 280/1000 | Loss: 0.00002492
Iteration 281/1000 | Loss: 0.00002492
Iteration 282/1000 | Loss: 0.00002492
Iteration 283/1000 | Loss: 0.00002492
Iteration 284/1000 | Loss: 0.00002492
Iteration 285/1000 | Loss: 0.00002492
Iteration 286/1000 | Loss: 0.00002492
Iteration 287/1000 | Loss: 0.00002492
Iteration 288/1000 | Loss: 0.00002492
Iteration 289/1000 | Loss: 0.00002492
Iteration 290/1000 | Loss: 0.00002492
Iteration 291/1000 | Loss: 0.00002492
Iteration 292/1000 | Loss: 0.00002492
Iteration 293/1000 | Loss: 0.00002492
Iteration 294/1000 | Loss: 0.00002492
Iteration 295/1000 | Loss: 0.00002492
Iteration 296/1000 | Loss: 0.00002492
Iteration 297/1000 | Loss: 0.00002492
Iteration 298/1000 | Loss: 0.00002492
Iteration 299/1000 | Loss: 0.00002492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [2.4922510419855826e-05, 2.4922510419855826e-05, 2.4922510419855826e-05, 2.4922510419855826e-05, 2.4922510419855826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4922510419855826e-05

Optimization complete. Final v2v error: 4.166462421417236 mm

Highest mean error: 5.9158034324646 mm for frame 98

Lowest mean error: 3.2261250019073486 mm for frame 144

Saving results

Total time: 201.2453272342682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798792
Iteration 2/25 | Loss: 0.00140740
Iteration 3/25 | Loss: 0.00131488
Iteration 4/25 | Loss: 0.00129878
Iteration 5/25 | Loss: 0.00129362
Iteration 6/25 | Loss: 0.00129309
Iteration 7/25 | Loss: 0.00129309
Iteration 8/25 | Loss: 0.00129309
Iteration 9/25 | Loss: 0.00129309
Iteration 10/25 | Loss: 0.00129309
Iteration 11/25 | Loss: 0.00129309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001293088193051517, 0.001293088193051517, 0.001293088193051517, 0.001293088193051517, 0.001293088193051517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001293088193051517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15650427
Iteration 2/25 | Loss: 0.00176877
Iteration 3/25 | Loss: 0.00176877
Iteration 4/25 | Loss: 0.00176877
Iteration 5/25 | Loss: 0.00176877
Iteration 6/25 | Loss: 0.00176877
Iteration 7/25 | Loss: 0.00176876
Iteration 8/25 | Loss: 0.00176876
Iteration 9/25 | Loss: 0.00176876
Iteration 10/25 | Loss: 0.00176876
Iteration 11/25 | Loss: 0.00176876
Iteration 12/25 | Loss: 0.00176876
Iteration 13/25 | Loss: 0.00176876
Iteration 14/25 | Loss: 0.00176876
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017687639920040965, 0.0017687639920040965, 0.0017687639920040965, 0.0017687639920040965, 0.0017687639920040965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017687639920040965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176876
Iteration 2/1000 | Loss: 0.00004574
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002649
Iteration 5/1000 | Loss: 0.00002527
Iteration 6/1000 | Loss: 0.00002423
Iteration 7/1000 | Loss: 0.00002369
Iteration 8/1000 | Loss: 0.00002311
Iteration 9/1000 | Loss: 0.00002268
Iteration 10/1000 | Loss: 0.00002235
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002177
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002139
Iteration 15/1000 | Loss: 0.00002124
Iteration 16/1000 | Loss: 0.00002121
Iteration 17/1000 | Loss: 0.00002120
Iteration 18/1000 | Loss: 0.00002116
Iteration 19/1000 | Loss: 0.00002103
Iteration 20/1000 | Loss: 0.00002098
Iteration 21/1000 | Loss: 0.00002098
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002093
Iteration 25/1000 | Loss: 0.00002093
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002092
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002088
Iteration 33/1000 | Loss: 0.00002088
Iteration 34/1000 | Loss: 0.00002088
Iteration 35/1000 | Loss: 0.00002088
Iteration 36/1000 | Loss: 0.00002088
Iteration 37/1000 | Loss: 0.00002088
Iteration 38/1000 | Loss: 0.00002087
Iteration 39/1000 | Loss: 0.00002087
Iteration 40/1000 | Loss: 0.00002087
Iteration 41/1000 | Loss: 0.00002086
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002086
Iteration 44/1000 | Loss: 0.00002085
Iteration 45/1000 | Loss: 0.00002085
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002084
Iteration 48/1000 | Loss: 0.00002084
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002082
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002082
Iteration 55/1000 | Loss: 0.00002082
Iteration 56/1000 | Loss: 0.00002082
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00002080
Iteration 59/1000 | Loss: 0.00002080
Iteration 60/1000 | Loss: 0.00002080
Iteration 61/1000 | Loss: 0.00002079
Iteration 62/1000 | Loss: 0.00002079
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002078
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002076
Iteration 68/1000 | Loss: 0.00002075
Iteration 69/1000 | Loss: 0.00002075
Iteration 70/1000 | Loss: 0.00002075
Iteration 71/1000 | Loss: 0.00002075
Iteration 72/1000 | Loss: 0.00002075
Iteration 73/1000 | Loss: 0.00002075
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002075
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002074
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002074
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002072
Iteration 85/1000 | Loss: 0.00002072
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002071
Iteration 88/1000 | Loss: 0.00002071
Iteration 89/1000 | Loss: 0.00002071
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002070
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002070
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002066
Iteration 121/1000 | Loss: 0.00002066
Iteration 122/1000 | Loss: 0.00002066
Iteration 123/1000 | Loss: 0.00002065
Iteration 124/1000 | Loss: 0.00002065
Iteration 125/1000 | Loss: 0.00002065
Iteration 126/1000 | Loss: 0.00002065
Iteration 127/1000 | Loss: 0.00002065
Iteration 128/1000 | Loss: 0.00002065
Iteration 129/1000 | Loss: 0.00002065
Iteration 130/1000 | Loss: 0.00002065
Iteration 131/1000 | Loss: 0.00002065
Iteration 132/1000 | Loss: 0.00002065
Iteration 133/1000 | Loss: 0.00002065
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00002064
Iteration 136/1000 | Loss: 0.00002064
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002064
Iteration 139/1000 | Loss: 0.00002064
Iteration 140/1000 | Loss: 0.00002064
Iteration 141/1000 | Loss: 0.00002064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.0644540200009942e-05, 2.0644540200009942e-05, 2.0644540200009942e-05, 2.0644540200009942e-05, 2.0644540200009942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0644540200009942e-05

Optimization complete. Final v2v error: 3.9094398021698 mm

Highest mean error: 4.251720428466797 mm for frame 0

Lowest mean error: 3.6853697299957275 mm for frame 189

Saving results

Total time: 42.7978720664978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501144
Iteration 2/25 | Loss: 0.00155119
Iteration 3/25 | Loss: 0.00137425
Iteration 4/25 | Loss: 0.00133830
Iteration 5/25 | Loss: 0.00133148
Iteration 6/25 | Loss: 0.00133086
Iteration 7/25 | Loss: 0.00133086
Iteration 8/25 | Loss: 0.00133086
Iteration 9/25 | Loss: 0.00133086
Iteration 10/25 | Loss: 0.00133086
Iteration 11/25 | Loss: 0.00133086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013308636844158173, 0.0013308636844158173, 0.0013308636844158173, 0.0013308636844158173, 0.0013308636844158173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013308636844158173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23964643
Iteration 2/25 | Loss: 0.00128853
Iteration 3/25 | Loss: 0.00128852
Iteration 4/25 | Loss: 0.00128852
Iteration 5/25 | Loss: 0.00128852
Iteration 6/25 | Loss: 0.00128852
Iteration 7/25 | Loss: 0.00128852
Iteration 8/25 | Loss: 0.00128852
Iteration 9/25 | Loss: 0.00128852
Iteration 10/25 | Loss: 0.00128851
Iteration 11/25 | Loss: 0.00128851
Iteration 12/25 | Loss: 0.00128851
Iteration 13/25 | Loss: 0.00128851
Iteration 14/25 | Loss: 0.00128851
Iteration 15/25 | Loss: 0.00128851
Iteration 16/25 | Loss: 0.00128851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012885145843029022, 0.0012885145843029022, 0.0012885145843029022, 0.0012885145843029022, 0.0012885145843029022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012885145843029022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128851
Iteration 2/1000 | Loss: 0.00003827
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002288
Iteration 5/1000 | Loss: 0.00002189
Iteration 6/1000 | Loss: 0.00002090
Iteration 7/1000 | Loss: 0.00002028
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001884
Iteration 12/1000 | Loss: 0.00001871
Iteration 13/1000 | Loss: 0.00001866
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001859
Iteration 17/1000 | Loss: 0.00001851
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001847
Iteration 20/1000 | Loss: 0.00001846
Iteration 21/1000 | Loss: 0.00001846
Iteration 22/1000 | Loss: 0.00001845
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001844
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001843
Iteration 28/1000 | Loss: 0.00001842
Iteration 29/1000 | Loss: 0.00001842
Iteration 30/1000 | Loss: 0.00001841
Iteration 31/1000 | Loss: 0.00001841
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001840
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001836
Iteration 38/1000 | Loss: 0.00001836
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001835
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001834
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001830
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001829
Iteration 54/1000 | Loss: 0.00001829
Iteration 55/1000 | Loss: 0.00001829
Iteration 56/1000 | Loss: 0.00001828
Iteration 57/1000 | Loss: 0.00001828
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001827
Iteration 60/1000 | Loss: 0.00001827
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001827
Iteration 66/1000 | Loss: 0.00001827
Iteration 67/1000 | Loss: 0.00001827
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001825
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001818
Iteration 108/1000 | Loss: 0.00001818
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001817
Iteration 115/1000 | Loss: 0.00001816
Iteration 116/1000 | Loss: 0.00001816
Iteration 117/1000 | Loss: 0.00001816
Iteration 118/1000 | Loss: 0.00001816
Iteration 119/1000 | Loss: 0.00001815
Iteration 120/1000 | Loss: 0.00001815
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001814
Iteration 125/1000 | Loss: 0.00001814
Iteration 126/1000 | Loss: 0.00001814
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001814
Iteration 130/1000 | Loss: 0.00001814
Iteration 131/1000 | Loss: 0.00001814
Iteration 132/1000 | Loss: 0.00001814
Iteration 133/1000 | Loss: 0.00001814
Iteration 134/1000 | Loss: 0.00001813
Iteration 135/1000 | Loss: 0.00001813
Iteration 136/1000 | Loss: 0.00001813
Iteration 137/1000 | Loss: 0.00001813
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001812
Iteration 140/1000 | Loss: 0.00001812
Iteration 141/1000 | Loss: 0.00001812
Iteration 142/1000 | Loss: 0.00001811
Iteration 143/1000 | Loss: 0.00001811
Iteration 144/1000 | Loss: 0.00001811
Iteration 145/1000 | Loss: 0.00001811
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001810
Iteration 148/1000 | Loss: 0.00001810
Iteration 149/1000 | Loss: 0.00001810
Iteration 150/1000 | Loss: 0.00001810
Iteration 151/1000 | Loss: 0.00001809
Iteration 152/1000 | Loss: 0.00001809
Iteration 153/1000 | Loss: 0.00001809
Iteration 154/1000 | Loss: 0.00001809
Iteration 155/1000 | Loss: 0.00001809
Iteration 156/1000 | Loss: 0.00001809
Iteration 157/1000 | Loss: 0.00001809
Iteration 158/1000 | Loss: 0.00001809
Iteration 159/1000 | Loss: 0.00001809
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001808
Iteration 163/1000 | Loss: 0.00001808
Iteration 164/1000 | Loss: 0.00001808
Iteration 165/1000 | Loss: 0.00001808
Iteration 166/1000 | Loss: 0.00001808
Iteration 167/1000 | Loss: 0.00001808
Iteration 168/1000 | Loss: 0.00001808
Iteration 169/1000 | Loss: 0.00001808
Iteration 170/1000 | Loss: 0.00001808
Iteration 171/1000 | Loss: 0.00001808
Iteration 172/1000 | Loss: 0.00001807
Iteration 173/1000 | Loss: 0.00001807
Iteration 174/1000 | Loss: 0.00001807
Iteration 175/1000 | Loss: 0.00001807
Iteration 176/1000 | Loss: 0.00001806
Iteration 177/1000 | Loss: 0.00001806
Iteration 178/1000 | Loss: 0.00001806
Iteration 179/1000 | Loss: 0.00001805
Iteration 180/1000 | Loss: 0.00001805
Iteration 181/1000 | Loss: 0.00001805
Iteration 182/1000 | Loss: 0.00001805
Iteration 183/1000 | Loss: 0.00001805
Iteration 184/1000 | Loss: 0.00001805
Iteration 185/1000 | Loss: 0.00001804
Iteration 186/1000 | Loss: 0.00001804
Iteration 187/1000 | Loss: 0.00001803
Iteration 188/1000 | Loss: 0.00001803
Iteration 189/1000 | Loss: 0.00001803
Iteration 190/1000 | Loss: 0.00001803
Iteration 191/1000 | Loss: 0.00001803
Iteration 192/1000 | Loss: 0.00001803
Iteration 193/1000 | Loss: 0.00001803
Iteration 194/1000 | Loss: 0.00001803
Iteration 195/1000 | Loss: 0.00001803
Iteration 196/1000 | Loss: 0.00001803
Iteration 197/1000 | Loss: 0.00001803
Iteration 198/1000 | Loss: 0.00001802
Iteration 199/1000 | Loss: 0.00001802
Iteration 200/1000 | Loss: 0.00001802
Iteration 201/1000 | Loss: 0.00001802
Iteration 202/1000 | Loss: 0.00001802
Iteration 203/1000 | Loss: 0.00001802
Iteration 204/1000 | Loss: 0.00001801
Iteration 205/1000 | Loss: 0.00001801
Iteration 206/1000 | Loss: 0.00001801
Iteration 207/1000 | Loss: 0.00001801
Iteration 208/1000 | Loss: 0.00001801
Iteration 209/1000 | Loss: 0.00001801
Iteration 210/1000 | Loss: 0.00001801
Iteration 211/1000 | Loss: 0.00001801
Iteration 212/1000 | Loss: 0.00001801
Iteration 213/1000 | Loss: 0.00001801
Iteration 214/1000 | Loss: 0.00001801
Iteration 215/1000 | Loss: 0.00001801
Iteration 216/1000 | Loss: 0.00001801
Iteration 217/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.8005513993557543e-05, 1.8005513993557543e-05, 1.8005513993557543e-05, 1.8005513993557543e-05, 1.8005513993557543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8005513993557543e-05

Optimization complete. Final v2v error: 3.6359426975250244 mm

Highest mean error: 3.930126667022705 mm for frame 134

Lowest mean error: 3.4953927993774414 mm for frame 65

Saving results

Total time: 42.12257242202759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389998
Iteration 2/25 | Loss: 0.00137607
Iteration 3/25 | Loss: 0.00130511
Iteration 4/25 | Loss: 0.00128870
Iteration 5/25 | Loss: 0.00128215
Iteration 6/25 | Loss: 0.00128042
Iteration 7/25 | Loss: 0.00128004
Iteration 8/25 | Loss: 0.00128004
Iteration 9/25 | Loss: 0.00128004
Iteration 10/25 | Loss: 0.00128004
Iteration 11/25 | Loss: 0.00128004
Iteration 12/25 | Loss: 0.00128004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012800373369827867, 0.0012800373369827867, 0.0012800373369827867, 0.0012800373369827867, 0.0012800373369827867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012800373369827867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36473346
Iteration 2/25 | Loss: 0.00188813
Iteration 3/25 | Loss: 0.00188813
Iteration 4/25 | Loss: 0.00188813
Iteration 5/25 | Loss: 0.00188813
Iteration 6/25 | Loss: 0.00188813
Iteration 7/25 | Loss: 0.00188813
Iteration 8/25 | Loss: 0.00188813
Iteration 9/25 | Loss: 0.00188813
Iteration 10/25 | Loss: 0.00188813
Iteration 11/25 | Loss: 0.00188813
Iteration 12/25 | Loss: 0.00188813
Iteration 13/25 | Loss: 0.00188813
Iteration 14/25 | Loss: 0.00188813
Iteration 15/25 | Loss: 0.00188813
Iteration 16/25 | Loss: 0.00188813
Iteration 17/25 | Loss: 0.00188813
Iteration 18/25 | Loss: 0.00188813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018881313735619187, 0.0018881313735619187, 0.0018881313735619187, 0.0018881313735619187, 0.0018881313735619187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018881313735619187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188813
Iteration 2/1000 | Loss: 0.00005633
Iteration 3/1000 | Loss: 0.00004092
Iteration 4/1000 | Loss: 0.00003153
Iteration 5/1000 | Loss: 0.00002830
Iteration 6/1000 | Loss: 0.00002610
Iteration 7/1000 | Loss: 0.00002469
Iteration 8/1000 | Loss: 0.00002372
Iteration 9/1000 | Loss: 0.00002290
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002190
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002112
Iteration 15/1000 | Loss: 0.00002095
Iteration 16/1000 | Loss: 0.00002078
Iteration 17/1000 | Loss: 0.00002074
Iteration 18/1000 | Loss: 0.00002066
Iteration 19/1000 | Loss: 0.00002059
Iteration 20/1000 | Loss: 0.00002058
Iteration 21/1000 | Loss: 0.00002052
Iteration 22/1000 | Loss: 0.00002049
Iteration 23/1000 | Loss: 0.00002042
Iteration 24/1000 | Loss: 0.00002042
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002041
Iteration 28/1000 | Loss: 0.00002041
Iteration 29/1000 | Loss: 0.00002041
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002040
Iteration 32/1000 | Loss: 0.00002040
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002038
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002034
Iteration 50/1000 | Loss: 0.00002034
Iteration 51/1000 | Loss: 0.00002034
Iteration 52/1000 | Loss: 0.00002034
Iteration 53/1000 | Loss: 0.00002034
Iteration 54/1000 | Loss: 0.00002034
Iteration 55/1000 | Loss: 0.00002034
Iteration 56/1000 | Loss: 0.00002033
Iteration 57/1000 | Loss: 0.00002033
Iteration 58/1000 | Loss: 0.00002033
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002032
Iteration 61/1000 | Loss: 0.00002032
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002031
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002030
Iteration 67/1000 | Loss: 0.00002030
Iteration 68/1000 | Loss: 0.00002030
Iteration 69/1000 | Loss: 0.00002030
Iteration 70/1000 | Loss: 0.00002030
Iteration 71/1000 | Loss: 0.00002029
Iteration 72/1000 | Loss: 0.00002029
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00002028
Iteration 75/1000 | Loss: 0.00002028
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002024
Iteration 91/1000 | Loss: 0.00002024
Iteration 92/1000 | Loss: 0.00002024
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002023
Iteration 95/1000 | Loss: 0.00002023
Iteration 96/1000 | Loss: 0.00002023
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002022
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00002022
Iteration 102/1000 | Loss: 0.00002022
Iteration 103/1000 | Loss: 0.00002022
Iteration 104/1000 | Loss: 0.00002022
Iteration 105/1000 | Loss: 0.00002022
Iteration 106/1000 | Loss: 0.00002022
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002021
Iteration 110/1000 | Loss: 0.00002021
Iteration 111/1000 | Loss: 0.00002021
Iteration 112/1000 | Loss: 0.00002021
Iteration 113/1000 | Loss: 0.00002021
Iteration 114/1000 | Loss: 0.00002021
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002021
Iteration 118/1000 | Loss: 0.00002021
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002020
Iteration 121/1000 | Loss: 0.00002020
Iteration 122/1000 | Loss: 0.00002020
Iteration 123/1000 | Loss: 0.00002020
Iteration 124/1000 | Loss: 0.00002020
Iteration 125/1000 | Loss: 0.00002020
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002019
Iteration 128/1000 | Loss: 0.00002019
Iteration 129/1000 | Loss: 0.00002019
Iteration 130/1000 | Loss: 0.00002019
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002019
Iteration 133/1000 | Loss: 0.00002019
Iteration 134/1000 | Loss: 0.00002019
Iteration 135/1000 | Loss: 0.00002018
Iteration 136/1000 | Loss: 0.00002018
Iteration 137/1000 | Loss: 0.00002018
Iteration 138/1000 | Loss: 0.00002018
Iteration 139/1000 | Loss: 0.00002018
Iteration 140/1000 | Loss: 0.00002018
Iteration 141/1000 | Loss: 0.00002018
Iteration 142/1000 | Loss: 0.00002018
Iteration 143/1000 | Loss: 0.00002018
Iteration 144/1000 | Loss: 0.00002018
Iteration 145/1000 | Loss: 0.00002018
Iteration 146/1000 | Loss: 0.00002018
Iteration 147/1000 | Loss: 0.00002018
Iteration 148/1000 | Loss: 0.00002018
Iteration 149/1000 | Loss: 0.00002017
Iteration 150/1000 | Loss: 0.00002017
Iteration 151/1000 | Loss: 0.00002017
Iteration 152/1000 | Loss: 0.00002017
Iteration 153/1000 | Loss: 0.00002017
Iteration 154/1000 | Loss: 0.00002017
Iteration 155/1000 | Loss: 0.00002017
Iteration 156/1000 | Loss: 0.00002017
Iteration 157/1000 | Loss: 0.00002017
Iteration 158/1000 | Loss: 0.00002016
Iteration 159/1000 | Loss: 0.00002016
Iteration 160/1000 | Loss: 0.00002016
Iteration 161/1000 | Loss: 0.00002016
Iteration 162/1000 | Loss: 0.00002016
Iteration 163/1000 | Loss: 0.00002016
Iteration 164/1000 | Loss: 0.00002016
Iteration 165/1000 | Loss: 0.00002015
Iteration 166/1000 | Loss: 0.00002015
Iteration 167/1000 | Loss: 0.00002015
Iteration 168/1000 | Loss: 0.00002015
Iteration 169/1000 | Loss: 0.00002015
Iteration 170/1000 | Loss: 0.00002015
Iteration 171/1000 | Loss: 0.00002015
Iteration 172/1000 | Loss: 0.00002015
Iteration 173/1000 | Loss: 0.00002015
Iteration 174/1000 | Loss: 0.00002014
Iteration 175/1000 | Loss: 0.00002014
Iteration 176/1000 | Loss: 0.00002014
Iteration 177/1000 | Loss: 0.00002014
Iteration 178/1000 | Loss: 0.00002014
Iteration 179/1000 | Loss: 0.00002014
Iteration 180/1000 | Loss: 0.00002014
Iteration 181/1000 | Loss: 0.00002014
Iteration 182/1000 | Loss: 0.00002014
Iteration 183/1000 | Loss: 0.00002013
Iteration 184/1000 | Loss: 0.00002013
Iteration 185/1000 | Loss: 0.00002013
Iteration 186/1000 | Loss: 0.00002013
Iteration 187/1000 | Loss: 0.00002013
Iteration 188/1000 | Loss: 0.00002013
Iteration 189/1000 | Loss: 0.00002013
Iteration 190/1000 | Loss: 0.00002013
Iteration 191/1000 | Loss: 0.00002013
Iteration 192/1000 | Loss: 0.00002013
Iteration 193/1000 | Loss: 0.00002013
Iteration 194/1000 | Loss: 0.00002013
Iteration 195/1000 | Loss: 0.00002013
Iteration 196/1000 | Loss: 0.00002013
Iteration 197/1000 | Loss: 0.00002013
Iteration 198/1000 | Loss: 0.00002013
Iteration 199/1000 | Loss: 0.00002012
Iteration 200/1000 | Loss: 0.00002012
Iteration 201/1000 | Loss: 0.00002012
Iteration 202/1000 | Loss: 0.00002012
Iteration 203/1000 | Loss: 0.00002012
Iteration 204/1000 | Loss: 0.00002012
Iteration 205/1000 | Loss: 0.00002012
Iteration 206/1000 | Loss: 0.00002012
Iteration 207/1000 | Loss: 0.00002012
Iteration 208/1000 | Loss: 0.00002012
Iteration 209/1000 | Loss: 0.00002012
Iteration 210/1000 | Loss: 0.00002012
Iteration 211/1000 | Loss: 0.00002012
Iteration 212/1000 | Loss: 0.00002012
Iteration 213/1000 | Loss: 0.00002012
Iteration 214/1000 | Loss: 0.00002011
Iteration 215/1000 | Loss: 0.00002011
Iteration 216/1000 | Loss: 0.00002011
Iteration 217/1000 | Loss: 0.00002011
Iteration 218/1000 | Loss: 0.00002011
Iteration 219/1000 | Loss: 0.00002011
Iteration 220/1000 | Loss: 0.00002010
Iteration 221/1000 | Loss: 0.00002010
Iteration 222/1000 | Loss: 0.00002010
Iteration 223/1000 | Loss: 0.00002010
Iteration 224/1000 | Loss: 0.00002010
Iteration 225/1000 | Loss: 0.00002010
Iteration 226/1000 | Loss: 0.00002010
Iteration 227/1000 | Loss: 0.00002010
Iteration 228/1000 | Loss: 0.00002010
Iteration 229/1000 | Loss: 0.00002010
Iteration 230/1000 | Loss: 0.00002010
Iteration 231/1000 | Loss: 0.00002010
Iteration 232/1000 | Loss: 0.00002010
Iteration 233/1000 | Loss: 0.00002010
Iteration 234/1000 | Loss: 0.00002010
Iteration 235/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.009853233175818e-05, 2.009853233175818e-05, 2.009853233175818e-05, 2.009853233175818e-05, 2.009853233175818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.009853233175818e-05

Optimization complete. Final v2v error: 3.7097108364105225 mm

Highest mean error: 5.051444053649902 mm for frame 105

Lowest mean error: 2.7049427032470703 mm for frame 64

Saving results

Total time: 49.54607129096985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716255
Iteration 2/25 | Loss: 0.00202852
Iteration 3/25 | Loss: 0.00151536
Iteration 4/25 | Loss: 0.00144733
Iteration 5/25 | Loss: 0.00142755
Iteration 6/25 | Loss: 0.00142040
Iteration 7/25 | Loss: 0.00142209
Iteration 8/25 | Loss: 0.00141160
Iteration 9/25 | Loss: 0.00140977
Iteration 10/25 | Loss: 0.00141193
Iteration 11/25 | Loss: 0.00140935
Iteration 12/25 | Loss: 0.00141076
Iteration 13/25 | Loss: 0.00140800
Iteration 14/25 | Loss: 0.00140769
Iteration 15/25 | Loss: 0.00140761
Iteration 16/25 | Loss: 0.00140760
Iteration 17/25 | Loss: 0.00140759
Iteration 18/25 | Loss: 0.00140759
Iteration 19/25 | Loss: 0.00140758
Iteration 20/25 | Loss: 0.00140758
Iteration 21/25 | Loss: 0.00140758
Iteration 22/25 | Loss: 0.00140758
Iteration 23/25 | Loss: 0.00140758
Iteration 24/25 | Loss: 0.00140758
Iteration 25/25 | Loss: 0.00140758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.02562141
Iteration 2/25 | Loss: 0.00240689
Iteration 3/25 | Loss: 0.00240495
Iteration 4/25 | Loss: 0.00240494
Iteration 5/25 | Loss: 0.00240494
Iteration 6/25 | Loss: 0.00240494
Iteration 7/25 | Loss: 0.00240494
Iteration 8/25 | Loss: 0.00240494
Iteration 9/25 | Loss: 0.00240494
Iteration 10/25 | Loss: 0.00240494
Iteration 11/25 | Loss: 0.00240494
Iteration 12/25 | Loss: 0.00240494
Iteration 13/25 | Loss: 0.00240494
Iteration 14/25 | Loss: 0.00240494
Iteration 15/25 | Loss: 0.00240494
Iteration 16/25 | Loss: 0.00240494
Iteration 17/25 | Loss: 0.00240494
Iteration 18/25 | Loss: 0.00240494
Iteration 19/25 | Loss: 0.00240494
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0024049412459135056, 0.0024049412459135056, 0.0024049412459135056, 0.0024049412459135056, 0.0024049412459135056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024049412459135056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240494
Iteration 2/1000 | Loss: 0.00028841
Iteration 3/1000 | Loss: 0.00023784
Iteration 4/1000 | Loss: 0.00018636
Iteration 5/1000 | Loss: 0.00009095
Iteration 6/1000 | Loss: 0.00016839
Iteration 7/1000 | Loss: 0.00018100
Iteration 8/1000 | Loss: 0.00009941
Iteration 9/1000 | Loss: 0.00015625
Iteration 10/1000 | Loss: 0.00010159
Iteration 11/1000 | Loss: 0.00020046
Iteration 12/1000 | Loss: 0.00021451
Iteration 13/1000 | Loss: 0.00019776
Iteration 14/1000 | Loss: 0.00012222
Iteration 15/1000 | Loss: 0.00007781
Iteration 16/1000 | Loss: 0.00015373
Iteration 17/1000 | Loss: 0.00008908
Iteration 18/1000 | Loss: 0.00011058
Iteration 19/1000 | Loss: 0.00018556
Iteration 20/1000 | Loss: 0.00019379
Iteration 21/1000 | Loss: 0.00019741
Iteration 22/1000 | Loss: 0.00018177
Iteration 23/1000 | Loss: 0.00022828
Iteration 24/1000 | Loss: 0.00016243
Iteration 25/1000 | Loss: 0.00027642
Iteration 26/1000 | Loss: 0.00019466
Iteration 27/1000 | Loss: 0.00026543
Iteration 28/1000 | Loss: 0.00017630
Iteration 29/1000 | Loss: 0.00015945
Iteration 30/1000 | Loss: 0.00008334
Iteration 31/1000 | Loss: 0.00012780
Iteration 32/1000 | Loss: 0.00016465
Iteration 33/1000 | Loss: 0.00016241
Iteration 34/1000 | Loss: 0.00015464
Iteration 35/1000 | Loss: 0.00021730
Iteration 36/1000 | Loss: 0.00020483
Iteration 37/1000 | Loss: 0.00024276
Iteration 38/1000 | Loss: 0.00020763
Iteration 39/1000 | Loss: 0.00019845
Iteration 40/1000 | Loss: 0.00030977
Iteration 41/1000 | Loss: 0.00022624
Iteration 42/1000 | Loss: 0.00015907
Iteration 43/1000 | Loss: 0.00017652
Iteration 44/1000 | Loss: 0.00013665
Iteration 45/1000 | Loss: 0.00016304
Iteration 46/1000 | Loss: 0.00022211
Iteration 47/1000 | Loss: 0.00023612
Iteration 48/1000 | Loss: 0.00019460
Iteration 49/1000 | Loss: 0.00029469
Iteration 50/1000 | Loss: 0.00018655
Iteration 51/1000 | Loss: 0.00020245
Iteration 52/1000 | Loss: 0.00021972
Iteration 53/1000 | Loss: 0.00014194
Iteration 54/1000 | Loss: 0.00015368
Iteration 55/1000 | Loss: 0.00013526
Iteration 56/1000 | Loss: 0.00012285
Iteration 57/1000 | Loss: 0.00014244
Iteration 58/1000 | Loss: 0.00015096
Iteration 59/1000 | Loss: 0.00014726
Iteration 60/1000 | Loss: 0.00031937
Iteration 61/1000 | Loss: 0.00013960
Iteration 62/1000 | Loss: 0.00007369
Iteration 63/1000 | Loss: 0.00008232
Iteration 64/1000 | Loss: 0.00016534
Iteration 65/1000 | Loss: 0.00018108
Iteration 66/1000 | Loss: 0.00019910
Iteration 67/1000 | Loss: 0.00017749
Iteration 68/1000 | Loss: 0.00015927
Iteration 69/1000 | Loss: 0.00016474
Iteration 70/1000 | Loss: 0.00018304
Iteration 71/1000 | Loss: 0.00018189
Iteration 72/1000 | Loss: 0.00036106
Iteration 73/1000 | Loss: 0.00010017
Iteration 74/1000 | Loss: 0.00014338
Iteration 75/1000 | Loss: 0.00007799
Iteration 76/1000 | Loss: 0.00012961
Iteration 77/1000 | Loss: 0.00007969
Iteration 78/1000 | Loss: 0.00009736
Iteration 79/1000 | Loss: 0.00008264
Iteration 80/1000 | Loss: 0.00009373
Iteration 81/1000 | Loss: 0.00020339
Iteration 82/1000 | Loss: 0.00008786
Iteration 83/1000 | Loss: 0.00015330
Iteration 84/1000 | Loss: 0.00011450
Iteration 85/1000 | Loss: 0.00013990
Iteration 86/1000 | Loss: 0.00016012
Iteration 87/1000 | Loss: 0.00027406
Iteration 88/1000 | Loss: 0.00014530
Iteration 89/1000 | Loss: 0.00017869
Iteration 90/1000 | Loss: 0.00008782
Iteration 91/1000 | Loss: 0.00007414
Iteration 92/1000 | Loss: 0.00008003
Iteration 93/1000 | Loss: 0.00013118
Iteration 94/1000 | Loss: 0.00017295
Iteration 95/1000 | Loss: 0.00018631
Iteration 96/1000 | Loss: 0.00013647
Iteration 97/1000 | Loss: 0.00009784
Iteration 98/1000 | Loss: 0.00015322
Iteration 99/1000 | Loss: 0.00009498
Iteration 100/1000 | Loss: 0.00010877
Iteration 101/1000 | Loss: 0.00012117
Iteration 102/1000 | Loss: 0.00010380
Iteration 103/1000 | Loss: 0.00021179
Iteration 104/1000 | Loss: 0.00018513
Iteration 105/1000 | Loss: 0.00016410
Iteration 106/1000 | Loss: 0.00011314
Iteration 107/1000 | Loss: 0.00016006
Iteration 108/1000 | Loss: 0.00015590
Iteration 109/1000 | Loss: 0.00016091
Iteration 110/1000 | Loss: 0.00016978
Iteration 111/1000 | Loss: 0.00022209
Iteration 112/1000 | Loss: 0.00018755
Iteration 113/1000 | Loss: 0.00017493
Iteration 114/1000 | Loss: 0.00014485
Iteration 115/1000 | Loss: 0.00019624
Iteration 116/1000 | Loss: 0.00015206
Iteration 117/1000 | Loss: 0.00016156
Iteration 118/1000 | Loss: 0.00015402
Iteration 119/1000 | Loss: 0.00016057
Iteration 120/1000 | Loss: 0.00014949
Iteration 121/1000 | Loss: 0.00016013
Iteration 122/1000 | Loss: 0.00014500
Iteration 123/1000 | Loss: 0.00009158
Iteration 124/1000 | Loss: 0.00013694
Iteration 125/1000 | Loss: 0.00009222
Iteration 126/1000 | Loss: 0.00025235
Iteration 127/1000 | Loss: 0.00013207
Iteration 128/1000 | Loss: 0.00016038
Iteration 129/1000 | Loss: 0.00018637
Iteration 130/1000 | Loss: 0.00007669
Iteration 131/1000 | Loss: 0.00024027
Iteration 132/1000 | Loss: 0.00015847
Iteration 133/1000 | Loss: 0.00014523
Iteration 134/1000 | Loss: 0.00017002
Iteration 135/1000 | Loss: 0.00015356
Iteration 136/1000 | Loss: 0.00021193
Iteration 137/1000 | Loss: 0.00015830
Iteration 138/1000 | Loss: 0.00014497
Iteration 139/1000 | Loss: 0.00014547
Iteration 140/1000 | Loss: 0.00025764
Iteration 141/1000 | Loss: 0.00015697
Iteration 142/1000 | Loss: 0.00021979
Iteration 143/1000 | Loss: 0.00013115
Iteration 144/1000 | Loss: 0.00015179
Iteration 145/1000 | Loss: 0.00012558
Iteration 146/1000 | Loss: 0.00017600
Iteration 147/1000 | Loss: 0.00016795
Iteration 148/1000 | Loss: 0.00016911
Iteration 149/1000 | Loss: 0.00028397
Iteration 150/1000 | Loss: 0.00020210
Iteration 151/1000 | Loss: 0.00013405
Iteration 152/1000 | Loss: 0.00013000
Iteration 153/1000 | Loss: 0.00010509
Iteration 154/1000 | Loss: 0.00015381
Iteration 155/1000 | Loss: 0.00013768
Iteration 156/1000 | Loss: 0.00023574
Iteration 157/1000 | Loss: 0.00032696
Iteration 158/1000 | Loss: 0.00007475
Iteration 159/1000 | Loss: 0.00006784
Iteration 160/1000 | Loss: 0.00006671
Iteration 161/1000 | Loss: 0.00006623
Iteration 162/1000 | Loss: 0.00006580
Iteration 163/1000 | Loss: 0.00020352
Iteration 164/1000 | Loss: 0.00010901
Iteration 165/1000 | Loss: 0.00007118
Iteration 166/1000 | Loss: 0.00019567
Iteration 167/1000 | Loss: 0.00009380
Iteration 168/1000 | Loss: 0.00006638
Iteration 169/1000 | Loss: 0.00012807
Iteration 170/1000 | Loss: 0.00008218
Iteration 171/1000 | Loss: 0.00013567
Iteration 172/1000 | Loss: 0.00007392
Iteration 173/1000 | Loss: 0.00006684
Iteration 174/1000 | Loss: 0.00006495
Iteration 175/1000 | Loss: 0.00006369
Iteration 176/1000 | Loss: 0.00006293
Iteration 177/1000 | Loss: 0.00006236
Iteration 178/1000 | Loss: 0.00006179
Iteration 179/1000 | Loss: 0.00006154
Iteration 180/1000 | Loss: 0.00006129
Iteration 181/1000 | Loss: 0.00006112
Iteration 182/1000 | Loss: 0.00006109
Iteration 183/1000 | Loss: 0.00006107
Iteration 184/1000 | Loss: 0.00006104
Iteration 185/1000 | Loss: 0.00006103
Iteration 186/1000 | Loss: 0.00006100
Iteration 187/1000 | Loss: 0.00006087
Iteration 188/1000 | Loss: 0.00006074
Iteration 189/1000 | Loss: 0.00044389
Iteration 190/1000 | Loss: 0.00074871
Iteration 191/1000 | Loss: 0.00081278
Iteration 192/1000 | Loss: 0.00006351
Iteration 193/1000 | Loss: 0.00005875
Iteration 194/1000 | Loss: 0.00005576
Iteration 195/1000 | Loss: 0.00005360
Iteration 196/1000 | Loss: 0.00005132
Iteration 197/1000 | Loss: 0.00048445
Iteration 198/1000 | Loss: 0.00008125
Iteration 199/1000 | Loss: 0.00005411
Iteration 200/1000 | Loss: 0.00004903
Iteration 201/1000 | Loss: 0.00004771
Iteration 202/1000 | Loss: 0.00004701
Iteration 203/1000 | Loss: 0.00004628
Iteration 204/1000 | Loss: 0.00004583
Iteration 205/1000 | Loss: 0.00004545
Iteration 206/1000 | Loss: 0.00004512
Iteration 207/1000 | Loss: 0.00004488
Iteration 208/1000 | Loss: 0.00004467
Iteration 209/1000 | Loss: 0.00004455
Iteration 210/1000 | Loss: 0.00004441
Iteration 211/1000 | Loss: 0.00004439
Iteration 212/1000 | Loss: 0.00004438
Iteration 213/1000 | Loss: 0.00004437
Iteration 214/1000 | Loss: 0.00004436
Iteration 215/1000 | Loss: 0.00004436
Iteration 216/1000 | Loss: 0.00004434
Iteration 217/1000 | Loss: 0.00004434
Iteration 218/1000 | Loss: 0.00004433
Iteration 219/1000 | Loss: 0.00004433
Iteration 220/1000 | Loss: 0.00004429
Iteration 221/1000 | Loss: 0.00004429
Iteration 222/1000 | Loss: 0.00004429
Iteration 223/1000 | Loss: 0.00004429
Iteration 224/1000 | Loss: 0.00004428
Iteration 225/1000 | Loss: 0.00004425
Iteration 226/1000 | Loss: 0.00004422
Iteration 227/1000 | Loss: 0.00004419
Iteration 228/1000 | Loss: 0.00004418
Iteration 229/1000 | Loss: 0.00004418
Iteration 230/1000 | Loss: 0.00004418
Iteration 231/1000 | Loss: 0.00004418
Iteration 232/1000 | Loss: 0.00004418
Iteration 233/1000 | Loss: 0.00004418
Iteration 234/1000 | Loss: 0.00004417
Iteration 235/1000 | Loss: 0.00004417
Iteration 236/1000 | Loss: 0.00004417
Iteration 237/1000 | Loss: 0.00004417
Iteration 238/1000 | Loss: 0.00004417
Iteration 239/1000 | Loss: 0.00004417
Iteration 240/1000 | Loss: 0.00004417
Iteration 241/1000 | Loss: 0.00004417
Iteration 242/1000 | Loss: 0.00004415
Iteration 243/1000 | Loss: 0.00004415
Iteration 244/1000 | Loss: 0.00004415
Iteration 245/1000 | Loss: 0.00004415
Iteration 246/1000 | Loss: 0.00004415
Iteration 247/1000 | Loss: 0.00004415
Iteration 248/1000 | Loss: 0.00004415
Iteration 249/1000 | Loss: 0.00004415
Iteration 250/1000 | Loss: 0.00004415
Iteration 251/1000 | Loss: 0.00004414
Iteration 252/1000 | Loss: 0.00004414
Iteration 253/1000 | Loss: 0.00004413
Iteration 254/1000 | Loss: 0.00004413
Iteration 255/1000 | Loss: 0.00004413
Iteration 256/1000 | Loss: 0.00004412
Iteration 257/1000 | Loss: 0.00004412
Iteration 258/1000 | Loss: 0.00004412
Iteration 259/1000 | Loss: 0.00004412
Iteration 260/1000 | Loss: 0.00004411
Iteration 261/1000 | Loss: 0.00004411
Iteration 262/1000 | Loss: 0.00004411
Iteration 263/1000 | Loss: 0.00004410
Iteration 264/1000 | Loss: 0.00004410
Iteration 265/1000 | Loss: 0.00004410
Iteration 266/1000 | Loss: 0.00004410
Iteration 267/1000 | Loss: 0.00004410
Iteration 268/1000 | Loss: 0.00004409
Iteration 269/1000 | Loss: 0.00004409
Iteration 270/1000 | Loss: 0.00004409
Iteration 271/1000 | Loss: 0.00004408
Iteration 272/1000 | Loss: 0.00004408
Iteration 273/1000 | Loss: 0.00004408
Iteration 274/1000 | Loss: 0.00004407
Iteration 275/1000 | Loss: 0.00004407
Iteration 276/1000 | Loss: 0.00004407
Iteration 277/1000 | Loss: 0.00004406
Iteration 278/1000 | Loss: 0.00004406
Iteration 279/1000 | Loss: 0.00004406
Iteration 280/1000 | Loss: 0.00004405
Iteration 281/1000 | Loss: 0.00004405
Iteration 282/1000 | Loss: 0.00004405
Iteration 283/1000 | Loss: 0.00004405
Iteration 284/1000 | Loss: 0.00004404
Iteration 285/1000 | Loss: 0.00004404
Iteration 286/1000 | Loss: 0.00004404
Iteration 287/1000 | Loss: 0.00004403
Iteration 288/1000 | Loss: 0.00004403
Iteration 289/1000 | Loss: 0.00004403
Iteration 290/1000 | Loss: 0.00004403
Iteration 291/1000 | Loss: 0.00004402
Iteration 292/1000 | Loss: 0.00004402
Iteration 293/1000 | Loss: 0.00004402
Iteration 294/1000 | Loss: 0.00004402
Iteration 295/1000 | Loss: 0.00004402
Iteration 296/1000 | Loss: 0.00004402
Iteration 297/1000 | Loss: 0.00004401
Iteration 298/1000 | Loss: 0.00004401
Iteration 299/1000 | Loss: 0.00004401
Iteration 300/1000 | Loss: 0.00004401
Iteration 301/1000 | Loss: 0.00004401
Iteration 302/1000 | Loss: 0.00004401
Iteration 303/1000 | Loss: 0.00004401
Iteration 304/1000 | Loss: 0.00004401
Iteration 305/1000 | Loss: 0.00004401
Iteration 306/1000 | Loss: 0.00004401
Iteration 307/1000 | Loss: 0.00004401
Iteration 308/1000 | Loss: 0.00004400
Iteration 309/1000 | Loss: 0.00004400
Iteration 310/1000 | Loss: 0.00004400
Iteration 311/1000 | Loss: 0.00004400
Iteration 312/1000 | Loss: 0.00004399
Iteration 313/1000 | Loss: 0.00004399
Iteration 314/1000 | Loss: 0.00004399
Iteration 315/1000 | Loss: 0.00004398
Iteration 316/1000 | Loss: 0.00004398
Iteration 317/1000 | Loss: 0.00004398
Iteration 318/1000 | Loss: 0.00004398
Iteration 319/1000 | Loss: 0.00004397
Iteration 320/1000 | Loss: 0.00004397
Iteration 321/1000 | Loss: 0.00004397
Iteration 322/1000 | Loss: 0.00004397
Iteration 323/1000 | Loss: 0.00004397
Iteration 324/1000 | Loss: 0.00004396
Iteration 325/1000 | Loss: 0.00004396
Iteration 326/1000 | Loss: 0.00004396
Iteration 327/1000 | Loss: 0.00004396
Iteration 328/1000 | Loss: 0.00004396
Iteration 329/1000 | Loss: 0.00004396
Iteration 330/1000 | Loss: 0.00004395
Iteration 331/1000 | Loss: 0.00004395
Iteration 332/1000 | Loss: 0.00004395
Iteration 333/1000 | Loss: 0.00004395
Iteration 334/1000 | Loss: 0.00004395
Iteration 335/1000 | Loss: 0.00004395
Iteration 336/1000 | Loss: 0.00004395
Iteration 337/1000 | Loss: 0.00004395
Iteration 338/1000 | Loss: 0.00004395
Iteration 339/1000 | Loss: 0.00004395
Iteration 340/1000 | Loss: 0.00004395
Iteration 341/1000 | Loss: 0.00004395
Iteration 342/1000 | Loss: 0.00004395
Iteration 343/1000 | Loss: 0.00004395
Iteration 344/1000 | Loss: 0.00004394
Iteration 345/1000 | Loss: 0.00004394
Iteration 346/1000 | Loss: 0.00004394
Iteration 347/1000 | Loss: 0.00004394
Iteration 348/1000 | Loss: 0.00004394
Iteration 349/1000 | Loss: 0.00004394
Iteration 350/1000 | Loss: 0.00004394
Iteration 351/1000 | Loss: 0.00004394
Iteration 352/1000 | Loss: 0.00004394
Iteration 353/1000 | Loss: 0.00004394
Iteration 354/1000 | Loss: 0.00004394
Iteration 355/1000 | Loss: 0.00004394
Iteration 356/1000 | Loss: 0.00004394
Iteration 357/1000 | Loss: 0.00004394
Iteration 358/1000 | Loss: 0.00004394
Iteration 359/1000 | Loss: 0.00004394
Iteration 360/1000 | Loss: 0.00004394
Iteration 361/1000 | Loss: 0.00004394
Iteration 362/1000 | Loss: 0.00004394
Iteration 363/1000 | Loss: 0.00004394
Iteration 364/1000 | Loss: 0.00004394
Iteration 365/1000 | Loss: 0.00004394
Iteration 366/1000 | Loss: 0.00004394
Iteration 367/1000 | Loss: 0.00004394
Iteration 368/1000 | Loss: 0.00004394
Iteration 369/1000 | Loss: 0.00004394
Iteration 370/1000 | Loss: 0.00004394
Iteration 371/1000 | Loss: 0.00004394
Iteration 372/1000 | Loss: 0.00004394
Iteration 373/1000 | Loss: 0.00004394
Iteration 374/1000 | Loss: 0.00004394
Iteration 375/1000 | Loss: 0.00004394
Iteration 376/1000 | Loss: 0.00004394
Iteration 377/1000 | Loss: 0.00004394
Iteration 378/1000 | Loss: 0.00004394
Iteration 379/1000 | Loss: 0.00004394
Iteration 380/1000 | Loss: 0.00004394
Iteration 381/1000 | Loss: 0.00004394
Iteration 382/1000 | Loss: 0.00004394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 382. Stopping optimization.
Last 5 losses: [4.394390634843148e-05, 4.394390634843148e-05, 4.394390634843148e-05, 4.394390634843148e-05, 4.394390634843148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.394390634843148e-05

Optimization complete. Final v2v error: 4.527637958526611 mm

Highest mean error: 12.483981132507324 mm for frame 126

Lowest mean error: 3.09433913230896 mm for frame 82

Saving results

Total time: 340.9092378616333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393571
Iteration 2/25 | Loss: 0.00134093
Iteration 3/25 | Loss: 0.00128101
Iteration 4/25 | Loss: 0.00127581
Iteration 5/25 | Loss: 0.00127482
Iteration 6/25 | Loss: 0.00127482
Iteration 7/25 | Loss: 0.00127482
Iteration 8/25 | Loss: 0.00127482
Iteration 9/25 | Loss: 0.00127482
Iteration 10/25 | Loss: 0.00127482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012748213484883308, 0.0012748213484883308, 0.0012748213484883308, 0.0012748213484883308, 0.0012748213484883308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012748213484883308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29631984
Iteration 2/25 | Loss: 0.00118267
Iteration 3/25 | Loss: 0.00118266
Iteration 4/25 | Loss: 0.00118266
Iteration 5/25 | Loss: 0.00118266
Iteration 6/25 | Loss: 0.00118266
Iteration 7/25 | Loss: 0.00118266
Iteration 8/25 | Loss: 0.00118266
Iteration 9/25 | Loss: 0.00118266
Iteration 10/25 | Loss: 0.00118266
Iteration 11/25 | Loss: 0.00118266
Iteration 12/25 | Loss: 0.00118266
Iteration 13/25 | Loss: 0.00118266
Iteration 14/25 | Loss: 0.00118266
Iteration 15/25 | Loss: 0.00118266
Iteration 16/25 | Loss: 0.00118266
Iteration 17/25 | Loss: 0.00118266
Iteration 18/25 | Loss: 0.00118266
Iteration 19/25 | Loss: 0.00118266
Iteration 20/25 | Loss: 0.00118266
Iteration 21/25 | Loss: 0.00118266
Iteration 22/25 | Loss: 0.00118266
Iteration 23/25 | Loss: 0.00118266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011826573172584176, 0.0011826573172584176, 0.0011826573172584176, 0.0011826573172584176, 0.0011826573172584176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011826573172584176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118266
Iteration 2/1000 | Loss: 0.00002289
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001366
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001283
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001244
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001227
Iteration 16/1000 | Loss: 0.00001222
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001187
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001184
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001177
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001164
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001163
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001162
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001150
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001149
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001149
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001148
Iteration 126/1000 | Loss: 0.00001148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.147906732512638e-05, 1.147906732512638e-05, 1.147906732512638e-05, 1.147906732512638e-05, 1.147906732512638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.147906732512638e-05

Optimization complete. Final v2v error: 2.934535503387451 mm

Highest mean error: 2.988748788833618 mm for frame 59

Lowest mean error: 2.814131736755371 mm for frame 234

Saving results

Total time: 40.370386600494385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005497
Iteration 2/25 | Loss: 0.01005496
Iteration 3/25 | Loss: 0.01005496
Iteration 4/25 | Loss: 0.01005496
Iteration 5/25 | Loss: 0.00173082
Iteration 6/25 | Loss: 0.00137747
Iteration 7/25 | Loss: 0.00135923
Iteration 8/25 | Loss: 0.00136819
Iteration 9/25 | Loss: 0.00135424
Iteration 10/25 | Loss: 0.00134902
Iteration 11/25 | Loss: 0.00134815
Iteration 12/25 | Loss: 0.00134804
Iteration 13/25 | Loss: 0.00134802
Iteration 14/25 | Loss: 0.00134802
Iteration 15/25 | Loss: 0.00134802
Iteration 16/25 | Loss: 0.00134802
Iteration 17/25 | Loss: 0.00134801
Iteration 18/25 | Loss: 0.00134801
Iteration 19/25 | Loss: 0.00134801
Iteration 20/25 | Loss: 0.00134801
Iteration 21/25 | Loss: 0.00134801
Iteration 22/25 | Loss: 0.00135272
Iteration 23/25 | Loss: 0.00135271
Iteration 24/25 | Loss: 0.00134963
Iteration 25/25 | Loss: 0.00134907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40984726
Iteration 2/25 | Loss: 0.00191271
Iteration 3/25 | Loss: 0.00191271
Iteration 4/25 | Loss: 0.00191271
Iteration 5/25 | Loss: 0.00191270
Iteration 6/25 | Loss: 0.00191270
Iteration 7/25 | Loss: 0.00191270
Iteration 8/25 | Loss: 0.00191270
Iteration 9/25 | Loss: 0.00191270
Iteration 10/25 | Loss: 0.00191270
Iteration 11/25 | Loss: 0.00191270
Iteration 12/25 | Loss: 0.00191270
Iteration 13/25 | Loss: 0.00191270
Iteration 14/25 | Loss: 0.00191270
Iteration 15/25 | Loss: 0.00191270
Iteration 16/25 | Loss: 0.00191270
Iteration 17/25 | Loss: 0.00191270
Iteration 18/25 | Loss: 0.00191270
Iteration 19/25 | Loss: 0.00191270
Iteration 20/25 | Loss: 0.00191270
Iteration 21/25 | Loss: 0.00191270
Iteration 22/25 | Loss: 0.00191270
Iteration 23/25 | Loss: 0.00191270
Iteration 24/25 | Loss: 0.00191270
Iteration 25/25 | Loss: 0.00191270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191270
Iteration 2/1000 | Loss: 0.00004353
Iteration 3/1000 | Loss: 0.00002934
Iteration 4/1000 | Loss: 0.00002500
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002165
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001958
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001872
Iteration 12/1000 | Loss: 0.00001848
Iteration 13/1000 | Loss: 0.00001819
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001790
Iteration 18/1000 | Loss: 0.00001789
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001785
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001771
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001770
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001766
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001758
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001758
Iteration 74/1000 | Loss: 0.00001757
Iteration 75/1000 | Loss: 0.00001757
Iteration 76/1000 | Loss: 0.00001757
Iteration 77/1000 | Loss: 0.00001757
Iteration 78/1000 | Loss: 0.00001757
Iteration 79/1000 | Loss: 0.00001757
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001753
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001752
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001752
Iteration 112/1000 | Loss: 0.00001752
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001751
Iteration 115/1000 | Loss: 0.00001751
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001751
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001751
Iteration 122/1000 | Loss: 0.00001751
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001750
Iteration 125/1000 | Loss: 0.00001750
Iteration 126/1000 | Loss: 0.00001750
Iteration 127/1000 | Loss: 0.00001750
Iteration 128/1000 | Loss: 0.00001750
Iteration 129/1000 | Loss: 0.00001750
Iteration 130/1000 | Loss: 0.00001750
Iteration 131/1000 | Loss: 0.00001750
Iteration 132/1000 | Loss: 0.00001750
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001749
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001747
Iteration 147/1000 | Loss: 0.00001747
Iteration 148/1000 | Loss: 0.00001747
Iteration 149/1000 | Loss: 0.00001747
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00001746
Iteration 152/1000 | Loss: 0.00001746
Iteration 153/1000 | Loss: 0.00001746
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Iteration 159/1000 | Loss: 0.00001745
Iteration 160/1000 | Loss: 0.00001745
Iteration 161/1000 | Loss: 0.00001745
Iteration 162/1000 | Loss: 0.00001745
Iteration 163/1000 | Loss: 0.00001745
Iteration 164/1000 | Loss: 0.00001745
Iteration 165/1000 | Loss: 0.00001745
Iteration 166/1000 | Loss: 0.00001745
Iteration 167/1000 | Loss: 0.00001745
Iteration 168/1000 | Loss: 0.00001745
Iteration 169/1000 | Loss: 0.00001744
Iteration 170/1000 | Loss: 0.00001744
Iteration 171/1000 | Loss: 0.00001744
Iteration 172/1000 | Loss: 0.00001744
Iteration 173/1000 | Loss: 0.00001744
Iteration 174/1000 | Loss: 0.00001744
Iteration 175/1000 | Loss: 0.00001744
Iteration 176/1000 | Loss: 0.00001744
Iteration 177/1000 | Loss: 0.00001743
Iteration 178/1000 | Loss: 0.00001743
Iteration 179/1000 | Loss: 0.00001743
Iteration 180/1000 | Loss: 0.00001743
Iteration 181/1000 | Loss: 0.00001743
Iteration 182/1000 | Loss: 0.00001743
Iteration 183/1000 | Loss: 0.00001743
Iteration 184/1000 | Loss: 0.00001743
Iteration 185/1000 | Loss: 0.00001743
Iteration 186/1000 | Loss: 0.00001743
Iteration 187/1000 | Loss: 0.00001743
Iteration 188/1000 | Loss: 0.00001743
Iteration 189/1000 | Loss: 0.00001742
Iteration 190/1000 | Loss: 0.00001742
Iteration 191/1000 | Loss: 0.00001742
Iteration 192/1000 | Loss: 0.00001742
Iteration 193/1000 | Loss: 0.00001742
Iteration 194/1000 | Loss: 0.00001742
Iteration 195/1000 | Loss: 0.00001742
Iteration 196/1000 | Loss: 0.00001742
Iteration 197/1000 | Loss: 0.00001742
Iteration 198/1000 | Loss: 0.00001742
Iteration 199/1000 | Loss: 0.00001742
Iteration 200/1000 | Loss: 0.00001742
Iteration 201/1000 | Loss: 0.00001742
Iteration 202/1000 | Loss: 0.00001742
Iteration 203/1000 | Loss: 0.00001742
Iteration 204/1000 | Loss: 0.00001742
Iteration 205/1000 | Loss: 0.00001742
Iteration 206/1000 | Loss: 0.00001742
Iteration 207/1000 | Loss: 0.00001742
Iteration 208/1000 | Loss: 0.00001742
Iteration 209/1000 | Loss: 0.00001742
Iteration 210/1000 | Loss: 0.00001742
Iteration 211/1000 | Loss: 0.00001742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.741983214742504e-05, 1.741983214742504e-05, 1.741983214742504e-05, 1.741983214742504e-05, 1.741983214742504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.741983214742504e-05

Optimization complete. Final v2v error: 3.647479295730591 mm

Highest mean error: 3.837221384048462 mm for frame 151

Lowest mean error: 3.4287424087524414 mm for frame 91

Saving results

Total time: 67.78425168991089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500908
Iteration 2/25 | Loss: 0.00142056
Iteration 3/25 | Loss: 0.00134155
Iteration 4/25 | Loss: 0.00133580
Iteration 5/25 | Loss: 0.00133424
Iteration 6/25 | Loss: 0.00133424
Iteration 7/25 | Loss: 0.00133424
Iteration 8/25 | Loss: 0.00133424
Iteration 9/25 | Loss: 0.00133424
Iteration 10/25 | Loss: 0.00133424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001334240660071373, 0.001334240660071373, 0.001334240660071373, 0.001334240660071373, 0.001334240660071373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001334240660071373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79987031
Iteration 2/25 | Loss: 0.00098730
Iteration 3/25 | Loss: 0.00098729
Iteration 4/25 | Loss: 0.00098729
Iteration 5/25 | Loss: 0.00098729
Iteration 6/25 | Loss: 0.00098729
Iteration 7/25 | Loss: 0.00098728
Iteration 8/25 | Loss: 0.00098728
Iteration 9/25 | Loss: 0.00098728
Iteration 10/25 | Loss: 0.00098728
Iteration 11/25 | Loss: 0.00098728
Iteration 12/25 | Loss: 0.00098728
Iteration 13/25 | Loss: 0.00098728
Iteration 14/25 | Loss: 0.00098728
Iteration 15/25 | Loss: 0.00098728
Iteration 16/25 | Loss: 0.00098728
Iteration 17/25 | Loss: 0.00098728
Iteration 18/25 | Loss: 0.00098728
Iteration 19/25 | Loss: 0.00098728
Iteration 20/25 | Loss: 0.00098728
Iteration 21/25 | Loss: 0.00098728
Iteration 22/25 | Loss: 0.00098728
Iteration 23/25 | Loss: 0.00098728
Iteration 24/25 | Loss: 0.00098728
Iteration 25/25 | Loss: 0.00098728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098728
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001670
Iteration 7/1000 | Loss: 0.00001634
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001576
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001512
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001460
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001459
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001437
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001433
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001432
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00001432
Iteration 84/1000 | Loss: 0.00001432
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001432
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001432
Iteration 131/1000 | Loss: 0.00001432
Iteration 132/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.4318002286017872e-05, 1.4318002286017872e-05, 1.4318002286017872e-05, 1.4318002286017872e-05, 1.4318002286017872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4318002286017872e-05

Optimization complete. Final v2v error: 3.2066867351531982 mm

Highest mean error: 3.231677293777466 mm for frame 115

Lowest mean error: 3.1572341918945312 mm for frame 20

Saving results

Total time: 37.33052682876587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856966
Iteration 2/25 | Loss: 0.00147611
Iteration 3/25 | Loss: 0.00132916
Iteration 4/25 | Loss: 0.00131611
Iteration 5/25 | Loss: 0.00131274
Iteration 6/25 | Loss: 0.00131202
Iteration 7/25 | Loss: 0.00131202
Iteration 8/25 | Loss: 0.00131202
Iteration 9/25 | Loss: 0.00131202
Iteration 10/25 | Loss: 0.00131202
Iteration 11/25 | Loss: 0.00131202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013120246585458517, 0.0013120246585458517, 0.0013120246585458517, 0.0013120246585458517, 0.0013120246585458517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013120246585458517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38924086
Iteration 2/25 | Loss: 0.00132503
Iteration 3/25 | Loss: 0.00132503
Iteration 4/25 | Loss: 0.00132503
Iteration 5/25 | Loss: 0.00132503
Iteration 6/25 | Loss: 0.00132503
Iteration 7/25 | Loss: 0.00132503
Iteration 8/25 | Loss: 0.00132503
Iteration 9/25 | Loss: 0.00132503
Iteration 10/25 | Loss: 0.00132503
Iteration 11/25 | Loss: 0.00132503
Iteration 12/25 | Loss: 0.00132503
Iteration 13/25 | Loss: 0.00132503
Iteration 14/25 | Loss: 0.00132503
Iteration 15/25 | Loss: 0.00132503
Iteration 16/25 | Loss: 0.00132503
Iteration 17/25 | Loss: 0.00132503
Iteration 18/25 | Loss: 0.00132503
Iteration 19/25 | Loss: 0.00132503
Iteration 20/25 | Loss: 0.00132503
Iteration 21/25 | Loss: 0.00132503
Iteration 22/25 | Loss: 0.00132503
Iteration 23/25 | Loss: 0.00132503
Iteration 24/25 | Loss: 0.00132503
Iteration 25/25 | Loss: 0.00132503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132503
Iteration 2/1000 | Loss: 0.00003377
Iteration 3/1000 | Loss: 0.00002402
Iteration 4/1000 | Loss: 0.00002083
Iteration 5/1000 | Loss: 0.00001957
Iteration 6/1000 | Loss: 0.00001887
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001772
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001693
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001617
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001588
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001577
Iteration 23/1000 | Loss: 0.00001577
Iteration 24/1000 | Loss: 0.00001573
Iteration 25/1000 | Loss: 0.00001572
Iteration 26/1000 | Loss: 0.00001572
Iteration 27/1000 | Loss: 0.00001571
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001564
Iteration 32/1000 | Loss: 0.00001562
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001559
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001553
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001552
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001539
Iteration 119/1000 | Loss: 0.00001539
Iteration 120/1000 | Loss: 0.00001539
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001539
Iteration 138/1000 | Loss: 0.00001539
Iteration 139/1000 | Loss: 0.00001539
Iteration 140/1000 | Loss: 0.00001539
Iteration 141/1000 | Loss: 0.00001539
Iteration 142/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.5393994544865564e-05, 1.5393994544865564e-05, 1.5393994544865564e-05, 1.5393994544865564e-05, 1.5393994544865564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5393994544865564e-05

Optimization complete. Final v2v error: 3.278085947036743 mm

Highest mean error: 4.635946750640869 mm for frame 56

Lowest mean error: 2.9347827434539795 mm for frame 22

Saving results

Total time: 39.85253548622131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038580
Iteration 2/25 | Loss: 0.01038580
Iteration 3/25 | Loss: 0.01038579
Iteration 4/25 | Loss: 0.01038579
Iteration 5/25 | Loss: 0.01038579
Iteration 6/25 | Loss: 0.01038579
Iteration 7/25 | Loss: 0.01038579
Iteration 8/25 | Loss: 0.01038579
Iteration 9/25 | Loss: 0.01038578
Iteration 10/25 | Loss: 0.01038578
Iteration 11/25 | Loss: 0.01038578
Iteration 12/25 | Loss: 0.01038578
Iteration 13/25 | Loss: 0.01038577
Iteration 14/25 | Loss: 0.01038577
Iteration 15/25 | Loss: 0.01038577
Iteration 16/25 | Loss: 0.01038577
Iteration 17/25 | Loss: 0.01038576
Iteration 18/25 | Loss: 0.01038576
Iteration 19/25 | Loss: 0.01038576
Iteration 20/25 | Loss: 0.01038576
Iteration 21/25 | Loss: 0.01038576
Iteration 22/25 | Loss: 0.01038576
Iteration 23/25 | Loss: 0.01038575
Iteration 24/25 | Loss: 0.01038575
Iteration 25/25 | Loss: 0.01038575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90117252
Iteration 2/25 | Loss: 0.09283444
Iteration 3/25 | Loss: 0.09257117
Iteration 4/25 | Loss: 0.09251273
Iteration 5/25 | Loss: 0.09275874
Iteration 6/25 | Loss: 0.09257680
Iteration 7/25 | Loss: 0.09240001
Iteration 8/25 | Loss: 0.09230472
Iteration 9/25 | Loss: 0.09218729
Iteration 10/25 | Loss: 0.09218727
Iteration 11/25 | Loss: 0.09218727
Iteration 12/25 | Loss: 0.09218727
Iteration 13/25 | Loss: 0.09218726
Iteration 14/25 | Loss: 0.09218726
Iteration 15/25 | Loss: 0.09218726
Iteration 16/25 | Loss: 0.09218726
Iteration 17/25 | Loss: 0.09218726
Iteration 18/25 | Loss: 0.09218726
Iteration 19/25 | Loss: 0.09218726
Iteration 20/25 | Loss: 0.09218726
Iteration 21/25 | Loss: 0.09218726
Iteration 22/25 | Loss: 0.09218726
Iteration 23/25 | Loss: 0.09218726
Iteration 24/25 | Loss: 0.09218726
Iteration 25/25 | Loss: 0.09218726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09218726
Iteration 2/1000 | Loss: 0.00138722
Iteration 3/1000 | Loss: 0.00115007
Iteration 4/1000 | Loss: 0.00302241
Iteration 5/1000 | Loss: 0.00247640
Iteration 6/1000 | Loss: 0.00411088
Iteration 7/1000 | Loss: 0.00337181
Iteration 8/1000 | Loss: 0.00056386
Iteration 9/1000 | Loss: 0.00180466
Iteration 10/1000 | Loss: 0.00394546
Iteration 11/1000 | Loss: 0.00221693
Iteration 12/1000 | Loss: 0.00062002
Iteration 13/1000 | Loss: 0.00189274
Iteration 14/1000 | Loss: 0.00079915
Iteration 15/1000 | Loss: 0.00344682
Iteration 16/1000 | Loss: 0.00058727
Iteration 17/1000 | Loss: 0.00122158
Iteration 18/1000 | Loss: 0.00043912
Iteration 19/1000 | Loss: 0.00147604
Iteration 20/1000 | Loss: 0.00031049
Iteration 21/1000 | Loss: 0.00104141
Iteration 22/1000 | Loss: 0.00027678
Iteration 23/1000 | Loss: 0.00027365
Iteration 24/1000 | Loss: 0.00008200
Iteration 25/1000 | Loss: 0.00013764
Iteration 26/1000 | Loss: 0.00007927
Iteration 27/1000 | Loss: 0.00005163
Iteration 28/1000 | Loss: 0.00017494
Iteration 29/1000 | Loss: 0.00003810
Iteration 30/1000 | Loss: 0.00003325
Iteration 31/1000 | Loss: 0.00007614
Iteration 32/1000 | Loss: 0.00092044
Iteration 33/1000 | Loss: 0.00010729
Iteration 34/1000 | Loss: 0.00004332
Iteration 35/1000 | Loss: 0.00006433
Iteration 36/1000 | Loss: 0.00003099
Iteration 37/1000 | Loss: 0.00013150
Iteration 38/1000 | Loss: 0.00004082
Iteration 39/1000 | Loss: 0.00010341
Iteration 40/1000 | Loss: 0.00031136
Iteration 41/1000 | Loss: 0.00004298
Iteration 42/1000 | Loss: 0.00003503
Iteration 43/1000 | Loss: 0.00003571
Iteration 44/1000 | Loss: 0.00018466
Iteration 45/1000 | Loss: 0.00005113
Iteration 46/1000 | Loss: 0.00002446
Iteration 47/1000 | Loss: 0.00009521
Iteration 48/1000 | Loss: 0.00013915
Iteration 49/1000 | Loss: 0.00002341
Iteration 50/1000 | Loss: 0.00002602
Iteration 51/1000 | Loss: 0.00002519
Iteration 52/1000 | Loss: 0.00006724
Iteration 53/1000 | Loss: 0.00002339
Iteration 54/1000 | Loss: 0.00002392
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00003633
Iteration 57/1000 | Loss: 0.00006936
Iteration 58/1000 | Loss: 0.00002143
Iteration 59/1000 | Loss: 0.00006945
Iteration 60/1000 | Loss: 0.00014323
Iteration 61/1000 | Loss: 0.00003581
Iteration 62/1000 | Loss: 0.00002947
Iteration 63/1000 | Loss: 0.00002053
Iteration 64/1000 | Loss: 0.00002482
Iteration 65/1000 | Loss: 0.00043934
Iteration 66/1000 | Loss: 0.00043934
Iteration 67/1000 | Loss: 0.00040732
Iteration 68/1000 | Loss: 0.00057621
Iteration 69/1000 | Loss: 0.00013992
Iteration 70/1000 | Loss: 0.00003529
Iteration 71/1000 | Loss: 0.00025369
Iteration 72/1000 | Loss: 0.00022259
Iteration 73/1000 | Loss: 0.00005044
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002681
Iteration 76/1000 | Loss: 0.00002073
Iteration 77/1000 | Loss: 0.00003924
Iteration 78/1000 | Loss: 0.00002730
Iteration 79/1000 | Loss: 0.00002680
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00011230
Iteration 82/1000 | Loss: 0.00041340
Iteration 83/1000 | Loss: 0.00002328
Iteration 84/1000 | Loss: 0.00003058
Iteration 85/1000 | Loss: 0.00002757
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001982
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001981
Iteration 92/1000 | Loss: 0.00001981
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001981
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00014416
Iteration 98/1000 | Loss: 0.00026447
Iteration 99/1000 | Loss: 0.00005428
Iteration 100/1000 | Loss: 0.00006801
Iteration 101/1000 | Loss: 0.00003865
Iteration 102/1000 | Loss: 0.00002211
Iteration 103/1000 | Loss: 0.00004669
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002692
Iteration 106/1000 | Loss: 0.00002040
Iteration 107/1000 | Loss: 0.00002036
Iteration 108/1000 | Loss: 0.00002549
Iteration 109/1000 | Loss: 0.00005123
Iteration 110/1000 | Loss: 0.00005928
Iteration 111/1000 | Loss: 0.00002007
Iteration 112/1000 | Loss: 0.00001951
Iteration 113/1000 | Loss: 0.00003266
Iteration 114/1000 | Loss: 0.00002279
Iteration 115/1000 | Loss: 0.00002760
Iteration 116/1000 | Loss: 0.00011415
Iteration 117/1000 | Loss: 0.00001983
Iteration 118/1000 | Loss: 0.00001977
Iteration 119/1000 | Loss: 0.00002577
Iteration 120/1000 | Loss: 0.00002068
Iteration 121/1000 | Loss: 0.00002050
Iteration 122/1000 | Loss: 0.00001930
Iteration 123/1000 | Loss: 0.00001930
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Iteration 133/1000 | Loss: 0.00001929
Iteration 134/1000 | Loss: 0.00001929
Iteration 135/1000 | Loss: 0.00001929
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.928497113112826e-05, 1.928497113112826e-05, 1.928497113112826e-05, 1.928497113112826e-05, 1.928497113112826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.928497113112826e-05

Optimization complete. Final v2v error: 3.7593820095062256 mm

Highest mean error: 5.294065952301025 mm for frame 27

Lowest mean error: 2.9892780780792236 mm for frame 237

Saving results

Total time: 188.45225882530212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348247
Iteration 2/25 | Loss: 0.00160445
Iteration 3/25 | Loss: 0.00138962
Iteration 4/25 | Loss: 0.00131687
Iteration 5/25 | Loss: 0.00130496
Iteration 6/25 | Loss: 0.00130037
Iteration 7/25 | Loss: 0.00129984
Iteration 8/25 | Loss: 0.00129621
Iteration 9/25 | Loss: 0.00129383
Iteration 10/25 | Loss: 0.00129351
Iteration 11/25 | Loss: 0.00129229
Iteration 12/25 | Loss: 0.00129118
Iteration 13/25 | Loss: 0.00129100
Iteration 14/25 | Loss: 0.00129097
Iteration 15/25 | Loss: 0.00129097
Iteration 16/25 | Loss: 0.00129097
Iteration 17/25 | Loss: 0.00129096
Iteration 18/25 | Loss: 0.00129096
Iteration 19/25 | Loss: 0.00129096
Iteration 20/25 | Loss: 0.00129096
Iteration 21/25 | Loss: 0.00129096
Iteration 22/25 | Loss: 0.00129096
Iteration 23/25 | Loss: 0.00129096
Iteration 24/25 | Loss: 0.00129095
Iteration 25/25 | Loss: 0.00129095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30765808
Iteration 2/25 | Loss: 0.00173534
Iteration 3/25 | Loss: 0.00173534
Iteration 4/25 | Loss: 0.00173534
Iteration 5/25 | Loss: 0.00173534
Iteration 6/25 | Loss: 0.00173534
Iteration 7/25 | Loss: 0.00173534
Iteration 8/25 | Loss: 0.00173534
Iteration 9/25 | Loss: 0.00173534
Iteration 10/25 | Loss: 0.00173534
Iteration 11/25 | Loss: 0.00173534
Iteration 12/25 | Loss: 0.00173534
Iteration 13/25 | Loss: 0.00173534
Iteration 14/25 | Loss: 0.00173534
Iteration 15/25 | Loss: 0.00173534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001735340803861618, 0.001735340803861618, 0.001735340803861618, 0.001735340803861618, 0.001735340803861618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001735340803861618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173534
Iteration 2/1000 | Loss: 0.00004838
Iteration 3/1000 | Loss: 0.00003249
Iteration 4/1000 | Loss: 0.00002323
Iteration 5/1000 | Loss: 0.00002122
Iteration 6/1000 | Loss: 0.00001987
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001696
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001670
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001649
Iteration 18/1000 | Loss: 0.00001648
Iteration 19/1000 | Loss: 0.00001647
Iteration 20/1000 | Loss: 0.00001646
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001621
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001619
Iteration 34/1000 | Loss: 0.00001618
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001617
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001615
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001611
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001610
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001609
Iteration 59/1000 | Loss: 0.00001609
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001607
Iteration 65/1000 | Loss: 0.00001607
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001606
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001605
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001604
Iteration 74/1000 | Loss: 0.00001604
Iteration 75/1000 | Loss: 0.00001604
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001602
Iteration 87/1000 | Loss: 0.00001602
Iteration 88/1000 | Loss: 0.00001602
Iteration 89/1000 | Loss: 0.00001602
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001600
Iteration 95/1000 | Loss: 0.00001599
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001598
Iteration 99/1000 | Loss: 0.00001598
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001597
Iteration 102/1000 | Loss: 0.00001597
Iteration 103/1000 | Loss: 0.00001597
Iteration 104/1000 | Loss: 0.00001597
Iteration 105/1000 | Loss: 0.00001596
Iteration 106/1000 | Loss: 0.00001596
Iteration 107/1000 | Loss: 0.00001596
Iteration 108/1000 | Loss: 0.00001595
Iteration 109/1000 | Loss: 0.00001595
Iteration 110/1000 | Loss: 0.00001595
Iteration 111/1000 | Loss: 0.00001595
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001592
Iteration 131/1000 | Loss: 0.00001592
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001592
Iteration 134/1000 | Loss: 0.00001592
Iteration 135/1000 | Loss: 0.00001592
Iteration 136/1000 | Loss: 0.00001592
Iteration 137/1000 | Loss: 0.00001592
Iteration 138/1000 | Loss: 0.00001592
Iteration 139/1000 | Loss: 0.00001592
Iteration 140/1000 | Loss: 0.00001592
Iteration 141/1000 | Loss: 0.00001592
Iteration 142/1000 | Loss: 0.00001592
Iteration 143/1000 | Loss: 0.00001591
Iteration 144/1000 | Loss: 0.00001591
Iteration 145/1000 | Loss: 0.00001591
Iteration 146/1000 | Loss: 0.00001591
Iteration 147/1000 | Loss: 0.00001590
Iteration 148/1000 | Loss: 0.00001590
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001590
Iteration 151/1000 | Loss: 0.00001590
Iteration 152/1000 | Loss: 0.00001590
Iteration 153/1000 | Loss: 0.00001589
Iteration 154/1000 | Loss: 0.00001589
Iteration 155/1000 | Loss: 0.00001589
Iteration 156/1000 | Loss: 0.00001589
Iteration 157/1000 | Loss: 0.00001589
Iteration 158/1000 | Loss: 0.00001588
Iteration 159/1000 | Loss: 0.00001588
Iteration 160/1000 | Loss: 0.00001588
Iteration 161/1000 | Loss: 0.00001588
Iteration 162/1000 | Loss: 0.00001588
Iteration 163/1000 | Loss: 0.00001588
Iteration 164/1000 | Loss: 0.00001588
Iteration 165/1000 | Loss: 0.00001587
Iteration 166/1000 | Loss: 0.00001587
Iteration 167/1000 | Loss: 0.00001587
Iteration 168/1000 | Loss: 0.00001587
Iteration 169/1000 | Loss: 0.00001587
Iteration 170/1000 | Loss: 0.00001587
Iteration 171/1000 | Loss: 0.00001587
Iteration 172/1000 | Loss: 0.00001586
Iteration 173/1000 | Loss: 0.00001586
Iteration 174/1000 | Loss: 0.00001586
Iteration 175/1000 | Loss: 0.00001586
Iteration 176/1000 | Loss: 0.00001586
Iteration 177/1000 | Loss: 0.00001586
Iteration 178/1000 | Loss: 0.00001585
Iteration 179/1000 | Loss: 0.00001585
Iteration 180/1000 | Loss: 0.00001585
Iteration 181/1000 | Loss: 0.00001585
Iteration 182/1000 | Loss: 0.00001585
Iteration 183/1000 | Loss: 0.00001584
Iteration 184/1000 | Loss: 0.00001584
Iteration 185/1000 | Loss: 0.00001584
Iteration 186/1000 | Loss: 0.00001584
Iteration 187/1000 | Loss: 0.00001584
Iteration 188/1000 | Loss: 0.00001583
Iteration 189/1000 | Loss: 0.00001583
Iteration 190/1000 | Loss: 0.00001583
Iteration 191/1000 | Loss: 0.00001583
Iteration 192/1000 | Loss: 0.00001583
Iteration 193/1000 | Loss: 0.00001583
Iteration 194/1000 | Loss: 0.00001583
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001582
Iteration 198/1000 | Loss: 0.00001582
Iteration 199/1000 | Loss: 0.00001582
Iteration 200/1000 | Loss: 0.00001582
Iteration 201/1000 | Loss: 0.00001581
Iteration 202/1000 | Loss: 0.00001581
Iteration 203/1000 | Loss: 0.00001581
Iteration 204/1000 | Loss: 0.00001581
Iteration 205/1000 | Loss: 0.00001581
Iteration 206/1000 | Loss: 0.00001581
Iteration 207/1000 | Loss: 0.00001581
Iteration 208/1000 | Loss: 0.00001581
Iteration 209/1000 | Loss: 0.00001580
Iteration 210/1000 | Loss: 0.00001580
Iteration 211/1000 | Loss: 0.00001580
Iteration 212/1000 | Loss: 0.00001580
Iteration 213/1000 | Loss: 0.00001580
Iteration 214/1000 | Loss: 0.00001580
Iteration 215/1000 | Loss: 0.00001580
Iteration 216/1000 | Loss: 0.00001580
Iteration 217/1000 | Loss: 0.00001580
Iteration 218/1000 | Loss: 0.00001580
Iteration 219/1000 | Loss: 0.00001580
Iteration 220/1000 | Loss: 0.00001580
Iteration 221/1000 | Loss: 0.00001580
Iteration 222/1000 | Loss: 0.00001580
Iteration 223/1000 | Loss: 0.00001580
Iteration 224/1000 | Loss: 0.00001580
Iteration 225/1000 | Loss: 0.00001580
Iteration 226/1000 | Loss: 0.00001580
Iteration 227/1000 | Loss: 0.00001580
Iteration 228/1000 | Loss: 0.00001580
Iteration 229/1000 | Loss: 0.00001580
Iteration 230/1000 | Loss: 0.00001580
Iteration 231/1000 | Loss: 0.00001580
Iteration 232/1000 | Loss: 0.00001580
Iteration 233/1000 | Loss: 0.00001580
Iteration 234/1000 | Loss: 0.00001580
Iteration 235/1000 | Loss: 0.00001580
Iteration 236/1000 | Loss: 0.00001580
Iteration 237/1000 | Loss: 0.00001580
Iteration 238/1000 | Loss: 0.00001580
Iteration 239/1000 | Loss: 0.00001580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.5798672393430024e-05, 1.5798672393430024e-05, 1.5798672393430024e-05, 1.5798672393430024e-05, 1.5798672393430024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5798672393430024e-05

Optimization complete. Final v2v error: 3.3903799057006836 mm

Highest mean error: 4.152290344238281 mm for frame 70

Lowest mean error: 2.81820011138916 mm for frame 0

Saving results

Total time: 59.72059965133667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738433
Iteration 2/25 | Loss: 0.00149477
Iteration 3/25 | Loss: 0.00140484
Iteration 4/25 | Loss: 0.00139714
Iteration 5/25 | Loss: 0.00139489
Iteration 6/25 | Loss: 0.00139487
Iteration 7/25 | Loss: 0.00139487
Iteration 8/25 | Loss: 0.00139487
Iteration 9/25 | Loss: 0.00139487
Iteration 10/25 | Loss: 0.00139487
Iteration 11/25 | Loss: 0.00139487
Iteration 12/25 | Loss: 0.00139487
Iteration 13/25 | Loss: 0.00139487
Iteration 14/25 | Loss: 0.00139487
Iteration 15/25 | Loss: 0.00139487
Iteration 16/25 | Loss: 0.00139487
Iteration 17/25 | Loss: 0.00139487
Iteration 18/25 | Loss: 0.00139487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001394871505908668, 0.001394871505908668, 0.001394871505908668, 0.001394871505908668, 0.001394871505908668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001394871505908668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36973238
Iteration 2/25 | Loss: 0.00134846
Iteration 3/25 | Loss: 0.00134845
Iteration 4/25 | Loss: 0.00134845
Iteration 5/25 | Loss: 0.00134845
Iteration 6/25 | Loss: 0.00134845
Iteration 7/25 | Loss: 0.00134845
Iteration 8/25 | Loss: 0.00134845
Iteration 9/25 | Loss: 0.00134845
Iteration 10/25 | Loss: 0.00134845
Iteration 11/25 | Loss: 0.00134845
Iteration 12/25 | Loss: 0.00134845
Iteration 13/25 | Loss: 0.00134845
Iteration 14/25 | Loss: 0.00134845
Iteration 15/25 | Loss: 0.00134845
Iteration 16/25 | Loss: 0.00134845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013484510127454996, 0.0013484510127454996, 0.0013484510127454996, 0.0013484510127454996, 0.0013484510127454996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013484510127454996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134845
Iteration 2/1000 | Loss: 0.00004071
Iteration 3/1000 | Loss: 0.00002654
Iteration 4/1000 | Loss: 0.00002363
Iteration 5/1000 | Loss: 0.00002204
Iteration 6/1000 | Loss: 0.00002122
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001918
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001888
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001887
Iteration 21/1000 | Loss: 0.00001886
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001881
Iteration 29/1000 | Loss: 0.00001881
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001869
Iteration 37/1000 | Loss: 0.00001869
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001868
Iteration 40/1000 | Loss: 0.00001868
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001857
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001855
Iteration 55/1000 | Loss: 0.00001855
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001851
Iteration 58/1000 | Loss: 0.00001850
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001847
Iteration 62/1000 | Loss: 0.00001846
Iteration 63/1000 | Loss: 0.00001846
Iteration 64/1000 | Loss: 0.00001845
Iteration 65/1000 | Loss: 0.00001845
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001843
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001834
Iteration 114/1000 | Loss: 0.00001834
Iteration 115/1000 | Loss: 0.00001833
Iteration 116/1000 | Loss: 0.00001833
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001832
Iteration 119/1000 | Loss: 0.00001832
Iteration 120/1000 | Loss: 0.00001832
Iteration 121/1000 | Loss: 0.00001832
Iteration 122/1000 | Loss: 0.00001832
Iteration 123/1000 | Loss: 0.00001831
Iteration 124/1000 | Loss: 0.00001831
Iteration 125/1000 | Loss: 0.00001831
Iteration 126/1000 | Loss: 0.00001831
Iteration 127/1000 | Loss: 0.00001831
Iteration 128/1000 | Loss: 0.00001831
Iteration 129/1000 | Loss: 0.00001831
Iteration 130/1000 | Loss: 0.00001831
Iteration 131/1000 | Loss: 0.00001831
Iteration 132/1000 | Loss: 0.00001830
Iteration 133/1000 | Loss: 0.00001830
Iteration 134/1000 | Loss: 0.00001829
Iteration 135/1000 | Loss: 0.00001829
Iteration 136/1000 | Loss: 0.00001829
Iteration 137/1000 | Loss: 0.00001829
Iteration 138/1000 | Loss: 0.00001829
Iteration 139/1000 | Loss: 0.00001828
Iteration 140/1000 | Loss: 0.00001828
Iteration 141/1000 | Loss: 0.00001828
Iteration 142/1000 | Loss: 0.00001827
Iteration 143/1000 | Loss: 0.00001827
Iteration 144/1000 | Loss: 0.00001827
Iteration 145/1000 | Loss: 0.00001827
Iteration 146/1000 | Loss: 0.00001827
Iteration 147/1000 | Loss: 0.00001827
Iteration 148/1000 | Loss: 0.00001826
Iteration 149/1000 | Loss: 0.00001826
Iteration 150/1000 | Loss: 0.00001825
Iteration 151/1000 | Loss: 0.00001825
Iteration 152/1000 | Loss: 0.00001825
Iteration 153/1000 | Loss: 0.00001824
Iteration 154/1000 | Loss: 0.00001824
Iteration 155/1000 | Loss: 0.00001824
Iteration 156/1000 | Loss: 0.00001824
Iteration 157/1000 | Loss: 0.00001824
Iteration 158/1000 | Loss: 0.00001824
Iteration 159/1000 | Loss: 0.00001824
Iteration 160/1000 | Loss: 0.00001823
Iteration 161/1000 | Loss: 0.00001823
Iteration 162/1000 | Loss: 0.00001823
Iteration 163/1000 | Loss: 0.00001823
Iteration 164/1000 | Loss: 0.00001823
Iteration 165/1000 | Loss: 0.00001822
Iteration 166/1000 | Loss: 0.00001822
Iteration 167/1000 | Loss: 0.00001822
Iteration 168/1000 | Loss: 0.00001822
Iteration 169/1000 | Loss: 0.00001822
Iteration 170/1000 | Loss: 0.00001822
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001821
Iteration 173/1000 | Loss: 0.00001821
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Iteration 176/1000 | Loss: 0.00001820
Iteration 177/1000 | Loss: 0.00001820
Iteration 178/1000 | Loss: 0.00001820
Iteration 179/1000 | Loss: 0.00001820
Iteration 180/1000 | Loss: 0.00001820
Iteration 181/1000 | Loss: 0.00001819
Iteration 182/1000 | Loss: 0.00001819
Iteration 183/1000 | Loss: 0.00001819
Iteration 184/1000 | Loss: 0.00001819
Iteration 185/1000 | Loss: 0.00001819
Iteration 186/1000 | Loss: 0.00001819
Iteration 187/1000 | Loss: 0.00001819
Iteration 188/1000 | Loss: 0.00001819
Iteration 189/1000 | Loss: 0.00001819
Iteration 190/1000 | Loss: 0.00001819
Iteration 191/1000 | Loss: 0.00001819
Iteration 192/1000 | Loss: 0.00001819
Iteration 193/1000 | Loss: 0.00001818
Iteration 194/1000 | Loss: 0.00001818
Iteration 195/1000 | Loss: 0.00001818
Iteration 196/1000 | Loss: 0.00001817
Iteration 197/1000 | Loss: 0.00001817
Iteration 198/1000 | Loss: 0.00001817
Iteration 199/1000 | Loss: 0.00001817
Iteration 200/1000 | Loss: 0.00001817
Iteration 201/1000 | Loss: 0.00001817
Iteration 202/1000 | Loss: 0.00001817
Iteration 203/1000 | Loss: 0.00001816
Iteration 204/1000 | Loss: 0.00001816
Iteration 205/1000 | Loss: 0.00001816
Iteration 206/1000 | Loss: 0.00001816
Iteration 207/1000 | Loss: 0.00001816
Iteration 208/1000 | Loss: 0.00001816
Iteration 209/1000 | Loss: 0.00001816
Iteration 210/1000 | Loss: 0.00001816
Iteration 211/1000 | Loss: 0.00001816
Iteration 212/1000 | Loss: 0.00001816
Iteration 213/1000 | Loss: 0.00001816
Iteration 214/1000 | Loss: 0.00001816
Iteration 215/1000 | Loss: 0.00001816
Iteration 216/1000 | Loss: 0.00001816
Iteration 217/1000 | Loss: 0.00001816
Iteration 218/1000 | Loss: 0.00001816
Iteration 219/1000 | Loss: 0.00001816
Iteration 220/1000 | Loss: 0.00001816
Iteration 221/1000 | Loss: 0.00001815
Iteration 222/1000 | Loss: 0.00001815
Iteration 223/1000 | Loss: 0.00001815
Iteration 224/1000 | Loss: 0.00001815
Iteration 225/1000 | Loss: 0.00001815
Iteration 226/1000 | Loss: 0.00001815
Iteration 227/1000 | Loss: 0.00001815
Iteration 228/1000 | Loss: 0.00001815
Iteration 229/1000 | Loss: 0.00001814
Iteration 230/1000 | Loss: 0.00001814
Iteration 231/1000 | Loss: 0.00001814
Iteration 232/1000 | Loss: 0.00001814
Iteration 233/1000 | Loss: 0.00001814
Iteration 234/1000 | Loss: 0.00001814
Iteration 235/1000 | Loss: 0.00001814
Iteration 236/1000 | Loss: 0.00001814
Iteration 237/1000 | Loss: 0.00001814
Iteration 238/1000 | Loss: 0.00001814
Iteration 239/1000 | Loss: 0.00001814
Iteration 240/1000 | Loss: 0.00001814
Iteration 241/1000 | Loss: 0.00001814
Iteration 242/1000 | Loss: 0.00001814
Iteration 243/1000 | Loss: 0.00001814
Iteration 244/1000 | Loss: 0.00001814
Iteration 245/1000 | Loss: 0.00001814
Iteration 246/1000 | Loss: 0.00001814
Iteration 247/1000 | Loss: 0.00001814
Iteration 248/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.813616108847782e-05, 1.813616108847782e-05, 1.813616108847782e-05, 1.813616108847782e-05, 1.813616108847782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.813616108847782e-05

Optimization complete. Final v2v error: 3.5840673446655273 mm

Highest mean error: 3.825718402862549 mm for frame 138

Lowest mean error: 3.319082736968994 mm for frame 160

Saving results

Total time: 46.991814613342285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870563
Iteration 2/25 | Loss: 0.00142521
Iteration 3/25 | Loss: 0.00134245
Iteration 4/25 | Loss: 0.00132964
Iteration 5/25 | Loss: 0.00132549
Iteration 6/25 | Loss: 0.00132549
Iteration 7/25 | Loss: 0.00132549
Iteration 8/25 | Loss: 0.00132549
Iteration 9/25 | Loss: 0.00132549
Iteration 10/25 | Loss: 0.00132549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013254870427772403, 0.0013254870427772403, 0.0013254870427772403, 0.0013254870427772403, 0.0013254870427772403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013254870427772403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31243098
Iteration 2/25 | Loss: 0.00146835
Iteration 3/25 | Loss: 0.00146835
Iteration 4/25 | Loss: 0.00146835
Iteration 5/25 | Loss: 0.00146834
Iteration 6/25 | Loss: 0.00146834
Iteration 7/25 | Loss: 0.00146834
Iteration 8/25 | Loss: 0.00146834
Iteration 9/25 | Loss: 0.00146834
Iteration 10/25 | Loss: 0.00146834
Iteration 11/25 | Loss: 0.00146834
Iteration 12/25 | Loss: 0.00146834
Iteration 13/25 | Loss: 0.00146834
Iteration 14/25 | Loss: 0.00146834
Iteration 15/25 | Loss: 0.00146834
Iteration 16/25 | Loss: 0.00146834
Iteration 17/25 | Loss: 0.00146834
Iteration 18/25 | Loss: 0.00146834
Iteration 19/25 | Loss: 0.00146834
Iteration 20/25 | Loss: 0.00146834
Iteration 21/25 | Loss: 0.00146834
Iteration 22/25 | Loss: 0.00146834
Iteration 23/25 | Loss: 0.00146834
Iteration 24/25 | Loss: 0.00146834
Iteration 25/25 | Loss: 0.00146834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146834
Iteration 2/1000 | Loss: 0.00003977
Iteration 3/1000 | Loss: 0.00002379
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00001840
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001689
Iteration 8/1000 | Loss: 0.00001646
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001535
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001525
Iteration 15/1000 | Loss: 0.00001524
Iteration 16/1000 | Loss: 0.00001517
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001491
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001490
Iteration 36/1000 | Loss: 0.00001490
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001479
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001477
Iteration 42/1000 | Loss: 0.00001471
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001466
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001461
Iteration 55/1000 | Loss: 0.00001460
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001445
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001444
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001443
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001441
Iteration 91/1000 | Loss: 0.00001441
Iteration 92/1000 | Loss: 0.00001440
Iteration 93/1000 | Loss: 0.00001440
Iteration 94/1000 | Loss: 0.00001440
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001437
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001430
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001428
Iteration 151/1000 | Loss: 0.00001428
Iteration 152/1000 | Loss: 0.00001428
Iteration 153/1000 | Loss: 0.00001428
Iteration 154/1000 | Loss: 0.00001428
Iteration 155/1000 | Loss: 0.00001428
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001426
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001425
Iteration 176/1000 | Loss: 0.00001425
Iteration 177/1000 | Loss: 0.00001425
Iteration 178/1000 | Loss: 0.00001425
Iteration 179/1000 | Loss: 0.00001425
Iteration 180/1000 | Loss: 0.00001425
Iteration 181/1000 | Loss: 0.00001425
Iteration 182/1000 | Loss: 0.00001425
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001425
Iteration 189/1000 | Loss: 0.00001425
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001424
Iteration 194/1000 | Loss: 0.00001424
Iteration 195/1000 | Loss: 0.00001424
Iteration 196/1000 | Loss: 0.00001424
Iteration 197/1000 | Loss: 0.00001423
Iteration 198/1000 | Loss: 0.00001423
Iteration 199/1000 | Loss: 0.00001423
Iteration 200/1000 | Loss: 0.00001423
Iteration 201/1000 | Loss: 0.00001423
Iteration 202/1000 | Loss: 0.00001423
Iteration 203/1000 | Loss: 0.00001423
Iteration 204/1000 | Loss: 0.00001423
Iteration 205/1000 | Loss: 0.00001423
Iteration 206/1000 | Loss: 0.00001423
Iteration 207/1000 | Loss: 0.00001423
Iteration 208/1000 | Loss: 0.00001423
Iteration 209/1000 | Loss: 0.00001423
Iteration 210/1000 | Loss: 0.00001423
Iteration 211/1000 | Loss: 0.00001423
Iteration 212/1000 | Loss: 0.00001423
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Iteration 216/1000 | Loss: 0.00001423
Iteration 217/1000 | Loss: 0.00001423
Iteration 218/1000 | Loss: 0.00001423
Iteration 219/1000 | Loss: 0.00001423
Iteration 220/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.4227214705897495e-05, 1.4227214705897495e-05, 1.4227214705897495e-05, 1.4227214705897495e-05, 1.4227214705897495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4227214705897495e-05

Optimization complete. Final v2v error: 3.201427936553955 mm

Highest mean error: 3.759458303451538 mm for frame 37

Lowest mean error: 2.9489457607269287 mm for frame 29

Saving results

Total time: 54.126389503479004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053614
Iteration 2/25 | Loss: 0.00206647
Iteration 3/25 | Loss: 0.00159071
Iteration 4/25 | Loss: 0.00153363
Iteration 5/25 | Loss: 0.00152239
Iteration 6/25 | Loss: 0.00151944
Iteration 7/25 | Loss: 0.00151912
Iteration 8/25 | Loss: 0.00151912
Iteration 9/25 | Loss: 0.00151912
Iteration 10/25 | Loss: 0.00151912
Iteration 11/25 | Loss: 0.00151912
Iteration 12/25 | Loss: 0.00151912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00151912122964859, 0.00151912122964859, 0.00151912122964859, 0.00151912122964859, 0.00151912122964859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00151912122964859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77312016
Iteration 2/25 | Loss: 0.00163335
Iteration 3/25 | Loss: 0.00163330
Iteration 4/25 | Loss: 0.00163330
Iteration 5/25 | Loss: 0.00163329
Iteration 6/25 | Loss: 0.00163329
Iteration 7/25 | Loss: 0.00163329
Iteration 8/25 | Loss: 0.00163329
Iteration 9/25 | Loss: 0.00163329
Iteration 10/25 | Loss: 0.00163329
Iteration 11/25 | Loss: 0.00163329
Iteration 12/25 | Loss: 0.00163329
Iteration 13/25 | Loss: 0.00163329
Iteration 14/25 | Loss: 0.00163329
Iteration 15/25 | Loss: 0.00163329
Iteration 16/25 | Loss: 0.00163329
Iteration 17/25 | Loss: 0.00163329
Iteration 18/25 | Loss: 0.00163329
Iteration 19/25 | Loss: 0.00163329
Iteration 20/25 | Loss: 0.00163329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016332932282239199, 0.0016332932282239199, 0.0016332932282239199, 0.0016332932282239199, 0.0016332932282239199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016332932282239199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163329
Iteration 2/1000 | Loss: 0.00008167
Iteration 3/1000 | Loss: 0.00005446
Iteration 4/1000 | Loss: 0.00004560
Iteration 5/1000 | Loss: 0.00004358
Iteration 6/1000 | Loss: 0.00004187
Iteration 7/1000 | Loss: 0.00004097
Iteration 8/1000 | Loss: 0.00004031
Iteration 9/1000 | Loss: 0.00003971
Iteration 10/1000 | Loss: 0.00003917
Iteration 11/1000 | Loss: 0.00003874
Iteration 12/1000 | Loss: 0.00003845
Iteration 13/1000 | Loss: 0.00003825
Iteration 14/1000 | Loss: 0.00003807
Iteration 15/1000 | Loss: 0.00003805
Iteration 16/1000 | Loss: 0.00003795
Iteration 17/1000 | Loss: 0.00003782
Iteration 18/1000 | Loss: 0.00003780
Iteration 19/1000 | Loss: 0.00003779
Iteration 20/1000 | Loss: 0.00003777
Iteration 21/1000 | Loss: 0.00003777
Iteration 22/1000 | Loss: 0.00003776
Iteration 23/1000 | Loss: 0.00003776
Iteration 24/1000 | Loss: 0.00003776
Iteration 25/1000 | Loss: 0.00003773
Iteration 26/1000 | Loss: 0.00003772
Iteration 27/1000 | Loss: 0.00003772
Iteration 28/1000 | Loss: 0.00003772
Iteration 29/1000 | Loss: 0.00003772
Iteration 30/1000 | Loss: 0.00003772
Iteration 31/1000 | Loss: 0.00003771
Iteration 32/1000 | Loss: 0.00003771
Iteration 33/1000 | Loss: 0.00003771
Iteration 34/1000 | Loss: 0.00003771
Iteration 35/1000 | Loss: 0.00003771
Iteration 36/1000 | Loss: 0.00003768
Iteration 37/1000 | Loss: 0.00003768
Iteration 38/1000 | Loss: 0.00003768
Iteration 39/1000 | Loss: 0.00003765
Iteration 40/1000 | Loss: 0.00003763
Iteration 41/1000 | Loss: 0.00003763
Iteration 42/1000 | Loss: 0.00003762
Iteration 43/1000 | Loss: 0.00003762
Iteration 44/1000 | Loss: 0.00003761
Iteration 45/1000 | Loss: 0.00003758
Iteration 46/1000 | Loss: 0.00003757
Iteration 47/1000 | Loss: 0.00003753
Iteration 48/1000 | Loss: 0.00003753
Iteration 49/1000 | Loss: 0.00003753
Iteration 50/1000 | Loss: 0.00003753
Iteration 51/1000 | Loss: 0.00003753
Iteration 52/1000 | Loss: 0.00003752
Iteration 53/1000 | Loss: 0.00003752
Iteration 54/1000 | Loss: 0.00003751
Iteration 55/1000 | Loss: 0.00003751
Iteration 56/1000 | Loss: 0.00003751
Iteration 57/1000 | Loss: 0.00003751
Iteration 58/1000 | Loss: 0.00003751
Iteration 59/1000 | Loss: 0.00003750
Iteration 60/1000 | Loss: 0.00003749
Iteration 61/1000 | Loss: 0.00003749
Iteration 62/1000 | Loss: 0.00003748
Iteration 63/1000 | Loss: 0.00003748
Iteration 64/1000 | Loss: 0.00003748
Iteration 65/1000 | Loss: 0.00003748
Iteration 66/1000 | Loss: 0.00003748
Iteration 67/1000 | Loss: 0.00003748
Iteration 68/1000 | Loss: 0.00003748
Iteration 69/1000 | Loss: 0.00003748
Iteration 70/1000 | Loss: 0.00003748
Iteration 71/1000 | Loss: 0.00003747
Iteration 72/1000 | Loss: 0.00003747
Iteration 73/1000 | Loss: 0.00003747
Iteration 74/1000 | Loss: 0.00003747
Iteration 75/1000 | Loss: 0.00003747
Iteration 76/1000 | Loss: 0.00003746
Iteration 77/1000 | Loss: 0.00003746
Iteration 78/1000 | Loss: 0.00003746
Iteration 79/1000 | Loss: 0.00003746
Iteration 80/1000 | Loss: 0.00003746
Iteration 81/1000 | Loss: 0.00003746
Iteration 82/1000 | Loss: 0.00003745
Iteration 83/1000 | Loss: 0.00003745
Iteration 84/1000 | Loss: 0.00003745
Iteration 85/1000 | Loss: 0.00003745
Iteration 86/1000 | Loss: 0.00003745
Iteration 87/1000 | Loss: 0.00003744
Iteration 88/1000 | Loss: 0.00003744
Iteration 89/1000 | Loss: 0.00003744
Iteration 90/1000 | Loss: 0.00003744
Iteration 91/1000 | Loss: 0.00003744
Iteration 92/1000 | Loss: 0.00003744
Iteration 93/1000 | Loss: 0.00003744
Iteration 94/1000 | Loss: 0.00003744
Iteration 95/1000 | Loss: 0.00003744
Iteration 96/1000 | Loss: 0.00003743
Iteration 97/1000 | Loss: 0.00003743
Iteration 98/1000 | Loss: 0.00003743
Iteration 99/1000 | Loss: 0.00003743
Iteration 100/1000 | Loss: 0.00003743
Iteration 101/1000 | Loss: 0.00003743
Iteration 102/1000 | Loss: 0.00003742
Iteration 103/1000 | Loss: 0.00003742
Iteration 104/1000 | Loss: 0.00003742
Iteration 105/1000 | Loss: 0.00003742
Iteration 106/1000 | Loss: 0.00003742
Iteration 107/1000 | Loss: 0.00003742
Iteration 108/1000 | Loss: 0.00003741
Iteration 109/1000 | Loss: 0.00003741
Iteration 110/1000 | Loss: 0.00003741
Iteration 111/1000 | Loss: 0.00003741
Iteration 112/1000 | Loss: 0.00003741
Iteration 113/1000 | Loss: 0.00003741
Iteration 114/1000 | Loss: 0.00003741
Iteration 115/1000 | Loss: 0.00003740
Iteration 116/1000 | Loss: 0.00003740
Iteration 117/1000 | Loss: 0.00003740
Iteration 118/1000 | Loss: 0.00003740
Iteration 119/1000 | Loss: 0.00003740
Iteration 120/1000 | Loss: 0.00003740
Iteration 121/1000 | Loss: 0.00003740
Iteration 122/1000 | Loss: 0.00003740
Iteration 123/1000 | Loss: 0.00003739
Iteration 124/1000 | Loss: 0.00003739
Iteration 125/1000 | Loss: 0.00003739
Iteration 126/1000 | Loss: 0.00003739
Iteration 127/1000 | Loss: 0.00003739
Iteration 128/1000 | Loss: 0.00003739
Iteration 129/1000 | Loss: 0.00003739
Iteration 130/1000 | Loss: 0.00003739
Iteration 131/1000 | Loss: 0.00003739
Iteration 132/1000 | Loss: 0.00003739
Iteration 133/1000 | Loss: 0.00003739
Iteration 134/1000 | Loss: 0.00003739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.739176463568583e-05, 3.739176463568583e-05, 3.739176463568583e-05, 3.739176463568583e-05, 3.739176463568583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.739176463568583e-05

Optimization complete. Final v2v error: 5.077511787414551 mm

Highest mean error: 6.179033279418945 mm for frame 211

Lowest mean error: 4.0824055671691895 mm for frame 70

Saving results

Total time: 49.64669990539551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603155
Iteration 2/25 | Loss: 0.00164668
Iteration 3/25 | Loss: 0.00141239
Iteration 4/25 | Loss: 0.00138519
Iteration 5/25 | Loss: 0.00136991
Iteration 6/25 | Loss: 0.00136735
Iteration 7/25 | Loss: 0.00136897
Iteration 8/25 | Loss: 0.00136239
Iteration 9/25 | Loss: 0.00135857
Iteration 10/25 | Loss: 0.00135782
Iteration 11/25 | Loss: 0.00135755
Iteration 12/25 | Loss: 0.00135751
Iteration 13/25 | Loss: 0.00135751
Iteration 14/25 | Loss: 0.00135751
Iteration 15/25 | Loss: 0.00135751
Iteration 16/25 | Loss: 0.00135750
Iteration 17/25 | Loss: 0.00135750
Iteration 18/25 | Loss: 0.00135750
Iteration 19/25 | Loss: 0.00135750
Iteration 20/25 | Loss: 0.00135750
Iteration 21/25 | Loss: 0.00135750
Iteration 22/25 | Loss: 0.00135750
Iteration 23/25 | Loss: 0.00135750
Iteration 24/25 | Loss: 0.00135750
Iteration 25/25 | Loss: 0.00135750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32433152
Iteration 2/25 | Loss: 0.00160400
Iteration 3/25 | Loss: 0.00160396
Iteration 4/25 | Loss: 0.00160395
Iteration 5/25 | Loss: 0.00160395
Iteration 6/25 | Loss: 0.00160395
Iteration 7/25 | Loss: 0.00160395
Iteration 8/25 | Loss: 0.00160395
Iteration 9/25 | Loss: 0.00160395
Iteration 10/25 | Loss: 0.00160395
Iteration 11/25 | Loss: 0.00160395
Iteration 12/25 | Loss: 0.00160395
Iteration 13/25 | Loss: 0.00160395
Iteration 14/25 | Loss: 0.00160395
Iteration 15/25 | Loss: 0.00160395
Iteration 16/25 | Loss: 0.00160395
Iteration 17/25 | Loss: 0.00160395
Iteration 18/25 | Loss: 0.00160395
Iteration 19/25 | Loss: 0.00160395
Iteration 20/25 | Loss: 0.00160395
Iteration 21/25 | Loss: 0.00160395
Iteration 22/25 | Loss: 0.00160395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00160395132843405, 0.00160395132843405, 0.00160395132843405, 0.00160395132843405, 0.00160395132843405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00160395132843405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160395
Iteration 2/1000 | Loss: 0.00004962
Iteration 3/1000 | Loss: 0.00003147
Iteration 4/1000 | Loss: 0.00002732
Iteration 5/1000 | Loss: 0.00002550
Iteration 6/1000 | Loss: 0.00002405
Iteration 7/1000 | Loss: 0.00002308
Iteration 8/1000 | Loss: 0.00002235
Iteration 9/1000 | Loss: 0.00002190
Iteration 10/1000 | Loss: 0.00002145
Iteration 11/1000 | Loss: 0.00002102
Iteration 12/1000 | Loss: 0.00002071
Iteration 13/1000 | Loss: 0.00002048
Iteration 14/1000 | Loss: 0.00002030
Iteration 15/1000 | Loss: 0.00002027
Iteration 16/1000 | Loss: 0.00002027
Iteration 17/1000 | Loss: 0.00002021
Iteration 18/1000 | Loss: 0.00002013
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002004
Iteration 22/1000 | Loss: 0.00002003
Iteration 23/1000 | Loss: 0.00001999
Iteration 24/1000 | Loss: 0.00001993
Iteration 25/1000 | Loss: 0.00001990
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001988
Iteration 29/1000 | Loss: 0.00001987
Iteration 30/1000 | Loss: 0.00001987
Iteration 31/1000 | Loss: 0.00001986
Iteration 32/1000 | Loss: 0.00001986
Iteration 33/1000 | Loss: 0.00001985
Iteration 34/1000 | Loss: 0.00001985
Iteration 35/1000 | Loss: 0.00001984
Iteration 36/1000 | Loss: 0.00001984
Iteration 37/1000 | Loss: 0.00001984
Iteration 38/1000 | Loss: 0.00001983
Iteration 39/1000 | Loss: 0.00001983
Iteration 40/1000 | Loss: 0.00001982
Iteration 41/1000 | Loss: 0.00001982
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001981
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001980
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00001979
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001977
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001976
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001973
Iteration 67/1000 | Loss: 0.00001973
Iteration 68/1000 | Loss: 0.00001973
Iteration 69/1000 | Loss: 0.00001973
Iteration 70/1000 | Loss: 0.00001973
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001971
Iteration 73/1000 | Loss: 0.00001971
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00001970
Iteration 76/1000 | Loss: 0.00001970
Iteration 77/1000 | Loss: 0.00001970
Iteration 78/1000 | Loss: 0.00001969
Iteration 79/1000 | Loss: 0.00001969
Iteration 80/1000 | Loss: 0.00001969
Iteration 81/1000 | Loss: 0.00001968
Iteration 82/1000 | Loss: 0.00001968
Iteration 83/1000 | Loss: 0.00001968
Iteration 84/1000 | Loss: 0.00001968
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001967
Iteration 88/1000 | Loss: 0.00001967
Iteration 89/1000 | Loss: 0.00001967
Iteration 90/1000 | Loss: 0.00001967
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001966
Iteration 93/1000 | Loss: 0.00001966
Iteration 94/1000 | Loss: 0.00001966
Iteration 95/1000 | Loss: 0.00001966
Iteration 96/1000 | Loss: 0.00001966
Iteration 97/1000 | Loss: 0.00001966
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001965
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00001965
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001963
Iteration 106/1000 | Loss: 0.00001963
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001962
Iteration 113/1000 | Loss: 0.00001962
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001958
Iteration 124/1000 | Loss: 0.00001958
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001957
Iteration 127/1000 | Loss: 0.00001957
Iteration 128/1000 | Loss: 0.00001957
Iteration 129/1000 | Loss: 0.00001956
Iteration 130/1000 | Loss: 0.00001956
Iteration 131/1000 | Loss: 0.00001955
Iteration 132/1000 | Loss: 0.00001955
Iteration 133/1000 | Loss: 0.00001954
Iteration 134/1000 | Loss: 0.00001954
Iteration 135/1000 | Loss: 0.00001954
Iteration 136/1000 | Loss: 0.00001953
Iteration 137/1000 | Loss: 0.00001953
Iteration 138/1000 | Loss: 0.00001953
Iteration 139/1000 | Loss: 0.00001953
Iteration 140/1000 | Loss: 0.00001952
Iteration 141/1000 | Loss: 0.00001952
Iteration 142/1000 | Loss: 0.00001952
Iteration 143/1000 | Loss: 0.00001952
Iteration 144/1000 | Loss: 0.00001952
Iteration 145/1000 | Loss: 0.00001951
Iteration 146/1000 | Loss: 0.00001951
Iteration 147/1000 | Loss: 0.00001951
Iteration 148/1000 | Loss: 0.00001951
Iteration 149/1000 | Loss: 0.00001950
Iteration 150/1000 | Loss: 0.00001950
Iteration 151/1000 | Loss: 0.00001950
Iteration 152/1000 | Loss: 0.00001949
Iteration 153/1000 | Loss: 0.00001949
Iteration 154/1000 | Loss: 0.00001949
Iteration 155/1000 | Loss: 0.00001949
Iteration 156/1000 | Loss: 0.00001949
Iteration 157/1000 | Loss: 0.00001949
Iteration 158/1000 | Loss: 0.00001948
Iteration 159/1000 | Loss: 0.00001948
Iteration 160/1000 | Loss: 0.00001948
Iteration 161/1000 | Loss: 0.00001948
Iteration 162/1000 | Loss: 0.00001948
Iteration 163/1000 | Loss: 0.00001948
Iteration 164/1000 | Loss: 0.00001948
Iteration 165/1000 | Loss: 0.00001947
Iteration 166/1000 | Loss: 0.00001947
Iteration 167/1000 | Loss: 0.00001947
Iteration 168/1000 | Loss: 0.00001947
Iteration 169/1000 | Loss: 0.00001947
Iteration 170/1000 | Loss: 0.00001947
Iteration 171/1000 | Loss: 0.00001947
Iteration 172/1000 | Loss: 0.00001947
Iteration 173/1000 | Loss: 0.00001947
Iteration 174/1000 | Loss: 0.00001947
Iteration 175/1000 | Loss: 0.00001947
Iteration 176/1000 | Loss: 0.00001947
Iteration 177/1000 | Loss: 0.00001946
Iteration 178/1000 | Loss: 0.00001946
Iteration 179/1000 | Loss: 0.00001946
Iteration 180/1000 | Loss: 0.00001946
Iteration 181/1000 | Loss: 0.00001946
Iteration 182/1000 | Loss: 0.00001946
Iteration 183/1000 | Loss: 0.00001946
Iteration 184/1000 | Loss: 0.00001946
Iteration 185/1000 | Loss: 0.00001946
Iteration 186/1000 | Loss: 0.00001945
Iteration 187/1000 | Loss: 0.00001945
Iteration 188/1000 | Loss: 0.00001945
Iteration 189/1000 | Loss: 0.00001945
Iteration 190/1000 | Loss: 0.00001945
Iteration 191/1000 | Loss: 0.00001945
Iteration 192/1000 | Loss: 0.00001945
Iteration 193/1000 | Loss: 0.00001945
Iteration 194/1000 | Loss: 0.00001945
Iteration 195/1000 | Loss: 0.00001945
Iteration 196/1000 | Loss: 0.00001945
Iteration 197/1000 | Loss: 0.00001945
Iteration 198/1000 | Loss: 0.00001945
Iteration 199/1000 | Loss: 0.00001945
Iteration 200/1000 | Loss: 0.00001945
Iteration 201/1000 | Loss: 0.00001945
Iteration 202/1000 | Loss: 0.00001945
Iteration 203/1000 | Loss: 0.00001945
Iteration 204/1000 | Loss: 0.00001945
Iteration 205/1000 | Loss: 0.00001944
Iteration 206/1000 | Loss: 0.00001944
Iteration 207/1000 | Loss: 0.00001944
Iteration 208/1000 | Loss: 0.00001944
Iteration 209/1000 | Loss: 0.00001944
Iteration 210/1000 | Loss: 0.00001944
Iteration 211/1000 | Loss: 0.00001944
Iteration 212/1000 | Loss: 0.00001944
Iteration 213/1000 | Loss: 0.00001944
Iteration 214/1000 | Loss: 0.00001944
Iteration 215/1000 | Loss: 0.00001944
Iteration 216/1000 | Loss: 0.00001944
Iteration 217/1000 | Loss: 0.00001944
Iteration 218/1000 | Loss: 0.00001944
Iteration 219/1000 | Loss: 0.00001944
Iteration 220/1000 | Loss: 0.00001943
Iteration 221/1000 | Loss: 0.00001943
Iteration 222/1000 | Loss: 0.00001943
Iteration 223/1000 | Loss: 0.00001943
Iteration 224/1000 | Loss: 0.00001943
Iteration 225/1000 | Loss: 0.00001943
Iteration 226/1000 | Loss: 0.00001943
Iteration 227/1000 | Loss: 0.00001943
Iteration 228/1000 | Loss: 0.00001943
Iteration 229/1000 | Loss: 0.00001943
Iteration 230/1000 | Loss: 0.00001943
Iteration 231/1000 | Loss: 0.00001943
Iteration 232/1000 | Loss: 0.00001943
Iteration 233/1000 | Loss: 0.00001943
Iteration 234/1000 | Loss: 0.00001943
Iteration 235/1000 | Loss: 0.00001943
Iteration 236/1000 | Loss: 0.00001943
Iteration 237/1000 | Loss: 0.00001942
Iteration 238/1000 | Loss: 0.00001942
Iteration 239/1000 | Loss: 0.00001942
Iteration 240/1000 | Loss: 0.00001942
Iteration 241/1000 | Loss: 0.00001942
Iteration 242/1000 | Loss: 0.00001942
Iteration 243/1000 | Loss: 0.00001942
Iteration 244/1000 | Loss: 0.00001942
Iteration 245/1000 | Loss: 0.00001942
Iteration 246/1000 | Loss: 0.00001942
Iteration 247/1000 | Loss: 0.00001942
Iteration 248/1000 | Loss: 0.00001942
Iteration 249/1000 | Loss: 0.00001942
Iteration 250/1000 | Loss: 0.00001942
Iteration 251/1000 | Loss: 0.00001942
Iteration 252/1000 | Loss: 0.00001942
Iteration 253/1000 | Loss: 0.00001942
Iteration 254/1000 | Loss: 0.00001942
Iteration 255/1000 | Loss: 0.00001942
Iteration 256/1000 | Loss: 0.00001942
Iteration 257/1000 | Loss: 0.00001942
Iteration 258/1000 | Loss: 0.00001942
Iteration 259/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.9415703718550503e-05, 1.9415703718550503e-05, 1.9415703718550503e-05, 1.9415703718550503e-05, 1.9415703718550503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9415703718550503e-05

Optimization complete. Final v2v error: 3.7163569927215576 mm

Highest mean error: 6.091293811798096 mm for frame 166

Lowest mean error: 2.9802091121673584 mm for frame 191

Saving results

Total time: 70.0833842754364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799965
Iteration 2/25 | Loss: 0.00157258
Iteration 3/25 | Loss: 0.00133107
Iteration 4/25 | Loss: 0.00131257
Iteration 5/25 | Loss: 0.00131048
Iteration 6/25 | Loss: 0.00131010
Iteration 7/25 | Loss: 0.00131010
Iteration 8/25 | Loss: 0.00131010
Iteration 9/25 | Loss: 0.00131010
Iteration 10/25 | Loss: 0.00131010
Iteration 11/25 | Loss: 0.00131010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013100990327075124, 0.0013100990327075124, 0.0013100990327075124, 0.0013100990327075124, 0.0013100990327075124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013100990327075124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30313861
Iteration 2/25 | Loss: 0.00135074
Iteration 3/25 | Loss: 0.00135073
Iteration 4/25 | Loss: 0.00135073
Iteration 5/25 | Loss: 0.00135073
Iteration 6/25 | Loss: 0.00135073
Iteration 7/25 | Loss: 0.00135073
Iteration 8/25 | Loss: 0.00135073
Iteration 9/25 | Loss: 0.00135073
Iteration 10/25 | Loss: 0.00135073
Iteration 11/25 | Loss: 0.00135073
Iteration 12/25 | Loss: 0.00135073
Iteration 13/25 | Loss: 0.00135073
Iteration 14/25 | Loss: 0.00135073
Iteration 15/25 | Loss: 0.00135073
Iteration 16/25 | Loss: 0.00135073
Iteration 17/25 | Loss: 0.00135073
Iteration 18/25 | Loss: 0.00135073
Iteration 19/25 | Loss: 0.00135073
Iteration 20/25 | Loss: 0.00135073
Iteration 21/25 | Loss: 0.00135073
Iteration 22/25 | Loss: 0.00135073
Iteration 23/25 | Loss: 0.00135073
Iteration 24/25 | Loss: 0.00135073
Iteration 25/25 | Loss: 0.00135073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135073
Iteration 2/1000 | Loss: 0.00002542
Iteration 3/1000 | Loss: 0.00001805
Iteration 4/1000 | Loss: 0.00001607
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001449
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001251
Iteration 14/1000 | Loss: 0.00001244
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001233
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001233
Iteration 20/1000 | Loss: 0.00001233
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001232
Iteration 24/1000 | Loss: 0.00001232
Iteration 25/1000 | Loss: 0.00001231
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001226
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001218
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001216
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001198
Iteration 101/1000 | Loss: 0.00001198
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1908849955943879e-05, 1.1908849955943879e-05, 1.1908849955943879e-05, 1.1908849955943879e-05, 1.1908849955943879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1908849955943879e-05

Optimization complete. Final v2v error: 2.9700448513031006 mm

Highest mean error: 3.267636299133301 mm for frame 58

Lowest mean error: 2.829637050628662 mm for frame 5

Saving results

Total time: 40.18762016296387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857808
Iteration 2/25 | Loss: 0.00177039
Iteration 3/25 | Loss: 0.00147248
Iteration 4/25 | Loss: 0.00144088
Iteration 5/25 | Loss: 0.00143509
Iteration 6/25 | Loss: 0.00143529
Iteration 7/25 | Loss: 0.00142874
Iteration 8/25 | Loss: 0.00141742
Iteration 9/25 | Loss: 0.00141011
Iteration 10/25 | Loss: 0.00139743
Iteration 11/25 | Loss: 0.00139210
Iteration 12/25 | Loss: 0.00138682
Iteration 13/25 | Loss: 0.00137903
Iteration 14/25 | Loss: 0.00137506
Iteration 15/25 | Loss: 0.00137401
Iteration 16/25 | Loss: 0.00137370
Iteration 17/25 | Loss: 0.00137364
Iteration 18/25 | Loss: 0.00137364
Iteration 19/25 | Loss: 0.00137364
Iteration 20/25 | Loss: 0.00137364
Iteration 21/25 | Loss: 0.00137364
Iteration 22/25 | Loss: 0.00137364
Iteration 23/25 | Loss: 0.00137364
Iteration 24/25 | Loss: 0.00137364
Iteration 25/25 | Loss: 0.00137364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87634587
Iteration 2/25 | Loss: 0.00082128
Iteration 3/25 | Loss: 0.00082128
Iteration 4/25 | Loss: 0.00082127
Iteration 5/25 | Loss: 0.00082127
Iteration 6/25 | Loss: 0.00082127
Iteration 7/25 | Loss: 0.00082127
Iteration 8/25 | Loss: 0.00082127
Iteration 9/25 | Loss: 0.00082127
Iteration 10/25 | Loss: 0.00082127
Iteration 11/25 | Loss: 0.00082127
Iteration 12/25 | Loss: 0.00082127
Iteration 13/25 | Loss: 0.00082127
Iteration 14/25 | Loss: 0.00082127
Iteration 15/25 | Loss: 0.00082127
Iteration 16/25 | Loss: 0.00082127
Iteration 17/25 | Loss: 0.00082127
Iteration 18/25 | Loss: 0.00082127
Iteration 19/25 | Loss: 0.00082127
Iteration 20/25 | Loss: 0.00082127
Iteration 21/25 | Loss: 0.00082127
Iteration 22/25 | Loss: 0.00082127
Iteration 23/25 | Loss: 0.00082127
Iteration 24/25 | Loss: 0.00082127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008212708635255694, 0.0008212708635255694, 0.0008212708635255694, 0.0008212708635255694, 0.0008212708635255694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008212708635255694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082127
Iteration 2/1000 | Loss: 0.00004702
Iteration 3/1000 | Loss: 0.00003700
Iteration 4/1000 | Loss: 0.00003356
Iteration 5/1000 | Loss: 0.00003212
Iteration 6/1000 | Loss: 0.00003115
Iteration 7/1000 | Loss: 0.00003021
Iteration 8/1000 | Loss: 0.00002978
Iteration 9/1000 | Loss: 0.00002930
Iteration 10/1000 | Loss: 0.00002885
Iteration 11/1000 | Loss: 0.00002859
Iteration 12/1000 | Loss: 0.00002846
Iteration 13/1000 | Loss: 0.00002829
Iteration 14/1000 | Loss: 0.00002826
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002809
Iteration 17/1000 | Loss: 0.00002808
Iteration 18/1000 | Loss: 0.00002798
Iteration 19/1000 | Loss: 0.00002798
Iteration 20/1000 | Loss: 0.00002798
Iteration 21/1000 | Loss: 0.00002798
Iteration 22/1000 | Loss: 0.00002798
Iteration 23/1000 | Loss: 0.00002798
Iteration 24/1000 | Loss: 0.00002798
Iteration 25/1000 | Loss: 0.00002797
Iteration 26/1000 | Loss: 0.00002797
Iteration 27/1000 | Loss: 0.00002797
Iteration 28/1000 | Loss: 0.00002796
Iteration 29/1000 | Loss: 0.00002795
Iteration 30/1000 | Loss: 0.00002795
Iteration 31/1000 | Loss: 0.00002795
Iteration 32/1000 | Loss: 0.00002795
Iteration 33/1000 | Loss: 0.00002794
Iteration 34/1000 | Loss: 0.00002794
Iteration 35/1000 | Loss: 0.00002793
Iteration 36/1000 | Loss: 0.00002793
Iteration 37/1000 | Loss: 0.00002792
Iteration 38/1000 | Loss: 0.00002792
Iteration 39/1000 | Loss: 0.00002791
Iteration 40/1000 | Loss: 0.00002791
Iteration 41/1000 | Loss: 0.00002791
Iteration 42/1000 | Loss: 0.00002791
Iteration 43/1000 | Loss: 0.00002791
Iteration 44/1000 | Loss: 0.00002790
Iteration 45/1000 | Loss: 0.00002790
Iteration 46/1000 | Loss: 0.00002790
Iteration 47/1000 | Loss: 0.00002790
Iteration 48/1000 | Loss: 0.00002790
Iteration 49/1000 | Loss: 0.00002790
Iteration 50/1000 | Loss: 0.00002790
Iteration 51/1000 | Loss: 0.00002790
Iteration 52/1000 | Loss: 0.00002790
Iteration 53/1000 | Loss: 0.00002790
Iteration 54/1000 | Loss: 0.00002790
Iteration 55/1000 | Loss: 0.00002790
Iteration 56/1000 | Loss: 0.00002790
Iteration 57/1000 | Loss: 0.00002790
Iteration 58/1000 | Loss: 0.00002790
Iteration 59/1000 | Loss: 0.00002790
Iteration 60/1000 | Loss: 0.00002790
Iteration 61/1000 | Loss: 0.00002790
Iteration 62/1000 | Loss: 0.00002790
Iteration 63/1000 | Loss: 0.00002790
Iteration 64/1000 | Loss: 0.00002790
Iteration 65/1000 | Loss: 0.00002790
Iteration 66/1000 | Loss: 0.00002790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.7898042390006594e-05, 2.7898042390006594e-05, 2.7898042390006594e-05, 2.7898042390006594e-05, 2.7898042390006594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7898042390006594e-05

Optimization complete. Final v2v error: 4.481955051422119 mm

Highest mean error: 4.727204322814941 mm for frame 147

Lowest mean error: 4.3664937019348145 mm for frame 68

Saving results

Total time: 52.84162402153015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609485
Iteration 2/25 | Loss: 0.00157699
Iteration 3/25 | Loss: 0.00138128
Iteration 4/25 | Loss: 0.00133270
Iteration 5/25 | Loss: 0.00133281
Iteration 6/25 | Loss: 0.00133377
Iteration 7/25 | Loss: 0.00131087
Iteration 8/25 | Loss: 0.00130479
Iteration 9/25 | Loss: 0.00130440
Iteration 10/25 | Loss: 0.00129956
Iteration 11/25 | Loss: 0.00129836
Iteration 12/25 | Loss: 0.00129963
Iteration 13/25 | Loss: 0.00129711
Iteration 14/25 | Loss: 0.00129575
Iteration 15/25 | Loss: 0.00129701
Iteration 16/25 | Loss: 0.00129555
Iteration 17/25 | Loss: 0.00129555
Iteration 18/25 | Loss: 0.00129555
Iteration 19/25 | Loss: 0.00129555
Iteration 20/25 | Loss: 0.00129554
Iteration 21/25 | Loss: 0.00129553
Iteration 22/25 | Loss: 0.00129553
Iteration 23/25 | Loss: 0.00129553
Iteration 24/25 | Loss: 0.00129553
Iteration 25/25 | Loss: 0.00129553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.51481438
Iteration 2/25 | Loss: 0.00130222
Iteration 3/25 | Loss: 0.00125870
Iteration 4/25 | Loss: 0.00125870
Iteration 5/25 | Loss: 0.00125869
Iteration 6/25 | Loss: 0.00125869
Iteration 7/25 | Loss: 0.00125869
Iteration 8/25 | Loss: 0.00125869
Iteration 9/25 | Loss: 0.00125869
Iteration 10/25 | Loss: 0.00125869
Iteration 11/25 | Loss: 0.00125869
Iteration 12/25 | Loss: 0.00125869
Iteration 13/25 | Loss: 0.00125869
Iteration 14/25 | Loss: 0.00125869
Iteration 15/25 | Loss: 0.00125869
Iteration 16/25 | Loss: 0.00125869
Iteration 17/25 | Loss: 0.00125869
Iteration 18/25 | Loss: 0.00125869
Iteration 19/25 | Loss: 0.00125869
Iteration 20/25 | Loss: 0.00125869
Iteration 21/25 | Loss: 0.00125869
Iteration 22/25 | Loss: 0.00125869
Iteration 23/25 | Loss: 0.00125869
Iteration 24/25 | Loss: 0.00125869
Iteration 25/25 | Loss: 0.00125869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125869
Iteration 2/1000 | Loss: 0.00005563
Iteration 3/1000 | Loss: 0.00002149
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001513
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001339
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001338
Iteration 40/1000 | Loss: 0.00001337
Iteration 41/1000 | Loss: 0.00001337
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001335
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001334
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001333
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001327
Iteration 73/1000 | Loss: 0.00001327
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001322
Iteration 108/1000 | Loss: 0.00001322
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001318
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.318387694482226e-05, 1.318387694482226e-05, 1.318387694482226e-05, 1.318387694482226e-05, 1.318387694482226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.318387694482226e-05

Optimization complete. Final v2v error: 3.100409746170044 mm

Highest mean error: 3.444556951522827 mm for frame 231

Lowest mean error: 2.855795383453369 mm for frame 133

Saving results

Total time: 71.38993310928345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040699
Iteration 2/25 | Loss: 0.01040699
Iteration 3/25 | Loss: 0.01040699
Iteration 4/25 | Loss: 0.01040699
Iteration 5/25 | Loss: 0.01040699
Iteration 6/25 | Loss: 0.01040699
Iteration 7/25 | Loss: 0.01040699
Iteration 8/25 | Loss: 0.01040699
Iteration 9/25 | Loss: 0.01040699
Iteration 10/25 | Loss: 0.01040699
Iteration 11/25 | Loss: 0.01040699
Iteration 12/25 | Loss: 0.01040699
Iteration 13/25 | Loss: 0.01040698
Iteration 14/25 | Loss: 0.01040698
Iteration 15/25 | Loss: 0.01040698
Iteration 16/25 | Loss: 0.01040698
Iteration 17/25 | Loss: 0.01040698
Iteration 18/25 | Loss: 0.01040698
Iteration 19/25 | Loss: 0.01040698
Iteration 20/25 | Loss: 0.01040698
Iteration 21/25 | Loss: 0.01040698
Iteration 22/25 | Loss: 0.01040698
Iteration 23/25 | Loss: 0.01040697
Iteration 24/25 | Loss: 0.01040697
Iteration 25/25 | Loss: 0.01040697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54852819
Iteration 2/25 | Loss: 0.08480442
Iteration 3/25 | Loss: 0.08479968
Iteration 4/25 | Loss: 0.08479965
Iteration 5/25 | Loss: 0.08479965
Iteration 6/25 | Loss: 0.08479964
Iteration 7/25 | Loss: 0.08479964
Iteration 8/25 | Loss: 0.08479964
Iteration 9/25 | Loss: 0.08479964
Iteration 10/25 | Loss: 0.08479965
Iteration 11/25 | Loss: 0.08479965
Iteration 12/25 | Loss: 0.08479964
Iteration 13/25 | Loss: 0.08479964
Iteration 14/25 | Loss: 0.08479964
Iteration 15/25 | Loss: 0.08479964
Iteration 16/25 | Loss: 0.08479964
Iteration 17/25 | Loss: 0.08479964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0847996398806572, 0.0847996398806572, 0.0847996398806572, 0.0847996398806572, 0.0847996398806572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0847996398806572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08479964
Iteration 2/1000 | Loss: 0.00045610
Iteration 3/1000 | Loss: 0.00010624
Iteration 4/1000 | Loss: 0.00005113
Iteration 5/1000 | Loss: 0.00003240
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002135
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00001878
Iteration 11/1000 | Loss: 0.00001760
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001139
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001138
Iteration 48/1000 | Loss: 0.00001138
Iteration 49/1000 | Loss: 0.00001137
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001137
Iteration 53/1000 | Loss: 0.00001137
Iteration 54/1000 | Loss: 0.00001137
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001134
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001133
Iteration 66/1000 | Loss: 0.00001133
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001124
Iteration 100/1000 | Loss: 0.00001124
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001121
Iteration 121/1000 | Loss: 0.00001121
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001121
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001120
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001119
Iteration 135/1000 | Loss: 0.00001119
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001118
Iteration 140/1000 | Loss: 0.00001117
Iteration 141/1000 | Loss: 0.00001117
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001116
Iteration 147/1000 | Loss: 0.00001116
Iteration 148/1000 | Loss: 0.00001116
Iteration 149/1000 | Loss: 0.00001116
Iteration 150/1000 | Loss: 0.00001116
Iteration 151/1000 | Loss: 0.00001116
Iteration 152/1000 | Loss: 0.00001116
Iteration 153/1000 | Loss: 0.00001116
Iteration 154/1000 | Loss: 0.00001116
Iteration 155/1000 | Loss: 0.00001116
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001116
Iteration 161/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.1161831025674473e-05, 1.1161831025674473e-05, 1.1161831025674473e-05, 1.1161831025674473e-05, 1.1161831025674473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1161831025674473e-05

Optimization complete. Final v2v error: 2.877612352371216 mm

Highest mean error: 3.0658671855926514 mm for frame 159

Lowest mean error: 2.7046029567718506 mm for frame 189

Saving results

Total time: 57.93502354621887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991616
Iteration 2/25 | Loss: 0.00232372
Iteration 3/25 | Loss: 0.00213075
Iteration 4/25 | Loss: 0.00225247
Iteration 5/25 | Loss: 0.00169174
Iteration 6/25 | Loss: 0.00156863
Iteration 7/25 | Loss: 0.00148736
Iteration 8/25 | Loss: 0.00147445
Iteration 9/25 | Loss: 0.00147329
Iteration 10/25 | Loss: 0.00147314
Iteration 11/25 | Loss: 0.00147314
Iteration 12/25 | Loss: 0.00147314
Iteration 13/25 | Loss: 0.00147314
Iteration 14/25 | Loss: 0.00147314
Iteration 15/25 | Loss: 0.00147314
Iteration 16/25 | Loss: 0.00147314
Iteration 17/25 | Loss: 0.00147314
Iteration 18/25 | Loss: 0.00147314
Iteration 19/25 | Loss: 0.00147314
Iteration 20/25 | Loss: 0.00147314
Iteration 21/25 | Loss: 0.00147314
Iteration 22/25 | Loss: 0.00147314
Iteration 23/25 | Loss: 0.00147314
Iteration 24/25 | Loss: 0.00147314
Iteration 25/25 | Loss: 0.00147314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26190579
Iteration 2/25 | Loss: 0.00123501
Iteration 3/25 | Loss: 0.00123501
Iteration 4/25 | Loss: 0.00123501
Iteration 5/25 | Loss: 0.00123501
Iteration 6/25 | Loss: 0.00123501
Iteration 7/25 | Loss: 0.00123501
Iteration 8/25 | Loss: 0.00123501
Iteration 9/25 | Loss: 0.00123500
Iteration 10/25 | Loss: 0.00123500
Iteration 11/25 | Loss: 0.00123500
Iteration 12/25 | Loss: 0.00123500
Iteration 13/25 | Loss: 0.00123500
Iteration 14/25 | Loss: 0.00123500
Iteration 15/25 | Loss: 0.00123500
Iteration 16/25 | Loss: 0.00123500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012350046308711171, 0.0012350046308711171, 0.0012350046308711171, 0.0012350046308711171, 0.0012350046308711171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012350046308711171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123500
Iteration 2/1000 | Loss: 0.00005955
Iteration 3/1000 | Loss: 0.00004103
Iteration 4/1000 | Loss: 0.00003809
Iteration 5/1000 | Loss: 0.00003700
Iteration 6/1000 | Loss: 0.00003607
Iteration 7/1000 | Loss: 0.00003550
Iteration 8/1000 | Loss: 0.00003508
Iteration 9/1000 | Loss: 0.00003463
Iteration 10/1000 | Loss: 0.00003430
Iteration 11/1000 | Loss: 0.00003399
Iteration 12/1000 | Loss: 0.00003376
Iteration 13/1000 | Loss: 0.00003352
Iteration 14/1000 | Loss: 0.00003340
Iteration 15/1000 | Loss: 0.00003335
Iteration 16/1000 | Loss: 0.00003319
Iteration 17/1000 | Loss: 0.00003318
Iteration 18/1000 | Loss: 0.00003313
Iteration 19/1000 | Loss: 0.00003309
Iteration 20/1000 | Loss: 0.00003309
Iteration 21/1000 | Loss: 0.00003309
Iteration 22/1000 | Loss: 0.00003309
Iteration 23/1000 | Loss: 0.00003309
Iteration 24/1000 | Loss: 0.00003309
Iteration 25/1000 | Loss: 0.00003308
Iteration 26/1000 | Loss: 0.00003308
Iteration 27/1000 | Loss: 0.00003307
Iteration 28/1000 | Loss: 0.00003307
Iteration 29/1000 | Loss: 0.00003307
Iteration 30/1000 | Loss: 0.00003307
Iteration 31/1000 | Loss: 0.00003307
Iteration 32/1000 | Loss: 0.00003306
Iteration 33/1000 | Loss: 0.00003305
Iteration 34/1000 | Loss: 0.00003304
Iteration 35/1000 | Loss: 0.00003304
Iteration 36/1000 | Loss: 0.00003303
Iteration 37/1000 | Loss: 0.00003299
Iteration 38/1000 | Loss: 0.00003299
Iteration 39/1000 | Loss: 0.00003297
Iteration 40/1000 | Loss: 0.00003297
Iteration 41/1000 | Loss: 0.00003296
Iteration 42/1000 | Loss: 0.00003296
Iteration 43/1000 | Loss: 0.00003295
Iteration 44/1000 | Loss: 0.00003295
Iteration 45/1000 | Loss: 0.00003295
Iteration 46/1000 | Loss: 0.00003295
Iteration 47/1000 | Loss: 0.00003295
Iteration 48/1000 | Loss: 0.00003295
Iteration 49/1000 | Loss: 0.00003295
Iteration 50/1000 | Loss: 0.00003295
Iteration 51/1000 | Loss: 0.00003294
Iteration 52/1000 | Loss: 0.00003294
Iteration 53/1000 | Loss: 0.00003294
Iteration 54/1000 | Loss: 0.00003294
Iteration 55/1000 | Loss: 0.00003292
Iteration 56/1000 | Loss: 0.00003292
Iteration 57/1000 | Loss: 0.00003292
Iteration 58/1000 | Loss: 0.00003292
Iteration 59/1000 | Loss: 0.00003292
Iteration 60/1000 | Loss: 0.00003292
Iteration 61/1000 | Loss: 0.00003292
Iteration 62/1000 | Loss: 0.00003292
Iteration 63/1000 | Loss: 0.00003292
Iteration 64/1000 | Loss: 0.00003292
Iteration 65/1000 | Loss: 0.00003292
Iteration 66/1000 | Loss: 0.00003292
Iteration 67/1000 | Loss: 0.00003292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [3.2915286283241585e-05, 3.2915286283241585e-05, 3.2915286283241585e-05, 3.2915286283241585e-05, 3.2915286283241585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2915286283241585e-05

Optimization complete. Final v2v error: 4.928947448730469 mm

Highest mean error: 5.275816917419434 mm for frame 204

Lowest mean error: 4.7786149978637695 mm for frame 156

Saving results

Total time: 48.40855312347412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00701457
Iteration 2/25 | Loss: 0.00143070
Iteration 3/25 | Loss: 0.00133949
Iteration 4/25 | Loss: 0.00130210
Iteration 5/25 | Loss: 0.00129740
Iteration 6/25 | Loss: 0.00129624
Iteration 7/25 | Loss: 0.00129593
Iteration 8/25 | Loss: 0.00129578
Iteration 9/25 | Loss: 0.00129569
Iteration 10/25 | Loss: 0.00129569
Iteration 11/25 | Loss: 0.00129569
Iteration 12/25 | Loss: 0.00129569
Iteration 13/25 | Loss: 0.00129569
Iteration 14/25 | Loss: 0.00129569
Iteration 15/25 | Loss: 0.00129568
Iteration 16/25 | Loss: 0.00129568
Iteration 17/25 | Loss: 0.00129568
Iteration 18/25 | Loss: 0.00129568
Iteration 19/25 | Loss: 0.00129568
Iteration 20/25 | Loss: 0.00129568
Iteration 21/25 | Loss: 0.00129568
Iteration 22/25 | Loss: 0.00129568
Iteration 23/25 | Loss: 0.00129568
Iteration 24/25 | Loss: 0.00129568
Iteration 25/25 | Loss: 0.00129568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93499172
Iteration 2/25 | Loss: 0.00146751
Iteration 3/25 | Loss: 0.00146751
Iteration 4/25 | Loss: 0.00146751
Iteration 5/25 | Loss: 0.00146751
Iteration 6/25 | Loss: 0.00146751
Iteration 7/25 | Loss: 0.00146751
Iteration 8/25 | Loss: 0.00146751
Iteration 9/25 | Loss: 0.00146751
Iteration 10/25 | Loss: 0.00146750
Iteration 11/25 | Loss: 0.00146750
Iteration 12/25 | Loss: 0.00146750
Iteration 13/25 | Loss: 0.00146750
Iteration 14/25 | Loss: 0.00146750
Iteration 15/25 | Loss: 0.00146750
Iteration 16/25 | Loss: 0.00146750
Iteration 17/25 | Loss: 0.00146750
Iteration 18/25 | Loss: 0.00146750
Iteration 19/25 | Loss: 0.00146750
Iteration 20/25 | Loss: 0.00146750
Iteration 21/25 | Loss: 0.00146750
Iteration 22/25 | Loss: 0.00146750
Iteration 23/25 | Loss: 0.00146750
Iteration 24/25 | Loss: 0.00146750
Iteration 25/25 | Loss: 0.00146750
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014675039565190673, 0.0014675039565190673, 0.0014675039565190673, 0.0014675039565190673, 0.0014675039565190673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014675039565190673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146750
Iteration 2/1000 | Loss: 0.00001974
Iteration 3/1000 | Loss: 0.00001655
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001220
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001182
Iteration 66/1000 | Loss: 0.00001182
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001181
Iteration 69/1000 | Loss: 0.00001181
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1700481991283596e-05, 1.1700481991283596e-05, 1.1700481991283596e-05, 1.1700481991283596e-05, 1.1700481991283596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1700481991283596e-05

Optimization complete. Final v2v error: 2.961744785308838 mm

Highest mean error: 3.2330312728881836 mm for frame 82

Lowest mean error: 2.7936959266662598 mm for frame 149

Saving results

Total time: 52.67157053947449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033261
Iteration 2/25 | Loss: 0.00226028
Iteration 3/25 | Loss: 0.00175837
Iteration 4/25 | Loss: 0.00162887
Iteration 5/25 | Loss: 0.00161526
Iteration 6/25 | Loss: 0.00157908
Iteration 7/25 | Loss: 0.00147069
Iteration 8/25 | Loss: 0.00140571
Iteration 9/25 | Loss: 0.00140113
Iteration 10/25 | Loss: 0.00137808
Iteration 11/25 | Loss: 0.00136714
Iteration 12/25 | Loss: 0.00134802
Iteration 13/25 | Loss: 0.00133054
Iteration 14/25 | Loss: 0.00132971
Iteration 15/25 | Loss: 0.00132950
Iteration 16/25 | Loss: 0.00132845
Iteration 17/25 | Loss: 0.00132627
Iteration 18/25 | Loss: 0.00132453
Iteration 19/25 | Loss: 0.00132785
Iteration 20/25 | Loss: 0.00132209
Iteration 21/25 | Loss: 0.00132454
Iteration 22/25 | Loss: 0.00132326
Iteration 23/25 | Loss: 0.00131839
Iteration 24/25 | Loss: 0.00131700
Iteration 25/25 | Loss: 0.00131505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42692757
Iteration 2/25 | Loss: 0.00193422
Iteration 3/25 | Loss: 0.00193422
Iteration 4/25 | Loss: 0.00157018
Iteration 5/25 | Loss: 0.00157018
Iteration 6/25 | Loss: 0.00157018
Iteration 7/25 | Loss: 0.00157017
Iteration 8/25 | Loss: 0.00157017
Iteration 9/25 | Loss: 0.00157017
Iteration 10/25 | Loss: 0.00157017
Iteration 11/25 | Loss: 0.00157017
Iteration 12/25 | Loss: 0.00157017
Iteration 13/25 | Loss: 0.00157017
Iteration 14/25 | Loss: 0.00157017
Iteration 15/25 | Loss: 0.00157017
Iteration 16/25 | Loss: 0.00157017
Iteration 17/25 | Loss: 0.00157017
Iteration 18/25 | Loss: 0.00157017
Iteration 19/25 | Loss: 0.00157017
Iteration 20/25 | Loss: 0.00157017
Iteration 21/25 | Loss: 0.00157017
Iteration 22/25 | Loss: 0.00157017
Iteration 23/25 | Loss: 0.00157017
Iteration 24/25 | Loss: 0.00157017
Iteration 25/25 | Loss: 0.00157017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157017
Iteration 2/1000 | Loss: 0.00024910
Iteration 3/1000 | Loss: 0.00018103
Iteration 4/1000 | Loss: 0.00048800
Iteration 5/1000 | Loss: 0.00022444
Iteration 6/1000 | Loss: 0.00013983
Iteration 7/1000 | Loss: 0.00003539
Iteration 8/1000 | Loss: 0.00005882
Iteration 9/1000 | Loss: 0.00002400
Iteration 10/1000 | Loss: 0.00005697
Iteration 11/1000 | Loss: 0.00003053
Iteration 12/1000 | Loss: 0.00002224
Iteration 13/1000 | Loss: 0.00004801
Iteration 14/1000 | Loss: 0.00002368
Iteration 15/1000 | Loss: 0.00002131
Iteration 16/1000 | Loss: 0.00033521
Iteration 17/1000 | Loss: 0.00013633
Iteration 18/1000 | Loss: 0.00013425
Iteration 19/1000 | Loss: 0.00002478
Iteration 20/1000 | Loss: 0.00002216
Iteration 21/1000 | Loss: 0.00006781
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002734
Iteration 24/1000 | Loss: 0.00005595
Iteration 25/1000 | Loss: 0.00023545
Iteration 26/1000 | Loss: 0.00003385
Iteration 27/1000 | Loss: 0.00008006
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00003178
Iteration 30/1000 | Loss: 0.00001962
Iteration 31/1000 | Loss: 0.00005260
Iteration 32/1000 | Loss: 0.00008149
Iteration 33/1000 | Loss: 0.00003585
Iteration 34/1000 | Loss: 0.00001987
Iteration 35/1000 | Loss: 0.00003720
Iteration 36/1000 | Loss: 0.00001912
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002109
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001797
Iteration 41/1000 | Loss: 0.00001797
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001796
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001796
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001795
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00004325
Iteration 57/1000 | Loss: 0.00005124
Iteration 58/1000 | Loss: 0.00001800
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001791
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001787
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001787
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001786
Iteration 74/1000 | Loss: 0.00001786
Iteration 75/1000 | Loss: 0.00001786
Iteration 76/1000 | Loss: 0.00001786
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001790
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001785
Iteration 84/1000 | Loss: 0.00001785
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001785
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001785
Iteration 104/1000 | Loss: 0.00001785
Iteration 105/1000 | Loss: 0.00001785
Iteration 106/1000 | Loss: 0.00001785
Iteration 107/1000 | Loss: 0.00001785
Iteration 108/1000 | Loss: 0.00001785
Iteration 109/1000 | Loss: 0.00001785
Iteration 110/1000 | Loss: 0.00001785
Iteration 111/1000 | Loss: 0.00001785
Iteration 112/1000 | Loss: 0.00001785
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001785
Iteration 115/1000 | Loss: 0.00001785
Iteration 116/1000 | Loss: 0.00001785
Iteration 117/1000 | Loss: 0.00001785
Iteration 118/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.7848713468993083e-05, 1.7848713468993083e-05, 1.7848713468993083e-05, 1.7848713468993083e-05, 1.7848713468993083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7848713468993083e-05

Optimization complete. Final v2v error: 3.5296828746795654 mm

Highest mean error: 5.860296249389648 mm for frame 197

Lowest mean error: 2.8537566661834717 mm for frame 14

Saving results

Total time: 127.90580201148987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747627
Iteration 2/25 | Loss: 0.00160757
Iteration 3/25 | Loss: 0.00135920
Iteration 4/25 | Loss: 0.00134693
Iteration 5/25 | Loss: 0.00134668
Iteration 6/25 | Loss: 0.00134668
Iteration 7/25 | Loss: 0.00134668
Iteration 8/25 | Loss: 0.00134668
Iteration 9/25 | Loss: 0.00134668
Iteration 10/25 | Loss: 0.00134668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013466804521158338, 0.0013466804521158338, 0.0013466804521158338, 0.0013466804521158338, 0.0013466804521158338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013466804521158338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26580608
Iteration 2/25 | Loss: 0.00100930
Iteration 3/25 | Loss: 0.00100928
Iteration 4/25 | Loss: 0.00100928
Iteration 5/25 | Loss: 0.00100928
Iteration 6/25 | Loss: 0.00100928
Iteration 7/25 | Loss: 0.00100928
Iteration 8/25 | Loss: 0.00100928
Iteration 9/25 | Loss: 0.00100928
Iteration 10/25 | Loss: 0.00100928
Iteration 11/25 | Loss: 0.00100928
Iteration 12/25 | Loss: 0.00100928
Iteration 13/25 | Loss: 0.00100928
Iteration 14/25 | Loss: 0.00100928
Iteration 15/25 | Loss: 0.00100928
Iteration 16/25 | Loss: 0.00100928
Iteration 17/25 | Loss: 0.00100928
Iteration 18/25 | Loss: 0.00100928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010092784650623798, 0.0010092784650623798, 0.0010092784650623798, 0.0010092784650623798, 0.0010092784650623798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010092784650623798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100928
Iteration 2/1000 | Loss: 0.00002960
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001791
Iteration 5/1000 | Loss: 0.00001704
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001599
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001439
Iteration 14/1000 | Loss: 0.00001436
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001404
Iteration 18/1000 | Loss: 0.00001393
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001391
Iteration 22/1000 | Loss: 0.00001390
Iteration 23/1000 | Loss: 0.00001390
Iteration 24/1000 | Loss: 0.00001389
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001386
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001380
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001376
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001372
Iteration 54/1000 | Loss: 0.00001372
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001370
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001362
Iteration 118/1000 | Loss: 0.00001362
Iteration 119/1000 | Loss: 0.00001362
Iteration 120/1000 | Loss: 0.00001362
Iteration 121/1000 | Loss: 0.00001362
Iteration 122/1000 | Loss: 0.00001362
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001361
Iteration 125/1000 | Loss: 0.00001361
Iteration 126/1000 | Loss: 0.00001361
Iteration 127/1000 | Loss: 0.00001361
Iteration 128/1000 | Loss: 0.00001361
Iteration 129/1000 | Loss: 0.00001361
Iteration 130/1000 | Loss: 0.00001361
Iteration 131/1000 | Loss: 0.00001361
Iteration 132/1000 | Loss: 0.00001361
Iteration 133/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.361297836410813e-05, 1.361297836410813e-05, 1.361297836410813e-05, 1.361297836410813e-05, 1.361297836410813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.361297836410813e-05

Optimization complete. Final v2v error: 3.127598524093628 mm

Highest mean error: 3.3161838054656982 mm for frame 137

Lowest mean error: 2.9832327365875244 mm for frame 177

Saving results

Total time: 42.401976108551025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980133
Iteration 2/25 | Loss: 0.00289590
Iteration 3/25 | Loss: 0.00193430
Iteration 4/25 | Loss: 0.00177811
Iteration 5/25 | Loss: 0.00190579
Iteration 6/25 | Loss: 0.00187830
Iteration 7/25 | Loss: 0.00153378
Iteration 8/25 | Loss: 0.00149838
Iteration 9/25 | Loss: 0.00142292
Iteration 10/25 | Loss: 0.00141674
Iteration 11/25 | Loss: 0.00139016
Iteration 12/25 | Loss: 0.00138424
Iteration 13/25 | Loss: 0.00138397
Iteration 14/25 | Loss: 0.00138295
Iteration 15/25 | Loss: 0.00138294
Iteration 16/25 | Loss: 0.00138294
Iteration 17/25 | Loss: 0.00138294
Iteration 18/25 | Loss: 0.00138294
Iteration 19/25 | Loss: 0.00138294
Iteration 20/25 | Loss: 0.00138294
Iteration 21/25 | Loss: 0.00138294
Iteration 22/25 | Loss: 0.00138294
Iteration 23/25 | Loss: 0.00138294
Iteration 24/25 | Loss: 0.00138294
Iteration 25/25 | Loss: 0.00138294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28353214
Iteration 2/25 | Loss: 0.00143656
Iteration 3/25 | Loss: 0.00136917
Iteration 4/25 | Loss: 0.00136916
Iteration 5/25 | Loss: 0.00136916
Iteration 6/25 | Loss: 0.00136916
Iteration 7/25 | Loss: 0.00136916
Iteration 8/25 | Loss: 0.00136916
Iteration 9/25 | Loss: 0.00136916
Iteration 10/25 | Loss: 0.00136916
Iteration 11/25 | Loss: 0.00136916
Iteration 12/25 | Loss: 0.00136916
Iteration 13/25 | Loss: 0.00136916
Iteration 14/25 | Loss: 0.00136916
Iteration 15/25 | Loss: 0.00136916
Iteration 16/25 | Loss: 0.00136916
Iteration 17/25 | Loss: 0.00136916
Iteration 18/25 | Loss: 0.00136916
Iteration 19/25 | Loss: 0.00136916
Iteration 20/25 | Loss: 0.00136916
Iteration 21/25 | Loss: 0.00136916
Iteration 22/25 | Loss: 0.00136916
Iteration 23/25 | Loss: 0.00136916
Iteration 24/25 | Loss: 0.00136916
Iteration 25/25 | Loss: 0.00136916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136916
Iteration 2/1000 | Loss: 0.00012610
Iteration 3/1000 | Loss: 0.00044830
Iteration 4/1000 | Loss: 0.00004024
Iteration 5/1000 | Loss: 0.00024286
Iteration 6/1000 | Loss: 0.00228477
Iteration 7/1000 | Loss: 0.00003826
Iteration 8/1000 | Loss: 0.00005038
Iteration 9/1000 | Loss: 0.00007807
Iteration 10/1000 | Loss: 0.00003641
Iteration 11/1000 | Loss: 0.00006413
Iteration 12/1000 | Loss: 0.00025169
Iteration 13/1000 | Loss: 0.00006395
Iteration 14/1000 | Loss: 0.00007523
Iteration 15/1000 | Loss: 0.00003467
Iteration 16/1000 | Loss: 0.00003341
Iteration 17/1000 | Loss: 0.00007773
Iteration 18/1000 | Loss: 0.00016599
Iteration 19/1000 | Loss: 0.00007764
Iteration 20/1000 | Loss: 0.00003542
Iteration 21/1000 | Loss: 0.00003371
Iteration 22/1000 | Loss: 0.00003001
Iteration 23/1000 | Loss: 0.00007953
Iteration 24/1000 | Loss: 0.00006135
Iteration 25/1000 | Loss: 0.00004187
Iteration 26/1000 | Loss: 0.00005168
Iteration 27/1000 | Loss: 0.00003243
Iteration 28/1000 | Loss: 0.00002979
Iteration 29/1000 | Loss: 0.00002979
Iteration 30/1000 | Loss: 0.00002979
Iteration 31/1000 | Loss: 0.00002979
Iteration 32/1000 | Loss: 0.00002979
Iteration 33/1000 | Loss: 0.00002979
Iteration 34/1000 | Loss: 0.00002978
Iteration 35/1000 | Loss: 0.00002977
Iteration 36/1000 | Loss: 0.00006290
Iteration 37/1000 | Loss: 0.00003131
Iteration 38/1000 | Loss: 0.00008092
Iteration 39/1000 | Loss: 0.00003088
Iteration 40/1000 | Loss: 0.00004365
Iteration 41/1000 | Loss: 0.00003175
Iteration 42/1000 | Loss: 0.00002965
Iteration 43/1000 | Loss: 0.00002967
Iteration 44/1000 | Loss: 0.00004728
Iteration 45/1000 | Loss: 0.00003067
Iteration 46/1000 | Loss: 0.00002959
Iteration 47/1000 | Loss: 0.00002959
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00005353
Iteration 50/1000 | Loss: 0.00005746
Iteration 51/1000 | Loss: 0.00004269
Iteration 52/1000 | Loss: 0.00005399
Iteration 53/1000 | Loss: 0.00004461
Iteration 54/1000 | Loss: 0.00003304
Iteration 55/1000 | Loss: 0.00003038
Iteration 56/1000 | Loss: 0.00002954
Iteration 57/1000 | Loss: 0.00002954
Iteration 58/1000 | Loss: 0.00002954
Iteration 59/1000 | Loss: 0.00002954
Iteration 60/1000 | Loss: 0.00003372
Iteration 61/1000 | Loss: 0.00014291
Iteration 62/1000 | Loss: 0.00003358
Iteration 63/1000 | Loss: 0.00007443
Iteration 64/1000 | Loss: 0.00003144
Iteration 65/1000 | Loss: 0.00004733
Iteration 66/1000 | Loss: 0.00054472
Iteration 67/1000 | Loss: 0.00003858
Iteration 68/1000 | Loss: 0.00005321
Iteration 69/1000 | Loss: 0.00003512
Iteration 70/1000 | Loss: 0.00003584
Iteration 71/1000 | Loss: 0.00003446
Iteration 72/1000 | Loss: 0.00002967
Iteration 73/1000 | Loss: 0.00003959
Iteration 74/1000 | Loss: 0.00002949
Iteration 75/1000 | Loss: 0.00003294
Iteration 76/1000 | Loss: 0.00003515
Iteration 77/1000 | Loss: 0.00005048
Iteration 78/1000 | Loss: 0.00003148
Iteration 79/1000 | Loss: 0.00003474
Iteration 80/1000 | Loss: 0.00004416
Iteration 81/1000 | Loss: 0.00003148
Iteration 82/1000 | Loss: 0.00002943
Iteration 83/1000 | Loss: 0.00002940
Iteration 84/1000 | Loss: 0.00002940
Iteration 85/1000 | Loss: 0.00002940
Iteration 86/1000 | Loss: 0.00002940
Iteration 87/1000 | Loss: 0.00002940
Iteration 88/1000 | Loss: 0.00002940
Iteration 89/1000 | Loss: 0.00002940
Iteration 90/1000 | Loss: 0.00002940
Iteration 91/1000 | Loss: 0.00002940
Iteration 92/1000 | Loss: 0.00002940
Iteration 93/1000 | Loss: 0.00002940
Iteration 94/1000 | Loss: 0.00002940
Iteration 95/1000 | Loss: 0.00002940
Iteration 96/1000 | Loss: 0.00002940
Iteration 97/1000 | Loss: 0.00002940
Iteration 98/1000 | Loss: 0.00002940
Iteration 99/1000 | Loss: 0.00002940
Iteration 100/1000 | Loss: 0.00002940
Iteration 101/1000 | Loss: 0.00002940
Iteration 102/1000 | Loss: 0.00002940
Iteration 103/1000 | Loss: 0.00002940
Iteration 104/1000 | Loss: 0.00002940
Iteration 105/1000 | Loss: 0.00002940
Iteration 106/1000 | Loss: 0.00002940
Iteration 107/1000 | Loss: 0.00002940
Iteration 108/1000 | Loss: 0.00002940
Iteration 109/1000 | Loss: 0.00002940
Iteration 110/1000 | Loss: 0.00002940
Iteration 111/1000 | Loss: 0.00002940
Iteration 112/1000 | Loss: 0.00002940
Iteration 113/1000 | Loss: 0.00002940
Iteration 114/1000 | Loss: 0.00002940
Iteration 115/1000 | Loss: 0.00002940
Iteration 116/1000 | Loss: 0.00002940
Iteration 117/1000 | Loss: 0.00002940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.9397908292594366e-05, 2.9397908292594366e-05, 2.9397908292594366e-05, 2.9397908292594366e-05, 2.9397908292594366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9397908292594366e-05

Optimization complete. Final v2v error: 4.556731700897217 mm

Highest mean error: 4.906655311584473 mm for frame 170

Lowest mean error: 4.336232662200928 mm for frame 208

Saving results

Total time: 128.24700927734375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_isabelle_posed_012/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_isabelle_posed_012/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066007
Iteration 2/25 | Loss: 0.00264301
Iteration 3/25 | Loss: 0.00224709
Iteration 4/25 | Loss: 0.00241461
Iteration 5/25 | Loss: 0.00158125
Iteration 6/25 | Loss: 0.00152261
Iteration 7/25 | Loss: 0.00149843
Iteration 8/25 | Loss: 0.00148802
Iteration 9/25 | Loss: 0.00148430
Iteration 10/25 | Loss: 0.00148337
Iteration 11/25 | Loss: 0.00148317
Iteration 12/25 | Loss: 0.00148316
Iteration 13/25 | Loss: 0.00148315
Iteration 14/25 | Loss: 0.00148315
Iteration 15/25 | Loss: 0.00148315
Iteration 16/25 | Loss: 0.00148315
Iteration 17/25 | Loss: 0.00148314
Iteration 18/25 | Loss: 0.00148314
Iteration 19/25 | Loss: 0.00148314
Iteration 20/25 | Loss: 0.00148314
Iteration 21/25 | Loss: 0.00148314
Iteration 22/25 | Loss: 0.00148313
Iteration 23/25 | Loss: 0.00148313
Iteration 24/25 | Loss: 0.00148313
Iteration 25/25 | Loss: 0.00148313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58074093
Iteration 2/25 | Loss: 0.00159178
Iteration 3/25 | Loss: 0.00158368
Iteration 4/25 | Loss: 0.00158368
Iteration 5/25 | Loss: 0.00158368
Iteration 6/25 | Loss: 0.00158368
Iteration 7/25 | Loss: 0.00158368
Iteration 8/25 | Loss: 0.00158368
Iteration 9/25 | Loss: 0.00158368
Iteration 10/25 | Loss: 0.00158368
Iteration 11/25 | Loss: 0.00158368
Iteration 12/25 | Loss: 0.00158368
Iteration 13/25 | Loss: 0.00158368
Iteration 14/25 | Loss: 0.00158368
Iteration 15/25 | Loss: 0.00158368
Iteration 16/25 | Loss: 0.00158368
Iteration 17/25 | Loss: 0.00158368
Iteration 18/25 | Loss: 0.00158368
Iteration 19/25 | Loss: 0.00158368
Iteration 20/25 | Loss: 0.00158368
Iteration 21/25 | Loss: 0.00158368
Iteration 22/25 | Loss: 0.00158368
Iteration 23/25 | Loss: 0.00158368
Iteration 24/25 | Loss: 0.00158368
Iteration 25/25 | Loss: 0.00158368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001583678531460464, 0.001583678531460464, 0.001583678531460464, 0.001583678531460464, 0.001583678531460464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001583678531460464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158368
Iteration 2/1000 | Loss: 0.00006075
Iteration 3/1000 | Loss: 0.00004853
Iteration 4/1000 | Loss: 0.00003469
Iteration 5/1000 | Loss: 0.00004097
Iteration 6/1000 | Loss: 0.00003189
Iteration 7/1000 | Loss: 0.00003119
Iteration 8/1000 | Loss: 0.00003063
Iteration 9/1000 | Loss: 0.00003020
Iteration 10/1000 | Loss: 0.00002995
Iteration 11/1000 | Loss: 0.00002968
Iteration 12/1000 | Loss: 0.00002945
Iteration 13/1000 | Loss: 0.00002933
Iteration 14/1000 | Loss: 0.00002932
Iteration 15/1000 | Loss: 0.00002931
Iteration 16/1000 | Loss: 0.00002927
Iteration 17/1000 | Loss: 0.00002916
Iteration 18/1000 | Loss: 0.00002904
Iteration 19/1000 | Loss: 0.00002900
Iteration 20/1000 | Loss: 0.00002900
Iteration 21/1000 | Loss: 0.00002898
Iteration 22/1000 | Loss: 0.00002898
Iteration 23/1000 | Loss: 0.00002897
Iteration 24/1000 | Loss: 0.00002897
Iteration 25/1000 | Loss: 0.00002897
Iteration 26/1000 | Loss: 0.00002897
Iteration 27/1000 | Loss: 0.00002897
Iteration 28/1000 | Loss: 0.00002896
Iteration 29/1000 | Loss: 0.00002895
Iteration 30/1000 | Loss: 0.00002895
Iteration 31/1000 | Loss: 0.00002894
Iteration 32/1000 | Loss: 0.00002894
Iteration 33/1000 | Loss: 0.00002894
Iteration 34/1000 | Loss: 0.00002893
Iteration 35/1000 | Loss: 0.00002893
Iteration 36/1000 | Loss: 0.00002892
Iteration 37/1000 | Loss: 0.00002892
Iteration 38/1000 | Loss: 0.00002892
Iteration 39/1000 | Loss: 0.00002891
Iteration 40/1000 | Loss: 0.00002890
Iteration 41/1000 | Loss: 0.00002890
Iteration 42/1000 | Loss: 0.00002890
Iteration 43/1000 | Loss: 0.00002890
Iteration 44/1000 | Loss: 0.00002890
Iteration 45/1000 | Loss: 0.00002890
Iteration 46/1000 | Loss: 0.00002890
Iteration 47/1000 | Loss: 0.00002890
Iteration 48/1000 | Loss: 0.00002889
Iteration 49/1000 | Loss: 0.00002889
Iteration 50/1000 | Loss: 0.00002888
Iteration 51/1000 | Loss: 0.00002888
Iteration 52/1000 | Loss: 0.00002888
Iteration 53/1000 | Loss: 0.00002888
Iteration 54/1000 | Loss: 0.00002888
Iteration 55/1000 | Loss: 0.00002888
Iteration 56/1000 | Loss: 0.00002888
Iteration 57/1000 | Loss: 0.00002888
Iteration 58/1000 | Loss: 0.00002888
Iteration 59/1000 | Loss: 0.00002888
Iteration 60/1000 | Loss: 0.00002888
Iteration 61/1000 | Loss: 0.00002888
Iteration 62/1000 | Loss: 0.00002888
Iteration 63/1000 | Loss: 0.00002888
Iteration 64/1000 | Loss: 0.00002888
Iteration 65/1000 | Loss: 0.00002887
Iteration 66/1000 | Loss: 0.00002887
Iteration 67/1000 | Loss: 0.00002887
Iteration 68/1000 | Loss: 0.00002887
Iteration 69/1000 | Loss: 0.00002887
Iteration 70/1000 | Loss: 0.00002886
Iteration 71/1000 | Loss: 0.00002886
Iteration 72/1000 | Loss: 0.00002886
Iteration 73/1000 | Loss: 0.00002885
Iteration 74/1000 | Loss: 0.00002885
Iteration 75/1000 | Loss: 0.00002885
Iteration 76/1000 | Loss: 0.00002885
Iteration 77/1000 | Loss: 0.00002885
Iteration 78/1000 | Loss: 0.00002885
Iteration 79/1000 | Loss: 0.00002884
Iteration 80/1000 | Loss: 0.00002884
Iteration 81/1000 | Loss: 0.00002884
Iteration 82/1000 | Loss: 0.00002884
Iteration 83/1000 | Loss: 0.00002884
Iteration 84/1000 | Loss: 0.00002884
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002883
Iteration 87/1000 | Loss: 0.00002883
Iteration 88/1000 | Loss: 0.00002883
Iteration 89/1000 | Loss: 0.00002883
Iteration 90/1000 | Loss: 0.00002883
Iteration 91/1000 | Loss: 0.00002883
Iteration 92/1000 | Loss: 0.00002883
Iteration 93/1000 | Loss: 0.00002883
Iteration 94/1000 | Loss: 0.00002883
Iteration 95/1000 | Loss: 0.00002883
Iteration 96/1000 | Loss: 0.00002882
Iteration 97/1000 | Loss: 0.00002882
Iteration 98/1000 | Loss: 0.00002882
Iteration 99/1000 | Loss: 0.00002882
Iteration 100/1000 | Loss: 0.00002882
Iteration 101/1000 | Loss: 0.00002881
Iteration 102/1000 | Loss: 0.00002881
Iteration 103/1000 | Loss: 0.00002881
Iteration 104/1000 | Loss: 0.00002881
Iteration 105/1000 | Loss: 0.00002881
Iteration 106/1000 | Loss: 0.00002881
Iteration 107/1000 | Loss: 0.00002881
Iteration 108/1000 | Loss: 0.00002881
Iteration 109/1000 | Loss: 0.00002881
Iteration 110/1000 | Loss: 0.00002881
Iteration 111/1000 | Loss: 0.00002880
Iteration 112/1000 | Loss: 0.00002880
Iteration 113/1000 | Loss: 0.00002880
Iteration 114/1000 | Loss: 0.00002879
Iteration 115/1000 | Loss: 0.00002879
Iteration 116/1000 | Loss: 0.00002879
Iteration 117/1000 | Loss: 0.00002878
Iteration 118/1000 | Loss: 0.00002878
Iteration 119/1000 | Loss: 0.00002878
Iteration 120/1000 | Loss: 0.00002878
Iteration 121/1000 | Loss: 0.00002878
Iteration 122/1000 | Loss: 0.00002877
Iteration 123/1000 | Loss: 0.00002877
Iteration 124/1000 | Loss: 0.00002877
Iteration 125/1000 | Loss: 0.00002877
Iteration 126/1000 | Loss: 0.00002877
Iteration 127/1000 | Loss: 0.00002877
Iteration 128/1000 | Loss: 0.00002877
Iteration 129/1000 | Loss: 0.00002877
Iteration 130/1000 | Loss: 0.00002877
Iteration 131/1000 | Loss: 0.00002877
Iteration 132/1000 | Loss: 0.00002877
Iteration 133/1000 | Loss: 0.00002877
Iteration 134/1000 | Loss: 0.00002877
Iteration 135/1000 | Loss: 0.00002877
Iteration 136/1000 | Loss: 0.00002876
Iteration 137/1000 | Loss: 0.00002876
Iteration 138/1000 | Loss: 0.00002876
Iteration 139/1000 | Loss: 0.00002876
Iteration 140/1000 | Loss: 0.00002876
Iteration 141/1000 | Loss: 0.00002876
Iteration 142/1000 | Loss: 0.00002876
Iteration 143/1000 | Loss: 0.00002876
Iteration 144/1000 | Loss: 0.00002875
Iteration 145/1000 | Loss: 0.00002875
Iteration 146/1000 | Loss: 0.00002875
Iteration 147/1000 | Loss: 0.00002875
Iteration 148/1000 | Loss: 0.00002875
Iteration 149/1000 | Loss: 0.00002875
Iteration 150/1000 | Loss: 0.00002875
Iteration 151/1000 | Loss: 0.00002874
Iteration 152/1000 | Loss: 0.00002874
Iteration 153/1000 | Loss: 0.00002874
Iteration 154/1000 | Loss: 0.00002874
Iteration 155/1000 | Loss: 0.00002874
Iteration 156/1000 | Loss: 0.00002874
Iteration 157/1000 | Loss: 0.00002874
Iteration 158/1000 | Loss: 0.00002874
Iteration 159/1000 | Loss: 0.00002874
Iteration 160/1000 | Loss: 0.00002874
Iteration 161/1000 | Loss: 0.00002874
Iteration 162/1000 | Loss: 0.00002874
Iteration 163/1000 | Loss: 0.00002874
Iteration 164/1000 | Loss: 0.00002874
Iteration 165/1000 | Loss: 0.00002874
Iteration 166/1000 | Loss: 0.00002874
Iteration 167/1000 | Loss: 0.00002874
Iteration 168/1000 | Loss: 0.00002874
Iteration 169/1000 | Loss: 0.00002874
Iteration 170/1000 | Loss: 0.00002874
Iteration 171/1000 | Loss: 0.00002874
Iteration 172/1000 | Loss: 0.00002874
Iteration 173/1000 | Loss: 0.00002874
Iteration 174/1000 | Loss: 0.00002874
Iteration 175/1000 | Loss: 0.00002874
Iteration 176/1000 | Loss: 0.00002874
Iteration 177/1000 | Loss: 0.00002874
Iteration 178/1000 | Loss: 0.00002874
Iteration 179/1000 | Loss: 0.00002874
Iteration 180/1000 | Loss: 0.00002874
Iteration 181/1000 | Loss: 0.00002874
Iteration 182/1000 | Loss: 0.00002874
Iteration 183/1000 | Loss: 0.00002874
Iteration 184/1000 | Loss: 0.00002874
Iteration 185/1000 | Loss: 0.00002874
Iteration 186/1000 | Loss: 0.00002874
Iteration 187/1000 | Loss: 0.00002874
Iteration 188/1000 | Loss: 0.00002874
Iteration 189/1000 | Loss: 0.00002874
Iteration 190/1000 | Loss: 0.00002874
Iteration 191/1000 | Loss: 0.00002874
Iteration 192/1000 | Loss: 0.00002874
Iteration 193/1000 | Loss: 0.00002874
Iteration 194/1000 | Loss: 0.00002874
Iteration 195/1000 | Loss: 0.00002874
Iteration 196/1000 | Loss: 0.00002874
Iteration 197/1000 | Loss: 0.00002874
Iteration 198/1000 | Loss: 0.00002874
Iteration 199/1000 | Loss: 0.00002874
Iteration 200/1000 | Loss: 0.00002874
Iteration 201/1000 | Loss: 0.00002874
Iteration 202/1000 | Loss: 0.00002874
Iteration 203/1000 | Loss: 0.00002874
Iteration 204/1000 | Loss: 0.00002874
Iteration 205/1000 | Loss: 0.00002874
Iteration 206/1000 | Loss: 0.00002874
Iteration 207/1000 | Loss: 0.00002874
Iteration 208/1000 | Loss: 0.00002874
Iteration 209/1000 | Loss: 0.00002874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.8741496862494387e-05, 2.8741496862494387e-05, 2.8741496862494387e-05, 2.8741496862494387e-05, 2.8741496862494387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8741496862494387e-05

Optimization complete. Final v2v error: 3.9260923862457275 mm

Highest mean error: 9.83505630493164 mm for frame 50

Lowest mean error: 3.1438047885894775 mm for frame 149

Saving results

Total time: 52.9049973487854
