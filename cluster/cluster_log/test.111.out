Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=111, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6216-6271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529568
Iteration 2/25 | Loss: 0.00134174
Iteration 3/25 | Loss: 0.00107033
Iteration 4/25 | Loss: 0.00102958
Iteration 5/25 | Loss: 0.00102415
Iteration 6/25 | Loss: 0.00102253
Iteration 7/25 | Loss: 0.00102245
Iteration 8/25 | Loss: 0.00102245
Iteration 9/25 | Loss: 0.00102245
Iteration 10/25 | Loss: 0.00102245
Iteration 11/25 | Loss: 0.00102245
Iteration 12/25 | Loss: 0.00102245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010224479483440518, 0.0010224479483440518, 0.0010224479483440518, 0.0010224479483440518, 0.0010224479483440518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010224479483440518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62184381
Iteration 2/25 | Loss: 0.00082966
Iteration 3/25 | Loss: 0.00082966
Iteration 4/25 | Loss: 0.00082966
Iteration 5/25 | Loss: 0.00082966
Iteration 6/25 | Loss: 0.00082966
Iteration 7/25 | Loss: 0.00082966
Iteration 8/25 | Loss: 0.00082966
Iteration 9/25 | Loss: 0.00082966
Iteration 10/25 | Loss: 0.00082966
Iteration 11/25 | Loss: 0.00082966
Iteration 12/25 | Loss: 0.00082966
Iteration 13/25 | Loss: 0.00082966
Iteration 14/25 | Loss: 0.00082966
Iteration 15/25 | Loss: 0.00082966
Iteration 16/25 | Loss: 0.00082966
Iteration 17/25 | Loss: 0.00082966
Iteration 18/25 | Loss: 0.00082966
Iteration 19/25 | Loss: 0.00082966
Iteration 20/25 | Loss: 0.00082966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008296617306768894, 0.0008296617306768894, 0.0008296617306768894, 0.0008296617306768894, 0.0008296617306768894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008296617306768894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082966
Iteration 2/1000 | Loss: 0.00004703
Iteration 3/1000 | Loss: 0.00003335
Iteration 4/1000 | Loss: 0.00002914
Iteration 5/1000 | Loss: 0.00002656
Iteration 6/1000 | Loss: 0.00002548
Iteration 7/1000 | Loss: 0.00002461
Iteration 8/1000 | Loss: 0.00002424
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002371
Iteration 11/1000 | Loss: 0.00002351
Iteration 12/1000 | Loss: 0.00002331
Iteration 13/1000 | Loss: 0.00002318
Iteration 14/1000 | Loss: 0.00002315
Iteration 15/1000 | Loss: 0.00002315
Iteration 16/1000 | Loss: 0.00002314
Iteration 17/1000 | Loss: 0.00002314
Iteration 18/1000 | Loss: 0.00002313
Iteration 19/1000 | Loss: 0.00002313
Iteration 20/1000 | Loss: 0.00002313
Iteration 21/1000 | Loss: 0.00002312
Iteration 22/1000 | Loss: 0.00002312
Iteration 23/1000 | Loss: 0.00002312
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002311
Iteration 26/1000 | Loss: 0.00002311
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002310
Iteration 29/1000 | Loss: 0.00002310
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00002309
Iteration 32/1000 | Loss: 0.00002309
Iteration 33/1000 | Loss: 0.00002309
Iteration 34/1000 | Loss: 0.00002308
Iteration 35/1000 | Loss: 0.00002308
Iteration 36/1000 | Loss: 0.00002308
Iteration 37/1000 | Loss: 0.00002307
Iteration 38/1000 | Loss: 0.00002307
Iteration 39/1000 | Loss: 0.00002307
Iteration 40/1000 | Loss: 0.00002306
Iteration 41/1000 | Loss: 0.00002306
Iteration 42/1000 | Loss: 0.00002306
Iteration 43/1000 | Loss: 0.00002306
Iteration 44/1000 | Loss: 0.00002305
Iteration 45/1000 | Loss: 0.00002305
Iteration 46/1000 | Loss: 0.00002305
Iteration 47/1000 | Loss: 0.00002305
Iteration 48/1000 | Loss: 0.00002305
Iteration 49/1000 | Loss: 0.00002305
Iteration 50/1000 | Loss: 0.00002304
Iteration 51/1000 | Loss: 0.00002304
Iteration 52/1000 | Loss: 0.00002304
Iteration 53/1000 | Loss: 0.00002304
Iteration 54/1000 | Loss: 0.00002304
Iteration 55/1000 | Loss: 0.00002304
Iteration 56/1000 | Loss: 0.00002304
Iteration 57/1000 | Loss: 0.00002304
Iteration 58/1000 | Loss: 0.00002303
Iteration 59/1000 | Loss: 0.00002303
Iteration 60/1000 | Loss: 0.00002303
Iteration 61/1000 | Loss: 0.00002303
Iteration 62/1000 | Loss: 0.00002303
Iteration 63/1000 | Loss: 0.00002303
Iteration 64/1000 | Loss: 0.00002303
Iteration 65/1000 | Loss: 0.00002303
Iteration 66/1000 | Loss: 0.00002303
Iteration 67/1000 | Loss: 0.00002303
Iteration 68/1000 | Loss: 0.00002303
Iteration 69/1000 | Loss: 0.00002302
Iteration 70/1000 | Loss: 0.00002302
Iteration 71/1000 | Loss: 0.00002302
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002301
Iteration 75/1000 | Loss: 0.00002301
Iteration 76/1000 | Loss: 0.00002301
Iteration 77/1000 | Loss: 0.00002301
Iteration 78/1000 | Loss: 0.00002301
Iteration 79/1000 | Loss: 0.00002300
Iteration 80/1000 | Loss: 0.00002300
Iteration 81/1000 | Loss: 0.00002300
Iteration 82/1000 | Loss: 0.00002300
Iteration 83/1000 | Loss: 0.00002299
Iteration 84/1000 | Loss: 0.00002299
Iteration 85/1000 | Loss: 0.00002299
Iteration 86/1000 | Loss: 0.00002299
Iteration 87/1000 | Loss: 0.00002298
Iteration 88/1000 | Loss: 0.00002298
Iteration 89/1000 | Loss: 0.00002298
Iteration 90/1000 | Loss: 0.00002298
Iteration 91/1000 | Loss: 0.00002298
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002298
Iteration 94/1000 | Loss: 0.00002298
Iteration 95/1000 | Loss: 0.00002298
Iteration 96/1000 | Loss: 0.00002298
Iteration 97/1000 | Loss: 0.00002298
Iteration 98/1000 | Loss: 0.00002297
Iteration 99/1000 | Loss: 0.00002297
Iteration 100/1000 | Loss: 0.00002297
Iteration 101/1000 | Loss: 0.00002297
Iteration 102/1000 | Loss: 0.00002296
Iteration 103/1000 | Loss: 0.00002296
Iteration 104/1000 | Loss: 0.00002296
Iteration 105/1000 | Loss: 0.00002296
Iteration 106/1000 | Loss: 0.00002296
Iteration 107/1000 | Loss: 0.00002296
Iteration 108/1000 | Loss: 0.00002296
Iteration 109/1000 | Loss: 0.00002296
Iteration 110/1000 | Loss: 0.00002296
Iteration 111/1000 | Loss: 0.00002296
Iteration 112/1000 | Loss: 0.00002296
Iteration 113/1000 | Loss: 0.00002295
Iteration 114/1000 | Loss: 0.00002295
Iteration 115/1000 | Loss: 0.00002295
Iteration 116/1000 | Loss: 0.00002295
Iteration 117/1000 | Loss: 0.00002295
Iteration 118/1000 | Loss: 0.00002295
Iteration 119/1000 | Loss: 0.00002295
Iteration 120/1000 | Loss: 0.00002295
Iteration 121/1000 | Loss: 0.00002295
Iteration 122/1000 | Loss: 0.00002294
Iteration 123/1000 | Loss: 0.00002294
Iteration 124/1000 | Loss: 0.00002294
Iteration 125/1000 | Loss: 0.00002294
Iteration 126/1000 | Loss: 0.00002294
Iteration 127/1000 | Loss: 0.00002294
Iteration 128/1000 | Loss: 0.00002294
Iteration 129/1000 | Loss: 0.00002294
Iteration 130/1000 | Loss: 0.00002293
Iteration 131/1000 | Loss: 0.00002293
Iteration 132/1000 | Loss: 0.00002292
Iteration 133/1000 | Loss: 0.00002292
Iteration 134/1000 | Loss: 0.00002292
Iteration 135/1000 | Loss: 0.00002291
Iteration 136/1000 | Loss: 0.00002291
Iteration 137/1000 | Loss: 0.00002291
Iteration 138/1000 | Loss: 0.00002291
Iteration 139/1000 | Loss: 0.00002291
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002289
Iteration 143/1000 | Loss: 0.00002289
Iteration 144/1000 | Loss: 0.00002288
Iteration 145/1000 | Loss: 0.00002288
Iteration 146/1000 | Loss: 0.00002288
Iteration 147/1000 | Loss: 0.00002288
Iteration 148/1000 | Loss: 0.00002288
Iteration 149/1000 | Loss: 0.00002288
Iteration 150/1000 | Loss: 0.00002287
Iteration 151/1000 | Loss: 0.00002287
Iteration 152/1000 | Loss: 0.00002287
Iteration 153/1000 | Loss: 0.00002287
Iteration 154/1000 | Loss: 0.00002287
Iteration 155/1000 | Loss: 0.00002286
Iteration 156/1000 | Loss: 0.00002285
Iteration 157/1000 | Loss: 0.00002285
Iteration 158/1000 | Loss: 0.00002285
Iteration 159/1000 | Loss: 0.00002285
Iteration 160/1000 | Loss: 0.00002285
Iteration 161/1000 | Loss: 0.00002284
Iteration 162/1000 | Loss: 0.00002284
Iteration 163/1000 | Loss: 0.00002284
Iteration 164/1000 | Loss: 0.00002284
Iteration 165/1000 | Loss: 0.00002284
Iteration 166/1000 | Loss: 0.00002284
Iteration 167/1000 | Loss: 0.00002283
Iteration 168/1000 | Loss: 0.00002279
Iteration 169/1000 | Loss: 0.00002279
Iteration 170/1000 | Loss: 0.00002279
Iteration 171/1000 | Loss: 0.00002279
Iteration 172/1000 | Loss: 0.00002279
Iteration 173/1000 | Loss: 0.00002279
Iteration 174/1000 | Loss: 0.00002279
Iteration 175/1000 | Loss: 0.00002278
Iteration 176/1000 | Loss: 0.00002278
Iteration 177/1000 | Loss: 0.00002278
Iteration 178/1000 | Loss: 0.00002278
Iteration 179/1000 | Loss: 0.00002278
Iteration 180/1000 | Loss: 0.00002277
Iteration 181/1000 | Loss: 0.00002277
Iteration 182/1000 | Loss: 0.00002277
Iteration 183/1000 | Loss: 0.00002276
Iteration 184/1000 | Loss: 0.00002275
Iteration 185/1000 | Loss: 0.00002275
Iteration 186/1000 | Loss: 0.00002275
Iteration 187/1000 | Loss: 0.00002275
Iteration 188/1000 | Loss: 0.00002275
Iteration 189/1000 | Loss: 0.00002275
Iteration 190/1000 | Loss: 0.00002275
Iteration 191/1000 | Loss: 0.00002274
Iteration 192/1000 | Loss: 0.00002274
Iteration 193/1000 | Loss: 0.00002274
Iteration 194/1000 | Loss: 0.00002274
Iteration 195/1000 | Loss: 0.00002274
Iteration 196/1000 | Loss: 0.00002274
Iteration 197/1000 | Loss: 0.00002273
Iteration 198/1000 | Loss: 0.00002273
Iteration 199/1000 | Loss: 0.00002273
Iteration 200/1000 | Loss: 0.00002273
Iteration 201/1000 | Loss: 0.00002273
Iteration 202/1000 | Loss: 0.00002273
Iteration 203/1000 | Loss: 0.00002273
Iteration 204/1000 | Loss: 0.00002273
Iteration 205/1000 | Loss: 0.00002273
Iteration 206/1000 | Loss: 0.00002273
Iteration 207/1000 | Loss: 0.00002273
Iteration 208/1000 | Loss: 0.00002273
Iteration 209/1000 | Loss: 0.00002273
Iteration 210/1000 | Loss: 0.00002273
Iteration 211/1000 | Loss: 0.00002273
Iteration 212/1000 | Loss: 0.00002273
Iteration 213/1000 | Loss: 0.00002273
Iteration 214/1000 | Loss: 0.00002272
Iteration 215/1000 | Loss: 0.00002272
Iteration 216/1000 | Loss: 0.00002272
Iteration 217/1000 | Loss: 0.00002272
Iteration 218/1000 | Loss: 0.00002272
Iteration 219/1000 | Loss: 0.00002272
Iteration 220/1000 | Loss: 0.00002272
Iteration 221/1000 | Loss: 0.00002272
Iteration 222/1000 | Loss: 0.00002272
Iteration 223/1000 | Loss: 0.00002272
Iteration 224/1000 | Loss: 0.00002272
Iteration 225/1000 | Loss: 0.00002271
Iteration 226/1000 | Loss: 0.00002271
Iteration 227/1000 | Loss: 0.00002271
Iteration 228/1000 | Loss: 0.00002271
Iteration 229/1000 | Loss: 0.00002270
Iteration 230/1000 | Loss: 0.00002270
Iteration 231/1000 | Loss: 0.00002270
Iteration 232/1000 | Loss: 0.00002270
Iteration 233/1000 | Loss: 0.00002270
Iteration 234/1000 | Loss: 0.00002270
Iteration 235/1000 | Loss: 0.00002270
Iteration 236/1000 | Loss: 0.00002270
Iteration 237/1000 | Loss: 0.00002270
Iteration 238/1000 | Loss: 0.00002270
Iteration 239/1000 | Loss: 0.00002269
Iteration 240/1000 | Loss: 0.00002269
Iteration 241/1000 | Loss: 0.00002269
Iteration 242/1000 | Loss: 0.00002269
Iteration 243/1000 | Loss: 0.00002269
Iteration 244/1000 | Loss: 0.00002269
Iteration 245/1000 | Loss: 0.00002269
Iteration 246/1000 | Loss: 0.00002269
Iteration 247/1000 | Loss: 0.00002269
Iteration 248/1000 | Loss: 0.00002269
Iteration 249/1000 | Loss: 0.00002268
Iteration 250/1000 | Loss: 0.00002268
Iteration 251/1000 | Loss: 0.00002268
Iteration 252/1000 | Loss: 0.00002268
Iteration 253/1000 | Loss: 0.00002268
Iteration 254/1000 | Loss: 0.00002268
Iteration 255/1000 | Loss: 0.00002268
Iteration 256/1000 | Loss: 0.00002268
Iteration 257/1000 | Loss: 0.00002268
Iteration 258/1000 | Loss: 0.00002268
Iteration 259/1000 | Loss: 0.00002268
Iteration 260/1000 | Loss: 0.00002268
Iteration 261/1000 | Loss: 0.00002268
Iteration 262/1000 | Loss: 0.00002268
Iteration 263/1000 | Loss: 0.00002268
Iteration 264/1000 | Loss: 0.00002268
Iteration 265/1000 | Loss: 0.00002268
Iteration 266/1000 | Loss: 0.00002268
Iteration 267/1000 | Loss: 0.00002268
Iteration 268/1000 | Loss: 0.00002268
Iteration 269/1000 | Loss: 0.00002268
Iteration 270/1000 | Loss: 0.00002268
Iteration 271/1000 | Loss: 0.00002268
Iteration 272/1000 | Loss: 0.00002268
Iteration 273/1000 | Loss: 0.00002268
Iteration 274/1000 | Loss: 0.00002268
Iteration 275/1000 | Loss: 0.00002268
Iteration 276/1000 | Loss: 0.00002268
Iteration 277/1000 | Loss: 0.00002268
Iteration 278/1000 | Loss: 0.00002268
Iteration 279/1000 | Loss: 0.00002268
Iteration 280/1000 | Loss: 0.00002268
Iteration 281/1000 | Loss: 0.00002268
Iteration 282/1000 | Loss: 0.00002268
Iteration 283/1000 | Loss: 0.00002268
Iteration 284/1000 | Loss: 0.00002268
Iteration 285/1000 | Loss: 0.00002268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.2680922484141774e-05, 2.2680922484141774e-05, 2.2680922484141774e-05, 2.2680922484141774e-05, 2.2680922484141774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2680922484141774e-05

Optimization complete. Final v2v error: 4.143439292907715 mm

Highest mean error: 4.389902114868164 mm for frame 110

Lowest mean error: 3.795905351638794 mm for frame 6

Saving results

Total time: 45.78782844543457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389460
Iteration 2/25 | Loss: 0.00099610
Iteration 3/25 | Loss: 0.00093503
Iteration 4/25 | Loss: 0.00092807
Iteration 5/25 | Loss: 0.00092584
Iteration 6/25 | Loss: 0.00092506
Iteration 7/25 | Loss: 0.00092506
Iteration 8/25 | Loss: 0.00092506
Iteration 9/25 | Loss: 0.00092506
Iteration 10/25 | Loss: 0.00092506
Iteration 11/25 | Loss: 0.00092506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009250562288798392, 0.0009250562288798392, 0.0009250562288798392, 0.0009250562288798392, 0.0009250562288798392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009250562288798392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.17519760
Iteration 2/25 | Loss: 0.00118791
Iteration 3/25 | Loss: 0.00118791
Iteration 4/25 | Loss: 0.00118791
Iteration 5/25 | Loss: 0.00118791
Iteration 6/25 | Loss: 0.00118791
Iteration 7/25 | Loss: 0.00118791
Iteration 8/25 | Loss: 0.00118791
Iteration 9/25 | Loss: 0.00118791
Iteration 10/25 | Loss: 0.00118791
Iteration 11/25 | Loss: 0.00118791
Iteration 12/25 | Loss: 0.00118791
Iteration 13/25 | Loss: 0.00118791
Iteration 14/25 | Loss: 0.00118791
Iteration 15/25 | Loss: 0.00118791
Iteration 16/25 | Loss: 0.00118791
Iteration 17/25 | Loss: 0.00118791
Iteration 18/25 | Loss: 0.00118791
Iteration 19/25 | Loss: 0.00118791
Iteration 20/25 | Loss: 0.00118791
Iteration 21/25 | Loss: 0.00118791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011879083467647433, 0.0011879083467647433, 0.0011879083467647433, 0.0011879083467647433, 0.0011879083467647433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011879083467647433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118791
Iteration 2/1000 | Loss: 0.00003495
Iteration 3/1000 | Loss: 0.00002483
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001764
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001461
Iteration 14/1000 | Loss: 0.00001458
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001450
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001448
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001441
Iteration 36/1000 | Loss: 0.00001441
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001434
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001422
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001421
Iteration 65/1000 | Loss: 0.00001418
Iteration 66/1000 | Loss: 0.00001417
Iteration 67/1000 | Loss: 0.00001416
Iteration 68/1000 | Loss: 0.00001416
Iteration 69/1000 | Loss: 0.00001416
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001415
Iteration 72/1000 | Loss: 0.00001415
Iteration 73/1000 | Loss: 0.00001415
Iteration 74/1000 | Loss: 0.00001415
Iteration 75/1000 | Loss: 0.00001414
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001411
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001410
Iteration 102/1000 | Loss: 0.00001410
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Iteration 105/1000 | Loss: 0.00001409
Iteration 106/1000 | Loss: 0.00001409
Iteration 107/1000 | Loss: 0.00001409
Iteration 108/1000 | Loss: 0.00001409
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001408
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001408
Iteration 117/1000 | Loss: 0.00001408
Iteration 118/1000 | Loss: 0.00001408
Iteration 119/1000 | Loss: 0.00001408
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001408
Iteration 122/1000 | Loss: 0.00001408
Iteration 123/1000 | Loss: 0.00001408
Iteration 124/1000 | Loss: 0.00001408
Iteration 125/1000 | Loss: 0.00001408
Iteration 126/1000 | Loss: 0.00001408
Iteration 127/1000 | Loss: 0.00001408
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001408
Iteration 130/1000 | Loss: 0.00001408
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001408
Iteration 133/1000 | Loss: 0.00001408
Iteration 134/1000 | Loss: 0.00001408
Iteration 135/1000 | Loss: 0.00001408
Iteration 136/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.4075942999625113e-05, 1.4075942999625113e-05, 1.4075942999625113e-05, 1.4075942999625113e-05, 1.4075942999625113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4075942999625113e-05

Optimization complete. Final v2v error: 3.2298269271850586 mm

Highest mean error: 3.8906030654907227 mm for frame 45

Lowest mean error: 2.7119319438934326 mm for frame 148

Saving results

Total time: 34.99456214904785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838164
Iteration 2/25 | Loss: 0.00120625
Iteration 3/25 | Loss: 0.00101463
Iteration 4/25 | Loss: 0.00098450
Iteration 5/25 | Loss: 0.00098016
Iteration 6/25 | Loss: 0.00097920
Iteration 7/25 | Loss: 0.00097914
Iteration 8/25 | Loss: 0.00097914
Iteration 9/25 | Loss: 0.00097914
Iteration 10/25 | Loss: 0.00097914
Iteration 11/25 | Loss: 0.00097914
Iteration 12/25 | Loss: 0.00097914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009791351621970534, 0.0009791351621970534, 0.0009791351621970534, 0.0009791351621970534, 0.0009791351621970534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009791351621970534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16074824
Iteration 2/25 | Loss: 0.00095107
Iteration 3/25 | Loss: 0.00095103
Iteration 4/25 | Loss: 0.00095103
Iteration 5/25 | Loss: 0.00095103
Iteration 6/25 | Loss: 0.00095103
Iteration 7/25 | Loss: 0.00095103
Iteration 8/25 | Loss: 0.00095103
Iteration 9/25 | Loss: 0.00095103
Iteration 10/25 | Loss: 0.00095103
Iteration 11/25 | Loss: 0.00095103
Iteration 12/25 | Loss: 0.00095103
Iteration 13/25 | Loss: 0.00095103
Iteration 14/25 | Loss: 0.00095103
Iteration 15/25 | Loss: 0.00095103
Iteration 16/25 | Loss: 0.00095103
Iteration 17/25 | Loss: 0.00095103
Iteration 18/25 | Loss: 0.00095103
Iteration 19/25 | Loss: 0.00095103
Iteration 20/25 | Loss: 0.00095103
Iteration 21/25 | Loss: 0.00095103
Iteration 22/25 | Loss: 0.00095103
Iteration 23/25 | Loss: 0.00095103
Iteration 24/25 | Loss: 0.00095103
Iteration 25/25 | Loss: 0.00095103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095103
Iteration 2/1000 | Loss: 0.00004929
Iteration 3/1000 | Loss: 0.00003493
Iteration 4/1000 | Loss: 0.00002576
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002036
Iteration 8/1000 | Loss: 0.00001936
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001824
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001760
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001753
Iteration 20/1000 | Loss: 0.00001753
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001741
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001738
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001737
Iteration 46/1000 | Loss: 0.00001737
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001736
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001731
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001728
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001728
Iteration 88/1000 | Loss: 0.00001728
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001722
Iteration 113/1000 | Loss: 0.00001722
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001722
Iteration 118/1000 | Loss: 0.00001722
Iteration 119/1000 | Loss: 0.00001722
Iteration 120/1000 | Loss: 0.00001721
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001720
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001719
Iteration 147/1000 | Loss: 0.00001719
Iteration 148/1000 | Loss: 0.00001719
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001719
Iteration 151/1000 | Loss: 0.00001719
Iteration 152/1000 | Loss: 0.00001719
Iteration 153/1000 | Loss: 0.00001719
Iteration 154/1000 | Loss: 0.00001719
Iteration 155/1000 | Loss: 0.00001719
Iteration 156/1000 | Loss: 0.00001719
Iteration 157/1000 | Loss: 0.00001719
Iteration 158/1000 | Loss: 0.00001718
Iteration 159/1000 | Loss: 0.00001718
Iteration 160/1000 | Loss: 0.00001718
Iteration 161/1000 | Loss: 0.00001718
Iteration 162/1000 | Loss: 0.00001718
Iteration 163/1000 | Loss: 0.00001718
Iteration 164/1000 | Loss: 0.00001718
Iteration 165/1000 | Loss: 0.00001718
Iteration 166/1000 | Loss: 0.00001718
Iteration 167/1000 | Loss: 0.00001717
Iteration 168/1000 | Loss: 0.00001717
Iteration 169/1000 | Loss: 0.00001717
Iteration 170/1000 | Loss: 0.00001717
Iteration 171/1000 | Loss: 0.00001717
Iteration 172/1000 | Loss: 0.00001717
Iteration 173/1000 | Loss: 0.00001717
Iteration 174/1000 | Loss: 0.00001717
Iteration 175/1000 | Loss: 0.00001717
Iteration 176/1000 | Loss: 0.00001717
Iteration 177/1000 | Loss: 0.00001717
Iteration 178/1000 | Loss: 0.00001717
Iteration 179/1000 | Loss: 0.00001716
Iteration 180/1000 | Loss: 0.00001716
Iteration 181/1000 | Loss: 0.00001716
Iteration 182/1000 | Loss: 0.00001716
Iteration 183/1000 | Loss: 0.00001716
Iteration 184/1000 | Loss: 0.00001715
Iteration 185/1000 | Loss: 0.00001715
Iteration 186/1000 | Loss: 0.00001715
Iteration 187/1000 | Loss: 0.00001715
Iteration 188/1000 | Loss: 0.00001714
Iteration 189/1000 | Loss: 0.00001714
Iteration 190/1000 | Loss: 0.00001714
Iteration 191/1000 | Loss: 0.00001714
Iteration 192/1000 | Loss: 0.00001714
Iteration 193/1000 | Loss: 0.00001713
Iteration 194/1000 | Loss: 0.00001713
Iteration 195/1000 | Loss: 0.00001713
Iteration 196/1000 | Loss: 0.00001713
Iteration 197/1000 | Loss: 0.00001713
Iteration 198/1000 | Loss: 0.00001713
Iteration 199/1000 | Loss: 0.00001713
Iteration 200/1000 | Loss: 0.00001713
Iteration 201/1000 | Loss: 0.00001713
Iteration 202/1000 | Loss: 0.00001713
Iteration 203/1000 | Loss: 0.00001712
Iteration 204/1000 | Loss: 0.00001712
Iteration 205/1000 | Loss: 0.00001712
Iteration 206/1000 | Loss: 0.00001712
Iteration 207/1000 | Loss: 0.00001712
Iteration 208/1000 | Loss: 0.00001712
Iteration 209/1000 | Loss: 0.00001712
Iteration 210/1000 | Loss: 0.00001712
Iteration 211/1000 | Loss: 0.00001712
Iteration 212/1000 | Loss: 0.00001712
Iteration 213/1000 | Loss: 0.00001712
Iteration 214/1000 | Loss: 0.00001712
Iteration 215/1000 | Loss: 0.00001712
Iteration 216/1000 | Loss: 0.00001712
Iteration 217/1000 | Loss: 0.00001712
Iteration 218/1000 | Loss: 0.00001711
Iteration 219/1000 | Loss: 0.00001711
Iteration 220/1000 | Loss: 0.00001711
Iteration 221/1000 | Loss: 0.00001711
Iteration 222/1000 | Loss: 0.00001711
Iteration 223/1000 | Loss: 0.00001711
Iteration 224/1000 | Loss: 0.00001711
Iteration 225/1000 | Loss: 0.00001711
Iteration 226/1000 | Loss: 0.00001711
Iteration 227/1000 | Loss: 0.00001711
Iteration 228/1000 | Loss: 0.00001711
Iteration 229/1000 | Loss: 0.00001711
Iteration 230/1000 | Loss: 0.00001711
Iteration 231/1000 | Loss: 0.00001711
Iteration 232/1000 | Loss: 0.00001711
Iteration 233/1000 | Loss: 0.00001711
Iteration 234/1000 | Loss: 0.00001711
Iteration 235/1000 | Loss: 0.00001710
Iteration 236/1000 | Loss: 0.00001710
Iteration 237/1000 | Loss: 0.00001710
Iteration 238/1000 | Loss: 0.00001710
Iteration 239/1000 | Loss: 0.00001710
Iteration 240/1000 | Loss: 0.00001709
Iteration 241/1000 | Loss: 0.00001709
Iteration 242/1000 | Loss: 0.00001709
Iteration 243/1000 | Loss: 0.00001709
Iteration 244/1000 | Loss: 0.00001709
Iteration 245/1000 | Loss: 0.00001709
Iteration 246/1000 | Loss: 0.00001709
Iteration 247/1000 | Loss: 0.00001709
Iteration 248/1000 | Loss: 0.00001709
Iteration 249/1000 | Loss: 0.00001709
Iteration 250/1000 | Loss: 0.00001709
Iteration 251/1000 | Loss: 0.00001709
Iteration 252/1000 | Loss: 0.00001709
Iteration 253/1000 | Loss: 0.00001709
Iteration 254/1000 | Loss: 0.00001709
Iteration 255/1000 | Loss: 0.00001709
Iteration 256/1000 | Loss: 0.00001709
Iteration 257/1000 | Loss: 0.00001709
Iteration 258/1000 | Loss: 0.00001709
Iteration 259/1000 | Loss: 0.00001709
Iteration 260/1000 | Loss: 0.00001709
Iteration 261/1000 | Loss: 0.00001709
Iteration 262/1000 | Loss: 0.00001709
Iteration 263/1000 | Loss: 0.00001709
Iteration 264/1000 | Loss: 0.00001709
Iteration 265/1000 | Loss: 0.00001709
Iteration 266/1000 | Loss: 0.00001709
Iteration 267/1000 | Loss: 0.00001709
Iteration 268/1000 | Loss: 0.00001709
Iteration 269/1000 | Loss: 0.00001709
Iteration 270/1000 | Loss: 0.00001709
Iteration 271/1000 | Loss: 0.00001709
Iteration 272/1000 | Loss: 0.00001709
Iteration 273/1000 | Loss: 0.00001709
Iteration 274/1000 | Loss: 0.00001709
Iteration 275/1000 | Loss: 0.00001709
Iteration 276/1000 | Loss: 0.00001709
Iteration 277/1000 | Loss: 0.00001709
Iteration 278/1000 | Loss: 0.00001709
Iteration 279/1000 | Loss: 0.00001709
Iteration 280/1000 | Loss: 0.00001709
Iteration 281/1000 | Loss: 0.00001709
Iteration 282/1000 | Loss: 0.00001709
Iteration 283/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [1.7090829715016298e-05, 1.7090829715016298e-05, 1.7090829715016298e-05, 1.7090829715016298e-05, 1.7090829715016298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7090829715016298e-05

Optimization complete. Final v2v error: 3.607327461242676 mm

Highest mean error: 4.118663787841797 mm for frame 93

Lowest mean error: 3.1588244438171387 mm for frame 170

Saving results

Total time: 48.80914354324341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946343
Iteration 2/25 | Loss: 0.00117088
Iteration 3/25 | Loss: 0.00102409
Iteration 4/25 | Loss: 0.00099512
Iteration 5/25 | Loss: 0.00098299
Iteration 6/25 | Loss: 0.00097938
Iteration 7/25 | Loss: 0.00097864
Iteration 8/25 | Loss: 0.00097864
Iteration 9/25 | Loss: 0.00097864
Iteration 10/25 | Loss: 0.00097864
Iteration 11/25 | Loss: 0.00097864
Iteration 12/25 | Loss: 0.00097864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000978637719526887, 0.000978637719526887, 0.000978637719526887, 0.000978637719526887, 0.000978637719526887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978637719526887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29376817
Iteration 2/25 | Loss: 0.00117588
Iteration 3/25 | Loss: 0.00117588
Iteration 4/25 | Loss: 0.00117588
Iteration 5/25 | Loss: 0.00117588
Iteration 6/25 | Loss: 0.00117588
Iteration 7/25 | Loss: 0.00117588
Iteration 8/25 | Loss: 0.00117588
Iteration 9/25 | Loss: 0.00117588
Iteration 10/25 | Loss: 0.00117588
Iteration 11/25 | Loss: 0.00117588
Iteration 12/25 | Loss: 0.00117588
Iteration 13/25 | Loss: 0.00117588
Iteration 14/25 | Loss: 0.00117588
Iteration 15/25 | Loss: 0.00117588
Iteration 16/25 | Loss: 0.00117588
Iteration 17/25 | Loss: 0.00117588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011758797336369753, 0.0011758797336369753, 0.0011758797336369753, 0.0011758797336369753, 0.0011758797336369753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011758797336369753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117588
Iteration 2/1000 | Loss: 0.00006644
Iteration 3/1000 | Loss: 0.00004654
Iteration 4/1000 | Loss: 0.00003639
Iteration 5/1000 | Loss: 0.00003213
Iteration 6/1000 | Loss: 0.00002930
Iteration 7/1000 | Loss: 0.00002787
Iteration 8/1000 | Loss: 0.00002669
Iteration 9/1000 | Loss: 0.00002599
Iteration 10/1000 | Loss: 0.00002549
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00002492
Iteration 13/1000 | Loss: 0.00002469
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002454
Iteration 16/1000 | Loss: 0.00002454
Iteration 17/1000 | Loss: 0.00002454
Iteration 18/1000 | Loss: 0.00002443
Iteration 19/1000 | Loss: 0.00002442
Iteration 20/1000 | Loss: 0.00002442
Iteration 21/1000 | Loss: 0.00002442
Iteration 22/1000 | Loss: 0.00002441
Iteration 23/1000 | Loss: 0.00002441
Iteration 24/1000 | Loss: 0.00002440
Iteration 25/1000 | Loss: 0.00002438
Iteration 26/1000 | Loss: 0.00002437
Iteration 27/1000 | Loss: 0.00002437
Iteration 28/1000 | Loss: 0.00002437
Iteration 29/1000 | Loss: 0.00002437
Iteration 30/1000 | Loss: 0.00002437
Iteration 31/1000 | Loss: 0.00002436
Iteration 32/1000 | Loss: 0.00002436
Iteration 33/1000 | Loss: 0.00002436
Iteration 34/1000 | Loss: 0.00002436
Iteration 35/1000 | Loss: 0.00002435
Iteration 36/1000 | Loss: 0.00002435
Iteration 37/1000 | Loss: 0.00002435
Iteration 38/1000 | Loss: 0.00002434
Iteration 39/1000 | Loss: 0.00002433
Iteration 40/1000 | Loss: 0.00002432
Iteration 41/1000 | Loss: 0.00002432
Iteration 42/1000 | Loss: 0.00002432
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002431
Iteration 45/1000 | Loss: 0.00002428
Iteration 46/1000 | Loss: 0.00002428
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00002427
Iteration 49/1000 | Loss: 0.00002427
Iteration 50/1000 | Loss: 0.00002426
Iteration 51/1000 | Loss: 0.00002426
Iteration 52/1000 | Loss: 0.00002426
Iteration 53/1000 | Loss: 0.00002424
Iteration 54/1000 | Loss: 0.00002424
Iteration 55/1000 | Loss: 0.00002424
Iteration 56/1000 | Loss: 0.00002424
Iteration 57/1000 | Loss: 0.00002424
Iteration 58/1000 | Loss: 0.00002424
Iteration 59/1000 | Loss: 0.00002424
Iteration 60/1000 | Loss: 0.00002424
Iteration 61/1000 | Loss: 0.00002423
Iteration 62/1000 | Loss: 0.00002423
Iteration 63/1000 | Loss: 0.00002423
Iteration 64/1000 | Loss: 0.00002423
Iteration 65/1000 | Loss: 0.00002423
Iteration 66/1000 | Loss: 0.00002423
Iteration 67/1000 | Loss: 0.00002423
Iteration 68/1000 | Loss: 0.00002422
Iteration 69/1000 | Loss: 0.00002422
Iteration 70/1000 | Loss: 0.00002422
Iteration 71/1000 | Loss: 0.00002420
Iteration 72/1000 | Loss: 0.00002420
Iteration 73/1000 | Loss: 0.00002420
Iteration 74/1000 | Loss: 0.00002420
Iteration 75/1000 | Loss: 0.00002420
Iteration 76/1000 | Loss: 0.00002420
Iteration 77/1000 | Loss: 0.00002420
Iteration 78/1000 | Loss: 0.00002420
Iteration 79/1000 | Loss: 0.00002419
Iteration 80/1000 | Loss: 0.00002419
Iteration 81/1000 | Loss: 0.00002419
Iteration 82/1000 | Loss: 0.00002419
Iteration 83/1000 | Loss: 0.00002418
Iteration 84/1000 | Loss: 0.00002418
Iteration 85/1000 | Loss: 0.00002418
Iteration 86/1000 | Loss: 0.00002417
Iteration 87/1000 | Loss: 0.00002417
Iteration 88/1000 | Loss: 0.00002417
Iteration 89/1000 | Loss: 0.00002417
Iteration 90/1000 | Loss: 0.00002417
Iteration 91/1000 | Loss: 0.00002417
Iteration 92/1000 | Loss: 0.00002417
Iteration 93/1000 | Loss: 0.00002417
Iteration 94/1000 | Loss: 0.00002417
Iteration 95/1000 | Loss: 0.00002416
Iteration 96/1000 | Loss: 0.00002416
Iteration 97/1000 | Loss: 0.00002416
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002416
Iteration 100/1000 | Loss: 0.00002416
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002415
Iteration 104/1000 | Loss: 0.00002415
Iteration 105/1000 | Loss: 0.00002415
Iteration 106/1000 | Loss: 0.00002414
Iteration 107/1000 | Loss: 0.00002414
Iteration 108/1000 | Loss: 0.00002414
Iteration 109/1000 | Loss: 0.00002414
Iteration 110/1000 | Loss: 0.00002414
Iteration 111/1000 | Loss: 0.00002414
Iteration 112/1000 | Loss: 0.00002414
Iteration 113/1000 | Loss: 0.00002414
Iteration 114/1000 | Loss: 0.00002414
Iteration 115/1000 | Loss: 0.00002414
Iteration 116/1000 | Loss: 0.00002414
Iteration 117/1000 | Loss: 0.00002414
Iteration 118/1000 | Loss: 0.00002414
Iteration 119/1000 | Loss: 0.00002414
Iteration 120/1000 | Loss: 0.00002414
Iteration 121/1000 | Loss: 0.00002414
Iteration 122/1000 | Loss: 0.00002414
Iteration 123/1000 | Loss: 0.00002414
Iteration 124/1000 | Loss: 0.00002413
Iteration 125/1000 | Loss: 0.00002413
Iteration 126/1000 | Loss: 0.00002413
Iteration 127/1000 | Loss: 0.00002413
Iteration 128/1000 | Loss: 0.00002413
Iteration 129/1000 | Loss: 0.00002413
Iteration 130/1000 | Loss: 0.00002413
Iteration 131/1000 | Loss: 0.00002413
Iteration 132/1000 | Loss: 0.00002413
Iteration 133/1000 | Loss: 0.00002412
Iteration 134/1000 | Loss: 0.00002412
Iteration 135/1000 | Loss: 0.00002412
Iteration 136/1000 | Loss: 0.00002412
Iteration 137/1000 | Loss: 0.00002412
Iteration 138/1000 | Loss: 0.00002412
Iteration 139/1000 | Loss: 0.00002412
Iteration 140/1000 | Loss: 0.00002412
Iteration 141/1000 | Loss: 0.00002412
Iteration 142/1000 | Loss: 0.00002412
Iteration 143/1000 | Loss: 0.00002412
Iteration 144/1000 | Loss: 0.00002412
Iteration 145/1000 | Loss: 0.00002412
Iteration 146/1000 | Loss: 0.00002412
Iteration 147/1000 | Loss: 0.00002412
Iteration 148/1000 | Loss: 0.00002412
Iteration 149/1000 | Loss: 0.00002412
Iteration 150/1000 | Loss: 0.00002412
Iteration 151/1000 | Loss: 0.00002412
Iteration 152/1000 | Loss: 0.00002412
Iteration 153/1000 | Loss: 0.00002412
Iteration 154/1000 | Loss: 0.00002412
Iteration 155/1000 | Loss: 0.00002412
Iteration 156/1000 | Loss: 0.00002412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.4122922695823945e-05, 2.4122922695823945e-05, 2.4122922695823945e-05, 2.4122922695823945e-05, 2.4122922695823945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4122922695823945e-05

Optimization complete. Final v2v error: 4.021239280700684 mm

Highest mean error: 6.214793682098389 mm for frame 69

Lowest mean error: 3.1700239181518555 mm for frame 141

Saving results

Total time: 40.18594932556152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00701999
Iteration 2/25 | Loss: 0.00150815
Iteration 3/25 | Loss: 0.00121060
Iteration 4/25 | Loss: 0.00115178
Iteration 5/25 | Loss: 0.00115698
Iteration 6/25 | Loss: 0.00114207
Iteration 7/25 | Loss: 0.00116371
Iteration 8/25 | Loss: 0.00111201
Iteration 9/25 | Loss: 0.00115113
Iteration 10/25 | Loss: 0.00106955
Iteration 11/25 | Loss: 0.00106771
Iteration 12/25 | Loss: 0.00107240
Iteration 13/25 | Loss: 0.00106859
Iteration 14/25 | Loss: 0.00106493
Iteration 15/25 | Loss: 0.00106436
Iteration 16/25 | Loss: 0.00106426
Iteration 17/25 | Loss: 0.00106426
Iteration 18/25 | Loss: 0.00106426
Iteration 19/25 | Loss: 0.00106426
Iteration 20/25 | Loss: 0.00106426
Iteration 21/25 | Loss: 0.00106425
Iteration 22/25 | Loss: 0.00106425
Iteration 23/25 | Loss: 0.00106425
Iteration 24/25 | Loss: 0.00106425
Iteration 25/25 | Loss: 0.00106425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15258908
Iteration 2/25 | Loss: 0.00105667
Iteration 3/25 | Loss: 0.00105663
Iteration 4/25 | Loss: 0.00105663
Iteration 5/25 | Loss: 0.00105663
Iteration 6/25 | Loss: 0.00105663
Iteration 7/25 | Loss: 0.00105663
Iteration 8/25 | Loss: 0.00105663
Iteration 9/25 | Loss: 0.00105663
Iteration 10/25 | Loss: 0.00105663
Iteration 11/25 | Loss: 0.00105663
Iteration 12/25 | Loss: 0.00105663
Iteration 13/25 | Loss: 0.00105663
Iteration 14/25 | Loss: 0.00105663
Iteration 15/25 | Loss: 0.00105663
Iteration 16/25 | Loss: 0.00105663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010566296987235546, 0.0010566296987235546, 0.0010566296987235546, 0.0010566296987235546, 0.0010566296987235546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010566296987235546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105663
Iteration 2/1000 | Loss: 0.00004705
Iteration 3/1000 | Loss: 0.00003428
Iteration 4/1000 | Loss: 0.00002808
Iteration 5/1000 | Loss: 0.00002606
Iteration 6/1000 | Loss: 0.00002473
Iteration 7/1000 | Loss: 0.00002354
Iteration 8/1000 | Loss: 0.00002278
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00002190
Iteration 11/1000 | Loss: 0.00012637
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002205
Iteration 14/1000 | Loss: 0.00002104
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00001981
Iteration 17/1000 | Loss: 0.00001942
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001919
Iteration 21/1000 | Loss: 0.00001907
Iteration 22/1000 | Loss: 0.00001902
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001897
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001865
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001860
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001852
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001848
Iteration 37/1000 | Loss: 0.00001848
Iteration 38/1000 | Loss: 0.00001847
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001835
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001816
Iteration 47/1000 | Loss: 0.00001816
Iteration 48/1000 | Loss: 0.00001816
Iteration 49/1000 | Loss: 0.00001815
Iteration 50/1000 | Loss: 0.00001815
Iteration 51/1000 | Loss: 0.00001815
Iteration 52/1000 | Loss: 0.00001812
Iteration 53/1000 | Loss: 0.00001812
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001807
Iteration 60/1000 | Loss: 0.00001807
Iteration 61/1000 | Loss: 0.00001807
Iteration 62/1000 | Loss: 0.00001807
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001806
Iteration 68/1000 | Loss: 0.00001806
Iteration 69/1000 | Loss: 0.00001806
Iteration 70/1000 | Loss: 0.00001806
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001805
Iteration 74/1000 | Loss: 0.00001805
Iteration 75/1000 | Loss: 0.00001805
Iteration 76/1000 | Loss: 0.00001805
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001804
Iteration 80/1000 | Loss: 0.00001804
Iteration 81/1000 | Loss: 0.00001804
Iteration 82/1000 | Loss: 0.00001804
Iteration 83/1000 | Loss: 0.00001804
Iteration 84/1000 | Loss: 0.00001804
Iteration 85/1000 | Loss: 0.00001804
Iteration 86/1000 | Loss: 0.00001804
Iteration 87/1000 | Loss: 0.00001804
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001803
Iteration 92/1000 | Loss: 0.00001803
Iteration 93/1000 | Loss: 0.00001803
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001803
Iteration 96/1000 | Loss: 0.00001803
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001803
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001802
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001801
Iteration 107/1000 | Loss: 0.00001801
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001801
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001800
Iteration 126/1000 | Loss: 0.00001800
Iteration 127/1000 | Loss: 0.00001800
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001799
Iteration 134/1000 | Loss: 0.00001799
Iteration 135/1000 | Loss: 0.00001799
Iteration 136/1000 | Loss: 0.00001799
Iteration 137/1000 | Loss: 0.00001799
Iteration 138/1000 | Loss: 0.00001799
Iteration 139/1000 | Loss: 0.00001799
Iteration 140/1000 | Loss: 0.00001799
Iteration 141/1000 | Loss: 0.00001799
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.798732773750089e-05, 1.798732773750089e-05, 1.798732773750089e-05, 1.798732773750089e-05, 1.798732773750089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.798732773750089e-05

Optimization complete. Final v2v error: 3.65746808052063 mm

Highest mean error: 4.758789539337158 mm for frame 18

Lowest mean error: 3.151738405227661 mm for frame 134

Saving results

Total time: 71.88808250427246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856122
Iteration 2/25 | Loss: 0.00126137
Iteration 3/25 | Loss: 0.00101328
Iteration 4/25 | Loss: 0.00097428
Iteration 5/25 | Loss: 0.00096662
Iteration 6/25 | Loss: 0.00096528
Iteration 7/25 | Loss: 0.00096511
Iteration 8/25 | Loss: 0.00096511
Iteration 9/25 | Loss: 0.00096511
Iteration 10/25 | Loss: 0.00096511
Iteration 11/25 | Loss: 0.00096511
Iteration 12/25 | Loss: 0.00096511
Iteration 13/25 | Loss: 0.00096511
Iteration 14/25 | Loss: 0.00096511
Iteration 15/25 | Loss: 0.00096511
Iteration 16/25 | Loss: 0.00096511
Iteration 17/25 | Loss: 0.00096511
Iteration 18/25 | Loss: 0.00096511
Iteration 19/25 | Loss: 0.00096511
Iteration 20/25 | Loss: 0.00096511
Iteration 21/25 | Loss: 0.00096511
Iteration 22/25 | Loss: 0.00096511
Iteration 23/25 | Loss: 0.00096511
Iteration 24/25 | Loss: 0.00096511
Iteration 25/25 | Loss: 0.00096511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87548316
Iteration 2/25 | Loss: 0.00070703
Iteration 3/25 | Loss: 0.00070702
Iteration 4/25 | Loss: 0.00070702
Iteration 5/25 | Loss: 0.00070702
Iteration 6/25 | Loss: 0.00070702
Iteration 7/25 | Loss: 0.00070702
Iteration 8/25 | Loss: 0.00070702
Iteration 9/25 | Loss: 0.00070702
Iteration 10/25 | Loss: 0.00070702
Iteration 11/25 | Loss: 0.00070702
Iteration 12/25 | Loss: 0.00070702
Iteration 13/25 | Loss: 0.00070702
Iteration 14/25 | Loss: 0.00070702
Iteration 15/25 | Loss: 0.00070702
Iteration 16/25 | Loss: 0.00070702
Iteration 17/25 | Loss: 0.00070702
Iteration 18/25 | Loss: 0.00070702
Iteration 19/25 | Loss: 0.00070702
Iteration 20/25 | Loss: 0.00070702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007070166757330298, 0.0007070166757330298, 0.0007070166757330298, 0.0007070166757330298, 0.0007070166757330298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007070166757330298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070702
Iteration 2/1000 | Loss: 0.00006463
Iteration 3/1000 | Loss: 0.00004842
Iteration 4/1000 | Loss: 0.00003857
Iteration 5/1000 | Loss: 0.00003341
Iteration 6/1000 | Loss: 0.00003000
Iteration 7/1000 | Loss: 0.00002843
Iteration 8/1000 | Loss: 0.00002728
Iteration 9/1000 | Loss: 0.00002671
Iteration 10/1000 | Loss: 0.00002632
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00002574
Iteration 13/1000 | Loss: 0.00002558
Iteration 14/1000 | Loss: 0.00002557
Iteration 15/1000 | Loss: 0.00002554
Iteration 16/1000 | Loss: 0.00002551
Iteration 17/1000 | Loss: 0.00002547
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002542
Iteration 20/1000 | Loss: 0.00002539
Iteration 21/1000 | Loss: 0.00002539
Iteration 22/1000 | Loss: 0.00002538
Iteration 23/1000 | Loss: 0.00002538
Iteration 24/1000 | Loss: 0.00002538
Iteration 25/1000 | Loss: 0.00002537
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002537
Iteration 28/1000 | Loss: 0.00002537
Iteration 29/1000 | Loss: 0.00002537
Iteration 30/1000 | Loss: 0.00002537
Iteration 31/1000 | Loss: 0.00002536
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002536
Iteration 34/1000 | Loss: 0.00002536
Iteration 35/1000 | Loss: 0.00002536
Iteration 36/1000 | Loss: 0.00002536
Iteration 37/1000 | Loss: 0.00002536
Iteration 38/1000 | Loss: 0.00002536
Iteration 39/1000 | Loss: 0.00002536
Iteration 40/1000 | Loss: 0.00002535
Iteration 41/1000 | Loss: 0.00002535
Iteration 42/1000 | Loss: 0.00002535
Iteration 43/1000 | Loss: 0.00002535
Iteration 44/1000 | Loss: 0.00002535
Iteration 45/1000 | Loss: 0.00002535
Iteration 46/1000 | Loss: 0.00002534
Iteration 47/1000 | Loss: 0.00002534
Iteration 48/1000 | Loss: 0.00002534
Iteration 49/1000 | Loss: 0.00002534
Iteration 50/1000 | Loss: 0.00002534
Iteration 51/1000 | Loss: 0.00002534
Iteration 52/1000 | Loss: 0.00002533
Iteration 53/1000 | Loss: 0.00002533
Iteration 54/1000 | Loss: 0.00002533
Iteration 55/1000 | Loss: 0.00002533
Iteration 56/1000 | Loss: 0.00002533
Iteration 57/1000 | Loss: 0.00002532
Iteration 58/1000 | Loss: 0.00002532
Iteration 59/1000 | Loss: 0.00002532
Iteration 60/1000 | Loss: 0.00002532
Iteration 61/1000 | Loss: 0.00002532
Iteration 62/1000 | Loss: 0.00002532
Iteration 63/1000 | Loss: 0.00002532
Iteration 64/1000 | Loss: 0.00002532
Iteration 65/1000 | Loss: 0.00002532
Iteration 66/1000 | Loss: 0.00002532
Iteration 67/1000 | Loss: 0.00002532
Iteration 68/1000 | Loss: 0.00002531
Iteration 69/1000 | Loss: 0.00002531
Iteration 70/1000 | Loss: 0.00002531
Iteration 71/1000 | Loss: 0.00002531
Iteration 72/1000 | Loss: 0.00002531
Iteration 73/1000 | Loss: 0.00002531
Iteration 74/1000 | Loss: 0.00002530
Iteration 75/1000 | Loss: 0.00002530
Iteration 76/1000 | Loss: 0.00002530
Iteration 77/1000 | Loss: 0.00002530
Iteration 78/1000 | Loss: 0.00002530
Iteration 79/1000 | Loss: 0.00002530
Iteration 80/1000 | Loss: 0.00002530
Iteration 81/1000 | Loss: 0.00002530
Iteration 82/1000 | Loss: 0.00002530
Iteration 83/1000 | Loss: 0.00002530
Iteration 84/1000 | Loss: 0.00002530
Iteration 85/1000 | Loss: 0.00002530
Iteration 86/1000 | Loss: 0.00002529
Iteration 87/1000 | Loss: 0.00002529
Iteration 88/1000 | Loss: 0.00002529
Iteration 89/1000 | Loss: 0.00002529
Iteration 90/1000 | Loss: 0.00002529
Iteration 91/1000 | Loss: 0.00002529
Iteration 92/1000 | Loss: 0.00002529
Iteration 93/1000 | Loss: 0.00002529
Iteration 94/1000 | Loss: 0.00002529
Iteration 95/1000 | Loss: 0.00002529
Iteration 96/1000 | Loss: 0.00002529
Iteration 97/1000 | Loss: 0.00002528
Iteration 98/1000 | Loss: 0.00002528
Iteration 99/1000 | Loss: 0.00002528
Iteration 100/1000 | Loss: 0.00002528
Iteration 101/1000 | Loss: 0.00002528
Iteration 102/1000 | Loss: 0.00002528
Iteration 103/1000 | Loss: 0.00002528
Iteration 104/1000 | Loss: 0.00002528
Iteration 105/1000 | Loss: 0.00002528
Iteration 106/1000 | Loss: 0.00002528
Iteration 107/1000 | Loss: 0.00002528
Iteration 108/1000 | Loss: 0.00002528
Iteration 109/1000 | Loss: 0.00002527
Iteration 110/1000 | Loss: 0.00002527
Iteration 111/1000 | Loss: 0.00002527
Iteration 112/1000 | Loss: 0.00002527
Iteration 113/1000 | Loss: 0.00002526
Iteration 114/1000 | Loss: 0.00002526
Iteration 115/1000 | Loss: 0.00002526
Iteration 116/1000 | Loss: 0.00002526
Iteration 117/1000 | Loss: 0.00002526
Iteration 118/1000 | Loss: 0.00002526
Iteration 119/1000 | Loss: 0.00002526
Iteration 120/1000 | Loss: 0.00002526
Iteration 121/1000 | Loss: 0.00002526
Iteration 122/1000 | Loss: 0.00002526
Iteration 123/1000 | Loss: 0.00002526
Iteration 124/1000 | Loss: 0.00002525
Iteration 125/1000 | Loss: 0.00002525
Iteration 126/1000 | Loss: 0.00002525
Iteration 127/1000 | Loss: 0.00002525
Iteration 128/1000 | Loss: 0.00002524
Iteration 129/1000 | Loss: 0.00002524
Iteration 130/1000 | Loss: 0.00002524
Iteration 131/1000 | Loss: 0.00002524
Iteration 132/1000 | Loss: 0.00002524
Iteration 133/1000 | Loss: 0.00002523
Iteration 134/1000 | Loss: 0.00002523
Iteration 135/1000 | Loss: 0.00002523
Iteration 136/1000 | Loss: 0.00002523
Iteration 137/1000 | Loss: 0.00002523
Iteration 138/1000 | Loss: 0.00002523
Iteration 139/1000 | Loss: 0.00002523
Iteration 140/1000 | Loss: 0.00002523
Iteration 141/1000 | Loss: 0.00002523
Iteration 142/1000 | Loss: 0.00002523
Iteration 143/1000 | Loss: 0.00002523
Iteration 144/1000 | Loss: 0.00002523
Iteration 145/1000 | Loss: 0.00002523
Iteration 146/1000 | Loss: 0.00002523
Iteration 147/1000 | Loss: 0.00002523
Iteration 148/1000 | Loss: 0.00002523
Iteration 149/1000 | Loss: 0.00002523
Iteration 150/1000 | Loss: 0.00002523
Iteration 151/1000 | Loss: 0.00002523
Iteration 152/1000 | Loss: 0.00002522
Iteration 153/1000 | Loss: 0.00002522
Iteration 154/1000 | Loss: 0.00002522
Iteration 155/1000 | Loss: 0.00002522
Iteration 156/1000 | Loss: 0.00002522
Iteration 157/1000 | Loss: 0.00002522
Iteration 158/1000 | Loss: 0.00002522
Iteration 159/1000 | Loss: 0.00002522
Iteration 160/1000 | Loss: 0.00002522
Iteration 161/1000 | Loss: 0.00002522
Iteration 162/1000 | Loss: 0.00002522
Iteration 163/1000 | Loss: 0.00002522
Iteration 164/1000 | Loss: 0.00002522
Iteration 165/1000 | Loss: 0.00002522
Iteration 166/1000 | Loss: 0.00002522
Iteration 167/1000 | Loss: 0.00002522
Iteration 168/1000 | Loss: 0.00002522
Iteration 169/1000 | Loss: 0.00002522
Iteration 170/1000 | Loss: 0.00002522
Iteration 171/1000 | Loss: 0.00002522
Iteration 172/1000 | Loss: 0.00002522
Iteration 173/1000 | Loss: 0.00002522
Iteration 174/1000 | Loss: 0.00002522
Iteration 175/1000 | Loss: 0.00002522
Iteration 176/1000 | Loss: 0.00002522
Iteration 177/1000 | Loss: 0.00002522
Iteration 178/1000 | Loss: 0.00002522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.522306931496132e-05, 2.522306931496132e-05, 2.522306931496132e-05, 2.522306931496132e-05, 2.522306931496132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.522306931496132e-05

Optimization complete. Final v2v error: 4.305734157562256 mm

Highest mean error: 4.680605411529541 mm for frame 131

Lowest mean error: 4.137826442718506 mm for frame 107

Saving results

Total time: 38.43123960494995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789508
Iteration 2/25 | Loss: 0.00168175
Iteration 3/25 | Loss: 0.00105283
Iteration 4/25 | Loss: 0.00098330
Iteration 5/25 | Loss: 0.00096803
Iteration 6/25 | Loss: 0.00097427
Iteration 7/25 | Loss: 0.00097339
Iteration 8/25 | Loss: 0.00097260
Iteration 9/25 | Loss: 0.00096443
Iteration 10/25 | Loss: 0.00095900
Iteration 11/25 | Loss: 0.00095555
Iteration 12/25 | Loss: 0.00095394
Iteration 13/25 | Loss: 0.00095334
Iteration 14/25 | Loss: 0.00095318
Iteration 15/25 | Loss: 0.00095310
Iteration 16/25 | Loss: 0.00095310
Iteration 17/25 | Loss: 0.00095310
Iteration 18/25 | Loss: 0.00095310
Iteration 19/25 | Loss: 0.00095310
Iteration 20/25 | Loss: 0.00095309
Iteration 21/25 | Loss: 0.00095309
Iteration 22/25 | Loss: 0.00095309
Iteration 23/25 | Loss: 0.00095309
Iteration 24/25 | Loss: 0.00095309
Iteration 25/25 | Loss: 0.00095309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77799463
Iteration 2/25 | Loss: 0.00119237
Iteration 3/25 | Loss: 0.00119237
Iteration 4/25 | Loss: 0.00119237
Iteration 5/25 | Loss: 0.00119237
Iteration 6/25 | Loss: 0.00119237
Iteration 7/25 | Loss: 0.00119237
Iteration 8/25 | Loss: 0.00119237
Iteration 9/25 | Loss: 0.00119237
Iteration 10/25 | Loss: 0.00119237
Iteration 11/25 | Loss: 0.00119237
Iteration 12/25 | Loss: 0.00119237
Iteration 13/25 | Loss: 0.00119237
Iteration 14/25 | Loss: 0.00119237
Iteration 15/25 | Loss: 0.00119237
Iteration 16/25 | Loss: 0.00119237
Iteration 17/25 | Loss: 0.00119237
Iteration 18/25 | Loss: 0.00119237
Iteration 19/25 | Loss: 0.00119237
Iteration 20/25 | Loss: 0.00119237
Iteration 21/25 | Loss: 0.00119237
Iteration 22/25 | Loss: 0.00119237
Iteration 23/25 | Loss: 0.00119237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011923658894374967, 0.0011923658894374967, 0.0011923658894374967, 0.0011923658894374967, 0.0011923658894374967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011923658894374967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119237
Iteration 2/1000 | Loss: 0.00003765
Iteration 3/1000 | Loss: 0.00002841
Iteration 4/1000 | Loss: 0.00002436
Iteration 5/1000 | Loss: 0.00002203
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002010
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001914
Iteration 11/1000 | Loss: 0.00001911
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001908
Iteration 14/1000 | Loss: 0.00001905
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001902
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001896
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001885
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001861
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001860
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001857
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001853
Iteration 69/1000 | Loss: 0.00001853
Iteration 70/1000 | Loss: 0.00001853
Iteration 71/1000 | Loss: 0.00001853
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001852
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001852
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00001851
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001850
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001848
Iteration 108/1000 | Loss: 0.00001848
Iteration 109/1000 | Loss: 0.00001848
Iteration 110/1000 | Loss: 0.00001848
Iteration 111/1000 | Loss: 0.00001848
Iteration 112/1000 | Loss: 0.00001848
Iteration 113/1000 | Loss: 0.00001848
Iteration 114/1000 | Loss: 0.00001848
Iteration 115/1000 | Loss: 0.00001848
Iteration 116/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.8484772226656787e-05, 1.8484772226656787e-05, 1.8484772226656787e-05, 1.8484772226656787e-05, 1.8484772226656787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8484772226656787e-05

Optimization complete. Final v2v error: 3.6947240829467773 mm

Highest mean error: 3.9549107551574707 mm for frame 35

Lowest mean error: 3.4220080375671387 mm for frame 127

Saving results

Total time: 53.76491928100586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852223
Iteration 2/25 | Loss: 0.00103903
Iteration 3/25 | Loss: 0.00094790
Iteration 4/25 | Loss: 0.00093012
Iteration 5/25 | Loss: 0.00092411
Iteration 6/25 | Loss: 0.00092269
Iteration 7/25 | Loss: 0.00092269
Iteration 8/25 | Loss: 0.00092269
Iteration 9/25 | Loss: 0.00092269
Iteration 10/25 | Loss: 0.00092269
Iteration 11/25 | Loss: 0.00092269
Iteration 12/25 | Loss: 0.00092269
Iteration 13/25 | Loss: 0.00092269
Iteration 14/25 | Loss: 0.00092269
Iteration 15/25 | Loss: 0.00092269
Iteration 16/25 | Loss: 0.00092269
Iteration 17/25 | Loss: 0.00092269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009226917172782123, 0.0009226917172782123, 0.0009226917172782123, 0.0009226917172782123, 0.0009226917172782123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009226917172782123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27613914
Iteration 2/25 | Loss: 0.00115030
Iteration 3/25 | Loss: 0.00115030
Iteration 4/25 | Loss: 0.00115030
Iteration 5/25 | Loss: 0.00115030
Iteration 6/25 | Loss: 0.00115030
Iteration 7/25 | Loss: 0.00115030
Iteration 8/25 | Loss: 0.00115030
Iteration 9/25 | Loss: 0.00115030
Iteration 10/25 | Loss: 0.00115030
Iteration 11/25 | Loss: 0.00115030
Iteration 12/25 | Loss: 0.00115030
Iteration 13/25 | Loss: 0.00115030
Iteration 14/25 | Loss: 0.00115030
Iteration 15/25 | Loss: 0.00115030
Iteration 16/25 | Loss: 0.00115030
Iteration 17/25 | Loss: 0.00115030
Iteration 18/25 | Loss: 0.00115030
Iteration 19/25 | Loss: 0.00115030
Iteration 20/25 | Loss: 0.00115030
Iteration 21/25 | Loss: 0.00115030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011502953711897135, 0.0011502953711897135, 0.0011502953711897135, 0.0011502953711897135, 0.0011502953711897135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011502953711897135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115030
Iteration 2/1000 | Loss: 0.00003929
Iteration 3/1000 | Loss: 0.00002262
Iteration 4/1000 | Loss: 0.00001800
Iteration 5/1000 | Loss: 0.00001616
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001385
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001303
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001302
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001298
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001284
Iteration 70/1000 | Loss: 0.00001284
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001283
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001278
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001277
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001276
Iteration 113/1000 | Loss: 0.00001276
Iteration 114/1000 | Loss: 0.00001276
Iteration 115/1000 | Loss: 0.00001276
Iteration 116/1000 | Loss: 0.00001276
Iteration 117/1000 | Loss: 0.00001276
Iteration 118/1000 | Loss: 0.00001275
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001275
Iteration 125/1000 | Loss: 0.00001275
Iteration 126/1000 | Loss: 0.00001275
Iteration 127/1000 | Loss: 0.00001275
Iteration 128/1000 | Loss: 0.00001275
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001274
Iteration 137/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.2743371371470857e-05, 1.2743371371470857e-05, 1.2743371371470857e-05, 1.2743371371470857e-05, 1.2743371371470857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2743371371470857e-05

Optimization complete. Final v2v error: 3.0615828037261963 mm

Highest mean error: 3.4044244289398193 mm for frame 26

Lowest mean error: 2.808518409729004 mm for frame 57

Saving results

Total time: 34.64184904098511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871271
Iteration 2/25 | Loss: 0.00143618
Iteration 3/25 | Loss: 0.00107622
Iteration 4/25 | Loss: 0.00103842
Iteration 5/25 | Loss: 0.00103110
Iteration 6/25 | Loss: 0.00102866
Iteration 7/25 | Loss: 0.00102840
Iteration 8/25 | Loss: 0.00102840
Iteration 9/25 | Loss: 0.00102840
Iteration 10/25 | Loss: 0.00102840
Iteration 11/25 | Loss: 0.00102840
Iteration 12/25 | Loss: 0.00102840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010284028248861432, 0.0010284028248861432, 0.0010284028248861432, 0.0010284028248861432, 0.0010284028248861432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010284028248861432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26261902
Iteration 2/25 | Loss: 0.00107082
Iteration 3/25 | Loss: 0.00107082
Iteration 4/25 | Loss: 0.00107082
Iteration 5/25 | Loss: 0.00107082
Iteration 6/25 | Loss: 0.00107081
Iteration 7/25 | Loss: 0.00107081
Iteration 8/25 | Loss: 0.00107081
Iteration 9/25 | Loss: 0.00107081
Iteration 10/25 | Loss: 0.00107081
Iteration 11/25 | Loss: 0.00107081
Iteration 12/25 | Loss: 0.00107081
Iteration 13/25 | Loss: 0.00107081
Iteration 14/25 | Loss: 0.00107081
Iteration 15/25 | Loss: 0.00107081
Iteration 16/25 | Loss: 0.00107081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010708137415349483, 0.0010708137415349483, 0.0010708137415349483, 0.0010708137415349483, 0.0010708137415349483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010708137415349483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107081
Iteration 2/1000 | Loss: 0.00004713
Iteration 3/1000 | Loss: 0.00003428
Iteration 4/1000 | Loss: 0.00003119
Iteration 5/1000 | Loss: 0.00002865
Iteration 6/1000 | Loss: 0.00002715
Iteration 7/1000 | Loss: 0.00002655
Iteration 8/1000 | Loss: 0.00002607
Iteration 9/1000 | Loss: 0.00002576
Iteration 10/1000 | Loss: 0.00002553
Iteration 11/1000 | Loss: 0.00002531
Iteration 12/1000 | Loss: 0.00002508
Iteration 13/1000 | Loss: 0.00002498
Iteration 14/1000 | Loss: 0.00002496
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00002487
Iteration 17/1000 | Loss: 0.00002485
Iteration 18/1000 | Loss: 0.00002484
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002478
Iteration 21/1000 | Loss: 0.00002470
Iteration 22/1000 | Loss: 0.00002463
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002457
Iteration 25/1000 | Loss: 0.00002453
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00002450
Iteration 28/1000 | Loss: 0.00002448
Iteration 29/1000 | Loss: 0.00002448
Iteration 30/1000 | Loss: 0.00002447
Iteration 31/1000 | Loss: 0.00002447
Iteration 32/1000 | Loss: 0.00002446
Iteration 33/1000 | Loss: 0.00002446
Iteration 34/1000 | Loss: 0.00002446
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002445
Iteration 37/1000 | Loss: 0.00002445
Iteration 38/1000 | Loss: 0.00002443
Iteration 39/1000 | Loss: 0.00002443
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002443
Iteration 42/1000 | Loss: 0.00002443
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002443
Iteration 45/1000 | Loss: 0.00002443
Iteration 46/1000 | Loss: 0.00002443
Iteration 47/1000 | Loss: 0.00002443
Iteration 48/1000 | Loss: 0.00002443
Iteration 49/1000 | Loss: 0.00002442
Iteration 50/1000 | Loss: 0.00002441
Iteration 51/1000 | Loss: 0.00002441
Iteration 52/1000 | Loss: 0.00002440
Iteration 53/1000 | Loss: 0.00002440
Iteration 54/1000 | Loss: 0.00002440
Iteration 55/1000 | Loss: 0.00002440
Iteration 56/1000 | Loss: 0.00002439
Iteration 57/1000 | Loss: 0.00002439
Iteration 58/1000 | Loss: 0.00002438
Iteration 59/1000 | Loss: 0.00002437
Iteration 60/1000 | Loss: 0.00002437
Iteration 61/1000 | Loss: 0.00002437
Iteration 62/1000 | Loss: 0.00002437
Iteration 63/1000 | Loss: 0.00002436
Iteration 64/1000 | Loss: 0.00002436
Iteration 65/1000 | Loss: 0.00002436
Iteration 66/1000 | Loss: 0.00002435
Iteration 67/1000 | Loss: 0.00002435
Iteration 68/1000 | Loss: 0.00002435
Iteration 69/1000 | Loss: 0.00002435
Iteration 70/1000 | Loss: 0.00002434
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002434
Iteration 73/1000 | Loss: 0.00002434
Iteration 74/1000 | Loss: 0.00002434
Iteration 75/1000 | Loss: 0.00002434
Iteration 76/1000 | Loss: 0.00002434
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002433
Iteration 79/1000 | Loss: 0.00002432
Iteration 80/1000 | Loss: 0.00002432
Iteration 81/1000 | Loss: 0.00002432
Iteration 82/1000 | Loss: 0.00002432
Iteration 83/1000 | Loss: 0.00002432
Iteration 84/1000 | Loss: 0.00002432
Iteration 85/1000 | Loss: 0.00002432
Iteration 86/1000 | Loss: 0.00002432
Iteration 87/1000 | Loss: 0.00002431
Iteration 88/1000 | Loss: 0.00002431
Iteration 89/1000 | Loss: 0.00002431
Iteration 90/1000 | Loss: 0.00002431
Iteration 91/1000 | Loss: 0.00002431
Iteration 92/1000 | Loss: 0.00002431
Iteration 93/1000 | Loss: 0.00002431
Iteration 94/1000 | Loss: 0.00002430
Iteration 95/1000 | Loss: 0.00002430
Iteration 96/1000 | Loss: 0.00002430
Iteration 97/1000 | Loss: 0.00002429
Iteration 98/1000 | Loss: 0.00002429
Iteration 99/1000 | Loss: 0.00002429
Iteration 100/1000 | Loss: 0.00002429
Iteration 101/1000 | Loss: 0.00002429
Iteration 102/1000 | Loss: 0.00002429
Iteration 103/1000 | Loss: 0.00002428
Iteration 104/1000 | Loss: 0.00002428
Iteration 105/1000 | Loss: 0.00002428
Iteration 106/1000 | Loss: 0.00002428
Iteration 107/1000 | Loss: 0.00002428
Iteration 108/1000 | Loss: 0.00002428
Iteration 109/1000 | Loss: 0.00002428
Iteration 110/1000 | Loss: 0.00002428
Iteration 111/1000 | Loss: 0.00002428
Iteration 112/1000 | Loss: 0.00002428
Iteration 113/1000 | Loss: 0.00002428
Iteration 114/1000 | Loss: 0.00002428
Iteration 115/1000 | Loss: 0.00002428
Iteration 116/1000 | Loss: 0.00002428
Iteration 117/1000 | Loss: 0.00002427
Iteration 118/1000 | Loss: 0.00002427
Iteration 119/1000 | Loss: 0.00002427
Iteration 120/1000 | Loss: 0.00002427
Iteration 121/1000 | Loss: 0.00002427
Iteration 122/1000 | Loss: 0.00002427
Iteration 123/1000 | Loss: 0.00002427
Iteration 124/1000 | Loss: 0.00002426
Iteration 125/1000 | Loss: 0.00002426
Iteration 126/1000 | Loss: 0.00002426
Iteration 127/1000 | Loss: 0.00002426
Iteration 128/1000 | Loss: 0.00002426
Iteration 129/1000 | Loss: 0.00002426
Iteration 130/1000 | Loss: 0.00002426
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00002426
Iteration 133/1000 | Loss: 0.00002426
Iteration 134/1000 | Loss: 0.00002426
Iteration 135/1000 | Loss: 0.00002426
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002425
Iteration 140/1000 | Loss: 0.00002425
Iteration 141/1000 | Loss: 0.00002425
Iteration 142/1000 | Loss: 0.00002425
Iteration 143/1000 | Loss: 0.00002425
Iteration 144/1000 | Loss: 0.00002425
Iteration 145/1000 | Loss: 0.00002425
Iteration 146/1000 | Loss: 0.00002424
Iteration 147/1000 | Loss: 0.00002424
Iteration 148/1000 | Loss: 0.00002424
Iteration 149/1000 | Loss: 0.00002424
Iteration 150/1000 | Loss: 0.00002424
Iteration 151/1000 | Loss: 0.00002424
Iteration 152/1000 | Loss: 0.00002424
Iteration 153/1000 | Loss: 0.00002424
Iteration 154/1000 | Loss: 0.00002424
Iteration 155/1000 | Loss: 0.00002424
Iteration 156/1000 | Loss: 0.00002424
Iteration 157/1000 | Loss: 0.00002424
Iteration 158/1000 | Loss: 0.00002424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.4240116545115598e-05, 2.4240116545115598e-05, 2.4240116545115598e-05, 2.4240116545115598e-05, 2.4240116545115598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4240116545115598e-05

Optimization complete. Final v2v error: 4.00874137878418 mm

Highest mean error: 5.490372657775879 mm for frame 149

Lowest mean error: 2.862548351287842 mm for frame 6

Saving results

Total time: 43.78059387207031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846437
Iteration 2/25 | Loss: 0.00112394
Iteration 3/25 | Loss: 0.00080899
Iteration 4/25 | Loss: 0.00076876
Iteration 5/25 | Loss: 0.00076278
Iteration 6/25 | Loss: 0.00076144
Iteration 7/25 | Loss: 0.00076144
Iteration 8/25 | Loss: 0.00076144
Iteration 9/25 | Loss: 0.00076144
Iteration 10/25 | Loss: 0.00076144
Iteration 11/25 | Loss: 0.00076144
Iteration 12/25 | Loss: 0.00076144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007614415371790528, 0.0007614415371790528, 0.0007614415371790528, 0.0007614415371790528, 0.0007614415371790528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007614415371790528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99219126
Iteration 2/25 | Loss: 0.00033362
Iteration 3/25 | Loss: 0.00033362
Iteration 4/25 | Loss: 0.00033362
Iteration 5/25 | Loss: 0.00033362
Iteration 6/25 | Loss: 0.00033362
Iteration 7/25 | Loss: 0.00033362
Iteration 8/25 | Loss: 0.00033362
Iteration 9/25 | Loss: 0.00033362
Iteration 10/25 | Loss: 0.00033362
Iteration 11/25 | Loss: 0.00033362
Iteration 12/25 | Loss: 0.00033362
Iteration 13/25 | Loss: 0.00033362
Iteration 14/25 | Loss: 0.00033362
Iteration 15/25 | Loss: 0.00033362
Iteration 16/25 | Loss: 0.00033362
Iteration 17/25 | Loss: 0.00033362
Iteration 18/25 | Loss: 0.00033362
Iteration 19/25 | Loss: 0.00033362
Iteration 20/25 | Loss: 0.00033362
Iteration 21/25 | Loss: 0.00033362
Iteration 22/25 | Loss: 0.00033362
Iteration 23/25 | Loss: 0.00033362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003336152294650674, 0.0003336152294650674, 0.0003336152294650674, 0.0003336152294650674, 0.0003336152294650674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003336152294650674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033362
Iteration 2/1000 | Loss: 0.00002750
Iteration 3/1000 | Loss: 0.00002392
Iteration 4/1000 | Loss: 0.00002213
Iteration 5/1000 | Loss: 0.00002133
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002026
Iteration 8/1000 | Loss: 0.00001999
Iteration 9/1000 | Loss: 0.00001986
Iteration 10/1000 | Loss: 0.00001970
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001966
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001964
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001964
Iteration 21/1000 | Loss: 0.00001964
Iteration 22/1000 | Loss: 0.00001963
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001959
Iteration 26/1000 | Loss: 0.00001959
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00001958
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001956
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001955
Iteration 36/1000 | Loss: 0.00001955
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001954
Iteration 40/1000 | Loss: 0.00001954
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001953
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001950
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001948
Iteration 61/1000 | Loss: 0.00001948
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001948
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001948
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001947
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001947
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001944
Iteration 88/1000 | Loss: 0.00001944
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001940
Iteration 112/1000 | Loss: 0.00001939
Iteration 113/1000 | Loss: 0.00001939
Iteration 114/1000 | Loss: 0.00001939
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001938
Iteration 118/1000 | Loss: 0.00001938
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001938
Iteration 123/1000 | Loss: 0.00001938
Iteration 124/1000 | Loss: 0.00001938
Iteration 125/1000 | Loss: 0.00001938
Iteration 126/1000 | Loss: 0.00001938
Iteration 127/1000 | Loss: 0.00001938
Iteration 128/1000 | Loss: 0.00001937
Iteration 129/1000 | Loss: 0.00001937
Iteration 130/1000 | Loss: 0.00001937
Iteration 131/1000 | Loss: 0.00001937
Iteration 132/1000 | Loss: 0.00001937
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001937
Iteration 135/1000 | Loss: 0.00001937
Iteration 136/1000 | Loss: 0.00001937
Iteration 137/1000 | Loss: 0.00001937
Iteration 138/1000 | Loss: 0.00001936
Iteration 139/1000 | Loss: 0.00001936
Iteration 140/1000 | Loss: 0.00001936
Iteration 141/1000 | Loss: 0.00001936
Iteration 142/1000 | Loss: 0.00001936
Iteration 143/1000 | Loss: 0.00001936
Iteration 144/1000 | Loss: 0.00001936
Iteration 145/1000 | Loss: 0.00001936
Iteration 146/1000 | Loss: 0.00001936
Iteration 147/1000 | Loss: 0.00001936
Iteration 148/1000 | Loss: 0.00001936
Iteration 149/1000 | Loss: 0.00001936
Iteration 150/1000 | Loss: 0.00001936
Iteration 151/1000 | Loss: 0.00001936
Iteration 152/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.9364719264558516e-05, 1.9364719264558516e-05, 1.9364719264558516e-05, 1.9364719264558516e-05, 1.9364719264558516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9364719264558516e-05

Optimization complete. Final v2v error: 3.742628574371338 mm

Highest mean error: 3.9557976722717285 mm for frame 154

Lowest mean error: 3.562809944152832 mm for frame 168

Saving results

Total time: 37.6864058971405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429037
Iteration 2/25 | Loss: 0.00080757
Iteration 3/25 | Loss: 0.00070772
Iteration 4/25 | Loss: 0.00068160
Iteration 5/25 | Loss: 0.00067217
Iteration 6/25 | Loss: 0.00067078
Iteration 7/25 | Loss: 0.00067045
Iteration 8/25 | Loss: 0.00067045
Iteration 9/25 | Loss: 0.00067045
Iteration 10/25 | Loss: 0.00067045
Iteration 11/25 | Loss: 0.00067045
Iteration 12/25 | Loss: 0.00067045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006704474217258394, 0.0006704474217258394, 0.0006704474217258394, 0.0006704474217258394, 0.0006704474217258394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006704474217258394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.33197498
Iteration 2/25 | Loss: 0.00029870
Iteration 3/25 | Loss: 0.00029870
Iteration 4/25 | Loss: 0.00029870
Iteration 5/25 | Loss: 0.00029870
Iteration 6/25 | Loss: 0.00029870
Iteration 7/25 | Loss: 0.00029870
Iteration 8/25 | Loss: 0.00029870
Iteration 9/25 | Loss: 0.00029870
Iteration 10/25 | Loss: 0.00029870
Iteration 11/25 | Loss: 0.00029870
Iteration 12/25 | Loss: 0.00029870
Iteration 13/25 | Loss: 0.00029870
Iteration 14/25 | Loss: 0.00029870
Iteration 15/25 | Loss: 0.00029870
Iteration 16/25 | Loss: 0.00029870
Iteration 17/25 | Loss: 0.00029870
Iteration 18/25 | Loss: 0.00029870
Iteration 19/25 | Loss: 0.00029870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002986953768413514, 0.0002986953768413514, 0.0002986953768413514, 0.0002986953768413514, 0.0002986953768413514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002986953768413514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029870
Iteration 2/1000 | Loss: 0.00002593
Iteration 3/1000 | Loss: 0.00002146
Iteration 4/1000 | Loss: 0.00002037
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001791
Iteration 9/1000 | Loss: 0.00001768
Iteration 10/1000 | Loss: 0.00001754
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001728
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001709
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001707
Iteration 80/1000 | Loss: 0.00001707
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001701
Iteration 109/1000 | Loss: 0.00001701
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001701
Iteration 113/1000 | Loss: 0.00001701
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001700
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001697
Iteration 147/1000 | Loss: 0.00001697
Iteration 148/1000 | Loss: 0.00001697
Iteration 149/1000 | Loss: 0.00001697
Iteration 150/1000 | Loss: 0.00001697
Iteration 151/1000 | Loss: 0.00001697
Iteration 152/1000 | Loss: 0.00001697
Iteration 153/1000 | Loss: 0.00001697
Iteration 154/1000 | Loss: 0.00001697
Iteration 155/1000 | Loss: 0.00001697
Iteration 156/1000 | Loss: 0.00001697
Iteration 157/1000 | Loss: 0.00001697
Iteration 158/1000 | Loss: 0.00001697
Iteration 159/1000 | Loss: 0.00001697
Iteration 160/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6973996025626548e-05, 1.6973996025626548e-05, 1.6973996025626548e-05, 1.6973996025626548e-05, 1.6973996025626548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6973996025626548e-05

Optimization complete. Final v2v error: 3.463937520980835 mm

Highest mean error: 3.745033025741577 mm for frame 87

Lowest mean error: 3.184751272201538 mm for frame 209

Saving results

Total time: 41.41038274765015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135032
Iteration 2/25 | Loss: 0.00177563
Iteration 3/25 | Loss: 0.00105758
Iteration 4/25 | Loss: 0.00096843
Iteration 5/25 | Loss: 0.00093679
Iteration 6/25 | Loss: 0.00094293
Iteration 7/25 | Loss: 0.00094377
Iteration 8/25 | Loss: 0.00094047
Iteration 9/25 | Loss: 0.00092190
Iteration 10/25 | Loss: 0.00092219
Iteration 11/25 | Loss: 0.00091355
Iteration 12/25 | Loss: 0.00091924
Iteration 13/25 | Loss: 0.00091589
Iteration 14/25 | Loss: 0.00091962
Iteration 15/25 | Loss: 0.00091454
Iteration 16/25 | Loss: 0.00091023
Iteration 17/25 | Loss: 0.00090586
Iteration 18/25 | Loss: 0.00090465
Iteration 19/25 | Loss: 0.00090213
Iteration 20/25 | Loss: 0.00089654
Iteration 21/25 | Loss: 0.00089710
Iteration 22/25 | Loss: 0.00089721
Iteration 23/25 | Loss: 0.00089594
Iteration 24/25 | Loss: 0.00089560
Iteration 25/25 | Loss: 0.00089916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06142390
Iteration 2/25 | Loss: 0.00056002
Iteration 3/25 | Loss: 0.00056001
Iteration 4/25 | Loss: 0.00056001
Iteration 5/25 | Loss: 0.00056001
Iteration 6/25 | Loss: 0.00056001
Iteration 7/25 | Loss: 0.00056001
Iteration 8/25 | Loss: 0.00056000
Iteration 9/25 | Loss: 0.00056000
Iteration 10/25 | Loss: 0.00056000
Iteration 11/25 | Loss: 0.00056000
Iteration 12/25 | Loss: 0.00056000
Iteration 13/25 | Loss: 0.00056000
Iteration 14/25 | Loss: 0.00056000
Iteration 15/25 | Loss: 0.00056000
Iteration 16/25 | Loss: 0.00056000
Iteration 17/25 | Loss: 0.00056000
Iteration 18/25 | Loss: 0.00056000
Iteration 19/25 | Loss: 0.00056000
Iteration 20/25 | Loss: 0.00056000
Iteration 21/25 | Loss: 0.00056000
Iteration 22/25 | Loss: 0.00056000
Iteration 23/25 | Loss: 0.00056000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005600040312856436, 0.0005600040312856436, 0.0005600040312856436, 0.0005600040312856436, 0.0005600040312856436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005600040312856436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056000
Iteration 2/1000 | Loss: 0.00044951
Iteration 3/1000 | Loss: 0.00069571
Iteration 4/1000 | Loss: 0.00071112
Iteration 5/1000 | Loss: 0.00082447
Iteration 6/1000 | Loss: 0.00081694
Iteration 7/1000 | Loss: 0.00086667
Iteration 8/1000 | Loss: 0.00094308
Iteration 9/1000 | Loss: 0.00076670
Iteration 10/1000 | Loss: 0.00091666
Iteration 11/1000 | Loss: 0.00078204
Iteration 12/1000 | Loss: 0.00065853
Iteration 13/1000 | Loss: 0.00083721
Iteration 14/1000 | Loss: 0.00086184
Iteration 15/1000 | Loss: 0.00059950
Iteration 16/1000 | Loss: 0.00099862
Iteration 17/1000 | Loss: 0.00077576
Iteration 18/1000 | Loss: 0.00088244
Iteration 19/1000 | Loss: 0.00079606
Iteration 20/1000 | Loss: 0.00074338
Iteration 21/1000 | Loss: 0.00077140
Iteration 22/1000 | Loss: 0.00067981
Iteration 23/1000 | Loss: 0.00070928
Iteration 24/1000 | Loss: 0.00067472
Iteration 25/1000 | Loss: 0.00067534
Iteration 26/1000 | Loss: 0.00076567
Iteration 27/1000 | Loss: 0.00052105
Iteration 28/1000 | Loss: 0.00060130
Iteration 29/1000 | Loss: 0.00070414
Iteration 30/1000 | Loss: 0.00048085
Iteration 31/1000 | Loss: 0.00069839
Iteration 32/1000 | Loss: 0.00068798
Iteration 33/1000 | Loss: 0.00080533
Iteration 34/1000 | Loss: 0.00094877
Iteration 35/1000 | Loss: 0.00100268
Iteration 36/1000 | Loss: 0.00056271
Iteration 37/1000 | Loss: 0.00051558
Iteration 38/1000 | Loss: 0.00042540
Iteration 39/1000 | Loss: 0.00064163
Iteration 40/1000 | Loss: 0.00065983
Iteration 41/1000 | Loss: 0.00082814
Iteration 42/1000 | Loss: 0.00046540
Iteration 43/1000 | Loss: 0.00067206
Iteration 44/1000 | Loss: 0.00178095
Iteration 45/1000 | Loss: 0.00034631
Iteration 46/1000 | Loss: 0.00053499
Iteration 47/1000 | Loss: 0.00031824
Iteration 48/1000 | Loss: 0.00089261
Iteration 49/1000 | Loss: 0.00126034
Iteration 50/1000 | Loss: 0.00052686
Iteration 51/1000 | Loss: 0.00060243
Iteration 52/1000 | Loss: 0.00081949
Iteration 53/1000 | Loss: 0.00048415
Iteration 54/1000 | Loss: 0.00045281
Iteration 55/1000 | Loss: 0.00070273
Iteration 56/1000 | Loss: 0.00057613
Iteration 57/1000 | Loss: 0.00064095
Iteration 58/1000 | Loss: 0.00053093
Iteration 59/1000 | Loss: 0.00075024
Iteration 60/1000 | Loss: 0.00066918
Iteration 61/1000 | Loss: 0.00068198
Iteration 62/1000 | Loss: 0.00067325
Iteration 63/1000 | Loss: 0.00089526
Iteration 64/1000 | Loss: 0.00046725
Iteration 65/1000 | Loss: 0.00068381
Iteration 66/1000 | Loss: 0.00062023
Iteration 67/1000 | Loss: 0.00052680
Iteration 68/1000 | Loss: 0.00148246
Iteration 69/1000 | Loss: 0.00075293
Iteration 70/1000 | Loss: 0.00076315
Iteration 71/1000 | Loss: 0.00069585
Iteration 72/1000 | Loss: 0.00122598
Iteration 73/1000 | Loss: 0.00107549
Iteration 74/1000 | Loss: 0.00141950
Iteration 75/1000 | Loss: 0.00043119
Iteration 76/1000 | Loss: 0.00050298
Iteration 77/1000 | Loss: 0.00044597
Iteration 78/1000 | Loss: 0.00038681
Iteration 79/1000 | Loss: 0.00035531
Iteration 80/1000 | Loss: 0.00042855
Iteration 81/1000 | Loss: 0.00042523
Iteration 82/1000 | Loss: 0.00043552
Iteration 83/1000 | Loss: 0.00044185
Iteration 84/1000 | Loss: 0.00064216
Iteration 85/1000 | Loss: 0.00036449
Iteration 86/1000 | Loss: 0.00025443
Iteration 87/1000 | Loss: 0.00038257
Iteration 88/1000 | Loss: 0.00041206
Iteration 89/1000 | Loss: 0.00066518
Iteration 90/1000 | Loss: 0.00064216
Iteration 91/1000 | Loss: 0.00091486
Iteration 92/1000 | Loss: 0.00070704
Iteration 93/1000 | Loss: 0.00072321
Iteration 94/1000 | Loss: 0.00054449
Iteration 95/1000 | Loss: 0.00058881
Iteration 96/1000 | Loss: 0.00068696
Iteration 97/1000 | Loss: 0.00074016
Iteration 98/1000 | Loss: 0.00065000
Iteration 99/1000 | Loss: 0.00075754
Iteration 100/1000 | Loss: 0.00074815
Iteration 101/1000 | Loss: 0.00069253
Iteration 102/1000 | Loss: 0.00066152
Iteration 103/1000 | Loss: 0.00070556
Iteration 104/1000 | Loss: 0.00072994
Iteration 105/1000 | Loss: 0.00078646
Iteration 106/1000 | Loss: 0.00071084
Iteration 107/1000 | Loss: 0.00061382
Iteration 108/1000 | Loss: 0.00061062
Iteration 109/1000 | Loss: 0.00062485
Iteration 110/1000 | Loss: 0.00052043
Iteration 111/1000 | Loss: 0.00079784
Iteration 112/1000 | Loss: 0.00068168
Iteration 113/1000 | Loss: 0.00083128
Iteration 114/1000 | Loss: 0.00080249
Iteration 115/1000 | Loss: 0.00084850
Iteration 116/1000 | Loss: 0.00088820
Iteration 117/1000 | Loss: 0.00068201
Iteration 118/1000 | Loss: 0.00082347
Iteration 119/1000 | Loss: 0.00039662
Iteration 120/1000 | Loss: 0.00039020
Iteration 121/1000 | Loss: 0.00062211
Iteration 122/1000 | Loss: 0.00048990
Iteration 123/1000 | Loss: 0.00053627
Iteration 124/1000 | Loss: 0.00053894
Iteration 125/1000 | Loss: 0.00088843
Iteration 126/1000 | Loss: 0.00071173
Iteration 127/1000 | Loss: 0.00071893
Iteration 128/1000 | Loss: 0.00061121
Iteration 129/1000 | Loss: 0.00075941
Iteration 130/1000 | Loss: 0.00070162
Iteration 131/1000 | Loss: 0.00076493
Iteration 132/1000 | Loss: 0.00066203
Iteration 133/1000 | Loss: 0.00053146
Iteration 134/1000 | Loss: 0.00049406
Iteration 135/1000 | Loss: 0.00041939
Iteration 136/1000 | Loss: 0.00064409
Iteration 137/1000 | Loss: 0.00087865
Iteration 138/1000 | Loss: 0.00071480
Iteration 139/1000 | Loss: 0.00060997
Iteration 140/1000 | Loss: 0.00045647
Iteration 141/1000 | Loss: 0.00070681
Iteration 142/1000 | Loss: 0.00077847
Iteration 143/1000 | Loss: 0.00045174
Iteration 144/1000 | Loss: 0.00059838
Iteration 145/1000 | Loss: 0.00042935
Iteration 146/1000 | Loss: 0.00067181
Iteration 147/1000 | Loss: 0.00044315
Iteration 148/1000 | Loss: 0.00054702
Iteration 149/1000 | Loss: 0.00045886
Iteration 150/1000 | Loss: 0.00058323
Iteration 151/1000 | Loss: 0.00045473
Iteration 152/1000 | Loss: 0.00057360
Iteration 153/1000 | Loss: 0.00035852
Iteration 154/1000 | Loss: 0.00054192
Iteration 155/1000 | Loss: 0.00049820
Iteration 156/1000 | Loss: 0.00046150
Iteration 157/1000 | Loss: 0.00029489
Iteration 158/1000 | Loss: 0.00040309
Iteration 159/1000 | Loss: 0.00031042
Iteration 160/1000 | Loss: 0.00040750
Iteration 161/1000 | Loss: 0.00050365
Iteration 162/1000 | Loss: 0.00049497
Iteration 163/1000 | Loss: 0.00036963
Iteration 164/1000 | Loss: 0.00049211
Iteration 165/1000 | Loss: 0.00046353
Iteration 166/1000 | Loss: 0.00070229
Iteration 167/1000 | Loss: 0.00039138
Iteration 168/1000 | Loss: 0.00041196
Iteration 169/1000 | Loss: 0.00056948
Iteration 170/1000 | Loss: 0.00110333
Iteration 171/1000 | Loss: 0.00068506
Iteration 172/1000 | Loss: 0.00034892
Iteration 173/1000 | Loss: 0.00009310
Iteration 174/1000 | Loss: 0.00050462
Iteration 175/1000 | Loss: 0.00049135
Iteration 176/1000 | Loss: 0.00046614
Iteration 177/1000 | Loss: 0.00045715
Iteration 178/1000 | Loss: 0.00040404
Iteration 179/1000 | Loss: 0.00053453
Iteration 180/1000 | Loss: 0.00039614
Iteration 181/1000 | Loss: 0.00052838
Iteration 182/1000 | Loss: 0.00029873
Iteration 183/1000 | Loss: 0.00043891
Iteration 184/1000 | Loss: 0.00025100
Iteration 185/1000 | Loss: 0.00044930
Iteration 186/1000 | Loss: 0.00041759
Iteration 187/1000 | Loss: 0.00037517
Iteration 188/1000 | Loss: 0.00042892
Iteration 189/1000 | Loss: 0.00061969
Iteration 190/1000 | Loss: 0.00028117
Iteration 191/1000 | Loss: 0.00033690
Iteration 192/1000 | Loss: 0.00042501
Iteration 193/1000 | Loss: 0.00039290
Iteration 194/1000 | Loss: 0.00041232
Iteration 195/1000 | Loss: 0.00015844
Iteration 196/1000 | Loss: 0.00005937
Iteration 197/1000 | Loss: 0.00009924
Iteration 198/1000 | Loss: 0.00019396
Iteration 199/1000 | Loss: 0.00019341
Iteration 200/1000 | Loss: 0.00005619
Iteration 201/1000 | Loss: 0.00011512
Iteration 202/1000 | Loss: 0.00009703
Iteration 203/1000 | Loss: 0.00012565
Iteration 204/1000 | Loss: 0.00003934
Iteration 205/1000 | Loss: 0.00006763
Iteration 206/1000 | Loss: 0.00009311
Iteration 207/1000 | Loss: 0.00007053
Iteration 208/1000 | Loss: 0.00008662
Iteration 209/1000 | Loss: 0.00009004
Iteration 210/1000 | Loss: 0.00003662
Iteration 211/1000 | Loss: 0.00003592
Iteration 212/1000 | Loss: 0.00004348
Iteration 213/1000 | Loss: 0.00003769
Iteration 214/1000 | Loss: 0.00003543
Iteration 215/1000 | Loss: 0.00003462
Iteration 216/1000 | Loss: 0.00011431
Iteration 217/1000 | Loss: 0.00013984
Iteration 218/1000 | Loss: 0.00015806
Iteration 219/1000 | Loss: 0.00015476
Iteration 220/1000 | Loss: 0.00016386
Iteration 221/1000 | Loss: 0.00015848
Iteration 222/1000 | Loss: 0.00020495
Iteration 223/1000 | Loss: 0.00015830
Iteration 224/1000 | Loss: 0.00018914
Iteration 225/1000 | Loss: 0.00015232
Iteration 226/1000 | Loss: 0.00004907
Iteration 227/1000 | Loss: 0.00008093
Iteration 228/1000 | Loss: 0.00016710
Iteration 229/1000 | Loss: 0.00014748
Iteration 230/1000 | Loss: 0.00014751
Iteration 231/1000 | Loss: 0.00016137
Iteration 232/1000 | Loss: 0.00022979
Iteration 233/1000 | Loss: 0.00003874
Iteration 234/1000 | Loss: 0.00003487
Iteration 235/1000 | Loss: 0.00012803
Iteration 236/1000 | Loss: 0.00006457
Iteration 237/1000 | Loss: 0.00007860
Iteration 238/1000 | Loss: 0.00006329
Iteration 239/1000 | Loss: 0.00026603
Iteration 240/1000 | Loss: 0.00014955
Iteration 241/1000 | Loss: 0.00015982
Iteration 242/1000 | Loss: 0.00015128
Iteration 243/1000 | Loss: 0.00015623
Iteration 244/1000 | Loss: 0.00025216
Iteration 245/1000 | Loss: 0.00003486
Iteration 246/1000 | Loss: 0.00012703
Iteration 247/1000 | Loss: 0.00010473
Iteration 248/1000 | Loss: 0.00011580
Iteration 249/1000 | Loss: 0.00018912
Iteration 250/1000 | Loss: 0.00016915
Iteration 251/1000 | Loss: 0.00010053
Iteration 252/1000 | Loss: 0.00013707
Iteration 253/1000 | Loss: 0.00011063
Iteration 254/1000 | Loss: 0.00021734
Iteration 255/1000 | Loss: 0.00003915
Iteration 256/1000 | Loss: 0.00003222
Iteration 257/1000 | Loss: 0.00003057
Iteration 258/1000 | Loss: 0.00012269
Iteration 259/1000 | Loss: 0.00016963
Iteration 260/1000 | Loss: 0.00004618
Iteration 261/1000 | Loss: 0.00010054
Iteration 262/1000 | Loss: 0.00024464
Iteration 263/1000 | Loss: 0.00005170
Iteration 264/1000 | Loss: 0.00003778
Iteration 265/1000 | Loss: 0.00026118
Iteration 266/1000 | Loss: 0.00016432
Iteration 267/1000 | Loss: 0.00014081
Iteration 268/1000 | Loss: 0.00017800
Iteration 269/1000 | Loss: 0.00010323
Iteration 270/1000 | Loss: 0.00012281
Iteration 271/1000 | Loss: 0.00011318
Iteration 272/1000 | Loss: 0.00011035
Iteration 273/1000 | Loss: 0.00012066
Iteration 274/1000 | Loss: 0.00011941
Iteration 275/1000 | Loss: 0.00017811
Iteration 276/1000 | Loss: 0.00021876
Iteration 277/1000 | Loss: 0.00006936
Iteration 278/1000 | Loss: 0.00003466
Iteration 279/1000 | Loss: 0.00011122
Iteration 280/1000 | Loss: 0.00003803
Iteration 281/1000 | Loss: 0.00009023
Iteration 282/1000 | Loss: 0.00018066
Iteration 283/1000 | Loss: 0.00010657
Iteration 284/1000 | Loss: 0.00015753
Iteration 285/1000 | Loss: 0.00017181
Iteration 286/1000 | Loss: 0.00018876
Iteration 287/1000 | Loss: 0.00016498
Iteration 288/1000 | Loss: 0.00009098
Iteration 289/1000 | Loss: 0.00016321
Iteration 290/1000 | Loss: 0.00018722
Iteration 291/1000 | Loss: 0.00014061
Iteration 292/1000 | Loss: 0.00012057
Iteration 293/1000 | Loss: 0.00010069
Iteration 294/1000 | Loss: 0.00006592
Iteration 295/1000 | Loss: 0.00016030
Iteration 296/1000 | Loss: 0.00011539
Iteration 297/1000 | Loss: 0.00011817
Iteration 298/1000 | Loss: 0.00010030
Iteration 299/1000 | Loss: 0.00008580
Iteration 300/1000 | Loss: 0.00007031
Iteration 301/1000 | Loss: 0.00011697
Iteration 302/1000 | Loss: 0.00009456
Iteration 303/1000 | Loss: 0.00012389
Iteration 304/1000 | Loss: 0.00009252
Iteration 305/1000 | Loss: 0.00011821
Iteration 306/1000 | Loss: 0.00009236
Iteration 307/1000 | Loss: 0.00011554
Iteration 308/1000 | Loss: 0.00009169
Iteration 309/1000 | Loss: 0.00010096
Iteration 310/1000 | Loss: 0.00014148
Iteration 311/1000 | Loss: 0.00018935
Iteration 312/1000 | Loss: 0.00017189
Iteration 313/1000 | Loss: 0.00017701
Iteration 314/1000 | Loss: 0.00021853
Iteration 315/1000 | Loss: 0.00018389
Iteration 316/1000 | Loss: 0.00010037
Iteration 317/1000 | Loss: 0.00011232
Iteration 318/1000 | Loss: 0.00014768
Iteration 319/1000 | Loss: 0.00013916
Iteration 320/1000 | Loss: 0.00015049
Iteration 321/1000 | Loss: 0.00011166
Iteration 322/1000 | Loss: 0.00004192
Iteration 323/1000 | Loss: 0.00011424
Iteration 324/1000 | Loss: 0.00003831
Iteration 325/1000 | Loss: 0.00003478
Iteration 326/1000 | Loss: 0.00003077
Iteration 327/1000 | Loss: 0.00003011
Iteration 328/1000 | Loss: 0.00002952
Iteration 329/1000 | Loss: 0.00002891
Iteration 330/1000 | Loss: 0.00002839
Iteration 331/1000 | Loss: 0.00002808
Iteration 332/1000 | Loss: 0.00002777
Iteration 333/1000 | Loss: 0.00002759
Iteration 334/1000 | Loss: 0.00002741
Iteration 335/1000 | Loss: 0.00002725
Iteration 336/1000 | Loss: 0.00002718
Iteration 337/1000 | Loss: 0.00002716
Iteration 338/1000 | Loss: 0.00002715
Iteration 339/1000 | Loss: 0.00002714
Iteration 340/1000 | Loss: 0.00002714
Iteration 341/1000 | Loss: 0.00002713
Iteration 342/1000 | Loss: 0.00002708
Iteration 343/1000 | Loss: 0.00002703
Iteration 344/1000 | Loss: 0.00002703
Iteration 345/1000 | Loss: 0.00002703
Iteration 346/1000 | Loss: 0.00002703
Iteration 347/1000 | Loss: 0.00002702
Iteration 348/1000 | Loss: 0.00002702
Iteration 349/1000 | Loss: 0.00002700
Iteration 350/1000 | Loss: 0.00002700
Iteration 351/1000 | Loss: 0.00002699
Iteration 352/1000 | Loss: 0.00002698
Iteration 353/1000 | Loss: 0.00002697
Iteration 354/1000 | Loss: 0.00002697
Iteration 355/1000 | Loss: 0.00002696
Iteration 356/1000 | Loss: 0.00002695
Iteration 357/1000 | Loss: 0.00002692
Iteration 358/1000 | Loss: 0.00002692
Iteration 359/1000 | Loss: 0.00002691
Iteration 360/1000 | Loss: 0.00002690
Iteration 361/1000 | Loss: 0.00002690
Iteration 362/1000 | Loss: 0.00002690
Iteration 363/1000 | Loss: 0.00002690
Iteration 364/1000 | Loss: 0.00002690
Iteration 365/1000 | Loss: 0.00002690
Iteration 366/1000 | Loss: 0.00002690
Iteration 367/1000 | Loss: 0.00002690
Iteration 368/1000 | Loss: 0.00002689
Iteration 369/1000 | Loss: 0.00002689
Iteration 370/1000 | Loss: 0.00002689
Iteration 371/1000 | Loss: 0.00002689
Iteration 372/1000 | Loss: 0.00002689
Iteration 373/1000 | Loss: 0.00002689
Iteration 374/1000 | Loss: 0.00002689
Iteration 375/1000 | Loss: 0.00002688
Iteration 376/1000 | Loss: 0.00002688
Iteration 377/1000 | Loss: 0.00002688
Iteration 378/1000 | Loss: 0.00002688
Iteration 379/1000 | Loss: 0.00002688
Iteration 380/1000 | Loss: 0.00002688
Iteration 381/1000 | Loss: 0.00002687
Iteration 382/1000 | Loss: 0.00002687
Iteration 383/1000 | Loss: 0.00002687
Iteration 384/1000 | Loss: 0.00002687
Iteration 385/1000 | Loss: 0.00002687
Iteration 386/1000 | Loss: 0.00002687
Iteration 387/1000 | Loss: 0.00002687
Iteration 388/1000 | Loss: 0.00002687
Iteration 389/1000 | Loss: 0.00002687
Iteration 390/1000 | Loss: 0.00002687
Iteration 391/1000 | Loss: 0.00002687
Iteration 392/1000 | Loss: 0.00002687
Iteration 393/1000 | Loss: 0.00002687
Iteration 394/1000 | Loss: 0.00002687
Iteration 395/1000 | Loss: 0.00002687
Iteration 396/1000 | Loss: 0.00002687
Iteration 397/1000 | Loss: 0.00002687
Iteration 398/1000 | Loss: 0.00002686
Iteration 399/1000 | Loss: 0.00002686
Iteration 400/1000 | Loss: 0.00002686
Iteration 401/1000 | Loss: 0.00002685
Iteration 402/1000 | Loss: 0.00002685
Iteration 403/1000 | Loss: 0.00002685
Iteration 404/1000 | Loss: 0.00002685
Iteration 405/1000 | Loss: 0.00002685
Iteration 406/1000 | Loss: 0.00002685
Iteration 407/1000 | Loss: 0.00002685
Iteration 408/1000 | Loss: 0.00002685
Iteration 409/1000 | Loss: 0.00002684
Iteration 410/1000 | Loss: 0.00002684
Iteration 411/1000 | Loss: 0.00002684
Iteration 412/1000 | Loss: 0.00002684
Iteration 413/1000 | Loss: 0.00002684
Iteration 414/1000 | Loss: 0.00002684
Iteration 415/1000 | Loss: 0.00002684
Iteration 416/1000 | Loss: 0.00002683
Iteration 417/1000 | Loss: 0.00002683
Iteration 418/1000 | Loss: 0.00002683
Iteration 419/1000 | Loss: 0.00002683
Iteration 420/1000 | Loss: 0.00002682
Iteration 421/1000 | Loss: 0.00002682
Iteration 422/1000 | Loss: 0.00002682
Iteration 423/1000 | Loss: 0.00002682
Iteration 424/1000 | Loss: 0.00002682
Iteration 425/1000 | Loss: 0.00002682
Iteration 426/1000 | Loss: 0.00002682
Iteration 427/1000 | Loss: 0.00002682
Iteration 428/1000 | Loss: 0.00002682
Iteration 429/1000 | Loss: 0.00002682
Iteration 430/1000 | Loss: 0.00002682
Iteration 431/1000 | Loss: 0.00002681
Iteration 432/1000 | Loss: 0.00002681
Iteration 433/1000 | Loss: 0.00002681
Iteration 434/1000 | Loss: 0.00002681
Iteration 435/1000 | Loss: 0.00002681
Iteration 436/1000 | Loss: 0.00002681
Iteration 437/1000 | Loss: 0.00002681
Iteration 438/1000 | Loss: 0.00002681
Iteration 439/1000 | Loss: 0.00002681
Iteration 440/1000 | Loss: 0.00002681
Iteration 441/1000 | Loss: 0.00002681
Iteration 442/1000 | Loss: 0.00002680
Iteration 443/1000 | Loss: 0.00002680
Iteration 444/1000 | Loss: 0.00002680
Iteration 445/1000 | Loss: 0.00002680
Iteration 446/1000 | Loss: 0.00002680
Iteration 447/1000 | Loss: 0.00002680
Iteration 448/1000 | Loss: 0.00002680
Iteration 449/1000 | Loss: 0.00002680
Iteration 450/1000 | Loss: 0.00002680
Iteration 451/1000 | Loss: 0.00002680
Iteration 452/1000 | Loss: 0.00002680
Iteration 453/1000 | Loss: 0.00002680
Iteration 454/1000 | Loss: 0.00002680
Iteration 455/1000 | Loss: 0.00002680
Iteration 456/1000 | Loss: 0.00002680
Iteration 457/1000 | Loss: 0.00002680
Iteration 458/1000 | Loss: 0.00002680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 458. Stopping optimization.
Last 5 losses: [2.6800193154485896e-05, 2.6800193154485896e-05, 2.6800193154485896e-05, 2.6800193154485896e-05, 2.6800193154485896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6800193154485896e-05

Optimization complete. Final v2v error: 4.201948165893555 mm

Highest mean error: 5.6832475662231445 mm for frame 152

Lowest mean error: 3.7641539573669434 mm for frame 127

Saving results

Total time: 559.6417093276978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416961
Iteration 2/25 | Loss: 0.00086051
Iteration 3/25 | Loss: 0.00071898
Iteration 4/25 | Loss: 0.00070110
Iteration 5/25 | Loss: 0.00069687
Iteration 6/25 | Loss: 0.00069545
Iteration 7/25 | Loss: 0.00069540
Iteration 8/25 | Loss: 0.00069540
Iteration 9/25 | Loss: 0.00069540
Iteration 10/25 | Loss: 0.00069540
Iteration 11/25 | Loss: 0.00069540
Iteration 12/25 | Loss: 0.00069540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006954044802114367, 0.0006954044802114367, 0.0006954044802114367, 0.0006954044802114367, 0.0006954044802114367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006954044802114367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49493432
Iteration 2/25 | Loss: 0.00033491
Iteration 3/25 | Loss: 0.00033491
Iteration 4/25 | Loss: 0.00033491
Iteration 5/25 | Loss: 0.00033491
Iteration 6/25 | Loss: 0.00033491
Iteration 7/25 | Loss: 0.00033491
Iteration 8/25 | Loss: 0.00033491
Iteration 9/25 | Loss: 0.00033491
Iteration 10/25 | Loss: 0.00033491
Iteration 11/25 | Loss: 0.00033491
Iteration 12/25 | Loss: 0.00033491
Iteration 13/25 | Loss: 0.00033491
Iteration 14/25 | Loss: 0.00033491
Iteration 15/25 | Loss: 0.00033491
Iteration 16/25 | Loss: 0.00033491
Iteration 17/25 | Loss: 0.00033491
Iteration 18/25 | Loss: 0.00033491
Iteration 19/25 | Loss: 0.00033491
Iteration 20/25 | Loss: 0.00033491
Iteration 21/25 | Loss: 0.00033491
Iteration 22/25 | Loss: 0.00033491
Iteration 23/25 | Loss: 0.00033491
Iteration 24/25 | Loss: 0.00033491
Iteration 25/25 | Loss: 0.00033491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003349103208165616, 0.0003349103208165616, 0.0003349103208165616, 0.0003349103208165616, 0.0003349103208165616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003349103208165616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033491
Iteration 2/1000 | Loss: 0.00001785
Iteration 3/1000 | Loss: 0.00001477
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001310
Iteration 8/1000 | Loss: 0.00001289
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001271
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001266
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001265
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001264
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001262
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001262
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001262
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00001262
Iteration 54/1000 | Loss: 0.00001262
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001261
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001261
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.2611304555321112e-05, 1.2611304555321112e-05, 1.2611304555321112e-05, 1.2611304555321112e-05, 1.2611304555321112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2611304555321112e-05

Optimization complete. Final v2v error: 2.995245933532715 mm

Highest mean error: 3.0346288681030273 mm for frame 85

Lowest mean error: 2.9458720684051514 mm for frame 218

Saving results

Total time: 28.795769214630127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833473
Iteration 2/25 | Loss: 0.00099904
Iteration 3/25 | Loss: 0.00082400
Iteration 4/25 | Loss: 0.00077721
Iteration 5/25 | Loss: 0.00075392
Iteration 6/25 | Loss: 0.00074855
Iteration 7/25 | Loss: 0.00074595
Iteration 8/25 | Loss: 0.00074495
Iteration 9/25 | Loss: 0.00074486
Iteration 10/25 | Loss: 0.00074486
Iteration 11/25 | Loss: 0.00074486
Iteration 12/25 | Loss: 0.00074486
Iteration 13/25 | Loss: 0.00074486
Iteration 14/25 | Loss: 0.00074486
Iteration 15/25 | Loss: 0.00074486
Iteration 16/25 | Loss: 0.00074486
Iteration 17/25 | Loss: 0.00074486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007448612013831735, 0.0007448612013831735, 0.0007448612013831735, 0.0007448612013831735, 0.0007448612013831735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007448612013831735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59230053
Iteration 2/25 | Loss: 0.00051104
Iteration 3/25 | Loss: 0.00051104
Iteration 4/25 | Loss: 0.00051104
Iteration 5/25 | Loss: 0.00051104
Iteration 6/25 | Loss: 0.00051104
Iteration 7/25 | Loss: 0.00051104
Iteration 8/25 | Loss: 0.00051104
Iteration 9/25 | Loss: 0.00051104
Iteration 10/25 | Loss: 0.00051104
Iteration 11/25 | Loss: 0.00051104
Iteration 12/25 | Loss: 0.00051104
Iteration 13/25 | Loss: 0.00051104
Iteration 14/25 | Loss: 0.00051104
Iteration 15/25 | Loss: 0.00051104
Iteration 16/25 | Loss: 0.00051104
Iteration 17/25 | Loss: 0.00051104
Iteration 18/25 | Loss: 0.00051104
Iteration 19/25 | Loss: 0.00051104
Iteration 20/25 | Loss: 0.00051104
Iteration 21/25 | Loss: 0.00051104
Iteration 22/25 | Loss: 0.00051104
Iteration 23/25 | Loss: 0.00051104
Iteration 24/25 | Loss: 0.00051104
Iteration 25/25 | Loss: 0.00051104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051104
Iteration 2/1000 | Loss: 0.00004701
Iteration 3/1000 | Loss: 0.00003622
Iteration 4/1000 | Loss: 0.00002954
Iteration 5/1000 | Loss: 0.00002739
Iteration 6/1000 | Loss: 0.00002635
Iteration 7/1000 | Loss: 0.00002550
Iteration 8/1000 | Loss: 0.00002497
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00002407
Iteration 11/1000 | Loss: 0.00002380
Iteration 12/1000 | Loss: 0.00002365
Iteration 13/1000 | Loss: 0.00002351
Iteration 14/1000 | Loss: 0.00002348
Iteration 15/1000 | Loss: 0.00002341
Iteration 16/1000 | Loss: 0.00002336
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002333
Iteration 20/1000 | Loss: 0.00002332
Iteration 21/1000 | Loss: 0.00002330
Iteration 22/1000 | Loss: 0.00002330
Iteration 23/1000 | Loss: 0.00002329
Iteration 24/1000 | Loss: 0.00002328
Iteration 25/1000 | Loss: 0.00002327
Iteration 26/1000 | Loss: 0.00002327
Iteration 27/1000 | Loss: 0.00002324
Iteration 28/1000 | Loss: 0.00002324
Iteration 29/1000 | Loss: 0.00002323
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002322
Iteration 32/1000 | Loss: 0.00002322
Iteration 33/1000 | Loss: 0.00002321
Iteration 34/1000 | Loss: 0.00002321
Iteration 35/1000 | Loss: 0.00002320
Iteration 36/1000 | Loss: 0.00002320
Iteration 37/1000 | Loss: 0.00002320
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002316
Iteration 40/1000 | Loss: 0.00002316
Iteration 41/1000 | Loss: 0.00002316
Iteration 42/1000 | Loss: 0.00002315
Iteration 43/1000 | Loss: 0.00002315
Iteration 44/1000 | Loss: 0.00002315
Iteration 45/1000 | Loss: 0.00002315
Iteration 46/1000 | Loss: 0.00002314
Iteration 47/1000 | Loss: 0.00002314
Iteration 48/1000 | Loss: 0.00002314
Iteration 49/1000 | Loss: 0.00002313
Iteration 50/1000 | Loss: 0.00002313
Iteration 51/1000 | Loss: 0.00002312
Iteration 52/1000 | Loss: 0.00002312
Iteration 53/1000 | Loss: 0.00002312
Iteration 54/1000 | Loss: 0.00002311
Iteration 55/1000 | Loss: 0.00002311
Iteration 56/1000 | Loss: 0.00002311
Iteration 57/1000 | Loss: 0.00002310
Iteration 58/1000 | Loss: 0.00002310
Iteration 59/1000 | Loss: 0.00002310
Iteration 60/1000 | Loss: 0.00002310
Iteration 61/1000 | Loss: 0.00002309
Iteration 62/1000 | Loss: 0.00002309
Iteration 63/1000 | Loss: 0.00002309
Iteration 64/1000 | Loss: 0.00002308
Iteration 65/1000 | Loss: 0.00002308
Iteration 66/1000 | Loss: 0.00002308
Iteration 67/1000 | Loss: 0.00002307
Iteration 68/1000 | Loss: 0.00002307
Iteration 69/1000 | Loss: 0.00002307
Iteration 70/1000 | Loss: 0.00002307
Iteration 71/1000 | Loss: 0.00002306
Iteration 72/1000 | Loss: 0.00002306
Iteration 73/1000 | Loss: 0.00002306
Iteration 74/1000 | Loss: 0.00002306
Iteration 75/1000 | Loss: 0.00002306
Iteration 76/1000 | Loss: 0.00002306
Iteration 77/1000 | Loss: 0.00002305
Iteration 78/1000 | Loss: 0.00002305
Iteration 79/1000 | Loss: 0.00002305
Iteration 80/1000 | Loss: 0.00002305
Iteration 81/1000 | Loss: 0.00002305
Iteration 82/1000 | Loss: 0.00002305
Iteration 83/1000 | Loss: 0.00002305
Iteration 84/1000 | Loss: 0.00002305
Iteration 85/1000 | Loss: 0.00002304
Iteration 86/1000 | Loss: 0.00002304
Iteration 87/1000 | Loss: 0.00002304
Iteration 88/1000 | Loss: 0.00002304
Iteration 89/1000 | Loss: 0.00002304
Iteration 90/1000 | Loss: 0.00002304
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002304
Iteration 95/1000 | Loss: 0.00002304
Iteration 96/1000 | Loss: 0.00002304
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002303
Iteration 99/1000 | Loss: 0.00002303
Iteration 100/1000 | Loss: 0.00002303
Iteration 101/1000 | Loss: 0.00002303
Iteration 102/1000 | Loss: 0.00002303
Iteration 103/1000 | Loss: 0.00002303
Iteration 104/1000 | Loss: 0.00002303
Iteration 105/1000 | Loss: 0.00002303
Iteration 106/1000 | Loss: 0.00002303
Iteration 107/1000 | Loss: 0.00002303
Iteration 108/1000 | Loss: 0.00002302
Iteration 109/1000 | Loss: 0.00002302
Iteration 110/1000 | Loss: 0.00002302
Iteration 111/1000 | Loss: 0.00002302
Iteration 112/1000 | Loss: 0.00002302
Iteration 113/1000 | Loss: 0.00002302
Iteration 114/1000 | Loss: 0.00002302
Iteration 115/1000 | Loss: 0.00002302
Iteration 116/1000 | Loss: 0.00002302
Iteration 117/1000 | Loss: 0.00002302
Iteration 118/1000 | Loss: 0.00002301
Iteration 119/1000 | Loss: 0.00002301
Iteration 120/1000 | Loss: 0.00002301
Iteration 121/1000 | Loss: 0.00002301
Iteration 122/1000 | Loss: 0.00002301
Iteration 123/1000 | Loss: 0.00002301
Iteration 124/1000 | Loss: 0.00002301
Iteration 125/1000 | Loss: 0.00002301
Iteration 126/1000 | Loss: 0.00002301
Iteration 127/1000 | Loss: 0.00002301
Iteration 128/1000 | Loss: 0.00002301
Iteration 129/1000 | Loss: 0.00002301
Iteration 130/1000 | Loss: 0.00002301
Iteration 131/1000 | Loss: 0.00002300
Iteration 132/1000 | Loss: 0.00002300
Iteration 133/1000 | Loss: 0.00002300
Iteration 134/1000 | Loss: 0.00002300
Iteration 135/1000 | Loss: 0.00002300
Iteration 136/1000 | Loss: 0.00002300
Iteration 137/1000 | Loss: 0.00002300
Iteration 138/1000 | Loss: 0.00002300
Iteration 139/1000 | Loss: 0.00002300
Iteration 140/1000 | Loss: 0.00002300
Iteration 141/1000 | Loss: 0.00002300
Iteration 142/1000 | Loss: 0.00002300
Iteration 143/1000 | Loss: 0.00002299
Iteration 144/1000 | Loss: 0.00002299
Iteration 145/1000 | Loss: 0.00002299
Iteration 146/1000 | Loss: 0.00002299
Iteration 147/1000 | Loss: 0.00002299
Iteration 148/1000 | Loss: 0.00002299
Iteration 149/1000 | Loss: 0.00002299
Iteration 150/1000 | Loss: 0.00002299
Iteration 151/1000 | Loss: 0.00002299
Iteration 152/1000 | Loss: 0.00002299
Iteration 153/1000 | Loss: 0.00002299
Iteration 154/1000 | Loss: 0.00002299
Iteration 155/1000 | Loss: 0.00002299
Iteration 156/1000 | Loss: 0.00002299
Iteration 157/1000 | Loss: 0.00002299
Iteration 158/1000 | Loss: 0.00002299
Iteration 159/1000 | Loss: 0.00002299
Iteration 160/1000 | Loss: 0.00002299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.2985914256423712e-05, 2.2985914256423712e-05, 2.2985914256423712e-05, 2.2985914256423712e-05, 2.2985914256423712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2985914256423712e-05

Optimization complete. Final v2v error: 3.901211738586426 mm

Highest mean error: 5.7166619300842285 mm for frame 128

Lowest mean error: 3.087148666381836 mm for frame 37

Saving results

Total time: 42.38488698005676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830575
Iteration 2/25 | Loss: 0.00107901
Iteration 3/25 | Loss: 0.00084676
Iteration 4/25 | Loss: 0.00078174
Iteration 5/25 | Loss: 0.00076918
Iteration 6/25 | Loss: 0.00076734
Iteration 7/25 | Loss: 0.00076655
Iteration 8/25 | Loss: 0.00076645
Iteration 9/25 | Loss: 0.00076645
Iteration 10/25 | Loss: 0.00076645
Iteration 11/25 | Loss: 0.00076645
Iteration 12/25 | Loss: 0.00076645
Iteration 13/25 | Loss: 0.00076645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007664532749913633, 0.0007664532749913633, 0.0007664532749913633, 0.0007664532749913633, 0.0007664532749913633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007664532749913633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61556363
Iteration 2/25 | Loss: 0.00047682
Iteration 3/25 | Loss: 0.00047680
Iteration 4/25 | Loss: 0.00047680
Iteration 5/25 | Loss: 0.00047680
Iteration 6/25 | Loss: 0.00047680
Iteration 7/25 | Loss: 0.00047680
Iteration 8/25 | Loss: 0.00047680
Iteration 9/25 | Loss: 0.00047680
Iteration 10/25 | Loss: 0.00047680
Iteration 11/25 | Loss: 0.00047680
Iteration 12/25 | Loss: 0.00047680
Iteration 13/25 | Loss: 0.00047680
Iteration 14/25 | Loss: 0.00047680
Iteration 15/25 | Loss: 0.00047680
Iteration 16/25 | Loss: 0.00047680
Iteration 17/25 | Loss: 0.00047680
Iteration 18/25 | Loss: 0.00047680
Iteration 19/25 | Loss: 0.00047680
Iteration 20/25 | Loss: 0.00047680
Iteration 21/25 | Loss: 0.00047680
Iteration 22/25 | Loss: 0.00047680
Iteration 23/25 | Loss: 0.00047680
Iteration 24/25 | Loss: 0.00047680
Iteration 25/25 | Loss: 0.00047680

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047680
Iteration 2/1000 | Loss: 0.00004381
Iteration 3/1000 | Loss: 0.00002709
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00002053
Iteration 6/1000 | Loss: 0.00001968
Iteration 7/1000 | Loss: 0.00001910
Iteration 8/1000 | Loss: 0.00001878
Iteration 9/1000 | Loss: 0.00001848
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001795
Iteration 15/1000 | Loss: 0.00001791
Iteration 16/1000 | Loss: 0.00001790
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001783
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001773
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001763
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001755
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001753
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001752
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001751
Iteration 59/1000 | Loss: 0.00001751
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001750
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001749
Iteration 68/1000 | Loss: 0.00001749
Iteration 69/1000 | Loss: 0.00001749
Iteration 70/1000 | Loss: 0.00001749
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001748
Iteration 73/1000 | Loss: 0.00001748
Iteration 74/1000 | Loss: 0.00001748
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001746
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001743
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001743
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001740
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001739
Iteration 141/1000 | Loss: 0.00001739
Iteration 142/1000 | Loss: 0.00001739
Iteration 143/1000 | Loss: 0.00001739
Iteration 144/1000 | Loss: 0.00001739
Iteration 145/1000 | Loss: 0.00001739
Iteration 146/1000 | Loss: 0.00001739
Iteration 147/1000 | Loss: 0.00001739
Iteration 148/1000 | Loss: 0.00001739
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001739
Iteration 153/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.738885293889325e-05, 1.738885293889325e-05, 1.738885293889325e-05, 1.738885293889325e-05, 1.738885293889325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.738885293889325e-05

Optimization complete. Final v2v error: 3.4678072929382324 mm

Highest mean error: 3.919090509414673 mm for frame 53

Lowest mean error: 2.853285551071167 mm for frame 148

Saving results

Total time: 42.54318714141846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101965
Iteration 2/25 | Loss: 0.01101964
Iteration 3/25 | Loss: 0.00292594
Iteration 4/25 | Loss: 0.00184765
Iteration 5/25 | Loss: 0.00211492
Iteration 6/25 | Loss: 0.00131096
Iteration 7/25 | Loss: 0.00107850
Iteration 8/25 | Loss: 0.00095927
Iteration 9/25 | Loss: 0.00089969
Iteration 10/25 | Loss: 0.00088688
Iteration 11/25 | Loss: 0.00087565
Iteration 12/25 | Loss: 0.00087365
Iteration 13/25 | Loss: 0.00087530
Iteration 14/25 | Loss: 0.00087602
Iteration 15/25 | Loss: 0.00087589
Iteration 16/25 | Loss: 0.00087006
Iteration 17/25 | Loss: 0.00087255
Iteration 18/25 | Loss: 0.00087041
Iteration 19/25 | Loss: 0.00087058
Iteration 20/25 | Loss: 0.00087092
Iteration 21/25 | Loss: 0.00087153
Iteration 22/25 | Loss: 0.00087171
Iteration 23/25 | Loss: 0.00087225
Iteration 24/25 | Loss: 0.00087035
Iteration 25/25 | Loss: 0.00086923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30473089
Iteration 2/25 | Loss: 0.00052348
Iteration 3/25 | Loss: 0.00052348
Iteration 4/25 | Loss: 0.00052348
Iteration 5/25 | Loss: 0.00052348
Iteration 6/25 | Loss: 0.00052348
Iteration 7/25 | Loss: 0.00052348
Iteration 8/25 | Loss: 0.00052348
Iteration 9/25 | Loss: 0.00052348
Iteration 10/25 | Loss: 0.00052348
Iteration 11/25 | Loss: 0.00052348
Iteration 12/25 | Loss: 0.00052348
Iteration 13/25 | Loss: 0.00052348
Iteration 14/25 | Loss: 0.00052348
Iteration 15/25 | Loss: 0.00052348
Iteration 16/25 | Loss: 0.00052348
Iteration 17/25 | Loss: 0.00052348
Iteration 18/25 | Loss: 0.00052348
Iteration 19/25 | Loss: 0.00052348
Iteration 20/25 | Loss: 0.00052348
Iteration 21/25 | Loss: 0.00052348
Iteration 22/25 | Loss: 0.00052348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005234802956692874, 0.0005234802956692874, 0.0005234802956692874, 0.0005234802956692874, 0.0005234802956692874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005234802956692874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052348
Iteration 2/1000 | Loss: 0.00004833
Iteration 3/1000 | Loss: 0.00005393
Iteration 4/1000 | Loss: 0.00003195
Iteration 5/1000 | Loss: 0.00016560
Iteration 6/1000 | Loss: 0.00004505
Iteration 7/1000 | Loss: 0.00004520
Iteration 8/1000 | Loss: 0.00003628
Iteration 9/1000 | Loss: 0.00004523
Iteration 10/1000 | Loss: 0.00004397
Iteration 11/1000 | Loss: 0.00003173
Iteration 12/1000 | Loss: 0.00003679
Iteration 13/1000 | Loss: 0.00002999
Iteration 14/1000 | Loss: 0.00003300
Iteration 15/1000 | Loss: 0.00004115
Iteration 16/1000 | Loss: 0.00003867
Iteration 17/1000 | Loss: 0.00004055
Iteration 18/1000 | Loss: 0.00003287
Iteration 19/1000 | Loss: 0.00004431
Iteration 20/1000 | Loss: 0.00003381
Iteration 21/1000 | Loss: 0.00004023
Iteration 22/1000 | Loss: 0.00004264
Iteration 23/1000 | Loss: 0.00003479
Iteration 24/1000 | Loss: 0.00003365
Iteration 25/1000 | Loss: 0.00004014
Iteration 26/1000 | Loss: 0.00003983
Iteration 27/1000 | Loss: 0.00004096
Iteration 28/1000 | Loss: 0.00003951
Iteration 29/1000 | Loss: 0.00004017
Iteration 30/1000 | Loss: 0.00003994
Iteration 31/1000 | Loss: 0.00004039
Iteration 32/1000 | Loss: 0.00003451
Iteration 33/1000 | Loss: 0.00002802
Iteration 34/1000 | Loss: 0.00004888
Iteration 35/1000 | Loss: 0.00003942
Iteration 36/1000 | Loss: 0.00003892
Iteration 37/1000 | Loss: 0.00003886
Iteration 38/1000 | Loss: 0.00003926
Iteration 39/1000 | Loss: 0.00005505
Iteration 40/1000 | Loss: 0.00004550
Iteration 41/1000 | Loss: 0.00006469
Iteration 42/1000 | Loss: 0.00004427
Iteration 43/1000 | Loss: 0.00004213
Iteration 44/1000 | Loss: 0.00006204
Iteration 45/1000 | Loss: 0.00003738
Iteration 46/1000 | Loss: 0.00003938
Iteration 47/1000 | Loss: 0.00003022
Iteration 48/1000 | Loss: 0.00002850
Iteration 49/1000 | Loss: 0.00003368
Iteration 50/1000 | Loss: 0.00003836
Iteration 51/1000 | Loss: 0.00005617
Iteration 52/1000 | Loss: 0.00003722
Iteration 53/1000 | Loss: 0.00004504
Iteration 54/1000 | Loss: 0.00003498
Iteration 55/1000 | Loss: 0.00003283
Iteration 56/1000 | Loss: 0.00004063
Iteration 57/1000 | Loss: 0.00004235
Iteration 58/1000 | Loss: 0.00003944
Iteration 59/1000 | Loss: 0.00003673
Iteration 60/1000 | Loss: 0.00005196
Iteration 61/1000 | Loss: 0.00003448
Iteration 62/1000 | Loss: 0.00004382
Iteration 63/1000 | Loss: 0.00003174
Iteration 64/1000 | Loss: 0.00005487
Iteration 65/1000 | Loss: 0.00002795
Iteration 66/1000 | Loss: 0.00003140
Iteration 67/1000 | Loss: 0.00004341
Iteration 68/1000 | Loss: 0.00003356
Iteration 69/1000 | Loss: 0.00004738
Iteration 70/1000 | Loss: 0.00003220
Iteration 71/1000 | Loss: 0.00003919
Iteration 72/1000 | Loss: 0.00003358
Iteration 73/1000 | Loss: 0.00005316
Iteration 74/1000 | Loss: 0.00003280
Iteration 75/1000 | Loss: 0.00005283
Iteration 76/1000 | Loss: 0.00002741
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002644
Iteration 79/1000 | Loss: 0.00002457
Iteration 80/1000 | Loss: 0.00002394
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002339
Iteration 84/1000 | Loss: 0.00002339
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002335
Iteration 87/1000 | Loss: 0.00002334
Iteration 88/1000 | Loss: 0.00002334
Iteration 89/1000 | Loss: 0.00002334
Iteration 90/1000 | Loss: 0.00002334
Iteration 91/1000 | Loss: 0.00002334
Iteration 92/1000 | Loss: 0.00002334
Iteration 93/1000 | Loss: 0.00002334
Iteration 94/1000 | Loss: 0.00002334
Iteration 95/1000 | Loss: 0.00002334
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002325
Iteration 101/1000 | Loss: 0.00002325
Iteration 102/1000 | Loss: 0.00002324
Iteration 103/1000 | Loss: 0.00002324
Iteration 104/1000 | Loss: 0.00002324
Iteration 105/1000 | Loss: 0.00002323
Iteration 106/1000 | Loss: 0.00002323
Iteration 107/1000 | Loss: 0.00002323
Iteration 108/1000 | Loss: 0.00002323
Iteration 109/1000 | Loss: 0.00002322
Iteration 110/1000 | Loss: 0.00002322
Iteration 111/1000 | Loss: 0.00002322
Iteration 112/1000 | Loss: 0.00002321
Iteration 113/1000 | Loss: 0.00002321
Iteration 114/1000 | Loss: 0.00002320
Iteration 115/1000 | Loss: 0.00002319
Iteration 116/1000 | Loss: 0.00002319
Iteration 117/1000 | Loss: 0.00002318
Iteration 118/1000 | Loss: 0.00002318
Iteration 119/1000 | Loss: 0.00002318
Iteration 120/1000 | Loss: 0.00002317
Iteration 121/1000 | Loss: 0.00002316
Iteration 122/1000 | Loss: 0.00002315
Iteration 123/1000 | Loss: 0.00002315
Iteration 124/1000 | Loss: 0.00002315
Iteration 125/1000 | Loss: 0.00002315
Iteration 126/1000 | Loss: 0.00002315
Iteration 127/1000 | Loss: 0.00002315
Iteration 128/1000 | Loss: 0.00002315
Iteration 129/1000 | Loss: 0.00002315
Iteration 130/1000 | Loss: 0.00002315
Iteration 131/1000 | Loss: 0.00002314
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002314
Iteration 135/1000 | Loss: 0.00002314
Iteration 136/1000 | Loss: 0.00002314
Iteration 137/1000 | Loss: 0.00002313
Iteration 138/1000 | Loss: 0.00002313
Iteration 139/1000 | Loss: 0.00002313
Iteration 140/1000 | Loss: 0.00002313
Iteration 141/1000 | Loss: 0.00002313
Iteration 142/1000 | Loss: 0.00002313
Iteration 143/1000 | Loss: 0.00002313
Iteration 144/1000 | Loss: 0.00002313
Iteration 145/1000 | Loss: 0.00002313
Iteration 146/1000 | Loss: 0.00002313
Iteration 147/1000 | Loss: 0.00002312
Iteration 148/1000 | Loss: 0.00002312
Iteration 149/1000 | Loss: 0.00002312
Iteration 150/1000 | Loss: 0.00002312
Iteration 151/1000 | Loss: 0.00002312
Iteration 152/1000 | Loss: 0.00002312
Iteration 153/1000 | Loss: 0.00002312
Iteration 154/1000 | Loss: 0.00002312
Iteration 155/1000 | Loss: 0.00002312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.312352808075957e-05, 2.312352808075957e-05, 2.312352808075957e-05, 2.312352808075957e-05, 2.312352808075957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.312352808075957e-05

Optimization complete. Final v2v error: 3.7587149143218994 mm

Highest mean error: 4.888203144073486 mm for frame 141

Lowest mean error: 3.396857738494873 mm for frame 12

Saving results

Total time: 188.28160500526428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886684
Iteration 2/25 | Loss: 0.00133480
Iteration 3/25 | Loss: 0.00086649
Iteration 4/25 | Loss: 0.00079485
Iteration 5/25 | Loss: 0.00077018
Iteration 6/25 | Loss: 0.00076262
Iteration 7/25 | Loss: 0.00076009
Iteration 8/25 | Loss: 0.00075881
Iteration 9/25 | Loss: 0.00075841
Iteration 10/25 | Loss: 0.00075839
Iteration 11/25 | Loss: 0.00075839
Iteration 12/25 | Loss: 0.00075839
Iteration 13/25 | Loss: 0.00075839
Iteration 14/25 | Loss: 0.00075839
Iteration 15/25 | Loss: 0.00075839
Iteration 16/25 | Loss: 0.00075839
Iteration 17/25 | Loss: 0.00075839
Iteration 18/25 | Loss: 0.00075839
Iteration 19/25 | Loss: 0.00075839
Iteration 20/25 | Loss: 0.00075839
Iteration 21/25 | Loss: 0.00075839
Iteration 22/25 | Loss: 0.00075839
Iteration 23/25 | Loss: 0.00075839
Iteration 24/25 | Loss: 0.00075839
Iteration 25/25 | Loss: 0.00075839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29318619
Iteration 2/25 | Loss: 0.00032507
Iteration 3/25 | Loss: 0.00032507
Iteration 4/25 | Loss: 0.00032507
Iteration 5/25 | Loss: 0.00032507
Iteration 6/25 | Loss: 0.00032506
Iteration 7/25 | Loss: 0.00032506
Iteration 8/25 | Loss: 0.00032506
Iteration 9/25 | Loss: 0.00032506
Iteration 10/25 | Loss: 0.00032506
Iteration 11/25 | Loss: 0.00032506
Iteration 12/25 | Loss: 0.00032506
Iteration 13/25 | Loss: 0.00032506
Iteration 14/25 | Loss: 0.00032506
Iteration 15/25 | Loss: 0.00032506
Iteration 16/25 | Loss: 0.00032506
Iteration 17/25 | Loss: 0.00032506
Iteration 18/25 | Loss: 0.00032506
Iteration 19/25 | Loss: 0.00032506
Iteration 20/25 | Loss: 0.00032506
Iteration 21/25 | Loss: 0.00032506
Iteration 22/25 | Loss: 0.00032506
Iteration 23/25 | Loss: 0.00032506
Iteration 24/25 | Loss: 0.00032506
Iteration 25/25 | Loss: 0.00032506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032506
Iteration 2/1000 | Loss: 0.00004181
Iteration 3/1000 | Loss: 0.00002960
Iteration 4/1000 | Loss: 0.00002618
Iteration 5/1000 | Loss: 0.00002480
Iteration 6/1000 | Loss: 0.00002396
Iteration 7/1000 | Loss: 0.00002336
Iteration 8/1000 | Loss: 0.00002290
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002217
Iteration 11/1000 | Loss: 0.00002195
Iteration 12/1000 | Loss: 0.00002177
Iteration 13/1000 | Loss: 0.00002166
Iteration 14/1000 | Loss: 0.00002150
Iteration 15/1000 | Loss: 0.00002136
Iteration 16/1000 | Loss: 0.00002135
Iteration 17/1000 | Loss: 0.00002132
Iteration 18/1000 | Loss: 0.00002131
Iteration 19/1000 | Loss: 0.00002130
Iteration 20/1000 | Loss: 0.00002129
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002127
Iteration 25/1000 | Loss: 0.00002126
Iteration 26/1000 | Loss: 0.00002126
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002125
Iteration 29/1000 | Loss: 0.00002123
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002122
Iteration 32/1000 | Loss: 0.00002122
Iteration 33/1000 | Loss: 0.00002121
Iteration 34/1000 | Loss: 0.00002121
Iteration 35/1000 | Loss: 0.00002121
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002119
Iteration 39/1000 | Loss: 0.00002119
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002118
Iteration 43/1000 | Loss: 0.00002118
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002117
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002117
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002116
Iteration 51/1000 | Loss: 0.00002116
Iteration 52/1000 | Loss: 0.00002116
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002114
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00002112
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002111
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002110
Iteration 81/1000 | Loss: 0.00002110
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002110
Iteration 84/1000 | Loss: 0.00002109
Iteration 85/1000 | Loss: 0.00002109
Iteration 86/1000 | Loss: 0.00002109
Iteration 87/1000 | Loss: 0.00002108
Iteration 88/1000 | Loss: 0.00002108
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00002108
Iteration 91/1000 | Loss: 0.00002107
Iteration 92/1000 | Loss: 0.00002107
Iteration 93/1000 | Loss: 0.00002107
Iteration 94/1000 | Loss: 0.00002107
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002105
Iteration 99/1000 | Loss: 0.00002105
Iteration 100/1000 | Loss: 0.00002105
Iteration 101/1000 | Loss: 0.00002105
Iteration 102/1000 | Loss: 0.00002105
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00002104
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002104
Iteration 107/1000 | Loss: 0.00002104
Iteration 108/1000 | Loss: 0.00002103
Iteration 109/1000 | Loss: 0.00002103
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002102
Iteration 112/1000 | Loss: 0.00002102
Iteration 113/1000 | Loss: 0.00002102
Iteration 114/1000 | Loss: 0.00002102
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002100
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002099
Iteration 124/1000 | Loss: 0.00002099
Iteration 125/1000 | Loss: 0.00002099
Iteration 126/1000 | Loss: 0.00002099
Iteration 127/1000 | Loss: 0.00002099
Iteration 128/1000 | Loss: 0.00002099
Iteration 129/1000 | Loss: 0.00002099
Iteration 130/1000 | Loss: 0.00002099
Iteration 131/1000 | Loss: 0.00002099
Iteration 132/1000 | Loss: 0.00002098
Iteration 133/1000 | Loss: 0.00002098
Iteration 134/1000 | Loss: 0.00002098
Iteration 135/1000 | Loss: 0.00002098
Iteration 136/1000 | Loss: 0.00002098
Iteration 137/1000 | Loss: 0.00002097
Iteration 138/1000 | Loss: 0.00002097
Iteration 139/1000 | Loss: 0.00002097
Iteration 140/1000 | Loss: 0.00002097
Iteration 141/1000 | Loss: 0.00002097
Iteration 142/1000 | Loss: 0.00002096
Iteration 143/1000 | Loss: 0.00002096
Iteration 144/1000 | Loss: 0.00002096
Iteration 145/1000 | Loss: 0.00002096
Iteration 146/1000 | Loss: 0.00002095
Iteration 147/1000 | Loss: 0.00002095
Iteration 148/1000 | Loss: 0.00002095
Iteration 149/1000 | Loss: 0.00002095
Iteration 150/1000 | Loss: 0.00002095
Iteration 151/1000 | Loss: 0.00002095
Iteration 152/1000 | Loss: 0.00002095
Iteration 153/1000 | Loss: 0.00002095
Iteration 154/1000 | Loss: 0.00002095
Iteration 155/1000 | Loss: 0.00002095
Iteration 156/1000 | Loss: 0.00002095
Iteration 157/1000 | Loss: 0.00002095
Iteration 158/1000 | Loss: 0.00002095
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002094
Iteration 165/1000 | Loss: 0.00002094
Iteration 166/1000 | Loss: 0.00002094
Iteration 167/1000 | Loss: 0.00002093
Iteration 168/1000 | Loss: 0.00002093
Iteration 169/1000 | Loss: 0.00002093
Iteration 170/1000 | Loss: 0.00002093
Iteration 171/1000 | Loss: 0.00002093
Iteration 172/1000 | Loss: 0.00002093
Iteration 173/1000 | Loss: 0.00002093
Iteration 174/1000 | Loss: 0.00002093
Iteration 175/1000 | Loss: 0.00002092
Iteration 176/1000 | Loss: 0.00002092
Iteration 177/1000 | Loss: 0.00002092
Iteration 178/1000 | Loss: 0.00002092
Iteration 179/1000 | Loss: 0.00002092
Iteration 180/1000 | Loss: 0.00002092
Iteration 181/1000 | Loss: 0.00002092
Iteration 182/1000 | Loss: 0.00002092
Iteration 183/1000 | Loss: 0.00002092
Iteration 184/1000 | Loss: 0.00002092
Iteration 185/1000 | Loss: 0.00002092
Iteration 186/1000 | Loss: 0.00002092
Iteration 187/1000 | Loss: 0.00002092
Iteration 188/1000 | Loss: 0.00002092
Iteration 189/1000 | Loss: 0.00002092
Iteration 190/1000 | Loss: 0.00002092
Iteration 191/1000 | Loss: 0.00002092
Iteration 192/1000 | Loss: 0.00002092
Iteration 193/1000 | Loss: 0.00002092
Iteration 194/1000 | Loss: 0.00002092
Iteration 195/1000 | Loss: 0.00002092
Iteration 196/1000 | Loss: 0.00002092
Iteration 197/1000 | Loss: 0.00002092
Iteration 198/1000 | Loss: 0.00002092
Iteration 199/1000 | Loss: 0.00002092
Iteration 200/1000 | Loss: 0.00002092
Iteration 201/1000 | Loss: 0.00002092
Iteration 202/1000 | Loss: 0.00002092
Iteration 203/1000 | Loss: 0.00002092
Iteration 204/1000 | Loss: 0.00002092
Iteration 205/1000 | Loss: 0.00002092
Iteration 206/1000 | Loss: 0.00002092
Iteration 207/1000 | Loss: 0.00002092
Iteration 208/1000 | Loss: 0.00002092
Iteration 209/1000 | Loss: 0.00002092
Iteration 210/1000 | Loss: 0.00002092
Iteration 211/1000 | Loss: 0.00002092
Iteration 212/1000 | Loss: 0.00002092
Iteration 213/1000 | Loss: 0.00002092
Iteration 214/1000 | Loss: 0.00002092
Iteration 215/1000 | Loss: 0.00002092
Iteration 216/1000 | Loss: 0.00002092
Iteration 217/1000 | Loss: 0.00002092
Iteration 218/1000 | Loss: 0.00002092
Iteration 219/1000 | Loss: 0.00002092
Iteration 220/1000 | Loss: 0.00002092
Iteration 221/1000 | Loss: 0.00002092
Iteration 222/1000 | Loss: 0.00002092
Iteration 223/1000 | Loss: 0.00002092
Iteration 224/1000 | Loss: 0.00002092
Iteration 225/1000 | Loss: 0.00002092
Iteration 226/1000 | Loss: 0.00002092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.092272916343063e-05, 2.092272916343063e-05, 2.092272916343063e-05, 2.092272916343063e-05, 2.092272916343063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.092272916343063e-05

Optimization complete. Final v2v error: 3.734708786010742 mm

Highest mean error: 6.174987316131592 mm for frame 91

Lowest mean error: 2.708280563354492 mm for frame 13

Saving results

Total time: 48.39919376373291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864242
Iteration 2/25 | Loss: 0.00138878
Iteration 3/25 | Loss: 0.00088214
Iteration 4/25 | Loss: 0.00084082
Iteration 5/25 | Loss: 0.00083210
Iteration 6/25 | Loss: 0.00083024
Iteration 7/25 | Loss: 0.00082980
Iteration 8/25 | Loss: 0.00082980
Iteration 9/25 | Loss: 0.00082980
Iteration 10/25 | Loss: 0.00082980
Iteration 11/25 | Loss: 0.00082980
Iteration 12/25 | Loss: 0.00082980
Iteration 13/25 | Loss: 0.00082980
Iteration 14/25 | Loss: 0.00082980
Iteration 15/25 | Loss: 0.00082980
Iteration 16/25 | Loss: 0.00082980
Iteration 17/25 | Loss: 0.00082980
Iteration 18/25 | Loss: 0.00082980
Iteration 19/25 | Loss: 0.00082980
Iteration 20/25 | Loss: 0.00082980
Iteration 21/25 | Loss: 0.00082980
Iteration 22/25 | Loss: 0.00082980
Iteration 23/25 | Loss: 0.00082980
Iteration 24/25 | Loss: 0.00082980
Iteration 25/25 | Loss: 0.00082980

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61668503
Iteration 2/25 | Loss: 0.00033648
Iteration 3/25 | Loss: 0.00033647
Iteration 4/25 | Loss: 0.00033647
Iteration 5/25 | Loss: 0.00033647
Iteration 6/25 | Loss: 0.00033647
Iteration 7/25 | Loss: 0.00033647
Iteration 8/25 | Loss: 0.00033647
Iteration 9/25 | Loss: 0.00033647
Iteration 10/25 | Loss: 0.00033647
Iteration 11/25 | Loss: 0.00033647
Iteration 12/25 | Loss: 0.00033647
Iteration 13/25 | Loss: 0.00033647
Iteration 14/25 | Loss: 0.00033647
Iteration 15/25 | Loss: 0.00033647
Iteration 16/25 | Loss: 0.00033647
Iteration 17/25 | Loss: 0.00033647
Iteration 18/25 | Loss: 0.00033647
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003364660660736263, 0.0003364660660736263, 0.0003364660660736263, 0.0003364660660736263, 0.0003364660660736263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003364660660736263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033647
Iteration 2/1000 | Loss: 0.00004738
Iteration 3/1000 | Loss: 0.00003684
Iteration 4/1000 | Loss: 0.00003398
Iteration 5/1000 | Loss: 0.00003292
Iteration 6/1000 | Loss: 0.00003211
Iteration 7/1000 | Loss: 0.00003159
Iteration 8/1000 | Loss: 0.00003093
Iteration 9/1000 | Loss: 0.00003048
Iteration 10/1000 | Loss: 0.00003014
Iteration 11/1000 | Loss: 0.00002989
Iteration 12/1000 | Loss: 0.00002970
Iteration 13/1000 | Loss: 0.00002966
Iteration 14/1000 | Loss: 0.00002958
Iteration 15/1000 | Loss: 0.00002951
Iteration 16/1000 | Loss: 0.00002951
Iteration 17/1000 | Loss: 0.00002942
Iteration 18/1000 | Loss: 0.00002942
Iteration 19/1000 | Loss: 0.00002939
Iteration 20/1000 | Loss: 0.00002937
Iteration 21/1000 | Loss: 0.00002935
Iteration 22/1000 | Loss: 0.00002935
Iteration 23/1000 | Loss: 0.00002934
Iteration 24/1000 | Loss: 0.00002934
Iteration 25/1000 | Loss: 0.00002934
Iteration 26/1000 | Loss: 0.00002934
Iteration 27/1000 | Loss: 0.00002932
Iteration 28/1000 | Loss: 0.00002932
Iteration 29/1000 | Loss: 0.00002931
Iteration 30/1000 | Loss: 0.00002931
Iteration 31/1000 | Loss: 0.00002930
Iteration 32/1000 | Loss: 0.00002929
Iteration 33/1000 | Loss: 0.00002929
Iteration 34/1000 | Loss: 0.00002926
Iteration 35/1000 | Loss: 0.00002925
Iteration 36/1000 | Loss: 0.00002925
Iteration 37/1000 | Loss: 0.00002925
Iteration 38/1000 | Loss: 0.00002923
Iteration 39/1000 | Loss: 0.00002923
Iteration 40/1000 | Loss: 0.00002923
Iteration 41/1000 | Loss: 0.00002922
Iteration 42/1000 | Loss: 0.00002922
Iteration 43/1000 | Loss: 0.00002921
Iteration 44/1000 | Loss: 0.00002921
Iteration 45/1000 | Loss: 0.00002920
Iteration 46/1000 | Loss: 0.00002916
Iteration 47/1000 | Loss: 0.00002916
Iteration 48/1000 | Loss: 0.00002912
Iteration 49/1000 | Loss: 0.00002911
Iteration 50/1000 | Loss: 0.00002911
Iteration 51/1000 | Loss: 0.00002910
Iteration 52/1000 | Loss: 0.00002910
Iteration 53/1000 | Loss: 0.00002910
Iteration 54/1000 | Loss: 0.00002910
Iteration 55/1000 | Loss: 0.00002910
Iteration 56/1000 | Loss: 0.00002909
Iteration 57/1000 | Loss: 0.00002909
Iteration 58/1000 | Loss: 0.00002908
Iteration 59/1000 | Loss: 0.00002908
Iteration 60/1000 | Loss: 0.00002908
Iteration 61/1000 | Loss: 0.00002908
Iteration 62/1000 | Loss: 0.00002908
Iteration 63/1000 | Loss: 0.00002908
Iteration 64/1000 | Loss: 0.00002908
Iteration 65/1000 | Loss: 0.00002908
Iteration 66/1000 | Loss: 0.00002908
Iteration 67/1000 | Loss: 0.00002908
Iteration 68/1000 | Loss: 0.00002907
Iteration 69/1000 | Loss: 0.00002907
Iteration 70/1000 | Loss: 0.00002907
Iteration 71/1000 | Loss: 0.00002907
Iteration 72/1000 | Loss: 0.00002907
Iteration 73/1000 | Loss: 0.00002907
Iteration 74/1000 | Loss: 0.00002907
Iteration 75/1000 | Loss: 0.00002907
Iteration 76/1000 | Loss: 0.00002906
Iteration 77/1000 | Loss: 0.00002906
Iteration 78/1000 | Loss: 0.00002905
Iteration 79/1000 | Loss: 0.00002905
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002903
Iteration 82/1000 | Loss: 0.00002903
Iteration 83/1000 | Loss: 0.00002903
Iteration 84/1000 | Loss: 0.00002903
Iteration 85/1000 | Loss: 0.00002902
Iteration 86/1000 | Loss: 0.00002902
Iteration 87/1000 | Loss: 0.00002902
Iteration 88/1000 | Loss: 0.00002902
Iteration 89/1000 | Loss: 0.00002902
Iteration 90/1000 | Loss: 0.00002902
Iteration 91/1000 | Loss: 0.00002902
Iteration 92/1000 | Loss: 0.00002901
Iteration 93/1000 | Loss: 0.00002901
Iteration 94/1000 | Loss: 0.00002901
Iteration 95/1000 | Loss: 0.00002901
Iteration 96/1000 | Loss: 0.00002901
Iteration 97/1000 | Loss: 0.00002901
Iteration 98/1000 | Loss: 0.00002901
Iteration 99/1000 | Loss: 0.00002901
Iteration 100/1000 | Loss: 0.00002901
Iteration 101/1000 | Loss: 0.00002901
Iteration 102/1000 | Loss: 0.00002901
Iteration 103/1000 | Loss: 0.00002901
Iteration 104/1000 | Loss: 0.00002901
Iteration 105/1000 | Loss: 0.00002900
Iteration 106/1000 | Loss: 0.00002900
Iteration 107/1000 | Loss: 0.00002900
Iteration 108/1000 | Loss: 0.00002900
Iteration 109/1000 | Loss: 0.00002900
Iteration 110/1000 | Loss: 0.00002899
Iteration 111/1000 | Loss: 0.00002899
Iteration 112/1000 | Loss: 0.00002899
Iteration 113/1000 | Loss: 0.00002899
Iteration 114/1000 | Loss: 0.00002899
Iteration 115/1000 | Loss: 0.00002899
Iteration 116/1000 | Loss: 0.00002899
Iteration 117/1000 | Loss: 0.00002898
Iteration 118/1000 | Loss: 0.00002898
Iteration 119/1000 | Loss: 0.00002898
Iteration 120/1000 | Loss: 0.00002898
Iteration 121/1000 | Loss: 0.00002898
Iteration 122/1000 | Loss: 0.00002898
Iteration 123/1000 | Loss: 0.00002898
Iteration 124/1000 | Loss: 0.00002897
Iteration 125/1000 | Loss: 0.00002897
Iteration 126/1000 | Loss: 0.00002897
Iteration 127/1000 | Loss: 0.00002897
Iteration 128/1000 | Loss: 0.00002897
Iteration 129/1000 | Loss: 0.00002897
Iteration 130/1000 | Loss: 0.00002897
Iteration 131/1000 | Loss: 0.00002897
Iteration 132/1000 | Loss: 0.00002896
Iteration 133/1000 | Loss: 0.00002896
Iteration 134/1000 | Loss: 0.00002896
Iteration 135/1000 | Loss: 0.00002896
Iteration 136/1000 | Loss: 0.00002896
Iteration 137/1000 | Loss: 0.00002896
Iteration 138/1000 | Loss: 0.00002896
Iteration 139/1000 | Loss: 0.00002896
Iteration 140/1000 | Loss: 0.00002896
Iteration 141/1000 | Loss: 0.00002896
Iteration 142/1000 | Loss: 0.00002896
Iteration 143/1000 | Loss: 0.00002895
Iteration 144/1000 | Loss: 0.00002895
Iteration 145/1000 | Loss: 0.00002895
Iteration 146/1000 | Loss: 0.00002895
Iteration 147/1000 | Loss: 0.00002895
Iteration 148/1000 | Loss: 0.00002895
Iteration 149/1000 | Loss: 0.00002895
Iteration 150/1000 | Loss: 0.00002895
Iteration 151/1000 | Loss: 0.00002895
Iteration 152/1000 | Loss: 0.00002895
Iteration 153/1000 | Loss: 0.00002895
Iteration 154/1000 | Loss: 0.00002894
Iteration 155/1000 | Loss: 0.00002894
Iteration 156/1000 | Loss: 0.00002894
Iteration 157/1000 | Loss: 0.00002894
Iteration 158/1000 | Loss: 0.00002894
Iteration 159/1000 | Loss: 0.00002894
Iteration 160/1000 | Loss: 0.00002894
Iteration 161/1000 | Loss: 0.00002894
Iteration 162/1000 | Loss: 0.00002894
Iteration 163/1000 | Loss: 0.00002893
Iteration 164/1000 | Loss: 0.00002893
Iteration 165/1000 | Loss: 0.00002893
Iteration 166/1000 | Loss: 0.00002893
Iteration 167/1000 | Loss: 0.00002893
Iteration 168/1000 | Loss: 0.00002893
Iteration 169/1000 | Loss: 0.00002893
Iteration 170/1000 | Loss: 0.00002893
Iteration 171/1000 | Loss: 0.00002893
Iteration 172/1000 | Loss: 0.00002892
Iteration 173/1000 | Loss: 0.00002892
Iteration 174/1000 | Loss: 0.00002892
Iteration 175/1000 | Loss: 0.00002892
Iteration 176/1000 | Loss: 0.00002892
Iteration 177/1000 | Loss: 0.00002892
Iteration 178/1000 | Loss: 0.00002892
Iteration 179/1000 | Loss: 0.00002892
Iteration 180/1000 | Loss: 0.00002891
Iteration 181/1000 | Loss: 0.00002891
Iteration 182/1000 | Loss: 0.00002891
Iteration 183/1000 | Loss: 0.00002891
Iteration 184/1000 | Loss: 0.00002891
Iteration 185/1000 | Loss: 0.00002891
Iteration 186/1000 | Loss: 0.00002891
Iteration 187/1000 | Loss: 0.00002891
Iteration 188/1000 | Loss: 0.00002891
Iteration 189/1000 | Loss: 0.00002891
Iteration 190/1000 | Loss: 0.00002891
Iteration 191/1000 | Loss: 0.00002890
Iteration 192/1000 | Loss: 0.00002890
Iteration 193/1000 | Loss: 0.00002890
Iteration 194/1000 | Loss: 0.00002890
Iteration 195/1000 | Loss: 0.00002890
Iteration 196/1000 | Loss: 0.00002890
Iteration 197/1000 | Loss: 0.00002890
Iteration 198/1000 | Loss: 0.00002890
Iteration 199/1000 | Loss: 0.00002890
Iteration 200/1000 | Loss: 0.00002890
Iteration 201/1000 | Loss: 0.00002890
Iteration 202/1000 | Loss: 0.00002890
Iteration 203/1000 | Loss: 0.00002889
Iteration 204/1000 | Loss: 0.00002889
Iteration 205/1000 | Loss: 0.00002889
Iteration 206/1000 | Loss: 0.00002889
Iteration 207/1000 | Loss: 0.00002889
Iteration 208/1000 | Loss: 0.00002889
Iteration 209/1000 | Loss: 0.00002889
Iteration 210/1000 | Loss: 0.00002889
Iteration 211/1000 | Loss: 0.00002889
Iteration 212/1000 | Loss: 0.00002889
Iteration 213/1000 | Loss: 0.00002889
Iteration 214/1000 | Loss: 0.00002889
Iteration 215/1000 | Loss: 0.00002889
Iteration 216/1000 | Loss: 0.00002888
Iteration 217/1000 | Loss: 0.00002888
Iteration 218/1000 | Loss: 0.00002888
Iteration 219/1000 | Loss: 0.00002888
Iteration 220/1000 | Loss: 0.00002888
Iteration 221/1000 | Loss: 0.00002888
Iteration 222/1000 | Loss: 0.00002888
Iteration 223/1000 | Loss: 0.00002888
Iteration 224/1000 | Loss: 0.00002888
Iteration 225/1000 | Loss: 0.00002888
Iteration 226/1000 | Loss: 0.00002887
Iteration 227/1000 | Loss: 0.00002887
Iteration 228/1000 | Loss: 0.00002887
Iteration 229/1000 | Loss: 0.00002887
Iteration 230/1000 | Loss: 0.00002887
Iteration 231/1000 | Loss: 0.00002887
Iteration 232/1000 | Loss: 0.00002887
Iteration 233/1000 | Loss: 0.00002887
Iteration 234/1000 | Loss: 0.00002887
Iteration 235/1000 | Loss: 0.00002887
Iteration 236/1000 | Loss: 0.00002887
Iteration 237/1000 | Loss: 0.00002886
Iteration 238/1000 | Loss: 0.00002886
Iteration 239/1000 | Loss: 0.00002886
Iteration 240/1000 | Loss: 0.00002886
Iteration 241/1000 | Loss: 0.00002886
Iteration 242/1000 | Loss: 0.00002886
Iteration 243/1000 | Loss: 0.00002886
Iteration 244/1000 | Loss: 0.00002886
Iteration 245/1000 | Loss: 0.00002886
Iteration 246/1000 | Loss: 0.00002885
Iteration 247/1000 | Loss: 0.00002885
Iteration 248/1000 | Loss: 0.00002885
Iteration 249/1000 | Loss: 0.00002885
Iteration 250/1000 | Loss: 0.00002885
Iteration 251/1000 | Loss: 0.00002885
Iteration 252/1000 | Loss: 0.00002885
Iteration 253/1000 | Loss: 0.00002885
Iteration 254/1000 | Loss: 0.00002885
Iteration 255/1000 | Loss: 0.00002885
Iteration 256/1000 | Loss: 0.00002885
Iteration 257/1000 | Loss: 0.00002885
Iteration 258/1000 | Loss: 0.00002885
Iteration 259/1000 | Loss: 0.00002885
Iteration 260/1000 | Loss: 0.00002885
Iteration 261/1000 | Loss: 0.00002885
Iteration 262/1000 | Loss: 0.00002885
Iteration 263/1000 | Loss: 0.00002885
Iteration 264/1000 | Loss: 0.00002885
Iteration 265/1000 | Loss: 0.00002885
Iteration 266/1000 | Loss: 0.00002885
Iteration 267/1000 | Loss: 0.00002884
Iteration 268/1000 | Loss: 0.00002884
Iteration 269/1000 | Loss: 0.00002884
Iteration 270/1000 | Loss: 0.00002884
Iteration 271/1000 | Loss: 0.00002884
Iteration 272/1000 | Loss: 0.00002884
Iteration 273/1000 | Loss: 0.00002884
Iteration 274/1000 | Loss: 0.00002884
Iteration 275/1000 | Loss: 0.00002884
Iteration 276/1000 | Loss: 0.00002884
Iteration 277/1000 | Loss: 0.00002884
Iteration 278/1000 | Loss: 0.00002884
Iteration 279/1000 | Loss: 0.00002884
Iteration 280/1000 | Loss: 0.00002884
Iteration 281/1000 | Loss: 0.00002884
Iteration 282/1000 | Loss: 0.00002884
Iteration 283/1000 | Loss: 0.00002884
Iteration 284/1000 | Loss: 0.00002884
Iteration 285/1000 | Loss: 0.00002884
Iteration 286/1000 | Loss: 0.00002884
Iteration 287/1000 | Loss: 0.00002884
Iteration 288/1000 | Loss: 0.00002884
Iteration 289/1000 | Loss: 0.00002884
Iteration 290/1000 | Loss: 0.00002884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [2.8840671802754514e-05, 2.8840671802754514e-05, 2.8840671802754514e-05, 2.8840671802754514e-05, 2.8840671802754514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8840671802754514e-05

Optimization complete. Final v2v error: 4.455257892608643 mm

Highest mean error: 5.80247688293457 mm for frame 0

Lowest mean error: 3.7126615047454834 mm for frame 12

Saving results

Total time: 52.114009380340576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823357
Iteration 2/25 | Loss: 0.00084796
Iteration 3/25 | Loss: 0.00069766
Iteration 4/25 | Loss: 0.00067617
Iteration 5/25 | Loss: 0.00067094
Iteration 6/25 | Loss: 0.00066886
Iteration 7/25 | Loss: 0.00066826
Iteration 8/25 | Loss: 0.00066826
Iteration 9/25 | Loss: 0.00066826
Iteration 10/25 | Loss: 0.00066826
Iteration 11/25 | Loss: 0.00066826
Iteration 12/25 | Loss: 0.00066826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006682585226371884, 0.0006682585226371884, 0.0006682585226371884, 0.0006682585226371884, 0.0006682585226371884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006682585226371884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48676419
Iteration 2/25 | Loss: 0.00032427
Iteration 3/25 | Loss: 0.00032427
Iteration 4/25 | Loss: 0.00032427
Iteration 5/25 | Loss: 0.00032427
Iteration 6/25 | Loss: 0.00032427
Iteration 7/25 | Loss: 0.00032427
Iteration 8/25 | Loss: 0.00032427
Iteration 9/25 | Loss: 0.00032427
Iteration 10/25 | Loss: 0.00032427
Iteration 11/25 | Loss: 0.00032427
Iteration 12/25 | Loss: 0.00032427
Iteration 13/25 | Loss: 0.00032427
Iteration 14/25 | Loss: 0.00032427
Iteration 15/25 | Loss: 0.00032427
Iteration 16/25 | Loss: 0.00032427
Iteration 17/25 | Loss: 0.00032427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003242688253521919, 0.0003242688253521919, 0.0003242688253521919, 0.0003242688253521919, 0.0003242688253521919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003242688253521919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032427
Iteration 2/1000 | Loss: 0.00002257
Iteration 3/1000 | Loss: 0.00001451
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001263
Iteration 6/1000 | Loss: 0.00001195
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001142
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001128
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001113
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001110
Iteration 18/1000 | Loss: 0.00001104
Iteration 19/1000 | Loss: 0.00001096
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001091
Iteration 27/1000 | Loss: 0.00001091
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001088
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001085
Iteration 39/1000 | Loss: 0.00001085
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001082
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001080
Iteration 53/1000 | Loss: 0.00001080
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001078
Iteration 65/1000 | Loss: 0.00001078
Iteration 66/1000 | Loss: 0.00001078
Iteration 67/1000 | Loss: 0.00001078
Iteration 68/1000 | Loss: 0.00001078
Iteration 69/1000 | Loss: 0.00001078
Iteration 70/1000 | Loss: 0.00001078
Iteration 71/1000 | Loss: 0.00001078
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001077
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001075
Iteration 111/1000 | Loss: 0.00001075
Iteration 112/1000 | Loss: 0.00001075
Iteration 113/1000 | Loss: 0.00001075
Iteration 114/1000 | Loss: 0.00001075
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001073
Iteration 121/1000 | Loss: 0.00001073
Iteration 122/1000 | Loss: 0.00001073
Iteration 123/1000 | Loss: 0.00001073
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001072
Iteration 127/1000 | Loss: 0.00001072
Iteration 128/1000 | Loss: 0.00001072
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001072
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001072
Iteration 150/1000 | Loss: 0.00001072
Iteration 151/1000 | Loss: 0.00001072
Iteration 152/1000 | Loss: 0.00001072
Iteration 153/1000 | Loss: 0.00001072
Iteration 154/1000 | Loss: 0.00001072
Iteration 155/1000 | Loss: 0.00001072
Iteration 156/1000 | Loss: 0.00001072
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001072
Iteration 166/1000 | Loss: 0.00001072
Iteration 167/1000 | Loss: 0.00001072
Iteration 168/1000 | Loss: 0.00001072
Iteration 169/1000 | Loss: 0.00001072
Iteration 170/1000 | Loss: 0.00001072
Iteration 171/1000 | Loss: 0.00001072
Iteration 172/1000 | Loss: 0.00001072
Iteration 173/1000 | Loss: 0.00001072
Iteration 174/1000 | Loss: 0.00001072
Iteration 175/1000 | Loss: 0.00001072
Iteration 176/1000 | Loss: 0.00001072
Iteration 177/1000 | Loss: 0.00001072
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Iteration 181/1000 | Loss: 0.00001072
Iteration 182/1000 | Loss: 0.00001072
Iteration 183/1000 | Loss: 0.00001072
Iteration 184/1000 | Loss: 0.00001072
Iteration 185/1000 | Loss: 0.00001072
Iteration 186/1000 | Loss: 0.00001072
Iteration 187/1000 | Loss: 0.00001072
Iteration 188/1000 | Loss: 0.00001072
Iteration 189/1000 | Loss: 0.00001072
Iteration 190/1000 | Loss: 0.00001072
Iteration 191/1000 | Loss: 0.00001072
Iteration 192/1000 | Loss: 0.00001072
Iteration 193/1000 | Loss: 0.00001072
Iteration 194/1000 | Loss: 0.00001072
Iteration 195/1000 | Loss: 0.00001072
Iteration 196/1000 | Loss: 0.00001072
Iteration 197/1000 | Loss: 0.00001072
Iteration 198/1000 | Loss: 0.00001072
Iteration 199/1000 | Loss: 0.00001072
Iteration 200/1000 | Loss: 0.00001072
Iteration 201/1000 | Loss: 0.00001072
Iteration 202/1000 | Loss: 0.00001072
Iteration 203/1000 | Loss: 0.00001072
Iteration 204/1000 | Loss: 0.00001072
Iteration 205/1000 | Loss: 0.00001072
Iteration 206/1000 | Loss: 0.00001072
Iteration 207/1000 | Loss: 0.00001072
Iteration 208/1000 | Loss: 0.00001072
Iteration 209/1000 | Loss: 0.00001072
Iteration 210/1000 | Loss: 0.00001072
Iteration 211/1000 | Loss: 0.00001072
Iteration 212/1000 | Loss: 0.00001072
Iteration 213/1000 | Loss: 0.00001072
Iteration 214/1000 | Loss: 0.00001072
Iteration 215/1000 | Loss: 0.00001072
Iteration 216/1000 | Loss: 0.00001072
Iteration 217/1000 | Loss: 0.00001072
Iteration 218/1000 | Loss: 0.00001072
Iteration 219/1000 | Loss: 0.00001072
Iteration 220/1000 | Loss: 0.00001072
Iteration 221/1000 | Loss: 0.00001072
Iteration 222/1000 | Loss: 0.00001072
Iteration 223/1000 | Loss: 0.00001072
Iteration 224/1000 | Loss: 0.00001072
Iteration 225/1000 | Loss: 0.00001072
Iteration 226/1000 | Loss: 0.00001072
Iteration 227/1000 | Loss: 0.00001072
Iteration 228/1000 | Loss: 0.00001072
Iteration 229/1000 | Loss: 0.00001072
Iteration 230/1000 | Loss: 0.00001072
Iteration 231/1000 | Loss: 0.00001072
Iteration 232/1000 | Loss: 0.00001072
Iteration 233/1000 | Loss: 0.00001072
Iteration 234/1000 | Loss: 0.00001072
Iteration 235/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.0717912118707318e-05, 1.0717912118707318e-05, 1.0717912118707318e-05, 1.0717912118707318e-05, 1.0717912118707318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0717912118707318e-05

Optimization complete. Final v2v error: 2.751765251159668 mm

Highest mean error: 3.171032190322876 mm for frame 91

Lowest mean error: 2.6122114658355713 mm for frame 39

Saving results

Total time: 37.58119535446167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892363
Iteration 2/25 | Loss: 0.00121163
Iteration 3/25 | Loss: 0.00086955
Iteration 4/25 | Loss: 0.00082935
Iteration 5/25 | Loss: 0.00082109
Iteration 6/25 | Loss: 0.00081973
Iteration 7/25 | Loss: 0.00081971
Iteration 8/25 | Loss: 0.00081971
Iteration 9/25 | Loss: 0.00081971
Iteration 10/25 | Loss: 0.00081971
Iteration 11/25 | Loss: 0.00081971
Iteration 12/25 | Loss: 0.00081971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008197072893381119, 0.0008197072893381119, 0.0008197072893381119, 0.0008197072893381119, 0.0008197072893381119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008197072893381119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02042377
Iteration 2/25 | Loss: 0.00034441
Iteration 3/25 | Loss: 0.00034440
Iteration 4/25 | Loss: 0.00034440
Iteration 5/25 | Loss: 0.00034440
Iteration 6/25 | Loss: 0.00034440
Iteration 7/25 | Loss: 0.00034440
Iteration 8/25 | Loss: 0.00034440
Iteration 9/25 | Loss: 0.00034440
Iteration 10/25 | Loss: 0.00034440
Iteration 11/25 | Loss: 0.00034440
Iteration 12/25 | Loss: 0.00034440
Iteration 13/25 | Loss: 0.00034440
Iteration 14/25 | Loss: 0.00034440
Iteration 15/25 | Loss: 0.00034440
Iteration 16/25 | Loss: 0.00034440
Iteration 17/25 | Loss: 0.00034440
Iteration 18/25 | Loss: 0.00034440
Iteration 19/25 | Loss: 0.00034440
Iteration 20/25 | Loss: 0.00034440
Iteration 21/25 | Loss: 0.00034440
Iteration 22/25 | Loss: 0.00034440
Iteration 23/25 | Loss: 0.00034440
Iteration 24/25 | Loss: 0.00034440
Iteration 25/25 | Loss: 0.00034440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00034440148738212883, 0.00034440148738212883, 0.00034440148738212883, 0.00034440148738212883, 0.00034440148738212883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034440148738212883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034440
Iteration 2/1000 | Loss: 0.00004226
Iteration 3/1000 | Loss: 0.00003435
Iteration 4/1000 | Loss: 0.00003244
Iteration 5/1000 | Loss: 0.00003102
Iteration 6/1000 | Loss: 0.00002994
Iteration 7/1000 | Loss: 0.00002897
Iteration 8/1000 | Loss: 0.00002865
Iteration 9/1000 | Loss: 0.00002837
Iteration 10/1000 | Loss: 0.00002812
Iteration 11/1000 | Loss: 0.00002803
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002791
Iteration 14/1000 | Loss: 0.00002791
Iteration 15/1000 | Loss: 0.00002782
Iteration 16/1000 | Loss: 0.00002781
Iteration 17/1000 | Loss: 0.00002780
Iteration 18/1000 | Loss: 0.00002777
Iteration 19/1000 | Loss: 0.00002776
Iteration 20/1000 | Loss: 0.00002772
Iteration 21/1000 | Loss: 0.00002772
Iteration 22/1000 | Loss: 0.00002771
Iteration 23/1000 | Loss: 0.00002771
Iteration 24/1000 | Loss: 0.00002771
Iteration 25/1000 | Loss: 0.00002771
Iteration 26/1000 | Loss: 0.00002770
Iteration 27/1000 | Loss: 0.00002770
Iteration 28/1000 | Loss: 0.00002770
Iteration 29/1000 | Loss: 0.00002770
Iteration 30/1000 | Loss: 0.00002770
Iteration 31/1000 | Loss: 0.00002770
Iteration 32/1000 | Loss: 0.00002769
Iteration 33/1000 | Loss: 0.00002769
Iteration 34/1000 | Loss: 0.00002769
Iteration 35/1000 | Loss: 0.00002769
Iteration 36/1000 | Loss: 0.00002769
Iteration 37/1000 | Loss: 0.00002769
Iteration 38/1000 | Loss: 0.00002769
Iteration 39/1000 | Loss: 0.00002769
Iteration 40/1000 | Loss: 0.00002769
Iteration 41/1000 | Loss: 0.00002768
Iteration 42/1000 | Loss: 0.00002768
Iteration 43/1000 | Loss: 0.00002768
Iteration 44/1000 | Loss: 0.00002768
Iteration 45/1000 | Loss: 0.00002768
Iteration 46/1000 | Loss: 0.00002768
Iteration 47/1000 | Loss: 0.00002768
Iteration 48/1000 | Loss: 0.00002768
Iteration 49/1000 | Loss: 0.00002768
Iteration 50/1000 | Loss: 0.00002768
Iteration 51/1000 | Loss: 0.00002767
Iteration 52/1000 | Loss: 0.00002767
Iteration 53/1000 | Loss: 0.00002767
Iteration 54/1000 | Loss: 0.00002767
Iteration 55/1000 | Loss: 0.00002767
Iteration 56/1000 | Loss: 0.00002767
Iteration 57/1000 | Loss: 0.00002767
Iteration 58/1000 | Loss: 0.00002767
Iteration 59/1000 | Loss: 0.00002766
Iteration 60/1000 | Loss: 0.00002766
Iteration 61/1000 | Loss: 0.00002766
Iteration 62/1000 | Loss: 0.00002766
Iteration 63/1000 | Loss: 0.00002766
Iteration 64/1000 | Loss: 0.00002766
Iteration 65/1000 | Loss: 0.00002766
Iteration 66/1000 | Loss: 0.00002766
Iteration 67/1000 | Loss: 0.00002766
Iteration 68/1000 | Loss: 0.00002766
Iteration 69/1000 | Loss: 0.00002766
Iteration 70/1000 | Loss: 0.00002766
Iteration 71/1000 | Loss: 0.00002766
Iteration 72/1000 | Loss: 0.00002766
Iteration 73/1000 | Loss: 0.00002766
Iteration 74/1000 | Loss: 0.00002766
Iteration 75/1000 | Loss: 0.00002766
Iteration 76/1000 | Loss: 0.00002766
Iteration 77/1000 | Loss: 0.00002766
Iteration 78/1000 | Loss: 0.00002766
Iteration 79/1000 | Loss: 0.00002766
Iteration 80/1000 | Loss: 0.00002766
Iteration 81/1000 | Loss: 0.00002766
Iteration 82/1000 | Loss: 0.00002766
Iteration 83/1000 | Loss: 0.00002766
Iteration 84/1000 | Loss: 0.00002766
Iteration 85/1000 | Loss: 0.00002766
Iteration 86/1000 | Loss: 0.00002766
Iteration 87/1000 | Loss: 0.00002766
Iteration 88/1000 | Loss: 0.00002766
Iteration 89/1000 | Loss: 0.00002766
Iteration 90/1000 | Loss: 0.00002766
Iteration 91/1000 | Loss: 0.00002766
Iteration 92/1000 | Loss: 0.00002766
Iteration 93/1000 | Loss: 0.00002766
Iteration 94/1000 | Loss: 0.00002766
Iteration 95/1000 | Loss: 0.00002766
Iteration 96/1000 | Loss: 0.00002766
Iteration 97/1000 | Loss: 0.00002766
Iteration 98/1000 | Loss: 0.00002766
Iteration 99/1000 | Loss: 0.00002766
Iteration 100/1000 | Loss: 0.00002766
Iteration 101/1000 | Loss: 0.00002766
Iteration 102/1000 | Loss: 0.00002766
Iteration 103/1000 | Loss: 0.00002766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.7662430511554703e-05, 2.7662430511554703e-05, 2.7662430511554703e-05, 2.7662430511554703e-05, 2.7662430511554703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7662430511554703e-05

Optimization complete. Final v2v error: 4.498425006866455 mm

Highest mean error: 4.820913314819336 mm for frame 119

Lowest mean error: 4.263318061828613 mm for frame 0

Saving results

Total time: 31.394824266433716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373393
Iteration 2/25 | Loss: 0.00094387
Iteration 3/25 | Loss: 0.00077487
Iteration 4/25 | Loss: 0.00072682
Iteration 5/25 | Loss: 0.00071512
Iteration 6/25 | Loss: 0.00071241
Iteration 7/25 | Loss: 0.00071160
Iteration 8/25 | Loss: 0.00071136
Iteration 9/25 | Loss: 0.00071136
Iteration 10/25 | Loss: 0.00071136
Iteration 11/25 | Loss: 0.00071136
Iteration 12/25 | Loss: 0.00071136
Iteration 13/25 | Loss: 0.00071136
Iteration 14/25 | Loss: 0.00071136
Iteration 15/25 | Loss: 0.00071136
Iteration 16/25 | Loss: 0.00071136
Iteration 17/25 | Loss: 0.00071136
Iteration 18/25 | Loss: 0.00071136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007113582687452435, 0.0007113582687452435, 0.0007113582687452435, 0.0007113582687452435, 0.0007113582687452435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007113582687452435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45408690
Iteration 2/25 | Loss: 0.00036928
Iteration 3/25 | Loss: 0.00036928
Iteration 4/25 | Loss: 0.00036928
Iteration 5/25 | Loss: 0.00036928
Iteration 6/25 | Loss: 0.00036928
Iteration 7/25 | Loss: 0.00036928
Iteration 8/25 | Loss: 0.00036928
Iteration 9/25 | Loss: 0.00036928
Iteration 10/25 | Loss: 0.00036928
Iteration 11/25 | Loss: 0.00036928
Iteration 12/25 | Loss: 0.00036928
Iteration 13/25 | Loss: 0.00036928
Iteration 14/25 | Loss: 0.00036928
Iteration 15/25 | Loss: 0.00036928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00036927792825736105, 0.00036927792825736105, 0.00036927792825736105, 0.00036927792825736105, 0.00036927792825736105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036927792825736105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036928
Iteration 2/1000 | Loss: 0.00005088
Iteration 3/1000 | Loss: 0.00003565
Iteration 4/1000 | Loss: 0.00002895
Iteration 5/1000 | Loss: 0.00002641
Iteration 6/1000 | Loss: 0.00002482
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002303
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002212
Iteration 11/1000 | Loss: 0.00002187
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002154
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002124
Iteration 17/1000 | Loss: 0.00002122
Iteration 18/1000 | Loss: 0.00002120
Iteration 19/1000 | Loss: 0.00002118
Iteration 20/1000 | Loss: 0.00002115
Iteration 21/1000 | Loss: 0.00002113
Iteration 22/1000 | Loss: 0.00002112
Iteration 23/1000 | Loss: 0.00002112
Iteration 24/1000 | Loss: 0.00002111
Iteration 25/1000 | Loss: 0.00002111
Iteration 26/1000 | Loss: 0.00002110
Iteration 27/1000 | Loss: 0.00002107
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002104
Iteration 30/1000 | Loss: 0.00002103
Iteration 31/1000 | Loss: 0.00002102
Iteration 32/1000 | Loss: 0.00002102
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002101
Iteration 35/1000 | Loss: 0.00002100
Iteration 36/1000 | Loss: 0.00002100
Iteration 37/1000 | Loss: 0.00002100
Iteration 38/1000 | Loss: 0.00002100
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002098
Iteration 42/1000 | Loss: 0.00002097
Iteration 43/1000 | Loss: 0.00002097
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002096
Iteration 46/1000 | Loss: 0.00002096
Iteration 47/1000 | Loss: 0.00002096
Iteration 48/1000 | Loss: 0.00002096
Iteration 49/1000 | Loss: 0.00002096
Iteration 50/1000 | Loss: 0.00002096
Iteration 51/1000 | Loss: 0.00002096
Iteration 52/1000 | Loss: 0.00002096
Iteration 53/1000 | Loss: 0.00002096
Iteration 54/1000 | Loss: 0.00002095
Iteration 55/1000 | Loss: 0.00002095
Iteration 56/1000 | Loss: 0.00002094
Iteration 57/1000 | Loss: 0.00002094
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002093
Iteration 60/1000 | Loss: 0.00002093
Iteration 61/1000 | Loss: 0.00002093
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002090
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00002090
Iteration 71/1000 | Loss: 0.00002089
Iteration 72/1000 | Loss: 0.00002089
Iteration 73/1000 | Loss: 0.00002089
Iteration 74/1000 | Loss: 0.00002088
Iteration 75/1000 | Loss: 0.00002088
Iteration 76/1000 | Loss: 0.00002088
Iteration 77/1000 | Loss: 0.00002087
Iteration 78/1000 | Loss: 0.00002087
Iteration 79/1000 | Loss: 0.00002087
Iteration 80/1000 | Loss: 0.00002087
Iteration 81/1000 | Loss: 0.00002086
Iteration 82/1000 | Loss: 0.00002086
Iteration 83/1000 | Loss: 0.00002086
Iteration 84/1000 | Loss: 0.00002086
Iteration 85/1000 | Loss: 0.00002086
Iteration 86/1000 | Loss: 0.00002086
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002085
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002083
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002082
Iteration 97/1000 | Loss: 0.00002082
Iteration 98/1000 | Loss: 0.00002082
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002080
Iteration 106/1000 | Loss: 0.00002080
Iteration 107/1000 | Loss: 0.00002080
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002079
Iteration 112/1000 | Loss: 0.00002079
Iteration 113/1000 | Loss: 0.00002079
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002079
Iteration 117/1000 | Loss: 0.00002079
Iteration 118/1000 | Loss: 0.00002079
Iteration 119/1000 | Loss: 0.00002079
Iteration 120/1000 | Loss: 0.00002079
Iteration 121/1000 | Loss: 0.00002079
Iteration 122/1000 | Loss: 0.00002079
Iteration 123/1000 | Loss: 0.00002078
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002078
Iteration 128/1000 | Loss: 0.00002078
Iteration 129/1000 | Loss: 0.00002078
Iteration 130/1000 | Loss: 0.00002078
Iteration 131/1000 | Loss: 0.00002078
Iteration 132/1000 | Loss: 0.00002078
Iteration 133/1000 | Loss: 0.00002078
Iteration 134/1000 | Loss: 0.00002078
Iteration 135/1000 | Loss: 0.00002078
Iteration 136/1000 | Loss: 0.00002078
Iteration 137/1000 | Loss: 0.00002078
Iteration 138/1000 | Loss: 0.00002078
Iteration 139/1000 | Loss: 0.00002078
Iteration 140/1000 | Loss: 0.00002078
Iteration 141/1000 | Loss: 0.00002078
Iteration 142/1000 | Loss: 0.00002078
Iteration 143/1000 | Loss: 0.00002078
Iteration 144/1000 | Loss: 0.00002078
Iteration 145/1000 | Loss: 0.00002078
Iteration 146/1000 | Loss: 0.00002078
Iteration 147/1000 | Loss: 0.00002078
Iteration 148/1000 | Loss: 0.00002078
Iteration 149/1000 | Loss: 0.00002078
Iteration 150/1000 | Loss: 0.00002078
Iteration 151/1000 | Loss: 0.00002078
Iteration 152/1000 | Loss: 0.00002078
Iteration 153/1000 | Loss: 0.00002078
Iteration 154/1000 | Loss: 0.00002078
Iteration 155/1000 | Loss: 0.00002078
Iteration 156/1000 | Loss: 0.00002078
Iteration 157/1000 | Loss: 0.00002078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.0778443285962567e-05, 2.0778443285962567e-05, 2.0778443285962567e-05, 2.0778443285962567e-05, 2.0778443285962567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0778443285962567e-05

Optimization complete. Final v2v error: 3.7908570766448975 mm

Highest mean error: 4.4065728187561035 mm for frame 117

Lowest mean error: 3.145475149154663 mm for frame 13

Saving results

Total time: 43.591387033462524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923212
Iteration 2/25 | Loss: 0.00098911
Iteration 3/25 | Loss: 0.00081839
Iteration 4/25 | Loss: 0.00077897
Iteration 5/25 | Loss: 0.00076116
Iteration 6/25 | Loss: 0.00075707
Iteration 7/25 | Loss: 0.00075584
Iteration 8/25 | Loss: 0.00075561
Iteration 9/25 | Loss: 0.00075561
Iteration 10/25 | Loss: 0.00075561
Iteration 11/25 | Loss: 0.00075561
Iteration 12/25 | Loss: 0.00075561
Iteration 13/25 | Loss: 0.00075561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007556143100373447, 0.0007556143100373447, 0.0007556143100373447, 0.0007556143100373447, 0.0007556143100373447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007556143100373447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48385370
Iteration 2/25 | Loss: 0.00035978
Iteration 3/25 | Loss: 0.00035976
Iteration 4/25 | Loss: 0.00035976
Iteration 5/25 | Loss: 0.00035976
Iteration 6/25 | Loss: 0.00035976
Iteration 7/25 | Loss: 0.00035976
Iteration 8/25 | Loss: 0.00035976
Iteration 9/25 | Loss: 0.00035976
Iteration 10/25 | Loss: 0.00035976
Iteration 11/25 | Loss: 0.00035976
Iteration 12/25 | Loss: 0.00035976
Iteration 13/25 | Loss: 0.00035976
Iteration 14/25 | Loss: 0.00035976
Iteration 15/25 | Loss: 0.00035976
Iteration 16/25 | Loss: 0.00035976
Iteration 17/25 | Loss: 0.00035976
Iteration 18/25 | Loss: 0.00035976
Iteration 19/25 | Loss: 0.00035976
Iteration 20/25 | Loss: 0.00035976
Iteration 21/25 | Loss: 0.00035976
Iteration 22/25 | Loss: 0.00035976
Iteration 23/25 | Loss: 0.00035976
Iteration 24/25 | Loss: 0.00035976
Iteration 25/25 | Loss: 0.00035976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035976
Iteration 2/1000 | Loss: 0.00004621
Iteration 3/1000 | Loss: 0.00003453
Iteration 4/1000 | Loss: 0.00003122
Iteration 5/1000 | Loss: 0.00002964
Iteration 6/1000 | Loss: 0.00002847
Iteration 7/1000 | Loss: 0.00002756
Iteration 8/1000 | Loss: 0.00002685
Iteration 9/1000 | Loss: 0.00002642
Iteration 10/1000 | Loss: 0.00002610
Iteration 11/1000 | Loss: 0.00002589
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00002568
Iteration 14/1000 | Loss: 0.00002568
Iteration 15/1000 | Loss: 0.00002565
Iteration 16/1000 | Loss: 0.00002564
Iteration 17/1000 | Loss: 0.00002562
Iteration 18/1000 | Loss: 0.00002555
Iteration 19/1000 | Loss: 0.00002552
Iteration 20/1000 | Loss: 0.00002546
Iteration 21/1000 | Loss: 0.00002546
Iteration 22/1000 | Loss: 0.00002544
Iteration 23/1000 | Loss: 0.00002544
Iteration 24/1000 | Loss: 0.00002543
Iteration 25/1000 | Loss: 0.00002543
Iteration 26/1000 | Loss: 0.00002542
Iteration 27/1000 | Loss: 0.00002540
Iteration 28/1000 | Loss: 0.00002540
Iteration 29/1000 | Loss: 0.00002540
Iteration 30/1000 | Loss: 0.00002539
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00002538
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002537
Iteration 35/1000 | Loss: 0.00002537
Iteration 36/1000 | Loss: 0.00002536
Iteration 37/1000 | Loss: 0.00002536
Iteration 38/1000 | Loss: 0.00002536
Iteration 39/1000 | Loss: 0.00002536
Iteration 40/1000 | Loss: 0.00002536
Iteration 41/1000 | Loss: 0.00002535
Iteration 42/1000 | Loss: 0.00002535
Iteration 43/1000 | Loss: 0.00002535
Iteration 44/1000 | Loss: 0.00002535
Iteration 45/1000 | Loss: 0.00002535
Iteration 46/1000 | Loss: 0.00002535
Iteration 47/1000 | Loss: 0.00002535
Iteration 48/1000 | Loss: 0.00002535
Iteration 49/1000 | Loss: 0.00002535
Iteration 50/1000 | Loss: 0.00002535
Iteration 51/1000 | Loss: 0.00002535
Iteration 52/1000 | Loss: 0.00002535
Iteration 53/1000 | Loss: 0.00002535
Iteration 54/1000 | Loss: 0.00002535
Iteration 55/1000 | Loss: 0.00002535
Iteration 56/1000 | Loss: 0.00002535
Iteration 57/1000 | Loss: 0.00002535
Iteration 58/1000 | Loss: 0.00002535
Iteration 59/1000 | Loss: 0.00002535
Iteration 60/1000 | Loss: 0.00002535
Iteration 61/1000 | Loss: 0.00002535
Iteration 62/1000 | Loss: 0.00002535
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002535
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002535
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002535
Iteration 70/1000 | Loss: 0.00002535
Iteration 71/1000 | Loss: 0.00002535
Iteration 72/1000 | Loss: 0.00002535
Iteration 73/1000 | Loss: 0.00002535
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002535
Iteration 76/1000 | Loss: 0.00002535
Iteration 77/1000 | Loss: 0.00002535
Iteration 78/1000 | Loss: 0.00002535
Iteration 79/1000 | Loss: 0.00002535
Iteration 80/1000 | Loss: 0.00002535
Iteration 81/1000 | Loss: 0.00002535
Iteration 82/1000 | Loss: 0.00002535
Iteration 83/1000 | Loss: 0.00002535
Iteration 84/1000 | Loss: 0.00002535
Iteration 85/1000 | Loss: 0.00002535
Iteration 86/1000 | Loss: 0.00002535
Iteration 87/1000 | Loss: 0.00002535
Iteration 88/1000 | Loss: 0.00002535
Iteration 89/1000 | Loss: 0.00002535
Iteration 90/1000 | Loss: 0.00002535
Iteration 91/1000 | Loss: 0.00002535
Iteration 92/1000 | Loss: 0.00002535
Iteration 93/1000 | Loss: 0.00002535
Iteration 94/1000 | Loss: 0.00002535
Iteration 95/1000 | Loss: 0.00002535
Iteration 96/1000 | Loss: 0.00002535
Iteration 97/1000 | Loss: 0.00002535
Iteration 98/1000 | Loss: 0.00002535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.5354062017868273e-05, 2.5354062017868273e-05, 2.5354062017868273e-05, 2.5354062017868273e-05, 2.5354062017868273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5354062017868273e-05

Optimization complete. Final v2v error: 4.214496612548828 mm

Highest mean error: 5.949306011199951 mm for frame 69

Lowest mean error: 3.719585418701172 mm for frame 44

Saving results

Total time: 34.32210636138916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00272077
Iteration 2/25 | Loss: 0.00102661
Iteration 3/25 | Loss: 0.00078589
Iteration 4/25 | Loss: 0.00072551
Iteration 5/25 | Loss: 0.00070509
Iteration 6/25 | Loss: 0.00069980
Iteration 7/25 | Loss: 0.00069846
Iteration 8/25 | Loss: 0.00069780
Iteration 9/25 | Loss: 0.00069780
Iteration 10/25 | Loss: 0.00069780
Iteration 11/25 | Loss: 0.00069780
Iteration 12/25 | Loss: 0.00069780
Iteration 13/25 | Loss: 0.00069780
Iteration 14/25 | Loss: 0.00069780
Iteration 15/25 | Loss: 0.00069780
Iteration 16/25 | Loss: 0.00069780
Iteration 17/25 | Loss: 0.00069780
Iteration 18/25 | Loss: 0.00069780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006977990851737559, 0.0006977990851737559, 0.0006977990851737559, 0.0006977990851737559, 0.0006977990851737559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006977990851737559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48974466
Iteration 2/25 | Loss: 0.00034679
Iteration 3/25 | Loss: 0.00034678
Iteration 4/25 | Loss: 0.00034678
Iteration 5/25 | Loss: 0.00034678
Iteration 6/25 | Loss: 0.00034678
Iteration 7/25 | Loss: 0.00034678
Iteration 8/25 | Loss: 0.00034678
Iteration 9/25 | Loss: 0.00034678
Iteration 10/25 | Loss: 0.00034678
Iteration 11/25 | Loss: 0.00034678
Iteration 12/25 | Loss: 0.00034678
Iteration 13/25 | Loss: 0.00034678
Iteration 14/25 | Loss: 0.00034678
Iteration 15/25 | Loss: 0.00034678
Iteration 16/25 | Loss: 0.00034678
Iteration 17/25 | Loss: 0.00034678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00034678212250582874, 0.00034678212250582874, 0.00034678212250582874, 0.00034678212250582874, 0.00034678212250582874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034678212250582874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034678
Iteration 2/1000 | Loss: 0.00003241
Iteration 3/1000 | Loss: 0.00002028
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001555
Iteration 7/1000 | Loss: 0.00001512
Iteration 8/1000 | Loss: 0.00001475
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001419
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001414
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001411
Iteration 19/1000 | Loss: 0.00001411
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001403
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001397
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001376
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001371
Iteration 114/1000 | Loss: 0.00001371
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001369
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001368
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001368
Iteration 148/1000 | Loss: 0.00001368
Iteration 149/1000 | Loss: 0.00001368
Iteration 150/1000 | Loss: 0.00001368
Iteration 151/1000 | Loss: 0.00001368
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001367
Iteration 157/1000 | Loss: 0.00001367
Iteration 158/1000 | Loss: 0.00001367
Iteration 159/1000 | Loss: 0.00001367
Iteration 160/1000 | Loss: 0.00001367
Iteration 161/1000 | Loss: 0.00001367
Iteration 162/1000 | Loss: 0.00001367
Iteration 163/1000 | Loss: 0.00001367
Iteration 164/1000 | Loss: 0.00001367
Iteration 165/1000 | Loss: 0.00001367
Iteration 166/1000 | Loss: 0.00001367
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001367
Iteration 169/1000 | Loss: 0.00001367
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001367
Iteration 172/1000 | Loss: 0.00001367
Iteration 173/1000 | Loss: 0.00001366
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001366
Iteration 176/1000 | Loss: 0.00001366
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Iteration 179/1000 | Loss: 0.00001366
Iteration 180/1000 | Loss: 0.00001366
Iteration 181/1000 | Loss: 0.00001366
Iteration 182/1000 | Loss: 0.00001366
Iteration 183/1000 | Loss: 0.00001366
Iteration 184/1000 | Loss: 0.00001366
Iteration 185/1000 | Loss: 0.00001366
Iteration 186/1000 | Loss: 0.00001366
Iteration 187/1000 | Loss: 0.00001366
Iteration 188/1000 | Loss: 0.00001366
Iteration 189/1000 | Loss: 0.00001366
Iteration 190/1000 | Loss: 0.00001366
Iteration 191/1000 | Loss: 0.00001366
Iteration 192/1000 | Loss: 0.00001366
Iteration 193/1000 | Loss: 0.00001366
Iteration 194/1000 | Loss: 0.00001366
Iteration 195/1000 | Loss: 0.00001366
Iteration 196/1000 | Loss: 0.00001366
Iteration 197/1000 | Loss: 0.00001366
Iteration 198/1000 | Loss: 0.00001366
Iteration 199/1000 | Loss: 0.00001366
Iteration 200/1000 | Loss: 0.00001366
Iteration 201/1000 | Loss: 0.00001366
Iteration 202/1000 | Loss: 0.00001366
Iteration 203/1000 | Loss: 0.00001366
Iteration 204/1000 | Loss: 0.00001366
Iteration 205/1000 | Loss: 0.00001366
Iteration 206/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.366006017633481e-05, 1.366006017633481e-05, 1.366006017633481e-05, 1.366006017633481e-05, 1.366006017633481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.366006017633481e-05

Optimization complete. Final v2v error: 3.1559040546417236 mm

Highest mean error: 3.2957770824432373 mm for frame 195

Lowest mean error: 2.915257215499878 mm for frame 0

Saving results

Total time: 48.77582550048828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811446
Iteration 2/25 | Loss: 0.00092765
Iteration 3/25 | Loss: 0.00071906
Iteration 4/25 | Loss: 0.00067618
Iteration 5/25 | Loss: 0.00066346
Iteration 6/25 | Loss: 0.00066011
Iteration 7/25 | Loss: 0.00065959
Iteration 8/25 | Loss: 0.00065959
Iteration 9/25 | Loss: 0.00065959
Iteration 10/25 | Loss: 0.00065959
Iteration 11/25 | Loss: 0.00065959
Iteration 12/25 | Loss: 0.00065959
Iteration 13/25 | Loss: 0.00065959
Iteration 14/25 | Loss: 0.00065959
Iteration 15/25 | Loss: 0.00065959
Iteration 16/25 | Loss: 0.00065959
Iteration 17/25 | Loss: 0.00065959
Iteration 18/25 | Loss: 0.00065959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006595940212719142, 0.0006595940212719142, 0.0006595940212719142, 0.0006595940212719142, 0.0006595940212719142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006595940212719142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47845888
Iteration 2/25 | Loss: 0.00029769
Iteration 3/25 | Loss: 0.00029769
Iteration 4/25 | Loss: 0.00029769
Iteration 5/25 | Loss: 0.00029769
Iteration 6/25 | Loss: 0.00029769
Iteration 7/25 | Loss: 0.00029768
Iteration 8/25 | Loss: 0.00029768
Iteration 9/25 | Loss: 0.00029768
Iteration 10/25 | Loss: 0.00029768
Iteration 11/25 | Loss: 0.00029768
Iteration 12/25 | Loss: 0.00029768
Iteration 13/25 | Loss: 0.00029768
Iteration 14/25 | Loss: 0.00029768
Iteration 15/25 | Loss: 0.00029768
Iteration 16/25 | Loss: 0.00029768
Iteration 17/25 | Loss: 0.00029768
Iteration 18/25 | Loss: 0.00029768
Iteration 19/25 | Loss: 0.00029768
Iteration 20/25 | Loss: 0.00029768
Iteration 21/25 | Loss: 0.00029768
Iteration 22/25 | Loss: 0.00029768
Iteration 23/25 | Loss: 0.00029768
Iteration 24/25 | Loss: 0.00029768
Iteration 25/25 | Loss: 0.00029768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00029768404783681035, 0.00029768404783681035, 0.00029768404783681035, 0.00029768404783681035, 0.00029768404783681035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029768404783681035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029768
Iteration 2/1000 | Loss: 0.00002007
Iteration 3/1000 | Loss: 0.00001288
Iteration 4/1000 | Loss: 0.00001213
Iteration 5/1000 | Loss: 0.00001145
Iteration 6/1000 | Loss: 0.00001112
Iteration 7/1000 | Loss: 0.00001084
Iteration 8/1000 | Loss: 0.00001079
Iteration 9/1000 | Loss: 0.00001075
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001068
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001061
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001057
Iteration 19/1000 | Loss: 0.00001056
Iteration 20/1000 | Loss: 0.00001055
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001053
Iteration 24/1000 | Loss: 0.00001052
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001041
Iteration 33/1000 | Loss: 0.00001039
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001035
Iteration 40/1000 | Loss: 0.00001035
Iteration 41/1000 | Loss: 0.00001035
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001033
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001033
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001032
Iteration 53/1000 | Loss: 0.00001030
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001029
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001026
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001022
Iteration 83/1000 | Loss: 0.00001022
Iteration 84/1000 | Loss: 0.00001022
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001021
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001020
Iteration 100/1000 | Loss: 0.00001020
Iteration 101/1000 | Loss: 0.00001020
Iteration 102/1000 | Loss: 0.00001020
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001020
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001019
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001019
Iteration 114/1000 | Loss: 0.00001019
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001018
Iteration 121/1000 | Loss: 0.00001018
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001017
Iteration 135/1000 | Loss: 0.00001017
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001016
Iteration 140/1000 | Loss: 0.00001016
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001015
Iteration 144/1000 | Loss: 0.00001015
Iteration 145/1000 | Loss: 0.00001015
Iteration 146/1000 | Loss: 0.00001014
Iteration 147/1000 | Loss: 0.00001014
Iteration 148/1000 | Loss: 0.00001014
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001013
Iteration 152/1000 | Loss: 0.00001013
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001013
Iteration 160/1000 | Loss: 0.00001013
Iteration 161/1000 | Loss: 0.00001013
Iteration 162/1000 | Loss: 0.00001013
Iteration 163/1000 | Loss: 0.00001013
Iteration 164/1000 | Loss: 0.00001013
Iteration 165/1000 | Loss: 0.00001013
Iteration 166/1000 | Loss: 0.00001013
Iteration 167/1000 | Loss: 0.00001013
Iteration 168/1000 | Loss: 0.00001013
Iteration 169/1000 | Loss: 0.00001012
Iteration 170/1000 | Loss: 0.00001012
Iteration 171/1000 | Loss: 0.00001012
Iteration 172/1000 | Loss: 0.00001012
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001012
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001012
Iteration 179/1000 | Loss: 0.00001012
Iteration 180/1000 | Loss: 0.00001012
Iteration 181/1000 | Loss: 0.00001012
Iteration 182/1000 | Loss: 0.00001012
Iteration 183/1000 | Loss: 0.00001012
Iteration 184/1000 | Loss: 0.00001012
Iteration 185/1000 | Loss: 0.00001012
Iteration 186/1000 | Loss: 0.00001012
Iteration 187/1000 | Loss: 0.00001012
Iteration 188/1000 | Loss: 0.00001012
Iteration 189/1000 | Loss: 0.00001012
Iteration 190/1000 | Loss: 0.00001011
Iteration 191/1000 | Loss: 0.00001011
Iteration 192/1000 | Loss: 0.00001011
Iteration 193/1000 | Loss: 0.00001011
Iteration 194/1000 | Loss: 0.00001011
Iteration 195/1000 | Loss: 0.00001011
Iteration 196/1000 | Loss: 0.00001011
Iteration 197/1000 | Loss: 0.00001011
Iteration 198/1000 | Loss: 0.00001011
Iteration 199/1000 | Loss: 0.00001011
Iteration 200/1000 | Loss: 0.00001011
Iteration 201/1000 | Loss: 0.00001011
Iteration 202/1000 | Loss: 0.00001011
Iteration 203/1000 | Loss: 0.00001011
Iteration 204/1000 | Loss: 0.00001011
Iteration 205/1000 | Loss: 0.00001011
Iteration 206/1000 | Loss: 0.00001011
Iteration 207/1000 | Loss: 0.00001011
Iteration 208/1000 | Loss: 0.00001011
Iteration 209/1000 | Loss: 0.00001010
Iteration 210/1000 | Loss: 0.00001010
Iteration 211/1000 | Loss: 0.00001010
Iteration 212/1000 | Loss: 0.00001010
Iteration 213/1000 | Loss: 0.00001010
Iteration 214/1000 | Loss: 0.00001010
Iteration 215/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0104167813551612e-05, 1.0104167813551612e-05, 1.0104167813551612e-05, 1.0104167813551612e-05, 1.0104167813551612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0104167813551612e-05

Optimization complete. Final v2v error: 2.6620495319366455 mm

Highest mean error: 2.889831781387329 mm for frame 115

Lowest mean error: 2.510438919067383 mm for frame 208

Saving results

Total time: 39.936119079589844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993106
Iteration 2/25 | Loss: 0.00374442
Iteration 3/25 | Loss: 0.00206954
Iteration 4/25 | Loss: 0.00186608
Iteration 5/25 | Loss: 0.00152040
Iteration 6/25 | Loss: 0.00168914
Iteration 7/25 | Loss: 0.00180130
Iteration 8/25 | Loss: 0.00145557
Iteration 9/25 | Loss: 0.00128730
Iteration 10/25 | Loss: 0.00113290
Iteration 11/25 | Loss: 0.00109790
Iteration 12/25 | Loss: 0.00108198
Iteration 13/25 | Loss: 0.00107486
Iteration 14/25 | Loss: 0.00105951
Iteration 15/25 | Loss: 0.00105058
Iteration 16/25 | Loss: 0.00105387
Iteration 17/25 | Loss: 0.00105740
Iteration 18/25 | Loss: 0.00105654
Iteration 19/25 | Loss: 0.00104396
Iteration 20/25 | Loss: 0.00104245
Iteration 21/25 | Loss: 0.00104194
Iteration 22/25 | Loss: 0.00104590
Iteration 23/25 | Loss: 0.00118078
Iteration 24/25 | Loss: 0.00096957
Iteration 25/25 | Loss: 0.00084835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49149883
Iteration 2/25 | Loss: 0.00101039
Iteration 3/25 | Loss: 0.00076751
Iteration 4/25 | Loss: 0.00076750
Iteration 5/25 | Loss: 0.00076750
Iteration 6/25 | Loss: 0.00076750
Iteration 7/25 | Loss: 0.00076750
Iteration 8/25 | Loss: 0.00076750
Iteration 9/25 | Loss: 0.00076750
Iteration 10/25 | Loss: 0.00076750
Iteration 11/25 | Loss: 0.00076750
Iteration 12/25 | Loss: 0.00076750
Iteration 13/25 | Loss: 0.00076750
Iteration 14/25 | Loss: 0.00076750
Iteration 15/25 | Loss: 0.00076750
Iteration 16/25 | Loss: 0.00076750
Iteration 17/25 | Loss: 0.00076750
Iteration 18/25 | Loss: 0.00076750
Iteration 19/25 | Loss: 0.00076750
Iteration 20/25 | Loss: 0.00076750
Iteration 21/25 | Loss: 0.00076750
Iteration 22/25 | Loss: 0.00076750
Iteration 23/25 | Loss: 0.00076750
Iteration 24/25 | Loss: 0.00076750
Iteration 25/25 | Loss: 0.00076750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076750
Iteration 2/1000 | Loss: 0.00167495
Iteration 3/1000 | Loss: 0.00026475
Iteration 4/1000 | Loss: 0.00017321
Iteration 5/1000 | Loss: 0.00018323
Iteration 6/1000 | Loss: 0.00020901
Iteration 7/1000 | Loss: 0.00020511
Iteration 8/1000 | Loss: 0.00015194
Iteration 9/1000 | Loss: 0.00132820
Iteration 10/1000 | Loss: 0.00045606
Iteration 11/1000 | Loss: 0.00012778
Iteration 12/1000 | Loss: 0.00027746
Iteration 13/1000 | Loss: 0.00011723
Iteration 14/1000 | Loss: 0.00019717
Iteration 15/1000 | Loss: 0.00017754
Iteration 16/1000 | Loss: 0.00149729
Iteration 17/1000 | Loss: 0.00015448
Iteration 18/1000 | Loss: 0.00005607
Iteration 19/1000 | Loss: 0.00025840
Iteration 20/1000 | Loss: 0.00005527
Iteration 21/1000 | Loss: 0.00009916
Iteration 22/1000 | Loss: 0.00003434
Iteration 23/1000 | Loss: 0.00002879
Iteration 24/1000 | Loss: 0.00003241
Iteration 25/1000 | Loss: 0.00002410
Iteration 26/1000 | Loss: 0.00002203
Iteration 27/1000 | Loss: 0.00009735
Iteration 28/1000 | Loss: 0.00002600
Iteration 29/1000 | Loss: 0.00001986
Iteration 30/1000 | Loss: 0.00001918
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002067
Iteration 34/1000 | Loss: 0.00002014
Iteration 35/1000 | Loss: 0.00015980
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001790
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00011265
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00006337
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001740
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001707
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001697
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001696
Iteration 86/1000 | Loss: 0.00001696
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001693
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001689
Iteration 105/1000 | Loss: 0.00001689
Iteration 106/1000 | Loss: 0.00001689
Iteration 107/1000 | Loss: 0.00001689
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001688
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001687
Iteration 145/1000 | Loss: 0.00001687
Iteration 146/1000 | Loss: 0.00001687
Iteration 147/1000 | Loss: 0.00001687
Iteration 148/1000 | Loss: 0.00001687
Iteration 149/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.686518226051703e-05, 1.686518226051703e-05, 1.686518226051703e-05, 1.686518226051703e-05, 1.686518226051703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.686518226051703e-05

Optimization complete. Final v2v error: 3.4182395935058594 mm

Highest mean error: 10.338112831115723 mm for frame 18

Lowest mean error: 3.0943191051483154 mm for frame 110

Saving results

Total time: 124.54802227020264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958725
Iteration 2/25 | Loss: 0.00123083
Iteration 3/25 | Loss: 0.00086812
Iteration 4/25 | Loss: 0.00082359
Iteration 5/25 | Loss: 0.00081516
Iteration 6/25 | Loss: 0.00081432
Iteration 7/25 | Loss: 0.00081432
Iteration 8/25 | Loss: 0.00081432
Iteration 9/25 | Loss: 0.00081432
Iteration 10/25 | Loss: 0.00081432
Iteration 11/25 | Loss: 0.00081432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008143161539919674, 0.0008143161539919674, 0.0008143161539919674, 0.0008143161539919674, 0.0008143161539919674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008143161539919674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05499721
Iteration 2/25 | Loss: 0.00039864
Iteration 3/25 | Loss: 0.00039860
Iteration 4/25 | Loss: 0.00039860
Iteration 5/25 | Loss: 0.00039860
Iteration 6/25 | Loss: 0.00039860
Iteration 7/25 | Loss: 0.00039860
Iteration 8/25 | Loss: 0.00039860
Iteration 9/25 | Loss: 0.00039860
Iteration 10/25 | Loss: 0.00039860
Iteration 11/25 | Loss: 0.00039860
Iteration 12/25 | Loss: 0.00039860
Iteration 13/25 | Loss: 0.00039860
Iteration 14/25 | Loss: 0.00039860
Iteration 15/25 | Loss: 0.00039860
Iteration 16/25 | Loss: 0.00039860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00039859552634879947, 0.00039859552634879947, 0.00039859552634879947, 0.00039859552634879947, 0.00039859552634879947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039859552634879947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039860
Iteration 2/1000 | Loss: 0.00003902
Iteration 3/1000 | Loss: 0.00002795
Iteration 4/1000 | Loss: 0.00002610
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002465
Iteration 7/1000 | Loss: 0.00002433
Iteration 8/1000 | Loss: 0.00002398
Iteration 9/1000 | Loss: 0.00002361
Iteration 10/1000 | Loss: 0.00002335
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002315
Iteration 13/1000 | Loss: 0.00002313
Iteration 14/1000 | Loss: 0.00002300
Iteration 15/1000 | Loss: 0.00002298
Iteration 16/1000 | Loss: 0.00002294
Iteration 17/1000 | Loss: 0.00002294
Iteration 18/1000 | Loss: 0.00002294
Iteration 19/1000 | Loss: 0.00002293
Iteration 20/1000 | Loss: 0.00002293
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002291
Iteration 23/1000 | Loss: 0.00002291
Iteration 24/1000 | Loss: 0.00002290
Iteration 25/1000 | Loss: 0.00002289
Iteration 26/1000 | Loss: 0.00002289
Iteration 27/1000 | Loss: 0.00002287
Iteration 28/1000 | Loss: 0.00002287
Iteration 29/1000 | Loss: 0.00002285
Iteration 30/1000 | Loss: 0.00002285
Iteration 31/1000 | Loss: 0.00002283
Iteration 32/1000 | Loss: 0.00002282
Iteration 33/1000 | Loss: 0.00002282
Iteration 34/1000 | Loss: 0.00002282
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002281
Iteration 37/1000 | Loss: 0.00002281
Iteration 38/1000 | Loss: 0.00002281
Iteration 39/1000 | Loss: 0.00002280
Iteration 40/1000 | Loss: 0.00002280
Iteration 41/1000 | Loss: 0.00002280
Iteration 42/1000 | Loss: 0.00002280
Iteration 43/1000 | Loss: 0.00002279
Iteration 44/1000 | Loss: 0.00002279
Iteration 45/1000 | Loss: 0.00002279
Iteration 46/1000 | Loss: 0.00002279
Iteration 47/1000 | Loss: 0.00002279
Iteration 48/1000 | Loss: 0.00002279
Iteration 49/1000 | Loss: 0.00002278
Iteration 50/1000 | Loss: 0.00002278
Iteration 51/1000 | Loss: 0.00002278
Iteration 52/1000 | Loss: 0.00002278
Iteration 53/1000 | Loss: 0.00002278
Iteration 54/1000 | Loss: 0.00002278
Iteration 55/1000 | Loss: 0.00002277
Iteration 56/1000 | Loss: 0.00002277
Iteration 57/1000 | Loss: 0.00002277
Iteration 58/1000 | Loss: 0.00002277
Iteration 59/1000 | Loss: 0.00002276
Iteration 60/1000 | Loss: 0.00002276
Iteration 61/1000 | Loss: 0.00002275
Iteration 62/1000 | Loss: 0.00002275
Iteration 63/1000 | Loss: 0.00002275
Iteration 64/1000 | Loss: 0.00002275
Iteration 65/1000 | Loss: 0.00002275
Iteration 66/1000 | Loss: 0.00002275
Iteration 67/1000 | Loss: 0.00002274
Iteration 68/1000 | Loss: 0.00002274
Iteration 69/1000 | Loss: 0.00002274
Iteration 70/1000 | Loss: 0.00002274
Iteration 71/1000 | Loss: 0.00002274
Iteration 72/1000 | Loss: 0.00002274
Iteration 73/1000 | Loss: 0.00002274
Iteration 74/1000 | Loss: 0.00002274
Iteration 75/1000 | Loss: 0.00002274
Iteration 76/1000 | Loss: 0.00002274
Iteration 77/1000 | Loss: 0.00002274
Iteration 78/1000 | Loss: 0.00002274
Iteration 79/1000 | Loss: 0.00002274
Iteration 80/1000 | Loss: 0.00002274
Iteration 81/1000 | Loss: 0.00002273
Iteration 82/1000 | Loss: 0.00002273
Iteration 83/1000 | Loss: 0.00002273
Iteration 84/1000 | Loss: 0.00002273
Iteration 85/1000 | Loss: 0.00002273
Iteration 86/1000 | Loss: 0.00002273
Iteration 87/1000 | Loss: 0.00002273
Iteration 88/1000 | Loss: 0.00002273
Iteration 89/1000 | Loss: 0.00002273
Iteration 90/1000 | Loss: 0.00002272
Iteration 91/1000 | Loss: 0.00002272
Iteration 92/1000 | Loss: 0.00002272
Iteration 93/1000 | Loss: 0.00002272
Iteration 94/1000 | Loss: 0.00002272
Iteration 95/1000 | Loss: 0.00002272
Iteration 96/1000 | Loss: 0.00002271
Iteration 97/1000 | Loss: 0.00002271
Iteration 98/1000 | Loss: 0.00002271
Iteration 99/1000 | Loss: 0.00002271
Iteration 100/1000 | Loss: 0.00002271
Iteration 101/1000 | Loss: 0.00002271
Iteration 102/1000 | Loss: 0.00002270
Iteration 103/1000 | Loss: 0.00002270
Iteration 104/1000 | Loss: 0.00002270
Iteration 105/1000 | Loss: 0.00002270
Iteration 106/1000 | Loss: 0.00002269
Iteration 107/1000 | Loss: 0.00002269
Iteration 108/1000 | Loss: 0.00002269
Iteration 109/1000 | Loss: 0.00002269
Iteration 110/1000 | Loss: 0.00002269
Iteration 111/1000 | Loss: 0.00002269
Iteration 112/1000 | Loss: 0.00002269
Iteration 113/1000 | Loss: 0.00002269
Iteration 114/1000 | Loss: 0.00002268
Iteration 115/1000 | Loss: 0.00002268
Iteration 116/1000 | Loss: 0.00002268
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002267
Iteration 120/1000 | Loss: 0.00002267
Iteration 121/1000 | Loss: 0.00002266
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002266
Iteration 126/1000 | Loss: 0.00002266
Iteration 127/1000 | Loss: 0.00002266
Iteration 128/1000 | Loss: 0.00002265
Iteration 129/1000 | Loss: 0.00002265
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002265
Iteration 132/1000 | Loss: 0.00002264
Iteration 133/1000 | Loss: 0.00002264
Iteration 134/1000 | Loss: 0.00002264
Iteration 135/1000 | Loss: 0.00002264
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002263
Iteration 138/1000 | Loss: 0.00002263
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00002263
Iteration 141/1000 | Loss: 0.00002262
Iteration 142/1000 | Loss: 0.00002262
Iteration 143/1000 | Loss: 0.00002262
Iteration 144/1000 | Loss: 0.00002262
Iteration 145/1000 | Loss: 0.00002262
Iteration 146/1000 | Loss: 0.00002262
Iteration 147/1000 | Loss: 0.00002262
Iteration 148/1000 | Loss: 0.00002262
Iteration 149/1000 | Loss: 0.00002262
Iteration 150/1000 | Loss: 0.00002261
Iteration 151/1000 | Loss: 0.00002261
Iteration 152/1000 | Loss: 0.00002261
Iteration 153/1000 | Loss: 0.00002260
Iteration 154/1000 | Loss: 0.00002260
Iteration 155/1000 | Loss: 0.00002260
Iteration 156/1000 | Loss: 0.00002260
Iteration 157/1000 | Loss: 0.00002260
Iteration 158/1000 | Loss: 0.00002260
Iteration 159/1000 | Loss: 0.00002259
Iteration 160/1000 | Loss: 0.00002259
Iteration 161/1000 | Loss: 0.00002259
Iteration 162/1000 | Loss: 0.00002259
Iteration 163/1000 | Loss: 0.00002259
Iteration 164/1000 | Loss: 0.00002259
Iteration 165/1000 | Loss: 0.00002258
Iteration 166/1000 | Loss: 0.00002258
Iteration 167/1000 | Loss: 0.00002258
Iteration 168/1000 | Loss: 0.00002258
Iteration 169/1000 | Loss: 0.00002257
Iteration 170/1000 | Loss: 0.00002257
Iteration 171/1000 | Loss: 0.00002257
Iteration 172/1000 | Loss: 0.00002257
Iteration 173/1000 | Loss: 0.00002257
Iteration 174/1000 | Loss: 0.00002257
Iteration 175/1000 | Loss: 0.00002257
Iteration 176/1000 | Loss: 0.00002257
Iteration 177/1000 | Loss: 0.00002257
Iteration 178/1000 | Loss: 0.00002257
Iteration 179/1000 | Loss: 0.00002256
Iteration 180/1000 | Loss: 0.00002256
Iteration 181/1000 | Loss: 0.00002256
Iteration 182/1000 | Loss: 0.00002256
Iteration 183/1000 | Loss: 0.00002256
Iteration 184/1000 | Loss: 0.00002256
Iteration 185/1000 | Loss: 0.00002256
Iteration 186/1000 | Loss: 0.00002255
Iteration 187/1000 | Loss: 0.00002255
Iteration 188/1000 | Loss: 0.00002255
Iteration 189/1000 | Loss: 0.00002255
Iteration 190/1000 | Loss: 0.00002255
Iteration 191/1000 | Loss: 0.00002255
Iteration 192/1000 | Loss: 0.00002255
Iteration 193/1000 | Loss: 0.00002255
Iteration 194/1000 | Loss: 0.00002255
Iteration 195/1000 | Loss: 0.00002255
Iteration 196/1000 | Loss: 0.00002255
Iteration 197/1000 | Loss: 0.00002255
Iteration 198/1000 | Loss: 0.00002255
Iteration 199/1000 | Loss: 0.00002255
Iteration 200/1000 | Loss: 0.00002255
Iteration 201/1000 | Loss: 0.00002255
Iteration 202/1000 | Loss: 0.00002255
Iteration 203/1000 | Loss: 0.00002255
Iteration 204/1000 | Loss: 0.00002255
Iteration 205/1000 | Loss: 0.00002255
Iteration 206/1000 | Loss: 0.00002255
Iteration 207/1000 | Loss: 0.00002255
Iteration 208/1000 | Loss: 0.00002255
Iteration 209/1000 | Loss: 0.00002255
Iteration 210/1000 | Loss: 0.00002255
Iteration 211/1000 | Loss: 0.00002255
Iteration 212/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.2551024812855758e-05, 2.2551024812855758e-05, 2.2551024812855758e-05, 2.2551024812855758e-05, 2.2551024812855758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2551024812855758e-05

Optimization complete. Final v2v error: 3.887361764907837 mm

Highest mean error: 4.557356357574463 mm for frame 81

Lowest mean error: 3.393660545349121 mm for frame 142

Saving results

Total time: 41.632123708724976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00674948
Iteration 2/25 | Loss: 0.00094776
Iteration 3/25 | Loss: 0.00081136
Iteration 4/25 | Loss: 0.00076851
Iteration 5/25 | Loss: 0.00075745
Iteration 6/25 | Loss: 0.00075567
Iteration 7/25 | Loss: 0.00075506
Iteration 8/25 | Loss: 0.00075506
Iteration 9/25 | Loss: 0.00075506
Iteration 10/25 | Loss: 0.00075506
Iteration 11/25 | Loss: 0.00075506
Iteration 12/25 | Loss: 0.00075506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007550644804723561, 0.0007550644804723561, 0.0007550644804723561, 0.0007550644804723561, 0.0007550644804723561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007550644804723561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47546220
Iteration 2/25 | Loss: 0.00040140
Iteration 3/25 | Loss: 0.00040139
Iteration 4/25 | Loss: 0.00040139
Iteration 5/25 | Loss: 0.00040139
Iteration 6/25 | Loss: 0.00040139
Iteration 7/25 | Loss: 0.00040139
Iteration 8/25 | Loss: 0.00040139
Iteration 9/25 | Loss: 0.00040139
Iteration 10/25 | Loss: 0.00040139
Iteration 11/25 | Loss: 0.00040139
Iteration 12/25 | Loss: 0.00040139
Iteration 13/25 | Loss: 0.00040139
Iteration 14/25 | Loss: 0.00040139
Iteration 15/25 | Loss: 0.00040139
Iteration 16/25 | Loss: 0.00040138
Iteration 17/25 | Loss: 0.00040139
Iteration 18/25 | Loss: 0.00040138
Iteration 19/25 | Loss: 0.00040138
Iteration 20/25 | Loss: 0.00040138
Iteration 21/25 | Loss: 0.00040138
Iteration 22/25 | Loss: 0.00040138
Iteration 23/25 | Loss: 0.00040138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004013849829789251, 0.0004013849829789251, 0.0004013849829789251, 0.0004013849829789251, 0.0004013849829789251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004013849829789251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040138
Iteration 2/1000 | Loss: 0.00004910
Iteration 3/1000 | Loss: 0.00003619
Iteration 4/1000 | Loss: 0.00003358
Iteration 5/1000 | Loss: 0.00003187
Iteration 6/1000 | Loss: 0.00003083
Iteration 7/1000 | Loss: 0.00003016
Iteration 8/1000 | Loss: 0.00002950
Iteration 9/1000 | Loss: 0.00002913
Iteration 10/1000 | Loss: 0.00002879
Iteration 11/1000 | Loss: 0.00002855
Iteration 12/1000 | Loss: 0.00002852
Iteration 13/1000 | Loss: 0.00002837
Iteration 14/1000 | Loss: 0.00002835
Iteration 15/1000 | Loss: 0.00002829
Iteration 16/1000 | Loss: 0.00002826
Iteration 17/1000 | Loss: 0.00002817
Iteration 18/1000 | Loss: 0.00002813
Iteration 19/1000 | Loss: 0.00002810
Iteration 20/1000 | Loss: 0.00002810
Iteration 21/1000 | Loss: 0.00002809
Iteration 22/1000 | Loss: 0.00002809
Iteration 23/1000 | Loss: 0.00002809
Iteration 24/1000 | Loss: 0.00002809
Iteration 25/1000 | Loss: 0.00002808
Iteration 26/1000 | Loss: 0.00002808
Iteration 27/1000 | Loss: 0.00002808
Iteration 28/1000 | Loss: 0.00002808
Iteration 29/1000 | Loss: 0.00002808
Iteration 30/1000 | Loss: 0.00002807
Iteration 31/1000 | Loss: 0.00002807
Iteration 32/1000 | Loss: 0.00002806
Iteration 33/1000 | Loss: 0.00002806
Iteration 34/1000 | Loss: 0.00002805
Iteration 35/1000 | Loss: 0.00002805
Iteration 36/1000 | Loss: 0.00002805
Iteration 37/1000 | Loss: 0.00002804
Iteration 38/1000 | Loss: 0.00002804
Iteration 39/1000 | Loss: 0.00002803
Iteration 40/1000 | Loss: 0.00002803
Iteration 41/1000 | Loss: 0.00002803
Iteration 42/1000 | Loss: 0.00002803
Iteration 43/1000 | Loss: 0.00002803
Iteration 44/1000 | Loss: 0.00002803
Iteration 45/1000 | Loss: 0.00002803
Iteration 46/1000 | Loss: 0.00002803
Iteration 47/1000 | Loss: 0.00002803
Iteration 48/1000 | Loss: 0.00002802
Iteration 49/1000 | Loss: 0.00002802
Iteration 50/1000 | Loss: 0.00002802
Iteration 51/1000 | Loss: 0.00002802
Iteration 52/1000 | Loss: 0.00002802
Iteration 53/1000 | Loss: 0.00002802
Iteration 54/1000 | Loss: 0.00002802
Iteration 55/1000 | Loss: 0.00002802
Iteration 56/1000 | Loss: 0.00002802
Iteration 57/1000 | Loss: 0.00002802
Iteration 58/1000 | Loss: 0.00002802
Iteration 59/1000 | Loss: 0.00002802
Iteration 60/1000 | Loss: 0.00002802
Iteration 61/1000 | Loss: 0.00002802
Iteration 62/1000 | Loss: 0.00002802
Iteration 63/1000 | Loss: 0.00002802
Iteration 64/1000 | Loss: 0.00002802
Iteration 65/1000 | Loss: 0.00002802
Iteration 66/1000 | Loss: 0.00002801
Iteration 67/1000 | Loss: 0.00002801
Iteration 68/1000 | Loss: 0.00002801
Iteration 69/1000 | Loss: 0.00002801
Iteration 70/1000 | Loss: 0.00002801
Iteration 71/1000 | Loss: 0.00002801
Iteration 72/1000 | Loss: 0.00002800
Iteration 73/1000 | Loss: 0.00002800
Iteration 74/1000 | Loss: 0.00002800
Iteration 75/1000 | Loss: 0.00002800
Iteration 76/1000 | Loss: 0.00002800
Iteration 77/1000 | Loss: 0.00002800
Iteration 78/1000 | Loss: 0.00002799
Iteration 79/1000 | Loss: 0.00002799
Iteration 80/1000 | Loss: 0.00002799
Iteration 81/1000 | Loss: 0.00002799
Iteration 82/1000 | Loss: 0.00002799
Iteration 83/1000 | Loss: 0.00002798
Iteration 84/1000 | Loss: 0.00002798
Iteration 85/1000 | Loss: 0.00002798
Iteration 86/1000 | Loss: 0.00002797
Iteration 87/1000 | Loss: 0.00002797
Iteration 88/1000 | Loss: 0.00002797
Iteration 89/1000 | Loss: 0.00002796
Iteration 90/1000 | Loss: 0.00002796
Iteration 91/1000 | Loss: 0.00002796
Iteration 92/1000 | Loss: 0.00002795
Iteration 93/1000 | Loss: 0.00002795
Iteration 94/1000 | Loss: 0.00002794
Iteration 95/1000 | Loss: 0.00002794
Iteration 96/1000 | Loss: 0.00002794
Iteration 97/1000 | Loss: 0.00002794
Iteration 98/1000 | Loss: 0.00002794
Iteration 99/1000 | Loss: 0.00002794
Iteration 100/1000 | Loss: 0.00002794
Iteration 101/1000 | Loss: 0.00002794
Iteration 102/1000 | Loss: 0.00002794
Iteration 103/1000 | Loss: 0.00002794
Iteration 104/1000 | Loss: 0.00002794
Iteration 105/1000 | Loss: 0.00002794
Iteration 106/1000 | Loss: 0.00002794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.7937332561123185e-05, 2.7937332561123185e-05, 2.7937332561123185e-05, 2.7937332561123185e-05, 2.7937332561123185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7937332561123185e-05

Optimization complete. Final v2v error: 4.327167987823486 mm

Highest mean error: 4.694633960723877 mm for frame 27

Lowest mean error: 3.822068452835083 mm for frame 205

Saving results

Total time: 40.78088355064392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877596
Iteration 2/25 | Loss: 0.00092242
Iteration 3/25 | Loss: 0.00081265
Iteration 4/25 | Loss: 0.00077230
Iteration 5/25 | Loss: 0.00076211
Iteration 6/25 | Loss: 0.00075787
Iteration 7/25 | Loss: 0.00075685
Iteration 8/25 | Loss: 0.00075685
Iteration 9/25 | Loss: 0.00075685
Iteration 10/25 | Loss: 0.00075685
Iteration 11/25 | Loss: 0.00075685
Iteration 12/25 | Loss: 0.00075685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007568500004708767, 0.0007568500004708767, 0.0007568500004708767, 0.0007568500004708767, 0.0007568500004708767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007568500004708767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45121169
Iteration 2/25 | Loss: 0.00031966
Iteration 3/25 | Loss: 0.00031966
Iteration 4/25 | Loss: 0.00031966
Iteration 5/25 | Loss: 0.00031966
Iteration 6/25 | Loss: 0.00031966
Iteration 7/25 | Loss: 0.00031966
Iteration 8/25 | Loss: 0.00031966
Iteration 9/25 | Loss: 0.00031966
Iteration 10/25 | Loss: 0.00031966
Iteration 11/25 | Loss: 0.00031966
Iteration 12/25 | Loss: 0.00031966
Iteration 13/25 | Loss: 0.00031966
Iteration 14/25 | Loss: 0.00031966
Iteration 15/25 | Loss: 0.00031966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000319657294312492, 0.000319657294312492, 0.000319657294312492, 0.000319657294312492, 0.000319657294312492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000319657294312492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031966
Iteration 2/1000 | Loss: 0.00003153
Iteration 3/1000 | Loss: 0.00002630
Iteration 4/1000 | Loss: 0.00002519
Iteration 5/1000 | Loss: 0.00002415
Iteration 6/1000 | Loss: 0.00002374
Iteration 7/1000 | Loss: 0.00002340
Iteration 8/1000 | Loss: 0.00002310
Iteration 9/1000 | Loss: 0.00002309
Iteration 10/1000 | Loss: 0.00002284
Iteration 11/1000 | Loss: 0.00002269
Iteration 12/1000 | Loss: 0.00002266
Iteration 13/1000 | Loss: 0.00002264
Iteration 14/1000 | Loss: 0.00002263
Iteration 15/1000 | Loss: 0.00002258
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002250
Iteration 18/1000 | Loss: 0.00002250
Iteration 19/1000 | Loss: 0.00002250
Iteration 20/1000 | Loss: 0.00002249
Iteration 21/1000 | Loss: 0.00002249
Iteration 22/1000 | Loss: 0.00002247
Iteration 23/1000 | Loss: 0.00002247
Iteration 24/1000 | Loss: 0.00002247
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002246
Iteration 27/1000 | Loss: 0.00002246
Iteration 28/1000 | Loss: 0.00002245
Iteration 29/1000 | Loss: 0.00002245
Iteration 30/1000 | Loss: 0.00002244
Iteration 31/1000 | Loss: 0.00002244
Iteration 32/1000 | Loss: 0.00002244
Iteration 33/1000 | Loss: 0.00002243
Iteration 34/1000 | Loss: 0.00002243
Iteration 35/1000 | Loss: 0.00002243
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002241
Iteration 39/1000 | Loss: 0.00002241
Iteration 40/1000 | Loss: 0.00002241
Iteration 41/1000 | Loss: 0.00002241
Iteration 42/1000 | Loss: 0.00002241
Iteration 43/1000 | Loss: 0.00002240
Iteration 44/1000 | Loss: 0.00002240
Iteration 45/1000 | Loss: 0.00002239
Iteration 46/1000 | Loss: 0.00002239
Iteration 47/1000 | Loss: 0.00002239
Iteration 48/1000 | Loss: 0.00002239
Iteration 49/1000 | Loss: 0.00002239
Iteration 50/1000 | Loss: 0.00002239
Iteration 51/1000 | Loss: 0.00002239
Iteration 52/1000 | Loss: 0.00002238
Iteration 53/1000 | Loss: 0.00002238
Iteration 54/1000 | Loss: 0.00002238
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00002237
Iteration 57/1000 | Loss: 0.00002237
Iteration 58/1000 | Loss: 0.00002235
Iteration 59/1000 | Loss: 0.00002235
Iteration 60/1000 | Loss: 0.00002235
Iteration 61/1000 | Loss: 0.00002234
Iteration 62/1000 | Loss: 0.00002234
Iteration 63/1000 | Loss: 0.00002233
Iteration 64/1000 | Loss: 0.00002233
Iteration 65/1000 | Loss: 0.00002232
Iteration 66/1000 | Loss: 0.00002232
Iteration 67/1000 | Loss: 0.00002232
Iteration 68/1000 | Loss: 0.00002232
Iteration 69/1000 | Loss: 0.00002231
Iteration 70/1000 | Loss: 0.00002231
Iteration 71/1000 | Loss: 0.00002231
Iteration 72/1000 | Loss: 0.00002231
Iteration 73/1000 | Loss: 0.00002230
Iteration 74/1000 | Loss: 0.00002230
Iteration 75/1000 | Loss: 0.00002230
Iteration 76/1000 | Loss: 0.00002229
Iteration 77/1000 | Loss: 0.00002229
Iteration 78/1000 | Loss: 0.00002229
Iteration 79/1000 | Loss: 0.00002229
Iteration 80/1000 | Loss: 0.00002228
Iteration 81/1000 | Loss: 0.00002228
Iteration 82/1000 | Loss: 0.00002228
Iteration 83/1000 | Loss: 0.00002228
Iteration 84/1000 | Loss: 0.00002228
Iteration 85/1000 | Loss: 0.00002228
Iteration 86/1000 | Loss: 0.00002227
Iteration 87/1000 | Loss: 0.00002227
Iteration 88/1000 | Loss: 0.00002227
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002226
Iteration 92/1000 | Loss: 0.00002226
Iteration 93/1000 | Loss: 0.00002226
Iteration 94/1000 | Loss: 0.00002225
Iteration 95/1000 | Loss: 0.00002225
Iteration 96/1000 | Loss: 0.00002225
Iteration 97/1000 | Loss: 0.00002225
Iteration 98/1000 | Loss: 0.00002225
Iteration 99/1000 | Loss: 0.00002225
Iteration 100/1000 | Loss: 0.00002225
Iteration 101/1000 | Loss: 0.00002225
Iteration 102/1000 | Loss: 0.00002225
Iteration 103/1000 | Loss: 0.00002225
Iteration 104/1000 | Loss: 0.00002225
Iteration 105/1000 | Loss: 0.00002224
Iteration 106/1000 | Loss: 0.00002224
Iteration 107/1000 | Loss: 0.00002224
Iteration 108/1000 | Loss: 0.00002224
Iteration 109/1000 | Loss: 0.00002224
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002224
Iteration 112/1000 | Loss: 0.00002224
Iteration 113/1000 | Loss: 0.00002224
Iteration 114/1000 | Loss: 0.00002224
Iteration 115/1000 | Loss: 0.00002224
Iteration 116/1000 | Loss: 0.00002224
Iteration 117/1000 | Loss: 0.00002224
Iteration 118/1000 | Loss: 0.00002224
Iteration 119/1000 | Loss: 0.00002223
Iteration 120/1000 | Loss: 0.00002223
Iteration 121/1000 | Loss: 0.00002223
Iteration 122/1000 | Loss: 0.00002223
Iteration 123/1000 | Loss: 0.00002223
Iteration 124/1000 | Loss: 0.00002223
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002223
Iteration 128/1000 | Loss: 0.00002223
Iteration 129/1000 | Loss: 0.00002223
Iteration 130/1000 | Loss: 0.00002222
Iteration 131/1000 | Loss: 0.00002222
Iteration 132/1000 | Loss: 0.00002222
Iteration 133/1000 | Loss: 0.00002222
Iteration 134/1000 | Loss: 0.00002222
Iteration 135/1000 | Loss: 0.00002222
Iteration 136/1000 | Loss: 0.00002222
Iteration 137/1000 | Loss: 0.00002222
Iteration 138/1000 | Loss: 0.00002222
Iteration 139/1000 | Loss: 0.00002222
Iteration 140/1000 | Loss: 0.00002222
Iteration 141/1000 | Loss: 0.00002222
Iteration 142/1000 | Loss: 0.00002222
Iteration 143/1000 | Loss: 0.00002222
Iteration 144/1000 | Loss: 0.00002222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.221788054157514e-05, 2.221788054157514e-05, 2.221788054157514e-05, 2.221788054157514e-05, 2.221788054157514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.221788054157514e-05

Optimization complete. Final v2v error: 3.9164600372314453 mm

Highest mean error: 3.9707770347595215 mm for frame 40

Lowest mean error: 3.8742988109588623 mm for frame 215

Saving results

Total time: 39.75405931472778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521512
Iteration 2/25 | Loss: 0.00097414
Iteration 3/25 | Loss: 0.00081181
Iteration 4/25 | Loss: 0.00078344
Iteration 5/25 | Loss: 0.00077737
Iteration 6/25 | Loss: 0.00077561
Iteration 7/25 | Loss: 0.00077561
Iteration 8/25 | Loss: 0.00077561
Iteration 9/25 | Loss: 0.00077561
Iteration 10/25 | Loss: 0.00077561
Iteration 11/25 | Loss: 0.00077561
Iteration 12/25 | Loss: 0.00077561
Iteration 13/25 | Loss: 0.00077561
Iteration 14/25 | Loss: 0.00077561
Iteration 15/25 | Loss: 0.00077561
Iteration 16/25 | Loss: 0.00077561
Iteration 17/25 | Loss: 0.00077561
Iteration 18/25 | Loss: 0.00077561
Iteration 19/25 | Loss: 0.00077561
Iteration 20/25 | Loss: 0.00077561
Iteration 21/25 | Loss: 0.00077561
Iteration 22/25 | Loss: 0.00077561
Iteration 23/25 | Loss: 0.00077561
Iteration 24/25 | Loss: 0.00077561
Iteration 25/25 | Loss: 0.00077561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06085324
Iteration 2/25 | Loss: 0.00041321
Iteration 3/25 | Loss: 0.00041319
Iteration 4/25 | Loss: 0.00041319
Iteration 5/25 | Loss: 0.00041319
Iteration 6/25 | Loss: 0.00041319
Iteration 7/25 | Loss: 0.00041319
Iteration 8/25 | Loss: 0.00041319
Iteration 9/25 | Loss: 0.00041319
Iteration 10/25 | Loss: 0.00041319
Iteration 11/25 | Loss: 0.00041319
Iteration 12/25 | Loss: 0.00041319
Iteration 13/25 | Loss: 0.00041319
Iteration 14/25 | Loss: 0.00041319
Iteration 15/25 | Loss: 0.00041319
Iteration 16/25 | Loss: 0.00041319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00041318603325635195, 0.00041318603325635195, 0.00041318603325635195, 0.00041318603325635195, 0.00041318603325635195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041318603325635195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041319
Iteration 2/1000 | Loss: 0.00004616
Iteration 3/1000 | Loss: 0.00003538
Iteration 4/1000 | Loss: 0.00003084
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002724
Iteration 7/1000 | Loss: 0.00002637
Iteration 8/1000 | Loss: 0.00002605
Iteration 9/1000 | Loss: 0.00002580
Iteration 10/1000 | Loss: 0.00002561
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002541
Iteration 13/1000 | Loss: 0.00002541
Iteration 14/1000 | Loss: 0.00002540
Iteration 15/1000 | Loss: 0.00002540
Iteration 16/1000 | Loss: 0.00002540
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002539
Iteration 19/1000 | Loss: 0.00002539
Iteration 20/1000 | Loss: 0.00002538
Iteration 21/1000 | Loss: 0.00002537
Iteration 22/1000 | Loss: 0.00002537
Iteration 23/1000 | Loss: 0.00002534
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002534
Iteration 26/1000 | Loss: 0.00002534
Iteration 27/1000 | Loss: 0.00002533
Iteration 28/1000 | Loss: 0.00002533
Iteration 29/1000 | Loss: 0.00002533
Iteration 30/1000 | Loss: 0.00002533
Iteration 31/1000 | Loss: 0.00002533
Iteration 32/1000 | Loss: 0.00002533
Iteration 33/1000 | Loss: 0.00002533
Iteration 34/1000 | Loss: 0.00002533
Iteration 35/1000 | Loss: 0.00002533
Iteration 36/1000 | Loss: 0.00002532
Iteration 37/1000 | Loss: 0.00002532
Iteration 38/1000 | Loss: 0.00002532
Iteration 39/1000 | Loss: 0.00002532
Iteration 40/1000 | Loss: 0.00002532
Iteration 41/1000 | Loss: 0.00002531
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002531
Iteration 44/1000 | Loss: 0.00002531
Iteration 45/1000 | Loss: 0.00002531
Iteration 46/1000 | Loss: 0.00002531
Iteration 47/1000 | Loss: 0.00002531
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002531
Iteration 50/1000 | Loss: 0.00002531
Iteration 51/1000 | Loss: 0.00002531
Iteration 52/1000 | Loss: 0.00002530
Iteration 53/1000 | Loss: 0.00002530
Iteration 54/1000 | Loss: 0.00002530
Iteration 55/1000 | Loss: 0.00002530
Iteration 56/1000 | Loss: 0.00002530
Iteration 57/1000 | Loss: 0.00002530
Iteration 58/1000 | Loss: 0.00002530
Iteration 59/1000 | Loss: 0.00002529
Iteration 60/1000 | Loss: 0.00002529
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002528
Iteration 63/1000 | Loss: 0.00002528
Iteration 64/1000 | Loss: 0.00002527
Iteration 65/1000 | Loss: 0.00002527
Iteration 66/1000 | Loss: 0.00002527
Iteration 67/1000 | Loss: 0.00002527
Iteration 68/1000 | Loss: 0.00002527
Iteration 69/1000 | Loss: 0.00002527
Iteration 70/1000 | Loss: 0.00002527
Iteration 71/1000 | Loss: 0.00002527
Iteration 72/1000 | Loss: 0.00002527
Iteration 73/1000 | Loss: 0.00002526
Iteration 74/1000 | Loss: 0.00002526
Iteration 75/1000 | Loss: 0.00002526
Iteration 76/1000 | Loss: 0.00002526
Iteration 77/1000 | Loss: 0.00002526
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002526
Iteration 82/1000 | Loss: 0.00002526
Iteration 83/1000 | Loss: 0.00002526
Iteration 84/1000 | Loss: 0.00002526
Iteration 85/1000 | Loss: 0.00002525
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002525
Iteration 90/1000 | Loss: 0.00002525
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002525
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002525
Iteration 98/1000 | Loss: 0.00002524
Iteration 99/1000 | Loss: 0.00002524
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002524
Iteration 102/1000 | Loss: 0.00002524
Iteration 103/1000 | Loss: 0.00002524
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002523
Iteration 106/1000 | Loss: 0.00002523
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002522
Iteration 109/1000 | Loss: 0.00002522
Iteration 110/1000 | Loss: 0.00002522
Iteration 111/1000 | Loss: 0.00002521
Iteration 112/1000 | Loss: 0.00002521
Iteration 113/1000 | Loss: 0.00002521
Iteration 114/1000 | Loss: 0.00002521
Iteration 115/1000 | Loss: 0.00002521
Iteration 116/1000 | Loss: 0.00002521
Iteration 117/1000 | Loss: 0.00002520
Iteration 118/1000 | Loss: 0.00002520
Iteration 119/1000 | Loss: 0.00002520
Iteration 120/1000 | Loss: 0.00002520
Iteration 121/1000 | Loss: 0.00002520
Iteration 122/1000 | Loss: 0.00002520
Iteration 123/1000 | Loss: 0.00002520
Iteration 124/1000 | Loss: 0.00002520
Iteration 125/1000 | Loss: 0.00002520
Iteration 126/1000 | Loss: 0.00002520
Iteration 127/1000 | Loss: 0.00002520
Iteration 128/1000 | Loss: 0.00002520
Iteration 129/1000 | Loss: 0.00002520
Iteration 130/1000 | Loss: 0.00002520
Iteration 131/1000 | Loss: 0.00002520
Iteration 132/1000 | Loss: 0.00002520
Iteration 133/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.5196937713189982e-05, 2.5196937713189982e-05, 2.5196937713189982e-05, 2.5196937713189982e-05, 2.5196937713189982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5196937713189982e-05

Optimization complete. Final v2v error: 4.23065185546875 mm

Highest mean error: 4.625110149383545 mm for frame 82

Lowest mean error: 4.0883307456970215 mm for frame 152

Saving results

Total time: 39.15366816520691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830099
Iteration 2/25 | Loss: 0.00175498
Iteration 3/25 | Loss: 0.00106179
Iteration 4/25 | Loss: 0.00093917
Iteration 5/25 | Loss: 0.00090145
Iteration 6/25 | Loss: 0.00087741
Iteration 7/25 | Loss: 0.00084269
Iteration 8/25 | Loss: 0.00081685
Iteration 9/25 | Loss: 0.00080294
Iteration 10/25 | Loss: 0.00079910
Iteration 11/25 | Loss: 0.00079633
Iteration 12/25 | Loss: 0.00079582
Iteration 13/25 | Loss: 0.00079559
Iteration 14/25 | Loss: 0.00079558
Iteration 15/25 | Loss: 0.00079557
Iteration 16/25 | Loss: 0.00079557
Iteration 17/25 | Loss: 0.00079557
Iteration 18/25 | Loss: 0.00079557
Iteration 19/25 | Loss: 0.00079557
Iteration 20/25 | Loss: 0.00079557
Iteration 21/25 | Loss: 0.00079557
Iteration 22/25 | Loss: 0.00079557
Iteration 23/25 | Loss: 0.00079557
Iteration 24/25 | Loss: 0.00079557
Iteration 25/25 | Loss: 0.00079557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68691337
Iteration 2/25 | Loss: 0.00037333
Iteration 3/25 | Loss: 0.00037331
Iteration 4/25 | Loss: 0.00037331
Iteration 5/25 | Loss: 0.00037331
Iteration 6/25 | Loss: 0.00037331
Iteration 7/25 | Loss: 0.00037331
Iteration 8/25 | Loss: 0.00037331
Iteration 9/25 | Loss: 0.00037331
Iteration 10/25 | Loss: 0.00037331
Iteration 11/25 | Loss: 0.00037331
Iteration 12/25 | Loss: 0.00037331
Iteration 13/25 | Loss: 0.00037331
Iteration 14/25 | Loss: 0.00037331
Iteration 15/25 | Loss: 0.00037331
Iteration 16/25 | Loss: 0.00037331
Iteration 17/25 | Loss: 0.00037331
Iteration 18/25 | Loss: 0.00037331
Iteration 19/25 | Loss: 0.00037331
Iteration 20/25 | Loss: 0.00037331
Iteration 21/25 | Loss: 0.00037331
Iteration 22/25 | Loss: 0.00037331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003733069752342999, 0.0003733069752342999, 0.0003733069752342999, 0.0003733069752342999, 0.0003733069752342999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003733069752342999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037331
Iteration 2/1000 | Loss: 0.00003665
Iteration 3/1000 | Loss: 0.00002760
Iteration 4/1000 | Loss: 0.00002569
Iteration 5/1000 | Loss: 0.00002440
Iteration 6/1000 | Loss: 0.00002371
Iteration 7/1000 | Loss: 0.00002325
Iteration 8/1000 | Loss: 0.00002288
Iteration 9/1000 | Loss: 0.00002264
Iteration 10/1000 | Loss: 0.00002248
Iteration 11/1000 | Loss: 0.00002244
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002237
Iteration 14/1000 | Loss: 0.00002234
Iteration 15/1000 | Loss: 0.00002230
Iteration 16/1000 | Loss: 0.00002225
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002210
Iteration 21/1000 | Loss: 0.00002210
Iteration 22/1000 | Loss: 0.00002209
Iteration 23/1000 | Loss: 0.00002209
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002209
Iteration 29/1000 | Loss: 0.00002209
Iteration 30/1000 | Loss: 0.00002209
Iteration 31/1000 | Loss: 0.00002208
Iteration 32/1000 | Loss: 0.00002208
Iteration 33/1000 | Loss: 0.00002207
Iteration 34/1000 | Loss: 0.00002206
Iteration 35/1000 | Loss: 0.00002206
Iteration 36/1000 | Loss: 0.00002204
Iteration 37/1000 | Loss: 0.00002204
Iteration 38/1000 | Loss: 0.00002204
Iteration 39/1000 | Loss: 0.00002204
Iteration 40/1000 | Loss: 0.00002204
Iteration 41/1000 | Loss: 0.00002204
Iteration 42/1000 | Loss: 0.00002204
Iteration 43/1000 | Loss: 0.00002204
Iteration 44/1000 | Loss: 0.00002203
Iteration 45/1000 | Loss: 0.00002203
Iteration 46/1000 | Loss: 0.00002203
Iteration 47/1000 | Loss: 0.00002203
Iteration 48/1000 | Loss: 0.00002203
Iteration 49/1000 | Loss: 0.00002203
Iteration 50/1000 | Loss: 0.00002203
Iteration 51/1000 | Loss: 0.00002203
Iteration 52/1000 | Loss: 0.00002203
Iteration 53/1000 | Loss: 0.00002203
Iteration 54/1000 | Loss: 0.00002203
Iteration 55/1000 | Loss: 0.00002203
Iteration 56/1000 | Loss: 0.00002203
Iteration 57/1000 | Loss: 0.00002203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [2.2033354980521835e-05, 2.2033354980521835e-05, 2.2033354980521835e-05, 2.2033354980521835e-05, 2.2033354980521835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2033354980521835e-05

Optimization complete. Final v2v error: 3.7940239906311035 mm

Highest mean error: 5.931828498840332 mm for frame 188

Lowest mean error: 2.8651647567749023 mm for frame 109

Saving results

Total time: 50.158318281173706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033468
Iteration 2/25 | Loss: 0.01033468
Iteration 3/25 | Loss: 0.00180500
Iteration 4/25 | Loss: 0.00121914
Iteration 5/25 | Loss: 0.00111011
Iteration 6/25 | Loss: 0.00111136
Iteration 7/25 | Loss: 0.00122223
Iteration 8/25 | Loss: 0.00112924
Iteration 9/25 | Loss: 0.00108097
Iteration 10/25 | Loss: 0.00099383
Iteration 11/25 | Loss: 0.00094202
Iteration 12/25 | Loss: 0.00092148
Iteration 13/25 | Loss: 0.00090793
Iteration 14/25 | Loss: 0.00090066
Iteration 15/25 | Loss: 0.00088941
Iteration 16/25 | Loss: 0.00088309
Iteration 17/25 | Loss: 0.00087988
Iteration 18/25 | Loss: 0.00088307
Iteration 19/25 | Loss: 0.00088447
Iteration 20/25 | Loss: 0.00089287
Iteration 21/25 | Loss: 0.00088792
Iteration 22/25 | Loss: 0.00088958
Iteration 23/25 | Loss: 0.00089339
Iteration 24/25 | Loss: 0.00088726
Iteration 25/25 | Loss: 0.00088863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00090599
Iteration 2/25 | Loss: 0.00106747
Iteration 3/25 | Loss: 0.00106747
Iteration 4/25 | Loss: 0.00106747
Iteration 5/25 | Loss: 0.00106747
Iteration 6/25 | Loss: 0.00106747
Iteration 7/25 | Loss: 0.00106747
Iteration 8/25 | Loss: 0.00106747
Iteration 9/25 | Loss: 0.00106747
Iteration 10/25 | Loss: 0.00106747
Iteration 11/25 | Loss: 0.00106747
Iteration 12/25 | Loss: 0.00106747
Iteration 13/25 | Loss: 0.00106747
Iteration 14/25 | Loss: 0.00106747
Iteration 15/25 | Loss: 0.00106747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010674702934920788, 0.0010674702934920788, 0.0010674702934920788, 0.0010674702934920788, 0.0010674702934920788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010674702934920788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106747
Iteration 2/1000 | Loss: 0.00022351
Iteration 3/1000 | Loss: 0.00016026
Iteration 4/1000 | Loss: 0.00013777
Iteration 5/1000 | Loss: 0.00021074
Iteration 6/1000 | Loss: 0.00013651
Iteration 7/1000 | Loss: 0.00018217
Iteration 8/1000 | Loss: 0.00016900
Iteration 9/1000 | Loss: 0.00013572
Iteration 10/1000 | Loss: 0.00016960
Iteration 11/1000 | Loss: 0.00017902
Iteration 12/1000 | Loss: 0.00017290
Iteration 13/1000 | Loss: 0.00016723
Iteration 14/1000 | Loss: 0.00012825
Iteration 15/1000 | Loss: 0.00013048
Iteration 16/1000 | Loss: 0.00016010
Iteration 17/1000 | Loss: 0.00014834
Iteration 18/1000 | Loss: 0.00018294
Iteration 19/1000 | Loss: 0.00013882
Iteration 20/1000 | Loss: 0.00010854
Iteration 21/1000 | Loss: 0.00009063
Iteration 22/1000 | Loss: 0.00009703
Iteration 23/1000 | Loss: 0.00010133
Iteration 24/1000 | Loss: 0.00014248
Iteration 25/1000 | Loss: 0.00012337
Iteration 26/1000 | Loss: 0.00015379
Iteration 27/1000 | Loss: 0.00014016
Iteration 28/1000 | Loss: 0.00013117
Iteration 29/1000 | Loss: 0.00011148
Iteration 30/1000 | Loss: 0.00010244
Iteration 31/1000 | Loss: 0.00012595
Iteration 32/1000 | Loss: 0.00012520
Iteration 33/1000 | Loss: 0.00013413
Iteration 34/1000 | Loss: 0.00014026
Iteration 35/1000 | Loss: 0.00015142
Iteration 36/1000 | Loss: 0.00016877
Iteration 37/1000 | Loss: 0.00013605
Iteration 38/1000 | Loss: 0.00011484
Iteration 39/1000 | Loss: 0.00010904
Iteration 40/1000 | Loss: 0.00009370
Iteration 41/1000 | Loss: 0.00016383
Iteration 42/1000 | Loss: 0.00015158
Iteration 43/1000 | Loss: 0.00010242
Iteration 44/1000 | Loss: 0.00011676
Iteration 45/1000 | Loss: 0.00012303
Iteration 46/1000 | Loss: 0.00013911
Iteration 47/1000 | Loss: 0.00012300
Iteration 48/1000 | Loss: 0.00013502
Iteration 49/1000 | Loss: 0.00014655
Iteration 50/1000 | Loss: 0.00015367
Iteration 51/1000 | Loss: 0.00015487
Iteration 52/1000 | Loss: 0.00015970
Iteration 53/1000 | Loss: 0.00012517
Iteration 54/1000 | Loss: 0.00010659
Iteration 55/1000 | Loss: 0.00013372
Iteration 56/1000 | Loss: 0.00009939
Iteration 57/1000 | Loss: 0.00013086
Iteration 58/1000 | Loss: 0.00013836
Iteration 59/1000 | Loss: 0.00013794
Iteration 60/1000 | Loss: 0.00014305
Iteration 61/1000 | Loss: 0.00013801
Iteration 62/1000 | Loss: 0.00014999
Iteration 63/1000 | Loss: 0.00011437
Iteration 64/1000 | Loss: 0.00013402
Iteration 65/1000 | Loss: 0.00011907
Iteration 66/1000 | Loss: 0.00013587
Iteration 67/1000 | Loss: 0.00013658
Iteration 68/1000 | Loss: 0.00015095
Iteration 69/1000 | Loss: 0.00013372
Iteration 70/1000 | Loss: 0.00015306
Iteration 71/1000 | Loss: 0.00011989
Iteration 72/1000 | Loss: 0.00015284
Iteration 73/1000 | Loss: 0.00016301
Iteration 74/1000 | Loss: 0.00015568
Iteration 75/1000 | Loss: 0.00013140
Iteration 76/1000 | Loss: 0.00013251
Iteration 77/1000 | Loss: 0.00013283
Iteration 78/1000 | Loss: 0.00015609
Iteration 79/1000 | Loss: 0.00013609
Iteration 80/1000 | Loss: 0.00014326
Iteration 81/1000 | Loss: 0.00012177
Iteration 82/1000 | Loss: 0.00012293
Iteration 83/1000 | Loss: 0.00009428
Iteration 84/1000 | Loss: 0.00009298
Iteration 85/1000 | Loss: 0.00014529
Iteration 86/1000 | Loss: 0.00014486
Iteration 87/1000 | Loss: 0.00012484
Iteration 88/1000 | Loss: 0.00012440
Iteration 89/1000 | Loss: 0.00008645
Iteration 90/1000 | Loss: 0.00009256
Iteration 91/1000 | Loss: 0.00011407
Iteration 92/1000 | Loss: 0.00017312
Iteration 93/1000 | Loss: 0.00015319
Iteration 94/1000 | Loss: 0.00011852
Iteration 95/1000 | Loss: 0.00013501
Iteration 96/1000 | Loss: 0.00012794
Iteration 97/1000 | Loss: 0.00014606
Iteration 98/1000 | Loss: 0.00013327
Iteration 99/1000 | Loss: 0.00014382
Iteration 100/1000 | Loss: 0.00013335
Iteration 101/1000 | Loss: 0.00013644
Iteration 102/1000 | Loss: 0.00013724
Iteration 103/1000 | Loss: 0.00014338
Iteration 104/1000 | Loss: 0.00014567
Iteration 105/1000 | Loss: 0.00012238
Iteration 106/1000 | Loss: 0.00007746
Iteration 107/1000 | Loss: 0.00012290
Iteration 108/1000 | Loss: 0.00011339
Iteration 109/1000 | Loss: 0.00014231
Iteration 110/1000 | Loss: 0.00012212
Iteration 111/1000 | Loss: 0.00010414
Iteration 112/1000 | Loss: 0.00011381
Iteration 113/1000 | Loss: 0.00010352
Iteration 114/1000 | Loss: 0.00010430
Iteration 115/1000 | Loss: 0.00008656
Iteration 116/1000 | Loss: 0.00007372
Iteration 117/1000 | Loss: 0.00012111
Iteration 118/1000 | Loss: 0.00010583
Iteration 119/1000 | Loss: 0.00009391
Iteration 120/1000 | Loss: 0.00009118
Iteration 121/1000 | Loss: 0.00010268
Iteration 122/1000 | Loss: 0.00010010
Iteration 123/1000 | Loss: 0.00011953
Iteration 124/1000 | Loss: 0.00008366
Iteration 125/1000 | Loss: 0.00010103
Iteration 126/1000 | Loss: 0.00009091
Iteration 127/1000 | Loss: 0.00009664
Iteration 128/1000 | Loss: 0.00011227
Iteration 129/1000 | Loss: 0.00008687
Iteration 130/1000 | Loss: 0.00008382
Iteration 131/1000 | Loss: 0.00008470
Iteration 132/1000 | Loss: 0.00008800
Iteration 133/1000 | Loss: 0.00009541
Iteration 134/1000 | Loss: 0.00009264
Iteration 135/1000 | Loss: 0.00014672
Iteration 136/1000 | Loss: 0.00014995
Iteration 137/1000 | Loss: 0.00011082
Iteration 138/1000 | Loss: 0.00010732
Iteration 139/1000 | Loss: 0.00010584
Iteration 140/1000 | Loss: 0.00012120
Iteration 141/1000 | Loss: 0.00010499
Iteration 142/1000 | Loss: 0.00013034
Iteration 143/1000 | Loss: 0.00010999
Iteration 144/1000 | Loss: 0.00011933
Iteration 145/1000 | Loss: 0.00010691
Iteration 146/1000 | Loss: 0.00009167
Iteration 147/1000 | Loss: 0.00009600
Iteration 148/1000 | Loss: 0.00011009
Iteration 149/1000 | Loss: 0.00011519
Iteration 150/1000 | Loss: 0.00011159
Iteration 151/1000 | Loss: 0.00009965
Iteration 152/1000 | Loss: 0.00010557
Iteration 153/1000 | Loss: 0.00008741
Iteration 154/1000 | Loss: 0.00010463
Iteration 155/1000 | Loss: 0.00012346
Iteration 156/1000 | Loss: 0.00012230
Iteration 157/1000 | Loss: 0.00012606
Iteration 158/1000 | Loss: 0.00011883
Iteration 159/1000 | Loss: 0.00012106
Iteration 160/1000 | Loss: 0.00011193
Iteration 161/1000 | Loss: 0.00011308
Iteration 162/1000 | Loss: 0.00013486
Iteration 163/1000 | Loss: 0.00014166
Iteration 164/1000 | Loss: 0.00013085
Iteration 165/1000 | Loss: 0.00011882
Iteration 166/1000 | Loss: 0.00012830
Iteration 167/1000 | Loss: 0.00012995
Iteration 168/1000 | Loss: 0.00013621
Iteration 169/1000 | Loss: 0.00015658
Iteration 170/1000 | Loss: 0.00013972
Iteration 171/1000 | Loss: 0.00011843
Iteration 172/1000 | Loss: 0.00012469
Iteration 173/1000 | Loss: 0.00015183
Iteration 174/1000 | Loss: 0.00016715
Iteration 175/1000 | Loss: 0.00013550
Iteration 176/1000 | Loss: 0.00011243
Iteration 177/1000 | Loss: 0.00012293
Iteration 178/1000 | Loss: 0.00009587
Iteration 179/1000 | Loss: 0.00012435
Iteration 180/1000 | Loss: 0.00013381
Iteration 181/1000 | Loss: 0.00012583
Iteration 182/1000 | Loss: 0.00010211
Iteration 183/1000 | Loss: 0.00012694
Iteration 184/1000 | Loss: 0.00010449
Iteration 185/1000 | Loss: 0.00013290
Iteration 186/1000 | Loss: 0.00010985
Iteration 187/1000 | Loss: 0.00013093
Iteration 188/1000 | Loss: 0.00012327
Iteration 189/1000 | Loss: 0.00012445
Iteration 190/1000 | Loss: 0.00011895
Iteration 191/1000 | Loss: 0.00013031
Iteration 192/1000 | Loss: 0.00009853
Iteration 193/1000 | Loss: 0.00014101
Iteration 194/1000 | Loss: 0.00010343
Iteration 195/1000 | Loss: 0.00010727
Iteration 196/1000 | Loss: 0.00009083
Iteration 197/1000 | Loss: 0.00012207
Iteration 198/1000 | Loss: 0.00015612
Iteration 199/1000 | Loss: 0.00013908
Iteration 200/1000 | Loss: 0.00008031
Iteration 201/1000 | Loss: 0.00009364
Iteration 202/1000 | Loss: 0.00010938
Iteration 203/1000 | Loss: 0.00008689
Iteration 204/1000 | Loss: 0.00007766
Iteration 205/1000 | Loss: 0.00008364
Iteration 206/1000 | Loss: 0.00010104
Iteration 207/1000 | Loss: 0.00012812
Iteration 208/1000 | Loss: 0.00010224
Iteration 209/1000 | Loss: 0.00010498
Iteration 210/1000 | Loss: 0.00011497
Iteration 211/1000 | Loss: 0.00012117
Iteration 212/1000 | Loss: 0.00009465
Iteration 213/1000 | Loss: 0.00010633
Iteration 214/1000 | Loss: 0.00012962
Iteration 215/1000 | Loss: 0.00012192
Iteration 216/1000 | Loss: 0.00009379
Iteration 217/1000 | Loss: 0.00009926
Iteration 218/1000 | Loss: 0.00009033
Iteration 219/1000 | Loss: 0.00011325
Iteration 220/1000 | Loss: 0.00011399
Iteration 221/1000 | Loss: 0.00013824
Iteration 222/1000 | Loss: 0.00009287
Iteration 223/1000 | Loss: 0.00005723
Iteration 224/1000 | Loss: 0.00009077
Iteration 225/1000 | Loss: 0.00010054
Iteration 226/1000 | Loss: 0.00011764
Iteration 227/1000 | Loss: 0.00008838
Iteration 228/1000 | Loss: 0.00007113
Iteration 229/1000 | Loss: 0.00006930
Iteration 230/1000 | Loss: 0.00006708
Iteration 231/1000 | Loss: 0.00008374
Iteration 232/1000 | Loss: 0.00008478
Iteration 233/1000 | Loss: 0.00009423
Iteration 234/1000 | Loss: 0.00009279
Iteration 235/1000 | Loss: 0.00011038
Iteration 236/1000 | Loss: 0.00008181
Iteration 237/1000 | Loss: 0.00011424
Iteration 238/1000 | Loss: 0.00007392
Iteration 239/1000 | Loss: 0.00010285
Iteration 240/1000 | Loss: 0.00006205
Iteration 241/1000 | Loss: 0.00008659
Iteration 242/1000 | Loss: 0.00008709
Iteration 243/1000 | Loss: 0.00009792
Iteration 244/1000 | Loss: 0.00008387
Iteration 245/1000 | Loss: 0.00008994
Iteration 246/1000 | Loss: 0.00007265
Iteration 247/1000 | Loss: 0.00009028
Iteration 248/1000 | Loss: 0.00007678
Iteration 249/1000 | Loss: 0.00009455
Iteration 250/1000 | Loss: 0.00009514
Iteration 251/1000 | Loss: 0.00009828
Iteration 252/1000 | Loss: 0.00008953
Iteration 253/1000 | Loss: 0.00009169
Iteration 254/1000 | Loss: 0.00008780
Iteration 255/1000 | Loss: 0.00008869
Iteration 256/1000 | Loss: 0.00009014
Iteration 257/1000 | Loss: 0.00009346
Iteration 258/1000 | Loss: 0.00009471
Iteration 259/1000 | Loss: 0.00010039
Iteration 260/1000 | Loss: 0.00008416
Iteration 261/1000 | Loss: 0.00009883
Iteration 262/1000 | Loss: 0.00007898
Iteration 263/1000 | Loss: 0.00009199
Iteration 264/1000 | Loss: 0.00008594
Iteration 265/1000 | Loss: 0.00005241
Iteration 266/1000 | Loss: 0.00008127
Iteration 267/1000 | Loss: 0.00007303
Iteration 268/1000 | Loss: 0.00006138
Iteration 269/1000 | Loss: 0.00005635
Iteration 270/1000 | Loss: 0.00007029
Iteration 271/1000 | Loss: 0.00008001
Iteration 272/1000 | Loss: 0.00006038
Iteration 273/1000 | Loss: 0.00010686
Iteration 274/1000 | Loss: 0.00007970
Iteration 275/1000 | Loss: 0.00008334
Iteration 276/1000 | Loss: 0.00008292
Iteration 277/1000 | Loss: 0.00007744
Iteration 278/1000 | Loss: 0.00008702
Iteration 279/1000 | Loss: 0.00008499
Iteration 280/1000 | Loss: 0.00006887
Iteration 281/1000 | Loss: 0.00006518
Iteration 282/1000 | Loss: 0.00006601
Iteration 283/1000 | Loss: 0.00007215
Iteration 284/1000 | Loss: 0.00007689
Iteration 285/1000 | Loss: 0.00006655
Iteration 286/1000 | Loss: 0.00005979
Iteration 287/1000 | Loss: 0.00007955
Iteration 288/1000 | Loss: 0.00006566
Iteration 289/1000 | Loss: 0.00004119
Iteration 290/1000 | Loss: 0.00005252
Iteration 291/1000 | Loss: 0.00003228
Iteration 292/1000 | Loss: 0.00004447
Iteration 293/1000 | Loss: 0.00004627
Iteration 294/1000 | Loss: 0.00003976
Iteration 295/1000 | Loss: 0.00005217
Iteration 296/1000 | Loss: 0.00015763
Iteration 297/1000 | Loss: 0.00004721
Iteration 298/1000 | Loss: 0.00004843
Iteration 299/1000 | Loss: 0.00004959
Iteration 300/1000 | Loss: 0.00004039
Iteration 301/1000 | Loss: 0.00003343
Iteration 302/1000 | Loss: 0.00004583
Iteration 303/1000 | Loss: 0.00004510
Iteration 304/1000 | Loss: 0.00004978
Iteration 305/1000 | Loss: 0.00004471
Iteration 306/1000 | Loss: 0.00006348
Iteration 307/1000 | Loss: 0.00005408
Iteration 308/1000 | Loss: 0.00004758
Iteration 309/1000 | Loss: 0.00005129
Iteration 310/1000 | Loss: 0.00004704
Iteration 311/1000 | Loss: 0.00004924
Iteration 312/1000 | Loss: 0.00003078
Iteration 313/1000 | Loss: 0.00002822
Iteration 314/1000 | Loss: 0.00002658
Iteration 315/1000 | Loss: 0.00002584
Iteration 316/1000 | Loss: 0.00002533
Iteration 317/1000 | Loss: 0.00002462
Iteration 318/1000 | Loss: 0.00002405
Iteration 319/1000 | Loss: 0.00002376
Iteration 320/1000 | Loss: 0.00002348
Iteration 321/1000 | Loss: 0.00002324
Iteration 322/1000 | Loss: 0.00002318
Iteration 323/1000 | Loss: 0.00002294
Iteration 324/1000 | Loss: 0.00002289
Iteration 325/1000 | Loss: 0.00002272
Iteration 326/1000 | Loss: 0.00002266
Iteration 327/1000 | Loss: 0.00002264
Iteration 328/1000 | Loss: 0.00002262
Iteration 329/1000 | Loss: 0.00002253
Iteration 330/1000 | Loss: 0.00002252
Iteration 331/1000 | Loss: 0.00002252
Iteration 332/1000 | Loss: 0.00002252
Iteration 333/1000 | Loss: 0.00002252
Iteration 334/1000 | Loss: 0.00002251
Iteration 335/1000 | Loss: 0.00002251
Iteration 336/1000 | Loss: 0.00002251
Iteration 337/1000 | Loss: 0.00002251
Iteration 338/1000 | Loss: 0.00002251
Iteration 339/1000 | Loss: 0.00002249
Iteration 340/1000 | Loss: 0.00002248
Iteration 341/1000 | Loss: 0.00002244
Iteration 342/1000 | Loss: 0.00002244
Iteration 343/1000 | Loss: 0.00002243
Iteration 344/1000 | Loss: 0.00002242
Iteration 345/1000 | Loss: 0.00002241
Iteration 346/1000 | Loss: 0.00002241
Iteration 347/1000 | Loss: 0.00002241
Iteration 348/1000 | Loss: 0.00002241
Iteration 349/1000 | Loss: 0.00002241
Iteration 350/1000 | Loss: 0.00002241
Iteration 351/1000 | Loss: 0.00002240
Iteration 352/1000 | Loss: 0.00002238
Iteration 353/1000 | Loss: 0.00002238
Iteration 354/1000 | Loss: 0.00002238
Iteration 355/1000 | Loss: 0.00002238
Iteration 356/1000 | Loss: 0.00002238
Iteration 357/1000 | Loss: 0.00002237
Iteration 358/1000 | Loss: 0.00002236
Iteration 359/1000 | Loss: 0.00002236
Iteration 360/1000 | Loss: 0.00002236
Iteration 361/1000 | Loss: 0.00002236
Iteration 362/1000 | Loss: 0.00002236
Iteration 363/1000 | Loss: 0.00002235
Iteration 364/1000 | Loss: 0.00002235
Iteration 365/1000 | Loss: 0.00002235
Iteration 366/1000 | Loss: 0.00002235
Iteration 367/1000 | Loss: 0.00002235
Iteration 368/1000 | Loss: 0.00002235
Iteration 369/1000 | Loss: 0.00002235
Iteration 370/1000 | Loss: 0.00002235
Iteration 371/1000 | Loss: 0.00002235
Iteration 372/1000 | Loss: 0.00002235
Iteration 373/1000 | Loss: 0.00002235
Iteration 374/1000 | Loss: 0.00002235
Iteration 375/1000 | Loss: 0.00002234
Iteration 376/1000 | Loss: 0.00002234
Iteration 377/1000 | Loss: 0.00002234
Iteration 378/1000 | Loss: 0.00002234
Iteration 379/1000 | Loss: 0.00002234
Iteration 380/1000 | Loss: 0.00002234
Iteration 381/1000 | Loss: 0.00002234
Iteration 382/1000 | Loss: 0.00002234
Iteration 383/1000 | Loss: 0.00002234
Iteration 384/1000 | Loss: 0.00002234
Iteration 385/1000 | Loss: 0.00002234
Iteration 386/1000 | Loss: 0.00002234
Iteration 387/1000 | Loss: 0.00002234
Iteration 388/1000 | Loss: 0.00002234
Iteration 389/1000 | Loss: 0.00002234
Iteration 390/1000 | Loss: 0.00002234
Iteration 391/1000 | Loss: 0.00002234
Iteration 392/1000 | Loss: 0.00002233
Iteration 393/1000 | Loss: 0.00002233
Iteration 394/1000 | Loss: 0.00002233
Iteration 395/1000 | Loss: 0.00002233
Iteration 396/1000 | Loss: 0.00002233
Iteration 397/1000 | Loss: 0.00002233
Iteration 398/1000 | Loss: 0.00002233
Iteration 399/1000 | Loss: 0.00002233
Iteration 400/1000 | Loss: 0.00002233
Iteration 401/1000 | Loss: 0.00002233
Iteration 402/1000 | Loss: 0.00002233
Iteration 403/1000 | Loss: 0.00002232
Iteration 404/1000 | Loss: 0.00002232
Iteration 405/1000 | Loss: 0.00002232
Iteration 406/1000 | Loss: 0.00002232
Iteration 407/1000 | Loss: 0.00002232
Iteration 408/1000 | Loss: 0.00002232
Iteration 409/1000 | Loss: 0.00002232
Iteration 410/1000 | Loss: 0.00002232
Iteration 411/1000 | Loss: 0.00002232
Iteration 412/1000 | Loss: 0.00002232
Iteration 413/1000 | Loss: 0.00002232
Iteration 414/1000 | Loss: 0.00002232
Iteration 415/1000 | Loss: 0.00002232
Iteration 416/1000 | Loss: 0.00002231
Iteration 417/1000 | Loss: 0.00002231
Iteration 418/1000 | Loss: 0.00002231
Iteration 419/1000 | Loss: 0.00002231
Iteration 420/1000 | Loss: 0.00002230
Iteration 421/1000 | Loss: 0.00002230
Iteration 422/1000 | Loss: 0.00002230
Iteration 423/1000 | Loss: 0.00002230
Iteration 424/1000 | Loss: 0.00002230
Iteration 425/1000 | Loss: 0.00002230
Iteration 426/1000 | Loss: 0.00002230
Iteration 427/1000 | Loss: 0.00002230
Iteration 428/1000 | Loss: 0.00002230
Iteration 429/1000 | Loss: 0.00002230
Iteration 430/1000 | Loss: 0.00002230
Iteration 431/1000 | Loss: 0.00002230
Iteration 432/1000 | Loss: 0.00002229
Iteration 433/1000 | Loss: 0.00002229
Iteration 434/1000 | Loss: 0.00002229
Iteration 435/1000 | Loss: 0.00002229
Iteration 436/1000 | Loss: 0.00002228
Iteration 437/1000 | Loss: 0.00002228
Iteration 438/1000 | Loss: 0.00002228
Iteration 439/1000 | Loss: 0.00002228
Iteration 440/1000 | Loss: 0.00002227
Iteration 441/1000 | Loss: 0.00002227
Iteration 442/1000 | Loss: 0.00002227
Iteration 443/1000 | Loss: 0.00002227
Iteration 444/1000 | Loss: 0.00002227
Iteration 445/1000 | Loss: 0.00002227
Iteration 446/1000 | Loss: 0.00002226
Iteration 447/1000 | Loss: 0.00002226
Iteration 448/1000 | Loss: 0.00002226
Iteration 449/1000 | Loss: 0.00002226
Iteration 450/1000 | Loss: 0.00002226
Iteration 451/1000 | Loss: 0.00002226
Iteration 452/1000 | Loss: 0.00002226
Iteration 453/1000 | Loss: 0.00002225
Iteration 454/1000 | Loss: 0.00002225
Iteration 455/1000 | Loss: 0.00002225
Iteration 456/1000 | Loss: 0.00002225
Iteration 457/1000 | Loss: 0.00002225
Iteration 458/1000 | Loss: 0.00002225
Iteration 459/1000 | Loss: 0.00002225
Iteration 460/1000 | Loss: 0.00002225
Iteration 461/1000 | Loss: 0.00002224
Iteration 462/1000 | Loss: 0.00002224
Iteration 463/1000 | Loss: 0.00002224
Iteration 464/1000 | Loss: 0.00002224
Iteration 465/1000 | Loss: 0.00002224
Iteration 466/1000 | Loss: 0.00002224
Iteration 467/1000 | Loss: 0.00002224
Iteration 468/1000 | Loss: 0.00002223
Iteration 469/1000 | Loss: 0.00002223
Iteration 470/1000 | Loss: 0.00002223
Iteration 471/1000 | Loss: 0.00002223
Iteration 472/1000 | Loss: 0.00002223
Iteration 473/1000 | Loss: 0.00002223
Iteration 474/1000 | Loss: 0.00002222
Iteration 475/1000 | Loss: 0.00002222
Iteration 476/1000 | Loss: 0.00002222
Iteration 477/1000 | Loss: 0.00002222
Iteration 478/1000 | Loss: 0.00002222
Iteration 479/1000 | Loss: 0.00002222
Iteration 480/1000 | Loss: 0.00002222
Iteration 481/1000 | Loss: 0.00002221
Iteration 482/1000 | Loss: 0.00002221
Iteration 483/1000 | Loss: 0.00002221
Iteration 484/1000 | Loss: 0.00002221
Iteration 485/1000 | Loss: 0.00002221
Iteration 486/1000 | Loss: 0.00002221
Iteration 487/1000 | Loss: 0.00002221
Iteration 488/1000 | Loss: 0.00002221
Iteration 489/1000 | Loss: 0.00002221
Iteration 490/1000 | Loss: 0.00002220
Iteration 491/1000 | Loss: 0.00002220
Iteration 492/1000 | Loss: 0.00002220
Iteration 493/1000 | Loss: 0.00002220
Iteration 494/1000 | Loss: 0.00002220
Iteration 495/1000 | Loss: 0.00002220
Iteration 496/1000 | Loss: 0.00002220
Iteration 497/1000 | Loss: 0.00002220
Iteration 498/1000 | Loss: 0.00002220
Iteration 499/1000 | Loss: 0.00002220
Iteration 500/1000 | Loss: 0.00002220
Iteration 501/1000 | Loss: 0.00002220
Iteration 502/1000 | Loss: 0.00002220
Iteration 503/1000 | Loss: 0.00002220
Iteration 504/1000 | Loss: 0.00002220
Iteration 505/1000 | Loss: 0.00002220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 505. Stopping optimization.
Last 5 losses: [2.220424903498497e-05, 2.220424903498497e-05, 2.220424903498497e-05, 2.220424903498497e-05, 2.220424903498497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.220424903498497e-05

Optimization complete. Final v2v error: 3.7871217727661133 mm

Highest mean error: 6.48232889175415 mm for frame 111

Lowest mean error: 3.4461936950683594 mm for frame 1

Saving results

Total time: 588.2956051826477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081578
Iteration 2/25 | Loss: 0.00262819
Iteration 3/25 | Loss: 0.00217453
Iteration 4/25 | Loss: 0.00210937
Iteration 5/25 | Loss: 0.00169716
Iteration 6/25 | Loss: 0.00167300
Iteration 7/25 | Loss: 0.00114104
Iteration 8/25 | Loss: 0.00107244
Iteration 9/25 | Loss: 0.00103875
Iteration 10/25 | Loss: 0.00096469
Iteration 11/25 | Loss: 0.00096353
Iteration 12/25 | Loss: 0.00095130
Iteration 13/25 | Loss: 0.00094271
Iteration 14/25 | Loss: 0.00093299
Iteration 15/25 | Loss: 0.00093055
Iteration 16/25 | Loss: 0.00092267
Iteration 17/25 | Loss: 0.00092044
Iteration 18/25 | Loss: 0.00092419
Iteration 19/25 | Loss: 0.00091865
Iteration 20/25 | Loss: 0.00091779
Iteration 21/25 | Loss: 0.00091765
Iteration 22/25 | Loss: 0.00092253
Iteration 23/25 | Loss: 0.00091598
Iteration 24/25 | Loss: 0.00092175
Iteration 25/25 | Loss: 0.00091456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40670764
Iteration 2/25 | Loss: 0.00164061
Iteration 3/25 | Loss: 0.00109984
Iteration 4/25 | Loss: 0.00109984
Iteration 5/25 | Loss: 0.00109984
Iteration 6/25 | Loss: 0.00109984
Iteration 7/25 | Loss: 0.00109984
Iteration 8/25 | Loss: 0.00109984
Iteration 9/25 | Loss: 0.00109984
Iteration 10/25 | Loss: 0.00109984
Iteration 11/25 | Loss: 0.00109984
Iteration 12/25 | Loss: 0.00109984
Iteration 13/25 | Loss: 0.00109984
Iteration 14/25 | Loss: 0.00109984
Iteration 15/25 | Loss: 0.00109984
Iteration 16/25 | Loss: 0.00109984
Iteration 17/25 | Loss: 0.00109984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010998351499438286, 0.0010998351499438286, 0.0010998351499438286, 0.0010998351499438286, 0.0010998351499438286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010998351499438286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109984
Iteration 2/1000 | Loss: 0.00179605
Iteration 3/1000 | Loss: 0.00352252
Iteration 4/1000 | Loss: 0.00110750
Iteration 5/1000 | Loss: 0.00061407
Iteration 6/1000 | Loss: 0.00167150
Iteration 7/1000 | Loss: 0.00013627
Iteration 8/1000 | Loss: 0.00029322
Iteration 9/1000 | Loss: 0.00006360
Iteration 10/1000 | Loss: 0.00005165
Iteration 11/1000 | Loss: 0.00037048
Iteration 12/1000 | Loss: 0.00005135
Iteration 13/1000 | Loss: 0.00003870
Iteration 14/1000 | Loss: 0.00060126
Iteration 15/1000 | Loss: 0.00135676
Iteration 16/1000 | Loss: 0.00103369
Iteration 17/1000 | Loss: 0.00117232
Iteration 18/1000 | Loss: 0.00319917
Iteration 19/1000 | Loss: 0.00008315
Iteration 20/1000 | Loss: 0.00003873
Iteration 21/1000 | Loss: 0.00047913
Iteration 22/1000 | Loss: 0.00039055
Iteration 23/1000 | Loss: 0.00018924
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002370
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002238
Iteration 28/1000 | Loss: 0.00054350
Iteration 29/1000 | Loss: 0.00005903
Iteration 30/1000 | Loss: 0.00034169
Iteration 31/1000 | Loss: 0.00002218
Iteration 32/1000 | Loss: 0.00002173
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002143
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002132
Iteration 42/1000 | Loss: 0.00002132
Iteration 43/1000 | Loss: 0.00002132
Iteration 44/1000 | Loss: 0.00002132
Iteration 45/1000 | Loss: 0.00002132
Iteration 46/1000 | Loss: 0.00002132
Iteration 47/1000 | Loss: 0.00002132
Iteration 48/1000 | Loss: 0.00002132
Iteration 49/1000 | Loss: 0.00002132
Iteration 50/1000 | Loss: 0.00002132
Iteration 51/1000 | Loss: 0.00002131
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002131
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002128
Iteration 57/1000 | Loss: 0.00002128
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002121
Iteration 64/1000 | Loss: 0.00002118
Iteration 65/1000 | Loss: 0.00002118
Iteration 66/1000 | Loss: 0.00002118
Iteration 67/1000 | Loss: 0.00002117
Iteration 68/1000 | Loss: 0.00002117
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002115
Iteration 76/1000 | Loss: 0.00002115
Iteration 77/1000 | Loss: 0.00002115
Iteration 78/1000 | Loss: 0.00002114
Iteration 79/1000 | Loss: 0.00002114
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002112
Iteration 91/1000 | Loss: 0.00002112
Iteration 92/1000 | Loss: 0.00002112
Iteration 93/1000 | Loss: 0.00002112
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002112
Iteration 96/1000 | Loss: 0.00002112
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002111
Iteration 103/1000 | Loss: 0.00002111
Iteration 104/1000 | Loss: 0.00002111
Iteration 105/1000 | Loss: 0.00002111
Iteration 106/1000 | Loss: 0.00002111
Iteration 107/1000 | Loss: 0.00002111
Iteration 108/1000 | Loss: 0.00002111
Iteration 109/1000 | Loss: 0.00002111
Iteration 110/1000 | Loss: 0.00002111
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002110
Iteration 113/1000 | Loss: 0.00002110
Iteration 114/1000 | Loss: 0.00002110
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00002110
Iteration 118/1000 | Loss: 0.00002110
Iteration 119/1000 | Loss: 0.00002110
Iteration 120/1000 | Loss: 0.00002110
Iteration 121/1000 | Loss: 0.00002110
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002110
Iteration 129/1000 | Loss: 0.00002110
Iteration 130/1000 | Loss: 0.00002110
Iteration 131/1000 | Loss: 0.00002110
Iteration 132/1000 | Loss: 0.00002110
Iteration 133/1000 | Loss: 0.00002110
Iteration 134/1000 | Loss: 0.00002110
Iteration 135/1000 | Loss: 0.00002110
Iteration 136/1000 | Loss: 0.00002110
Iteration 137/1000 | Loss: 0.00002110
Iteration 138/1000 | Loss: 0.00002110
Iteration 139/1000 | Loss: 0.00002110
Iteration 140/1000 | Loss: 0.00002110
Iteration 141/1000 | Loss: 0.00002110
Iteration 142/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.1099984223837964e-05, 2.1099984223837964e-05, 2.1099984223837964e-05, 2.1099984223837964e-05, 2.1099984223837964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1099984223837964e-05

Optimization complete. Final v2v error: 3.8753249645233154 mm

Highest mean error: 4.087298393249512 mm for frame 95

Lowest mean error: 3.4784750938415527 mm for frame 17

Saving results

Total time: 101.71062231063843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560024
Iteration 2/25 | Loss: 0.00115333
Iteration 3/25 | Loss: 0.00091224
Iteration 4/25 | Loss: 0.00087484
Iteration 5/25 | Loss: 0.00086869
Iteration 6/25 | Loss: 0.00086807
Iteration 7/25 | Loss: 0.00086807
Iteration 8/25 | Loss: 0.00086807
Iteration 9/25 | Loss: 0.00086807
Iteration 10/25 | Loss: 0.00086807
Iteration 11/25 | Loss: 0.00086807
Iteration 12/25 | Loss: 0.00086807
Iteration 13/25 | Loss: 0.00086807
Iteration 14/25 | Loss: 0.00086807
Iteration 15/25 | Loss: 0.00086807
Iteration 16/25 | Loss: 0.00086807
Iteration 17/25 | Loss: 0.00086807
Iteration 18/25 | Loss: 0.00086807
Iteration 19/25 | Loss: 0.00086807
Iteration 20/25 | Loss: 0.00086807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008680651080794632, 0.0008680651080794632, 0.0008680651080794632, 0.0008680651080794632, 0.0008680651080794632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008680651080794632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52550125
Iteration 2/25 | Loss: 0.00042674
Iteration 3/25 | Loss: 0.00042674
Iteration 4/25 | Loss: 0.00042674
Iteration 5/25 | Loss: 0.00042674
Iteration 6/25 | Loss: 0.00042674
Iteration 7/25 | Loss: 0.00042673
Iteration 8/25 | Loss: 0.00042673
Iteration 9/25 | Loss: 0.00042673
Iteration 10/25 | Loss: 0.00042673
Iteration 11/25 | Loss: 0.00042673
Iteration 12/25 | Loss: 0.00042673
Iteration 13/25 | Loss: 0.00042673
Iteration 14/25 | Loss: 0.00042673
Iteration 15/25 | Loss: 0.00042673
Iteration 16/25 | Loss: 0.00042673
Iteration 17/25 | Loss: 0.00042673
Iteration 18/25 | Loss: 0.00042673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004267338081263006, 0.0004267338081263006, 0.0004267338081263006, 0.0004267338081263006, 0.0004267338081263006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004267338081263006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042673
Iteration 2/1000 | Loss: 0.00003747
Iteration 3/1000 | Loss: 0.00003019
Iteration 4/1000 | Loss: 0.00002907
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002761
Iteration 7/1000 | Loss: 0.00002714
Iteration 8/1000 | Loss: 0.00002686
Iteration 9/1000 | Loss: 0.00002671
Iteration 10/1000 | Loss: 0.00002657
Iteration 11/1000 | Loss: 0.00002647
Iteration 12/1000 | Loss: 0.00002645
Iteration 13/1000 | Loss: 0.00002644
Iteration 14/1000 | Loss: 0.00002643
Iteration 15/1000 | Loss: 0.00002643
Iteration 16/1000 | Loss: 0.00002643
Iteration 17/1000 | Loss: 0.00002643
Iteration 18/1000 | Loss: 0.00002642
Iteration 19/1000 | Loss: 0.00002642
Iteration 20/1000 | Loss: 0.00002642
Iteration 21/1000 | Loss: 0.00002642
Iteration 22/1000 | Loss: 0.00002642
Iteration 23/1000 | Loss: 0.00002641
Iteration 24/1000 | Loss: 0.00002641
Iteration 25/1000 | Loss: 0.00002641
Iteration 26/1000 | Loss: 0.00002639
Iteration 27/1000 | Loss: 0.00002639
Iteration 28/1000 | Loss: 0.00002639
Iteration 29/1000 | Loss: 0.00002639
Iteration 30/1000 | Loss: 0.00002639
Iteration 31/1000 | Loss: 0.00002639
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002639
Iteration 34/1000 | Loss: 0.00002639
Iteration 35/1000 | Loss: 0.00002639
Iteration 36/1000 | Loss: 0.00002639
Iteration 37/1000 | Loss: 0.00002638
Iteration 38/1000 | Loss: 0.00002638
Iteration 39/1000 | Loss: 0.00002638
Iteration 40/1000 | Loss: 0.00002637
Iteration 41/1000 | Loss: 0.00002637
Iteration 42/1000 | Loss: 0.00002637
Iteration 43/1000 | Loss: 0.00002637
Iteration 44/1000 | Loss: 0.00002637
Iteration 45/1000 | Loss: 0.00002637
Iteration 46/1000 | Loss: 0.00002636
Iteration 47/1000 | Loss: 0.00002636
Iteration 48/1000 | Loss: 0.00002636
Iteration 49/1000 | Loss: 0.00002636
Iteration 50/1000 | Loss: 0.00002636
Iteration 51/1000 | Loss: 0.00002636
Iteration 52/1000 | Loss: 0.00002636
Iteration 53/1000 | Loss: 0.00002636
Iteration 54/1000 | Loss: 0.00002636
Iteration 55/1000 | Loss: 0.00002635
Iteration 56/1000 | Loss: 0.00002635
Iteration 57/1000 | Loss: 0.00002635
Iteration 58/1000 | Loss: 0.00002635
Iteration 59/1000 | Loss: 0.00002635
Iteration 60/1000 | Loss: 0.00002635
Iteration 61/1000 | Loss: 0.00002635
Iteration 62/1000 | Loss: 0.00002634
Iteration 63/1000 | Loss: 0.00002634
Iteration 64/1000 | Loss: 0.00002634
Iteration 65/1000 | Loss: 0.00002634
Iteration 66/1000 | Loss: 0.00002634
Iteration 67/1000 | Loss: 0.00002634
Iteration 68/1000 | Loss: 0.00002634
Iteration 69/1000 | Loss: 0.00002634
Iteration 70/1000 | Loss: 0.00002634
Iteration 71/1000 | Loss: 0.00002633
Iteration 72/1000 | Loss: 0.00002633
Iteration 73/1000 | Loss: 0.00002633
Iteration 74/1000 | Loss: 0.00002633
Iteration 75/1000 | Loss: 0.00002633
Iteration 76/1000 | Loss: 0.00002633
Iteration 77/1000 | Loss: 0.00002632
Iteration 78/1000 | Loss: 0.00002632
Iteration 79/1000 | Loss: 0.00002632
Iteration 80/1000 | Loss: 0.00002632
Iteration 81/1000 | Loss: 0.00002632
Iteration 82/1000 | Loss: 0.00002632
Iteration 83/1000 | Loss: 0.00002632
Iteration 84/1000 | Loss: 0.00002632
Iteration 85/1000 | Loss: 0.00002632
Iteration 86/1000 | Loss: 0.00002632
Iteration 87/1000 | Loss: 0.00002632
Iteration 88/1000 | Loss: 0.00002632
Iteration 89/1000 | Loss: 0.00002632
Iteration 90/1000 | Loss: 0.00002632
Iteration 91/1000 | Loss: 0.00002632
Iteration 92/1000 | Loss: 0.00002632
Iteration 93/1000 | Loss: 0.00002632
Iteration 94/1000 | Loss: 0.00002632
Iteration 95/1000 | Loss: 0.00002632
Iteration 96/1000 | Loss: 0.00002632
Iteration 97/1000 | Loss: 0.00002632
Iteration 98/1000 | Loss: 0.00002632
Iteration 99/1000 | Loss: 0.00002632
Iteration 100/1000 | Loss: 0.00002632
Iteration 101/1000 | Loss: 0.00002632
Iteration 102/1000 | Loss: 0.00002632
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002632
Iteration 105/1000 | Loss: 0.00002632
Iteration 106/1000 | Loss: 0.00002632
Iteration 107/1000 | Loss: 0.00002632
Iteration 108/1000 | Loss: 0.00002632
Iteration 109/1000 | Loss: 0.00002632
Iteration 110/1000 | Loss: 0.00002632
Iteration 111/1000 | Loss: 0.00002632
Iteration 112/1000 | Loss: 0.00002632
Iteration 113/1000 | Loss: 0.00002632
Iteration 114/1000 | Loss: 0.00002632
Iteration 115/1000 | Loss: 0.00002632
Iteration 116/1000 | Loss: 0.00002632
Iteration 117/1000 | Loss: 0.00002632
Iteration 118/1000 | Loss: 0.00002632
Iteration 119/1000 | Loss: 0.00002632
Iteration 120/1000 | Loss: 0.00002632
Iteration 121/1000 | Loss: 0.00002632
Iteration 122/1000 | Loss: 0.00002632
Iteration 123/1000 | Loss: 0.00002632
Iteration 124/1000 | Loss: 0.00002632
Iteration 125/1000 | Loss: 0.00002632
Iteration 126/1000 | Loss: 0.00002632
Iteration 127/1000 | Loss: 0.00002632
Iteration 128/1000 | Loss: 0.00002632
Iteration 129/1000 | Loss: 0.00002632
Iteration 130/1000 | Loss: 0.00002632
Iteration 131/1000 | Loss: 0.00002632
Iteration 132/1000 | Loss: 0.00002632
Iteration 133/1000 | Loss: 0.00002632
Iteration 134/1000 | Loss: 0.00002632
Iteration 135/1000 | Loss: 0.00002632
Iteration 136/1000 | Loss: 0.00002632
Iteration 137/1000 | Loss: 0.00002632
Iteration 138/1000 | Loss: 0.00002632
Iteration 139/1000 | Loss: 0.00002632
Iteration 140/1000 | Loss: 0.00002632
Iteration 141/1000 | Loss: 0.00002632
Iteration 142/1000 | Loss: 0.00002632
Iteration 143/1000 | Loss: 0.00002632
Iteration 144/1000 | Loss: 0.00002632
Iteration 145/1000 | Loss: 0.00002632
Iteration 146/1000 | Loss: 0.00002632
Iteration 147/1000 | Loss: 0.00002632
Iteration 148/1000 | Loss: 0.00002632
Iteration 149/1000 | Loss: 0.00002632
Iteration 150/1000 | Loss: 0.00002632
Iteration 151/1000 | Loss: 0.00002632
Iteration 152/1000 | Loss: 0.00002632
Iteration 153/1000 | Loss: 0.00002632
Iteration 154/1000 | Loss: 0.00002632
Iteration 155/1000 | Loss: 0.00002632
Iteration 156/1000 | Loss: 0.00002632
Iteration 157/1000 | Loss: 0.00002632
Iteration 158/1000 | Loss: 0.00002632
Iteration 159/1000 | Loss: 0.00002632
Iteration 160/1000 | Loss: 0.00002632
Iteration 161/1000 | Loss: 0.00002632
Iteration 162/1000 | Loss: 0.00002632
Iteration 163/1000 | Loss: 0.00002632
Iteration 164/1000 | Loss: 0.00002632
Iteration 165/1000 | Loss: 0.00002632
Iteration 166/1000 | Loss: 0.00002632
Iteration 167/1000 | Loss: 0.00002632
Iteration 168/1000 | Loss: 0.00002632
Iteration 169/1000 | Loss: 0.00002632
Iteration 170/1000 | Loss: 0.00002632
Iteration 171/1000 | Loss: 0.00002632
Iteration 172/1000 | Loss: 0.00002632
Iteration 173/1000 | Loss: 0.00002632
Iteration 174/1000 | Loss: 0.00002632
Iteration 175/1000 | Loss: 0.00002632
Iteration 176/1000 | Loss: 0.00002632
Iteration 177/1000 | Loss: 0.00002632
Iteration 178/1000 | Loss: 0.00002632
Iteration 179/1000 | Loss: 0.00002632
Iteration 180/1000 | Loss: 0.00002632
Iteration 181/1000 | Loss: 0.00002632
Iteration 182/1000 | Loss: 0.00002632
Iteration 183/1000 | Loss: 0.00002632
Iteration 184/1000 | Loss: 0.00002632
Iteration 185/1000 | Loss: 0.00002632
Iteration 186/1000 | Loss: 0.00002632
Iteration 187/1000 | Loss: 0.00002632
Iteration 188/1000 | Loss: 0.00002632
Iteration 189/1000 | Loss: 0.00002632
Iteration 190/1000 | Loss: 0.00002632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.6315548893762752e-05, 2.6315548893762752e-05, 2.6315548893762752e-05, 2.6315548893762752e-05, 2.6315548893762752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6315548893762752e-05

Optimization complete. Final v2v error: 4.3064398765563965 mm

Highest mean error: 5.051519393920898 mm for frame 88

Lowest mean error: 3.8155229091644287 mm for frame 141

Saving results

Total time: 35.27759885787964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455264
Iteration 2/25 | Loss: 0.00079810
Iteration 3/25 | Loss: 0.00068477
Iteration 4/25 | Loss: 0.00066675
Iteration 5/25 | Loss: 0.00066165
Iteration 6/25 | Loss: 0.00065975
Iteration 7/25 | Loss: 0.00065903
Iteration 8/25 | Loss: 0.00065891
Iteration 9/25 | Loss: 0.00065891
Iteration 10/25 | Loss: 0.00065891
Iteration 11/25 | Loss: 0.00065891
Iteration 12/25 | Loss: 0.00065891
Iteration 13/25 | Loss: 0.00065891
Iteration 14/25 | Loss: 0.00065891
Iteration 15/25 | Loss: 0.00065891
Iteration 16/25 | Loss: 0.00065891
Iteration 17/25 | Loss: 0.00065891
Iteration 18/25 | Loss: 0.00065891
Iteration 19/25 | Loss: 0.00065891
Iteration 20/25 | Loss: 0.00065891
Iteration 21/25 | Loss: 0.00065891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006589109543710947, 0.0006589109543710947, 0.0006589109543710947, 0.0006589109543710947, 0.0006589109543710947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006589109543710947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48798692
Iteration 2/25 | Loss: 0.00025287
Iteration 3/25 | Loss: 0.00025285
Iteration 4/25 | Loss: 0.00025285
Iteration 5/25 | Loss: 0.00025285
Iteration 6/25 | Loss: 0.00025285
Iteration 7/25 | Loss: 0.00025285
Iteration 8/25 | Loss: 0.00025285
Iteration 9/25 | Loss: 0.00025285
Iteration 10/25 | Loss: 0.00025285
Iteration 11/25 | Loss: 0.00025285
Iteration 12/25 | Loss: 0.00025285
Iteration 13/25 | Loss: 0.00025285
Iteration 14/25 | Loss: 0.00025285
Iteration 15/25 | Loss: 0.00025285
Iteration 16/25 | Loss: 0.00025285
Iteration 17/25 | Loss: 0.00025285
Iteration 18/25 | Loss: 0.00025285
Iteration 19/25 | Loss: 0.00025285
Iteration 20/25 | Loss: 0.00025285
Iteration 21/25 | Loss: 0.00025285
Iteration 22/25 | Loss: 0.00025285
Iteration 23/25 | Loss: 0.00025285
Iteration 24/25 | Loss: 0.00025285
Iteration 25/25 | Loss: 0.00025285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025285
Iteration 2/1000 | Loss: 0.00002660
Iteration 3/1000 | Loss: 0.00001536
Iteration 4/1000 | Loss: 0.00001306
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001184
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001155
Iteration 9/1000 | Loss: 0.00001144
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001137
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001131
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001127
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001123
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001122
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001118
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001117
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001115
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001115
Iteration 62/1000 | Loss: 0.00001115
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001111
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001111
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001111
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001111
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001108
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001107
Iteration 84/1000 | Loss: 0.00001107
Iteration 85/1000 | Loss: 0.00001107
Iteration 86/1000 | Loss: 0.00001106
Iteration 87/1000 | Loss: 0.00001106
Iteration 88/1000 | Loss: 0.00001106
Iteration 89/1000 | Loss: 0.00001105
Iteration 90/1000 | Loss: 0.00001105
Iteration 91/1000 | Loss: 0.00001105
Iteration 92/1000 | Loss: 0.00001104
Iteration 93/1000 | Loss: 0.00001104
Iteration 94/1000 | Loss: 0.00001104
Iteration 95/1000 | Loss: 0.00001104
Iteration 96/1000 | Loss: 0.00001104
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001103
Iteration 102/1000 | Loss: 0.00001103
Iteration 103/1000 | Loss: 0.00001103
Iteration 104/1000 | Loss: 0.00001102
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001102
Iteration 109/1000 | Loss: 0.00001102
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001101
Iteration 114/1000 | Loss: 0.00001101
Iteration 115/1000 | Loss: 0.00001101
Iteration 116/1000 | Loss: 0.00001101
Iteration 117/1000 | Loss: 0.00001101
Iteration 118/1000 | Loss: 0.00001101
Iteration 119/1000 | Loss: 0.00001101
Iteration 120/1000 | Loss: 0.00001101
Iteration 121/1000 | Loss: 0.00001101
Iteration 122/1000 | Loss: 0.00001101
Iteration 123/1000 | Loss: 0.00001101
Iteration 124/1000 | Loss: 0.00001101
Iteration 125/1000 | Loss: 0.00001101
Iteration 126/1000 | Loss: 0.00001100
Iteration 127/1000 | Loss: 0.00001100
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001100
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001099
Iteration 140/1000 | Loss: 0.00001099
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001099
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001099
Iteration 147/1000 | Loss: 0.00001099
Iteration 148/1000 | Loss: 0.00001099
Iteration 149/1000 | Loss: 0.00001099
Iteration 150/1000 | Loss: 0.00001099
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001098
Iteration 155/1000 | Loss: 0.00001098
Iteration 156/1000 | Loss: 0.00001098
Iteration 157/1000 | Loss: 0.00001098
Iteration 158/1000 | Loss: 0.00001098
Iteration 159/1000 | Loss: 0.00001098
Iteration 160/1000 | Loss: 0.00001098
Iteration 161/1000 | Loss: 0.00001098
Iteration 162/1000 | Loss: 0.00001098
Iteration 163/1000 | Loss: 0.00001098
Iteration 164/1000 | Loss: 0.00001098
Iteration 165/1000 | Loss: 0.00001098
Iteration 166/1000 | Loss: 0.00001097
Iteration 167/1000 | Loss: 0.00001097
Iteration 168/1000 | Loss: 0.00001097
Iteration 169/1000 | Loss: 0.00001097
Iteration 170/1000 | Loss: 0.00001097
Iteration 171/1000 | Loss: 0.00001097
Iteration 172/1000 | Loss: 0.00001097
Iteration 173/1000 | Loss: 0.00001097
Iteration 174/1000 | Loss: 0.00001097
Iteration 175/1000 | Loss: 0.00001097
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001096
Iteration 178/1000 | Loss: 0.00001096
Iteration 179/1000 | Loss: 0.00001096
Iteration 180/1000 | Loss: 0.00001096
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001096
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001096
Iteration 194/1000 | Loss: 0.00001096
Iteration 195/1000 | Loss: 0.00001096
Iteration 196/1000 | Loss: 0.00001096
Iteration 197/1000 | Loss: 0.00001096
Iteration 198/1000 | Loss: 0.00001096
Iteration 199/1000 | Loss: 0.00001096
Iteration 200/1000 | Loss: 0.00001096
Iteration 201/1000 | Loss: 0.00001096
Iteration 202/1000 | Loss: 0.00001096
Iteration 203/1000 | Loss: 0.00001096
Iteration 204/1000 | Loss: 0.00001096
Iteration 205/1000 | Loss: 0.00001096
Iteration 206/1000 | Loss: 0.00001096
Iteration 207/1000 | Loss: 0.00001096
Iteration 208/1000 | Loss: 0.00001096
Iteration 209/1000 | Loss: 0.00001096
Iteration 210/1000 | Loss: 0.00001096
Iteration 211/1000 | Loss: 0.00001096
Iteration 212/1000 | Loss: 0.00001096
Iteration 213/1000 | Loss: 0.00001096
Iteration 214/1000 | Loss: 0.00001096
Iteration 215/1000 | Loss: 0.00001096
Iteration 216/1000 | Loss: 0.00001096
Iteration 217/1000 | Loss: 0.00001096
Iteration 218/1000 | Loss: 0.00001096
Iteration 219/1000 | Loss: 0.00001096
Iteration 220/1000 | Loss: 0.00001096
Iteration 221/1000 | Loss: 0.00001096
Iteration 222/1000 | Loss: 0.00001096
Iteration 223/1000 | Loss: 0.00001096
Iteration 224/1000 | Loss: 0.00001096
Iteration 225/1000 | Loss: 0.00001096
Iteration 226/1000 | Loss: 0.00001096
Iteration 227/1000 | Loss: 0.00001096
Iteration 228/1000 | Loss: 0.00001096
Iteration 229/1000 | Loss: 0.00001096
Iteration 230/1000 | Loss: 0.00001096
Iteration 231/1000 | Loss: 0.00001096
Iteration 232/1000 | Loss: 0.00001096
Iteration 233/1000 | Loss: 0.00001096
Iteration 234/1000 | Loss: 0.00001096
Iteration 235/1000 | Loss: 0.00001096
Iteration 236/1000 | Loss: 0.00001096
Iteration 237/1000 | Loss: 0.00001096
Iteration 238/1000 | Loss: 0.00001096
Iteration 239/1000 | Loss: 0.00001096
Iteration 240/1000 | Loss: 0.00001096
Iteration 241/1000 | Loss: 0.00001096
Iteration 242/1000 | Loss: 0.00001096
Iteration 243/1000 | Loss: 0.00001096
Iteration 244/1000 | Loss: 0.00001096
Iteration 245/1000 | Loss: 0.00001096
Iteration 246/1000 | Loss: 0.00001096
Iteration 247/1000 | Loss: 0.00001096
Iteration 248/1000 | Loss: 0.00001096
Iteration 249/1000 | Loss: 0.00001096
Iteration 250/1000 | Loss: 0.00001096
Iteration 251/1000 | Loss: 0.00001096
Iteration 252/1000 | Loss: 0.00001096
Iteration 253/1000 | Loss: 0.00001096
Iteration 254/1000 | Loss: 0.00001096
Iteration 255/1000 | Loss: 0.00001096
Iteration 256/1000 | Loss: 0.00001096
Iteration 257/1000 | Loss: 0.00001096
Iteration 258/1000 | Loss: 0.00001096
Iteration 259/1000 | Loss: 0.00001096
Iteration 260/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.0958251550619025e-05, 1.0958251550619025e-05, 1.0958251550619025e-05, 1.0958251550619025e-05, 1.0958251550619025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0958251550619025e-05

Optimization complete. Final v2v error: 2.69048810005188 mm

Highest mean error: 3.306347608566284 mm for frame 61

Lowest mean error: 2.43017578125 mm for frame 20

Saving results

Total time: 37.29063701629639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012722
Iteration 2/25 | Loss: 0.00233910
Iteration 3/25 | Loss: 0.00183209
Iteration 4/25 | Loss: 0.00175673
Iteration 5/25 | Loss: 0.00172506
Iteration 6/25 | Loss: 0.00166181
Iteration 7/25 | Loss: 0.00160228
Iteration 8/25 | Loss: 0.00155864
Iteration 9/25 | Loss: 0.00151980
Iteration 10/25 | Loss: 0.00148759
Iteration 11/25 | Loss: 0.00149560
Iteration 12/25 | Loss: 0.00147364
Iteration 13/25 | Loss: 0.00148395
Iteration 14/25 | Loss: 0.00144448
Iteration 15/25 | Loss: 0.00143031
Iteration 16/25 | Loss: 0.00142700
Iteration 17/25 | Loss: 0.00143157
Iteration 18/25 | Loss: 0.00142515
Iteration 19/25 | Loss: 0.00142362
Iteration 20/25 | Loss: 0.00142311
Iteration 21/25 | Loss: 0.00142270
Iteration 22/25 | Loss: 0.00142219
Iteration 23/25 | Loss: 0.00142185
Iteration 24/25 | Loss: 0.00142151
Iteration 25/25 | Loss: 0.00142130

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49324358
Iteration 2/25 | Loss: 0.00409692
Iteration 3/25 | Loss: 0.00409692
Iteration 4/25 | Loss: 0.00409692
Iteration 5/25 | Loss: 0.00409692
Iteration 6/25 | Loss: 0.00409692
Iteration 7/25 | Loss: 0.00409692
Iteration 8/25 | Loss: 0.00409692
Iteration 9/25 | Loss: 0.00409692
Iteration 10/25 | Loss: 0.00409692
Iteration 11/25 | Loss: 0.00409692
Iteration 12/25 | Loss: 0.00409692
Iteration 13/25 | Loss: 0.00409692
Iteration 14/25 | Loss: 0.00409692
Iteration 15/25 | Loss: 0.00409692
Iteration 16/25 | Loss: 0.00409692
Iteration 17/25 | Loss: 0.00409692
Iteration 18/25 | Loss: 0.00409692
Iteration 19/25 | Loss: 0.00409692
Iteration 20/25 | Loss: 0.00409692
Iteration 21/25 | Loss: 0.00409692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0040969206020236015, 0.0040969206020236015, 0.0040969206020236015, 0.0040969206020236015, 0.0040969206020236015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0040969206020236015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00409692
Iteration 2/1000 | Loss: 0.00127895
Iteration 3/1000 | Loss: 0.00062070
Iteration 4/1000 | Loss: 0.00146124
Iteration 5/1000 | Loss: 0.00111666
Iteration 6/1000 | Loss: 0.00095792
Iteration 7/1000 | Loss: 0.00043987
Iteration 8/1000 | Loss: 0.00040177
Iteration 9/1000 | Loss: 0.00035673
Iteration 10/1000 | Loss: 0.00054019
Iteration 11/1000 | Loss: 0.00035806
Iteration 12/1000 | Loss: 0.00048524
Iteration 13/1000 | Loss: 0.00028926
Iteration 14/1000 | Loss: 0.00027552
Iteration 15/1000 | Loss: 0.00026240
Iteration 16/1000 | Loss: 0.00025562
Iteration 17/1000 | Loss: 0.00025061
Iteration 18/1000 | Loss: 0.00024457
Iteration 19/1000 | Loss: 0.00024135
Iteration 20/1000 | Loss: 0.00051236
Iteration 21/1000 | Loss: 0.01653415
Iteration 22/1000 | Loss: 0.00560661
Iteration 23/1000 | Loss: 0.00066059
Iteration 24/1000 | Loss: 0.00074788
Iteration 25/1000 | Loss: 0.00036540
Iteration 26/1000 | Loss: 0.00058297
Iteration 27/1000 | Loss: 0.00017930
Iteration 28/1000 | Loss: 0.00010169
Iteration 29/1000 | Loss: 0.00007344
Iteration 30/1000 | Loss: 0.00005598
Iteration 31/1000 | Loss: 0.00004666
Iteration 32/1000 | Loss: 0.00003854
Iteration 33/1000 | Loss: 0.00003251
Iteration 34/1000 | Loss: 0.00002855
Iteration 35/1000 | Loss: 0.00002628
Iteration 36/1000 | Loss: 0.00002481
Iteration 37/1000 | Loss: 0.00002320
Iteration 38/1000 | Loss: 0.00002176
Iteration 39/1000 | Loss: 0.00002095
Iteration 40/1000 | Loss: 0.00002048
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002007
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001979
Iteration 50/1000 | Loss: 0.00001978
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001977
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001975
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001972
Iteration 63/1000 | Loss: 0.00001972
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001971
Iteration 66/1000 | Loss: 0.00001971
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001970
Iteration 69/1000 | Loss: 0.00001970
Iteration 70/1000 | Loss: 0.00001970
Iteration 71/1000 | Loss: 0.00001970
Iteration 72/1000 | Loss: 0.00001970
Iteration 73/1000 | Loss: 0.00001969
Iteration 74/1000 | Loss: 0.00001969
Iteration 75/1000 | Loss: 0.00001969
Iteration 76/1000 | Loss: 0.00001969
Iteration 77/1000 | Loss: 0.00001969
Iteration 78/1000 | Loss: 0.00001969
Iteration 79/1000 | Loss: 0.00001969
Iteration 80/1000 | Loss: 0.00001969
Iteration 81/1000 | Loss: 0.00001969
Iteration 82/1000 | Loss: 0.00001969
Iteration 83/1000 | Loss: 0.00001968
Iteration 84/1000 | Loss: 0.00001968
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001967
Iteration 88/1000 | Loss: 0.00001967
Iteration 89/1000 | Loss: 0.00001967
Iteration 90/1000 | Loss: 0.00001967
Iteration 91/1000 | Loss: 0.00001967
Iteration 92/1000 | Loss: 0.00001967
Iteration 93/1000 | Loss: 0.00001967
Iteration 94/1000 | Loss: 0.00001967
Iteration 95/1000 | Loss: 0.00001967
Iteration 96/1000 | Loss: 0.00001966
Iteration 97/1000 | Loss: 0.00001966
Iteration 98/1000 | Loss: 0.00001966
Iteration 99/1000 | Loss: 0.00001966
Iteration 100/1000 | Loss: 0.00001966
Iteration 101/1000 | Loss: 0.00001966
Iteration 102/1000 | Loss: 0.00001966
Iteration 103/1000 | Loss: 0.00001966
Iteration 104/1000 | Loss: 0.00001966
Iteration 105/1000 | Loss: 0.00001966
Iteration 106/1000 | Loss: 0.00001966
Iteration 107/1000 | Loss: 0.00001965
Iteration 108/1000 | Loss: 0.00001965
Iteration 109/1000 | Loss: 0.00001965
Iteration 110/1000 | Loss: 0.00001965
Iteration 111/1000 | Loss: 0.00001965
Iteration 112/1000 | Loss: 0.00001965
Iteration 113/1000 | Loss: 0.00001965
Iteration 114/1000 | Loss: 0.00001965
Iteration 115/1000 | Loss: 0.00001964
Iteration 116/1000 | Loss: 0.00001964
Iteration 117/1000 | Loss: 0.00001964
Iteration 118/1000 | Loss: 0.00001964
Iteration 119/1000 | Loss: 0.00001964
Iteration 120/1000 | Loss: 0.00001964
Iteration 121/1000 | Loss: 0.00001964
Iteration 122/1000 | Loss: 0.00001964
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001963
Iteration 125/1000 | Loss: 0.00001963
Iteration 126/1000 | Loss: 0.00001963
Iteration 127/1000 | Loss: 0.00001963
Iteration 128/1000 | Loss: 0.00001963
Iteration 129/1000 | Loss: 0.00001963
Iteration 130/1000 | Loss: 0.00001963
Iteration 131/1000 | Loss: 0.00001963
Iteration 132/1000 | Loss: 0.00001963
Iteration 133/1000 | Loss: 0.00001963
Iteration 134/1000 | Loss: 0.00001963
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001963
Iteration 138/1000 | Loss: 0.00001963
Iteration 139/1000 | Loss: 0.00001963
Iteration 140/1000 | Loss: 0.00001963
Iteration 141/1000 | Loss: 0.00001963
Iteration 142/1000 | Loss: 0.00001963
Iteration 143/1000 | Loss: 0.00001962
Iteration 144/1000 | Loss: 0.00001962
Iteration 145/1000 | Loss: 0.00001962
Iteration 146/1000 | Loss: 0.00001962
Iteration 147/1000 | Loss: 0.00001962
Iteration 148/1000 | Loss: 0.00001962
Iteration 149/1000 | Loss: 0.00001962
Iteration 150/1000 | Loss: 0.00001962
Iteration 151/1000 | Loss: 0.00001962
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001962
Iteration 154/1000 | Loss: 0.00001962
Iteration 155/1000 | Loss: 0.00001962
Iteration 156/1000 | Loss: 0.00001962
Iteration 157/1000 | Loss: 0.00001962
Iteration 158/1000 | Loss: 0.00001962
Iteration 159/1000 | Loss: 0.00001962
Iteration 160/1000 | Loss: 0.00001962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.9618028090917505e-05, 1.9618028090917505e-05, 1.9618028090917505e-05, 1.9618028090917505e-05, 1.9618028090917505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9618028090917505e-05

Optimization complete. Final v2v error: 3.420149564743042 mm

Highest mean error: 12.364823341369629 mm for frame 127

Lowest mean error: 3.3035717010498047 mm for frame 143

Saving results

Total time: 112.6843831539154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943094
Iteration 2/25 | Loss: 0.00238555
Iteration 3/25 | Loss: 0.00155654
Iteration 4/25 | Loss: 0.00141729
Iteration 5/25 | Loss: 0.00122453
Iteration 6/25 | Loss: 0.00112401
Iteration 7/25 | Loss: 0.00101191
Iteration 8/25 | Loss: 0.00099016
Iteration 9/25 | Loss: 0.00096677
Iteration 10/25 | Loss: 0.00094069
Iteration 11/25 | Loss: 0.00093104
Iteration 12/25 | Loss: 0.00092785
Iteration 13/25 | Loss: 0.00091762
Iteration 14/25 | Loss: 0.00090435
Iteration 15/25 | Loss: 0.00091280
Iteration 16/25 | Loss: 0.00086342
Iteration 17/25 | Loss: 0.00083677
Iteration 18/25 | Loss: 0.00083765
Iteration 19/25 | Loss: 0.00080079
Iteration 20/25 | Loss: 0.00079238
Iteration 21/25 | Loss: 0.00079047
Iteration 22/25 | Loss: 0.00079000
Iteration 23/25 | Loss: 0.00078977
Iteration 24/25 | Loss: 0.00078967
Iteration 25/25 | Loss: 0.00078967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43235862
Iteration 2/25 | Loss: 0.00037567
Iteration 3/25 | Loss: 0.00032564
Iteration 4/25 | Loss: 0.00032563
Iteration 5/25 | Loss: 0.00032563
Iteration 6/25 | Loss: 0.00032563
Iteration 7/25 | Loss: 0.00032563
Iteration 8/25 | Loss: 0.00032563
Iteration 9/25 | Loss: 0.00032563
Iteration 10/25 | Loss: 0.00032563
Iteration 11/25 | Loss: 0.00032563
Iteration 12/25 | Loss: 0.00032563
Iteration 13/25 | Loss: 0.00032563
Iteration 14/25 | Loss: 0.00032563
Iteration 15/25 | Loss: 0.00032563
Iteration 16/25 | Loss: 0.00032563
Iteration 17/25 | Loss: 0.00032563
Iteration 18/25 | Loss: 0.00032563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00032563155400566757, 0.00032563155400566757, 0.00032563155400566757, 0.00032563155400566757, 0.00032563155400566757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032563155400566757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032563
Iteration 2/1000 | Loss: 0.00013340
Iteration 3/1000 | Loss: 0.00003722
Iteration 4/1000 | Loss: 0.00002902
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002437
Iteration 7/1000 | Loss: 0.00002336
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002247
Iteration 10/1000 | Loss: 0.00002211
Iteration 11/1000 | Loss: 0.00002184
Iteration 12/1000 | Loss: 0.00002178
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002159
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00002145
Iteration 18/1000 | Loss: 0.00002134
Iteration 19/1000 | Loss: 0.00002134
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002124
Iteration 22/1000 | Loss: 0.00002124
Iteration 23/1000 | Loss: 0.00002123
Iteration 24/1000 | Loss: 0.00002123
Iteration 25/1000 | Loss: 0.00002119
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002115
Iteration 28/1000 | Loss: 0.00002114
Iteration 29/1000 | Loss: 0.00002114
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002112
Iteration 34/1000 | Loss: 0.00002112
Iteration 35/1000 | Loss: 0.00002111
Iteration 36/1000 | Loss: 0.00002111
Iteration 37/1000 | Loss: 0.00002110
Iteration 38/1000 | Loss: 0.00002110
Iteration 39/1000 | Loss: 0.00002109
Iteration 40/1000 | Loss: 0.00002109
Iteration 41/1000 | Loss: 0.00002108
Iteration 42/1000 | Loss: 0.00002107
Iteration 43/1000 | Loss: 0.00002107
Iteration 44/1000 | Loss: 0.00002106
Iteration 45/1000 | Loss: 0.00002106
Iteration 46/1000 | Loss: 0.00002105
Iteration 47/1000 | Loss: 0.00002105
Iteration 48/1000 | Loss: 0.00002105
Iteration 49/1000 | Loss: 0.00002105
Iteration 50/1000 | Loss: 0.00002105
Iteration 51/1000 | Loss: 0.00002104
Iteration 52/1000 | Loss: 0.00002104
Iteration 53/1000 | Loss: 0.00002104
Iteration 54/1000 | Loss: 0.00002103
Iteration 55/1000 | Loss: 0.00002103
Iteration 56/1000 | Loss: 0.00002102
Iteration 57/1000 | Loss: 0.00002102
Iteration 58/1000 | Loss: 0.00002102
Iteration 59/1000 | Loss: 0.00002101
Iteration 60/1000 | Loss: 0.00002101
Iteration 61/1000 | Loss: 0.00002101
Iteration 62/1000 | Loss: 0.00002101
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002100
Iteration 65/1000 | Loss: 0.00002100
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002099
Iteration 69/1000 | Loss: 0.00002099
Iteration 70/1000 | Loss: 0.00002099
Iteration 71/1000 | Loss: 0.00002099
Iteration 72/1000 | Loss: 0.00002098
Iteration 73/1000 | Loss: 0.00002098
Iteration 74/1000 | Loss: 0.00002098
Iteration 75/1000 | Loss: 0.00002098
Iteration 76/1000 | Loss: 0.00002098
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002097
Iteration 79/1000 | Loss: 0.00002097
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002095
Iteration 83/1000 | Loss: 0.00002095
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002094
Iteration 86/1000 | Loss: 0.00002094
Iteration 87/1000 | Loss: 0.00002094
Iteration 88/1000 | Loss: 0.00002093
Iteration 89/1000 | Loss: 0.00002093
Iteration 90/1000 | Loss: 0.00002093
Iteration 91/1000 | Loss: 0.00002093
Iteration 92/1000 | Loss: 0.00002092
Iteration 93/1000 | Loss: 0.00002092
Iteration 94/1000 | Loss: 0.00002092
Iteration 95/1000 | Loss: 0.00002091
Iteration 96/1000 | Loss: 0.00002091
Iteration 97/1000 | Loss: 0.00002091
Iteration 98/1000 | Loss: 0.00002091
Iteration 99/1000 | Loss: 0.00002091
Iteration 100/1000 | Loss: 0.00002091
Iteration 101/1000 | Loss: 0.00002091
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002091
Iteration 104/1000 | Loss: 0.00002091
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002090
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00002090
Iteration 112/1000 | Loss: 0.00002090
Iteration 113/1000 | Loss: 0.00002090
Iteration 114/1000 | Loss: 0.00002089
Iteration 115/1000 | Loss: 0.00002089
Iteration 116/1000 | Loss: 0.00002089
Iteration 117/1000 | Loss: 0.00002089
Iteration 118/1000 | Loss: 0.00002089
Iteration 119/1000 | Loss: 0.00002089
Iteration 120/1000 | Loss: 0.00002089
Iteration 121/1000 | Loss: 0.00002089
Iteration 122/1000 | Loss: 0.00002089
Iteration 123/1000 | Loss: 0.00002089
Iteration 124/1000 | Loss: 0.00002089
Iteration 125/1000 | Loss: 0.00002089
Iteration 126/1000 | Loss: 0.00002089
Iteration 127/1000 | Loss: 0.00002089
Iteration 128/1000 | Loss: 0.00002089
Iteration 129/1000 | Loss: 0.00002089
Iteration 130/1000 | Loss: 0.00002089
Iteration 131/1000 | Loss: 0.00002089
Iteration 132/1000 | Loss: 0.00002089
Iteration 133/1000 | Loss: 0.00002089
Iteration 134/1000 | Loss: 0.00002088
Iteration 135/1000 | Loss: 0.00002088
Iteration 136/1000 | Loss: 0.00002088
Iteration 137/1000 | Loss: 0.00002088
Iteration 138/1000 | Loss: 0.00002088
Iteration 139/1000 | Loss: 0.00002088
Iteration 140/1000 | Loss: 0.00002088
Iteration 141/1000 | Loss: 0.00002088
Iteration 142/1000 | Loss: 0.00002088
Iteration 143/1000 | Loss: 0.00002088
Iteration 144/1000 | Loss: 0.00002088
Iteration 145/1000 | Loss: 0.00002088
Iteration 146/1000 | Loss: 0.00002087
Iteration 147/1000 | Loss: 0.00002087
Iteration 148/1000 | Loss: 0.00002087
Iteration 149/1000 | Loss: 0.00002087
Iteration 150/1000 | Loss: 0.00002087
Iteration 151/1000 | Loss: 0.00002087
Iteration 152/1000 | Loss: 0.00002087
Iteration 153/1000 | Loss: 0.00002087
Iteration 154/1000 | Loss: 0.00002087
Iteration 155/1000 | Loss: 0.00002087
Iteration 156/1000 | Loss: 0.00002087
Iteration 157/1000 | Loss: 0.00002087
Iteration 158/1000 | Loss: 0.00002087
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002087
Iteration 161/1000 | Loss: 0.00002086
Iteration 162/1000 | Loss: 0.00002086
Iteration 163/1000 | Loss: 0.00002086
Iteration 164/1000 | Loss: 0.00002086
Iteration 165/1000 | Loss: 0.00002086
Iteration 166/1000 | Loss: 0.00002086
Iteration 167/1000 | Loss: 0.00002086
Iteration 168/1000 | Loss: 0.00002086
Iteration 169/1000 | Loss: 0.00002086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.0864210455329157e-05, 2.0864210455329157e-05, 2.0864210455329157e-05, 2.0864210455329157e-05, 2.0864210455329157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0864210455329157e-05

Optimization complete. Final v2v error: 3.8960351943969727 mm

Highest mean error: 4.135966777801514 mm for frame 203

Lowest mean error: 3.7128477096557617 mm for frame 72

Saving results

Total time: 84.26595497131348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711655
Iteration 2/25 | Loss: 0.00099183
Iteration 3/25 | Loss: 0.00076062
Iteration 4/25 | Loss: 0.00069429
Iteration 5/25 | Loss: 0.00067671
Iteration 6/25 | Loss: 0.00067502
Iteration 7/25 | Loss: 0.00067448
Iteration 8/25 | Loss: 0.00067448
Iteration 9/25 | Loss: 0.00067448
Iteration 10/25 | Loss: 0.00067448
Iteration 11/25 | Loss: 0.00067448
Iteration 12/25 | Loss: 0.00067448
Iteration 13/25 | Loss: 0.00067448
Iteration 14/25 | Loss: 0.00067448
Iteration 15/25 | Loss: 0.00067448
Iteration 16/25 | Loss: 0.00067448
Iteration 17/25 | Loss: 0.00067448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006744751008227468, 0.0006744751008227468, 0.0006744751008227468, 0.0006744751008227468, 0.0006744751008227468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006744751008227468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.17015266
Iteration 2/25 | Loss: 0.00034861
Iteration 3/25 | Loss: 0.00034861
Iteration 4/25 | Loss: 0.00034861
Iteration 5/25 | Loss: 0.00034861
Iteration 6/25 | Loss: 0.00034861
Iteration 7/25 | Loss: 0.00034860
Iteration 8/25 | Loss: 0.00034860
Iteration 9/25 | Loss: 0.00034860
Iteration 10/25 | Loss: 0.00034860
Iteration 11/25 | Loss: 0.00034860
Iteration 12/25 | Loss: 0.00034860
Iteration 13/25 | Loss: 0.00034860
Iteration 14/25 | Loss: 0.00034860
Iteration 15/25 | Loss: 0.00034860
Iteration 16/25 | Loss: 0.00034860
Iteration 17/25 | Loss: 0.00034860
Iteration 18/25 | Loss: 0.00034860
Iteration 19/25 | Loss: 0.00034860
Iteration 20/25 | Loss: 0.00034860
Iteration 21/25 | Loss: 0.00034860
Iteration 22/25 | Loss: 0.00034860
Iteration 23/25 | Loss: 0.00034860
Iteration 24/25 | Loss: 0.00034860
Iteration 25/25 | Loss: 0.00034860

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034860
Iteration 2/1000 | Loss: 0.00003239
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00001978
Iteration 5/1000 | Loss: 0.00001841
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001548
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001537
Iteration 17/1000 | Loss: 0.00001536
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001531
Iteration 24/1000 | Loss: 0.00001531
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001529
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001523
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001512
Iteration 104/1000 | Loss: 0.00001512
Iteration 105/1000 | Loss: 0.00001512
Iteration 106/1000 | Loss: 0.00001512
Iteration 107/1000 | Loss: 0.00001512
Iteration 108/1000 | Loss: 0.00001512
Iteration 109/1000 | Loss: 0.00001512
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Iteration 120/1000 | Loss: 0.00001512
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001512
Iteration 123/1000 | Loss: 0.00001512
Iteration 124/1000 | Loss: 0.00001512
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001512
Iteration 128/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5119271665753331e-05, 1.5119271665753331e-05, 1.5119271665753331e-05, 1.5119271665753331e-05, 1.5119271665753331e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5119271665753331e-05

Optimization complete. Final v2v error: 3.3354506492614746 mm

Highest mean error: 3.786944627761841 mm for frame 23

Lowest mean error: 3.116832971572876 mm for frame 111

Saving results

Total time: 41.95018649101257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922771
Iteration 2/25 | Loss: 0.00100031
Iteration 3/25 | Loss: 0.00080616
Iteration 4/25 | Loss: 0.00076417
Iteration 5/25 | Loss: 0.00074622
Iteration 6/25 | Loss: 0.00074230
Iteration 7/25 | Loss: 0.00074106
Iteration 8/25 | Loss: 0.00074086
Iteration 9/25 | Loss: 0.00074086
Iteration 10/25 | Loss: 0.00074086
Iteration 11/25 | Loss: 0.00074086
Iteration 12/25 | Loss: 0.00074086
Iteration 13/25 | Loss: 0.00074086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007408623350784183, 0.0007408623350784183, 0.0007408623350784183, 0.0007408623350784183, 0.0007408623350784183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007408623350784183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47591865
Iteration 2/25 | Loss: 0.00034165
Iteration 3/25 | Loss: 0.00034164
Iteration 4/25 | Loss: 0.00034164
Iteration 5/25 | Loss: 0.00034164
Iteration 6/25 | Loss: 0.00034164
Iteration 7/25 | Loss: 0.00034164
Iteration 8/25 | Loss: 0.00034164
Iteration 9/25 | Loss: 0.00034164
Iteration 10/25 | Loss: 0.00034164
Iteration 11/25 | Loss: 0.00034164
Iteration 12/25 | Loss: 0.00034164
Iteration 13/25 | Loss: 0.00034164
Iteration 14/25 | Loss: 0.00034164
Iteration 15/25 | Loss: 0.00034164
Iteration 16/25 | Loss: 0.00034164
Iteration 17/25 | Loss: 0.00034164
Iteration 18/25 | Loss: 0.00034164
Iteration 19/25 | Loss: 0.00034164
Iteration 20/25 | Loss: 0.00034164
Iteration 21/25 | Loss: 0.00034164
Iteration 22/25 | Loss: 0.00034164
Iteration 23/25 | Loss: 0.00034164
Iteration 24/25 | Loss: 0.00034164
Iteration 25/25 | Loss: 0.00034164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034164
Iteration 2/1000 | Loss: 0.00004536
Iteration 3/1000 | Loss: 0.00003298
Iteration 4/1000 | Loss: 0.00002920
Iteration 5/1000 | Loss: 0.00002794
Iteration 6/1000 | Loss: 0.00002664
Iteration 7/1000 | Loss: 0.00002580
Iteration 8/1000 | Loss: 0.00002503
Iteration 9/1000 | Loss: 0.00002462
Iteration 10/1000 | Loss: 0.00002433
Iteration 11/1000 | Loss: 0.00002412
Iteration 12/1000 | Loss: 0.00002406
Iteration 13/1000 | Loss: 0.00002393
Iteration 14/1000 | Loss: 0.00002385
Iteration 15/1000 | Loss: 0.00002384
Iteration 16/1000 | Loss: 0.00002383
Iteration 17/1000 | Loss: 0.00002376
Iteration 18/1000 | Loss: 0.00002370
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002366
Iteration 21/1000 | Loss: 0.00002365
Iteration 22/1000 | Loss: 0.00002364
Iteration 23/1000 | Loss: 0.00002364
Iteration 24/1000 | Loss: 0.00002363
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002362
Iteration 29/1000 | Loss: 0.00002361
Iteration 30/1000 | Loss: 0.00002361
Iteration 31/1000 | Loss: 0.00002361
Iteration 32/1000 | Loss: 0.00002361
Iteration 33/1000 | Loss: 0.00002361
Iteration 34/1000 | Loss: 0.00002361
Iteration 35/1000 | Loss: 0.00002360
Iteration 36/1000 | Loss: 0.00002360
Iteration 37/1000 | Loss: 0.00002360
Iteration 38/1000 | Loss: 0.00002360
Iteration 39/1000 | Loss: 0.00002360
Iteration 40/1000 | Loss: 0.00002360
Iteration 41/1000 | Loss: 0.00002360
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002359
Iteration 44/1000 | Loss: 0.00002359
Iteration 45/1000 | Loss: 0.00002359
Iteration 46/1000 | Loss: 0.00002359
Iteration 47/1000 | Loss: 0.00002358
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002357
Iteration 50/1000 | Loss: 0.00002357
Iteration 51/1000 | Loss: 0.00002356
Iteration 52/1000 | Loss: 0.00002356
Iteration 53/1000 | Loss: 0.00002356
Iteration 54/1000 | Loss: 0.00002355
Iteration 55/1000 | Loss: 0.00002355
Iteration 56/1000 | Loss: 0.00002355
Iteration 57/1000 | Loss: 0.00002355
Iteration 58/1000 | Loss: 0.00002354
Iteration 59/1000 | Loss: 0.00002354
Iteration 60/1000 | Loss: 0.00002354
Iteration 61/1000 | Loss: 0.00002353
Iteration 62/1000 | Loss: 0.00002353
Iteration 63/1000 | Loss: 0.00002353
Iteration 64/1000 | Loss: 0.00002353
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002350
Iteration 71/1000 | Loss: 0.00002350
Iteration 72/1000 | Loss: 0.00002349
Iteration 73/1000 | Loss: 0.00002349
Iteration 74/1000 | Loss: 0.00002349
Iteration 75/1000 | Loss: 0.00002349
Iteration 76/1000 | Loss: 0.00002349
Iteration 77/1000 | Loss: 0.00002349
Iteration 78/1000 | Loss: 0.00002349
Iteration 79/1000 | Loss: 0.00002349
Iteration 80/1000 | Loss: 0.00002348
Iteration 81/1000 | Loss: 0.00002348
Iteration 82/1000 | Loss: 0.00002348
Iteration 83/1000 | Loss: 0.00002348
Iteration 84/1000 | Loss: 0.00002347
Iteration 85/1000 | Loss: 0.00002346
Iteration 86/1000 | Loss: 0.00002346
Iteration 87/1000 | Loss: 0.00002345
Iteration 88/1000 | Loss: 0.00002345
Iteration 89/1000 | Loss: 0.00002345
Iteration 90/1000 | Loss: 0.00002345
Iteration 91/1000 | Loss: 0.00002345
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002344
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002343
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00002343
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002342
Iteration 114/1000 | Loss: 0.00002342
Iteration 115/1000 | Loss: 0.00002342
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002341
Iteration 118/1000 | Loss: 0.00002341
Iteration 119/1000 | Loss: 0.00002341
Iteration 120/1000 | Loss: 0.00002341
Iteration 121/1000 | Loss: 0.00002341
Iteration 122/1000 | Loss: 0.00002341
Iteration 123/1000 | Loss: 0.00002341
Iteration 124/1000 | Loss: 0.00002341
Iteration 125/1000 | Loss: 0.00002340
Iteration 126/1000 | Loss: 0.00002340
Iteration 127/1000 | Loss: 0.00002340
Iteration 128/1000 | Loss: 0.00002340
Iteration 129/1000 | Loss: 0.00002340
Iteration 130/1000 | Loss: 0.00002339
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00002339
Iteration 133/1000 | Loss: 0.00002339
Iteration 134/1000 | Loss: 0.00002339
Iteration 135/1000 | Loss: 0.00002339
Iteration 136/1000 | Loss: 0.00002339
Iteration 137/1000 | Loss: 0.00002339
Iteration 138/1000 | Loss: 0.00002339
Iteration 139/1000 | Loss: 0.00002338
Iteration 140/1000 | Loss: 0.00002338
Iteration 141/1000 | Loss: 0.00002338
Iteration 142/1000 | Loss: 0.00002338
Iteration 143/1000 | Loss: 0.00002338
Iteration 144/1000 | Loss: 0.00002338
Iteration 145/1000 | Loss: 0.00002338
Iteration 146/1000 | Loss: 0.00002338
Iteration 147/1000 | Loss: 0.00002338
Iteration 148/1000 | Loss: 0.00002338
Iteration 149/1000 | Loss: 0.00002338
Iteration 150/1000 | Loss: 0.00002338
Iteration 151/1000 | Loss: 0.00002337
Iteration 152/1000 | Loss: 0.00002337
Iteration 153/1000 | Loss: 0.00002337
Iteration 154/1000 | Loss: 0.00002337
Iteration 155/1000 | Loss: 0.00002337
Iteration 156/1000 | Loss: 0.00002337
Iteration 157/1000 | Loss: 0.00002337
Iteration 158/1000 | Loss: 0.00002337
Iteration 159/1000 | Loss: 0.00002337
Iteration 160/1000 | Loss: 0.00002336
Iteration 161/1000 | Loss: 0.00002336
Iteration 162/1000 | Loss: 0.00002336
Iteration 163/1000 | Loss: 0.00002336
Iteration 164/1000 | Loss: 0.00002336
Iteration 165/1000 | Loss: 0.00002336
Iteration 166/1000 | Loss: 0.00002336
Iteration 167/1000 | Loss: 0.00002336
Iteration 168/1000 | Loss: 0.00002336
Iteration 169/1000 | Loss: 0.00002335
Iteration 170/1000 | Loss: 0.00002335
Iteration 171/1000 | Loss: 0.00002335
Iteration 172/1000 | Loss: 0.00002335
Iteration 173/1000 | Loss: 0.00002335
Iteration 174/1000 | Loss: 0.00002335
Iteration 175/1000 | Loss: 0.00002335
Iteration 176/1000 | Loss: 0.00002335
Iteration 177/1000 | Loss: 0.00002335
Iteration 178/1000 | Loss: 0.00002335
Iteration 179/1000 | Loss: 0.00002334
Iteration 180/1000 | Loss: 0.00002334
Iteration 181/1000 | Loss: 0.00002334
Iteration 182/1000 | Loss: 0.00002334
Iteration 183/1000 | Loss: 0.00002334
Iteration 184/1000 | Loss: 0.00002334
Iteration 185/1000 | Loss: 0.00002334
Iteration 186/1000 | Loss: 0.00002334
Iteration 187/1000 | Loss: 0.00002334
Iteration 188/1000 | Loss: 0.00002334
Iteration 189/1000 | Loss: 0.00002334
Iteration 190/1000 | Loss: 0.00002334
Iteration 191/1000 | Loss: 0.00002334
Iteration 192/1000 | Loss: 0.00002334
Iteration 193/1000 | Loss: 0.00002333
Iteration 194/1000 | Loss: 0.00002333
Iteration 195/1000 | Loss: 0.00002333
Iteration 196/1000 | Loss: 0.00002333
Iteration 197/1000 | Loss: 0.00002333
Iteration 198/1000 | Loss: 0.00002333
Iteration 199/1000 | Loss: 0.00002333
Iteration 200/1000 | Loss: 0.00002332
Iteration 201/1000 | Loss: 0.00002332
Iteration 202/1000 | Loss: 0.00002332
Iteration 203/1000 | Loss: 0.00002332
Iteration 204/1000 | Loss: 0.00002332
Iteration 205/1000 | Loss: 0.00002332
Iteration 206/1000 | Loss: 0.00002332
Iteration 207/1000 | Loss: 0.00002332
Iteration 208/1000 | Loss: 0.00002332
Iteration 209/1000 | Loss: 0.00002332
Iteration 210/1000 | Loss: 0.00002332
Iteration 211/1000 | Loss: 0.00002332
Iteration 212/1000 | Loss: 0.00002332
Iteration 213/1000 | Loss: 0.00002332
Iteration 214/1000 | Loss: 0.00002332
Iteration 215/1000 | Loss: 0.00002332
Iteration 216/1000 | Loss: 0.00002332
Iteration 217/1000 | Loss: 0.00002332
Iteration 218/1000 | Loss: 0.00002332
Iteration 219/1000 | Loss: 0.00002332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.3316255465033464e-05, 2.3316255465033464e-05, 2.3316255465033464e-05, 2.3316255465033464e-05, 2.3316255465033464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3316255465033464e-05

Optimization complete. Final v2v error: 4.043560028076172 mm

Highest mean error: 5.775105953216553 mm for frame 70

Lowest mean error: 3.6002821922302246 mm for frame 95

Saving results

Total time: 44.358412742614746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996816
Iteration 2/25 | Loss: 0.00209051
Iteration 3/25 | Loss: 0.00115356
Iteration 4/25 | Loss: 0.00094589
Iteration 5/25 | Loss: 0.00087600
Iteration 6/25 | Loss: 0.00087006
Iteration 7/25 | Loss: 0.00081395
Iteration 8/25 | Loss: 0.00077171
Iteration 9/25 | Loss: 0.00077292
Iteration 10/25 | Loss: 0.00075529
Iteration 11/25 | Loss: 0.00074280
Iteration 12/25 | Loss: 0.00073690
Iteration 13/25 | Loss: 0.00072347
Iteration 14/25 | Loss: 0.00071751
Iteration 15/25 | Loss: 0.00071324
Iteration 16/25 | Loss: 0.00071475
Iteration 17/25 | Loss: 0.00071908
Iteration 18/25 | Loss: 0.00071625
Iteration 19/25 | Loss: 0.00071429
Iteration 20/25 | Loss: 0.00071450
Iteration 21/25 | Loss: 0.00071382
Iteration 22/25 | Loss: 0.00071126
Iteration 23/25 | Loss: 0.00071219
Iteration 24/25 | Loss: 0.00071602
Iteration 25/25 | Loss: 0.00070995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45470214
Iteration 2/25 | Loss: 0.00037310
Iteration 3/25 | Loss: 0.00037310
Iteration 4/25 | Loss: 0.00037310
Iteration 5/25 | Loss: 0.00037310
Iteration 6/25 | Loss: 0.00037310
Iteration 7/25 | Loss: 0.00037310
Iteration 8/25 | Loss: 0.00037310
Iteration 9/25 | Loss: 0.00037310
Iteration 10/25 | Loss: 0.00037310
Iteration 11/25 | Loss: 0.00037310
Iteration 12/25 | Loss: 0.00037310
Iteration 13/25 | Loss: 0.00037310
Iteration 14/25 | Loss: 0.00037310
Iteration 15/25 | Loss: 0.00037310
Iteration 16/25 | Loss: 0.00037310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00037309981416910887, 0.00037309981416910887, 0.00037309981416910887, 0.00037309981416910887, 0.00037309981416910887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037309981416910887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037310
Iteration 2/1000 | Loss: 0.00002744
Iteration 3/1000 | Loss: 0.00002071
Iteration 4/1000 | Loss: 0.00001870
Iteration 5/1000 | Loss: 0.00001740
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00003754
Iteration 9/1000 | Loss: 0.00001737
Iteration 10/1000 | Loss: 0.00013445
Iteration 11/1000 | Loss: 0.00007118
Iteration 12/1000 | Loss: 0.00013624
Iteration 13/1000 | Loss: 0.00007507
Iteration 14/1000 | Loss: 0.00011491
Iteration 15/1000 | Loss: 0.00007244
Iteration 16/1000 | Loss: 0.00011404
Iteration 17/1000 | Loss: 0.00007948
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001398
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001310
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001307
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001299
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001297
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001294
Iteration 91/1000 | Loss: 0.00001294
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001293
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001290
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001290
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001289
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001289
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001289
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001289
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001289
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001289
Iteration 175/1000 | Loss: 0.00001289
Iteration 176/1000 | Loss: 0.00001289
Iteration 177/1000 | Loss: 0.00001289
Iteration 178/1000 | Loss: 0.00001289
Iteration 179/1000 | Loss: 0.00001289
Iteration 180/1000 | Loss: 0.00001289
Iteration 181/1000 | Loss: 0.00001289
Iteration 182/1000 | Loss: 0.00001289
Iteration 183/1000 | Loss: 0.00001289
Iteration 184/1000 | Loss: 0.00001289
Iteration 185/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2886159311165102e-05, 1.2886159311165102e-05, 1.2886159311165102e-05, 1.2886159311165102e-05, 1.2886159311165102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2886159311165102e-05

Optimization complete. Final v2v error: 3.035992383956909 mm

Highest mean error: 3.9133880138397217 mm for frame 139

Lowest mean error: 2.5090253353118896 mm for frame 182

Saving results

Total time: 110.32402968406677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415620
Iteration 2/25 | Loss: 0.00085259
Iteration 3/25 | Loss: 0.00074551
Iteration 4/25 | Loss: 0.00070957
Iteration 5/25 | Loss: 0.00069768
Iteration 6/25 | Loss: 0.00069599
Iteration 7/25 | Loss: 0.00069532
Iteration 8/25 | Loss: 0.00069522
Iteration 9/25 | Loss: 0.00069522
Iteration 10/25 | Loss: 0.00069522
Iteration 11/25 | Loss: 0.00069522
Iteration 12/25 | Loss: 0.00069522
Iteration 13/25 | Loss: 0.00069522
Iteration 14/25 | Loss: 0.00069522
Iteration 15/25 | Loss: 0.00069522
Iteration 16/25 | Loss: 0.00069522
Iteration 17/25 | Loss: 0.00069522
Iteration 18/25 | Loss: 0.00069522
Iteration 19/25 | Loss: 0.00069522
Iteration 20/25 | Loss: 0.00069522
Iteration 21/25 | Loss: 0.00069522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006952178082428873, 0.0006952178082428873, 0.0006952178082428873, 0.0006952178082428873, 0.0006952178082428873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006952178082428873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90927851
Iteration 2/25 | Loss: 0.00031692
Iteration 3/25 | Loss: 0.00031692
Iteration 4/25 | Loss: 0.00031692
Iteration 5/25 | Loss: 0.00031692
Iteration 6/25 | Loss: 0.00031692
Iteration 7/25 | Loss: 0.00031692
Iteration 8/25 | Loss: 0.00031692
Iteration 9/25 | Loss: 0.00031692
Iteration 10/25 | Loss: 0.00031692
Iteration 11/25 | Loss: 0.00031692
Iteration 12/25 | Loss: 0.00031692
Iteration 13/25 | Loss: 0.00031692
Iteration 14/25 | Loss: 0.00031692
Iteration 15/25 | Loss: 0.00031692
Iteration 16/25 | Loss: 0.00031692
Iteration 17/25 | Loss: 0.00031692
Iteration 18/25 | Loss: 0.00031692
Iteration 19/25 | Loss: 0.00031692
Iteration 20/25 | Loss: 0.00031692
Iteration 21/25 | Loss: 0.00031692
Iteration 22/25 | Loss: 0.00031692
Iteration 23/25 | Loss: 0.00031692
Iteration 24/25 | Loss: 0.00031692
Iteration 25/25 | Loss: 0.00031692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031692
Iteration 2/1000 | Loss: 0.00003477
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00002023
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001783
Iteration 8/1000 | Loss: 0.00001758
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001716
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001713
Iteration 17/1000 | Loss: 0.00001713
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001707
Iteration 21/1000 | Loss: 0.00001707
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001699
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001696
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001695
Iteration 50/1000 | Loss: 0.00001695
Iteration 51/1000 | Loss: 0.00001695
Iteration 52/1000 | Loss: 0.00001695
Iteration 53/1000 | Loss: 0.00001695
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.6946012692642398e-05, 1.6946012692642398e-05, 1.6946012692642398e-05, 1.6946012692642398e-05, 1.6946012692642398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6946012692642398e-05

Optimization complete. Final v2v error: 3.4593191146850586 mm

Highest mean error: 4.047323703765869 mm for frame 106

Lowest mean error: 3.2527222633361816 mm for frame 130

Saving results

Total time: 29.841572284698486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447853
Iteration 2/25 | Loss: 0.00114565
Iteration 3/25 | Loss: 0.00084484
Iteration 4/25 | Loss: 0.00080235
Iteration 5/25 | Loss: 0.00078931
Iteration 6/25 | Loss: 0.00078658
Iteration 7/25 | Loss: 0.00078558
Iteration 8/25 | Loss: 0.00078532
Iteration 9/25 | Loss: 0.00078532
Iteration 10/25 | Loss: 0.00078532
Iteration 11/25 | Loss: 0.00078532
Iteration 12/25 | Loss: 0.00078532
Iteration 13/25 | Loss: 0.00078532
Iteration 14/25 | Loss: 0.00078532
Iteration 15/25 | Loss: 0.00078532
Iteration 16/25 | Loss: 0.00078532
Iteration 17/25 | Loss: 0.00078532
Iteration 18/25 | Loss: 0.00078532
Iteration 19/25 | Loss: 0.00078532
Iteration 20/25 | Loss: 0.00078532
Iteration 21/25 | Loss: 0.00078532
Iteration 22/25 | Loss: 0.00078532
Iteration 23/25 | Loss: 0.00078532
Iteration 24/25 | Loss: 0.00078532
Iteration 25/25 | Loss: 0.00078532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007853228016756475, 0.0007853228016756475, 0.0007853228016756475, 0.0007853228016756475, 0.0007853228016756475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007853228016756475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22858858
Iteration 2/25 | Loss: 0.00030069
Iteration 3/25 | Loss: 0.00030067
Iteration 4/25 | Loss: 0.00030067
Iteration 5/25 | Loss: 0.00030067
Iteration 6/25 | Loss: 0.00030067
Iteration 7/25 | Loss: 0.00030067
Iteration 8/25 | Loss: 0.00030067
Iteration 9/25 | Loss: 0.00030067
Iteration 10/25 | Loss: 0.00030067
Iteration 11/25 | Loss: 0.00030067
Iteration 12/25 | Loss: 0.00030067
Iteration 13/25 | Loss: 0.00030067
Iteration 14/25 | Loss: 0.00030067
Iteration 15/25 | Loss: 0.00030067
Iteration 16/25 | Loss: 0.00030067
Iteration 17/25 | Loss: 0.00030067
Iteration 18/25 | Loss: 0.00030067
Iteration 19/25 | Loss: 0.00030067
Iteration 20/25 | Loss: 0.00030067
Iteration 21/25 | Loss: 0.00030067
Iteration 22/25 | Loss: 0.00030067
Iteration 23/25 | Loss: 0.00030067
Iteration 24/25 | Loss: 0.00030067
Iteration 25/25 | Loss: 0.00030067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030067
Iteration 2/1000 | Loss: 0.00006019
Iteration 3/1000 | Loss: 0.00004324
Iteration 4/1000 | Loss: 0.00003816
Iteration 5/1000 | Loss: 0.00003627
Iteration 6/1000 | Loss: 0.00003506
Iteration 7/1000 | Loss: 0.00003434
Iteration 8/1000 | Loss: 0.00003375
Iteration 9/1000 | Loss: 0.00003311
Iteration 10/1000 | Loss: 0.00003276
Iteration 11/1000 | Loss: 0.00003242
Iteration 12/1000 | Loss: 0.00003219
Iteration 13/1000 | Loss: 0.00003194
Iteration 14/1000 | Loss: 0.00003176
Iteration 15/1000 | Loss: 0.00003168
Iteration 16/1000 | Loss: 0.00003150
Iteration 17/1000 | Loss: 0.00003140
Iteration 18/1000 | Loss: 0.00003136
Iteration 19/1000 | Loss: 0.00003135
Iteration 20/1000 | Loss: 0.00003134
Iteration 21/1000 | Loss: 0.00003134
Iteration 22/1000 | Loss: 0.00003131
Iteration 23/1000 | Loss: 0.00003131
Iteration 24/1000 | Loss: 0.00003131
Iteration 25/1000 | Loss: 0.00003129
Iteration 26/1000 | Loss: 0.00003128
Iteration 27/1000 | Loss: 0.00003124
Iteration 28/1000 | Loss: 0.00003120
Iteration 29/1000 | Loss: 0.00003116
Iteration 30/1000 | Loss: 0.00003115
Iteration 31/1000 | Loss: 0.00003115
Iteration 32/1000 | Loss: 0.00003111
Iteration 33/1000 | Loss: 0.00003111
Iteration 34/1000 | Loss: 0.00003109
Iteration 35/1000 | Loss: 0.00003108
Iteration 36/1000 | Loss: 0.00003106
Iteration 37/1000 | Loss: 0.00003105
Iteration 38/1000 | Loss: 0.00003102
Iteration 39/1000 | Loss: 0.00003099
Iteration 40/1000 | Loss: 0.00003099
Iteration 41/1000 | Loss: 0.00003098
Iteration 42/1000 | Loss: 0.00003098
Iteration 43/1000 | Loss: 0.00003097
Iteration 44/1000 | Loss: 0.00003096
Iteration 45/1000 | Loss: 0.00003096
Iteration 46/1000 | Loss: 0.00003095
Iteration 47/1000 | Loss: 0.00003095
Iteration 48/1000 | Loss: 0.00003095
Iteration 49/1000 | Loss: 0.00003094
Iteration 50/1000 | Loss: 0.00003094
Iteration 51/1000 | Loss: 0.00003093
Iteration 52/1000 | Loss: 0.00003093
Iteration 53/1000 | Loss: 0.00003093
Iteration 54/1000 | Loss: 0.00003093
Iteration 55/1000 | Loss: 0.00003093
Iteration 56/1000 | Loss: 0.00003092
Iteration 57/1000 | Loss: 0.00003092
Iteration 58/1000 | Loss: 0.00003092
Iteration 59/1000 | Loss: 0.00003092
Iteration 60/1000 | Loss: 0.00003092
Iteration 61/1000 | Loss: 0.00003092
Iteration 62/1000 | Loss: 0.00003091
Iteration 63/1000 | Loss: 0.00003091
Iteration 64/1000 | Loss: 0.00003090
Iteration 65/1000 | Loss: 0.00003090
Iteration 66/1000 | Loss: 0.00003090
Iteration 67/1000 | Loss: 0.00003089
Iteration 68/1000 | Loss: 0.00003089
Iteration 69/1000 | Loss: 0.00003087
Iteration 70/1000 | Loss: 0.00003087
Iteration 71/1000 | Loss: 0.00003087
Iteration 72/1000 | Loss: 0.00003086
Iteration 73/1000 | Loss: 0.00003086
Iteration 74/1000 | Loss: 0.00003085
Iteration 75/1000 | Loss: 0.00003085
Iteration 76/1000 | Loss: 0.00003085
Iteration 77/1000 | Loss: 0.00003085
Iteration 78/1000 | Loss: 0.00003085
Iteration 79/1000 | Loss: 0.00003084
Iteration 80/1000 | Loss: 0.00003084
Iteration 81/1000 | Loss: 0.00003084
Iteration 82/1000 | Loss: 0.00003084
Iteration 83/1000 | Loss: 0.00003084
Iteration 84/1000 | Loss: 0.00003084
Iteration 85/1000 | Loss: 0.00003083
Iteration 86/1000 | Loss: 0.00003083
Iteration 87/1000 | Loss: 0.00003083
Iteration 88/1000 | Loss: 0.00003083
Iteration 89/1000 | Loss: 0.00003083
Iteration 90/1000 | Loss: 0.00003083
Iteration 91/1000 | Loss: 0.00003083
Iteration 92/1000 | Loss: 0.00003083
Iteration 93/1000 | Loss: 0.00003083
Iteration 94/1000 | Loss: 0.00003083
Iteration 95/1000 | Loss: 0.00003083
Iteration 96/1000 | Loss: 0.00003083
Iteration 97/1000 | Loss: 0.00003083
Iteration 98/1000 | Loss: 0.00003083
Iteration 99/1000 | Loss: 0.00003083
Iteration 100/1000 | Loss: 0.00003083
Iteration 101/1000 | Loss: 0.00003083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [3.083080082433298e-05, 3.083080082433298e-05, 3.083080082433298e-05, 3.083080082433298e-05, 3.083080082433298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.083080082433298e-05

Optimization complete. Final v2v error: 4.486014366149902 mm

Highest mean error: 6.207952499389648 mm for frame 80

Lowest mean error: 3.404958963394165 mm for frame 48

Saving results

Total time: 45.460914850234985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567851
Iteration 2/25 | Loss: 0.00089137
Iteration 3/25 | Loss: 0.00075623
Iteration 4/25 | Loss: 0.00072745
Iteration 5/25 | Loss: 0.00072297
Iteration 6/25 | Loss: 0.00072195
Iteration 7/25 | Loss: 0.00072195
Iteration 8/25 | Loss: 0.00072195
Iteration 9/25 | Loss: 0.00072195
Iteration 10/25 | Loss: 0.00072195
Iteration 11/25 | Loss: 0.00072195
Iteration 12/25 | Loss: 0.00072195
Iteration 13/25 | Loss: 0.00072195
Iteration 14/25 | Loss: 0.00072195
Iteration 15/25 | Loss: 0.00072195
Iteration 16/25 | Loss: 0.00072195
Iteration 17/25 | Loss: 0.00072195
Iteration 18/25 | Loss: 0.00072195
Iteration 19/25 | Loss: 0.00072195
Iteration 20/25 | Loss: 0.00072195
Iteration 21/25 | Loss: 0.00072195
Iteration 22/25 | Loss: 0.00072195
Iteration 23/25 | Loss: 0.00072195
Iteration 24/25 | Loss: 0.00072195
Iteration 25/25 | Loss: 0.00072195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.92132854
Iteration 2/25 | Loss: 0.00029605
Iteration 3/25 | Loss: 0.00029602
Iteration 4/25 | Loss: 0.00029602
Iteration 5/25 | Loss: 0.00029602
Iteration 6/25 | Loss: 0.00029602
Iteration 7/25 | Loss: 0.00029602
Iteration 8/25 | Loss: 0.00029601
Iteration 9/25 | Loss: 0.00029601
Iteration 10/25 | Loss: 0.00029601
Iteration 11/25 | Loss: 0.00029601
Iteration 12/25 | Loss: 0.00029601
Iteration 13/25 | Loss: 0.00029601
Iteration 14/25 | Loss: 0.00029601
Iteration 15/25 | Loss: 0.00029601
Iteration 16/25 | Loss: 0.00029601
Iteration 17/25 | Loss: 0.00029601
Iteration 18/25 | Loss: 0.00029601
Iteration 19/25 | Loss: 0.00029601
Iteration 20/25 | Loss: 0.00029601
Iteration 21/25 | Loss: 0.00029601
Iteration 22/25 | Loss: 0.00029601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00029601415735669434, 0.00029601415735669434, 0.00029601415735669434, 0.00029601415735669434, 0.00029601415735669434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029601415735669434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029601
Iteration 2/1000 | Loss: 0.00002741
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001644
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001570
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001543
Iteration 10/1000 | Loss: 0.00001533
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001531
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001527
Iteration 18/1000 | Loss: 0.00001527
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001521
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001515
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001514
Iteration 37/1000 | Loss: 0.00001514
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001514
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001514
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001513
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001512
Iteration 51/1000 | Loss: 0.00001512
Iteration 52/1000 | Loss: 0.00001512
Iteration 53/1000 | Loss: 0.00001512
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001510
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.5100045857252553e-05, 1.5100045857252553e-05, 1.5100045857252553e-05, 1.5100045857252553e-05, 1.5100045857252553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5100045857252553e-05

Optimization complete. Final v2v error: 3.3423187732696533 mm

Highest mean error: 3.5823910236358643 mm for frame 233

Lowest mean error: 3.180586338043213 mm for frame 173

Saving results

Total time: 31.104442596435547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945391
Iteration 2/25 | Loss: 0.00100300
Iteration 3/25 | Loss: 0.00081674
Iteration 4/25 | Loss: 0.00077410
Iteration 5/25 | Loss: 0.00075599
Iteration 6/25 | Loss: 0.00075188
Iteration 7/25 | Loss: 0.00075073
Iteration 8/25 | Loss: 0.00075060
Iteration 9/25 | Loss: 0.00075060
Iteration 10/25 | Loss: 0.00075060
Iteration 11/25 | Loss: 0.00075060
Iteration 12/25 | Loss: 0.00075060
Iteration 13/25 | Loss: 0.00075060
Iteration 14/25 | Loss: 0.00075060
Iteration 15/25 | Loss: 0.00075060
Iteration 16/25 | Loss: 0.00075060
Iteration 17/25 | Loss: 0.00075060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007505973917432129, 0.0007505973917432129, 0.0007505973917432129, 0.0007505973917432129, 0.0007505973917432129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007505973917432129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46216071
Iteration 2/25 | Loss: 0.00034735
Iteration 3/25 | Loss: 0.00034733
Iteration 4/25 | Loss: 0.00034733
Iteration 5/25 | Loss: 0.00034733
Iteration 6/25 | Loss: 0.00034733
Iteration 7/25 | Loss: 0.00034733
Iteration 8/25 | Loss: 0.00034733
Iteration 9/25 | Loss: 0.00034733
Iteration 10/25 | Loss: 0.00034733
Iteration 11/25 | Loss: 0.00034733
Iteration 12/25 | Loss: 0.00034733
Iteration 13/25 | Loss: 0.00034733
Iteration 14/25 | Loss: 0.00034733
Iteration 15/25 | Loss: 0.00034733
Iteration 16/25 | Loss: 0.00034733
Iteration 17/25 | Loss: 0.00034733
Iteration 18/25 | Loss: 0.00034733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003473297692835331, 0.0003473297692835331, 0.0003473297692835331, 0.0003473297692835331, 0.0003473297692835331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003473297692835331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034733
Iteration 2/1000 | Loss: 0.00004363
Iteration 3/1000 | Loss: 0.00003290
Iteration 4/1000 | Loss: 0.00002967
Iteration 5/1000 | Loss: 0.00002821
Iteration 6/1000 | Loss: 0.00002725
Iteration 7/1000 | Loss: 0.00002634
Iteration 8/1000 | Loss: 0.00002567
Iteration 9/1000 | Loss: 0.00002530
Iteration 10/1000 | Loss: 0.00002505
Iteration 11/1000 | Loss: 0.00002495
Iteration 12/1000 | Loss: 0.00002495
Iteration 13/1000 | Loss: 0.00002479
Iteration 14/1000 | Loss: 0.00002466
Iteration 15/1000 | Loss: 0.00002463
Iteration 16/1000 | Loss: 0.00002463
Iteration 17/1000 | Loss: 0.00002461
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00002459
Iteration 20/1000 | Loss: 0.00002459
Iteration 21/1000 | Loss: 0.00002459
Iteration 22/1000 | Loss: 0.00002459
Iteration 23/1000 | Loss: 0.00002459
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002459
Iteration 27/1000 | Loss: 0.00002459
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00002459
Iteration 30/1000 | Loss: 0.00002458
Iteration 31/1000 | Loss: 0.00002458
Iteration 32/1000 | Loss: 0.00002458
Iteration 33/1000 | Loss: 0.00002458
Iteration 34/1000 | Loss: 0.00002458
Iteration 35/1000 | Loss: 0.00002458
Iteration 36/1000 | Loss: 0.00002458
Iteration 37/1000 | Loss: 0.00002458
Iteration 38/1000 | Loss: 0.00002456
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002455
Iteration 41/1000 | Loss: 0.00002455
Iteration 42/1000 | Loss: 0.00002455
Iteration 43/1000 | Loss: 0.00002454
Iteration 44/1000 | Loss: 0.00002453
Iteration 45/1000 | Loss: 0.00002453
Iteration 46/1000 | Loss: 0.00002452
Iteration 47/1000 | Loss: 0.00002452
Iteration 48/1000 | Loss: 0.00002452
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002451
Iteration 51/1000 | Loss: 0.00002451
Iteration 52/1000 | Loss: 0.00002451
Iteration 53/1000 | Loss: 0.00002450
Iteration 54/1000 | Loss: 0.00002450
Iteration 55/1000 | Loss: 0.00002450
Iteration 56/1000 | Loss: 0.00002450
Iteration 57/1000 | Loss: 0.00002450
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002449
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002449
Iteration 62/1000 | Loss: 0.00002449
Iteration 63/1000 | Loss: 0.00002449
Iteration 64/1000 | Loss: 0.00002449
Iteration 65/1000 | Loss: 0.00002449
Iteration 66/1000 | Loss: 0.00002449
Iteration 67/1000 | Loss: 0.00002449
Iteration 68/1000 | Loss: 0.00002449
Iteration 69/1000 | Loss: 0.00002449
Iteration 70/1000 | Loss: 0.00002449
Iteration 71/1000 | Loss: 0.00002448
Iteration 72/1000 | Loss: 0.00002448
Iteration 73/1000 | Loss: 0.00002448
Iteration 74/1000 | Loss: 0.00002448
Iteration 75/1000 | Loss: 0.00002447
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002447
Iteration 78/1000 | Loss: 0.00002447
Iteration 79/1000 | Loss: 0.00002446
Iteration 80/1000 | Loss: 0.00002446
Iteration 81/1000 | Loss: 0.00002446
Iteration 82/1000 | Loss: 0.00002445
Iteration 83/1000 | Loss: 0.00002445
Iteration 84/1000 | Loss: 0.00002444
Iteration 85/1000 | Loss: 0.00002444
Iteration 86/1000 | Loss: 0.00002444
Iteration 87/1000 | Loss: 0.00002443
Iteration 88/1000 | Loss: 0.00002442
Iteration 89/1000 | Loss: 0.00002442
Iteration 90/1000 | Loss: 0.00002442
Iteration 91/1000 | Loss: 0.00002441
Iteration 92/1000 | Loss: 0.00002441
Iteration 93/1000 | Loss: 0.00002441
Iteration 94/1000 | Loss: 0.00002441
Iteration 95/1000 | Loss: 0.00002441
Iteration 96/1000 | Loss: 0.00002440
Iteration 97/1000 | Loss: 0.00002440
Iteration 98/1000 | Loss: 0.00002440
Iteration 99/1000 | Loss: 0.00002439
Iteration 100/1000 | Loss: 0.00002438
Iteration 101/1000 | Loss: 0.00002438
Iteration 102/1000 | Loss: 0.00002438
Iteration 103/1000 | Loss: 0.00002438
Iteration 104/1000 | Loss: 0.00002437
Iteration 105/1000 | Loss: 0.00002437
Iteration 106/1000 | Loss: 0.00002437
Iteration 107/1000 | Loss: 0.00002436
Iteration 108/1000 | Loss: 0.00002436
Iteration 109/1000 | Loss: 0.00002436
Iteration 110/1000 | Loss: 0.00002436
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002435
Iteration 115/1000 | Loss: 0.00002435
Iteration 116/1000 | Loss: 0.00002435
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002434
Iteration 122/1000 | Loss: 0.00002434
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002434
Iteration 125/1000 | Loss: 0.00002434
Iteration 126/1000 | Loss: 0.00002434
Iteration 127/1000 | Loss: 0.00002434
Iteration 128/1000 | Loss: 0.00002433
Iteration 129/1000 | Loss: 0.00002433
Iteration 130/1000 | Loss: 0.00002433
Iteration 131/1000 | Loss: 0.00002433
Iteration 132/1000 | Loss: 0.00002433
Iteration 133/1000 | Loss: 0.00002433
Iteration 134/1000 | Loss: 0.00002433
Iteration 135/1000 | Loss: 0.00002433
Iteration 136/1000 | Loss: 0.00002433
Iteration 137/1000 | Loss: 0.00002433
Iteration 138/1000 | Loss: 0.00002433
Iteration 139/1000 | Loss: 0.00002433
Iteration 140/1000 | Loss: 0.00002433
Iteration 141/1000 | Loss: 0.00002433
Iteration 142/1000 | Loss: 0.00002433
Iteration 143/1000 | Loss: 0.00002432
Iteration 144/1000 | Loss: 0.00002432
Iteration 145/1000 | Loss: 0.00002432
Iteration 146/1000 | Loss: 0.00002432
Iteration 147/1000 | Loss: 0.00002432
Iteration 148/1000 | Loss: 0.00002432
Iteration 149/1000 | Loss: 0.00002431
Iteration 150/1000 | Loss: 0.00002431
Iteration 151/1000 | Loss: 0.00002431
Iteration 152/1000 | Loss: 0.00002431
Iteration 153/1000 | Loss: 0.00002431
Iteration 154/1000 | Loss: 0.00002431
Iteration 155/1000 | Loss: 0.00002431
Iteration 156/1000 | Loss: 0.00002431
Iteration 157/1000 | Loss: 0.00002431
Iteration 158/1000 | Loss: 0.00002431
Iteration 159/1000 | Loss: 0.00002431
Iteration 160/1000 | Loss: 0.00002431
Iteration 161/1000 | Loss: 0.00002430
Iteration 162/1000 | Loss: 0.00002430
Iteration 163/1000 | Loss: 0.00002430
Iteration 164/1000 | Loss: 0.00002430
Iteration 165/1000 | Loss: 0.00002430
Iteration 166/1000 | Loss: 0.00002430
Iteration 167/1000 | Loss: 0.00002430
Iteration 168/1000 | Loss: 0.00002429
Iteration 169/1000 | Loss: 0.00002429
Iteration 170/1000 | Loss: 0.00002429
Iteration 171/1000 | Loss: 0.00002429
Iteration 172/1000 | Loss: 0.00002429
Iteration 173/1000 | Loss: 0.00002429
Iteration 174/1000 | Loss: 0.00002429
Iteration 175/1000 | Loss: 0.00002429
Iteration 176/1000 | Loss: 0.00002428
Iteration 177/1000 | Loss: 0.00002428
Iteration 178/1000 | Loss: 0.00002428
Iteration 179/1000 | Loss: 0.00002428
Iteration 180/1000 | Loss: 0.00002427
Iteration 181/1000 | Loss: 0.00002427
Iteration 182/1000 | Loss: 0.00002427
Iteration 183/1000 | Loss: 0.00002427
Iteration 184/1000 | Loss: 0.00002427
Iteration 185/1000 | Loss: 0.00002427
Iteration 186/1000 | Loss: 0.00002427
Iteration 187/1000 | Loss: 0.00002427
Iteration 188/1000 | Loss: 0.00002427
Iteration 189/1000 | Loss: 0.00002426
Iteration 190/1000 | Loss: 0.00002426
Iteration 191/1000 | Loss: 0.00002426
Iteration 192/1000 | Loss: 0.00002426
Iteration 193/1000 | Loss: 0.00002426
Iteration 194/1000 | Loss: 0.00002426
Iteration 195/1000 | Loss: 0.00002426
Iteration 196/1000 | Loss: 0.00002426
Iteration 197/1000 | Loss: 0.00002426
Iteration 198/1000 | Loss: 0.00002426
Iteration 199/1000 | Loss: 0.00002426
Iteration 200/1000 | Loss: 0.00002426
Iteration 201/1000 | Loss: 0.00002426
Iteration 202/1000 | Loss: 0.00002426
Iteration 203/1000 | Loss: 0.00002426
Iteration 204/1000 | Loss: 0.00002426
Iteration 205/1000 | Loss: 0.00002426
Iteration 206/1000 | Loss: 0.00002425
Iteration 207/1000 | Loss: 0.00002425
Iteration 208/1000 | Loss: 0.00002425
Iteration 209/1000 | Loss: 0.00002425
Iteration 210/1000 | Loss: 0.00002425
Iteration 211/1000 | Loss: 0.00002425
Iteration 212/1000 | Loss: 0.00002424
Iteration 213/1000 | Loss: 0.00002424
Iteration 214/1000 | Loss: 0.00002424
Iteration 215/1000 | Loss: 0.00002424
Iteration 216/1000 | Loss: 0.00002424
Iteration 217/1000 | Loss: 0.00002424
Iteration 218/1000 | Loss: 0.00002424
Iteration 219/1000 | Loss: 0.00002424
Iteration 220/1000 | Loss: 0.00002424
Iteration 221/1000 | Loss: 0.00002424
Iteration 222/1000 | Loss: 0.00002424
Iteration 223/1000 | Loss: 0.00002423
Iteration 224/1000 | Loss: 0.00002423
Iteration 225/1000 | Loss: 0.00002423
Iteration 226/1000 | Loss: 0.00002423
Iteration 227/1000 | Loss: 0.00002423
Iteration 228/1000 | Loss: 0.00002423
Iteration 229/1000 | Loss: 0.00002423
Iteration 230/1000 | Loss: 0.00002423
Iteration 231/1000 | Loss: 0.00002423
Iteration 232/1000 | Loss: 0.00002423
Iteration 233/1000 | Loss: 0.00002423
Iteration 234/1000 | Loss: 0.00002423
Iteration 235/1000 | Loss: 0.00002423
Iteration 236/1000 | Loss: 0.00002423
Iteration 237/1000 | Loss: 0.00002423
Iteration 238/1000 | Loss: 0.00002423
Iteration 239/1000 | Loss: 0.00002423
Iteration 240/1000 | Loss: 0.00002422
Iteration 241/1000 | Loss: 0.00002422
Iteration 242/1000 | Loss: 0.00002422
Iteration 243/1000 | Loss: 0.00002422
Iteration 244/1000 | Loss: 0.00002422
Iteration 245/1000 | Loss: 0.00002422
Iteration 246/1000 | Loss: 0.00002422
Iteration 247/1000 | Loss: 0.00002422
Iteration 248/1000 | Loss: 0.00002422
Iteration 249/1000 | Loss: 0.00002422
Iteration 250/1000 | Loss: 0.00002422
Iteration 251/1000 | Loss: 0.00002422
Iteration 252/1000 | Loss: 0.00002422
Iteration 253/1000 | Loss: 0.00002422
Iteration 254/1000 | Loss: 0.00002422
Iteration 255/1000 | Loss: 0.00002422
Iteration 256/1000 | Loss: 0.00002422
Iteration 257/1000 | Loss: 0.00002422
Iteration 258/1000 | Loss: 0.00002422
Iteration 259/1000 | Loss: 0.00002422
Iteration 260/1000 | Loss: 0.00002422
Iteration 261/1000 | Loss: 0.00002422
Iteration 262/1000 | Loss: 0.00002422
Iteration 263/1000 | Loss: 0.00002422
Iteration 264/1000 | Loss: 0.00002422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.4223156287916936e-05, 2.4223156287916936e-05, 2.4223156287916936e-05, 2.4223156287916936e-05, 2.4223156287916936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4223156287916936e-05

Optimization complete. Final v2v error: 4.028892517089844 mm

Highest mean error: 6.056715965270996 mm for frame 70

Lowest mean error: 3.4166903495788574 mm for frame 94

Saving results

Total time: 45.22594618797302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600122
Iteration 2/25 | Loss: 0.00099737
Iteration 3/25 | Loss: 0.00088237
Iteration 4/25 | Loss: 0.00083892
Iteration 5/25 | Loss: 0.00082751
Iteration 6/25 | Loss: 0.00082336
Iteration 7/25 | Loss: 0.00082205
Iteration 8/25 | Loss: 0.00082187
Iteration 9/25 | Loss: 0.00082187
Iteration 10/25 | Loss: 0.00082187
Iteration 11/25 | Loss: 0.00082187
Iteration 12/25 | Loss: 0.00082187
Iteration 13/25 | Loss: 0.00082187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008218706352636218, 0.0008218706352636218, 0.0008218706352636218, 0.0008218706352636218, 0.0008218706352636218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008218706352636218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10134482
Iteration 2/25 | Loss: 0.00047540
Iteration 3/25 | Loss: 0.00047531
Iteration 4/25 | Loss: 0.00047531
Iteration 5/25 | Loss: 0.00047531
Iteration 6/25 | Loss: 0.00047531
Iteration 7/25 | Loss: 0.00047531
Iteration 8/25 | Loss: 0.00047531
Iteration 9/25 | Loss: 0.00047531
Iteration 10/25 | Loss: 0.00047531
Iteration 11/25 | Loss: 0.00047531
Iteration 12/25 | Loss: 0.00047531
Iteration 13/25 | Loss: 0.00047531
Iteration 14/25 | Loss: 0.00047531
Iteration 15/25 | Loss: 0.00047531
Iteration 16/25 | Loss: 0.00047531
Iteration 17/25 | Loss: 0.00047531
Iteration 18/25 | Loss: 0.00047531
Iteration 19/25 | Loss: 0.00047531
Iteration 20/25 | Loss: 0.00047531
Iteration 21/25 | Loss: 0.00047531
Iteration 22/25 | Loss: 0.00047531
Iteration 23/25 | Loss: 0.00047531
Iteration 24/25 | Loss: 0.00047531
Iteration 25/25 | Loss: 0.00047531

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047531
Iteration 2/1000 | Loss: 0.00005792
Iteration 3/1000 | Loss: 0.00003779
Iteration 4/1000 | Loss: 0.00003070
Iteration 5/1000 | Loss: 0.00002940
Iteration 6/1000 | Loss: 0.00002825
Iteration 7/1000 | Loss: 0.00002776
Iteration 8/1000 | Loss: 0.00002720
Iteration 9/1000 | Loss: 0.00002678
Iteration 10/1000 | Loss: 0.00002657
Iteration 11/1000 | Loss: 0.00002634
Iteration 12/1000 | Loss: 0.00002632
Iteration 13/1000 | Loss: 0.00002619
Iteration 14/1000 | Loss: 0.00002609
Iteration 15/1000 | Loss: 0.00002599
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002589
Iteration 18/1000 | Loss: 0.00002586
Iteration 19/1000 | Loss: 0.00002586
Iteration 20/1000 | Loss: 0.00002586
Iteration 21/1000 | Loss: 0.00002586
Iteration 22/1000 | Loss: 0.00002585
Iteration 23/1000 | Loss: 0.00002585
Iteration 24/1000 | Loss: 0.00002585
Iteration 25/1000 | Loss: 0.00002585
Iteration 26/1000 | Loss: 0.00002585
Iteration 27/1000 | Loss: 0.00002585
Iteration 28/1000 | Loss: 0.00002585
Iteration 29/1000 | Loss: 0.00002585
Iteration 30/1000 | Loss: 0.00002585
Iteration 31/1000 | Loss: 0.00002585
Iteration 32/1000 | Loss: 0.00002585
Iteration 33/1000 | Loss: 0.00002585
Iteration 34/1000 | Loss: 0.00002584
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002581
Iteration 37/1000 | Loss: 0.00002581
Iteration 38/1000 | Loss: 0.00002581
Iteration 39/1000 | Loss: 0.00002581
Iteration 40/1000 | Loss: 0.00002581
Iteration 41/1000 | Loss: 0.00002581
Iteration 42/1000 | Loss: 0.00002581
Iteration 43/1000 | Loss: 0.00002581
Iteration 44/1000 | Loss: 0.00002580
Iteration 45/1000 | Loss: 0.00002580
Iteration 46/1000 | Loss: 0.00002580
Iteration 47/1000 | Loss: 0.00002579
Iteration 48/1000 | Loss: 0.00002579
Iteration 49/1000 | Loss: 0.00002578
Iteration 50/1000 | Loss: 0.00002577
Iteration 51/1000 | Loss: 0.00002577
Iteration 52/1000 | Loss: 0.00002577
Iteration 53/1000 | Loss: 0.00002576
Iteration 54/1000 | Loss: 0.00002576
Iteration 55/1000 | Loss: 0.00002576
Iteration 56/1000 | Loss: 0.00002572
Iteration 57/1000 | Loss: 0.00002572
Iteration 58/1000 | Loss: 0.00002571
Iteration 59/1000 | Loss: 0.00002571
Iteration 60/1000 | Loss: 0.00002570
Iteration 61/1000 | Loss: 0.00002569
Iteration 62/1000 | Loss: 0.00002569
Iteration 63/1000 | Loss: 0.00002569
Iteration 64/1000 | Loss: 0.00002568
Iteration 65/1000 | Loss: 0.00002568
Iteration 66/1000 | Loss: 0.00002567
Iteration 67/1000 | Loss: 0.00002567
Iteration 68/1000 | Loss: 0.00002566
Iteration 69/1000 | Loss: 0.00002566
Iteration 70/1000 | Loss: 0.00002565
Iteration 71/1000 | Loss: 0.00002565
Iteration 72/1000 | Loss: 0.00002564
Iteration 73/1000 | Loss: 0.00002564
Iteration 74/1000 | Loss: 0.00002563
Iteration 75/1000 | Loss: 0.00002563
Iteration 76/1000 | Loss: 0.00002563
Iteration 77/1000 | Loss: 0.00002563
Iteration 78/1000 | Loss: 0.00002563
Iteration 79/1000 | Loss: 0.00002563
Iteration 80/1000 | Loss: 0.00002563
Iteration 81/1000 | Loss: 0.00002563
Iteration 82/1000 | Loss: 0.00002563
Iteration 83/1000 | Loss: 0.00002563
Iteration 84/1000 | Loss: 0.00002563
Iteration 85/1000 | Loss: 0.00002563
Iteration 86/1000 | Loss: 0.00002563
Iteration 87/1000 | Loss: 0.00002563
Iteration 88/1000 | Loss: 0.00002563
Iteration 89/1000 | Loss: 0.00002563
Iteration 90/1000 | Loss: 0.00002563
Iteration 91/1000 | Loss: 0.00002563
Iteration 92/1000 | Loss: 0.00002563
Iteration 93/1000 | Loss: 0.00002563
Iteration 94/1000 | Loss: 0.00002563
Iteration 95/1000 | Loss: 0.00002563
Iteration 96/1000 | Loss: 0.00002563
Iteration 97/1000 | Loss: 0.00002563
Iteration 98/1000 | Loss: 0.00002563
Iteration 99/1000 | Loss: 0.00002563
Iteration 100/1000 | Loss: 0.00002563
Iteration 101/1000 | Loss: 0.00002563
Iteration 102/1000 | Loss: 0.00002563
Iteration 103/1000 | Loss: 0.00002563
Iteration 104/1000 | Loss: 0.00002563
Iteration 105/1000 | Loss: 0.00002563
Iteration 106/1000 | Loss: 0.00002563
Iteration 107/1000 | Loss: 0.00002563
Iteration 108/1000 | Loss: 0.00002563
Iteration 109/1000 | Loss: 0.00002563
Iteration 110/1000 | Loss: 0.00002563
Iteration 111/1000 | Loss: 0.00002563
Iteration 112/1000 | Loss: 0.00002563
Iteration 113/1000 | Loss: 0.00002563
Iteration 114/1000 | Loss: 0.00002563
Iteration 115/1000 | Loss: 0.00002563
Iteration 116/1000 | Loss: 0.00002563
Iteration 117/1000 | Loss: 0.00002563
Iteration 118/1000 | Loss: 0.00002563
Iteration 119/1000 | Loss: 0.00002563
Iteration 120/1000 | Loss: 0.00002563
Iteration 121/1000 | Loss: 0.00002563
Iteration 122/1000 | Loss: 0.00002563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.5625613488955423e-05, 2.5625613488955423e-05, 2.5625613488955423e-05, 2.5625613488955423e-05, 2.5625613488955423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5625613488955423e-05

Optimization complete. Final v2v error: 4.173959255218506 mm

Highest mean error: 4.433258533477783 mm for frame 62

Lowest mean error: 3.9293038845062256 mm for frame 11

Saving results

Total time: 38.15738368034363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787619
Iteration 2/25 | Loss: 0.00114914
Iteration 3/25 | Loss: 0.00078432
Iteration 4/25 | Loss: 0.00073271
Iteration 5/25 | Loss: 0.00071988
Iteration 6/25 | Loss: 0.00071478
Iteration 7/25 | Loss: 0.00071335
Iteration 8/25 | Loss: 0.00071320
Iteration 9/25 | Loss: 0.00071320
Iteration 10/25 | Loss: 0.00071320
Iteration 11/25 | Loss: 0.00071320
Iteration 12/25 | Loss: 0.00071320
Iteration 13/25 | Loss: 0.00071320
Iteration 14/25 | Loss: 0.00071320
Iteration 15/25 | Loss: 0.00071320
Iteration 16/25 | Loss: 0.00071320
Iteration 17/25 | Loss: 0.00071320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007132018217816949, 0.0007132018217816949, 0.0007132018217816949, 0.0007132018217816949, 0.0007132018217816949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007132018217816949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47102904
Iteration 2/25 | Loss: 0.00038223
Iteration 3/25 | Loss: 0.00038222
Iteration 4/25 | Loss: 0.00038222
Iteration 5/25 | Loss: 0.00038222
Iteration 6/25 | Loss: 0.00038222
Iteration 7/25 | Loss: 0.00038222
Iteration 8/25 | Loss: 0.00038222
Iteration 9/25 | Loss: 0.00038222
Iteration 10/25 | Loss: 0.00038222
Iteration 11/25 | Loss: 0.00038222
Iteration 12/25 | Loss: 0.00038222
Iteration 13/25 | Loss: 0.00038222
Iteration 14/25 | Loss: 0.00038222
Iteration 15/25 | Loss: 0.00038222
Iteration 16/25 | Loss: 0.00038222
Iteration 17/25 | Loss: 0.00038222
Iteration 18/25 | Loss: 0.00038222
Iteration 19/25 | Loss: 0.00038222
Iteration 20/25 | Loss: 0.00038222
Iteration 21/25 | Loss: 0.00038222
Iteration 22/25 | Loss: 0.00038222
Iteration 23/25 | Loss: 0.00038222
Iteration 24/25 | Loss: 0.00038222
Iteration 25/25 | Loss: 0.00038222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038222
Iteration 2/1000 | Loss: 0.00003284
Iteration 3/1000 | Loss: 0.00001906
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001578
Iteration 8/1000 | Loss: 0.00001555
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001512
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001498
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001495
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001492
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001485
Iteration 45/1000 | Loss: 0.00001485
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001484
Iteration 55/1000 | Loss: 0.00001484
Iteration 56/1000 | Loss: 0.00001484
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001483
Iteration 59/1000 | Loss: 0.00001483
Iteration 60/1000 | Loss: 0.00001483
Iteration 61/1000 | Loss: 0.00001483
Iteration 62/1000 | Loss: 0.00001483
Iteration 63/1000 | Loss: 0.00001483
Iteration 64/1000 | Loss: 0.00001482
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001482
Iteration 68/1000 | Loss: 0.00001482
Iteration 69/1000 | Loss: 0.00001482
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001481
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00001481
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001480
Iteration 95/1000 | Loss: 0.00001480
Iteration 96/1000 | Loss: 0.00001480
Iteration 97/1000 | Loss: 0.00001480
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.4800667486269958e-05, 1.4800667486269958e-05, 1.4800667486269958e-05, 1.4800667486269958e-05, 1.4800667486269958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4800667486269958e-05

Optimization complete. Final v2v error: 3.2422854900360107 mm

Highest mean error: 4.408341407775879 mm for frame 73

Lowest mean error: 2.8235552310943604 mm for frame 239

Saving results

Total time: 39.664199113845825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790510
Iteration 2/25 | Loss: 0.00125998
Iteration 3/25 | Loss: 0.00098844
Iteration 4/25 | Loss: 0.00089042
Iteration 5/25 | Loss: 0.00084374
Iteration 6/25 | Loss: 0.00082868
Iteration 7/25 | Loss: 0.00082438
Iteration 8/25 | Loss: 0.00082377
Iteration 9/25 | Loss: 0.00082359
Iteration 10/25 | Loss: 0.00082352
Iteration 11/25 | Loss: 0.00082352
Iteration 12/25 | Loss: 0.00082352
Iteration 13/25 | Loss: 0.00082352
Iteration 14/25 | Loss: 0.00082352
Iteration 15/25 | Loss: 0.00082352
Iteration 16/25 | Loss: 0.00082352
Iteration 17/25 | Loss: 0.00082351
Iteration 18/25 | Loss: 0.00082351
Iteration 19/25 | Loss: 0.00082351
Iteration 20/25 | Loss: 0.00082351
Iteration 21/25 | Loss: 0.00082351
Iteration 22/25 | Loss: 0.00082351
Iteration 23/25 | Loss: 0.00082351
Iteration 24/25 | Loss: 0.00082351
Iteration 25/25 | Loss: 0.00082351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92244959
Iteration 2/25 | Loss: 0.00044981
Iteration 3/25 | Loss: 0.00044980
Iteration 4/25 | Loss: 0.00044980
Iteration 5/25 | Loss: 0.00044980
Iteration 6/25 | Loss: 0.00044980
Iteration 7/25 | Loss: 0.00044980
Iteration 8/25 | Loss: 0.00044980
Iteration 9/25 | Loss: 0.00044980
Iteration 10/25 | Loss: 0.00044980
Iteration 11/25 | Loss: 0.00044980
Iteration 12/25 | Loss: 0.00044980
Iteration 13/25 | Loss: 0.00044980
Iteration 14/25 | Loss: 0.00044980
Iteration 15/25 | Loss: 0.00044980
Iteration 16/25 | Loss: 0.00044980
Iteration 17/25 | Loss: 0.00044980
Iteration 18/25 | Loss: 0.00044980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004497997579164803, 0.0004497997579164803, 0.0004497997579164803, 0.0004497997579164803, 0.0004497997579164803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004497997579164803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044980
Iteration 2/1000 | Loss: 0.00005363
Iteration 3/1000 | Loss: 0.00003708
Iteration 4/1000 | Loss: 0.00003235
Iteration 5/1000 | Loss: 0.00003127
Iteration 6/1000 | Loss: 0.00003013
Iteration 7/1000 | Loss: 0.00002968
Iteration 8/1000 | Loss: 0.00002922
Iteration 9/1000 | Loss: 0.00002872
Iteration 10/1000 | Loss: 0.00002840
Iteration 11/1000 | Loss: 0.00002819
Iteration 12/1000 | Loss: 0.00002804
Iteration 13/1000 | Loss: 0.00002802
Iteration 14/1000 | Loss: 0.00002797
Iteration 15/1000 | Loss: 0.00002793
Iteration 16/1000 | Loss: 0.00002789
Iteration 17/1000 | Loss: 0.00002789
Iteration 18/1000 | Loss: 0.00002787
Iteration 19/1000 | Loss: 0.00002786
Iteration 20/1000 | Loss: 0.00002786
Iteration 21/1000 | Loss: 0.00002785
Iteration 22/1000 | Loss: 0.00002784
Iteration 23/1000 | Loss: 0.00002784
Iteration 24/1000 | Loss: 0.00002784
Iteration 25/1000 | Loss: 0.00002783
Iteration 26/1000 | Loss: 0.00002783
Iteration 27/1000 | Loss: 0.00002782
Iteration 28/1000 | Loss: 0.00002782
Iteration 29/1000 | Loss: 0.00002781
Iteration 30/1000 | Loss: 0.00002781
Iteration 31/1000 | Loss: 0.00002781
Iteration 32/1000 | Loss: 0.00002780
Iteration 33/1000 | Loss: 0.00002780
Iteration 34/1000 | Loss: 0.00002776
Iteration 35/1000 | Loss: 0.00002774
Iteration 36/1000 | Loss: 0.00002774
Iteration 37/1000 | Loss: 0.00002773
Iteration 38/1000 | Loss: 0.00002773
Iteration 39/1000 | Loss: 0.00002773
Iteration 40/1000 | Loss: 0.00002773
Iteration 41/1000 | Loss: 0.00002773
Iteration 42/1000 | Loss: 0.00002773
Iteration 43/1000 | Loss: 0.00002773
Iteration 44/1000 | Loss: 0.00002773
Iteration 45/1000 | Loss: 0.00002773
Iteration 46/1000 | Loss: 0.00002773
Iteration 47/1000 | Loss: 0.00002773
Iteration 48/1000 | Loss: 0.00002772
Iteration 49/1000 | Loss: 0.00002772
Iteration 50/1000 | Loss: 0.00002772
Iteration 51/1000 | Loss: 0.00002772
Iteration 52/1000 | Loss: 0.00002772
Iteration 53/1000 | Loss: 0.00002772
Iteration 54/1000 | Loss: 0.00002772
Iteration 55/1000 | Loss: 0.00002771
Iteration 56/1000 | Loss: 0.00002771
Iteration 57/1000 | Loss: 0.00002770
Iteration 58/1000 | Loss: 0.00002770
Iteration 59/1000 | Loss: 0.00002770
Iteration 60/1000 | Loss: 0.00002769
Iteration 61/1000 | Loss: 0.00002769
Iteration 62/1000 | Loss: 0.00002769
Iteration 63/1000 | Loss: 0.00002767
Iteration 64/1000 | Loss: 0.00002767
Iteration 65/1000 | Loss: 0.00002767
Iteration 66/1000 | Loss: 0.00002767
Iteration 67/1000 | Loss: 0.00002767
Iteration 68/1000 | Loss: 0.00002767
Iteration 69/1000 | Loss: 0.00002767
Iteration 70/1000 | Loss: 0.00002767
Iteration 71/1000 | Loss: 0.00002767
Iteration 72/1000 | Loss: 0.00002767
Iteration 73/1000 | Loss: 0.00002767
Iteration 74/1000 | Loss: 0.00002767
Iteration 75/1000 | Loss: 0.00002766
Iteration 76/1000 | Loss: 0.00002766
Iteration 77/1000 | Loss: 0.00002766
Iteration 78/1000 | Loss: 0.00002766
Iteration 79/1000 | Loss: 0.00002766
Iteration 80/1000 | Loss: 0.00002766
Iteration 81/1000 | Loss: 0.00002765
Iteration 82/1000 | Loss: 0.00002765
Iteration 83/1000 | Loss: 0.00002765
Iteration 84/1000 | Loss: 0.00002764
Iteration 85/1000 | Loss: 0.00002764
Iteration 86/1000 | Loss: 0.00002764
Iteration 87/1000 | Loss: 0.00002763
Iteration 88/1000 | Loss: 0.00002763
Iteration 89/1000 | Loss: 0.00002762
Iteration 90/1000 | Loss: 0.00002762
Iteration 91/1000 | Loss: 0.00002761
Iteration 92/1000 | Loss: 0.00002761
Iteration 93/1000 | Loss: 0.00002761
Iteration 94/1000 | Loss: 0.00002761
Iteration 95/1000 | Loss: 0.00002760
Iteration 96/1000 | Loss: 0.00002760
Iteration 97/1000 | Loss: 0.00002760
Iteration 98/1000 | Loss: 0.00002760
Iteration 99/1000 | Loss: 0.00002759
Iteration 100/1000 | Loss: 0.00002759
Iteration 101/1000 | Loss: 0.00002759
Iteration 102/1000 | Loss: 0.00002759
Iteration 103/1000 | Loss: 0.00002758
Iteration 104/1000 | Loss: 0.00002758
Iteration 105/1000 | Loss: 0.00002758
Iteration 106/1000 | Loss: 0.00002758
Iteration 107/1000 | Loss: 0.00002758
Iteration 108/1000 | Loss: 0.00002758
Iteration 109/1000 | Loss: 0.00002757
Iteration 110/1000 | Loss: 0.00002757
Iteration 111/1000 | Loss: 0.00002757
Iteration 112/1000 | Loss: 0.00002757
Iteration 113/1000 | Loss: 0.00002757
Iteration 114/1000 | Loss: 0.00002756
Iteration 115/1000 | Loss: 0.00002756
Iteration 116/1000 | Loss: 0.00002756
Iteration 117/1000 | Loss: 0.00002756
Iteration 118/1000 | Loss: 0.00002756
Iteration 119/1000 | Loss: 0.00002756
Iteration 120/1000 | Loss: 0.00002756
Iteration 121/1000 | Loss: 0.00002755
Iteration 122/1000 | Loss: 0.00002755
Iteration 123/1000 | Loss: 0.00002755
Iteration 124/1000 | Loss: 0.00002755
Iteration 125/1000 | Loss: 0.00002755
Iteration 126/1000 | Loss: 0.00002755
Iteration 127/1000 | Loss: 0.00002755
Iteration 128/1000 | Loss: 0.00002755
Iteration 129/1000 | Loss: 0.00002754
Iteration 130/1000 | Loss: 0.00002754
Iteration 131/1000 | Loss: 0.00002754
Iteration 132/1000 | Loss: 0.00002754
Iteration 133/1000 | Loss: 0.00002753
Iteration 134/1000 | Loss: 0.00002753
Iteration 135/1000 | Loss: 0.00002753
Iteration 136/1000 | Loss: 0.00002753
Iteration 137/1000 | Loss: 0.00002753
Iteration 138/1000 | Loss: 0.00002753
Iteration 139/1000 | Loss: 0.00002753
Iteration 140/1000 | Loss: 0.00002753
Iteration 141/1000 | Loss: 0.00002753
Iteration 142/1000 | Loss: 0.00002753
Iteration 143/1000 | Loss: 0.00002753
Iteration 144/1000 | Loss: 0.00002753
Iteration 145/1000 | Loss: 0.00002753
Iteration 146/1000 | Loss: 0.00002753
Iteration 147/1000 | Loss: 0.00002753
Iteration 148/1000 | Loss: 0.00002752
Iteration 149/1000 | Loss: 0.00002752
Iteration 150/1000 | Loss: 0.00002752
Iteration 151/1000 | Loss: 0.00002752
Iteration 152/1000 | Loss: 0.00002752
Iteration 153/1000 | Loss: 0.00002752
Iteration 154/1000 | Loss: 0.00002752
Iteration 155/1000 | Loss: 0.00002752
Iteration 156/1000 | Loss: 0.00002752
Iteration 157/1000 | Loss: 0.00002752
Iteration 158/1000 | Loss: 0.00002752
Iteration 159/1000 | Loss: 0.00002752
Iteration 160/1000 | Loss: 0.00002752
Iteration 161/1000 | Loss: 0.00002752
Iteration 162/1000 | Loss: 0.00002752
Iteration 163/1000 | Loss: 0.00002751
Iteration 164/1000 | Loss: 0.00002751
Iteration 165/1000 | Loss: 0.00002751
Iteration 166/1000 | Loss: 0.00002751
Iteration 167/1000 | Loss: 0.00002751
Iteration 168/1000 | Loss: 0.00002751
Iteration 169/1000 | Loss: 0.00002751
Iteration 170/1000 | Loss: 0.00002751
Iteration 171/1000 | Loss: 0.00002751
Iteration 172/1000 | Loss: 0.00002751
Iteration 173/1000 | Loss: 0.00002751
Iteration 174/1000 | Loss: 0.00002751
Iteration 175/1000 | Loss: 0.00002751
Iteration 176/1000 | Loss: 0.00002751
Iteration 177/1000 | Loss: 0.00002751
Iteration 178/1000 | Loss: 0.00002751
Iteration 179/1000 | Loss: 0.00002751
Iteration 180/1000 | Loss: 0.00002751
Iteration 181/1000 | Loss: 0.00002751
Iteration 182/1000 | Loss: 0.00002750
Iteration 183/1000 | Loss: 0.00002750
Iteration 184/1000 | Loss: 0.00002750
Iteration 185/1000 | Loss: 0.00002750
Iteration 186/1000 | Loss: 0.00002750
Iteration 187/1000 | Loss: 0.00002750
Iteration 188/1000 | Loss: 0.00002750
Iteration 189/1000 | Loss: 0.00002750
Iteration 190/1000 | Loss: 0.00002750
Iteration 191/1000 | Loss: 0.00002750
Iteration 192/1000 | Loss: 0.00002750
Iteration 193/1000 | Loss: 0.00002750
Iteration 194/1000 | Loss: 0.00002750
Iteration 195/1000 | Loss: 0.00002750
Iteration 196/1000 | Loss: 0.00002750
Iteration 197/1000 | Loss: 0.00002750
Iteration 198/1000 | Loss: 0.00002750
Iteration 199/1000 | Loss: 0.00002750
Iteration 200/1000 | Loss: 0.00002750
Iteration 201/1000 | Loss: 0.00002750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.7497120754560456e-05, 2.7497120754560456e-05, 2.7497120754560456e-05, 2.7497120754560456e-05, 2.7497120754560456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7497120754560456e-05

Optimization complete. Final v2v error: 4.29188871383667 mm

Highest mean error: 5.142358779907227 mm for frame 5

Lowest mean error: 3.355177402496338 mm for frame 138

Saving results

Total time: 47.824280977249146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424602
Iteration 2/25 | Loss: 0.00091057
Iteration 3/25 | Loss: 0.00073828
Iteration 4/25 | Loss: 0.00069925
Iteration 5/25 | Loss: 0.00069090
Iteration 6/25 | Loss: 0.00068943
Iteration 7/25 | Loss: 0.00068894
Iteration 8/25 | Loss: 0.00068887
Iteration 9/25 | Loss: 0.00068887
Iteration 10/25 | Loss: 0.00068887
Iteration 11/25 | Loss: 0.00068887
Iteration 12/25 | Loss: 0.00068887
Iteration 13/25 | Loss: 0.00068887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000688872707542032, 0.000688872707542032, 0.000688872707542032, 0.000688872707542032, 0.000688872707542032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000688872707542032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60722661
Iteration 2/25 | Loss: 0.00029110
Iteration 3/25 | Loss: 0.00029110
Iteration 4/25 | Loss: 0.00029110
Iteration 5/25 | Loss: 0.00029110
Iteration 6/25 | Loss: 0.00029110
Iteration 7/25 | Loss: 0.00029110
Iteration 8/25 | Loss: 0.00029110
Iteration 9/25 | Loss: 0.00029110
Iteration 10/25 | Loss: 0.00029110
Iteration 11/25 | Loss: 0.00029110
Iteration 12/25 | Loss: 0.00029110
Iteration 13/25 | Loss: 0.00029110
Iteration 14/25 | Loss: 0.00029110
Iteration 15/25 | Loss: 0.00029110
Iteration 16/25 | Loss: 0.00029110
Iteration 17/25 | Loss: 0.00029110
Iteration 18/25 | Loss: 0.00029110
Iteration 19/25 | Loss: 0.00029110
Iteration 20/25 | Loss: 0.00029110
Iteration 21/25 | Loss: 0.00029110
Iteration 22/25 | Loss: 0.00029110
Iteration 23/25 | Loss: 0.00029110
Iteration 24/25 | Loss: 0.00029110
Iteration 25/25 | Loss: 0.00029110

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029110
Iteration 2/1000 | Loss: 0.00003419
Iteration 3/1000 | Loss: 0.00002316
Iteration 4/1000 | Loss: 0.00002059
Iteration 5/1000 | Loss: 0.00001963
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001730
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001703
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001691
Iteration 19/1000 | Loss: 0.00001690
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001682
Iteration 23/1000 | Loss: 0.00001681
Iteration 24/1000 | Loss: 0.00001681
Iteration 25/1000 | Loss: 0.00001679
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001677
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001673
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001671
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001670
Iteration 35/1000 | Loss: 0.00001669
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001668
Iteration 41/1000 | Loss: 0.00001668
Iteration 42/1000 | Loss: 0.00001668
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001666
Iteration 46/1000 | Loss: 0.00001666
Iteration 47/1000 | Loss: 0.00001666
Iteration 48/1000 | Loss: 0.00001666
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001665
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001654
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001653
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001648
Iteration 132/1000 | Loss: 0.00001648
Iteration 133/1000 | Loss: 0.00001648
Iteration 134/1000 | Loss: 0.00001648
Iteration 135/1000 | Loss: 0.00001648
Iteration 136/1000 | Loss: 0.00001648
Iteration 137/1000 | Loss: 0.00001648
Iteration 138/1000 | Loss: 0.00001647
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001647
Iteration 141/1000 | Loss: 0.00001647
Iteration 142/1000 | Loss: 0.00001647
Iteration 143/1000 | Loss: 0.00001647
Iteration 144/1000 | Loss: 0.00001647
Iteration 145/1000 | Loss: 0.00001647
Iteration 146/1000 | Loss: 0.00001647
Iteration 147/1000 | Loss: 0.00001647
Iteration 148/1000 | Loss: 0.00001647
Iteration 149/1000 | Loss: 0.00001647
Iteration 150/1000 | Loss: 0.00001647
Iteration 151/1000 | Loss: 0.00001647
Iteration 152/1000 | Loss: 0.00001647
Iteration 153/1000 | Loss: 0.00001647
Iteration 154/1000 | Loss: 0.00001647
Iteration 155/1000 | Loss: 0.00001646
Iteration 156/1000 | Loss: 0.00001646
Iteration 157/1000 | Loss: 0.00001646
Iteration 158/1000 | Loss: 0.00001646
Iteration 159/1000 | Loss: 0.00001646
Iteration 160/1000 | Loss: 0.00001646
Iteration 161/1000 | Loss: 0.00001646
Iteration 162/1000 | Loss: 0.00001646
Iteration 163/1000 | Loss: 0.00001646
Iteration 164/1000 | Loss: 0.00001646
Iteration 165/1000 | Loss: 0.00001646
Iteration 166/1000 | Loss: 0.00001646
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001646
Iteration 169/1000 | Loss: 0.00001646
Iteration 170/1000 | Loss: 0.00001646
Iteration 171/1000 | Loss: 0.00001646
Iteration 172/1000 | Loss: 0.00001646
Iteration 173/1000 | Loss: 0.00001646
Iteration 174/1000 | Loss: 0.00001646
Iteration 175/1000 | Loss: 0.00001646
Iteration 176/1000 | Loss: 0.00001646
Iteration 177/1000 | Loss: 0.00001646
Iteration 178/1000 | Loss: 0.00001646
Iteration 179/1000 | Loss: 0.00001646
Iteration 180/1000 | Loss: 0.00001646
Iteration 181/1000 | Loss: 0.00001646
Iteration 182/1000 | Loss: 0.00001646
Iteration 183/1000 | Loss: 0.00001646
Iteration 184/1000 | Loss: 0.00001646
Iteration 185/1000 | Loss: 0.00001646
Iteration 186/1000 | Loss: 0.00001646
Iteration 187/1000 | Loss: 0.00001646
Iteration 188/1000 | Loss: 0.00001646
Iteration 189/1000 | Loss: 0.00001646
Iteration 190/1000 | Loss: 0.00001646
Iteration 191/1000 | Loss: 0.00001646
Iteration 192/1000 | Loss: 0.00001646
Iteration 193/1000 | Loss: 0.00001646
Iteration 194/1000 | Loss: 0.00001646
Iteration 195/1000 | Loss: 0.00001646
Iteration 196/1000 | Loss: 0.00001646
Iteration 197/1000 | Loss: 0.00001646
Iteration 198/1000 | Loss: 0.00001646
Iteration 199/1000 | Loss: 0.00001646
Iteration 200/1000 | Loss: 0.00001646
Iteration 201/1000 | Loss: 0.00001646
Iteration 202/1000 | Loss: 0.00001646
Iteration 203/1000 | Loss: 0.00001646
Iteration 204/1000 | Loss: 0.00001646
Iteration 205/1000 | Loss: 0.00001646
Iteration 206/1000 | Loss: 0.00001646
Iteration 207/1000 | Loss: 0.00001646
Iteration 208/1000 | Loss: 0.00001646
Iteration 209/1000 | Loss: 0.00001646
Iteration 210/1000 | Loss: 0.00001646
Iteration 211/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.6458265235996805e-05, 1.6458265235996805e-05, 1.6458265235996805e-05, 1.6458265235996805e-05, 1.6458265235996805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6458265235996805e-05

Optimization complete. Final v2v error: 3.431185483932495 mm

Highest mean error: 4.859639644622803 mm for frame 46

Lowest mean error: 2.897632360458374 mm for frame 23

Saving results

Total time: 42.7325119972229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010656
Iteration 2/25 | Loss: 0.00191267
Iteration 3/25 | Loss: 0.00120336
Iteration 4/25 | Loss: 0.00108726
Iteration 5/25 | Loss: 0.00113851
Iteration 6/25 | Loss: 0.00086751
Iteration 7/25 | Loss: 0.00078551
Iteration 8/25 | Loss: 0.00078053
Iteration 9/25 | Loss: 0.00076782
Iteration 10/25 | Loss: 0.00075364
Iteration 11/25 | Loss: 0.00074807
Iteration 12/25 | Loss: 0.00074979
Iteration 13/25 | Loss: 0.00074448
Iteration 14/25 | Loss: 0.00074434
Iteration 15/25 | Loss: 0.00074523
Iteration 16/25 | Loss: 0.00074341
Iteration 17/25 | Loss: 0.00074384
Iteration 18/25 | Loss: 0.00074222
Iteration 19/25 | Loss: 0.00074211
Iteration 20/25 | Loss: 0.00074180
Iteration 21/25 | Loss: 0.00074015
Iteration 22/25 | Loss: 0.00073786
Iteration 23/25 | Loss: 0.00074257
Iteration 24/25 | Loss: 0.00074402
Iteration 25/25 | Loss: 0.00074221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46580470
Iteration 2/25 | Loss: 0.00045603
Iteration 3/25 | Loss: 0.00045602
Iteration 4/25 | Loss: 0.00045602
Iteration 5/25 | Loss: 0.00045602
Iteration 6/25 | Loss: 0.00045602
Iteration 7/25 | Loss: 0.00045602
Iteration 8/25 | Loss: 0.00045602
Iteration 9/25 | Loss: 0.00045602
Iteration 10/25 | Loss: 0.00045602
Iteration 11/25 | Loss: 0.00045602
Iteration 12/25 | Loss: 0.00045602
Iteration 13/25 | Loss: 0.00045602
Iteration 14/25 | Loss: 0.00045602
Iteration 15/25 | Loss: 0.00045602
Iteration 16/25 | Loss: 0.00045602
Iteration 17/25 | Loss: 0.00045602
Iteration 18/25 | Loss: 0.00045602
Iteration 19/25 | Loss: 0.00045602
Iteration 20/25 | Loss: 0.00045602
Iteration 21/25 | Loss: 0.00045602
Iteration 22/25 | Loss: 0.00045602
Iteration 23/25 | Loss: 0.00045602
Iteration 24/25 | Loss: 0.00045602
Iteration 25/25 | Loss: 0.00045602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045602
Iteration 2/1000 | Loss: 0.00008681
Iteration 3/1000 | Loss: 0.00009645
Iteration 4/1000 | Loss: 0.00013646
Iteration 5/1000 | Loss: 0.00010175
Iteration 6/1000 | Loss: 0.00013751
Iteration 7/1000 | Loss: 0.00013364
Iteration 8/1000 | Loss: 0.00011434
Iteration 9/1000 | Loss: 0.00009603
Iteration 10/1000 | Loss: 0.00012087
Iteration 11/1000 | Loss: 0.00022596
Iteration 12/1000 | Loss: 0.00015537
Iteration 13/1000 | Loss: 0.00008829
Iteration 14/1000 | Loss: 0.00008946
Iteration 15/1000 | Loss: 0.00010407
Iteration 16/1000 | Loss: 0.00011873
Iteration 17/1000 | Loss: 0.00012179
Iteration 18/1000 | Loss: 0.00012484
Iteration 19/1000 | Loss: 0.00011291
Iteration 20/1000 | Loss: 0.00014600
Iteration 21/1000 | Loss: 0.00011503
Iteration 22/1000 | Loss: 0.00007704
Iteration 23/1000 | Loss: 0.00007309
Iteration 24/1000 | Loss: 0.00009107
Iteration 25/1000 | Loss: 0.00008035
Iteration 26/1000 | Loss: 0.00012173
Iteration 27/1000 | Loss: 0.00007433
Iteration 28/1000 | Loss: 0.00010117
Iteration 29/1000 | Loss: 0.00008697
Iteration 30/1000 | Loss: 0.00011146
Iteration 31/1000 | Loss: 0.00008017
Iteration 32/1000 | Loss: 0.00007881
Iteration 33/1000 | Loss: 0.00007421
Iteration 34/1000 | Loss: 0.00008708
Iteration 35/1000 | Loss: 0.00009815
Iteration 36/1000 | Loss: 0.00007259
Iteration 37/1000 | Loss: 0.00009031
Iteration 38/1000 | Loss: 0.00008287
Iteration 39/1000 | Loss: 0.00010586
Iteration 40/1000 | Loss: 0.00007926
Iteration 41/1000 | Loss: 0.00005813
Iteration 42/1000 | Loss: 0.00004494
Iteration 43/1000 | Loss: 0.00006266
Iteration 44/1000 | Loss: 0.00007546
Iteration 45/1000 | Loss: 0.00007192
Iteration 46/1000 | Loss: 0.00006758
Iteration 47/1000 | Loss: 0.00006848
Iteration 48/1000 | Loss: 0.00004373
Iteration 49/1000 | Loss: 0.00003032
Iteration 50/1000 | Loss: 0.00004537
Iteration 51/1000 | Loss: 0.00006850
Iteration 52/1000 | Loss: 0.00006537
Iteration 53/1000 | Loss: 0.00007458
Iteration 54/1000 | Loss: 0.00005828
Iteration 55/1000 | Loss: 0.00006875
Iteration 56/1000 | Loss: 0.00006059
Iteration 57/1000 | Loss: 0.00006024
Iteration 58/1000 | Loss: 0.00006045
Iteration 59/1000 | Loss: 0.00005765
Iteration 60/1000 | Loss: 0.00004473
Iteration 61/1000 | Loss: 0.00003598
Iteration 62/1000 | Loss: 0.00004816
Iteration 63/1000 | Loss: 0.00005348
Iteration 64/1000 | Loss: 0.00014735
Iteration 65/1000 | Loss: 0.00003720
Iteration 66/1000 | Loss: 0.00002432
Iteration 67/1000 | Loss: 0.00003368
Iteration 68/1000 | Loss: 0.00002044
Iteration 69/1000 | Loss: 0.00002334
Iteration 70/1000 | Loss: 0.00003098
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002985
Iteration 73/1000 | Loss: 0.00002557
Iteration 74/1000 | Loss: 0.00002312
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00002225
Iteration 77/1000 | Loss: 0.00002378
Iteration 78/1000 | Loss: 0.00002057
Iteration 79/1000 | Loss: 0.00001996
Iteration 80/1000 | Loss: 0.00002377
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00002408
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00002485
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00002443
Iteration 89/1000 | Loss: 0.00002546
Iteration 90/1000 | Loss: 0.00002180
Iteration 91/1000 | Loss: 0.00002612
Iteration 92/1000 | Loss: 0.00002447
Iteration 93/1000 | Loss: 0.00002492
Iteration 94/1000 | Loss: 0.00002763
Iteration 95/1000 | Loss: 0.00002528
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00003761
Iteration 100/1000 | Loss: 0.00002549
Iteration 101/1000 | Loss: 0.00002891
Iteration 102/1000 | Loss: 0.00003144
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001501
Iteration 115/1000 | Loss: 0.00001501
Iteration 116/1000 | Loss: 0.00001499
Iteration 117/1000 | Loss: 0.00001499
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001493
Iteration 121/1000 | Loss: 0.00001492
Iteration 122/1000 | Loss: 0.00001486
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00002803
Iteration 125/1000 | Loss: 0.00001742
Iteration 126/1000 | Loss: 0.00001634
Iteration 127/1000 | Loss: 0.00003605
Iteration 128/1000 | Loss: 0.00001796
Iteration 129/1000 | Loss: 0.00003510
Iteration 130/1000 | Loss: 0.00003231
Iteration 131/1000 | Loss: 0.00003373
Iteration 132/1000 | Loss: 0.00003156
Iteration 133/1000 | Loss: 0.00003296
Iteration 134/1000 | Loss: 0.00002922
Iteration 135/1000 | Loss: 0.00003575
Iteration 136/1000 | Loss: 0.00002880
Iteration 137/1000 | Loss: 0.00002348
Iteration 138/1000 | Loss: 0.00001791
Iteration 139/1000 | Loss: 0.00001580
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001528
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001476
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001464
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001460
Iteration 151/1000 | Loss: 0.00001460
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001458
Iteration 154/1000 | Loss: 0.00001453
Iteration 155/1000 | Loss: 0.00001452
Iteration 156/1000 | Loss: 0.00001452
Iteration 157/1000 | Loss: 0.00001452
Iteration 158/1000 | Loss: 0.00001451
Iteration 159/1000 | Loss: 0.00001451
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001449
Iteration 163/1000 | Loss: 0.00001449
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001449
Iteration 169/1000 | Loss: 0.00001449
Iteration 170/1000 | Loss: 0.00001449
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001448
Iteration 173/1000 | Loss: 0.00001448
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001448
Iteration 176/1000 | Loss: 0.00001448
Iteration 177/1000 | Loss: 0.00001448
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001448
Iteration 180/1000 | Loss: 0.00001448
Iteration 181/1000 | Loss: 0.00001448
Iteration 182/1000 | Loss: 0.00001448
Iteration 183/1000 | Loss: 0.00001448
Iteration 184/1000 | Loss: 0.00001447
Iteration 185/1000 | Loss: 0.00001447
Iteration 186/1000 | Loss: 0.00001447
Iteration 187/1000 | Loss: 0.00001447
Iteration 188/1000 | Loss: 0.00001447
Iteration 189/1000 | Loss: 0.00001447
Iteration 190/1000 | Loss: 0.00001447
Iteration 191/1000 | Loss: 0.00001447
Iteration 192/1000 | Loss: 0.00001447
Iteration 193/1000 | Loss: 0.00001446
Iteration 194/1000 | Loss: 0.00001446
Iteration 195/1000 | Loss: 0.00001446
Iteration 196/1000 | Loss: 0.00001446
Iteration 197/1000 | Loss: 0.00001446
Iteration 198/1000 | Loss: 0.00001446
Iteration 199/1000 | Loss: 0.00001446
Iteration 200/1000 | Loss: 0.00001446
Iteration 201/1000 | Loss: 0.00001446
Iteration 202/1000 | Loss: 0.00001445
Iteration 203/1000 | Loss: 0.00001445
Iteration 204/1000 | Loss: 0.00001445
Iteration 205/1000 | Loss: 0.00001445
Iteration 206/1000 | Loss: 0.00001445
Iteration 207/1000 | Loss: 0.00001445
Iteration 208/1000 | Loss: 0.00001445
Iteration 209/1000 | Loss: 0.00001445
Iteration 210/1000 | Loss: 0.00001445
Iteration 211/1000 | Loss: 0.00001445
Iteration 212/1000 | Loss: 0.00001445
Iteration 213/1000 | Loss: 0.00001445
Iteration 214/1000 | Loss: 0.00001445
Iteration 215/1000 | Loss: 0.00001445
Iteration 216/1000 | Loss: 0.00001445
Iteration 217/1000 | Loss: 0.00001445
Iteration 218/1000 | Loss: 0.00001445
Iteration 219/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.4447455214394722e-05, 1.4447455214394722e-05, 1.4447455214394722e-05, 1.4447455214394722e-05, 1.4447455214394722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4447455214394722e-05

Optimization complete. Final v2v error: 3.1387975215911865 mm

Highest mean error: 3.8118677139282227 mm for frame 233

Lowest mean error: 3.0005037784576416 mm for frame 235

Saving results

Total time: 265.77721118927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489993
Iteration 2/25 | Loss: 0.00093536
Iteration 3/25 | Loss: 0.00081228
Iteration 4/25 | Loss: 0.00078609
Iteration 5/25 | Loss: 0.00078130
Iteration 6/25 | Loss: 0.00078018
Iteration 7/25 | Loss: 0.00078004
Iteration 8/25 | Loss: 0.00078004
Iteration 9/25 | Loss: 0.00078004
Iteration 10/25 | Loss: 0.00078004
Iteration 11/25 | Loss: 0.00078004
Iteration 12/25 | Loss: 0.00078004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000780044007115066, 0.000780044007115066, 0.000780044007115066, 0.000780044007115066, 0.000780044007115066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000780044007115066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44203115
Iteration 2/25 | Loss: 0.00037489
Iteration 3/25 | Loss: 0.00037488
Iteration 4/25 | Loss: 0.00037488
Iteration 5/25 | Loss: 0.00037488
Iteration 6/25 | Loss: 0.00037488
Iteration 7/25 | Loss: 0.00037488
Iteration 8/25 | Loss: 0.00037488
Iteration 9/25 | Loss: 0.00037488
Iteration 10/25 | Loss: 0.00037488
Iteration 11/25 | Loss: 0.00037488
Iteration 12/25 | Loss: 0.00037488
Iteration 13/25 | Loss: 0.00037488
Iteration 14/25 | Loss: 0.00037488
Iteration 15/25 | Loss: 0.00037488
Iteration 16/25 | Loss: 0.00037488
Iteration 17/25 | Loss: 0.00037488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003748806775547564, 0.0003748806775547564, 0.0003748806775547564, 0.0003748806775547564, 0.0003748806775547564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003748806775547564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037488
Iteration 2/1000 | Loss: 0.00006249
Iteration 3/1000 | Loss: 0.00003998
Iteration 4/1000 | Loss: 0.00003648
Iteration 5/1000 | Loss: 0.00003459
Iteration 6/1000 | Loss: 0.00003342
Iteration 7/1000 | Loss: 0.00003256
Iteration 8/1000 | Loss: 0.00003191
Iteration 9/1000 | Loss: 0.00003152
Iteration 10/1000 | Loss: 0.00003124
Iteration 11/1000 | Loss: 0.00003102
Iteration 12/1000 | Loss: 0.00003081
Iteration 13/1000 | Loss: 0.00003077
Iteration 14/1000 | Loss: 0.00003076
Iteration 15/1000 | Loss: 0.00003076
Iteration 16/1000 | Loss: 0.00003075
Iteration 17/1000 | Loss: 0.00003074
Iteration 18/1000 | Loss: 0.00003073
Iteration 19/1000 | Loss: 0.00003072
Iteration 20/1000 | Loss: 0.00003070
Iteration 21/1000 | Loss: 0.00003069
Iteration 22/1000 | Loss: 0.00003069
Iteration 23/1000 | Loss: 0.00003063
Iteration 24/1000 | Loss: 0.00003063
Iteration 25/1000 | Loss: 0.00003060
Iteration 26/1000 | Loss: 0.00003056
Iteration 27/1000 | Loss: 0.00003056
Iteration 28/1000 | Loss: 0.00003056
Iteration 29/1000 | Loss: 0.00003046
Iteration 30/1000 | Loss: 0.00003042
Iteration 31/1000 | Loss: 0.00003042
Iteration 32/1000 | Loss: 0.00003041
Iteration 33/1000 | Loss: 0.00003041
Iteration 34/1000 | Loss: 0.00003041
Iteration 35/1000 | Loss: 0.00003041
Iteration 36/1000 | Loss: 0.00003041
Iteration 37/1000 | Loss: 0.00003041
Iteration 38/1000 | Loss: 0.00003041
Iteration 39/1000 | Loss: 0.00003041
Iteration 40/1000 | Loss: 0.00003041
Iteration 41/1000 | Loss: 0.00003038
Iteration 42/1000 | Loss: 0.00003038
Iteration 43/1000 | Loss: 0.00003037
Iteration 44/1000 | Loss: 0.00003036
Iteration 45/1000 | Loss: 0.00003036
Iteration 46/1000 | Loss: 0.00003036
Iteration 47/1000 | Loss: 0.00003035
Iteration 48/1000 | Loss: 0.00003035
Iteration 49/1000 | Loss: 0.00003035
Iteration 50/1000 | Loss: 0.00003034
Iteration 51/1000 | Loss: 0.00003034
Iteration 52/1000 | Loss: 0.00003034
Iteration 53/1000 | Loss: 0.00003033
Iteration 54/1000 | Loss: 0.00003033
Iteration 55/1000 | Loss: 0.00003033
Iteration 56/1000 | Loss: 0.00003033
Iteration 57/1000 | Loss: 0.00003032
Iteration 58/1000 | Loss: 0.00003032
Iteration 59/1000 | Loss: 0.00003032
Iteration 60/1000 | Loss: 0.00003031
Iteration 61/1000 | Loss: 0.00003031
Iteration 62/1000 | Loss: 0.00003030
Iteration 63/1000 | Loss: 0.00003030
Iteration 64/1000 | Loss: 0.00003030
Iteration 65/1000 | Loss: 0.00003030
Iteration 66/1000 | Loss: 0.00003030
Iteration 67/1000 | Loss: 0.00003030
Iteration 68/1000 | Loss: 0.00003030
Iteration 69/1000 | Loss: 0.00003030
Iteration 70/1000 | Loss: 0.00003029
Iteration 71/1000 | Loss: 0.00003029
Iteration 72/1000 | Loss: 0.00003029
Iteration 73/1000 | Loss: 0.00003029
Iteration 74/1000 | Loss: 0.00003029
Iteration 75/1000 | Loss: 0.00003028
Iteration 76/1000 | Loss: 0.00003028
Iteration 77/1000 | Loss: 0.00003028
Iteration 78/1000 | Loss: 0.00003028
Iteration 79/1000 | Loss: 0.00003027
Iteration 80/1000 | Loss: 0.00003027
Iteration 81/1000 | Loss: 0.00003027
Iteration 82/1000 | Loss: 0.00003027
Iteration 83/1000 | Loss: 0.00003027
Iteration 84/1000 | Loss: 0.00003026
Iteration 85/1000 | Loss: 0.00003026
Iteration 86/1000 | Loss: 0.00003026
Iteration 87/1000 | Loss: 0.00003025
Iteration 88/1000 | Loss: 0.00003025
Iteration 89/1000 | Loss: 0.00003025
Iteration 90/1000 | Loss: 0.00003024
Iteration 91/1000 | Loss: 0.00003024
Iteration 92/1000 | Loss: 0.00003024
Iteration 93/1000 | Loss: 0.00003024
Iteration 94/1000 | Loss: 0.00003024
Iteration 95/1000 | Loss: 0.00003024
Iteration 96/1000 | Loss: 0.00003024
Iteration 97/1000 | Loss: 0.00003023
Iteration 98/1000 | Loss: 0.00003023
Iteration 99/1000 | Loss: 0.00003023
Iteration 100/1000 | Loss: 0.00003023
Iteration 101/1000 | Loss: 0.00003023
Iteration 102/1000 | Loss: 0.00003023
Iteration 103/1000 | Loss: 0.00003023
Iteration 104/1000 | Loss: 0.00003023
Iteration 105/1000 | Loss: 0.00003023
Iteration 106/1000 | Loss: 0.00003023
Iteration 107/1000 | Loss: 0.00003023
Iteration 108/1000 | Loss: 0.00003023
Iteration 109/1000 | Loss: 0.00003023
Iteration 110/1000 | Loss: 0.00003023
Iteration 111/1000 | Loss: 0.00003022
Iteration 112/1000 | Loss: 0.00003022
Iteration 113/1000 | Loss: 0.00003022
Iteration 114/1000 | Loss: 0.00003022
Iteration 115/1000 | Loss: 0.00003022
Iteration 116/1000 | Loss: 0.00003022
Iteration 117/1000 | Loss: 0.00003022
Iteration 118/1000 | Loss: 0.00003022
Iteration 119/1000 | Loss: 0.00003022
Iteration 120/1000 | Loss: 0.00003022
Iteration 121/1000 | Loss: 0.00003022
Iteration 122/1000 | Loss: 0.00003022
Iteration 123/1000 | Loss: 0.00003022
Iteration 124/1000 | Loss: 0.00003022
Iteration 125/1000 | Loss: 0.00003022
Iteration 126/1000 | Loss: 0.00003022
Iteration 127/1000 | Loss: 0.00003022
Iteration 128/1000 | Loss: 0.00003022
Iteration 129/1000 | Loss: 0.00003021
Iteration 130/1000 | Loss: 0.00003021
Iteration 131/1000 | Loss: 0.00003021
Iteration 132/1000 | Loss: 0.00003021
Iteration 133/1000 | Loss: 0.00003021
Iteration 134/1000 | Loss: 0.00003021
Iteration 135/1000 | Loss: 0.00003021
Iteration 136/1000 | Loss: 0.00003021
Iteration 137/1000 | Loss: 0.00003021
Iteration 138/1000 | Loss: 0.00003021
Iteration 139/1000 | Loss: 0.00003021
Iteration 140/1000 | Loss: 0.00003021
Iteration 141/1000 | Loss: 0.00003021
Iteration 142/1000 | Loss: 0.00003021
Iteration 143/1000 | Loss: 0.00003021
Iteration 144/1000 | Loss: 0.00003021
Iteration 145/1000 | Loss: 0.00003021
Iteration 146/1000 | Loss: 0.00003021
Iteration 147/1000 | Loss: 0.00003021
Iteration 148/1000 | Loss: 0.00003021
Iteration 149/1000 | Loss: 0.00003021
Iteration 150/1000 | Loss: 0.00003021
Iteration 151/1000 | Loss: 0.00003021
Iteration 152/1000 | Loss: 0.00003020
Iteration 153/1000 | Loss: 0.00003020
Iteration 154/1000 | Loss: 0.00003020
Iteration 155/1000 | Loss: 0.00003020
Iteration 156/1000 | Loss: 0.00003020
Iteration 157/1000 | Loss: 0.00003020
Iteration 158/1000 | Loss: 0.00003020
Iteration 159/1000 | Loss: 0.00003020
Iteration 160/1000 | Loss: 0.00003020
Iteration 161/1000 | Loss: 0.00003020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.0202631023712456e-05, 3.0202631023712456e-05, 3.0202631023712456e-05, 3.0202631023712456e-05, 3.0202631023712456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0202631023712456e-05

Optimization complete. Final v2v error: 4.314379692077637 mm

Highest mean error: 5.445975303649902 mm for frame 79

Lowest mean error: 3.677372694015503 mm for frame 113

Saving results

Total time: 40.54800295829773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376579
Iteration 2/25 | Loss: 0.00095743
Iteration 3/25 | Loss: 0.00075261
Iteration 4/25 | Loss: 0.00070582
Iteration 5/25 | Loss: 0.00069699
Iteration 6/25 | Loss: 0.00069577
Iteration 7/25 | Loss: 0.00069529
Iteration 8/25 | Loss: 0.00069529
Iteration 9/25 | Loss: 0.00069529
Iteration 10/25 | Loss: 0.00069529
Iteration 11/25 | Loss: 0.00069529
Iteration 12/25 | Loss: 0.00069529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006952896947041154, 0.0006952896947041154, 0.0006952896947041154, 0.0006952896947041154, 0.0006952896947041154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006952896947041154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48963583
Iteration 2/25 | Loss: 0.00034375
Iteration 3/25 | Loss: 0.00034375
Iteration 4/25 | Loss: 0.00034375
Iteration 5/25 | Loss: 0.00034375
Iteration 6/25 | Loss: 0.00034375
Iteration 7/25 | Loss: 0.00034375
Iteration 8/25 | Loss: 0.00034375
Iteration 9/25 | Loss: 0.00034375
Iteration 10/25 | Loss: 0.00034375
Iteration 11/25 | Loss: 0.00034375
Iteration 12/25 | Loss: 0.00034375
Iteration 13/25 | Loss: 0.00034375
Iteration 14/25 | Loss: 0.00034375
Iteration 15/25 | Loss: 0.00034375
Iteration 16/25 | Loss: 0.00034375
Iteration 17/25 | Loss: 0.00034375
Iteration 18/25 | Loss: 0.00034375
Iteration 19/25 | Loss: 0.00034375
Iteration 20/25 | Loss: 0.00034375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003437478153500706, 0.0003437478153500706, 0.0003437478153500706, 0.0003437478153500706, 0.0003437478153500706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003437478153500706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034375
Iteration 2/1000 | Loss: 0.00003369
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00001946
Iteration 5/1000 | Loss: 0.00001841
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001658
Iteration 10/1000 | Loss: 0.00001633
Iteration 11/1000 | Loss: 0.00001621
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00001597
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001588
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001585
Iteration 18/1000 | Loss: 0.00001584
Iteration 19/1000 | Loss: 0.00001584
Iteration 20/1000 | Loss: 0.00001583
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001582
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001581
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001574
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001569
Iteration 56/1000 | Loss: 0.00001568
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001566
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001563
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001562
Iteration 81/1000 | Loss: 0.00001562
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001562
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001557
Iteration 123/1000 | Loss: 0.00001557
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001556
Iteration 130/1000 | Loss: 0.00001556
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001556
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001555
Iteration 135/1000 | Loss: 0.00001555
Iteration 136/1000 | Loss: 0.00001555
Iteration 137/1000 | Loss: 0.00001555
Iteration 138/1000 | Loss: 0.00001555
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001555
Iteration 142/1000 | Loss: 0.00001555
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.555317794554867e-05, 1.555317794554867e-05, 1.555317794554867e-05, 1.555317794554867e-05, 1.555317794554867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555317794554867e-05

Optimization complete. Final v2v error: 3.274369716644287 mm

Highest mean error: 3.640333414077759 mm for frame 99

Lowest mean error: 2.9316790103912354 mm for frame 131

Saving results

Total time: 44.277697801589966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774130
Iteration 2/25 | Loss: 0.00111955
Iteration 3/25 | Loss: 0.00085518
Iteration 4/25 | Loss: 0.00077465
Iteration 5/25 | Loss: 0.00075574
Iteration 6/25 | Loss: 0.00074217
Iteration 7/25 | Loss: 0.00075264
Iteration 8/25 | Loss: 0.00073821
Iteration 9/25 | Loss: 0.00073269
Iteration 10/25 | Loss: 0.00073092
Iteration 11/25 | Loss: 0.00073034
Iteration 12/25 | Loss: 0.00072948
Iteration 13/25 | Loss: 0.00072913
Iteration 14/25 | Loss: 0.00072889
Iteration 15/25 | Loss: 0.00072875
Iteration 16/25 | Loss: 0.00072865
Iteration 17/25 | Loss: 0.00072865
Iteration 18/25 | Loss: 0.00072865
Iteration 19/25 | Loss: 0.00072865
Iteration 20/25 | Loss: 0.00072865
Iteration 21/25 | Loss: 0.00072864
Iteration 22/25 | Loss: 0.00072864
Iteration 23/25 | Loss: 0.00072864
Iteration 24/25 | Loss: 0.00072864
Iteration 25/25 | Loss: 0.00072864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.07088280
Iteration 2/25 | Loss: 0.00036432
Iteration 3/25 | Loss: 0.00036424
Iteration 4/25 | Loss: 0.00036424
Iteration 5/25 | Loss: 0.00036424
Iteration 6/25 | Loss: 0.00036424
Iteration 7/25 | Loss: 0.00036424
Iteration 8/25 | Loss: 0.00036424
Iteration 9/25 | Loss: 0.00036424
Iteration 10/25 | Loss: 0.00036424
Iteration 11/25 | Loss: 0.00036424
Iteration 12/25 | Loss: 0.00036424
Iteration 13/25 | Loss: 0.00036424
Iteration 14/25 | Loss: 0.00036424
Iteration 15/25 | Loss: 0.00036424
Iteration 16/25 | Loss: 0.00036424
Iteration 17/25 | Loss: 0.00036424
Iteration 18/25 | Loss: 0.00036424
Iteration 19/25 | Loss: 0.00036424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00036424168501980603, 0.00036424168501980603, 0.00036424168501980603, 0.00036424168501980603, 0.00036424168501980603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036424168501980603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036424
Iteration 2/1000 | Loss: 0.00003122
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00001983
Iteration 5/1000 | Loss: 0.00006748
Iteration 6/1000 | Loss: 0.00001859
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001753
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001721
Iteration 12/1000 | Loss: 0.00001720
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00001715
Iteration 16/1000 | Loss: 0.00001713
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001699
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001694
Iteration 22/1000 | Loss: 0.00001693
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001692
Iteration 25/1000 | Loss: 0.00001691
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001689
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001681
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001675
Iteration 51/1000 | Loss: 0.00001675
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001673
Iteration 59/1000 | Loss: 0.00001673
Iteration 60/1000 | Loss: 0.00001673
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001668
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001664
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001662
Iteration 110/1000 | Loss: 0.00001662
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001658
Iteration 128/1000 | Loss: 0.00001658
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001658
Iteration 137/1000 | Loss: 0.00001658
Iteration 138/1000 | Loss: 0.00001658
Iteration 139/1000 | Loss: 0.00001658
Iteration 140/1000 | Loss: 0.00001658
Iteration 141/1000 | Loss: 0.00001658
Iteration 142/1000 | Loss: 0.00001658
Iteration 143/1000 | Loss: 0.00001658
Iteration 144/1000 | Loss: 0.00001658
Iteration 145/1000 | Loss: 0.00001658
Iteration 146/1000 | Loss: 0.00001658
Iteration 147/1000 | Loss: 0.00001658
Iteration 148/1000 | Loss: 0.00001658
Iteration 149/1000 | Loss: 0.00001658
Iteration 150/1000 | Loss: 0.00001658
Iteration 151/1000 | Loss: 0.00001658
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001658
Iteration 154/1000 | Loss: 0.00001658
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001658
Iteration 158/1000 | Loss: 0.00001658
Iteration 159/1000 | Loss: 0.00001658
Iteration 160/1000 | Loss: 0.00001658
Iteration 161/1000 | Loss: 0.00001658
Iteration 162/1000 | Loss: 0.00001658
Iteration 163/1000 | Loss: 0.00001658
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001658
Iteration 166/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.6576534108025953e-05, 1.6576534108025953e-05, 1.6576534108025953e-05, 1.6576534108025953e-05, 1.6576534108025953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6576534108025953e-05

Optimization complete. Final v2v error: 3.400963306427002 mm

Highest mean error: 4.275063991546631 mm for frame 138

Lowest mean error: 2.828460454940796 mm for frame 231

Saving results

Total time: 64.08812046051025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880725
Iteration 2/25 | Loss: 0.00123644
Iteration 3/25 | Loss: 0.00085921
Iteration 4/25 | Loss: 0.00079274
Iteration 5/25 | Loss: 0.00078522
Iteration 6/25 | Loss: 0.00078457
Iteration 7/25 | Loss: 0.00078457
Iteration 8/25 | Loss: 0.00078457
Iteration 9/25 | Loss: 0.00078457
Iteration 10/25 | Loss: 0.00078457
Iteration 11/25 | Loss: 0.00078457
Iteration 12/25 | Loss: 0.00078457
Iteration 13/25 | Loss: 0.00078457
Iteration 14/25 | Loss: 0.00078457
Iteration 15/25 | Loss: 0.00078457
Iteration 16/25 | Loss: 0.00078457
Iteration 17/25 | Loss: 0.00078457
Iteration 18/25 | Loss: 0.00078457
Iteration 19/25 | Loss: 0.00078457
Iteration 20/25 | Loss: 0.00078457
Iteration 21/25 | Loss: 0.00078457
Iteration 22/25 | Loss: 0.00078457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007845736690796912, 0.0007845736690796912, 0.0007845736690796912, 0.0007845736690796912, 0.0007845736690796912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007845736690796912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00043035
Iteration 2/25 | Loss: 0.00033260
Iteration 3/25 | Loss: 0.00033259
Iteration 4/25 | Loss: 0.00033259
Iteration 5/25 | Loss: 0.00033259
Iteration 6/25 | Loss: 0.00033259
Iteration 7/25 | Loss: 0.00033259
Iteration 8/25 | Loss: 0.00033259
Iteration 9/25 | Loss: 0.00033259
Iteration 10/25 | Loss: 0.00033259
Iteration 11/25 | Loss: 0.00033259
Iteration 12/25 | Loss: 0.00033259
Iteration 13/25 | Loss: 0.00033259
Iteration 14/25 | Loss: 0.00033259
Iteration 15/25 | Loss: 0.00033259
Iteration 16/25 | Loss: 0.00033259
Iteration 17/25 | Loss: 0.00033259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00033258675830438733, 0.00033258675830438733, 0.00033258675830438733, 0.00033258675830438733, 0.00033258675830438733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033258675830438733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033259
Iteration 2/1000 | Loss: 0.00003984
Iteration 3/1000 | Loss: 0.00003100
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002630
Iteration 6/1000 | Loss: 0.00002511
Iteration 7/1000 | Loss: 0.00002432
Iteration 8/1000 | Loss: 0.00002347
Iteration 9/1000 | Loss: 0.00002320
Iteration 10/1000 | Loss: 0.00002298
Iteration 11/1000 | Loss: 0.00002290
Iteration 12/1000 | Loss: 0.00002274
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002264
Iteration 15/1000 | Loss: 0.00002261
Iteration 16/1000 | Loss: 0.00002261
Iteration 17/1000 | Loss: 0.00002260
Iteration 18/1000 | Loss: 0.00002260
Iteration 19/1000 | Loss: 0.00002259
Iteration 20/1000 | Loss: 0.00002258
Iteration 21/1000 | Loss: 0.00002258
Iteration 22/1000 | Loss: 0.00002258
Iteration 23/1000 | Loss: 0.00002258
Iteration 24/1000 | Loss: 0.00002257
Iteration 25/1000 | Loss: 0.00002257
Iteration 26/1000 | Loss: 0.00002256
Iteration 27/1000 | Loss: 0.00002256
Iteration 28/1000 | Loss: 0.00002256
Iteration 29/1000 | Loss: 0.00002256
Iteration 30/1000 | Loss: 0.00002255
Iteration 31/1000 | Loss: 0.00002255
Iteration 32/1000 | Loss: 0.00002255
Iteration 33/1000 | Loss: 0.00002255
Iteration 34/1000 | Loss: 0.00002255
Iteration 35/1000 | Loss: 0.00002255
Iteration 36/1000 | Loss: 0.00002255
Iteration 37/1000 | Loss: 0.00002255
Iteration 38/1000 | Loss: 0.00002255
Iteration 39/1000 | Loss: 0.00002254
Iteration 40/1000 | Loss: 0.00002253
Iteration 41/1000 | Loss: 0.00002253
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002249
Iteration 44/1000 | Loss: 0.00002249
Iteration 45/1000 | Loss: 0.00002249
Iteration 46/1000 | Loss: 0.00002249
Iteration 47/1000 | Loss: 0.00002249
Iteration 48/1000 | Loss: 0.00002249
Iteration 49/1000 | Loss: 0.00002248
Iteration 50/1000 | Loss: 0.00002248
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002246
Iteration 53/1000 | Loss: 0.00002246
Iteration 54/1000 | Loss: 0.00002245
Iteration 55/1000 | Loss: 0.00002245
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00002244
Iteration 59/1000 | Loss: 0.00002244
Iteration 60/1000 | Loss: 0.00002243
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002243
Iteration 63/1000 | Loss: 0.00002242
Iteration 64/1000 | Loss: 0.00002242
Iteration 65/1000 | Loss: 0.00002242
Iteration 66/1000 | Loss: 0.00002242
Iteration 67/1000 | Loss: 0.00002242
Iteration 68/1000 | Loss: 0.00002242
Iteration 69/1000 | Loss: 0.00002242
Iteration 70/1000 | Loss: 0.00002242
Iteration 71/1000 | Loss: 0.00002241
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00002241
Iteration 75/1000 | Loss: 0.00002241
Iteration 76/1000 | Loss: 0.00002241
Iteration 77/1000 | Loss: 0.00002241
Iteration 78/1000 | Loss: 0.00002241
Iteration 79/1000 | Loss: 0.00002241
Iteration 80/1000 | Loss: 0.00002241
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00002241
Iteration 84/1000 | Loss: 0.00002241
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002241
Iteration 87/1000 | Loss: 0.00002241
Iteration 88/1000 | Loss: 0.00002241
Iteration 89/1000 | Loss: 0.00002241
Iteration 90/1000 | Loss: 0.00002241
Iteration 91/1000 | Loss: 0.00002241
Iteration 92/1000 | Loss: 0.00002241
Iteration 93/1000 | Loss: 0.00002241
Iteration 94/1000 | Loss: 0.00002241
Iteration 95/1000 | Loss: 0.00002241
Iteration 96/1000 | Loss: 0.00002241
Iteration 97/1000 | Loss: 0.00002241
Iteration 98/1000 | Loss: 0.00002241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.2413756596506573e-05, 2.2413756596506573e-05, 2.2413756596506573e-05, 2.2413756596506573e-05, 2.2413756596506573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2413756596506573e-05

Optimization complete. Final v2v error: 4.016663551330566 mm

Highest mean error: 4.661883354187012 mm for frame 0

Lowest mean error: 3.8803493976593018 mm for frame 29

Saving results

Total time: 31.29535722732544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375522
Iteration 2/25 | Loss: 0.00086706
Iteration 3/25 | Loss: 0.00074582
Iteration 4/25 | Loss: 0.00071197
Iteration 5/25 | Loss: 0.00070210
Iteration 6/25 | Loss: 0.00069929
Iteration 7/25 | Loss: 0.00069890
Iteration 8/25 | Loss: 0.00069890
Iteration 9/25 | Loss: 0.00069890
Iteration 10/25 | Loss: 0.00069890
Iteration 11/25 | Loss: 0.00069890
Iteration 12/25 | Loss: 0.00069890
Iteration 13/25 | Loss: 0.00069890
Iteration 14/25 | Loss: 0.00069890
Iteration 15/25 | Loss: 0.00069890
Iteration 16/25 | Loss: 0.00069890
Iteration 17/25 | Loss: 0.00069890
Iteration 18/25 | Loss: 0.00069890
Iteration 19/25 | Loss: 0.00069890
Iteration 20/25 | Loss: 0.00069890
Iteration 21/25 | Loss: 0.00069890
Iteration 22/25 | Loss: 0.00069890
Iteration 23/25 | Loss: 0.00069890
Iteration 24/25 | Loss: 0.00069890
Iteration 25/25 | Loss: 0.00069890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92858869
Iteration 2/25 | Loss: 0.00028610
Iteration 3/25 | Loss: 0.00028610
Iteration 4/25 | Loss: 0.00028610
Iteration 5/25 | Loss: 0.00028610
Iteration 6/25 | Loss: 0.00028609
Iteration 7/25 | Loss: 0.00028609
Iteration 8/25 | Loss: 0.00028609
Iteration 9/25 | Loss: 0.00028609
Iteration 10/25 | Loss: 0.00028609
Iteration 11/25 | Loss: 0.00028609
Iteration 12/25 | Loss: 0.00028609
Iteration 13/25 | Loss: 0.00028609
Iteration 14/25 | Loss: 0.00028609
Iteration 15/25 | Loss: 0.00028609
Iteration 16/25 | Loss: 0.00028609
Iteration 17/25 | Loss: 0.00028609
Iteration 18/25 | Loss: 0.00028609
Iteration 19/25 | Loss: 0.00028609
Iteration 20/25 | Loss: 0.00028609
Iteration 21/25 | Loss: 0.00028609
Iteration 22/25 | Loss: 0.00028609
Iteration 23/25 | Loss: 0.00028609
Iteration 24/25 | Loss: 0.00028609
Iteration 25/25 | Loss: 0.00028609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028609
Iteration 2/1000 | Loss: 0.00002663
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001661
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001542
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001499
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001495
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001486
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001479
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001478
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001478
Iteration 31/1000 | Loss: 0.00001477
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001443
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001442
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001440
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001423
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001419
Iteration 79/1000 | Loss: 0.00001419
Iteration 80/1000 | Loss: 0.00001419
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001418
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001413
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001408
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001407
Iteration 108/1000 | Loss: 0.00001407
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001406
Iteration 111/1000 | Loss: 0.00001406
Iteration 112/1000 | Loss: 0.00001406
Iteration 113/1000 | Loss: 0.00001406
Iteration 114/1000 | Loss: 0.00001405
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001405
Iteration 117/1000 | Loss: 0.00001405
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001403
Iteration 129/1000 | Loss: 0.00001403
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001403
Iteration 133/1000 | Loss: 0.00001403
Iteration 134/1000 | Loss: 0.00001403
Iteration 135/1000 | Loss: 0.00001403
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001402
Iteration 143/1000 | Loss: 0.00001402
Iteration 144/1000 | Loss: 0.00001402
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001401
Iteration 147/1000 | Loss: 0.00001401
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001399
Iteration 154/1000 | Loss: 0.00001399
Iteration 155/1000 | Loss: 0.00001399
Iteration 156/1000 | Loss: 0.00001399
Iteration 157/1000 | Loss: 0.00001399
Iteration 158/1000 | Loss: 0.00001399
Iteration 159/1000 | Loss: 0.00001399
Iteration 160/1000 | Loss: 0.00001398
Iteration 161/1000 | Loss: 0.00001398
Iteration 162/1000 | Loss: 0.00001398
Iteration 163/1000 | Loss: 0.00001398
Iteration 164/1000 | Loss: 0.00001398
Iteration 165/1000 | Loss: 0.00001398
Iteration 166/1000 | Loss: 0.00001398
Iteration 167/1000 | Loss: 0.00001398
Iteration 168/1000 | Loss: 0.00001397
Iteration 169/1000 | Loss: 0.00001397
Iteration 170/1000 | Loss: 0.00001397
Iteration 171/1000 | Loss: 0.00001397
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001396
Iteration 175/1000 | Loss: 0.00001396
Iteration 176/1000 | Loss: 0.00001396
Iteration 177/1000 | Loss: 0.00001396
Iteration 178/1000 | Loss: 0.00001396
Iteration 179/1000 | Loss: 0.00001396
Iteration 180/1000 | Loss: 0.00001396
Iteration 181/1000 | Loss: 0.00001396
Iteration 182/1000 | Loss: 0.00001396
Iteration 183/1000 | Loss: 0.00001396
Iteration 184/1000 | Loss: 0.00001395
Iteration 185/1000 | Loss: 0.00001395
Iteration 186/1000 | Loss: 0.00001395
Iteration 187/1000 | Loss: 0.00001395
Iteration 188/1000 | Loss: 0.00001395
Iteration 189/1000 | Loss: 0.00001395
Iteration 190/1000 | Loss: 0.00001395
Iteration 191/1000 | Loss: 0.00001395
Iteration 192/1000 | Loss: 0.00001395
Iteration 193/1000 | Loss: 0.00001394
Iteration 194/1000 | Loss: 0.00001394
Iteration 195/1000 | Loss: 0.00001394
Iteration 196/1000 | Loss: 0.00001394
Iteration 197/1000 | Loss: 0.00001394
Iteration 198/1000 | Loss: 0.00001394
Iteration 199/1000 | Loss: 0.00001394
Iteration 200/1000 | Loss: 0.00001394
Iteration 201/1000 | Loss: 0.00001394
Iteration 202/1000 | Loss: 0.00001394
Iteration 203/1000 | Loss: 0.00001393
Iteration 204/1000 | Loss: 0.00001393
Iteration 205/1000 | Loss: 0.00001393
Iteration 206/1000 | Loss: 0.00001393
Iteration 207/1000 | Loss: 0.00001393
Iteration 208/1000 | Loss: 0.00001393
Iteration 209/1000 | Loss: 0.00001393
Iteration 210/1000 | Loss: 0.00001393
Iteration 211/1000 | Loss: 0.00001393
Iteration 212/1000 | Loss: 0.00001393
Iteration 213/1000 | Loss: 0.00001393
Iteration 214/1000 | Loss: 0.00001393
Iteration 215/1000 | Loss: 0.00001393
Iteration 216/1000 | Loss: 0.00001393
Iteration 217/1000 | Loss: 0.00001393
Iteration 218/1000 | Loss: 0.00001393
Iteration 219/1000 | Loss: 0.00001393
Iteration 220/1000 | Loss: 0.00001393
Iteration 221/1000 | Loss: 0.00001393
Iteration 222/1000 | Loss: 0.00001393
Iteration 223/1000 | Loss: 0.00001393
Iteration 224/1000 | Loss: 0.00001393
Iteration 225/1000 | Loss: 0.00001393
Iteration 226/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3931624380347785e-05, 1.3931624380347785e-05, 1.3931624380347785e-05, 1.3931624380347785e-05, 1.3931624380347785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3931624380347785e-05

Optimization complete. Final v2v error: 3.1545963287353516 mm

Highest mean error: 3.25502610206604 mm for frame 8

Lowest mean error: 3.088941812515259 mm for frame 178

Saving results

Total time: 44.20223689079285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00354719
Iteration 2/25 | Loss: 0.00106055
Iteration 3/25 | Loss: 0.00076570
Iteration 4/25 | Loss: 0.00070626
Iteration 5/25 | Loss: 0.00069278
Iteration 6/25 | Loss: 0.00068965
Iteration 7/25 | Loss: 0.00068875
Iteration 8/25 | Loss: 0.00068867
Iteration 9/25 | Loss: 0.00068867
Iteration 10/25 | Loss: 0.00068867
Iteration 11/25 | Loss: 0.00068867
Iteration 12/25 | Loss: 0.00068867
Iteration 13/25 | Loss: 0.00068867
Iteration 14/25 | Loss: 0.00068867
Iteration 15/25 | Loss: 0.00068867
Iteration 16/25 | Loss: 0.00068867
Iteration 17/25 | Loss: 0.00068867
Iteration 18/25 | Loss: 0.00068867
Iteration 19/25 | Loss: 0.00068867
Iteration 20/25 | Loss: 0.00068867
Iteration 21/25 | Loss: 0.00068867
Iteration 22/25 | Loss: 0.00068867
Iteration 23/25 | Loss: 0.00068867
Iteration 24/25 | Loss: 0.00068867
Iteration 25/25 | Loss: 0.00068867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55902636
Iteration 2/25 | Loss: 0.00037711
Iteration 3/25 | Loss: 0.00037711
Iteration 4/25 | Loss: 0.00037711
Iteration 5/25 | Loss: 0.00037711
Iteration 6/25 | Loss: 0.00037711
Iteration 7/25 | Loss: 0.00037711
Iteration 8/25 | Loss: 0.00037711
Iteration 9/25 | Loss: 0.00037711
Iteration 10/25 | Loss: 0.00037711
Iteration 11/25 | Loss: 0.00037711
Iteration 12/25 | Loss: 0.00037711
Iteration 13/25 | Loss: 0.00037711
Iteration 14/25 | Loss: 0.00037711
Iteration 15/25 | Loss: 0.00037711
Iteration 16/25 | Loss: 0.00037711
Iteration 17/25 | Loss: 0.00037711
Iteration 18/25 | Loss: 0.00037711
Iteration 19/25 | Loss: 0.00037711
Iteration 20/25 | Loss: 0.00037711
Iteration 21/25 | Loss: 0.00037711
Iteration 22/25 | Loss: 0.00037711
Iteration 23/25 | Loss: 0.00037711
Iteration 24/25 | Loss: 0.00037711
Iteration 25/25 | Loss: 0.00037711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037711
Iteration 2/1000 | Loss: 0.00002449
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001453
Iteration 5/1000 | Loss: 0.00001368
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001283
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001249
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001232
Iteration 13/1000 | Loss: 0.00001231
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001188
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001180
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001179
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001177
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001174
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001170
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001167
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001166
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001166
Iteration 94/1000 | Loss: 0.00001166
Iteration 95/1000 | Loss: 0.00001166
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001166
Iteration 101/1000 | Loss: 0.00001165
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001164
Iteration 107/1000 | Loss: 0.00001164
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001163
Iteration 110/1000 | Loss: 0.00001163
Iteration 111/1000 | Loss: 0.00001163
Iteration 112/1000 | Loss: 0.00001163
Iteration 113/1000 | Loss: 0.00001163
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001162
Iteration 117/1000 | Loss: 0.00001162
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001161
Iteration 122/1000 | Loss: 0.00001161
Iteration 123/1000 | Loss: 0.00001161
Iteration 124/1000 | Loss: 0.00001161
Iteration 125/1000 | Loss: 0.00001161
Iteration 126/1000 | Loss: 0.00001161
Iteration 127/1000 | Loss: 0.00001161
Iteration 128/1000 | Loss: 0.00001161
Iteration 129/1000 | Loss: 0.00001161
Iteration 130/1000 | Loss: 0.00001161
Iteration 131/1000 | Loss: 0.00001161
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001161
Iteration 137/1000 | Loss: 0.00001161
Iteration 138/1000 | Loss: 0.00001161
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.1606908628891688e-05, 1.1606908628891688e-05, 1.1606908628891688e-05, 1.1606908628891688e-05, 1.1606908628891688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1606908628891688e-05

Optimization complete. Final v2v error: 2.9566221237182617 mm

Highest mean error: 3.1270711421966553 mm for frame 35

Lowest mean error: 2.7867703437805176 mm for frame 5

Saving results

Total time: 37.422497034072876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014863
Iteration 2/25 | Loss: 0.01014863
Iteration 3/25 | Loss: 0.01014862
Iteration 4/25 | Loss: 0.01014862
Iteration 5/25 | Loss: 0.01014862
Iteration 6/25 | Loss: 0.01014861
Iteration 7/25 | Loss: 0.01014861
Iteration 8/25 | Loss: 0.01014861
Iteration 9/25 | Loss: 0.01014861
Iteration 10/25 | Loss: 0.01014861
Iteration 11/25 | Loss: 0.01014860
Iteration 12/25 | Loss: 0.01014860
Iteration 13/25 | Loss: 0.01014860
Iteration 14/25 | Loss: 0.01014860
Iteration 15/25 | Loss: 0.01014859
Iteration 16/25 | Loss: 0.01014859
Iteration 17/25 | Loss: 0.01014859
Iteration 18/25 | Loss: 0.01014858
Iteration 19/25 | Loss: 0.01014858
Iteration 20/25 | Loss: 0.01014858
Iteration 21/25 | Loss: 0.01014857
Iteration 22/25 | Loss: 0.01014857
Iteration 23/25 | Loss: 0.01014857
Iteration 24/25 | Loss: 0.01014857
Iteration 25/25 | Loss: 0.01014857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73091364
Iteration 2/25 | Loss: 0.17431229
Iteration 3/25 | Loss: 0.17431207
Iteration 4/25 | Loss: 0.17431203
Iteration 5/25 | Loss: 0.17431200
Iteration 6/25 | Loss: 0.17431200
Iteration 7/25 | Loss: 0.17431195
Iteration 8/25 | Loss: 0.17431195
Iteration 9/25 | Loss: 0.17431195
Iteration 10/25 | Loss: 0.17431195
Iteration 11/25 | Loss: 0.17431195
Iteration 12/25 | Loss: 0.17431195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.17431195080280304, 0.17431195080280304, 0.17431195080280304, 0.17431195080280304, 0.17431195080280304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17431195080280304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17431195
Iteration 2/1000 | Loss: 0.01269604
Iteration 3/1000 | Loss: 0.00716857
Iteration 4/1000 | Loss: 0.00840154
Iteration 5/1000 | Loss: 0.00279504
Iteration 6/1000 | Loss: 0.00082519
Iteration 7/1000 | Loss: 0.00052640
Iteration 8/1000 | Loss: 0.00182176
Iteration 9/1000 | Loss: 0.00230630
Iteration 10/1000 | Loss: 0.00034749
Iteration 11/1000 | Loss: 0.00107120
Iteration 12/1000 | Loss: 0.00069169
Iteration 13/1000 | Loss: 0.00039451
Iteration 14/1000 | Loss: 0.00039831
Iteration 15/1000 | Loss: 0.00143426
Iteration 16/1000 | Loss: 0.00013768
Iteration 17/1000 | Loss: 0.00028077
Iteration 18/1000 | Loss: 0.00017870
Iteration 19/1000 | Loss: 0.00053306
Iteration 20/1000 | Loss: 0.00007540
Iteration 21/1000 | Loss: 0.00025689
Iteration 22/1000 | Loss: 0.00004141
Iteration 23/1000 | Loss: 0.00094556
Iteration 24/1000 | Loss: 0.00156592
Iteration 25/1000 | Loss: 0.00005739
Iteration 26/1000 | Loss: 0.00021931
Iteration 27/1000 | Loss: 0.00070004
Iteration 28/1000 | Loss: 0.00141732
Iteration 29/1000 | Loss: 0.00036132
Iteration 30/1000 | Loss: 0.00198950
Iteration 31/1000 | Loss: 0.00055975
Iteration 32/1000 | Loss: 0.00149636
Iteration 33/1000 | Loss: 0.00199591
Iteration 34/1000 | Loss: 0.00081792
Iteration 35/1000 | Loss: 0.00059298
Iteration 36/1000 | Loss: 0.00003660
Iteration 37/1000 | Loss: 0.00005626
Iteration 38/1000 | Loss: 0.00002873
Iteration 39/1000 | Loss: 0.00004175
Iteration 40/1000 | Loss: 0.00012268
Iteration 41/1000 | Loss: 0.00002595
Iteration 42/1000 | Loss: 0.00014820
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00012555
Iteration 45/1000 | Loss: 0.00002409
Iteration 46/1000 | Loss: 0.00002340
Iteration 47/1000 | Loss: 0.00002284
Iteration 48/1000 | Loss: 0.00013537
Iteration 49/1000 | Loss: 0.00018628
Iteration 50/1000 | Loss: 0.00002452
Iteration 51/1000 | Loss: 0.00002239
Iteration 52/1000 | Loss: 0.00002166
Iteration 53/1000 | Loss: 0.00002133
Iteration 54/1000 | Loss: 0.00002106
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00009143
Iteration 57/1000 | Loss: 0.00002089
Iteration 58/1000 | Loss: 0.00005015
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002934
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002064
Iteration 63/1000 | Loss: 0.00002064
Iteration 64/1000 | Loss: 0.00002063
Iteration 65/1000 | Loss: 0.00002058
Iteration 66/1000 | Loss: 0.00002058
Iteration 67/1000 | Loss: 0.00002058
Iteration 68/1000 | Loss: 0.00002058
Iteration 69/1000 | Loss: 0.00002058
Iteration 70/1000 | Loss: 0.00002058
Iteration 71/1000 | Loss: 0.00002056
Iteration 72/1000 | Loss: 0.00002056
Iteration 73/1000 | Loss: 0.00002055
Iteration 74/1000 | Loss: 0.00002055
Iteration 75/1000 | Loss: 0.00002055
Iteration 76/1000 | Loss: 0.00002055
Iteration 77/1000 | Loss: 0.00002055
Iteration 78/1000 | Loss: 0.00002055
Iteration 79/1000 | Loss: 0.00002055
Iteration 80/1000 | Loss: 0.00002055
Iteration 81/1000 | Loss: 0.00002055
Iteration 82/1000 | Loss: 0.00002055
Iteration 83/1000 | Loss: 0.00002055
Iteration 84/1000 | Loss: 0.00002055
Iteration 85/1000 | Loss: 0.00002055
Iteration 86/1000 | Loss: 0.00002054
Iteration 87/1000 | Loss: 0.00002054
Iteration 88/1000 | Loss: 0.00002054
Iteration 89/1000 | Loss: 0.00002053
Iteration 90/1000 | Loss: 0.00002053
Iteration 91/1000 | Loss: 0.00002053
Iteration 92/1000 | Loss: 0.00002053
Iteration 93/1000 | Loss: 0.00002053
Iteration 94/1000 | Loss: 0.00002053
Iteration 95/1000 | Loss: 0.00002053
Iteration 96/1000 | Loss: 0.00002053
Iteration 97/1000 | Loss: 0.00002053
Iteration 98/1000 | Loss: 0.00002053
Iteration 99/1000 | Loss: 0.00002053
Iteration 100/1000 | Loss: 0.00002053
Iteration 101/1000 | Loss: 0.00002053
Iteration 102/1000 | Loss: 0.00002053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.0531422705971636e-05, 2.0531422705971636e-05, 2.0531422705971636e-05, 2.0531422705971636e-05, 2.0531422705971636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0531422705971636e-05

Optimization complete. Final v2v error: 3.754678249359131 mm

Highest mean error: 4.086430072784424 mm for frame 11

Lowest mean error: 3.5024311542510986 mm for frame 177

Saving results

Total time: 109.91370582580566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086537
Iteration 2/25 | Loss: 0.00188132
Iteration 3/25 | Loss: 0.00149681
Iteration 4/25 | Loss: 0.00127052
Iteration 5/25 | Loss: 0.00108721
Iteration 6/25 | Loss: 0.00111316
Iteration 7/25 | Loss: 0.00097421
Iteration 8/25 | Loss: 0.00093903
Iteration 9/25 | Loss: 0.00090680
Iteration 10/25 | Loss: 0.00094147
Iteration 11/25 | Loss: 0.00089151
Iteration 12/25 | Loss: 0.00084518
Iteration 13/25 | Loss: 0.00083925
Iteration 14/25 | Loss: 0.00080114
Iteration 15/25 | Loss: 0.00078865
Iteration 16/25 | Loss: 0.00079121
Iteration 17/25 | Loss: 0.00082757
Iteration 18/25 | Loss: 0.00079399
Iteration 19/25 | Loss: 0.00077966
Iteration 20/25 | Loss: 0.00077392
Iteration 21/25 | Loss: 0.00077789
Iteration 22/25 | Loss: 0.00077700
Iteration 23/25 | Loss: 0.00078110
Iteration 24/25 | Loss: 0.00077506
Iteration 25/25 | Loss: 0.00077047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48447394
Iteration 2/25 | Loss: 0.00114344
Iteration 3/25 | Loss: 0.00105537
Iteration 4/25 | Loss: 0.00105537
Iteration 5/25 | Loss: 0.00105537
Iteration 6/25 | Loss: 0.00105537
Iteration 7/25 | Loss: 0.00105537
Iteration 8/25 | Loss: 0.00105537
Iteration 9/25 | Loss: 0.00105537
Iteration 10/25 | Loss: 0.00105537
Iteration 11/25 | Loss: 0.00105537
Iteration 12/25 | Loss: 0.00105537
Iteration 13/25 | Loss: 0.00105537
Iteration 14/25 | Loss: 0.00105537
Iteration 15/25 | Loss: 0.00105537
Iteration 16/25 | Loss: 0.00105537
Iteration 17/25 | Loss: 0.00105537
Iteration 18/25 | Loss: 0.00105537
Iteration 19/25 | Loss: 0.00105537
Iteration 20/25 | Loss: 0.00105537
Iteration 21/25 | Loss: 0.00105537
Iteration 22/25 | Loss: 0.00105537
Iteration 23/25 | Loss: 0.00105537
Iteration 24/25 | Loss: 0.00105537
Iteration 25/25 | Loss: 0.00105537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105537
Iteration 2/1000 | Loss: 0.00128442
Iteration 3/1000 | Loss: 0.00011835
Iteration 4/1000 | Loss: 0.00227419
Iteration 5/1000 | Loss: 0.00028930
Iteration 6/1000 | Loss: 0.00010100
Iteration 7/1000 | Loss: 0.00005709
Iteration 8/1000 | Loss: 0.00004758
Iteration 9/1000 | Loss: 0.00134781
Iteration 10/1000 | Loss: 0.00018976
Iteration 11/1000 | Loss: 0.00114120
Iteration 12/1000 | Loss: 0.00095889
Iteration 13/1000 | Loss: 0.00148980
Iteration 14/1000 | Loss: 0.00314499
Iteration 15/1000 | Loss: 0.00172368
Iteration 16/1000 | Loss: 0.00160335
Iteration 17/1000 | Loss: 0.00021876
Iteration 18/1000 | Loss: 0.00150128
Iteration 19/1000 | Loss: 0.00099110
Iteration 20/1000 | Loss: 0.00184695
Iteration 21/1000 | Loss: 0.00120907
Iteration 22/1000 | Loss: 0.00166597
Iteration 23/1000 | Loss: 0.00088686
Iteration 24/1000 | Loss: 0.00167399
Iteration 25/1000 | Loss: 0.00110032
Iteration 26/1000 | Loss: 0.00076959
Iteration 27/1000 | Loss: 0.00124180
Iteration 28/1000 | Loss: 0.00132815
Iteration 29/1000 | Loss: 0.00146128
Iteration 30/1000 | Loss: 0.00210359
Iteration 31/1000 | Loss: 0.00127058
Iteration 32/1000 | Loss: 0.00023813
Iteration 33/1000 | Loss: 0.00009902
Iteration 34/1000 | Loss: 0.00147300
Iteration 35/1000 | Loss: 0.00176862
Iteration 36/1000 | Loss: 0.00107709
Iteration 37/1000 | Loss: 0.00172202
Iteration 38/1000 | Loss: 0.00120878
Iteration 39/1000 | Loss: 0.00112011
Iteration 40/1000 | Loss: 0.00184285
Iteration 41/1000 | Loss: 0.00122259
Iteration 42/1000 | Loss: 0.00228559
Iteration 43/1000 | Loss: 0.00256389
Iteration 44/1000 | Loss: 0.00205946
Iteration 45/1000 | Loss: 0.00024365
Iteration 46/1000 | Loss: 0.00024038
Iteration 47/1000 | Loss: 0.00016303
Iteration 48/1000 | Loss: 0.00004724
Iteration 49/1000 | Loss: 0.00005963
Iteration 50/1000 | Loss: 0.00146406
Iteration 51/1000 | Loss: 0.00099873
Iteration 52/1000 | Loss: 0.00158786
Iteration 53/1000 | Loss: 0.00289471
Iteration 54/1000 | Loss: 0.00185114
Iteration 55/1000 | Loss: 0.00217469
Iteration 56/1000 | Loss: 0.00113247
Iteration 57/1000 | Loss: 0.00173818
Iteration 58/1000 | Loss: 0.00026410
Iteration 59/1000 | Loss: 0.00161694
Iteration 60/1000 | Loss: 0.00100509
Iteration 61/1000 | Loss: 0.00081186
Iteration 62/1000 | Loss: 0.00109311
Iteration 63/1000 | Loss: 0.00131611
Iteration 64/1000 | Loss: 0.00206606
Iteration 65/1000 | Loss: 0.00025828
Iteration 66/1000 | Loss: 0.00078233
Iteration 67/1000 | Loss: 0.00218267
Iteration 68/1000 | Loss: 0.00059731
Iteration 69/1000 | Loss: 0.00047428
Iteration 70/1000 | Loss: 0.00025162
Iteration 71/1000 | Loss: 0.00030367
Iteration 72/1000 | Loss: 0.00098770
Iteration 73/1000 | Loss: 0.00121032
Iteration 74/1000 | Loss: 0.00190644
Iteration 75/1000 | Loss: 0.00302366
Iteration 76/1000 | Loss: 0.00183491
Iteration 77/1000 | Loss: 0.00202885
Iteration 78/1000 | Loss: 0.00139988
Iteration 79/1000 | Loss: 0.00019524
Iteration 80/1000 | Loss: 0.00115789
Iteration 81/1000 | Loss: 0.00153761
Iteration 82/1000 | Loss: 0.00015995
Iteration 83/1000 | Loss: 0.00133451
Iteration 84/1000 | Loss: 0.00105387
Iteration 85/1000 | Loss: 0.00213220
Iteration 86/1000 | Loss: 0.00108121
Iteration 87/1000 | Loss: 0.00140980
Iteration 88/1000 | Loss: 0.00140877
Iteration 89/1000 | Loss: 0.00247392
Iteration 90/1000 | Loss: 0.00267968
Iteration 91/1000 | Loss: 0.00087048
Iteration 92/1000 | Loss: 0.00184489
Iteration 93/1000 | Loss: 0.00060822
Iteration 94/1000 | Loss: 0.00231409
Iteration 95/1000 | Loss: 0.00124719
Iteration 96/1000 | Loss: 0.00211116
Iteration 97/1000 | Loss: 0.00226414
Iteration 98/1000 | Loss: 0.00203687
Iteration 99/1000 | Loss: 0.00007588
Iteration 100/1000 | Loss: 0.00004969
Iteration 101/1000 | Loss: 0.00061162
Iteration 102/1000 | Loss: 0.00048916
Iteration 103/1000 | Loss: 0.00060794
Iteration 104/1000 | Loss: 0.00035555
Iteration 105/1000 | Loss: 0.00005667
Iteration 106/1000 | Loss: 0.00004485
Iteration 107/1000 | Loss: 0.00004191
Iteration 108/1000 | Loss: 0.00002882
Iteration 109/1000 | Loss: 0.00002310
Iteration 110/1000 | Loss: 0.00033576
Iteration 111/1000 | Loss: 0.00003672
Iteration 112/1000 | Loss: 0.00002830
Iteration 113/1000 | Loss: 0.00003076
Iteration 114/1000 | Loss: 0.00002839
Iteration 115/1000 | Loss: 0.00002198
Iteration 116/1000 | Loss: 0.00002189
Iteration 117/1000 | Loss: 0.00001922
Iteration 118/1000 | Loss: 0.00004878
Iteration 119/1000 | Loss: 0.00002502
Iteration 120/1000 | Loss: 0.00001788
Iteration 121/1000 | Loss: 0.00003351
Iteration 122/1000 | Loss: 0.00001727
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00003347
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001697
Iteration 130/1000 | Loss: 0.00002715
Iteration 131/1000 | Loss: 0.00003123
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001679
Iteration 141/1000 | Loss: 0.00001679
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001676
Iteration 144/1000 | Loss: 0.00001676
Iteration 145/1000 | Loss: 0.00001674
Iteration 146/1000 | Loss: 0.00001674
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001672
Iteration 149/1000 | Loss: 0.00001672
Iteration 150/1000 | Loss: 0.00001672
Iteration 151/1000 | Loss: 0.00001671
Iteration 152/1000 | Loss: 0.00001671
Iteration 153/1000 | Loss: 0.00001671
Iteration 154/1000 | Loss: 0.00001671
Iteration 155/1000 | Loss: 0.00001671
Iteration 156/1000 | Loss: 0.00001671
Iteration 157/1000 | Loss: 0.00001671
Iteration 158/1000 | Loss: 0.00001671
Iteration 159/1000 | Loss: 0.00001670
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001670
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001670
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001668
Iteration 167/1000 | Loss: 0.00001668
Iteration 168/1000 | Loss: 0.00001668
Iteration 169/1000 | Loss: 0.00001668
Iteration 170/1000 | Loss: 0.00001667
Iteration 171/1000 | Loss: 0.00001667
Iteration 172/1000 | Loss: 0.00001667
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Iteration 175/1000 | Loss: 0.00001666
Iteration 176/1000 | Loss: 0.00001665
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001664
Iteration 180/1000 | Loss: 0.00001664
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001664
Iteration 184/1000 | Loss: 0.00001664
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001663
Iteration 187/1000 | Loss: 0.00001663
Iteration 188/1000 | Loss: 0.00001663
Iteration 189/1000 | Loss: 0.00001663
Iteration 190/1000 | Loss: 0.00001662
Iteration 191/1000 | Loss: 0.00001662
Iteration 192/1000 | Loss: 0.00001662
Iteration 193/1000 | Loss: 0.00001662
Iteration 194/1000 | Loss: 0.00001662
Iteration 195/1000 | Loss: 0.00001662
Iteration 196/1000 | Loss: 0.00001662
Iteration 197/1000 | Loss: 0.00001662
Iteration 198/1000 | Loss: 0.00001661
Iteration 199/1000 | Loss: 0.00001661
Iteration 200/1000 | Loss: 0.00001661
Iteration 201/1000 | Loss: 0.00001661
Iteration 202/1000 | Loss: 0.00001661
Iteration 203/1000 | Loss: 0.00001661
Iteration 204/1000 | Loss: 0.00001661
Iteration 205/1000 | Loss: 0.00001661
Iteration 206/1000 | Loss: 0.00001661
Iteration 207/1000 | Loss: 0.00001661
Iteration 208/1000 | Loss: 0.00001661
Iteration 209/1000 | Loss: 0.00001660
Iteration 210/1000 | Loss: 0.00001660
Iteration 211/1000 | Loss: 0.00001660
Iteration 212/1000 | Loss: 0.00001660
Iteration 213/1000 | Loss: 0.00001660
Iteration 214/1000 | Loss: 0.00001660
Iteration 215/1000 | Loss: 0.00001660
Iteration 216/1000 | Loss: 0.00001660
Iteration 217/1000 | Loss: 0.00001660
Iteration 218/1000 | Loss: 0.00001660
Iteration 219/1000 | Loss: 0.00001660
Iteration 220/1000 | Loss: 0.00001660
Iteration 221/1000 | Loss: 0.00001660
Iteration 222/1000 | Loss: 0.00001660
Iteration 223/1000 | Loss: 0.00001660
Iteration 224/1000 | Loss: 0.00001660
Iteration 225/1000 | Loss: 0.00001659
Iteration 226/1000 | Loss: 0.00001659
Iteration 227/1000 | Loss: 0.00001659
Iteration 228/1000 | Loss: 0.00001659
Iteration 229/1000 | Loss: 0.00001659
Iteration 230/1000 | Loss: 0.00001659
Iteration 231/1000 | Loss: 0.00001659
Iteration 232/1000 | Loss: 0.00001659
Iteration 233/1000 | Loss: 0.00001659
Iteration 234/1000 | Loss: 0.00001659
Iteration 235/1000 | Loss: 0.00001659
Iteration 236/1000 | Loss: 0.00001659
Iteration 237/1000 | Loss: 0.00001659
Iteration 238/1000 | Loss: 0.00001659
Iteration 239/1000 | Loss: 0.00001658
Iteration 240/1000 | Loss: 0.00001658
Iteration 241/1000 | Loss: 0.00001658
Iteration 242/1000 | Loss: 0.00001658
Iteration 243/1000 | Loss: 0.00001658
Iteration 244/1000 | Loss: 0.00001658
Iteration 245/1000 | Loss: 0.00001658
Iteration 246/1000 | Loss: 0.00001658
Iteration 247/1000 | Loss: 0.00001658
Iteration 248/1000 | Loss: 0.00001658
Iteration 249/1000 | Loss: 0.00001658
Iteration 250/1000 | Loss: 0.00001658
Iteration 251/1000 | Loss: 0.00001658
Iteration 252/1000 | Loss: 0.00001657
Iteration 253/1000 | Loss: 0.00001657
Iteration 254/1000 | Loss: 0.00001657
Iteration 255/1000 | Loss: 0.00001657
Iteration 256/1000 | Loss: 0.00001657
Iteration 257/1000 | Loss: 0.00001657
Iteration 258/1000 | Loss: 0.00001657
Iteration 259/1000 | Loss: 0.00001656
Iteration 260/1000 | Loss: 0.00001656
Iteration 261/1000 | Loss: 0.00001656
Iteration 262/1000 | Loss: 0.00001656
Iteration 263/1000 | Loss: 0.00001656
Iteration 264/1000 | Loss: 0.00001656
Iteration 265/1000 | Loss: 0.00001656
Iteration 266/1000 | Loss: 0.00001656
Iteration 267/1000 | Loss: 0.00001656
Iteration 268/1000 | Loss: 0.00001656
Iteration 269/1000 | Loss: 0.00001655
Iteration 270/1000 | Loss: 0.00001655
Iteration 271/1000 | Loss: 0.00001655
Iteration 272/1000 | Loss: 0.00001655
Iteration 273/1000 | Loss: 0.00001655
Iteration 274/1000 | Loss: 0.00001655
Iteration 275/1000 | Loss: 0.00001655
Iteration 276/1000 | Loss: 0.00001655
Iteration 277/1000 | Loss: 0.00001655
Iteration 278/1000 | Loss: 0.00001654
Iteration 279/1000 | Loss: 0.00001654
Iteration 280/1000 | Loss: 0.00001654
Iteration 281/1000 | Loss: 0.00001654
Iteration 282/1000 | Loss: 0.00001654
Iteration 283/1000 | Loss: 0.00001654
Iteration 284/1000 | Loss: 0.00001654
Iteration 285/1000 | Loss: 0.00001654
Iteration 286/1000 | Loss: 0.00001654
Iteration 287/1000 | Loss: 0.00001654
Iteration 288/1000 | Loss: 0.00001654
Iteration 289/1000 | Loss: 0.00001654
Iteration 290/1000 | Loss: 0.00001654
Iteration 291/1000 | Loss: 0.00001654
Iteration 292/1000 | Loss: 0.00001654
Iteration 293/1000 | Loss: 0.00001654
Iteration 294/1000 | Loss: 0.00001654
Iteration 295/1000 | Loss: 0.00001654
Iteration 296/1000 | Loss: 0.00001654
Iteration 297/1000 | Loss: 0.00001654
Iteration 298/1000 | Loss: 0.00001654
Iteration 299/1000 | Loss: 0.00001654
Iteration 300/1000 | Loss: 0.00001654
Iteration 301/1000 | Loss: 0.00001654
Iteration 302/1000 | Loss: 0.00001654
Iteration 303/1000 | Loss: 0.00001654
Iteration 304/1000 | Loss: 0.00001654
Iteration 305/1000 | Loss: 0.00001654
Iteration 306/1000 | Loss: 0.00001654
Iteration 307/1000 | Loss: 0.00001654
Iteration 308/1000 | Loss: 0.00001654
Iteration 309/1000 | Loss: 0.00001654
Iteration 310/1000 | Loss: 0.00001654
Iteration 311/1000 | Loss: 0.00001654
Iteration 312/1000 | Loss: 0.00001654
Iteration 313/1000 | Loss: 0.00001654
Iteration 314/1000 | Loss: 0.00001654
Iteration 315/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.6540940123377368e-05, 1.6540940123377368e-05, 1.6540940123377368e-05, 1.6540940123377368e-05, 1.6540940123377368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6540940123377368e-05

Optimization complete. Final v2v error: 3.360759973526001 mm

Highest mean error: 5.527661323547363 mm for frame 68

Lowest mean error: 2.991330623626709 mm for frame 124

Saving results

Total time: 237.80476784706116
