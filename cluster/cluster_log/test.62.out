Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=62, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3472-3527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793242
Iteration 2/25 | Loss: 0.00149001
Iteration 3/25 | Loss: 0.00133613
Iteration 4/25 | Loss: 0.00131699
Iteration 5/25 | Loss: 0.00131265
Iteration 6/25 | Loss: 0.00131226
Iteration 7/25 | Loss: 0.00131226
Iteration 8/25 | Loss: 0.00131226
Iteration 9/25 | Loss: 0.00131226
Iteration 10/25 | Loss: 0.00131226
Iteration 11/25 | Loss: 0.00131226
Iteration 12/25 | Loss: 0.00131226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013122629607096314, 0.0013122629607096314, 0.0013122629607096314, 0.0013122629607096314, 0.0013122629607096314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013122629607096314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38197541
Iteration 2/25 | Loss: 0.00077146
Iteration 3/25 | Loss: 0.00077142
Iteration 4/25 | Loss: 0.00077141
Iteration 5/25 | Loss: 0.00077141
Iteration 6/25 | Loss: 0.00077141
Iteration 7/25 | Loss: 0.00077141
Iteration 8/25 | Loss: 0.00077141
Iteration 9/25 | Loss: 0.00077141
Iteration 10/25 | Loss: 0.00077141
Iteration 11/25 | Loss: 0.00077141
Iteration 12/25 | Loss: 0.00077141
Iteration 13/25 | Loss: 0.00077141
Iteration 14/25 | Loss: 0.00077141
Iteration 15/25 | Loss: 0.00077141
Iteration 16/25 | Loss: 0.00077141
Iteration 17/25 | Loss: 0.00077141
Iteration 18/25 | Loss: 0.00077141
Iteration 19/25 | Loss: 0.00077141
Iteration 20/25 | Loss: 0.00077141
Iteration 21/25 | Loss: 0.00077141
Iteration 22/25 | Loss: 0.00077141
Iteration 23/25 | Loss: 0.00077141
Iteration 24/25 | Loss: 0.00077141
Iteration 25/25 | Loss: 0.00077141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077141
Iteration 2/1000 | Loss: 0.00004308
Iteration 3/1000 | Loss: 0.00003137
Iteration 4/1000 | Loss: 0.00002542
Iteration 5/1000 | Loss: 0.00002324
Iteration 6/1000 | Loss: 0.00002211
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002059
Iteration 9/1000 | Loss: 0.00002016
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001944
Iteration 12/1000 | Loss: 0.00001923
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001901
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001887
Iteration 17/1000 | Loss: 0.00001887
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001879
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001872
Iteration 30/1000 | Loss: 0.00001870
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001862
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001859
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001856
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001853
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001852
Iteration 95/1000 | Loss: 0.00001852
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001851
Iteration 102/1000 | Loss: 0.00001851
Iteration 103/1000 | Loss: 0.00001851
Iteration 104/1000 | Loss: 0.00001851
Iteration 105/1000 | Loss: 0.00001851
Iteration 106/1000 | Loss: 0.00001851
Iteration 107/1000 | Loss: 0.00001851
Iteration 108/1000 | Loss: 0.00001851
Iteration 109/1000 | Loss: 0.00001850
Iteration 110/1000 | Loss: 0.00001850
Iteration 111/1000 | Loss: 0.00001850
Iteration 112/1000 | Loss: 0.00001850
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001849
Iteration 121/1000 | Loss: 0.00001849
Iteration 122/1000 | Loss: 0.00001849
Iteration 123/1000 | Loss: 0.00001849
Iteration 124/1000 | Loss: 0.00001849
Iteration 125/1000 | Loss: 0.00001849
Iteration 126/1000 | Loss: 0.00001849
Iteration 127/1000 | Loss: 0.00001849
Iteration 128/1000 | Loss: 0.00001849
Iteration 129/1000 | Loss: 0.00001849
Iteration 130/1000 | Loss: 0.00001848
Iteration 131/1000 | Loss: 0.00001848
Iteration 132/1000 | Loss: 0.00001848
Iteration 133/1000 | Loss: 0.00001848
Iteration 134/1000 | Loss: 0.00001848
Iteration 135/1000 | Loss: 0.00001848
Iteration 136/1000 | Loss: 0.00001848
Iteration 137/1000 | Loss: 0.00001848
Iteration 138/1000 | Loss: 0.00001847
Iteration 139/1000 | Loss: 0.00001847
Iteration 140/1000 | Loss: 0.00001847
Iteration 141/1000 | Loss: 0.00001847
Iteration 142/1000 | Loss: 0.00001847
Iteration 143/1000 | Loss: 0.00001847
Iteration 144/1000 | Loss: 0.00001847
Iteration 145/1000 | Loss: 0.00001847
Iteration 146/1000 | Loss: 0.00001847
Iteration 147/1000 | Loss: 0.00001846
Iteration 148/1000 | Loss: 0.00001846
Iteration 149/1000 | Loss: 0.00001846
Iteration 150/1000 | Loss: 0.00001846
Iteration 151/1000 | Loss: 0.00001846
Iteration 152/1000 | Loss: 0.00001846
Iteration 153/1000 | Loss: 0.00001846
Iteration 154/1000 | Loss: 0.00001846
Iteration 155/1000 | Loss: 0.00001846
Iteration 156/1000 | Loss: 0.00001846
Iteration 157/1000 | Loss: 0.00001846
Iteration 158/1000 | Loss: 0.00001846
Iteration 159/1000 | Loss: 0.00001845
Iteration 160/1000 | Loss: 0.00001845
Iteration 161/1000 | Loss: 0.00001845
Iteration 162/1000 | Loss: 0.00001845
Iteration 163/1000 | Loss: 0.00001845
Iteration 164/1000 | Loss: 0.00001845
Iteration 165/1000 | Loss: 0.00001845
Iteration 166/1000 | Loss: 0.00001845
Iteration 167/1000 | Loss: 0.00001845
Iteration 168/1000 | Loss: 0.00001845
Iteration 169/1000 | Loss: 0.00001845
Iteration 170/1000 | Loss: 0.00001844
Iteration 171/1000 | Loss: 0.00001844
Iteration 172/1000 | Loss: 0.00001844
Iteration 173/1000 | Loss: 0.00001844
Iteration 174/1000 | Loss: 0.00001844
Iteration 175/1000 | Loss: 0.00001844
Iteration 176/1000 | Loss: 0.00001844
Iteration 177/1000 | Loss: 0.00001844
Iteration 178/1000 | Loss: 0.00001844
Iteration 179/1000 | Loss: 0.00001843
Iteration 180/1000 | Loss: 0.00001843
Iteration 181/1000 | Loss: 0.00001843
Iteration 182/1000 | Loss: 0.00001843
Iteration 183/1000 | Loss: 0.00001843
Iteration 184/1000 | Loss: 0.00001843
Iteration 185/1000 | Loss: 0.00001842
Iteration 186/1000 | Loss: 0.00001842
Iteration 187/1000 | Loss: 0.00001842
Iteration 188/1000 | Loss: 0.00001842
Iteration 189/1000 | Loss: 0.00001842
Iteration 190/1000 | Loss: 0.00001842
Iteration 191/1000 | Loss: 0.00001842
Iteration 192/1000 | Loss: 0.00001842
Iteration 193/1000 | Loss: 0.00001842
Iteration 194/1000 | Loss: 0.00001841
Iteration 195/1000 | Loss: 0.00001841
Iteration 196/1000 | Loss: 0.00001841
Iteration 197/1000 | Loss: 0.00001841
Iteration 198/1000 | Loss: 0.00001841
Iteration 199/1000 | Loss: 0.00001841
Iteration 200/1000 | Loss: 0.00001841
Iteration 201/1000 | Loss: 0.00001841
Iteration 202/1000 | Loss: 0.00001841
Iteration 203/1000 | Loss: 0.00001840
Iteration 204/1000 | Loss: 0.00001840
Iteration 205/1000 | Loss: 0.00001840
Iteration 206/1000 | Loss: 0.00001840
Iteration 207/1000 | Loss: 0.00001840
Iteration 208/1000 | Loss: 0.00001840
Iteration 209/1000 | Loss: 0.00001840
Iteration 210/1000 | Loss: 0.00001840
Iteration 211/1000 | Loss: 0.00001839
Iteration 212/1000 | Loss: 0.00001839
Iteration 213/1000 | Loss: 0.00001839
Iteration 214/1000 | Loss: 0.00001839
Iteration 215/1000 | Loss: 0.00001839
Iteration 216/1000 | Loss: 0.00001839
Iteration 217/1000 | Loss: 0.00001839
Iteration 218/1000 | Loss: 0.00001839
Iteration 219/1000 | Loss: 0.00001839
Iteration 220/1000 | Loss: 0.00001839
Iteration 221/1000 | Loss: 0.00001839
Iteration 222/1000 | Loss: 0.00001839
Iteration 223/1000 | Loss: 0.00001838
Iteration 224/1000 | Loss: 0.00001838
Iteration 225/1000 | Loss: 0.00001838
Iteration 226/1000 | Loss: 0.00001838
Iteration 227/1000 | Loss: 0.00001838
Iteration 228/1000 | Loss: 0.00001838
Iteration 229/1000 | Loss: 0.00001838
Iteration 230/1000 | Loss: 0.00001838
Iteration 231/1000 | Loss: 0.00001838
Iteration 232/1000 | Loss: 0.00001838
Iteration 233/1000 | Loss: 0.00001837
Iteration 234/1000 | Loss: 0.00001837
Iteration 235/1000 | Loss: 0.00001837
Iteration 236/1000 | Loss: 0.00001837
Iteration 237/1000 | Loss: 0.00001837
Iteration 238/1000 | Loss: 0.00001837
Iteration 239/1000 | Loss: 0.00001837
Iteration 240/1000 | Loss: 0.00001837
Iteration 241/1000 | Loss: 0.00001837
Iteration 242/1000 | Loss: 0.00001837
Iteration 243/1000 | Loss: 0.00001837
Iteration 244/1000 | Loss: 0.00001837
Iteration 245/1000 | Loss: 0.00001837
Iteration 246/1000 | Loss: 0.00001837
Iteration 247/1000 | Loss: 0.00001836
Iteration 248/1000 | Loss: 0.00001836
Iteration 249/1000 | Loss: 0.00001836
Iteration 250/1000 | Loss: 0.00001836
Iteration 251/1000 | Loss: 0.00001836
Iteration 252/1000 | Loss: 0.00001836
Iteration 253/1000 | Loss: 0.00001836
Iteration 254/1000 | Loss: 0.00001836
Iteration 255/1000 | Loss: 0.00001836
Iteration 256/1000 | Loss: 0.00001836
Iteration 257/1000 | Loss: 0.00001836
Iteration 258/1000 | Loss: 0.00001836
Iteration 259/1000 | Loss: 0.00001836
Iteration 260/1000 | Loss: 0.00001836
Iteration 261/1000 | Loss: 0.00001836
Iteration 262/1000 | Loss: 0.00001836
Iteration 263/1000 | Loss: 0.00001836
Iteration 264/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.836495903262403e-05, 1.836495903262403e-05, 1.836495903262403e-05, 1.836495903262403e-05, 1.836495903262403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.836495903262403e-05

Optimization complete. Final v2v error: 3.56453275680542 mm

Highest mean error: 4.895667552947998 mm for frame 23

Lowest mean error: 3.0403687953948975 mm for frame 206

Saving results

Total time: 53.158634424209595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452640
Iteration 2/25 | Loss: 0.00131059
Iteration 3/25 | Loss: 0.00123163
Iteration 4/25 | Loss: 0.00121881
Iteration 5/25 | Loss: 0.00121510
Iteration 6/25 | Loss: 0.00121430
Iteration 7/25 | Loss: 0.00121430
Iteration 8/25 | Loss: 0.00121430
Iteration 9/25 | Loss: 0.00121430
Iteration 10/25 | Loss: 0.00121430
Iteration 11/25 | Loss: 0.00121430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012142971390858293, 0.0012142971390858293, 0.0012142971390858293, 0.0012142971390858293, 0.0012142971390858293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012142971390858293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56594956
Iteration 2/25 | Loss: 0.00076780
Iteration 3/25 | Loss: 0.00076780
Iteration 4/25 | Loss: 0.00076780
Iteration 5/25 | Loss: 0.00076780
Iteration 6/25 | Loss: 0.00076780
Iteration 7/25 | Loss: 0.00076780
Iteration 8/25 | Loss: 0.00076780
Iteration 9/25 | Loss: 0.00076780
Iteration 10/25 | Loss: 0.00076780
Iteration 11/25 | Loss: 0.00076780
Iteration 12/25 | Loss: 0.00076780
Iteration 13/25 | Loss: 0.00076780
Iteration 14/25 | Loss: 0.00076780
Iteration 15/25 | Loss: 0.00076780
Iteration 16/25 | Loss: 0.00076780
Iteration 17/25 | Loss: 0.00076780
Iteration 18/25 | Loss: 0.00076780
Iteration 19/25 | Loss: 0.00076780
Iteration 20/25 | Loss: 0.00076780
Iteration 21/25 | Loss: 0.00076780
Iteration 22/25 | Loss: 0.00076780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007677980465814471, 0.0007677980465814471, 0.0007677980465814471, 0.0007677980465814471, 0.0007677980465814471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007677980465814471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076780
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001584
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001244
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001216
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001182
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001175
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001168
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001160
Iteration 71/1000 | Loss: 0.00001160
Iteration 72/1000 | Loss: 0.00001160
Iteration 73/1000 | Loss: 0.00001160
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001159
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001150
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001148
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001145
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001144
Iteration 135/1000 | Loss: 0.00001144
Iteration 136/1000 | Loss: 0.00001144
Iteration 137/1000 | Loss: 0.00001144
Iteration 138/1000 | Loss: 0.00001144
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001143
Iteration 141/1000 | Loss: 0.00001143
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001142
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001142
Iteration 151/1000 | Loss: 0.00001142
Iteration 152/1000 | Loss: 0.00001142
Iteration 153/1000 | Loss: 0.00001142
Iteration 154/1000 | Loss: 0.00001142
Iteration 155/1000 | Loss: 0.00001142
Iteration 156/1000 | Loss: 0.00001142
Iteration 157/1000 | Loss: 0.00001142
Iteration 158/1000 | Loss: 0.00001142
Iteration 159/1000 | Loss: 0.00001142
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Iteration 167/1000 | Loss: 0.00001142
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001142
Iteration 176/1000 | Loss: 0.00001142
Iteration 177/1000 | Loss: 0.00001142
Iteration 178/1000 | Loss: 0.00001142
Iteration 179/1000 | Loss: 0.00001142
Iteration 180/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.1420921509852633e-05, 1.1420921509852633e-05, 1.1420921509852633e-05, 1.1420921509852633e-05, 1.1420921509852633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1420921509852633e-05

Optimization complete. Final v2v error: 2.91308856010437 mm

Highest mean error: 3.069671392440796 mm for frame 40

Lowest mean error: 2.7942118644714355 mm for frame 104

Saving results

Total time: 35.70681405067444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806510
Iteration 2/25 | Loss: 0.00138191
Iteration 3/25 | Loss: 0.00129673
Iteration 4/25 | Loss: 0.00128713
Iteration 5/25 | Loss: 0.00128435
Iteration 6/25 | Loss: 0.00128435
Iteration 7/25 | Loss: 0.00128435
Iteration 8/25 | Loss: 0.00128435
Iteration 9/25 | Loss: 0.00128435
Iteration 10/25 | Loss: 0.00128435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012843533186241984, 0.0012843533186241984, 0.0012843533186241984, 0.0012843533186241984, 0.0012843533186241984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012843533186241984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.52755833
Iteration 2/25 | Loss: 0.00077384
Iteration 3/25 | Loss: 0.00077374
Iteration 4/25 | Loss: 0.00077374
Iteration 5/25 | Loss: 0.00077374
Iteration 6/25 | Loss: 0.00077374
Iteration 7/25 | Loss: 0.00077374
Iteration 8/25 | Loss: 0.00077374
Iteration 9/25 | Loss: 0.00077374
Iteration 10/25 | Loss: 0.00077374
Iteration 11/25 | Loss: 0.00077374
Iteration 12/25 | Loss: 0.00077374
Iteration 13/25 | Loss: 0.00077374
Iteration 14/25 | Loss: 0.00077374
Iteration 15/25 | Loss: 0.00077374
Iteration 16/25 | Loss: 0.00077374
Iteration 17/25 | Loss: 0.00077374
Iteration 18/25 | Loss: 0.00077374
Iteration 19/25 | Loss: 0.00077374
Iteration 20/25 | Loss: 0.00077374
Iteration 21/25 | Loss: 0.00077374
Iteration 22/25 | Loss: 0.00077374
Iteration 23/25 | Loss: 0.00077374
Iteration 24/25 | Loss: 0.00077374
Iteration 25/25 | Loss: 0.00077374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077374
Iteration 2/1000 | Loss: 0.00003936
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002438
Iteration 5/1000 | Loss: 0.00002305
Iteration 6/1000 | Loss: 0.00002204
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002085
Iteration 9/1000 | Loss: 0.00002035
Iteration 10/1000 | Loss: 0.00002007
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001965
Iteration 13/1000 | Loss: 0.00001964
Iteration 14/1000 | Loss: 0.00001954
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001928
Iteration 19/1000 | Loss: 0.00001923
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001919
Iteration 22/1000 | Loss: 0.00001919
Iteration 23/1000 | Loss: 0.00001918
Iteration 24/1000 | Loss: 0.00001916
Iteration 25/1000 | Loss: 0.00001915
Iteration 26/1000 | Loss: 0.00001914
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001903
Iteration 34/1000 | Loss: 0.00001903
Iteration 35/1000 | Loss: 0.00001901
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00001900
Iteration 39/1000 | Loss: 0.00001900
Iteration 40/1000 | Loss: 0.00001900
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001899
Iteration 45/1000 | Loss: 0.00001898
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001893
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001891
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001886
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001885
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001884
Iteration 84/1000 | Loss: 0.00001884
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001884
Iteration 88/1000 | Loss: 0.00001884
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001883
Iteration 92/1000 | Loss: 0.00001883
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001882
Iteration 99/1000 | Loss: 0.00001882
Iteration 100/1000 | Loss: 0.00001882
Iteration 101/1000 | Loss: 0.00001882
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00001882
Iteration 104/1000 | Loss: 0.00001881
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001881
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001880
Iteration 118/1000 | Loss: 0.00001880
Iteration 119/1000 | Loss: 0.00001880
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001880
Iteration 122/1000 | Loss: 0.00001880
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001880
Iteration 133/1000 | Loss: 0.00001880
Iteration 134/1000 | Loss: 0.00001880
Iteration 135/1000 | Loss: 0.00001879
Iteration 136/1000 | Loss: 0.00001879
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001879
Iteration 141/1000 | Loss: 0.00001879
Iteration 142/1000 | Loss: 0.00001879
Iteration 143/1000 | Loss: 0.00001879
Iteration 144/1000 | Loss: 0.00001879
Iteration 145/1000 | Loss: 0.00001879
Iteration 146/1000 | Loss: 0.00001879
Iteration 147/1000 | Loss: 0.00001879
Iteration 148/1000 | Loss: 0.00001879
Iteration 149/1000 | Loss: 0.00001879
Iteration 150/1000 | Loss: 0.00001879
Iteration 151/1000 | Loss: 0.00001878
Iteration 152/1000 | Loss: 0.00001878
Iteration 153/1000 | Loss: 0.00001878
Iteration 154/1000 | Loss: 0.00001878
Iteration 155/1000 | Loss: 0.00001878
Iteration 156/1000 | Loss: 0.00001878
Iteration 157/1000 | Loss: 0.00001878
Iteration 158/1000 | Loss: 0.00001878
Iteration 159/1000 | Loss: 0.00001878
Iteration 160/1000 | Loss: 0.00001878
Iteration 161/1000 | Loss: 0.00001878
Iteration 162/1000 | Loss: 0.00001878
Iteration 163/1000 | Loss: 0.00001878
Iteration 164/1000 | Loss: 0.00001878
Iteration 165/1000 | Loss: 0.00001878
Iteration 166/1000 | Loss: 0.00001878
Iteration 167/1000 | Loss: 0.00001878
Iteration 168/1000 | Loss: 0.00001878
Iteration 169/1000 | Loss: 0.00001878
Iteration 170/1000 | Loss: 0.00001878
Iteration 171/1000 | Loss: 0.00001878
Iteration 172/1000 | Loss: 0.00001878
Iteration 173/1000 | Loss: 0.00001878
Iteration 174/1000 | Loss: 0.00001878
Iteration 175/1000 | Loss: 0.00001878
Iteration 176/1000 | Loss: 0.00001878
Iteration 177/1000 | Loss: 0.00001878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.8775759599520825e-05, 1.8775759599520825e-05, 1.8775759599520825e-05, 1.8775759599520825e-05, 1.8775759599520825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8775759599520825e-05

Optimization complete. Final v2v error: 3.66378116607666 mm

Highest mean error: 4.655570983886719 mm for frame 109

Lowest mean error: 3.171449899673462 mm for frame 24

Saving results

Total time: 46.72533702850342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089738
Iteration 2/25 | Loss: 0.00273442
Iteration 3/25 | Loss: 0.00208990
Iteration 4/25 | Loss: 0.00190289
Iteration 5/25 | Loss: 0.00179292
Iteration 6/25 | Loss: 0.00176273
Iteration 7/25 | Loss: 0.00175506
Iteration 8/25 | Loss: 0.00176784
Iteration 9/25 | Loss: 0.00178392
Iteration 10/25 | Loss: 0.00179414
Iteration 11/25 | Loss: 0.00177580
Iteration 12/25 | Loss: 0.00178301
Iteration 13/25 | Loss: 0.00178178
Iteration 14/25 | Loss: 0.00176355
Iteration 15/25 | Loss: 0.00175816
Iteration 16/25 | Loss: 0.00174859
Iteration 17/25 | Loss: 0.00174263
Iteration 18/25 | Loss: 0.00173586
Iteration 19/25 | Loss: 0.00173658
Iteration 20/25 | Loss: 0.00173323
Iteration 21/25 | Loss: 0.00173799
Iteration 22/25 | Loss: 0.00173048
Iteration 23/25 | Loss: 0.00172941
Iteration 24/25 | Loss: 0.00172560
Iteration 25/25 | Loss: 0.00172609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67785931
Iteration 2/25 | Loss: 0.00552919
Iteration 3/25 | Loss: 0.00535258
Iteration 4/25 | Loss: 0.00535258
Iteration 5/25 | Loss: 0.00535258
Iteration 6/25 | Loss: 0.00535258
Iteration 7/25 | Loss: 0.00535258
Iteration 8/25 | Loss: 0.00535257
Iteration 9/25 | Loss: 0.00535257
Iteration 10/25 | Loss: 0.00535257
Iteration 11/25 | Loss: 0.00535257
Iteration 12/25 | Loss: 0.00535257
Iteration 13/25 | Loss: 0.00535257
Iteration 14/25 | Loss: 0.00535257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.005352573934942484, 0.005352573934942484, 0.005352573934942484, 0.005352573934942484, 0.005352573934942484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005352573934942484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00535257
Iteration 2/1000 | Loss: 0.00074983
Iteration 3/1000 | Loss: 0.00063213
Iteration 4/1000 | Loss: 0.00329347
Iteration 5/1000 | Loss: 0.00033182
Iteration 6/1000 | Loss: 0.00224605
Iteration 7/1000 | Loss: 0.00024707
Iteration 8/1000 | Loss: 0.00024030
Iteration 9/1000 | Loss: 0.00023081
Iteration 10/1000 | Loss: 0.00022568
Iteration 11/1000 | Loss: 0.00019670
Iteration 12/1000 | Loss: 0.00016515
Iteration 13/1000 | Loss: 0.00022250
Iteration 14/1000 | Loss: 0.00018614
Iteration 15/1000 | Loss: 0.00020663
Iteration 16/1000 | Loss: 0.00021614
Iteration 17/1000 | Loss: 0.00021111
Iteration 18/1000 | Loss: 0.00020235
Iteration 19/1000 | Loss: 0.00026481
Iteration 20/1000 | Loss: 0.00021510
Iteration 21/1000 | Loss: 0.00021526
Iteration 22/1000 | Loss: 0.00019971
Iteration 23/1000 | Loss: 0.00023696
Iteration 24/1000 | Loss: 0.00020073
Iteration 25/1000 | Loss: 0.00022842
Iteration 26/1000 | Loss: 0.00020403
Iteration 27/1000 | Loss: 0.00020968
Iteration 28/1000 | Loss: 0.00020500
Iteration 29/1000 | Loss: 0.00020467
Iteration 30/1000 | Loss: 0.00022701
Iteration 31/1000 | Loss: 0.00021029
Iteration 32/1000 | Loss: 0.00020522
Iteration 33/1000 | Loss: 0.00022159
Iteration 34/1000 | Loss: 0.00022394
Iteration 35/1000 | Loss: 0.00020731
Iteration 36/1000 | Loss: 0.00017595
Iteration 37/1000 | Loss: 0.00021562
Iteration 38/1000 | Loss: 0.00026528
Iteration 39/1000 | Loss: 0.00020805
Iteration 40/1000 | Loss: 0.00017387
Iteration 41/1000 | Loss: 0.00020270
Iteration 42/1000 | Loss: 0.00020976
Iteration 43/1000 | Loss: 0.00021969
Iteration 44/1000 | Loss: 0.00021669
Iteration 45/1000 | Loss: 0.00022380
Iteration 46/1000 | Loss: 0.00019769
Iteration 47/1000 | Loss: 0.00019468
Iteration 48/1000 | Loss: 0.00020950
Iteration 49/1000 | Loss: 0.00017578
Iteration 50/1000 | Loss: 0.00019274
Iteration 51/1000 | Loss: 0.00018090
Iteration 52/1000 | Loss: 0.00018116
Iteration 53/1000 | Loss: 0.00019794
Iteration 54/1000 | Loss: 0.00018431
Iteration 55/1000 | Loss: 0.00017287
Iteration 56/1000 | Loss: 0.00021391
Iteration 57/1000 | Loss: 0.00019957
Iteration 58/1000 | Loss: 0.00019291
Iteration 59/1000 | Loss: 0.00019377
Iteration 60/1000 | Loss: 0.00018571
Iteration 61/1000 | Loss: 0.00018422
Iteration 62/1000 | Loss: 0.00017090
Iteration 63/1000 | Loss: 0.00019869
Iteration 64/1000 | Loss: 0.00017302
Iteration 65/1000 | Loss: 0.00015365
Iteration 66/1000 | Loss: 0.00018359
Iteration 67/1000 | Loss: 0.00019844
Iteration 68/1000 | Loss: 0.00019989
Iteration 69/1000 | Loss: 0.00019186
Iteration 70/1000 | Loss: 0.00017645
Iteration 71/1000 | Loss: 0.00021000
Iteration 72/1000 | Loss: 0.00018979
Iteration 73/1000 | Loss: 0.00021420
Iteration 74/1000 | Loss: 0.00021121
Iteration 75/1000 | Loss: 0.00020203
Iteration 76/1000 | Loss: 0.00018523
Iteration 77/1000 | Loss: 0.00018596
Iteration 78/1000 | Loss: 0.00018473
Iteration 79/1000 | Loss: 0.00020343
Iteration 80/1000 | Loss: 0.00018850
Iteration 81/1000 | Loss: 0.00019436
Iteration 82/1000 | Loss: 0.00019553
Iteration 83/1000 | Loss: 0.00019046
Iteration 84/1000 | Loss: 0.00019422
Iteration 85/1000 | Loss: 0.00015385
Iteration 86/1000 | Loss: 0.00020530
Iteration 87/1000 | Loss: 0.00103894
Iteration 88/1000 | Loss: 0.00058298
Iteration 89/1000 | Loss: 0.00075170
Iteration 90/1000 | Loss: 0.00075419
Iteration 91/1000 | Loss: 0.00078837
Iteration 92/1000 | Loss: 0.00020201
Iteration 93/1000 | Loss: 0.00018772
Iteration 94/1000 | Loss: 0.00018345
Iteration 95/1000 | Loss: 0.00018226
Iteration 96/1000 | Loss: 0.00018842
Iteration 97/1000 | Loss: 0.00019179
Iteration 98/1000 | Loss: 0.00017764
Iteration 99/1000 | Loss: 0.00016713
Iteration 100/1000 | Loss: 0.00019598
Iteration 101/1000 | Loss: 0.00014815
Iteration 102/1000 | Loss: 0.00014323
Iteration 103/1000 | Loss: 0.00013992
Iteration 104/1000 | Loss: 0.00013672
Iteration 105/1000 | Loss: 0.00013511
Iteration 106/1000 | Loss: 0.00013473
Iteration 107/1000 | Loss: 0.00013433
Iteration 108/1000 | Loss: 0.00013389
Iteration 109/1000 | Loss: 0.00013353
Iteration 110/1000 | Loss: 0.00013315
Iteration 111/1000 | Loss: 0.00013267
Iteration 112/1000 | Loss: 0.00096999
Iteration 113/1000 | Loss: 0.00014992
Iteration 114/1000 | Loss: 0.00013629
Iteration 115/1000 | Loss: 0.00013278
Iteration 116/1000 | Loss: 0.00013116
Iteration 117/1000 | Loss: 0.00012988
Iteration 118/1000 | Loss: 0.00012921
Iteration 119/1000 | Loss: 0.00012887
Iteration 120/1000 | Loss: 0.00012860
Iteration 121/1000 | Loss: 0.00012843
Iteration 122/1000 | Loss: 0.00012826
Iteration 123/1000 | Loss: 0.00012825
Iteration 124/1000 | Loss: 0.00012817
Iteration 125/1000 | Loss: 0.00012817
Iteration 126/1000 | Loss: 0.00012817
Iteration 127/1000 | Loss: 0.00012817
Iteration 128/1000 | Loss: 0.00012816
Iteration 129/1000 | Loss: 0.00012816
Iteration 130/1000 | Loss: 0.00012816
Iteration 131/1000 | Loss: 0.00012816
Iteration 132/1000 | Loss: 0.00012816
Iteration 133/1000 | Loss: 0.00012816
Iteration 134/1000 | Loss: 0.00012816
Iteration 135/1000 | Loss: 0.00012816
Iteration 136/1000 | Loss: 0.00012816
Iteration 137/1000 | Loss: 0.00012815
Iteration 138/1000 | Loss: 0.00012814
Iteration 139/1000 | Loss: 0.00012814
Iteration 140/1000 | Loss: 0.00012813
Iteration 141/1000 | Loss: 0.00012811
Iteration 142/1000 | Loss: 0.00012811
Iteration 143/1000 | Loss: 0.00012810
Iteration 144/1000 | Loss: 0.00012810
Iteration 145/1000 | Loss: 0.00012810
Iteration 146/1000 | Loss: 0.00012810
Iteration 147/1000 | Loss: 0.00012809
Iteration 148/1000 | Loss: 0.00012809
Iteration 149/1000 | Loss: 0.00012809
Iteration 150/1000 | Loss: 0.00012809
Iteration 151/1000 | Loss: 0.00012809
Iteration 152/1000 | Loss: 0.00012809
Iteration 153/1000 | Loss: 0.00012808
Iteration 154/1000 | Loss: 0.00012808
Iteration 155/1000 | Loss: 0.00012808
Iteration 156/1000 | Loss: 0.00012808
Iteration 157/1000 | Loss: 0.00012808
Iteration 158/1000 | Loss: 0.00012808
Iteration 159/1000 | Loss: 0.00012808
Iteration 160/1000 | Loss: 0.00012807
Iteration 161/1000 | Loss: 0.00012807
Iteration 162/1000 | Loss: 0.00012807
Iteration 163/1000 | Loss: 0.00012807
Iteration 164/1000 | Loss: 0.00012806
Iteration 165/1000 | Loss: 0.00012806
Iteration 166/1000 | Loss: 0.00012806
Iteration 167/1000 | Loss: 0.00012805
Iteration 168/1000 | Loss: 0.00012805
Iteration 169/1000 | Loss: 0.00012805
Iteration 170/1000 | Loss: 0.00012805
Iteration 171/1000 | Loss: 0.00012804
Iteration 172/1000 | Loss: 0.00012804
Iteration 173/1000 | Loss: 0.00012804
Iteration 174/1000 | Loss: 0.00012804
Iteration 175/1000 | Loss: 0.00012803
Iteration 176/1000 | Loss: 0.00012803
Iteration 177/1000 | Loss: 0.00012803
Iteration 178/1000 | Loss: 0.00012803
Iteration 179/1000 | Loss: 0.00012803
Iteration 180/1000 | Loss: 0.00012803
Iteration 181/1000 | Loss: 0.00012802
Iteration 182/1000 | Loss: 0.00012802
Iteration 183/1000 | Loss: 0.00012802
Iteration 184/1000 | Loss: 0.00012802
Iteration 185/1000 | Loss: 0.00012801
Iteration 186/1000 | Loss: 0.00012801
Iteration 187/1000 | Loss: 0.00012801
Iteration 188/1000 | Loss: 0.00012801
Iteration 189/1000 | Loss: 0.00012801
Iteration 190/1000 | Loss: 0.00012801
Iteration 191/1000 | Loss: 0.00012801
Iteration 192/1000 | Loss: 0.00012801
Iteration 193/1000 | Loss: 0.00012801
Iteration 194/1000 | Loss: 0.00012801
Iteration 195/1000 | Loss: 0.00012801
Iteration 196/1000 | Loss: 0.00012801
Iteration 197/1000 | Loss: 0.00012801
Iteration 198/1000 | Loss: 0.00012801
Iteration 199/1000 | Loss: 0.00012801
Iteration 200/1000 | Loss: 0.00012801
Iteration 201/1000 | Loss: 0.00012801
Iteration 202/1000 | Loss: 0.00012801
Iteration 203/1000 | Loss: 0.00012800
Iteration 204/1000 | Loss: 0.00012800
Iteration 205/1000 | Loss: 0.00012800
Iteration 206/1000 | Loss: 0.00012800
Iteration 207/1000 | Loss: 0.00012800
Iteration 208/1000 | Loss: 0.00012800
Iteration 209/1000 | Loss: 0.00012800
Iteration 210/1000 | Loss: 0.00012800
Iteration 211/1000 | Loss: 0.00012799
Iteration 212/1000 | Loss: 0.00012799
Iteration 213/1000 | Loss: 0.00012799
Iteration 214/1000 | Loss: 0.00012799
Iteration 215/1000 | Loss: 0.00012799
Iteration 216/1000 | Loss: 0.00012798
Iteration 217/1000 | Loss: 0.00012798
Iteration 218/1000 | Loss: 0.00012798
Iteration 219/1000 | Loss: 0.00012798
Iteration 220/1000 | Loss: 0.00012798
Iteration 221/1000 | Loss: 0.00012798
Iteration 222/1000 | Loss: 0.00012798
Iteration 223/1000 | Loss: 0.00012798
Iteration 224/1000 | Loss: 0.00012798
Iteration 225/1000 | Loss: 0.00012798
Iteration 226/1000 | Loss: 0.00012798
Iteration 227/1000 | Loss: 0.00012798
Iteration 228/1000 | Loss: 0.00012798
Iteration 229/1000 | Loss: 0.00012798
Iteration 230/1000 | Loss: 0.00012798
Iteration 231/1000 | Loss: 0.00012798
Iteration 232/1000 | Loss: 0.00012797
Iteration 233/1000 | Loss: 0.00012797
Iteration 234/1000 | Loss: 0.00012797
Iteration 235/1000 | Loss: 0.00012797
Iteration 236/1000 | Loss: 0.00012797
Iteration 237/1000 | Loss: 0.00012797
Iteration 238/1000 | Loss: 0.00012797
Iteration 239/1000 | Loss: 0.00012797
Iteration 240/1000 | Loss: 0.00012797
Iteration 241/1000 | Loss: 0.00012797
Iteration 242/1000 | Loss: 0.00012797
Iteration 243/1000 | Loss: 0.00012797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [0.00012797341332770884, 0.00012797341332770884, 0.00012797341332770884, 0.00012797341332770884, 0.00012797341332770884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012797341332770884

Optimization complete. Final v2v error: 6.158647060394287 mm

Highest mean error: 13.536697387695312 mm for frame 0

Lowest mean error: 3.9157140254974365 mm for frame 24

Saving results

Total time: 214.6641776561737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941806
Iteration 2/25 | Loss: 0.00153852
Iteration 3/25 | Loss: 0.00140099
Iteration 4/25 | Loss: 0.00138264
Iteration 5/25 | Loss: 0.00137565
Iteration 6/25 | Loss: 0.00137378
Iteration 7/25 | Loss: 0.00137359
Iteration 8/25 | Loss: 0.00137359
Iteration 9/25 | Loss: 0.00137359
Iteration 10/25 | Loss: 0.00137359
Iteration 11/25 | Loss: 0.00137359
Iteration 12/25 | Loss: 0.00137359
Iteration 13/25 | Loss: 0.00137359
Iteration 14/25 | Loss: 0.00137359
Iteration 15/25 | Loss: 0.00137359
Iteration 16/25 | Loss: 0.00137359
Iteration 17/25 | Loss: 0.00137359
Iteration 18/25 | Loss: 0.00137359
Iteration 19/25 | Loss: 0.00137359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013735858956351876, 0.0013735858956351876, 0.0013735858956351876, 0.0013735858956351876, 0.0013735858956351876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013735858956351876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60308254
Iteration 2/25 | Loss: 0.00234366
Iteration 3/25 | Loss: 0.00234363
Iteration 4/25 | Loss: 0.00234363
Iteration 5/25 | Loss: 0.00234363
Iteration 6/25 | Loss: 0.00234363
Iteration 7/25 | Loss: 0.00234363
Iteration 8/25 | Loss: 0.00234363
Iteration 9/25 | Loss: 0.00234363
Iteration 10/25 | Loss: 0.00234363
Iteration 11/25 | Loss: 0.00234363
Iteration 12/25 | Loss: 0.00234363
Iteration 13/25 | Loss: 0.00234363
Iteration 14/25 | Loss: 0.00234363
Iteration 15/25 | Loss: 0.00234363
Iteration 16/25 | Loss: 0.00234363
Iteration 17/25 | Loss: 0.00234363
Iteration 18/25 | Loss: 0.00234363
Iteration 19/25 | Loss: 0.00234363
Iteration 20/25 | Loss: 0.00234363
Iteration 21/25 | Loss: 0.00234363
Iteration 22/25 | Loss: 0.00234363
Iteration 23/25 | Loss: 0.00234363
Iteration 24/25 | Loss: 0.00234363
Iteration 25/25 | Loss: 0.00234363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234363
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002583
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002019
Iteration 10/1000 | Loss: 0.00001994
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001976
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001969
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001950
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001945
Iteration 34/1000 | Loss: 0.00001945
Iteration 35/1000 | Loss: 0.00001945
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001945
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 42. Stopping optimization.
Last 5 losses: [1.9445320504019037e-05, 1.9445320504019037e-05, 1.9445320504019037e-05, 1.9445320504019037e-05, 1.9445320504019037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9445320504019037e-05

Optimization complete. Final v2v error: 3.9048616886138916 mm

Highest mean error: 4.195981979370117 mm for frame 70

Lowest mean error: 3.6502838134765625 mm for frame 127

Saving results

Total time: 27.9516863822937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084322
Iteration 2/25 | Loss: 0.00191883
Iteration 3/25 | Loss: 0.00156225
Iteration 4/25 | Loss: 0.00152377
Iteration 5/25 | Loss: 0.00151448
Iteration 6/25 | Loss: 0.00151301
Iteration 7/25 | Loss: 0.00151301
Iteration 8/25 | Loss: 0.00151301
Iteration 9/25 | Loss: 0.00151301
Iteration 10/25 | Loss: 0.00151301
Iteration 11/25 | Loss: 0.00151301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015130091924220324, 0.0015130091924220324, 0.0015130091924220324, 0.0015130091924220324, 0.0015130091924220324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015130091924220324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54572773
Iteration 2/25 | Loss: 0.00292088
Iteration 3/25 | Loss: 0.00292086
Iteration 4/25 | Loss: 0.00292086
Iteration 5/25 | Loss: 0.00292086
Iteration 6/25 | Loss: 0.00292086
Iteration 7/25 | Loss: 0.00292086
Iteration 8/25 | Loss: 0.00292086
Iteration 9/25 | Loss: 0.00292086
Iteration 10/25 | Loss: 0.00292086
Iteration 11/25 | Loss: 0.00292086
Iteration 12/25 | Loss: 0.00292086
Iteration 13/25 | Loss: 0.00292086
Iteration 14/25 | Loss: 0.00292086
Iteration 15/25 | Loss: 0.00292086
Iteration 16/25 | Loss: 0.00292086
Iteration 17/25 | Loss: 0.00292086
Iteration 18/25 | Loss: 0.00292086
Iteration 19/25 | Loss: 0.00292086
Iteration 20/25 | Loss: 0.00292086
Iteration 21/25 | Loss: 0.00292086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002920860890299082, 0.002920860890299082, 0.002920860890299082, 0.002920860890299082, 0.002920860890299082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002920860890299082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292086
Iteration 2/1000 | Loss: 0.00007341
Iteration 3/1000 | Loss: 0.00005697
Iteration 4/1000 | Loss: 0.00005051
Iteration 5/1000 | Loss: 0.00004802
Iteration 6/1000 | Loss: 0.00004604
Iteration 7/1000 | Loss: 0.00004500
Iteration 8/1000 | Loss: 0.00004404
Iteration 9/1000 | Loss: 0.00004321
Iteration 10/1000 | Loss: 0.00004259
Iteration 11/1000 | Loss: 0.00004203
Iteration 12/1000 | Loss: 0.00004159
Iteration 13/1000 | Loss: 0.00004125
Iteration 14/1000 | Loss: 0.00004099
Iteration 15/1000 | Loss: 0.00004078
Iteration 16/1000 | Loss: 0.00004060
Iteration 17/1000 | Loss: 0.00004046
Iteration 18/1000 | Loss: 0.00004033
Iteration 19/1000 | Loss: 0.00004031
Iteration 20/1000 | Loss: 0.00004025
Iteration 21/1000 | Loss: 0.00004021
Iteration 22/1000 | Loss: 0.00004021
Iteration 23/1000 | Loss: 0.00004019
Iteration 24/1000 | Loss: 0.00004019
Iteration 25/1000 | Loss: 0.00004017
Iteration 26/1000 | Loss: 0.00004017
Iteration 27/1000 | Loss: 0.00004016
Iteration 28/1000 | Loss: 0.00004016
Iteration 29/1000 | Loss: 0.00004016
Iteration 30/1000 | Loss: 0.00004016
Iteration 31/1000 | Loss: 0.00004015
Iteration 32/1000 | Loss: 0.00004015
Iteration 33/1000 | Loss: 0.00004015
Iteration 34/1000 | Loss: 0.00004015
Iteration 35/1000 | Loss: 0.00004015
Iteration 36/1000 | Loss: 0.00004015
Iteration 37/1000 | Loss: 0.00004014
Iteration 38/1000 | Loss: 0.00004014
Iteration 39/1000 | Loss: 0.00004014
Iteration 40/1000 | Loss: 0.00004014
Iteration 41/1000 | Loss: 0.00004014
Iteration 42/1000 | Loss: 0.00004014
Iteration 43/1000 | Loss: 0.00004014
Iteration 44/1000 | Loss: 0.00004014
Iteration 45/1000 | Loss: 0.00004013
Iteration 46/1000 | Loss: 0.00004013
Iteration 47/1000 | Loss: 0.00004013
Iteration 48/1000 | Loss: 0.00004012
Iteration 49/1000 | Loss: 0.00004012
Iteration 50/1000 | Loss: 0.00004012
Iteration 51/1000 | Loss: 0.00004012
Iteration 52/1000 | Loss: 0.00004011
Iteration 53/1000 | Loss: 0.00004011
Iteration 54/1000 | Loss: 0.00004011
Iteration 55/1000 | Loss: 0.00004011
Iteration 56/1000 | Loss: 0.00004011
Iteration 57/1000 | Loss: 0.00004011
Iteration 58/1000 | Loss: 0.00004011
Iteration 59/1000 | Loss: 0.00004011
Iteration 60/1000 | Loss: 0.00004011
Iteration 61/1000 | Loss: 0.00004011
Iteration 62/1000 | Loss: 0.00004011
Iteration 63/1000 | Loss: 0.00004010
Iteration 64/1000 | Loss: 0.00004010
Iteration 65/1000 | Loss: 0.00004010
Iteration 66/1000 | Loss: 0.00004010
Iteration 67/1000 | Loss: 0.00004010
Iteration 68/1000 | Loss: 0.00004010
Iteration 69/1000 | Loss: 0.00004010
Iteration 70/1000 | Loss: 0.00004009
Iteration 71/1000 | Loss: 0.00004008
Iteration 72/1000 | Loss: 0.00004007
Iteration 73/1000 | Loss: 0.00004007
Iteration 74/1000 | Loss: 0.00004007
Iteration 75/1000 | Loss: 0.00004007
Iteration 76/1000 | Loss: 0.00004006
Iteration 77/1000 | Loss: 0.00004006
Iteration 78/1000 | Loss: 0.00004006
Iteration 79/1000 | Loss: 0.00004006
Iteration 80/1000 | Loss: 0.00004006
Iteration 81/1000 | Loss: 0.00004004
Iteration 82/1000 | Loss: 0.00004004
Iteration 83/1000 | Loss: 0.00004004
Iteration 84/1000 | Loss: 0.00004004
Iteration 85/1000 | Loss: 0.00004004
Iteration 86/1000 | Loss: 0.00004004
Iteration 87/1000 | Loss: 0.00004004
Iteration 88/1000 | Loss: 0.00004004
Iteration 89/1000 | Loss: 0.00004004
Iteration 90/1000 | Loss: 0.00004004
Iteration 91/1000 | Loss: 0.00004003
Iteration 92/1000 | Loss: 0.00004003
Iteration 93/1000 | Loss: 0.00004002
Iteration 94/1000 | Loss: 0.00004002
Iteration 95/1000 | Loss: 0.00004002
Iteration 96/1000 | Loss: 0.00004002
Iteration 97/1000 | Loss: 0.00004001
Iteration 98/1000 | Loss: 0.00004001
Iteration 99/1000 | Loss: 0.00004001
Iteration 100/1000 | Loss: 0.00004001
Iteration 101/1000 | Loss: 0.00004001
Iteration 102/1000 | Loss: 0.00004000
Iteration 103/1000 | Loss: 0.00004000
Iteration 104/1000 | Loss: 0.00004000
Iteration 105/1000 | Loss: 0.00004000
Iteration 106/1000 | Loss: 0.00004000
Iteration 107/1000 | Loss: 0.00004000
Iteration 108/1000 | Loss: 0.00004000
Iteration 109/1000 | Loss: 0.00004000
Iteration 110/1000 | Loss: 0.00004000
Iteration 111/1000 | Loss: 0.00004000
Iteration 112/1000 | Loss: 0.00003999
Iteration 113/1000 | Loss: 0.00003998
Iteration 114/1000 | Loss: 0.00003998
Iteration 115/1000 | Loss: 0.00003997
Iteration 116/1000 | Loss: 0.00003997
Iteration 117/1000 | Loss: 0.00003997
Iteration 118/1000 | Loss: 0.00003997
Iteration 119/1000 | Loss: 0.00003996
Iteration 120/1000 | Loss: 0.00003996
Iteration 121/1000 | Loss: 0.00003996
Iteration 122/1000 | Loss: 0.00003995
Iteration 123/1000 | Loss: 0.00003995
Iteration 124/1000 | Loss: 0.00003995
Iteration 125/1000 | Loss: 0.00003995
Iteration 126/1000 | Loss: 0.00003994
Iteration 127/1000 | Loss: 0.00003994
Iteration 128/1000 | Loss: 0.00003994
Iteration 129/1000 | Loss: 0.00003994
Iteration 130/1000 | Loss: 0.00003994
Iteration 131/1000 | Loss: 0.00003994
Iteration 132/1000 | Loss: 0.00003994
Iteration 133/1000 | Loss: 0.00003993
Iteration 134/1000 | Loss: 0.00003993
Iteration 135/1000 | Loss: 0.00003993
Iteration 136/1000 | Loss: 0.00003993
Iteration 137/1000 | Loss: 0.00003993
Iteration 138/1000 | Loss: 0.00003993
Iteration 139/1000 | Loss: 0.00003993
Iteration 140/1000 | Loss: 0.00003993
Iteration 141/1000 | Loss: 0.00003993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.993287464254536e-05, 3.993287464254536e-05, 3.993287464254536e-05, 3.993287464254536e-05, 3.993287464254536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.993287464254536e-05

Optimization complete. Final v2v error: 5.278393745422363 mm

Highest mean error: 6.406322479248047 mm for frame 207

Lowest mean error: 4.4156904220581055 mm for frame 107

Saving results

Total time: 49.667128562927246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535231
Iteration 2/25 | Loss: 0.00153257
Iteration 3/25 | Loss: 0.00143691
Iteration 4/25 | Loss: 0.00140994
Iteration 5/25 | Loss: 0.00139961
Iteration 6/25 | Loss: 0.00139680
Iteration 7/25 | Loss: 0.00139640
Iteration 8/25 | Loss: 0.00139640
Iteration 9/25 | Loss: 0.00139640
Iteration 10/25 | Loss: 0.00139640
Iteration 11/25 | Loss: 0.00139640
Iteration 12/25 | Loss: 0.00139640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013964008539915085, 0.0013964008539915085, 0.0013964008539915085, 0.0013964008539915085, 0.0013964008539915085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013964008539915085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31074095
Iteration 2/25 | Loss: 0.00210660
Iteration 3/25 | Loss: 0.00210659
Iteration 4/25 | Loss: 0.00210659
Iteration 5/25 | Loss: 0.00210659
Iteration 6/25 | Loss: 0.00210659
Iteration 7/25 | Loss: 0.00210659
Iteration 8/25 | Loss: 0.00210659
Iteration 9/25 | Loss: 0.00210659
Iteration 10/25 | Loss: 0.00210659
Iteration 11/25 | Loss: 0.00210659
Iteration 12/25 | Loss: 0.00210659
Iteration 13/25 | Loss: 0.00210659
Iteration 14/25 | Loss: 0.00210659
Iteration 15/25 | Loss: 0.00210659
Iteration 16/25 | Loss: 0.00210659
Iteration 17/25 | Loss: 0.00210659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002106589497998357, 0.002106589497998357, 0.002106589497998357, 0.002106589497998357, 0.002106589497998357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002106589497998357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210659
Iteration 2/1000 | Loss: 0.00004553
Iteration 3/1000 | Loss: 0.00003429
Iteration 4/1000 | Loss: 0.00003177
Iteration 5/1000 | Loss: 0.00003011
Iteration 6/1000 | Loss: 0.00002934
Iteration 7/1000 | Loss: 0.00002860
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002769
Iteration 10/1000 | Loss: 0.00002743
Iteration 11/1000 | Loss: 0.00002743
Iteration 12/1000 | Loss: 0.00002739
Iteration 13/1000 | Loss: 0.00002732
Iteration 14/1000 | Loss: 0.00002731
Iteration 15/1000 | Loss: 0.00002727
Iteration 16/1000 | Loss: 0.00002724
Iteration 17/1000 | Loss: 0.00002724
Iteration 18/1000 | Loss: 0.00002723
Iteration 19/1000 | Loss: 0.00002723
Iteration 20/1000 | Loss: 0.00002722
Iteration 21/1000 | Loss: 0.00002722
Iteration 22/1000 | Loss: 0.00002722
Iteration 23/1000 | Loss: 0.00002721
Iteration 24/1000 | Loss: 0.00002721
Iteration 25/1000 | Loss: 0.00002720
Iteration 26/1000 | Loss: 0.00002720
Iteration 27/1000 | Loss: 0.00002719
Iteration 28/1000 | Loss: 0.00002719
Iteration 29/1000 | Loss: 0.00002718
Iteration 30/1000 | Loss: 0.00002718
Iteration 31/1000 | Loss: 0.00002717
Iteration 32/1000 | Loss: 0.00002717
Iteration 33/1000 | Loss: 0.00002716
Iteration 34/1000 | Loss: 0.00002716
Iteration 35/1000 | Loss: 0.00002716
Iteration 36/1000 | Loss: 0.00002715
Iteration 37/1000 | Loss: 0.00002715
Iteration 38/1000 | Loss: 0.00002715
Iteration 39/1000 | Loss: 0.00002715
Iteration 40/1000 | Loss: 0.00002714
Iteration 41/1000 | Loss: 0.00002714
Iteration 42/1000 | Loss: 0.00002713
Iteration 43/1000 | Loss: 0.00002712
Iteration 44/1000 | Loss: 0.00002711
Iteration 45/1000 | Loss: 0.00002710
Iteration 46/1000 | Loss: 0.00002709
Iteration 47/1000 | Loss: 0.00002709
Iteration 48/1000 | Loss: 0.00002708
Iteration 49/1000 | Loss: 0.00002707
Iteration 50/1000 | Loss: 0.00002707
Iteration 51/1000 | Loss: 0.00002706
Iteration 52/1000 | Loss: 0.00002706
Iteration 53/1000 | Loss: 0.00002705
Iteration 54/1000 | Loss: 0.00002704
Iteration 55/1000 | Loss: 0.00002704
Iteration 56/1000 | Loss: 0.00002700
Iteration 57/1000 | Loss: 0.00002700
Iteration 58/1000 | Loss: 0.00002700
Iteration 59/1000 | Loss: 0.00002700
Iteration 60/1000 | Loss: 0.00002700
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002697
Iteration 64/1000 | Loss: 0.00002696
Iteration 65/1000 | Loss: 0.00002696
Iteration 66/1000 | Loss: 0.00002695
Iteration 67/1000 | Loss: 0.00002695
Iteration 68/1000 | Loss: 0.00002695
Iteration 69/1000 | Loss: 0.00002694
Iteration 70/1000 | Loss: 0.00002694
Iteration 71/1000 | Loss: 0.00002694
Iteration 72/1000 | Loss: 0.00002693
Iteration 73/1000 | Loss: 0.00002693
Iteration 74/1000 | Loss: 0.00002693
Iteration 75/1000 | Loss: 0.00002692
Iteration 76/1000 | Loss: 0.00002692
Iteration 77/1000 | Loss: 0.00002692
Iteration 78/1000 | Loss: 0.00002692
Iteration 79/1000 | Loss: 0.00002691
Iteration 80/1000 | Loss: 0.00002691
Iteration 81/1000 | Loss: 0.00002691
Iteration 82/1000 | Loss: 0.00002691
Iteration 83/1000 | Loss: 0.00002691
Iteration 84/1000 | Loss: 0.00002690
Iteration 85/1000 | Loss: 0.00002690
Iteration 86/1000 | Loss: 0.00002690
Iteration 87/1000 | Loss: 0.00002690
Iteration 88/1000 | Loss: 0.00002690
Iteration 89/1000 | Loss: 0.00002690
Iteration 90/1000 | Loss: 0.00002690
Iteration 91/1000 | Loss: 0.00002689
Iteration 92/1000 | Loss: 0.00002689
Iteration 93/1000 | Loss: 0.00002689
Iteration 94/1000 | Loss: 0.00002689
Iteration 95/1000 | Loss: 0.00002689
Iteration 96/1000 | Loss: 0.00002689
Iteration 97/1000 | Loss: 0.00002689
Iteration 98/1000 | Loss: 0.00002689
Iteration 99/1000 | Loss: 0.00002689
Iteration 100/1000 | Loss: 0.00002688
Iteration 101/1000 | Loss: 0.00002688
Iteration 102/1000 | Loss: 0.00002688
Iteration 103/1000 | Loss: 0.00002688
Iteration 104/1000 | Loss: 0.00002688
Iteration 105/1000 | Loss: 0.00002688
Iteration 106/1000 | Loss: 0.00002688
Iteration 107/1000 | Loss: 0.00002688
Iteration 108/1000 | Loss: 0.00002688
Iteration 109/1000 | Loss: 0.00002688
Iteration 110/1000 | Loss: 0.00002688
Iteration 111/1000 | Loss: 0.00002688
Iteration 112/1000 | Loss: 0.00002688
Iteration 113/1000 | Loss: 0.00002688
Iteration 114/1000 | Loss: 0.00002688
Iteration 115/1000 | Loss: 0.00002688
Iteration 116/1000 | Loss: 0.00002688
Iteration 117/1000 | Loss: 0.00002688
Iteration 118/1000 | Loss: 0.00002688
Iteration 119/1000 | Loss: 0.00002688
Iteration 120/1000 | Loss: 0.00002688
Iteration 121/1000 | Loss: 0.00002688
Iteration 122/1000 | Loss: 0.00002688
Iteration 123/1000 | Loss: 0.00002688
Iteration 124/1000 | Loss: 0.00002688
Iteration 125/1000 | Loss: 0.00002688
Iteration 126/1000 | Loss: 0.00002688
Iteration 127/1000 | Loss: 0.00002688
Iteration 128/1000 | Loss: 0.00002688
Iteration 129/1000 | Loss: 0.00002688
Iteration 130/1000 | Loss: 0.00002688
Iteration 131/1000 | Loss: 0.00002688
Iteration 132/1000 | Loss: 0.00002688
Iteration 133/1000 | Loss: 0.00002688
Iteration 134/1000 | Loss: 0.00002688
Iteration 135/1000 | Loss: 0.00002688
Iteration 136/1000 | Loss: 0.00002688
Iteration 137/1000 | Loss: 0.00002688
Iteration 138/1000 | Loss: 0.00002688
Iteration 139/1000 | Loss: 0.00002688
Iteration 140/1000 | Loss: 0.00002688
Iteration 141/1000 | Loss: 0.00002688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.6878193239099346e-05, 2.6878193239099346e-05, 2.6878193239099346e-05, 2.6878193239099346e-05, 2.6878193239099346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6878193239099346e-05

Optimization complete. Final v2v error: 4.469445705413818 mm

Highest mean error: 5.139866352081299 mm for frame 184

Lowest mean error: 4.022527694702148 mm for frame 137

Saving results

Total time: 36.5091233253479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944511
Iteration 2/25 | Loss: 0.00153194
Iteration 3/25 | Loss: 0.00141728
Iteration 4/25 | Loss: 0.00140078
Iteration 5/25 | Loss: 0.00139482
Iteration 6/25 | Loss: 0.00139331
Iteration 7/25 | Loss: 0.00139324
Iteration 8/25 | Loss: 0.00139324
Iteration 9/25 | Loss: 0.00139324
Iteration 10/25 | Loss: 0.00139324
Iteration 11/25 | Loss: 0.00139324
Iteration 12/25 | Loss: 0.00139324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013932350557297468, 0.0013932350557297468, 0.0013932350557297468, 0.0013932350557297468, 0.0013932350557297468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013932350557297468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60025656
Iteration 2/25 | Loss: 0.00237818
Iteration 3/25 | Loss: 0.00237816
Iteration 4/25 | Loss: 0.00237816
Iteration 5/25 | Loss: 0.00237816
Iteration 6/25 | Loss: 0.00237816
Iteration 7/25 | Loss: 0.00237816
Iteration 8/25 | Loss: 0.00237816
Iteration 9/25 | Loss: 0.00237816
Iteration 10/25 | Loss: 0.00237816
Iteration 11/25 | Loss: 0.00237816
Iteration 12/25 | Loss: 0.00237816
Iteration 13/25 | Loss: 0.00237816
Iteration 14/25 | Loss: 0.00237816
Iteration 15/25 | Loss: 0.00237816
Iteration 16/25 | Loss: 0.00237816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002378156641498208, 0.002378156641498208, 0.002378156641498208, 0.002378156641498208, 0.002378156641498208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002378156641498208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237816
Iteration 2/1000 | Loss: 0.00003611
Iteration 3/1000 | Loss: 0.00002557
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00002122
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001997
Iteration 10/1000 | Loss: 0.00001982
Iteration 11/1000 | Loss: 0.00001978
Iteration 12/1000 | Loss: 0.00001973
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001959
Iteration 17/1000 | Loss: 0.00001957
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001941
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001931
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001929
Iteration 48/1000 | Loss: 0.00001929
Iteration 49/1000 | Loss: 0.00001929
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001928
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001928
Iteration 60/1000 | Loss: 0.00001928
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001927
Iteration 65/1000 | Loss: 0.00001927
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001926
Iteration 73/1000 | Loss: 0.00001926
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001926
Iteration 76/1000 | Loss: 0.00001926
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001926
Iteration 83/1000 | Loss: 0.00001926
Iteration 84/1000 | Loss: 0.00001926
Iteration 85/1000 | Loss: 0.00001926
Iteration 86/1000 | Loss: 0.00001926
Iteration 87/1000 | Loss: 0.00001926
Iteration 88/1000 | Loss: 0.00001926
Iteration 89/1000 | Loss: 0.00001926
Iteration 90/1000 | Loss: 0.00001926
Iteration 91/1000 | Loss: 0.00001926
Iteration 92/1000 | Loss: 0.00001926
Iteration 93/1000 | Loss: 0.00001926
Iteration 94/1000 | Loss: 0.00001926
Iteration 95/1000 | Loss: 0.00001926
Iteration 96/1000 | Loss: 0.00001926
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.9260694898548536e-05, 1.9260694898548536e-05, 1.9260694898548536e-05, 1.9260694898548536e-05, 1.9260694898548536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9260694898548536e-05

Optimization complete. Final v2v error: 3.845905065536499 mm

Highest mean error: 4.040797233581543 mm for frame 77

Lowest mean error: 3.6061930656433105 mm for frame 136

Saving results

Total time: 30.27489185333252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026312
Iteration 2/25 | Loss: 0.00202094
Iteration 3/25 | Loss: 0.00164636
Iteration 4/25 | Loss: 0.00157251
Iteration 5/25 | Loss: 0.00155503
Iteration 6/25 | Loss: 0.00155188
Iteration 7/25 | Loss: 0.00155050
Iteration 8/25 | Loss: 0.00155027
Iteration 9/25 | Loss: 0.00155027
Iteration 10/25 | Loss: 0.00155027
Iteration 11/25 | Loss: 0.00155027
Iteration 12/25 | Loss: 0.00155027
Iteration 13/25 | Loss: 0.00155027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001550269895233214, 0.001550269895233214, 0.001550269895233214, 0.001550269895233214, 0.001550269895233214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001550269895233214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73503911
Iteration 2/25 | Loss: 0.00262799
Iteration 3/25 | Loss: 0.00262790
Iteration 4/25 | Loss: 0.00262790
Iteration 5/25 | Loss: 0.00262790
Iteration 6/25 | Loss: 0.00262790
Iteration 7/25 | Loss: 0.00262790
Iteration 8/25 | Loss: 0.00262790
Iteration 9/25 | Loss: 0.00262790
Iteration 10/25 | Loss: 0.00262790
Iteration 11/25 | Loss: 0.00262790
Iteration 12/25 | Loss: 0.00262790
Iteration 13/25 | Loss: 0.00262790
Iteration 14/25 | Loss: 0.00262790
Iteration 15/25 | Loss: 0.00262790
Iteration 16/25 | Loss: 0.00262790
Iteration 17/25 | Loss: 0.00262790
Iteration 18/25 | Loss: 0.00262790
Iteration 19/25 | Loss: 0.00262790
Iteration 20/25 | Loss: 0.00262790
Iteration 21/25 | Loss: 0.00262790
Iteration 22/25 | Loss: 0.00262790
Iteration 23/25 | Loss: 0.00262790
Iteration 24/25 | Loss: 0.00262790
Iteration 25/25 | Loss: 0.00262790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002627895912155509, 0.002627895912155509, 0.002627895912155509, 0.002627895912155509, 0.002627895912155509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002627895912155509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262790
Iteration 2/1000 | Loss: 0.00011491
Iteration 3/1000 | Loss: 0.00007713
Iteration 4/1000 | Loss: 0.00006223
Iteration 5/1000 | Loss: 0.00005435
Iteration 6/1000 | Loss: 0.00005016
Iteration 7/1000 | Loss: 0.00004788
Iteration 8/1000 | Loss: 0.00004654
Iteration 9/1000 | Loss: 0.00004534
Iteration 10/1000 | Loss: 0.00004422
Iteration 11/1000 | Loss: 0.00004338
Iteration 12/1000 | Loss: 0.00004282
Iteration 13/1000 | Loss: 0.00004234
Iteration 14/1000 | Loss: 0.00004188
Iteration 15/1000 | Loss: 0.00004152
Iteration 16/1000 | Loss: 0.00004126
Iteration 17/1000 | Loss: 0.00004105
Iteration 18/1000 | Loss: 0.00004087
Iteration 19/1000 | Loss: 0.00004079
Iteration 20/1000 | Loss: 0.00004065
Iteration 21/1000 | Loss: 0.00004061
Iteration 22/1000 | Loss: 0.00004047
Iteration 23/1000 | Loss: 0.00004047
Iteration 24/1000 | Loss: 0.00004046
Iteration 25/1000 | Loss: 0.00004045
Iteration 26/1000 | Loss: 0.00004045
Iteration 27/1000 | Loss: 0.00004043
Iteration 28/1000 | Loss: 0.00004043
Iteration 29/1000 | Loss: 0.00004042
Iteration 30/1000 | Loss: 0.00004041
Iteration 31/1000 | Loss: 0.00004041
Iteration 32/1000 | Loss: 0.00004040
Iteration 33/1000 | Loss: 0.00004038
Iteration 34/1000 | Loss: 0.00004037
Iteration 35/1000 | Loss: 0.00004036
Iteration 36/1000 | Loss: 0.00004035
Iteration 37/1000 | Loss: 0.00004035
Iteration 38/1000 | Loss: 0.00004034
Iteration 39/1000 | Loss: 0.00004034
Iteration 40/1000 | Loss: 0.00004032
Iteration 41/1000 | Loss: 0.00004030
Iteration 42/1000 | Loss: 0.00004030
Iteration 43/1000 | Loss: 0.00004029
Iteration 44/1000 | Loss: 0.00004028
Iteration 45/1000 | Loss: 0.00004028
Iteration 46/1000 | Loss: 0.00004027
Iteration 47/1000 | Loss: 0.00004026
Iteration 48/1000 | Loss: 0.00004026
Iteration 49/1000 | Loss: 0.00004025
Iteration 50/1000 | Loss: 0.00004025
Iteration 51/1000 | Loss: 0.00004024
Iteration 52/1000 | Loss: 0.00004024
Iteration 53/1000 | Loss: 0.00004024
Iteration 54/1000 | Loss: 0.00004023
Iteration 55/1000 | Loss: 0.00004023
Iteration 56/1000 | Loss: 0.00004023
Iteration 57/1000 | Loss: 0.00004023
Iteration 58/1000 | Loss: 0.00004023
Iteration 59/1000 | Loss: 0.00004023
Iteration 60/1000 | Loss: 0.00004022
Iteration 61/1000 | Loss: 0.00004022
Iteration 62/1000 | Loss: 0.00004022
Iteration 63/1000 | Loss: 0.00004021
Iteration 64/1000 | Loss: 0.00004021
Iteration 65/1000 | Loss: 0.00004021
Iteration 66/1000 | Loss: 0.00004020
Iteration 67/1000 | Loss: 0.00004020
Iteration 68/1000 | Loss: 0.00004020
Iteration 69/1000 | Loss: 0.00004020
Iteration 70/1000 | Loss: 0.00004020
Iteration 71/1000 | Loss: 0.00004020
Iteration 72/1000 | Loss: 0.00004020
Iteration 73/1000 | Loss: 0.00004020
Iteration 74/1000 | Loss: 0.00004020
Iteration 75/1000 | Loss: 0.00004020
Iteration 76/1000 | Loss: 0.00004020
Iteration 77/1000 | Loss: 0.00004020
Iteration 78/1000 | Loss: 0.00004020
Iteration 79/1000 | Loss: 0.00004020
Iteration 80/1000 | Loss: 0.00004020
Iteration 81/1000 | Loss: 0.00004019
Iteration 82/1000 | Loss: 0.00004019
Iteration 83/1000 | Loss: 0.00004019
Iteration 84/1000 | Loss: 0.00004018
Iteration 85/1000 | Loss: 0.00004018
Iteration 86/1000 | Loss: 0.00004018
Iteration 87/1000 | Loss: 0.00004018
Iteration 88/1000 | Loss: 0.00004018
Iteration 89/1000 | Loss: 0.00004017
Iteration 90/1000 | Loss: 0.00004017
Iteration 91/1000 | Loss: 0.00004017
Iteration 92/1000 | Loss: 0.00004017
Iteration 93/1000 | Loss: 0.00004017
Iteration 94/1000 | Loss: 0.00004017
Iteration 95/1000 | Loss: 0.00004016
Iteration 96/1000 | Loss: 0.00004016
Iteration 97/1000 | Loss: 0.00004016
Iteration 98/1000 | Loss: 0.00004016
Iteration 99/1000 | Loss: 0.00004016
Iteration 100/1000 | Loss: 0.00004016
Iteration 101/1000 | Loss: 0.00004016
Iteration 102/1000 | Loss: 0.00004016
Iteration 103/1000 | Loss: 0.00004016
Iteration 104/1000 | Loss: 0.00004015
Iteration 105/1000 | Loss: 0.00004015
Iteration 106/1000 | Loss: 0.00004014
Iteration 107/1000 | Loss: 0.00004014
Iteration 108/1000 | Loss: 0.00004014
Iteration 109/1000 | Loss: 0.00004014
Iteration 110/1000 | Loss: 0.00004014
Iteration 111/1000 | Loss: 0.00004013
Iteration 112/1000 | Loss: 0.00004013
Iteration 113/1000 | Loss: 0.00004013
Iteration 114/1000 | Loss: 0.00004013
Iteration 115/1000 | Loss: 0.00004013
Iteration 116/1000 | Loss: 0.00004013
Iteration 117/1000 | Loss: 0.00004013
Iteration 118/1000 | Loss: 0.00004013
Iteration 119/1000 | Loss: 0.00004013
Iteration 120/1000 | Loss: 0.00004012
Iteration 121/1000 | Loss: 0.00004012
Iteration 122/1000 | Loss: 0.00004012
Iteration 123/1000 | Loss: 0.00004012
Iteration 124/1000 | Loss: 0.00004012
Iteration 125/1000 | Loss: 0.00004012
Iteration 126/1000 | Loss: 0.00004012
Iteration 127/1000 | Loss: 0.00004012
Iteration 128/1000 | Loss: 0.00004012
Iteration 129/1000 | Loss: 0.00004012
Iteration 130/1000 | Loss: 0.00004012
Iteration 131/1000 | Loss: 0.00004012
Iteration 132/1000 | Loss: 0.00004012
Iteration 133/1000 | Loss: 0.00004011
Iteration 134/1000 | Loss: 0.00004011
Iteration 135/1000 | Loss: 0.00004011
Iteration 136/1000 | Loss: 0.00004011
Iteration 137/1000 | Loss: 0.00004011
Iteration 138/1000 | Loss: 0.00004011
Iteration 139/1000 | Loss: 0.00004011
Iteration 140/1000 | Loss: 0.00004011
Iteration 141/1000 | Loss: 0.00004011
Iteration 142/1000 | Loss: 0.00004011
Iteration 143/1000 | Loss: 0.00004010
Iteration 144/1000 | Loss: 0.00004010
Iteration 145/1000 | Loss: 0.00004010
Iteration 146/1000 | Loss: 0.00004010
Iteration 147/1000 | Loss: 0.00004010
Iteration 148/1000 | Loss: 0.00004010
Iteration 149/1000 | Loss: 0.00004010
Iteration 150/1000 | Loss: 0.00004010
Iteration 151/1000 | Loss: 0.00004009
Iteration 152/1000 | Loss: 0.00004009
Iteration 153/1000 | Loss: 0.00004009
Iteration 154/1000 | Loss: 0.00004009
Iteration 155/1000 | Loss: 0.00004009
Iteration 156/1000 | Loss: 0.00004009
Iteration 157/1000 | Loss: 0.00004008
Iteration 158/1000 | Loss: 0.00004008
Iteration 159/1000 | Loss: 0.00004008
Iteration 160/1000 | Loss: 0.00004008
Iteration 161/1000 | Loss: 0.00004008
Iteration 162/1000 | Loss: 0.00004008
Iteration 163/1000 | Loss: 0.00004008
Iteration 164/1000 | Loss: 0.00004008
Iteration 165/1000 | Loss: 0.00004008
Iteration 166/1000 | Loss: 0.00004008
Iteration 167/1000 | Loss: 0.00004008
Iteration 168/1000 | Loss: 0.00004008
Iteration 169/1000 | Loss: 0.00004008
Iteration 170/1000 | Loss: 0.00004008
Iteration 171/1000 | Loss: 0.00004008
Iteration 172/1000 | Loss: 0.00004008
Iteration 173/1000 | Loss: 0.00004007
Iteration 174/1000 | Loss: 0.00004007
Iteration 175/1000 | Loss: 0.00004007
Iteration 176/1000 | Loss: 0.00004007
Iteration 177/1000 | Loss: 0.00004007
Iteration 178/1000 | Loss: 0.00004007
Iteration 179/1000 | Loss: 0.00004007
Iteration 180/1000 | Loss: 0.00004007
Iteration 181/1000 | Loss: 0.00004007
Iteration 182/1000 | Loss: 0.00004007
Iteration 183/1000 | Loss: 0.00004007
Iteration 184/1000 | Loss: 0.00004007
Iteration 185/1000 | Loss: 0.00004006
Iteration 186/1000 | Loss: 0.00004006
Iteration 187/1000 | Loss: 0.00004006
Iteration 188/1000 | Loss: 0.00004006
Iteration 189/1000 | Loss: 0.00004006
Iteration 190/1000 | Loss: 0.00004006
Iteration 191/1000 | Loss: 0.00004006
Iteration 192/1000 | Loss: 0.00004006
Iteration 193/1000 | Loss: 0.00004006
Iteration 194/1000 | Loss: 0.00004006
Iteration 195/1000 | Loss: 0.00004006
Iteration 196/1000 | Loss: 0.00004006
Iteration 197/1000 | Loss: 0.00004006
Iteration 198/1000 | Loss: 0.00004006
Iteration 199/1000 | Loss: 0.00004006
Iteration 200/1000 | Loss: 0.00004006
Iteration 201/1000 | Loss: 0.00004006
Iteration 202/1000 | Loss: 0.00004005
Iteration 203/1000 | Loss: 0.00004005
Iteration 204/1000 | Loss: 0.00004005
Iteration 205/1000 | Loss: 0.00004005
Iteration 206/1000 | Loss: 0.00004005
Iteration 207/1000 | Loss: 0.00004005
Iteration 208/1000 | Loss: 0.00004005
Iteration 209/1000 | Loss: 0.00004005
Iteration 210/1000 | Loss: 0.00004005
Iteration 211/1000 | Loss: 0.00004005
Iteration 212/1000 | Loss: 0.00004005
Iteration 213/1000 | Loss: 0.00004005
Iteration 214/1000 | Loss: 0.00004005
Iteration 215/1000 | Loss: 0.00004005
Iteration 216/1000 | Loss: 0.00004005
Iteration 217/1000 | Loss: 0.00004005
Iteration 218/1000 | Loss: 0.00004005
Iteration 219/1000 | Loss: 0.00004004
Iteration 220/1000 | Loss: 0.00004004
Iteration 221/1000 | Loss: 0.00004004
Iteration 222/1000 | Loss: 0.00004004
Iteration 223/1000 | Loss: 0.00004004
Iteration 224/1000 | Loss: 0.00004004
Iteration 225/1000 | Loss: 0.00004004
Iteration 226/1000 | Loss: 0.00004004
Iteration 227/1000 | Loss: 0.00004004
Iteration 228/1000 | Loss: 0.00004004
Iteration 229/1000 | Loss: 0.00004004
Iteration 230/1000 | Loss: 0.00004004
Iteration 231/1000 | Loss: 0.00004004
Iteration 232/1000 | Loss: 0.00004004
Iteration 233/1000 | Loss: 0.00004004
Iteration 234/1000 | Loss: 0.00004004
Iteration 235/1000 | Loss: 0.00004004
Iteration 236/1000 | Loss: 0.00004004
Iteration 237/1000 | Loss: 0.00004004
Iteration 238/1000 | Loss: 0.00004004
Iteration 239/1000 | Loss: 0.00004003
Iteration 240/1000 | Loss: 0.00004003
Iteration 241/1000 | Loss: 0.00004003
Iteration 242/1000 | Loss: 0.00004003
Iteration 243/1000 | Loss: 0.00004003
Iteration 244/1000 | Loss: 0.00004003
Iteration 245/1000 | Loss: 0.00004003
Iteration 246/1000 | Loss: 0.00004003
Iteration 247/1000 | Loss: 0.00004003
Iteration 248/1000 | Loss: 0.00004003
Iteration 249/1000 | Loss: 0.00004003
Iteration 250/1000 | Loss: 0.00004003
Iteration 251/1000 | Loss: 0.00004003
Iteration 252/1000 | Loss: 0.00004003
Iteration 253/1000 | Loss: 0.00004003
Iteration 254/1000 | Loss: 0.00004003
Iteration 255/1000 | Loss: 0.00004003
Iteration 256/1000 | Loss: 0.00004003
Iteration 257/1000 | Loss: 0.00004003
Iteration 258/1000 | Loss: 0.00004003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [4.0030834497883916e-05, 4.0030834497883916e-05, 4.0030834497883916e-05, 4.0030834497883916e-05, 4.0030834497883916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0030834497883916e-05

Optimization complete. Final v2v error: 5.227969169616699 mm

Highest mean error: 6.916184902191162 mm for frame 83

Lowest mean error: 4.166856288909912 mm for frame 39

Saving results

Total time: 53.43946170806885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493531
Iteration 2/25 | Loss: 0.00149724
Iteration 3/25 | Loss: 0.00139883
Iteration 4/25 | Loss: 0.00137159
Iteration 5/25 | Loss: 0.00136205
Iteration 6/25 | Loss: 0.00135995
Iteration 7/25 | Loss: 0.00135977
Iteration 8/25 | Loss: 0.00135977
Iteration 9/25 | Loss: 0.00135977
Iteration 10/25 | Loss: 0.00135977
Iteration 11/25 | Loss: 0.00135977
Iteration 12/25 | Loss: 0.00135977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013597678625956178, 0.0013597678625956178, 0.0013597678625956178, 0.0013597678625956178, 0.0013597678625956178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013597678625956178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.79750299
Iteration 2/25 | Loss: 0.00216039
Iteration 3/25 | Loss: 0.00216037
Iteration 4/25 | Loss: 0.00216037
Iteration 5/25 | Loss: 0.00216037
Iteration 6/25 | Loss: 0.00216037
Iteration 7/25 | Loss: 0.00216037
Iteration 8/25 | Loss: 0.00216037
Iteration 9/25 | Loss: 0.00216037
Iteration 10/25 | Loss: 0.00216037
Iteration 11/25 | Loss: 0.00216037
Iteration 12/25 | Loss: 0.00216037
Iteration 13/25 | Loss: 0.00216037
Iteration 14/25 | Loss: 0.00216037
Iteration 15/25 | Loss: 0.00216037
Iteration 16/25 | Loss: 0.00216037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021603668574243784, 0.0021603668574243784, 0.0021603668574243784, 0.0021603668574243784, 0.0021603668574243784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021603668574243784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216037
Iteration 2/1000 | Loss: 0.00005214
Iteration 3/1000 | Loss: 0.00003149
Iteration 4/1000 | Loss: 0.00002782
Iteration 5/1000 | Loss: 0.00002628
Iteration 6/1000 | Loss: 0.00002524
Iteration 7/1000 | Loss: 0.00002473
Iteration 8/1000 | Loss: 0.00002420
Iteration 9/1000 | Loss: 0.00002387
Iteration 10/1000 | Loss: 0.00002364
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002338
Iteration 13/1000 | Loss: 0.00002333
Iteration 14/1000 | Loss: 0.00002329
Iteration 15/1000 | Loss: 0.00002328
Iteration 16/1000 | Loss: 0.00002324
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002323
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00002322
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002322
Iteration 23/1000 | Loss: 0.00002322
Iteration 24/1000 | Loss: 0.00002322
Iteration 25/1000 | Loss: 0.00002322
Iteration 26/1000 | Loss: 0.00002322
Iteration 27/1000 | Loss: 0.00002322
Iteration 28/1000 | Loss: 0.00002322
Iteration 29/1000 | Loss: 0.00002321
Iteration 30/1000 | Loss: 0.00002321
Iteration 31/1000 | Loss: 0.00002321
Iteration 32/1000 | Loss: 0.00002321
Iteration 33/1000 | Loss: 0.00002321
Iteration 34/1000 | Loss: 0.00002321
Iteration 35/1000 | Loss: 0.00002321
Iteration 36/1000 | Loss: 0.00002321
Iteration 37/1000 | Loss: 0.00002321
Iteration 38/1000 | Loss: 0.00002321
Iteration 39/1000 | Loss: 0.00002321
Iteration 40/1000 | Loss: 0.00002320
Iteration 41/1000 | Loss: 0.00002320
Iteration 42/1000 | Loss: 0.00002319
Iteration 43/1000 | Loss: 0.00002319
Iteration 44/1000 | Loss: 0.00002318
Iteration 45/1000 | Loss: 0.00002318
Iteration 46/1000 | Loss: 0.00002318
Iteration 47/1000 | Loss: 0.00002317
Iteration 48/1000 | Loss: 0.00002317
Iteration 49/1000 | Loss: 0.00002317
Iteration 50/1000 | Loss: 0.00002317
Iteration 51/1000 | Loss: 0.00002316
Iteration 52/1000 | Loss: 0.00002316
Iteration 53/1000 | Loss: 0.00002315
Iteration 54/1000 | Loss: 0.00002315
Iteration 55/1000 | Loss: 0.00002315
Iteration 56/1000 | Loss: 0.00002314
Iteration 57/1000 | Loss: 0.00002314
Iteration 58/1000 | Loss: 0.00002313
Iteration 59/1000 | Loss: 0.00002313
Iteration 60/1000 | Loss: 0.00002313
Iteration 61/1000 | Loss: 0.00002312
Iteration 62/1000 | Loss: 0.00002312
Iteration 63/1000 | Loss: 0.00002311
Iteration 64/1000 | Loss: 0.00002310
Iteration 65/1000 | Loss: 0.00002310
Iteration 66/1000 | Loss: 0.00002310
Iteration 67/1000 | Loss: 0.00002310
Iteration 68/1000 | Loss: 0.00002310
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002309
Iteration 71/1000 | Loss: 0.00002309
Iteration 72/1000 | Loss: 0.00002309
Iteration 73/1000 | Loss: 0.00002307
Iteration 74/1000 | Loss: 0.00002307
Iteration 75/1000 | Loss: 0.00002306
Iteration 76/1000 | Loss: 0.00002306
Iteration 77/1000 | Loss: 0.00002306
Iteration 78/1000 | Loss: 0.00002305
Iteration 79/1000 | Loss: 0.00002305
Iteration 80/1000 | Loss: 0.00002305
Iteration 81/1000 | Loss: 0.00002305
Iteration 82/1000 | Loss: 0.00002304
Iteration 83/1000 | Loss: 0.00002304
Iteration 84/1000 | Loss: 0.00002304
Iteration 85/1000 | Loss: 0.00002304
Iteration 86/1000 | Loss: 0.00002304
Iteration 87/1000 | Loss: 0.00002303
Iteration 88/1000 | Loss: 0.00002303
Iteration 89/1000 | Loss: 0.00002303
Iteration 90/1000 | Loss: 0.00002303
Iteration 91/1000 | Loss: 0.00002303
Iteration 92/1000 | Loss: 0.00002303
Iteration 93/1000 | Loss: 0.00002303
Iteration 94/1000 | Loss: 0.00002302
Iteration 95/1000 | Loss: 0.00002302
Iteration 96/1000 | Loss: 0.00002302
Iteration 97/1000 | Loss: 0.00002302
Iteration 98/1000 | Loss: 0.00002302
Iteration 99/1000 | Loss: 0.00002302
Iteration 100/1000 | Loss: 0.00002302
Iteration 101/1000 | Loss: 0.00002302
Iteration 102/1000 | Loss: 0.00002302
Iteration 103/1000 | Loss: 0.00002302
Iteration 104/1000 | Loss: 0.00002301
Iteration 105/1000 | Loss: 0.00002301
Iteration 106/1000 | Loss: 0.00002301
Iteration 107/1000 | Loss: 0.00002301
Iteration 108/1000 | Loss: 0.00002301
Iteration 109/1000 | Loss: 0.00002301
Iteration 110/1000 | Loss: 0.00002301
Iteration 111/1000 | Loss: 0.00002301
Iteration 112/1000 | Loss: 0.00002300
Iteration 113/1000 | Loss: 0.00002300
Iteration 114/1000 | Loss: 0.00002300
Iteration 115/1000 | Loss: 0.00002300
Iteration 116/1000 | Loss: 0.00002300
Iteration 117/1000 | Loss: 0.00002300
Iteration 118/1000 | Loss: 0.00002300
Iteration 119/1000 | Loss: 0.00002300
Iteration 120/1000 | Loss: 0.00002300
Iteration 121/1000 | Loss: 0.00002300
Iteration 122/1000 | Loss: 0.00002299
Iteration 123/1000 | Loss: 0.00002299
Iteration 124/1000 | Loss: 0.00002299
Iteration 125/1000 | Loss: 0.00002299
Iteration 126/1000 | Loss: 0.00002299
Iteration 127/1000 | Loss: 0.00002299
Iteration 128/1000 | Loss: 0.00002298
Iteration 129/1000 | Loss: 0.00002298
Iteration 130/1000 | Loss: 0.00002298
Iteration 131/1000 | Loss: 0.00002298
Iteration 132/1000 | Loss: 0.00002298
Iteration 133/1000 | Loss: 0.00002298
Iteration 134/1000 | Loss: 0.00002298
Iteration 135/1000 | Loss: 0.00002298
Iteration 136/1000 | Loss: 0.00002298
Iteration 137/1000 | Loss: 0.00002298
Iteration 138/1000 | Loss: 0.00002298
Iteration 139/1000 | Loss: 0.00002298
Iteration 140/1000 | Loss: 0.00002298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.298163053637836e-05, 2.298163053637836e-05, 2.298163053637836e-05, 2.298163053637836e-05, 2.298163053637836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.298163053637836e-05

Optimization complete. Final v2v error: 4.1530046463012695 mm

Highest mean error: 4.562814235687256 mm for frame 211

Lowest mean error: 3.749412775039673 mm for frame 84

Saving results

Total time: 39.26199460029602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663967
Iteration 2/25 | Loss: 0.00150104
Iteration 3/25 | Loss: 0.00139153
Iteration 4/25 | Loss: 0.00136785
Iteration 5/25 | Loss: 0.00135861
Iteration 6/25 | Loss: 0.00135651
Iteration 7/25 | Loss: 0.00135616
Iteration 8/25 | Loss: 0.00135616
Iteration 9/25 | Loss: 0.00135616
Iteration 10/25 | Loss: 0.00135616
Iteration 11/25 | Loss: 0.00135616
Iteration 12/25 | Loss: 0.00135616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013561648083850741, 0.0013561648083850741, 0.0013561648083850741, 0.0013561648083850741, 0.0013561648083850741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013561648083850741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.66351986
Iteration 2/25 | Loss: 0.00243085
Iteration 3/25 | Loss: 0.00243082
Iteration 4/25 | Loss: 0.00243082
Iteration 5/25 | Loss: 0.00243082
Iteration 6/25 | Loss: 0.00243082
Iteration 7/25 | Loss: 0.00243082
Iteration 8/25 | Loss: 0.00243082
Iteration 9/25 | Loss: 0.00243082
Iteration 10/25 | Loss: 0.00243082
Iteration 11/25 | Loss: 0.00243082
Iteration 12/25 | Loss: 0.00243082
Iteration 13/25 | Loss: 0.00243082
Iteration 14/25 | Loss: 0.00243082
Iteration 15/25 | Loss: 0.00243082
Iteration 16/25 | Loss: 0.00243082
Iteration 17/25 | Loss: 0.00243082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002430819906294346, 0.002430819906294346, 0.002430819906294346, 0.002430819906294346, 0.002430819906294346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002430819906294346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243082
Iteration 2/1000 | Loss: 0.00005719
Iteration 3/1000 | Loss: 0.00003289
Iteration 4/1000 | Loss: 0.00002814
Iteration 5/1000 | Loss: 0.00002580
Iteration 6/1000 | Loss: 0.00002419
Iteration 7/1000 | Loss: 0.00002360
Iteration 8/1000 | Loss: 0.00002317
Iteration 9/1000 | Loss: 0.00002290
Iteration 10/1000 | Loss: 0.00002266
Iteration 11/1000 | Loss: 0.00002242
Iteration 12/1000 | Loss: 0.00002239
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002233
Iteration 15/1000 | Loss: 0.00002233
Iteration 16/1000 | Loss: 0.00002225
Iteration 17/1000 | Loss: 0.00002225
Iteration 18/1000 | Loss: 0.00002223
Iteration 19/1000 | Loss: 0.00002222
Iteration 20/1000 | Loss: 0.00002222
Iteration 21/1000 | Loss: 0.00002221
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002221
Iteration 24/1000 | Loss: 0.00002221
Iteration 25/1000 | Loss: 0.00002221
Iteration 26/1000 | Loss: 0.00002221
Iteration 27/1000 | Loss: 0.00002221
Iteration 28/1000 | Loss: 0.00002221
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002221
Iteration 31/1000 | Loss: 0.00002220
Iteration 32/1000 | Loss: 0.00002220
Iteration 33/1000 | Loss: 0.00002217
Iteration 34/1000 | Loss: 0.00002216
Iteration 35/1000 | Loss: 0.00002216
Iteration 36/1000 | Loss: 0.00002215
Iteration 37/1000 | Loss: 0.00002214
Iteration 38/1000 | Loss: 0.00002214
Iteration 39/1000 | Loss: 0.00002214
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002213
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002212
Iteration 44/1000 | Loss: 0.00002212
Iteration 45/1000 | Loss: 0.00002211
Iteration 46/1000 | Loss: 0.00002211
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002210
Iteration 49/1000 | Loss: 0.00002209
Iteration 50/1000 | Loss: 0.00002209
Iteration 51/1000 | Loss: 0.00002209
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002208
Iteration 56/1000 | Loss: 0.00002207
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002207
Iteration 59/1000 | Loss: 0.00002207
Iteration 60/1000 | Loss: 0.00002207
Iteration 61/1000 | Loss: 0.00002206
Iteration 62/1000 | Loss: 0.00002206
Iteration 63/1000 | Loss: 0.00002205
Iteration 64/1000 | Loss: 0.00002205
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002204
Iteration 67/1000 | Loss: 0.00002204
Iteration 68/1000 | Loss: 0.00002204
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002203
Iteration 72/1000 | Loss: 0.00002202
Iteration 73/1000 | Loss: 0.00002202
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002202
Iteration 76/1000 | Loss: 0.00002202
Iteration 77/1000 | Loss: 0.00002202
Iteration 78/1000 | Loss: 0.00002202
Iteration 79/1000 | Loss: 0.00002202
Iteration 80/1000 | Loss: 0.00002202
Iteration 81/1000 | Loss: 0.00002202
Iteration 82/1000 | Loss: 0.00002202
Iteration 83/1000 | Loss: 0.00002202
Iteration 84/1000 | Loss: 0.00002202
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002201
Iteration 87/1000 | Loss: 0.00002201
Iteration 88/1000 | Loss: 0.00002201
Iteration 89/1000 | Loss: 0.00002201
Iteration 90/1000 | Loss: 0.00002201
Iteration 91/1000 | Loss: 0.00002201
Iteration 92/1000 | Loss: 0.00002201
Iteration 93/1000 | Loss: 0.00002201
Iteration 94/1000 | Loss: 0.00002201
Iteration 95/1000 | Loss: 0.00002201
Iteration 96/1000 | Loss: 0.00002200
Iteration 97/1000 | Loss: 0.00002200
Iteration 98/1000 | Loss: 0.00002200
Iteration 99/1000 | Loss: 0.00002200
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002200
Iteration 103/1000 | Loss: 0.00002200
Iteration 104/1000 | Loss: 0.00002200
Iteration 105/1000 | Loss: 0.00002200
Iteration 106/1000 | Loss: 0.00002200
Iteration 107/1000 | Loss: 0.00002200
Iteration 108/1000 | Loss: 0.00002200
Iteration 109/1000 | Loss: 0.00002200
Iteration 110/1000 | Loss: 0.00002200
Iteration 111/1000 | Loss: 0.00002200
Iteration 112/1000 | Loss: 0.00002200
Iteration 113/1000 | Loss: 0.00002200
Iteration 114/1000 | Loss: 0.00002200
Iteration 115/1000 | Loss: 0.00002200
Iteration 116/1000 | Loss: 0.00002200
Iteration 117/1000 | Loss: 0.00002200
Iteration 118/1000 | Loss: 0.00002200
Iteration 119/1000 | Loss: 0.00002200
Iteration 120/1000 | Loss: 0.00002200
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.1995045244693756e-05, 2.1995045244693756e-05, 2.1995045244693756e-05, 2.1995045244693756e-05, 2.1995045244693756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1995045244693756e-05

Optimization complete. Final v2v error: 4.150805473327637 mm

Highest mean error: 4.708849906921387 mm for frame 154

Lowest mean error: 3.717308521270752 mm for frame 43

Saving results

Total time: 33.265769481658936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840673
Iteration 2/25 | Loss: 0.00158215
Iteration 3/25 | Loss: 0.00139442
Iteration 4/25 | Loss: 0.00134225
Iteration 5/25 | Loss: 0.00131507
Iteration 6/25 | Loss: 0.00131255
Iteration 7/25 | Loss: 0.00131175
Iteration 8/25 | Loss: 0.00131140
Iteration 9/25 | Loss: 0.00131131
Iteration 10/25 | Loss: 0.00131128
Iteration 11/25 | Loss: 0.00131128
Iteration 12/25 | Loss: 0.00131128
Iteration 13/25 | Loss: 0.00131128
Iteration 14/25 | Loss: 0.00131128
Iteration 15/25 | Loss: 0.00131126
Iteration 16/25 | Loss: 0.00131126
Iteration 17/25 | Loss: 0.00131126
Iteration 18/25 | Loss: 0.00131126
Iteration 19/25 | Loss: 0.00131126
Iteration 20/25 | Loss: 0.00131126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013112566666677594, 0.0013112566666677594, 0.0013112566666677594, 0.0013112566666677594, 0.0013112566666677594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013112566666677594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.21706581
Iteration 2/25 | Loss: 0.00227928
Iteration 3/25 | Loss: 0.00227921
Iteration 4/25 | Loss: 0.00227921
Iteration 5/25 | Loss: 0.00227921
Iteration 6/25 | Loss: 0.00227921
Iteration 7/25 | Loss: 0.00227921
Iteration 8/25 | Loss: 0.00227921
Iteration 9/25 | Loss: 0.00227921
Iteration 10/25 | Loss: 0.00227921
Iteration 11/25 | Loss: 0.00227921
Iteration 12/25 | Loss: 0.00227921
Iteration 13/25 | Loss: 0.00227921
Iteration 14/25 | Loss: 0.00227921
Iteration 15/25 | Loss: 0.00227921
Iteration 16/25 | Loss: 0.00227921
Iteration 17/25 | Loss: 0.00227921
Iteration 18/25 | Loss: 0.00227921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002279209904372692, 0.002279209904372692, 0.002279209904372692, 0.002279209904372692, 0.002279209904372692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002279209904372692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227921
Iteration 2/1000 | Loss: 0.00004764
Iteration 3/1000 | Loss: 0.00003455
Iteration 4/1000 | Loss: 0.00003046
Iteration 5/1000 | Loss: 0.00002853
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002540
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002481
Iteration 13/1000 | Loss: 0.00002468
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002453
Iteration 16/1000 | Loss: 0.00002451
Iteration 17/1000 | Loss: 0.00002449
Iteration 18/1000 | Loss: 0.00002441
Iteration 19/1000 | Loss: 0.00002438
Iteration 20/1000 | Loss: 0.00002433
Iteration 21/1000 | Loss: 0.00002425
Iteration 22/1000 | Loss: 0.00002423
Iteration 23/1000 | Loss: 0.00002408
Iteration 24/1000 | Loss: 0.00002406
Iteration 25/1000 | Loss: 0.00002403
Iteration 26/1000 | Loss: 0.00002398
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002388
Iteration 29/1000 | Loss: 0.00002388
Iteration 30/1000 | Loss: 0.00002386
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002385
Iteration 33/1000 | Loss: 0.00002385
Iteration 34/1000 | Loss: 0.00002385
Iteration 35/1000 | Loss: 0.00002384
Iteration 36/1000 | Loss: 0.00002384
Iteration 37/1000 | Loss: 0.00002383
Iteration 38/1000 | Loss: 0.00002383
Iteration 39/1000 | Loss: 0.00002382
Iteration 40/1000 | Loss: 0.00002382
Iteration 41/1000 | Loss: 0.00002381
Iteration 42/1000 | Loss: 0.00002380
Iteration 43/1000 | Loss: 0.00002380
Iteration 44/1000 | Loss: 0.00002379
Iteration 45/1000 | Loss: 0.00002376
Iteration 46/1000 | Loss: 0.00002372
Iteration 47/1000 | Loss: 0.00002370
Iteration 48/1000 | Loss: 0.00002367
Iteration 49/1000 | Loss: 0.00002360
Iteration 50/1000 | Loss: 0.00002359
Iteration 51/1000 | Loss: 0.00002358
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002348
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002343
Iteration 56/1000 | Loss: 0.00002339
Iteration 57/1000 | Loss: 0.00002338
Iteration 58/1000 | Loss: 0.00002338
Iteration 59/1000 | Loss: 0.00002337
Iteration 60/1000 | Loss: 0.00002336
Iteration 61/1000 | Loss: 0.00002335
Iteration 62/1000 | Loss: 0.00002334
Iteration 63/1000 | Loss: 0.00002334
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00002333
Iteration 66/1000 | Loss: 0.00002332
Iteration 67/1000 | Loss: 0.00002331
Iteration 68/1000 | Loss: 0.00002331
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002330
Iteration 71/1000 | Loss: 0.00002329
Iteration 72/1000 | Loss: 0.00002608
Iteration 73/1000 | Loss: 0.00002398
Iteration 74/1000 | Loss: 0.00002323
Iteration 75/1000 | Loss: 0.00002323
Iteration 76/1000 | Loss: 0.00002323
Iteration 77/1000 | Loss: 0.00002323
Iteration 78/1000 | Loss: 0.00002509
Iteration 79/1000 | Loss: 0.00002362
Iteration 80/1000 | Loss: 0.00002428
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002432
Iteration 83/1000 | Loss: 0.00002399
Iteration 84/1000 | Loss: 0.00002421
Iteration 85/1000 | Loss: 0.00002410
Iteration 86/1000 | Loss: 0.00002416
Iteration 87/1000 | Loss: 0.00002416
Iteration 88/1000 | Loss: 0.00002430
Iteration 89/1000 | Loss: 0.00002415
Iteration 90/1000 | Loss: 0.00002412
Iteration 91/1000 | Loss: 0.00002419
Iteration 92/1000 | Loss: 0.00002399
Iteration 93/1000 | Loss: 0.00002417
Iteration 94/1000 | Loss: 0.00002412
Iteration 95/1000 | Loss: 0.00002395
Iteration 96/1000 | Loss: 0.00002401
Iteration 97/1000 | Loss: 0.00002424
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002386
Iteration 100/1000 | Loss: 0.00002418
Iteration 101/1000 | Loss: 0.00002408
Iteration 102/1000 | Loss: 0.00002431
Iteration 103/1000 | Loss: 0.00002412
Iteration 104/1000 | Loss: 0.00002413
Iteration 105/1000 | Loss: 0.00002395
Iteration 106/1000 | Loss: 0.00002357
Iteration 107/1000 | Loss: 0.00002389
Iteration 108/1000 | Loss: 0.00002406
Iteration 109/1000 | Loss: 0.00002414
Iteration 110/1000 | Loss: 0.00002404
Iteration 111/1000 | Loss: 0.00002412
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002409
Iteration 114/1000 | Loss: 0.00002385
Iteration 115/1000 | Loss: 0.00002380
Iteration 116/1000 | Loss: 0.00002417
Iteration 117/1000 | Loss: 0.00002393
Iteration 118/1000 | Loss: 0.00002412
Iteration 119/1000 | Loss: 0.00002414
Iteration 120/1000 | Loss: 0.00002416
Iteration 121/1000 | Loss: 0.00002392
Iteration 122/1000 | Loss: 0.00002415
Iteration 123/1000 | Loss: 0.00002398
Iteration 124/1000 | Loss: 0.00002408
Iteration 125/1000 | Loss: 0.00002393
Iteration 126/1000 | Loss: 0.00002403
Iteration 127/1000 | Loss: 0.00002408
Iteration 128/1000 | Loss: 0.00002422
Iteration 129/1000 | Loss: 0.00002412
Iteration 130/1000 | Loss: 0.00002403
Iteration 131/1000 | Loss: 0.00002390
Iteration 132/1000 | Loss: 0.00002397
Iteration 133/1000 | Loss: 0.00002386
Iteration 134/1000 | Loss: 0.00002389
Iteration 135/1000 | Loss: 0.00002405
Iteration 136/1000 | Loss: 0.00002413
Iteration 137/1000 | Loss: 0.00002418
Iteration 138/1000 | Loss: 0.00002421
Iteration 139/1000 | Loss: 0.00002418
Iteration 140/1000 | Loss: 0.00002410
Iteration 141/1000 | Loss: 0.00002423
Iteration 142/1000 | Loss: 0.00002422
Iteration 143/1000 | Loss: 0.00002393
Iteration 144/1000 | Loss: 0.00002406
Iteration 145/1000 | Loss: 0.00002409
Iteration 146/1000 | Loss: 0.00002367
Iteration 147/1000 | Loss: 0.00002379
Iteration 148/1000 | Loss: 0.00002414
Iteration 149/1000 | Loss: 0.00002415
Iteration 150/1000 | Loss: 0.00002423
Iteration 151/1000 | Loss: 0.00002413
Iteration 152/1000 | Loss: 0.00002381
Iteration 153/1000 | Loss: 0.00002395
Iteration 154/1000 | Loss: 0.00002414
Iteration 155/1000 | Loss: 0.00002411
Iteration 156/1000 | Loss: 0.00002414
Iteration 157/1000 | Loss: 0.00002402
Iteration 158/1000 | Loss: 0.00002416
Iteration 159/1000 | Loss: 0.00002410
Iteration 160/1000 | Loss: 0.00002420
Iteration 161/1000 | Loss: 0.00002412
Iteration 162/1000 | Loss: 0.00002434
Iteration 163/1000 | Loss: 0.00002405
Iteration 164/1000 | Loss: 0.00002392
Iteration 165/1000 | Loss: 0.00002386
Iteration 166/1000 | Loss: 0.00002391
Iteration 167/1000 | Loss: 0.00002409
Iteration 168/1000 | Loss: 0.00002397
Iteration 169/1000 | Loss: 0.00002394
Iteration 170/1000 | Loss: 0.00002377
Iteration 171/1000 | Loss: 0.00002397
Iteration 172/1000 | Loss: 0.00002376
Iteration 173/1000 | Loss: 0.00002400
Iteration 174/1000 | Loss: 0.00002387
Iteration 175/1000 | Loss: 0.00002410
Iteration 176/1000 | Loss: 0.00002386
Iteration 177/1000 | Loss: 0.00002402
Iteration 178/1000 | Loss: 0.00002402
Iteration 179/1000 | Loss: 0.00002402
Iteration 180/1000 | Loss: 0.00002405
Iteration 181/1000 | Loss: 0.00002371
Iteration 182/1000 | Loss: 0.00002404
Iteration 183/1000 | Loss: 0.00002404
Iteration 184/1000 | Loss: 0.00002389
Iteration 185/1000 | Loss: 0.00002398
Iteration 186/1000 | Loss: 0.00002398
Iteration 187/1000 | Loss: 0.00002397
Iteration 188/1000 | Loss: 0.00002396
Iteration 189/1000 | Loss: 0.00002396
Iteration 190/1000 | Loss: 0.00002396
Iteration 191/1000 | Loss: 0.00002312
Iteration 192/1000 | Loss: 0.00002381
Iteration 193/1000 | Loss: 0.00002381
Iteration 194/1000 | Loss: 0.00002381
Iteration 195/1000 | Loss: 0.00002380
Iteration 196/1000 | Loss: 0.00002339
Iteration 197/1000 | Loss: 0.00002384
Iteration 198/1000 | Loss: 0.00002394
Iteration 199/1000 | Loss: 0.00002391
Iteration 200/1000 | Loss: 0.00002298
Iteration 201/1000 | Loss: 0.00002298
Iteration 202/1000 | Loss: 0.00002298
Iteration 203/1000 | Loss: 0.00002298
Iteration 204/1000 | Loss: 0.00002298
Iteration 205/1000 | Loss: 0.00002297
Iteration 206/1000 | Loss: 0.00002297
Iteration 207/1000 | Loss: 0.00002297
Iteration 208/1000 | Loss: 0.00002297
Iteration 209/1000 | Loss: 0.00002296
Iteration 210/1000 | Loss: 0.00002296
Iteration 211/1000 | Loss: 0.00002296
Iteration 212/1000 | Loss: 0.00002295
Iteration 213/1000 | Loss: 0.00002295
Iteration 214/1000 | Loss: 0.00002295
Iteration 215/1000 | Loss: 0.00002295
Iteration 216/1000 | Loss: 0.00002295
Iteration 217/1000 | Loss: 0.00002295
Iteration 218/1000 | Loss: 0.00002295
Iteration 219/1000 | Loss: 0.00002295
Iteration 220/1000 | Loss: 0.00002295
Iteration 221/1000 | Loss: 0.00002295
Iteration 222/1000 | Loss: 0.00002295
Iteration 223/1000 | Loss: 0.00002295
Iteration 224/1000 | Loss: 0.00002295
Iteration 225/1000 | Loss: 0.00002295
Iteration 226/1000 | Loss: 0.00002295
Iteration 227/1000 | Loss: 0.00002295
Iteration 228/1000 | Loss: 0.00002295
Iteration 229/1000 | Loss: 0.00002295
Iteration 230/1000 | Loss: 0.00002295
Iteration 231/1000 | Loss: 0.00002295
Iteration 232/1000 | Loss: 0.00002295
Iteration 233/1000 | Loss: 0.00002294
Iteration 234/1000 | Loss: 0.00002294
Iteration 235/1000 | Loss: 0.00002294
Iteration 236/1000 | Loss: 0.00002294
Iteration 237/1000 | Loss: 0.00002294
Iteration 238/1000 | Loss: 0.00002294
Iteration 239/1000 | Loss: 0.00002294
Iteration 240/1000 | Loss: 0.00002294
Iteration 241/1000 | Loss: 0.00002294
Iteration 242/1000 | Loss: 0.00002294
Iteration 243/1000 | Loss: 0.00002294
Iteration 244/1000 | Loss: 0.00002294
Iteration 245/1000 | Loss: 0.00002294
Iteration 246/1000 | Loss: 0.00002294
Iteration 247/1000 | Loss: 0.00002294
Iteration 248/1000 | Loss: 0.00002294
Iteration 249/1000 | Loss: 0.00002294
Iteration 250/1000 | Loss: 0.00002294
Iteration 251/1000 | Loss: 0.00002294
Iteration 252/1000 | Loss: 0.00002294
Iteration 253/1000 | Loss: 0.00002294
Iteration 254/1000 | Loss: 0.00002294
Iteration 255/1000 | Loss: 0.00002294
Iteration 256/1000 | Loss: 0.00002294
Iteration 257/1000 | Loss: 0.00002294
Iteration 258/1000 | Loss: 0.00002294
Iteration 259/1000 | Loss: 0.00002294
Iteration 260/1000 | Loss: 0.00002294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.294284240633715e-05, 2.294284240633715e-05, 2.294284240633715e-05, 2.294284240633715e-05, 2.294284240633715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.294284240633715e-05

Optimization complete. Final v2v error: 3.9744713306427 mm

Highest mean error: 15.979122161865234 mm for frame 158

Lowest mean error: 3.4554855823516846 mm for frame 232

Saving results

Total time: 205.58780193328857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035118
Iteration 2/25 | Loss: 0.00177098
Iteration 3/25 | Loss: 0.00154083
Iteration 4/25 | Loss: 0.00149785
Iteration 5/25 | Loss: 0.00148105
Iteration 6/25 | Loss: 0.00147834
Iteration 7/25 | Loss: 0.00147799
Iteration 8/25 | Loss: 0.00147799
Iteration 9/25 | Loss: 0.00147799
Iteration 10/25 | Loss: 0.00147799
Iteration 11/25 | Loss: 0.00147799
Iteration 12/25 | Loss: 0.00147799
Iteration 13/25 | Loss: 0.00147799
Iteration 14/25 | Loss: 0.00147799
Iteration 15/25 | Loss: 0.00147799
Iteration 16/25 | Loss: 0.00147799
Iteration 17/25 | Loss: 0.00147799
Iteration 18/25 | Loss: 0.00147799
Iteration 19/25 | Loss: 0.00147799
Iteration 20/25 | Loss: 0.00147799
Iteration 21/25 | Loss: 0.00147799
Iteration 22/25 | Loss: 0.00147799
Iteration 23/25 | Loss: 0.00147799
Iteration 24/25 | Loss: 0.00147799
Iteration 25/25 | Loss: 0.00147799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53519487
Iteration 2/25 | Loss: 0.00218241
Iteration 3/25 | Loss: 0.00218239
Iteration 4/25 | Loss: 0.00218239
Iteration 5/25 | Loss: 0.00218239
Iteration 6/25 | Loss: 0.00218239
Iteration 7/25 | Loss: 0.00218239
Iteration 8/25 | Loss: 0.00218239
Iteration 9/25 | Loss: 0.00218239
Iteration 10/25 | Loss: 0.00218239
Iteration 11/25 | Loss: 0.00218239
Iteration 12/25 | Loss: 0.00218239
Iteration 13/25 | Loss: 0.00218239
Iteration 14/25 | Loss: 0.00218239
Iteration 15/25 | Loss: 0.00218239
Iteration 16/25 | Loss: 0.00218239
Iteration 17/25 | Loss: 0.00218239
Iteration 18/25 | Loss: 0.00218239
Iteration 19/25 | Loss: 0.00218239
Iteration 20/25 | Loss: 0.00218239
Iteration 21/25 | Loss: 0.00218239
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002182386117056012, 0.002182386117056012, 0.002182386117056012, 0.002182386117056012, 0.002182386117056012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002182386117056012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218239
Iteration 2/1000 | Loss: 0.00007560
Iteration 3/1000 | Loss: 0.00005410
Iteration 4/1000 | Loss: 0.00004768
Iteration 5/1000 | Loss: 0.00004530
Iteration 6/1000 | Loss: 0.00004289
Iteration 7/1000 | Loss: 0.00004173
Iteration 8/1000 | Loss: 0.00004061
Iteration 9/1000 | Loss: 0.00003987
Iteration 10/1000 | Loss: 0.00003927
Iteration 11/1000 | Loss: 0.00003887
Iteration 12/1000 | Loss: 0.00003855
Iteration 13/1000 | Loss: 0.00003835
Iteration 14/1000 | Loss: 0.00003821
Iteration 15/1000 | Loss: 0.00003817
Iteration 16/1000 | Loss: 0.00003816
Iteration 17/1000 | Loss: 0.00003815
Iteration 18/1000 | Loss: 0.00003813
Iteration 19/1000 | Loss: 0.00003812
Iteration 20/1000 | Loss: 0.00003811
Iteration 21/1000 | Loss: 0.00003811
Iteration 22/1000 | Loss: 0.00003809
Iteration 23/1000 | Loss: 0.00003806
Iteration 24/1000 | Loss: 0.00003806
Iteration 25/1000 | Loss: 0.00003806
Iteration 26/1000 | Loss: 0.00003805
Iteration 27/1000 | Loss: 0.00003802
Iteration 28/1000 | Loss: 0.00003801
Iteration 29/1000 | Loss: 0.00003798
Iteration 30/1000 | Loss: 0.00003796
Iteration 31/1000 | Loss: 0.00003796
Iteration 32/1000 | Loss: 0.00003796
Iteration 33/1000 | Loss: 0.00003795
Iteration 34/1000 | Loss: 0.00003795
Iteration 35/1000 | Loss: 0.00003795
Iteration 36/1000 | Loss: 0.00003794
Iteration 37/1000 | Loss: 0.00003794
Iteration 38/1000 | Loss: 0.00003794
Iteration 39/1000 | Loss: 0.00003794
Iteration 40/1000 | Loss: 0.00003793
Iteration 41/1000 | Loss: 0.00003793
Iteration 42/1000 | Loss: 0.00003793
Iteration 43/1000 | Loss: 0.00003793
Iteration 44/1000 | Loss: 0.00003793
Iteration 45/1000 | Loss: 0.00003793
Iteration 46/1000 | Loss: 0.00003793
Iteration 47/1000 | Loss: 0.00003792
Iteration 48/1000 | Loss: 0.00003792
Iteration 49/1000 | Loss: 0.00003792
Iteration 50/1000 | Loss: 0.00003792
Iteration 51/1000 | Loss: 0.00003792
Iteration 52/1000 | Loss: 0.00003792
Iteration 53/1000 | Loss: 0.00003791
Iteration 54/1000 | Loss: 0.00003791
Iteration 55/1000 | Loss: 0.00003791
Iteration 56/1000 | Loss: 0.00003790
Iteration 57/1000 | Loss: 0.00003790
Iteration 58/1000 | Loss: 0.00003790
Iteration 59/1000 | Loss: 0.00003790
Iteration 60/1000 | Loss: 0.00003790
Iteration 61/1000 | Loss: 0.00003790
Iteration 62/1000 | Loss: 0.00003790
Iteration 63/1000 | Loss: 0.00003789
Iteration 64/1000 | Loss: 0.00003789
Iteration 65/1000 | Loss: 0.00003789
Iteration 66/1000 | Loss: 0.00003788
Iteration 67/1000 | Loss: 0.00003788
Iteration 68/1000 | Loss: 0.00003787
Iteration 69/1000 | Loss: 0.00003787
Iteration 70/1000 | Loss: 0.00003787
Iteration 71/1000 | Loss: 0.00003787
Iteration 72/1000 | Loss: 0.00003787
Iteration 73/1000 | Loss: 0.00003786
Iteration 74/1000 | Loss: 0.00003786
Iteration 75/1000 | Loss: 0.00003786
Iteration 76/1000 | Loss: 0.00003786
Iteration 77/1000 | Loss: 0.00003785
Iteration 78/1000 | Loss: 0.00003785
Iteration 79/1000 | Loss: 0.00003785
Iteration 80/1000 | Loss: 0.00003785
Iteration 81/1000 | Loss: 0.00003785
Iteration 82/1000 | Loss: 0.00003785
Iteration 83/1000 | Loss: 0.00003785
Iteration 84/1000 | Loss: 0.00003785
Iteration 85/1000 | Loss: 0.00003785
Iteration 86/1000 | Loss: 0.00003785
Iteration 87/1000 | Loss: 0.00003785
Iteration 88/1000 | Loss: 0.00003785
Iteration 89/1000 | Loss: 0.00003785
Iteration 90/1000 | Loss: 0.00003785
Iteration 91/1000 | Loss: 0.00003785
Iteration 92/1000 | Loss: 0.00003785
Iteration 93/1000 | Loss: 0.00003785
Iteration 94/1000 | Loss: 0.00003785
Iteration 95/1000 | Loss: 0.00003785
Iteration 96/1000 | Loss: 0.00003785
Iteration 97/1000 | Loss: 0.00003785
Iteration 98/1000 | Loss: 0.00003785
Iteration 99/1000 | Loss: 0.00003785
Iteration 100/1000 | Loss: 0.00003785
Iteration 101/1000 | Loss: 0.00003785
Iteration 102/1000 | Loss: 0.00003785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [3.784540604101494e-05, 3.784540604101494e-05, 3.784540604101494e-05, 3.784540604101494e-05, 3.784540604101494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.784540604101494e-05

Optimization complete. Final v2v error: 5.218420028686523 mm

Highest mean error: 6.409722328186035 mm for frame 63

Lowest mean error: 4.33327054977417 mm for frame 131

Saving results

Total time: 36.05845355987549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064701
Iteration 2/25 | Loss: 0.00170310
Iteration 3/25 | Loss: 0.00149248
Iteration 4/25 | Loss: 0.00143881
Iteration 5/25 | Loss: 0.00141991
Iteration 6/25 | Loss: 0.00141711
Iteration 7/25 | Loss: 0.00141647
Iteration 8/25 | Loss: 0.00141647
Iteration 9/25 | Loss: 0.00141647
Iteration 10/25 | Loss: 0.00141647
Iteration 11/25 | Loss: 0.00141647
Iteration 12/25 | Loss: 0.00141647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014164714375510812, 0.0014164714375510812, 0.0014164714375510812, 0.0014164714375510812, 0.0014164714375510812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014164714375510812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.31623602
Iteration 2/25 | Loss: 0.00275075
Iteration 3/25 | Loss: 0.00275071
Iteration 4/25 | Loss: 0.00275071
Iteration 5/25 | Loss: 0.00275071
Iteration 6/25 | Loss: 0.00275071
Iteration 7/25 | Loss: 0.00275071
Iteration 8/25 | Loss: 0.00275071
Iteration 9/25 | Loss: 0.00275071
Iteration 10/25 | Loss: 0.00275071
Iteration 11/25 | Loss: 0.00275071
Iteration 12/25 | Loss: 0.00275071
Iteration 13/25 | Loss: 0.00275071
Iteration 14/25 | Loss: 0.00275071
Iteration 15/25 | Loss: 0.00275071
Iteration 16/25 | Loss: 0.00275071
Iteration 17/25 | Loss: 0.00275071
Iteration 18/25 | Loss: 0.00275071
Iteration 19/25 | Loss: 0.00275071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0027507070917636156, 0.0027507070917636156, 0.0027507070917636156, 0.0027507070917636156, 0.0027507070917636156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027507070917636156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275071
Iteration 2/1000 | Loss: 0.00005388
Iteration 3/1000 | Loss: 0.00003906
Iteration 4/1000 | Loss: 0.00003536
Iteration 5/1000 | Loss: 0.00003373
Iteration 6/1000 | Loss: 0.00003205
Iteration 7/1000 | Loss: 0.00003146
Iteration 8/1000 | Loss: 0.00003104
Iteration 9/1000 | Loss: 0.00003069
Iteration 10/1000 | Loss: 0.00003035
Iteration 11/1000 | Loss: 0.00003016
Iteration 12/1000 | Loss: 0.00003005
Iteration 13/1000 | Loss: 0.00003005
Iteration 14/1000 | Loss: 0.00003004
Iteration 15/1000 | Loss: 0.00003004
Iteration 16/1000 | Loss: 0.00003004
Iteration 17/1000 | Loss: 0.00003004
Iteration 18/1000 | Loss: 0.00003004
Iteration 19/1000 | Loss: 0.00003002
Iteration 20/1000 | Loss: 0.00003002
Iteration 21/1000 | Loss: 0.00003001
Iteration 22/1000 | Loss: 0.00002999
Iteration 23/1000 | Loss: 0.00002998
Iteration 24/1000 | Loss: 0.00002998
Iteration 25/1000 | Loss: 0.00002997
Iteration 26/1000 | Loss: 0.00002997
Iteration 27/1000 | Loss: 0.00002997
Iteration 28/1000 | Loss: 0.00002997
Iteration 29/1000 | Loss: 0.00002997
Iteration 30/1000 | Loss: 0.00002997
Iteration 31/1000 | Loss: 0.00002996
Iteration 32/1000 | Loss: 0.00002996
Iteration 33/1000 | Loss: 0.00002996
Iteration 34/1000 | Loss: 0.00002996
Iteration 35/1000 | Loss: 0.00002996
Iteration 36/1000 | Loss: 0.00002996
Iteration 37/1000 | Loss: 0.00002996
Iteration 38/1000 | Loss: 0.00002996
Iteration 39/1000 | Loss: 0.00002996
Iteration 40/1000 | Loss: 0.00002995
Iteration 41/1000 | Loss: 0.00002995
Iteration 42/1000 | Loss: 0.00002995
Iteration 43/1000 | Loss: 0.00002994
Iteration 44/1000 | Loss: 0.00002994
Iteration 45/1000 | Loss: 0.00002993
Iteration 46/1000 | Loss: 0.00002993
Iteration 47/1000 | Loss: 0.00002993
Iteration 48/1000 | Loss: 0.00002993
Iteration 49/1000 | Loss: 0.00002992
Iteration 50/1000 | Loss: 0.00002992
Iteration 51/1000 | Loss: 0.00002992
Iteration 52/1000 | Loss: 0.00002992
Iteration 53/1000 | Loss: 0.00002992
Iteration 54/1000 | Loss: 0.00002992
Iteration 55/1000 | Loss: 0.00002992
Iteration 56/1000 | Loss: 0.00002992
Iteration 57/1000 | Loss: 0.00002992
Iteration 58/1000 | Loss: 0.00002992
Iteration 59/1000 | Loss: 0.00002992
Iteration 60/1000 | Loss: 0.00002991
Iteration 61/1000 | Loss: 0.00002991
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002991
Iteration 67/1000 | Loss: 0.00002991
Iteration 68/1000 | Loss: 0.00002991
Iteration 69/1000 | Loss: 0.00002991
Iteration 70/1000 | Loss: 0.00002990
Iteration 71/1000 | Loss: 0.00002990
Iteration 72/1000 | Loss: 0.00002990
Iteration 73/1000 | Loss: 0.00002990
Iteration 74/1000 | Loss: 0.00002990
Iteration 75/1000 | Loss: 0.00002990
Iteration 76/1000 | Loss: 0.00002990
Iteration 77/1000 | Loss: 0.00002990
Iteration 78/1000 | Loss: 0.00002990
Iteration 79/1000 | Loss: 0.00002990
Iteration 80/1000 | Loss: 0.00002990
Iteration 81/1000 | Loss: 0.00002990
Iteration 82/1000 | Loss: 0.00002990
Iteration 83/1000 | Loss: 0.00002990
Iteration 84/1000 | Loss: 0.00002990
Iteration 85/1000 | Loss: 0.00002989
Iteration 86/1000 | Loss: 0.00002989
Iteration 87/1000 | Loss: 0.00002989
Iteration 88/1000 | Loss: 0.00002989
Iteration 89/1000 | Loss: 0.00002989
Iteration 90/1000 | Loss: 0.00002989
Iteration 91/1000 | Loss: 0.00002989
Iteration 92/1000 | Loss: 0.00002989
Iteration 93/1000 | Loss: 0.00002989
Iteration 94/1000 | Loss: 0.00002989
Iteration 95/1000 | Loss: 0.00002988
Iteration 96/1000 | Loss: 0.00002988
Iteration 97/1000 | Loss: 0.00002988
Iteration 98/1000 | Loss: 0.00002988
Iteration 99/1000 | Loss: 0.00002988
Iteration 100/1000 | Loss: 0.00002988
Iteration 101/1000 | Loss: 0.00002988
Iteration 102/1000 | Loss: 0.00002987
Iteration 103/1000 | Loss: 0.00002987
Iteration 104/1000 | Loss: 0.00002987
Iteration 105/1000 | Loss: 0.00002987
Iteration 106/1000 | Loss: 0.00002987
Iteration 107/1000 | Loss: 0.00002986
Iteration 108/1000 | Loss: 0.00002986
Iteration 109/1000 | Loss: 0.00002986
Iteration 110/1000 | Loss: 0.00002986
Iteration 111/1000 | Loss: 0.00002986
Iteration 112/1000 | Loss: 0.00002986
Iteration 113/1000 | Loss: 0.00002986
Iteration 114/1000 | Loss: 0.00002986
Iteration 115/1000 | Loss: 0.00002986
Iteration 116/1000 | Loss: 0.00002986
Iteration 117/1000 | Loss: 0.00002986
Iteration 118/1000 | Loss: 0.00002986
Iteration 119/1000 | Loss: 0.00002986
Iteration 120/1000 | Loss: 0.00002986
Iteration 121/1000 | Loss: 0.00002986
Iteration 122/1000 | Loss: 0.00002986
Iteration 123/1000 | Loss: 0.00002986
Iteration 124/1000 | Loss: 0.00002986
Iteration 125/1000 | Loss: 0.00002986
Iteration 126/1000 | Loss: 0.00002986
Iteration 127/1000 | Loss: 0.00002986
Iteration 128/1000 | Loss: 0.00002986
Iteration 129/1000 | Loss: 0.00002986
Iteration 130/1000 | Loss: 0.00002986
Iteration 131/1000 | Loss: 0.00002986
Iteration 132/1000 | Loss: 0.00002986
Iteration 133/1000 | Loss: 0.00002986
Iteration 134/1000 | Loss: 0.00002986
Iteration 135/1000 | Loss: 0.00002986
Iteration 136/1000 | Loss: 0.00002986
Iteration 137/1000 | Loss: 0.00002986
Iteration 138/1000 | Loss: 0.00002986
Iteration 139/1000 | Loss: 0.00002986
Iteration 140/1000 | Loss: 0.00002986
Iteration 141/1000 | Loss: 0.00002986
Iteration 142/1000 | Loss: 0.00002986
Iteration 143/1000 | Loss: 0.00002986
Iteration 144/1000 | Loss: 0.00002986
Iteration 145/1000 | Loss: 0.00002986
Iteration 146/1000 | Loss: 0.00002986
Iteration 147/1000 | Loss: 0.00002986
Iteration 148/1000 | Loss: 0.00002986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.9863003874197602e-05, 2.9863003874197602e-05, 2.9863003874197602e-05, 2.9863003874197602e-05, 2.9863003874197602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9863003874197602e-05

Optimization complete. Final v2v error: 4.686910152435303 mm

Highest mean error: 4.926218032836914 mm for frame 28

Lowest mean error: 4.380596160888672 mm for frame 71

Saving results

Total time: 33.41848063468933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01186770
Iteration 2/25 | Loss: 0.00260172
Iteration 3/25 | Loss: 0.00373635
Iteration 4/25 | Loss: 0.00303227
Iteration 5/25 | Loss: 0.00230507
Iteration 6/25 | Loss: 0.00243618
Iteration 7/25 | Loss: 0.00277558
Iteration 8/25 | Loss: 0.00232885
Iteration 9/25 | Loss: 0.00207025
Iteration 10/25 | Loss: 0.00187867
Iteration 11/25 | Loss: 0.00163520
Iteration 12/25 | Loss: 0.00158682
Iteration 13/25 | Loss: 0.00148787
Iteration 14/25 | Loss: 0.00151006
Iteration 15/25 | Loss: 0.00146050
Iteration 16/25 | Loss: 0.00144468
Iteration 17/25 | Loss: 0.00144676
Iteration 18/25 | Loss: 0.00137731
Iteration 19/25 | Loss: 0.00137050
Iteration 20/25 | Loss: 0.00134881
Iteration 21/25 | Loss: 0.00139601
Iteration 22/25 | Loss: 0.00135291
Iteration 23/25 | Loss: 0.00134768
Iteration 24/25 | Loss: 0.00134889
Iteration 25/25 | Loss: 0.00135625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63176024
Iteration 2/25 | Loss: 0.00388750
Iteration 3/25 | Loss: 0.00388748
Iteration 4/25 | Loss: 0.00388748
Iteration 5/25 | Loss: 0.00388748
Iteration 6/25 | Loss: 0.00388748
Iteration 7/25 | Loss: 0.00388748
Iteration 8/25 | Loss: 0.00388748
Iteration 9/25 | Loss: 0.00388748
Iteration 10/25 | Loss: 0.00388748
Iteration 11/25 | Loss: 0.00388748
Iteration 12/25 | Loss: 0.00388748
Iteration 13/25 | Loss: 0.00388748
Iteration 14/25 | Loss: 0.00388748
Iteration 15/25 | Loss: 0.00388748
Iteration 16/25 | Loss: 0.00388748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003887477098032832, 0.003887477098032832, 0.003887477098032832, 0.003887477098032832, 0.003887477098032832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003887477098032832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00388748
Iteration 2/1000 | Loss: 0.00239428
Iteration 3/1000 | Loss: 0.00249243
Iteration 4/1000 | Loss: 0.00383895
Iteration 5/1000 | Loss: 0.00274756
Iteration 6/1000 | Loss: 0.00186148
Iteration 7/1000 | Loss: 0.00169948
Iteration 8/1000 | Loss: 0.00326748
Iteration 9/1000 | Loss: 0.00176428
Iteration 10/1000 | Loss: 0.00306288
Iteration 11/1000 | Loss: 0.00331838
Iteration 12/1000 | Loss: 0.00315162
Iteration 13/1000 | Loss: 0.00373131
Iteration 14/1000 | Loss: 0.00261050
Iteration 15/1000 | Loss: 0.00290771
Iteration 16/1000 | Loss: 0.00188919
Iteration 17/1000 | Loss: 0.00257000
Iteration 18/1000 | Loss: 0.00236038
Iteration 19/1000 | Loss: 0.00115809
Iteration 20/1000 | Loss: 0.00194822
Iteration 21/1000 | Loss: 0.00172668
Iteration 22/1000 | Loss: 0.00058408
Iteration 23/1000 | Loss: 0.00091592
Iteration 24/1000 | Loss: 0.00224226
Iteration 25/1000 | Loss: 0.00083111
Iteration 26/1000 | Loss: 0.00337417
Iteration 27/1000 | Loss: 0.00296882
Iteration 28/1000 | Loss: 0.00296905
Iteration 29/1000 | Loss: 0.00282596
Iteration 30/1000 | Loss: 0.00304411
Iteration 31/1000 | Loss: 0.00386487
Iteration 32/1000 | Loss: 0.00102465
Iteration 33/1000 | Loss: 0.00132015
Iteration 34/1000 | Loss: 0.00044396
Iteration 35/1000 | Loss: 0.00231871
Iteration 36/1000 | Loss: 0.00161333
Iteration 37/1000 | Loss: 0.00149559
Iteration 38/1000 | Loss: 0.00093752
Iteration 39/1000 | Loss: 0.00362090
Iteration 40/1000 | Loss: 0.00428604
Iteration 41/1000 | Loss: 0.00300860
Iteration 42/1000 | Loss: 0.00327849
Iteration 43/1000 | Loss: 0.00155707
Iteration 44/1000 | Loss: 0.00139803
Iteration 45/1000 | Loss: 0.00145462
Iteration 46/1000 | Loss: 0.00126675
Iteration 47/1000 | Loss: 0.00087697
Iteration 48/1000 | Loss: 0.00127980
Iteration 49/1000 | Loss: 0.00183353
Iteration 50/1000 | Loss: 0.00108763
Iteration 51/1000 | Loss: 0.00193336
Iteration 52/1000 | Loss: 0.00014315
Iteration 53/1000 | Loss: 0.00009021
Iteration 54/1000 | Loss: 0.00006895
Iteration 55/1000 | Loss: 0.00006256
Iteration 56/1000 | Loss: 0.00007270
Iteration 57/1000 | Loss: 0.00006719
Iteration 58/1000 | Loss: 0.00008218
Iteration 59/1000 | Loss: 0.00122528
Iteration 60/1000 | Loss: 0.00329044
Iteration 61/1000 | Loss: 0.00204073
Iteration 62/1000 | Loss: 0.00275770
Iteration 63/1000 | Loss: 0.00292786
Iteration 64/1000 | Loss: 0.00349259
Iteration 65/1000 | Loss: 0.00246569
Iteration 66/1000 | Loss: 0.00218070
Iteration 67/1000 | Loss: 0.00128332
Iteration 68/1000 | Loss: 0.00270450
Iteration 69/1000 | Loss: 0.00360785
Iteration 70/1000 | Loss: 0.00299588
Iteration 71/1000 | Loss: 0.00273999
Iteration 72/1000 | Loss: 0.00299200
Iteration 73/1000 | Loss: 0.00459696
Iteration 74/1000 | Loss: 0.00251423
Iteration 75/1000 | Loss: 0.00237337
Iteration 76/1000 | Loss: 0.00179031
Iteration 77/1000 | Loss: 0.00204619
Iteration 78/1000 | Loss: 0.00474534
Iteration 79/1000 | Loss: 0.00306015
Iteration 80/1000 | Loss: 0.00210406
Iteration 81/1000 | Loss: 0.00211546
Iteration 82/1000 | Loss: 0.00314411
Iteration 83/1000 | Loss: 0.00311367
Iteration 84/1000 | Loss: 0.00279627
Iteration 85/1000 | Loss: 0.00292383
Iteration 86/1000 | Loss: 0.00328221
Iteration 87/1000 | Loss: 0.00159870
Iteration 88/1000 | Loss: 0.00153204
Iteration 89/1000 | Loss: 0.00124141
Iteration 90/1000 | Loss: 0.00133815
Iteration 91/1000 | Loss: 0.00332502
Iteration 92/1000 | Loss: 0.00181993
Iteration 93/1000 | Loss: 0.00191568
Iteration 94/1000 | Loss: 0.00133577
Iteration 95/1000 | Loss: 0.00131194
Iteration 96/1000 | Loss: 0.00134286
Iteration 97/1000 | Loss: 0.00201628
Iteration 98/1000 | Loss: 0.00160562
Iteration 99/1000 | Loss: 0.00252662
Iteration 100/1000 | Loss: 0.00139684
Iteration 101/1000 | Loss: 0.00146637
Iteration 102/1000 | Loss: 0.00161038
Iteration 103/1000 | Loss: 0.00152161
Iteration 104/1000 | Loss: 0.00166011
Iteration 105/1000 | Loss: 0.00176712
Iteration 106/1000 | Loss: 0.00010129
Iteration 107/1000 | Loss: 0.00142519
Iteration 108/1000 | Loss: 0.00014374
Iteration 109/1000 | Loss: 0.00142983
Iteration 110/1000 | Loss: 0.00231421
Iteration 111/1000 | Loss: 0.00187351
Iteration 112/1000 | Loss: 0.00142496
Iteration 113/1000 | Loss: 0.00060183
Iteration 114/1000 | Loss: 0.00181066
Iteration 115/1000 | Loss: 0.00172549
Iteration 116/1000 | Loss: 0.00084373
Iteration 117/1000 | Loss: 0.00338635
Iteration 118/1000 | Loss: 0.00034920
Iteration 119/1000 | Loss: 0.00021609
Iteration 120/1000 | Loss: 0.00033034
Iteration 121/1000 | Loss: 0.00025095
Iteration 122/1000 | Loss: 0.00039139
Iteration 123/1000 | Loss: 0.00054192
Iteration 124/1000 | Loss: 0.00060678
Iteration 125/1000 | Loss: 0.00031338
Iteration 126/1000 | Loss: 0.00062755
Iteration 127/1000 | Loss: 0.00063294
Iteration 128/1000 | Loss: 0.00056312
Iteration 129/1000 | Loss: 0.00038708
Iteration 130/1000 | Loss: 0.00036539
Iteration 131/1000 | Loss: 0.00026393
Iteration 132/1000 | Loss: 0.00034909
Iteration 133/1000 | Loss: 0.00021983
Iteration 134/1000 | Loss: 0.00069036
Iteration 135/1000 | Loss: 0.00008612
Iteration 136/1000 | Loss: 0.00007862
Iteration 137/1000 | Loss: 0.00079944
Iteration 138/1000 | Loss: 0.00080518
Iteration 139/1000 | Loss: 0.00068729
Iteration 140/1000 | Loss: 0.00091200
Iteration 141/1000 | Loss: 0.00084253
Iteration 142/1000 | Loss: 0.00074254
Iteration 143/1000 | Loss: 0.00029267
Iteration 144/1000 | Loss: 0.00094396
Iteration 145/1000 | Loss: 0.00060653
Iteration 146/1000 | Loss: 0.00007113
Iteration 147/1000 | Loss: 0.00065420
Iteration 148/1000 | Loss: 0.00080792
Iteration 149/1000 | Loss: 0.00008452
Iteration 150/1000 | Loss: 0.00062428
Iteration 151/1000 | Loss: 0.00010669
Iteration 152/1000 | Loss: 0.00080940
Iteration 153/1000 | Loss: 0.00074056
Iteration 154/1000 | Loss: 0.00012422
Iteration 155/1000 | Loss: 0.00135384
Iteration 156/1000 | Loss: 0.00012117
Iteration 157/1000 | Loss: 0.00007895
Iteration 158/1000 | Loss: 0.00006163
Iteration 159/1000 | Loss: 0.00006977
Iteration 160/1000 | Loss: 0.00006408
Iteration 161/1000 | Loss: 0.00006236
Iteration 162/1000 | Loss: 0.00005656
Iteration 163/1000 | Loss: 0.00010649
Iteration 164/1000 | Loss: 0.00043443
Iteration 165/1000 | Loss: 0.00011158
Iteration 166/1000 | Loss: 0.00010466
Iteration 167/1000 | Loss: 0.00007905
Iteration 168/1000 | Loss: 0.00006029
Iteration 169/1000 | Loss: 0.00005199
Iteration 170/1000 | Loss: 0.00005473
Iteration 171/1000 | Loss: 0.00006261
Iteration 172/1000 | Loss: 0.00006117
Iteration 173/1000 | Loss: 0.00006083
Iteration 174/1000 | Loss: 0.00005802
Iteration 175/1000 | Loss: 0.00006140
Iteration 176/1000 | Loss: 0.00005570
Iteration 177/1000 | Loss: 0.00005990
Iteration 178/1000 | Loss: 0.00005531
Iteration 179/1000 | Loss: 0.00005882
Iteration 180/1000 | Loss: 0.00006121
Iteration 181/1000 | Loss: 0.00005181
Iteration 182/1000 | Loss: 0.00005829
Iteration 183/1000 | Loss: 0.00005748
Iteration 184/1000 | Loss: 0.00005782
Iteration 185/1000 | Loss: 0.00005745
Iteration 186/1000 | Loss: 0.00006865
Iteration 187/1000 | Loss: 0.00005293
Iteration 188/1000 | Loss: 0.00005606
Iteration 189/1000 | Loss: 0.00004607
Iteration 190/1000 | Loss: 0.00005358
Iteration 191/1000 | Loss: 0.00005380
Iteration 192/1000 | Loss: 0.00007644
Iteration 193/1000 | Loss: 0.00005888
Iteration 194/1000 | Loss: 0.00006086
Iteration 195/1000 | Loss: 0.00005556
Iteration 196/1000 | Loss: 0.00006225
Iteration 197/1000 | Loss: 0.00005238
Iteration 198/1000 | Loss: 0.00005732
Iteration 199/1000 | Loss: 0.00005454
Iteration 200/1000 | Loss: 0.00007453
Iteration 201/1000 | Loss: 0.00005421
Iteration 202/1000 | Loss: 0.00007124
Iteration 203/1000 | Loss: 0.00005717
Iteration 204/1000 | Loss: 0.00005770
Iteration 205/1000 | Loss: 0.00004875
Iteration 206/1000 | Loss: 0.00005689
Iteration 207/1000 | Loss: 0.00004759
Iteration 208/1000 | Loss: 0.00005470
Iteration 209/1000 | Loss: 0.00004781
Iteration 210/1000 | Loss: 0.00006912
Iteration 211/1000 | Loss: 0.00005771
Iteration 212/1000 | Loss: 0.00004874
Iteration 213/1000 | Loss: 0.00005695
Iteration 214/1000 | Loss: 0.00004632
Iteration 215/1000 | Loss: 0.00005255
Iteration 216/1000 | Loss: 0.00005389
Iteration 217/1000 | Loss: 0.00006001
Iteration 218/1000 | Loss: 0.00008356
Iteration 219/1000 | Loss: 0.00006012
Iteration 220/1000 | Loss: 0.00005534
Iteration 221/1000 | Loss: 0.00005206
Iteration 222/1000 | Loss: 0.00005399
Iteration 223/1000 | Loss: 0.00006507
Iteration 224/1000 | Loss: 0.00006479
Iteration 225/1000 | Loss: 0.00005662
Iteration 226/1000 | Loss: 0.00005162
Iteration 227/1000 | Loss: 0.00005624
Iteration 228/1000 | Loss: 0.00005562
Iteration 229/1000 | Loss: 0.00007241
Iteration 230/1000 | Loss: 0.00005581
Iteration 231/1000 | Loss: 0.00005262
Iteration 232/1000 | Loss: 0.00005703
Iteration 233/1000 | Loss: 0.00004211
Iteration 234/1000 | Loss: 0.00005747
Iteration 235/1000 | Loss: 0.00005612
Iteration 236/1000 | Loss: 0.00004803
Iteration 237/1000 | Loss: 0.00006041
Iteration 238/1000 | Loss: 0.00005576
Iteration 239/1000 | Loss: 0.00005476
Iteration 240/1000 | Loss: 0.00006573
Iteration 241/1000 | Loss: 0.00007825
Iteration 242/1000 | Loss: 0.00006537
Iteration 243/1000 | Loss: 0.00005735
Iteration 244/1000 | Loss: 0.00005573
Iteration 245/1000 | Loss: 0.00005349
Iteration 246/1000 | Loss: 0.00005668
Iteration 247/1000 | Loss: 0.00005454
Iteration 248/1000 | Loss: 0.00006062
Iteration 249/1000 | Loss: 0.00006473
Iteration 250/1000 | Loss: 0.00005764
Iteration 251/1000 | Loss: 0.00004957
Iteration 252/1000 | Loss: 0.00005657
Iteration 253/1000 | Loss: 0.00005331
Iteration 254/1000 | Loss: 0.00006240
Iteration 255/1000 | Loss: 0.00006921
Iteration 256/1000 | Loss: 0.00003702
Iteration 257/1000 | Loss: 0.00003242
Iteration 258/1000 | Loss: 0.00003035
Iteration 259/1000 | Loss: 0.00002920
Iteration 260/1000 | Loss: 0.00002864
Iteration 261/1000 | Loss: 0.00030941
Iteration 262/1000 | Loss: 0.00003615
Iteration 263/1000 | Loss: 0.00003320
Iteration 264/1000 | Loss: 0.00003144
Iteration 265/1000 | Loss: 0.00003073
Iteration 266/1000 | Loss: 0.00002992
Iteration 267/1000 | Loss: 0.00002947
Iteration 268/1000 | Loss: 0.00002894
Iteration 269/1000 | Loss: 0.00002865
Iteration 270/1000 | Loss: 0.00002843
Iteration 271/1000 | Loss: 0.00002830
Iteration 272/1000 | Loss: 0.00002829
Iteration 273/1000 | Loss: 0.00002822
Iteration 274/1000 | Loss: 0.00029439
Iteration 275/1000 | Loss: 0.00028713
Iteration 276/1000 | Loss: 0.00018698
Iteration 277/1000 | Loss: 0.00024259
Iteration 278/1000 | Loss: 0.00012131
Iteration 279/1000 | Loss: 0.00017150
Iteration 280/1000 | Loss: 0.00004613
Iteration 281/1000 | Loss: 0.00003912
Iteration 282/1000 | Loss: 0.00039161
Iteration 283/1000 | Loss: 0.00004635
Iteration 284/1000 | Loss: 0.00003778
Iteration 285/1000 | Loss: 0.00003244
Iteration 286/1000 | Loss: 0.00033705
Iteration 287/1000 | Loss: 0.00007351
Iteration 288/1000 | Loss: 0.00004006
Iteration 289/1000 | Loss: 0.00003681
Iteration 290/1000 | Loss: 0.00003149
Iteration 291/1000 | Loss: 0.00003029
Iteration 292/1000 | Loss: 0.00002925
Iteration 293/1000 | Loss: 0.00002865
Iteration 294/1000 | Loss: 0.00002804
Iteration 295/1000 | Loss: 0.00002750
Iteration 296/1000 | Loss: 0.00002719
Iteration 297/1000 | Loss: 0.00002700
Iteration 298/1000 | Loss: 0.00002699
Iteration 299/1000 | Loss: 0.00002692
Iteration 300/1000 | Loss: 0.00002687
Iteration 301/1000 | Loss: 0.00002678
Iteration 302/1000 | Loss: 0.00002664
Iteration 303/1000 | Loss: 0.00002658
Iteration 304/1000 | Loss: 0.00002658
Iteration 305/1000 | Loss: 0.00002658
Iteration 306/1000 | Loss: 0.00002657
Iteration 307/1000 | Loss: 0.00002656
Iteration 308/1000 | Loss: 0.00002656
Iteration 309/1000 | Loss: 0.00002655
Iteration 310/1000 | Loss: 0.00002654
Iteration 311/1000 | Loss: 0.00002653
Iteration 312/1000 | Loss: 0.00002653
Iteration 313/1000 | Loss: 0.00002652
Iteration 314/1000 | Loss: 0.00002648
Iteration 315/1000 | Loss: 0.00002645
Iteration 316/1000 | Loss: 0.00002645
Iteration 317/1000 | Loss: 0.00002644
Iteration 318/1000 | Loss: 0.00002644
Iteration 319/1000 | Loss: 0.00002640
Iteration 320/1000 | Loss: 0.00002640
Iteration 321/1000 | Loss: 0.00002635
Iteration 322/1000 | Loss: 0.00002633
Iteration 323/1000 | Loss: 0.00002632
Iteration 324/1000 | Loss: 0.00002632
Iteration 325/1000 | Loss: 0.00002629
Iteration 326/1000 | Loss: 0.00002629
Iteration 327/1000 | Loss: 0.00002629
Iteration 328/1000 | Loss: 0.00002628
Iteration 329/1000 | Loss: 0.00002627
Iteration 330/1000 | Loss: 0.00002627
Iteration 331/1000 | Loss: 0.00002627
Iteration 332/1000 | Loss: 0.00002626
Iteration 333/1000 | Loss: 0.00002626
Iteration 334/1000 | Loss: 0.00002625
Iteration 335/1000 | Loss: 0.00002624
Iteration 336/1000 | Loss: 0.00002624
Iteration 337/1000 | Loss: 0.00002620
Iteration 338/1000 | Loss: 0.00002620
Iteration 339/1000 | Loss: 0.00002618
Iteration 340/1000 | Loss: 0.00002618
Iteration 341/1000 | Loss: 0.00002617
Iteration 342/1000 | Loss: 0.00002617
Iteration 343/1000 | Loss: 0.00002616
Iteration 344/1000 | Loss: 0.00002616
Iteration 345/1000 | Loss: 0.00002615
Iteration 346/1000 | Loss: 0.00002615
Iteration 347/1000 | Loss: 0.00002615
Iteration 348/1000 | Loss: 0.00002614
Iteration 349/1000 | Loss: 0.00002613
Iteration 350/1000 | Loss: 0.00002613
Iteration 351/1000 | Loss: 0.00002612
Iteration 352/1000 | Loss: 0.00002612
Iteration 353/1000 | Loss: 0.00002612
Iteration 354/1000 | Loss: 0.00002612
Iteration 355/1000 | Loss: 0.00002611
Iteration 356/1000 | Loss: 0.00002611
Iteration 357/1000 | Loss: 0.00002611
Iteration 358/1000 | Loss: 0.00002611
Iteration 359/1000 | Loss: 0.00002610
Iteration 360/1000 | Loss: 0.00002610
Iteration 361/1000 | Loss: 0.00002610
Iteration 362/1000 | Loss: 0.00002610
Iteration 363/1000 | Loss: 0.00002610
Iteration 364/1000 | Loss: 0.00002610
Iteration 365/1000 | Loss: 0.00002610
Iteration 366/1000 | Loss: 0.00002610
Iteration 367/1000 | Loss: 0.00002610
Iteration 368/1000 | Loss: 0.00002609
Iteration 369/1000 | Loss: 0.00002609
Iteration 370/1000 | Loss: 0.00002609
Iteration 371/1000 | Loss: 0.00002609
Iteration 372/1000 | Loss: 0.00002609
Iteration 373/1000 | Loss: 0.00002609
Iteration 374/1000 | Loss: 0.00028913
Iteration 375/1000 | Loss: 0.00003426
Iteration 376/1000 | Loss: 0.00003104
Iteration 377/1000 | Loss: 0.00002918
Iteration 378/1000 | Loss: 0.00002840
Iteration 379/1000 | Loss: 0.00002781
Iteration 380/1000 | Loss: 0.00002750
Iteration 381/1000 | Loss: 0.00002714
Iteration 382/1000 | Loss: 0.00002695
Iteration 383/1000 | Loss: 0.00002675
Iteration 384/1000 | Loss: 0.00002647
Iteration 385/1000 | Loss: 0.00002645
Iteration 386/1000 | Loss: 0.00002631
Iteration 387/1000 | Loss: 0.00002629
Iteration 388/1000 | Loss: 0.00002628
Iteration 389/1000 | Loss: 0.00002623
Iteration 390/1000 | Loss: 0.00002623
Iteration 391/1000 | Loss: 0.00002623
Iteration 392/1000 | Loss: 0.00002623
Iteration 393/1000 | Loss: 0.00002623
Iteration 394/1000 | Loss: 0.00002622
Iteration 395/1000 | Loss: 0.00002622
Iteration 396/1000 | Loss: 0.00002622
Iteration 397/1000 | Loss: 0.00002621
Iteration 398/1000 | Loss: 0.00002621
Iteration 399/1000 | Loss: 0.00002621
Iteration 400/1000 | Loss: 0.00002620
Iteration 401/1000 | Loss: 0.00002620
Iteration 402/1000 | Loss: 0.00002620
Iteration 403/1000 | Loss: 0.00002619
Iteration 404/1000 | Loss: 0.00002619
Iteration 405/1000 | Loss: 0.00002619
Iteration 406/1000 | Loss: 0.00002618
Iteration 407/1000 | Loss: 0.00002618
Iteration 408/1000 | Loss: 0.00002618
Iteration 409/1000 | Loss: 0.00002617
Iteration 410/1000 | Loss: 0.00002617
Iteration 411/1000 | Loss: 0.00002617
Iteration 412/1000 | Loss: 0.00002616
Iteration 413/1000 | Loss: 0.00002616
Iteration 414/1000 | Loss: 0.00002616
Iteration 415/1000 | Loss: 0.00002616
Iteration 416/1000 | Loss: 0.00002616
Iteration 417/1000 | Loss: 0.00002616
Iteration 418/1000 | Loss: 0.00002616
Iteration 419/1000 | Loss: 0.00002616
Iteration 420/1000 | Loss: 0.00002615
Iteration 421/1000 | Loss: 0.00002615
Iteration 422/1000 | Loss: 0.00002615
Iteration 423/1000 | Loss: 0.00002614
Iteration 424/1000 | Loss: 0.00002614
Iteration 425/1000 | Loss: 0.00002614
Iteration 426/1000 | Loss: 0.00002614
Iteration 427/1000 | Loss: 0.00002614
Iteration 428/1000 | Loss: 0.00002613
Iteration 429/1000 | Loss: 0.00002613
Iteration 430/1000 | Loss: 0.00002613
Iteration 431/1000 | Loss: 0.00002613
Iteration 432/1000 | Loss: 0.00002612
Iteration 433/1000 | Loss: 0.00002612
Iteration 434/1000 | Loss: 0.00002612
Iteration 435/1000 | Loss: 0.00030946
Iteration 436/1000 | Loss: 0.00003236
Iteration 437/1000 | Loss: 0.00002902
Iteration 438/1000 | Loss: 0.00002747
Iteration 439/1000 | Loss: 0.00002683
Iteration 440/1000 | Loss: 0.00002636
Iteration 441/1000 | Loss: 0.00002598
Iteration 442/1000 | Loss: 0.00002575
Iteration 443/1000 | Loss: 0.00002566
Iteration 444/1000 | Loss: 0.00002566
Iteration 445/1000 | Loss: 0.00002565
Iteration 446/1000 | Loss: 0.00002561
Iteration 447/1000 | Loss: 0.00002561
Iteration 448/1000 | Loss: 0.00002559
Iteration 449/1000 | Loss: 0.00002559
Iteration 450/1000 | Loss: 0.00002558
Iteration 451/1000 | Loss: 0.00002555
Iteration 452/1000 | Loss: 0.00002554
Iteration 453/1000 | Loss: 0.00002554
Iteration 454/1000 | Loss: 0.00002553
Iteration 455/1000 | Loss: 0.00002553
Iteration 456/1000 | Loss: 0.00002553
Iteration 457/1000 | Loss: 0.00002552
Iteration 458/1000 | Loss: 0.00002552
Iteration 459/1000 | Loss: 0.00002552
Iteration 460/1000 | Loss: 0.00002552
Iteration 461/1000 | Loss: 0.00002552
Iteration 462/1000 | Loss: 0.00002551
Iteration 463/1000 | Loss: 0.00002551
Iteration 464/1000 | Loss: 0.00002551
Iteration 465/1000 | Loss: 0.00002551
Iteration 466/1000 | Loss: 0.00002551
Iteration 467/1000 | Loss: 0.00002551
Iteration 468/1000 | Loss: 0.00002551
Iteration 469/1000 | Loss: 0.00002550
Iteration 470/1000 | Loss: 0.00002550
Iteration 471/1000 | Loss: 0.00002550
Iteration 472/1000 | Loss: 0.00002550
Iteration 473/1000 | Loss: 0.00002550
Iteration 474/1000 | Loss: 0.00002550
Iteration 475/1000 | Loss: 0.00002550
Iteration 476/1000 | Loss: 0.00002550
Iteration 477/1000 | Loss: 0.00002550
Iteration 478/1000 | Loss: 0.00002550
Iteration 479/1000 | Loss: 0.00002550
Iteration 480/1000 | Loss: 0.00002549
Iteration 481/1000 | Loss: 0.00002549
Iteration 482/1000 | Loss: 0.00002549
Iteration 483/1000 | Loss: 0.00002549
Iteration 484/1000 | Loss: 0.00002549
Iteration 485/1000 | Loss: 0.00002549
Iteration 486/1000 | Loss: 0.00002549
Iteration 487/1000 | Loss: 0.00002549
Iteration 488/1000 | Loss: 0.00002549
Iteration 489/1000 | Loss: 0.00002549
Iteration 490/1000 | Loss: 0.00002549
Iteration 491/1000 | Loss: 0.00002549
Iteration 492/1000 | Loss: 0.00002549
Iteration 493/1000 | Loss: 0.00002549
Iteration 494/1000 | Loss: 0.00002549
Iteration 495/1000 | Loss: 0.00002549
Iteration 496/1000 | Loss: 0.00002549
Iteration 497/1000 | Loss: 0.00002549
Iteration 498/1000 | Loss: 0.00002549
Iteration 499/1000 | Loss: 0.00002549
Iteration 500/1000 | Loss: 0.00002549
Iteration 501/1000 | Loss: 0.00002549
Iteration 502/1000 | Loss: 0.00002549
Iteration 503/1000 | Loss: 0.00002549
Iteration 504/1000 | Loss: 0.00002549
Iteration 505/1000 | Loss: 0.00002549
Iteration 506/1000 | Loss: 0.00002549
Iteration 507/1000 | Loss: 0.00002549
Iteration 508/1000 | Loss: 0.00002549
Iteration 509/1000 | Loss: 0.00002549
Iteration 510/1000 | Loss: 0.00002549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 510. Stopping optimization.
Last 5 losses: [2.5490617190371267e-05, 2.5490617190371267e-05, 2.5490617190371267e-05, 2.5490617190371267e-05, 2.5490617190371267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5490617190371267e-05

Optimization complete. Final v2v error: 4.233468532562256 mm

Highest mean error: 10.558053016662598 mm for frame 64

Lowest mean error: 3.5583295822143555 mm for frame 74

Saving results

Total time: 486.7874617576599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442526
Iteration 2/25 | Loss: 0.00194300
Iteration 3/25 | Loss: 0.00149435
Iteration 4/25 | Loss: 0.00142108
Iteration 5/25 | Loss: 0.00140587
Iteration 6/25 | Loss: 0.00140327
Iteration 7/25 | Loss: 0.00140295
Iteration 8/25 | Loss: 0.00140295
Iteration 9/25 | Loss: 0.00140295
Iteration 10/25 | Loss: 0.00140295
Iteration 11/25 | Loss: 0.00140295
Iteration 12/25 | Loss: 0.00140295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014029530575498939, 0.0014029530575498939, 0.0014029530575498939, 0.0014029530575498939, 0.0014029530575498939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014029530575498939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64525712
Iteration 2/25 | Loss: 0.00262806
Iteration 3/25 | Loss: 0.00262806
Iteration 4/25 | Loss: 0.00262806
Iteration 5/25 | Loss: 0.00262806
Iteration 6/25 | Loss: 0.00262806
Iteration 7/25 | Loss: 0.00262806
Iteration 8/25 | Loss: 0.00262806
Iteration 9/25 | Loss: 0.00262806
Iteration 10/25 | Loss: 0.00262806
Iteration 11/25 | Loss: 0.00262806
Iteration 12/25 | Loss: 0.00262806
Iteration 13/25 | Loss: 0.00262806
Iteration 14/25 | Loss: 0.00262806
Iteration 15/25 | Loss: 0.00262806
Iteration 16/25 | Loss: 0.00262806
Iteration 17/25 | Loss: 0.00262806
Iteration 18/25 | Loss: 0.00262806
Iteration 19/25 | Loss: 0.00262806
Iteration 20/25 | Loss: 0.00262806
Iteration 21/25 | Loss: 0.00262806
Iteration 22/25 | Loss: 0.00262806
Iteration 23/25 | Loss: 0.00262806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0026280563324689865, 0.0026280563324689865, 0.0026280563324689865, 0.0026280563324689865, 0.0026280563324689865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026280563324689865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262806
Iteration 2/1000 | Loss: 0.00004473
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002749
Iteration 5/1000 | Loss: 0.00002554
Iteration 6/1000 | Loss: 0.00002444
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002317
Iteration 9/1000 | Loss: 0.00002275
Iteration 10/1000 | Loss: 0.00002250
Iteration 11/1000 | Loss: 0.00002222
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002198
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002192
Iteration 16/1000 | Loss: 0.00002190
Iteration 17/1000 | Loss: 0.00002189
Iteration 18/1000 | Loss: 0.00002189
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002187
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002185
Iteration 28/1000 | Loss: 0.00002184
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002183
Iteration 31/1000 | Loss: 0.00002182
Iteration 32/1000 | Loss: 0.00002182
Iteration 33/1000 | Loss: 0.00002181
Iteration 34/1000 | Loss: 0.00002181
Iteration 35/1000 | Loss: 0.00002181
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002181
Iteration 40/1000 | Loss: 0.00002181
Iteration 41/1000 | Loss: 0.00002181
Iteration 42/1000 | Loss: 0.00002181
Iteration 43/1000 | Loss: 0.00002180
Iteration 44/1000 | Loss: 0.00002180
Iteration 45/1000 | Loss: 0.00002180
Iteration 46/1000 | Loss: 0.00002180
Iteration 47/1000 | Loss: 0.00002180
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002179
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002179
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002179
Iteration 62/1000 | Loss: 0.00002179
Iteration 63/1000 | Loss: 0.00002179
Iteration 64/1000 | Loss: 0.00002179
Iteration 65/1000 | Loss: 0.00002179
Iteration 66/1000 | Loss: 0.00002179
Iteration 67/1000 | Loss: 0.00002179
Iteration 68/1000 | Loss: 0.00002179
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002179
Iteration 74/1000 | Loss: 0.00002179
Iteration 75/1000 | Loss: 0.00002179
Iteration 76/1000 | Loss: 0.00002179
Iteration 77/1000 | Loss: 0.00002179
Iteration 78/1000 | Loss: 0.00002179
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.1787478544865735e-05, 2.1787478544865735e-05, 2.1787478544865735e-05, 2.1787478544865735e-05, 2.1787478544865735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1787478544865735e-05

Optimization complete. Final v2v error: 4.092799186706543 mm

Highest mean error: 4.364662170410156 mm for frame 120

Lowest mean error: 3.8390321731567383 mm for frame 31

Saving results

Total time: 30.06992506980896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01138849
Iteration 2/25 | Loss: 0.00270568
Iteration 3/25 | Loss: 0.00250920
Iteration 4/25 | Loss: 0.00243855
Iteration 5/25 | Loss: 0.00224661
Iteration 6/25 | Loss: 0.00234947
Iteration 7/25 | Loss: 0.00208911
Iteration 8/25 | Loss: 0.00184600
Iteration 9/25 | Loss: 0.00172818
Iteration 10/25 | Loss: 0.00170317
Iteration 11/25 | Loss: 0.00168259
Iteration 12/25 | Loss: 0.00167732
Iteration 13/25 | Loss: 0.00165829
Iteration 14/25 | Loss: 0.00165949
Iteration 15/25 | Loss: 0.00164629
Iteration 16/25 | Loss: 0.00163605
Iteration 17/25 | Loss: 0.00163319
Iteration 18/25 | Loss: 0.00163251
Iteration 19/25 | Loss: 0.00163208
Iteration 20/25 | Loss: 0.00162894
Iteration 21/25 | Loss: 0.00162705
Iteration 22/25 | Loss: 0.00162679
Iteration 23/25 | Loss: 0.00163400
Iteration 24/25 | Loss: 0.00162667
Iteration 25/25 | Loss: 0.00162538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58735168
Iteration 2/25 | Loss: 0.00410409
Iteration 3/25 | Loss: 0.00410409
Iteration 4/25 | Loss: 0.00410409
Iteration 5/25 | Loss: 0.00410409
Iteration 6/25 | Loss: 0.00410409
Iteration 7/25 | Loss: 0.00410409
Iteration 8/25 | Loss: 0.00410409
Iteration 9/25 | Loss: 0.00410409
Iteration 10/25 | Loss: 0.00410409
Iteration 11/25 | Loss: 0.00410409
Iteration 12/25 | Loss: 0.00410408
Iteration 13/25 | Loss: 0.00410409
Iteration 14/25 | Loss: 0.00410409
Iteration 15/25 | Loss: 0.00410409
Iteration 16/25 | Loss: 0.00410409
Iteration 17/25 | Loss: 0.00410409
Iteration 18/25 | Loss: 0.00410409
Iteration 19/25 | Loss: 0.00410409
Iteration 20/25 | Loss: 0.00410409
Iteration 21/25 | Loss: 0.00410409
Iteration 22/25 | Loss: 0.00410409
Iteration 23/25 | Loss: 0.00410409
Iteration 24/25 | Loss: 0.00410409
Iteration 25/25 | Loss: 0.00410409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00410409
Iteration 2/1000 | Loss: 0.00052135
Iteration 3/1000 | Loss: 0.00035278
Iteration 4/1000 | Loss: 0.00024284
Iteration 5/1000 | Loss: 0.00090676
Iteration 6/1000 | Loss: 0.00019169
Iteration 7/1000 | Loss: 0.00052266
Iteration 8/1000 | Loss: 0.00016585
Iteration 9/1000 | Loss: 0.00033125
Iteration 10/1000 | Loss: 0.00024413
Iteration 11/1000 | Loss: 0.00036875
Iteration 12/1000 | Loss: 0.00057678
Iteration 13/1000 | Loss: 0.00016609
Iteration 14/1000 | Loss: 0.00015878
Iteration 15/1000 | Loss: 0.00015359
Iteration 16/1000 | Loss: 0.00015194
Iteration 17/1000 | Loss: 0.00015104
Iteration 18/1000 | Loss: 0.00019277
Iteration 19/1000 | Loss: 0.00016305
Iteration 20/1000 | Loss: 0.00041990
Iteration 21/1000 | Loss: 0.00026928
Iteration 22/1000 | Loss: 0.00019565
Iteration 23/1000 | Loss: 0.00037994
Iteration 24/1000 | Loss: 0.00027554
Iteration 25/1000 | Loss: 0.00037553
Iteration 26/1000 | Loss: 0.00040540
Iteration 27/1000 | Loss: 0.00023208
Iteration 28/1000 | Loss: 0.00047416
Iteration 29/1000 | Loss: 0.00019684
Iteration 30/1000 | Loss: 0.00034053
Iteration 31/1000 | Loss: 0.00042924
Iteration 32/1000 | Loss: 0.00023326
Iteration 33/1000 | Loss: 0.00052795
Iteration 34/1000 | Loss: 0.00022685
Iteration 35/1000 | Loss: 0.00025009
Iteration 36/1000 | Loss: 0.00023436
Iteration 37/1000 | Loss: 0.00024099
Iteration 38/1000 | Loss: 0.00023020
Iteration 39/1000 | Loss: 0.00024462
Iteration 40/1000 | Loss: 0.00040317
Iteration 41/1000 | Loss: 0.00043446
Iteration 42/1000 | Loss: 0.00034222
Iteration 43/1000 | Loss: 0.00027379
Iteration 44/1000 | Loss: 0.00020057
Iteration 45/1000 | Loss: 0.00024401
Iteration 46/1000 | Loss: 0.00014993
Iteration 47/1000 | Loss: 0.00023093
Iteration 48/1000 | Loss: 0.00037366
Iteration 49/1000 | Loss: 0.00054460
Iteration 50/1000 | Loss: 0.00018066
Iteration 51/1000 | Loss: 0.00016963
Iteration 52/1000 | Loss: 0.00015264
Iteration 53/1000 | Loss: 0.00014793
Iteration 54/1000 | Loss: 0.00014670
Iteration 55/1000 | Loss: 0.00014454
Iteration 56/1000 | Loss: 0.00014362
Iteration 57/1000 | Loss: 0.00014303
Iteration 58/1000 | Loss: 0.00014276
Iteration 59/1000 | Loss: 0.00014275
Iteration 60/1000 | Loss: 0.00014260
Iteration 61/1000 | Loss: 0.00014256
Iteration 62/1000 | Loss: 0.00014246
Iteration 63/1000 | Loss: 0.00014244
Iteration 64/1000 | Loss: 0.00014244
Iteration 65/1000 | Loss: 0.00014243
Iteration 66/1000 | Loss: 0.00014240
Iteration 67/1000 | Loss: 0.00014239
Iteration 68/1000 | Loss: 0.00014239
Iteration 69/1000 | Loss: 0.00014239
Iteration 70/1000 | Loss: 0.00014238
Iteration 71/1000 | Loss: 0.00014238
Iteration 72/1000 | Loss: 0.00014238
Iteration 73/1000 | Loss: 0.00014238
Iteration 74/1000 | Loss: 0.00014238
Iteration 75/1000 | Loss: 0.00014238
Iteration 76/1000 | Loss: 0.00014238
Iteration 77/1000 | Loss: 0.00014238
Iteration 78/1000 | Loss: 0.00014238
Iteration 79/1000 | Loss: 0.00014238
Iteration 80/1000 | Loss: 0.00014237
Iteration 81/1000 | Loss: 0.00014237
Iteration 82/1000 | Loss: 0.00014237
Iteration 83/1000 | Loss: 0.00014236
Iteration 84/1000 | Loss: 0.00014236
Iteration 85/1000 | Loss: 0.00014234
Iteration 86/1000 | Loss: 0.00014233
Iteration 87/1000 | Loss: 0.00014233
Iteration 88/1000 | Loss: 0.00014232
Iteration 89/1000 | Loss: 0.00014232
Iteration 90/1000 | Loss: 0.00014232
Iteration 91/1000 | Loss: 0.00014231
Iteration 92/1000 | Loss: 0.00014231
Iteration 93/1000 | Loss: 0.00014231
Iteration 94/1000 | Loss: 0.00014230
Iteration 95/1000 | Loss: 0.00014229
Iteration 96/1000 | Loss: 0.00014229
Iteration 97/1000 | Loss: 0.00014228
Iteration 98/1000 | Loss: 0.00014228
Iteration 99/1000 | Loss: 0.00014227
Iteration 100/1000 | Loss: 0.00014227
Iteration 101/1000 | Loss: 0.00014226
Iteration 102/1000 | Loss: 0.00014225
Iteration 103/1000 | Loss: 0.00014225
Iteration 104/1000 | Loss: 0.00014225
Iteration 105/1000 | Loss: 0.00014225
Iteration 106/1000 | Loss: 0.00014225
Iteration 107/1000 | Loss: 0.00014224
Iteration 108/1000 | Loss: 0.00014224
Iteration 109/1000 | Loss: 0.00014224
Iteration 110/1000 | Loss: 0.00014224
Iteration 111/1000 | Loss: 0.00014224
Iteration 112/1000 | Loss: 0.00014224
Iteration 113/1000 | Loss: 0.00014224
Iteration 114/1000 | Loss: 0.00014223
Iteration 115/1000 | Loss: 0.00014223
Iteration 116/1000 | Loss: 0.00014223
Iteration 117/1000 | Loss: 0.00014223
Iteration 118/1000 | Loss: 0.00014223
Iteration 119/1000 | Loss: 0.00014222
Iteration 120/1000 | Loss: 0.00014221
Iteration 121/1000 | Loss: 0.00014221
Iteration 122/1000 | Loss: 0.00014221
Iteration 123/1000 | Loss: 0.00014221
Iteration 124/1000 | Loss: 0.00014221
Iteration 125/1000 | Loss: 0.00014221
Iteration 126/1000 | Loss: 0.00014220
Iteration 127/1000 | Loss: 0.00014220
Iteration 128/1000 | Loss: 0.00014220
Iteration 129/1000 | Loss: 0.00014220
Iteration 130/1000 | Loss: 0.00014220
Iteration 131/1000 | Loss: 0.00014219
Iteration 132/1000 | Loss: 0.00014219
Iteration 133/1000 | Loss: 0.00014219
Iteration 134/1000 | Loss: 0.00014219
Iteration 135/1000 | Loss: 0.00014218
Iteration 136/1000 | Loss: 0.00014218
Iteration 137/1000 | Loss: 0.00014218
Iteration 138/1000 | Loss: 0.00014218
Iteration 139/1000 | Loss: 0.00014218
Iteration 140/1000 | Loss: 0.00014218
Iteration 141/1000 | Loss: 0.00014218
Iteration 142/1000 | Loss: 0.00014218
Iteration 143/1000 | Loss: 0.00014218
Iteration 144/1000 | Loss: 0.00014218
Iteration 145/1000 | Loss: 0.00014218
Iteration 146/1000 | Loss: 0.00014218
Iteration 147/1000 | Loss: 0.00014217
Iteration 148/1000 | Loss: 0.00014217
Iteration 149/1000 | Loss: 0.00014217
Iteration 150/1000 | Loss: 0.00014217
Iteration 151/1000 | Loss: 0.00014217
Iteration 152/1000 | Loss: 0.00014217
Iteration 153/1000 | Loss: 0.00014217
Iteration 154/1000 | Loss: 0.00014217
Iteration 155/1000 | Loss: 0.00014217
Iteration 156/1000 | Loss: 0.00014217
Iteration 157/1000 | Loss: 0.00014217
Iteration 158/1000 | Loss: 0.00014217
Iteration 159/1000 | Loss: 0.00014217
Iteration 160/1000 | Loss: 0.00014217
Iteration 161/1000 | Loss: 0.00014217
Iteration 162/1000 | Loss: 0.00014217
Iteration 163/1000 | Loss: 0.00014217
Iteration 164/1000 | Loss: 0.00014217
Iteration 165/1000 | Loss: 0.00014217
Iteration 166/1000 | Loss: 0.00014217
Iteration 167/1000 | Loss: 0.00014217
Iteration 168/1000 | Loss: 0.00014217
Iteration 169/1000 | Loss: 0.00014217
Iteration 170/1000 | Loss: 0.00014217
Iteration 171/1000 | Loss: 0.00014217
Iteration 172/1000 | Loss: 0.00014217
Iteration 173/1000 | Loss: 0.00014217
Iteration 174/1000 | Loss: 0.00014217
Iteration 175/1000 | Loss: 0.00014217
Iteration 176/1000 | Loss: 0.00014217
Iteration 177/1000 | Loss: 0.00014217
Iteration 178/1000 | Loss: 0.00014217
Iteration 179/1000 | Loss: 0.00014217
Iteration 180/1000 | Loss: 0.00014217
Iteration 181/1000 | Loss: 0.00014217
Iteration 182/1000 | Loss: 0.00014217
Iteration 183/1000 | Loss: 0.00014217
Iteration 184/1000 | Loss: 0.00014217
Iteration 185/1000 | Loss: 0.00014217
Iteration 186/1000 | Loss: 0.00014217
Iteration 187/1000 | Loss: 0.00014217
Iteration 188/1000 | Loss: 0.00014217
Iteration 189/1000 | Loss: 0.00014217
Iteration 190/1000 | Loss: 0.00014217
Iteration 191/1000 | Loss: 0.00014217
Iteration 192/1000 | Loss: 0.00014217
Iteration 193/1000 | Loss: 0.00014217
Iteration 194/1000 | Loss: 0.00014217
Iteration 195/1000 | Loss: 0.00014217
Iteration 196/1000 | Loss: 0.00014217
Iteration 197/1000 | Loss: 0.00014217
Iteration 198/1000 | Loss: 0.00014217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [0.00014216759882401675, 0.00014216759882401675, 0.00014216759882401675, 0.00014216759882401675, 0.00014216759882401675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014216759882401675

Optimization complete. Final v2v error: 6.470963478088379 mm

Highest mean error: 13.298254013061523 mm for frame 6

Lowest mean error: 4.677530765533447 mm for frame 124

Saving results

Total time: 129.80345487594604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804517
Iteration 2/25 | Loss: 0.00150337
Iteration 3/25 | Loss: 0.00140781
Iteration 4/25 | Loss: 0.00137481
Iteration 5/25 | Loss: 0.00135474
Iteration 6/25 | Loss: 0.00135061
Iteration 7/25 | Loss: 0.00134806
Iteration 8/25 | Loss: 0.00134719
Iteration 9/25 | Loss: 0.00134669
Iteration 10/25 | Loss: 0.00134643
Iteration 11/25 | Loss: 0.00134638
Iteration 12/25 | Loss: 0.00134638
Iteration 13/25 | Loss: 0.00134637
Iteration 14/25 | Loss: 0.00134637
Iteration 15/25 | Loss: 0.00134637
Iteration 16/25 | Loss: 0.00134637
Iteration 17/25 | Loss: 0.00134637
Iteration 18/25 | Loss: 0.00134637
Iteration 19/25 | Loss: 0.00134637
Iteration 20/25 | Loss: 0.00134636
Iteration 21/25 | Loss: 0.00134636
Iteration 22/25 | Loss: 0.00134635
Iteration 23/25 | Loss: 0.00134635
Iteration 24/25 | Loss: 0.00134635
Iteration 25/25 | Loss: 0.00134635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20992708
Iteration 2/25 | Loss: 0.00202821
Iteration 3/25 | Loss: 0.00202821
Iteration 4/25 | Loss: 0.00202821
Iteration 5/25 | Loss: 0.00202821
Iteration 6/25 | Loss: 0.00202821
Iteration 7/25 | Loss: 0.00202821
Iteration 8/25 | Loss: 0.00202821
Iteration 9/25 | Loss: 0.00202821
Iteration 10/25 | Loss: 0.00202821
Iteration 11/25 | Loss: 0.00202821
Iteration 12/25 | Loss: 0.00202821
Iteration 13/25 | Loss: 0.00202821
Iteration 14/25 | Loss: 0.00202821
Iteration 15/25 | Loss: 0.00202821
Iteration 16/25 | Loss: 0.00202821
Iteration 17/25 | Loss: 0.00202821
Iteration 18/25 | Loss: 0.00202821
Iteration 19/25 | Loss: 0.00202821
Iteration 20/25 | Loss: 0.00202821
Iteration 21/25 | Loss: 0.00202821
Iteration 22/25 | Loss: 0.00202821
Iteration 23/25 | Loss: 0.00202821
Iteration 24/25 | Loss: 0.00202821
Iteration 25/25 | Loss: 0.00202821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202821
Iteration 2/1000 | Loss: 0.00003876
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002509
Iteration 5/1000 | Loss: 0.00002374
Iteration 6/1000 | Loss: 0.00002304
Iteration 7/1000 | Loss: 0.00002248
Iteration 8/1000 | Loss: 0.00002208
Iteration 9/1000 | Loss: 0.00002185
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002146
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002142
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002141
Iteration 20/1000 | Loss: 0.00002141
Iteration 21/1000 | Loss: 0.00002141
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002141
Iteration 25/1000 | Loss: 0.00002141
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002141
Iteration 29/1000 | Loss: 0.00002140
Iteration 30/1000 | Loss: 0.00002140
Iteration 31/1000 | Loss: 0.00002139
Iteration 32/1000 | Loss: 0.00002138
Iteration 33/1000 | Loss: 0.00002138
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002136
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002135
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002132
Iteration 44/1000 | Loss: 0.00002131
Iteration 45/1000 | Loss: 0.00002131
Iteration 46/1000 | Loss: 0.00002131
Iteration 47/1000 | Loss: 0.00002131
Iteration 48/1000 | Loss: 0.00002131
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00002131
Iteration 51/1000 | Loss: 0.00002131
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002131
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002127
Iteration 56/1000 | Loss: 0.00002127
Iteration 57/1000 | Loss: 0.00002127
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002127
Iteration 60/1000 | Loss: 0.00002127
Iteration 61/1000 | Loss: 0.00002127
Iteration 62/1000 | Loss: 0.00002126
Iteration 63/1000 | Loss: 0.00002125
Iteration 64/1000 | Loss: 0.00002124
Iteration 65/1000 | Loss: 0.00002124
Iteration 66/1000 | Loss: 0.00002124
Iteration 67/1000 | Loss: 0.00002124
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002123
Iteration 71/1000 | Loss: 0.00002123
Iteration 72/1000 | Loss: 0.00002122
Iteration 73/1000 | Loss: 0.00002122
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002121
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002121
Iteration 87/1000 | Loss: 0.00002120
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002120
Iteration 90/1000 | Loss: 0.00002120
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002119
Iteration 93/1000 | Loss: 0.00002119
Iteration 94/1000 | Loss: 0.00002119
Iteration 95/1000 | Loss: 0.00002119
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002118
Iteration 99/1000 | Loss: 0.00002118
Iteration 100/1000 | Loss: 0.00002118
Iteration 101/1000 | Loss: 0.00002118
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002118
Iteration 104/1000 | Loss: 0.00002118
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00002118
Iteration 107/1000 | Loss: 0.00002118
Iteration 108/1000 | Loss: 0.00002118
Iteration 109/1000 | Loss: 0.00002118
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002118
Iteration 112/1000 | Loss: 0.00002118
Iteration 113/1000 | Loss: 0.00002118
Iteration 114/1000 | Loss: 0.00002118
Iteration 115/1000 | Loss: 0.00002118
Iteration 116/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.1182928321650252e-05, 2.1182928321650252e-05, 2.1182928321650252e-05, 2.1182928321650252e-05, 2.1182928321650252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1182928321650252e-05

Optimization complete. Final v2v error: 3.9988691806793213 mm

Highest mean error: 4.410854339599609 mm for frame 190

Lowest mean error: 3.7566792964935303 mm for frame 0

Saving results

Total time: 47.10542178153992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156907
Iteration 2/25 | Loss: 0.01156907
Iteration 3/25 | Loss: 0.01156907
Iteration 4/25 | Loss: 0.01156906
Iteration 5/25 | Loss: 0.01156906
Iteration 6/25 | Loss: 0.01156906
Iteration 7/25 | Loss: 0.01156906
Iteration 8/25 | Loss: 0.01156906
Iteration 9/25 | Loss: 0.01156906
Iteration 10/25 | Loss: 0.01156906
Iteration 11/25 | Loss: 0.01156906
Iteration 12/25 | Loss: 0.01156906
Iteration 13/25 | Loss: 0.01156906
Iteration 14/25 | Loss: 0.01156906
Iteration 15/25 | Loss: 0.01156906
Iteration 16/25 | Loss: 0.01156906
Iteration 17/25 | Loss: 0.01156905
Iteration 18/25 | Loss: 0.01156905
Iteration 19/25 | Loss: 0.01156905
Iteration 20/25 | Loss: 0.01156905
Iteration 21/25 | Loss: 0.01156905
Iteration 22/25 | Loss: 0.01156905
Iteration 23/25 | Loss: 0.01156905
Iteration 24/25 | Loss: 0.01156905
Iteration 25/25 | Loss: 0.01156905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81133199
Iteration 2/25 | Loss: 0.09287470
Iteration 3/25 | Loss: 0.08388284
Iteration 4/25 | Loss: 0.08343887
Iteration 5/25 | Loss: 0.08343886
Iteration 6/25 | Loss: 0.08343886
Iteration 7/25 | Loss: 0.08343884
Iteration 8/25 | Loss: 0.08343884
Iteration 9/25 | Loss: 0.08343884
Iteration 10/25 | Loss: 0.08343884
Iteration 11/25 | Loss: 0.08343884
Iteration 12/25 | Loss: 0.08343884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08343884348869324, 0.08343884348869324, 0.08343884348869324, 0.08343884348869324, 0.08343884348869324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08343884348869324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08343884
Iteration 2/1000 | Loss: 0.00555621
Iteration 3/1000 | Loss: 0.00231542
Iteration 4/1000 | Loss: 0.00032335
Iteration 5/1000 | Loss: 0.00394155
Iteration 6/1000 | Loss: 0.00065318
Iteration 7/1000 | Loss: 0.00050280
Iteration 8/1000 | Loss: 0.00008797
Iteration 9/1000 | Loss: 0.00081618
Iteration 10/1000 | Loss: 0.00020921
Iteration 11/1000 | Loss: 0.00021924
Iteration 12/1000 | Loss: 0.00020231
Iteration 13/1000 | Loss: 0.00029105
Iteration 14/1000 | Loss: 0.00034775
Iteration 15/1000 | Loss: 0.00004174
Iteration 16/1000 | Loss: 0.00003691
Iteration 17/1000 | Loss: 0.00003360
Iteration 18/1000 | Loss: 0.00003109
Iteration 19/1000 | Loss: 0.00003152
Iteration 20/1000 | Loss: 0.00002891
Iteration 21/1000 | Loss: 0.00009212
Iteration 22/1000 | Loss: 0.00142368
Iteration 23/1000 | Loss: 0.00010577
Iteration 24/1000 | Loss: 0.00019314
Iteration 25/1000 | Loss: 0.00002682
Iteration 26/1000 | Loss: 0.00002579
Iteration 27/1000 | Loss: 0.00002508
Iteration 28/1000 | Loss: 0.00011470
Iteration 29/1000 | Loss: 0.00041197
Iteration 30/1000 | Loss: 0.00002433
Iteration 31/1000 | Loss: 0.00002533
Iteration 32/1000 | Loss: 0.00033234
Iteration 33/1000 | Loss: 0.00017369
Iteration 34/1000 | Loss: 0.00002428
Iteration 35/1000 | Loss: 0.00006189
Iteration 36/1000 | Loss: 0.00002922
Iteration 37/1000 | Loss: 0.00003714
Iteration 38/1000 | Loss: 0.00003563
Iteration 39/1000 | Loss: 0.00007166
Iteration 40/1000 | Loss: 0.00002401
Iteration 41/1000 | Loss: 0.00002456
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002457
Iteration 44/1000 | Loss: 0.00002433
Iteration 45/1000 | Loss: 0.00002564
Iteration 46/1000 | Loss: 0.00002497
Iteration 47/1000 | Loss: 0.00002541
Iteration 48/1000 | Loss: 0.00002504
Iteration 49/1000 | Loss: 0.00002757
Iteration 50/1000 | Loss: 0.00020996
Iteration 51/1000 | Loss: 0.00002429
Iteration 52/1000 | Loss: 0.00011010
Iteration 53/1000 | Loss: 0.00002496
Iteration 54/1000 | Loss: 0.00002660
Iteration 55/1000 | Loss: 0.00002443
Iteration 56/1000 | Loss: 0.00002644
Iteration 57/1000 | Loss: 0.00002458
Iteration 58/1000 | Loss: 0.00002479
Iteration 59/1000 | Loss: 0.00002333
Iteration 60/1000 | Loss: 0.00002538
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002608
Iteration 63/1000 | Loss: 0.00008454
Iteration 64/1000 | Loss: 0.00002598
Iteration 65/1000 | Loss: 0.00002587
Iteration 66/1000 | Loss: 0.00002641
Iteration 67/1000 | Loss: 0.00002587
Iteration 68/1000 | Loss: 0.00002619
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002557
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002616
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002583
Iteration 75/1000 | Loss: 0.00002563
Iteration 76/1000 | Loss: 0.00010550
Iteration 77/1000 | Loss: 0.00037207
Iteration 78/1000 | Loss: 0.00002895
Iteration 79/1000 | Loss: 0.00002556
Iteration 80/1000 | Loss: 0.00002634
Iteration 81/1000 | Loss: 0.00002570
Iteration 82/1000 | Loss: 0.00002611
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002609
Iteration 85/1000 | Loss: 0.00002585
Iteration 86/1000 | Loss: 0.00002651
Iteration 87/1000 | Loss: 0.00002651
Iteration 88/1000 | Loss: 0.00007907
Iteration 89/1000 | Loss: 0.00017735
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002669
Iteration 92/1000 | Loss: 0.00002558
Iteration 93/1000 | Loss: 0.00002601
Iteration 94/1000 | Loss: 0.00002540
Iteration 95/1000 | Loss: 0.00002648
Iteration 96/1000 | Loss: 0.00002447
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002425
Iteration 99/1000 | Loss: 0.00002347
Iteration 100/1000 | Loss: 0.00009823
Iteration 101/1000 | Loss: 0.00003408
Iteration 102/1000 | Loss: 0.00002617
Iteration 103/1000 | Loss: 0.00002870
Iteration 104/1000 | Loss: 0.00002397
Iteration 105/1000 | Loss: 0.00002358
Iteration 106/1000 | Loss: 0.00002385
Iteration 107/1000 | Loss: 0.00005200
Iteration 108/1000 | Loss: 0.00002641
Iteration 109/1000 | Loss: 0.00002442
Iteration 110/1000 | Loss: 0.00002414
Iteration 111/1000 | Loss: 0.00002419
Iteration 112/1000 | Loss: 0.00005506
Iteration 113/1000 | Loss: 0.00003282
Iteration 114/1000 | Loss: 0.00002692
Iteration 115/1000 | Loss: 0.00002475
Iteration 116/1000 | Loss: 0.00003315
Iteration 117/1000 | Loss: 0.00002556
Iteration 118/1000 | Loss: 0.00002420
Iteration 119/1000 | Loss: 0.00002483
Iteration 120/1000 | Loss: 0.00004022
Iteration 121/1000 | Loss: 0.00003345
Iteration 122/1000 | Loss: 0.00003698
Iteration 123/1000 | Loss: 0.00002821
Iteration 124/1000 | Loss: 0.00002718
Iteration 125/1000 | Loss: 0.00002816
Iteration 126/1000 | Loss: 0.00002328
Iteration 127/1000 | Loss: 0.00002319
Iteration 128/1000 | Loss: 0.00002333
Iteration 129/1000 | Loss: 0.00002316
Iteration 130/1000 | Loss: 0.00002318
Iteration 131/1000 | Loss: 0.00002308
Iteration 132/1000 | Loss: 0.00002312
Iteration 133/1000 | Loss: 0.00002269
Iteration 134/1000 | Loss: 0.00002268
Iteration 135/1000 | Loss: 0.00002268
Iteration 136/1000 | Loss: 0.00002268
Iteration 137/1000 | Loss: 0.00002268
Iteration 138/1000 | Loss: 0.00002268
Iteration 139/1000 | Loss: 0.00002268
Iteration 140/1000 | Loss: 0.00002268
Iteration 141/1000 | Loss: 0.00002267
Iteration 142/1000 | Loss: 0.00002267
Iteration 143/1000 | Loss: 0.00002267
Iteration 144/1000 | Loss: 0.00002267
Iteration 145/1000 | Loss: 0.00002267
Iteration 146/1000 | Loss: 0.00002267
Iteration 147/1000 | Loss: 0.00002305
Iteration 148/1000 | Loss: 0.00002319
Iteration 149/1000 | Loss: 0.00002267
Iteration 150/1000 | Loss: 0.00002266
Iteration 151/1000 | Loss: 0.00002266
Iteration 152/1000 | Loss: 0.00002266
Iteration 153/1000 | Loss: 0.00002266
Iteration 154/1000 | Loss: 0.00002266
Iteration 155/1000 | Loss: 0.00002266
Iteration 156/1000 | Loss: 0.00002305
Iteration 157/1000 | Loss: 0.00002314
Iteration 158/1000 | Loss: 0.00002308
Iteration 159/1000 | Loss: 0.00002308
Iteration 160/1000 | Loss: 0.00002305
Iteration 161/1000 | Loss: 0.00002305
Iteration 162/1000 | Loss: 0.00002318
Iteration 163/1000 | Loss: 0.00002296
Iteration 164/1000 | Loss: 0.00002304
Iteration 165/1000 | Loss: 0.00002300
Iteration 166/1000 | Loss: 0.00002315
Iteration 167/1000 | Loss: 0.00002302
Iteration 168/1000 | Loss: 0.00002311
Iteration 169/1000 | Loss: 0.00002304
Iteration 170/1000 | Loss: 0.00002307
Iteration 171/1000 | Loss: 0.00002303
Iteration 172/1000 | Loss: 0.00002301
Iteration 173/1000 | Loss: 0.00002302
Iteration 174/1000 | Loss: 0.00002299
Iteration 175/1000 | Loss: 0.00002301
Iteration 176/1000 | Loss: 0.00002302
Iteration 177/1000 | Loss: 0.00002301
Iteration 178/1000 | Loss: 0.00002304
Iteration 179/1000 | Loss: 0.00002301
Iteration 180/1000 | Loss: 0.00002311
Iteration 181/1000 | Loss: 0.00002303
Iteration 182/1000 | Loss: 0.00002310
Iteration 183/1000 | Loss: 0.00002304
Iteration 184/1000 | Loss: 0.00002302
Iteration 185/1000 | Loss: 0.00002264
Iteration 186/1000 | Loss: 0.00002263
Iteration 187/1000 | Loss: 0.00002263
Iteration 188/1000 | Loss: 0.00002262
Iteration 189/1000 | Loss: 0.00002262
Iteration 190/1000 | Loss: 0.00002311
Iteration 191/1000 | Loss: 0.00002307
Iteration 192/1000 | Loss: 0.00002262
Iteration 193/1000 | Loss: 0.00002262
Iteration 194/1000 | Loss: 0.00002262
Iteration 195/1000 | Loss: 0.00002262
Iteration 196/1000 | Loss: 0.00002262
Iteration 197/1000 | Loss: 0.00002316
Iteration 198/1000 | Loss: 0.00002316
Iteration 199/1000 | Loss: 0.00002315
Iteration 200/1000 | Loss: 0.00002262
Iteration 201/1000 | Loss: 0.00002262
Iteration 202/1000 | Loss: 0.00002262
Iteration 203/1000 | Loss: 0.00002262
Iteration 204/1000 | Loss: 0.00002262
Iteration 205/1000 | Loss: 0.00002262
Iteration 206/1000 | Loss: 0.00002262
Iteration 207/1000 | Loss: 0.00002314
Iteration 208/1000 | Loss: 0.00002314
Iteration 209/1000 | Loss: 0.00002308
Iteration 210/1000 | Loss: 0.00002262
Iteration 211/1000 | Loss: 0.00002262
Iteration 212/1000 | Loss: 0.00002262
Iteration 213/1000 | Loss: 0.00002262
Iteration 214/1000 | Loss: 0.00002262
Iteration 215/1000 | Loss: 0.00002262
Iteration 216/1000 | Loss: 0.00002262
Iteration 217/1000 | Loss: 0.00002262
Iteration 218/1000 | Loss: 0.00002262
Iteration 219/1000 | Loss: 0.00002262
Iteration 220/1000 | Loss: 0.00002262
Iteration 221/1000 | Loss: 0.00002262
Iteration 222/1000 | Loss: 0.00002262
Iteration 223/1000 | Loss: 0.00002262
Iteration 224/1000 | Loss: 0.00002261
Iteration 225/1000 | Loss: 0.00002313
Iteration 226/1000 | Loss: 0.00002307
Iteration 227/1000 | Loss: 0.00002305
Iteration 228/1000 | Loss: 0.00002308
Iteration 229/1000 | Loss: 0.00002322
Iteration 230/1000 | Loss: 0.00002311
Iteration 231/1000 | Loss: 0.00002261
Iteration 232/1000 | Loss: 0.00002261
Iteration 233/1000 | Loss: 0.00002261
Iteration 234/1000 | Loss: 0.00002261
Iteration 235/1000 | Loss: 0.00002261
Iteration 236/1000 | Loss: 0.00002261
Iteration 237/1000 | Loss: 0.00002261
Iteration 238/1000 | Loss: 0.00002314
Iteration 239/1000 | Loss: 0.00002314
Iteration 240/1000 | Loss: 0.00002310
Iteration 241/1000 | Loss: 0.00002310
Iteration 242/1000 | Loss: 0.00002262
Iteration 243/1000 | Loss: 0.00002262
Iteration 244/1000 | Loss: 0.00002262
Iteration 245/1000 | Loss: 0.00002261
Iteration 246/1000 | Loss: 0.00002261
Iteration 247/1000 | Loss: 0.00002260
Iteration 248/1000 | Loss: 0.00002260
Iteration 249/1000 | Loss: 0.00002260
Iteration 250/1000 | Loss: 0.00002260
Iteration 251/1000 | Loss: 0.00002260
Iteration 252/1000 | Loss: 0.00002260
Iteration 253/1000 | Loss: 0.00002260
Iteration 254/1000 | Loss: 0.00002260
Iteration 255/1000 | Loss: 0.00002260
Iteration 256/1000 | Loss: 0.00002260
Iteration 257/1000 | Loss: 0.00002321
Iteration 258/1000 | Loss: 0.00002309
Iteration 259/1000 | Loss: 0.00002309
Iteration 260/1000 | Loss: 0.00002309
Iteration 261/1000 | Loss: 0.00002309
Iteration 262/1000 | Loss: 0.00002267
Iteration 263/1000 | Loss: 0.00002264
Iteration 264/1000 | Loss: 0.00002263
Iteration 265/1000 | Loss: 0.00002263
Iteration 266/1000 | Loss: 0.00002262
Iteration 267/1000 | Loss: 0.00002260
Iteration 268/1000 | Loss: 0.00002260
Iteration 269/1000 | Loss: 0.00002260
Iteration 270/1000 | Loss: 0.00002260
Iteration 271/1000 | Loss: 0.00002260
Iteration 272/1000 | Loss: 0.00002260
Iteration 273/1000 | Loss: 0.00002260
Iteration 274/1000 | Loss: 0.00002260
Iteration 275/1000 | Loss: 0.00002259
Iteration 276/1000 | Loss: 0.00002259
Iteration 277/1000 | Loss: 0.00002259
Iteration 278/1000 | Loss: 0.00002259
Iteration 279/1000 | Loss: 0.00002259
Iteration 280/1000 | Loss: 0.00002259
Iteration 281/1000 | Loss: 0.00002259
Iteration 282/1000 | Loss: 0.00002259
Iteration 283/1000 | Loss: 0.00002259
Iteration 284/1000 | Loss: 0.00002259
Iteration 285/1000 | Loss: 0.00002259
Iteration 286/1000 | Loss: 0.00002259
Iteration 287/1000 | Loss: 0.00002259
Iteration 288/1000 | Loss: 0.00002259
Iteration 289/1000 | Loss: 0.00002259
Iteration 290/1000 | Loss: 0.00002259
Iteration 291/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [2.2590676962863654e-05, 2.2590676962863654e-05, 2.2590676962863654e-05, 2.2590676962863654e-05, 2.2590676962863654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2590676962863654e-05

Optimization complete. Final v2v error: 3.776989459991455 mm

Highest mean error: 11.684442520141602 mm for frame 131

Lowest mean error: 2.982182741165161 mm for frame 88

Saving results

Total time: 261.78308367729187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878527
Iteration 2/25 | Loss: 0.00210752
Iteration 3/25 | Loss: 0.00139942
Iteration 4/25 | Loss: 0.00126480
Iteration 5/25 | Loss: 0.00123622
Iteration 6/25 | Loss: 0.00124158
Iteration 7/25 | Loss: 0.00121427
Iteration 8/25 | Loss: 0.00121665
Iteration 9/25 | Loss: 0.00119852
Iteration 10/25 | Loss: 0.00118874
Iteration 11/25 | Loss: 0.00118584
Iteration 12/25 | Loss: 0.00118472
Iteration 13/25 | Loss: 0.00118427
Iteration 14/25 | Loss: 0.00118400
Iteration 15/25 | Loss: 0.00118380
Iteration 16/25 | Loss: 0.00118369
Iteration 17/25 | Loss: 0.00118368
Iteration 18/25 | Loss: 0.00118367
Iteration 19/25 | Loss: 0.00118367
Iteration 20/25 | Loss: 0.00118367
Iteration 21/25 | Loss: 0.00118367
Iteration 22/25 | Loss: 0.00118367
Iteration 23/25 | Loss: 0.00118367
Iteration 24/25 | Loss: 0.00118367
Iteration 25/25 | Loss: 0.00118366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16007876
Iteration 2/25 | Loss: 0.00194287
Iteration 3/25 | Loss: 0.00190195
Iteration 4/25 | Loss: 0.00190195
Iteration 5/25 | Loss: 0.00190195
Iteration 6/25 | Loss: 0.00190195
Iteration 7/25 | Loss: 0.00190195
Iteration 8/25 | Loss: 0.00190195
Iteration 9/25 | Loss: 0.00190195
Iteration 10/25 | Loss: 0.00190195
Iteration 11/25 | Loss: 0.00190195
Iteration 12/25 | Loss: 0.00190195
Iteration 13/25 | Loss: 0.00190195
Iteration 14/25 | Loss: 0.00190195
Iteration 15/25 | Loss: 0.00190195
Iteration 16/25 | Loss: 0.00190195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019019452156499028, 0.0019019452156499028, 0.0019019452156499028, 0.0019019452156499028, 0.0019019452156499028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019019452156499028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190195
Iteration 2/1000 | Loss: 0.00005776
Iteration 3/1000 | Loss: 0.00004143
Iteration 4/1000 | Loss: 0.00003584
Iteration 5/1000 | Loss: 0.00003322
Iteration 6/1000 | Loss: 0.00003178
Iteration 7/1000 | Loss: 0.00003112
Iteration 8/1000 | Loss: 0.00003069
Iteration 9/1000 | Loss: 0.00003035
Iteration 10/1000 | Loss: 0.00003006
Iteration 11/1000 | Loss: 0.00002987
Iteration 12/1000 | Loss: 0.00002980
Iteration 13/1000 | Loss: 0.00002974
Iteration 14/1000 | Loss: 0.00002973
Iteration 15/1000 | Loss: 0.00002971
Iteration 16/1000 | Loss: 0.00002969
Iteration 17/1000 | Loss: 0.00002968
Iteration 18/1000 | Loss: 0.00002968
Iteration 19/1000 | Loss: 0.00002966
Iteration 20/1000 | Loss: 0.00002962
Iteration 21/1000 | Loss: 0.00002962
Iteration 22/1000 | Loss: 0.00002960
Iteration 23/1000 | Loss: 0.00002960
Iteration 24/1000 | Loss: 0.00002959
Iteration 25/1000 | Loss: 0.00002959
Iteration 26/1000 | Loss: 0.00002959
Iteration 27/1000 | Loss: 0.00002958
Iteration 28/1000 | Loss: 0.00002958
Iteration 29/1000 | Loss: 0.00002954
Iteration 30/1000 | Loss: 0.00002954
Iteration 31/1000 | Loss: 0.00002948
Iteration 32/1000 | Loss: 0.00002944
Iteration 33/1000 | Loss: 0.00002943
Iteration 34/1000 | Loss: 0.00002943
Iteration 35/1000 | Loss: 0.00002942
Iteration 36/1000 | Loss: 0.00002941
Iteration 37/1000 | Loss: 0.00002941
Iteration 38/1000 | Loss: 0.00002940
Iteration 39/1000 | Loss: 0.00002938
Iteration 40/1000 | Loss: 0.00002938
Iteration 41/1000 | Loss: 0.00002937
Iteration 42/1000 | Loss: 0.00002936
Iteration 43/1000 | Loss: 0.00002936
Iteration 44/1000 | Loss: 0.00002935
Iteration 45/1000 | Loss: 0.00002934
Iteration 46/1000 | Loss: 0.00002934
Iteration 47/1000 | Loss: 0.00002933
Iteration 48/1000 | Loss: 0.00002933
Iteration 49/1000 | Loss: 0.00009142
Iteration 50/1000 | Loss: 0.00002937
Iteration 51/1000 | Loss: 0.00002934
Iteration 52/1000 | Loss: 0.00002931
Iteration 53/1000 | Loss: 0.00002931
Iteration 54/1000 | Loss: 0.00002931
Iteration 55/1000 | Loss: 0.00002931
Iteration 56/1000 | Loss: 0.00002930
Iteration 57/1000 | Loss: 0.00002930
Iteration 58/1000 | Loss: 0.00002930
Iteration 59/1000 | Loss: 0.00002930
Iteration 60/1000 | Loss: 0.00002930
Iteration 61/1000 | Loss: 0.00002930
Iteration 62/1000 | Loss: 0.00002930
Iteration 63/1000 | Loss: 0.00002929
Iteration 64/1000 | Loss: 0.00002929
Iteration 65/1000 | Loss: 0.00002929
Iteration 66/1000 | Loss: 0.00002929
Iteration 67/1000 | Loss: 0.00002929
Iteration 68/1000 | Loss: 0.00002928
Iteration 69/1000 | Loss: 0.00002928
Iteration 70/1000 | Loss: 0.00002928
Iteration 71/1000 | Loss: 0.00002928
Iteration 72/1000 | Loss: 0.00002928
Iteration 73/1000 | Loss: 0.00002928
Iteration 74/1000 | Loss: 0.00002927
Iteration 75/1000 | Loss: 0.00002927
Iteration 76/1000 | Loss: 0.00002927
Iteration 77/1000 | Loss: 0.00002927
Iteration 78/1000 | Loss: 0.00002927
Iteration 79/1000 | Loss: 0.00002927
Iteration 80/1000 | Loss: 0.00002927
Iteration 81/1000 | Loss: 0.00002926
Iteration 82/1000 | Loss: 0.00002926
Iteration 83/1000 | Loss: 0.00002926
Iteration 84/1000 | Loss: 0.00002926
Iteration 85/1000 | Loss: 0.00002926
Iteration 86/1000 | Loss: 0.00002926
Iteration 87/1000 | Loss: 0.00002925
Iteration 88/1000 | Loss: 0.00002925
Iteration 89/1000 | Loss: 0.00002925
Iteration 90/1000 | Loss: 0.00002925
Iteration 91/1000 | Loss: 0.00002925
Iteration 92/1000 | Loss: 0.00002925
Iteration 93/1000 | Loss: 0.00002925
Iteration 94/1000 | Loss: 0.00002925
Iteration 95/1000 | Loss: 0.00002925
Iteration 96/1000 | Loss: 0.00002925
Iteration 97/1000 | Loss: 0.00002924
Iteration 98/1000 | Loss: 0.00002924
Iteration 99/1000 | Loss: 0.00002924
Iteration 100/1000 | Loss: 0.00002924
Iteration 101/1000 | Loss: 0.00002924
Iteration 102/1000 | Loss: 0.00002924
Iteration 103/1000 | Loss: 0.00002923
Iteration 104/1000 | Loss: 0.00002923
Iteration 105/1000 | Loss: 0.00002923
Iteration 106/1000 | Loss: 0.00002923
Iteration 107/1000 | Loss: 0.00002923
Iteration 108/1000 | Loss: 0.00002923
Iteration 109/1000 | Loss: 0.00002923
Iteration 110/1000 | Loss: 0.00002922
Iteration 111/1000 | Loss: 0.00002922
Iteration 112/1000 | Loss: 0.00002922
Iteration 113/1000 | Loss: 0.00002922
Iteration 114/1000 | Loss: 0.00002922
Iteration 115/1000 | Loss: 0.00002922
Iteration 116/1000 | Loss: 0.00002922
Iteration 117/1000 | Loss: 0.00002922
Iteration 118/1000 | Loss: 0.00002922
Iteration 119/1000 | Loss: 0.00002922
Iteration 120/1000 | Loss: 0.00002922
Iteration 121/1000 | Loss: 0.00002922
Iteration 122/1000 | Loss: 0.00002921
Iteration 123/1000 | Loss: 0.00002921
Iteration 124/1000 | Loss: 0.00002921
Iteration 125/1000 | Loss: 0.00002921
Iteration 126/1000 | Loss: 0.00002921
Iteration 127/1000 | Loss: 0.00002921
Iteration 128/1000 | Loss: 0.00002921
Iteration 129/1000 | Loss: 0.00002921
Iteration 130/1000 | Loss: 0.00002921
Iteration 131/1000 | Loss: 0.00002921
Iteration 132/1000 | Loss: 0.00002921
Iteration 133/1000 | Loss: 0.00002921
Iteration 134/1000 | Loss: 0.00002921
Iteration 135/1000 | Loss: 0.00002921
Iteration 136/1000 | Loss: 0.00002921
Iteration 137/1000 | Loss: 0.00002921
Iteration 138/1000 | Loss: 0.00002921
Iteration 139/1000 | Loss: 0.00002921
Iteration 140/1000 | Loss: 0.00002921
Iteration 141/1000 | Loss: 0.00002921
Iteration 142/1000 | Loss: 0.00002921
Iteration 143/1000 | Loss: 0.00002920
Iteration 144/1000 | Loss: 0.00002920
Iteration 145/1000 | Loss: 0.00002920
Iteration 146/1000 | Loss: 0.00002920
Iteration 147/1000 | Loss: 0.00002920
Iteration 148/1000 | Loss: 0.00002920
Iteration 149/1000 | Loss: 0.00002920
Iteration 150/1000 | Loss: 0.00002920
Iteration 151/1000 | Loss: 0.00002920
Iteration 152/1000 | Loss: 0.00002920
Iteration 153/1000 | Loss: 0.00002920
Iteration 154/1000 | Loss: 0.00002920
Iteration 155/1000 | Loss: 0.00002920
Iteration 156/1000 | Loss: 0.00002920
Iteration 157/1000 | Loss: 0.00002920
Iteration 158/1000 | Loss: 0.00002920
Iteration 159/1000 | Loss: 0.00002920
Iteration 160/1000 | Loss: 0.00002919
Iteration 161/1000 | Loss: 0.00002919
Iteration 162/1000 | Loss: 0.00002919
Iteration 163/1000 | Loss: 0.00002919
Iteration 164/1000 | Loss: 0.00002919
Iteration 165/1000 | Loss: 0.00002919
Iteration 166/1000 | Loss: 0.00002919
Iteration 167/1000 | Loss: 0.00002919
Iteration 168/1000 | Loss: 0.00002919
Iteration 169/1000 | Loss: 0.00002919
Iteration 170/1000 | Loss: 0.00002918
Iteration 171/1000 | Loss: 0.00002918
Iteration 172/1000 | Loss: 0.00002918
Iteration 173/1000 | Loss: 0.00003065
Iteration 174/1000 | Loss: 0.00002945
Iteration 175/1000 | Loss: 0.00002983
Iteration 176/1000 | Loss: 0.00002947
Iteration 177/1000 | Loss: 0.00002984
Iteration 178/1000 | Loss: 0.00002951
Iteration 179/1000 | Loss: 0.00002919
Iteration 180/1000 | Loss: 0.00002918
Iteration 181/1000 | Loss: 0.00002991
Iteration 182/1000 | Loss: 0.00002947
Iteration 183/1000 | Loss: 0.00002918
Iteration 184/1000 | Loss: 0.00002983
Iteration 185/1000 | Loss: 0.00002961
Iteration 186/1000 | Loss: 0.00002917
Iteration 187/1000 | Loss: 0.00002917
Iteration 188/1000 | Loss: 0.00002917
Iteration 189/1000 | Loss: 0.00002992
Iteration 190/1000 | Loss: 0.00002992
Iteration 191/1000 | Loss: 0.00002991
Iteration 192/1000 | Loss: 0.00006467
Iteration 193/1000 | Loss: 0.00005013
Iteration 194/1000 | Loss: 0.00003002
Iteration 195/1000 | Loss: 0.00002953
Iteration 196/1000 | Loss: 0.00002975
Iteration 197/1000 | Loss: 0.00002952
Iteration 198/1000 | Loss: 0.00002997
Iteration 199/1000 | Loss: 0.00007927
Iteration 200/1000 | Loss: 0.00004339
Iteration 201/1000 | Loss: 0.00003007
Iteration 202/1000 | Loss: 0.00006168
Iteration 203/1000 | Loss: 0.00004869
Iteration 204/1000 | Loss: 0.00002919
Iteration 205/1000 | Loss: 0.00002918
Iteration 206/1000 | Loss: 0.00002918
Iteration 207/1000 | Loss: 0.00002918
Iteration 208/1000 | Loss: 0.00002918
Iteration 209/1000 | Loss: 0.00002917
Iteration 210/1000 | Loss: 0.00002917
Iteration 211/1000 | Loss: 0.00002917
Iteration 212/1000 | Loss: 0.00002917
Iteration 213/1000 | Loss: 0.00002991
Iteration 214/1000 | Loss: 0.00006367
Iteration 215/1000 | Loss: 0.00017494
Iteration 216/1000 | Loss: 0.00022324
Iteration 217/1000 | Loss: 0.00012116
Iteration 218/1000 | Loss: 0.00004029
Iteration 219/1000 | Loss: 0.00003592
Iteration 220/1000 | Loss: 0.00003019
Iteration 221/1000 | Loss: 0.00005556
Iteration 222/1000 | Loss: 0.00002940
Iteration 223/1000 | Loss: 0.00002953
Iteration 224/1000 | Loss: 0.00002971
Iteration 225/1000 | Loss: 0.00005501
Iteration 226/1000 | Loss: 0.00003239
Iteration 227/1000 | Loss: 0.00003887
Iteration 228/1000 | Loss: 0.00003655
Iteration 229/1000 | Loss: 0.00002985
Iteration 230/1000 | Loss: 0.00002986
Iteration 231/1000 | Loss: 0.00002963
Iteration 232/1000 | Loss: 0.00002971
Iteration 233/1000 | Loss: 0.00002954
Iteration 234/1000 | Loss: 0.00002959
Iteration 235/1000 | Loss: 0.00002972
Iteration 236/1000 | Loss: 0.00002971
Iteration 237/1000 | Loss: 0.00002972
Iteration 238/1000 | Loss: 0.00002973
Iteration 239/1000 | Loss: 0.00002972
Iteration 240/1000 | Loss: 0.00008934
Iteration 241/1000 | Loss: 0.00002959
Iteration 242/1000 | Loss: 0.00002967
Iteration 243/1000 | Loss: 0.00002966
Iteration 244/1000 | Loss: 0.00002966
Iteration 245/1000 | Loss: 0.00005409
Iteration 246/1000 | Loss: 0.00003490
Iteration 247/1000 | Loss: 0.00003055
Iteration 248/1000 | Loss: 0.00005432
Iteration 249/1000 | Loss: 0.00003019
Iteration 250/1000 | Loss: 0.00002985
Iteration 251/1000 | Loss: 0.00002977
Iteration 252/1000 | Loss: 0.00002982
Iteration 253/1000 | Loss: 0.00002924
Iteration 254/1000 | Loss: 0.00002958
Iteration 255/1000 | Loss: 0.00002970
Iteration 256/1000 | Loss: 0.00002959
Iteration 257/1000 | Loss: 0.00006519
Iteration 258/1000 | Loss: 0.00006519
Iteration 259/1000 | Loss: 0.00004131
Iteration 260/1000 | Loss: 0.00003003
Iteration 261/1000 | Loss: 0.00002973
Iteration 262/1000 | Loss: 0.00005181
Iteration 263/1000 | Loss: 0.00003514
Iteration 264/1000 | Loss: 0.00003274
Iteration 265/1000 | Loss: 0.00004349
Iteration 266/1000 | Loss: 0.00003199
Iteration 267/1000 | Loss: 0.00003545
Iteration 268/1000 | Loss: 0.00003174
Iteration 269/1000 | Loss: 0.00003689
Iteration 270/1000 | Loss: 0.00005725
Iteration 271/1000 | Loss: 0.00003507
Iteration 272/1000 | Loss: 0.00003059
Iteration 273/1000 | Loss: 0.00003357
Iteration 274/1000 | Loss: 0.00003267
Iteration 275/1000 | Loss: 0.00003573
Iteration 276/1000 | Loss: 0.00003112
Iteration 277/1000 | Loss: 0.00003029
Iteration 278/1000 | Loss: 0.00002966
Iteration 279/1000 | Loss: 0.00002966
Iteration 280/1000 | Loss: 0.00002920
Iteration 281/1000 | Loss: 0.00002964
Iteration 282/1000 | Loss: 0.00002963
Iteration 283/1000 | Loss: 0.00002962
Iteration 284/1000 | Loss: 0.00002961
Iteration 285/1000 | Loss: 0.00002964
Iteration 286/1000 | Loss: 0.00002959
Iteration 287/1000 | Loss: 0.00002969
Iteration 288/1000 | Loss: 0.00002955
Iteration 289/1000 | Loss: 0.00002959
Iteration 290/1000 | Loss: 0.00002962
Iteration 291/1000 | Loss: 0.00002960
Iteration 292/1000 | Loss: 0.00002922
Iteration 293/1000 | Loss: 0.00002916
Iteration 294/1000 | Loss: 0.00002916
Iteration 295/1000 | Loss: 0.00002916
Iteration 296/1000 | Loss: 0.00002916
Iteration 297/1000 | Loss: 0.00002916
Iteration 298/1000 | Loss: 0.00002916
Iteration 299/1000 | Loss: 0.00002916
Iteration 300/1000 | Loss: 0.00002916
Iteration 301/1000 | Loss: 0.00002916
Iteration 302/1000 | Loss: 0.00002916
Iteration 303/1000 | Loss: 0.00002916
Iteration 304/1000 | Loss: 0.00002916
Iteration 305/1000 | Loss: 0.00002915
Iteration 306/1000 | Loss: 0.00002915
Iteration 307/1000 | Loss: 0.00002915
Iteration 308/1000 | Loss: 0.00002915
Iteration 309/1000 | Loss: 0.00002915
Iteration 310/1000 | Loss: 0.00002915
Iteration 311/1000 | Loss: 0.00002915
Iteration 312/1000 | Loss: 0.00002915
Iteration 313/1000 | Loss: 0.00002915
Iteration 314/1000 | Loss: 0.00002915
Iteration 315/1000 | Loss: 0.00002915
Iteration 316/1000 | Loss: 0.00002915
Iteration 317/1000 | Loss: 0.00002914
Iteration 318/1000 | Loss: 0.00002914
Iteration 319/1000 | Loss: 0.00002914
Iteration 320/1000 | Loss: 0.00002914
Iteration 321/1000 | Loss: 0.00002914
Iteration 322/1000 | Loss: 0.00002914
Iteration 323/1000 | Loss: 0.00002914
Iteration 324/1000 | Loss: 0.00002914
Iteration 325/1000 | Loss: 0.00002914
Iteration 326/1000 | Loss: 0.00002914
Iteration 327/1000 | Loss: 0.00002914
Iteration 328/1000 | Loss: 0.00002914
Iteration 329/1000 | Loss: 0.00002914
Iteration 330/1000 | Loss: 0.00002914
Iteration 331/1000 | Loss: 0.00002914
Iteration 332/1000 | Loss: 0.00002914
Iteration 333/1000 | Loss: 0.00002914
Iteration 334/1000 | Loss: 0.00002914
Iteration 335/1000 | Loss: 0.00002914
Iteration 336/1000 | Loss: 0.00002914
Iteration 337/1000 | Loss: 0.00002914
Iteration 338/1000 | Loss: 0.00002914
Iteration 339/1000 | Loss: 0.00002914
Iteration 340/1000 | Loss: 0.00002914
Iteration 341/1000 | Loss: 0.00002914
Iteration 342/1000 | Loss: 0.00002914
Iteration 343/1000 | Loss: 0.00002914
Iteration 344/1000 | Loss: 0.00002914
Iteration 345/1000 | Loss: 0.00002914
Iteration 346/1000 | Loss: 0.00002914
Iteration 347/1000 | Loss: 0.00002914
Iteration 348/1000 | Loss: 0.00002914
Iteration 349/1000 | Loss: 0.00002914
Iteration 350/1000 | Loss: 0.00002914
Iteration 351/1000 | Loss: 0.00002914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [2.913626485678833e-05, 2.913626485678833e-05, 2.913626485678833e-05, 2.913626485678833e-05, 2.913626485678833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.913626485678833e-05

Optimization complete. Final v2v error: 4.61033821105957 mm

Highest mean error: 10.625076293945312 mm for frame 164

Lowest mean error: 3.948251247406006 mm for frame 16

Saving results

Total time: 180.56649613380432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137798
Iteration 2/25 | Loss: 0.00207313
Iteration 3/25 | Loss: 0.00161210
Iteration 4/25 | Loss: 0.00157286
Iteration 5/25 | Loss: 0.00155832
Iteration 6/25 | Loss: 0.00155511
Iteration 7/25 | Loss: 0.00155373
Iteration 8/25 | Loss: 0.00155366
Iteration 9/25 | Loss: 0.00155366
Iteration 10/25 | Loss: 0.00155366
Iteration 11/25 | Loss: 0.00155366
Iteration 12/25 | Loss: 0.00155366
Iteration 13/25 | Loss: 0.00155366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015536565333604813, 0.0015536565333604813, 0.0015536565333604813, 0.0015536565333604813, 0.0015536565333604813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015536565333604813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64698970
Iteration 2/25 | Loss: 0.00182655
Iteration 3/25 | Loss: 0.00182655
Iteration 4/25 | Loss: 0.00182655
Iteration 5/25 | Loss: 0.00182655
Iteration 6/25 | Loss: 0.00182655
Iteration 7/25 | Loss: 0.00182655
Iteration 8/25 | Loss: 0.00182655
Iteration 9/25 | Loss: 0.00182655
Iteration 10/25 | Loss: 0.00182655
Iteration 11/25 | Loss: 0.00182655
Iteration 12/25 | Loss: 0.00182655
Iteration 13/25 | Loss: 0.00182655
Iteration 14/25 | Loss: 0.00182655
Iteration 15/25 | Loss: 0.00182655
Iteration 16/25 | Loss: 0.00182655
Iteration 17/25 | Loss: 0.00182655
Iteration 18/25 | Loss: 0.00182655
Iteration 19/25 | Loss: 0.00182655
Iteration 20/25 | Loss: 0.00182655
Iteration 21/25 | Loss: 0.00182655
Iteration 22/25 | Loss: 0.00182655
Iteration 23/25 | Loss: 0.00182655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018265499966219068, 0.0018265499966219068, 0.0018265499966219068, 0.0018265499966219068, 0.0018265499966219068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018265499966219068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182655
Iteration 2/1000 | Loss: 0.00008881
Iteration 3/1000 | Loss: 0.00007094
Iteration 4/1000 | Loss: 0.00006408
Iteration 5/1000 | Loss: 0.00006001
Iteration 6/1000 | Loss: 0.00005703
Iteration 7/1000 | Loss: 0.00005551
Iteration 8/1000 | Loss: 0.00005435
Iteration 9/1000 | Loss: 0.00005330
Iteration 10/1000 | Loss: 0.00005267
Iteration 11/1000 | Loss: 0.00005216
Iteration 12/1000 | Loss: 0.00005183
Iteration 13/1000 | Loss: 0.00005166
Iteration 14/1000 | Loss: 0.00005148
Iteration 15/1000 | Loss: 0.00005134
Iteration 16/1000 | Loss: 0.00005128
Iteration 17/1000 | Loss: 0.00005128
Iteration 18/1000 | Loss: 0.00005122
Iteration 19/1000 | Loss: 0.00005119
Iteration 20/1000 | Loss: 0.00005119
Iteration 21/1000 | Loss: 0.00005118
Iteration 22/1000 | Loss: 0.00005118
Iteration 23/1000 | Loss: 0.00005118
Iteration 24/1000 | Loss: 0.00005117
Iteration 25/1000 | Loss: 0.00005117
Iteration 26/1000 | Loss: 0.00005117
Iteration 27/1000 | Loss: 0.00005117
Iteration 28/1000 | Loss: 0.00005116
Iteration 29/1000 | Loss: 0.00005116
Iteration 30/1000 | Loss: 0.00005116
Iteration 31/1000 | Loss: 0.00005116
Iteration 32/1000 | Loss: 0.00005116
Iteration 33/1000 | Loss: 0.00005115
Iteration 34/1000 | Loss: 0.00005115
Iteration 35/1000 | Loss: 0.00005115
Iteration 36/1000 | Loss: 0.00005115
Iteration 37/1000 | Loss: 0.00005115
Iteration 38/1000 | Loss: 0.00005115
Iteration 39/1000 | Loss: 0.00005115
Iteration 40/1000 | Loss: 0.00005114
Iteration 41/1000 | Loss: 0.00005114
Iteration 42/1000 | Loss: 0.00005114
Iteration 43/1000 | Loss: 0.00005114
Iteration 44/1000 | Loss: 0.00005114
Iteration 45/1000 | Loss: 0.00005113
Iteration 46/1000 | Loss: 0.00005113
Iteration 47/1000 | Loss: 0.00005113
Iteration 48/1000 | Loss: 0.00005112
Iteration 49/1000 | Loss: 0.00005112
Iteration 50/1000 | Loss: 0.00005112
Iteration 51/1000 | Loss: 0.00005110
Iteration 52/1000 | Loss: 0.00005110
Iteration 53/1000 | Loss: 0.00005109
Iteration 54/1000 | Loss: 0.00005109
Iteration 55/1000 | Loss: 0.00005108
Iteration 56/1000 | Loss: 0.00005108
Iteration 57/1000 | Loss: 0.00005108
Iteration 58/1000 | Loss: 0.00005108
Iteration 59/1000 | Loss: 0.00005108
Iteration 60/1000 | Loss: 0.00005108
Iteration 61/1000 | Loss: 0.00005108
Iteration 62/1000 | Loss: 0.00005108
Iteration 63/1000 | Loss: 0.00005107
Iteration 64/1000 | Loss: 0.00005107
Iteration 65/1000 | Loss: 0.00005107
Iteration 66/1000 | Loss: 0.00005107
Iteration 67/1000 | Loss: 0.00005107
Iteration 68/1000 | Loss: 0.00005107
Iteration 69/1000 | Loss: 0.00005107
Iteration 70/1000 | Loss: 0.00005106
Iteration 71/1000 | Loss: 0.00005103
Iteration 72/1000 | Loss: 0.00005103
Iteration 73/1000 | Loss: 0.00005103
Iteration 74/1000 | Loss: 0.00005102
Iteration 75/1000 | Loss: 0.00005102
Iteration 76/1000 | Loss: 0.00005101
Iteration 77/1000 | Loss: 0.00005101
Iteration 78/1000 | Loss: 0.00005101
Iteration 79/1000 | Loss: 0.00005100
Iteration 80/1000 | Loss: 0.00005100
Iteration 81/1000 | Loss: 0.00005100
Iteration 82/1000 | Loss: 0.00005100
Iteration 83/1000 | Loss: 0.00005099
Iteration 84/1000 | Loss: 0.00005099
Iteration 85/1000 | Loss: 0.00005099
Iteration 86/1000 | Loss: 0.00005099
Iteration 87/1000 | Loss: 0.00005099
Iteration 88/1000 | Loss: 0.00005099
Iteration 89/1000 | Loss: 0.00005099
Iteration 90/1000 | Loss: 0.00005098
Iteration 91/1000 | Loss: 0.00005098
Iteration 92/1000 | Loss: 0.00005098
Iteration 93/1000 | Loss: 0.00005098
Iteration 94/1000 | Loss: 0.00005098
Iteration 95/1000 | Loss: 0.00005098
Iteration 96/1000 | Loss: 0.00005098
Iteration 97/1000 | Loss: 0.00005098
Iteration 98/1000 | Loss: 0.00005098
Iteration 99/1000 | Loss: 0.00005098
Iteration 100/1000 | Loss: 0.00005098
Iteration 101/1000 | Loss: 0.00005097
Iteration 102/1000 | Loss: 0.00005097
Iteration 103/1000 | Loss: 0.00005097
Iteration 104/1000 | Loss: 0.00005096
Iteration 105/1000 | Loss: 0.00005096
Iteration 106/1000 | Loss: 0.00005096
Iteration 107/1000 | Loss: 0.00005096
Iteration 108/1000 | Loss: 0.00005096
Iteration 109/1000 | Loss: 0.00005096
Iteration 110/1000 | Loss: 0.00005096
Iteration 111/1000 | Loss: 0.00005096
Iteration 112/1000 | Loss: 0.00005096
Iteration 113/1000 | Loss: 0.00005096
Iteration 114/1000 | Loss: 0.00005096
Iteration 115/1000 | Loss: 0.00005096
Iteration 116/1000 | Loss: 0.00005095
Iteration 117/1000 | Loss: 0.00005095
Iteration 118/1000 | Loss: 0.00005095
Iteration 119/1000 | Loss: 0.00005095
Iteration 120/1000 | Loss: 0.00005095
Iteration 121/1000 | Loss: 0.00005095
Iteration 122/1000 | Loss: 0.00005095
Iteration 123/1000 | Loss: 0.00005095
Iteration 124/1000 | Loss: 0.00005095
Iteration 125/1000 | Loss: 0.00005095
Iteration 126/1000 | Loss: 0.00005095
Iteration 127/1000 | Loss: 0.00005095
Iteration 128/1000 | Loss: 0.00005095
Iteration 129/1000 | Loss: 0.00005095
Iteration 130/1000 | Loss: 0.00005095
Iteration 131/1000 | Loss: 0.00005095
Iteration 132/1000 | Loss: 0.00005095
Iteration 133/1000 | Loss: 0.00005094
Iteration 134/1000 | Loss: 0.00005094
Iteration 135/1000 | Loss: 0.00005094
Iteration 136/1000 | Loss: 0.00005094
Iteration 137/1000 | Loss: 0.00005094
Iteration 138/1000 | Loss: 0.00005094
Iteration 139/1000 | Loss: 0.00005094
Iteration 140/1000 | Loss: 0.00005094
Iteration 141/1000 | Loss: 0.00005094
Iteration 142/1000 | Loss: 0.00005094
Iteration 143/1000 | Loss: 0.00005093
Iteration 144/1000 | Loss: 0.00005093
Iteration 145/1000 | Loss: 0.00005093
Iteration 146/1000 | Loss: 0.00005093
Iteration 147/1000 | Loss: 0.00005093
Iteration 148/1000 | Loss: 0.00005093
Iteration 149/1000 | Loss: 0.00005093
Iteration 150/1000 | Loss: 0.00005093
Iteration 151/1000 | Loss: 0.00005092
Iteration 152/1000 | Loss: 0.00005092
Iteration 153/1000 | Loss: 0.00005092
Iteration 154/1000 | Loss: 0.00005092
Iteration 155/1000 | Loss: 0.00005092
Iteration 156/1000 | Loss: 0.00005092
Iteration 157/1000 | Loss: 0.00005092
Iteration 158/1000 | Loss: 0.00005092
Iteration 159/1000 | Loss: 0.00005092
Iteration 160/1000 | Loss: 0.00005092
Iteration 161/1000 | Loss: 0.00005092
Iteration 162/1000 | Loss: 0.00005092
Iteration 163/1000 | Loss: 0.00005092
Iteration 164/1000 | Loss: 0.00005092
Iteration 165/1000 | Loss: 0.00005092
Iteration 166/1000 | Loss: 0.00005092
Iteration 167/1000 | Loss: 0.00005092
Iteration 168/1000 | Loss: 0.00005092
Iteration 169/1000 | Loss: 0.00005091
Iteration 170/1000 | Loss: 0.00005091
Iteration 171/1000 | Loss: 0.00005091
Iteration 172/1000 | Loss: 0.00005091
Iteration 173/1000 | Loss: 0.00005091
Iteration 174/1000 | Loss: 0.00005091
Iteration 175/1000 | Loss: 0.00005091
Iteration 176/1000 | Loss: 0.00005091
Iteration 177/1000 | Loss: 0.00005091
Iteration 178/1000 | Loss: 0.00005091
Iteration 179/1000 | Loss: 0.00005091
Iteration 180/1000 | Loss: 0.00005090
Iteration 181/1000 | Loss: 0.00005090
Iteration 182/1000 | Loss: 0.00005090
Iteration 183/1000 | Loss: 0.00005090
Iteration 184/1000 | Loss: 0.00005090
Iteration 185/1000 | Loss: 0.00005090
Iteration 186/1000 | Loss: 0.00005090
Iteration 187/1000 | Loss: 0.00005090
Iteration 188/1000 | Loss: 0.00005090
Iteration 189/1000 | Loss: 0.00005090
Iteration 190/1000 | Loss: 0.00005090
Iteration 191/1000 | Loss: 0.00005090
Iteration 192/1000 | Loss: 0.00005090
Iteration 193/1000 | Loss: 0.00005090
Iteration 194/1000 | Loss: 0.00005089
Iteration 195/1000 | Loss: 0.00005089
Iteration 196/1000 | Loss: 0.00005089
Iteration 197/1000 | Loss: 0.00005089
Iteration 198/1000 | Loss: 0.00005089
Iteration 199/1000 | Loss: 0.00005089
Iteration 200/1000 | Loss: 0.00005089
Iteration 201/1000 | Loss: 0.00005089
Iteration 202/1000 | Loss: 0.00005089
Iteration 203/1000 | Loss: 0.00005089
Iteration 204/1000 | Loss: 0.00005089
Iteration 205/1000 | Loss: 0.00005089
Iteration 206/1000 | Loss: 0.00005089
Iteration 207/1000 | Loss: 0.00005089
Iteration 208/1000 | Loss: 0.00005088
Iteration 209/1000 | Loss: 0.00005088
Iteration 210/1000 | Loss: 0.00005088
Iteration 211/1000 | Loss: 0.00005088
Iteration 212/1000 | Loss: 0.00005088
Iteration 213/1000 | Loss: 0.00005088
Iteration 214/1000 | Loss: 0.00005088
Iteration 215/1000 | Loss: 0.00005088
Iteration 216/1000 | Loss: 0.00005088
Iteration 217/1000 | Loss: 0.00005088
Iteration 218/1000 | Loss: 0.00005088
Iteration 219/1000 | Loss: 0.00005088
Iteration 220/1000 | Loss: 0.00005088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [5.0884013035101816e-05, 5.0884013035101816e-05, 5.0884013035101816e-05, 5.0884013035101816e-05, 5.0884013035101816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.0884013035101816e-05

Optimization complete. Final v2v error: 5.885157108306885 mm

Highest mean error: 7.1233296394348145 mm for frame 0

Lowest mean error: 5.378392219543457 mm for frame 33

Saving results

Total time: 44.81055474281311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448149
Iteration 2/25 | Loss: 0.00151995
Iteration 3/25 | Loss: 0.00140055
Iteration 4/25 | Loss: 0.00138779
Iteration 5/25 | Loss: 0.00138279
Iteration 6/25 | Loss: 0.00138119
Iteration 7/25 | Loss: 0.00138059
Iteration 8/25 | Loss: 0.00138055
Iteration 9/25 | Loss: 0.00138055
Iteration 10/25 | Loss: 0.00138055
Iteration 11/25 | Loss: 0.00138055
Iteration 12/25 | Loss: 0.00138055
Iteration 13/25 | Loss: 0.00138055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013805520720779896, 0.0013805520720779896, 0.0013805520720779896, 0.0013805520720779896, 0.0013805520720779896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013805520720779896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59830880
Iteration 2/25 | Loss: 0.00286069
Iteration 3/25 | Loss: 0.00286069
Iteration 4/25 | Loss: 0.00286069
Iteration 5/25 | Loss: 0.00286069
Iteration 6/25 | Loss: 0.00286069
Iteration 7/25 | Loss: 0.00286069
Iteration 8/25 | Loss: 0.00286069
Iteration 9/25 | Loss: 0.00286069
Iteration 10/25 | Loss: 0.00286069
Iteration 11/25 | Loss: 0.00286069
Iteration 12/25 | Loss: 0.00286069
Iteration 13/25 | Loss: 0.00286069
Iteration 14/25 | Loss: 0.00286069
Iteration 15/25 | Loss: 0.00286069
Iteration 16/25 | Loss: 0.00286069
Iteration 17/25 | Loss: 0.00286069
Iteration 18/25 | Loss: 0.00286069
Iteration 19/25 | Loss: 0.00286069
Iteration 20/25 | Loss: 0.00286069
Iteration 21/25 | Loss: 0.00286069
Iteration 22/25 | Loss: 0.00286069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002860687905922532, 0.002860687905922532, 0.002860687905922532, 0.002860687905922532, 0.002860687905922532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002860687905922532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286069
Iteration 2/1000 | Loss: 0.00004023
Iteration 3/1000 | Loss: 0.00002896
Iteration 4/1000 | Loss: 0.00002478
Iteration 5/1000 | Loss: 0.00002315
Iteration 6/1000 | Loss: 0.00002228
Iteration 7/1000 | Loss: 0.00002171
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002008
Iteration 11/1000 | Loss: 0.00001996
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001942
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001940
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001937
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001936
Iteration 27/1000 | Loss: 0.00001936
Iteration 28/1000 | Loss: 0.00001935
Iteration 29/1000 | Loss: 0.00001935
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001933
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001921
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001920
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001919
Iteration 73/1000 | Loss: 0.00001919
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001918
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001909
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001909
Iteration 113/1000 | Loss: 0.00001909
Iteration 114/1000 | Loss: 0.00001909
Iteration 115/1000 | Loss: 0.00001909
Iteration 116/1000 | Loss: 0.00001909
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001908
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001906
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001906
Iteration 132/1000 | Loss: 0.00001906
Iteration 133/1000 | Loss: 0.00001906
Iteration 134/1000 | Loss: 0.00001906
Iteration 135/1000 | Loss: 0.00001906
Iteration 136/1000 | Loss: 0.00001906
Iteration 137/1000 | Loss: 0.00001906
Iteration 138/1000 | Loss: 0.00001906
Iteration 139/1000 | Loss: 0.00001905
Iteration 140/1000 | Loss: 0.00001905
Iteration 141/1000 | Loss: 0.00001905
Iteration 142/1000 | Loss: 0.00001905
Iteration 143/1000 | Loss: 0.00001905
Iteration 144/1000 | Loss: 0.00001905
Iteration 145/1000 | Loss: 0.00001905
Iteration 146/1000 | Loss: 0.00001905
Iteration 147/1000 | Loss: 0.00001905
Iteration 148/1000 | Loss: 0.00001905
Iteration 149/1000 | Loss: 0.00001905
Iteration 150/1000 | Loss: 0.00001905
Iteration 151/1000 | Loss: 0.00001905
Iteration 152/1000 | Loss: 0.00001905
Iteration 153/1000 | Loss: 0.00001905
Iteration 154/1000 | Loss: 0.00001905
Iteration 155/1000 | Loss: 0.00001904
Iteration 156/1000 | Loss: 0.00001904
Iteration 157/1000 | Loss: 0.00001904
Iteration 158/1000 | Loss: 0.00001904
Iteration 159/1000 | Loss: 0.00001904
Iteration 160/1000 | Loss: 0.00001904
Iteration 161/1000 | Loss: 0.00001904
Iteration 162/1000 | Loss: 0.00001904
Iteration 163/1000 | Loss: 0.00001904
Iteration 164/1000 | Loss: 0.00001904
Iteration 165/1000 | Loss: 0.00001904
Iteration 166/1000 | Loss: 0.00001904
Iteration 167/1000 | Loss: 0.00001904
Iteration 168/1000 | Loss: 0.00001904
Iteration 169/1000 | Loss: 0.00001904
Iteration 170/1000 | Loss: 0.00001904
Iteration 171/1000 | Loss: 0.00001904
Iteration 172/1000 | Loss: 0.00001904
Iteration 173/1000 | Loss: 0.00001904
Iteration 174/1000 | Loss: 0.00001904
Iteration 175/1000 | Loss: 0.00001904
Iteration 176/1000 | Loss: 0.00001904
Iteration 177/1000 | Loss: 0.00001904
Iteration 178/1000 | Loss: 0.00001904
Iteration 179/1000 | Loss: 0.00001904
Iteration 180/1000 | Loss: 0.00001904
Iteration 181/1000 | Loss: 0.00001904
Iteration 182/1000 | Loss: 0.00001904
Iteration 183/1000 | Loss: 0.00001904
Iteration 184/1000 | Loss: 0.00001904
Iteration 185/1000 | Loss: 0.00001904
Iteration 186/1000 | Loss: 0.00001904
Iteration 187/1000 | Loss: 0.00001904
Iteration 188/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.9043483916902915e-05, 1.9043483916902915e-05, 1.9043483916902915e-05, 1.9043483916902915e-05, 1.9043483916902915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9043483916902915e-05

Optimization complete. Final v2v error: 3.85431170463562 mm

Highest mean error: 4.185139179229736 mm for frame 115

Lowest mean error: 3.467015027999878 mm for frame 4

Saving results

Total time: 39.484715938568115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546097
Iteration 2/25 | Loss: 0.00148841
Iteration 3/25 | Loss: 0.00137186
Iteration 4/25 | Loss: 0.00135704
Iteration 5/25 | Loss: 0.00135282
Iteration 6/25 | Loss: 0.00135128
Iteration 7/25 | Loss: 0.00135128
Iteration 8/25 | Loss: 0.00135128
Iteration 9/25 | Loss: 0.00135128
Iteration 10/25 | Loss: 0.00135128
Iteration 11/25 | Loss: 0.00135128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001351280719973147, 0.001351280719973147, 0.001351280719973147, 0.001351280719973147, 0.001351280719973147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001351280719973147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62052488
Iteration 2/25 | Loss: 0.00230079
Iteration 3/25 | Loss: 0.00230077
Iteration 4/25 | Loss: 0.00230077
Iteration 5/25 | Loss: 0.00230077
Iteration 6/25 | Loss: 0.00230077
Iteration 7/25 | Loss: 0.00230077
Iteration 8/25 | Loss: 0.00230077
Iteration 9/25 | Loss: 0.00230077
Iteration 10/25 | Loss: 0.00230077
Iteration 11/25 | Loss: 0.00230077
Iteration 12/25 | Loss: 0.00230077
Iteration 13/25 | Loss: 0.00230077
Iteration 14/25 | Loss: 0.00230077
Iteration 15/25 | Loss: 0.00230077
Iteration 16/25 | Loss: 0.00230077
Iteration 17/25 | Loss: 0.00230077
Iteration 18/25 | Loss: 0.00230077
Iteration 19/25 | Loss: 0.00230077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023007667623460293, 0.0023007667623460293, 0.0023007667623460293, 0.0023007667623460293, 0.0023007667623460293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023007667623460293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230077
Iteration 2/1000 | Loss: 0.00003259
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00002237
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002028
Iteration 8/1000 | Loss: 0.00001990
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001940
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001921
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001919
Iteration 16/1000 | Loss: 0.00001917
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001909
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001908
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001905
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001896
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001896
Iteration 41/1000 | Loss: 0.00001896
Iteration 42/1000 | Loss: 0.00001895
Iteration 43/1000 | Loss: 0.00001895
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001894
Iteration 47/1000 | Loss: 0.00001894
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001893
Iteration 50/1000 | Loss: 0.00001893
Iteration 51/1000 | Loss: 0.00001893
Iteration 52/1000 | Loss: 0.00001893
Iteration 53/1000 | Loss: 0.00001893
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001892
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001892
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001891
Iteration 62/1000 | Loss: 0.00001891
Iteration 63/1000 | Loss: 0.00001891
Iteration 64/1000 | Loss: 0.00001891
Iteration 65/1000 | Loss: 0.00001891
Iteration 66/1000 | Loss: 0.00001891
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001890
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001890
Iteration 76/1000 | Loss: 0.00001890
Iteration 77/1000 | Loss: 0.00001890
Iteration 78/1000 | Loss: 0.00001890
Iteration 79/1000 | Loss: 0.00001890
Iteration 80/1000 | Loss: 0.00001890
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001890
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001890
Iteration 85/1000 | Loss: 0.00001890
Iteration 86/1000 | Loss: 0.00001890
Iteration 87/1000 | Loss: 0.00001890
Iteration 88/1000 | Loss: 0.00001890
Iteration 89/1000 | Loss: 0.00001890
Iteration 90/1000 | Loss: 0.00001890
Iteration 91/1000 | Loss: 0.00001890
Iteration 92/1000 | Loss: 0.00001890
Iteration 93/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.8895323592005298e-05, 1.8895323592005298e-05, 1.8895323592005298e-05, 1.8895323592005298e-05, 1.8895323592005298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8895323592005298e-05

Optimization complete. Final v2v error: 3.865546226501465 mm

Highest mean error: 4.374646186828613 mm for frame 103

Lowest mean error: 3.498586654663086 mm for frame 57

Saving results

Total time: 36.59607744216919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050057
Iteration 2/25 | Loss: 0.00153994
Iteration 3/25 | Loss: 0.00140631
Iteration 4/25 | Loss: 0.00137993
Iteration 5/25 | Loss: 0.00138568
Iteration 6/25 | Loss: 0.00136974
Iteration 7/25 | Loss: 0.00136896
Iteration 8/25 | Loss: 0.00136877
Iteration 9/25 | Loss: 0.00136874
Iteration 10/25 | Loss: 0.00136874
Iteration 11/25 | Loss: 0.00136874
Iteration 12/25 | Loss: 0.00136874
Iteration 13/25 | Loss: 0.00136874
Iteration 14/25 | Loss: 0.00136873
Iteration 15/25 | Loss: 0.00136873
Iteration 16/25 | Loss: 0.00136873
Iteration 17/25 | Loss: 0.00136873
Iteration 18/25 | Loss: 0.00136873
Iteration 19/25 | Loss: 0.00136873
Iteration 20/25 | Loss: 0.00136873
Iteration 21/25 | Loss: 0.00136873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013687291648238897, 0.0013687291648238897, 0.0013687291648238897, 0.0013687291648238897, 0.0013687291648238897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013687291648238897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69983125
Iteration 2/25 | Loss: 0.00199587
Iteration 3/25 | Loss: 0.00199587
Iteration 4/25 | Loss: 0.00199587
Iteration 5/25 | Loss: 0.00199587
Iteration 6/25 | Loss: 0.00199587
Iteration 7/25 | Loss: 0.00199587
Iteration 8/25 | Loss: 0.00199587
Iteration 9/25 | Loss: 0.00199587
Iteration 10/25 | Loss: 0.00199587
Iteration 11/25 | Loss: 0.00199587
Iteration 12/25 | Loss: 0.00199587
Iteration 13/25 | Loss: 0.00199587
Iteration 14/25 | Loss: 0.00199587
Iteration 15/25 | Loss: 0.00199587
Iteration 16/25 | Loss: 0.00199587
Iteration 17/25 | Loss: 0.00199587
Iteration 18/25 | Loss: 0.00199587
Iteration 19/25 | Loss: 0.00199587
Iteration 20/25 | Loss: 0.00199587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019958687480539083, 0.0019958687480539083, 0.0019958687480539083, 0.0019958687480539083, 0.0019958687480539083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019958687480539083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199587
Iteration 2/1000 | Loss: 0.00005973
Iteration 3/1000 | Loss: 0.00003846
Iteration 4/1000 | Loss: 0.00003454
Iteration 5/1000 | Loss: 0.00003312
Iteration 6/1000 | Loss: 0.00003175
Iteration 7/1000 | Loss: 0.00003098
Iteration 8/1000 | Loss: 0.00003000
Iteration 9/1000 | Loss: 0.00002933
Iteration 10/1000 | Loss: 0.00002893
Iteration 11/1000 | Loss: 0.00002868
Iteration 12/1000 | Loss: 0.00002850
Iteration 13/1000 | Loss: 0.00002845
Iteration 14/1000 | Loss: 0.00002843
Iteration 15/1000 | Loss: 0.00002835
Iteration 16/1000 | Loss: 0.00002828
Iteration 17/1000 | Loss: 0.00002827
Iteration 18/1000 | Loss: 0.00002826
Iteration 19/1000 | Loss: 0.00002825
Iteration 20/1000 | Loss: 0.00002821
Iteration 21/1000 | Loss: 0.00002821
Iteration 22/1000 | Loss: 0.00002820
Iteration 23/1000 | Loss: 0.00002814
Iteration 24/1000 | Loss: 0.00002810
Iteration 25/1000 | Loss: 0.00002809
Iteration 26/1000 | Loss: 0.00002809
Iteration 27/1000 | Loss: 0.00002808
Iteration 28/1000 | Loss: 0.00002808
Iteration 29/1000 | Loss: 0.00002807
Iteration 30/1000 | Loss: 0.00002806
Iteration 31/1000 | Loss: 0.00002804
Iteration 32/1000 | Loss: 0.00002804
Iteration 33/1000 | Loss: 0.00002803
Iteration 34/1000 | Loss: 0.00002803
Iteration 35/1000 | Loss: 0.00002801
Iteration 36/1000 | Loss: 0.00002801
Iteration 37/1000 | Loss: 0.00002801
Iteration 38/1000 | Loss: 0.00002800
Iteration 39/1000 | Loss: 0.00002800
Iteration 40/1000 | Loss: 0.00002800
Iteration 41/1000 | Loss: 0.00002800
Iteration 42/1000 | Loss: 0.00002799
Iteration 43/1000 | Loss: 0.00002799
Iteration 44/1000 | Loss: 0.00002798
Iteration 45/1000 | Loss: 0.00002798
Iteration 46/1000 | Loss: 0.00002798
Iteration 47/1000 | Loss: 0.00002798
Iteration 48/1000 | Loss: 0.00002798
Iteration 49/1000 | Loss: 0.00002798
Iteration 50/1000 | Loss: 0.00002798
Iteration 51/1000 | Loss: 0.00002798
Iteration 52/1000 | Loss: 0.00002798
Iteration 53/1000 | Loss: 0.00002798
Iteration 54/1000 | Loss: 0.00002798
Iteration 55/1000 | Loss: 0.00002797
Iteration 56/1000 | Loss: 0.00002797
Iteration 57/1000 | Loss: 0.00002797
Iteration 58/1000 | Loss: 0.00002797
Iteration 59/1000 | Loss: 0.00002797
Iteration 60/1000 | Loss: 0.00002797
Iteration 61/1000 | Loss: 0.00002797
Iteration 62/1000 | Loss: 0.00002797
Iteration 63/1000 | Loss: 0.00002796
Iteration 64/1000 | Loss: 0.00002796
Iteration 65/1000 | Loss: 0.00002796
Iteration 66/1000 | Loss: 0.00002796
Iteration 67/1000 | Loss: 0.00002795
Iteration 68/1000 | Loss: 0.00002795
Iteration 69/1000 | Loss: 0.00002795
Iteration 70/1000 | Loss: 0.00002795
Iteration 71/1000 | Loss: 0.00002795
Iteration 72/1000 | Loss: 0.00002795
Iteration 73/1000 | Loss: 0.00002795
Iteration 74/1000 | Loss: 0.00002795
Iteration 75/1000 | Loss: 0.00002795
Iteration 76/1000 | Loss: 0.00002795
Iteration 77/1000 | Loss: 0.00002794
Iteration 78/1000 | Loss: 0.00002794
Iteration 79/1000 | Loss: 0.00002794
Iteration 80/1000 | Loss: 0.00002794
Iteration 81/1000 | Loss: 0.00002794
Iteration 82/1000 | Loss: 0.00002794
Iteration 83/1000 | Loss: 0.00002794
Iteration 84/1000 | Loss: 0.00002794
Iteration 85/1000 | Loss: 0.00002794
Iteration 86/1000 | Loss: 0.00002794
Iteration 87/1000 | Loss: 0.00002793
Iteration 88/1000 | Loss: 0.00002793
Iteration 89/1000 | Loss: 0.00002793
Iteration 90/1000 | Loss: 0.00002793
Iteration 91/1000 | Loss: 0.00002793
Iteration 92/1000 | Loss: 0.00002793
Iteration 93/1000 | Loss: 0.00002793
Iteration 94/1000 | Loss: 0.00002793
Iteration 95/1000 | Loss: 0.00002793
Iteration 96/1000 | Loss: 0.00002793
Iteration 97/1000 | Loss: 0.00002793
Iteration 98/1000 | Loss: 0.00002793
Iteration 99/1000 | Loss: 0.00002793
Iteration 100/1000 | Loss: 0.00002792
Iteration 101/1000 | Loss: 0.00002792
Iteration 102/1000 | Loss: 0.00002792
Iteration 103/1000 | Loss: 0.00002792
Iteration 104/1000 | Loss: 0.00002792
Iteration 105/1000 | Loss: 0.00002792
Iteration 106/1000 | Loss: 0.00002792
Iteration 107/1000 | Loss: 0.00002792
Iteration 108/1000 | Loss: 0.00002792
Iteration 109/1000 | Loss: 0.00002792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.792075611068867e-05, 2.792075611068867e-05, 2.792075611068867e-05, 2.792075611068867e-05, 2.792075611068867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.792075611068867e-05

Optimization complete. Final v2v error: 4.2752885818481445 mm

Highest mean error: 6.677734375 mm for frame 179

Lowest mean error: 3.5917344093322754 mm for frame 111

Saving results

Total time: 45.69543671607971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908260
Iteration 2/25 | Loss: 0.00168623
Iteration 3/25 | Loss: 0.00147874
Iteration 4/25 | Loss: 0.00144998
Iteration 5/25 | Loss: 0.00144118
Iteration 6/25 | Loss: 0.00143981
Iteration 7/25 | Loss: 0.00143981
Iteration 8/25 | Loss: 0.00143981
Iteration 9/25 | Loss: 0.00143981
Iteration 10/25 | Loss: 0.00143981
Iteration 11/25 | Loss: 0.00143981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001439807703718543, 0.001439807703718543, 0.001439807703718543, 0.001439807703718543, 0.001439807703718543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001439807703718543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01884222
Iteration 2/25 | Loss: 0.00288554
Iteration 3/25 | Loss: 0.00288554
Iteration 4/25 | Loss: 0.00288554
Iteration 5/25 | Loss: 0.00288554
Iteration 6/25 | Loss: 0.00288554
Iteration 7/25 | Loss: 0.00288554
Iteration 8/25 | Loss: 0.00288554
Iteration 9/25 | Loss: 0.00288554
Iteration 10/25 | Loss: 0.00288554
Iteration 11/25 | Loss: 0.00288554
Iteration 12/25 | Loss: 0.00288554
Iteration 13/25 | Loss: 0.00288554
Iteration 14/25 | Loss: 0.00288554
Iteration 15/25 | Loss: 0.00288554
Iteration 16/25 | Loss: 0.00288554
Iteration 17/25 | Loss: 0.00288554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002885539084672928, 0.002885539084672928, 0.002885539084672928, 0.002885539084672928, 0.002885539084672928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002885539084672928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288554
Iteration 2/1000 | Loss: 0.00004445
Iteration 3/1000 | Loss: 0.00003355
Iteration 4/1000 | Loss: 0.00002916
Iteration 5/1000 | Loss: 0.00002746
Iteration 6/1000 | Loss: 0.00002607
Iteration 7/1000 | Loss: 0.00002538
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002388
Iteration 12/1000 | Loss: 0.00002367
Iteration 13/1000 | Loss: 0.00002350
Iteration 14/1000 | Loss: 0.00002338
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002335
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002334
Iteration 19/1000 | Loss: 0.00002334
Iteration 20/1000 | Loss: 0.00002333
Iteration 21/1000 | Loss: 0.00002333
Iteration 22/1000 | Loss: 0.00002331
Iteration 23/1000 | Loss: 0.00002331
Iteration 24/1000 | Loss: 0.00002328
Iteration 25/1000 | Loss: 0.00002327
Iteration 26/1000 | Loss: 0.00002327
Iteration 27/1000 | Loss: 0.00002327
Iteration 28/1000 | Loss: 0.00002325
Iteration 29/1000 | Loss: 0.00002325
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002323
Iteration 32/1000 | Loss: 0.00002323
Iteration 33/1000 | Loss: 0.00002323
Iteration 34/1000 | Loss: 0.00002320
Iteration 35/1000 | Loss: 0.00002319
Iteration 36/1000 | Loss: 0.00002319
Iteration 37/1000 | Loss: 0.00002319
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002319
Iteration 40/1000 | Loss: 0.00002319
Iteration 41/1000 | Loss: 0.00002318
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00002318
Iteration 44/1000 | Loss: 0.00002317
Iteration 45/1000 | Loss: 0.00002317
Iteration 46/1000 | Loss: 0.00002317
Iteration 47/1000 | Loss: 0.00002317
Iteration 48/1000 | Loss: 0.00002317
Iteration 49/1000 | Loss: 0.00002317
Iteration 50/1000 | Loss: 0.00002317
Iteration 51/1000 | Loss: 0.00002317
Iteration 52/1000 | Loss: 0.00002317
Iteration 53/1000 | Loss: 0.00002317
Iteration 54/1000 | Loss: 0.00002317
Iteration 55/1000 | Loss: 0.00002317
Iteration 56/1000 | Loss: 0.00002317
Iteration 57/1000 | Loss: 0.00002317
Iteration 58/1000 | Loss: 0.00002317
Iteration 59/1000 | Loss: 0.00002317
Iteration 60/1000 | Loss: 0.00002317
Iteration 61/1000 | Loss: 0.00002317
Iteration 62/1000 | Loss: 0.00002317
Iteration 63/1000 | Loss: 0.00002317
Iteration 64/1000 | Loss: 0.00002317
Iteration 65/1000 | Loss: 0.00002317
Iteration 66/1000 | Loss: 0.00002317
Iteration 67/1000 | Loss: 0.00002317
Iteration 68/1000 | Loss: 0.00002317
Iteration 69/1000 | Loss: 0.00002317
Iteration 70/1000 | Loss: 0.00002317
Iteration 71/1000 | Loss: 0.00002317
Iteration 72/1000 | Loss: 0.00002317
Iteration 73/1000 | Loss: 0.00002317
Iteration 74/1000 | Loss: 0.00002317
Iteration 75/1000 | Loss: 0.00002317
Iteration 76/1000 | Loss: 0.00002317
Iteration 77/1000 | Loss: 0.00002317
Iteration 78/1000 | Loss: 0.00002317
Iteration 79/1000 | Loss: 0.00002317
Iteration 80/1000 | Loss: 0.00002317
Iteration 81/1000 | Loss: 0.00002317
Iteration 82/1000 | Loss: 0.00002317
Iteration 83/1000 | Loss: 0.00002317
Iteration 84/1000 | Loss: 0.00002317
Iteration 85/1000 | Loss: 0.00002317
Iteration 86/1000 | Loss: 0.00002317
Iteration 87/1000 | Loss: 0.00002317
Iteration 88/1000 | Loss: 0.00002317
Iteration 89/1000 | Loss: 0.00002317
Iteration 90/1000 | Loss: 0.00002317
Iteration 91/1000 | Loss: 0.00002317
Iteration 92/1000 | Loss: 0.00002317
Iteration 93/1000 | Loss: 0.00002317
Iteration 94/1000 | Loss: 0.00002317
Iteration 95/1000 | Loss: 0.00002317
Iteration 96/1000 | Loss: 0.00002317
Iteration 97/1000 | Loss: 0.00002317
Iteration 98/1000 | Loss: 0.00002317
Iteration 99/1000 | Loss: 0.00002317
Iteration 100/1000 | Loss: 0.00002317
Iteration 101/1000 | Loss: 0.00002317
Iteration 102/1000 | Loss: 0.00002317
Iteration 103/1000 | Loss: 0.00002317
Iteration 104/1000 | Loss: 0.00002317
Iteration 105/1000 | Loss: 0.00002317
Iteration 106/1000 | Loss: 0.00002317
Iteration 107/1000 | Loss: 0.00002317
Iteration 108/1000 | Loss: 0.00002317
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002317
Iteration 115/1000 | Loss: 0.00002317
Iteration 116/1000 | Loss: 0.00002317
Iteration 117/1000 | Loss: 0.00002317
Iteration 118/1000 | Loss: 0.00002317
Iteration 119/1000 | Loss: 0.00002317
Iteration 120/1000 | Loss: 0.00002317
Iteration 121/1000 | Loss: 0.00002317
Iteration 122/1000 | Loss: 0.00002317
Iteration 123/1000 | Loss: 0.00002317
Iteration 124/1000 | Loss: 0.00002317
Iteration 125/1000 | Loss: 0.00002317
Iteration 126/1000 | Loss: 0.00002317
Iteration 127/1000 | Loss: 0.00002317
Iteration 128/1000 | Loss: 0.00002317
Iteration 129/1000 | Loss: 0.00002317
Iteration 130/1000 | Loss: 0.00002317
Iteration 131/1000 | Loss: 0.00002317
Iteration 132/1000 | Loss: 0.00002317
Iteration 133/1000 | Loss: 0.00002317
Iteration 134/1000 | Loss: 0.00002317
Iteration 135/1000 | Loss: 0.00002317
Iteration 136/1000 | Loss: 0.00002317
Iteration 137/1000 | Loss: 0.00002317
Iteration 138/1000 | Loss: 0.00002317
Iteration 139/1000 | Loss: 0.00002317
Iteration 140/1000 | Loss: 0.00002317
Iteration 141/1000 | Loss: 0.00002317
Iteration 142/1000 | Loss: 0.00002317
Iteration 143/1000 | Loss: 0.00002317
Iteration 144/1000 | Loss: 0.00002317
Iteration 145/1000 | Loss: 0.00002317
Iteration 146/1000 | Loss: 0.00002317
Iteration 147/1000 | Loss: 0.00002317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.316874815733172e-05, 2.316874815733172e-05, 2.316874815733172e-05, 2.316874815733172e-05, 2.316874815733172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.316874815733172e-05

Optimization complete. Final v2v error: 4.249234199523926 mm

Highest mean error: 5.008155822753906 mm for frame 168

Lowest mean error: 3.834169387817383 mm for frame 71

Saving results

Total time: 37.09072732925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909959
Iteration 2/25 | Loss: 0.00147367
Iteration 3/25 | Loss: 0.00139404
Iteration 4/25 | Loss: 0.00137136
Iteration 5/25 | Loss: 0.00136651
Iteration 6/25 | Loss: 0.00136578
Iteration 7/25 | Loss: 0.00136578
Iteration 8/25 | Loss: 0.00136578
Iteration 9/25 | Loss: 0.00136578
Iteration 10/25 | Loss: 0.00136578
Iteration 11/25 | Loss: 0.00136578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001365779316984117, 0.001365779316984117, 0.001365779316984117, 0.001365779316984117, 0.001365779316984117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365779316984117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60971189
Iteration 2/25 | Loss: 0.00214485
Iteration 3/25 | Loss: 0.00214485
Iteration 4/25 | Loss: 0.00214485
Iteration 5/25 | Loss: 0.00214485
Iteration 6/25 | Loss: 0.00214485
Iteration 7/25 | Loss: 0.00214485
Iteration 8/25 | Loss: 0.00214485
Iteration 9/25 | Loss: 0.00214485
Iteration 10/25 | Loss: 0.00214485
Iteration 11/25 | Loss: 0.00214485
Iteration 12/25 | Loss: 0.00214485
Iteration 13/25 | Loss: 0.00214485
Iteration 14/25 | Loss: 0.00214485
Iteration 15/25 | Loss: 0.00214485
Iteration 16/25 | Loss: 0.00214485
Iteration 17/25 | Loss: 0.00214485
Iteration 18/25 | Loss: 0.00214485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021448510233312845, 0.0021448510233312845, 0.0021448510233312845, 0.0021448510233312845, 0.0021448510233312845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021448510233312845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214485
Iteration 2/1000 | Loss: 0.00004601
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00002956
Iteration 5/1000 | Loss: 0.00002840
Iteration 6/1000 | Loss: 0.00002712
Iteration 7/1000 | Loss: 0.00002635
Iteration 8/1000 | Loss: 0.00002583
Iteration 9/1000 | Loss: 0.00002556
Iteration 10/1000 | Loss: 0.00002532
Iteration 11/1000 | Loss: 0.00002529
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002522
Iteration 14/1000 | Loss: 0.00002521
Iteration 15/1000 | Loss: 0.00002520
Iteration 16/1000 | Loss: 0.00002520
Iteration 17/1000 | Loss: 0.00002516
Iteration 18/1000 | Loss: 0.00002516
Iteration 19/1000 | Loss: 0.00002515
Iteration 20/1000 | Loss: 0.00002514
Iteration 21/1000 | Loss: 0.00002514
Iteration 22/1000 | Loss: 0.00002514
Iteration 23/1000 | Loss: 0.00002513
Iteration 24/1000 | Loss: 0.00002513
Iteration 25/1000 | Loss: 0.00002512
Iteration 26/1000 | Loss: 0.00002512
Iteration 27/1000 | Loss: 0.00002511
Iteration 28/1000 | Loss: 0.00002511
Iteration 29/1000 | Loss: 0.00002511
Iteration 30/1000 | Loss: 0.00002509
Iteration 31/1000 | Loss: 0.00002509
Iteration 32/1000 | Loss: 0.00002509
Iteration 33/1000 | Loss: 0.00002509
Iteration 34/1000 | Loss: 0.00002509
Iteration 35/1000 | Loss: 0.00002509
Iteration 36/1000 | Loss: 0.00002509
Iteration 37/1000 | Loss: 0.00002509
Iteration 38/1000 | Loss: 0.00002509
Iteration 39/1000 | Loss: 0.00002509
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002509
Iteration 43/1000 | Loss: 0.00002509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [2.508606849005446e-05, 2.508606849005446e-05, 2.508606849005446e-05, 2.508606849005446e-05, 2.508606849005446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.508606849005446e-05

Optimization complete. Final v2v error: 4.273305416107178 mm

Highest mean error: 4.694462776184082 mm for frame 63

Lowest mean error: 3.991048812866211 mm for frame 143

Saving results

Total time: 25.335503339767456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797977
Iteration 2/25 | Loss: 0.00160446
Iteration 3/25 | Loss: 0.00149867
Iteration 4/25 | Loss: 0.00148205
Iteration 5/25 | Loss: 0.00147608
Iteration 6/25 | Loss: 0.00147498
Iteration 7/25 | Loss: 0.00147498
Iteration 8/25 | Loss: 0.00147498
Iteration 9/25 | Loss: 0.00147498
Iteration 10/25 | Loss: 0.00147498
Iteration 11/25 | Loss: 0.00147498
Iteration 12/25 | Loss: 0.00147498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014749829424545169, 0.0014749829424545169, 0.0014749829424545169, 0.0014749829424545169, 0.0014749829424545169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014749829424545169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.04784298
Iteration 2/25 | Loss: 0.00239194
Iteration 3/25 | Loss: 0.00239194
Iteration 4/25 | Loss: 0.00239193
Iteration 5/25 | Loss: 0.00239193
Iteration 6/25 | Loss: 0.00239193
Iteration 7/25 | Loss: 0.00239193
Iteration 8/25 | Loss: 0.00239193
Iteration 9/25 | Loss: 0.00239193
Iteration 10/25 | Loss: 0.00239193
Iteration 11/25 | Loss: 0.00239193
Iteration 12/25 | Loss: 0.00239193
Iteration 13/25 | Loss: 0.00239193
Iteration 14/25 | Loss: 0.00239193
Iteration 15/25 | Loss: 0.00239193
Iteration 16/25 | Loss: 0.00239193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002391933696344495, 0.002391933696344495, 0.002391933696344495, 0.002391933696344495, 0.002391933696344495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002391933696344495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239193
Iteration 2/1000 | Loss: 0.00005710
Iteration 3/1000 | Loss: 0.00004270
Iteration 4/1000 | Loss: 0.00003742
Iteration 5/1000 | Loss: 0.00003514
Iteration 6/1000 | Loss: 0.00003356
Iteration 7/1000 | Loss: 0.00003246
Iteration 8/1000 | Loss: 0.00003182
Iteration 9/1000 | Loss: 0.00003139
Iteration 10/1000 | Loss: 0.00003117
Iteration 11/1000 | Loss: 0.00003089
Iteration 12/1000 | Loss: 0.00003068
Iteration 13/1000 | Loss: 0.00003065
Iteration 14/1000 | Loss: 0.00003050
Iteration 15/1000 | Loss: 0.00003047
Iteration 16/1000 | Loss: 0.00003044
Iteration 17/1000 | Loss: 0.00003044
Iteration 18/1000 | Loss: 0.00003044
Iteration 19/1000 | Loss: 0.00003044
Iteration 20/1000 | Loss: 0.00003044
Iteration 21/1000 | Loss: 0.00003044
Iteration 22/1000 | Loss: 0.00003044
Iteration 23/1000 | Loss: 0.00003044
Iteration 24/1000 | Loss: 0.00003044
Iteration 25/1000 | Loss: 0.00003044
Iteration 26/1000 | Loss: 0.00003043
Iteration 27/1000 | Loss: 0.00003040
Iteration 28/1000 | Loss: 0.00003039
Iteration 29/1000 | Loss: 0.00003039
Iteration 30/1000 | Loss: 0.00003039
Iteration 31/1000 | Loss: 0.00003039
Iteration 32/1000 | Loss: 0.00003037
Iteration 33/1000 | Loss: 0.00003037
Iteration 34/1000 | Loss: 0.00003035
Iteration 35/1000 | Loss: 0.00003034
Iteration 36/1000 | Loss: 0.00003034
Iteration 37/1000 | Loss: 0.00003034
Iteration 38/1000 | Loss: 0.00003033
Iteration 39/1000 | Loss: 0.00003033
Iteration 40/1000 | Loss: 0.00003032
Iteration 41/1000 | Loss: 0.00003032
Iteration 42/1000 | Loss: 0.00003032
Iteration 43/1000 | Loss: 0.00003031
Iteration 44/1000 | Loss: 0.00003031
Iteration 45/1000 | Loss: 0.00003031
Iteration 46/1000 | Loss: 0.00003030
Iteration 47/1000 | Loss: 0.00003030
Iteration 48/1000 | Loss: 0.00003030
Iteration 49/1000 | Loss: 0.00003030
Iteration 50/1000 | Loss: 0.00003030
Iteration 51/1000 | Loss: 0.00003030
Iteration 52/1000 | Loss: 0.00003030
Iteration 53/1000 | Loss: 0.00003030
Iteration 54/1000 | Loss: 0.00003030
Iteration 55/1000 | Loss: 0.00003029
Iteration 56/1000 | Loss: 0.00003029
Iteration 57/1000 | Loss: 0.00003029
Iteration 58/1000 | Loss: 0.00003029
Iteration 59/1000 | Loss: 0.00003029
Iteration 60/1000 | Loss: 0.00003029
Iteration 61/1000 | Loss: 0.00003028
Iteration 62/1000 | Loss: 0.00003028
Iteration 63/1000 | Loss: 0.00003028
Iteration 64/1000 | Loss: 0.00003028
Iteration 65/1000 | Loss: 0.00003028
Iteration 66/1000 | Loss: 0.00003028
Iteration 67/1000 | Loss: 0.00003028
Iteration 68/1000 | Loss: 0.00003028
Iteration 69/1000 | Loss: 0.00003028
Iteration 70/1000 | Loss: 0.00003028
Iteration 71/1000 | Loss: 0.00003028
Iteration 72/1000 | Loss: 0.00003028
Iteration 73/1000 | Loss: 0.00003028
Iteration 74/1000 | Loss: 0.00003028
Iteration 75/1000 | Loss: 0.00003027
Iteration 76/1000 | Loss: 0.00003027
Iteration 77/1000 | Loss: 0.00003027
Iteration 78/1000 | Loss: 0.00003027
Iteration 79/1000 | Loss: 0.00003027
Iteration 80/1000 | Loss: 0.00003027
Iteration 81/1000 | Loss: 0.00003027
Iteration 82/1000 | Loss: 0.00003027
Iteration 83/1000 | Loss: 0.00003027
Iteration 84/1000 | Loss: 0.00003026
Iteration 85/1000 | Loss: 0.00003026
Iteration 86/1000 | Loss: 0.00003026
Iteration 87/1000 | Loss: 0.00003026
Iteration 88/1000 | Loss: 0.00003026
Iteration 89/1000 | Loss: 0.00003026
Iteration 90/1000 | Loss: 0.00003026
Iteration 91/1000 | Loss: 0.00003026
Iteration 92/1000 | Loss: 0.00003026
Iteration 93/1000 | Loss: 0.00003026
Iteration 94/1000 | Loss: 0.00003026
Iteration 95/1000 | Loss: 0.00003026
Iteration 96/1000 | Loss: 0.00003026
Iteration 97/1000 | Loss: 0.00003025
Iteration 98/1000 | Loss: 0.00003025
Iteration 99/1000 | Loss: 0.00003025
Iteration 100/1000 | Loss: 0.00003025
Iteration 101/1000 | Loss: 0.00003025
Iteration 102/1000 | Loss: 0.00003025
Iteration 103/1000 | Loss: 0.00003025
Iteration 104/1000 | Loss: 0.00003025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [3.0252791475504637e-05, 3.0252791475504637e-05, 3.0252791475504637e-05, 3.0252791475504637e-05, 3.0252791475504637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0252791475504637e-05

Optimization complete. Final v2v error: 4.799656867980957 mm

Highest mean error: 5.273068904876709 mm for frame 9

Lowest mean error: 4.297008037567139 mm for frame 214

Saving results

Total time: 37.12628364562988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6248/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6248/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01174057
Iteration 2/25 | Loss: 0.00501898
Iteration 3/25 | Loss: 0.00307237
Iteration 4/25 | Loss: 0.00230728
Iteration 5/25 | Loss: 0.00226418
Iteration 6/25 | Loss: 0.00200140
Iteration 7/25 | Loss: 0.00191274
Iteration 8/25 | Loss: 0.00181221
Iteration 9/25 | Loss: 0.00186141
Iteration 10/25 | Loss: 0.00171140
Iteration 11/25 | Loss: 0.00170095
Iteration 12/25 | Loss: 0.00168860
Iteration 13/25 | Loss: 0.00166680
Iteration 14/25 | Loss: 0.00167359
Iteration 15/25 | Loss: 0.00166711
Iteration 16/25 | Loss: 0.00166322
Iteration 17/25 | Loss: 0.00165258
Iteration 18/25 | Loss: 0.00165353
Iteration 19/25 | Loss: 0.00164971
Iteration 20/25 | Loss: 0.00164282
Iteration 21/25 | Loss: 0.00164149
Iteration 22/25 | Loss: 0.00164146
Iteration 23/25 | Loss: 0.00164162
Iteration 24/25 | Loss: 0.00164349
Iteration 25/25 | Loss: 0.00164272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62034941
Iteration 2/25 | Loss: 0.00394119
Iteration 3/25 | Loss: 0.00388570
Iteration 4/25 | Loss: 0.00388570
Iteration 5/25 | Loss: 0.00388569
Iteration 6/25 | Loss: 0.00388569
Iteration 7/25 | Loss: 0.00388569
Iteration 8/25 | Loss: 0.00388569
Iteration 9/25 | Loss: 0.00388569
Iteration 10/25 | Loss: 0.00388569
Iteration 11/25 | Loss: 0.00388569
Iteration 12/25 | Loss: 0.00388569
Iteration 13/25 | Loss: 0.00388569
Iteration 14/25 | Loss: 0.00388569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0038856924511492252, 0.0038856924511492252, 0.0038856924511492252, 0.0038856924511492252, 0.0038856924511492252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038856924511492252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00388569
Iteration 2/1000 | Loss: 0.00082167
Iteration 3/1000 | Loss: 0.00142574
Iteration 4/1000 | Loss: 0.00077260
Iteration 5/1000 | Loss: 0.00429984
Iteration 6/1000 | Loss: 0.00048517
Iteration 7/1000 | Loss: 0.00263824
Iteration 8/1000 | Loss: 0.00096146
Iteration 9/1000 | Loss: 0.00127990
Iteration 10/1000 | Loss: 0.00033767
Iteration 11/1000 | Loss: 0.00040444
Iteration 12/1000 | Loss: 0.00242114
Iteration 13/1000 | Loss: 0.00360747
Iteration 14/1000 | Loss: 0.00319952
Iteration 15/1000 | Loss: 0.00249488
Iteration 16/1000 | Loss: 0.00514683
Iteration 17/1000 | Loss: 0.00393183
Iteration 18/1000 | Loss: 0.00047828
Iteration 19/1000 | Loss: 0.00035373
Iteration 20/1000 | Loss: 0.00186703
Iteration 21/1000 | Loss: 0.00014114
Iteration 22/1000 | Loss: 0.00050535
Iteration 23/1000 | Loss: 0.00062511
Iteration 24/1000 | Loss: 0.00008961
Iteration 25/1000 | Loss: 0.00044251
Iteration 26/1000 | Loss: 0.00008235
Iteration 27/1000 | Loss: 0.00041039
Iteration 28/1000 | Loss: 0.00008420
Iteration 29/1000 | Loss: 0.00008096
Iteration 30/1000 | Loss: 0.00029400
Iteration 31/1000 | Loss: 0.00018352
Iteration 32/1000 | Loss: 0.00074660
Iteration 33/1000 | Loss: 0.00009271
Iteration 34/1000 | Loss: 0.00017230
Iteration 35/1000 | Loss: 0.00010775
Iteration 36/1000 | Loss: 0.00026687
Iteration 37/1000 | Loss: 0.00012967
Iteration 38/1000 | Loss: 0.00145285
Iteration 39/1000 | Loss: 0.00029780
Iteration 40/1000 | Loss: 0.00008694
Iteration 41/1000 | Loss: 0.00008276
Iteration 42/1000 | Loss: 0.00005086
Iteration 43/1000 | Loss: 0.00007771
Iteration 44/1000 | Loss: 0.00003631
Iteration 45/1000 | Loss: 0.00020801
Iteration 46/1000 | Loss: 0.00032949
Iteration 47/1000 | Loss: 0.00004984
Iteration 48/1000 | Loss: 0.00006235
Iteration 49/1000 | Loss: 0.00004158
Iteration 50/1000 | Loss: 0.00006438
Iteration 51/1000 | Loss: 0.00015157
Iteration 52/1000 | Loss: 0.00003818
Iteration 53/1000 | Loss: 0.00008734
Iteration 54/1000 | Loss: 0.00056922
Iteration 55/1000 | Loss: 0.00227308
Iteration 56/1000 | Loss: 0.00104025
Iteration 57/1000 | Loss: 0.00040330
Iteration 58/1000 | Loss: 0.00263140
Iteration 59/1000 | Loss: 0.00015024
Iteration 60/1000 | Loss: 0.00081411
Iteration 61/1000 | Loss: 0.00008180
Iteration 62/1000 | Loss: 0.00003851
Iteration 63/1000 | Loss: 0.00008894
Iteration 64/1000 | Loss: 0.00033403
Iteration 65/1000 | Loss: 0.00004429
Iteration 66/1000 | Loss: 0.00006193
Iteration 67/1000 | Loss: 0.00011012
Iteration 68/1000 | Loss: 0.00005071
Iteration 69/1000 | Loss: 0.00007107
Iteration 70/1000 | Loss: 0.00003520
Iteration 71/1000 | Loss: 0.00003515
Iteration 72/1000 | Loss: 0.00006851
Iteration 73/1000 | Loss: 0.00028556
Iteration 74/1000 | Loss: 0.00014256
Iteration 75/1000 | Loss: 0.00004415
Iteration 76/1000 | Loss: 0.00011798
Iteration 77/1000 | Loss: 0.00005784
Iteration 78/1000 | Loss: 0.00006973
Iteration 79/1000 | Loss: 0.00012219
Iteration 80/1000 | Loss: 0.00003474
Iteration 81/1000 | Loss: 0.00004782
Iteration 82/1000 | Loss: 0.00015250
Iteration 83/1000 | Loss: 0.00006649
Iteration 84/1000 | Loss: 0.00006180
Iteration 85/1000 | Loss: 0.00017758
Iteration 86/1000 | Loss: 0.00004846
Iteration 87/1000 | Loss: 0.00004118
Iteration 88/1000 | Loss: 0.00005569
Iteration 89/1000 | Loss: 0.00007143
Iteration 90/1000 | Loss: 0.00005901
Iteration 91/1000 | Loss: 0.00063190
Iteration 92/1000 | Loss: 0.00018269
Iteration 93/1000 | Loss: 0.00004102
Iteration 94/1000 | Loss: 0.00011459
Iteration 95/1000 | Loss: 0.00004656
Iteration 96/1000 | Loss: 0.00007203
Iteration 97/1000 | Loss: 0.00003570
Iteration 98/1000 | Loss: 0.00007796
Iteration 99/1000 | Loss: 0.00004000
Iteration 100/1000 | Loss: 0.00004787
Iteration 101/1000 | Loss: 0.00004225
Iteration 102/1000 | Loss: 0.00003461
Iteration 103/1000 | Loss: 0.00006895
Iteration 104/1000 | Loss: 0.00003894
Iteration 105/1000 | Loss: 0.00003456
Iteration 106/1000 | Loss: 0.00003584
Iteration 107/1000 | Loss: 0.00003521
Iteration 108/1000 | Loss: 0.00003607
Iteration 109/1000 | Loss: 0.00005285
Iteration 110/1000 | Loss: 0.00003748
Iteration 111/1000 | Loss: 0.00003806
Iteration 112/1000 | Loss: 0.00003597
Iteration 113/1000 | Loss: 0.00005128
Iteration 114/1000 | Loss: 0.00004420
Iteration 115/1000 | Loss: 0.00003583
Iteration 116/1000 | Loss: 0.00003455
Iteration 117/1000 | Loss: 0.00003455
Iteration 118/1000 | Loss: 0.00003455
Iteration 119/1000 | Loss: 0.00003455
Iteration 120/1000 | Loss: 0.00003455
Iteration 121/1000 | Loss: 0.00003455
Iteration 122/1000 | Loss: 0.00003455
Iteration 123/1000 | Loss: 0.00003455
Iteration 124/1000 | Loss: 0.00003455
Iteration 125/1000 | Loss: 0.00003455
Iteration 126/1000 | Loss: 0.00004682
Iteration 127/1000 | Loss: 0.00036759
Iteration 128/1000 | Loss: 0.00008016
Iteration 129/1000 | Loss: 0.00004709
Iteration 130/1000 | Loss: 0.00005615
Iteration 131/1000 | Loss: 0.00004894
Iteration 132/1000 | Loss: 0.00022530
Iteration 133/1000 | Loss: 0.00024228
Iteration 134/1000 | Loss: 0.00004074
Iteration 135/1000 | Loss: 0.00003481
Iteration 136/1000 | Loss: 0.00003747
Iteration 137/1000 | Loss: 0.00006453
Iteration 138/1000 | Loss: 0.00003577
Iteration 139/1000 | Loss: 0.00003466
Iteration 140/1000 | Loss: 0.00003466
Iteration 141/1000 | Loss: 0.00003604
Iteration 142/1000 | Loss: 0.00004448
Iteration 143/1000 | Loss: 0.00007320
Iteration 144/1000 | Loss: 0.00004755
Iteration 145/1000 | Loss: 0.00003453
Iteration 146/1000 | Loss: 0.00003453
Iteration 147/1000 | Loss: 0.00003453
Iteration 148/1000 | Loss: 0.00003452
Iteration 149/1000 | Loss: 0.00003452
Iteration 150/1000 | Loss: 0.00003452
Iteration 151/1000 | Loss: 0.00003452
Iteration 152/1000 | Loss: 0.00003452
Iteration 153/1000 | Loss: 0.00003452
Iteration 154/1000 | Loss: 0.00003914
Iteration 155/1000 | Loss: 0.00003539
Iteration 156/1000 | Loss: 0.00003451
Iteration 157/1000 | Loss: 0.00003451
Iteration 158/1000 | Loss: 0.00003451
Iteration 159/1000 | Loss: 0.00003451
Iteration 160/1000 | Loss: 0.00003451
Iteration 161/1000 | Loss: 0.00003451
Iteration 162/1000 | Loss: 0.00003451
Iteration 163/1000 | Loss: 0.00003451
Iteration 164/1000 | Loss: 0.00003451
Iteration 165/1000 | Loss: 0.00003451
Iteration 166/1000 | Loss: 0.00003451
Iteration 167/1000 | Loss: 0.00003451
Iteration 168/1000 | Loss: 0.00003451
Iteration 169/1000 | Loss: 0.00003451
Iteration 170/1000 | Loss: 0.00003451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [3.450547228567302e-05, 3.450547228567302e-05, 3.450547228567302e-05, 3.450547228567302e-05, 3.450547228567302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.450547228567302e-05

Optimization complete. Final v2v error: 4.784693241119385 mm

Highest mean error: 11.401512145996094 mm for frame 80

Lowest mean error: 4.39616584777832 mm for frame 142

Saving results

Total time: 221.08050632476807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991226
Iteration 2/25 | Loss: 0.00991226
Iteration 3/25 | Loss: 0.00991226
Iteration 4/25 | Loss: 0.00991226
Iteration 5/25 | Loss: 0.00991226
Iteration 6/25 | Loss: 0.00991225
Iteration 7/25 | Loss: 0.00991225
Iteration 8/25 | Loss: 0.00991225
Iteration 9/25 | Loss: 0.00991225
Iteration 10/25 | Loss: 0.00991225
Iteration 11/25 | Loss: 0.00991224
Iteration 12/25 | Loss: 0.00991224
Iteration 13/25 | Loss: 0.00991224
Iteration 14/25 | Loss: 0.00991224
Iteration 15/25 | Loss: 0.00991223
Iteration 16/25 | Loss: 0.00991223
Iteration 17/25 | Loss: 0.00991223
Iteration 18/25 | Loss: 0.00991223
Iteration 19/25 | Loss: 0.00991223
Iteration 20/25 | Loss: 0.00991223
Iteration 21/25 | Loss: 0.00991222
Iteration 22/25 | Loss: 0.00991222
Iteration 23/25 | Loss: 0.00991222
Iteration 24/25 | Loss: 0.00991221
Iteration 25/25 | Loss: 0.00991221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48152351
Iteration 2/25 | Loss: 0.13508438
Iteration 3/25 | Loss: 0.13503410
Iteration 4/25 | Loss: 0.13503408
Iteration 5/25 | Loss: 0.13503408
Iteration 6/25 | Loss: 0.13503407
Iteration 7/25 | Loss: 0.13503407
Iteration 8/25 | Loss: 0.13503407
Iteration 9/25 | Loss: 0.13503407
Iteration 10/25 | Loss: 0.13507073
Iteration 11/25 | Loss: 0.13507073
Iteration 12/25 | Loss: 0.13503407
Iteration 13/25 | Loss: 0.13503407
Iteration 14/25 | Loss: 0.13503407
Iteration 15/25 | Loss: 0.13503407
Iteration 16/25 | Loss: 0.13503407
Iteration 17/25 | Loss: 0.13503407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.13503406941890717, 0.13503406941890717, 0.13503406941890717, 0.13503406941890717, 0.13503406941890717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13503406941890717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13503407
Iteration 2/1000 | Loss: 0.00257707
Iteration 3/1000 | Loss: 0.00068003
Iteration 4/1000 | Loss: 0.00049564
Iteration 5/1000 | Loss: 0.00020658
Iteration 6/1000 | Loss: 0.00013434
Iteration 7/1000 | Loss: 0.00010291
Iteration 8/1000 | Loss: 0.00004436
Iteration 9/1000 | Loss: 0.00008089
Iteration 10/1000 | Loss: 0.00003422
Iteration 11/1000 | Loss: 0.00010406
Iteration 12/1000 | Loss: 0.00002541
Iteration 13/1000 | Loss: 0.00003828
Iteration 14/1000 | Loss: 0.00003804
Iteration 15/1000 | Loss: 0.00007128
Iteration 16/1000 | Loss: 0.00004421
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002041
Iteration 19/1000 | Loss: 0.00003588
Iteration 20/1000 | Loss: 0.00006250
Iteration 21/1000 | Loss: 0.00001776
Iteration 22/1000 | Loss: 0.00003344
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00002440
Iteration 26/1000 | Loss: 0.00003336
Iteration 27/1000 | Loss: 0.00001915
Iteration 28/1000 | Loss: 0.00006814
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00002785
Iteration 31/1000 | Loss: 0.00001742
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001494
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00003221
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001483
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001473
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001471
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001470
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001469
Iteration 114/1000 | Loss: 0.00001469
Iteration 115/1000 | Loss: 0.00001469
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001468
Iteration 119/1000 | Loss: 0.00001468
Iteration 120/1000 | Loss: 0.00001468
Iteration 121/1000 | Loss: 0.00001468
Iteration 122/1000 | Loss: 0.00001468
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001468
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00003379
Iteration 135/1000 | Loss: 0.00001463
Iteration 136/1000 | Loss: 0.00001463
Iteration 137/1000 | Loss: 0.00001463
Iteration 138/1000 | Loss: 0.00001463
Iteration 139/1000 | Loss: 0.00001463
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4618360182794277e-05, 1.4618360182794277e-05, 1.4618360182794277e-05, 1.4618360182794277e-05, 1.4618360182794277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4618360182794277e-05

Optimization complete. Final v2v error: 3.2283401489257812 mm

Highest mean error: 3.424443006515503 mm for frame 80

Lowest mean error: 3.014162302017212 mm for frame 8

Saving results

Total time: 75.35387659072876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987323
Iteration 2/25 | Loss: 0.00987323
Iteration 3/25 | Loss: 0.00351264
Iteration 4/25 | Loss: 0.00252838
Iteration 5/25 | Loss: 0.00224150
Iteration 6/25 | Loss: 0.00213234
Iteration 7/25 | Loss: 0.00213659
Iteration 8/25 | Loss: 0.00213443
Iteration 9/25 | Loss: 0.00203387
Iteration 10/25 | Loss: 0.00192344
Iteration 11/25 | Loss: 0.00189145
Iteration 12/25 | Loss: 0.00186970
Iteration 13/25 | Loss: 0.00186148
Iteration 14/25 | Loss: 0.00186916
Iteration 15/25 | Loss: 0.00185954
Iteration 16/25 | Loss: 0.00185951
Iteration 17/25 | Loss: 0.00184936
Iteration 18/25 | Loss: 0.00185100
Iteration 19/25 | Loss: 0.00184198
Iteration 20/25 | Loss: 0.00183990
Iteration 21/25 | Loss: 0.00183420
Iteration 22/25 | Loss: 0.00183718
Iteration 23/25 | Loss: 0.00183621
Iteration 24/25 | Loss: 0.00183620
Iteration 25/25 | Loss: 0.00183161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30304468
Iteration 2/25 | Loss: 0.00741821
Iteration 3/25 | Loss: 0.00562616
Iteration 4/25 | Loss: 0.00562616
Iteration 5/25 | Loss: 0.00562616
Iteration 6/25 | Loss: 0.00562616
Iteration 7/25 | Loss: 0.00562616
Iteration 8/25 | Loss: 0.00562616
Iteration 9/25 | Loss: 0.00562616
Iteration 10/25 | Loss: 0.00562616
Iteration 11/25 | Loss: 0.00562616
Iteration 12/25 | Loss: 0.00562616
Iteration 13/25 | Loss: 0.00562616
Iteration 14/25 | Loss: 0.00562616
Iteration 15/25 | Loss: 0.00562616
Iteration 16/25 | Loss: 0.00562616
Iteration 17/25 | Loss: 0.00562616
Iteration 18/25 | Loss: 0.00562616
Iteration 19/25 | Loss: 0.00562616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0056261587888002396, 0.0056261587888002396, 0.0056261587888002396, 0.0056261587888002396, 0.0056261587888002396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0056261587888002396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00562616
Iteration 2/1000 | Loss: 0.01007593
Iteration 3/1000 | Loss: 0.00121742
Iteration 4/1000 | Loss: 0.00073239
Iteration 5/1000 | Loss: 0.00114585
Iteration 6/1000 | Loss: 0.00069105
Iteration 7/1000 | Loss: 0.00109769
Iteration 8/1000 | Loss: 0.00122088
Iteration 9/1000 | Loss: 0.00062278
Iteration 10/1000 | Loss: 0.00038888
Iteration 11/1000 | Loss: 0.00033512
Iteration 12/1000 | Loss: 0.00047758
Iteration 13/1000 | Loss: 0.00031944
Iteration 14/1000 | Loss: 0.00036059
Iteration 15/1000 | Loss: 0.00035684
Iteration 16/1000 | Loss: 0.00035714
Iteration 17/1000 | Loss: 0.00036373
Iteration 18/1000 | Loss: 0.00049645
Iteration 19/1000 | Loss: 0.00036402
Iteration 20/1000 | Loss: 0.00053844
Iteration 21/1000 | Loss: 0.00043878
Iteration 22/1000 | Loss: 0.00034091
Iteration 23/1000 | Loss: 0.00030094
Iteration 24/1000 | Loss: 0.00071482
Iteration 25/1000 | Loss: 0.00094333
Iteration 26/1000 | Loss: 0.00577893
Iteration 27/1000 | Loss: 0.01594257
Iteration 28/1000 | Loss: 0.01863903
Iteration 29/1000 | Loss: 0.01531523
Iteration 30/1000 | Loss: 0.00699653
Iteration 31/1000 | Loss: 0.00526782
Iteration 32/1000 | Loss: 0.00350404
Iteration 33/1000 | Loss: 0.00223942
Iteration 34/1000 | Loss: 0.00348977
Iteration 35/1000 | Loss: 0.00162735
Iteration 36/1000 | Loss: 0.00184167
Iteration 37/1000 | Loss: 0.00198385
Iteration 38/1000 | Loss: 0.00129208
Iteration 39/1000 | Loss: 0.00085928
Iteration 40/1000 | Loss: 0.00018511
Iteration 41/1000 | Loss: 0.00302190
Iteration 42/1000 | Loss: 0.00035248
Iteration 43/1000 | Loss: 0.00048609
Iteration 44/1000 | Loss: 0.00074429
Iteration 45/1000 | Loss: 0.00120019
Iteration 46/1000 | Loss: 0.00025802
Iteration 47/1000 | Loss: 0.00019594
Iteration 48/1000 | Loss: 0.00018907
Iteration 49/1000 | Loss: 0.00017550
Iteration 50/1000 | Loss: 0.00093209
Iteration 51/1000 | Loss: 0.00038126
Iteration 52/1000 | Loss: 0.00029234
Iteration 53/1000 | Loss: 0.00021341
Iteration 54/1000 | Loss: 0.00048620
Iteration 55/1000 | Loss: 0.00052616
Iteration 56/1000 | Loss: 0.00019567
Iteration 57/1000 | Loss: 0.00086401
Iteration 58/1000 | Loss: 0.00043986
Iteration 59/1000 | Loss: 0.00037605
Iteration 60/1000 | Loss: 0.00014695
Iteration 61/1000 | Loss: 0.00012082
Iteration 62/1000 | Loss: 0.00053307
Iteration 63/1000 | Loss: 0.00013307
Iteration 64/1000 | Loss: 0.00014683
Iteration 65/1000 | Loss: 0.00010640
Iteration 66/1000 | Loss: 0.00012715
Iteration 67/1000 | Loss: 0.00009671
Iteration 68/1000 | Loss: 0.00018217
Iteration 69/1000 | Loss: 0.00020431
Iteration 70/1000 | Loss: 0.00015500
Iteration 71/1000 | Loss: 0.00049692
Iteration 72/1000 | Loss: 0.00019737
Iteration 73/1000 | Loss: 0.00009522
Iteration 74/1000 | Loss: 0.00032370
Iteration 75/1000 | Loss: 0.00093308
Iteration 76/1000 | Loss: 0.00037928
Iteration 77/1000 | Loss: 0.00020881
Iteration 78/1000 | Loss: 0.00021419
Iteration 79/1000 | Loss: 0.00005357
Iteration 80/1000 | Loss: 0.00004541
Iteration 81/1000 | Loss: 0.00004372
Iteration 82/1000 | Loss: 0.00007658
Iteration 83/1000 | Loss: 0.00003793
Iteration 84/1000 | Loss: 0.00005379
Iteration 85/1000 | Loss: 0.00003621
Iteration 86/1000 | Loss: 0.00005345
Iteration 87/1000 | Loss: 0.00008694
Iteration 88/1000 | Loss: 0.00003002
Iteration 89/1000 | Loss: 0.00004879
Iteration 90/1000 | Loss: 0.00005483
Iteration 91/1000 | Loss: 0.00003127
Iteration 92/1000 | Loss: 0.00022962
Iteration 93/1000 | Loss: 0.00007739
Iteration 94/1000 | Loss: 0.00003383
Iteration 95/1000 | Loss: 0.00006143
Iteration 96/1000 | Loss: 0.00003593
Iteration 97/1000 | Loss: 0.00005567
Iteration 98/1000 | Loss: 0.00003886
Iteration 99/1000 | Loss: 0.00020917
Iteration 100/1000 | Loss: 0.00008971
Iteration 101/1000 | Loss: 0.00002989
Iteration 102/1000 | Loss: 0.00019641
Iteration 103/1000 | Loss: 0.00009230
Iteration 104/1000 | Loss: 0.00004234
Iteration 105/1000 | Loss: 0.00003558
Iteration 106/1000 | Loss: 0.00005919
Iteration 107/1000 | Loss: 0.00002657
Iteration 108/1000 | Loss: 0.00002972
Iteration 109/1000 | Loss: 0.00003300
Iteration 110/1000 | Loss: 0.00003276
Iteration 111/1000 | Loss: 0.00002351
Iteration 112/1000 | Loss: 0.00003400
Iteration 113/1000 | Loss: 0.00002481
Iteration 114/1000 | Loss: 0.00002707
Iteration 115/1000 | Loss: 0.00002468
Iteration 116/1000 | Loss: 0.00002200
Iteration 117/1000 | Loss: 0.00006535
Iteration 118/1000 | Loss: 0.00002198
Iteration 119/1000 | Loss: 0.00006566
Iteration 120/1000 | Loss: 0.00019493
Iteration 121/1000 | Loss: 0.00002472
Iteration 122/1000 | Loss: 0.00002650
Iteration 123/1000 | Loss: 0.00003031
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002241
Iteration 126/1000 | Loss: 0.00005089
Iteration 127/1000 | Loss: 0.00002434
Iteration 128/1000 | Loss: 0.00002502
Iteration 129/1000 | Loss: 0.00002649
Iteration 130/1000 | Loss: 0.00003652
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002104
Iteration 133/1000 | Loss: 0.00002104
Iteration 134/1000 | Loss: 0.00002104
Iteration 135/1000 | Loss: 0.00002756
Iteration 136/1000 | Loss: 0.00003953
Iteration 137/1000 | Loss: 0.00015593
Iteration 138/1000 | Loss: 0.00002235
Iteration 139/1000 | Loss: 0.00002694
Iteration 140/1000 | Loss: 0.00002095
Iteration 141/1000 | Loss: 0.00002095
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002094
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002094
Iteration 146/1000 | Loss: 0.00002094
Iteration 147/1000 | Loss: 0.00002093
Iteration 148/1000 | Loss: 0.00002093
Iteration 149/1000 | Loss: 0.00002093
Iteration 150/1000 | Loss: 0.00002093
Iteration 151/1000 | Loss: 0.00002093
Iteration 152/1000 | Loss: 0.00002093
Iteration 153/1000 | Loss: 0.00002093
Iteration 154/1000 | Loss: 0.00002093
Iteration 155/1000 | Loss: 0.00002093
Iteration 156/1000 | Loss: 0.00002093
Iteration 157/1000 | Loss: 0.00002092
Iteration 158/1000 | Loss: 0.00002092
Iteration 159/1000 | Loss: 0.00002092
Iteration 160/1000 | Loss: 0.00002092
Iteration 161/1000 | Loss: 0.00002092
Iteration 162/1000 | Loss: 0.00002092
Iteration 163/1000 | Loss: 0.00002359
Iteration 164/1000 | Loss: 0.00002358
Iteration 165/1000 | Loss: 0.00003953
Iteration 166/1000 | Loss: 0.00002302
Iteration 167/1000 | Loss: 0.00002741
Iteration 168/1000 | Loss: 0.00002249
Iteration 169/1000 | Loss: 0.00003752
Iteration 170/1000 | Loss: 0.00002118
Iteration 171/1000 | Loss: 0.00002092
Iteration 172/1000 | Loss: 0.00002092
Iteration 173/1000 | Loss: 0.00002092
Iteration 174/1000 | Loss: 0.00002091
Iteration 175/1000 | Loss: 0.00002091
Iteration 176/1000 | Loss: 0.00002091
Iteration 177/1000 | Loss: 0.00002091
Iteration 178/1000 | Loss: 0.00002091
Iteration 179/1000 | Loss: 0.00002091
Iteration 180/1000 | Loss: 0.00002091
Iteration 181/1000 | Loss: 0.00002091
Iteration 182/1000 | Loss: 0.00002091
Iteration 183/1000 | Loss: 0.00002090
Iteration 184/1000 | Loss: 0.00002090
Iteration 185/1000 | Loss: 0.00002090
Iteration 186/1000 | Loss: 0.00002090
Iteration 187/1000 | Loss: 0.00002090
Iteration 188/1000 | Loss: 0.00002090
Iteration 189/1000 | Loss: 0.00002090
Iteration 190/1000 | Loss: 0.00002090
Iteration 191/1000 | Loss: 0.00002090
Iteration 192/1000 | Loss: 0.00002090
Iteration 193/1000 | Loss: 0.00002090
Iteration 194/1000 | Loss: 0.00002089
Iteration 195/1000 | Loss: 0.00002089
Iteration 196/1000 | Loss: 0.00002089
Iteration 197/1000 | Loss: 0.00002089
Iteration 198/1000 | Loss: 0.00002089
Iteration 199/1000 | Loss: 0.00002089
Iteration 200/1000 | Loss: 0.00002089
Iteration 201/1000 | Loss: 0.00002089
Iteration 202/1000 | Loss: 0.00002089
Iteration 203/1000 | Loss: 0.00002089
Iteration 204/1000 | Loss: 0.00002089
Iteration 205/1000 | Loss: 0.00002088
Iteration 206/1000 | Loss: 0.00002088
Iteration 207/1000 | Loss: 0.00002088
Iteration 208/1000 | Loss: 0.00002088
Iteration 209/1000 | Loss: 0.00002088
Iteration 210/1000 | Loss: 0.00002088
Iteration 211/1000 | Loss: 0.00002088
Iteration 212/1000 | Loss: 0.00002088
Iteration 213/1000 | Loss: 0.00002124
Iteration 214/1000 | Loss: 0.00002106
Iteration 215/1000 | Loss: 0.00002088
Iteration 216/1000 | Loss: 0.00002088
Iteration 217/1000 | Loss: 0.00002088
Iteration 218/1000 | Loss: 0.00002365
Iteration 219/1000 | Loss: 0.00002089
Iteration 220/1000 | Loss: 0.00002089
Iteration 221/1000 | Loss: 0.00002088
Iteration 222/1000 | Loss: 0.00002088
Iteration 223/1000 | Loss: 0.00002088
Iteration 224/1000 | Loss: 0.00002105
Iteration 225/1000 | Loss: 0.00002103
Iteration 226/1000 | Loss: 0.00002087
Iteration 227/1000 | Loss: 0.00002087
Iteration 228/1000 | Loss: 0.00002087
Iteration 229/1000 | Loss: 0.00002092
Iteration 230/1000 | Loss: 0.00002091
Iteration 231/1000 | Loss: 0.00002091
Iteration 232/1000 | Loss: 0.00002098
Iteration 233/1000 | Loss: 0.00002334
Iteration 234/1000 | Loss: 0.00002334
Iteration 235/1000 | Loss: 0.00002334
Iteration 236/1000 | Loss: 0.00003664
Iteration 237/1000 | Loss: 0.00002115
Iteration 238/1000 | Loss: 0.00002087
Iteration 239/1000 | Loss: 0.00002091
Iteration 240/1000 | Loss: 0.00002090
Iteration 241/1000 | Loss: 0.00002086
Iteration 242/1000 | Loss: 0.00002086
Iteration 243/1000 | Loss: 0.00002086
Iteration 244/1000 | Loss: 0.00002086
Iteration 245/1000 | Loss: 0.00002085
Iteration 246/1000 | Loss: 0.00002085
Iteration 247/1000 | Loss: 0.00002085
Iteration 248/1000 | Loss: 0.00002085
Iteration 249/1000 | Loss: 0.00002085
Iteration 250/1000 | Loss: 0.00002143
Iteration 251/1000 | Loss: 0.00002085
Iteration 252/1000 | Loss: 0.00002084
Iteration 253/1000 | Loss: 0.00002084
Iteration 254/1000 | Loss: 0.00002084
Iteration 255/1000 | Loss: 0.00002084
Iteration 256/1000 | Loss: 0.00002084
Iteration 257/1000 | Loss: 0.00002084
Iteration 258/1000 | Loss: 0.00002084
Iteration 259/1000 | Loss: 0.00002084
Iteration 260/1000 | Loss: 0.00002084
Iteration 261/1000 | Loss: 0.00002084
Iteration 262/1000 | Loss: 0.00002084
Iteration 263/1000 | Loss: 0.00002084
Iteration 264/1000 | Loss: 0.00002084
Iteration 265/1000 | Loss: 0.00002084
Iteration 266/1000 | Loss: 0.00002084
Iteration 267/1000 | Loss: 0.00002084
Iteration 268/1000 | Loss: 0.00002084
Iteration 269/1000 | Loss: 0.00002084
Iteration 270/1000 | Loss: 0.00002084
Iteration 271/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [2.0839857825194485e-05, 2.0839857825194485e-05, 2.0839857825194485e-05, 2.0839857825194485e-05, 2.0839857825194485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0839857825194485e-05

Optimization complete. Final v2v error: 3.6782772541046143 mm

Highest mean error: 11.82425594329834 mm for frame 58

Lowest mean error: 3.1571147441864014 mm for frame 44

Saving results

Total time: 284.6410799026489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774210
Iteration 2/25 | Loss: 0.00138429
Iteration 3/25 | Loss: 0.00124914
Iteration 4/25 | Loss: 0.00123771
Iteration 5/25 | Loss: 0.00123554
Iteration 6/25 | Loss: 0.00123554
Iteration 7/25 | Loss: 0.00123554
Iteration 8/25 | Loss: 0.00123554
Iteration 9/25 | Loss: 0.00123554
Iteration 10/25 | Loss: 0.00123554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012355438666418195, 0.0012355438666418195, 0.0012355438666418195, 0.0012355438666418195, 0.0012355438666418195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012355438666418195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34999216
Iteration 2/25 | Loss: 0.00097227
Iteration 3/25 | Loss: 0.00097227
Iteration 4/25 | Loss: 0.00097226
Iteration 5/25 | Loss: 0.00097226
Iteration 6/25 | Loss: 0.00097226
Iteration 7/25 | Loss: 0.00097226
Iteration 8/25 | Loss: 0.00097226
Iteration 9/25 | Loss: 0.00097226
Iteration 10/25 | Loss: 0.00097226
Iteration 11/25 | Loss: 0.00097226
Iteration 12/25 | Loss: 0.00097226
Iteration 13/25 | Loss: 0.00097226
Iteration 14/25 | Loss: 0.00097226
Iteration 15/25 | Loss: 0.00097226
Iteration 16/25 | Loss: 0.00097226
Iteration 17/25 | Loss: 0.00097226
Iteration 18/25 | Loss: 0.00097226
Iteration 19/25 | Loss: 0.00097226
Iteration 20/25 | Loss: 0.00097226
Iteration 21/25 | Loss: 0.00097226
Iteration 22/25 | Loss: 0.00097226
Iteration 23/25 | Loss: 0.00097226
Iteration 24/25 | Loss: 0.00097226
Iteration 25/25 | Loss: 0.00097226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097226
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00001814
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001453
Iteration 7/1000 | Loss: 0.00001410
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001299
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001297
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001296
Iteration 37/1000 | Loss: 0.00001295
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001287
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001285
Iteration 55/1000 | Loss: 0.00001285
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001284
Iteration 62/1000 | Loss: 0.00001284
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.278085710509913e-05, 1.278085710509913e-05, 1.278085710509913e-05, 1.278085710509913e-05, 1.278085710509913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.278085710509913e-05

Optimization complete. Final v2v error: 3.050116539001465 mm

Highest mean error: 3.2168025970458984 mm for frame 63

Lowest mean error: 2.8055224418640137 mm for frame 235

Saving results

Total time: 36.584611892700195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827509
Iteration 2/25 | Loss: 0.00141400
Iteration 3/25 | Loss: 0.00132642
Iteration 4/25 | Loss: 0.00131818
Iteration 5/25 | Loss: 0.00131547
Iteration 6/25 | Loss: 0.00131496
Iteration 7/25 | Loss: 0.00131496
Iteration 8/25 | Loss: 0.00131496
Iteration 9/25 | Loss: 0.00131496
Iteration 10/25 | Loss: 0.00131496
Iteration 11/25 | Loss: 0.00131496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00131496402900666, 0.00131496402900666, 0.00131496402900666, 0.00131496402900666, 0.00131496402900666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00131496402900666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33090091
Iteration 2/25 | Loss: 0.00113209
Iteration 3/25 | Loss: 0.00113209
Iteration 4/25 | Loss: 0.00113209
Iteration 5/25 | Loss: 0.00113209
Iteration 6/25 | Loss: 0.00113209
Iteration 7/25 | Loss: 0.00113209
Iteration 8/25 | Loss: 0.00113209
Iteration 9/25 | Loss: 0.00113209
Iteration 10/25 | Loss: 0.00113209
Iteration 11/25 | Loss: 0.00113209
Iteration 12/25 | Loss: 0.00113209
Iteration 13/25 | Loss: 0.00113209
Iteration 14/25 | Loss: 0.00113209
Iteration 15/25 | Loss: 0.00113209
Iteration 16/25 | Loss: 0.00113209
Iteration 17/25 | Loss: 0.00113209
Iteration 18/25 | Loss: 0.00113209
Iteration 19/25 | Loss: 0.00113209
Iteration 20/25 | Loss: 0.00113209
Iteration 21/25 | Loss: 0.00113209
Iteration 22/25 | Loss: 0.00113209
Iteration 23/25 | Loss: 0.00113209
Iteration 24/25 | Loss: 0.00113209
Iteration 25/25 | Loss: 0.00113209
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011320901103317738, 0.0011320901103317738, 0.0011320901103317738, 0.0011320901103317738, 0.0011320901103317738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011320901103317738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113209
Iteration 2/1000 | Loss: 0.00003704
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002338
Iteration 6/1000 | Loss: 0.00002259
Iteration 7/1000 | Loss: 0.00002198
Iteration 8/1000 | Loss: 0.00002158
Iteration 9/1000 | Loss: 0.00002125
Iteration 10/1000 | Loss: 0.00002093
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002054
Iteration 13/1000 | Loss: 0.00002053
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002045
Iteration 17/1000 | Loss: 0.00002040
Iteration 18/1000 | Loss: 0.00002040
Iteration 19/1000 | Loss: 0.00002039
Iteration 20/1000 | Loss: 0.00002039
Iteration 21/1000 | Loss: 0.00002038
Iteration 22/1000 | Loss: 0.00002037
Iteration 23/1000 | Loss: 0.00002037
Iteration 24/1000 | Loss: 0.00002036
Iteration 25/1000 | Loss: 0.00002036
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002034
Iteration 28/1000 | Loss: 0.00002033
Iteration 29/1000 | Loss: 0.00002029
Iteration 30/1000 | Loss: 0.00002026
Iteration 31/1000 | Loss: 0.00002026
Iteration 32/1000 | Loss: 0.00002025
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002022
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002021
Iteration 40/1000 | Loss: 0.00002021
Iteration 41/1000 | Loss: 0.00002020
Iteration 42/1000 | Loss: 0.00002020
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002019
Iteration 46/1000 | Loss: 0.00002019
Iteration 47/1000 | Loss: 0.00002019
Iteration 48/1000 | Loss: 0.00002019
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00002019
Iteration 53/1000 | Loss: 0.00002018
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002017
Iteration 56/1000 | Loss: 0.00002017
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00002015
Iteration 59/1000 | Loss: 0.00002015
Iteration 60/1000 | Loss: 0.00002015
Iteration 61/1000 | Loss: 0.00002015
Iteration 62/1000 | Loss: 0.00002015
Iteration 63/1000 | Loss: 0.00002015
Iteration 64/1000 | Loss: 0.00002015
Iteration 65/1000 | Loss: 0.00002015
Iteration 66/1000 | Loss: 0.00002015
Iteration 67/1000 | Loss: 0.00002015
Iteration 68/1000 | Loss: 0.00002015
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002014
Iteration 71/1000 | Loss: 0.00002014
Iteration 72/1000 | Loss: 0.00002014
Iteration 73/1000 | Loss: 0.00002012
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002011
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002009
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002007
Iteration 82/1000 | Loss: 0.00002007
Iteration 83/1000 | Loss: 0.00002006
Iteration 84/1000 | Loss: 0.00002006
Iteration 85/1000 | Loss: 0.00002006
Iteration 86/1000 | Loss: 0.00002006
Iteration 87/1000 | Loss: 0.00002006
Iteration 88/1000 | Loss: 0.00002006
Iteration 89/1000 | Loss: 0.00002006
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002005
Iteration 92/1000 | Loss: 0.00002005
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002003
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002001
Iteration 98/1000 | Loss: 0.00002001
Iteration 99/1000 | Loss: 0.00002001
Iteration 100/1000 | Loss: 0.00002000
Iteration 101/1000 | Loss: 0.00002000
Iteration 102/1000 | Loss: 0.00002000
Iteration 103/1000 | Loss: 0.00001999
Iteration 104/1000 | Loss: 0.00001999
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001997
Iteration 110/1000 | Loss: 0.00001997
Iteration 111/1000 | Loss: 0.00001997
Iteration 112/1000 | Loss: 0.00001997
Iteration 113/1000 | Loss: 0.00001997
Iteration 114/1000 | Loss: 0.00001996
Iteration 115/1000 | Loss: 0.00001996
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001996
Iteration 118/1000 | Loss: 0.00001996
Iteration 119/1000 | Loss: 0.00001996
Iteration 120/1000 | Loss: 0.00001996
Iteration 121/1000 | Loss: 0.00001996
Iteration 122/1000 | Loss: 0.00001995
Iteration 123/1000 | Loss: 0.00001995
Iteration 124/1000 | Loss: 0.00001995
Iteration 125/1000 | Loss: 0.00001995
Iteration 126/1000 | Loss: 0.00001995
Iteration 127/1000 | Loss: 0.00001995
Iteration 128/1000 | Loss: 0.00001995
Iteration 129/1000 | Loss: 0.00001994
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001994
Iteration 133/1000 | Loss: 0.00001994
Iteration 134/1000 | Loss: 0.00001993
Iteration 135/1000 | Loss: 0.00001993
Iteration 136/1000 | Loss: 0.00001993
Iteration 137/1000 | Loss: 0.00001993
Iteration 138/1000 | Loss: 0.00001993
Iteration 139/1000 | Loss: 0.00001993
Iteration 140/1000 | Loss: 0.00001993
Iteration 141/1000 | Loss: 0.00001992
Iteration 142/1000 | Loss: 0.00001992
Iteration 143/1000 | Loss: 0.00001992
Iteration 144/1000 | Loss: 0.00001992
Iteration 145/1000 | Loss: 0.00001991
Iteration 146/1000 | Loss: 0.00001991
Iteration 147/1000 | Loss: 0.00001991
Iteration 148/1000 | Loss: 0.00001991
Iteration 149/1000 | Loss: 0.00001991
Iteration 150/1000 | Loss: 0.00001991
Iteration 151/1000 | Loss: 0.00001991
Iteration 152/1000 | Loss: 0.00001991
Iteration 153/1000 | Loss: 0.00001991
Iteration 154/1000 | Loss: 0.00001991
Iteration 155/1000 | Loss: 0.00001991
Iteration 156/1000 | Loss: 0.00001991
Iteration 157/1000 | Loss: 0.00001991
Iteration 158/1000 | Loss: 0.00001991
Iteration 159/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.991080898733344e-05, 1.991080898733344e-05, 1.991080898733344e-05, 1.991080898733344e-05, 1.991080898733344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.991080898733344e-05

Optimization complete. Final v2v error: 3.748516321182251 mm

Highest mean error: 3.9735958576202393 mm for frame 84

Lowest mean error: 3.3967344760894775 mm for frame 20

Saving results

Total time: 35.935296058654785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398411
Iteration 2/25 | Loss: 0.00134665
Iteration 3/25 | Loss: 0.00124586
Iteration 4/25 | Loss: 0.00123662
Iteration 5/25 | Loss: 0.00123256
Iteration 6/25 | Loss: 0.00123078
Iteration 7/25 | Loss: 0.00123078
Iteration 8/25 | Loss: 0.00123078
Iteration 9/25 | Loss: 0.00123078
Iteration 10/25 | Loss: 0.00123078
Iteration 11/25 | Loss: 0.00123078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012307795695960522, 0.0012307795695960522, 0.0012307795695960522, 0.0012307795695960522, 0.0012307795695960522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012307795695960522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59871614
Iteration 2/25 | Loss: 0.00087664
Iteration 3/25 | Loss: 0.00087664
Iteration 4/25 | Loss: 0.00087664
Iteration 5/25 | Loss: 0.00087664
Iteration 6/25 | Loss: 0.00087664
Iteration 7/25 | Loss: 0.00087664
Iteration 8/25 | Loss: 0.00087664
Iteration 9/25 | Loss: 0.00087664
Iteration 10/25 | Loss: 0.00087664
Iteration 11/25 | Loss: 0.00087664
Iteration 12/25 | Loss: 0.00087664
Iteration 13/25 | Loss: 0.00087664
Iteration 14/25 | Loss: 0.00087664
Iteration 15/25 | Loss: 0.00087664
Iteration 16/25 | Loss: 0.00087664
Iteration 17/25 | Loss: 0.00087664
Iteration 18/25 | Loss: 0.00087664
Iteration 19/25 | Loss: 0.00087664
Iteration 20/25 | Loss: 0.00087664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008766387472860515, 0.0008766387472860515, 0.0008766387472860515, 0.0008766387472860515, 0.0008766387472860515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008766387472860515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087664
Iteration 2/1000 | Loss: 0.00003299
Iteration 3/1000 | Loss: 0.00002252
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001394
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001154
Iteration 12/1000 | Loss: 0.00001143
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001131
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001128
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001125
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001121
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001105
Iteration 69/1000 | Loss: 0.00001105
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001104
Iteration 72/1000 | Loss: 0.00001104
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001101
Iteration 93/1000 | Loss: 0.00001101
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001101
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001100
Iteration 107/1000 | Loss: 0.00001100
Iteration 108/1000 | Loss: 0.00001100
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001100
Iteration 111/1000 | Loss: 0.00001100
Iteration 112/1000 | Loss: 0.00001100
Iteration 113/1000 | Loss: 0.00001100
Iteration 114/1000 | Loss: 0.00001100
Iteration 115/1000 | Loss: 0.00001100
Iteration 116/1000 | Loss: 0.00001100
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001100
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001099
Iteration 129/1000 | Loss: 0.00001099
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001099
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001098
Iteration 138/1000 | Loss: 0.00001098
Iteration 139/1000 | Loss: 0.00001098
Iteration 140/1000 | Loss: 0.00001097
Iteration 141/1000 | Loss: 0.00001097
Iteration 142/1000 | Loss: 0.00001097
Iteration 143/1000 | Loss: 0.00001097
Iteration 144/1000 | Loss: 0.00001097
Iteration 145/1000 | Loss: 0.00001097
Iteration 146/1000 | Loss: 0.00001097
Iteration 147/1000 | Loss: 0.00001097
Iteration 148/1000 | Loss: 0.00001097
Iteration 149/1000 | Loss: 0.00001097
Iteration 150/1000 | Loss: 0.00001097
Iteration 151/1000 | Loss: 0.00001097
Iteration 152/1000 | Loss: 0.00001096
Iteration 153/1000 | Loss: 0.00001096
Iteration 154/1000 | Loss: 0.00001096
Iteration 155/1000 | Loss: 0.00001096
Iteration 156/1000 | Loss: 0.00001096
Iteration 157/1000 | Loss: 0.00001096
Iteration 158/1000 | Loss: 0.00001096
Iteration 159/1000 | Loss: 0.00001096
Iteration 160/1000 | Loss: 0.00001096
Iteration 161/1000 | Loss: 0.00001096
Iteration 162/1000 | Loss: 0.00001096
Iteration 163/1000 | Loss: 0.00001096
Iteration 164/1000 | Loss: 0.00001096
Iteration 165/1000 | Loss: 0.00001096
Iteration 166/1000 | Loss: 0.00001096
Iteration 167/1000 | Loss: 0.00001096
Iteration 168/1000 | Loss: 0.00001096
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001096
Iteration 172/1000 | Loss: 0.00001096
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.095755669666687e-05, 1.095755669666687e-05, 1.095755669666687e-05, 1.095755669666687e-05, 1.095755669666687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.095755669666687e-05

Optimization complete. Final v2v error: 2.8459312915802 mm

Highest mean error: 3.149514675140381 mm for frame 109

Lowest mean error: 2.6333134174346924 mm for frame 72

Saving results

Total time: 35.53666543960571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401886
Iteration 2/25 | Loss: 0.00134749
Iteration 3/25 | Loss: 0.00126598
Iteration 4/25 | Loss: 0.00125572
Iteration 5/25 | Loss: 0.00125332
Iteration 6/25 | Loss: 0.00125269
Iteration 7/25 | Loss: 0.00125265
Iteration 8/25 | Loss: 0.00125265
Iteration 9/25 | Loss: 0.00125265
Iteration 10/25 | Loss: 0.00125265
Iteration 11/25 | Loss: 0.00125265
Iteration 12/25 | Loss: 0.00125265
Iteration 13/25 | Loss: 0.00125265
Iteration 14/25 | Loss: 0.00125265
Iteration 15/25 | Loss: 0.00125265
Iteration 16/25 | Loss: 0.00125265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00125264679081738, 0.00125264679081738, 0.00125264679081738, 0.00125264679081738, 0.00125264679081738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00125264679081738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36398923
Iteration 2/25 | Loss: 0.00099208
Iteration 3/25 | Loss: 0.00099207
Iteration 4/25 | Loss: 0.00099207
Iteration 5/25 | Loss: 0.00099207
Iteration 6/25 | Loss: 0.00099207
Iteration 7/25 | Loss: 0.00099207
Iteration 8/25 | Loss: 0.00099207
Iteration 9/25 | Loss: 0.00099207
Iteration 10/25 | Loss: 0.00099207
Iteration 11/25 | Loss: 0.00099207
Iteration 12/25 | Loss: 0.00099207
Iteration 13/25 | Loss: 0.00099207
Iteration 14/25 | Loss: 0.00099207
Iteration 15/25 | Loss: 0.00099207
Iteration 16/25 | Loss: 0.00099207
Iteration 17/25 | Loss: 0.00099207
Iteration 18/25 | Loss: 0.00099207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009920704178512096, 0.0009920704178512096, 0.0009920704178512096, 0.0009920704178512096, 0.0009920704178512096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009920704178512096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099207
Iteration 2/1000 | Loss: 0.00003275
Iteration 3/1000 | Loss: 0.00002168
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001689
Iteration 9/1000 | Loss: 0.00001660
Iteration 10/1000 | Loss: 0.00001637
Iteration 11/1000 | Loss: 0.00001623
Iteration 12/1000 | Loss: 0.00001622
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001591
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001578
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001573
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001559
Iteration 47/1000 | Loss: 0.00001559
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001558
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001555
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001547
Iteration 78/1000 | Loss: 0.00001547
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001547
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001544
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001539
Iteration 103/1000 | Loss: 0.00001539
Iteration 104/1000 | Loss: 0.00001539
Iteration 105/1000 | Loss: 0.00001539
Iteration 106/1000 | Loss: 0.00001539
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001538
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001537
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001537
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001536
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001536
Iteration 129/1000 | Loss: 0.00001536
Iteration 130/1000 | Loss: 0.00001536
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001535
Iteration 133/1000 | Loss: 0.00001535
Iteration 134/1000 | Loss: 0.00001535
Iteration 135/1000 | Loss: 0.00001535
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001534
Iteration 139/1000 | Loss: 0.00001534
Iteration 140/1000 | Loss: 0.00001534
Iteration 141/1000 | Loss: 0.00001534
Iteration 142/1000 | Loss: 0.00001534
Iteration 143/1000 | Loss: 0.00001534
Iteration 144/1000 | Loss: 0.00001534
Iteration 145/1000 | Loss: 0.00001534
Iteration 146/1000 | Loss: 0.00001534
Iteration 147/1000 | Loss: 0.00001534
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001533
Iteration 150/1000 | Loss: 0.00001533
Iteration 151/1000 | Loss: 0.00001533
Iteration 152/1000 | Loss: 0.00001533
Iteration 153/1000 | Loss: 0.00001533
Iteration 154/1000 | Loss: 0.00001533
Iteration 155/1000 | Loss: 0.00001533
Iteration 156/1000 | Loss: 0.00001533
Iteration 157/1000 | Loss: 0.00001533
Iteration 158/1000 | Loss: 0.00001533
Iteration 159/1000 | Loss: 0.00001533
Iteration 160/1000 | Loss: 0.00001533
Iteration 161/1000 | Loss: 0.00001533
Iteration 162/1000 | Loss: 0.00001533
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001532
Iteration 165/1000 | Loss: 0.00001532
Iteration 166/1000 | Loss: 0.00001532
Iteration 167/1000 | Loss: 0.00001532
Iteration 168/1000 | Loss: 0.00001532
Iteration 169/1000 | Loss: 0.00001532
Iteration 170/1000 | Loss: 0.00001532
Iteration 171/1000 | Loss: 0.00001532
Iteration 172/1000 | Loss: 0.00001532
Iteration 173/1000 | Loss: 0.00001532
Iteration 174/1000 | Loss: 0.00001532
Iteration 175/1000 | Loss: 0.00001532
Iteration 176/1000 | Loss: 0.00001532
Iteration 177/1000 | Loss: 0.00001531
Iteration 178/1000 | Loss: 0.00001531
Iteration 179/1000 | Loss: 0.00001531
Iteration 180/1000 | Loss: 0.00001531
Iteration 181/1000 | Loss: 0.00001531
Iteration 182/1000 | Loss: 0.00001531
Iteration 183/1000 | Loss: 0.00001531
Iteration 184/1000 | Loss: 0.00001531
Iteration 185/1000 | Loss: 0.00001531
Iteration 186/1000 | Loss: 0.00001531
Iteration 187/1000 | Loss: 0.00001531
Iteration 188/1000 | Loss: 0.00001531
Iteration 189/1000 | Loss: 0.00001530
Iteration 190/1000 | Loss: 0.00001530
Iteration 191/1000 | Loss: 0.00001530
Iteration 192/1000 | Loss: 0.00001530
Iteration 193/1000 | Loss: 0.00001530
Iteration 194/1000 | Loss: 0.00001530
Iteration 195/1000 | Loss: 0.00001530
Iteration 196/1000 | Loss: 0.00001530
Iteration 197/1000 | Loss: 0.00001530
Iteration 198/1000 | Loss: 0.00001530
Iteration 199/1000 | Loss: 0.00001530
Iteration 200/1000 | Loss: 0.00001530
Iteration 201/1000 | Loss: 0.00001530
Iteration 202/1000 | Loss: 0.00001530
Iteration 203/1000 | Loss: 0.00001529
Iteration 204/1000 | Loss: 0.00001529
Iteration 205/1000 | Loss: 0.00001529
Iteration 206/1000 | Loss: 0.00001529
Iteration 207/1000 | Loss: 0.00001529
Iteration 208/1000 | Loss: 0.00001529
Iteration 209/1000 | Loss: 0.00001529
Iteration 210/1000 | Loss: 0.00001529
Iteration 211/1000 | Loss: 0.00001529
Iteration 212/1000 | Loss: 0.00001529
Iteration 213/1000 | Loss: 0.00001529
Iteration 214/1000 | Loss: 0.00001529
Iteration 215/1000 | Loss: 0.00001529
Iteration 216/1000 | Loss: 0.00001529
Iteration 217/1000 | Loss: 0.00001529
Iteration 218/1000 | Loss: 0.00001529
Iteration 219/1000 | Loss: 0.00001529
Iteration 220/1000 | Loss: 0.00001529
Iteration 221/1000 | Loss: 0.00001529
Iteration 222/1000 | Loss: 0.00001529
Iteration 223/1000 | Loss: 0.00001529
Iteration 224/1000 | Loss: 0.00001529
Iteration 225/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.528601933387108e-05, 1.528601933387108e-05, 1.528601933387108e-05, 1.528601933387108e-05, 1.528601933387108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.528601933387108e-05

Optimization complete. Final v2v error: 3.2627296447753906 mm

Highest mean error: 3.491152048110962 mm for frame 21

Lowest mean error: 2.918891668319702 mm for frame 65

Saving results

Total time: 40.87814402580261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992576
Iteration 2/25 | Loss: 0.00181894
Iteration 3/25 | Loss: 0.00140253
Iteration 4/25 | Loss: 0.00134985
Iteration 5/25 | Loss: 0.00134173
Iteration 6/25 | Loss: 0.00134457
Iteration 7/25 | Loss: 0.00133111
Iteration 8/25 | Loss: 0.00131845
Iteration 9/25 | Loss: 0.00131462
Iteration 10/25 | Loss: 0.00131405
Iteration 11/25 | Loss: 0.00131390
Iteration 12/25 | Loss: 0.00131383
Iteration 13/25 | Loss: 0.00131383
Iteration 14/25 | Loss: 0.00131383
Iteration 15/25 | Loss: 0.00131383
Iteration 16/25 | Loss: 0.00131383
Iteration 17/25 | Loss: 0.00131383
Iteration 18/25 | Loss: 0.00131383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013138267677277327, 0.0013138267677277327, 0.0013138267677277327, 0.0013138267677277327, 0.0013138267677277327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013138267677277327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36511576
Iteration 2/25 | Loss: 0.00105024
Iteration 3/25 | Loss: 0.00105024
Iteration 4/25 | Loss: 0.00105024
Iteration 5/25 | Loss: 0.00105024
Iteration 6/25 | Loss: 0.00105024
Iteration 7/25 | Loss: 0.00105024
Iteration 8/25 | Loss: 0.00105024
Iteration 9/25 | Loss: 0.00105024
Iteration 10/25 | Loss: 0.00105024
Iteration 11/25 | Loss: 0.00105024
Iteration 12/25 | Loss: 0.00105024
Iteration 13/25 | Loss: 0.00105024
Iteration 14/25 | Loss: 0.00105024
Iteration 15/25 | Loss: 0.00105024
Iteration 16/25 | Loss: 0.00105024
Iteration 17/25 | Loss: 0.00105024
Iteration 18/25 | Loss: 0.00105024
Iteration 19/25 | Loss: 0.00105024
Iteration 20/25 | Loss: 0.00105024
Iteration 21/25 | Loss: 0.00105024
Iteration 22/25 | Loss: 0.00105024
Iteration 23/25 | Loss: 0.00105024
Iteration 24/25 | Loss: 0.00105024
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010502396617084742, 0.0010502396617084742, 0.0010502396617084742, 0.0010502396617084742, 0.0010502396617084742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010502396617084742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105024
Iteration 2/1000 | Loss: 0.00003215
Iteration 3/1000 | Loss: 0.00002220
Iteration 4/1000 | Loss: 0.00002028
Iteration 5/1000 | Loss: 0.00001955
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001767
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001744
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001735
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001735
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.7045931599568576e-05, 1.7045931599568576e-05, 1.7045931599568576e-05, 1.7045931599568576e-05, 1.7045931599568576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7045931599568576e-05

Optimization complete. Final v2v error: 3.5782837867736816 mm

Highest mean error: 3.6390767097473145 mm for frame 54

Lowest mean error: 3.533703565597534 mm for frame 1

Saving results

Total time: 43.43400526046753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034862
Iteration 2/25 | Loss: 0.01034862
Iteration 3/25 | Loss: 0.00261782
Iteration 4/25 | Loss: 0.00182327
Iteration 5/25 | Loss: 0.00169448
Iteration 6/25 | Loss: 0.00168123
Iteration 7/25 | Loss: 0.00159063
Iteration 8/25 | Loss: 0.00150776
Iteration 9/25 | Loss: 0.00136470
Iteration 10/25 | Loss: 0.00133373
Iteration 11/25 | Loss: 0.00128416
Iteration 12/25 | Loss: 0.00126687
Iteration 13/25 | Loss: 0.00124165
Iteration 14/25 | Loss: 0.00124413
Iteration 15/25 | Loss: 0.00123992
Iteration 16/25 | Loss: 0.00124281
Iteration 17/25 | Loss: 0.00123955
Iteration 18/25 | Loss: 0.00123955
Iteration 19/25 | Loss: 0.00123950
Iteration 20/25 | Loss: 0.00123949
Iteration 21/25 | Loss: 0.00123949
Iteration 22/25 | Loss: 0.00123949
Iteration 23/25 | Loss: 0.00123949
Iteration 24/25 | Loss: 0.00123949
Iteration 25/25 | Loss: 0.00123949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30201948
Iteration 2/25 | Loss: 0.00125823
Iteration 3/25 | Loss: 0.00097638
Iteration 4/25 | Loss: 0.00097637
Iteration 5/25 | Loss: 0.00097637
Iteration 6/25 | Loss: 0.00097637
Iteration 7/25 | Loss: 0.00097637
Iteration 8/25 | Loss: 0.00097637
Iteration 9/25 | Loss: 0.00097637
Iteration 10/25 | Loss: 0.00097637
Iteration 11/25 | Loss: 0.00097637
Iteration 12/25 | Loss: 0.00097637
Iteration 13/25 | Loss: 0.00097637
Iteration 14/25 | Loss: 0.00097637
Iteration 15/25 | Loss: 0.00097637
Iteration 16/25 | Loss: 0.00097637
Iteration 17/25 | Loss: 0.00097637
Iteration 18/25 | Loss: 0.00097637
Iteration 19/25 | Loss: 0.00097637
Iteration 20/25 | Loss: 0.00097637
Iteration 21/25 | Loss: 0.00097637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009763715206645429, 0.0009763715206645429, 0.0009763715206645429, 0.0009763715206645429, 0.0009763715206645429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009763715206645429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097637
Iteration 2/1000 | Loss: 0.00004709
Iteration 3/1000 | Loss: 0.00003623
Iteration 4/1000 | Loss: 0.00003153
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002792
Iteration 7/1000 | Loss: 0.00002705
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002571
Iteration 10/1000 | Loss: 0.00002510
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002422
Iteration 13/1000 | Loss: 0.00002397
Iteration 14/1000 | Loss: 0.00002374
Iteration 15/1000 | Loss: 0.00002355
Iteration 16/1000 | Loss: 0.00002351
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002330
Iteration 19/1000 | Loss: 0.00002322
Iteration 20/1000 | Loss: 0.00002315
Iteration 21/1000 | Loss: 0.00002314
Iteration 22/1000 | Loss: 0.00002314
Iteration 23/1000 | Loss: 0.00002313
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00005676
Iteration 27/1000 | Loss: 0.00006108
Iteration 28/1000 | Loss: 0.00004085
Iteration 29/1000 | Loss: 0.00117989
Iteration 30/1000 | Loss: 0.00274153
Iteration 31/1000 | Loss: 0.00118416
Iteration 32/1000 | Loss: 0.00004306
Iteration 33/1000 | Loss: 0.00003192
Iteration 34/1000 | Loss: 0.00002215
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00010050
Iteration 37/1000 | Loss: 0.00008983
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00004597
Iteration 42/1000 | Loss: 0.00004603
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001086
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001021
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001018
Iteration 50/1000 | Loss: 0.00001017
Iteration 51/1000 | Loss: 0.00001017
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001015
Iteration 54/1000 | Loss: 0.00001015
Iteration 55/1000 | Loss: 0.00001015
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00000996
Iteration 60/1000 | Loss: 0.00000995
Iteration 61/1000 | Loss: 0.00000995
Iteration 62/1000 | Loss: 0.00000995
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000990
Iteration 73/1000 | Loss: 0.00000990
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000988
Iteration 80/1000 | Loss: 0.00000988
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000987
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000986
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000985
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000984
Iteration 110/1000 | Loss: 0.00000984
Iteration 111/1000 | Loss: 0.00000984
Iteration 112/1000 | Loss: 0.00000984
Iteration 113/1000 | Loss: 0.00000984
Iteration 114/1000 | Loss: 0.00000984
Iteration 115/1000 | Loss: 0.00000984
Iteration 116/1000 | Loss: 0.00000984
Iteration 117/1000 | Loss: 0.00000984
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000984
Iteration 121/1000 | Loss: 0.00000984
Iteration 122/1000 | Loss: 0.00000984
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000984
Iteration 126/1000 | Loss: 0.00000984
Iteration 127/1000 | Loss: 0.00000984
Iteration 128/1000 | Loss: 0.00000984
Iteration 129/1000 | Loss: 0.00000984
Iteration 130/1000 | Loss: 0.00000984
Iteration 131/1000 | Loss: 0.00000984
Iteration 132/1000 | Loss: 0.00000984
Iteration 133/1000 | Loss: 0.00000984
Iteration 134/1000 | Loss: 0.00000984
Iteration 135/1000 | Loss: 0.00000984
Iteration 136/1000 | Loss: 0.00000984
Iteration 137/1000 | Loss: 0.00000984
Iteration 138/1000 | Loss: 0.00000984
Iteration 139/1000 | Loss: 0.00000984
Iteration 140/1000 | Loss: 0.00000984
Iteration 141/1000 | Loss: 0.00000984
Iteration 142/1000 | Loss: 0.00000984
Iteration 143/1000 | Loss: 0.00000984
Iteration 144/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [9.83695008471841e-06, 9.83695008471841e-06, 9.83695008471841e-06, 9.83695008471841e-06, 9.83695008471841e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.83695008471841e-06

Optimization complete. Final v2v error: 2.692939519882202 mm

Highest mean error: 3.0279531478881836 mm for frame 79

Lowest mean error: 2.606466293334961 mm for frame 12

Saving results

Total time: 88.71245980262756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584109
Iteration 2/25 | Loss: 0.00136613
Iteration 3/25 | Loss: 0.00129017
Iteration 4/25 | Loss: 0.00128303
Iteration 5/25 | Loss: 0.00128197
Iteration 6/25 | Loss: 0.00128197
Iteration 7/25 | Loss: 0.00128197
Iteration 8/25 | Loss: 0.00128197
Iteration 9/25 | Loss: 0.00128197
Iteration 10/25 | Loss: 0.00128197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012819726252928376, 0.0012819726252928376, 0.0012819726252928376, 0.0012819726252928376, 0.0012819726252928376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012819726252928376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.73013830
Iteration 2/25 | Loss: 0.00103077
Iteration 3/25 | Loss: 0.00103074
Iteration 4/25 | Loss: 0.00103074
Iteration 5/25 | Loss: 0.00103074
Iteration 6/25 | Loss: 0.00103074
Iteration 7/25 | Loss: 0.00103074
Iteration 8/25 | Loss: 0.00103074
Iteration 9/25 | Loss: 0.00103074
Iteration 10/25 | Loss: 0.00103073
Iteration 11/25 | Loss: 0.00103073
Iteration 12/25 | Loss: 0.00103073
Iteration 13/25 | Loss: 0.00103073
Iteration 14/25 | Loss: 0.00103073
Iteration 15/25 | Loss: 0.00103073
Iteration 16/25 | Loss: 0.00103073
Iteration 17/25 | Loss: 0.00103073
Iteration 18/25 | Loss: 0.00103073
Iteration 19/25 | Loss: 0.00103073
Iteration 20/25 | Loss: 0.00103073
Iteration 21/25 | Loss: 0.00103073
Iteration 22/25 | Loss: 0.00103073
Iteration 23/25 | Loss: 0.00103073
Iteration 24/25 | Loss: 0.00103073
Iteration 25/25 | Loss: 0.00103073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103073
Iteration 2/1000 | Loss: 0.00002683
Iteration 3/1000 | Loss: 0.00002031
Iteration 4/1000 | Loss: 0.00001862
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001702
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001609
Iteration 13/1000 | Loss: 0.00001598
Iteration 14/1000 | Loss: 0.00001595
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001593
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001591
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001579
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001564
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001562
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001541
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001540
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001539
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001537
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001533
Iteration 107/1000 | Loss: 0.00001533
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001532
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001531
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001530
Iteration 122/1000 | Loss: 0.00001530
Iteration 123/1000 | Loss: 0.00001530
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001530
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001529
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001526
Iteration 149/1000 | Loss: 0.00001526
Iteration 150/1000 | Loss: 0.00001526
Iteration 151/1000 | Loss: 0.00001526
Iteration 152/1000 | Loss: 0.00001526
Iteration 153/1000 | Loss: 0.00001526
Iteration 154/1000 | Loss: 0.00001526
Iteration 155/1000 | Loss: 0.00001526
Iteration 156/1000 | Loss: 0.00001525
Iteration 157/1000 | Loss: 0.00001525
Iteration 158/1000 | Loss: 0.00001525
Iteration 159/1000 | Loss: 0.00001525
Iteration 160/1000 | Loss: 0.00001525
Iteration 161/1000 | Loss: 0.00001525
Iteration 162/1000 | Loss: 0.00001525
Iteration 163/1000 | Loss: 0.00001525
Iteration 164/1000 | Loss: 0.00001525
Iteration 165/1000 | Loss: 0.00001525
Iteration 166/1000 | Loss: 0.00001525
Iteration 167/1000 | Loss: 0.00001524
Iteration 168/1000 | Loss: 0.00001524
Iteration 169/1000 | Loss: 0.00001524
Iteration 170/1000 | Loss: 0.00001524
Iteration 171/1000 | Loss: 0.00001524
Iteration 172/1000 | Loss: 0.00001524
Iteration 173/1000 | Loss: 0.00001524
Iteration 174/1000 | Loss: 0.00001524
Iteration 175/1000 | Loss: 0.00001524
Iteration 176/1000 | Loss: 0.00001524
Iteration 177/1000 | Loss: 0.00001524
Iteration 178/1000 | Loss: 0.00001524
Iteration 179/1000 | Loss: 0.00001524
Iteration 180/1000 | Loss: 0.00001523
Iteration 181/1000 | Loss: 0.00001523
Iteration 182/1000 | Loss: 0.00001523
Iteration 183/1000 | Loss: 0.00001523
Iteration 184/1000 | Loss: 0.00001523
Iteration 185/1000 | Loss: 0.00001523
Iteration 186/1000 | Loss: 0.00001523
Iteration 187/1000 | Loss: 0.00001523
Iteration 188/1000 | Loss: 0.00001523
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00001523
Iteration 198/1000 | Loss: 0.00001523
Iteration 199/1000 | Loss: 0.00001523
Iteration 200/1000 | Loss: 0.00001523
Iteration 201/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.5226141840685159e-05, 1.5226141840685159e-05, 1.5226141840685159e-05, 1.5226141840685159e-05, 1.5226141840685159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5226141840685159e-05

Optimization complete. Final v2v error: 3.309889793395996 mm

Highest mean error: 3.6449718475341797 mm for frame 177

Lowest mean error: 3.066415786743164 mm for frame 201

Saving results

Total time: 44.83372235298157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486662
Iteration 2/25 | Loss: 0.00141395
Iteration 3/25 | Loss: 0.00130522
Iteration 4/25 | Loss: 0.00129451
Iteration 5/25 | Loss: 0.00129168
Iteration 6/25 | Loss: 0.00129168
Iteration 7/25 | Loss: 0.00129168
Iteration 8/25 | Loss: 0.00129168
Iteration 9/25 | Loss: 0.00129168
Iteration 10/25 | Loss: 0.00129168
Iteration 11/25 | Loss: 0.00129168
Iteration 12/25 | Loss: 0.00129168
Iteration 13/25 | Loss: 0.00129168
Iteration 14/25 | Loss: 0.00129168
Iteration 15/25 | Loss: 0.00129168
Iteration 16/25 | Loss: 0.00129168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012916761916130781, 0.0012916761916130781, 0.0012916761916130781, 0.0012916761916130781, 0.0012916761916130781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012916761916130781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35417855
Iteration 2/25 | Loss: 0.00094349
Iteration 3/25 | Loss: 0.00094346
Iteration 4/25 | Loss: 0.00094346
Iteration 5/25 | Loss: 0.00094346
Iteration 6/25 | Loss: 0.00094345
Iteration 7/25 | Loss: 0.00094345
Iteration 8/25 | Loss: 0.00094345
Iteration 9/25 | Loss: 0.00094345
Iteration 10/25 | Loss: 0.00094345
Iteration 11/25 | Loss: 0.00094345
Iteration 12/25 | Loss: 0.00094345
Iteration 13/25 | Loss: 0.00094345
Iteration 14/25 | Loss: 0.00094345
Iteration 15/25 | Loss: 0.00094345
Iteration 16/25 | Loss: 0.00094345
Iteration 17/25 | Loss: 0.00094345
Iteration 18/25 | Loss: 0.00094345
Iteration 19/25 | Loss: 0.00094345
Iteration 20/25 | Loss: 0.00094345
Iteration 21/25 | Loss: 0.00094345
Iteration 22/25 | Loss: 0.00094345
Iteration 23/25 | Loss: 0.00094345
Iteration 24/25 | Loss: 0.00094345
Iteration 25/25 | Loss: 0.00094345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094345
Iteration 2/1000 | Loss: 0.00003910
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00002307
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001951
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001894
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001870
Iteration 21/1000 | Loss: 0.00001864
Iteration 22/1000 | Loss: 0.00001863
Iteration 23/1000 | Loss: 0.00001861
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001857
Iteration 26/1000 | Loss: 0.00001856
Iteration 27/1000 | Loss: 0.00001855
Iteration 28/1000 | Loss: 0.00001854
Iteration 29/1000 | Loss: 0.00001854
Iteration 30/1000 | Loss: 0.00001853
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001843
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001842
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00001839
Iteration 54/1000 | Loss: 0.00001839
Iteration 55/1000 | Loss: 0.00001838
Iteration 56/1000 | Loss: 0.00001838
Iteration 57/1000 | Loss: 0.00001838
Iteration 58/1000 | Loss: 0.00001838
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001836
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001828
Iteration 79/1000 | Loss: 0.00001828
Iteration 80/1000 | Loss: 0.00001827
Iteration 81/1000 | Loss: 0.00001827
Iteration 82/1000 | Loss: 0.00001827
Iteration 83/1000 | Loss: 0.00001826
Iteration 84/1000 | Loss: 0.00001826
Iteration 85/1000 | Loss: 0.00001826
Iteration 86/1000 | Loss: 0.00001826
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001824
Iteration 91/1000 | Loss: 0.00001824
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001823
Iteration 95/1000 | Loss: 0.00001823
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001817
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001816
Iteration 115/1000 | Loss: 0.00001816
Iteration 116/1000 | Loss: 0.00001816
Iteration 117/1000 | Loss: 0.00001816
Iteration 118/1000 | Loss: 0.00001815
Iteration 119/1000 | Loss: 0.00001815
Iteration 120/1000 | Loss: 0.00001815
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001814
Iteration 126/1000 | Loss: 0.00001814
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001813
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00001812
Iteration 133/1000 | Loss: 0.00001812
Iteration 134/1000 | Loss: 0.00001812
Iteration 135/1000 | Loss: 0.00001812
Iteration 136/1000 | Loss: 0.00001812
Iteration 137/1000 | Loss: 0.00001812
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001812
Iteration 140/1000 | Loss: 0.00001812
Iteration 141/1000 | Loss: 0.00001812
Iteration 142/1000 | Loss: 0.00001812
Iteration 143/1000 | Loss: 0.00001812
Iteration 144/1000 | Loss: 0.00001812
Iteration 145/1000 | Loss: 0.00001812
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001811
Iteration 148/1000 | Loss: 0.00001811
Iteration 149/1000 | Loss: 0.00001811
Iteration 150/1000 | Loss: 0.00001811
Iteration 151/1000 | Loss: 0.00001811
Iteration 152/1000 | Loss: 0.00001811
Iteration 153/1000 | Loss: 0.00001811
Iteration 154/1000 | Loss: 0.00001811
Iteration 155/1000 | Loss: 0.00001811
Iteration 156/1000 | Loss: 0.00001811
Iteration 157/1000 | Loss: 0.00001811
Iteration 158/1000 | Loss: 0.00001811
Iteration 159/1000 | Loss: 0.00001810
Iteration 160/1000 | Loss: 0.00001810
Iteration 161/1000 | Loss: 0.00001810
Iteration 162/1000 | Loss: 0.00001810
Iteration 163/1000 | Loss: 0.00001810
Iteration 164/1000 | Loss: 0.00001810
Iteration 165/1000 | Loss: 0.00001810
Iteration 166/1000 | Loss: 0.00001810
Iteration 167/1000 | Loss: 0.00001810
Iteration 168/1000 | Loss: 0.00001810
Iteration 169/1000 | Loss: 0.00001810
Iteration 170/1000 | Loss: 0.00001810
Iteration 171/1000 | Loss: 0.00001810
Iteration 172/1000 | Loss: 0.00001809
Iteration 173/1000 | Loss: 0.00001809
Iteration 174/1000 | Loss: 0.00001809
Iteration 175/1000 | Loss: 0.00001809
Iteration 176/1000 | Loss: 0.00001809
Iteration 177/1000 | Loss: 0.00001809
Iteration 178/1000 | Loss: 0.00001809
Iteration 179/1000 | Loss: 0.00001809
Iteration 180/1000 | Loss: 0.00001809
Iteration 181/1000 | Loss: 0.00001809
Iteration 182/1000 | Loss: 0.00001809
Iteration 183/1000 | Loss: 0.00001809
Iteration 184/1000 | Loss: 0.00001809
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001809
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00001809
Iteration 189/1000 | Loss: 0.00001809
Iteration 190/1000 | Loss: 0.00001809
Iteration 191/1000 | Loss: 0.00001809
Iteration 192/1000 | Loss: 0.00001809
Iteration 193/1000 | Loss: 0.00001809
Iteration 194/1000 | Loss: 0.00001809
Iteration 195/1000 | Loss: 0.00001809
Iteration 196/1000 | Loss: 0.00001809
Iteration 197/1000 | Loss: 0.00001809
Iteration 198/1000 | Loss: 0.00001809
Iteration 199/1000 | Loss: 0.00001809
Iteration 200/1000 | Loss: 0.00001809
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.809116656659171e-05, 1.809116656659171e-05, 1.809116656659171e-05, 1.809116656659171e-05, 1.809116656659171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.809116656659171e-05

Optimization complete. Final v2v error: 3.4822843074798584 mm

Highest mean error: 4.206957817077637 mm for frame 2

Lowest mean error: 3.134934902191162 mm for frame 146

Saving results

Total time: 46.7627899646759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821569
Iteration 2/25 | Loss: 0.00129836
Iteration 3/25 | Loss: 0.00122752
Iteration 4/25 | Loss: 0.00121903
Iteration 5/25 | Loss: 0.00121613
Iteration 6/25 | Loss: 0.00121569
Iteration 7/25 | Loss: 0.00121569
Iteration 8/25 | Loss: 0.00121569
Iteration 9/25 | Loss: 0.00121569
Iteration 10/25 | Loss: 0.00121569
Iteration 11/25 | Loss: 0.00121569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012156949378550053, 0.0012156949378550053, 0.0012156949378550053, 0.0012156949378550053, 0.0012156949378550053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012156949378550053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58412325
Iteration 2/25 | Loss: 0.00105733
Iteration 3/25 | Loss: 0.00105733
Iteration 4/25 | Loss: 0.00105733
Iteration 5/25 | Loss: 0.00105733
Iteration 6/25 | Loss: 0.00105733
Iteration 7/25 | Loss: 0.00105733
Iteration 8/25 | Loss: 0.00105733
Iteration 9/25 | Loss: 0.00105733
Iteration 10/25 | Loss: 0.00105733
Iteration 11/25 | Loss: 0.00105733
Iteration 12/25 | Loss: 0.00105733
Iteration 13/25 | Loss: 0.00105733
Iteration 14/25 | Loss: 0.00105733
Iteration 15/25 | Loss: 0.00105733
Iteration 16/25 | Loss: 0.00105733
Iteration 17/25 | Loss: 0.00105733
Iteration 18/25 | Loss: 0.00105733
Iteration 19/25 | Loss: 0.00105733
Iteration 20/25 | Loss: 0.00105733
Iteration 21/25 | Loss: 0.00105733
Iteration 22/25 | Loss: 0.00105733
Iteration 23/25 | Loss: 0.00105733
Iteration 24/25 | Loss: 0.00105733
Iteration 25/25 | Loss: 0.00105733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105733
Iteration 2/1000 | Loss: 0.00002113
Iteration 3/1000 | Loss: 0.00001501
Iteration 4/1000 | Loss: 0.00001361
Iteration 5/1000 | Loss: 0.00001287
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001194
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001123
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001112
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001080
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001079
Iteration 30/1000 | Loss: 0.00001079
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001078
Iteration 33/1000 | Loss: 0.00001077
Iteration 34/1000 | Loss: 0.00001076
Iteration 35/1000 | Loss: 0.00001076
Iteration 36/1000 | Loss: 0.00001075
Iteration 37/1000 | Loss: 0.00001074
Iteration 38/1000 | Loss: 0.00001074
Iteration 39/1000 | Loss: 0.00001073
Iteration 40/1000 | Loss: 0.00001072
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001070
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001059
Iteration 68/1000 | Loss: 0.00001059
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001058
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001056
Iteration 76/1000 | Loss: 0.00001056
Iteration 77/1000 | Loss: 0.00001055
Iteration 78/1000 | Loss: 0.00001055
Iteration 79/1000 | Loss: 0.00001055
Iteration 80/1000 | Loss: 0.00001055
Iteration 81/1000 | Loss: 0.00001055
Iteration 82/1000 | Loss: 0.00001055
Iteration 83/1000 | Loss: 0.00001055
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001053
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001052
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001051
Iteration 96/1000 | Loss: 0.00001051
Iteration 97/1000 | Loss: 0.00001051
Iteration 98/1000 | Loss: 0.00001051
Iteration 99/1000 | Loss: 0.00001050
Iteration 100/1000 | Loss: 0.00001050
Iteration 101/1000 | Loss: 0.00001049
Iteration 102/1000 | Loss: 0.00001049
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001047
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001043
Iteration 124/1000 | Loss: 0.00001043
Iteration 125/1000 | Loss: 0.00001043
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001043
Iteration 133/1000 | Loss: 0.00001043
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001042
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001042
Iteration 142/1000 | Loss: 0.00001042
Iteration 143/1000 | Loss: 0.00001042
Iteration 144/1000 | Loss: 0.00001042
Iteration 145/1000 | Loss: 0.00001041
Iteration 146/1000 | Loss: 0.00001041
Iteration 147/1000 | Loss: 0.00001041
Iteration 148/1000 | Loss: 0.00001041
Iteration 149/1000 | Loss: 0.00001041
Iteration 150/1000 | Loss: 0.00001041
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001041
Iteration 160/1000 | Loss: 0.00001041
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001040
Iteration 163/1000 | Loss: 0.00001040
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001040
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001039
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001039
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001039
Iteration 179/1000 | Loss: 0.00001039
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001039
Iteration 183/1000 | Loss: 0.00001039
Iteration 184/1000 | Loss: 0.00001039
Iteration 185/1000 | Loss: 0.00001039
Iteration 186/1000 | Loss: 0.00001039
Iteration 187/1000 | Loss: 0.00001039
Iteration 188/1000 | Loss: 0.00001039
Iteration 189/1000 | Loss: 0.00001039
Iteration 190/1000 | Loss: 0.00001039
Iteration 191/1000 | Loss: 0.00001039
Iteration 192/1000 | Loss: 0.00001039
Iteration 193/1000 | Loss: 0.00001039
Iteration 194/1000 | Loss: 0.00001039
Iteration 195/1000 | Loss: 0.00001039
Iteration 196/1000 | Loss: 0.00001039
Iteration 197/1000 | Loss: 0.00001039
Iteration 198/1000 | Loss: 0.00001039
Iteration 199/1000 | Loss: 0.00001039
Iteration 200/1000 | Loss: 0.00001039
Iteration 201/1000 | Loss: 0.00001039
Iteration 202/1000 | Loss: 0.00001039
Iteration 203/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.0386989742983133e-05, 1.0386989742983133e-05, 1.0386989742983133e-05, 1.0386989742983133e-05, 1.0386989742983133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0386989742983133e-05

Optimization complete. Final v2v error: 2.7837705612182617 mm

Highest mean error: 3.1039655208587646 mm for frame 87

Lowest mean error: 2.639859199523926 mm for frame 127

Saving results

Total time: 36.33623766899109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897935
Iteration 2/25 | Loss: 0.00143115
Iteration 3/25 | Loss: 0.00134278
Iteration 4/25 | Loss: 0.00133452
Iteration 5/25 | Loss: 0.00133239
Iteration 6/25 | Loss: 0.00133201
Iteration 7/25 | Loss: 0.00133201
Iteration 8/25 | Loss: 0.00133201
Iteration 9/25 | Loss: 0.00133201
Iteration 10/25 | Loss: 0.00133201
Iteration 11/25 | Loss: 0.00133201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013320095604285598, 0.0013320095604285598, 0.0013320095604285598, 0.0013320095604285598, 0.0013320095604285598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013320095604285598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21099234
Iteration 2/25 | Loss: 0.00113970
Iteration 3/25 | Loss: 0.00113969
Iteration 4/25 | Loss: 0.00113968
Iteration 5/25 | Loss: 0.00113968
Iteration 6/25 | Loss: 0.00113968
Iteration 7/25 | Loss: 0.00113968
Iteration 8/25 | Loss: 0.00113968
Iteration 9/25 | Loss: 0.00113968
Iteration 10/25 | Loss: 0.00113968
Iteration 11/25 | Loss: 0.00113968
Iteration 12/25 | Loss: 0.00113968
Iteration 13/25 | Loss: 0.00113968
Iteration 14/25 | Loss: 0.00113968
Iteration 15/25 | Loss: 0.00113968
Iteration 16/25 | Loss: 0.00113968
Iteration 17/25 | Loss: 0.00113968
Iteration 18/25 | Loss: 0.00113968
Iteration 19/25 | Loss: 0.00113968
Iteration 20/25 | Loss: 0.00113968
Iteration 21/25 | Loss: 0.00113968
Iteration 22/25 | Loss: 0.00113968
Iteration 23/25 | Loss: 0.00113968
Iteration 24/25 | Loss: 0.00113968
Iteration 25/25 | Loss: 0.00113968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011396800400689244, 0.0011396800400689244, 0.0011396800400689244, 0.0011396800400689244, 0.0011396800400689244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011396800400689244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113968
Iteration 2/1000 | Loss: 0.00004211
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001953
Iteration 6/1000 | Loss: 0.00001898
Iteration 7/1000 | Loss: 0.00001859
Iteration 8/1000 | Loss: 0.00001825
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001778
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001680
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001680
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001678
Iteration 24/1000 | Loss: 0.00001677
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001677
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001675
Iteration 30/1000 | Loss: 0.00001675
Iteration 31/1000 | Loss: 0.00001673
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001672
Iteration 35/1000 | Loss: 0.00001672
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001667
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001666
Iteration 53/1000 | Loss: 0.00001665
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001664
Iteration 56/1000 | Loss: 0.00001664
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001661
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001660
Iteration 64/1000 | Loss: 0.00001660
Iteration 65/1000 | Loss: 0.00001659
Iteration 66/1000 | Loss: 0.00001659
Iteration 67/1000 | Loss: 0.00001659
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001657
Iteration 72/1000 | Loss: 0.00001657
Iteration 73/1000 | Loss: 0.00001656
Iteration 74/1000 | Loss: 0.00001655
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001651
Iteration 79/1000 | Loss: 0.00001651
Iteration 80/1000 | Loss: 0.00001651
Iteration 81/1000 | Loss: 0.00001650
Iteration 82/1000 | Loss: 0.00001650
Iteration 83/1000 | Loss: 0.00001650
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001648
Iteration 100/1000 | Loss: 0.00001648
Iteration 101/1000 | Loss: 0.00001648
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001646
Iteration 105/1000 | Loss: 0.00001646
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001645
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001645
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001644
Iteration 116/1000 | Loss: 0.00001644
Iteration 117/1000 | Loss: 0.00001644
Iteration 118/1000 | Loss: 0.00001644
Iteration 119/1000 | Loss: 0.00001644
Iteration 120/1000 | Loss: 0.00001644
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001643
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001641
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001641
Iteration 136/1000 | Loss: 0.00001641
Iteration 137/1000 | Loss: 0.00001641
Iteration 138/1000 | Loss: 0.00001641
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001640
Iteration 144/1000 | Loss: 0.00001640
Iteration 145/1000 | Loss: 0.00001640
Iteration 146/1000 | Loss: 0.00001640
Iteration 147/1000 | Loss: 0.00001640
Iteration 148/1000 | Loss: 0.00001640
Iteration 149/1000 | Loss: 0.00001640
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001639
Iteration 152/1000 | Loss: 0.00001639
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001638
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001637
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001636
Iteration 162/1000 | Loss: 0.00001636
Iteration 163/1000 | Loss: 0.00001635
Iteration 164/1000 | Loss: 0.00001635
Iteration 165/1000 | Loss: 0.00001635
Iteration 166/1000 | Loss: 0.00001635
Iteration 167/1000 | Loss: 0.00001635
Iteration 168/1000 | Loss: 0.00001635
Iteration 169/1000 | Loss: 0.00001635
Iteration 170/1000 | Loss: 0.00001635
Iteration 171/1000 | Loss: 0.00001635
Iteration 172/1000 | Loss: 0.00001635
Iteration 173/1000 | Loss: 0.00001635
Iteration 174/1000 | Loss: 0.00001635
Iteration 175/1000 | Loss: 0.00001635
Iteration 176/1000 | Loss: 0.00001634
Iteration 177/1000 | Loss: 0.00001634
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001633
Iteration 180/1000 | Loss: 0.00001633
Iteration 181/1000 | Loss: 0.00001633
Iteration 182/1000 | Loss: 0.00001633
Iteration 183/1000 | Loss: 0.00001633
Iteration 184/1000 | Loss: 0.00001633
Iteration 185/1000 | Loss: 0.00001633
Iteration 186/1000 | Loss: 0.00001633
Iteration 187/1000 | Loss: 0.00001632
Iteration 188/1000 | Loss: 0.00001632
Iteration 189/1000 | Loss: 0.00001632
Iteration 190/1000 | Loss: 0.00001632
Iteration 191/1000 | Loss: 0.00001632
Iteration 192/1000 | Loss: 0.00001632
Iteration 193/1000 | Loss: 0.00001631
Iteration 194/1000 | Loss: 0.00001631
Iteration 195/1000 | Loss: 0.00001631
Iteration 196/1000 | Loss: 0.00001631
Iteration 197/1000 | Loss: 0.00001631
Iteration 198/1000 | Loss: 0.00001631
Iteration 199/1000 | Loss: 0.00001631
Iteration 200/1000 | Loss: 0.00001631
Iteration 201/1000 | Loss: 0.00001631
Iteration 202/1000 | Loss: 0.00001631
Iteration 203/1000 | Loss: 0.00001630
Iteration 204/1000 | Loss: 0.00001630
Iteration 205/1000 | Loss: 0.00001630
Iteration 206/1000 | Loss: 0.00001630
Iteration 207/1000 | Loss: 0.00001630
Iteration 208/1000 | Loss: 0.00001630
Iteration 209/1000 | Loss: 0.00001630
Iteration 210/1000 | Loss: 0.00001630
Iteration 211/1000 | Loss: 0.00001630
Iteration 212/1000 | Loss: 0.00001630
Iteration 213/1000 | Loss: 0.00001630
Iteration 214/1000 | Loss: 0.00001630
Iteration 215/1000 | Loss: 0.00001630
Iteration 216/1000 | Loss: 0.00001630
Iteration 217/1000 | Loss: 0.00001630
Iteration 218/1000 | Loss: 0.00001630
Iteration 219/1000 | Loss: 0.00001630
Iteration 220/1000 | Loss: 0.00001630
Iteration 221/1000 | Loss: 0.00001630
Iteration 222/1000 | Loss: 0.00001630
Iteration 223/1000 | Loss: 0.00001630
Iteration 224/1000 | Loss: 0.00001630
Iteration 225/1000 | Loss: 0.00001630
Iteration 226/1000 | Loss: 0.00001630
Iteration 227/1000 | Loss: 0.00001630
Iteration 228/1000 | Loss: 0.00001630
Iteration 229/1000 | Loss: 0.00001630
Iteration 230/1000 | Loss: 0.00001630
Iteration 231/1000 | Loss: 0.00001630
Iteration 232/1000 | Loss: 0.00001630
Iteration 233/1000 | Loss: 0.00001630
Iteration 234/1000 | Loss: 0.00001630
Iteration 235/1000 | Loss: 0.00001630
Iteration 236/1000 | Loss: 0.00001630
Iteration 237/1000 | Loss: 0.00001630
Iteration 238/1000 | Loss: 0.00001630
Iteration 239/1000 | Loss: 0.00001630
Iteration 240/1000 | Loss: 0.00001630
Iteration 241/1000 | Loss: 0.00001630
Iteration 242/1000 | Loss: 0.00001630
Iteration 243/1000 | Loss: 0.00001630
Iteration 244/1000 | Loss: 0.00001630
Iteration 245/1000 | Loss: 0.00001630
Iteration 246/1000 | Loss: 0.00001630
Iteration 247/1000 | Loss: 0.00001630
Iteration 248/1000 | Loss: 0.00001630
Iteration 249/1000 | Loss: 0.00001630
Iteration 250/1000 | Loss: 0.00001630
Iteration 251/1000 | Loss: 0.00001630
Iteration 252/1000 | Loss: 0.00001630
Iteration 253/1000 | Loss: 0.00001630
Iteration 254/1000 | Loss: 0.00001630
Iteration 255/1000 | Loss: 0.00001630
Iteration 256/1000 | Loss: 0.00001630
Iteration 257/1000 | Loss: 0.00001630
Iteration 258/1000 | Loss: 0.00001630
Iteration 259/1000 | Loss: 0.00001630
Iteration 260/1000 | Loss: 0.00001630
Iteration 261/1000 | Loss: 0.00001630
Iteration 262/1000 | Loss: 0.00001630
Iteration 263/1000 | Loss: 0.00001630
Iteration 264/1000 | Loss: 0.00001630
Iteration 265/1000 | Loss: 0.00001630
Iteration 266/1000 | Loss: 0.00001630
Iteration 267/1000 | Loss: 0.00001630
Iteration 268/1000 | Loss: 0.00001630
Iteration 269/1000 | Loss: 0.00001630
Iteration 270/1000 | Loss: 0.00001630
Iteration 271/1000 | Loss: 0.00001630
Iteration 272/1000 | Loss: 0.00001630
Iteration 273/1000 | Loss: 0.00001630
Iteration 274/1000 | Loss: 0.00001630
Iteration 275/1000 | Loss: 0.00001630
Iteration 276/1000 | Loss: 0.00001630
Iteration 277/1000 | Loss: 0.00001630
Iteration 278/1000 | Loss: 0.00001630
Iteration 279/1000 | Loss: 0.00001630
Iteration 280/1000 | Loss: 0.00001630
Iteration 281/1000 | Loss: 0.00001630
Iteration 282/1000 | Loss: 0.00001630
Iteration 283/1000 | Loss: 0.00001630
Iteration 284/1000 | Loss: 0.00001630
Iteration 285/1000 | Loss: 0.00001630
Iteration 286/1000 | Loss: 0.00001630
Iteration 287/1000 | Loss: 0.00001630
Iteration 288/1000 | Loss: 0.00001630
Iteration 289/1000 | Loss: 0.00001630
Iteration 290/1000 | Loss: 0.00001630
Iteration 291/1000 | Loss: 0.00001630
Iteration 292/1000 | Loss: 0.00001630
Iteration 293/1000 | Loss: 0.00001630
Iteration 294/1000 | Loss: 0.00001630
Iteration 295/1000 | Loss: 0.00001630
Iteration 296/1000 | Loss: 0.00001630
Iteration 297/1000 | Loss: 0.00001630
Iteration 298/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.6299616618198343e-05, 1.6299616618198343e-05, 1.6299616618198343e-05, 1.6299616618198343e-05, 1.6299616618198343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6299616618198343e-05

Optimization complete. Final v2v error: 3.3632142543792725 mm

Highest mean error: 3.4775068759918213 mm for frame 0

Lowest mean error: 3.223280191421509 mm for frame 117

Saving results

Total time: 45.10913610458374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796503
Iteration 2/25 | Loss: 0.00141360
Iteration 3/25 | Loss: 0.00127964
Iteration 4/25 | Loss: 0.00126424
Iteration 5/25 | Loss: 0.00125917
Iteration 6/25 | Loss: 0.00125890
Iteration 7/25 | Loss: 0.00125890
Iteration 8/25 | Loss: 0.00125890
Iteration 9/25 | Loss: 0.00125890
Iteration 10/25 | Loss: 0.00125890
Iteration 11/25 | Loss: 0.00125890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012588977115228772, 0.0012588977115228772, 0.0012588977115228772, 0.0012588977115228772, 0.0012588977115228772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012588977115228772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23930800
Iteration 2/25 | Loss: 0.00084077
Iteration 3/25 | Loss: 0.00084074
Iteration 4/25 | Loss: 0.00084074
Iteration 5/25 | Loss: 0.00084074
Iteration 6/25 | Loss: 0.00084074
Iteration 7/25 | Loss: 0.00084074
Iteration 8/25 | Loss: 0.00084074
Iteration 9/25 | Loss: 0.00084074
Iteration 10/25 | Loss: 0.00084074
Iteration 11/25 | Loss: 0.00084074
Iteration 12/25 | Loss: 0.00084074
Iteration 13/25 | Loss: 0.00084074
Iteration 14/25 | Loss: 0.00084074
Iteration 15/25 | Loss: 0.00084074
Iteration 16/25 | Loss: 0.00084074
Iteration 17/25 | Loss: 0.00084074
Iteration 18/25 | Loss: 0.00084074
Iteration 19/25 | Loss: 0.00084074
Iteration 20/25 | Loss: 0.00084074
Iteration 21/25 | Loss: 0.00084074
Iteration 22/25 | Loss: 0.00084074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008407356217503548, 0.0008407356217503548, 0.0008407356217503548, 0.0008407356217503548, 0.0008407356217503548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008407356217503548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084074
Iteration 2/1000 | Loss: 0.00003717
Iteration 3/1000 | Loss: 0.00002650
Iteration 4/1000 | Loss: 0.00002376
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002093
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001965
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001855
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001819
Iteration 14/1000 | Loss: 0.00001807
Iteration 15/1000 | Loss: 0.00001804
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001777
Iteration 27/1000 | Loss: 0.00001777
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001771
Iteration 30/1000 | Loss: 0.00001771
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001770
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001767
Iteration 35/1000 | Loss: 0.00001767
Iteration 36/1000 | Loss: 0.00001766
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001765
Iteration 39/1000 | Loss: 0.00001765
Iteration 40/1000 | Loss: 0.00001764
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001760
Iteration 51/1000 | Loss: 0.00001760
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001759
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001759
Iteration 58/1000 | Loss: 0.00001759
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001756
Iteration 78/1000 | Loss: 0.00001756
Iteration 79/1000 | Loss: 0.00001755
Iteration 80/1000 | Loss: 0.00001755
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001755
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00001755
Iteration 86/1000 | Loss: 0.00001754
Iteration 87/1000 | Loss: 0.00001754
Iteration 88/1000 | Loss: 0.00001754
Iteration 89/1000 | Loss: 0.00001754
Iteration 90/1000 | Loss: 0.00001754
Iteration 91/1000 | Loss: 0.00001754
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001753
Iteration 95/1000 | Loss: 0.00001753
Iteration 96/1000 | Loss: 0.00001753
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001749
Iteration 107/1000 | Loss: 0.00001749
Iteration 108/1000 | Loss: 0.00001749
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001748
Iteration 114/1000 | Loss: 0.00001748
Iteration 115/1000 | Loss: 0.00001748
Iteration 116/1000 | Loss: 0.00001748
Iteration 117/1000 | Loss: 0.00001748
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001747
Iteration 121/1000 | Loss: 0.00001747
Iteration 122/1000 | Loss: 0.00001747
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001746
Iteration 125/1000 | Loss: 0.00001746
Iteration 126/1000 | Loss: 0.00001746
Iteration 127/1000 | Loss: 0.00001746
Iteration 128/1000 | Loss: 0.00001746
Iteration 129/1000 | Loss: 0.00001746
Iteration 130/1000 | Loss: 0.00001746
Iteration 131/1000 | Loss: 0.00001746
Iteration 132/1000 | Loss: 0.00001746
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001745
Iteration 139/1000 | Loss: 0.00001745
Iteration 140/1000 | Loss: 0.00001745
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001744
Iteration 148/1000 | Loss: 0.00001744
Iteration 149/1000 | Loss: 0.00001744
Iteration 150/1000 | Loss: 0.00001744
Iteration 151/1000 | Loss: 0.00001744
Iteration 152/1000 | Loss: 0.00001744
Iteration 153/1000 | Loss: 0.00001744
Iteration 154/1000 | Loss: 0.00001744
Iteration 155/1000 | Loss: 0.00001744
Iteration 156/1000 | Loss: 0.00001744
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001743
Iteration 162/1000 | Loss: 0.00001743
Iteration 163/1000 | Loss: 0.00001743
Iteration 164/1000 | Loss: 0.00001743
Iteration 165/1000 | Loss: 0.00001742
Iteration 166/1000 | Loss: 0.00001742
Iteration 167/1000 | Loss: 0.00001742
Iteration 168/1000 | Loss: 0.00001742
Iteration 169/1000 | Loss: 0.00001742
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Iteration 172/1000 | Loss: 0.00001741
Iteration 173/1000 | Loss: 0.00001741
Iteration 174/1000 | Loss: 0.00001741
Iteration 175/1000 | Loss: 0.00001741
Iteration 176/1000 | Loss: 0.00001741
Iteration 177/1000 | Loss: 0.00001741
Iteration 178/1000 | Loss: 0.00001741
Iteration 179/1000 | Loss: 0.00001741
Iteration 180/1000 | Loss: 0.00001741
Iteration 181/1000 | Loss: 0.00001741
Iteration 182/1000 | Loss: 0.00001741
Iteration 183/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.7407779523637146e-05, 1.7407779523637146e-05, 1.7407779523637146e-05, 1.7407779523637146e-05, 1.7407779523637146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7407779523637146e-05

Optimization complete. Final v2v error: 3.4745426177978516 mm

Highest mean error: 4.298314571380615 mm for frame 61

Lowest mean error: 3.135690212249756 mm for frame 160

Saving results

Total time: 46.73178458213806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390586
Iteration 2/25 | Loss: 0.00137088
Iteration 3/25 | Loss: 0.00122285
Iteration 4/25 | Loss: 0.00120548
Iteration 5/25 | Loss: 0.00120240
Iteration 6/25 | Loss: 0.00120139
Iteration 7/25 | Loss: 0.00120139
Iteration 8/25 | Loss: 0.00120139
Iteration 9/25 | Loss: 0.00120139
Iteration 10/25 | Loss: 0.00120139
Iteration 11/25 | Loss: 0.00120139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012013927334919572, 0.0012013927334919572, 0.0012013927334919572, 0.0012013927334919572, 0.0012013927334919572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012013927334919572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35137808
Iteration 2/25 | Loss: 0.00088134
Iteration 3/25 | Loss: 0.00088134
Iteration 4/25 | Loss: 0.00088134
Iteration 5/25 | Loss: 0.00088134
Iteration 6/25 | Loss: 0.00088134
Iteration 7/25 | Loss: 0.00088134
Iteration 8/25 | Loss: 0.00088134
Iteration 9/25 | Loss: 0.00088133
Iteration 10/25 | Loss: 0.00088133
Iteration 11/25 | Loss: 0.00088133
Iteration 12/25 | Loss: 0.00088133
Iteration 13/25 | Loss: 0.00088133
Iteration 14/25 | Loss: 0.00088133
Iteration 15/25 | Loss: 0.00088133
Iteration 16/25 | Loss: 0.00088133
Iteration 17/25 | Loss: 0.00088133
Iteration 18/25 | Loss: 0.00088133
Iteration 19/25 | Loss: 0.00088133
Iteration 20/25 | Loss: 0.00088133
Iteration 21/25 | Loss: 0.00088133
Iteration 22/25 | Loss: 0.00088133
Iteration 23/25 | Loss: 0.00088133
Iteration 24/25 | Loss: 0.00088133
Iteration 25/25 | Loss: 0.00088133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088133
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001462
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001264
Iteration 17/1000 | Loss: 0.00001262
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001229
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001223
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001222
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001221
Iteration 97/1000 | Loss: 0.00001221
Iteration 98/1000 | Loss: 0.00001221
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001212
Iteration 117/1000 | Loss: 0.00001212
Iteration 118/1000 | Loss: 0.00001212
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001211
Iteration 122/1000 | Loss: 0.00001211
Iteration 123/1000 | Loss: 0.00001211
Iteration 124/1000 | Loss: 0.00001211
Iteration 125/1000 | Loss: 0.00001211
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001210
Iteration 129/1000 | Loss: 0.00001209
Iteration 130/1000 | Loss: 0.00001209
Iteration 131/1000 | Loss: 0.00001209
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001205
Iteration 142/1000 | Loss: 0.00001205
Iteration 143/1000 | Loss: 0.00001205
Iteration 144/1000 | Loss: 0.00001205
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001204
Iteration 150/1000 | Loss: 0.00001204
Iteration 151/1000 | Loss: 0.00001204
Iteration 152/1000 | Loss: 0.00001204
Iteration 153/1000 | Loss: 0.00001204
Iteration 154/1000 | Loss: 0.00001204
Iteration 155/1000 | Loss: 0.00001204
Iteration 156/1000 | Loss: 0.00001203
Iteration 157/1000 | Loss: 0.00001203
Iteration 158/1000 | Loss: 0.00001203
Iteration 159/1000 | Loss: 0.00001202
Iteration 160/1000 | Loss: 0.00001202
Iteration 161/1000 | Loss: 0.00001202
Iteration 162/1000 | Loss: 0.00001202
Iteration 163/1000 | Loss: 0.00001202
Iteration 164/1000 | Loss: 0.00001202
Iteration 165/1000 | Loss: 0.00001202
Iteration 166/1000 | Loss: 0.00001202
Iteration 167/1000 | Loss: 0.00001202
Iteration 168/1000 | Loss: 0.00001202
Iteration 169/1000 | Loss: 0.00001202
Iteration 170/1000 | Loss: 0.00001201
Iteration 171/1000 | Loss: 0.00001201
Iteration 172/1000 | Loss: 0.00001201
Iteration 173/1000 | Loss: 0.00001201
Iteration 174/1000 | Loss: 0.00001201
Iteration 175/1000 | Loss: 0.00001201
Iteration 176/1000 | Loss: 0.00001201
Iteration 177/1000 | Loss: 0.00001201
Iteration 178/1000 | Loss: 0.00001201
Iteration 179/1000 | Loss: 0.00001201
Iteration 180/1000 | Loss: 0.00001201
Iteration 181/1000 | Loss: 0.00001201
Iteration 182/1000 | Loss: 0.00001201
Iteration 183/1000 | Loss: 0.00001201
Iteration 184/1000 | Loss: 0.00001201
Iteration 185/1000 | Loss: 0.00001200
Iteration 186/1000 | Loss: 0.00001200
Iteration 187/1000 | Loss: 0.00001200
Iteration 188/1000 | Loss: 0.00001200
Iteration 189/1000 | Loss: 0.00001200
Iteration 190/1000 | Loss: 0.00001200
Iteration 191/1000 | Loss: 0.00001199
Iteration 192/1000 | Loss: 0.00001199
Iteration 193/1000 | Loss: 0.00001199
Iteration 194/1000 | Loss: 0.00001199
Iteration 195/1000 | Loss: 0.00001199
Iteration 196/1000 | Loss: 0.00001199
Iteration 197/1000 | Loss: 0.00001199
Iteration 198/1000 | Loss: 0.00001199
Iteration 199/1000 | Loss: 0.00001199
Iteration 200/1000 | Loss: 0.00001199
Iteration 201/1000 | Loss: 0.00001199
Iteration 202/1000 | Loss: 0.00001199
Iteration 203/1000 | Loss: 0.00001199
Iteration 204/1000 | Loss: 0.00001199
Iteration 205/1000 | Loss: 0.00001199
Iteration 206/1000 | Loss: 0.00001199
Iteration 207/1000 | Loss: 0.00001199
Iteration 208/1000 | Loss: 0.00001199
Iteration 209/1000 | Loss: 0.00001199
Iteration 210/1000 | Loss: 0.00001199
Iteration 211/1000 | Loss: 0.00001199
Iteration 212/1000 | Loss: 0.00001199
Iteration 213/1000 | Loss: 0.00001198
Iteration 214/1000 | Loss: 0.00001198
Iteration 215/1000 | Loss: 0.00001198
Iteration 216/1000 | Loss: 0.00001198
Iteration 217/1000 | Loss: 0.00001198
Iteration 218/1000 | Loss: 0.00001198
Iteration 219/1000 | Loss: 0.00001198
Iteration 220/1000 | Loss: 0.00001198
Iteration 221/1000 | Loss: 0.00001198
Iteration 222/1000 | Loss: 0.00001198
Iteration 223/1000 | Loss: 0.00001198
Iteration 224/1000 | Loss: 0.00001198
Iteration 225/1000 | Loss: 0.00001198
Iteration 226/1000 | Loss: 0.00001197
Iteration 227/1000 | Loss: 0.00001197
Iteration 228/1000 | Loss: 0.00001197
Iteration 229/1000 | Loss: 0.00001197
Iteration 230/1000 | Loss: 0.00001197
Iteration 231/1000 | Loss: 0.00001197
Iteration 232/1000 | Loss: 0.00001197
Iteration 233/1000 | Loss: 0.00001197
Iteration 234/1000 | Loss: 0.00001197
Iteration 235/1000 | Loss: 0.00001197
Iteration 236/1000 | Loss: 0.00001197
Iteration 237/1000 | Loss: 0.00001197
Iteration 238/1000 | Loss: 0.00001197
Iteration 239/1000 | Loss: 0.00001197
Iteration 240/1000 | Loss: 0.00001196
Iteration 241/1000 | Loss: 0.00001196
Iteration 242/1000 | Loss: 0.00001196
Iteration 243/1000 | Loss: 0.00001196
Iteration 244/1000 | Loss: 0.00001196
Iteration 245/1000 | Loss: 0.00001196
Iteration 246/1000 | Loss: 0.00001196
Iteration 247/1000 | Loss: 0.00001196
Iteration 248/1000 | Loss: 0.00001196
Iteration 249/1000 | Loss: 0.00001196
Iteration 250/1000 | Loss: 0.00001196
Iteration 251/1000 | Loss: 0.00001195
Iteration 252/1000 | Loss: 0.00001195
Iteration 253/1000 | Loss: 0.00001195
Iteration 254/1000 | Loss: 0.00001195
Iteration 255/1000 | Loss: 0.00001195
Iteration 256/1000 | Loss: 0.00001195
Iteration 257/1000 | Loss: 0.00001195
Iteration 258/1000 | Loss: 0.00001195
Iteration 259/1000 | Loss: 0.00001195
Iteration 260/1000 | Loss: 0.00001195
Iteration 261/1000 | Loss: 0.00001195
Iteration 262/1000 | Loss: 0.00001195
Iteration 263/1000 | Loss: 0.00001195
Iteration 264/1000 | Loss: 0.00001195
Iteration 265/1000 | Loss: 0.00001194
Iteration 266/1000 | Loss: 0.00001194
Iteration 267/1000 | Loss: 0.00001194
Iteration 268/1000 | Loss: 0.00001194
Iteration 269/1000 | Loss: 0.00001194
Iteration 270/1000 | Loss: 0.00001194
Iteration 271/1000 | Loss: 0.00001194
Iteration 272/1000 | Loss: 0.00001194
Iteration 273/1000 | Loss: 0.00001194
Iteration 274/1000 | Loss: 0.00001194
Iteration 275/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.1944648576900363e-05, 1.1944648576900363e-05, 1.1944648576900363e-05, 1.1944648576900363e-05, 1.1944648576900363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1944648576900363e-05

Optimization complete. Final v2v error: 2.9731836318969727 mm

Highest mean error: 3.2160401344299316 mm for frame 40

Lowest mean error: 2.819458246231079 mm for frame 2

Saving results

Total time: 43.919556856155396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487665
Iteration 2/25 | Loss: 0.00154449
Iteration 3/25 | Loss: 0.00133759
Iteration 4/25 | Loss: 0.00131685
Iteration 5/25 | Loss: 0.00131361
Iteration 6/25 | Loss: 0.00131287
Iteration 7/25 | Loss: 0.00131287
Iteration 8/25 | Loss: 0.00131287
Iteration 9/25 | Loss: 0.00131287
Iteration 10/25 | Loss: 0.00131287
Iteration 11/25 | Loss: 0.00131287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013128716964274645, 0.0013128716964274645, 0.0013128716964274645, 0.0013128716964274645, 0.0013128716964274645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013128716964274645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46048844
Iteration 2/25 | Loss: 0.00107783
Iteration 3/25 | Loss: 0.00107779
Iteration 4/25 | Loss: 0.00107779
Iteration 5/25 | Loss: 0.00107779
Iteration 6/25 | Loss: 0.00107779
Iteration 7/25 | Loss: 0.00107779
Iteration 8/25 | Loss: 0.00107779
Iteration 9/25 | Loss: 0.00107779
Iteration 10/25 | Loss: 0.00107779
Iteration 11/25 | Loss: 0.00107779
Iteration 12/25 | Loss: 0.00107779
Iteration 13/25 | Loss: 0.00107779
Iteration 14/25 | Loss: 0.00107779
Iteration 15/25 | Loss: 0.00107779
Iteration 16/25 | Loss: 0.00107779
Iteration 17/25 | Loss: 0.00107779
Iteration 18/25 | Loss: 0.00107779
Iteration 19/25 | Loss: 0.00107779
Iteration 20/25 | Loss: 0.00107779
Iteration 21/25 | Loss: 0.00107779
Iteration 22/25 | Loss: 0.00107779
Iteration 23/25 | Loss: 0.00107779
Iteration 24/25 | Loss: 0.00107779
Iteration 25/25 | Loss: 0.00107779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107779
Iteration 2/1000 | Loss: 0.00003933
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002203
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001780
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001672
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001649
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001647
Iteration 33/1000 | Loss: 0.00001647
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001638
Iteration 86/1000 | Loss: 0.00001638
Iteration 87/1000 | Loss: 0.00001638
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001638
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001638
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001638
Iteration 99/1000 | Loss: 0.00001638
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001638
Iteration 102/1000 | Loss: 0.00001638
Iteration 103/1000 | Loss: 0.00001638
Iteration 104/1000 | Loss: 0.00001638
Iteration 105/1000 | Loss: 0.00001638
Iteration 106/1000 | Loss: 0.00001638
Iteration 107/1000 | Loss: 0.00001638
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001638
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.638356116018258e-05, 1.638356116018258e-05, 1.638356116018258e-05, 1.638356116018258e-05, 1.638356116018258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.638356116018258e-05

Optimization complete. Final v2v error: 3.4069902896881104 mm

Highest mean error: 3.955735445022583 mm for frame 69

Lowest mean error: 2.8820536136627197 mm for frame 93

Saving results

Total time: 32.30150032043457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789428
Iteration 2/25 | Loss: 0.00189165
Iteration 3/25 | Loss: 0.00140036
Iteration 4/25 | Loss: 0.00136329
Iteration 5/25 | Loss: 0.00136010
Iteration 6/25 | Loss: 0.00135986
Iteration 7/25 | Loss: 0.00135986
Iteration 8/25 | Loss: 0.00135986
Iteration 9/25 | Loss: 0.00135986
Iteration 10/25 | Loss: 0.00135986
Iteration 11/25 | Loss: 0.00135986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001359855756163597, 0.001359855756163597, 0.001359855756163597, 0.001359855756163597, 0.001359855756163597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001359855756163597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22839653
Iteration 2/25 | Loss: 0.00074628
Iteration 3/25 | Loss: 0.00074625
Iteration 4/25 | Loss: 0.00074625
Iteration 5/25 | Loss: 0.00074625
Iteration 6/25 | Loss: 0.00074625
Iteration 7/25 | Loss: 0.00074625
Iteration 8/25 | Loss: 0.00074625
Iteration 9/25 | Loss: 0.00074625
Iteration 10/25 | Loss: 0.00074625
Iteration 11/25 | Loss: 0.00074625
Iteration 12/25 | Loss: 0.00074625
Iteration 13/25 | Loss: 0.00074625
Iteration 14/25 | Loss: 0.00074625
Iteration 15/25 | Loss: 0.00074625
Iteration 16/25 | Loss: 0.00074625
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007462523644790053, 0.0007462523644790053, 0.0007462523644790053, 0.0007462523644790053, 0.0007462523644790053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007462523644790053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074625
Iteration 2/1000 | Loss: 0.00004084
Iteration 3/1000 | Loss: 0.00002895
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002562
Iteration 6/1000 | Loss: 0.00002454
Iteration 7/1000 | Loss: 0.00002412
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002317
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002248
Iteration 13/1000 | Loss: 0.00002247
Iteration 14/1000 | Loss: 0.00002232
Iteration 15/1000 | Loss: 0.00002224
Iteration 16/1000 | Loss: 0.00002222
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002217
Iteration 20/1000 | Loss: 0.00002216
Iteration 21/1000 | Loss: 0.00002216
Iteration 22/1000 | Loss: 0.00002215
Iteration 23/1000 | Loss: 0.00002213
Iteration 24/1000 | Loss: 0.00002213
Iteration 25/1000 | Loss: 0.00002213
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002212
Iteration 28/1000 | Loss: 0.00002212
Iteration 29/1000 | Loss: 0.00002212
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002210
Iteration 33/1000 | Loss: 0.00002210
Iteration 34/1000 | Loss: 0.00002209
Iteration 35/1000 | Loss: 0.00002209
Iteration 36/1000 | Loss: 0.00002209
Iteration 37/1000 | Loss: 0.00002208
Iteration 38/1000 | Loss: 0.00002208
Iteration 39/1000 | Loss: 0.00002208
Iteration 40/1000 | Loss: 0.00002208
Iteration 41/1000 | Loss: 0.00002208
Iteration 42/1000 | Loss: 0.00002206
Iteration 43/1000 | Loss: 0.00002206
Iteration 44/1000 | Loss: 0.00002205
Iteration 45/1000 | Loss: 0.00002205
Iteration 46/1000 | Loss: 0.00002205
Iteration 47/1000 | Loss: 0.00002205
Iteration 48/1000 | Loss: 0.00002205
Iteration 49/1000 | Loss: 0.00002204
Iteration 50/1000 | Loss: 0.00002204
Iteration 51/1000 | Loss: 0.00002204
Iteration 52/1000 | Loss: 0.00002203
Iteration 53/1000 | Loss: 0.00002202
Iteration 54/1000 | Loss: 0.00002202
Iteration 55/1000 | Loss: 0.00002202
Iteration 56/1000 | Loss: 0.00002202
Iteration 57/1000 | Loss: 0.00002201
Iteration 58/1000 | Loss: 0.00002201
Iteration 59/1000 | Loss: 0.00002201
Iteration 60/1000 | Loss: 0.00002201
Iteration 61/1000 | Loss: 0.00002201
Iteration 62/1000 | Loss: 0.00002201
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002201
Iteration 65/1000 | Loss: 0.00002200
Iteration 66/1000 | Loss: 0.00002200
Iteration 67/1000 | Loss: 0.00002199
Iteration 68/1000 | Loss: 0.00002198
Iteration 69/1000 | Loss: 0.00002198
Iteration 70/1000 | Loss: 0.00002198
Iteration 71/1000 | Loss: 0.00002198
Iteration 72/1000 | Loss: 0.00002198
Iteration 73/1000 | Loss: 0.00002198
Iteration 74/1000 | Loss: 0.00002198
Iteration 75/1000 | Loss: 0.00002197
Iteration 76/1000 | Loss: 0.00002197
Iteration 77/1000 | Loss: 0.00002197
Iteration 78/1000 | Loss: 0.00002197
Iteration 79/1000 | Loss: 0.00002196
Iteration 80/1000 | Loss: 0.00002196
Iteration 81/1000 | Loss: 0.00002196
Iteration 82/1000 | Loss: 0.00002196
Iteration 83/1000 | Loss: 0.00002196
Iteration 84/1000 | Loss: 0.00002196
Iteration 85/1000 | Loss: 0.00002196
Iteration 86/1000 | Loss: 0.00002196
Iteration 87/1000 | Loss: 0.00002196
Iteration 88/1000 | Loss: 0.00002196
Iteration 89/1000 | Loss: 0.00002196
Iteration 90/1000 | Loss: 0.00002195
Iteration 91/1000 | Loss: 0.00002195
Iteration 92/1000 | Loss: 0.00002195
Iteration 93/1000 | Loss: 0.00002195
Iteration 94/1000 | Loss: 0.00002195
Iteration 95/1000 | Loss: 0.00002195
Iteration 96/1000 | Loss: 0.00002195
Iteration 97/1000 | Loss: 0.00002195
Iteration 98/1000 | Loss: 0.00002195
Iteration 99/1000 | Loss: 0.00002195
Iteration 100/1000 | Loss: 0.00002195
Iteration 101/1000 | Loss: 0.00002195
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002195
Iteration 104/1000 | Loss: 0.00002195
Iteration 105/1000 | Loss: 0.00002195
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002194
Iteration 114/1000 | Loss: 0.00002194
Iteration 115/1000 | Loss: 0.00002194
Iteration 116/1000 | Loss: 0.00002194
Iteration 117/1000 | Loss: 0.00002194
Iteration 118/1000 | Loss: 0.00002194
Iteration 119/1000 | Loss: 0.00002194
Iteration 120/1000 | Loss: 0.00002194
Iteration 121/1000 | Loss: 0.00002194
Iteration 122/1000 | Loss: 0.00002194
Iteration 123/1000 | Loss: 0.00002194
Iteration 124/1000 | Loss: 0.00002194
Iteration 125/1000 | Loss: 0.00002194
Iteration 126/1000 | Loss: 0.00002194
Iteration 127/1000 | Loss: 0.00002194
Iteration 128/1000 | Loss: 0.00002194
Iteration 129/1000 | Loss: 0.00002194
Iteration 130/1000 | Loss: 0.00002194
Iteration 131/1000 | Loss: 0.00002194
Iteration 132/1000 | Loss: 0.00002194
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002194
Iteration 135/1000 | Loss: 0.00002194
Iteration 136/1000 | Loss: 0.00002194
Iteration 137/1000 | Loss: 0.00002194
Iteration 138/1000 | Loss: 0.00002194
Iteration 139/1000 | Loss: 0.00002194
Iteration 140/1000 | Loss: 0.00002194
Iteration 141/1000 | Loss: 0.00002194
Iteration 142/1000 | Loss: 0.00002194
Iteration 143/1000 | Loss: 0.00002194
Iteration 144/1000 | Loss: 0.00002194
Iteration 145/1000 | Loss: 0.00002194
Iteration 146/1000 | Loss: 0.00002194
Iteration 147/1000 | Loss: 0.00002194
Iteration 148/1000 | Loss: 0.00002194
Iteration 149/1000 | Loss: 0.00002194
Iteration 150/1000 | Loss: 0.00002194
Iteration 151/1000 | Loss: 0.00002194
Iteration 152/1000 | Loss: 0.00002194
Iteration 153/1000 | Loss: 0.00002194
Iteration 154/1000 | Loss: 0.00002194
Iteration 155/1000 | Loss: 0.00002194
Iteration 156/1000 | Loss: 0.00002194
Iteration 157/1000 | Loss: 0.00002194
Iteration 158/1000 | Loss: 0.00002194
Iteration 159/1000 | Loss: 0.00002194
Iteration 160/1000 | Loss: 0.00002194
Iteration 161/1000 | Loss: 0.00002194
Iteration 162/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.193615000578575e-05, 2.193615000578575e-05, 2.193615000578575e-05, 2.193615000578575e-05, 2.193615000578575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.193615000578575e-05

Optimization complete. Final v2v error: 3.9274086952209473 mm

Highest mean error: 4.304864883422852 mm for frame 120

Lowest mean error: 3.707754373550415 mm for frame 207

Saving results

Total time: 39.73827338218689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997993
Iteration 2/25 | Loss: 0.00254311
Iteration 3/25 | Loss: 0.00190221
Iteration 4/25 | Loss: 0.00170703
Iteration 5/25 | Loss: 0.00157342
Iteration 6/25 | Loss: 0.00151154
Iteration 7/25 | Loss: 0.00149782
Iteration 8/25 | Loss: 0.00144762
Iteration 9/25 | Loss: 0.00142792
Iteration 10/25 | Loss: 0.00141958
Iteration 11/25 | Loss: 0.00141395
Iteration 12/25 | Loss: 0.00142752
Iteration 13/25 | Loss: 0.00141478
Iteration 14/25 | Loss: 0.00141180
Iteration 15/25 | Loss: 0.00141164
Iteration 16/25 | Loss: 0.00140772
Iteration 17/25 | Loss: 0.00140704
Iteration 18/25 | Loss: 0.00140699
Iteration 19/25 | Loss: 0.00140699
Iteration 20/25 | Loss: 0.00140699
Iteration 21/25 | Loss: 0.00140699
Iteration 22/25 | Loss: 0.00140698
Iteration 23/25 | Loss: 0.00140698
Iteration 24/25 | Loss: 0.00140698
Iteration 25/25 | Loss: 0.00140698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39421654
Iteration 2/25 | Loss: 0.00174350
Iteration 3/25 | Loss: 0.00171987
Iteration 4/25 | Loss: 0.00171987
Iteration 5/25 | Loss: 0.00171987
Iteration 6/25 | Loss: 0.00171987
Iteration 7/25 | Loss: 0.00171987
Iteration 8/25 | Loss: 0.00171987
Iteration 9/25 | Loss: 0.00171987
Iteration 10/25 | Loss: 0.00171987
Iteration 11/25 | Loss: 0.00171987
Iteration 12/25 | Loss: 0.00171987
Iteration 13/25 | Loss: 0.00171987
Iteration 14/25 | Loss: 0.00171987
Iteration 15/25 | Loss: 0.00171987
Iteration 16/25 | Loss: 0.00171987
Iteration 17/25 | Loss: 0.00171987
Iteration 18/25 | Loss: 0.00171987
Iteration 19/25 | Loss: 0.00171987
Iteration 20/25 | Loss: 0.00171987
Iteration 21/25 | Loss: 0.00171987
Iteration 22/25 | Loss: 0.00171987
Iteration 23/25 | Loss: 0.00171987
Iteration 24/25 | Loss: 0.00171987
Iteration 25/25 | Loss: 0.00171987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017198658315464854, 0.0017198658315464854, 0.0017198658315464854, 0.0017198658315464854, 0.0017198658315464854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017198658315464854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171987
Iteration 2/1000 | Loss: 0.00024071
Iteration 3/1000 | Loss: 0.00020057
Iteration 4/1000 | Loss: 0.00068927
Iteration 5/1000 | Loss: 0.00009026
Iteration 6/1000 | Loss: 0.00024264
Iteration 7/1000 | Loss: 0.00007498
Iteration 8/1000 | Loss: 0.00028445
Iteration 9/1000 | Loss: 0.00010626
Iteration 10/1000 | Loss: 0.00020682
Iteration 11/1000 | Loss: 0.00006390
Iteration 12/1000 | Loss: 0.00013888
Iteration 13/1000 | Loss: 0.00006133
Iteration 14/1000 | Loss: 0.00023799
Iteration 15/1000 | Loss: 0.00042907
Iteration 16/1000 | Loss: 0.00024069
Iteration 17/1000 | Loss: 0.00013809
Iteration 18/1000 | Loss: 0.00005963
Iteration 19/1000 | Loss: 0.00005820
Iteration 20/1000 | Loss: 0.00020633
Iteration 21/1000 | Loss: 0.00020929
Iteration 22/1000 | Loss: 0.00006463
Iteration 23/1000 | Loss: 0.00008739
Iteration 24/1000 | Loss: 0.00056814
Iteration 25/1000 | Loss: 0.00032572
Iteration 26/1000 | Loss: 0.00065899
Iteration 27/1000 | Loss: 0.00384072
Iteration 28/1000 | Loss: 0.00049299
Iteration 29/1000 | Loss: 0.00021506
Iteration 30/1000 | Loss: 0.00028516
Iteration 31/1000 | Loss: 0.00194363
Iteration 32/1000 | Loss: 0.00032189
Iteration 33/1000 | Loss: 0.00011809
Iteration 34/1000 | Loss: 0.00030279
Iteration 35/1000 | Loss: 0.00018814
Iteration 36/1000 | Loss: 0.00038553
Iteration 37/1000 | Loss: 0.00007460
Iteration 38/1000 | Loss: 0.00010901
Iteration 39/1000 | Loss: 0.00003366
Iteration 40/1000 | Loss: 0.00015456
Iteration 41/1000 | Loss: 0.00021296
Iteration 42/1000 | Loss: 0.00006416
Iteration 43/1000 | Loss: 0.00004422
Iteration 44/1000 | Loss: 0.00004370
Iteration 45/1000 | Loss: 0.00003892
Iteration 46/1000 | Loss: 0.00007124
Iteration 47/1000 | Loss: 0.00004575
Iteration 48/1000 | Loss: 0.00006090
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002597
Iteration 51/1000 | Loss: 0.00021223
Iteration 52/1000 | Loss: 0.00002539
Iteration 53/1000 | Loss: 0.00013956
Iteration 54/1000 | Loss: 0.00003933
Iteration 55/1000 | Loss: 0.00004890
Iteration 56/1000 | Loss: 0.00003288
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00010962
Iteration 59/1000 | Loss: 0.00005693
Iteration 60/1000 | Loss: 0.00001895
Iteration 61/1000 | Loss: 0.00018170
Iteration 62/1000 | Loss: 0.00003712
Iteration 63/1000 | Loss: 0.00002506
Iteration 64/1000 | Loss: 0.00001846
Iteration 65/1000 | Loss: 0.00006524
Iteration 66/1000 | Loss: 0.00010707
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00002253
Iteration 69/1000 | Loss: 0.00001815
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00002414
Iteration 73/1000 | Loss: 0.00001812
Iteration 74/1000 | Loss: 0.00001806
Iteration 75/1000 | Loss: 0.00001806
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001806
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001803
Iteration 83/1000 | Loss: 0.00001800
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001798
Iteration 86/1000 | Loss: 0.00001798
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001795
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001794
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001793
Iteration 108/1000 | Loss: 0.00005106
Iteration 109/1000 | Loss: 0.00002223
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00006148
Iteration 113/1000 | Loss: 0.00002224
Iteration 114/1000 | Loss: 0.00007544
Iteration 115/1000 | Loss: 0.00002528
Iteration 116/1000 | Loss: 0.00004550
Iteration 117/1000 | Loss: 0.00002362
Iteration 118/1000 | Loss: 0.00003675
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00003601
Iteration 121/1000 | Loss: 0.00002263
Iteration 122/1000 | Loss: 0.00002391
Iteration 123/1000 | Loss: 0.00001799
Iteration 124/1000 | Loss: 0.00001797
Iteration 125/1000 | Loss: 0.00001797
Iteration 126/1000 | Loss: 0.00001797
Iteration 127/1000 | Loss: 0.00001797
Iteration 128/1000 | Loss: 0.00001797
Iteration 129/1000 | Loss: 0.00001797
Iteration 130/1000 | Loss: 0.00001797
Iteration 131/1000 | Loss: 0.00001797
Iteration 132/1000 | Loss: 0.00001797
Iteration 133/1000 | Loss: 0.00001796
Iteration 134/1000 | Loss: 0.00001796
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001796
Iteration 137/1000 | Loss: 0.00001796
Iteration 138/1000 | Loss: 0.00001795
Iteration 139/1000 | Loss: 0.00001795
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001794
Iteration 142/1000 | Loss: 0.00001794
Iteration 143/1000 | Loss: 0.00001794
Iteration 144/1000 | Loss: 0.00001794
Iteration 145/1000 | Loss: 0.00001794
Iteration 146/1000 | Loss: 0.00001794
Iteration 147/1000 | Loss: 0.00001794
Iteration 148/1000 | Loss: 0.00001793
Iteration 149/1000 | Loss: 0.00001793
Iteration 150/1000 | Loss: 0.00001793
Iteration 151/1000 | Loss: 0.00001792
Iteration 152/1000 | Loss: 0.00002780
Iteration 153/1000 | Loss: 0.00007157
Iteration 154/1000 | Loss: 0.00003574
Iteration 155/1000 | Loss: 0.00001818
Iteration 156/1000 | Loss: 0.00001795
Iteration 157/1000 | Loss: 0.00001795
Iteration 158/1000 | Loss: 0.00001795
Iteration 159/1000 | Loss: 0.00001795
Iteration 160/1000 | Loss: 0.00001795
Iteration 161/1000 | Loss: 0.00001795
Iteration 162/1000 | Loss: 0.00001795
Iteration 163/1000 | Loss: 0.00001842
Iteration 164/1000 | Loss: 0.00001801
Iteration 165/1000 | Loss: 0.00001792
Iteration 166/1000 | Loss: 0.00001791
Iteration 167/1000 | Loss: 0.00001791
Iteration 168/1000 | Loss: 0.00001791
Iteration 169/1000 | Loss: 0.00001791
Iteration 170/1000 | Loss: 0.00001791
Iteration 171/1000 | Loss: 0.00001791
Iteration 172/1000 | Loss: 0.00001791
Iteration 173/1000 | Loss: 0.00001791
Iteration 174/1000 | Loss: 0.00003836
Iteration 175/1000 | Loss: 0.00001876
Iteration 176/1000 | Loss: 0.00001859
Iteration 177/1000 | Loss: 0.00003999
Iteration 178/1000 | Loss: 0.00001791
Iteration 179/1000 | Loss: 0.00001788
Iteration 180/1000 | Loss: 0.00001788
Iteration 181/1000 | Loss: 0.00001788
Iteration 182/1000 | Loss: 0.00001788
Iteration 183/1000 | Loss: 0.00001788
Iteration 184/1000 | Loss: 0.00001787
Iteration 185/1000 | Loss: 0.00001787
Iteration 186/1000 | Loss: 0.00001787
Iteration 187/1000 | Loss: 0.00001787
Iteration 188/1000 | Loss: 0.00001787
Iteration 189/1000 | Loss: 0.00001786
Iteration 190/1000 | Loss: 0.00001786
Iteration 191/1000 | Loss: 0.00001786
Iteration 192/1000 | Loss: 0.00001785
Iteration 193/1000 | Loss: 0.00001785
Iteration 194/1000 | Loss: 0.00001785
Iteration 195/1000 | Loss: 0.00001785
Iteration 196/1000 | Loss: 0.00001785
Iteration 197/1000 | Loss: 0.00001785
Iteration 198/1000 | Loss: 0.00001784
Iteration 199/1000 | Loss: 0.00001784
Iteration 200/1000 | Loss: 0.00001784
Iteration 201/1000 | Loss: 0.00001784
Iteration 202/1000 | Loss: 0.00001784
Iteration 203/1000 | Loss: 0.00001784
Iteration 204/1000 | Loss: 0.00001784
Iteration 205/1000 | Loss: 0.00001784
Iteration 206/1000 | Loss: 0.00001784
Iteration 207/1000 | Loss: 0.00001784
Iteration 208/1000 | Loss: 0.00001783
Iteration 209/1000 | Loss: 0.00001783
Iteration 210/1000 | Loss: 0.00001783
Iteration 211/1000 | Loss: 0.00001783
Iteration 212/1000 | Loss: 0.00001783
Iteration 213/1000 | Loss: 0.00001783
Iteration 214/1000 | Loss: 0.00001783
Iteration 215/1000 | Loss: 0.00001783
Iteration 216/1000 | Loss: 0.00001783
Iteration 217/1000 | Loss: 0.00001783
Iteration 218/1000 | Loss: 0.00001783
Iteration 219/1000 | Loss: 0.00001783
Iteration 220/1000 | Loss: 0.00001783
Iteration 221/1000 | Loss: 0.00001783
Iteration 222/1000 | Loss: 0.00001783
Iteration 223/1000 | Loss: 0.00001783
Iteration 224/1000 | Loss: 0.00001783
Iteration 225/1000 | Loss: 0.00001783
Iteration 226/1000 | Loss: 0.00001783
Iteration 227/1000 | Loss: 0.00001782
Iteration 228/1000 | Loss: 0.00001782
Iteration 229/1000 | Loss: 0.00001782
Iteration 230/1000 | Loss: 0.00001782
Iteration 231/1000 | Loss: 0.00001782
Iteration 232/1000 | Loss: 0.00001782
Iteration 233/1000 | Loss: 0.00001782
Iteration 234/1000 | Loss: 0.00001782
Iteration 235/1000 | Loss: 0.00001782
Iteration 236/1000 | Loss: 0.00001782
Iteration 237/1000 | Loss: 0.00001781
Iteration 238/1000 | Loss: 0.00001781
Iteration 239/1000 | Loss: 0.00001781
Iteration 240/1000 | Loss: 0.00001781
Iteration 241/1000 | Loss: 0.00001781
Iteration 242/1000 | Loss: 0.00001781
Iteration 243/1000 | Loss: 0.00001781
Iteration 244/1000 | Loss: 0.00001781
Iteration 245/1000 | Loss: 0.00001781
Iteration 246/1000 | Loss: 0.00001781
Iteration 247/1000 | Loss: 0.00001781
Iteration 248/1000 | Loss: 0.00001781
Iteration 249/1000 | Loss: 0.00001781
Iteration 250/1000 | Loss: 0.00001781
Iteration 251/1000 | Loss: 0.00001781
Iteration 252/1000 | Loss: 0.00001780
Iteration 253/1000 | Loss: 0.00001780
Iteration 254/1000 | Loss: 0.00001780
Iteration 255/1000 | Loss: 0.00001780
Iteration 256/1000 | Loss: 0.00001780
Iteration 257/1000 | Loss: 0.00001780
Iteration 258/1000 | Loss: 0.00001780
Iteration 259/1000 | Loss: 0.00001780
Iteration 260/1000 | Loss: 0.00001780
Iteration 261/1000 | Loss: 0.00001780
Iteration 262/1000 | Loss: 0.00001780
Iteration 263/1000 | Loss: 0.00001780
Iteration 264/1000 | Loss: 0.00001780
Iteration 265/1000 | Loss: 0.00001780
Iteration 266/1000 | Loss: 0.00001780
Iteration 267/1000 | Loss: 0.00001779
Iteration 268/1000 | Loss: 0.00001779
Iteration 269/1000 | Loss: 0.00001779
Iteration 270/1000 | Loss: 0.00001779
Iteration 271/1000 | Loss: 0.00001779
Iteration 272/1000 | Loss: 0.00001779
Iteration 273/1000 | Loss: 0.00001779
Iteration 274/1000 | Loss: 0.00001779
Iteration 275/1000 | Loss: 0.00001779
Iteration 276/1000 | Loss: 0.00001779
Iteration 277/1000 | Loss: 0.00001779
Iteration 278/1000 | Loss: 0.00001779
Iteration 279/1000 | Loss: 0.00001779
Iteration 280/1000 | Loss: 0.00001779
Iteration 281/1000 | Loss: 0.00001779
Iteration 282/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.7787047909223475e-05, 1.7787047909223475e-05, 1.7787047909223475e-05, 1.7787047909223475e-05, 1.7787047909223475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7787047909223475e-05

Optimization complete. Final v2v error: 3.5355257987976074 mm

Highest mean error: 3.9179646968841553 mm for frame 81

Lowest mean error: 3.1591744422912598 mm for frame 65

Saving results

Total time: 161.57282638549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790165
Iteration 2/25 | Loss: 0.00139335
Iteration 3/25 | Loss: 0.00129209
Iteration 4/25 | Loss: 0.00127796
Iteration 5/25 | Loss: 0.00127408
Iteration 6/25 | Loss: 0.00127340
Iteration 7/25 | Loss: 0.00127341
Iteration 8/25 | Loss: 0.00127340
Iteration 9/25 | Loss: 0.00127341
Iteration 10/25 | Loss: 0.00127340
Iteration 11/25 | Loss: 0.00127340
Iteration 12/25 | Loss: 0.00127341
Iteration 13/25 | Loss: 0.00127341
Iteration 14/25 | Loss: 0.00127340
Iteration 15/25 | Loss: 0.00127340
Iteration 16/25 | Loss: 0.00127340
Iteration 17/25 | Loss: 0.00127341
Iteration 18/25 | Loss: 0.00127340
Iteration 19/25 | Loss: 0.00127340
Iteration 20/25 | Loss: 0.00127340
Iteration 21/25 | Loss: 0.00127341
Iteration 22/25 | Loss: 0.00127341
Iteration 23/25 | Loss: 0.00127340
Iteration 24/25 | Loss: 0.00127340
Iteration 25/25 | Loss: 0.00127341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46076035
Iteration 2/25 | Loss: 0.00093905
Iteration 3/25 | Loss: 0.00093904
Iteration 4/25 | Loss: 0.00093904
Iteration 5/25 | Loss: 0.00093904
Iteration 6/25 | Loss: 0.00093904
Iteration 7/25 | Loss: 0.00093904
Iteration 8/25 | Loss: 0.00093904
Iteration 9/25 | Loss: 0.00093904
Iteration 10/25 | Loss: 0.00093904
Iteration 11/25 | Loss: 0.00093904
Iteration 12/25 | Loss: 0.00093904
Iteration 13/25 | Loss: 0.00093904
Iteration 14/25 | Loss: 0.00093904
Iteration 15/25 | Loss: 0.00093904
Iteration 16/25 | Loss: 0.00093904
Iteration 17/25 | Loss: 0.00093904
Iteration 18/25 | Loss: 0.00093904
Iteration 19/25 | Loss: 0.00093904
Iteration 20/25 | Loss: 0.00093904
Iteration 21/25 | Loss: 0.00093904
Iteration 22/25 | Loss: 0.00093904
Iteration 23/25 | Loss: 0.00093904
Iteration 24/25 | Loss: 0.00093904
Iteration 25/25 | Loss: 0.00093904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093904
Iteration 2/1000 | Loss: 0.00003898
Iteration 3/1000 | Loss: 0.00002573
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002177
Iteration 6/1000 | Loss: 0.00002080
Iteration 7/1000 | Loss: 0.00002028
Iteration 8/1000 | Loss: 0.00001982
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001892
Iteration 12/1000 | Loss: 0.00001872
Iteration 13/1000 | Loss: 0.00001872
Iteration 14/1000 | Loss: 0.00001860
Iteration 15/1000 | Loss: 0.00001859
Iteration 16/1000 | Loss: 0.00001853
Iteration 17/1000 | Loss: 0.00001843
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001824
Iteration 23/1000 | Loss: 0.00001823
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001820
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001818
Iteration 31/1000 | Loss: 0.00001818
Iteration 32/1000 | Loss: 0.00001817
Iteration 33/1000 | Loss: 0.00001816
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001815
Iteration 37/1000 | Loss: 0.00001815
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001814
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001813
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00001812
Iteration 47/1000 | Loss: 0.00001812
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001809
Iteration 51/1000 | Loss: 0.00001809
Iteration 52/1000 | Loss: 0.00001809
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001809
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.808720662666019e-05, 1.808720662666019e-05, 1.808720662666019e-05, 1.808720662666019e-05, 1.808720662666019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.808720662666019e-05

Optimization complete. Final v2v error: 3.5710484981536865 mm

Highest mean error: 4.437306880950928 mm for frame 121

Lowest mean error: 2.8518848419189453 mm for frame 145

Saving results

Total time: 32.60248327255249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445404
Iteration 2/25 | Loss: 0.00129849
Iteration 3/25 | Loss: 0.00124131
Iteration 4/25 | Loss: 0.00123240
Iteration 5/25 | Loss: 0.00122964
Iteration 6/25 | Loss: 0.00122964
Iteration 7/25 | Loss: 0.00122964
Iteration 8/25 | Loss: 0.00122964
Iteration 9/25 | Loss: 0.00122964
Iteration 10/25 | Loss: 0.00122964
Iteration 11/25 | Loss: 0.00122964
Iteration 12/25 | Loss: 0.00122964
Iteration 13/25 | Loss: 0.00122964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012296391651034355, 0.0012296391651034355, 0.0012296391651034355, 0.0012296391651034355, 0.0012296391651034355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012296391651034355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36673117
Iteration 2/25 | Loss: 0.00104031
Iteration 3/25 | Loss: 0.00104031
Iteration 4/25 | Loss: 0.00104031
Iteration 5/25 | Loss: 0.00104031
Iteration 6/25 | Loss: 0.00104031
Iteration 7/25 | Loss: 0.00104031
Iteration 8/25 | Loss: 0.00104031
Iteration 9/25 | Loss: 0.00104031
Iteration 10/25 | Loss: 0.00104031
Iteration 11/25 | Loss: 0.00104030
Iteration 12/25 | Loss: 0.00104030
Iteration 13/25 | Loss: 0.00104030
Iteration 14/25 | Loss: 0.00104030
Iteration 15/25 | Loss: 0.00104030
Iteration 16/25 | Loss: 0.00104030
Iteration 17/25 | Loss: 0.00104030
Iteration 18/25 | Loss: 0.00104030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010403045453131199, 0.0010403045453131199, 0.0010403045453131199, 0.0010403045453131199, 0.0010403045453131199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010403045453131199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104030
Iteration 2/1000 | Loss: 0.00002227
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001574
Iteration 5/1000 | Loss: 0.00001501
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001414
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001368
Iteration 13/1000 | Loss: 0.00001367
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001320
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00001311
Iteration 34/1000 | Loss: 0.00001311
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001296
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001284
Iteration 73/1000 | Loss: 0.00001284
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001280
Iteration 90/1000 | Loss: 0.00001280
Iteration 91/1000 | Loss: 0.00001280
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001278
Iteration 98/1000 | Loss: 0.00001278
Iteration 99/1000 | Loss: 0.00001278
Iteration 100/1000 | Loss: 0.00001278
Iteration 101/1000 | Loss: 0.00001278
Iteration 102/1000 | Loss: 0.00001277
Iteration 103/1000 | Loss: 0.00001277
Iteration 104/1000 | Loss: 0.00001277
Iteration 105/1000 | Loss: 0.00001276
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001276
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.276312832487747e-05, 1.276312832487747e-05, 1.276312832487747e-05, 1.276312832487747e-05, 1.276312832487747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.276312832487747e-05

Optimization complete. Final v2v error: 3.0524239540100098 mm

Highest mean error: 3.162202835083008 mm for frame 115

Lowest mean error: 2.9445366859436035 mm for frame 190

Saving results

Total time: 35.592830657958984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526748
Iteration 2/25 | Loss: 0.00142975
Iteration 3/25 | Loss: 0.00130523
Iteration 4/25 | Loss: 0.00129595
Iteration 5/25 | Loss: 0.00129488
Iteration 6/25 | Loss: 0.00129488
Iteration 7/25 | Loss: 0.00129488
Iteration 8/25 | Loss: 0.00129488
Iteration 9/25 | Loss: 0.00129488
Iteration 10/25 | Loss: 0.00129488
Iteration 11/25 | Loss: 0.00129488
Iteration 12/25 | Loss: 0.00129488
Iteration 13/25 | Loss: 0.00129488
Iteration 14/25 | Loss: 0.00129488
Iteration 15/25 | Loss: 0.00129488
Iteration 16/25 | Loss: 0.00129488
Iteration 17/25 | Loss: 0.00129488
Iteration 18/25 | Loss: 0.00129488
Iteration 19/25 | Loss: 0.00129488
Iteration 20/25 | Loss: 0.00129488
Iteration 21/25 | Loss: 0.00129488
Iteration 22/25 | Loss: 0.00129488
Iteration 23/25 | Loss: 0.00129488
Iteration 24/25 | Loss: 0.00129488
Iteration 25/25 | Loss: 0.00129488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35201859
Iteration 2/25 | Loss: 0.00076141
Iteration 3/25 | Loss: 0.00076141
Iteration 4/25 | Loss: 0.00076141
Iteration 5/25 | Loss: 0.00076141
Iteration 6/25 | Loss: 0.00076141
Iteration 7/25 | Loss: 0.00076141
Iteration 8/25 | Loss: 0.00076141
Iteration 9/25 | Loss: 0.00076141
Iteration 10/25 | Loss: 0.00076141
Iteration 11/25 | Loss: 0.00076141
Iteration 12/25 | Loss: 0.00076141
Iteration 13/25 | Loss: 0.00076141
Iteration 14/25 | Loss: 0.00076141
Iteration 15/25 | Loss: 0.00076141
Iteration 16/25 | Loss: 0.00076141
Iteration 17/25 | Loss: 0.00076141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007614055648446083, 0.0007614055648446083, 0.0007614055648446083, 0.0007614055648446083, 0.0007614055648446083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007614055648446083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076141
Iteration 2/1000 | Loss: 0.00003880
Iteration 3/1000 | Loss: 0.00002610
Iteration 4/1000 | Loss: 0.00002382
Iteration 5/1000 | Loss: 0.00002280
Iteration 6/1000 | Loss: 0.00002204
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002033
Iteration 11/1000 | Loss: 0.00002031
Iteration 12/1000 | Loss: 0.00002022
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00002015
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002002
Iteration 19/1000 | Loss: 0.00002002
Iteration 20/1000 | Loss: 0.00001997
Iteration 21/1000 | Loss: 0.00001997
Iteration 22/1000 | Loss: 0.00001997
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001995
Iteration 26/1000 | Loss: 0.00001995
Iteration 27/1000 | Loss: 0.00001994
Iteration 28/1000 | Loss: 0.00001994
Iteration 29/1000 | Loss: 0.00001994
Iteration 30/1000 | Loss: 0.00001993
Iteration 31/1000 | Loss: 0.00001993
Iteration 32/1000 | Loss: 0.00001993
Iteration 33/1000 | Loss: 0.00001993
Iteration 34/1000 | Loss: 0.00001992
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001992
Iteration 37/1000 | Loss: 0.00001991
Iteration 38/1000 | Loss: 0.00001991
Iteration 39/1000 | Loss: 0.00001991
Iteration 40/1000 | Loss: 0.00001991
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001989
Iteration 43/1000 | Loss: 0.00001989
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001987
Iteration 51/1000 | Loss: 0.00001986
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001985
Iteration 54/1000 | Loss: 0.00001985
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001982
Iteration 61/1000 | Loss: 0.00001982
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001981
Iteration 65/1000 | Loss: 0.00001981
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001980
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001979
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001979
Iteration 72/1000 | Loss: 0.00001979
Iteration 73/1000 | Loss: 0.00001979
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001978
Iteration 77/1000 | Loss: 0.00001978
Iteration 78/1000 | Loss: 0.00001978
Iteration 79/1000 | Loss: 0.00001978
Iteration 80/1000 | Loss: 0.00001978
Iteration 81/1000 | Loss: 0.00001978
Iteration 82/1000 | Loss: 0.00001978
Iteration 83/1000 | Loss: 0.00001977
Iteration 84/1000 | Loss: 0.00001977
Iteration 85/1000 | Loss: 0.00001977
Iteration 86/1000 | Loss: 0.00001977
Iteration 87/1000 | Loss: 0.00001977
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001976
Iteration 95/1000 | Loss: 0.00001976
Iteration 96/1000 | Loss: 0.00001976
Iteration 97/1000 | Loss: 0.00001976
Iteration 98/1000 | Loss: 0.00001976
Iteration 99/1000 | Loss: 0.00001976
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001976
Iteration 104/1000 | Loss: 0.00001976
Iteration 105/1000 | Loss: 0.00001976
Iteration 106/1000 | Loss: 0.00001976
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001976
Iteration 120/1000 | Loss: 0.00001976
Iteration 121/1000 | Loss: 0.00001976
Iteration 122/1000 | Loss: 0.00001976
Iteration 123/1000 | Loss: 0.00001976
Iteration 124/1000 | Loss: 0.00001976
Iteration 125/1000 | Loss: 0.00001976
Iteration 126/1000 | Loss: 0.00001976
Iteration 127/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.9758183043450117e-05, 1.9758183043450117e-05, 1.9758183043450117e-05, 1.9758183043450117e-05, 1.9758183043450117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9758183043450117e-05

Optimization complete. Final v2v error: 3.7630608081817627 mm

Highest mean error: 4.134993553161621 mm for frame 162

Lowest mean error: 3.5497567653656006 mm for frame 205

Saving results

Total time: 32.74764084815979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124822
Iteration 2/25 | Loss: 0.01124822
Iteration 3/25 | Loss: 0.01124822
Iteration 4/25 | Loss: 0.01124821
Iteration 5/25 | Loss: 0.01124821
Iteration 6/25 | Loss: 0.01124821
Iteration 7/25 | Loss: 0.01124821
Iteration 8/25 | Loss: 0.01124821
Iteration 9/25 | Loss: 0.01124821
Iteration 10/25 | Loss: 0.01124821
Iteration 11/25 | Loss: 0.01124821
Iteration 12/25 | Loss: 0.01124821
Iteration 13/25 | Loss: 0.01124821
Iteration 14/25 | Loss: 0.01124821
Iteration 15/25 | Loss: 0.01124821
Iteration 16/25 | Loss: 0.01124821
Iteration 17/25 | Loss: 0.01124821
Iteration 18/25 | Loss: 0.01124821
Iteration 19/25 | Loss: 0.01124821
Iteration 20/25 | Loss: 0.01124821
Iteration 21/25 | Loss: 0.01124821
Iteration 22/25 | Loss: 0.01124821
Iteration 23/25 | Loss: 0.01124821
Iteration 24/25 | Loss: 0.01124820
Iteration 25/25 | Loss: 0.01124820

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.64259529
Iteration 2/25 | Loss: 0.18628243
Iteration 3/25 | Loss: 0.18399128
Iteration 4/25 | Loss: 0.18399125
Iteration 5/25 | Loss: 0.18399124
Iteration 6/25 | Loss: 0.18399121
Iteration 7/25 | Loss: 0.18399121
Iteration 8/25 | Loss: 0.18399121
Iteration 9/25 | Loss: 0.18399121
Iteration 10/25 | Loss: 0.18399121
Iteration 11/25 | Loss: 0.18399121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.1839912086725235, 0.1839912086725235, 0.1839912086725235, 0.1839912086725235, 0.1839912086725235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1839912086725235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18399121
Iteration 2/1000 | Loss: 0.00505067
Iteration 3/1000 | Loss: 0.00156253
Iteration 4/1000 | Loss: 0.00029366
Iteration 5/1000 | Loss: 0.00037987
Iteration 6/1000 | Loss: 0.00023440
Iteration 7/1000 | Loss: 0.00010011
Iteration 8/1000 | Loss: 0.00007865
Iteration 9/1000 | Loss: 0.00006383
Iteration 10/1000 | Loss: 0.00005425
Iteration 11/1000 | Loss: 0.00012125
Iteration 12/1000 | Loss: 0.00002784
Iteration 13/1000 | Loss: 0.00002617
Iteration 14/1000 | Loss: 0.00006753
Iteration 15/1000 | Loss: 0.00004704
Iteration 16/1000 | Loss: 0.00002877
Iteration 17/1000 | Loss: 0.00002307
Iteration 18/1000 | Loss: 0.00008360
Iteration 19/1000 | Loss: 0.00002389
Iteration 20/1000 | Loss: 0.00004081
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002314
Iteration 23/1000 | Loss: 0.00007673
Iteration 24/1000 | Loss: 0.00011068
Iteration 25/1000 | Loss: 0.00004317
Iteration 26/1000 | Loss: 0.00003411
Iteration 27/1000 | Loss: 0.00003801
Iteration 28/1000 | Loss: 0.00002495
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00011801
Iteration 31/1000 | Loss: 0.00002049
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00002239
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00009045
Iteration 39/1000 | Loss: 0.00005754
Iteration 40/1000 | Loss: 0.00004833
Iteration 41/1000 | Loss: 0.00001878
Iteration 42/1000 | Loss: 0.00003531
Iteration 43/1000 | Loss: 0.00006886
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00004719
Iteration 51/1000 | Loss: 0.00002321
Iteration 52/1000 | Loss: 0.00003980
Iteration 53/1000 | Loss: 0.00003347
Iteration 54/1000 | Loss: 0.00001756
Iteration 55/1000 | Loss: 0.00007464
Iteration 56/1000 | Loss: 0.00001764
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00004155
Iteration 61/1000 | Loss: 0.00001872
Iteration 62/1000 | Loss: 0.00001969
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00005930
Iteration 65/1000 | Loss: 0.00006118
Iteration 66/1000 | Loss: 0.00002793
Iteration 67/1000 | Loss: 0.00002644
Iteration 68/1000 | Loss: 0.00004007
Iteration 69/1000 | Loss: 0.00002522
Iteration 70/1000 | Loss: 0.00001740
Iteration 71/1000 | Loss: 0.00002805
Iteration 72/1000 | Loss: 0.00001738
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001725
Iteration 80/1000 | Loss: 0.00001725
Iteration 81/1000 | Loss: 0.00001725
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001721
Iteration 90/1000 | Loss: 0.00001721
Iteration 91/1000 | Loss: 0.00001721
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001720
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.718341081868857e-05, 1.718341081868857e-05, 1.718341081868857e-05, 1.718341081868857e-05, 1.718341081868857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718341081868857e-05

Optimization complete. Final v2v error: 3.5067827701568604 mm

Highest mean error: 3.7300140857696533 mm for frame 101

Lowest mean error: 3.2875308990478516 mm for frame 211

Saving results

Total time: 107.63142371177673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928852
Iteration 2/25 | Loss: 0.00225215
Iteration 3/25 | Loss: 0.00164368
Iteration 4/25 | Loss: 0.00149853
Iteration 5/25 | Loss: 0.00145081
Iteration 6/25 | Loss: 0.00138776
Iteration 7/25 | Loss: 0.00136476
Iteration 8/25 | Loss: 0.00135406
Iteration 9/25 | Loss: 0.00131531
Iteration 10/25 | Loss: 0.00131132
Iteration 11/25 | Loss: 0.00130827
Iteration 12/25 | Loss: 0.00130240
Iteration 13/25 | Loss: 0.00129965
Iteration 14/25 | Loss: 0.00129787
Iteration 15/25 | Loss: 0.00129661
Iteration 16/25 | Loss: 0.00129607
Iteration 17/25 | Loss: 0.00129563
Iteration 18/25 | Loss: 0.00129513
Iteration 19/25 | Loss: 0.00129342
Iteration 20/25 | Loss: 0.00129789
Iteration 21/25 | Loss: 0.00129441
Iteration 22/25 | Loss: 0.00128982
Iteration 23/25 | Loss: 0.00128863
Iteration 24/25 | Loss: 0.00128778
Iteration 25/25 | Loss: 0.00129087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41606534
Iteration 2/25 | Loss: 0.00131202
Iteration 3/25 | Loss: 0.00099941
Iteration 4/25 | Loss: 0.00099941
Iteration 5/25 | Loss: 0.00099941
Iteration 6/25 | Loss: 0.00099941
Iteration 7/25 | Loss: 0.00099941
Iteration 8/25 | Loss: 0.00099941
Iteration 9/25 | Loss: 0.00099941
Iteration 10/25 | Loss: 0.00099941
Iteration 11/25 | Loss: 0.00099941
Iteration 12/25 | Loss: 0.00099941
Iteration 13/25 | Loss: 0.00099941
Iteration 14/25 | Loss: 0.00099941
Iteration 15/25 | Loss: 0.00099941
Iteration 16/25 | Loss: 0.00099941
Iteration 17/25 | Loss: 0.00099941
Iteration 18/25 | Loss: 0.00099941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009994077263399959, 0.0009994077263399959, 0.0009994077263399959, 0.0009994077263399959, 0.0009994077263399959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009994077263399959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099941
Iteration 2/1000 | Loss: 0.00005158
Iteration 3/1000 | Loss: 0.00025811
Iteration 4/1000 | Loss: 0.00023391
Iteration 5/1000 | Loss: 0.00040803
Iteration 6/1000 | Loss: 0.00011423
Iteration 7/1000 | Loss: 0.00002754
Iteration 8/1000 | Loss: 0.00012057
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002349
Iteration 13/1000 | Loss: 0.00010314
Iteration 14/1000 | Loss: 0.00085002
Iteration 15/1000 | Loss: 0.00022943
Iteration 16/1000 | Loss: 0.00007923
Iteration 17/1000 | Loss: 0.00004893
Iteration 18/1000 | Loss: 0.00037338
Iteration 19/1000 | Loss: 0.00002506
Iteration 20/1000 | Loss: 0.00011528
Iteration 21/1000 | Loss: 0.00002589
Iteration 22/1000 | Loss: 0.00002298
Iteration 23/1000 | Loss: 0.00002206
Iteration 24/1000 | Loss: 0.00002139
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002054
Iteration 29/1000 | Loss: 0.00002047
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00002024
Iteration 35/1000 | Loss: 0.00002023
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002023
Iteration 39/1000 | Loss: 0.00002023
Iteration 40/1000 | Loss: 0.00002023
Iteration 41/1000 | Loss: 0.00002023
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002023
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002022
Iteration 52/1000 | Loss: 0.00002020
Iteration 53/1000 | Loss: 0.00002019
Iteration 54/1000 | Loss: 0.00002019
Iteration 55/1000 | Loss: 0.00002018
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002017
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002017
Iteration 62/1000 | Loss: 0.00002017
Iteration 63/1000 | Loss: 0.00002017
Iteration 64/1000 | Loss: 0.00002016
Iteration 65/1000 | Loss: 0.00002016
Iteration 66/1000 | Loss: 0.00002016
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002015
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002014
Iteration 74/1000 | Loss: 0.00002014
Iteration 75/1000 | Loss: 0.00002014
Iteration 76/1000 | Loss: 0.00002014
Iteration 77/1000 | Loss: 0.00002014
Iteration 78/1000 | Loss: 0.00002014
Iteration 79/1000 | Loss: 0.00002014
Iteration 80/1000 | Loss: 0.00002014
Iteration 81/1000 | Loss: 0.00002014
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002013
Iteration 84/1000 | Loss: 0.00002013
Iteration 85/1000 | Loss: 0.00002013
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002011
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002010
Iteration 93/1000 | Loss: 0.00002010
Iteration 94/1000 | Loss: 0.00002010
Iteration 95/1000 | Loss: 0.00002009
Iteration 96/1000 | Loss: 0.00002009
Iteration 97/1000 | Loss: 0.00002009
Iteration 98/1000 | Loss: 0.00002009
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002008
Iteration 110/1000 | Loss: 0.00002007
Iteration 111/1000 | Loss: 0.00002007
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002003
Iteration 119/1000 | Loss: 0.00002003
Iteration 120/1000 | Loss: 0.00002003
Iteration 121/1000 | Loss: 0.00002003
Iteration 122/1000 | Loss: 0.00002003
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Iteration 127/1000 | Loss: 0.00002002
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002001
Iteration 133/1000 | Loss: 0.00002001
Iteration 134/1000 | Loss: 0.00002001
Iteration 135/1000 | Loss: 0.00002001
Iteration 136/1000 | Loss: 0.00002001
Iteration 137/1000 | Loss: 0.00002001
Iteration 138/1000 | Loss: 0.00002001
Iteration 139/1000 | Loss: 0.00002001
Iteration 140/1000 | Loss: 0.00002001
Iteration 141/1000 | Loss: 0.00002000
Iteration 142/1000 | Loss: 0.00002000
Iteration 143/1000 | Loss: 0.00002000
Iteration 144/1000 | Loss: 0.00002000
Iteration 145/1000 | Loss: 0.00002000
Iteration 146/1000 | Loss: 0.00002000
Iteration 147/1000 | Loss: 0.00001999
Iteration 148/1000 | Loss: 0.00001999
Iteration 149/1000 | Loss: 0.00001999
Iteration 150/1000 | Loss: 0.00001999
Iteration 151/1000 | Loss: 0.00001999
Iteration 152/1000 | Loss: 0.00001999
Iteration 153/1000 | Loss: 0.00001998
Iteration 154/1000 | Loss: 0.00001998
Iteration 155/1000 | Loss: 0.00001998
Iteration 156/1000 | Loss: 0.00001998
Iteration 157/1000 | Loss: 0.00001998
Iteration 158/1000 | Loss: 0.00001998
Iteration 159/1000 | Loss: 0.00001998
Iteration 160/1000 | Loss: 0.00001998
Iteration 161/1000 | Loss: 0.00001997
Iteration 162/1000 | Loss: 0.00001997
Iteration 163/1000 | Loss: 0.00001997
Iteration 164/1000 | Loss: 0.00001997
Iteration 165/1000 | Loss: 0.00001997
Iteration 166/1000 | Loss: 0.00001997
Iteration 167/1000 | Loss: 0.00001997
Iteration 168/1000 | Loss: 0.00001997
Iteration 169/1000 | Loss: 0.00001997
Iteration 170/1000 | Loss: 0.00001997
Iteration 171/1000 | Loss: 0.00001997
Iteration 172/1000 | Loss: 0.00001997
Iteration 173/1000 | Loss: 0.00001997
Iteration 174/1000 | Loss: 0.00001997
Iteration 175/1000 | Loss: 0.00001996
Iteration 176/1000 | Loss: 0.00001996
Iteration 177/1000 | Loss: 0.00001996
Iteration 178/1000 | Loss: 0.00001996
Iteration 179/1000 | Loss: 0.00001996
Iteration 180/1000 | Loss: 0.00001996
Iteration 181/1000 | Loss: 0.00001996
Iteration 182/1000 | Loss: 0.00001996
Iteration 183/1000 | Loss: 0.00001996
Iteration 184/1000 | Loss: 0.00001996
Iteration 185/1000 | Loss: 0.00001996
Iteration 186/1000 | Loss: 0.00001996
Iteration 187/1000 | Loss: 0.00001996
Iteration 188/1000 | Loss: 0.00001996
Iteration 189/1000 | Loss: 0.00001996
Iteration 190/1000 | Loss: 0.00001996
Iteration 191/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.9956960386480205e-05, 1.9956960386480205e-05, 1.9956960386480205e-05, 1.9956960386480205e-05, 1.9956960386480205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9956960386480205e-05

Optimization complete. Final v2v error: 3.8427083492279053 mm

Highest mean error: 4.852974891662598 mm for frame 133

Lowest mean error: 3.4285943508148193 mm for frame 128

Saving results

Total time: 109.95404243469238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423442
Iteration 2/25 | Loss: 0.00129928
Iteration 3/25 | Loss: 0.00124018
Iteration 4/25 | Loss: 0.00122874
Iteration 5/25 | Loss: 0.00122586
Iteration 6/25 | Loss: 0.00122532
Iteration 7/25 | Loss: 0.00122532
Iteration 8/25 | Loss: 0.00122532
Iteration 9/25 | Loss: 0.00122532
Iteration 10/25 | Loss: 0.00122531
Iteration 11/25 | Loss: 0.00122531
Iteration 12/25 | Loss: 0.00122531
Iteration 13/25 | Loss: 0.00122531
Iteration 14/25 | Loss: 0.00122531
Iteration 15/25 | Loss: 0.00122531
Iteration 16/25 | Loss: 0.00122531
Iteration 17/25 | Loss: 0.00122531
Iteration 18/25 | Loss: 0.00122531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012253125896677375, 0.0012253125896677375, 0.0012253125896677375, 0.0012253125896677375, 0.0012253125896677375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012253125896677375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53084671
Iteration 2/25 | Loss: 0.00096844
Iteration 3/25 | Loss: 0.00096844
Iteration 4/25 | Loss: 0.00096844
Iteration 5/25 | Loss: 0.00096844
Iteration 6/25 | Loss: 0.00096843
Iteration 7/25 | Loss: 0.00096843
Iteration 8/25 | Loss: 0.00096843
Iteration 9/25 | Loss: 0.00096843
Iteration 10/25 | Loss: 0.00096843
Iteration 11/25 | Loss: 0.00096843
Iteration 12/25 | Loss: 0.00096843
Iteration 13/25 | Loss: 0.00096843
Iteration 14/25 | Loss: 0.00096843
Iteration 15/25 | Loss: 0.00096843
Iteration 16/25 | Loss: 0.00096843
Iteration 17/25 | Loss: 0.00096843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000968433334492147, 0.000968433334492147, 0.000968433334492147, 0.000968433334492147, 0.000968433334492147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000968433334492147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096843
Iteration 2/1000 | Loss: 0.00002570
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001580
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001418
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001379
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00001375
Iteration 34/1000 | Loss: 0.00001375
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001365
Iteration 48/1000 | Loss: 0.00001364
Iteration 49/1000 | Loss: 0.00001364
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001362
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001353
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001348
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001347
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001343
Iteration 116/1000 | Loss: 0.00001343
Iteration 117/1000 | Loss: 0.00001343
Iteration 118/1000 | Loss: 0.00001343
Iteration 119/1000 | Loss: 0.00001343
Iteration 120/1000 | Loss: 0.00001343
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001342
Iteration 125/1000 | Loss: 0.00001342
Iteration 126/1000 | Loss: 0.00001342
Iteration 127/1000 | Loss: 0.00001342
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001341
Iteration 145/1000 | Loss: 0.00001341
Iteration 146/1000 | Loss: 0.00001341
Iteration 147/1000 | Loss: 0.00001341
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001340
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Iteration 159/1000 | Loss: 0.00001340
Iteration 160/1000 | Loss: 0.00001340
Iteration 161/1000 | Loss: 0.00001340
Iteration 162/1000 | Loss: 0.00001340
Iteration 163/1000 | Loss: 0.00001340
Iteration 164/1000 | Loss: 0.00001340
Iteration 165/1000 | Loss: 0.00001339
Iteration 166/1000 | Loss: 0.00001339
Iteration 167/1000 | Loss: 0.00001339
Iteration 168/1000 | Loss: 0.00001339
Iteration 169/1000 | Loss: 0.00001339
Iteration 170/1000 | Loss: 0.00001339
Iteration 171/1000 | Loss: 0.00001339
Iteration 172/1000 | Loss: 0.00001339
Iteration 173/1000 | Loss: 0.00001339
Iteration 174/1000 | Loss: 0.00001339
Iteration 175/1000 | Loss: 0.00001339
Iteration 176/1000 | Loss: 0.00001339
Iteration 177/1000 | Loss: 0.00001339
Iteration 178/1000 | Loss: 0.00001339
Iteration 179/1000 | Loss: 0.00001339
Iteration 180/1000 | Loss: 0.00001338
Iteration 181/1000 | Loss: 0.00001338
Iteration 182/1000 | Loss: 0.00001338
Iteration 183/1000 | Loss: 0.00001338
Iteration 184/1000 | Loss: 0.00001338
Iteration 185/1000 | Loss: 0.00001338
Iteration 186/1000 | Loss: 0.00001338
Iteration 187/1000 | Loss: 0.00001338
Iteration 188/1000 | Loss: 0.00001338
Iteration 189/1000 | Loss: 0.00001338
Iteration 190/1000 | Loss: 0.00001338
Iteration 191/1000 | Loss: 0.00001338
Iteration 192/1000 | Loss: 0.00001338
Iteration 193/1000 | Loss: 0.00001338
Iteration 194/1000 | Loss: 0.00001338
Iteration 195/1000 | Loss: 0.00001338
Iteration 196/1000 | Loss: 0.00001338
Iteration 197/1000 | Loss: 0.00001338
Iteration 198/1000 | Loss: 0.00001338
Iteration 199/1000 | Loss: 0.00001338
Iteration 200/1000 | Loss: 0.00001337
Iteration 201/1000 | Loss: 0.00001337
Iteration 202/1000 | Loss: 0.00001337
Iteration 203/1000 | Loss: 0.00001337
Iteration 204/1000 | Loss: 0.00001337
Iteration 205/1000 | Loss: 0.00001337
Iteration 206/1000 | Loss: 0.00001337
Iteration 207/1000 | Loss: 0.00001337
Iteration 208/1000 | Loss: 0.00001337
Iteration 209/1000 | Loss: 0.00001337
Iteration 210/1000 | Loss: 0.00001337
Iteration 211/1000 | Loss: 0.00001337
Iteration 212/1000 | Loss: 0.00001337
Iteration 213/1000 | Loss: 0.00001337
Iteration 214/1000 | Loss: 0.00001337
Iteration 215/1000 | Loss: 0.00001337
Iteration 216/1000 | Loss: 0.00001337
Iteration 217/1000 | Loss: 0.00001336
Iteration 218/1000 | Loss: 0.00001336
Iteration 219/1000 | Loss: 0.00001336
Iteration 220/1000 | Loss: 0.00001336
Iteration 221/1000 | Loss: 0.00001336
Iteration 222/1000 | Loss: 0.00001336
Iteration 223/1000 | Loss: 0.00001336
Iteration 224/1000 | Loss: 0.00001336
Iteration 225/1000 | Loss: 0.00001336
Iteration 226/1000 | Loss: 0.00001336
Iteration 227/1000 | Loss: 0.00001336
Iteration 228/1000 | Loss: 0.00001336
Iteration 229/1000 | Loss: 0.00001336
Iteration 230/1000 | Loss: 0.00001336
Iteration 231/1000 | Loss: 0.00001336
Iteration 232/1000 | Loss: 0.00001336
Iteration 233/1000 | Loss: 0.00001336
Iteration 234/1000 | Loss: 0.00001336
Iteration 235/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.3360554476093967e-05, 1.3360554476093967e-05, 1.3360554476093967e-05, 1.3360554476093967e-05, 1.3360554476093967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3360554476093967e-05

Optimization complete. Final v2v error: 3.158942699432373 mm

Highest mean error: 3.546886682510376 mm for frame 89

Lowest mean error: 3.0281965732574463 mm for frame 124

Saving results

Total time: 41.99502658843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398893
Iteration 2/25 | Loss: 0.00131892
Iteration 3/25 | Loss: 0.00121571
Iteration 4/25 | Loss: 0.00120614
Iteration 5/25 | Loss: 0.00120356
Iteration 6/25 | Loss: 0.00120315
Iteration 7/25 | Loss: 0.00120315
Iteration 8/25 | Loss: 0.00120315
Iteration 9/25 | Loss: 0.00120315
Iteration 10/25 | Loss: 0.00120315
Iteration 11/25 | Loss: 0.00120315
Iteration 12/25 | Loss: 0.00120315
Iteration 13/25 | Loss: 0.00120315
Iteration 14/25 | Loss: 0.00120315
Iteration 15/25 | Loss: 0.00120315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012031520018354058, 0.0012031520018354058, 0.0012031520018354058, 0.0012031520018354058, 0.0012031520018354058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012031520018354058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36587882
Iteration 2/25 | Loss: 0.00107431
Iteration 3/25 | Loss: 0.00107431
Iteration 4/25 | Loss: 0.00107431
Iteration 5/25 | Loss: 0.00107431
Iteration 6/25 | Loss: 0.00107431
Iteration 7/25 | Loss: 0.00107431
Iteration 8/25 | Loss: 0.00107431
Iteration 9/25 | Loss: 0.00107431
Iteration 10/25 | Loss: 0.00107430
Iteration 11/25 | Loss: 0.00107430
Iteration 12/25 | Loss: 0.00107430
Iteration 13/25 | Loss: 0.00107430
Iteration 14/25 | Loss: 0.00107430
Iteration 15/25 | Loss: 0.00107430
Iteration 16/25 | Loss: 0.00107430
Iteration 17/25 | Loss: 0.00107430
Iteration 18/25 | Loss: 0.00107430
Iteration 19/25 | Loss: 0.00107430
Iteration 20/25 | Loss: 0.00107430
Iteration 21/25 | Loss: 0.00107430
Iteration 22/25 | Loss: 0.00107430
Iteration 23/25 | Loss: 0.00107430
Iteration 24/25 | Loss: 0.00107430
Iteration 25/25 | Loss: 0.00107430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107430
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001503
Iteration 4/1000 | Loss: 0.00001329
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001165
Iteration 7/1000 | Loss: 0.00001118
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001070
Iteration 12/1000 | Loss: 0.00001062
Iteration 13/1000 | Loss: 0.00001061
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001059
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001054
Iteration 20/1000 | Loss: 0.00001053
Iteration 21/1000 | Loss: 0.00001053
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001051
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001048
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001041
Iteration 32/1000 | Loss: 0.00001041
Iteration 33/1000 | Loss: 0.00001040
Iteration 34/1000 | Loss: 0.00001039
Iteration 35/1000 | Loss: 0.00001030
Iteration 36/1000 | Loss: 0.00001030
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001023
Iteration 41/1000 | Loss: 0.00001022
Iteration 42/1000 | Loss: 0.00001022
Iteration 43/1000 | Loss: 0.00001022
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001021
Iteration 46/1000 | Loss: 0.00001021
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001020
Iteration 50/1000 | Loss: 0.00001020
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001019
Iteration 53/1000 | Loss: 0.00001018
Iteration 54/1000 | Loss: 0.00001018
Iteration 55/1000 | Loss: 0.00001017
Iteration 56/1000 | Loss: 0.00001017
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001016
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001014
Iteration 62/1000 | Loss: 0.00001014
Iteration 63/1000 | Loss: 0.00001013
Iteration 64/1000 | Loss: 0.00001013
Iteration 65/1000 | Loss: 0.00001013
Iteration 66/1000 | Loss: 0.00001013
Iteration 67/1000 | Loss: 0.00001013
Iteration 68/1000 | Loss: 0.00001012
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001012
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001009
Iteration 79/1000 | Loss: 0.00001009
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001007
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Iteration 87/1000 | Loss: 0.00001005
Iteration 88/1000 | Loss: 0.00001005
Iteration 89/1000 | Loss: 0.00001005
Iteration 90/1000 | Loss: 0.00001004
Iteration 91/1000 | Loss: 0.00001004
Iteration 92/1000 | Loss: 0.00001003
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001000
Iteration 101/1000 | Loss: 0.00001000
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000998
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000994
Iteration 130/1000 | Loss: 0.00000994
Iteration 131/1000 | Loss: 0.00000994
Iteration 132/1000 | Loss: 0.00000994
Iteration 133/1000 | Loss: 0.00000994
Iteration 134/1000 | Loss: 0.00000994
Iteration 135/1000 | Loss: 0.00000994
Iteration 136/1000 | Loss: 0.00000993
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000993
Iteration 139/1000 | Loss: 0.00000993
Iteration 140/1000 | Loss: 0.00000993
Iteration 141/1000 | Loss: 0.00000993
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000992
Iteration 144/1000 | Loss: 0.00000992
Iteration 145/1000 | Loss: 0.00000992
Iteration 146/1000 | Loss: 0.00000992
Iteration 147/1000 | Loss: 0.00000992
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000991
Iteration 151/1000 | Loss: 0.00000991
Iteration 152/1000 | Loss: 0.00000991
Iteration 153/1000 | Loss: 0.00000991
Iteration 154/1000 | Loss: 0.00000991
Iteration 155/1000 | Loss: 0.00000991
Iteration 156/1000 | Loss: 0.00000991
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000989
Iteration 168/1000 | Loss: 0.00000989
Iteration 169/1000 | Loss: 0.00000989
Iteration 170/1000 | Loss: 0.00000989
Iteration 171/1000 | Loss: 0.00000989
Iteration 172/1000 | Loss: 0.00000989
Iteration 173/1000 | Loss: 0.00000989
Iteration 174/1000 | Loss: 0.00000989
Iteration 175/1000 | Loss: 0.00000989
Iteration 176/1000 | Loss: 0.00000989
Iteration 177/1000 | Loss: 0.00000989
Iteration 178/1000 | Loss: 0.00000989
Iteration 179/1000 | Loss: 0.00000989
Iteration 180/1000 | Loss: 0.00000989
Iteration 181/1000 | Loss: 0.00000989
Iteration 182/1000 | Loss: 0.00000989
Iteration 183/1000 | Loss: 0.00000989
Iteration 184/1000 | Loss: 0.00000989
Iteration 185/1000 | Loss: 0.00000989
Iteration 186/1000 | Loss: 0.00000989
Iteration 187/1000 | Loss: 0.00000989
Iteration 188/1000 | Loss: 0.00000989
Iteration 189/1000 | Loss: 0.00000989
Iteration 190/1000 | Loss: 0.00000989
Iteration 191/1000 | Loss: 0.00000989
Iteration 192/1000 | Loss: 0.00000989
Iteration 193/1000 | Loss: 0.00000989
Iteration 194/1000 | Loss: 0.00000989
Iteration 195/1000 | Loss: 0.00000989
Iteration 196/1000 | Loss: 0.00000989
Iteration 197/1000 | Loss: 0.00000989
Iteration 198/1000 | Loss: 0.00000989
Iteration 199/1000 | Loss: 0.00000989
Iteration 200/1000 | Loss: 0.00000989
Iteration 201/1000 | Loss: 0.00000989
Iteration 202/1000 | Loss: 0.00000989
Iteration 203/1000 | Loss: 0.00000989
Iteration 204/1000 | Loss: 0.00000989
Iteration 205/1000 | Loss: 0.00000989
Iteration 206/1000 | Loss: 0.00000989
Iteration 207/1000 | Loss: 0.00000989
Iteration 208/1000 | Loss: 0.00000989
Iteration 209/1000 | Loss: 0.00000989
Iteration 210/1000 | Loss: 0.00000989
Iteration 211/1000 | Loss: 0.00000989
Iteration 212/1000 | Loss: 0.00000989
Iteration 213/1000 | Loss: 0.00000989
Iteration 214/1000 | Loss: 0.00000989
Iteration 215/1000 | Loss: 0.00000989
Iteration 216/1000 | Loss: 0.00000989
Iteration 217/1000 | Loss: 0.00000989
Iteration 218/1000 | Loss: 0.00000989
Iteration 219/1000 | Loss: 0.00000989
Iteration 220/1000 | Loss: 0.00000989
Iteration 221/1000 | Loss: 0.00000989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [9.886485713650472e-06, 9.886485713650472e-06, 9.886485713650472e-06, 9.886485713650472e-06, 9.886485713650472e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.886485713650472e-06

Optimization complete. Final v2v error: 2.6974546909332275 mm

Highest mean error: 3.5732738971710205 mm for frame 72

Lowest mean error: 2.545863628387451 mm for frame 42

Saving results

Total time: 38.788409948349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813954
Iteration 2/25 | Loss: 0.00130753
Iteration 3/25 | Loss: 0.00121329
Iteration 4/25 | Loss: 0.00120308
Iteration 5/25 | Loss: 0.00120110
Iteration 6/25 | Loss: 0.00120110
Iteration 7/25 | Loss: 0.00120110
Iteration 8/25 | Loss: 0.00120110
Iteration 9/25 | Loss: 0.00120110
Iteration 10/25 | Loss: 0.00120110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012011030921712518, 0.0012011030921712518, 0.0012011030921712518, 0.0012011030921712518, 0.0012011030921712518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012011030921712518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35294354
Iteration 2/25 | Loss: 0.00088058
Iteration 3/25 | Loss: 0.00088057
Iteration 4/25 | Loss: 0.00088057
Iteration 5/25 | Loss: 0.00088057
Iteration 6/25 | Loss: 0.00088057
Iteration 7/25 | Loss: 0.00088057
Iteration 8/25 | Loss: 0.00088057
Iteration 9/25 | Loss: 0.00088057
Iteration 10/25 | Loss: 0.00088057
Iteration 11/25 | Loss: 0.00088057
Iteration 12/25 | Loss: 0.00088057
Iteration 13/25 | Loss: 0.00088057
Iteration 14/25 | Loss: 0.00088057
Iteration 15/25 | Loss: 0.00088057
Iteration 16/25 | Loss: 0.00088057
Iteration 17/25 | Loss: 0.00088057
Iteration 18/25 | Loss: 0.00088057
Iteration 19/25 | Loss: 0.00088057
Iteration 20/25 | Loss: 0.00088057
Iteration 21/25 | Loss: 0.00088057
Iteration 22/25 | Loss: 0.00088057
Iteration 23/25 | Loss: 0.00088057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008805672987364233, 0.0008805672987364233, 0.0008805672987364233, 0.0008805672987364233, 0.0008805672987364233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008805672987364233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088057
Iteration 2/1000 | Loss: 0.00002360
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001450
Iteration 5/1000 | Loss: 0.00001350
Iteration 6/1000 | Loss: 0.00001281
Iteration 7/1000 | Loss: 0.00001245
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001160
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001152
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001110
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001108
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001105
Iteration 71/1000 | Loss: 0.00001105
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001104
Iteration 74/1000 | Loss: 0.00001104
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001104
Iteration 77/1000 | Loss: 0.00001104
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001091
Iteration 100/1000 | Loss: 0.00001091
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001090
Iteration 103/1000 | Loss: 0.00001090
Iteration 104/1000 | Loss: 0.00001090
Iteration 105/1000 | Loss: 0.00001090
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001089
Iteration 108/1000 | Loss: 0.00001089
Iteration 109/1000 | Loss: 0.00001089
Iteration 110/1000 | Loss: 0.00001089
Iteration 111/1000 | Loss: 0.00001089
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001084
Iteration 121/1000 | Loss: 0.00001083
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001083
Iteration 124/1000 | Loss: 0.00001083
Iteration 125/1000 | Loss: 0.00001083
Iteration 126/1000 | Loss: 0.00001083
Iteration 127/1000 | Loss: 0.00001083
Iteration 128/1000 | Loss: 0.00001083
Iteration 129/1000 | Loss: 0.00001083
Iteration 130/1000 | Loss: 0.00001083
Iteration 131/1000 | Loss: 0.00001082
Iteration 132/1000 | Loss: 0.00001081
Iteration 133/1000 | Loss: 0.00001081
Iteration 134/1000 | Loss: 0.00001080
Iteration 135/1000 | Loss: 0.00001080
Iteration 136/1000 | Loss: 0.00001079
Iteration 137/1000 | Loss: 0.00001079
Iteration 138/1000 | Loss: 0.00001079
Iteration 139/1000 | Loss: 0.00001078
Iteration 140/1000 | Loss: 0.00001078
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001077
Iteration 146/1000 | Loss: 0.00001077
Iteration 147/1000 | Loss: 0.00001077
Iteration 148/1000 | Loss: 0.00001077
Iteration 149/1000 | Loss: 0.00001077
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001076
Iteration 153/1000 | Loss: 0.00001076
Iteration 154/1000 | Loss: 0.00001076
Iteration 155/1000 | Loss: 0.00001075
Iteration 156/1000 | Loss: 0.00001075
Iteration 157/1000 | Loss: 0.00001074
Iteration 158/1000 | Loss: 0.00001074
Iteration 159/1000 | Loss: 0.00001074
Iteration 160/1000 | Loss: 0.00001074
Iteration 161/1000 | Loss: 0.00001073
Iteration 162/1000 | Loss: 0.00001073
Iteration 163/1000 | Loss: 0.00001073
Iteration 164/1000 | Loss: 0.00001073
Iteration 165/1000 | Loss: 0.00001073
Iteration 166/1000 | Loss: 0.00001073
Iteration 167/1000 | Loss: 0.00001073
Iteration 168/1000 | Loss: 0.00001073
Iteration 169/1000 | Loss: 0.00001073
Iteration 170/1000 | Loss: 0.00001073
Iteration 171/1000 | Loss: 0.00001073
Iteration 172/1000 | Loss: 0.00001072
Iteration 173/1000 | Loss: 0.00001072
Iteration 174/1000 | Loss: 0.00001072
Iteration 175/1000 | Loss: 0.00001072
Iteration 176/1000 | Loss: 0.00001072
Iteration 177/1000 | Loss: 0.00001072
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Iteration 181/1000 | Loss: 0.00001071
Iteration 182/1000 | Loss: 0.00001071
Iteration 183/1000 | Loss: 0.00001071
Iteration 184/1000 | Loss: 0.00001071
Iteration 185/1000 | Loss: 0.00001071
Iteration 186/1000 | Loss: 0.00001070
Iteration 187/1000 | Loss: 0.00001070
Iteration 188/1000 | Loss: 0.00001070
Iteration 189/1000 | Loss: 0.00001070
Iteration 190/1000 | Loss: 0.00001070
Iteration 191/1000 | Loss: 0.00001070
Iteration 192/1000 | Loss: 0.00001070
Iteration 193/1000 | Loss: 0.00001070
Iteration 194/1000 | Loss: 0.00001070
Iteration 195/1000 | Loss: 0.00001070
Iteration 196/1000 | Loss: 0.00001070
Iteration 197/1000 | Loss: 0.00001070
Iteration 198/1000 | Loss: 0.00001070
Iteration 199/1000 | Loss: 0.00001069
Iteration 200/1000 | Loss: 0.00001069
Iteration 201/1000 | Loss: 0.00001069
Iteration 202/1000 | Loss: 0.00001069
Iteration 203/1000 | Loss: 0.00001069
Iteration 204/1000 | Loss: 0.00001069
Iteration 205/1000 | Loss: 0.00001069
Iteration 206/1000 | Loss: 0.00001069
Iteration 207/1000 | Loss: 0.00001068
Iteration 208/1000 | Loss: 0.00001068
Iteration 209/1000 | Loss: 0.00001068
Iteration 210/1000 | Loss: 0.00001068
Iteration 211/1000 | Loss: 0.00001068
Iteration 212/1000 | Loss: 0.00001068
Iteration 213/1000 | Loss: 0.00001068
Iteration 214/1000 | Loss: 0.00001068
Iteration 215/1000 | Loss: 0.00001068
Iteration 216/1000 | Loss: 0.00001068
Iteration 217/1000 | Loss: 0.00001068
Iteration 218/1000 | Loss: 0.00001068
Iteration 219/1000 | Loss: 0.00001068
Iteration 220/1000 | Loss: 0.00001068
Iteration 221/1000 | Loss: 0.00001068
Iteration 222/1000 | Loss: 0.00001068
Iteration 223/1000 | Loss: 0.00001067
Iteration 224/1000 | Loss: 0.00001067
Iteration 225/1000 | Loss: 0.00001067
Iteration 226/1000 | Loss: 0.00001067
Iteration 227/1000 | Loss: 0.00001067
Iteration 228/1000 | Loss: 0.00001067
Iteration 229/1000 | Loss: 0.00001066
Iteration 230/1000 | Loss: 0.00001066
Iteration 231/1000 | Loss: 0.00001066
Iteration 232/1000 | Loss: 0.00001066
Iteration 233/1000 | Loss: 0.00001066
Iteration 234/1000 | Loss: 0.00001066
Iteration 235/1000 | Loss: 0.00001066
Iteration 236/1000 | Loss: 0.00001066
Iteration 237/1000 | Loss: 0.00001066
Iteration 238/1000 | Loss: 0.00001066
Iteration 239/1000 | Loss: 0.00001066
Iteration 240/1000 | Loss: 0.00001066
Iteration 241/1000 | Loss: 0.00001066
Iteration 242/1000 | Loss: 0.00001066
Iteration 243/1000 | Loss: 0.00001066
Iteration 244/1000 | Loss: 0.00001066
Iteration 245/1000 | Loss: 0.00001066
Iteration 246/1000 | Loss: 0.00001066
Iteration 247/1000 | Loss: 0.00001066
Iteration 248/1000 | Loss: 0.00001066
Iteration 249/1000 | Loss: 0.00001066
Iteration 250/1000 | Loss: 0.00001066
Iteration 251/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.0655763617251068e-05, 1.0655763617251068e-05, 1.0655763617251068e-05, 1.0655763617251068e-05, 1.0655763617251068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0655763617251068e-05

Optimization complete. Final v2v error: 2.808426856994629 mm

Highest mean error: 3.0129430294036865 mm for frame 66

Lowest mean error: 2.6106066703796387 mm for frame 240

Saving results

Total time: 49.86293363571167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801646
Iteration 2/25 | Loss: 0.00155669
Iteration 3/25 | Loss: 0.00129040
Iteration 4/25 | Loss: 0.00126646
Iteration 5/25 | Loss: 0.00126390
Iteration 6/25 | Loss: 0.00126390
Iteration 7/25 | Loss: 0.00126390
Iteration 8/25 | Loss: 0.00126390
Iteration 9/25 | Loss: 0.00126390
Iteration 10/25 | Loss: 0.00126390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012638963526114821, 0.0012638963526114821, 0.0012638963526114821, 0.0012638963526114821, 0.0012638963526114821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012638963526114821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32813680
Iteration 2/25 | Loss: 0.00072779
Iteration 3/25 | Loss: 0.00072778
Iteration 4/25 | Loss: 0.00072778
Iteration 5/25 | Loss: 0.00072778
Iteration 6/25 | Loss: 0.00072778
Iteration 7/25 | Loss: 0.00072778
Iteration 8/25 | Loss: 0.00072778
Iteration 9/25 | Loss: 0.00072778
Iteration 10/25 | Loss: 0.00072778
Iteration 11/25 | Loss: 0.00072777
Iteration 12/25 | Loss: 0.00072777
Iteration 13/25 | Loss: 0.00072777
Iteration 14/25 | Loss: 0.00072777
Iteration 15/25 | Loss: 0.00072777
Iteration 16/25 | Loss: 0.00072777
Iteration 17/25 | Loss: 0.00072777
Iteration 18/25 | Loss: 0.00072777
Iteration 19/25 | Loss: 0.00072777
Iteration 20/25 | Loss: 0.00072777
Iteration 21/25 | Loss: 0.00072777
Iteration 22/25 | Loss: 0.00072777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007277748663909733, 0.0007277748663909733, 0.0007277748663909733, 0.0007277748663909733, 0.0007277748663909733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007277748663909733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072777
Iteration 2/1000 | Loss: 0.00003208
Iteration 3/1000 | Loss: 0.00002263
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001942
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001775
Iteration 9/1000 | Loss: 0.00001734
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001676
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001589
Iteration 15/1000 | Loss: 0.00001581
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001572
Iteration 19/1000 | Loss: 0.00001571
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001561
Iteration 23/1000 | Loss: 0.00001560
Iteration 24/1000 | Loss: 0.00001559
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001559
Iteration 27/1000 | Loss: 0.00001558
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001556
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001545
Iteration 58/1000 | Loss: 0.00001545
Iteration 59/1000 | Loss: 0.00001545
Iteration 60/1000 | Loss: 0.00001544
Iteration 61/1000 | Loss: 0.00001544
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001544
Iteration 64/1000 | Loss: 0.00001544
Iteration 65/1000 | Loss: 0.00001544
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001544
Iteration 68/1000 | Loss: 0.00001544
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001543
Iteration 71/1000 | Loss: 0.00001543
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001543
Iteration 74/1000 | Loss: 0.00001543
Iteration 75/1000 | Loss: 0.00001543
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.5414390873047523e-05, 1.5414390873047523e-05, 1.5414390873047523e-05, 1.5414390873047523e-05, 1.5414390873047523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5414390873047523e-05

Optimization complete. Final v2v error: 3.2876522541046143 mm

Highest mean error: 3.408437967300415 mm for frame 35

Lowest mean error: 3.2127420902252197 mm for frame 106

Saving results

Total time: 34.71891260147095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797724
Iteration 2/25 | Loss: 0.00133953
Iteration 3/25 | Loss: 0.00126385
Iteration 4/25 | Loss: 0.00125501
Iteration 5/25 | Loss: 0.00125249
Iteration 6/25 | Loss: 0.00125249
Iteration 7/25 | Loss: 0.00125249
Iteration 8/25 | Loss: 0.00125249
Iteration 9/25 | Loss: 0.00125249
Iteration 10/25 | Loss: 0.00125249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012524909107014537, 0.0012524909107014537, 0.0012524909107014537, 0.0012524909107014537, 0.0012524909107014537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012524909107014537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.08102274
Iteration 2/25 | Loss: 0.00103430
Iteration 3/25 | Loss: 0.00103429
Iteration 4/25 | Loss: 0.00103429
Iteration 5/25 | Loss: 0.00103429
Iteration 6/25 | Loss: 0.00103429
Iteration 7/25 | Loss: 0.00103429
Iteration 8/25 | Loss: 0.00103429
Iteration 9/25 | Loss: 0.00103429
Iteration 10/25 | Loss: 0.00103429
Iteration 11/25 | Loss: 0.00103429
Iteration 12/25 | Loss: 0.00103429
Iteration 13/25 | Loss: 0.00103429
Iteration 14/25 | Loss: 0.00103429
Iteration 15/25 | Loss: 0.00103429
Iteration 16/25 | Loss: 0.00103429
Iteration 17/25 | Loss: 0.00103429
Iteration 18/25 | Loss: 0.00103429
Iteration 19/25 | Loss: 0.00103429
Iteration 20/25 | Loss: 0.00103429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010342909954488277, 0.0010342909954488277, 0.0010342909954488277, 0.0010342909954488277, 0.0010342909954488277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010342909954488277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103429
Iteration 2/1000 | Loss: 0.00002971
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002053
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001850
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001759
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001739
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001705
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001703
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001701
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001701
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001695
Iteration 56/1000 | Loss: 0.00001695
Iteration 57/1000 | Loss: 0.00001694
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00001694
Iteration 60/1000 | Loss: 0.00001694
Iteration 61/1000 | Loss: 0.00001693
Iteration 62/1000 | Loss: 0.00001693
Iteration 63/1000 | Loss: 0.00001692
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001687
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001679
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001675
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001671
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001670
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Iteration 112/1000 | Loss: 0.00001668
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001668
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.6678714018780738e-05, 1.6678714018780738e-05, 1.6678714018780738e-05, 1.6678714018780738e-05, 1.6678714018780738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6678714018780738e-05

Optimization complete. Final v2v error: 3.4490785598754883 mm

Highest mean error: 3.720799207687378 mm for frame 129

Lowest mean error: 3.245699167251587 mm for frame 203

Saving results

Total time: 40.49774360656738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849686
Iteration 2/25 | Loss: 0.00151747
Iteration 3/25 | Loss: 0.00135031
Iteration 4/25 | Loss: 0.00130664
Iteration 5/25 | Loss: 0.00130098
Iteration 6/25 | Loss: 0.00129943
Iteration 7/25 | Loss: 0.00129894
Iteration 8/25 | Loss: 0.00129894
Iteration 9/25 | Loss: 0.00129894
Iteration 10/25 | Loss: 0.00129894
Iteration 11/25 | Loss: 0.00129894
Iteration 12/25 | Loss: 0.00129894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001298935734666884, 0.001298935734666884, 0.001298935734666884, 0.001298935734666884, 0.001298935734666884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001298935734666884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30850148
Iteration 2/25 | Loss: 0.00092753
Iteration 3/25 | Loss: 0.00092753
Iteration 4/25 | Loss: 0.00092753
Iteration 5/25 | Loss: 0.00092753
Iteration 6/25 | Loss: 0.00092753
Iteration 7/25 | Loss: 0.00092753
Iteration 8/25 | Loss: 0.00092753
Iteration 9/25 | Loss: 0.00092753
Iteration 10/25 | Loss: 0.00092753
Iteration 11/25 | Loss: 0.00092753
Iteration 12/25 | Loss: 0.00092753
Iteration 13/25 | Loss: 0.00092753
Iteration 14/25 | Loss: 0.00092753
Iteration 15/25 | Loss: 0.00092753
Iteration 16/25 | Loss: 0.00092753
Iteration 17/25 | Loss: 0.00092753
Iteration 18/25 | Loss: 0.00092753
Iteration 19/25 | Loss: 0.00092753
Iteration 20/25 | Loss: 0.00092753
Iteration 21/25 | Loss: 0.00092753
Iteration 22/25 | Loss: 0.00092753
Iteration 23/25 | Loss: 0.00092753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009275307529605925, 0.0009275307529605925, 0.0009275307529605925, 0.0009275307529605925, 0.0009275307529605925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009275307529605925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092753
Iteration 2/1000 | Loss: 0.00010659
Iteration 3/1000 | Loss: 0.00006786
Iteration 4/1000 | Loss: 0.00004139
Iteration 5/1000 | Loss: 0.00003523
Iteration 6/1000 | Loss: 0.00003267
Iteration 7/1000 | Loss: 0.00003167
Iteration 8/1000 | Loss: 0.00003077
Iteration 9/1000 | Loss: 0.00003028
Iteration 10/1000 | Loss: 0.00002997
Iteration 11/1000 | Loss: 0.00002965
Iteration 12/1000 | Loss: 0.00002943
Iteration 13/1000 | Loss: 0.00002920
Iteration 14/1000 | Loss: 0.00002902
Iteration 15/1000 | Loss: 0.00002901
Iteration 16/1000 | Loss: 0.00002898
Iteration 17/1000 | Loss: 0.00002896
Iteration 18/1000 | Loss: 0.00002893
Iteration 19/1000 | Loss: 0.00002889
Iteration 20/1000 | Loss: 0.00002884
Iteration 21/1000 | Loss: 0.00002880
Iteration 22/1000 | Loss: 0.00002877
Iteration 23/1000 | Loss: 0.00002873
Iteration 24/1000 | Loss: 0.00002872
Iteration 25/1000 | Loss: 0.00002872
Iteration 26/1000 | Loss: 0.00002872
Iteration 27/1000 | Loss: 0.00002870
Iteration 28/1000 | Loss: 0.00002870
Iteration 29/1000 | Loss: 0.00002866
Iteration 30/1000 | Loss: 0.00002866
Iteration 31/1000 | Loss: 0.00002863
Iteration 32/1000 | Loss: 0.00002863
Iteration 33/1000 | Loss: 0.00002862
Iteration 34/1000 | Loss: 0.00002862
Iteration 35/1000 | Loss: 0.00002862
Iteration 36/1000 | Loss: 0.00002861
Iteration 37/1000 | Loss: 0.00002861
Iteration 38/1000 | Loss: 0.00002860
Iteration 39/1000 | Loss: 0.00002855
Iteration 40/1000 | Loss: 0.00002855
Iteration 41/1000 | Loss: 0.00002854
Iteration 42/1000 | Loss: 0.00002853
Iteration 43/1000 | Loss: 0.00002853
Iteration 44/1000 | Loss: 0.00002852
Iteration 45/1000 | Loss: 0.00002852
Iteration 46/1000 | Loss: 0.00002852
Iteration 47/1000 | Loss: 0.00002851
Iteration 48/1000 | Loss: 0.00002851
Iteration 49/1000 | Loss: 0.00002851
Iteration 50/1000 | Loss: 0.00002851
Iteration 51/1000 | Loss: 0.00002851
Iteration 52/1000 | Loss: 0.00002850
Iteration 53/1000 | Loss: 0.00002848
Iteration 54/1000 | Loss: 0.00002848
Iteration 55/1000 | Loss: 0.00002848
Iteration 56/1000 | Loss: 0.00002848
Iteration 57/1000 | Loss: 0.00002847
Iteration 58/1000 | Loss: 0.00002847
Iteration 59/1000 | Loss: 0.00002847
Iteration 60/1000 | Loss: 0.00002847
Iteration 61/1000 | Loss: 0.00002846
Iteration 62/1000 | Loss: 0.00002846
Iteration 63/1000 | Loss: 0.00002846
Iteration 64/1000 | Loss: 0.00002846
Iteration 65/1000 | Loss: 0.00002846
Iteration 66/1000 | Loss: 0.00002845
Iteration 67/1000 | Loss: 0.00002845
Iteration 68/1000 | Loss: 0.00002845
Iteration 69/1000 | Loss: 0.00002844
Iteration 70/1000 | Loss: 0.00002844
Iteration 71/1000 | Loss: 0.00002844
Iteration 72/1000 | Loss: 0.00002844
Iteration 73/1000 | Loss: 0.00002843
Iteration 74/1000 | Loss: 0.00002843
Iteration 75/1000 | Loss: 0.00002843
Iteration 76/1000 | Loss: 0.00002843
Iteration 77/1000 | Loss: 0.00002843
Iteration 78/1000 | Loss: 0.00002843
Iteration 79/1000 | Loss: 0.00002843
Iteration 80/1000 | Loss: 0.00002843
Iteration 81/1000 | Loss: 0.00002842
Iteration 82/1000 | Loss: 0.00002842
Iteration 83/1000 | Loss: 0.00002842
Iteration 84/1000 | Loss: 0.00002841
Iteration 85/1000 | Loss: 0.00002841
Iteration 86/1000 | Loss: 0.00002841
Iteration 87/1000 | Loss: 0.00002841
Iteration 88/1000 | Loss: 0.00002840
Iteration 89/1000 | Loss: 0.00002840
Iteration 90/1000 | Loss: 0.00002840
Iteration 91/1000 | Loss: 0.00002840
Iteration 92/1000 | Loss: 0.00002839
Iteration 93/1000 | Loss: 0.00002839
Iteration 94/1000 | Loss: 0.00002839
Iteration 95/1000 | Loss: 0.00002839
Iteration 96/1000 | Loss: 0.00002839
Iteration 97/1000 | Loss: 0.00002838
Iteration 98/1000 | Loss: 0.00002838
Iteration 99/1000 | Loss: 0.00002838
Iteration 100/1000 | Loss: 0.00002838
Iteration 101/1000 | Loss: 0.00002838
Iteration 102/1000 | Loss: 0.00002838
Iteration 103/1000 | Loss: 0.00002838
Iteration 104/1000 | Loss: 0.00002838
Iteration 105/1000 | Loss: 0.00002838
Iteration 106/1000 | Loss: 0.00002837
Iteration 107/1000 | Loss: 0.00002837
Iteration 108/1000 | Loss: 0.00002837
Iteration 109/1000 | Loss: 0.00002837
Iteration 110/1000 | Loss: 0.00002837
Iteration 111/1000 | Loss: 0.00002837
Iteration 112/1000 | Loss: 0.00002837
Iteration 113/1000 | Loss: 0.00002837
Iteration 114/1000 | Loss: 0.00002837
Iteration 115/1000 | Loss: 0.00002837
Iteration 116/1000 | Loss: 0.00002836
Iteration 117/1000 | Loss: 0.00002836
Iteration 118/1000 | Loss: 0.00002836
Iteration 119/1000 | Loss: 0.00002836
Iteration 120/1000 | Loss: 0.00002836
Iteration 121/1000 | Loss: 0.00002836
Iteration 122/1000 | Loss: 0.00002836
Iteration 123/1000 | Loss: 0.00002836
Iteration 124/1000 | Loss: 0.00002836
Iteration 125/1000 | Loss: 0.00002836
Iteration 126/1000 | Loss: 0.00002835
Iteration 127/1000 | Loss: 0.00002835
Iteration 128/1000 | Loss: 0.00002835
Iteration 129/1000 | Loss: 0.00002835
Iteration 130/1000 | Loss: 0.00002835
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002835
Iteration 134/1000 | Loss: 0.00002835
Iteration 135/1000 | Loss: 0.00002835
Iteration 136/1000 | Loss: 0.00002835
Iteration 137/1000 | Loss: 0.00002835
Iteration 138/1000 | Loss: 0.00002835
Iteration 139/1000 | Loss: 0.00002835
Iteration 140/1000 | Loss: 0.00002835
Iteration 141/1000 | Loss: 0.00002835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.8354061214486137e-05, 2.8354061214486137e-05, 2.8354061214486137e-05, 2.8354061214486137e-05, 2.8354061214486137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8354061214486137e-05

Optimization complete. Final v2v error: 4.482273101806641 mm

Highest mean error: 4.9505085945129395 mm for frame 75

Lowest mean error: 3.52923583984375 mm for frame 7

Saving results

Total time: 39.78138566017151
