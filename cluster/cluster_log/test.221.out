Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=221, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12376-12431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827121
Iteration 2/25 | Loss: 0.00154451
Iteration 3/25 | Loss: 0.00128392
Iteration 4/25 | Loss: 0.00127562
Iteration 5/25 | Loss: 0.00127440
Iteration 6/25 | Loss: 0.00127440
Iteration 7/25 | Loss: 0.00127440
Iteration 8/25 | Loss: 0.00127440
Iteration 9/25 | Loss: 0.00127440
Iteration 10/25 | Loss: 0.00127440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001274398760870099, 0.001274398760870099, 0.001274398760870099, 0.001274398760870099, 0.001274398760870099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274398760870099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64517903
Iteration 2/25 | Loss: 0.00099982
Iteration 3/25 | Loss: 0.00099981
Iteration 4/25 | Loss: 0.00099981
Iteration 5/25 | Loss: 0.00099981
Iteration 6/25 | Loss: 0.00099981
Iteration 7/25 | Loss: 0.00099981
Iteration 8/25 | Loss: 0.00099981
Iteration 9/25 | Loss: 0.00099981
Iteration 10/25 | Loss: 0.00099981
Iteration 11/25 | Loss: 0.00099981
Iteration 12/25 | Loss: 0.00099981
Iteration 13/25 | Loss: 0.00099981
Iteration 14/25 | Loss: 0.00099981
Iteration 15/25 | Loss: 0.00099981
Iteration 16/25 | Loss: 0.00099981
Iteration 17/25 | Loss: 0.00099981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009998086607083678, 0.0009998086607083678, 0.0009998086607083678, 0.0009998086607083678, 0.0009998086607083678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998086607083678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099981
Iteration 2/1000 | Loss: 0.00004795
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001829
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001648
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001645
Iteration 20/1000 | Loss: 0.00001645
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001636
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001630
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001628
Iteration 46/1000 | Loss: 0.00001628
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001627
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001626
Iteration 52/1000 | Loss: 0.00001626
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001624
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001621
Iteration 67/1000 | Loss: 0.00001621
Iteration 68/1000 | Loss: 0.00001621
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001620
Iteration 71/1000 | Loss: 0.00001620
Iteration 72/1000 | Loss: 0.00001620
Iteration 73/1000 | Loss: 0.00001620
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001619
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00001619
Iteration 78/1000 | Loss: 0.00001619
Iteration 79/1000 | Loss: 0.00001619
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001615
Iteration 105/1000 | Loss: 0.00001615
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001615
Iteration 108/1000 | Loss: 0.00001615
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001614
Iteration 111/1000 | Loss: 0.00001614
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001613
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001610
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001609
Iteration 125/1000 | Loss: 0.00001609
Iteration 126/1000 | Loss: 0.00001609
Iteration 127/1000 | Loss: 0.00001609
Iteration 128/1000 | Loss: 0.00001609
Iteration 129/1000 | Loss: 0.00001609
Iteration 130/1000 | Loss: 0.00001609
Iteration 131/1000 | Loss: 0.00001609
Iteration 132/1000 | Loss: 0.00001608
Iteration 133/1000 | Loss: 0.00001608
Iteration 134/1000 | Loss: 0.00001608
Iteration 135/1000 | Loss: 0.00001608
Iteration 136/1000 | Loss: 0.00001608
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001607
Iteration 144/1000 | Loss: 0.00001607
Iteration 145/1000 | Loss: 0.00001607
Iteration 146/1000 | Loss: 0.00001607
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00001606
Iteration 149/1000 | Loss: 0.00001606
Iteration 150/1000 | Loss: 0.00001605
Iteration 151/1000 | Loss: 0.00001605
Iteration 152/1000 | Loss: 0.00001605
Iteration 153/1000 | Loss: 0.00001605
Iteration 154/1000 | Loss: 0.00001605
Iteration 155/1000 | Loss: 0.00001605
Iteration 156/1000 | Loss: 0.00001605
Iteration 157/1000 | Loss: 0.00001605
Iteration 158/1000 | Loss: 0.00001605
Iteration 159/1000 | Loss: 0.00001605
Iteration 160/1000 | Loss: 0.00001605
Iteration 161/1000 | Loss: 0.00001604
Iteration 162/1000 | Loss: 0.00001604
Iteration 163/1000 | Loss: 0.00001604
Iteration 164/1000 | Loss: 0.00001604
Iteration 165/1000 | Loss: 0.00001603
Iteration 166/1000 | Loss: 0.00001603
Iteration 167/1000 | Loss: 0.00001603
Iteration 168/1000 | Loss: 0.00001603
Iteration 169/1000 | Loss: 0.00001603
Iteration 170/1000 | Loss: 0.00001603
Iteration 171/1000 | Loss: 0.00001603
Iteration 172/1000 | Loss: 0.00001603
Iteration 173/1000 | Loss: 0.00001602
Iteration 174/1000 | Loss: 0.00001602
Iteration 175/1000 | Loss: 0.00001602
Iteration 176/1000 | Loss: 0.00001602
Iteration 177/1000 | Loss: 0.00001602
Iteration 178/1000 | Loss: 0.00001602
Iteration 179/1000 | Loss: 0.00001602
Iteration 180/1000 | Loss: 0.00001602
Iteration 181/1000 | Loss: 0.00001601
Iteration 182/1000 | Loss: 0.00001601
Iteration 183/1000 | Loss: 0.00001601
Iteration 184/1000 | Loss: 0.00001601
Iteration 185/1000 | Loss: 0.00001601
Iteration 186/1000 | Loss: 0.00001601
Iteration 187/1000 | Loss: 0.00001601
Iteration 188/1000 | Loss: 0.00001601
Iteration 189/1000 | Loss: 0.00001601
Iteration 190/1000 | Loss: 0.00001601
Iteration 191/1000 | Loss: 0.00001601
Iteration 192/1000 | Loss: 0.00001601
Iteration 193/1000 | Loss: 0.00001601
Iteration 194/1000 | Loss: 0.00001601
Iteration 195/1000 | Loss: 0.00001601
Iteration 196/1000 | Loss: 0.00001601
Iteration 197/1000 | Loss: 0.00001601
Iteration 198/1000 | Loss: 0.00001601
Iteration 199/1000 | Loss: 0.00001601
Iteration 200/1000 | Loss: 0.00001601
Iteration 201/1000 | Loss: 0.00001601
Iteration 202/1000 | Loss: 0.00001600
Iteration 203/1000 | Loss: 0.00001600
Iteration 204/1000 | Loss: 0.00001600
Iteration 205/1000 | Loss: 0.00001600
Iteration 206/1000 | Loss: 0.00001600
Iteration 207/1000 | Loss: 0.00001600
Iteration 208/1000 | Loss: 0.00001600
Iteration 209/1000 | Loss: 0.00001600
Iteration 210/1000 | Loss: 0.00001600
Iteration 211/1000 | Loss: 0.00001600
Iteration 212/1000 | Loss: 0.00001600
Iteration 213/1000 | Loss: 0.00001600
Iteration 214/1000 | Loss: 0.00001600
Iteration 215/1000 | Loss: 0.00001600
Iteration 216/1000 | Loss: 0.00001600
Iteration 217/1000 | Loss: 0.00001600
Iteration 218/1000 | Loss: 0.00001600
Iteration 219/1000 | Loss: 0.00001600
Iteration 220/1000 | Loss: 0.00001600
Iteration 221/1000 | Loss: 0.00001600
Iteration 222/1000 | Loss: 0.00001600
Iteration 223/1000 | Loss: 0.00001600
Iteration 224/1000 | Loss: 0.00001600
Iteration 225/1000 | Loss: 0.00001600
Iteration 226/1000 | Loss: 0.00001600
Iteration 227/1000 | Loss: 0.00001600
Iteration 228/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.599992174305953e-05, 1.599992174305953e-05, 1.599992174305953e-05, 1.599992174305953e-05, 1.599992174305953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.599992174305953e-05

Optimization complete. Final v2v error: 3.391373872756958 mm

Highest mean error: 3.6399154663085938 mm for frame 181

Lowest mean error: 3.0347964763641357 mm for frame 12

Saving results

Total time: 47.091970920562744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770834
Iteration 2/25 | Loss: 0.00180930
Iteration 3/25 | Loss: 0.00139748
Iteration 4/25 | Loss: 0.00131395
Iteration 5/25 | Loss: 0.00126101
Iteration 6/25 | Loss: 0.00124064
Iteration 7/25 | Loss: 0.00123166
Iteration 8/25 | Loss: 0.00123027
Iteration 9/25 | Loss: 0.00122650
Iteration 10/25 | Loss: 0.00122828
Iteration 11/25 | Loss: 0.00122601
Iteration 12/25 | Loss: 0.00122567
Iteration 13/25 | Loss: 0.00122426
Iteration 14/25 | Loss: 0.00122166
Iteration 15/25 | Loss: 0.00122122
Iteration 16/25 | Loss: 0.00121827
Iteration 17/25 | Loss: 0.00121694
Iteration 18/25 | Loss: 0.00121647
Iteration 19/25 | Loss: 0.00121634
Iteration 20/25 | Loss: 0.00121628
Iteration 21/25 | Loss: 0.00121627
Iteration 22/25 | Loss: 0.00121627
Iteration 23/25 | Loss: 0.00121627
Iteration 24/25 | Loss: 0.00121627
Iteration 25/25 | Loss: 0.00121627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34148729
Iteration 2/25 | Loss: 0.00060290
Iteration 3/25 | Loss: 0.00060288
Iteration 4/25 | Loss: 0.00060288
Iteration 5/25 | Loss: 0.00060288
Iteration 6/25 | Loss: 0.00060288
Iteration 7/25 | Loss: 0.00060288
Iteration 8/25 | Loss: 0.00060288
Iteration 9/25 | Loss: 0.00060288
Iteration 10/25 | Loss: 0.00060288
Iteration 11/25 | Loss: 0.00060288
Iteration 12/25 | Loss: 0.00060288
Iteration 13/25 | Loss: 0.00060288
Iteration 14/25 | Loss: 0.00060288
Iteration 15/25 | Loss: 0.00060288
Iteration 16/25 | Loss: 0.00060288
Iteration 17/25 | Loss: 0.00060288
Iteration 18/25 | Loss: 0.00060288
Iteration 19/25 | Loss: 0.00060288
Iteration 20/25 | Loss: 0.00060288
Iteration 21/25 | Loss: 0.00060288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006028760108165443, 0.0006028760108165443, 0.0006028760108165443, 0.0006028760108165443, 0.0006028760108165443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006028760108165443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060288
Iteration 2/1000 | Loss: 0.00003747
Iteration 3/1000 | Loss: 0.00002390
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001975
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001859
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001828
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001823
Iteration 17/1000 | Loss: 0.00001820
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001817
Iteration 20/1000 | Loss: 0.00001817
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001811
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001809
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001807
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001807
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001806
Iteration 42/1000 | Loss: 0.00001806
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001806
Iteration 46/1000 | Loss: 0.00001806
Iteration 47/1000 | Loss: 0.00001806
Iteration 48/1000 | Loss: 0.00001806
Iteration 49/1000 | Loss: 0.00001806
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001802
Iteration 67/1000 | Loss: 0.00001802
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001801
Iteration 70/1000 | Loss: 0.00001801
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001799
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001799
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001798
Iteration 81/1000 | Loss: 0.00001798
Iteration 82/1000 | Loss: 0.00001797
Iteration 83/1000 | Loss: 0.00001797
Iteration 84/1000 | Loss: 0.00001797
Iteration 85/1000 | Loss: 0.00001797
Iteration 86/1000 | Loss: 0.00001797
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001792
Iteration 121/1000 | Loss: 0.00001792
Iteration 122/1000 | Loss: 0.00001791
Iteration 123/1000 | Loss: 0.00001791
Iteration 124/1000 | Loss: 0.00001791
Iteration 125/1000 | Loss: 0.00001791
Iteration 126/1000 | Loss: 0.00001791
Iteration 127/1000 | Loss: 0.00001791
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001791
Iteration 130/1000 | Loss: 0.00001790
Iteration 131/1000 | Loss: 0.00001790
Iteration 132/1000 | Loss: 0.00001790
Iteration 133/1000 | Loss: 0.00001790
Iteration 134/1000 | Loss: 0.00001790
Iteration 135/1000 | Loss: 0.00001789
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001789
Iteration 138/1000 | Loss: 0.00001789
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001789
Iteration 142/1000 | Loss: 0.00001789
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001788
Iteration 145/1000 | Loss: 0.00001788
Iteration 146/1000 | Loss: 0.00001788
Iteration 147/1000 | Loss: 0.00001788
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001788
Iteration 152/1000 | Loss: 0.00001788
Iteration 153/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.7883181499200873e-05, 1.7883181499200873e-05, 1.7883181499200873e-05, 1.7883181499200873e-05, 1.7883181499200873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7883181499200873e-05

Optimization complete. Final v2v error: 3.592902660369873 mm

Highest mean error: 3.9946959018707275 mm for frame 67

Lowest mean error: 3.2810237407684326 mm for frame 8

Saving results

Total time: 72.79604196548462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616144
Iteration 2/25 | Loss: 0.00127548
Iteration 3/25 | Loss: 0.00118143
Iteration 4/25 | Loss: 0.00116803
Iteration 5/25 | Loss: 0.00116376
Iteration 6/25 | Loss: 0.00116304
Iteration 7/25 | Loss: 0.00116304
Iteration 8/25 | Loss: 0.00116304
Iteration 9/25 | Loss: 0.00116304
Iteration 10/25 | Loss: 0.00116304
Iteration 11/25 | Loss: 0.00116304
Iteration 12/25 | Loss: 0.00116304
Iteration 13/25 | Loss: 0.00116304
Iteration 14/25 | Loss: 0.00116304
Iteration 15/25 | Loss: 0.00116304
Iteration 16/25 | Loss: 0.00116304
Iteration 17/25 | Loss: 0.00116304
Iteration 18/25 | Loss: 0.00116304
Iteration 19/25 | Loss: 0.00116304
Iteration 20/25 | Loss: 0.00116304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011630421504378319, 0.0011630421504378319, 0.0011630421504378319, 0.0011630421504378319, 0.0011630421504378319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011630421504378319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41203415
Iteration 2/25 | Loss: 0.00094343
Iteration 3/25 | Loss: 0.00094342
Iteration 4/25 | Loss: 0.00094342
Iteration 5/25 | Loss: 0.00094342
Iteration 6/25 | Loss: 0.00094342
Iteration 7/25 | Loss: 0.00094342
Iteration 8/25 | Loss: 0.00094342
Iteration 9/25 | Loss: 0.00094342
Iteration 10/25 | Loss: 0.00094342
Iteration 11/25 | Loss: 0.00094342
Iteration 12/25 | Loss: 0.00094342
Iteration 13/25 | Loss: 0.00094342
Iteration 14/25 | Loss: 0.00094342
Iteration 15/25 | Loss: 0.00094342
Iteration 16/25 | Loss: 0.00094342
Iteration 17/25 | Loss: 0.00094342
Iteration 18/25 | Loss: 0.00094342
Iteration 19/25 | Loss: 0.00094342
Iteration 20/25 | Loss: 0.00094342
Iteration 21/25 | Loss: 0.00094342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009434203384444118, 0.0009434203384444118, 0.0009434203384444118, 0.0009434203384444118, 0.0009434203384444118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009434203384444118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094342
Iteration 2/1000 | Loss: 0.00002144
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001391
Iteration 6/1000 | Loss: 0.00001343
Iteration 7/1000 | Loss: 0.00001314
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001233
Iteration 24/1000 | Loss: 0.00001232
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001206
Iteration 59/1000 | Loss: 0.00001206
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001203
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001199
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001199
Iteration 75/1000 | Loss: 0.00001199
Iteration 76/1000 | Loss: 0.00001199
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001199
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001199
Iteration 82/1000 | Loss: 0.00001198
Iteration 83/1000 | Loss: 0.00001198
Iteration 84/1000 | Loss: 0.00001198
Iteration 85/1000 | Loss: 0.00001198
Iteration 86/1000 | Loss: 0.00001198
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001197
Iteration 90/1000 | Loss: 0.00001197
Iteration 91/1000 | Loss: 0.00001197
Iteration 92/1000 | Loss: 0.00001196
Iteration 93/1000 | Loss: 0.00001196
Iteration 94/1000 | Loss: 0.00001196
Iteration 95/1000 | Loss: 0.00001196
Iteration 96/1000 | Loss: 0.00001196
Iteration 97/1000 | Loss: 0.00001196
Iteration 98/1000 | Loss: 0.00001196
Iteration 99/1000 | Loss: 0.00001196
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001191
Iteration 172/1000 | Loss: 0.00001191
Iteration 173/1000 | Loss: 0.00001191
Iteration 174/1000 | Loss: 0.00001191
Iteration 175/1000 | Loss: 0.00001191
Iteration 176/1000 | Loss: 0.00001191
Iteration 177/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.1905642168130726e-05, 1.1905642168130726e-05, 1.1905642168130726e-05, 1.1905642168130726e-05, 1.1905642168130726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1905642168130726e-05

Optimization complete. Final v2v error: 2.927504777908325 mm

Highest mean error: 3.460890293121338 mm for frame 7

Lowest mean error: 2.5889031887054443 mm for frame 66

Saving results

Total time: 40.32574200630188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537232
Iteration 2/25 | Loss: 0.00140327
Iteration 3/25 | Loss: 0.00123665
Iteration 4/25 | Loss: 0.00121818
Iteration 5/25 | Loss: 0.00122558
Iteration 6/25 | Loss: 0.00121285
Iteration 7/25 | Loss: 0.00120650
Iteration 8/25 | Loss: 0.00120490
Iteration 9/25 | Loss: 0.00120454
Iteration 10/25 | Loss: 0.00120454
Iteration 11/25 | Loss: 0.00120454
Iteration 12/25 | Loss: 0.00120454
Iteration 13/25 | Loss: 0.00120454
Iteration 14/25 | Loss: 0.00120454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012045445619150996, 0.0012045445619150996, 0.0012045445619150996, 0.0012045445619150996, 0.0012045445619150996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012045445619150996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39136279
Iteration 2/25 | Loss: 0.00096458
Iteration 3/25 | Loss: 0.00096458
Iteration 4/25 | Loss: 0.00096458
Iteration 5/25 | Loss: 0.00096458
Iteration 6/25 | Loss: 0.00096458
Iteration 7/25 | Loss: 0.00096458
Iteration 8/25 | Loss: 0.00096458
Iteration 9/25 | Loss: 0.00096458
Iteration 10/25 | Loss: 0.00096458
Iteration 11/25 | Loss: 0.00096458
Iteration 12/25 | Loss: 0.00096458
Iteration 13/25 | Loss: 0.00096458
Iteration 14/25 | Loss: 0.00096458
Iteration 15/25 | Loss: 0.00096458
Iteration 16/25 | Loss: 0.00096458
Iteration 17/25 | Loss: 0.00096458
Iteration 18/25 | Loss: 0.00096458
Iteration 19/25 | Loss: 0.00096458
Iteration 20/25 | Loss: 0.00096458
Iteration 21/25 | Loss: 0.00096458
Iteration 22/25 | Loss: 0.00096458
Iteration 23/25 | Loss: 0.00096458
Iteration 24/25 | Loss: 0.00096458
Iteration 25/25 | Loss: 0.00096458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009645795216783881, 0.0009645795216783881, 0.0009645795216783881, 0.0009645795216783881, 0.0009645795216783881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009645795216783881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096458
Iteration 2/1000 | Loss: 0.00004252
Iteration 3/1000 | Loss: 0.00003014
Iteration 4/1000 | Loss: 0.00002470
Iteration 5/1000 | Loss: 0.00002310
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002109
Iteration 8/1000 | Loss: 0.00002056
Iteration 9/1000 | Loss: 0.00002019
Iteration 10/1000 | Loss: 0.00001986
Iteration 11/1000 | Loss: 0.00001958
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001918
Iteration 14/1000 | Loss: 0.00001914
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001902
Iteration 21/1000 | Loss: 0.00001902
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001897
Iteration 27/1000 | Loss: 0.00001897
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001893
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001892
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001889
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001889
Iteration 45/1000 | Loss: 0.00001889
Iteration 46/1000 | Loss: 0.00001889
Iteration 47/1000 | Loss: 0.00001889
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001887
Iteration 54/1000 | Loss: 0.00001886
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001885
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001885
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001884
Iteration 62/1000 | Loss: 0.00001884
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001883
Iteration 65/1000 | Loss: 0.00001883
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001882
Iteration 68/1000 | Loss: 0.00001882
Iteration 69/1000 | Loss: 0.00001882
Iteration 70/1000 | Loss: 0.00001882
Iteration 71/1000 | Loss: 0.00001882
Iteration 72/1000 | Loss: 0.00001882
Iteration 73/1000 | Loss: 0.00001882
Iteration 74/1000 | Loss: 0.00001882
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001882
Iteration 77/1000 | Loss: 0.00001882
Iteration 78/1000 | Loss: 0.00001881
Iteration 79/1000 | Loss: 0.00001881
Iteration 80/1000 | Loss: 0.00001881
Iteration 81/1000 | Loss: 0.00001880
Iteration 82/1000 | Loss: 0.00001880
Iteration 83/1000 | Loss: 0.00001880
Iteration 84/1000 | Loss: 0.00001880
Iteration 85/1000 | Loss: 0.00001879
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001879
Iteration 88/1000 | Loss: 0.00001878
Iteration 89/1000 | Loss: 0.00001878
Iteration 90/1000 | Loss: 0.00001878
Iteration 91/1000 | Loss: 0.00001878
Iteration 92/1000 | Loss: 0.00001878
Iteration 93/1000 | Loss: 0.00001877
Iteration 94/1000 | Loss: 0.00001877
Iteration 95/1000 | Loss: 0.00001877
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001876
Iteration 98/1000 | Loss: 0.00001876
Iteration 99/1000 | Loss: 0.00001876
Iteration 100/1000 | Loss: 0.00001876
Iteration 101/1000 | Loss: 0.00001875
Iteration 102/1000 | Loss: 0.00001875
Iteration 103/1000 | Loss: 0.00001875
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001875
Iteration 107/1000 | Loss: 0.00001874
Iteration 108/1000 | Loss: 0.00001874
Iteration 109/1000 | Loss: 0.00001873
Iteration 110/1000 | Loss: 0.00001873
Iteration 111/1000 | Loss: 0.00001873
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001872
Iteration 115/1000 | Loss: 0.00001872
Iteration 116/1000 | Loss: 0.00001872
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001869
Iteration 130/1000 | Loss: 0.00001869
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001868
Iteration 134/1000 | Loss: 0.00001868
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001868
Iteration 137/1000 | Loss: 0.00001868
Iteration 138/1000 | Loss: 0.00001868
Iteration 139/1000 | Loss: 0.00001868
Iteration 140/1000 | Loss: 0.00001868
Iteration 141/1000 | Loss: 0.00001868
Iteration 142/1000 | Loss: 0.00001868
Iteration 143/1000 | Loss: 0.00001868
Iteration 144/1000 | Loss: 0.00001868
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001867
Iteration 148/1000 | Loss: 0.00001867
Iteration 149/1000 | Loss: 0.00001867
Iteration 150/1000 | Loss: 0.00001867
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001866
Iteration 157/1000 | Loss: 0.00001866
Iteration 158/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.866209822765086e-05, 1.866209822765086e-05, 1.866209822765086e-05, 1.866209822765086e-05, 1.866209822765086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.866209822765086e-05

Optimization complete. Final v2v error: 3.5391194820404053 mm

Highest mean error: 4.543667793273926 mm for frame 32

Lowest mean error: 2.7833056449890137 mm for frame 88

Saving results

Total time: 49.83853793144226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022504
Iteration 2/25 | Loss: 0.00176266
Iteration 3/25 | Loss: 0.00125691
Iteration 4/25 | Loss: 0.00119875
Iteration 5/25 | Loss: 0.00119627
Iteration 6/25 | Loss: 0.00118048
Iteration 7/25 | Loss: 0.00117036
Iteration 8/25 | Loss: 0.00115988
Iteration 9/25 | Loss: 0.00115006
Iteration 10/25 | Loss: 0.00114371
Iteration 11/25 | Loss: 0.00114250
Iteration 12/25 | Loss: 0.00114149
Iteration 13/25 | Loss: 0.00113956
Iteration 14/25 | Loss: 0.00113870
Iteration 15/25 | Loss: 0.00113842
Iteration 16/25 | Loss: 0.00113833
Iteration 17/25 | Loss: 0.00113832
Iteration 18/25 | Loss: 0.00113832
Iteration 19/25 | Loss: 0.00113832
Iteration 20/25 | Loss: 0.00113832
Iteration 21/25 | Loss: 0.00113832
Iteration 22/25 | Loss: 0.00113832
Iteration 23/25 | Loss: 0.00113832
Iteration 24/25 | Loss: 0.00113832
Iteration 25/25 | Loss: 0.00113832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38434052
Iteration 2/25 | Loss: 0.00088111
Iteration 3/25 | Loss: 0.00088090
Iteration 4/25 | Loss: 0.00087777
Iteration 5/25 | Loss: 0.00087777
Iteration 6/25 | Loss: 0.00087777
Iteration 7/25 | Loss: 0.00087777
Iteration 8/25 | Loss: 0.00087777
Iteration 9/25 | Loss: 0.00087777
Iteration 10/25 | Loss: 0.00087777
Iteration 11/25 | Loss: 0.00087777
Iteration 12/25 | Loss: 0.00087777
Iteration 13/25 | Loss: 0.00087777
Iteration 14/25 | Loss: 0.00087777
Iteration 15/25 | Loss: 0.00087777
Iteration 16/25 | Loss: 0.00087777
Iteration 17/25 | Loss: 0.00087777
Iteration 18/25 | Loss: 0.00087777
Iteration 19/25 | Loss: 0.00087777
Iteration 20/25 | Loss: 0.00087777
Iteration 21/25 | Loss: 0.00087777
Iteration 22/25 | Loss: 0.00087777
Iteration 23/25 | Loss: 0.00087777
Iteration 24/25 | Loss: 0.00087777
Iteration 25/25 | Loss: 0.00087777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087777
Iteration 2/1000 | Loss: 0.00003174
Iteration 3/1000 | Loss: 0.00001799
Iteration 4/1000 | Loss: 0.00003848
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00023801
Iteration 7/1000 | Loss: 0.00001431
Iteration 8/1000 | Loss: 0.00003531
Iteration 9/1000 | Loss: 0.00005713
Iteration 10/1000 | Loss: 0.00020396
Iteration 11/1000 | Loss: 0.00003831
Iteration 12/1000 | Loss: 0.00001562
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00002201
Iteration 15/1000 | Loss: 0.00003037
Iteration 16/1000 | Loss: 0.00002683
Iteration 17/1000 | Loss: 0.00001460
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001293
Iteration 21/1000 | Loss: 0.00002078
Iteration 22/1000 | Loss: 0.00004221
Iteration 23/1000 | Loss: 0.00001538
Iteration 24/1000 | Loss: 0.00002761
Iteration 25/1000 | Loss: 0.00018579
Iteration 26/1000 | Loss: 0.00003755
Iteration 27/1000 | Loss: 0.00001302
Iteration 28/1000 | Loss: 0.00002317
Iteration 29/1000 | Loss: 0.00005208
Iteration 30/1000 | Loss: 0.00002898
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00003105
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00002491
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00014140
Iteration 48/1000 | Loss: 0.00009175
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00012128
Iteration 51/1000 | Loss: 0.00007423
Iteration 52/1000 | Loss: 0.00006802
Iteration 53/1000 | Loss: 0.00005875
Iteration 54/1000 | Loss: 0.00001588
Iteration 55/1000 | Loss: 0.00006979
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00003497
Iteration 58/1000 | Loss: 0.00002723
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002473
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00002950
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.2342138688836712e-05, 1.2342138688836712e-05, 1.2342138688836712e-05, 1.2342138688836712e-05, 1.2342138688836712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2342138688836712e-05

Optimization complete. Final v2v error: 2.9951329231262207 mm

Highest mean error: 3.529728889465332 mm for frame 1

Lowest mean error: 2.5632667541503906 mm for frame 34

Saving results

Total time: 116.20469117164612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422218
Iteration 2/25 | Loss: 0.00124903
Iteration 3/25 | Loss: 0.00115476
Iteration 4/25 | Loss: 0.00114239
Iteration 5/25 | Loss: 0.00113905
Iteration 6/25 | Loss: 0.00113815
Iteration 7/25 | Loss: 0.00113815
Iteration 8/25 | Loss: 0.00113815
Iteration 9/25 | Loss: 0.00113815
Iteration 10/25 | Loss: 0.00113815
Iteration 11/25 | Loss: 0.00113815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011381511576473713, 0.0011381511576473713, 0.0011381511576473713, 0.0011381511576473713, 0.0011381511576473713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011381511576473713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35942149
Iteration 2/25 | Loss: 0.00089851
Iteration 3/25 | Loss: 0.00089851
Iteration 4/25 | Loss: 0.00089851
Iteration 5/25 | Loss: 0.00089851
Iteration 6/25 | Loss: 0.00089851
Iteration 7/25 | Loss: 0.00089851
Iteration 8/25 | Loss: 0.00089851
Iteration 9/25 | Loss: 0.00089851
Iteration 10/25 | Loss: 0.00089851
Iteration 11/25 | Loss: 0.00089851
Iteration 12/25 | Loss: 0.00089851
Iteration 13/25 | Loss: 0.00089851
Iteration 14/25 | Loss: 0.00089851
Iteration 15/25 | Loss: 0.00089851
Iteration 16/25 | Loss: 0.00089851
Iteration 17/25 | Loss: 0.00089851
Iteration 18/25 | Loss: 0.00089851
Iteration 19/25 | Loss: 0.00089851
Iteration 20/25 | Loss: 0.00089851
Iteration 21/25 | Loss: 0.00089851
Iteration 22/25 | Loss: 0.00089851
Iteration 23/25 | Loss: 0.00089851
Iteration 24/25 | Loss: 0.00089851
Iteration 25/25 | Loss: 0.00089851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089851
Iteration 2/1000 | Loss: 0.00002808
Iteration 3/1000 | Loss: 0.00001649
Iteration 4/1000 | Loss: 0.00001431
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001275
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001124
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001120
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001111
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001109
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001108
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001105
Iteration 43/1000 | Loss: 0.00001104
Iteration 44/1000 | Loss: 0.00001104
Iteration 45/1000 | Loss: 0.00001104
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001104
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001100
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001098
Iteration 74/1000 | Loss: 0.00001098
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001093
Iteration 98/1000 | Loss: 0.00001093
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.0888890756177716e-05, 1.0888890756177716e-05, 1.0888890756177716e-05, 1.0888890756177716e-05, 1.0888890756177716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0888890756177716e-05

Optimization complete. Final v2v error: 2.8219094276428223 mm

Highest mean error: 3.8693430423736572 mm for frame 64

Lowest mean error: 2.5433666706085205 mm for frame 182

Saving results

Total time: 40.84332728385925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421573
Iteration 2/25 | Loss: 0.00133651
Iteration 3/25 | Loss: 0.00121763
Iteration 4/25 | Loss: 0.00120647
Iteration 5/25 | Loss: 0.00120299
Iteration 6/25 | Loss: 0.00120201
Iteration 7/25 | Loss: 0.00120183
Iteration 8/25 | Loss: 0.00120183
Iteration 9/25 | Loss: 0.00120183
Iteration 10/25 | Loss: 0.00120183
Iteration 11/25 | Loss: 0.00120183
Iteration 12/25 | Loss: 0.00120183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012018269626423717, 0.0012018269626423717, 0.0012018269626423717, 0.0012018269626423717, 0.0012018269626423717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012018269626423717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88700485
Iteration 2/25 | Loss: 0.00079968
Iteration 3/25 | Loss: 0.00079966
Iteration 4/25 | Loss: 0.00079966
Iteration 5/25 | Loss: 0.00079966
Iteration 6/25 | Loss: 0.00079966
Iteration 7/25 | Loss: 0.00079966
Iteration 8/25 | Loss: 0.00079966
Iteration 9/25 | Loss: 0.00079966
Iteration 10/25 | Loss: 0.00079966
Iteration 11/25 | Loss: 0.00079966
Iteration 12/25 | Loss: 0.00079966
Iteration 13/25 | Loss: 0.00079966
Iteration 14/25 | Loss: 0.00079966
Iteration 15/25 | Loss: 0.00079966
Iteration 16/25 | Loss: 0.00079966
Iteration 17/25 | Loss: 0.00079966
Iteration 18/25 | Loss: 0.00079966
Iteration 19/25 | Loss: 0.00079966
Iteration 20/25 | Loss: 0.00079966
Iteration 21/25 | Loss: 0.00079966
Iteration 22/25 | Loss: 0.00079966
Iteration 23/25 | Loss: 0.00079966
Iteration 24/25 | Loss: 0.00079966
Iteration 25/25 | Loss: 0.00079966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079966
Iteration 2/1000 | Loss: 0.00003506
Iteration 3/1000 | Loss: 0.00002491
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002147
Iteration 7/1000 | Loss: 0.00002091
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00002016
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001952
Iteration 13/1000 | Loss: 0.00001939
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001932
Iteration 20/1000 | Loss: 0.00001932
Iteration 21/1000 | Loss: 0.00001931
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001923
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001919
Iteration 44/1000 | Loss: 0.00001919
Iteration 45/1000 | Loss: 0.00001919
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001916
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001914
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001912
Iteration 64/1000 | Loss: 0.00001912
Iteration 65/1000 | Loss: 0.00001912
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001912
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001911
Iteration 75/1000 | Loss: 0.00001911
Iteration 76/1000 | Loss: 0.00001911
Iteration 77/1000 | Loss: 0.00001911
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001910
Iteration 84/1000 | Loss: 0.00001910
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001910
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001907
Iteration 103/1000 | Loss: 0.00001907
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001907
Iteration 110/1000 | Loss: 0.00001907
Iteration 111/1000 | Loss: 0.00001907
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001903
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001903
Iteration 131/1000 | Loss: 0.00001902
Iteration 132/1000 | Loss: 0.00001902
Iteration 133/1000 | Loss: 0.00001902
Iteration 134/1000 | Loss: 0.00001901
Iteration 135/1000 | Loss: 0.00001901
Iteration 136/1000 | Loss: 0.00001901
Iteration 137/1000 | Loss: 0.00001901
Iteration 138/1000 | Loss: 0.00001901
Iteration 139/1000 | Loss: 0.00001901
Iteration 140/1000 | Loss: 0.00001901
Iteration 141/1000 | Loss: 0.00001901
Iteration 142/1000 | Loss: 0.00001901
Iteration 143/1000 | Loss: 0.00001901
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001900
Iteration 147/1000 | Loss: 0.00001900
Iteration 148/1000 | Loss: 0.00001900
Iteration 149/1000 | Loss: 0.00001900
Iteration 150/1000 | Loss: 0.00001900
Iteration 151/1000 | Loss: 0.00001900
Iteration 152/1000 | Loss: 0.00001900
Iteration 153/1000 | Loss: 0.00001900
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001899
Iteration 158/1000 | Loss: 0.00001899
Iteration 159/1000 | Loss: 0.00001899
Iteration 160/1000 | Loss: 0.00001899
Iteration 161/1000 | Loss: 0.00001899
Iteration 162/1000 | Loss: 0.00001899
Iteration 163/1000 | Loss: 0.00001899
Iteration 164/1000 | Loss: 0.00001899
Iteration 165/1000 | Loss: 0.00001899
Iteration 166/1000 | Loss: 0.00001899
Iteration 167/1000 | Loss: 0.00001899
Iteration 168/1000 | Loss: 0.00001899
Iteration 169/1000 | Loss: 0.00001899
Iteration 170/1000 | Loss: 0.00001899
Iteration 171/1000 | Loss: 0.00001899
Iteration 172/1000 | Loss: 0.00001898
Iteration 173/1000 | Loss: 0.00001898
Iteration 174/1000 | Loss: 0.00001898
Iteration 175/1000 | Loss: 0.00001898
Iteration 176/1000 | Loss: 0.00001898
Iteration 177/1000 | Loss: 0.00001898
Iteration 178/1000 | Loss: 0.00001898
Iteration 179/1000 | Loss: 0.00001898
Iteration 180/1000 | Loss: 0.00001898
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001898
Iteration 183/1000 | Loss: 0.00001898
Iteration 184/1000 | Loss: 0.00001898
Iteration 185/1000 | Loss: 0.00001897
Iteration 186/1000 | Loss: 0.00001897
Iteration 187/1000 | Loss: 0.00001897
Iteration 188/1000 | Loss: 0.00001897
Iteration 189/1000 | Loss: 0.00001897
Iteration 190/1000 | Loss: 0.00001897
Iteration 191/1000 | Loss: 0.00001897
Iteration 192/1000 | Loss: 0.00001897
Iteration 193/1000 | Loss: 0.00001897
Iteration 194/1000 | Loss: 0.00001897
Iteration 195/1000 | Loss: 0.00001897
Iteration 196/1000 | Loss: 0.00001897
Iteration 197/1000 | Loss: 0.00001897
Iteration 198/1000 | Loss: 0.00001897
Iteration 199/1000 | Loss: 0.00001897
Iteration 200/1000 | Loss: 0.00001897
Iteration 201/1000 | Loss: 0.00001897
Iteration 202/1000 | Loss: 0.00001897
Iteration 203/1000 | Loss: 0.00001896
Iteration 204/1000 | Loss: 0.00001896
Iteration 205/1000 | Loss: 0.00001896
Iteration 206/1000 | Loss: 0.00001896
Iteration 207/1000 | Loss: 0.00001896
Iteration 208/1000 | Loss: 0.00001896
Iteration 209/1000 | Loss: 0.00001896
Iteration 210/1000 | Loss: 0.00001896
Iteration 211/1000 | Loss: 0.00001896
Iteration 212/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.8963997717946768e-05, 1.8963997717946768e-05, 1.8963997717946768e-05, 1.8963997717946768e-05, 1.8963997717946768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8963997717946768e-05

Optimization complete. Final v2v error: 3.682201385498047 mm

Highest mean error: 4.358495235443115 mm for frame 49

Lowest mean error: 3.288134813308716 mm for frame 1

Saving results

Total time: 43.95160436630249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403900
Iteration 2/25 | Loss: 0.00125649
Iteration 3/25 | Loss: 0.00117134
Iteration 4/25 | Loss: 0.00116430
Iteration 5/25 | Loss: 0.00116291
Iteration 6/25 | Loss: 0.00116291
Iteration 7/25 | Loss: 0.00116291
Iteration 8/25 | Loss: 0.00116291
Iteration 9/25 | Loss: 0.00116291
Iteration 10/25 | Loss: 0.00116291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011629116488620639, 0.0011629116488620639, 0.0011629116488620639, 0.0011629116488620639, 0.0011629116488620639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011629116488620639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35649145
Iteration 2/25 | Loss: 0.00081644
Iteration 3/25 | Loss: 0.00081644
Iteration 4/25 | Loss: 0.00081644
Iteration 5/25 | Loss: 0.00081644
Iteration 6/25 | Loss: 0.00081644
Iteration 7/25 | Loss: 0.00081644
Iteration 8/25 | Loss: 0.00081644
Iteration 9/25 | Loss: 0.00081644
Iteration 10/25 | Loss: 0.00081644
Iteration 11/25 | Loss: 0.00081644
Iteration 12/25 | Loss: 0.00081644
Iteration 13/25 | Loss: 0.00081644
Iteration 14/25 | Loss: 0.00081644
Iteration 15/25 | Loss: 0.00081644
Iteration 16/25 | Loss: 0.00081644
Iteration 17/25 | Loss: 0.00081644
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008164398605003953, 0.0008164398605003953, 0.0008164398605003953, 0.0008164398605003953, 0.0008164398605003953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008164398605003953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081644
Iteration 2/1000 | Loss: 0.00002239
Iteration 3/1000 | Loss: 0.00001644
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001230
Iteration 19/1000 | Loss: 0.00001230
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001217
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001216
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001213
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001189
Iteration 155/1000 | Loss: 0.00001189
Iteration 156/1000 | Loss: 0.00001189
Iteration 157/1000 | Loss: 0.00001189
Iteration 158/1000 | Loss: 0.00001189
Iteration 159/1000 | Loss: 0.00001189
Iteration 160/1000 | Loss: 0.00001188
Iteration 161/1000 | Loss: 0.00001188
Iteration 162/1000 | Loss: 0.00001188
Iteration 163/1000 | Loss: 0.00001187
Iteration 164/1000 | Loss: 0.00001187
Iteration 165/1000 | Loss: 0.00001187
Iteration 166/1000 | Loss: 0.00001187
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001187
Iteration 169/1000 | Loss: 0.00001186
Iteration 170/1000 | Loss: 0.00001186
Iteration 171/1000 | Loss: 0.00001186
Iteration 172/1000 | Loss: 0.00001186
Iteration 173/1000 | Loss: 0.00001186
Iteration 174/1000 | Loss: 0.00001186
Iteration 175/1000 | Loss: 0.00001185
Iteration 176/1000 | Loss: 0.00001185
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001184
Iteration 182/1000 | Loss: 0.00001184
Iteration 183/1000 | Loss: 0.00001184
Iteration 184/1000 | Loss: 0.00001184
Iteration 185/1000 | Loss: 0.00001184
Iteration 186/1000 | Loss: 0.00001184
Iteration 187/1000 | Loss: 0.00001184
Iteration 188/1000 | Loss: 0.00001184
Iteration 189/1000 | Loss: 0.00001184
Iteration 190/1000 | Loss: 0.00001184
Iteration 191/1000 | Loss: 0.00001184
Iteration 192/1000 | Loss: 0.00001183
Iteration 193/1000 | Loss: 0.00001183
Iteration 194/1000 | Loss: 0.00001183
Iteration 195/1000 | Loss: 0.00001183
Iteration 196/1000 | Loss: 0.00001183
Iteration 197/1000 | Loss: 0.00001183
Iteration 198/1000 | Loss: 0.00001183
Iteration 199/1000 | Loss: 0.00001183
Iteration 200/1000 | Loss: 0.00001183
Iteration 201/1000 | Loss: 0.00001183
Iteration 202/1000 | Loss: 0.00001183
Iteration 203/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.1834567885671277e-05, 1.1834567885671277e-05, 1.1834567885671277e-05, 1.1834567885671277e-05, 1.1834567885671277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1834567885671277e-05

Optimization complete. Final v2v error: 2.9202229976654053 mm

Highest mean error: 3.2145943641662598 mm for frame 47

Lowest mean error: 2.7802040576934814 mm for frame 136

Saving results

Total time: 40.810428619384766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806120
Iteration 2/25 | Loss: 0.00138598
Iteration 3/25 | Loss: 0.00115868
Iteration 4/25 | Loss: 0.00113507
Iteration 5/25 | Loss: 0.00113184
Iteration 6/25 | Loss: 0.00113100
Iteration 7/25 | Loss: 0.00113073
Iteration 8/25 | Loss: 0.00113070
Iteration 9/25 | Loss: 0.00113070
Iteration 10/25 | Loss: 0.00113070
Iteration 11/25 | Loss: 0.00113070
Iteration 12/25 | Loss: 0.00113070
Iteration 13/25 | Loss: 0.00113070
Iteration 14/25 | Loss: 0.00113070
Iteration 15/25 | Loss: 0.00113070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011307018576189876, 0.0011307018576189876, 0.0011307018576189876, 0.0011307018576189876, 0.0011307018576189876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011307018576189876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36548221
Iteration 2/25 | Loss: 0.00084899
Iteration 3/25 | Loss: 0.00084899
Iteration 4/25 | Loss: 0.00084899
Iteration 5/25 | Loss: 0.00084899
Iteration 6/25 | Loss: 0.00084899
Iteration 7/25 | Loss: 0.00084899
Iteration 8/25 | Loss: 0.00084899
Iteration 9/25 | Loss: 0.00084899
Iteration 10/25 | Loss: 0.00084899
Iteration 11/25 | Loss: 0.00084899
Iteration 12/25 | Loss: 0.00084899
Iteration 13/25 | Loss: 0.00084899
Iteration 14/25 | Loss: 0.00084899
Iteration 15/25 | Loss: 0.00084899
Iteration 16/25 | Loss: 0.00084899
Iteration 17/25 | Loss: 0.00084899
Iteration 18/25 | Loss: 0.00084899
Iteration 19/25 | Loss: 0.00084899
Iteration 20/25 | Loss: 0.00084899
Iteration 21/25 | Loss: 0.00084899
Iteration 22/25 | Loss: 0.00084899
Iteration 23/25 | Loss: 0.00084899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008489852189086378, 0.0008489852189086378, 0.0008489852189086378, 0.0008489852189086378, 0.0008489852189086378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008489852189086378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084899
Iteration 2/1000 | Loss: 0.00002027
Iteration 3/1000 | Loss: 0.00001401
Iteration 4/1000 | Loss: 0.00001284
Iteration 5/1000 | Loss: 0.00001189
Iteration 6/1000 | Loss: 0.00001137
Iteration 7/1000 | Loss: 0.00001098
Iteration 8/1000 | Loss: 0.00001066
Iteration 9/1000 | Loss: 0.00001062
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001016
Iteration 12/1000 | Loss: 0.00001005
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00000997
Iteration 15/1000 | Loss: 0.00000997
Iteration 16/1000 | Loss: 0.00000996
Iteration 17/1000 | Loss: 0.00000995
Iteration 18/1000 | Loss: 0.00000995
Iteration 19/1000 | Loss: 0.00000995
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000994
Iteration 23/1000 | Loss: 0.00000994
Iteration 24/1000 | Loss: 0.00000994
Iteration 25/1000 | Loss: 0.00000993
Iteration 26/1000 | Loss: 0.00000992
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000992
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000990
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000988
Iteration 36/1000 | Loss: 0.00000988
Iteration 37/1000 | Loss: 0.00000987
Iteration 38/1000 | Loss: 0.00000987
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000987
Iteration 42/1000 | Loss: 0.00000987
Iteration 43/1000 | Loss: 0.00000987
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000985
Iteration 46/1000 | Loss: 0.00000985
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000984
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000984
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000983
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000982
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000981
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000981
Iteration 70/1000 | Loss: 0.00000981
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000981
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000980
Iteration 77/1000 | Loss: 0.00000980
Iteration 78/1000 | Loss: 0.00000980
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000980
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000978
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000974
Iteration 119/1000 | Loss: 0.00000974
Iteration 120/1000 | Loss: 0.00000974
Iteration 121/1000 | Loss: 0.00000974
Iteration 122/1000 | Loss: 0.00000974
Iteration 123/1000 | Loss: 0.00000974
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000972
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000972
Iteration 133/1000 | Loss: 0.00000972
Iteration 134/1000 | Loss: 0.00000972
Iteration 135/1000 | Loss: 0.00000972
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Iteration 138/1000 | Loss: 0.00000971
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000971
Iteration 144/1000 | Loss: 0.00000971
Iteration 145/1000 | Loss: 0.00000971
Iteration 146/1000 | Loss: 0.00000970
Iteration 147/1000 | Loss: 0.00000970
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000970
Iteration 156/1000 | Loss: 0.00000970
Iteration 157/1000 | Loss: 0.00000970
Iteration 158/1000 | Loss: 0.00000970
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000969
Iteration 161/1000 | Loss: 0.00000969
Iteration 162/1000 | Loss: 0.00000969
Iteration 163/1000 | Loss: 0.00000969
Iteration 164/1000 | Loss: 0.00000969
Iteration 165/1000 | Loss: 0.00000969
Iteration 166/1000 | Loss: 0.00000969
Iteration 167/1000 | Loss: 0.00000969
Iteration 168/1000 | Loss: 0.00000969
Iteration 169/1000 | Loss: 0.00000969
Iteration 170/1000 | Loss: 0.00000969
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000968
Iteration 173/1000 | Loss: 0.00000968
Iteration 174/1000 | Loss: 0.00000968
Iteration 175/1000 | Loss: 0.00000968
Iteration 176/1000 | Loss: 0.00000968
Iteration 177/1000 | Loss: 0.00000968
Iteration 178/1000 | Loss: 0.00000968
Iteration 179/1000 | Loss: 0.00000968
Iteration 180/1000 | Loss: 0.00000968
Iteration 181/1000 | Loss: 0.00000968
Iteration 182/1000 | Loss: 0.00000968
Iteration 183/1000 | Loss: 0.00000968
Iteration 184/1000 | Loss: 0.00000968
Iteration 185/1000 | Loss: 0.00000968
Iteration 186/1000 | Loss: 0.00000968
Iteration 187/1000 | Loss: 0.00000968
Iteration 188/1000 | Loss: 0.00000968
Iteration 189/1000 | Loss: 0.00000967
Iteration 190/1000 | Loss: 0.00000967
Iteration 191/1000 | Loss: 0.00000967
Iteration 192/1000 | Loss: 0.00000967
Iteration 193/1000 | Loss: 0.00000967
Iteration 194/1000 | Loss: 0.00000967
Iteration 195/1000 | Loss: 0.00000966
Iteration 196/1000 | Loss: 0.00000966
Iteration 197/1000 | Loss: 0.00000966
Iteration 198/1000 | Loss: 0.00000966
Iteration 199/1000 | Loss: 0.00000966
Iteration 200/1000 | Loss: 0.00000965
Iteration 201/1000 | Loss: 0.00000965
Iteration 202/1000 | Loss: 0.00000965
Iteration 203/1000 | Loss: 0.00000965
Iteration 204/1000 | Loss: 0.00000965
Iteration 205/1000 | Loss: 0.00000965
Iteration 206/1000 | Loss: 0.00000965
Iteration 207/1000 | Loss: 0.00000965
Iteration 208/1000 | Loss: 0.00000965
Iteration 209/1000 | Loss: 0.00000964
Iteration 210/1000 | Loss: 0.00000964
Iteration 211/1000 | Loss: 0.00000964
Iteration 212/1000 | Loss: 0.00000964
Iteration 213/1000 | Loss: 0.00000963
Iteration 214/1000 | Loss: 0.00000963
Iteration 215/1000 | Loss: 0.00000963
Iteration 216/1000 | Loss: 0.00000963
Iteration 217/1000 | Loss: 0.00000963
Iteration 218/1000 | Loss: 0.00000963
Iteration 219/1000 | Loss: 0.00000963
Iteration 220/1000 | Loss: 0.00000963
Iteration 221/1000 | Loss: 0.00000963
Iteration 222/1000 | Loss: 0.00000963
Iteration 223/1000 | Loss: 0.00000963
Iteration 224/1000 | Loss: 0.00000963
Iteration 225/1000 | Loss: 0.00000963
Iteration 226/1000 | Loss: 0.00000963
Iteration 227/1000 | Loss: 0.00000963
Iteration 228/1000 | Loss: 0.00000962
Iteration 229/1000 | Loss: 0.00000962
Iteration 230/1000 | Loss: 0.00000962
Iteration 231/1000 | Loss: 0.00000962
Iteration 232/1000 | Loss: 0.00000962
Iteration 233/1000 | Loss: 0.00000962
Iteration 234/1000 | Loss: 0.00000962
Iteration 235/1000 | Loss: 0.00000962
Iteration 236/1000 | Loss: 0.00000962
Iteration 237/1000 | Loss: 0.00000962
Iteration 238/1000 | Loss: 0.00000962
Iteration 239/1000 | Loss: 0.00000962
Iteration 240/1000 | Loss: 0.00000962
Iteration 241/1000 | Loss: 0.00000962
Iteration 242/1000 | Loss: 0.00000962
Iteration 243/1000 | Loss: 0.00000962
Iteration 244/1000 | Loss: 0.00000962
Iteration 245/1000 | Loss: 0.00000962
Iteration 246/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [9.615308044885751e-06, 9.615308044885751e-06, 9.615308044885751e-06, 9.615308044885751e-06, 9.615308044885751e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.615308044885751e-06

Optimization complete. Final v2v error: 2.6660804748535156 mm

Highest mean error: 3.013190507888794 mm for frame 103

Lowest mean error: 2.541667938232422 mm for frame 16

Saving results

Total time: 47.14135527610779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824558
Iteration 2/25 | Loss: 0.00129586
Iteration 3/25 | Loss: 0.00117899
Iteration 4/25 | Loss: 0.00115926
Iteration 5/25 | Loss: 0.00115260
Iteration 6/25 | Loss: 0.00115040
Iteration 7/25 | Loss: 0.00114943
Iteration 8/25 | Loss: 0.00114805
Iteration 9/25 | Loss: 0.00114732
Iteration 10/25 | Loss: 0.00114700
Iteration 11/25 | Loss: 0.00114678
Iteration 12/25 | Loss: 0.00114676
Iteration 13/25 | Loss: 0.00114675
Iteration 14/25 | Loss: 0.00114674
Iteration 15/25 | Loss: 0.00114674
Iteration 16/25 | Loss: 0.00114669
Iteration 17/25 | Loss: 0.00114668
Iteration 18/25 | Loss: 0.00114668
Iteration 19/25 | Loss: 0.00114668
Iteration 20/25 | Loss: 0.00114668
Iteration 21/25 | Loss: 0.00114668
Iteration 22/25 | Loss: 0.00114667
Iteration 23/25 | Loss: 0.00114666
Iteration 24/25 | Loss: 0.00114666
Iteration 25/25 | Loss: 0.00114666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65856647
Iteration 2/25 | Loss: 0.00080597
Iteration 3/25 | Loss: 0.00080596
Iteration 4/25 | Loss: 0.00080596
Iteration 5/25 | Loss: 0.00080596
Iteration 6/25 | Loss: 0.00080596
Iteration 7/25 | Loss: 0.00080596
Iteration 8/25 | Loss: 0.00080596
Iteration 9/25 | Loss: 0.00080596
Iteration 10/25 | Loss: 0.00080596
Iteration 11/25 | Loss: 0.00080596
Iteration 12/25 | Loss: 0.00080596
Iteration 13/25 | Loss: 0.00080596
Iteration 14/25 | Loss: 0.00080596
Iteration 15/25 | Loss: 0.00080596
Iteration 16/25 | Loss: 0.00080595
Iteration 17/25 | Loss: 0.00080595
Iteration 18/25 | Loss: 0.00080595
Iteration 19/25 | Loss: 0.00080595
Iteration 20/25 | Loss: 0.00080595
Iteration 21/25 | Loss: 0.00080595
Iteration 22/25 | Loss: 0.00080595
Iteration 23/25 | Loss: 0.00080595
Iteration 24/25 | Loss: 0.00080595
Iteration 25/25 | Loss: 0.00080595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080595
Iteration 2/1000 | Loss: 0.00002041
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001442
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001320
Iteration 7/1000 | Loss: 0.00006249
Iteration 8/1000 | Loss: 0.00001293
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001202
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00006518
Iteration 16/1000 | Loss: 0.00002820
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001174
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00004098
Iteration 25/1000 | Loss: 0.00001172
Iteration 26/1000 | Loss: 0.00001171
Iteration 27/1000 | Loss: 0.00001170
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001169
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001168
Iteration 32/1000 | Loss: 0.00001168
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001165
Iteration 40/1000 | Loss: 0.00001164
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00005658
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001150
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001147
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001146
Iteration 96/1000 | Loss: 0.00001146
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001145
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001145
Iteration 104/1000 | Loss: 0.00001145
Iteration 105/1000 | Loss: 0.00001145
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001143
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001142
Iteration 114/1000 | Loss: 0.00001142
Iteration 115/1000 | Loss: 0.00001142
Iteration 116/1000 | Loss: 0.00007841
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001140
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001138
Iteration 131/1000 | Loss: 0.00001138
Iteration 132/1000 | Loss: 0.00001138
Iteration 133/1000 | Loss: 0.00001138
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001137
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001136
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001134
Iteration 146/1000 | Loss: 0.00001134
Iteration 147/1000 | Loss: 0.00001134
Iteration 148/1000 | Loss: 0.00001134
Iteration 149/1000 | Loss: 0.00001134
Iteration 150/1000 | Loss: 0.00001134
Iteration 151/1000 | Loss: 0.00001133
Iteration 152/1000 | Loss: 0.00001133
Iteration 153/1000 | Loss: 0.00001133
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001133
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001132
Iteration 158/1000 | Loss: 0.00001132
Iteration 159/1000 | Loss: 0.00001132
Iteration 160/1000 | Loss: 0.00001132
Iteration 161/1000 | Loss: 0.00001132
Iteration 162/1000 | Loss: 0.00001132
Iteration 163/1000 | Loss: 0.00001132
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00001132
Iteration 166/1000 | Loss: 0.00001132
Iteration 167/1000 | Loss: 0.00001132
Iteration 168/1000 | Loss: 0.00001132
Iteration 169/1000 | Loss: 0.00001132
Iteration 170/1000 | Loss: 0.00001132
Iteration 171/1000 | Loss: 0.00001132
Iteration 172/1000 | Loss: 0.00001132
Iteration 173/1000 | Loss: 0.00001132
Iteration 174/1000 | Loss: 0.00001132
Iteration 175/1000 | Loss: 0.00001132
Iteration 176/1000 | Loss: 0.00001132
Iteration 177/1000 | Loss: 0.00001132
Iteration 178/1000 | Loss: 0.00001132
Iteration 179/1000 | Loss: 0.00001132
Iteration 180/1000 | Loss: 0.00001132
Iteration 181/1000 | Loss: 0.00001132
Iteration 182/1000 | Loss: 0.00001132
Iteration 183/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1315203664707951e-05, 1.1315203664707951e-05, 1.1315203664707951e-05, 1.1315203664707951e-05, 1.1315203664707951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1315203664707951e-05

Optimization complete. Final v2v error: 2.884614944458008 mm

Highest mean error: 3.1803691387176514 mm for frame 239

Lowest mean error: 2.638674736022949 mm for frame 37

Saving results

Total time: 75.99968528747559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416161
Iteration 2/25 | Loss: 0.00120586
Iteration 3/25 | Loss: 0.00114536
Iteration 4/25 | Loss: 0.00113881
Iteration 5/25 | Loss: 0.00113683
Iteration 6/25 | Loss: 0.00113654
Iteration 7/25 | Loss: 0.00113654
Iteration 8/25 | Loss: 0.00113654
Iteration 9/25 | Loss: 0.00113654
Iteration 10/25 | Loss: 0.00113654
Iteration 11/25 | Loss: 0.00113654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011365445097908378, 0.0011365445097908378, 0.0011365445097908378, 0.0011365445097908378, 0.0011365445097908378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011365445097908378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37712419
Iteration 2/25 | Loss: 0.00080118
Iteration 3/25 | Loss: 0.00080118
Iteration 4/25 | Loss: 0.00080118
Iteration 5/25 | Loss: 0.00080118
Iteration 6/25 | Loss: 0.00080118
Iteration 7/25 | Loss: 0.00080118
Iteration 8/25 | Loss: 0.00080118
Iteration 9/25 | Loss: 0.00080118
Iteration 10/25 | Loss: 0.00080118
Iteration 11/25 | Loss: 0.00080118
Iteration 12/25 | Loss: 0.00080118
Iteration 13/25 | Loss: 0.00080118
Iteration 14/25 | Loss: 0.00080118
Iteration 15/25 | Loss: 0.00080118
Iteration 16/25 | Loss: 0.00080118
Iteration 17/25 | Loss: 0.00080118
Iteration 18/25 | Loss: 0.00080118
Iteration 19/25 | Loss: 0.00080118
Iteration 20/25 | Loss: 0.00080118
Iteration 21/25 | Loss: 0.00080118
Iteration 22/25 | Loss: 0.00080118
Iteration 23/25 | Loss: 0.00080118
Iteration 24/25 | Loss: 0.00080118
Iteration 25/25 | Loss: 0.00080118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080118
Iteration 2/1000 | Loss: 0.00002057
Iteration 3/1000 | Loss: 0.00001353
Iteration 4/1000 | Loss: 0.00001254
Iteration 5/1000 | Loss: 0.00001202
Iteration 6/1000 | Loss: 0.00001195
Iteration 7/1000 | Loss: 0.00001161
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001111
Iteration 11/1000 | Loss: 0.00001108
Iteration 12/1000 | Loss: 0.00001107
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001082
Iteration 16/1000 | Loss: 0.00001079
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001072
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001067
Iteration 22/1000 | Loss: 0.00001067
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001066
Iteration 25/1000 | Loss: 0.00001065
Iteration 26/1000 | Loss: 0.00001065
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001064
Iteration 29/1000 | Loss: 0.00001064
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001064
Iteration 34/1000 | Loss: 0.00001064
Iteration 35/1000 | Loss: 0.00001064
Iteration 36/1000 | Loss: 0.00001064
Iteration 37/1000 | Loss: 0.00001064
Iteration 38/1000 | Loss: 0.00001064
Iteration 39/1000 | Loss: 0.00001064
Iteration 40/1000 | Loss: 0.00001064
Iteration 41/1000 | Loss: 0.00001064
Iteration 42/1000 | Loss: 0.00001064
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 44. Stopping optimization.
Last 5 losses: [1.0635915714374278e-05, 1.0635915714374278e-05, 1.0635915714374278e-05, 1.0635915714374278e-05, 1.0635915714374278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0635915714374278e-05

Optimization complete. Final v2v error: 2.8198952674865723 mm

Highest mean error: 2.906367063522339 mm for frame 64

Lowest mean error: 2.705700159072876 mm for frame 28

Saving results

Total time: 25.149855852127075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060828
Iteration 2/25 | Loss: 0.00336044
Iteration 3/25 | Loss: 0.00163480
Iteration 4/25 | Loss: 0.00150229
Iteration 5/25 | Loss: 0.00136127
Iteration 6/25 | Loss: 0.00131731
Iteration 7/25 | Loss: 0.00127434
Iteration 8/25 | Loss: 0.00121682
Iteration 9/25 | Loss: 0.00120214
Iteration 10/25 | Loss: 0.00119635
Iteration 11/25 | Loss: 0.00118950
Iteration 12/25 | Loss: 0.00117884
Iteration 13/25 | Loss: 0.00117722
Iteration 14/25 | Loss: 0.00117082
Iteration 15/25 | Loss: 0.00116396
Iteration 16/25 | Loss: 0.00115708
Iteration 17/25 | Loss: 0.00115419
Iteration 18/25 | Loss: 0.00115152
Iteration 19/25 | Loss: 0.00115055
Iteration 20/25 | Loss: 0.00115023
Iteration 21/25 | Loss: 0.00115497
Iteration 22/25 | Loss: 0.00115259
Iteration 23/25 | Loss: 0.00115773
Iteration 24/25 | Loss: 0.00115469
Iteration 25/25 | Loss: 0.00115103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36942005
Iteration 2/25 | Loss: 0.00087321
Iteration 3/25 | Loss: 0.00087321
Iteration 4/25 | Loss: 0.00087321
Iteration 5/25 | Loss: 0.00087321
Iteration 6/25 | Loss: 0.00087321
Iteration 7/25 | Loss: 0.00087321
Iteration 8/25 | Loss: 0.00087321
Iteration 9/25 | Loss: 0.00087321
Iteration 10/25 | Loss: 0.00087321
Iteration 11/25 | Loss: 0.00087321
Iteration 12/25 | Loss: 0.00087321
Iteration 13/25 | Loss: 0.00087321
Iteration 14/25 | Loss: 0.00087321
Iteration 15/25 | Loss: 0.00087321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008732092683203518, 0.0008732092683203518, 0.0008732092683203518, 0.0008732092683203518, 0.0008732092683203518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008732092683203518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087321
Iteration 2/1000 | Loss: 0.00003811
Iteration 3/1000 | Loss: 0.00002726
Iteration 4/1000 | Loss: 0.00002367
Iteration 5/1000 | Loss: 0.00002167
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00001972
Iteration 8/1000 | Loss: 0.00001866
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001682
Iteration 11/1000 | Loss: 0.00001648
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001594
Iteration 15/1000 | Loss: 0.00001581
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001542
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001521
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001520
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001519
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001508
Iteration 89/1000 | Loss: 0.00001508
Iteration 90/1000 | Loss: 0.00001508
Iteration 91/1000 | Loss: 0.00001508
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001507
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001507
Iteration 100/1000 | Loss: 0.00001507
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001505
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001502
Iteration 158/1000 | Loss: 0.00001502
Iteration 159/1000 | Loss: 0.00001502
Iteration 160/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.5016141333035193e-05, 1.5016141333035193e-05, 1.5016141333035193e-05, 1.5016141333035193e-05, 1.5016141333035193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5016141333035193e-05

Optimization complete. Final v2v error: 3.2416133880615234 mm

Highest mean error: 6.070709705352783 mm for frame 84

Lowest mean error: 3.0260396003723145 mm for frame 173

Saving results

Total time: 85.04524445533752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052025
Iteration 2/25 | Loss: 0.00281802
Iteration 3/25 | Loss: 0.00234907
Iteration 4/25 | Loss: 0.00194334
Iteration 5/25 | Loss: 0.00152106
Iteration 6/25 | Loss: 0.00140356
Iteration 7/25 | Loss: 0.00137696
Iteration 8/25 | Loss: 0.00132608
Iteration 9/25 | Loss: 0.00130518
Iteration 10/25 | Loss: 0.00126804
Iteration 11/25 | Loss: 0.00122849
Iteration 12/25 | Loss: 0.00120241
Iteration 13/25 | Loss: 0.00119227
Iteration 14/25 | Loss: 0.00118532
Iteration 15/25 | Loss: 0.00118453
Iteration 16/25 | Loss: 0.00118432
Iteration 17/25 | Loss: 0.00118415
Iteration 18/25 | Loss: 0.00118638
Iteration 19/25 | Loss: 0.00118513
Iteration 20/25 | Loss: 0.00118780
Iteration 21/25 | Loss: 0.00118203
Iteration 22/25 | Loss: 0.00117933
Iteration 23/25 | Loss: 0.00117841
Iteration 24/25 | Loss: 0.00117832
Iteration 25/25 | Loss: 0.00117832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40836358
Iteration 2/25 | Loss: 0.00132131
Iteration 3/25 | Loss: 0.00076590
Iteration 4/25 | Loss: 0.00076590
Iteration 5/25 | Loss: 0.00076590
Iteration 6/25 | Loss: 0.00076590
Iteration 7/25 | Loss: 0.00076590
Iteration 8/25 | Loss: 0.00076590
Iteration 9/25 | Loss: 0.00076590
Iteration 10/25 | Loss: 0.00076590
Iteration 11/25 | Loss: 0.00076590
Iteration 12/25 | Loss: 0.00076590
Iteration 13/25 | Loss: 0.00076590
Iteration 14/25 | Loss: 0.00076590
Iteration 15/25 | Loss: 0.00076590
Iteration 16/25 | Loss: 0.00076590
Iteration 17/25 | Loss: 0.00076590
Iteration 18/25 | Loss: 0.00076590
Iteration 19/25 | Loss: 0.00076590
Iteration 20/25 | Loss: 0.00076590
Iteration 21/25 | Loss: 0.00076590
Iteration 22/25 | Loss: 0.00076590
Iteration 23/25 | Loss: 0.00076590
Iteration 24/25 | Loss: 0.00076590
Iteration 25/25 | Loss: 0.00076590

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076590
Iteration 2/1000 | Loss: 0.00132189
Iteration 3/1000 | Loss: 0.00141375
Iteration 4/1000 | Loss: 0.00476746
Iteration 5/1000 | Loss: 0.00573902
Iteration 6/1000 | Loss: 0.00006889
Iteration 7/1000 | Loss: 0.00003632
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00048958
Iteration 10/1000 | Loss: 0.00002320
Iteration 11/1000 | Loss: 0.00002114
Iteration 12/1000 | Loss: 0.00002054
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001926
Iteration 16/1000 | Loss: 0.00001898
Iteration 17/1000 | Loss: 0.00051751
Iteration 18/1000 | Loss: 0.00035783
Iteration 19/1000 | Loss: 0.00006495
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00051271
Iteration 22/1000 | Loss: 0.00002304
Iteration 23/1000 | Loss: 0.00001809
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001672
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001591
Iteration 33/1000 | Loss: 0.00001590
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001589
Iteration 36/1000 | Loss: 0.00001586
Iteration 37/1000 | Loss: 0.00001586
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001569
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001568
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001560
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001560
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001558
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001556
Iteration 94/1000 | Loss: 0.00001556
Iteration 95/1000 | Loss: 0.00001556
Iteration 96/1000 | Loss: 0.00001556
Iteration 97/1000 | Loss: 0.00001556
Iteration 98/1000 | Loss: 0.00001556
Iteration 99/1000 | Loss: 0.00001556
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.5558460290776566e-05, 1.5558460290776566e-05, 1.5558460290776566e-05, 1.5558460290776566e-05, 1.5558460290776566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5558460290776566e-05

Optimization complete. Final v2v error: 3.3186392784118652 mm

Highest mean error: 3.969878911972046 mm for frame 12

Lowest mean error: 2.8164784908294678 mm for frame 122

Saving results

Total time: 90.71480989456177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348966
Iteration 2/25 | Loss: 0.00128089
Iteration 3/25 | Loss: 0.00112874
Iteration 4/25 | Loss: 0.00110934
Iteration 5/25 | Loss: 0.00110512
Iteration 6/25 | Loss: 0.00110381
Iteration 7/25 | Loss: 0.00110381
Iteration 8/25 | Loss: 0.00110381
Iteration 9/25 | Loss: 0.00110381
Iteration 10/25 | Loss: 0.00110381
Iteration 11/25 | Loss: 0.00110381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011038127122446895, 0.0011038127122446895, 0.0011038127122446895, 0.0011038127122446895, 0.0011038127122446895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011038127122446895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35364783
Iteration 2/25 | Loss: 0.00078604
Iteration 3/25 | Loss: 0.00078603
Iteration 4/25 | Loss: 0.00078603
Iteration 5/25 | Loss: 0.00078603
Iteration 6/25 | Loss: 0.00078603
Iteration 7/25 | Loss: 0.00078603
Iteration 8/25 | Loss: 0.00078603
Iteration 9/25 | Loss: 0.00078603
Iteration 10/25 | Loss: 0.00078603
Iteration 11/25 | Loss: 0.00078603
Iteration 12/25 | Loss: 0.00078603
Iteration 13/25 | Loss: 0.00078603
Iteration 14/25 | Loss: 0.00078603
Iteration 15/25 | Loss: 0.00078603
Iteration 16/25 | Loss: 0.00078603
Iteration 17/25 | Loss: 0.00078603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007860261830501258, 0.0007860261830501258, 0.0007860261830501258, 0.0007860261830501258, 0.0007860261830501258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007860261830501258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078603
Iteration 2/1000 | Loss: 0.00002598
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001355
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001196
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001128
Iteration 12/1000 | Loss: 0.00001127
Iteration 13/1000 | Loss: 0.00001125
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001095
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001086
Iteration 30/1000 | Loss: 0.00001086
Iteration 31/1000 | Loss: 0.00001085
Iteration 32/1000 | Loss: 0.00001085
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001083
Iteration 37/1000 | Loss: 0.00001082
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001081
Iteration 40/1000 | Loss: 0.00001081
Iteration 41/1000 | Loss: 0.00001080
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001076
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001072
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001069
Iteration 59/1000 | Loss: 0.00001068
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001067
Iteration 62/1000 | Loss: 0.00001067
Iteration 63/1000 | Loss: 0.00001067
Iteration 64/1000 | Loss: 0.00001067
Iteration 65/1000 | Loss: 0.00001066
Iteration 66/1000 | Loss: 0.00001066
Iteration 67/1000 | Loss: 0.00001066
Iteration 68/1000 | Loss: 0.00001066
Iteration 69/1000 | Loss: 0.00001066
Iteration 70/1000 | Loss: 0.00001065
Iteration 71/1000 | Loss: 0.00001065
Iteration 72/1000 | Loss: 0.00001065
Iteration 73/1000 | Loss: 0.00001065
Iteration 74/1000 | Loss: 0.00001064
Iteration 75/1000 | Loss: 0.00001064
Iteration 76/1000 | Loss: 0.00001064
Iteration 77/1000 | Loss: 0.00001064
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001063
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001062
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001061
Iteration 89/1000 | Loss: 0.00001061
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001060
Iteration 92/1000 | Loss: 0.00001060
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001058
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001057
Iteration 103/1000 | Loss: 0.00001057
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001056
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001055
Iteration 113/1000 | Loss: 0.00001055
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001054
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.0543107237026561e-05, 1.0543107237026561e-05, 1.0543107237026561e-05, 1.0543107237026561e-05, 1.0543107237026561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0543107237026561e-05

Optimization complete. Final v2v error: 2.7800357341766357 mm

Highest mean error: 3.068002700805664 mm for frame 159

Lowest mean error: 2.544278860092163 mm for frame 50

Saving results

Total time: 44.194448471069336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400761
Iteration 2/25 | Loss: 0.00118090
Iteration 3/25 | Loss: 0.00111110
Iteration 4/25 | Loss: 0.00110019
Iteration 5/25 | Loss: 0.00109651
Iteration 6/25 | Loss: 0.00109611
Iteration 7/25 | Loss: 0.00109611
Iteration 8/25 | Loss: 0.00109611
Iteration 9/25 | Loss: 0.00109611
Iteration 10/25 | Loss: 0.00109611
Iteration 11/25 | Loss: 0.00109611
Iteration 12/25 | Loss: 0.00109611
Iteration 13/25 | Loss: 0.00109611
Iteration 14/25 | Loss: 0.00109611
Iteration 15/25 | Loss: 0.00109611
Iteration 16/25 | Loss: 0.00109611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001096113002859056, 0.001096113002859056, 0.001096113002859056, 0.001096113002859056, 0.001096113002859056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001096113002859056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87860703
Iteration 2/25 | Loss: 0.00073820
Iteration 3/25 | Loss: 0.00073819
Iteration 4/25 | Loss: 0.00073819
Iteration 5/25 | Loss: 0.00073819
Iteration 6/25 | Loss: 0.00073819
Iteration 7/25 | Loss: 0.00073819
Iteration 8/25 | Loss: 0.00073819
Iteration 9/25 | Loss: 0.00073819
Iteration 10/25 | Loss: 0.00073819
Iteration 11/25 | Loss: 0.00073819
Iteration 12/25 | Loss: 0.00073819
Iteration 13/25 | Loss: 0.00073819
Iteration 14/25 | Loss: 0.00073819
Iteration 15/25 | Loss: 0.00073819
Iteration 16/25 | Loss: 0.00073819
Iteration 17/25 | Loss: 0.00073819
Iteration 18/25 | Loss: 0.00073819
Iteration 19/25 | Loss: 0.00073819
Iteration 20/25 | Loss: 0.00073819
Iteration 21/25 | Loss: 0.00073819
Iteration 22/25 | Loss: 0.00073819
Iteration 23/25 | Loss: 0.00073819
Iteration 24/25 | Loss: 0.00073819
Iteration 25/25 | Loss: 0.00073819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073819
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001326
Iteration 5/1000 | Loss: 0.00001229
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001130
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001064
Iteration 12/1000 | Loss: 0.00001049
Iteration 13/1000 | Loss: 0.00001047
Iteration 14/1000 | Loss: 0.00001046
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001039
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001032
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001016
Iteration 25/1000 | Loss: 0.00001016
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001009
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001008
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001008
Iteration 38/1000 | Loss: 0.00001008
Iteration 39/1000 | Loss: 0.00001008
Iteration 40/1000 | Loss: 0.00001008
Iteration 41/1000 | Loss: 0.00001007
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001006
Iteration 46/1000 | Loss: 0.00001006
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001003
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000998
Iteration 73/1000 | Loss: 0.00000998
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000997
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000993
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000993
Iteration 80/1000 | Loss: 0.00000993
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000992
Iteration 83/1000 | Loss: 0.00000992
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000991
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000990
Iteration 88/1000 | Loss: 0.00000990
Iteration 89/1000 | Loss: 0.00000990
Iteration 90/1000 | Loss: 0.00000990
Iteration 91/1000 | Loss: 0.00000990
Iteration 92/1000 | Loss: 0.00000989
Iteration 93/1000 | Loss: 0.00000989
Iteration 94/1000 | Loss: 0.00000989
Iteration 95/1000 | Loss: 0.00000989
Iteration 96/1000 | Loss: 0.00000989
Iteration 97/1000 | Loss: 0.00000988
Iteration 98/1000 | Loss: 0.00000988
Iteration 99/1000 | Loss: 0.00000988
Iteration 100/1000 | Loss: 0.00000988
Iteration 101/1000 | Loss: 0.00000988
Iteration 102/1000 | Loss: 0.00000988
Iteration 103/1000 | Loss: 0.00000988
Iteration 104/1000 | Loss: 0.00000988
Iteration 105/1000 | Loss: 0.00000987
Iteration 106/1000 | Loss: 0.00000987
Iteration 107/1000 | Loss: 0.00000987
Iteration 108/1000 | Loss: 0.00000987
Iteration 109/1000 | Loss: 0.00000986
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000986
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000985
Iteration 116/1000 | Loss: 0.00000985
Iteration 117/1000 | Loss: 0.00000985
Iteration 118/1000 | Loss: 0.00000985
Iteration 119/1000 | Loss: 0.00000985
Iteration 120/1000 | Loss: 0.00000985
Iteration 121/1000 | Loss: 0.00000985
Iteration 122/1000 | Loss: 0.00000984
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000984
Iteration 126/1000 | Loss: 0.00000984
Iteration 127/1000 | Loss: 0.00000984
Iteration 128/1000 | Loss: 0.00000984
Iteration 129/1000 | Loss: 0.00000983
Iteration 130/1000 | Loss: 0.00000983
Iteration 131/1000 | Loss: 0.00000983
Iteration 132/1000 | Loss: 0.00000982
Iteration 133/1000 | Loss: 0.00000982
Iteration 134/1000 | Loss: 0.00000982
Iteration 135/1000 | Loss: 0.00000982
Iteration 136/1000 | Loss: 0.00000982
Iteration 137/1000 | Loss: 0.00000982
Iteration 138/1000 | Loss: 0.00000982
Iteration 139/1000 | Loss: 0.00000982
Iteration 140/1000 | Loss: 0.00000982
Iteration 141/1000 | Loss: 0.00000981
Iteration 142/1000 | Loss: 0.00000981
Iteration 143/1000 | Loss: 0.00000981
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000980
Iteration 146/1000 | Loss: 0.00000980
Iteration 147/1000 | Loss: 0.00000979
Iteration 148/1000 | Loss: 0.00000979
Iteration 149/1000 | Loss: 0.00000979
Iteration 150/1000 | Loss: 0.00000979
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000978
Iteration 153/1000 | Loss: 0.00000978
Iteration 154/1000 | Loss: 0.00000978
Iteration 155/1000 | Loss: 0.00000978
Iteration 156/1000 | Loss: 0.00000978
Iteration 157/1000 | Loss: 0.00000978
Iteration 158/1000 | Loss: 0.00000978
Iteration 159/1000 | Loss: 0.00000978
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000977
Iteration 165/1000 | Loss: 0.00000977
Iteration 166/1000 | Loss: 0.00000977
Iteration 167/1000 | Loss: 0.00000977
Iteration 168/1000 | Loss: 0.00000976
Iteration 169/1000 | Loss: 0.00000976
Iteration 170/1000 | Loss: 0.00000976
Iteration 171/1000 | Loss: 0.00000976
Iteration 172/1000 | Loss: 0.00000976
Iteration 173/1000 | Loss: 0.00000976
Iteration 174/1000 | Loss: 0.00000976
Iteration 175/1000 | Loss: 0.00000976
Iteration 176/1000 | Loss: 0.00000976
Iteration 177/1000 | Loss: 0.00000976
Iteration 178/1000 | Loss: 0.00000976
Iteration 179/1000 | Loss: 0.00000976
Iteration 180/1000 | Loss: 0.00000976
Iteration 181/1000 | Loss: 0.00000976
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [9.76304363575764e-06, 9.76304363575764e-06, 9.76304363575764e-06, 9.76304363575764e-06, 9.76304363575764e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.76304363575764e-06

Optimization complete. Final v2v error: 2.693091869354248 mm

Highest mean error: 2.9296817779541016 mm for frame 119

Lowest mean error: 2.596048593521118 mm for frame 182

Saving results

Total time: 41.0855655670166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433143
Iteration 2/25 | Loss: 0.00127391
Iteration 3/25 | Loss: 0.00119973
Iteration 4/25 | Loss: 0.00118678
Iteration 5/25 | Loss: 0.00118276
Iteration 6/25 | Loss: 0.00118206
Iteration 7/25 | Loss: 0.00118206
Iteration 8/25 | Loss: 0.00118206
Iteration 9/25 | Loss: 0.00118206
Iteration 10/25 | Loss: 0.00118206
Iteration 11/25 | Loss: 0.00118206
Iteration 12/25 | Loss: 0.00118206
Iteration 13/25 | Loss: 0.00118206
Iteration 14/25 | Loss: 0.00118206
Iteration 15/25 | Loss: 0.00118206
Iteration 16/25 | Loss: 0.00118206
Iteration 17/25 | Loss: 0.00118206
Iteration 18/25 | Loss: 0.00118206
Iteration 19/25 | Loss: 0.00118206
Iteration 20/25 | Loss: 0.00118206
Iteration 21/25 | Loss: 0.00118206
Iteration 22/25 | Loss: 0.00118206
Iteration 23/25 | Loss: 0.00118206
Iteration 24/25 | Loss: 0.00118206
Iteration 25/25 | Loss: 0.00118206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77614617
Iteration 2/25 | Loss: 0.00095503
Iteration 3/25 | Loss: 0.00095502
Iteration 4/25 | Loss: 0.00095502
Iteration 5/25 | Loss: 0.00095502
Iteration 6/25 | Loss: 0.00095502
Iteration 7/25 | Loss: 0.00095502
Iteration 8/25 | Loss: 0.00095502
Iteration 9/25 | Loss: 0.00095502
Iteration 10/25 | Loss: 0.00095502
Iteration 11/25 | Loss: 0.00095502
Iteration 12/25 | Loss: 0.00095502
Iteration 13/25 | Loss: 0.00095502
Iteration 14/25 | Loss: 0.00095502
Iteration 15/25 | Loss: 0.00095502
Iteration 16/25 | Loss: 0.00095502
Iteration 17/25 | Loss: 0.00095502
Iteration 18/25 | Loss: 0.00095502
Iteration 19/25 | Loss: 0.00095502
Iteration 20/25 | Loss: 0.00095502
Iteration 21/25 | Loss: 0.00095502
Iteration 22/25 | Loss: 0.00095502
Iteration 23/25 | Loss: 0.00095502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009550214745104313, 0.0009550214745104313, 0.0009550214745104313, 0.0009550214745104313, 0.0009550214745104313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009550214745104313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095502
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00002010
Iteration 4/1000 | Loss: 0.00001875
Iteration 5/1000 | Loss: 0.00001803
Iteration 6/1000 | Loss: 0.00001791
Iteration 7/1000 | Loss: 0.00001758
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001658
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001645
Iteration 26/1000 | Loss: 0.00001645
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001644
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001640
Iteration 42/1000 | Loss: 0.00001640
Iteration 43/1000 | Loss: 0.00001640
Iteration 44/1000 | Loss: 0.00001640
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001638
Iteration 50/1000 | Loss: 0.00001638
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001637
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001636
Iteration 55/1000 | Loss: 0.00001636
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001635
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001635
Iteration 60/1000 | Loss: 0.00001634
Iteration 61/1000 | Loss: 0.00001634
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001633
Iteration 64/1000 | Loss: 0.00001633
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001630
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001629
Iteration 78/1000 | Loss: 0.00001628
Iteration 79/1000 | Loss: 0.00001628
Iteration 80/1000 | Loss: 0.00001628
Iteration 81/1000 | Loss: 0.00001628
Iteration 82/1000 | Loss: 0.00001628
Iteration 83/1000 | Loss: 0.00001628
Iteration 84/1000 | Loss: 0.00001628
Iteration 85/1000 | Loss: 0.00001628
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001627
Iteration 88/1000 | Loss: 0.00001627
Iteration 89/1000 | Loss: 0.00001627
Iteration 90/1000 | Loss: 0.00001627
Iteration 91/1000 | Loss: 0.00001627
Iteration 92/1000 | Loss: 0.00001627
Iteration 93/1000 | Loss: 0.00001627
Iteration 94/1000 | Loss: 0.00001627
Iteration 95/1000 | Loss: 0.00001627
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001626
Iteration 99/1000 | Loss: 0.00001626
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001626
Iteration 102/1000 | Loss: 0.00001626
Iteration 103/1000 | Loss: 0.00001626
Iteration 104/1000 | Loss: 0.00001626
Iteration 105/1000 | Loss: 0.00001626
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001625
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001625
Iteration 112/1000 | Loss: 0.00001625
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001624
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Iteration 127/1000 | Loss: 0.00001624
Iteration 128/1000 | Loss: 0.00001624
Iteration 129/1000 | Loss: 0.00001624
Iteration 130/1000 | Loss: 0.00001624
Iteration 131/1000 | Loss: 0.00001624
Iteration 132/1000 | Loss: 0.00001624
Iteration 133/1000 | Loss: 0.00001623
Iteration 134/1000 | Loss: 0.00001623
Iteration 135/1000 | Loss: 0.00001623
Iteration 136/1000 | Loss: 0.00001623
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001622
Iteration 142/1000 | Loss: 0.00001622
Iteration 143/1000 | Loss: 0.00001622
Iteration 144/1000 | Loss: 0.00001622
Iteration 145/1000 | Loss: 0.00001622
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001621
Iteration 149/1000 | Loss: 0.00001621
Iteration 150/1000 | Loss: 0.00001621
Iteration 151/1000 | Loss: 0.00001621
Iteration 152/1000 | Loss: 0.00001621
Iteration 153/1000 | Loss: 0.00001621
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001620
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001619
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001619
Iteration 167/1000 | Loss: 0.00001619
Iteration 168/1000 | Loss: 0.00001619
Iteration 169/1000 | Loss: 0.00001619
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001618
Iteration 174/1000 | Loss: 0.00001618
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001616
Iteration 180/1000 | Loss: 0.00001616
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001616
Iteration 187/1000 | Loss: 0.00001616
Iteration 188/1000 | Loss: 0.00001616
Iteration 189/1000 | Loss: 0.00001616
Iteration 190/1000 | Loss: 0.00001615
Iteration 191/1000 | Loss: 0.00001615
Iteration 192/1000 | Loss: 0.00001615
Iteration 193/1000 | Loss: 0.00001615
Iteration 194/1000 | Loss: 0.00001615
Iteration 195/1000 | Loss: 0.00001615
Iteration 196/1000 | Loss: 0.00001615
Iteration 197/1000 | Loss: 0.00001615
Iteration 198/1000 | Loss: 0.00001615
Iteration 199/1000 | Loss: 0.00001615
Iteration 200/1000 | Loss: 0.00001615
Iteration 201/1000 | Loss: 0.00001615
Iteration 202/1000 | Loss: 0.00001615
Iteration 203/1000 | Loss: 0.00001614
Iteration 204/1000 | Loss: 0.00001614
Iteration 205/1000 | Loss: 0.00001614
Iteration 206/1000 | Loss: 0.00001614
Iteration 207/1000 | Loss: 0.00001614
Iteration 208/1000 | Loss: 0.00001614
Iteration 209/1000 | Loss: 0.00001614
Iteration 210/1000 | Loss: 0.00001614
Iteration 211/1000 | Loss: 0.00001613
Iteration 212/1000 | Loss: 0.00001613
Iteration 213/1000 | Loss: 0.00001613
Iteration 214/1000 | Loss: 0.00001613
Iteration 215/1000 | Loss: 0.00001613
Iteration 216/1000 | Loss: 0.00001613
Iteration 217/1000 | Loss: 0.00001613
Iteration 218/1000 | Loss: 0.00001613
Iteration 219/1000 | Loss: 0.00001613
Iteration 220/1000 | Loss: 0.00001613
Iteration 221/1000 | Loss: 0.00001613
Iteration 222/1000 | Loss: 0.00001613
Iteration 223/1000 | Loss: 0.00001613
Iteration 224/1000 | Loss: 0.00001613
Iteration 225/1000 | Loss: 0.00001613
Iteration 226/1000 | Loss: 0.00001613
Iteration 227/1000 | Loss: 0.00001613
Iteration 228/1000 | Loss: 0.00001613
Iteration 229/1000 | Loss: 0.00001613
Iteration 230/1000 | Loss: 0.00001612
Iteration 231/1000 | Loss: 0.00001612
Iteration 232/1000 | Loss: 0.00001612
Iteration 233/1000 | Loss: 0.00001612
Iteration 234/1000 | Loss: 0.00001612
Iteration 235/1000 | Loss: 0.00001612
Iteration 236/1000 | Loss: 0.00001612
Iteration 237/1000 | Loss: 0.00001612
Iteration 238/1000 | Loss: 0.00001612
Iteration 239/1000 | Loss: 0.00001612
Iteration 240/1000 | Loss: 0.00001612
Iteration 241/1000 | Loss: 0.00001612
Iteration 242/1000 | Loss: 0.00001612
Iteration 243/1000 | Loss: 0.00001612
Iteration 244/1000 | Loss: 0.00001612
Iteration 245/1000 | Loss: 0.00001612
Iteration 246/1000 | Loss: 0.00001612
Iteration 247/1000 | Loss: 0.00001612
Iteration 248/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.6123176465043798e-05, 1.6123176465043798e-05, 1.6123176465043798e-05, 1.6123176465043798e-05, 1.6123176465043798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6123176465043798e-05

Optimization complete. Final v2v error: 3.3955743312835693 mm

Highest mean error: 3.976759910583496 mm for frame 157

Lowest mean error: 3.295259714126587 mm for frame 65

Saving results

Total time: 48.76917815208435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074529
Iteration 2/25 | Loss: 0.00161816
Iteration 3/25 | Loss: 0.00132570
Iteration 4/25 | Loss: 0.00130390
Iteration 5/25 | Loss: 0.00129754
Iteration 6/25 | Loss: 0.00129632
Iteration 7/25 | Loss: 0.00129632
Iteration 8/25 | Loss: 0.00129632
Iteration 9/25 | Loss: 0.00129632
Iteration 10/25 | Loss: 0.00129632
Iteration 11/25 | Loss: 0.00129632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012963248882442713, 0.0012963248882442713, 0.0012963248882442713, 0.0012963248882442713, 0.0012963248882442713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012963248882442713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94735867
Iteration 2/25 | Loss: 0.00097150
Iteration 3/25 | Loss: 0.00097149
Iteration 4/25 | Loss: 0.00097149
Iteration 5/25 | Loss: 0.00097149
Iteration 6/25 | Loss: 0.00097149
Iteration 7/25 | Loss: 0.00097149
Iteration 8/25 | Loss: 0.00097149
Iteration 9/25 | Loss: 0.00097149
Iteration 10/25 | Loss: 0.00097149
Iteration 11/25 | Loss: 0.00097149
Iteration 12/25 | Loss: 0.00097149
Iteration 13/25 | Loss: 0.00097149
Iteration 14/25 | Loss: 0.00097149
Iteration 15/25 | Loss: 0.00097149
Iteration 16/25 | Loss: 0.00097149
Iteration 17/25 | Loss: 0.00097149
Iteration 18/25 | Loss: 0.00097149
Iteration 19/25 | Loss: 0.00097149
Iteration 20/25 | Loss: 0.00097149
Iteration 21/25 | Loss: 0.00097149
Iteration 22/25 | Loss: 0.00097149
Iteration 23/25 | Loss: 0.00097149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009714909247122705, 0.0009714909247122705, 0.0009714909247122705, 0.0009714909247122705, 0.0009714909247122705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009714909247122705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097149
Iteration 2/1000 | Loss: 0.00005391
Iteration 3/1000 | Loss: 0.00003664
Iteration 4/1000 | Loss: 0.00003032
Iteration 5/1000 | Loss: 0.00002811
Iteration 6/1000 | Loss: 0.00002710
Iteration 7/1000 | Loss: 0.00002648
Iteration 8/1000 | Loss: 0.00002603
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002542
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00002498
Iteration 13/1000 | Loss: 0.00002478
Iteration 14/1000 | Loss: 0.00002460
Iteration 15/1000 | Loss: 0.00002459
Iteration 16/1000 | Loss: 0.00002454
Iteration 17/1000 | Loss: 0.00002452
Iteration 18/1000 | Loss: 0.00002446
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00002420
Iteration 21/1000 | Loss: 0.00002419
Iteration 22/1000 | Loss: 0.00002417
Iteration 23/1000 | Loss: 0.00002416
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002411
Iteration 26/1000 | Loss: 0.00002410
Iteration 27/1000 | Loss: 0.00002410
Iteration 28/1000 | Loss: 0.00002409
Iteration 29/1000 | Loss: 0.00002409
Iteration 30/1000 | Loss: 0.00002409
Iteration 31/1000 | Loss: 0.00002408
Iteration 32/1000 | Loss: 0.00002408
Iteration 33/1000 | Loss: 0.00002408
Iteration 34/1000 | Loss: 0.00002407
Iteration 35/1000 | Loss: 0.00002407
Iteration 36/1000 | Loss: 0.00002407
Iteration 37/1000 | Loss: 0.00002407
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002405
Iteration 41/1000 | Loss: 0.00002404
Iteration 42/1000 | Loss: 0.00002404
Iteration 43/1000 | Loss: 0.00002404
Iteration 44/1000 | Loss: 0.00002403
Iteration 45/1000 | Loss: 0.00002403
Iteration 46/1000 | Loss: 0.00002403
Iteration 47/1000 | Loss: 0.00002401
Iteration 48/1000 | Loss: 0.00002401
Iteration 49/1000 | Loss: 0.00002400
Iteration 50/1000 | Loss: 0.00002400
Iteration 51/1000 | Loss: 0.00002397
Iteration 52/1000 | Loss: 0.00002397
Iteration 53/1000 | Loss: 0.00002394
Iteration 54/1000 | Loss: 0.00002393
Iteration 55/1000 | Loss: 0.00002393
Iteration 56/1000 | Loss: 0.00002392
Iteration 57/1000 | Loss: 0.00002392
Iteration 58/1000 | Loss: 0.00002392
Iteration 59/1000 | Loss: 0.00002391
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00002390
Iteration 62/1000 | Loss: 0.00002390
Iteration 63/1000 | Loss: 0.00002390
Iteration 64/1000 | Loss: 0.00002390
Iteration 65/1000 | Loss: 0.00002390
Iteration 66/1000 | Loss: 0.00002390
Iteration 67/1000 | Loss: 0.00002390
Iteration 68/1000 | Loss: 0.00002390
Iteration 69/1000 | Loss: 0.00002389
Iteration 70/1000 | Loss: 0.00002389
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002389
Iteration 73/1000 | Loss: 0.00002388
Iteration 74/1000 | Loss: 0.00002388
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002388
Iteration 79/1000 | Loss: 0.00002388
Iteration 80/1000 | Loss: 0.00002387
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002387
Iteration 83/1000 | Loss: 0.00002386
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002386
Iteration 90/1000 | Loss: 0.00002386
Iteration 91/1000 | Loss: 0.00002386
Iteration 92/1000 | Loss: 0.00002386
Iteration 93/1000 | Loss: 0.00002386
Iteration 94/1000 | Loss: 0.00002386
Iteration 95/1000 | Loss: 0.00002386
Iteration 96/1000 | Loss: 0.00002386
Iteration 97/1000 | Loss: 0.00002386
Iteration 98/1000 | Loss: 0.00002386
Iteration 99/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.3860111468820833e-05, 2.3860111468820833e-05, 2.3860111468820833e-05, 2.3860111468820833e-05, 2.3860111468820833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3860111468820833e-05

Optimization complete. Final v2v error: 3.9929776191711426 mm

Highest mean error: 4.821200370788574 mm for frame 121

Lowest mean error: 3.2379703521728516 mm for frame 28

Saving results

Total time: 41.975929260253906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018509
Iteration 2/25 | Loss: 0.01018509
Iteration 3/25 | Loss: 0.01018509
Iteration 4/25 | Loss: 0.01018508
Iteration 5/25 | Loss: 0.01018508
Iteration 6/25 | Loss: 0.00323759
Iteration 7/25 | Loss: 0.00204045
Iteration 8/25 | Loss: 0.00155758
Iteration 9/25 | Loss: 0.00145300
Iteration 10/25 | Loss: 0.00140020
Iteration 11/25 | Loss: 0.00135404
Iteration 12/25 | Loss: 0.00128327
Iteration 13/25 | Loss: 0.00124368
Iteration 14/25 | Loss: 0.00122116
Iteration 15/25 | Loss: 0.00120794
Iteration 16/25 | Loss: 0.00120714
Iteration 17/25 | Loss: 0.00120384
Iteration 18/25 | Loss: 0.00119858
Iteration 19/25 | Loss: 0.00119693
Iteration 20/25 | Loss: 0.00120115
Iteration 21/25 | Loss: 0.00120099
Iteration 22/25 | Loss: 0.00119663
Iteration 23/25 | Loss: 0.00119671
Iteration 24/25 | Loss: 0.00119738
Iteration 25/25 | Loss: 0.00119681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99241316
Iteration 2/25 | Loss: 0.00108544
Iteration 3/25 | Loss: 0.00108544
Iteration 4/25 | Loss: 0.00108543
Iteration 5/25 | Loss: 0.00108543
Iteration 6/25 | Loss: 0.00108543
Iteration 7/25 | Loss: 0.00108543
Iteration 8/25 | Loss: 0.00108543
Iteration 9/25 | Loss: 0.00108543
Iteration 10/25 | Loss: 0.00108543
Iteration 11/25 | Loss: 0.00108543
Iteration 12/25 | Loss: 0.00108543
Iteration 13/25 | Loss: 0.00108543
Iteration 14/25 | Loss: 0.00108543
Iteration 15/25 | Loss: 0.00108543
Iteration 16/25 | Loss: 0.00108543
Iteration 17/25 | Loss: 0.00108543
Iteration 18/25 | Loss: 0.00108543
Iteration 19/25 | Loss: 0.00108543
Iteration 20/25 | Loss: 0.00108543
Iteration 21/25 | Loss: 0.00108543
Iteration 22/25 | Loss: 0.00108543
Iteration 23/25 | Loss: 0.00108543
Iteration 24/25 | Loss: 0.00108543
Iteration 25/25 | Loss: 0.00108543
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010854308493435383, 0.0010854308493435383, 0.0010854308493435383, 0.0010854308493435383, 0.0010854308493435383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010854308493435383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108543
Iteration 2/1000 | Loss: 0.00009902
Iteration 3/1000 | Loss: 0.00008911
Iteration 4/1000 | Loss: 0.00010700
Iteration 5/1000 | Loss: 0.00005191
Iteration 6/1000 | Loss: 0.00037645
Iteration 7/1000 | Loss: 0.00014587
Iteration 8/1000 | Loss: 0.00010832
Iteration 9/1000 | Loss: 0.00010609
Iteration 10/1000 | Loss: 0.00010138
Iteration 11/1000 | Loss: 0.00003794
Iteration 12/1000 | Loss: 0.00034599
Iteration 13/1000 | Loss: 0.00004782
Iteration 14/1000 | Loss: 0.00003495
Iteration 15/1000 | Loss: 0.00003150
Iteration 16/1000 | Loss: 0.00002927
Iteration 17/1000 | Loss: 0.00002827
Iteration 18/1000 | Loss: 0.00002773
Iteration 19/1000 | Loss: 0.00002732
Iteration 20/1000 | Loss: 0.00002693
Iteration 21/1000 | Loss: 0.00029362
Iteration 22/1000 | Loss: 0.00086882
Iteration 23/1000 | Loss: 0.00005405
Iteration 24/1000 | Loss: 0.00003382
Iteration 25/1000 | Loss: 0.00002883
Iteration 26/1000 | Loss: 0.00002517
Iteration 27/1000 | Loss: 0.00002207
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001681
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001600
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001583
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001569
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001563
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001562
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001562
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001558
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001558
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001558
Iteration 146/1000 | Loss: 0.00001558
Iteration 147/1000 | Loss: 0.00001558
Iteration 148/1000 | Loss: 0.00001558
Iteration 149/1000 | Loss: 0.00001558
Iteration 150/1000 | Loss: 0.00001558
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001558
Iteration 153/1000 | Loss: 0.00001558
Iteration 154/1000 | Loss: 0.00001558
Iteration 155/1000 | Loss: 0.00001558
Iteration 156/1000 | Loss: 0.00001558
Iteration 157/1000 | Loss: 0.00001558
Iteration 158/1000 | Loss: 0.00001558
Iteration 159/1000 | Loss: 0.00001558
Iteration 160/1000 | Loss: 0.00001558
Iteration 161/1000 | Loss: 0.00001558
Iteration 162/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.5582063497276977e-05, 1.5582063497276977e-05, 1.5582063497276977e-05, 1.5582063497276977e-05, 1.5582063497276977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5582063497276977e-05

Optimization complete. Final v2v error: 3.3704960346221924 mm

Highest mean error: 3.980302333831787 mm for frame 178

Lowest mean error: 3.033966064453125 mm for frame 131

Saving results

Total time: 116.22355532646179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781085
Iteration 2/25 | Loss: 0.00135267
Iteration 3/25 | Loss: 0.00122881
Iteration 4/25 | Loss: 0.00121201
Iteration 5/25 | Loss: 0.00120384
Iteration 6/25 | Loss: 0.00120253
Iteration 7/25 | Loss: 0.00120149
Iteration 8/25 | Loss: 0.00120754
Iteration 9/25 | Loss: 0.00119161
Iteration 10/25 | Loss: 0.00118728
Iteration 11/25 | Loss: 0.00119897
Iteration 12/25 | Loss: 0.00118647
Iteration 13/25 | Loss: 0.00118628
Iteration 14/25 | Loss: 0.00118591
Iteration 15/25 | Loss: 0.00118517
Iteration 16/25 | Loss: 0.00118497
Iteration 17/25 | Loss: 0.00118497
Iteration 18/25 | Loss: 0.00118496
Iteration 19/25 | Loss: 0.00118496
Iteration 20/25 | Loss: 0.00118493
Iteration 21/25 | Loss: 0.00118493
Iteration 22/25 | Loss: 0.00118492
Iteration 23/25 | Loss: 0.00118492
Iteration 24/25 | Loss: 0.00118492
Iteration 25/25 | Loss: 0.00118492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.54797840
Iteration 2/25 | Loss: 0.00071526
Iteration 3/25 | Loss: 0.00071520
Iteration 4/25 | Loss: 0.00071520
Iteration 5/25 | Loss: 0.00071520
Iteration 6/25 | Loss: 0.00071520
Iteration 7/25 | Loss: 0.00071520
Iteration 8/25 | Loss: 0.00071520
Iteration 9/25 | Loss: 0.00071520
Iteration 10/25 | Loss: 0.00071520
Iteration 11/25 | Loss: 0.00071520
Iteration 12/25 | Loss: 0.00071520
Iteration 13/25 | Loss: 0.00071520
Iteration 14/25 | Loss: 0.00071520
Iteration 15/25 | Loss: 0.00071519
Iteration 16/25 | Loss: 0.00071519
Iteration 17/25 | Loss: 0.00071519
Iteration 18/25 | Loss: 0.00071519
Iteration 19/25 | Loss: 0.00071519
Iteration 20/25 | Loss: 0.00071519
Iteration 21/25 | Loss: 0.00071519
Iteration 22/25 | Loss: 0.00071519
Iteration 23/25 | Loss: 0.00071519
Iteration 24/25 | Loss: 0.00071519
Iteration 25/25 | Loss: 0.00071519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071519
Iteration 2/1000 | Loss: 0.00002915
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001676
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001506
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001467
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001441
Iteration 19/1000 | Loss: 0.00001440
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001439
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001438
Iteration 25/1000 | Loss: 0.00001437
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001437
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001435
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001434
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001431
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001427
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001424
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.4199670658854302e-05, 1.4199670658854302e-05, 1.4199670658854302e-05, 1.4199670658854302e-05, 1.4199670658854302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4199670658854302e-05

Optimization complete. Final v2v error: 3.1761116981506348 mm

Highest mean error: 3.626044511795044 mm for frame 195

Lowest mean error: 2.777303457260132 mm for frame 1

Saving results

Total time: 58.64093828201294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086970
Iteration 2/25 | Loss: 0.00189352
Iteration 3/25 | Loss: 0.00142578
Iteration 4/25 | Loss: 0.00137520
Iteration 5/25 | Loss: 0.00136333
Iteration 6/25 | Loss: 0.00137540
Iteration 7/25 | Loss: 0.00136815
Iteration 8/25 | Loss: 0.00134541
Iteration 9/25 | Loss: 0.00135226
Iteration 10/25 | Loss: 0.00133800
Iteration 11/25 | Loss: 0.00133883
Iteration 12/25 | Loss: 0.00133336
Iteration 13/25 | Loss: 0.00132954
Iteration 14/25 | Loss: 0.00133122
Iteration 15/25 | Loss: 0.00132791
Iteration 16/25 | Loss: 0.00131641
Iteration 17/25 | Loss: 0.00131450
Iteration 18/25 | Loss: 0.00131088
Iteration 19/25 | Loss: 0.00131223
Iteration 20/25 | Loss: 0.00131241
Iteration 21/25 | Loss: 0.00131039
Iteration 22/25 | Loss: 0.00130707
Iteration 23/25 | Loss: 0.00130456
Iteration 24/25 | Loss: 0.00130551
Iteration 25/25 | Loss: 0.00130605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99429446
Iteration 2/25 | Loss: 0.00116697
Iteration 3/25 | Loss: 0.00116697
Iteration 4/25 | Loss: 0.00116697
Iteration 5/25 | Loss: 0.00116696
Iteration 6/25 | Loss: 0.00116696
Iteration 7/25 | Loss: 0.00116696
Iteration 8/25 | Loss: 0.00116696
Iteration 9/25 | Loss: 0.00116696
Iteration 10/25 | Loss: 0.00116696
Iteration 11/25 | Loss: 0.00116696
Iteration 12/25 | Loss: 0.00116696
Iteration 13/25 | Loss: 0.00116696
Iteration 14/25 | Loss: 0.00116696
Iteration 15/25 | Loss: 0.00116696
Iteration 16/25 | Loss: 0.00116696
Iteration 17/25 | Loss: 0.00116696
Iteration 18/25 | Loss: 0.00116696
Iteration 19/25 | Loss: 0.00116696
Iteration 20/25 | Loss: 0.00116696
Iteration 21/25 | Loss: 0.00116696
Iteration 22/25 | Loss: 0.00116696
Iteration 23/25 | Loss: 0.00116696
Iteration 24/25 | Loss: 0.00116696
Iteration 25/25 | Loss: 0.00116696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116696
Iteration 2/1000 | Loss: 0.00015790
Iteration 3/1000 | Loss: 0.00004953
Iteration 4/1000 | Loss: 0.00009919
Iteration 5/1000 | Loss: 0.00010686
Iteration 6/1000 | Loss: 0.00018986
Iteration 7/1000 | Loss: 0.00016581
Iteration 8/1000 | Loss: 0.00019816
Iteration 9/1000 | Loss: 0.00024327
Iteration 10/1000 | Loss: 0.00033286
Iteration 11/1000 | Loss: 0.00035959
Iteration 12/1000 | Loss: 0.00024305
Iteration 13/1000 | Loss: 0.00026812
Iteration 14/1000 | Loss: 0.00006766
Iteration 15/1000 | Loss: 0.00035853
Iteration 16/1000 | Loss: 0.00035145
Iteration 17/1000 | Loss: 0.00028175
Iteration 18/1000 | Loss: 0.00025983
Iteration 19/1000 | Loss: 0.00027700
Iteration 20/1000 | Loss: 0.00036039
Iteration 21/1000 | Loss: 0.00012106
Iteration 22/1000 | Loss: 0.00035607
Iteration 23/1000 | Loss: 0.00024524
Iteration 24/1000 | Loss: 0.00024896
Iteration 25/1000 | Loss: 0.00035061
Iteration 26/1000 | Loss: 0.00025122
Iteration 27/1000 | Loss: 0.00018277
Iteration 28/1000 | Loss: 0.00063583
Iteration 29/1000 | Loss: 0.00016902
Iteration 30/1000 | Loss: 0.00013682
Iteration 31/1000 | Loss: 0.00005206
Iteration 32/1000 | Loss: 0.00004526
Iteration 33/1000 | Loss: 0.00014798
Iteration 34/1000 | Loss: 0.00005161
Iteration 35/1000 | Loss: 0.00004426
Iteration 36/1000 | Loss: 0.00011971
Iteration 37/1000 | Loss: 0.00015902
Iteration 38/1000 | Loss: 0.00008047
Iteration 39/1000 | Loss: 0.00010628
Iteration 40/1000 | Loss: 0.00014917
Iteration 41/1000 | Loss: 0.00014904
Iteration 42/1000 | Loss: 0.00017831
Iteration 43/1000 | Loss: 0.00016109
Iteration 44/1000 | Loss: 0.00016289
Iteration 45/1000 | Loss: 0.00008190
Iteration 46/1000 | Loss: 0.00014455
Iteration 47/1000 | Loss: 0.00007515
Iteration 48/1000 | Loss: 0.00013189
Iteration 49/1000 | Loss: 0.00007943
Iteration 50/1000 | Loss: 0.00003910
Iteration 51/1000 | Loss: 0.00005305
Iteration 52/1000 | Loss: 0.00028332
Iteration 53/1000 | Loss: 0.00007103
Iteration 54/1000 | Loss: 0.00004358
Iteration 55/1000 | Loss: 0.00004149
Iteration 56/1000 | Loss: 0.00003735
Iteration 57/1000 | Loss: 0.00004838
Iteration 58/1000 | Loss: 0.00004237
Iteration 59/1000 | Loss: 0.00003592
Iteration 60/1000 | Loss: 0.00005309
Iteration 61/1000 | Loss: 0.00005305
Iteration 62/1000 | Loss: 0.00004267
Iteration 63/1000 | Loss: 0.00004968
Iteration 64/1000 | Loss: 0.00003988
Iteration 65/1000 | Loss: 0.00003925
Iteration 66/1000 | Loss: 0.00004538
Iteration 67/1000 | Loss: 0.00004472
Iteration 68/1000 | Loss: 0.00005102
Iteration 69/1000 | Loss: 0.00005006
Iteration 70/1000 | Loss: 0.00005144
Iteration 71/1000 | Loss: 0.00004041
Iteration 72/1000 | Loss: 0.00005633
Iteration 73/1000 | Loss: 0.00004209
Iteration 74/1000 | Loss: 0.00005554
Iteration 75/1000 | Loss: 0.00004691
Iteration 76/1000 | Loss: 0.00003509
Iteration 77/1000 | Loss: 0.00005203
Iteration 78/1000 | Loss: 0.00004465
Iteration 79/1000 | Loss: 0.00005068
Iteration 80/1000 | Loss: 0.00004335
Iteration 81/1000 | Loss: 0.00005894
Iteration 82/1000 | Loss: 0.00005150
Iteration 83/1000 | Loss: 0.00005169
Iteration 84/1000 | Loss: 0.00004312
Iteration 85/1000 | Loss: 0.00005359
Iteration 86/1000 | Loss: 0.00004177
Iteration 87/1000 | Loss: 0.00003983
Iteration 88/1000 | Loss: 0.00004349
Iteration 89/1000 | Loss: 0.00004206
Iteration 90/1000 | Loss: 0.00004394
Iteration 91/1000 | Loss: 0.00004052
Iteration 92/1000 | Loss: 0.00004543
Iteration 93/1000 | Loss: 0.00004891
Iteration 94/1000 | Loss: 0.00005055
Iteration 95/1000 | Loss: 0.00004824
Iteration 96/1000 | Loss: 0.00006566
Iteration 97/1000 | Loss: 0.00004948
Iteration 98/1000 | Loss: 0.00005013
Iteration 99/1000 | Loss: 0.00004844
Iteration 100/1000 | Loss: 0.00005251
Iteration 101/1000 | Loss: 0.00005986
Iteration 102/1000 | Loss: 0.00005186
Iteration 103/1000 | Loss: 0.00005651
Iteration 104/1000 | Loss: 0.00005149
Iteration 105/1000 | Loss: 0.00004737
Iteration 106/1000 | Loss: 0.00005146
Iteration 107/1000 | Loss: 0.00005127
Iteration 108/1000 | Loss: 0.00004989
Iteration 109/1000 | Loss: 0.00004871
Iteration 110/1000 | Loss: 0.00004970
Iteration 111/1000 | Loss: 0.00005236
Iteration 112/1000 | Loss: 0.00004064
Iteration 113/1000 | Loss: 0.00004129
Iteration 114/1000 | Loss: 0.00003989
Iteration 115/1000 | Loss: 0.00004990
Iteration 116/1000 | Loss: 0.00004256
Iteration 117/1000 | Loss: 0.00005546
Iteration 118/1000 | Loss: 0.00004623
Iteration 119/1000 | Loss: 0.00004900
Iteration 120/1000 | Loss: 0.00004693
Iteration 121/1000 | Loss: 0.00005598
Iteration 122/1000 | Loss: 0.00004021
Iteration 123/1000 | Loss: 0.00004390
Iteration 124/1000 | Loss: 0.00004547
Iteration 125/1000 | Loss: 0.00005022
Iteration 126/1000 | Loss: 0.00004631
Iteration 127/1000 | Loss: 0.00005551
Iteration 128/1000 | Loss: 0.00004629
Iteration 129/1000 | Loss: 0.00005004
Iteration 130/1000 | Loss: 0.00004618
Iteration 131/1000 | Loss: 0.00004979
Iteration 132/1000 | Loss: 0.00004558
Iteration 133/1000 | Loss: 0.00004927
Iteration 134/1000 | Loss: 0.00004513
Iteration 135/1000 | Loss: 0.00004859
Iteration 136/1000 | Loss: 0.00004489
Iteration 137/1000 | Loss: 0.00004816
Iteration 138/1000 | Loss: 0.00004484
Iteration 139/1000 | Loss: 0.00004769
Iteration 140/1000 | Loss: 0.00004523
Iteration 141/1000 | Loss: 0.00004858
Iteration 142/1000 | Loss: 0.00004890
Iteration 143/1000 | Loss: 0.00004797
Iteration 144/1000 | Loss: 0.00004910
Iteration 145/1000 | Loss: 0.00004704
Iteration 146/1000 | Loss: 0.00004150
Iteration 147/1000 | Loss: 0.00005118
Iteration 148/1000 | Loss: 0.00004632
Iteration 149/1000 | Loss: 0.00004726
Iteration 150/1000 | Loss: 0.00004175
Iteration 151/1000 | Loss: 0.00005037
Iteration 152/1000 | Loss: 0.00004622
Iteration 153/1000 | Loss: 0.00004866
Iteration 154/1000 | Loss: 0.00004639
Iteration 155/1000 | Loss: 0.00003441
Iteration 156/1000 | Loss: 0.00005116
Iteration 157/1000 | Loss: 0.00004685
Iteration 158/1000 | Loss: 0.00004472
Iteration 159/1000 | Loss: 0.00004069
Iteration 160/1000 | Loss: 0.00004824
Iteration 161/1000 | Loss: 0.00005311
Iteration 162/1000 | Loss: 0.00004406
Iteration 163/1000 | Loss: 0.00004728
Iteration 164/1000 | Loss: 0.00005549
Iteration 165/1000 | Loss: 0.00005689
Iteration 166/1000 | Loss: 0.00005221
Iteration 167/1000 | Loss: 0.00003207
Iteration 168/1000 | Loss: 0.00005342
Iteration 169/1000 | Loss: 0.00002802
Iteration 170/1000 | Loss: 0.00002649
Iteration 171/1000 | Loss: 0.00002542
Iteration 172/1000 | Loss: 0.00002480
Iteration 173/1000 | Loss: 0.00002451
Iteration 174/1000 | Loss: 0.00002425
Iteration 175/1000 | Loss: 0.00002405
Iteration 176/1000 | Loss: 0.00002381
Iteration 177/1000 | Loss: 0.00002362
Iteration 178/1000 | Loss: 0.00002346
Iteration 179/1000 | Loss: 0.00002331
Iteration 180/1000 | Loss: 0.00002319
Iteration 181/1000 | Loss: 0.00002319
Iteration 182/1000 | Loss: 0.00002306
Iteration 183/1000 | Loss: 0.00002301
Iteration 184/1000 | Loss: 0.00002297
Iteration 185/1000 | Loss: 0.00002294
Iteration 186/1000 | Loss: 0.00002294
Iteration 187/1000 | Loss: 0.00002293
Iteration 188/1000 | Loss: 0.00002293
Iteration 189/1000 | Loss: 0.00002292
Iteration 190/1000 | Loss: 0.00002292
Iteration 191/1000 | Loss: 0.00002292
Iteration 192/1000 | Loss: 0.00002291
Iteration 193/1000 | Loss: 0.00002288
Iteration 194/1000 | Loss: 0.00002288
Iteration 195/1000 | Loss: 0.00002288
Iteration 196/1000 | Loss: 0.00002288
Iteration 197/1000 | Loss: 0.00002287
Iteration 198/1000 | Loss: 0.00002287
Iteration 199/1000 | Loss: 0.00002287
Iteration 200/1000 | Loss: 0.00002286
Iteration 201/1000 | Loss: 0.00002286
Iteration 202/1000 | Loss: 0.00002286
Iteration 203/1000 | Loss: 0.00002285
Iteration 204/1000 | Loss: 0.00002285
Iteration 205/1000 | Loss: 0.00002285
Iteration 206/1000 | Loss: 0.00002285
Iteration 207/1000 | Loss: 0.00002285
Iteration 208/1000 | Loss: 0.00002285
Iteration 209/1000 | Loss: 0.00002285
Iteration 210/1000 | Loss: 0.00002284
Iteration 211/1000 | Loss: 0.00002284
Iteration 212/1000 | Loss: 0.00002284
Iteration 213/1000 | Loss: 0.00002284
Iteration 214/1000 | Loss: 0.00002284
Iteration 215/1000 | Loss: 0.00002283
Iteration 216/1000 | Loss: 0.00002283
Iteration 217/1000 | Loss: 0.00002283
Iteration 218/1000 | Loss: 0.00002283
Iteration 219/1000 | Loss: 0.00002283
Iteration 220/1000 | Loss: 0.00002283
Iteration 221/1000 | Loss: 0.00002283
Iteration 222/1000 | Loss: 0.00002283
Iteration 223/1000 | Loss: 0.00002282
Iteration 224/1000 | Loss: 0.00002282
Iteration 225/1000 | Loss: 0.00002282
Iteration 226/1000 | Loss: 0.00002282
Iteration 227/1000 | Loss: 0.00002281
Iteration 228/1000 | Loss: 0.00002281
Iteration 229/1000 | Loss: 0.00002281
Iteration 230/1000 | Loss: 0.00002281
Iteration 231/1000 | Loss: 0.00002281
Iteration 232/1000 | Loss: 0.00002281
Iteration 233/1000 | Loss: 0.00002281
Iteration 234/1000 | Loss: 0.00002280
Iteration 235/1000 | Loss: 0.00002280
Iteration 236/1000 | Loss: 0.00002280
Iteration 237/1000 | Loss: 0.00002280
Iteration 238/1000 | Loss: 0.00002280
Iteration 239/1000 | Loss: 0.00002280
Iteration 240/1000 | Loss: 0.00002280
Iteration 241/1000 | Loss: 0.00002280
Iteration 242/1000 | Loss: 0.00002280
Iteration 243/1000 | Loss: 0.00002280
Iteration 244/1000 | Loss: 0.00002279
Iteration 245/1000 | Loss: 0.00002279
Iteration 246/1000 | Loss: 0.00002279
Iteration 247/1000 | Loss: 0.00002279
Iteration 248/1000 | Loss: 0.00002279
Iteration 249/1000 | Loss: 0.00002279
Iteration 250/1000 | Loss: 0.00002279
Iteration 251/1000 | Loss: 0.00002279
Iteration 252/1000 | Loss: 0.00002279
Iteration 253/1000 | Loss: 0.00002279
Iteration 254/1000 | Loss: 0.00002279
Iteration 255/1000 | Loss: 0.00002279
Iteration 256/1000 | Loss: 0.00002278
Iteration 257/1000 | Loss: 0.00002278
Iteration 258/1000 | Loss: 0.00002278
Iteration 259/1000 | Loss: 0.00002278
Iteration 260/1000 | Loss: 0.00002278
Iteration 261/1000 | Loss: 0.00002278
Iteration 262/1000 | Loss: 0.00002278
Iteration 263/1000 | Loss: 0.00002278
Iteration 264/1000 | Loss: 0.00002278
Iteration 265/1000 | Loss: 0.00002277
Iteration 266/1000 | Loss: 0.00002277
Iteration 267/1000 | Loss: 0.00002277
Iteration 268/1000 | Loss: 0.00002277
Iteration 269/1000 | Loss: 0.00002276
Iteration 270/1000 | Loss: 0.00002276
Iteration 271/1000 | Loss: 0.00002276
Iteration 272/1000 | Loss: 0.00002275
Iteration 273/1000 | Loss: 0.00002275
Iteration 274/1000 | Loss: 0.00002275
Iteration 275/1000 | Loss: 0.00002275
Iteration 276/1000 | Loss: 0.00002275
Iteration 277/1000 | Loss: 0.00002275
Iteration 278/1000 | Loss: 0.00002275
Iteration 279/1000 | Loss: 0.00002274
Iteration 280/1000 | Loss: 0.00002274
Iteration 281/1000 | Loss: 0.00002274
Iteration 282/1000 | Loss: 0.00002274
Iteration 283/1000 | Loss: 0.00002274
Iteration 284/1000 | Loss: 0.00002274
Iteration 285/1000 | Loss: 0.00002274
Iteration 286/1000 | Loss: 0.00002274
Iteration 287/1000 | Loss: 0.00002273
Iteration 288/1000 | Loss: 0.00002273
Iteration 289/1000 | Loss: 0.00002273
Iteration 290/1000 | Loss: 0.00002273
Iteration 291/1000 | Loss: 0.00002273
Iteration 292/1000 | Loss: 0.00002273
Iteration 293/1000 | Loss: 0.00002273
Iteration 294/1000 | Loss: 0.00002273
Iteration 295/1000 | Loss: 0.00002273
Iteration 296/1000 | Loss: 0.00002272
Iteration 297/1000 | Loss: 0.00002272
Iteration 298/1000 | Loss: 0.00002272
Iteration 299/1000 | Loss: 0.00002272
Iteration 300/1000 | Loss: 0.00002272
Iteration 301/1000 | Loss: 0.00002272
Iteration 302/1000 | Loss: 0.00002272
Iteration 303/1000 | Loss: 0.00002272
Iteration 304/1000 | Loss: 0.00002272
Iteration 305/1000 | Loss: 0.00002272
Iteration 306/1000 | Loss: 0.00002272
Iteration 307/1000 | Loss: 0.00002272
Iteration 308/1000 | Loss: 0.00002272
Iteration 309/1000 | Loss: 0.00002272
Iteration 310/1000 | Loss: 0.00002272
Iteration 311/1000 | Loss: 0.00002272
Iteration 312/1000 | Loss: 0.00002272
Iteration 313/1000 | Loss: 0.00002272
Iteration 314/1000 | Loss: 0.00002272
Iteration 315/1000 | Loss: 0.00002272
Iteration 316/1000 | Loss: 0.00002272
Iteration 317/1000 | Loss: 0.00002272
Iteration 318/1000 | Loss: 0.00002272
Iteration 319/1000 | Loss: 0.00002272
Iteration 320/1000 | Loss: 0.00002272
Iteration 321/1000 | Loss: 0.00002272
Iteration 322/1000 | Loss: 0.00002272
Iteration 323/1000 | Loss: 0.00002272
Iteration 324/1000 | Loss: 0.00002272
Iteration 325/1000 | Loss: 0.00002272
Iteration 326/1000 | Loss: 0.00002272
Iteration 327/1000 | Loss: 0.00002272
Iteration 328/1000 | Loss: 0.00002272
Iteration 329/1000 | Loss: 0.00002272
Iteration 330/1000 | Loss: 0.00002272
Iteration 331/1000 | Loss: 0.00002272
Iteration 332/1000 | Loss: 0.00002272
Iteration 333/1000 | Loss: 0.00002272
Iteration 334/1000 | Loss: 0.00002272
Iteration 335/1000 | Loss: 0.00002272
Iteration 336/1000 | Loss: 0.00002272
Iteration 337/1000 | Loss: 0.00002272
Iteration 338/1000 | Loss: 0.00002272
Iteration 339/1000 | Loss: 0.00002272
Iteration 340/1000 | Loss: 0.00002272
Iteration 341/1000 | Loss: 0.00002272
Iteration 342/1000 | Loss: 0.00002272
Iteration 343/1000 | Loss: 0.00002272
Iteration 344/1000 | Loss: 0.00002272
Iteration 345/1000 | Loss: 0.00002272
Iteration 346/1000 | Loss: 0.00002272
Iteration 347/1000 | Loss: 0.00002272
Iteration 348/1000 | Loss: 0.00002272
Iteration 349/1000 | Loss: 0.00002272
Iteration 350/1000 | Loss: 0.00002272
Iteration 351/1000 | Loss: 0.00002272
Iteration 352/1000 | Loss: 0.00002272
Iteration 353/1000 | Loss: 0.00002272
Iteration 354/1000 | Loss: 0.00002272
Iteration 355/1000 | Loss: 0.00002272
Iteration 356/1000 | Loss: 0.00002272
Iteration 357/1000 | Loss: 0.00002272
Iteration 358/1000 | Loss: 0.00002272
Iteration 359/1000 | Loss: 0.00002272
Iteration 360/1000 | Loss: 0.00002272
Iteration 361/1000 | Loss: 0.00002272
Iteration 362/1000 | Loss: 0.00002272
Iteration 363/1000 | Loss: 0.00002272
Iteration 364/1000 | Loss: 0.00002272
Iteration 365/1000 | Loss: 0.00002272
Iteration 366/1000 | Loss: 0.00002272
Iteration 367/1000 | Loss: 0.00002272
Iteration 368/1000 | Loss: 0.00002272
Iteration 369/1000 | Loss: 0.00002272
Iteration 370/1000 | Loss: 0.00002272
Iteration 371/1000 | Loss: 0.00002272
Iteration 372/1000 | Loss: 0.00002272
Iteration 373/1000 | Loss: 0.00002272
Iteration 374/1000 | Loss: 0.00002272
Iteration 375/1000 | Loss: 0.00002272
Iteration 376/1000 | Loss: 0.00002272
Iteration 377/1000 | Loss: 0.00002272
Iteration 378/1000 | Loss: 0.00002272
Iteration 379/1000 | Loss: 0.00002272
Iteration 380/1000 | Loss: 0.00002272
Iteration 381/1000 | Loss: 0.00002272
Iteration 382/1000 | Loss: 0.00002272
Iteration 383/1000 | Loss: 0.00002272
Iteration 384/1000 | Loss: 0.00002272
Iteration 385/1000 | Loss: 0.00002272
Iteration 386/1000 | Loss: 0.00002272
Iteration 387/1000 | Loss: 0.00002272
Iteration 388/1000 | Loss: 0.00002272
Iteration 389/1000 | Loss: 0.00002272
Iteration 390/1000 | Loss: 0.00002272
Iteration 391/1000 | Loss: 0.00002272
Iteration 392/1000 | Loss: 0.00002272
Iteration 393/1000 | Loss: 0.00002272
Iteration 394/1000 | Loss: 0.00002272
Iteration 395/1000 | Loss: 0.00002272
Iteration 396/1000 | Loss: 0.00002272
Iteration 397/1000 | Loss: 0.00002272
Iteration 398/1000 | Loss: 0.00002272
Iteration 399/1000 | Loss: 0.00002272
Iteration 400/1000 | Loss: 0.00002272
Iteration 401/1000 | Loss: 0.00002272
Iteration 402/1000 | Loss: 0.00002272
Iteration 403/1000 | Loss: 0.00002272
Iteration 404/1000 | Loss: 0.00002272
Iteration 405/1000 | Loss: 0.00002272
Iteration 406/1000 | Loss: 0.00002272
Iteration 407/1000 | Loss: 0.00002272
Iteration 408/1000 | Loss: 0.00002272
Iteration 409/1000 | Loss: 0.00002272
Iteration 410/1000 | Loss: 0.00002272
Iteration 411/1000 | Loss: 0.00002272
Iteration 412/1000 | Loss: 0.00002272
Iteration 413/1000 | Loss: 0.00002272
Iteration 414/1000 | Loss: 0.00002272
Iteration 415/1000 | Loss: 0.00002272
Iteration 416/1000 | Loss: 0.00002272
Iteration 417/1000 | Loss: 0.00002272
Iteration 418/1000 | Loss: 0.00002272
Iteration 419/1000 | Loss: 0.00002272
Iteration 420/1000 | Loss: 0.00002272
Iteration 421/1000 | Loss: 0.00002272
Iteration 422/1000 | Loss: 0.00002272
Iteration 423/1000 | Loss: 0.00002272
Iteration 424/1000 | Loss: 0.00002272
Iteration 425/1000 | Loss: 0.00002272
Iteration 426/1000 | Loss: 0.00002272
Iteration 427/1000 | Loss: 0.00002272
Iteration 428/1000 | Loss: 0.00002272
Iteration 429/1000 | Loss: 0.00002272
Iteration 430/1000 | Loss: 0.00002272
Iteration 431/1000 | Loss: 0.00002272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 431. Stopping optimization.
Last 5 losses: [2.2716127205058e-05, 2.2716127205058e-05, 2.2716127205058e-05, 2.2716127205058e-05, 2.2716127205058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2716127205058e-05

Optimization complete. Final v2v error: 3.953591823577881 mm

Highest mean error: 4.933201789855957 mm for frame 156

Lowest mean error: 3.310356616973877 mm for frame 22

Saving results

Total time: 340.93198323249817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963563
Iteration 2/25 | Loss: 0.00302836
Iteration 3/25 | Loss: 0.00237146
Iteration 4/25 | Loss: 0.00243958
Iteration 5/25 | Loss: 0.00205805
Iteration 6/25 | Loss: 0.00196756
Iteration 7/25 | Loss: 0.00191968
Iteration 8/25 | Loss: 0.00187473
Iteration 9/25 | Loss: 0.00187989
Iteration 10/25 | Loss: 0.00189366
Iteration 11/25 | Loss: 0.00186144
Iteration 12/25 | Loss: 0.00185306
Iteration 13/25 | Loss: 0.00184662
Iteration 14/25 | Loss: 0.00184636
Iteration 15/25 | Loss: 0.00184608
Iteration 16/25 | Loss: 0.00184722
Iteration 17/25 | Loss: 0.00185686
Iteration 18/25 | Loss: 0.00184177
Iteration 19/25 | Loss: 0.00184176
Iteration 20/25 | Loss: 0.00184173
Iteration 21/25 | Loss: 0.00184172
Iteration 22/25 | Loss: 0.00184172
Iteration 23/25 | Loss: 0.00184172
Iteration 24/25 | Loss: 0.00184172
Iteration 25/25 | Loss: 0.00184172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30894232
Iteration 2/25 | Loss: 0.00694300
Iteration 3/25 | Loss: 0.00394979
Iteration 4/25 | Loss: 0.00394979
Iteration 5/25 | Loss: 0.00394979
Iteration 6/25 | Loss: 0.00394978
Iteration 7/25 | Loss: 0.00394978
Iteration 8/25 | Loss: 0.00394978
Iteration 9/25 | Loss: 0.00394978
Iteration 10/25 | Loss: 0.00394978
Iteration 11/25 | Loss: 0.00394978
Iteration 12/25 | Loss: 0.00394978
Iteration 13/25 | Loss: 0.00394978
Iteration 14/25 | Loss: 0.00394978
Iteration 15/25 | Loss: 0.00394978
Iteration 16/25 | Loss: 0.00394978
Iteration 17/25 | Loss: 0.00394978
Iteration 18/25 | Loss: 0.00394978
Iteration 19/25 | Loss: 0.00394978
Iteration 20/25 | Loss: 0.00394978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003949782345443964, 0.003949782345443964, 0.003949782345443964, 0.003949782345443964, 0.003949782345443964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003949782345443964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00394978
Iteration 2/1000 | Loss: 0.00301492
Iteration 3/1000 | Loss: 0.00087250
Iteration 4/1000 | Loss: 0.00127735
Iteration 5/1000 | Loss: 0.00262755
Iteration 6/1000 | Loss: 0.00048830
Iteration 7/1000 | Loss: 0.00070403
Iteration 8/1000 | Loss: 0.00034284
Iteration 9/1000 | Loss: 0.00032751
Iteration 10/1000 | Loss: 0.00031181
Iteration 11/1000 | Loss: 0.00029988
Iteration 12/1000 | Loss: 0.00585805
Iteration 13/1000 | Loss: 0.00717382
Iteration 14/1000 | Loss: 0.00267017
Iteration 15/1000 | Loss: 0.00032238
Iteration 16/1000 | Loss: 0.00269258
Iteration 17/1000 | Loss: 0.00331664
Iteration 18/1000 | Loss: 0.00029367
Iteration 19/1000 | Loss: 0.00040280
Iteration 20/1000 | Loss: 0.00037777
Iteration 21/1000 | Loss: 0.00142609
Iteration 22/1000 | Loss: 0.00532828
Iteration 23/1000 | Loss: 0.00174767
Iteration 24/1000 | Loss: 0.00017483
Iteration 25/1000 | Loss: 0.00008938
Iteration 26/1000 | Loss: 0.00187128
Iteration 27/1000 | Loss: 0.00267437
Iteration 28/1000 | Loss: 0.00027991
Iteration 29/1000 | Loss: 0.00039676
Iteration 30/1000 | Loss: 0.00011708
Iteration 31/1000 | Loss: 0.00038726
Iteration 32/1000 | Loss: 0.00116604
Iteration 33/1000 | Loss: 0.00005143
Iteration 34/1000 | Loss: 0.00015524
Iteration 35/1000 | Loss: 0.00014059
Iteration 36/1000 | Loss: 0.00064779
Iteration 37/1000 | Loss: 0.00022721
Iteration 38/1000 | Loss: 0.00034603
Iteration 39/1000 | Loss: 0.00776393
Iteration 40/1000 | Loss: 0.00044064
Iteration 41/1000 | Loss: 0.00016123
Iteration 42/1000 | Loss: 0.00016980
Iteration 43/1000 | Loss: 0.00021974
Iteration 44/1000 | Loss: 0.00002175
Iteration 45/1000 | Loss: 0.00006998
Iteration 46/1000 | Loss: 0.00035425
Iteration 47/1000 | Loss: 0.00008048
Iteration 48/1000 | Loss: 0.00003361
Iteration 49/1000 | Loss: 0.00002830
Iteration 50/1000 | Loss: 0.00005781
Iteration 51/1000 | Loss: 0.00010337
Iteration 52/1000 | Loss: 0.00001613
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00013578
Iteration 55/1000 | Loss: 0.00017031
Iteration 56/1000 | Loss: 0.00007698
Iteration 57/1000 | Loss: 0.00007837
Iteration 58/1000 | Loss: 0.00017928
Iteration 59/1000 | Loss: 0.00002931
Iteration 60/1000 | Loss: 0.00009949
Iteration 61/1000 | Loss: 0.00003554
Iteration 62/1000 | Loss: 0.00001408
Iteration 63/1000 | Loss: 0.00003334
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00006969
Iteration 67/1000 | Loss: 0.00007365
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00002701
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001353
Iteration 82/1000 | Loss: 0.00001353
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00004535
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00005850
Iteration 98/1000 | Loss: 0.00140975
Iteration 99/1000 | Loss: 0.00009463
Iteration 100/1000 | Loss: 0.00003284
Iteration 101/1000 | Loss: 0.00010343
Iteration 102/1000 | Loss: 0.00004163
Iteration 103/1000 | Loss: 0.00006821
Iteration 104/1000 | Loss: 0.00010017
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00003709
Iteration 108/1000 | Loss: 0.00024111
Iteration 109/1000 | Loss: 0.00002527
Iteration 110/1000 | Loss: 0.00002455
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001332
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001332
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001332
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00004019
Iteration 137/1000 | Loss: 0.00001938
Iteration 138/1000 | Loss: 0.00004998
Iteration 139/1000 | Loss: 0.00001993
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001335
Iteration 143/1000 | Loss: 0.00001335
Iteration 144/1000 | Loss: 0.00001335
Iteration 145/1000 | Loss: 0.00001335
Iteration 146/1000 | Loss: 0.00001335
Iteration 147/1000 | Loss: 0.00001335
Iteration 148/1000 | Loss: 0.00001335
Iteration 149/1000 | Loss: 0.00001335
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001335
Iteration 152/1000 | Loss: 0.00001334
Iteration 153/1000 | Loss: 0.00001334
Iteration 154/1000 | Loss: 0.00001334
Iteration 155/1000 | Loss: 0.00001334
Iteration 156/1000 | Loss: 0.00001333
Iteration 157/1000 | Loss: 0.00001333
Iteration 158/1000 | Loss: 0.00001333
Iteration 159/1000 | Loss: 0.00001332
Iteration 160/1000 | Loss: 0.00001332
Iteration 161/1000 | Loss: 0.00001332
Iteration 162/1000 | Loss: 0.00001332
Iteration 163/1000 | Loss: 0.00001332
Iteration 164/1000 | Loss: 0.00001332
Iteration 165/1000 | Loss: 0.00001332
Iteration 166/1000 | Loss: 0.00001332
Iteration 167/1000 | Loss: 0.00001332
Iteration 168/1000 | Loss: 0.00001331
Iteration 169/1000 | Loss: 0.00001331
Iteration 170/1000 | Loss: 0.00001331
Iteration 171/1000 | Loss: 0.00001331
Iteration 172/1000 | Loss: 0.00001331
Iteration 173/1000 | Loss: 0.00001331
Iteration 174/1000 | Loss: 0.00001331
Iteration 175/1000 | Loss: 0.00001331
Iteration 176/1000 | Loss: 0.00001331
Iteration 177/1000 | Loss: 0.00001331
Iteration 178/1000 | Loss: 0.00001331
Iteration 179/1000 | Loss: 0.00001330
Iteration 180/1000 | Loss: 0.00001330
Iteration 181/1000 | Loss: 0.00001330
Iteration 182/1000 | Loss: 0.00001330
Iteration 183/1000 | Loss: 0.00001330
Iteration 184/1000 | Loss: 0.00001330
Iteration 185/1000 | Loss: 0.00001330
Iteration 186/1000 | Loss: 0.00001329
Iteration 187/1000 | Loss: 0.00001329
Iteration 188/1000 | Loss: 0.00001329
Iteration 189/1000 | Loss: 0.00001329
Iteration 190/1000 | Loss: 0.00001329
Iteration 191/1000 | Loss: 0.00001329
Iteration 192/1000 | Loss: 0.00001329
Iteration 193/1000 | Loss: 0.00001329
Iteration 194/1000 | Loss: 0.00001329
Iteration 195/1000 | Loss: 0.00001329
Iteration 196/1000 | Loss: 0.00001329
Iteration 197/1000 | Loss: 0.00001329
Iteration 198/1000 | Loss: 0.00001329
Iteration 199/1000 | Loss: 0.00001329
Iteration 200/1000 | Loss: 0.00001329
Iteration 201/1000 | Loss: 0.00001329
Iteration 202/1000 | Loss: 0.00001329
Iteration 203/1000 | Loss: 0.00001329
Iteration 204/1000 | Loss: 0.00001329
Iteration 205/1000 | Loss: 0.00001329
Iteration 206/1000 | Loss: 0.00001329
Iteration 207/1000 | Loss: 0.00001329
Iteration 208/1000 | Loss: 0.00001329
Iteration 209/1000 | Loss: 0.00001329
Iteration 210/1000 | Loss: 0.00001329
Iteration 211/1000 | Loss: 0.00001329
Iteration 212/1000 | Loss: 0.00001329
Iteration 213/1000 | Loss: 0.00001329
Iteration 214/1000 | Loss: 0.00001329
Iteration 215/1000 | Loss: 0.00001329
Iteration 216/1000 | Loss: 0.00001329
Iteration 217/1000 | Loss: 0.00001329
Iteration 218/1000 | Loss: 0.00001329
Iteration 219/1000 | Loss: 0.00001329
Iteration 220/1000 | Loss: 0.00001329
Iteration 221/1000 | Loss: 0.00001329
Iteration 222/1000 | Loss: 0.00001329
Iteration 223/1000 | Loss: 0.00001329
Iteration 224/1000 | Loss: 0.00001329
Iteration 225/1000 | Loss: 0.00001329
Iteration 226/1000 | Loss: 0.00001329
Iteration 227/1000 | Loss: 0.00001329
Iteration 228/1000 | Loss: 0.00001329
Iteration 229/1000 | Loss: 0.00001329
Iteration 230/1000 | Loss: 0.00001329
Iteration 231/1000 | Loss: 0.00001329
Iteration 232/1000 | Loss: 0.00001329
Iteration 233/1000 | Loss: 0.00001329
Iteration 234/1000 | Loss: 0.00001329
Iteration 235/1000 | Loss: 0.00001329
Iteration 236/1000 | Loss: 0.00001329
Iteration 237/1000 | Loss: 0.00001329
Iteration 238/1000 | Loss: 0.00001329
Iteration 239/1000 | Loss: 0.00001329
Iteration 240/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.3290968126966618e-05, 1.3290968126966618e-05, 1.3290968126966618e-05, 1.3290968126966618e-05, 1.3290968126966618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3290968126966618e-05

Optimization complete. Final v2v error: 3.131911277770996 mm

Highest mean error: 3.5543391704559326 mm for frame 204

Lowest mean error: 2.9293413162231445 mm for frame 24

Saving results

Total time: 190.82880878448486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404589
Iteration 2/25 | Loss: 0.00129631
Iteration 3/25 | Loss: 0.00115095
Iteration 4/25 | Loss: 0.00113842
Iteration 5/25 | Loss: 0.00113665
Iteration 6/25 | Loss: 0.00113620
Iteration 7/25 | Loss: 0.00113620
Iteration 8/25 | Loss: 0.00113620
Iteration 9/25 | Loss: 0.00113620
Iteration 10/25 | Loss: 0.00113620
Iteration 11/25 | Loss: 0.00113620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011362028308212757, 0.0011362028308212757, 0.0011362028308212757, 0.0011362028308212757, 0.0011362028308212757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011362028308212757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35395324
Iteration 2/25 | Loss: 0.00064193
Iteration 3/25 | Loss: 0.00064193
Iteration 4/25 | Loss: 0.00064193
Iteration 5/25 | Loss: 0.00064192
Iteration 6/25 | Loss: 0.00064192
Iteration 7/25 | Loss: 0.00064192
Iteration 8/25 | Loss: 0.00064192
Iteration 9/25 | Loss: 0.00064192
Iteration 10/25 | Loss: 0.00064192
Iteration 11/25 | Loss: 0.00064192
Iteration 12/25 | Loss: 0.00064192
Iteration 13/25 | Loss: 0.00064192
Iteration 14/25 | Loss: 0.00064192
Iteration 15/25 | Loss: 0.00064192
Iteration 16/25 | Loss: 0.00064192
Iteration 17/25 | Loss: 0.00064192
Iteration 18/25 | Loss: 0.00064192
Iteration 19/25 | Loss: 0.00064192
Iteration 20/25 | Loss: 0.00064192
Iteration 21/25 | Loss: 0.00064192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006419227574951947, 0.0006419227574951947, 0.0006419227574951947, 0.0006419227574951947, 0.0006419227574951947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006419227574951947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064192
Iteration 2/1000 | Loss: 0.00002636
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001226
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001193
Iteration 113/1000 | Loss: 0.00001193
Iteration 114/1000 | Loss: 0.00001193
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.1905415703949984e-05, 1.1905415703949984e-05, 1.1905415703949984e-05, 1.1905415703949984e-05, 1.1905415703949984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1905415703949984e-05

Optimization complete. Final v2v error: 2.968473434448242 mm

Highest mean error: 3.0683469772338867 mm for frame 69

Lowest mean error: 2.8631932735443115 mm for frame 11

Saving results

Total time: 38.68722462654114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886920
Iteration 2/25 | Loss: 0.00129686
Iteration 3/25 | Loss: 0.00118655
Iteration 4/25 | Loss: 0.00116705
Iteration 5/25 | Loss: 0.00116037
Iteration 6/25 | Loss: 0.00115878
Iteration 7/25 | Loss: 0.00115878
Iteration 8/25 | Loss: 0.00115878
Iteration 9/25 | Loss: 0.00115878
Iteration 10/25 | Loss: 0.00115878
Iteration 11/25 | Loss: 0.00115878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011587791377678514, 0.0011587791377678514, 0.0011587791377678514, 0.0011587791377678514, 0.0011587791377678514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011587791377678514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37393379
Iteration 2/25 | Loss: 0.00080811
Iteration 3/25 | Loss: 0.00080809
Iteration 4/25 | Loss: 0.00080809
Iteration 5/25 | Loss: 0.00080809
Iteration 6/25 | Loss: 0.00080809
Iteration 7/25 | Loss: 0.00080809
Iteration 8/25 | Loss: 0.00080809
Iteration 9/25 | Loss: 0.00080809
Iteration 10/25 | Loss: 0.00080809
Iteration 11/25 | Loss: 0.00080809
Iteration 12/25 | Loss: 0.00080809
Iteration 13/25 | Loss: 0.00080809
Iteration 14/25 | Loss: 0.00080809
Iteration 15/25 | Loss: 0.00080809
Iteration 16/25 | Loss: 0.00080809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008080909610725939, 0.0008080909610725939, 0.0008080909610725939, 0.0008080909610725939, 0.0008080909610725939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008080909610725939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080809
Iteration 2/1000 | Loss: 0.00004101
Iteration 3/1000 | Loss: 0.00002567
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001908
Iteration 7/1000 | Loss: 0.00001852
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001744
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001686
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001680
Iteration 23/1000 | Loss: 0.00001680
Iteration 24/1000 | Loss: 0.00001679
Iteration 25/1000 | Loss: 0.00001673
Iteration 26/1000 | Loss: 0.00001673
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001664
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001662
Iteration 37/1000 | Loss: 0.00001662
Iteration 38/1000 | Loss: 0.00001661
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001661
Iteration 45/1000 | Loss: 0.00001661
Iteration 46/1000 | Loss: 0.00001661
Iteration 47/1000 | Loss: 0.00001661
Iteration 48/1000 | Loss: 0.00001661
Iteration 49/1000 | Loss: 0.00001660
Iteration 50/1000 | Loss: 0.00001660
Iteration 51/1000 | Loss: 0.00001660
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001653
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001648
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001638
Iteration 107/1000 | Loss: 0.00001638
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001637
Iteration 110/1000 | Loss: 0.00001637
Iteration 111/1000 | Loss: 0.00001637
Iteration 112/1000 | Loss: 0.00001637
Iteration 113/1000 | Loss: 0.00001637
Iteration 114/1000 | Loss: 0.00001637
Iteration 115/1000 | Loss: 0.00001637
Iteration 116/1000 | Loss: 0.00001636
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001635
Iteration 121/1000 | Loss: 0.00001635
Iteration 122/1000 | Loss: 0.00001635
Iteration 123/1000 | Loss: 0.00001634
Iteration 124/1000 | Loss: 0.00001634
Iteration 125/1000 | Loss: 0.00001634
Iteration 126/1000 | Loss: 0.00001634
Iteration 127/1000 | Loss: 0.00001634
Iteration 128/1000 | Loss: 0.00001634
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001633
Iteration 133/1000 | Loss: 0.00001632
Iteration 134/1000 | Loss: 0.00001632
Iteration 135/1000 | Loss: 0.00001632
Iteration 136/1000 | Loss: 0.00001631
Iteration 137/1000 | Loss: 0.00001631
Iteration 138/1000 | Loss: 0.00001631
Iteration 139/1000 | Loss: 0.00001631
Iteration 140/1000 | Loss: 0.00001631
Iteration 141/1000 | Loss: 0.00001631
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001630
Iteration 144/1000 | Loss: 0.00001630
Iteration 145/1000 | Loss: 0.00001630
Iteration 146/1000 | Loss: 0.00001630
Iteration 147/1000 | Loss: 0.00001630
Iteration 148/1000 | Loss: 0.00001630
Iteration 149/1000 | Loss: 0.00001630
Iteration 150/1000 | Loss: 0.00001630
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001628
Iteration 156/1000 | Loss: 0.00001628
Iteration 157/1000 | Loss: 0.00001628
Iteration 158/1000 | Loss: 0.00001628
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001627
Iteration 163/1000 | Loss: 0.00001627
Iteration 164/1000 | Loss: 0.00001627
Iteration 165/1000 | Loss: 0.00001627
Iteration 166/1000 | Loss: 0.00001627
Iteration 167/1000 | Loss: 0.00001627
Iteration 168/1000 | Loss: 0.00001627
Iteration 169/1000 | Loss: 0.00001627
Iteration 170/1000 | Loss: 0.00001627
Iteration 171/1000 | Loss: 0.00001627
Iteration 172/1000 | Loss: 0.00001627
Iteration 173/1000 | Loss: 0.00001627
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001626
Iteration 176/1000 | Loss: 0.00001626
Iteration 177/1000 | Loss: 0.00001626
Iteration 178/1000 | Loss: 0.00001626
Iteration 179/1000 | Loss: 0.00001626
Iteration 180/1000 | Loss: 0.00001626
Iteration 181/1000 | Loss: 0.00001626
Iteration 182/1000 | Loss: 0.00001626
Iteration 183/1000 | Loss: 0.00001625
Iteration 184/1000 | Loss: 0.00001625
Iteration 185/1000 | Loss: 0.00001625
Iteration 186/1000 | Loss: 0.00001625
Iteration 187/1000 | Loss: 0.00001625
Iteration 188/1000 | Loss: 0.00001625
Iteration 189/1000 | Loss: 0.00001625
Iteration 190/1000 | Loss: 0.00001625
Iteration 191/1000 | Loss: 0.00001625
Iteration 192/1000 | Loss: 0.00001625
Iteration 193/1000 | Loss: 0.00001625
Iteration 194/1000 | Loss: 0.00001625
Iteration 195/1000 | Loss: 0.00001625
Iteration 196/1000 | Loss: 0.00001625
Iteration 197/1000 | Loss: 0.00001625
Iteration 198/1000 | Loss: 0.00001625
Iteration 199/1000 | Loss: 0.00001625
Iteration 200/1000 | Loss: 0.00001625
Iteration 201/1000 | Loss: 0.00001625
Iteration 202/1000 | Loss: 0.00001625
Iteration 203/1000 | Loss: 0.00001625
Iteration 204/1000 | Loss: 0.00001625
Iteration 205/1000 | Loss: 0.00001624
Iteration 206/1000 | Loss: 0.00001624
Iteration 207/1000 | Loss: 0.00001624
Iteration 208/1000 | Loss: 0.00001624
Iteration 209/1000 | Loss: 0.00001624
Iteration 210/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.624489959795028e-05, 1.624489959795028e-05, 1.624489959795028e-05, 1.624489959795028e-05, 1.624489959795028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.624489959795028e-05

Optimization complete. Final v2v error: 3.4026336669921875 mm

Highest mean error: 5.023744106292725 mm for frame 69

Lowest mean error: 2.9587156772613525 mm for frame 51

Saving results

Total time: 44.90439057350159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091499
Iteration 2/25 | Loss: 0.00208555
Iteration 3/25 | Loss: 0.00151850
Iteration 4/25 | Loss: 0.00145445
Iteration 5/25 | Loss: 0.00148315
Iteration 6/25 | Loss: 0.00147918
Iteration 7/25 | Loss: 0.00143180
Iteration 8/25 | Loss: 0.00139391
Iteration 9/25 | Loss: 0.00138249
Iteration 10/25 | Loss: 0.00137493
Iteration 11/25 | Loss: 0.00136769
Iteration 12/25 | Loss: 0.00135718
Iteration 13/25 | Loss: 0.00136227
Iteration 14/25 | Loss: 0.00135904
Iteration 15/25 | Loss: 0.00135676
Iteration 16/25 | Loss: 0.00135923
Iteration 17/25 | Loss: 0.00136063
Iteration 18/25 | Loss: 0.00135566
Iteration 19/25 | Loss: 0.00135585
Iteration 20/25 | Loss: 0.00135552
Iteration 21/25 | Loss: 0.00135656
Iteration 22/25 | Loss: 0.00135536
Iteration 23/25 | Loss: 0.00135453
Iteration 24/25 | Loss: 0.00135152
Iteration 25/25 | Loss: 0.00135401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16162884
Iteration 2/25 | Loss: 0.00164118
Iteration 3/25 | Loss: 0.00164115
Iteration 4/25 | Loss: 0.00164115
Iteration 5/25 | Loss: 0.00164115
Iteration 6/25 | Loss: 0.00164115
Iteration 7/25 | Loss: 0.00164115
Iteration 8/25 | Loss: 0.00164115
Iteration 9/25 | Loss: 0.00164115
Iteration 10/25 | Loss: 0.00164115
Iteration 11/25 | Loss: 0.00164115
Iteration 12/25 | Loss: 0.00164115
Iteration 13/25 | Loss: 0.00164115
Iteration 14/25 | Loss: 0.00164115
Iteration 15/25 | Loss: 0.00164115
Iteration 16/25 | Loss: 0.00164115
Iteration 17/25 | Loss: 0.00164115
Iteration 18/25 | Loss: 0.00164115
Iteration 19/25 | Loss: 0.00164115
Iteration 20/25 | Loss: 0.00164115
Iteration 21/25 | Loss: 0.00164115
Iteration 22/25 | Loss: 0.00164115
Iteration 23/25 | Loss: 0.00164115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016411474207416177, 0.0016411474207416177, 0.0016411474207416177, 0.0016411474207416177, 0.0016411474207416177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016411474207416177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164115
Iteration 2/1000 | Loss: 0.00063172
Iteration 3/1000 | Loss: 0.00041287
Iteration 4/1000 | Loss: 0.00080049
Iteration 5/1000 | Loss: 0.00058570
Iteration 6/1000 | Loss: 0.00068853
Iteration 7/1000 | Loss: 0.00049917
Iteration 8/1000 | Loss: 0.00043799
Iteration 9/1000 | Loss: 0.00041986
Iteration 10/1000 | Loss: 0.00057649
Iteration 11/1000 | Loss: 0.00062315
Iteration 12/1000 | Loss: 0.00068287
Iteration 13/1000 | Loss: 0.00046433
Iteration 14/1000 | Loss: 0.00071821
Iteration 15/1000 | Loss: 0.00039118
Iteration 16/1000 | Loss: 0.00037014
Iteration 17/1000 | Loss: 0.00034459
Iteration 18/1000 | Loss: 0.00042642
Iteration 19/1000 | Loss: 0.00034391
Iteration 20/1000 | Loss: 0.00040586
Iteration 21/1000 | Loss: 0.00045821
Iteration 22/1000 | Loss: 0.00038044
Iteration 23/1000 | Loss: 0.00058504
Iteration 24/1000 | Loss: 0.00047935
Iteration 25/1000 | Loss: 0.00113950
Iteration 26/1000 | Loss: 0.00079590
Iteration 27/1000 | Loss: 0.00049154
Iteration 28/1000 | Loss: 0.00070349
Iteration 29/1000 | Loss: 0.00081228
Iteration 30/1000 | Loss: 0.00041293
Iteration 31/1000 | Loss: 0.00036213
Iteration 32/1000 | Loss: 0.00051135
Iteration 33/1000 | Loss: 0.00050300
Iteration 34/1000 | Loss: 0.00009446
Iteration 35/1000 | Loss: 0.00008844
Iteration 36/1000 | Loss: 0.00012303
Iteration 37/1000 | Loss: 0.00041305
Iteration 38/1000 | Loss: 0.00085960
Iteration 39/1000 | Loss: 0.00062340
Iteration 40/1000 | Loss: 0.00080190
Iteration 41/1000 | Loss: 0.00061101
Iteration 42/1000 | Loss: 0.00045413
Iteration 43/1000 | Loss: 0.00008278
Iteration 44/1000 | Loss: 0.00009204
Iteration 45/1000 | Loss: 0.00015785
Iteration 46/1000 | Loss: 0.00005510
Iteration 47/1000 | Loss: 0.00023490
Iteration 48/1000 | Loss: 0.00041419
Iteration 49/1000 | Loss: 0.00016356
Iteration 50/1000 | Loss: 0.00032339
Iteration 51/1000 | Loss: 0.00006677
Iteration 52/1000 | Loss: 0.00022475
Iteration 53/1000 | Loss: 0.00053355
Iteration 54/1000 | Loss: 0.00052510
Iteration 55/1000 | Loss: 0.00005513
Iteration 56/1000 | Loss: 0.00004887
Iteration 57/1000 | Loss: 0.00004671
Iteration 58/1000 | Loss: 0.00004091
Iteration 59/1000 | Loss: 0.00004243
Iteration 60/1000 | Loss: 0.00003733
Iteration 61/1000 | Loss: 0.00003660
Iteration 62/1000 | Loss: 0.00051634
Iteration 63/1000 | Loss: 0.00011247
Iteration 64/1000 | Loss: 0.00004928
Iteration 65/1000 | Loss: 0.00063581
Iteration 66/1000 | Loss: 0.00045641
Iteration 67/1000 | Loss: 0.00010078
Iteration 68/1000 | Loss: 0.00052727
Iteration 69/1000 | Loss: 0.00008064
Iteration 70/1000 | Loss: 0.00047983
Iteration 71/1000 | Loss: 0.00064455
Iteration 72/1000 | Loss: 0.00057071
Iteration 73/1000 | Loss: 0.00004127
Iteration 74/1000 | Loss: 0.00003635
Iteration 75/1000 | Loss: 0.00025535
Iteration 76/1000 | Loss: 0.00034525
Iteration 77/1000 | Loss: 0.00023222
Iteration 78/1000 | Loss: 0.00042973
Iteration 79/1000 | Loss: 0.00005232
Iteration 80/1000 | Loss: 0.00003960
Iteration 81/1000 | Loss: 0.00003517
Iteration 82/1000 | Loss: 0.00003345
Iteration 83/1000 | Loss: 0.00003201
Iteration 84/1000 | Loss: 0.00003105
Iteration 85/1000 | Loss: 0.00004666
Iteration 86/1000 | Loss: 0.00010533
Iteration 87/1000 | Loss: 0.00004281
Iteration 88/1000 | Loss: 0.00009322
Iteration 89/1000 | Loss: 0.00003059
Iteration 90/1000 | Loss: 0.00004148
Iteration 91/1000 | Loss: 0.00019451
Iteration 92/1000 | Loss: 0.00009842
Iteration 93/1000 | Loss: 0.00003386
Iteration 94/1000 | Loss: 0.00017725
Iteration 95/1000 | Loss: 0.00005050
Iteration 96/1000 | Loss: 0.00004822
Iteration 97/1000 | Loss: 0.00022066
Iteration 98/1000 | Loss: 0.00007846
Iteration 99/1000 | Loss: 0.00003367
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00005176
Iteration 102/1000 | Loss: 0.00003362
Iteration 103/1000 | Loss: 0.00007573
Iteration 104/1000 | Loss: 0.00005194
Iteration 105/1000 | Loss: 0.00009938
Iteration 106/1000 | Loss: 0.00003056
Iteration 107/1000 | Loss: 0.00002945
Iteration 108/1000 | Loss: 0.00002880
Iteration 109/1000 | Loss: 0.00002833
Iteration 110/1000 | Loss: 0.00002829
Iteration 111/1000 | Loss: 0.00004839
Iteration 112/1000 | Loss: 0.00004068
Iteration 113/1000 | Loss: 0.00004662
Iteration 114/1000 | Loss: 0.00003794
Iteration 115/1000 | Loss: 0.00004571
Iteration 116/1000 | Loss: 0.00003264
Iteration 117/1000 | Loss: 0.00003973
Iteration 118/1000 | Loss: 0.00003958
Iteration 119/1000 | Loss: 0.00004096
Iteration 120/1000 | Loss: 0.00003963
Iteration 121/1000 | Loss: 0.00003284
Iteration 122/1000 | Loss: 0.00005367
Iteration 123/1000 | Loss: 0.00002857
Iteration 124/1000 | Loss: 0.00004836
Iteration 125/1000 | Loss: 0.00003045
Iteration 126/1000 | Loss: 0.00002919
Iteration 127/1000 | Loss: 0.00002807
Iteration 128/1000 | Loss: 0.00002755
Iteration 129/1000 | Loss: 0.00002715
Iteration 130/1000 | Loss: 0.00002681
Iteration 131/1000 | Loss: 0.00002650
Iteration 132/1000 | Loss: 0.00002637
Iteration 133/1000 | Loss: 0.00002632
Iteration 134/1000 | Loss: 0.00002630
Iteration 135/1000 | Loss: 0.00002630
Iteration 136/1000 | Loss: 0.00002629
Iteration 137/1000 | Loss: 0.00002629
Iteration 138/1000 | Loss: 0.00002629
Iteration 139/1000 | Loss: 0.00002628
Iteration 140/1000 | Loss: 0.00002628
Iteration 141/1000 | Loss: 0.00002628
Iteration 142/1000 | Loss: 0.00002628
Iteration 143/1000 | Loss: 0.00002628
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002627
Iteration 147/1000 | Loss: 0.00002625
Iteration 148/1000 | Loss: 0.00002625
Iteration 149/1000 | Loss: 0.00002624
Iteration 150/1000 | Loss: 0.00002624
Iteration 151/1000 | Loss: 0.00002624
Iteration 152/1000 | Loss: 0.00002624
Iteration 153/1000 | Loss: 0.00002624
Iteration 154/1000 | Loss: 0.00002624
Iteration 155/1000 | Loss: 0.00002624
Iteration 156/1000 | Loss: 0.00002624
Iteration 157/1000 | Loss: 0.00002624
Iteration 158/1000 | Loss: 0.00002624
Iteration 159/1000 | Loss: 0.00002624
Iteration 160/1000 | Loss: 0.00002623
Iteration 161/1000 | Loss: 0.00002623
Iteration 162/1000 | Loss: 0.00002623
Iteration 163/1000 | Loss: 0.00002623
Iteration 164/1000 | Loss: 0.00002623
Iteration 165/1000 | Loss: 0.00002623
Iteration 166/1000 | Loss: 0.00002623
Iteration 167/1000 | Loss: 0.00002623
Iteration 168/1000 | Loss: 0.00002623
Iteration 169/1000 | Loss: 0.00002623
Iteration 170/1000 | Loss: 0.00002623
Iteration 171/1000 | Loss: 0.00002622
Iteration 172/1000 | Loss: 0.00002622
Iteration 173/1000 | Loss: 0.00002621
Iteration 174/1000 | Loss: 0.00002621
Iteration 175/1000 | Loss: 0.00002621
Iteration 176/1000 | Loss: 0.00002621
Iteration 177/1000 | Loss: 0.00002620
Iteration 178/1000 | Loss: 0.00002619
Iteration 179/1000 | Loss: 0.00002619
Iteration 180/1000 | Loss: 0.00002619
Iteration 181/1000 | Loss: 0.00002619
Iteration 182/1000 | Loss: 0.00002619
Iteration 183/1000 | Loss: 0.00002619
Iteration 184/1000 | Loss: 0.00002619
Iteration 185/1000 | Loss: 0.00002618
Iteration 186/1000 | Loss: 0.00002617
Iteration 187/1000 | Loss: 0.00002616
Iteration 188/1000 | Loss: 0.00002616
Iteration 189/1000 | Loss: 0.00002616
Iteration 190/1000 | Loss: 0.00002615
Iteration 191/1000 | Loss: 0.00002614
Iteration 192/1000 | Loss: 0.00002613
Iteration 193/1000 | Loss: 0.00002613
Iteration 194/1000 | Loss: 0.00002613
Iteration 195/1000 | Loss: 0.00002612
Iteration 196/1000 | Loss: 0.00002612
Iteration 197/1000 | Loss: 0.00002612
Iteration 198/1000 | Loss: 0.00002611
Iteration 199/1000 | Loss: 0.00002611
Iteration 200/1000 | Loss: 0.00002610
Iteration 201/1000 | Loss: 0.00002609
Iteration 202/1000 | Loss: 0.00002609
Iteration 203/1000 | Loss: 0.00002608
Iteration 204/1000 | Loss: 0.00002608
Iteration 205/1000 | Loss: 0.00002608
Iteration 206/1000 | Loss: 0.00002607
Iteration 207/1000 | Loss: 0.00002607
Iteration 208/1000 | Loss: 0.00002607
Iteration 209/1000 | Loss: 0.00002607
Iteration 210/1000 | Loss: 0.00002607
Iteration 211/1000 | Loss: 0.00002606
Iteration 212/1000 | Loss: 0.00002606
Iteration 213/1000 | Loss: 0.00002606
Iteration 214/1000 | Loss: 0.00002605
Iteration 215/1000 | Loss: 0.00002605
Iteration 216/1000 | Loss: 0.00002604
Iteration 217/1000 | Loss: 0.00002604
Iteration 218/1000 | Loss: 0.00002604
Iteration 219/1000 | Loss: 0.00002604
Iteration 220/1000 | Loss: 0.00002604
Iteration 221/1000 | Loss: 0.00002604
Iteration 222/1000 | Loss: 0.00002604
Iteration 223/1000 | Loss: 0.00002604
Iteration 224/1000 | Loss: 0.00002603
Iteration 225/1000 | Loss: 0.00002603
Iteration 226/1000 | Loss: 0.00002603
Iteration 227/1000 | Loss: 0.00002603
Iteration 228/1000 | Loss: 0.00002603
Iteration 229/1000 | Loss: 0.00002603
Iteration 230/1000 | Loss: 0.00002603
Iteration 231/1000 | Loss: 0.00002603
Iteration 232/1000 | Loss: 0.00002603
Iteration 233/1000 | Loss: 0.00002602
Iteration 234/1000 | Loss: 0.00002602
Iteration 235/1000 | Loss: 0.00002602
Iteration 236/1000 | Loss: 0.00002601
Iteration 237/1000 | Loss: 0.00002601
Iteration 238/1000 | Loss: 0.00002601
Iteration 239/1000 | Loss: 0.00002601
Iteration 240/1000 | Loss: 0.00002601
Iteration 241/1000 | Loss: 0.00002601
Iteration 242/1000 | Loss: 0.00002601
Iteration 243/1000 | Loss: 0.00002601
Iteration 244/1000 | Loss: 0.00002601
Iteration 245/1000 | Loss: 0.00002601
Iteration 246/1000 | Loss: 0.00002600
Iteration 247/1000 | Loss: 0.00002600
Iteration 248/1000 | Loss: 0.00002600
Iteration 249/1000 | Loss: 0.00002600
Iteration 250/1000 | Loss: 0.00002600
Iteration 251/1000 | Loss: 0.00002600
Iteration 252/1000 | Loss: 0.00002600
Iteration 253/1000 | Loss: 0.00002600
Iteration 254/1000 | Loss: 0.00002600
Iteration 255/1000 | Loss: 0.00002600
Iteration 256/1000 | Loss: 0.00002600
Iteration 257/1000 | Loss: 0.00002600
Iteration 258/1000 | Loss: 0.00002599
Iteration 259/1000 | Loss: 0.00002599
Iteration 260/1000 | Loss: 0.00002599
Iteration 261/1000 | Loss: 0.00002599
Iteration 262/1000 | Loss: 0.00002599
Iteration 263/1000 | Loss: 0.00002599
Iteration 264/1000 | Loss: 0.00002599
Iteration 265/1000 | Loss: 0.00002599
Iteration 266/1000 | Loss: 0.00002599
Iteration 267/1000 | Loss: 0.00002599
Iteration 268/1000 | Loss: 0.00002599
Iteration 269/1000 | Loss: 0.00002599
Iteration 270/1000 | Loss: 0.00002599
Iteration 271/1000 | Loss: 0.00002599
Iteration 272/1000 | Loss: 0.00002599
Iteration 273/1000 | Loss: 0.00002599
Iteration 274/1000 | Loss: 0.00002599
Iteration 275/1000 | Loss: 0.00002598
Iteration 276/1000 | Loss: 0.00002598
Iteration 277/1000 | Loss: 0.00002598
Iteration 278/1000 | Loss: 0.00002598
Iteration 279/1000 | Loss: 0.00002598
Iteration 280/1000 | Loss: 0.00002598
Iteration 281/1000 | Loss: 0.00002598
Iteration 282/1000 | Loss: 0.00002598
Iteration 283/1000 | Loss: 0.00002598
Iteration 284/1000 | Loss: 0.00002598
Iteration 285/1000 | Loss: 0.00002598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.598398168629501e-05, 2.598398168629501e-05, 2.598398168629501e-05, 2.598398168629501e-05, 2.598398168629501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.598398168629501e-05

Optimization complete. Final v2v error: 4.117321014404297 mm

Highest mean error: 5.964993000030518 mm for frame 180

Lowest mean error: 3.3242688179016113 mm for frame 55

Saving results

Total time: 276.34691858291626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463213
Iteration 2/25 | Loss: 0.00137299
Iteration 3/25 | Loss: 0.00117066
Iteration 4/25 | Loss: 0.00115185
Iteration 5/25 | Loss: 0.00114920
Iteration 6/25 | Loss: 0.00114843
Iteration 7/25 | Loss: 0.00114843
Iteration 8/25 | Loss: 0.00114843
Iteration 9/25 | Loss: 0.00114843
Iteration 10/25 | Loss: 0.00114843
Iteration 11/25 | Loss: 0.00114843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011484271381050348, 0.0011484271381050348, 0.0011484271381050348, 0.0011484271381050348, 0.0011484271381050348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011484271381050348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37077856
Iteration 2/25 | Loss: 0.00077392
Iteration 3/25 | Loss: 0.00077392
Iteration 4/25 | Loss: 0.00077392
Iteration 5/25 | Loss: 0.00077392
Iteration 6/25 | Loss: 0.00077392
Iteration 7/25 | Loss: 0.00077392
Iteration 8/25 | Loss: 0.00077392
Iteration 9/25 | Loss: 0.00077392
Iteration 10/25 | Loss: 0.00077392
Iteration 11/25 | Loss: 0.00077392
Iteration 12/25 | Loss: 0.00077392
Iteration 13/25 | Loss: 0.00077392
Iteration 14/25 | Loss: 0.00077392
Iteration 15/25 | Loss: 0.00077392
Iteration 16/25 | Loss: 0.00077392
Iteration 17/25 | Loss: 0.00077392
Iteration 18/25 | Loss: 0.00077392
Iteration 19/25 | Loss: 0.00077392
Iteration 20/25 | Loss: 0.00077392
Iteration 21/25 | Loss: 0.00077392
Iteration 22/25 | Loss: 0.00077392
Iteration 23/25 | Loss: 0.00077392
Iteration 24/25 | Loss: 0.00077392
Iteration 25/25 | Loss: 0.00077392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077392
Iteration 2/1000 | Loss: 0.00002442
Iteration 3/1000 | Loss: 0.00001546
Iteration 4/1000 | Loss: 0.00001382
Iteration 5/1000 | Loss: 0.00001317
Iteration 6/1000 | Loss: 0.00001254
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001189
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001168
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001131
Iteration 20/1000 | Loss: 0.00001130
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001126
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001113
Iteration 33/1000 | Loss: 0.00001112
Iteration 34/1000 | Loss: 0.00001111
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001108
Iteration 44/1000 | Loss: 0.00001108
Iteration 45/1000 | Loss: 0.00001108
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001105
Iteration 55/1000 | Loss: 0.00001104
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001102
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001100
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001098
Iteration 92/1000 | Loss: 0.00001098
Iteration 93/1000 | Loss: 0.00001098
Iteration 94/1000 | Loss: 0.00001098
Iteration 95/1000 | Loss: 0.00001098
Iteration 96/1000 | Loss: 0.00001098
Iteration 97/1000 | Loss: 0.00001097
Iteration 98/1000 | Loss: 0.00001097
Iteration 99/1000 | Loss: 0.00001097
Iteration 100/1000 | Loss: 0.00001097
Iteration 101/1000 | Loss: 0.00001097
Iteration 102/1000 | Loss: 0.00001097
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001097
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001096
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001095
Iteration 116/1000 | Loss: 0.00001095
Iteration 117/1000 | Loss: 0.00001095
Iteration 118/1000 | Loss: 0.00001095
Iteration 119/1000 | Loss: 0.00001095
Iteration 120/1000 | Loss: 0.00001095
Iteration 121/1000 | Loss: 0.00001094
Iteration 122/1000 | Loss: 0.00001094
Iteration 123/1000 | Loss: 0.00001094
Iteration 124/1000 | Loss: 0.00001094
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001093
Iteration 131/1000 | Loss: 0.00001093
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001092
Iteration 134/1000 | Loss: 0.00001092
Iteration 135/1000 | Loss: 0.00001092
Iteration 136/1000 | Loss: 0.00001092
Iteration 137/1000 | Loss: 0.00001092
Iteration 138/1000 | Loss: 0.00001091
Iteration 139/1000 | Loss: 0.00001091
Iteration 140/1000 | Loss: 0.00001091
Iteration 141/1000 | Loss: 0.00001091
Iteration 142/1000 | Loss: 0.00001091
Iteration 143/1000 | Loss: 0.00001091
Iteration 144/1000 | Loss: 0.00001090
Iteration 145/1000 | Loss: 0.00001090
Iteration 146/1000 | Loss: 0.00001090
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001089
Iteration 150/1000 | Loss: 0.00001088
Iteration 151/1000 | Loss: 0.00001088
Iteration 152/1000 | Loss: 0.00001088
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001088
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001088
Iteration 159/1000 | Loss: 0.00001087
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001087
Iteration 163/1000 | Loss: 0.00001087
Iteration 164/1000 | Loss: 0.00001086
Iteration 165/1000 | Loss: 0.00001086
Iteration 166/1000 | Loss: 0.00001086
Iteration 167/1000 | Loss: 0.00001086
Iteration 168/1000 | Loss: 0.00001086
Iteration 169/1000 | Loss: 0.00001086
Iteration 170/1000 | Loss: 0.00001086
Iteration 171/1000 | Loss: 0.00001086
Iteration 172/1000 | Loss: 0.00001086
Iteration 173/1000 | Loss: 0.00001086
Iteration 174/1000 | Loss: 0.00001086
Iteration 175/1000 | Loss: 0.00001086
Iteration 176/1000 | Loss: 0.00001086
Iteration 177/1000 | Loss: 0.00001086
Iteration 178/1000 | Loss: 0.00001086
Iteration 179/1000 | Loss: 0.00001086
Iteration 180/1000 | Loss: 0.00001086
Iteration 181/1000 | Loss: 0.00001086
Iteration 182/1000 | Loss: 0.00001086
Iteration 183/1000 | Loss: 0.00001086
Iteration 184/1000 | Loss: 0.00001086
Iteration 185/1000 | Loss: 0.00001086
Iteration 186/1000 | Loss: 0.00001086
Iteration 187/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0859846042876597e-05, 1.0859846042876597e-05, 1.0859846042876597e-05, 1.0859846042876597e-05, 1.0859846042876597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0859846042876597e-05

Optimization complete. Final v2v error: 2.7791082859039307 mm

Highest mean error: 3.5574417114257812 mm for frame 79

Lowest mean error: 2.4667458534240723 mm for frame 162

Saving results

Total time: 39.720925092697144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474716
Iteration 2/25 | Loss: 0.00123135
Iteration 3/25 | Loss: 0.00116238
Iteration 4/25 | Loss: 0.00114654
Iteration 5/25 | Loss: 0.00114137
Iteration 6/25 | Loss: 0.00114043
Iteration 7/25 | Loss: 0.00114043
Iteration 8/25 | Loss: 0.00114043
Iteration 9/25 | Loss: 0.00114043
Iteration 10/25 | Loss: 0.00114043
Iteration 11/25 | Loss: 0.00114043
Iteration 12/25 | Loss: 0.00114043
Iteration 13/25 | Loss: 0.00114043
Iteration 14/25 | Loss: 0.00114043
Iteration 15/25 | Loss: 0.00114043
Iteration 16/25 | Loss: 0.00114043
Iteration 17/25 | Loss: 0.00114043
Iteration 18/25 | Loss: 0.00114043
Iteration 19/25 | Loss: 0.00114043
Iteration 20/25 | Loss: 0.00114043
Iteration 21/25 | Loss: 0.00114043
Iteration 22/25 | Loss: 0.00114043
Iteration 23/25 | Loss: 0.00114043
Iteration 24/25 | Loss: 0.00114043
Iteration 25/25 | Loss: 0.00114043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.52610016
Iteration 2/25 | Loss: 0.00076202
Iteration 3/25 | Loss: 0.00076202
Iteration 4/25 | Loss: 0.00076202
Iteration 5/25 | Loss: 0.00076202
Iteration 6/25 | Loss: 0.00076202
Iteration 7/25 | Loss: 0.00076202
Iteration 8/25 | Loss: 0.00076202
Iteration 9/25 | Loss: 0.00076202
Iteration 10/25 | Loss: 0.00076202
Iteration 11/25 | Loss: 0.00076202
Iteration 12/25 | Loss: 0.00076202
Iteration 13/25 | Loss: 0.00076202
Iteration 14/25 | Loss: 0.00076202
Iteration 15/25 | Loss: 0.00076202
Iteration 16/25 | Loss: 0.00076202
Iteration 17/25 | Loss: 0.00076202
Iteration 18/25 | Loss: 0.00076202
Iteration 19/25 | Loss: 0.00076202
Iteration 20/25 | Loss: 0.00076202
Iteration 21/25 | Loss: 0.00076202
Iteration 22/25 | Loss: 0.00076202
Iteration 23/25 | Loss: 0.00076202
Iteration 24/25 | Loss: 0.00076202
Iteration 25/25 | Loss: 0.00076202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076202
Iteration 2/1000 | Loss: 0.00003308
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00002029
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001795
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001642
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001603
Iteration 18/1000 | Loss: 0.00001598
Iteration 19/1000 | Loss: 0.00001598
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001590
Iteration 22/1000 | Loss: 0.00001589
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001588
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001586
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001585
Iteration 36/1000 | Loss: 0.00001585
Iteration 37/1000 | Loss: 0.00001585
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001584
Iteration 40/1000 | Loss: 0.00001584
Iteration 41/1000 | Loss: 0.00001584
Iteration 42/1000 | Loss: 0.00001583
Iteration 43/1000 | Loss: 0.00001583
Iteration 44/1000 | Loss: 0.00001582
Iteration 45/1000 | Loss: 0.00001582
Iteration 46/1000 | Loss: 0.00001582
Iteration 47/1000 | Loss: 0.00001581
Iteration 48/1000 | Loss: 0.00001581
Iteration 49/1000 | Loss: 0.00001581
Iteration 50/1000 | Loss: 0.00001581
Iteration 51/1000 | Loss: 0.00001581
Iteration 52/1000 | Loss: 0.00001581
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001581
Iteration 55/1000 | Loss: 0.00001581
Iteration 56/1000 | Loss: 0.00001580
Iteration 57/1000 | Loss: 0.00001580
Iteration 58/1000 | Loss: 0.00001580
Iteration 59/1000 | Loss: 0.00001579
Iteration 60/1000 | Loss: 0.00001579
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001579
Iteration 63/1000 | Loss: 0.00001579
Iteration 64/1000 | Loss: 0.00001579
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001578
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001570
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001569
Iteration 90/1000 | Loss: 0.00001569
Iteration 91/1000 | Loss: 0.00001568
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001566
Iteration 98/1000 | Loss: 0.00001566
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001566
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001565
Iteration 103/1000 | Loss: 0.00001565
Iteration 104/1000 | Loss: 0.00001565
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001564
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001564
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001563
Iteration 113/1000 | Loss: 0.00001563
Iteration 114/1000 | Loss: 0.00001563
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001562
Iteration 117/1000 | Loss: 0.00001562
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001561
Iteration 121/1000 | Loss: 0.00001561
Iteration 122/1000 | Loss: 0.00001561
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001560
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001559
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001558
Iteration 146/1000 | Loss: 0.00001558
Iteration 147/1000 | Loss: 0.00001558
Iteration 148/1000 | Loss: 0.00001558
Iteration 149/1000 | Loss: 0.00001558
Iteration 150/1000 | Loss: 0.00001558
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001558
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001557
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001557
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001556
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001556
Iteration 169/1000 | Loss: 0.00001555
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001555
Iteration 172/1000 | Loss: 0.00001555
Iteration 173/1000 | Loss: 0.00001555
Iteration 174/1000 | Loss: 0.00001555
Iteration 175/1000 | Loss: 0.00001555
Iteration 176/1000 | Loss: 0.00001555
Iteration 177/1000 | Loss: 0.00001555
Iteration 178/1000 | Loss: 0.00001555
Iteration 179/1000 | Loss: 0.00001555
Iteration 180/1000 | Loss: 0.00001555
Iteration 181/1000 | Loss: 0.00001555
Iteration 182/1000 | Loss: 0.00001555
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001554
Iteration 185/1000 | Loss: 0.00001554
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001554
Iteration 188/1000 | Loss: 0.00001554
Iteration 189/1000 | Loss: 0.00001554
Iteration 190/1000 | Loss: 0.00001554
Iteration 191/1000 | Loss: 0.00001554
Iteration 192/1000 | Loss: 0.00001554
Iteration 193/1000 | Loss: 0.00001554
Iteration 194/1000 | Loss: 0.00001554
Iteration 195/1000 | Loss: 0.00001554
Iteration 196/1000 | Loss: 0.00001554
Iteration 197/1000 | Loss: 0.00001554
Iteration 198/1000 | Loss: 0.00001554
Iteration 199/1000 | Loss: 0.00001554
Iteration 200/1000 | Loss: 0.00001553
Iteration 201/1000 | Loss: 0.00001553
Iteration 202/1000 | Loss: 0.00001553
Iteration 203/1000 | Loss: 0.00001553
Iteration 204/1000 | Loss: 0.00001553
Iteration 205/1000 | Loss: 0.00001553
Iteration 206/1000 | Loss: 0.00001553
Iteration 207/1000 | Loss: 0.00001553
Iteration 208/1000 | Loss: 0.00001553
Iteration 209/1000 | Loss: 0.00001553
Iteration 210/1000 | Loss: 0.00001553
Iteration 211/1000 | Loss: 0.00001553
Iteration 212/1000 | Loss: 0.00001553
Iteration 213/1000 | Loss: 0.00001553
Iteration 214/1000 | Loss: 0.00001553
Iteration 215/1000 | Loss: 0.00001553
Iteration 216/1000 | Loss: 0.00001553
Iteration 217/1000 | Loss: 0.00001553
Iteration 218/1000 | Loss: 0.00001553
Iteration 219/1000 | Loss: 0.00001552
Iteration 220/1000 | Loss: 0.00001552
Iteration 221/1000 | Loss: 0.00001552
Iteration 222/1000 | Loss: 0.00001552
Iteration 223/1000 | Loss: 0.00001552
Iteration 224/1000 | Loss: 0.00001552
Iteration 225/1000 | Loss: 0.00001552
Iteration 226/1000 | Loss: 0.00001552
Iteration 227/1000 | Loss: 0.00001552
Iteration 228/1000 | Loss: 0.00001552
Iteration 229/1000 | Loss: 0.00001552
Iteration 230/1000 | Loss: 0.00001552
Iteration 231/1000 | Loss: 0.00001552
Iteration 232/1000 | Loss: 0.00001552
Iteration 233/1000 | Loss: 0.00001552
Iteration 234/1000 | Loss: 0.00001552
Iteration 235/1000 | Loss: 0.00001552
Iteration 236/1000 | Loss: 0.00001552
Iteration 237/1000 | Loss: 0.00001551
Iteration 238/1000 | Loss: 0.00001551
Iteration 239/1000 | Loss: 0.00001551
Iteration 240/1000 | Loss: 0.00001551
Iteration 241/1000 | Loss: 0.00001551
Iteration 242/1000 | Loss: 0.00001551
Iteration 243/1000 | Loss: 0.00001551
Iteration 244/1000 | Loss: 0.00001551
Iteration 245/1000 | Loss: 0.00001551
Iteration 246/1000 | Loss: 0.00001551
Iteration 247/1000 | Loss: 0.00001551
Iteration 248/1000 | Loss: 0.00001551
Iteration 249/1000 | Loss: 0.00001551
Iteration 250/1000 | Loss: 0.00001551
Iteration 251/1000 | Loss: 0.00001551
Iteration 252/1000 | Loss: 0.00001551
Iteration 253/1000 | Loss: 0.00001551
Iteration 254/1000 | Loss: 0.00001551
Iteration 255/1000 | Loss: 0.00001551
Iteration 256/1000 | Loss: 0.00001551
Iteration 257/1000 | Loss: 0.00001551
Iteration 258/1000 | Loss: 0.00001551
Iteration 259/1000 | Loss: 0.00001551
Iteration 260/1000 | Loss: 0.00001551
Iteration 261/1000 | Loss: 0.00001551
Iteration 262/1000 | Loss: 0.00001551
Iteration 263/1000 | Loss: 0.00001551
Iteration 264/1000 | Loss: 0.00001550
Iteration 265/1000 | Loss: 0.00001550
Iteration 266/1000 | Loss: 0.00001550
Iteration 267/1000 | Loss: 0.00001550
Iteration 268/1000 | Loss: 0.00001550
Iteration 269/1000 | Loss: 0.00001550
Iteration 270/1000 | Loss: 0.00001550
Iteration 271/1000 | Loss: 0.00001550
Iteration 272/1000 | Loss: 0.00001550
Iteration 273/1000 | Loss: 0.00001550
Iteration 274/1000 | Loss: 0.00001550
Iteration 275/1000 | Loss: 0.00001550
Iteration 276/1000 | Loss: 0.00001550
Iteration 277/1000 | Loss: 0.00001550
Iteration 278/1000 | Loss: 0.00001550
Iteration 279/1000 | Loss: 0.00001550
Iteration 280/1000 | Loss: 0.00001550
Iteration 281/1000 | Loss: 0.00001550
Iteration 282/1000 | Loss: 0.00001550
Iteration 283/1000 | Loss: 0.00001550
Iteration 284/1000 | Loss: 0.00001550
Iteration 285/1000 | Loss: 0.00001550
Iteration 286/1000 | Loss: 0.00001550
Iteration 287/1000 | Loss: 0.00001550
Iteration 288/1000 | Loss: 0.00001550
Iteration 289/1000 | Loss: 0.00001550
Iteration 290/1000 | Loss: 0.00001550
Iteration 291/1000 | Loss: 0.00001550
Iteration 292/1000 | Loss: 0.00001550
Iteration 293/1000 | Loss: 0.00001550
Iteration 294/1000 | Loss: 0.00001550
Iteration 295/1000 | Loss: 0.00001550
Iteration 296/1000 | Loss: 0.00001550
Iteration 297/1000 | Loss: 0.00001550
Iteration 298/1000 | Loss: 0.00001550
Iteration 299/1000 | Loss: 0.00001550
Iteration 300/1000 | Loss: 0.00001550
Iteration 301/1000 | Loss: 0.00001550
Iteration 302/1000 | Loss: 0.00001550
Iteration 303/1000 | Loss: 0.00001550
Iteration 304/1000 | Loss: 0.00001550
Iteration 305/1000 | Loss: 0.00001550
Iteration 306/1000 | Loss: 0.00001550
Iteration 307/1000 | Loss: 0.00001550
Iteration 308/1000 | Loss: 0.00001550
Iteration 309/1000 | Loss: 0.00001550
Iteration 310/1000 | Loss: 0.00001550
Iteration 311/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.550357228552457e-05, 1.550357228552457e-05, 1.550357228552457e-05, 1.550357228552457e-05, 1.550357228552457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.550357228552457e-05

Optimization complete. Final v2v error: 3.3108742237091064 mm

Highest mean error: 3.5818629264831543 mm for frame 78

Lowest mean error: 2.9270801544189453 mm for frame 133

Saving results

Total time: 49.66521668434143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434174
Iteration 2/25 | Loss: 0.00130324
Iteration 3/25 | Loss: 0.00118541
Iteration 4/25 | Loss: 0.00118135
Iteration 5/25 | Loss: 0.00118024
Iteration 6/25 | Loss: 0.00118024
Iteration 7/25 | Loss: 0.00118024
Iteration 8/25 | Loss: 0.00118024
Iteration 9/25 | Loss: 0.00118024
Iteration 10/25 | Loss: 0.00118024
Iteration 11/25 | Loss: 0.00118024
Iteration 12/25 | Loss: 0.00118024
Iteration 13/25 | Loss: 0.00118024
Iteration 14/25 | Loss: 0.00118024
Iteration 15/25 | Loss: 0.00118024
Iteration 16/25 | Loss: 0.00118024
Iteration 17/25 | Loss: 0.00118024
Iteration 18/25 | Loss: 0.00118024
Iteration 19/25 | Loss: 0.00118024
Iteration 20/25 | Loss: 0.00118024
Iteration 21/25 | Loss: 0.00118024
Iteration 22/25 | Loss: 0.00118023
Iteration 23/25 | Loss: 0.00118023
Iteration 24/25 | Loss: 0.00118023
Iteration 25/25 | Loss: 0.00118023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80711484
Iteration 2/25 | Loss: 0.00061231
Iteration 3/25 | Loss: 0.00061230
Iteration 4/25 | Loss: 0.00061230
Iteration 5/25 | Loss: 0.00061230
Iteration 6/25 | Loss: 0.00061230
Iteration 7/25 | Loss: 0.00061230
Iteration 8/25 | Loss: 0.00061230
Iteration 9/25 | Loss: 0.00061230
Iteration 10/25 | Loss: 0.00061230
Iteration 11/25 | Loss: 0.00061230
Iteration 12/25 | Loss: 0.00061230
Iteration 13/25 | Loss: 0.00061230
Iteration 14/25 | Loss: 0.00061230
Iteration 15/25 | Loss: 0.00061230
Iteration 16/25 | Loss: 0.00061230
Iteration 17/25 | Loss: 0.00061230
Iteration 18/25 | Loss: 0.00061230
Iteration 19/25 | Loss: 0.00061230
Iteration 20/25 | Loss: 0.00061230
Iteration 21/25 | Loss: 0.00061230
Iteration 22/25 | Loss: 0.00061230
Iteration 23/25 | Loss: 0.00061230
Iteration 24/25 | Loss: 0.00061230
Iteration 25/25 | Loss: 0.00061230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061230
Iteration 2/1000 | Loss: 0.00002866
Iteration 3/1000 | Loss: 0.00001814
Iteration 4/1000 | Loss: 0.00001643
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001446
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001357
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001329
Iteration 32/1000 | Loss: 0.00001329
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001328
Iteration 35/1000 | Loss: 0.00001328
Iteration 36/1000 | Loss: 0.00001328
Iteration 37/1000 | Loss: 0.00001328
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001327
Iteration 40/1000 | Loss: 0.00001327
Iteration 41/1000 | Loss: 0.00001326
Iteration 42/1000 | Loss: 0.00001326
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001326
Iteration 45/1000 | Loss: 0.00001325
Iteration 46/1000 | Loss: 0.00001325
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001324
Iteration 50/1000 | Loss: 0.00001324
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001321
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001320
Iteration 63/1000 | Loss: 0.00001320
Iteration 64/1000 | Loss: 0.00001320
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001319
Iteration 67/1000 | Loss: 0.00001319
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001315
Iteration 82/1000 | Loss: 0.00001315
Iteration 83/1000 | Loss: 0.00001315
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001308
Iteration 102/1000 | Loss: 0.00001308
Iteration 103/1000 | Loss: 0.00001308
Iteration 104/1000 | Loss: 0.00001307
Iteration 105/1000 | Loss: 0.00001307
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001306
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001306
Iteration 111/1000 | Loss: 0.00001306
Iteration 112/1000 | Loss: 0.00001306
Iteration 113/1000 | Loss: 0.00001306
Iteration 114/1000 | Loss: 0.00001306
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001301
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001301
Iteration 144/1000 | Loss: 0.00001301
Iteration 145/1000 | Loss: 0.00001301
Iteration 146/1000 | Loss: 0.00001301
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001299
Iteration 171/1000 | Loss: 0.00001299
Iteration 172/1000 | Loss: 0.00001299
Iteration 173/1000 | Loss: 0.00001299
Iteration 174/1000 | Loss: 0.00001299
Iteration 175/1000 | Loss: 0.00001299
Iteration 176/1000 | Loss: 0.00001299
Iteration 177/1000 | Loss: 0.00001299
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001299
Iteration 180/1000 | Loss: 0.00001299
Iteration 181/1000 | Loss: 0.00001299
Iteration 182/1000 | Loss: 0.00001299
Iteration 183/1000 | Loss: 0.00001299
Iteration 184/1000 | Loss: 0.00001299
Iteration 185/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2985911780560855e-05, 1.2985911780560855e-05, 1.2985911780560855e-05, 1.2985911780560855e-05, 1.2985911780560855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2985911780560855e-05

Optimization complete. Final v2v error: 3.0555388927459717 mm

Highest mean error: 3.5687239170074463 mm for frame 87

Lowest mean error: 2.8225300312042236 mm for frame 52

Saving results

Total time: 37.93763518333435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743410
Iteration 2/25 | Loss: 0.00161130
Iteration 3/25 | Loss: 0.00140607
Iteration 4/25 | Loss: 0.00138915
Iteration 5/25 | Loss: 0.00138468
Iteration 6/25 | Loss: 0.00138425
Iteration 7/25 | Loss: 0.00138425
Iteration 8/25 | Loss: 0.00138425
Iteration 9/25 | Loss: 0.00138425
Iteration 10/25 | Loss: 0.00138425
Iteration 11/25 | Loss: 0.00138425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013842503540217876, 0.0013842503540217876, 0.0013842503540217876, 0.0013842503540217876, 0.0013842503540217876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013842503540217876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.24914724
Iteration 2/25 | Loss: 0.00100963
Iteration 3/25 | Loss: 0.00100963
Iteration 4/25 | Loss: 0.00100963
Iteration 5/25 | Loss: 0.00100963
Iteration 6/25 | Loss: 0.00100963
Iteration 7/25 | Loss: 0.00100963
Iteration 8/25 | Loss: 0.00100963
Iteration 9/25 | Loss: 0.00100963
Iteration 10/25 | Loss: 0.00100963
Iteration 11/25 | Loss: 0.00100963
Iteration 12/25 | Loss: 0.00100963
Iteration 13/25 | Loss: 0.00100963
Iteration 14/25 | Loss: 0.00100963
Iteration 15/25 | Loss: 0.00100963
Iteration 16/25 | Loss: 0.00100963
Iteration 17/25 | Loss: 0.00100963
Iteration 18/25 | Loss: 0.00100963
Iteration 19/25 | Loss: 0.00100963
Iteration 20/25 | Loss: 0.00100963
Iteration 21/25 | Loss: 0.00100963
Iteration 22/25 | Loss: 0.00100963
Iteration 23/25 | Loss: 0.00100963
Iteration 24/25 | Loss: 0.00100963
Iteration 25/25 | Loss: 0.00100963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100963
Iteration 2/1000 | Loss: 0.00006534
Iteration 3/1000 | Loss: 0.00004454
Iteration 4/1000 | Loss: 0.00003978
Iteration 5/1000 | Loss: 0.00003816
Iteration 6/1000 | Loss: 0.00003717
Iteration 7/1000 | Loss: 0.00003635
Iteration 8/1000 | Loss: 0.00003571
Iteration 9/1000 | Loss: 0.00003521
Iteration 10/1000 | Loss: 0.00003488
Iteration 11/1000 | Loss: 0.00003463
Iteration 12/1000 | Loss: 0.00003444
Iteration 13/1000 | Loss: 0.00003424
Iteration 14/1000 | Loss: 0.00003415
Iteration 15/1000 | Loss: 0.00003409
Iteration 16/1000 | Loss: 0.00003397
Iteration 17/1000 | Loss: 0.00003388
Iteration 18/1000 | Loss: 0.00003381
Iteration 19/1000 | Loss: 0.00003369
Iteration 20/1000 | Loss: 0.00003356
Iteration 21/1000 | Loss: 0.00003356
Iteration 22/1000 | Loss: 0.00003356
Iteration 23/1000 | Loss: 0.00003355
Iteration 24/1000 | Loss: 0.00003354
Iteration 25/1000 | Loss: 0.00003354
Iteration 26/1000 | Loss: 0.00003353
Iteration 27/1000 | Loss: 0.00003353
Iteration 28/1000 | Loss: 0.00003352
Iteration 29/1000 | Loss: 0.00003350
Iteration 30/1000 | Loss: 0.00003346
Iteration 31/1000 | Loss: 0.00003345
Iteration 32/1000 | Loss: 0.00003345
Iteration 33/1000 | Loss: 0.00003344
Iteration 34/1000 | Loss: 0.00003342
Iteration 35/1000 | Loss: 0.00003341
Iteration 36/1000 | Loss: 0.00003341
Iteration 37/1000 | Loss: 0.00003341
Iteration 38/1000 | Loss: 0.00003341
Iteration 39/1000 | Loss: 0.00003340
Iteration 40/1000 | Loss: 0.00003339
Iteration 41/1000 | Loss: 0.00003339
Iteration 42/1000 | Loss: 0.00003339
Iteration 43/1000 | Loss: 0.00003339
Iteration 44/1000 | Loss: 0.00003338
Iteration 45/1000 | Loss: 0.00003338
Iteration 46/1000 | Loss: 0.00003338
Iteration 47/1000 | Loss: 0.00003337
Iteration 48/1000 | Loss: 0.00003337
Iteration 49/1000 | Loss: 0.00003336
Iteration 50/1000 | Loss: 0.00003336
Iteration 51/1000 | Loss: 0.00003336
Iteration 52/1000 | Loss: 0.00003335
Iteration 53/1000 | Loss: 0.00003335
Iteration 54/1000 | Loss: 0.00003335
Iteration 55/1000 | Loss: 0.00003335
Iteration 56/1000 | Loss: 0.00003334
Iteration 57/1000 | Loss: 0.00003334
Iteration 58/1000 | Loss: 0.00003334
Iteration 59/1000 | Loss: 0.00003334
Iteration 60/1000 | Loss: 0.00003333
Iteration 61/1000 | Loss: 0.00003333
Iteration 62/1000 | Loss: 0.00003333
Iteration 63/1000 | Loss: 0.00003333
Iteration 64/1000 | Loss: 0.00003333
Iteration 65/1000 | Loss: 0.00003333
Iteration 66/1000 | Loss: 0.00003333
Iteration 67/1000 | Loss: 0.00003332
Iteration 68/1000 | Loss: 0.00003332
Iteration 69/1000 | Loss: 0.00003332
Iteration 70/1000 | Loss: 0.00003331
Iteration 71/1000 | Loss: 0.00003331
Iteration 72/1000 | Loss: 0.00003330
Iteration 73/1000 | Loss: 0.00003330
Iteration 74/1000 | Loss: 0.00003330
Iteration 75/1000 | Loss: 0.00003329
Iteration 76/1000 | Loss: 0.00003329
Iteration 77/1000 | Loss: 0.00003329
Iteration 78/1000 | Loss: 0.00003329
Iteration 79/1000 | Loss: 0.00003329
Iteration 80/1000 | Loss: 0.00003329
Iteration 81/1000 | Loss: 0.00003329
Iteration 82/1000 | Loss: 0.00003329
Iteration 83/1000 | Loss: 0.00003329
Iteration 84/1000 | Loss: 0.00003328
Iteration 85/1000 | Loss: 0.00003328
Iteration 86/1000 | Loss: 0.00003328
Iteration 87/1000 | Loss: 0.00003327
Iteration 88/1000 | Loss: 0.00003327
Iteration 89/1000 | Loss: 0.00003327
Iteration 90/1000 | Loss: 0.00003327
Iteration 91/1000 | Loss: 0.00003327
Iteration 92/1000 | Loss: 0.00003327
Iteration 93/1000 | Loss: 0.00003327
Iteration 94/1000 | Loss: 0.00003327
Iteration 95/1000 | Loss: 0.00003326
Iteration 96/1000 | Loss: 0.00003326
Iteration 97/1000 | Loss: 0.00003326
Iteration 98/1000 | Loss: 0.00003326
Iteration 99/1000 | Loss: 0.00003326
Iteration 100/1000 | Loss: 0.00003326
Iteration 101/1000 | Loss: 0.00003326
Iteration 102/1000 | Loss: 0.00003326
Iteration 103/1000 | Loss: 0.00003326
Iteration 104/1000 | Loss: 0.00003325
Iteration 105/1000 | Loss: 0.00003325
Iteration 106/1000 | Loss: 0.00003325
Iteration 107/1000 | Loss: 0.00003325
Iteration 108/1000 | Loss: 0.00003325
Iteration 109/1000 | Loss: 0.00003325
Iteration 110/1000 | Loss: 0.00003325
Iteration 111/1000 | Loss: 0.00003325
Iteration 112/1000 | Loss: 0.00003324
Iteration 113/1000 | Loss: 0.00003324
Iteration 114/1000 | Loss: 0.00003324
Iteration 115/1000 | Loss: 0.00003324
Iteration 116/1000 | Loss: 0.00003324
Iteration 117/1000 | Loss: 0.00003324
Iteration 118/1000 | Loss: 0.00003324
Iteration 119/1000 | Loss: 0.00003324
Iteration 120/1000 | Loss: 0.00003324
Iteration 121/1000 | Loss: 0.00003323
Iteration 122/1000 | Loss: 0.00003323
Iteration 123/1000 | Loss: 0.00003323
Iteration 124/1000 | Loss: 0.00003323
Iteration 125/1000 | Loss: 0.00003323
Iteration 126/1000 | Loss: 0.00003323
Iteration 127/1000 | Loss: 0.00003323
Iteration 128/1000 | Loss: 0.00003323
Iteration 129/1000 | Loss: 0.00003323
Iteration 130/1000 | Loss: 0.00003323
Iteration 131/1000 | Loss: 0.00003322
Iteration 132/1000 | Loss: 0.00003322
Iteration 133/1000 | Loss: 0.00003322
Iteration 134/1000 | Loss: 0.00003322
Iteration 135/1000 | Loss: 0.00003322
Iteration 136/1000 | Loss: 0.00003321
Iteration 137/1000 | Loss: 0.00003321
Iteration 138/1000 | Loss: 0.00003321
Iteration 139/1000 | Loss: 0.00003321
Iteration 140/1000 | Loss: 0.00003321
Iteration 141/1000 | Loss: 0.00003321
Iteration 142/1000 | Loss: 0.00003321
Iteration 143/1000 | Loss: 0.00003320
Iteration 144/1000 | Loss: 0.00003320
Iteration 145/1000 | Loss: 0.00003320
Iteration 146/1000 | Loss: 0.00003320
Iteration 147/1000 | Loss: 0.00003320
Iteration 148/1000 | Loss: 0.00003320
Iteration 149/1000 | Loss: 0.00003320
Iteration 150/1000 | Loss: 0.00003319
Iteration 151/1000 | Loss: 0.00003319
Iteration 152/1000 | Loss: 0.00003319
Iteration 153/1000 | Loss: 0.00003319
Iteration 154/1000 | Loss: 0.00003319
Iteration 155/1000 | Loss: 0.00003319
Iteration 156/1000 | Loss: 0.00003319
Iteration 157/1000 | Loss: 0.00003319
Iteration 158/1000 | Loss: 0.00003319
Iteration 159/1000 | Loss: 0.00003319
Iteration 160/1000 | Loss: 0.00003319
Iteration 161/1000 | Loss: 0.00003319
Iteration 162/1000 | Loss: 0.00003319
Iteration 163/1000 | Loss: 0.00003318
Iteration 164/1000 | Loss: 0.00003318
Iteration 165/1000 | Loss: 0.00003318
Iteration 166/1000 | Loss: 0.00003318
Iteration 167/1000 | Loss: 0.00003318
Iteration 168/1000 | Loss: 0.00003317
Iteration 169/1000 | Loss: 0.00003317
Iteration 170/1000 | Loss: 0.00003317
Iteration 171/1000 | Loss: 0.00003317
Iteration 172/1000 | Loss: 0.00003317
Iteration 173/1000 | Loss: 0.00003317
Iteration 174/1000 | Loss: 0.00003317
Iteration 175/1000 | Loss: 0.00003317
Iteration 176/1000 | Loss: 0.00003317
Iteration 177/1000 | Loss: 0.00003316
Iteration 178/1000 | Loss: 0.00003316
Iteration 179/1000 | Loss: 0.00003316
Iteration 180/1000 | Loss: 0.00003316
Iteration 181/1000 | Loss: 0.00003316
Iteration 182/1000 | Loss: 0.00003316
Iteration 183/1000 | Loss: 0.00003316
Iteration 184/1000 | Loss: 0.00003316
Iteration 185/1000 | Loss: 0.00003316
Iteration 186/1000 | Loss: 0.00003316
Iteration 187/1000 | Loss: 0.00003316
Iteration 188/1000 | Loss: 0.00003316
Iteration 189/1000 | Loss: 0.00003316
Iteration 190/1000 | Loss: 0.00003316
Iteration 191/1000 | Loss: 0.00003316
Iteration 192/1000 | Loss: 0.00003316
Iteration 193/1000 | Loss: 0.00003316
Iteration 194/1000 | Loss: 0.00003316
Iteration 195/1000 | Loss: 0.00003316
Iteration 196/1000 | Loss: 0.00003316
Iteration 197/1000 | Loss: 0.00003316
Iteration 198/1000 | Loss: 0.00003316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [3.316259608254768e-05, 3.316259608254768e-05, 3.316259608254768e-05, 3.316259608254768e-05, 3.316259608254768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.316259608254768e-05

Optimization complete. Final v2v error: 4.7009382247924805 mm

Highest mean error: 5.836997032165527 mm for frame 1

Lowest mean error: 4.180314540863037 mm for frame 72

Saving results

Total time: 53.529350996017456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014366
Iteration 2/25 | Loss: 0.00158610
Iteration 3/25 | Loss: 0.00144815
Iteration 4/25 | Loss: 0.00122033
Iteration 5/25 | Loss: 0.00119291
Iteration 6/25 | Loss: 0.00118863
Iteration 7/25 | Loss: 0.00118630
Iteration 8/25 | Loss: 0.00118542
Iteration 9/25 | Loss: 0.00118514
Iteration 10/25 | Loss: 0.00119243
Iteration 11/25 | Loss: 0.00119631
Iteration 12/25 | Loss: 0.00119788
Iteration 13/25 | Loss: 0.00119606
Iteration 14/25 | Loss: 0.00119163
Iteration 15/25 | Loss: 0.00118490
Iteration 16/25 | Loss: 0.00118366
Iteration 17/25 | Loss: 0.00119051
Iteration 18/25 | Loss: 0.00119061
Iteration 19/25 | Loss: 0.00118335
Iteration 20/25 | Loss: 0.00118078
Iteration 21/25 | Loss: 0.00118252
Iteration 22/25 | Loss: 0.00118121
Iteration 23/25 | Loss: 0.00117979
Iteration 24/25 | Loss: 0.00117914
Iteration 25/25 | Loss: 0.00117569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42956531
Iteration 2/25 | Loss: 0.00089847
Iteration 3/25 | Loss: 0.00089847
Iteration 4/25 | Loss: 0.00089847
Iteration 5/25 | Loss: 0.00089847
Iteration 6/25 | Loss: 0.00089847
Iteration 7/25 | Loss: 0.00089847
Iteration 8/25 | Loss: 0.00089847
Iteration 9/25 | Loss: 0.00089847
Iteration 10/25 | Loss: 0.00089847
Iteration 11/25 | Loss: 0.00089847
Iteration 12/25 | Loss: 0.00089847
Iteration 13/25 | Loss: 0.00089847
Iteration 14/25 | Loss: 0.00089847
Iteration 15/25 | Loss: 0.00089847
Iteration 16/25 | Loss: 0.00089847
Iteration 17/25 | Loss: 0.00089847
Iteration 18/25 | Loss: 0.00089847
Iteration 19/25 | Loss: 0.00089847
Iteration 20/25 | Loss: 0.00089847
Iteration 21/25 | Loss: 0.00089847
Iteration 22/25 | Loss: 0.00089847
Iteration 23/25 | Loss: 0.00089847
Iteration 24/25 | Loss: 0.00089847
Iteration 25/25 | Loss: 0.00089847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008984652813524008, 0.0008984652813524008, 0.0008984652813524008, 0.0008984652813524008, 0.0008984652813524008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008984652813524008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089847
Iteration 2/1000 | Loss: 0.00002667
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001326
Iteration 8/1000 | Loss: 0.00001277
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00001227
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001192
Iteration 30/1000 | Loss: 0.00001192
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001176
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.175839042844018e-05, 1.175839042844018e-05, 1.175839042844018e-05, 1.175839042844018e-05, 1.175839042844018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.175839042844018e-05

Optimization complete. Final v2v error: 2.925391674041748 mm

Highest mean error: 3.426534414291382 mm for frame 72

Lowest mean error: 2.6254096031188965 mm for frame 111

Saving results

Total time: 70.87905669212341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963285
Iteration 2/25 | Loss: 0.00168701
Iteration 3/25 | Loss: 0.00133668
Iteration 4/25 | Loss: 0.00131465
Iteration 5/25 | Loss: 0.00130782
Iteration 6/25 | Loss: 0.00130653
Iteration 7/25 | Loss: 0.00130653
Iteration 8/25 | Loss: 0.00130653
Iteration 9/25 | Loss: 0.00130653
Iteration 10/25 | Loss: 0.00130653
Iteration 11/25 | Loss: 0.00130653
Iteration 12/25 | Loss: 0.00130653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001306530088186264, 0.001306530088186264, 0.001306530088186264, 0.001306530088186264, 0.001306530088186264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001306530088186264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77270198
Iteration 2/25 | Loss: 0.00094376
Iteration 3/25 | Loss: 0.00094376
Iteration 4/25 | Loss: 0.00094376
Iteration 5/25 | Loss: 0.00094376
Iteration 6/25 | Loss: 0.00094376
Iteration 7/25 | Loss: 0.00094376
Iteration 8/25 | Loss: 0.00094376
Iteration 9/25 | Loss: 0.00094376
Iteration 10/25 | Loss: 0.00094376
Iteration 11/25 | Loss: 0.00094376
Iteration 12/25 | Loss: 0.00094376
Iteration 13/25 | Loss: 0.00094376
Iteration 14/25 | Loss: 0.00094376
Iteration 15/25 | Loss: 0.00094376
Iteration 16/25 | Loss: 0.00094376
Iteration 17/25 | Loss: 0.00094375
Iteration 18/25 | Loss: 0.00094375
Iteration 19/25 | Loss: 0.00094375
Iteration 20/25 | Loss: 0.00094375
Iteration 21/25 | Loss: 0.00094375
Iteration 22/25 | Loss: 0.00094375
Iteration 23/25 | Loss: 0.00094375
Iteration 24/25 | Loss: 0.00094375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009437549160793424, 0.0009437549160793424, 0.0009437549160793424, 0.0009437549160793424, 0.0009437549160793424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009437549160793424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094375
Iteration 2/1000 | Loss: 0.00006192
Iteration 3/1000 | Loss: 0.00004333
Iteration 4/1000 | Loss: 0.00003678
Iteration 5/1000 | Loss: 0.00003471
Iteration 6/1000 | Loss: 0.00003344
Iteration 7/1000 | Loss: 0.00003236
Iteration 8/1000 | Loss: 0.00003162
Iteration 9/1000 | Loss: 0.00003100
Iteration 10/1000 | Loss: 0.00003072
Iteration 11/1000 | Loss: 0.00003032
Iteration 12/1000 | Loss: 0.00002994
Iteration 13/1000 | Loss: 0.00002962
Iteration 14/1000 | Loss: 0.00002936
Iteration 15/1000 | Loss: 0.00002906
Iteration 16/1000 | Loss: 0.00002879
Iteration 17/1000 | Loss: 0.00002855
Iteration 18/1000 | Loss: 0.00002837
Iteration 19/1000 | Loss: 0.00002823
Iteration 20/1000 | Loss: 0.00002817
Iteration 21/1000 | Loss: 0.00002816
Iteration 22/1000 | Loss: 0.00002813
Iteration 23/1000 | Loss: 0.00002813
Iteration 24/1000 | Loss: 0.00002808
Iteration 25/1000 | Loss: 0.00002801
Iteration 26/1000 | Loss: 0.00002799
Iteration 27/1000 | Loss: 0.00002798
Iteration 28/1000 | Loss: 0.00002795
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00002792
Iteration 31/1000 | Loss: 0.00002792
Iteration 32/1000 | Loss: 0.00002791
Iteration 33/1000 | Loss: 0.00002791
Iteration 34/1000 | Loss: 0.00002790
Iteration 35/1000 | Loss: 0.00002789
Iteration 36/1000 | Loss: 0.00002789
Iteration 37/1000 | Loss: 0.00002787
Iteration 38/1000 | Loss: 0.00002785
Iteration 39/1000 | Loss: 0.00002785
Iteration 40/1000 | Loss: 0.00002785
Iteration 41/1000 | Loss: 0.00002785
Iteration 42/1000 | Loss: 0.00002784
Iteration 43/1000 | Loss: 0.00002784
Iteration 44/1000 | Loss: 0.00002783
Iteration 45/1000 | Loss: 0.00002783
Iteration 46/1000 | Loss: 0.00002783
Iteration 47/1000 | Loss: 0.00002783
Iteration 48/1000 | Loss: 0.00002783
Iteration 49/1000 | Loss: 0.00002783
Iteration 50/1000 | Loss: 0.00002783
Iteration 51/1000 | Loss: 0.00002782
Iteration 52/1000 | Loss: 0.00002782
Iteration 53/1000 | Loss: 0.00002782
Iteration 54/1000 | Loss: 0.00002782
Iteration 55/1000 | Loss: 0.00002781
Iteration 56/1000 | Loss: 0.00002781
Iteration 57/1000 | Loss: 0.00002781
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002780
Iteration 61/1000 | Loss: 0.00002780
Iteration 62/1000 | Loss: 0.00002780
Iteration 63/1000 | Loss: 0.00002780
Iteration 64/1000 | Loss: 0.00002780
Iteration 65/1000 | Loss: 0.00002780
Iteration 66/1000 | Loss: 0.00002780
Iteration 67/1000 | Loss: 0.00002780
Iteration 68/1000 | Loss: 0.00002780
Iteration 69/1000 | Loss: 0.00002780
Iteration 70/1000 | Loss: 0.00002780
Iteration 71/1000 | Loss: 0.00002780
Iteration 72/1000 | Loss: 0.00002780
Iteration 73/1000 | Loss: 0.00002779
Iteration 74/1000 | Loss: 0.00002779
Iteration 75/1000 | Loss: 0.00002779
Iteration 76/1000 | Loss: 0.00002779
Iteration 77/1000 | Loss: 0.00002778
Iteration 78/1000 | Loss: 0.00002778
Iteration 79/1000 | Loss: 0.00002778
Iteration 80/1000 | Loss: 0.00002778
Iteration 81/1000 | Loss: 0.00002778
Iteration 82/1000 | Loss: 0.00002778
Iteration 83/1000 | Loss: 0.00002778
Iteration 84/1000 | Loss: 0.00002778
Iteration 85/1000 | Loss: 0.00002778
Iteration 86/1000 | Loss: 0.00002778
Iteration 87/1000 | Loss: 0.00002778
Iteration 88/1000 | Loss: 0.00002777
Iteration 89/1000 | Loss: 0.00002777
Iteration 90/1000 | Loss: 0.00002777
Iteration 91/1000 | Loss: 0.00002777
Iteration 92/1000 | Loss: 0.00002777
Iteration 93/1000 | Loss: 0.00002777
Iteration 94/1000 | Loss: 0.00002777
Iteration 95/1000 | Loss: 0.00002777
Iteration 96/1000 | Loss: 0.00002777
Iteration 97/1000 | Loss: 0.00002777
Iteration 98/1000 | Loss: 0.00002777
Iteration 99/1000 | Loss: 0.00002777
Iteration 100/1000 | Loss: 0.00002777
Iteration 101/1000 | Loss: 0.00002777
Iteration 102/1000 | Loss: 0.00002776
Iteration 103/1000 | Loss: 0.00002776
Iteration 104/1000 | Loss: 0.00002776
Iteration 105/1000 | Loss: 0.00002776
Iteration 106/1000 | Loss: 0.00002776
Iteration 107/1000 | Loss: 0.00002776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.776483233901672e-05, 2.776483233901672e-05, 2.776483233901672e-05, 2.776483233901672e-05, 2.776483233901672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.776483233901672e-05

Optimization complete. Final v2v error: 4.458326816558838 mm

Highest mean error: 5.009460926055908 mm for frame 190

Lowest mean error: 3.9398610591888428 mm for frame 47

Saving results

Total time: 48.48743152618408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379259
Iteration 2/25 | Loss: 0.00149945
Iteration 3/25 | Loss: 0.00117693
Iteration 4/25 | Loss: 0.00114568
Iteration 5/25 | Loss: 0.00113744
Iteration 6/25 | Loss: 0.00113582
Iteration 7/25 | Loss: 0.00113539
Iteration 8/25 | Loss: 0.00113539
Iteration 9/25 | Loss: 0.00113539
Iteration 10/25 | Loss: 0.00113539
Iteration 11/25 | Loss: 0.00113539
Iteration 12/25 | Loss: 0.00113539
Iteration 13/25 | Loss: 0.00113539
Iteration 14/25 | Loss: 0.00113539
Iteration 15/25 | Loss: 0.00113539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011353897862136364, 0.0011353897862136364, 0.0011353897862136364, 0.0011353897862136364, 0.0011353897862136364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011353897862136364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32519066
Iteration 2/25 | Loss: 0.00060317
Iteration 3/25 | Loss: 0.00060317
Iteration 4/25 | Loss: 0.00060316
Iteration 5/25 | Loss: 0.00060316
Iteration 6/25 | Loss: 0.00060316
Iteration 7/25 | Loss: 0.00060316
Iteration 8/25 | Loss: 0.00060316
Iteration 9/25 | Loss: 0.00060316
Iteration 10/25 | Loss: 0.00060316
Iteration 11/25 | Loss: 0.00060316
Iteration 12/25 | Loss: 0.00060316
Iteration 13/25 | Loss: 0.00060316
Iteration 14/25 | Loss: 0.00060316
Iteration 15/25 | Loss: 0.00060316
Iteration 16/25 | Loss: 0.00060316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006031632656231523, 0.0006031632656231523, 0.0006031632656231523, 0.0006031632656231523, 0.0006031632656231523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006031632656231523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060316
Iteration 2/1000 | Loss: 0.00005035
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001550
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001369
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001324
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001287
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001284
Iteration 45/1000 | Loss: 0.00001284
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001283
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001283
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00001278
Iteration 82/1000 | Loss: 0.00001278
Iteration 83/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.2777731171809137e-05, 1.2777731171809137e-05, 1.2777731171809137e-05, 1.2777731171809137e-05, 1.2777731171809137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2777731171809137e-05

Optimization complete. Final v2v error: 3.0339596271514893 mm

Highest mean error: 3.2526650428771973 mm for frame 103

Lowest mean error: 2.8704822063446045 mm for frame 25

Saving results

Total time: 35.10375452041626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452707
Iteration 2/25 | Loss: 0.00129159
Iteration 3/25 | Loss: 0.00114948
Iteration 4/25 | Loss: 0.00113373
Iteration 5/25 | Loss: 0.00112842
Iteration 6/25 | Loss: 0.00112737
Iteration 7/25 | Loss: 0.00112737
Iteration 8/25 | Loss: 0.00112737
Iteration 9/25 | Loss: 0.00112737
Iteration 10/25 | Loss: 0.00112737
Iteration 11/25 | Loss: 0.00112737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011273699346929789, 0.0011273699346929789, 0.0011273699346929789, 0.0011273699346929789, 0.0011273699346929789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011273699346929789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43306839
Iteration 2/25 | Loss: 0.00089445
Iteration 3/25 | Loss: 0.00089444
Iteration 4/25 | Loss: 0.00089444
Iteration 5/25 | Loss: 0.00089444
Iteration 6/25 | Loss: 0.00089444
Iteration 7/25 | Loss: 0.00089444
Iteration 8/25 | Loss: 0.00089444
Iteration 9/25 | Loss: 0.00089444
Iteration 10/25 | Loss: 0.00089444
Iteration 11/25 | Loss: 0.00089444
Iteration 12/25 | Loss: 0.00089444
Iteration 13/25 | Loss: 0.00089444
Iteration 14/25 | Loss: 0.00089444
Iteration 15/25 | Loss: 0.00089444
Iteration 16/25 | Loss: 0.00089444
Iteration 17/25 | Loss: 0.00089444
Iteration 18/25 | Loss: 0.00089444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000894436554517597, 0.000894436554517597, 0.000894436554517597, 0.000894436554517597, 0.000894436554517597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000894436554517597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089444
Iteration 2/1000 | Loss: 0.00002256
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001273
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001268
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001265
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001256
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001239
Iteration 66/1000 | Loss: 0.00001239
Iteration 67/1000 | Loss: 0.00001239
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.230439647770254e-05, 1.230439647770254e-05, 1.230439647770254e-05, 1.230439647770254e-05, 1.230439647770254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.230439647770254e-05

Optimization complete. Final v2v error: 2.9651107788085938 mm

Highest mean error: 3.901473045349121 mm for frame 159

Lowest mean error: 2.533031463623047 mm for frame 13

Saving results

Total time: 44.10051703453064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786815
Iteration 2/25 | Loss: 0.00135983
Iteration 3/25 | Loss: 0.00118453
Iteration 4/25 | Loss: 0.00116138
Iteration 5/25 | Loss: 0.00115670
Iteration 6/25 | Loss: 0.00115552
Iteration 7/25 | Loss: 0.00115550
Iteration 8/25 | Loss: 0.00115550
Iteration 9/25 | Loss: 0.00115550
Iteration 10/25 | Loss: 0.00115550
Iteration 11/25 | Loss: 0.00115550
Iteration 12/25 | Loss: 0.00115550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011555033270269632, 0.0011555033270269632, 0.0011555033270269632, 0.0011555033270269632, 0.0011555033270269632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011555033270269632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35792017
Iteration 2/25 | Loss: 0.00079549
Iteration 3/25 | Loss: 0.00079549
Iteration 4/25 | Loss: 0.00079549
Iteration 5/25 | Loss: 0.00079549
Iteration 6/25 | Loss: 0.00079549
Iteration 7/25 | Loss: 0.00079549
Iteration 8/25 | Loss: 0.00079549
Iteration 9/25 | Loss: 0.00079549
Iteration 10/25 | Loss: 0.00079549
Iteration 11/25 | Loss: 0.00079549
Iteration 12/25 | Loss: 0.00079549
Iteration 13/25 | Loss: 0.00079549
Iteration 14/25 | Loss: 0.00079549
Iteration 15/25 | Loss: 0.00079549
Iteration 16/25 | Loss: 0.00079549
Iteration 17/25 | Loss: 0.00079549
Iteration 18/25 | Loss: 0.00079549
Iteration 19/25 | Loss: 0.00079549
Iteration 20/25 | Loss: 0.00079549
Iteration 21/25 | Loss: 0.00079549
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007954890606924891, 0.0007954890606924891, 0.0007954890606924891, 0.0007954890606924891, 0.0007954890606924891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007954890606924891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079549
Iteration 2/1000 | Loss: 0.00002991
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00001926
Iteration 5/1000 | Loss: 0.00001831
Iteration 6/1000 | Loss: 0.00001744
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001613
Iteration 9/1000 | Loss: 0.00001579
Iteration 10/1000 | Loss: 0.00001553
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001504
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001499
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001498
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001491
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001485
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001478
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001474
Iteration 71/1000 | Loss: 0.00001474
Iteration 72/1000 | Loss: 0.00001474
Iteration 73/1000 | Loss: 0.00001474
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001473
Iteration 77/1000 | Loss: 0.00001473
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001472
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001467
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00001466
Iteration 136/1000 | Loss: 0.00001466
Iteration 137/1000 | Loss: 0.00001466
Iteration 138/1000 | Loss: 0.00001466
Iteration 139/1000 | Loss: 0.00001466
Iteration 140/1000 | Loss: 0.00001466
Iteration 141/1000 | Loss: 0.00001466
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001465
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001464
Iteration 147/1000 | Loss: 0.00001464
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001464
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001463
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00001463
Iteration 158/1000 | Loss: 0.00001463
Iteration 159/1000 | Loss: 0.00001463
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001463
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001462
Iteration 168/1000 | Loss: 0.00001462
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001460
Iteration 187/1000 | Loss: 0.00001460
Iteration 188/1000 | Loss: 0.00001460
Iteration 189/1000 | Loss: 0.00001460
Iteration 190/1000 | Loss: 0.00001460
Iteration 191/1000 | Loss: 0.00001460
Iteration 192/1000 | Loss: 0.00001460
Iteration 193/1000 | Loss: 0.00001460
Iteration 194/1000 | Loss: 0.00001460
Iteration 195/1000 | Loss: 0.00001460
Iteration 196/1000 | Loss: 0.00001460
Iteration 197/1000 | Loss: 0.00001460
Iteration 198/1000 | Loss: 0.00001459
Iteration 199/1000 | Loss: 0.00001459
Iteration 200/1000 | Loss: 0.00001459
Iteration 201/1000 | Loss: 0.00001459
Iteration 202/1000 | Loss: 0.00001459
Iteration 203/1000 | Loss: 0.00001459
Iteration 204/1000 | Loss: 0.00001459
Iteration 205/1000 | Loss: 0.00001459
Iteration 206/1000 | Loss: 0.00001459
Iteration 207/1000 | Loss: 0.00001459
Iteration 208/1000 | Loss: 0.00001459
Iteration 209/1000 | Loss: 0.00001459
Iteration 210/1000 | Loss: 0.00001459
Iteration 211/1000 | Loss: 0.00001459
Iteration 212/1000 | Loss: 0.00001459
Iteration 213/1000 | Loss: 0.00001459
Iteration 214/1000 | Loss: 0.00001459
Iteration 215/1000 | Loss: 0.00001459
Iteration 216/1000 | Loss: 0.00001459
Iteration 217/1000 | Loss: 0.00001459
Iteration 218/1000 | Loss: 0.00001458
Iteration 219/1000 | Loss: 0.00001458
Iteration 220/1000 | Loss: 0.00001458
Iteration 221/1000 | Loss: 0.00001458
Iteration 222/1000 | Loss: 0.00001458
Iteration 223/1000 | Loss: 0.00001458
Iteration 224/1000 | Loss: 0.00001458
Iteration 225/1000 | Loss: 0.00001458
Iteration 226/1000 | Loss: 0.00001458
Iteration 227/1000 | Loss: 0.00001458
Iteration 228/1000 | Loss: 0.00001458
Iteration 229/1000 | Loss: 0.00001458
Iteration 230/1000 | Loss: 0.00001458
Iteration 231/1000 | Loss: 0.00001458
Iteration 232/1000 | Loss: 0.00001458
Iteration 233/1000 | Loss: 0.00001458
Iteration 234/1000 | Loss: 0.00001458
Iteration 235/1000 | Loss: 0.00001458
Iteration 236/1000 | Loss: 0.00001458
Iteration 237/1000 | Loss: 0.00001458
Iteration 238/1000 | Loss: 0.00001458
Iteration 239/1000 | Loss: 0.00001458
Iteration 240/1000 | Loss: 0.00001458
Iteration 241/1000 | Loss: 0.00001458
Iteration 242/1000 | Loss: 0.00001458
Iteration 243/1000 | Loss: 0.00001458
Iteration 244/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.4584914424631279e-05, 1.4584914424631279e-05, 1.4584914424631279e-05, 1.4584914424631279e-05, 1.4584914424631279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4584914424631279e-05

Optimization complete. Final v2v error: 3.2065682411193848 mm

Highest mean error: 3.9389100074768066 mm for frame 89

Lowest mean error: 2.7915046215057373 mm for frame 56

Saving results

Total time: 43.43725609779358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996384
Iteration 2/25 | Loss: 0.00996384
Iteration 3/25 | Loss: 0.00996384
Iteration 4/25 | Loss: 0.00501544
Iteration 5/25 | Loss: 0.00405991
Iteration 6/25 | Loss: 0.00350891
Iteration 7/25 | Loss: 0.00254186
Iteration 8/25 | Loss: 0.00219119
Iteration 9/25 | Loss: 0.00210193
Iteration 10/25 | Loss: 0.00203285
Iteration 11/25 | Loss: 0.00197205
Iteration 12/25 | Loss: 0.00199483
Iteration 13/25 | Loss: 0.00196408
Iteration 14/25 | Loss: 0.00192719
Iteration 15/25 | Loss: 0.00191626
Iteration 16/25 | Loss: 0.00190744
Iteration 17/25 | Loss: 0.00189421
Iteration 18/25 | Loss: 0.00188029
Iteration 19/25 | Loss: 0.00188229
Iteration 20/25 | Loss: 0.00188791
Iteration 21/25 | Loss: 0.00187998
Iteration 22/25 | Loss: 0.00187499
Iteration 23/25 | Loss: 0.00187155
Iteration 24/25 | Loss: 0.00186948
Iteration 25/25 | Loss: 0.00186878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28088427
Iteration 2/25 | Loss: 0.00676884
Iteration 3/25 | Loss: 0.00645423
Iteration 4/25 | Loss: 0.00645423
Iteration 5/25 | Loss: 0.00645422
Iteration 6/25 | Loss: 0.00645422
Iteration 7/25 | Loss: 0.00645422
Iteration 8/25 | Loss: 0.00645422
Iteration 9/25 | Loss: 0.00645422
Iteration 10/25 | Loss: 0.00645422
Iteration 11/25 | Loss: 0.00645422
Iteration 12/25 | Loss: 0.00645422
Iteration 13/25 | Loss: 0.00645422
Iteration 14/25 | Loss: 0.00645422
Iteration 15/25 | Loss: 0.00645422
Iteration 16/25 | Loss: 0.00645422
Iteration 17/25 | Loss: 0.00645422
Iteration 18/25 | Loss: 0.00645422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.006454219110310078, 0.006454219110310078, 0.006454219110310078, 0.006454219110310078, 0.006454219110310078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006454219110310078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00645422
Iteration 2/1000 | Loss: 0.00107562
Iteration 3/1000 | Loss: 0.00058367
Iteration 4/1000 | Loss: 0.00050755
Iteration 5/1000 | Loss: 0.00047411
Iteration 6/1000 | Loss: 0.00043846
Iteration 7/1000 | Loss: 0.00041109
Iteration 8/1000 | Loss: 0.00038141
Iteration 9/1000 | Loss: 0.00036240
Iteration 10/1000 | Loss: 0.00035146
Iteration 11/1000 | Loss: 0.00033987
Iteration 12/1000 | Loss: 0.00054047
Iteration 13/1000 | Loss: 0.00373665
Iteration 14/1000 | Loss: 0.00323167
Iteration 15/1000 | Loss: 0.01475950
Iteration 16/1000 | Loss: 0.01618478
Iteration 17/1000 | Loss: 0.00522617
Iteration 18/1000 | Loss: 0.00082746
Iteration 19/1000 | Loss: 0.00064105
Iteration 20/1000 | Loss: 0.00059822
Iteration 21/1000 | Loss: 0.00053053
Iteration 22/1000 | Loss: 0.00154310
Iteration 23/1000 | Loss: 0.00051741
Iteration 24/1000 | Loss: 0.00032196
Iteration 25/1000 | Loss: 0.00028263
Iteration 26/1000 | Loss: 0.00028727
Iteration 27/1000 | Loss: 0.00010117
Iteration 28/1000 | Loss: 0.00008765
Iteration 29/1000 | Loss: 0.00007194
Iteration 30/1000 | Loss: 0.00006275
Iteration 31/1000 | Loss: 0.00007016
Iteration 32/1000 | Loss: 0.00038596
Iteration 33/1000 | Loss: 0.00019884
Iteration 34/1000 | Loss: 0.00006286
Iteration 35/1000 | Loss: 0.00025472
Iteration 36/1000 | Loss: 0.00017909
Iteration 37/1000 | Loss: 0.00018567
Iteration 38/1000 | Loss: 0.00018372
Iteration 39/1000 | Loss: 0.00043733
Iteration 40/1000 | Loss: 0.00008069
Iteration 41/1000 | Loss: 0.00022131
Iteration 42/1000 | Loss: 0.00005148
Iteration 43/1000 | Loss: 0.00013615
Iteration 44/1000 | Loss: 0.00023828
Iteration 45/1000 | Loss: 0.00012102
Iteration 46/1000 | Loss: 0.00009229
Iteration 47/1000 | Loss: 0.00019775
Iteration 48/1000 | Loss: 0.00019969
Iteration 49/1000 | Loss: 0.00014127
Iteration 50/1000 | Loss: 0.00022672
Iteration 51/1000 | Loss: 0.00014234
Iteration 52/1000 | Loss: 0.00014886
Iteration 53/1000 | Loss: 0.00014904
Iteration 54/1000 | Loss: 0.00023059
Iteration 55/1000 | Loss: 0.00012457
Iteration 56/1000 | Loss: 0.00017244
Iteration 57/1000 | Loss: 0.00016938
Iteration 58/1000 | Loss: 0.00021137
Iteration 59/1000 | Loss: 0.00022747
Iteration 60/1000 | Loss: 0.00021589
Iteration 61/1000 | Loss: 0.00032217
Iteration 62/1000 | Loss: 0.00028183
Iteration 63/1000 | Loss: 0.00017884
Iteration 64/1000 | Loss: 0.00024246
Iteration 65/1000 | Loss: 0.00005162
Iteration 66/1000 | Loss: 0.00028188
Iteration 67/1000 | Loss: 0.00026783
Iteration 68/1000 | Loss: 0.00023065
Iteration 69/1000 | Loss: 0.00020743
Iteration 70/1000 | Loss: 0.00050959
Iteration 71/1000 | Loss: 0.00045578
Iteration 72/1000 | Loss: 0.00010799
Iteration 73/1000 | Loss: 0.00009141
Iteration 74/1000 | Loss: 0.00022883
Iteration 75/1000 | Loss: 0.00034975
Iteration 76/1000 | Loss: 0.00026948
Iteration 77/1000 | Loss: 0.00020210
Iteration 78/1000 | Loss: 0.00028942
Iteration 79/1000 | Loss: 0.00004617
Iteration 80/1000 | Loss: 0.00004342
Iteration 81/1000 | Loss: 0.00004215
Iteration 82/1000 | Loss: 0.00005214
Iteration 83/1000 | Loss: 0.00019472
Iteration 84/1000 | Loss: 0.00050330
Iteration 85/1000 | Loss: 0.00032942
Iteration 86/1000 | Loss: 0.00008301
Iteration 87/1000 | Loss: 0.00007599
Iteration 88/1000 | Loss: 0.00006644
Iteration 89/1000 | Loss: 0.00007178
Iteration 90/1000 | Loss: 0.00005023
Iteration 91/1000 | Loss: 0.00004736
Iteration 92/1000 | Loss: 0.00013149
Iteration 93/1000 | Loss: 0.00005945
Iteration 94/1000 | Loss: 0.00004185
Iteration 95/1000 | Loss: 0.00005414
Iteration 96/1000 | Loss: 0.00004149
Iteration 97/1000 | Loss: 0.00003949
Iteration 98/1000 | Loss: 0.00003655
Iteration 99/1000 | Loss: 0.00003591
Iteration 100/1000 | Loss: 0.00004897
Iteration 101/1000 | Loss: 0.00004074
Iteration 102/1000 | Loss: 0.00003853
Iteration 103/1000 | Loss: 0.00005236
Iteration 104/1000 | Loss: 0.00004007
Iteration 105/1000 | Loss: 0.00003839
Iteration 106/1000 | Loss: 0.00003730
Iteration 107/1000 | Loss: 0.00004566
Iteration 108/1000 | Loss: 0.00004701
Iteration 109/1000 | Loss: 0.00004565
Iteration 110/1000 | Loss: 0.00004782
Iteration 111/1000 | Loss: 0.00017120
Iteration 112/1000 | Loss: 0.00015915
Iteration 113/1000 | Loss: 0.00004499
Iteration 114/1000 | Loss: 0.00004360
Iteration 115/1000 | Loss: 0.00006172
Iteration 116/1000 | Loss: 0.00003787
Iteration 117/1000 | Loss: 0.00003707
Iteration 118/1000 | Loss: 0.00003661
Iteration 119/1000 | Loss: 0.00003623
Iteration 120/1000 | Loss: 0.00003614
Iteration 121/1000 | Loss: 0.00003612
Iteration 122/1000 | Loss: 0.00004258
Iteration 123/1000 | Loss: 0.00003732
Iteration 124/1000 | Loss: 0.00003671
Iteration 125/1000 | Loss: 0.00004480
Iteration 126/1000 | Loss: 0.00021659
Iteration 127/1000 | Loss: 0.00042015
Iteration 128/1000 | Loss: 0.00004132
Iteration 129/1000 | Loss: 0.00003770
Iteration 130/1000 | Loss: 0.00003607
Iteration 131/1000 | Loss: 0.00003558
Iteration 132/1000 | Loss: 0.00003544
Iteration 133/1000 | Loss: 0.00003523
Iteration 134/1000 | Loss: 0.00003499
Iteration 135/1000 | Loss: 0.00003473
Iteration 136/1000 | Loss: 0.00003464
Iteration 137/1000 | Loss: 0.00003464
Iteration 138/1000 | Loss: 0.00003452
Iteration 139/1000 | Loss: 0.00003448
Iteration 140/1000 | Loss: 0.00003439
Iteration 141/1000 | Loss: 0.00003436
Iteration 142/1000 | Loss: 0.00003434
Iteration 143/1000 | Loss: 0.00003430
Iteration 144/1000 | Loss: 0.00003429
Iteration 145/1000 | Loss: 0.00003429
Iteration 146/1000 | Loss: 0.00003428
Iteration 147/1000 | Loss: 0.00003427
Iteration 148/1000 | Loss: 0.00003426
Iteration 149/1000 | Loss: 0.00003425
Iteration 150/1000 | Loss: 0.00003425
Iteration 151/1000 | Loss: 0.00003425
Iteration 152/1000 | Loss: 0.00003425
Iteration 153/1000 | Loss: 0.00003425
Iteration 154/1000 | Loss: 0.00003425
Iteration 155/1000 | Loss: 0.00003425
Iteration 156/1000 | Loss: 0.00003425
Iteration 157/1000 | Loss: 0.00003425
Iteration 158/1000 | Loss: 0.00003424
Iteration 159/1000 | Loss: 0.00003424
Iteration 160/1000 | Loss: 0.00003424
Iteration 161/1000 | Loss: 0.00003424
Iteration 162/1000 | Loss: 0.00003423
Iteration 163/1000 | Loss: 0.00003423
Iteration 164/1000 | Loss: 0.00003423
Iteration 165/1000 | Loss: 0.00003423
Iteration 166/1000 | Loss: 0.00003423
Iteration 167/1000 | Loss: 0.00003423
Iteration 168/1000 | Loss: 0.00003423
Iteration 169/1000 | Loss: 0.00003423
Iteration 170/1000 | Loss: 0.00003423
Iteration 171/1000 | Loss: 0.00003422
Iteration 172/1000 | Loss: 0.00003422
Iteration 173/1000 | Loss: 0.00003422
Iteration 174/1000 | Loss: 0.00003422
Iteration 175/1000 | Loss: 0.00003422
Iteration 176/1000 | Loss: 0.00003421
Iteration 177/1000 | Loss: 0.00003421
Iteration 178/1000 | Loss: 0.00003421
Iteration 179/1000 | Loss: 0.00003421
Iteration 180/1000 | Loss: 0.00003421
Iteration 181/1000 | Loss: 0.00003421
Iteration 182/1000 | Loss: 0.00003421
Iteration 183/1000 | Loss: 0.00003421
Iteration 184/1000 | Loss: 0.00003421
Iteration 185/1000 | Loss: 0.00003421
Iteration 186/1000 | Loss: 0.00003421
Iteration 187/1000 | Loss: 0.00003421
Iteration 188/1000 | Loss: 0.00003421
Iteration 189/1000 | Loss: 0.00003421
Iteration 190/1000 | Loss: 0.00003421
Iteration 191/1000 | Loss: 0.00003421
Iteration 192/1000 | Loss: 0.00003421
Iteration 193/1000 | Loss: 0.00003421
Iteration 194/1000 | Loss: 0.00003421
Iteration 195/1000 | Loss: 0.00003420
Iteration 196/1000 | Loss: 0.00003420
Iteration 197/1000 | Loss: 0.00003420
Iteration 198/1000 | Loss: 0.00003420
Iteration 199/1000 | Loss: 0.00003420
Iteration 200/1000 | Loss: 0.00003420
Iteration 201/1000 | Loss: 0.00003420
Iteration 202/1000 | Loss: 0.00003420
Iteration 203/1000 | Loss: 0.00003420
Iteration 204/1000 | Loss: 0.00003419
Iteration 205/1000 | Loss: 0.00003419
Iteration 206/1000 | Loss: 0.00003419
Iteration 207/1000 | Loss: 0.00003419
Iteration 208/1000 | Loss: 0.00003419
Iteration 209/1000 | Loss: 0.00003419
Iteration 210/1000 | Loss: 0.00003419
Iteration 211/1000 | Loss: 0.00003419
Iteration 212/1000 | Loss: 0.00003418
Iteration 213/1000 | Loss: 0.00003418
Iteration 214/1000 | Loss: 0.00003418
Iteration 215/1000 | Loss: 0.00003418
Iteration 216/1000 | Loss: 0.00003417
Iteration 217/1000 | Loss: 0.00003417
Iteration 218/1000 | Loss: 0.00003417
Iteration 219/1000 | Loss: 0.00003417
Iteration 220/1000 | Loss: 0.00003417
Iteration 221/1000 | Loss: 0.00003417
Iteration 222/1000 | Loss: 0.00003417
Iteration 223/1000 | Loss: 0.00003417
Iteration 224/1000 | Loss: 0.00003417
Iteration 225/1000 | Loss: 0.00003417
Iteration 226/1000 | Loss: 0.00003417
Iteration 227/1000 | Loss: 0.00003416
Iteration 228/1000 | Loss: 0.00003416
Iteration 229/1000 | Loss: 0.00003416
Iteration 230/1000 | Loss: 0.00003416
Iteration 231/1000 | Loss: 0.00003416
Iteration 232/1000 | Loss: 0.00003416
Iteration 233/1000 | Loss: 0.00003416
Iteration 234/1000 | Loss: 0.00003416
Iteration 235/1000 | Loss: 0.00003416
Iteration 236/1000 | Loss: 0.00004106
Iteration 237/1000 | Loss: 0.00003549
Iteration 238/1000 | Loss: 0.00003482
Iteration 239/1000 | Loss: 0.00003418
Iteration 240/1000 | Loss: 0.00003406
Iteration 241/1000 | Loss: 0.00003401
Iteration 242/1000 | Loss: 0.00003399
Iteration 243/1000 | Loss: 0.00003399
Iteration 244/1000 | Loss: 0.00003399
Iteration 245/1000 | Loss: 0.00003399
Iteration 246/1000 | Loss: 0.00003398
Iteration 247/1000 | Loss: 0.00003398
Iteration 248/1000 | Loss: 0.00003398
Iteration 249/1000 | Loss: 0.00003398
Iteration 250/1000 | Loss: 0.00003398
Iteration 251/1000 | Loss: 0.00003397
Iteration 252/1000 | Loss: 0.00003396
Iteration 253/1000 | Loss: 0.00003395
Iteration 254/1000 | Loss: 0.00003395
Iteration 255/1000 | Loss: 0.00003394
Iteration 256/1000 | Loss: 0.00003394
Iteration 257/1000 | Loss: 0.00003393
Iteration 258/1000 | Loss: 0.00003393
Iteration 259/1000 | Loss: 0.00003392
Iteration 260/1000 | Loss: 0.00003392
Iteration 261/1000 | Loss: 0.00003392
Iteration 262/1000 | Loss: 0.00003391
Iteration 263/1000 | Loss: 0.00003391
Iteration 264/1000 | Loss: 0.00003391
Iteration 265/1000 | Loss: 0.00003391
Iteration 266/1000 | Loss: 0.00003391
Iteration 267/1000 | Loss: 0.00003391
Iteration 268/1000 | Loss: 0.00003391
Iteration 269/1000 | Loss: 0.00003391
Iteration 270/1000 | Loss: 0.00003391
Iteration 271/1000 | Loss: 0.00003390
Iteration 272/1000 | Loss: 0.00003390
Iteration 273/1000 | Loss: 0.00003390
Iteration 274/1000 | Loss: 0.00003389
Iteration 275/1000 | Loss: 0.00003389
Iteration 276/1000 | Loss: 0.00003389
Iteration 277/1000 | Loss: 0.00003389
Iteration 278/1000 | Loss: 0.00003389
Iteration 279/1000 | Loss: 0.00003388
Iteration 280/1000 | Loss: 0.00003388
Iteration 281/1000 | Loss: 0.00003388
Iteration 282/1000 | Loss: 0.00003387
Iteration 283/1000 | Loss: 0.00003386
Iteration 284/1000 | Loss: 0.00003386
Iteration 285/1000 | Loss: 0.00003386
Iteration 286/1000 | Loss: 0.00003386
Iteration 287/1000 | Loss: 0.00003385
Iteration 288/1000 | Loss: 0.00003385
Iteration 289/1000 | Loss: 0.00003385
Iteration 290/1000 | Loss: 0.00003385
Iteration 291/1000 | Loss: 0.00003384
Iteration 292/1000 | Loss: 0.00003384
Iteration 293/1000 | Loss: 0.00003384
Iteration 294/1000 | Loss: 0.00003384
Iteration 295/1000 | Loss: 0.00003384
Iteration 296/1000 | Loss: 0.00003384
Iteration 297/1000 | Loss: 0.00003384
Iteration 298/1000 | Loss: 0.00003384
Iteration 299/1000 | Loss: 0.00003384
Iteration 300/1000 | Loss: 0.00003384
Iteration 301/1000 | Loss: 0.00003384
Iteration 302/1000 | Loss: 0.00003384
Iteration 303/1000 | Loss: 0.00003383
Iteration 304/1000 | Loss: 0.00003383
Iteration 305/1000 | Loss: 0.00003383
Iteration 306/1000 | Loss: 0.00003383
Iteration 307/1000 | Loss: 0.00003383
Iteration 308/1000 | Loss: 0.00003383
Iteration 309/1000 | Loss: 0.00003383
Iteration 310/1000 | Loss: 0.00003383
Iteration 311/1000 | Loss: 0.00003383
Iteration 312/1000 | Loss: 0.00003383
Iteration 313/1000 | Loss: 0.00003383
Iteration 314/1000 | Loss: 0.00003383
Iteration 315/1000 | Loss: 0.00003383
Iteration 316/1000 | Loss: 0.00003382
Iteration 317/1000 | Loss: 0.00003382
Iteration 318/1000 | Loss: 0.00003382
Iteration 319/1000 | Loss: 0.00003382
Iteration 320/1000 | Loss: 0.00003382
Iteration 321/1000 | Loss: 0.00003382
Iteration 322/1000 | Loss: 0.00003382
Iteration 323/1000 | Loss: 0.00003382
Iteration 324/1000 | Loss: 0.00003382
Iteration 325/1000 | Loss: 0.00003382
Iteration 326/1000 | Loss: 0.00003382
Iteration 327/1000 | Loss: 0.00003382
Iteration 328/1000 | Loss: 0.00003382
Iteration 329/1000 | Loss: 0.00003382
Iteration 330/1000 | Loss: 0.00003382
Iteration 331/1000 | Loss: 0.00003382
Iteration 332/1000 | Loss: 0.00003382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [3.3823038393165916e-05, 3.3823038393165916e-05, 3.3823038393165916e-05, 3.3823038393165916e-05, 3.3823038393165916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3823038393165916e-05

Optimization complete. Final v2v error: 3.762275457382202 mm

Highest mean error: 10.949237823486328 mm for frame 235

Lowest mean error: 3.2062289714813232 mm for frame 24

Saving results

Total time: 289.691819190979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405264
Iteration 2/25 | Loss: 0.00121687
Iteration 3/25 | Loss: 0.00114379
Iteration 4/25 | Loss: 0.00113455
Iteration 5/25 | Loss: 0.00113230
Iteration 6/25 | Loss: 0.00113179
Iteration 7/25 | Loss: 0.00113176
Iteration 8/25 | Loss: 0.00113176
Iteration 9/25 | Loss: 0.00113176
Iteration 10/25 | Loss: 0.00113176
Iteration 11/25 | Loss: 0.00113176
Iteration 12/25 | Loss: 0.00113176
Iteration 13/25 | Loss: 0.00113176
Iteration 14/25 | Loss: 0.00113176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011317571625113487, 0.0011317571625113487, 0.0011317571625113487, 0.0011317571625113487, 0.0011317571625113487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011317571625113487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44536662
Iteration 2/25 | Loss: 0.00084376
Iteration 3/25 | Loss: 0.00084376
Iteration 4/25 | Loss: 0.00084376
Iteration 5/25 | Loss: 0.00084376
Iteration 6/25 | Loss: 0.00084376
Iteration 7/25 | Loss: 0.00084376
Iteration 8/25 | Loss: 0.00084376
Iteration 9/25 | Loss: 0.00084376
Iteration 10/25 | Loss: 0.00084376
Iteration 11/25 | Loss: 0.00084376
Iteration 12/25 | Loss: 0.00084376
Iteration 13/25 | Loss: 0.00084376
Iteration 14/25 | Loss: 0.00084376
Iteration 15/25 | Loss: 0.00084376
Iteration 16/25 | Loss: 0.00084376
Iteration 17/25 | Loss: 0.00084376
Iteration 18/25 | Loss: 0.00084376
Iteration 19/25 | Loss: 0.00084376
Iteration 20/25 | Loss: 0.00084376
Iteration 21/25 | Loss: 0.00084376
Iteration 22/25 | Loss: 0.00084376
Iteration 23/25 | Loss: 0.00084376
Iteration 24/25 | Loss: 0.00084376
Iteration 25/25 | Loss: 0.00084376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084376
Iteration 2/1000 | Loss: 0.00002244
Iteration 3/1000 | Loss: 0.00001409
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001181
Iteration 6/1000 | Loss: 0.00001134
Iteration 7/1000 | Loss: 0.00001094
Iteration 8/1000 | Loss: 0.00001068
Iteration 9/1000 | Loss: 0.00001057
Iteration 10/1000 | Loss: 0.00001039
Iteration 11/1000 | Loss: 0.00001026
Iteration 12/1000 | Loss: 0.00001010
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001006
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00000999
Iteration 17/1000 | Loss: 0.00000995
Iteration 18/1000 | Loss: 0.00000995
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000994
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000989
Iteration 24/1000 | Loss: 0.00000989
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000989
Iteration 28/1000 | Loss: 0.00000987
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000984
Iteration 36/1000 | Loss: 0.00000984
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000980
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000979
Iteration 50/1000 | Loss: 0.00000979
Iteration 51/1000 | Loss: 0.00000979
Iteration 52/1000 | Loss: 0.00000979
Iteration 53/1000 | Loss: 0.00000979
Iteration 54/1000 | Loss: 0.00000979
Iteration 55/1000 | Loss: 0.00000979
Iteration 56/1000 | Loss: 0.00000979
Iteration 57/1000 | Loss: 0.00000979
Iteration 58/1000 | Loss: 0.00000978
Iteration 59/1000 | Loss: 0.00000978
Iteration 60/1000 | Loss: 0.00000978
Iteration 61/1000 | Loss: 0.00000978
Iteration 62/1000 | Loss: 0.00000978
Iteration 63/1000 | Loss: 0.00000978
Iteration 64/1000 | Loss: 0.00000978
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000977
Iteration 68/1000 | Loss: 0.00000977
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000976
Iteration 74/1000 | Loss: 0.00000975
Iteration 75/1000 | Loss: 0.00000975
Iteration 76/1000 | Loss: 0.00000975
Iteration 77/1000 | Loss: 0.00000975
Iteration 78/1000 | Loss: 0.00000975
Iteration 79/1000 | Loss: 0.00000975
Iteration 80/1000 | Loss: 0.00000975
Iteration 81/1000 | Loss: 0.00000975
Iteration 82/1000 | Loss: 0.00000975
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000974
Iteration 86/1000 | Loss: 0.00000974
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000973
Iteration 89/1000 | Loss: 0.00000973
Iteration 90/1000 | Loss: 0.00000973
Iteration 91/1000 | Loss: 0.00000973
Iteration 92/1000 | Loss: 0.00000972
Iteration 93/1000 | Loss: 0.00000972
Iteration 94/1000 | Loss: 0.00000972
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000971
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000970
Iteration 105/1000 | Loss: 0.00000970
Iteration 106/1000 | Loss: 0.00000970
Iteration 107/1000 | Loss: 0.00000970
Iteration 108/1000 | Loss: 0.00000970
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000969
Iteration 114/1000 | Loss: 0.00000969
Iteration 115/1000 | Loss: 0.00000969
Iteration 116/1000 | Loss: 0.00000969
Iteration 117/1000 | Loss: 0.00000969
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000968
Iteration 121/1000 | Loss: 0.00000968
Iteration 122/1000 | Loss: 0.00000968
Iteration 123/1000 | Loss: 0.00000968
Iteration 124/1000 | Loss: 0.00000968
Iteration 125/1000 | Loss: 0.00000968
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000967
Iteration 133/1000 | Loss: 0.00000967
Iteration 134/1000 | Loss: 0.00000967
Iteration 135/1000 | Loss: 0.00000967
Iteration 136/1000 | Loss: 0.00000967
Iteration 137/1000 | Loss: 0.00000967
Iteration 138/1000 | Loss: 0.00000967
Iteration 139/1000 | Loss: 0.00000967
Iteration 140/1000 | Loss: 0.00000967
Iteration 141/1000 | Loss: 0.00000967
Iteration 142/1000 | Loss: 0.00000967
Iteration 143/1000 | Loss: 0.00000967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [9.673040040070191e-06, 9.673040040070191e-06, 9.673040040070191e-06, 9.673040040070191e-06, 9.673040040070191e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.673040040070191e-06

Optimization complete. Final v2v error: 2.637378454208374 mm

Highest mean error: 3.4892635345458984 mm for frame 74

Lowest mean error: 2.4194886684417725 mm for frame 122

Saving results

Total time: 35.55980563163757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533317
Iteration 2/25 | Loss: 0.00156392
Iteration 3/25 | Loss: 0.00127182
Iteration 4/25 | Loss: 0.00124584
Iteration 5/25 | Loss: 0.00123843
Iteration 6/25 | Loss: 0.00123586
Iteration 7/25 | Loss: 0.00123586
Iteration 8/25 | Loss: 0.00123586
Iteration 9/25 | Loss: 0.00123586
Iteration 10/25 | Loss: 0.00123586
Iteration 11/25 | Loss: 0.00123586
Iteration 12/25 | Loss: 0.00123586
Iteration 13/25 | Loss: 0.00123586
Iteration 14/25 | Loss: 0.00123586
Iteration 15/25 | Loss: 0.00123586
Iteration 16/25 | Loss: 0.00123586
Iteration 17/25 | Loss: 0.00123586
Iteration 18/25 | Loss: 0.00123586
Iteration 19/25 | Loss: 0.00123586
Iteration 20/25 | Loss: 0.00123586
Iteration 21/25 | Loss: 0.00123586
Iteration 22/25 | Loss: 0.00123586
Iteration 23/25 | Loss: 0.00123586
Iteration 24/25 | Loss: 0.00123586
Iteration 25/25 | Loss: 0.00123586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91133159
Iteration 2/25 | Loss: 0.00097666
Iteration 3/25 | Loss: 0.00097666
Iteration 4/25 | Loss: 0.00097666
Iteration 5/25 | Loss: 0.00097666
Iteration 6/25 | Loss: 0.00097666
Iteration 7/25 | Loss: 0.00097666
Iteration 8/25 | Loss: 0.00097666
Iteration 9/25 | Loss: 0.00097666
Iteration 10/25 | Loss: 0.00097666
Iteration 11/25 | Loss: 0.00097666
Iteration 12/25 | Loss: 0.00097666
Iteration 13/25 | Loss: 0.00097666
Iteration 14/25 | Loss: 0.00097666
Iteration 15/25 | Loss: 0.00097666
Iteration 16/25 | Loss: 0.00097666
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009766599396243691, 0.0009766599396243691, 0.0009766599396243691, 0.0009766599396243691, 0.0009766599396243691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009766599396243691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097666
Iteration 2/1000 | Loss: 0.00006145
Iteration 3/1000 | Loss: 0.00003895
Iteration 4/1000 | Loss: 0.00003309
Iteration 5/1000 | Loss: 0.00003121
Iteration 6/1000 | Loss: 0.00002961
Iteration 7/1000 | Loss: 0.00002882
Iteration 8/1000 | Loss: 0.00002796
Iteration 9/1000 | Loss: 0.00002735
Iteration 10/1000 | Loss: 0.00002693
Iteration 11/1000 | Loss: 0.00002649
Iteration 12/1000 | Loss: 0.00002617
Iteration 13/1000 | Loss: 0.00002592
Iteration 14/1000 | Loss: 0.00002568
Iteration 15/1000 | Loss: 0.00002547
Iteration 16/1000 | Loss: 0.00002532
Iteration 17/1000 | Loss: 0.00002527
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002518
Iteration 20/1000 | Loss: 0.00002517
Iteration 21/1000 | Loss: 0.00002503
Iteration 22/1000 | Loss: 0.00002500
Iteration 23/1000 | Loss: 0.00002500
Iteration 24/1000 | Loss: 0.00002500
Iteration 25/1000 | Loss: 0.00002499
Iteration 26/1000 | Loss: 0.00002499
Iteration 27/1000 | Loss: 0.00002499
Iteration 28/1000 | Loss: 0.00002498
Iteration 29/1000 | Loss: 0.00002497
Iteration 30/1000 | Loss: 0.00002496
Iteration 31/1000 | Loss: 0.00002495
Iteration 32/1000 | Loss: 0.00002495
Iteration 33/1000 | Loss: 0.00002495
Iteration 34/1000 | Loss: 0.00002493
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002493
Iteration 37/1000 | Loss: 0.00002493
Iteration 38/1000 | Loss: 0.00002493
Iteration 39/1000 | Loss: 0.00002493
Iteration 40/1000 | Loss: 0.00002493
Iteration 41/1000 | Loss: 0.00002493
Iteration 42/1000 | Loss: 0.00002489
Iteration 43/1000 | Loss: 0.00002489
Iteration 44/1000 | Loss: 0.00002488
Iteration 45/1000 | Loss: 0.00002486
Iteration 46/1000 | Loss: 0.00002485
Iteration 47/1000 | Loss: 0.00002483
Iteration 48/1000 | Loss: 0.00002482
Iteration 49/1000 | Loss: 0.00002480
Iteration 50/1000 | Loss: 0.00002480
Iteration 51/1000 | Loss: 0.00002480
Iteration 52/1000 | Loss: 0.00002480
Iteration 53/1000 | Loss: 0.00002480
Iteration 54/1000 | Loss: 0.00002479
Iteration 55/1000 | Loss: 0.00002479
Iteration 56/1000 | Loss: 0.00002479
Iteration 57/1000 | Loss: 0.00002479
Iteration 58/1000 | Loss: 0.00002479
Iteration 59/1000 | Loss: 0.00002479
Iteration 60/1000 | Loss: 0.00002479
Iteration 61/1000 | Loss: 0.00002479
Iteration 62/1000 | Loss: 0.00002478
Iteration 63/1000 | Loss: 0.00002478
Iteration 64/1000 | Loss: 0.00002477
Iteration 65/1000 | Loss: 0.00002477
Iteration 66/1000 | Loss: 0.00002477
Iteration 67/1000 | Loss: 0.00002476
Iteration 68/1000 | Loss: 0.00002476
Iteration 69/1000 | Loss: 0.00002476
Iteration 70/1000 | Loss: 0.00002476
Iteration 71/1000 | Loss: 0.00002476
Iteration 72/1000 | Loss: 0.00002476
Iteration 73/1000 | Loss: 0.00002476
Iteration 74/1000 | Loss: 0.00002476
Iteration 75/1000 | Loss: 0.00002476
Iteration 76/1000 | Loss: 0.00002476
Iteration 77/1000 | Loss: 0.00002475
Iteration 78/1000 | Loss: 0.00002475
Iteration 79/1000 | Loss: 0.00002475
Iteration 80/1000 | Loss: 0.00002475
Iteration 81/1000 | Loss: 0.00002474
Iteration 82/1000 | Loss: 0.00002474
Iteration 83/1000 | Loss: 0.00002474
Iteration 84/1000 | Loss: 0.00002473
Iteration 85/1000 | Loss: 0.00002473
Iteration 86/1000 | Loss: 0.00002473
Iteration 87/1000 | Loss: 0.00002473
Iteration 88/1000 | Loss: 0.00002473
Iteration 89/1000 | Loss: 0.00002472
Iteration 90/1000 | Loss: 0.00002472
Iteration 91/1000 | Loss: 0.00002472
Iteration 92/1000 | Loss: 0.00002472
Iteration 93/1000 | Loss: 0.00002472
Iteration 94/1000 | Loss: 0.00002472
Iteration 95/1000 | Loss: 0.00002472
Iteration 96/1000 | Loss: 0.00002472
Iteration 97/1000 | Loss: 0.00002472
Iteration 98/1000 | Loss: 0.00002472
Iteration 99/1000 | Loss: 0.00002472
Iteration 100/1000 | Loss: 0.00002471
Iteration 101/1000 | Loss: 0.00002471
Iteration 102/1000 | Loss: 0.00002471
Iteration 103/1000 | Loss: 0.00002471
Iteration 104/1000 | Loss: 0.00002471
Iteration 105/1000 | Loss: 0.00002470
Iteration 106/1000 | Loss: 0.00002470
Iteration 107/1000 | Loss: 0.00002470
Iteration 108/1000 | Loss: 0.00002470
Iteration 109/1000 | Loss: 0.00002469
Iteration 110/1000 | Loss: 0.00002469
Iteration 111/1000 | Loss: 0.00002469
Iteration 112/1000 | Loss: 0.00002469
Iteration 113/1000 | Loss: 0.00002469
Iteration 114/1000 | Loss: 0.00002469
Iteration 115/1000 | Loss: 0.00002469
Iteration 116/1000 | Loss: 0.00002469
Iteration 117/1000 | Loss: 0.00002469
Iteration 118/1000 | Loss: 0.00002469
Iteration 119/1000 | Loss: 0.00002469
Iteration 120/1000 | Loss: 0.00002469
Iteration 121/1000 | Loss: 0.00002469
Iteration 122/1000 | Loss: 0.00002468
Iteration 123/1000 | Loss: 0.00002468
Iteration 124/1000 | Loss: 0.00002468
Iteration 125/1000 | Loss: 0.00002468
Iteration 126/1000 | Loss: 0.00002468
Iteration 127/1000 | Loss: 0.00002467
Iteration 128/1000 | Loss: 0.00002467
Iteration 129/1000 | Loss: 0.00002467
Iteration 130/1000 | Loss: 0.00002467
Iteration 131/1000 | Loss: 0.00002467
Iteration 132/1000 | Loss: 0.00002467
Iteration 133/1000 | Loss: 0.00002467
Iteration 134/1000 | Loss: 0.00002467
Iteration 135/1000 | Loss: 0.00002467
Iteration 136/1000 | Loss: 0.00002467
Iteration 137/1000 | Loss: 0.00002466
Iteration 138/1000 | Loss: 0.00002466
Iteration 139/1000 | Loss: 0.00002466
Iteration 140/1000 | Loss: 0.00002466
Iteration 141/1000 | Loss: 0.00002466
Iteration 142/1000 | Loss: 0.00002466
Iteration 143/1000 | Loss: 0.00002466
Iteration 144/1000 | Loss: 0.00002465
Iteration 145/1000 | Loss: 0.00002465
Iteration 146/1000 | Loss: 0.00002465
Iteration 147/1000 | Loss: 0.00002465
Iteration 148/1000 | Loss: 0.00002465
Iteration 149/1000 | Loss: 0.00002465
Iteration 150/1000 | Loss: 0.00002465
Iteration 151/1000 | Loss: 0.00002465
Iteration 152/1000 | Loss: 0.00002465
Iteration 153/1000 | Loss: 0.00002465
Iteration 154/1000 | Loss: 0.00002465
Iteration 155/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.465005854901392e-05, 2.465005854901392e-05, 2.465005854901392e-05, 2.465005854901392e-05, 2.465005854901392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.465005854901392e-05

Optimization complete. Final v2v error: 4.090063095092773 mm

Highest mean error: 5.136252403259277 mm for frame 18

Lowest mean error: 3.5042455196380615 mm for frame 92

Saving results

Total time: 48.067102670669556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982680
Iteration 2/25 | Loss: 0.00982680
Iteration 3/25 | Loss: 0.00982680
Iteration 4/25 | Loss: 0.00982680
Iteration 5/25 | Loss: 0.00982679
Iteration 6/25 | Loss: 0.00259715
Iteration 7/25 | Loss: 0.00187062
Iteration 8/25 | Loss: 0.00176038
Iteration 9/25 | Loss: 0.00176567
Iteration 10/25 | Loss: 0.00171123
Iteration 11/25 | Loss: 0.00153937
Iteration 12/25 | Loss: 0.00146939
Iteration 13/25 | Loss: 0.00140368
Iteration 14/25 | Loss: 0.00137876
Iteration 15/25 | Loss: 0.00137442
Iteration 16/25 | Loss: 0.00137394
Iteration 17/25 | Loss: 0.00139325
Iteration 18/25 | Loss: 0.00140966
Iteration 19/25 | Loss: 0.00142229
Iteration 20/25 | Loss: 0.00136133
Iteration 21/25 | Loss: 0.00134225
Iteration 22/25 | Loss: 0.00133957
Iteration 23/25 | Loss: 0.00133812
Iteration 24/25 | Loss: 0.00133997
Iteration 25/25 | Loss: 0.00133556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31348157
Iteration 2/25 | Loss: 0.00096993
Iteration 3/25 | Loss: 0.00096993
Iteration 4/25 | Loss: 0.00096993
Iteration 5/25 | Loss: 0.00096993
Iteration 6/25 | Loss: 0.00096993
Iteration 7/25 | Loss: 0.00096993
Iteration 8/25 | Loss: 0.00096993
Iteration 9/25 | Loss: 0.00096993
Iteration 10/25 | Loss: 0.00096993
Iteration 11/25 | Loss: 0.00096993
Iteration 12/25 | Loss: 0.00096993
Iteration 13/25 | Loss: 0.00096993
Iteration 14/25 | Loss: 0.00096993
Iteration 15/25 | Loss: 0.00096993
Iteration 16/25 | Loss: 0.00096993
Iteration 17/25 | Loss: 0.00096993
Iteration 18/25 | Loss: 0.00096993
Iteration 19/25 | Loss: 0.00096993
Iteration 20/25 | Loss: 0.00096993
Iteration 21/25 | Loss: 0.00096993
Iteration 22/25 | Loss: 0.00096993
Iteration 23/25 | Loss: 0.00096993
Iteration 24/25 | Loss: 0.00096993
Iteration 25/25 | Loss: 0.00096993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096993
Iteration 2/1000 | Loss: 0.00008628
Iteration 3/1000 | Loss: 0.00006288
Iteration 4/1000 | Loss: 0.00005594
Iteration 5/1000 | Loss: 0.00005219
Iteration 6/1000 | Loss: 0.00005050
Iteration 7/1000 | Loss: 0.00004863
Iteration 8/1000 | Loss: 0.00004695
Iteration 9/1000 | Loss: 0.00004598
Iteration 10/1000 | Loss: 0.00004514
Iteration 11/1000 | Loss: 0.00004427
Iteration 12/1000 | Loss: 0.00004355
Iteration 13/1000 | Loss: 0.00004320
Iteration 14/1000 | Loss: 0.00014223
Iteration 15/1000 | Loss: 0.00063897
Iteration 16/1000 | Loss: 0.00089881
Iteration 17/1000 | Loss: 0.00095891
Iteration 18/1000 | Loss: 0.00058142
Iteration 19/1000 | Loss: 0.00047234
Iteration 20/1000 | Loss: 0.00027788
Iteration 21/1000 | Loss: 0.00041309
Iteration 22/1000 | Loss: 0.00036336
Iteration 23/1000 | Loss: 0.00047568
Iteration 24/1000 | Loss: 0.00031047
Iteration 25/1000 | Loss: 0.00020470
Iteration 26/1000 | Loss: 0.00013750
Iteration 27/1000 | Loss: 0.00007512
Iteration 28/1000 | Loss: 0.00006573
Iteration 29/1000 | Loss: 0.00026277
Iteration 30/1000 | Loss: 0.00021691
Iteration 31/1000 | Loss: 0.00005146
Iteration 32/1000 | Loss: 0.00011015
Iteration 33/1000 | Loss: 0.00005414
Iteration 34/1000 | Loss: 0.00010807
Iteration 35/1000 | Loss: 0.00004455
Iteration 36/1000 | Loss: 0.00004074
Iteration 37/1000 | Loss: 0.00003893
Iteration 38/1000 | Loss: 0.00003728
Iteration 39/1000 | Loss: 0.00003596
Iteration 40/1000 | Loss: 0.00003514
Iteration 41/1000 | Loss: 0.00003456
Iteration 42/1000 | Loss: 0.00003392
Iteration 43/1000 | Loss: 0.00003341
Iteration 44/1000 | Loss: 0.00003298
Iteration 45/1000 | Loss: 0.00003270
Iteration 46/1000 | Loss: 0.00011332
Iteration 47/1000 | Loss: 0.00003435
Iteration 48/1000 | Loss: 0.00003285
Iteration 49/1000 | Loss: 0.00003201
Iteration 50/1000 | Loss: 0.00003181
Iteration 51/1000 | Loss: 0.00003176
Iteration 52/1000 | Loss: 0.00003171
Iteration 53/1000 | Loss: 0.00003171
Iteration 54/1000 | Loss: 0.00003170
Iteration 55/1000 | Loss: 0.00003170
Iteration 56/1000 | Loss: 0.00003169
Iteration 57/1000 | Loss: 0.00003168
Iteration 58/1000 | Loss: 0.00003167
Iteration 59/1000 | Loss: 0.00003166
Iteration 60/1000 | Loss: 0.00003165
Iteration 61/1000 | Loss: 0.00003165
Iteration 62/1000 | Loss: 0.00003164
Iteration 63/1000 | Loss: 0.00003164
Iteration 64/1000 | Loss: 0.00003163
Iteration 65/1000 | Loss: 0.00003163
Iteration 66/1000 | Loss: 0.00003162
Iteration 67/1000 | Loss: 0.00003162
Iteration 68/1000 | Loss: 0.00003161
Iteration 69/1000 | Loss: 0.00003161
Iteration 70/1000 | Loss: 0.00003160
Iteration 71/1000 | Loss: 0.00003160
Iteration 72/1000 | Loss: 0.00003159
Iteration 73/1000 | Loss: 0.00003159
Iteration 74/1000 | Loss: 0.00003158
Iteration 75/1000 | Loss: 0.00003158
Iteration 76/1000 | Loss: 0.00003158
Iteration 77/1000 | Loss: 0.00003157
Iteration 78/1000 | Loss: 0.00003157
Iteration 79/1000 | Loss: 0.00003157
Iteration 80/1000 | Loss: 0.00003157
Iteration 81/1000 | Loss: 0.00003156
Iteration 82/1000 | Loss: 0.00003156
Iteration 83/1000 | Loss: 0.00003155
Iteration 84/1000 | Loss: 0.00003155
Iteration 85/1000 | Loss: 0.00003154
Iteration 86/1000 | Loss: 0.00003154
Iteration 87/1000 | Loss: 0.00003154
Iteration 88/1000 | Loss: 0.00003154
Iteration 89/1000 | Loss: 0.00003154
Iteration 90/1000 | Loss: 0.00003154
Iteration 91/1000 | Loss: 0.00003154
Iteration 92/1000 | Loss: 0.00003154
Iteration 93/1000 | Loss: 0.00003154
Iteration 94/1000 | Loss: 0.00003152
Iteration 95/1000 | Loss: 0.00003152
Iteration 96/1000 | Loss: 0.00003151
Iteration 97/1000 | Loss: 0.00003151
Iteration 98/1000 | Loss: 0.00003151
Iteration 99/1000 | Loss: 0.00003151
Iteration 100/1000 | Loss: 0.00003150
Iteration 101/1000 | Loss: 0.00003149
Iteration 102/1000 | Loss: 0.00003149
Iteration 103/1000 | Loss: 0.00003149
Iteration 104/1000 | Loss: 0.00003149
Iteration 105/1000 | Loss: 0.00003149
Iteration 106/1000 | Loss: 0.00003148
Iteration 107/1000 | Loss: 0.00003148
Iteration 108/1000 | Loss: 0.00003148
Iteration 109/1000 | Loss: 0.00003148
Iteration 110/1000 | Loss: 0.00003147
Iteration 111/1000 | Loss: 0.00003145
Iteration 112/1000 | Loss: 0.00003145
Iteration 113/1000 | Loss: 0.00003145
Iteration 114/1000 | Loss: 0.00003144
Iteration 115/1000 | Loss: 0.00003144
Iteration 116/1000 | Loss: 0.00003144
Iteration 117/1000 | Loss: 0.00003144
Iteration 118/1000 | Loss: 0.00003144
Iteration 119/1000 | Loss: 0.00003144
Iteration 120/1000 | Loss: 0.00003144
Iteration 121/1000 | Loss: 0.00003144
Iteration 122/1000 | Loss: 0.00003143
Iteration 123/1000 | Loss: 0.00003143
Iteration 124/1000 | Loss: 0.00003143
Iteration 125/1000 | Loss: 0.00003142
Iteration 126/1000 | Loss: 0.00003142
Iteration 127/1000 | Loss: 0.00003142
Iteration 128/1000 | Loss: 0.00003142
Iteration 129/1000 | Loss: 0.00003142
Iteration 130/1000 | Loss: 0.00003142
Iteration 131/1000 | Loss: 0.00003142
Iteration 132/1000 | Loss: 0.00003141
Iteration 133/1000 | Loss: 0.00003141
Iteration 134/1000 | Loss: 0.00003141
Iteration 135/1000 | Loss: 0.00003141
Iteration 136/1000 | Loss: 0.00003141
Iteration 137/1000 | Loss: 0.00003141
Iteration 138/1000 | Loss: 0.00003141
Iteration 139/1000 | Loss: 0.00003141
Iteration 140/1000 | Loss: 0.00003141
Iteration 141/1000 | Loss: 0.00003141
Iteration 142/1000 | Loss: 0.00003141
Iteration 143/1000 | Loss: 0.00003141
Iteration 144/1000 | Loss: 0.00003141
Iteration 145/1000 | Loss: 0.00003141
Iteration 146/1000 | Loss: 0.00003141
Iteration 147/1000 | Loss: 0.00003141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [3.140676926705055e-05, 3.140676926705055e-05, 3.140676926705055e-05, 3.140676926705055e-05, 3.140676926705055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.140676926705055e-05

Optimization complete. Final v2v error: 4.161696434020996 mm

Highest mean error: 12.296468734741211 mm for frame 0

Lowest mean error: 3.459592342376709 mm for frame 32

Saving results

Total time: 138.51284646987915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820059
Iteration 2/25 | Loss: 0.00117429
Iteration 3/25 | Loss: 0.00110530
Iteration 4/25 | Loss: 0.00108616
Iteration 5/25 | Loss: 0.00107979
Iteration 6/25 | Loss: 0.00107913
Iteration 7/25 | Loss: 0.00107913
Iteration 8/25 | Loss: 0.00107913
Iteration 9/25 | Loss: 0.00107913
Iteration 10/25 | Loss: 0.00107913
Iteration 11/25 | Loss: 0.00107913
Iteration 12/25 | Loss: 0.00107913
Iteration 13/25 | Loss: 0.00107913
Iteration 14/25 | Loss: 0.00107913
Iteration 15/25 | Loss: 0.00107913
Iteration 16/25 | Loss: 0.00107913
Iteration 17/25 | Loss: 0.00107913
Iteration 18/25 | Loss: 0.00107913
Iteration 19/25 | Loss: 0.00107913
Iteration 20/25 | Loss: 0.00107913
Iteration 21/25 | Loss: 0.00107913
Iteration 22/25 | Loss: 0.00107913
Iteration 23/25 | Loss: 0.00107913
Iteration 24/25 | Loss: 0.00107913
Iteration 25/25 | Loss: 0.00107913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26261973
Iteration 2/25 | Loss: 0.00113972
Iteration 3/25 | Loss: 0.00113972
Iteration 4/25 | Loss: 0.00113972
Iteration 5/25 | Loss: 0.00113972
Iteration 6/25 | Loss: 0.00113972
Iteration 7/25 | Loss: 0.00113972
Iteration 8/25 | Loss: 0.00113972
Iteration 9/25 | Loss: 0.00113972
Iteration 10/25 | Loss: 0.00113972
Iteration 11/25 | Loss: 0.00113972
Iteration 12/25 | Loss: 0.00113972
Iteration 13/25 | Loss: 0.00113972
Iteration 14/25 | Loss: 0.00113972
Iteration 15/25 | Loss: 0.00113972
Iteration 16/25 | Loss: 0.00113972
Iteration 17/25 | Loss: 0.00113972
Iteration 18/25 | Loss: 0.00113972
Iteration 19/25 | Loss: 0.00113972
Iteration 20/25 | Loss: 0.00113972
Iteration 21/25 | Loss: 0.00113972
Iteration 22/25 | Loss: 0.00113972
Iteration 23/25 | Loss: 0.00113972
Iteration 24/25 | Loss: 0.00113972
Iteration 25/25 | Loss: 0.00113972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011397190392017365, 0.0011397190392017365, 0.0011397190392017365, 0.0011397190392017365, 0.0011397190392017365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011397190392017365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113972
Iteration 2/1000 | Loss: 0.00003007
Iteration 3/1000 | Loss: 0.00001876
Iteration 4/1000 | Loss: 0.00001553
Iteration 5/1000 | Loss: 0.00001439
Iteration 6/1000 | Loss: 0.00001318
Iteration 7/1000 | Loss: 0.00001262
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001145
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001122
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001109
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001098
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001087
Iteration 21/1000 | Loss: 0.00001086
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001085
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001084
Iteration 30/1000 | Loss: 0.00001084
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001082
Iteration 34/1000 | Loss: 0.00001082
Iteration 35/1000 | Loss: 0.00001082
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001080
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001079
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001076
Iteration 52/1000 | Loss: 0.00001076
Iteration 53/1000 | Loss: 0.00001075
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001071
Iteration 58/1000 | Loss: 0.00001071
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001070
Iteration 61/1000 | Loss: 0.00001070
Iteration 62/1000 | Loss: 0.00001069
Iteration 63/1000 | Loss: 0.00001069
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001068
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001067
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001066
Iteration 75/1000 | Loss: 0.00001066
Iteration 76/1000 | Loss: 0.00001066
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001065
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001064
Iteration 86/1000 | Loss: 0.00001064
Iteration 87/1000 | Loss: 0.00001064
Iteration 88/1000 | Loss: 0.00001064
Iteration 89/1000 | Loss: 0.00001064
Iteration 90/1000 | Loss: 0.00001064
Iteration 91/1000 | Loss: 0.00001064
Iteration 92/1000 | Loss: 0.00001064
Iteration 93/1000 | Loss: 0.00001064
Iteration 94/1000 | Loss: 0.00001064
Iteration 95/1000 | Loss: 0.00001064
Iteration 96/1000 | Loss: 0.00001064
Iteration 97/1000 | Loss: 0.00001063
Iteration 98/1000 | Loss: 0.00001063
Iteration 99/1000 | Loss: 0.00001063
Iteration 100/1000 | Loss: 0.00001063
Iteration 101/1000 | Loss: 0.00001063
Iteration 102/1000 | Loss: 0.00001062
Iteration 103/1000 | Loss: 0.00001062
Iteration 104/1000 | Loss: 0.00001062
Iteration 105/1000 | Loss: 0.00001062
Iteration 106/1000 | Loss: 0.00001062
Iteration 107/1000 | Loss: 0.00001062
Iteration 108/1000 | Loss: 0.00001061
Iteration 109/1000 | Loss: 0.00001061
Iteration 110/1000 | Loss: 0.00001061
Iteration 111/1000 | Loss: 0.00001061
Iteration 112/1000 | Loss: 0.00001061
Iteration 113/1000 | Loss: 0.00001061
Iteration 114/1000 | Loss: 0.00001061
Iteration 115/1000 | Loss: 0.00001061
Iteration 116/1000 | Loss: 0.00001061
Iteration 117/1000 | Loss: 0.00001061
Iteration 118/1000 | Loss: 0.00001061
Iteration 119/1000 | Loss: 0.00001061
Iteration 120/1000 | Loss: 0.00001061
Iteration 121/1000 | Loss: 0.00001061
Iteration 122/1000 | Loss: 0.00001061
Iteration 123/1000 | Loss: 0.00001060
Iteration 124/1000 | Loss: 0.00001060
Iteration 125/1000 | Loss: 0.00001060
Iteration 126/1000 | Loss: 0.00001060
Iteration 127/1000 | Loss: 0.00001060
Iteration 128/1000 | Loss: 0.00001060
Iteration 129/1000 | Loss: 0.00001060
Iteration 130/1000 | Loss: 0.00001060
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001060
Iteration 140/1000 | Loss: 0.00001060
Iteration 141/1000 | Loss: 0.00001060
Iteration 142/1000 | Loss: 0.00001060
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001059
Iteration 152/1000 | Loss: 0.00001059
Iteration 153/1000 | Loss: 0.00001059
Iteration 154/1000 | Loss: 0.00001059
Iteration 155/1000 | Loss: 0.00001059
Iteration 156/1000 | Loss: 0.00001059
Iteration 157/1000 | Loss: 0.00001059
Iteration 158/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.0592239050311036e-05, 1.0592239050311036e-05, 1.0592239050311036e-05, 1.0592239050311036e-05, 1.0592239050311036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0592239050311036e-05

Optimization complete. Final v2v error: 2.7905707359313965 mm

Highest mean error: 3.023402452468872 mm for frame 239

Lowest mean error: 2.535764455795288 mm for frame 51

Saving results

Total time: 42.149436712265015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00290181
Iteration 2/25 | Loss: 0.00120470
Iteration 3/25 | Loss: 0.00113462
Iteration 4/25 | Loss: 0.00111395
Iteration 5/25 | Loss: 0.00110482
Iteration 6/25 | Loss: 0.00110282
Iteration 7/25 | Loss: 0.00110170
Iteration 8/25 | Loss: 0.00110152
Iteration 9/25 | Loss: 0.00110152
Iteration 10/25 | Loss: 0.00110152
Iteration 11/25 | Loss: 0.00110152
Iteration 12/25 | Loss: 0.00110152
Iteration 13/25 | Loss: 0.00110152
Iteration 14/25 | Loss: 0.00110152
Iteration 15/25 | Loss: 0.00110152
Iteration 16/25 | Loss: 0.00110152
Iteration 17/25 | Loss: 0.00110152
Iteration 18/25 | Loss: 0.00110152
Iteration 19/25 | Loss: 0.00110152
Iteration 20/25 | Loss: 0.00110152
Iteration 21/25 | Loss: 0.00110152
Iteration 22/25 | Loss: 0.00110152
Iteration 23/25 | Loss: 0.00110152
Iteration 24/25 | Loss: 0.00110152
Iteration 25/25 | Loss: 0.00110152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22863173
Iteration 2/25 | Loss: 0.00108822
Iteration 3/25 | Loss: 0.00108821
Iteration 4/25 | Loss: 0.00108821
Iteration 5/25 | Loss: 0.00108821
Iteration 6/25 | Loss: 0.00108821
Iteration 7/25 | Loss: 0.00108821
Iteration 8/25 | Loss: 0.00108821
Iteration 9/25 | Loss: 0.00108821
Iteration 10/25 | Loss: 0.00108821
Iteration 11/25 | Loss: 0.00108821
Iteration 12/25 | Loss: 0.00108821
Iteration 13/25 | Loss: 0.00108821
Iteration 14/25 | Loss: 0.00108821
Iteration 15/25 | Loss: 0.00108821
Iteration 16/25 | Loss: 0.00108821
Iteration 17/25 | Loss: 0.00108821
Iteration 18/25 | Loss: 0.00108821
Iteration 19/25 | Loss: 0.00108821
Iteration 20/25 | Loss: 0.00108821
Iteration 21/25 | Loss: 0.00108821
Iteration 22/25 | Loss: 0.00108821
Iteration 23/25 | Loss: 0.00108821
Iteration 24/25 | Loss: 0.00108821
Iteration 25/25 | Loss: 0.00108821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108821
Iteration 2/1000 | Loss: 0.00004033
Iteration 3/1000 | Loss: 0.00002503
Iteration 4/1000 | Loss: 0.00001999
Iteration 5/1000 | Loss: 0.00001865
Iteration 6/1000 | Loss: 0.00001780
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001559
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001544
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001544
Iteration 54/1000 | Loss: 0.00001544
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001542
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001541
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001540
Iteration 68/1000 | Loss: 0.00001540
Iteration 69/1000 | Loss: 0.00001540
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001534
Iteration 88/1000 | Loss: 0.00001534
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001533
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001532
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001531
Iteration 110/1000 | Loss: 0.00001531
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001530
Iteration 113/1000 | Loss: 0.00001530
Iteration 114/1000 | Loss: 0.00001530
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001529
Iteration 128/1000 | Loss: 0.00001529
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001529
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001527
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001527
Iteration 147/1000 | Loss: 0.00001527
Iteration 148/1000 | Loss: 0.00001527
Iteration 149/1000 | Loss: 0.00001527
Iteration 150/1000 | Loss: 0.00001527
Iteration 151/1000 | Loss: 0.00001527
Iteration 152/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.5267318303813227e-05, 1.5267318303813227e-05, 1.5267318303813227e-05, 1.5267318303813227e-05, 1.5267318303813227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5267318303813227e-05

Optimization complete. Final v2v error: 3.318193197250366 mm

Highest mean error: 3.7786707878112793 mm for frame 136

Lowest mean error: 3.017249584197998 mm for frame 89

Saving results

Total time: 42.57064509391785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462151
Iteration 2/25 | Loss: 0.00125153
Iteration 3/25 | Loss: 0.00118660
Iteration 4/25 | Loss: 0.00117418
Iteration 5/25 | Loss: 0.00117048
Iteration 6/25 | Loss: 0.00117048
Iteration 7/25 | Loss: 0.00117048
Iteration 8/25 | Loss: 0.00117048
Iteration 9/25 | Loss: 0.00117048
Iteration 10/25 | Loss: 0.00117048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011704834178090096, 0.0011704834178090096, 0.0011704834178090096, 0.0011704834178090096, 0.0011704834178090096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011704834178090096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35695422
Iteration 2/25 | Loss: 0.00075924
Iteration 3/25 | Loss: 0.00075924
Iteration 4/25 | Loss: 0.00075924
Iteration 5/25 | Loss: 0.00075924
Iteration 6/25 | Loss: 0.00075924
Iteration 7/25 | Loss: 0.00075924
Iteration 8/25 | Loss: 0.00075924
Iteration 9/25 | Loss: 0.00075924
Iteration 10/25 | Loss: 0.00075924
Iteration 11/25 | Loss: 0.00075924
Iteration 12/25 | Loss: 0.00075924
Iteration 13/25 | Loss: 0.00075924
Iteration 14/25 | Loss: 0.00075924
Iteration 15/25 | Loss: 0.00075924
Iteration 16/25 | Loss: 0.00075924
Iteration 17/25 | Loss: 0.00075924
Iteration 18/25 | Loss: 0.00075924
Iteration 19/25 | Loss: 0.00075924
Iteration 20/25 | Loss: 0.00075924
Iteration 21/25 | Loss: 0.00075924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007592411711812019, 0.0007592411711812019, 0.0007592411711812019, 0.0007592411711812019, 0.0007592411711812019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007592411711812019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075924
Iteration 2/1000 | Loss: 0.00002604
Iteration 3/1000 | Loss: 0.00001886
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001550
Iteration 7/1000 | Loss: 0.00001517
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001438
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001428
Iteration 18/1000 | Loss: 0.00001428
Iteration 19/1000 | Loss: 0.00001426
Iteration 20/1000 | Loss: 0.00001424
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001406
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001402
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001396
Iteration 38/1000 | Loss: 0.00001395
Iteration 39/1000 | Loss: 0.00001395
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001386
Iteration 52/1000 | Loss: 0.00001386
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001381
Iteration 65/1000 | Loss: 0.00001381
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001376
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001373
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001372
Iteration 104/1000 | Loss: 0.00001372
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001371
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001368
Iteration 131/1000 | Loss: 0.00001368
Iteration 132/1000 | Loss: 0.00001368
Iteration 133/1000 | Loss: 0.00001368
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001367
Iteration 137/1000 | Loss: 0.00001367
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001367
Iteration 140/1000 | Loss: 0.00001367
Iteration 141/1000 | Loss: 0.00001367
Iteration 142/1000 | Loss: 0.00001367
Iteration 143/1000 | Loss: 0.00001367
Iteration 144/1000 | Loss: 0.00001366
Iteration 145/1000 | Loss: 0.00001366
Iteration 146/1000 | Loss: 0.00001366
Iteration 147/1000 | Loss: 0.00001366
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001365
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001365
Iteration 162/1000 | Loss: 0.00001365
Iteration 163/1000 | Loss: 0.00001365
Iteration 164/1000 | Loss: 0.00001365
Iteration 165/1000 | Loss: 0.00001365
Iteration 166/1000 | Loss: 0.00001365
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001365
Iteration 170/1000 | Loss: 0.00001365
Iteration 171/1000 | Loss: 0.00001365
Iteration 172/1000 | Loss: 0.00001365
Iteration 173/1000 | Loss: 0.00001365
Iteration 174/1000 | Loss: 0.00001365
Iteration 175/1000 | Loss: 0.00001365
Iteration 176/1000 | Loss: 0.00001365
Iteration 177/1000 | Loss: 0.00001365
Iteration 178/1000 | Loss: 0.00001365
Iteration 179/1000 | Loss: 0.00001365
Iteration 180/1000 | Loss: 0.00001365
Iteration 181/1000 | Loss: 0.00001365
Iteration 182/1000 | Loss: 0.00001365
Iteration 183/1000 | Loss: 0.00001365
Iteration 184/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.3648819731315598e-05, 1.3648819731315598e-05, 1.3648819731315598e-05, 1.3648819731315598e-05, 1.3648819731315598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3648819731315598e-05

Optimization complete. Final v2v error: 3.081188678741455 mm

Highest mean error: 3.4352645874023438 mm for frame 191

Lowest mean error: 2.8190393447875977 mm for frame 237

Saving results

Total time: 41.79703903198242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025002
Iteration 2/25 | Loss: 0.01025002
Iteration 3/25 | Loss: 0.01025002
Iteration 4/25 | Loss: 0.01025002
Iteration 5/25 | Loss: 0.01025001
Iteration 6/25 | Loss: 0.01025001
Iteration 7/25 | Loss: 0.01025001
Iteration 8/25 | Loss: 0.01025001
Iteration 9/25 | Loss: 0.01025001
Iteration 10/25 | Loss: 0.01025001
Iteration 11/25 | Loss: 0.01025001
Iteration 12/25 | Loss: 0.01025001
Iteration 13/25 | Loss: 0.01025001
Iteration 14/25 | Loss: 0.01025000
Iteration 15/25 | Loss: 0.01025000
Iteration 16/25 | Loss: 0.01025000
Iteration 17/25 | Loss: 0.01025000
Iteration 18/25 | Loss: 0.01025000
Iteration 19/25 | Loss: 0.01025000
Iteration 20/25 | Loss: 0.01025000
Iteration 21/25 | Loss: 0.01025000
Iteration 22/25 | Loss: 0.01024999
Iteration 23/25 | Loss: 0.01024999
Iteration 24/25 | Loss: 0.01024999
Iteration 25/25 | Loss: 0.01024999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32800794
Iteration 2/25 | Loss: 0.17886609
Iteration 3/25 | Loss: 0.17859672
Iteration 4/25 | Loss: 0.17604378
Iteration 5/25 | Loss: 0.17604373
Iteration 6/25 | Loss: 0.17604373
Iteration 7/25 | Loss: 0.17604373
Iteration 8/25 | Loss: 0.17604373
Iteration 9/25 | Loss: 0.17604373
Iteration 10/25 | Loss: 0.17604373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.17604373395442963, 0.17604373395442963, 0.17604373395442963, 0.17604373395442963, 0.17604373395442963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17604373395442963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17604373
Iteration 2/1000 | Loss: 0.00784878
Iteration 3/1000 | Loss: 0.00021487
Iteration 4/1000 | Loss: 0.00301821
Iteration 5/1000 | Loss: 0.00049733
Iteration 6/1000 | Loss: 0.00149963
Iteration 7/1000 | Loss: 0.00012178
Iteration 8/1000 | Loss: 0.00003163
Iteration 9/1000 | Loss: 0.00009532
Iteration 10/1000 | Loss: 0.00002420
Iteration 11/1000 | Loss: 0.00002101
Iteration 12/1000 | Loss: 0.00001809
Iteration 13/1000 | Loss: 0.00001658
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00011303
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001167
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001143
Iteration 37/1000 | Loss: 0.00001143
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001125
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001119
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001118
Iteration 59/1000 | Loss: 0.00001118
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001116
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001114
Iteration 69/1000 | Loss: 0.00001114
Iteration 70/1000 | Loss: 0.00001114
Iteration 71/1000 | Loss: 0.00001114
Iteration 72/1000 | Loss: 0.00001114
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001108
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001106
Iteration 124/1000 | Loss: 0.00001106
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001105
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001104
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001104
Iteration 137/1000 | Loss: 0.00001104
Iteration 138/1000 | Loss: 0.00001104
Iteration 139/1000 | Loss: 0.00001104
Iteration 140/1000 | Loss: 0.00001104
Iteration 141/1000 | Loss: 0.00001104
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001101
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001100
Iteration 160/1000 | Loss: 0.00001100
Iteration 161/1000 | Loss: 0.00001099
Iteration 162/1000 | Loss: 0.00001099
Iteration 163/1000 | Loss: 0.00001099
Iteration 164/1000 | Loss: 0.00001099
Iteration 165/1000 | Loss: 0.00001099
Iteration 166/1000 | Loss: 0.00001098
Iteration 167/1000 | Loss: 0.00001098
Iteration 168/1000 | Loss: 0.00001098
Iteration 169/1000 | Loss: 0.00001098
Iteration 170/1000 | Loss: 0.00001098
Iteration 171/1000 | Loss: 0.00001098
Iteration 172/1000 | Loss: 0.00001098
Iteration 173/1000 | Loss: 0.00001098
Iteration 174/1000 | Loss: 0.00001098
Iteration 175/1000 | Loss: 0.00001098
Iteration 176/1000 | Loss: 0.00001098
Iteration 177/1000 | Loss: 0.00001098
Iteration 178/1000 | Loss: 0.00001098
Iteration 179/1000 | Loss: 0.00001098
Iteration 180/1000 | Loss: 0.00001097
Iteration 181/1000 | Loss: 0.00001097
Iteration 182/1000 | Loss: 0.00001097
Iteration 183/1000 | Loss: 0.00001097
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001095
Iteration 194/1000 | Loss: 0.00001095
Iteration 195/1000 | Loss: 0.00001095
Iteration 196/1000 | Loss: 0.00001095
Iteration 197/1000 | Loss: 0.00001095
Iteration 198/1000 | Loss: 0.00001095
Iteration 199/1000 | Loss: 0.00001095
Iteration 200/1000 | Loss: 0.00001095
Iteration 201/1000 | Loss: 0.00001095
Iteration 202/1000 | Loss: 0.00001095
Iteration 203/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.0954708159260917e-05, 1.0954708159260917e-05, 1.0954708159260917e-05, 1.0954708159260917e-05, 1.0954708159260917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0954708159260917e-05

Optimization complete. Final v2v error: 2.755280017852783 mm

Highest mean error: 8.907175064086914 mm for frame 177

Lowest mean error: 2.575749635696411 mm for frame 165

Saving results

Total time: 64.0933768749237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810968
Iteration 2/25 | Loss: 0.00127260
Iteration 3/25 | Loss: 0.00117446
Iteration 4/25 | Loss: 0.00115974
Iteration 5/25 | Loss: 0.00115535
Iteration 6/25 | Loss: 0.00115535
Iteration 7/25 | Loss: 0.00115535
Iteration 8/25 | Loss: 0.00115535
Iteration 9/25 | Loss: 0.00115535
Iteration 10/25 | Loss: 0.00115535
Iteration 11/25 | Loss: 0.00115535
Iteration 12/25 | Loss: 0.00115535
Iteration 13/25 | Loss: 0.00115535
Iteration 14/25 | Loss: 0.00115535
Iteration 15/25 | Loss: 0.00115535
Iteration 16/25 | Loss: 0.00115535
Iteration 17/25 | Loss: 0.00115535
Iteration 18/25 | Loss: 0.00115535
Iteration 19/25 | Loss: 0.00115535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011553452350199223, 0.0011553452350199223, 0.0011553452350199223, 0.0011553452350199223, 0.0011553452350199223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011553452350199223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.81484127
Iteration 2/25 | Loss: 0.00085077
Iteration 3/25 | Loss: 0.00085077
Iteration 4/25 | Loss: 0.00085077
Iteration 5/25 | Loss: 0.00085076
Iteration 6/25 | Loss: 0.00085076
Iteration 7/25 | Loss: 0.00085076
Iteration 8/25 | Loss: 0.00085076
Iteration 9/25 | Loss: 0.00085076
Iteration 10/25 | Loss: 0.00085076
Iteration 11/25 | Loss: 0.00085076
Iteration 12/25 | Loss: 0.00085076
Iteration 13/25 | Loss: 0.00085076
Iteration 14/25 | Loss: 0.00085076
Iteration 15/25 | Loss: 0.00085076
Iteration 16/25 | Loss: 0.00085076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008507636375725269, 0.0008507636375725269, 0.0008507636375725269, 0.0008507636375725269, 0.0008507636375725269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008507636375725269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085076
Iteration 2/1000 | Loss: 0.00002579
Iteration 3/1000 | Loss: 0.00002057
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001837
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001677
Iteration 9/1000 | Loss: 0.00001620
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001575
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001544
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001531
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001529
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001526
Iteration 51/1000 | Loss: 0.00001526
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001525
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001521
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001513
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001508
Iteration 103/1000 | Loss: 0.00001508
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00001504
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001501
Iteration 130/1000 | Loss: 0.00001501
Iteration 131/1000 | Loss: 0.00001501
Iteration 132/1000 | Loss: 0.00001501
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001500
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001499
Iteration 145/1000 | Loss: 0.00001499
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001497
Iteration 157/1000 | Loss: 0.00001497
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.4974511032050941e-05, 1.4974511032050941e-05, 1.4974511032050941e-05, 1.4974511032050941e-05, 1.4974511032050941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4974511032050941e-05

Optimization complete. Final v2v error: 3.273653984069824 mm

Highest mean error: 3.546144723892212 mm for frame 161

Lowest mean error: 3.0489463806152344 mm for frame 237

Saving results

Total time: 44.48805332183838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039648
Iteration 2/25 | Loss: 0.01039648
Iteration 3/25 | Loss: 0.00271603
Iteration 4/25 | Loss: 0.00175176
Iteration 5/25 | Loss: 0.00176777
Iteration 6/25 | Loss: 0.00165129
Iteration 7/25 | Loss: 0.00154308
Iteration 8/25 | Loss: 0.00150555
Iteration 9/25 | Loss: 0.00149349
Iteration 10/25 | Loss: 0.00146423
Iteration 11/25 | Loss: 0.00146210
Iteration 12/25 | Loss: 0.00143553
Iteration 13/25 | Loss: 0.00142335
Iteration 14/25 | Loss: 0.00141367
Iteration 15/25 | Loss: 0.00140862
Iteration 16/25 | Loss: 0.00140814
Iteration 17/25 | Loss: 0.00140583
Iteration 18/25 | Loss: 0.00140517
Iteration 19/25 | Loss: 0.00140633
Iteration 20/25 | Loss: 0.00140652
Iteration 21/25 | Loss: 0.00140546
Iteration 22/25 | Loss: 0.00140940
Iteration 23/25 | Loss: 0.00140596
Iteration 24/25 | Loss: 0.00140629
Iteration 25/25 | Loss: 0.00140530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32177305
Iteration 2/25 | Loss: 0.00293784
Iteration 3/25 | Loss: 0.00285854
Iteration 4/25 | Loss: 0.00285854
Iteration 5/25 | Loss: 0.00285854
Iteration 6/25 | Loss: 0.00285854
Iteration 7/25 | Loss: 0.00285854
Iteration 8/25 | Loss: 0.00285854
Iteration 9/25 | Loss: 0.00285854
Iteration 10/25 | Loss: 0.00285854
Iteration 11/25 | Loss: 0.00285854
Iteration 12/25 | Loss: 0.00285854
Iteration 13/25 | Loss: 0.00285854
Iteration 14/25 | Loss: 0.00285854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0028585358522832394, 0.0028585358522832394, 0.0028585358522832394, 0.0028585358522832394, 0.0028585358522832394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028585358522832394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285854
Iteration 2/1000 | Loss: 0.00363224
Iteration 3/1000 | Loss: 0.00380953
Iteration 4/1000 | Loss: 0.00393172
Iteration 5/1000 | Loss: 0.00387026
Iteration 6/1000 | Loss: 0.00269720
Iteration 7/1000 | Loss: 0.00469901
Iteration 8/1000 | Loss: 0.00316609
Iteration 9/1000 | Loss: 0.00178512
Iteration 10/1000 | Loss: 0.00204641
Iteration 11/1000 | Loss: 0.00197142
Iteration 12/1000 | Loss: 0.00073480
Iteration 13/1000 | Loss: 0.00316647
Iteration 14/1000 | Loss: 0.00090319
Iteration 15/1000 | Loss: 0.00123689
Iteration 16/1000 | Loss: 0.00033351
Iteration 17/1000 | Loss: 0.00021935
Iteration 18/1000 | Loss: 0.00135745
Iteration 19/1000 | Loss: 0.00029625
Iteration 20/1000 | Loss: 0.00039605
Iteration 21/1000 | Loss: 0.00016452
Iteration 22/1000 | Loss: 0.00025067
Iteration 23/1000 | Loss: 0.00026558
Iteration 24/1000 | Loss: 0.00018072
Iteration 25/1000 | Loss: 0.00045699
Iteration 26/1000 | Loss: 0.00415419
Iteration 27/1000 | Loss: 0.00808979
Iteration 28/1000 | Loss: 0.00446301
Iteration 29/1000 | Loss: 0.00323543
Iteration 30/1000 | Loss: 0.00263161
Iteration 31/1000 | Loss: 0.00158190
Iteration 32/1000 | Loss: 0.00104730
Iteration 33/1000 | Loss: 0.00155616
Iteration 34/1000 | Loss: 0.00020489
Iteration 35/1000 | Loss: 0.00025388
Iteration 36/1000 | Loss: 0.00089853
Iteration 37/1000 | Loss: 0.00085334
Iteration 38/1000 | Loss: 0.00099832
Iteration 39/1000 | Loss: 0.00058260
Iteration 40/1000 | Loss: 0.00006066
Iteration 41/1000 | Loss: 0.00007974
Iteration 42/1000 | Loss: 0.00026108
Iteration 43/1000 | Loss: 0.00011979
Iteration 44/1000 | Loss: 0.00010399
Iteration 45/1000 | Loss: 0.00004061
Iteration 46/1000 | Loss: 0.00004589
Iteration 47/1000 | Loss: 0.00052331
Iteration 48/1000 | Loss: 0.00004675
Iteration 49/1000 | Loss: 0.00013908
Iteration 50/1000 | Loss: 0.00003903
Iteration 51/1000 | Loss: 0.00050158
Iteration 52/1000 | Loss: 0.00002918
Iteration 53/1000 | Loss: 0.00005714
Iteration 54/1000 | Loss: 0.00002337
Iteration 55/1000 | Loss: 0.00020372
Iteration 56/1000 | Loss: 0.00012285
Iteration 57/1000 | Loss: 0.00006492
Iteration 58/1000 | Loss: 0.00002523
Iteration 59/1000 | Loss: 0.00002173
Iteration 60/1000 | Loss: 0.00002549
Iteration 61/1000 | Loss: 0.00002112
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002484
Iteration 64/1000 | Loss: 0.00002215
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00001795
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00003139
Iteration 70/1000 | Loss: 0.00001897
Iteration 71/1000 | Loss: 0.00001741
Iteration 72/1000 | Loss: 0.00001740
Iteration 73/1000 | Loss: 0.00001740
Iteration 74/1000 | Loss: 0.00001739
Iteration 75/1000 | Loss: 0.00001739
Iteration 76/1000 | Loss: 0.00006783
Iteration 77/1000 | Loss: 0.00005118
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00002386
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00033378
Iteration 82/1000 | Loss: 0.00009387
Iteration 83/1000 | Loss: 0.00003869
Iteration 84/1000 | Loss: 0.00005951
Iteration 85/1000 | Loss: 0.00003278
Iteration 86/1000 | Loss: 0.00008477
Iteration 87/1000 | Loss: 0.00015395
Iteration 88/1000 | Loss: 0.00007216
Iteration 89/1000 | Loss: 0.00012369
Iteration 90/1000 | Loss: 0.00010967
Iteration 91/1000 | Loss: 0.00031790
Iteration 92/1000 | Loss: 0.00017180
Iteration 93/1000 | Loss: 0.00005244
Iteration 94/1000 | Loss: 0.00038100
Iteration 95/1000 | Loss: 0.00055274
Iteration 96/1000 | Loss: 0.00019681
Iteration 97/1000 | Loss: 0.00026029
Iteration 98/1000 | Loss: 0.00014849
Iteration 99/1000 | Loss: 0.00002550
Iteration 100/1000 | Loss: 0.00006467
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00005954
Iteration 103/1000 | Loss: 0.00005176
Iteration 104/1000 | Loss: 0.00004957
Iteration 105/1000 | Loss: 0.00003681
Iteration 106/1000 | Loss: 0.00026803
Iteration 107/1000 | Loss: 0.00010934
Iteration 108/1000 | Loss: 0.00015970
Iteration 109/1000 | Loss: 0.00006283
Iteration 110/1000 | Loss: 0.00003712
Iteration 111/1000 | Loss: 0.00004945
Iteration 112/1000 | Loss: 0.00028176
Iteration 113/1000 | Loss: 0.00017918
Iteration 114/1000 | Loss: 0.00007832
Iteration 115/1000 | Loss: 0.00001962
Iteration 116/1000 | Loss: 0.00001788
Iteration 117/1000 | Loss: 0.00001672
Iteration 118/1000 | Loss: 0.00011300
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00002596
Iteration 121/1000 | Loss: 0.00001941
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00002343
Iteration 124/1000 | Loss: 0.00006233
Iteration 125/1000 | Loss: 0.00001787
Iteration 126/1000 | Loss: 0.00002588
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00002070
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00002876
Iteration 132/1000 | Loss: 0.00002192
Iteration 133/1000 | Loss: 0.00001678
Iteration 134/1000 | Loss: 0.00002166
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001617
Iteration 140/1000 | Loss: 0.00001617
Iteration 141/1000 | Loss: 0.00002024
Iteration 142/1000 | Loss: 0.00001556
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00002177
Iteration 145/1000 | Loss: 0.00001501
Iteration 146/1000 | Loss: 0.00001469
Iteration 147/1000 | Loss: 0.00001469
Iteration 148/1000 | Loss: 0.00001469
Iteration 149/1000 | Loss: 0.00001469
Iteration 150/1000 | Loss: 0.00001469
Iteration 151/1000 | Loss: 0.00001469
Iteration 152/1000 | Loss: 0.00001469
Iteration 153/1000 | Loss: 0.00001598
Iteration 154/1000 | Loss: 0.00001469
Iteration 155/1000 | Loss: 0.00001469
Iteration 156/1000 | Loss: 0.00001468
Iteration 157/1000 | Loss: 0.00001468
Iteration 158/1000 | Loss: 0.00001468
Iteration 159/1000 | Loss: 0.00001468
Iteration 160/1000 | Loss: 0.00001468
Iteration 161/1000 | Loss: 0.00001468
Iteration 162/1000 | Loss: 0.00001468
Iteration 163/1000 | Loss: 0.00001468
Iteration 164/1000 | Loss: 0.00001468
Iteration 165/1000 | Loss: 0.00001468
Iteration 166/1000 | Loss: 0.00001468
Iteration 167/1000 | Loss: 0.00001467
Iteration 168/1000 | Loss: 0.00002268
Iteration 169/1000 | Loss: 0.00001730
Iteration 170/1000 | Loss: 0.00003223
Iteration 171/1000 | Loss: 0.00029009
Iteration 172/1000 | Loss: 0.00005998
Iteration 173/1000 | Loss: 0.00012585
Iteration 174/1000 | Loss: 0.00001569
Iteration 175/1000 | Loss: 0.00003460
Iteration 176/1000 | Loss: 0.00002180
Iteration 177/1000 | Loss: 0.00001458
Iteration 178/1000 | Loss: 0.00001457
Iteration 179/1000 | Loss: 0.00001457
Iteration 180/1000 | Loss: 0.00001457
Iteration 181/1000 | Loss: 0.00001457
Iteration 182/1000 | Loss: 0.00001457
Iteration 183/1000 | Loss: 0.00002138
Iteration 184/1000 | Loss: 0.00001453
Iteration 185/1000 | Loss: 0.00001453
Iteration 186/1000 | Loss: 0.00001453
Iteration 187/1000 | Loss: 0.00001453
Iteration 188/1000 | Loss: 0.00001453
Iteration 189/1000 | Loss: 0.00001453
Iteration 190/1000 | Loss: 0.00001453
Iteration 191/1000 | Loss: 0.00001452
Iteration 192/1000 | Loss: 0.00001452
Iteration 193/1000 | Loss: 0.00001451
Iteration 194/1000 | Loss: 0.00001451
Iteration 195/1000 | Loss: 0.00001451
Iteration 196/1000 | Loss: 0.00001451
Iteration 197/1000 | Loss: 0.00001451
Iteration 198/1000 | Loss: 0.00001451
Iteration 199/1000 | Loss: 0.00001451
Iteration 200/1000 | Loss: 0.00001451
Iteration 201/1000 | Loss: 0.00001451
Iteration 202/1000 | Loss: 0.00001451
Iteration 203/1000 | Loss: 0.00001451
Iteration 204/1000 | Loss: 0.00001451
Iteration 205/1000 | Loss: 0.00001451
Iteration 206/1000 | Loss: 0.00001451
Iteration 207/1000 | Loss: 0.00001451
Iteration 208/1000 | Loss: 0.00001451
Iteration 209/1000 | Loss: 0.00001451
Iteration 210/1000 | Loss: 0.00001451
Iteration 211/1000 | Loss: 0.00001451
Iteration 212/1000 | Loss: 0.00001451
Iteration 213/1000 | Loss: 0.00001451
Iteration 214/1000 | Loss: 0.00001451
Iteration 215/1000 | Loss: 0.00001451
Iteration 216/1000 | Loss: 0.00001451
Iteration 217/1000 | Loss: 0.00001451
Iteration 218/1000 | Loss: 0.00001451
Iteration 219/1000 | Loss: 0.00001451
Iteration 220/1000 | Loss: 0.00001451
Iteration 221/1000 | Loss: 0.00001451
Iteration 222/1000 | Loss: 0.00001451
Iteration 223/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.4505885701510124e-05, 1.4505885701510124e-05, 1.4505885701510124e-05, 1.4505885701510124e-05, 1.4505885701510124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4505885701510124e-05

Optimization complete. Final v2v error: 3.1391713619232178 mm

Highest mean error: 5.234200954437256 mm for frame 35

Lowest mean error: 2.6167795658111572 mm for frame 73

Saving results

Total time: 283.2595286369324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398310
Iteration 2/25 | Loss: 0.00126429
Iteration 3/25 | Loss: 0.00114332
Iteration 4/25 | Loss: 0.00113767
Iteration 5/25 | Loss: 0.00113732
Iteration 6/25 | Loss: 0.00113732
Iteration 7/25 | Loss: 0.00113732
Iteration 8/25 | Loss: 0.00113732
Iteration 9/25 | Loss: 0.00113732
Iteration 10/25 | Loss: 0.00113732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011373171582818031, 0.0011373171582818031, 0.0011373171582818031, 0.0011373171582818031, 0.0011373171582818031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011373171582818031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57345510
Iteration 2/25 | Loss: 0.00065756
Iteration 3/25 | Loss: 0.00065756
Iteration 4/25 | Loss: 0.00065756
Iteration 5/25 | Loss: 0.00065756
Iteration 6/25 | Loss: 0.00065756
Iteration 7/25 | Loss: 0.00065756
Iteration 8/25 | Loss: 0.00065756
Iteration 9/25 | Loss: 0.00065756
Iteration 10/25 | Loss: 0.00065756
Iteration 11/25 | Loss: 0.00065756
Iteration 12/25 | Loss: 0.00065755
Iteration 13/25 | Loss: 0.00065755
Iteration 14/25 | Loss: 0.00065755
Iteration 15/25 | Loss: 0.00065755
Iteration 16/25 | Loss: 0.00065755
Iteration 17/25 | Loss: 0.00065755
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006575549487024546, 0.0006575549487024546, 0.0006575549487024546, 0.0006575549487024546, 0.0006575549487024546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006575549487024546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065755
Iteration 2/1000 | Loss: 0.00002674
Iteration 3/1000 | Loss: 0.00001682
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001287
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001172
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001087
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001073
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001062
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001043
Iteration 23/1000 | Loss: 0.00001042
Iteration 24/1000 | Loss: 0.00001042
Iteration 25/1000 | Loss: 0.00001041
Iteration 26/1000 | Loss: 0.00001041
Iteration 27/1000 | Loss: 0.00001041
Iteration 28/1000 | Loss: 0.00001041
Iteration 29/1000 | Loss: 0.00001041
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001036
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001034
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001028
Iteration 55/1000 | Loss: 0.00001027
Iteration 56/1000 | Loss: 0.00001027
Iteration 57/1000 | Loss: 0.00001027
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001026
Iteration 60/1000 | Loss: 0.00001026
Iteration 61/1000 | Loss: 0.00001026
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001025
Iteration 67/1000 | Loss: 0.00001025
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001024
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001021
Iteration 76/1000 | Loss: 0.00001020
Iteration 77/1000 | Loss: 0.00001019
Iteration 78/1000 | Loss: 0.00001018
Iteration 79/1000 | Loss: 0.00001017
Iteration 80/1000 | Loss: 0.00001017
Iteration 81/1000 | Loss: 0.00001016
Iteration 82/1000 | Loss: 0.00001015
Iteration 83/1000 | Loss: 0.00001015
Iteration 84/1000 | Loss: 0.00001015
Iteration 85/1000 | Loss: 0.00001014
Iteration 86/1000 | Loss: 0.00001014
Iteration 87/1000 | Loss: 0.00001014
Iteration 88/1000 | Loss: 0.00001014
Iteration 89/1000 | Loss: 0.00001014
Iteration 90/1000 | Loss: 0.00001014
Iteration 91/1000 | Loss: 0.00001013
Iteration 92/1000 | Loss: 0.00001013
Iteration 93/1000 | Loss: 0.00001012
Iteration 94/1000 | Loss: 0.00001012
Iteration 95/1000 | Loss: 0.00001012
Iteration 96/1000 | Loss: 0.00001012
Iteration 97/1000 | Loss: 0.00001011
Iteration 98/1000 | Loss: 0.00001011
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Iteration 101/1000 | Loss: 0.00001011
Iteration 102/1000 | Loss: 0.00001011
Iteration 103/1000 | Loss: 0.00001011
Iteration 104/1000 | Loss: 0.00001011
Iteration 105/1000 | Loss: 0.00001010
Iteration 106/1000 | Loss: 0.00001010
Iteration 107/1000 | Loss: 0.00001010
Iteration 108/1000 | Loss: 0.00001010
Iteration 109/1000 | Loss: 0.00001009
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001008
Iteration 113/1000 | Loss: 0.00001008
Iteration 114/1000 | Loss: 0.00001008
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001007
Iteration 117/1000 | Loss: 0.00001007
Iteration 118/1000 | Loss: 0.00001007
Iteration 119/1000 | Loss: 0.00001007
Iteration 120/1000 | Loss: 0.00001007
Iteration 121/1000 | Loss: 0.00001007
Iteration 122/1000 | Loss: 0.00001006
Iteration 123/1000 | Loss: 0.00001006
Iteration 124/1000 | Loss: 0.00001006
Iteration 125/1000 | Loss: 0.00001006
Iteration 126/1000 | Loss: 0.00001006
Iteration 127/1000 | Loss: 0.00001006
Iteration 128/1000 | Loss: 0.00001006
Iteration 129/1000 | Loss: 0.00001006
Iteration 130/1000 | Loss: 0.00001006
Iteration 131/1000 | Loss: 0.00001006
Iteration 132/1000 | Loss: 0.00001005
Iteration 133/1000 | Loss: 0.00001005
Iteration 134/1000 | Loss: 0.00001005
Iteration 135/1000 | Loss: 0.00001005
Iteration 136/1000 | Loss: 0.00001005
Iteration 137/1000 | Loss: 0.00001004
Iteration 138/1000 | Loss: 0.00001004
Iteration 139/1000 | Loss: 0.00001003
Iteration 140/1000 | Loss: 0.00001003
Iteration 141/1000 | Loss: 0.00001003
Iteration 142/1000 | Loss: 0.00001003
Iteration 143/1000 | Loss: 0.00001003
Iteration 144/1000 | Loss: 0.00001003
Iteration 145/1000 | Loss: 0.00001003
Iteration 146/1000 | Loss: 0.00001003
Iteration 147/1000 | Loss: 0.00001003
Iteration 148/1000 | Loss: 0.00001003
Iteration 149/1000 | Loss: 0.00001003
Iteration 150/1000 | Loss: 0.00001003
Iteration 151/1000 | Loss: 0.00001002
Iteration 152/1000 | Loss: 0.00001002
Iteration 153/1000 | Loss: 0.00001002
Iteration 154/1000 | Loss: 0.00001002
Iteration 155/1000 | Loss: 0.00001002
Iteration 156/1000 | Loss: 0.00001002
Iteration 157/1000 | Loss: 0.00001002
Iteration 158/1000 | Loss: 0.00001002
Iteration 159/1000 | Loss: 0.00001002
Iteration 160/1000 | Loss: 0.00001002
Iteration 161/1000 | Loss: 0.00001002
Iteration 162/1000 | Loss: 0.00001002
Iteration 163/1000 | Loss: 0.00001001
Iteration 164/1000 | Loss: 0.00001001
Iteration 165/1000 | Loss: 0.00001001
Iteration 166/1000 | Loss: 0.00001001
Iteration 167/1000 | Loss: 0.00001001
Iteration 168/1000 | Loss: 0.00001001
Iteration 169/1000 | Loss: 0.00001001
Iteration 170/1000 | Loss: 0.00001001
Iteration 171/1000 | Loss: 0.00001001
Iteration 172/1000 | Loss: 0.00001001
Iteration 173/1000 | Loss: 0.00001001
Iteration 174/1000 | Loss: 0.00001001
Iteration 175/1000 | Loss: 0.00001001
Iteration 176/1000 | Loss: 0.00001001
Iteration 177/1000 | Loss: 0.00001001
Iteration 178/1000 | Loss: 0.00001001
Iteration 179/1000 | Loss: 0.00001000
Iteration 180/1000 | Loss: 0.00001000
Iteration 181/1000 | Loss: 0.00001000
Iteration 182/1000 | Loss: 0.00001000
Iteration 183/1000 | Loss: 0.00000999
Iteration 184/1000 | Loss: 0.00000999
Iteration 185/1000 | Loss: 0.00000999
Iteration 186/1000 | Loss: 0.00000999
Iteration 187/1000 | Loss: 0.00000999
Iteration 188/1000 | Loss: 0.00000999
Iteration 189/1000 | Loss: 0.00000998
Iteration 190/1000 | Loss: 0.00000998
Iteration 191/1000 | Loss: 0.00000998
Iteration 192/1000 | Loss: 0.00000998
Iteration 193/1000 | Loss: 0.00000998
Iteration 194/1000 | Loss: 0.00000998
Iteration 195/1000 | Loss: 0.00000998
Iteration 196/1000 | Loss: 0.00000998
Iteration 197/1000 | Loss: 0.00000998
Iteration 198/1000 | Loss: 0.00000998
Iteration 199/1000 | Loss: 0.00000998
Iteration 200/1000 | Loss: 0.00000998
Iteration 201/1000 | Loss: 0.00000998
Iteration 202/1000 | Loss: 0.00000998
Iteration 203/1000 | Loss: 0.00000998
Iteration 204/1000 | Loss: 0.00000998
Iteration 205/1000 | Loss: 0.00000998
Iteration 206/1000 | Loss: 0.00000998
Iteration 207/1000 | Loss: 0.00000998
Iteration 208/1000 | Loss: 0.00000998
Iteration 209/1000 | Loss: 0.00000998
Iteration 210/1000 | Loss: 0.00000998
Iteration 211/1000 | Loss: 0.00000998
Iteration 212/1000 | Loss: 0.00000998
Iteration 213/1000 | Loss: 0.00000998
Iteration 214/1000 | Loss: 0.00000998
Iteration 215/1000 | Loss: 0.00000998
Iteration 216/1000 | Loss: 0.00000998
Iteration 217/1000 | Loss: 0.00000998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.976708497561049e-06, 9.976708497561049e-06, 9.976708497561049e-06, 9.976708497561049e-06, 9.976708497561049e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.976708497561049e-06

Optimization complete. Final v2v error: 2.7111692428588867 mm

Highest mean error: 2.9433932304382324 mm for frame 169

Lowest mean error: 2.578364610671997 mm for frame 262

Saving results

Total time: 46.53544735908508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527812
Iteration 2/25 | Loss: 0.00138998
Iteration 3/25 | Loss: 0.00123521
Iteration 4/25 | Loss: 0.00122077
Iteration 5/25 | Loss: 0.00121676
Iteration 6/25 | Loss: 0.00121637
Iteration 7/25 | Loss: 0.00121637
Iteration 8/25 | Loss: 0.00121637
Iteration 9/25 | Loss: 0.00121637
Iteration 10/25 | Loss: 0.00121637
Iteration 11/25 | Loss: 0.00121637
Iteration 12/25 | Loss: 0.00121637
Iteration 13/25 | Loss: 0.00121637
Iteration 14/25 | Loss: 0.00121637
Iteration 15/25 | Loss: 0.00121637
Iteration 16/25 | Loss: 0.00121637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012163728242740035, 0.0012163728242740035, 0.0012163728242740035, 0.0012163728242740035, 0.0012163728242740035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012163728242740035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76528966
Iteration 2/25 | Loss: 0.00084979
Iteration 3/25 | Loss: 0.00084978
Iteration 4/25 | Loss: 0.00084978
Iteration 5/25 | Loss: 0.00084978
Iteration 6/25 | Loss: 0.00084978
Iteration 7/25 | Loss: 0.00084978
Iteration 8/25 | Loss: 0.00084978
Iteration 9/25 | Loss: 0.00084978
Iteration 10/25 | Loss: 0.00084978
Iteration 11/25 | Loss: 0.00084978
Iteration 12/25 | Loss: 0.00084978
Iteration 13/25 | Loss: 0.00084978
Iteration 14/25 | Loss: 0.00084978
Iteration 15/25 | Loss: 0.00084978
Iteration 16/25 | Loss: 0.00084978
Iteration 17/25 | Loss: 0.00084978
Iteration 18/25 | Loss: 0.00084978
Iteration 19/25 | Loss: 0.00084978
Iteration 20/25 | Loss: 0.00084978
Iteration 21/25 | Loss: 0.00084978
Iteration 22/25 | Loss: 0.00084978
Iteration 23/25 | Loss: 0.00084978
Iteration 24/25 | Loss: 0.00084978
Iteration 25/25 | Loss: 0.00084978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084978
Iteration 2/1000 | Loss: 0.00004013
Iteration 3/1000 | Loss: 0.00002044
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001655
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001360
Iteration 18/1000 | Loss: 0.00001359
Iteration 19/1000 | Loss: 0.00001359
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001345
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001310
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001306
Iteration 43/1000 | Loss: 0.00001306
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001299
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001293
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001291
Iteration 92/1000 | Loss: 0.00001291
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001290
Iteration 100/1000 | Loss: 0.00001290
Iteration 101/1000 | Loss: 0.00001290
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001288
Iteration 123/1000 | Loss: 0.00001288
Iteration 124/1000 | Loss: 0.00001288
Iteration 125/1000 | Loss: 0.00001288
Iteration 126/1000 | Loss: 0.00001288
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001287
Iteration 129/1000 | Loss: 0.00001287
Iteration 130/1000 | Loss: 0.00001287
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001286
Iteration 138/1000 | Loss: 0.00001286
Iteration 139/1000 | Loss: 0.00001286
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001286
Iteration 142/1000 | Loss: 0.00001286
Iteration 143/1000 | Loss: 0.00001286
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001285
Iteration 147/1000 | Loss: 0.00001285
Iteration 148/1000 | Loss: 0.00001285
Iteration 149/1000 | Loss: 0.00001284
Iteration 150/1000 | Loss: 0.00001284
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001284
Iteration 154/1000 | Loss: 0.00001283
Iteration 155/1000 | Loss: 0.00001283
Iteration 156/1000 | Loss: 0.00001283
Iteration 157/1000 | Loss: 0.00001282
Iteration 158/1000 | Loss: 0.00001281
Iteration 159/1000 | Loss: 0.00001281
Iteration 160/1000 | Loss: 0.00001281
Iteration 161/1000 | Loss: 0.00001281
Iteration 162/1000 | Loss: 0.00001281
Iteration 163/1000 | Loss: 0.00001280
Iteration 164/1000 | Loss: 0.00001280
Iteration 165/1000 | Loss: 0.00001280
Iteration 166/1000 | Loss: 0.00001280
Iteration 167/1000 | Loss: 0.00001280
Iteration 168/1000 | Loss: 0.00001280
Iteration 169/1000 | Loss: 0.00001280
Iteration 170/1000 | Loss: 0.00001280
Iteration 171/1000 | Loss: 0.00001279
Iteration 172/1000 | Loss: 0.00001279
Iteration 173/1000 | Loss: 0.00001279
Iteration 174/1000 | Loss: 0.00001279
Iteration 175/1000 | Loss: 0.00001279
Iteration 176/1000 | Loss: 0.00001279
Iteration 177/1000 | Loss: 0.00001279
Iteration 178/1000 | Loss: 0.00001279
Iteration 179/1000 | Loss: 0.00001279
Iteration 180/1000 | Loss: 0.00001278
Iteration 181/1000 | Loss: 0.00001278
Iteration 182/1000 | Loss: 0.00001278
Iteration 183/1000 | Loss: 0.00001278
Iteration 184/1000 | Loss: 0.00001278
Iteration 185/1000 | Loss: 0.00001278
Iteration 186/1000 | Loss: 0.00001278
Iteration 187/1000 | Loss: 0.00001278
Iteration 188/1000 | Loss: 0.00001278
Iteration 189/1000 | Loss: 0.00001278
Iteration 190/1000 | Loss: 0.00001278
Iteration 191/1000 | Loss: 0.00001278
Iteration 192/1000 | Loss: 0.00001278
Iteration 193/1000 | Loss: 0.00001278
Iteration 194/1000 | Loss: 0.00001278
Iteration 195/1000 | Loss: 0.00001278
Iteration 196/1000 | Loss: 0.00001278
Iteration 197/1000 | Loss: 0.00001277
Iteration 198/1000 | Loss: 0.00001277
Iteration 199/1000 | Loss: 0.00001277
Iteration 200/1000 | Loss: 0.00001277
Iteration 201/1000 | Loss: 0.00001277
Iteration 202/1000 | Loss: 0.00001277
Iteration 203/1000 | Loss: 0.00001277
Iteration 204/1000 | Loss: 0.00001277
Iteration 205/1000 | Loss: 0.00001277
Iteration 206/1000 | Loss: 0.00001277
Iteration 207/1000 | Loss: 0.00001277
Iteration 208/1000 | Loss: 0.00001277
Iteration 209/1000 | Loss: 0.00001277
Iteration 210/1000 | Loss: 0.00001277
Iteration 211/1000 | Loss: 0.00001277
Iteration 212/1000 | Loss: 0.00001277
Iteration 213/1000 | Loss: 0.00001277
Iteration 214/1000 | Loss: 0.00001277
Iteration 215/1000 | Loss: 0.00001277
Iteration 216/1000 | Loss: 0.00001277
Iteration 217/1000 | Loss: 0.00001277
Iteration 218/1000 | Loss: 0.00001277
Iteration 219/1000 | Loss: 0.00001277
Iteration 220/1000 | Loss: 0.00001277
Iteration 221/1000 | Loss: 0.00001277
Iteration 222/1000 | Loss: 0.00001277
Iteration 223/1000 | Loss: 0.00001277
Iteration 224/1000 | Loss: 0.00001277
Iteration 225/1000 | Loss: 0.00001277
Iteration 226/1000 | Loss: 0.00001277
Iteration 227/1000 | Loss: 0.00001277
Iteration 228/1000 | Loss: 0.00001277
Iteration 229/1000 | Loss: 0.00001277
Iteration 230/1000 | Loss: 0.00001277
Iteration 231/1000 | Loss: 0.00001277
Iteration 232/1000 | Loss: 0.00001277
Iteration 233/1000 | Loss: 0.00001277
Iteration 234/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.2772727131959982e-05, 1.2772727131959982e-05, 1.2772727131959982e-05, 1.2772727131959982e-05, 1.2772727131959982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2772727131959982e-05

Optimization complete. Final v2v error: 3.0366764068603516 mm

Highest mean error: 3.5799100399017334 mm for frame 239

Lowest mean error: 2.983088731765747 mm for frame 132

Saving results

Total time: 56.764816761016846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929793
Iteration 2/25 | Loss: 0.00268685
Iteration 3/25 | Loss: 0.00177248
Iteration 4/25 | Loss: 0.00159256
Iteration 5/25 | Loss: 0.00159014
Iteration 6/25 | Loss: 0.00159921
Iteration 7/25 | Loss: 0.00158213
Iteration 8/25 | Loss: 0.00150699
Iteration 9/25 | Loss: 0.00149971
Iteration 10/25 | Loss: 0.00149465
Iteration 11/25 | Loss: 0.00151276
Iteration 12/25 | Loss: 0.00150783
Iteration 13/25 | Loss: 0.00148950
Iteration 14/25 | Loss: 0.00149146
Iteration 15/25 | Loss: 0.00147861
Iteration 16/25 | Loss: 0.00146771
Iteration 17/25 | Loss: 0.00145928
Iteration 18/25 | Loss: 0.00145726
Iteration 19/25 | Loss: 0.00145911
Iteration 20/25 | Loss: 0.00145681
Iteration 21/25 | Loss: 0.00144248
Iteration 22/25 | Loss: 0.00143691
Iteration 23/25 | Loss: 0.00143567
Iteration 24/25 | Loss: 0.00143267
Iteration 25/25 | Loss: 0.00143247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70565033
Iteration 2/25 | Loss: 0.00334690
Iteration 3/25 | Loss: 0.00333967
Iteration 4/25 | Loss: 0.00333966
Iteration 5/25 | Loss: 0.00333966
Iteration 6/25 | Loss: 0.00333966
Iteration 7/25 | Loss: 0.00333966
Iteration 8/25 | Loss: 0.00333966
Iteration 9/25 | Loss: 0.00333966
Iteration 10/25 | Loss: 0.00333966
Iteration 11/25 | Loss: 0.00333966
Iteration 12/25 | Loss: 0.00333966
Iteration 13/25 | Loss: 0.00333966
Iteration 14/25 | Loss: 0.00333966
Iteration 15/25 | Loss: 0.00333966
Iteration 16/25 | Loss: 0.00333966
Iteration 17/25 | Loss: 0.00333966
Iteration 18/25 | Loss: 0.00333966
Iteration 19/25 | Loss: 0.00333966
Iteration 20/25 | Loss: 0.00333966
Iteration 21/25 | Loss: 0.00333966
Iteration 22/25 | Loss: 0.00333966
Iteration 23/25 | Loss: 0.00333966
Iteration 24/25 | Loss: 0.00333966
Iteration 25/25 | Loss: 0.00333966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333966
Iteration 2/1000 | Loss: 0.00066268
Iteration 3/1000 | Loss: 0.00037479
Iteration 4/1000 | Loss: 0.00037556
Iteration 5/1000 | Loss: 0.00055679
Iteration 6/1000 | Loss: 0.00131640
Iteration 7/1000 | Loss: 0.00028626
Iteration 8/1000 | Loss: 0.00040059
Iteration 9/1000 | Loss: 0.00274798
Iteration 10/1000 | Loss: 0.00172389
Iteration 11/1000 | Loss: 0.00042092
Iteration 12/1000 | Loss: 0.00053839
Iteration 13/1000 | Loss: 0.00031853
Iteration 14/1000 | Loss: 0.00029453
Iteration 15/1000 | Loss: 0.00019788
Iteration 16/1000 | Loss: 0.00076970
Iteration 17/1000 | Loss: 0.00077259
Iteration 18/1000 | Loss: 0.00048732
Iteration 19/1000 | Loss: 0.00043109
Iteration 20/1000 | Loss: 0.00041837
Iteration 21/1000 | Loss: 0.00034254
Iteration 22/1000 | Loss: 0.00035286
Iteration 23/1000 | Loss: 0.00035441
Iteration 24/1000 | Loss: 0.00100728
Iteration 25/1000 | Loss: 0.00044859
Iteration 26/1000 | Loss: 0.00058678
Iteration 27/1000 | Loss: 0.00035091
Iteration 28/1000 | Loss: 0.00039528
Iteration 29/1000 | Loss: 0.00046707
Iteration 30/1000 | Loss: 0.00029302
Iteration 31/1000 | Loss: 0.00026805
Iteration 32/1000 | Loss: 0.00058103
Iteration 33/1000 | Loss: 0.00039859
Iteration 34/1000 | Loss: 0.00155967
Iteration 35/1000 | Loss: 0.00038911
Iteration 36/1000 | Loss: 0.00046568
Iteration 37/1000 | Loss: 0.00089442
Iteration 38/1000 | Loss: 0.00069321
Iteration 39/1000 | Loss: 0.00129420
Iteration 40/1000 | Loss: 0.00069190
Iteration 41/1000 | Loss: 0.00027348
Iteration 42/1000 | Loss: 0.00029470
Iteration 43/1000 | Loss: 0.00035434
Iteration 44/1000 | Loss: 0.00016402
Iteration 45/1000 | Loss: 0.00015901
Iteration 46/1000 | Loss: 0.00062719
Iteration 47/1000 | Loss: 0.00196456
Iteration 48/1000 | Loss: 0.00277233
Iteration 49/1000 | Loss: 0.00226537
Iteration 50/1000 | Loss: 0.00128094
Iteration 51/1000 | Loss: 0.00082202
Iteration 52/1000 | Loss: 0.00032500
Iteration 53/1000 | Loss: 0.00079965
Iteration 54/1000 | Loss: 0.00024962
Iteration 55/1000 | Loss: 0.00076465
Iteration 56/1000 | Loss: 0.00107081
Iteration 57/1000 | Loss: 0.00094521
Iteration 58/1000 | Loss: 0.00030519
Iteration 59/1000 | Loss: 0.00174657
Iteration 60/1000 | Loss: 0.00025769
Iteration 61/1000 | Loss: 0.00023908
Iteration 62/1000 | Loss: 0.00024062
Iteration 63/1000 | Loss: 0.00022606
Iteration 64/1000 | Loss: 0.00173612
Iteration 65/1000 | Loss: 0.00062980
Iteration 66/1000 | Loss: 0.00014961
Iteration 67/1000 | Loss: 0.00176920
Iteration 68/1000 | Loss: 0.00133214
Iteration 69/1000 | Loss: 0.00166487
Iteration 70/1000 | Loss: 0.00118624
Iteration 71/1000 | Loss: 0.00064775
Iteration 72/1000 | Loss: 0.00017502
Iteration 73/1000 | Loss: 0.00024975
Iteration 74/1000 | Loss: 0.00020170
Iteration 75/1000 | Loss: 0.00016031
Iteration 76/1000 | Loss: 0.00069242
Iteration 77/1000 | Loss: 0.00026870
Iteration 78/1000 | Loss: 0.00077474
Iteration 79/1000 | Loss: 0.00161799
Iteration 80/1000 | Loss: 0.00057245
Iteration 81/1000 | Loss: 0.00146362
Iteration 82/1000 | Loss: 0.00060456
Iteration 83/1000 | Loss: 0.00015705
Iteration 84/1000 | Loss: 0.00013863
Iteration 85/1000 | Loss: 0.00015349
Iteration 86/1000 | Loss: 0.00071754
Iteration 87/1000 | Loss: 0.00050537
Iteration 88/1000 | Loss: 0.00031861
Iteration 89/1000 | Loss: 0.00037551
Iteration 90/1000 | Loss: 0.00023830
Iteration 91/1000 | Loss: 0.00024127
Iteration 92/1000 | Loss: 0.00026225
Iteration 93/1000 | Loss: 0.00025797
Iteration 94/1000 | Loss: 0.00018997
Iteration 95/1000 | Loss: 0.00019098
Iteration 96/1000 | Loss: 0.00118436
Iteration 97/1000 | Loss: 0.00094102
Iteration 98/1000 | Loss: 0.00109344
Iteration 99/1000 | Loss: 0.00155027
Iteration 100/1000 | Loss: 0.00041405
Iteration 101/1000 | Loss: 0.00042430
Iteration 102/1000 | Loss: 0.00015409
Iteration 103/1000 | Loss: 0.00060210
Iteration 104/1000 | Loss: 0.00052903
Iteration 105/1000 | Loss: 0.00013483
Iteration 106/1000 | Loss: 0.00014880
Iteration 107/1000 | Loss: 0.00039730
Iteration 108/1000 | Loss: 0.00029976
Iteration 109/1000 | Loss: 0.00028497
Iteration 110/1000 | Loss: 0.00042733
Iteration 111/1000 | Loss: 0.00017769
Iteration 112/1000 | Loss: 0.00041394
Iteration 113/1000 | Loss: 0.00032969
Iteration 114/1000 | Loss: 0.00013440
Iteration 115/1000 | Loss: 0.00038923
Iteration 116/1000 | Loss: 0.00027906
Iteration 117/1000 | Loss: 0.00033499
Iteration 118/1000 | Loss: 0.00044354
Iteration 119/1000 | Loss: 0.00019797
Iteration 120/1000 | Loss: 0.00013975
Iteration 121/1000 | Loss: 0.00021262
Iteration 122/1000 | Loss: 0.00025598
Iteration 123/1000 | Loss: 0.00022881
Iteration 124/1000 | Loss: 0.00023831
Iteration 125/1000 | Loss: 0.00025431
Iteration 126/1000 | Loss: 0.00026555
Iteration 127/1000 | Loss: 0.00077035
Iteration 128/1000 | Loss: 0.00036665
Iteration 129/1000 | Loss: 0.00125214
Iteration 130/1000 | Loss: 0.00067971
Iteration 131/1000 | Loss: 0.00030172
Iteration 132/1000 | Loss: 0.00047116
Iteration 133/1000 | Loss: 0.00065953
Iteration 134/1000 | Loss: 0.00040214
Iteration 135/1000 | Loss: 0.00138332
Iteration 136/1000 | Loss: 0.00118679
Iteration 137/1000 | Loss: 0.00144852
Iteration 138/1000 | Loss: 0.00161717
Iteration 139/1000 | Loss: 0.00101861
Iteration 140/1000 | Loss: 0.00112178
Iteration 141/1000 | Loss: 0.00097570
Iteration 142/1000 | Loss: 0.00089611
Iteration 143/1000 | Loss: 0.00069641
Iteration 144/1000 | Loss: 0.00051584
Iteration 145/1000 | Loss: 0.00055915
Iteration 146/1000 | Loss: 0.00044594
Iteration 147/1000 | Loss: 0.00032837
Iteration 148/1000 | Loss: 0.00026491
Iteration 149/1000 | Loss: 0.00064115
Iteration 150/1000 | Loss: 0.00034316
Iteration 151/1000 | Loss: 0.00011605
Iteration 152/1000 | Loss: 0.00010313
Iteration 153/1000 | Loss: 0.00013125
Iteration 154/1000 | Loss: 0.00019557
Iteration 155/1000 | Loss: 0.00024470
Iteration 156/1000 | Loss: 0.00186135
Iteration 157/1000 | Loss: 0.00082117
Iteration 158/1000 | Loss: 0.00143372
Iteration 159/1000 | Loss: 0.00022805
Iteration 160/1000 | Loss: 0.00012009
Iteration 161/1000 | Loss: 0.00010183
Iteration 162/1000 | Loss: 0.00011098
Iteration 163/1000 | Loss: 0.00008936
Iteration 164/1000 | Loss: 0.00062705
Iteration 165/1000 | Loss: 0.00080298
Iteration 166/1000 | Loss: 0.00071134
Iteration 167/1000 | Loss: 0.00042127
Iteration 168/1000 | Loss: 0.00021337
Iteration 169/1000 | Loss: 0.00039085
Iteration 170/1000 | Loss: 0.00018948
Iteration 171/1000 | Loss: 0.00015450
Iteration 172/1000 | Loss: 0.00017668
Iteration 173/1000 | Loss: 0.00016205
Iteration 174/1000 | Loss: 0.00020985
Iteration 175/1000 | Loss: 0.00104356
Iteration 176/1000 | Loss: 0.00048791
Iteration 177/1000 | Loss: 0.00184178
Iteration 178/1000 | Loss: 0.00028516
Iteration 179/1000 | Loss: 0.00010942
Iteration 180/1000 | Loss: 0.00024317
Iteration 181/1000 | Loss: 0.00022391
Iteration 182/1000 | Loss: 0.00023836
Iteration 183/1000 | Loss: 0.00007978
Iteration 184/1000 | Loss: 0.00006388
Iteration 185/1000 | Loss: 0.00010521
Iteration 186/1000 | Loss: 0.00007906
Iteration 187/1000 | Loss: 0.00008288
Iteration 188/1000 | Loss: 0.00009774
Iteration 189/1000 | Loss: 0.00010355
Iteration 190/1000 | Loss: 0.00008388
Iteration 191/1000 | Loss: 0.00010718
Iteration 192/1000 | Loss: 0.00009683
Iteration 193/1000 | Loss: 0.00007501
Iteration 194/1000 | Loss: 0.00008669
Iteration 195/1000 | Loss: 0.00010275
Iteration 196/1000 | Loss: 0.00011984
Iteration 197/1000 | Loss: 0.00009715
Iteration 198/1000 | Loss: 0.00009765
Iteration 199/1000 | Loss: 0.00009221
Iteration 200/1000 | Loss: 0.00010297
Iteration 201/1000 | Loss: 0.00007890
Iteration 202/1000 | Loss: 0.00011597
Iteration 203/1000 | Loss: 0.00009672
Iteration 204/1000 | Loss: 0.00008979
Iteration 205/1000 | Loss: 0.00007367
Iteration 206/1000 | Loss: 0.00008964
Iteration 207/1000 | Loss: 0.00008506
Iteration 208/1000 | Loss: 0.00010949
Iteration 209/1000 | Loss: 0.00009403
Iteration 210/1000 | Loss: 0.00008074
Iteration 211/1000 | Loss: 0.00009172
Iteration 212/1000 | Loss: 0.00008158
Iteration 213/1000 | Loss: 0.00007340
Iteration 214/1000 | Loss: 0.00008501
Iteration 215/1000 | Loss: 0.00008104
Iteration 216/1000 | Loss: 0.00009201
Iteration 217/1000 | Loss: 0.00010072
Iteration 218/1000 | Loss: 0.00007870
Iteration 219/1000 | Loss: 0.00018452
Iteration 220/1000 | Loss: 0.00013243
Iteration 221/1000 | Loss: 0.00017281
Iteration 222/1000 | Loss: 0.00006340
Iteration 223/1000 | Loss: 0.00008240
Iteration 224/1000 | Loss: 0.00007346
Iteration 225/1000 | Loss: 0.00006447
Iteration 226/1000 | Loss: 0.00008374
Iteration 227/1000 | Loss: 0.00007336
Iteration 228/1000 | Loss: 0.00005521
Iteration 229/1000 | Loss: 0.00005646
Iteration 230/1000 | Loss: 0.00005425
Iteration 231/1000 | Loss: 0.00010926
Iteration 232/1000 | Loss: 0.00009687
Iteration 233/1000 | Loss: 0.00008600
Iteration 234/1000 | Loss: 0.00006643
Iteration 235/1000 | Loss: 0.00006508
Iteration 236/1000 | Loss: 0.00006083
Iteration 237/1000 | Loss: 0.00007695
Iteration 238/1000 | Loss: 0.00008528
Iteration 239/1000 | Loss: 0.00009372
Iteration 240/1000 | Loss: 0.00008830
Iteration 241/1000 | Loss: 0.00008418
Iteration 242/1000 | Loss: 0.00008777
Iteration 243/1000 | Loss: 0.00009672
Iteration 244/1000 | Loss: 0.00008749
Iteration 245/1000 | Loss: 0.00009591
Iteration 246/1000 | Loss: 0.00008771
Iteration 247/1000 | Loss: 0.00008337
Iteration 248/1000 | Loss: 0.00006890
Iteration 249/1000 | Loss: 0.00008776
Iteration 250/1000 | Loss: 0.00007582
Iteration 251/1000 | Loss: 0.00006566
Iteration 252/1000 | Loss: 0.00007373
Iteration 253/1000 | Loss: 0.00005694
Iteration 254/1000 | Loss: 0.00006946
Iteration 255/1000 | Loss: 0.00007030
Iteration 256/1000 | Loss: 0.00009342
Iteration 257/1000 | Loss: 0.00009859
Iteration 258/1000 | Loss: 0.00020172
Iteration 259/1000 | Loss: 0.00027678
Iteration 260/1000 | Loss: 0.00013798
Iteration 261/1000 | Loss: 0.00007928
Iteration 262/1000 | Loss: 0.00006157
Iteration 263/1000 | Loss: 0.00013072
Iteration 264/1000 | Loss: 0.00010440
Iteration 265/1000 | Loss: 0.00010437
Iteration 266/1000 | Loss: 0.00009344
Iteration 267/1000 | Loss: 0.00011008
Iteration 268/1000 | Loss: 0.00021875
Iteration 269/1000 | Loss: 0.00008471
Iteration 270/1000 | Loss: 0.00012029
Iteration 271/1000 | Loss: 0.00008886
Iteration 272/1000 | Loss: 0.00015388
Iteration 273/1000 | Loss: 0.00009758
Iteration 274/1000 | Loss: 0.00014542
Iteration 275/1000 | Loss: 0.00009537
Iteration 276/1000 | Loss: 0.00014451
Iteration 277/1000 | Loss: 0.00008401
Iteration 278/1000 | Loss: 0.00018391
Iteration 279/1000 | Loss: 0.00005420
Iteration 280/1000 | Loss: 0.00004974
Iteration 281/1000 | Loss: 0.00004184
Iteration 282/1000 | Loss: 0.00004028
Iteration 283/1000 | Loss: 0.00003924
Iteration 284/1000 | Loss: 0.00003835
Iteration 285/1000 | Loss: 0.00003760
Iteration 286/1000 | Loss: 0.00003691
Iteration 287/1000 | Loss: 0.00004428
Iteration 288/1000 | Loss: 0.00003738
Iteration 289/1000 | Loss: 0.00003675
Iteration 290/1000 | Loss: 0.00003623
Iteration 291/1000 | Loss: 0.00003591
Iteration 292/1000 | Loss: 0.00003582
Iteration 293/1000 | Loss: 0.00003576
Iteration 294/1000 | Loss: 0.00003569
Iteration 295/1000 | Loss: 0.00003561
Iteration 296/1000 | Loss: 0.00003543
Iteration 297/1000 | Loss: 0.00003525
Iteration 298/1000 | Loss: 0.00003515
Iteration 299/1000 | Loss: 0.00003505
Iteration 300/1000 | Loss: 0.00003501
Iteration 301/1000 | Loss: 0.00003498
Iteration 302/1000 | Loss: 0.00003497
Iteration 303/1000 | Loss: 0.00003494
Iteration 304/1000 | Loss: 0.00003487
Iteration 305/1000 | Loss: 0.00003470
Iteration 306/1000 | Loss: 0.00003454
Iteration 307/1000 | Loss: 0.00003847
Iteration 308/1000 | Loss: 0.00003472
Iteration 309/1000 | Loss: 0.00003449
Iteration 310/1000 | Loss: 0.00003429
Iteration 311/1000 | Loss: 0.00003422
Iteration 312/1000 | Loss: 0.00003416
Iteration 313/1000 | Loss: 0.00003416
Iteration 314/1000 | Loss: 0.00003416
Iteration 315/1000 | Loss: 0.00003415
Iteration 316/1000 | Loss: 0.00003415
Iteration 317/1000 | Loss: 0.00003412
Iteration 318/1000 | Loss: 0.00003410
Iteration 319/1000 | Loss: 0.00003410
Iteration 320/1000 | Loss: 0.00003409
Iteration 321/1000 | Loss: 0.00003409
Iteration 322/1000 | Loss: 0.00003409
Iteration 323/1000 | Loss: 0.00003409
Iteration 324/1000 | Loss: 0.00003409
Iteration 325/1000 | Loss: 0.00003408
Iteration 326/1000 | Loss: 0.00003408
Iteration 327/1000 | Loss: 0.00003408
Iteration 328/1000 | Loss: 0.00003408
Iteration 329/1000 | Loss: 0.00003407
Iteration 330/1000 | Loss: 0.00003407
Iteration 331/1000 | Loss: 0.00003407
Iteration 332/1000 | Loss: 0.00003406
Iteration 333/1000 | Loss: 0.00003406
Iteration 334/1000 | Loss: 0.00003405
Iteration 335/1000 | Loss: 0.00003404
Iteration 336/1000 | Loss: 0.00003403
Iteration 337/1000 | Loss: 0.00003403
Iteration 338/1000 | Loss: 0.00003402
Iteration 339/1000 | Loss: 0.00003402
Iteration 340/1000 | Loss: 0.00003402
Iteration 341/1000 | Loss: 0.00003401
Iteration 342/1000 | Loss: 0.00003401
Iteration 343/1000 | Loss: 0.00003400
Iteration 344/1000 | Loss: 0.00003400
Iteration 345/1000 | Loss: 0.00003400
Iteration 346/1000 | Loss: 0.00003399
Iteration 347/1000 | Loss: 0.00003399
Iteration 348/1000 | Loss: 0.00003399
Iteration 349/1000 | Loss: 0.00003399
Iteration 350/1000 | Loss: 0.00003398
Iteration 351/1000 | Loss: 0.00003398
Iteration 352/1000 | Loss: 0.00003398
Iteration 353/1000 | Loss: 0.00003398
Iteration 354/1000 | Loss: 0.00003398
Iteration 355/1000 | Loss: 0.00003398
Iteration 356/1000 | Loss: 0.00003398
Iteration 357/1000 | Loss: 0.00003398
Iteration 358/1000 | Loss: 0.00003398
Iteration 359/1000 | Loss: 0.00003397
Iteration 360/1000 | Loss: 0.00003397
Iteration 361/1000 | Loss: 0.00003397
Iteration 362/1000 | Loss: 0.00003397
Iteration 363/1000 | Loss: 0.00003397
Iteration 364/1000 | Loss: 0.00003396
Iteration 365/1000 | Loss: 0.00003396
Iteration 366/1000 | Loss: 0.00003396
Iteration 367/1000 | Loss: 0.00003396
Iteration 368/1000 | Loss: 0.00003396
Iteration 369/1000 | Loss: 0.00003396
Iteration 370/1000 | Loss: 0.00003396
Iteration 371/1000 | Loss: 0.00003396
Iteration 372/1000 | Loss: 0.00003396
Iteration 373/1000 | Loss: 0.00003396
Iteration 374/1000 | Loss: 0.00003396
Iteration 375/1000 | Loss: 0.00003396
Iteration 376/1000 | Loss: 0.00003396
Iteration 377/1000 | Loss: 0.00003395
Iteration 378/1000 | Loss: 0.00003395
Iteration 379/1000 | Loss: 0.00003395
Iteration 380/1000 | Loss: 0.00003395
Iteration 381/1000 | Loss: 0.00003394
Iteration 382/1000 | Loss: 0.00003394
Iteration 383/1000 | Loss: 0.00003394
Iteration 384/1000 | Loss: 0.00003394
Iteration 385/1000 | Loss: 0.00003394
Iteration 386/1000 | Loss: 0.00003394
Iteration 387/1000 | Loss: 0.00003394
Iteration 388/1000 | Loss: 0.00003394
Iteration 389/1000 | Loss: 0.00003394
Iteration 390/1000 | Loss: 0.00003393
Iteration 391/1000 | Loss: 0.00003393
Iteration 392/1000 | Loss: 0.00003393
Iteration 393/1000 | Loss: 0.00003393
Iteration 394/1000 | Loss: 0.00003393
Iteration 395/1000 | Loss: 0.00003393
Iteration 396/1000 | Loss: 0.00003393
Iteration 397/1000 | Loss: 0.00003393
Iteration 398/1000 | Loss: 0.00003392
Iteration 399/1000 | Loss: 0.00003392
Iteration 400/1000 | Loss: 0.00003392
Iteration 401/1000 | Loss: 0.00003392
Iteration 402/1000 | Loss: 0.00003392
Iteration 403/1000 | Loss: 0.00003392
Iteration 404/1000 | Loss: 0.00003392
Iteration 405/1000 | Loss: 0.00003392
Iteration 406/1000 | Loss: 0.00003392
Iteration 407/1000 | Loss: 0.00003392
Iteration 408/1000 | Loss: 0.00003392
Iteration 409/1000 | Loss: 0.00003391
Iteration 410/1000 | Loss: 0.00003391
Iteration 411/1000 | Loss: 0.00003391
Iteration 412/1000 | Loss: 0.00003391
Iteration 413/1000 | Loss: 0.00003391
Iteration 414/1000 | Loss: 0.00003391
Iteration 415/1000 | Loss: 0.00003391
Iteration 416/1000 | Loss: 0.00003391
Iteration 417/1000 | Loss: 0.00003391
Iteration 418/1000 | Loss: 0.00003391
Iteration 419/1000 | Loss: 0.00003391
Iteration 420/1000 | Loss: 0.00003391
Iteration 421/1000 | Loss: 0.00003391
Iteration 422/1000 | Loss: 0.00003391
Iteration 423/1000 | Loss: 0.00003391
Iteration 424/1000 | Loss: 0.00003391
Iteration 425/1000 | Loss: 0.00003391
Iteration 426/1000 | Loss: 0.00003391
Iteration 427/1000 | Loss: 0.00003391
Iteration 428/1000 | Loss: 0.00003391
Iteration 429/1000 | Loss: 0.00003391
Iteration 430/1000 | Loss: 0.00003391
Iteration 431/1000 | Loss: 0.00003391
Iteration 432/1000 | Loss: 0.00003391
Iteration 433/1000 | Loss: 0.00003391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 433. Stopping optimization.
Last 5 losses: [3.3910266211023554e-05, 3.3910266211023554e-05, 3.3910266211023554e-05, 3.3910266211023554e-05, 3.3910266211023554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3910266211023554e-05

Optimization complete. Final v2v error: 3.577683210372925 mm

Highest mean error: 10.947742462158203 mm for frame 50

Lowest mean error: 2.752840042114258 mm for frame 160

Saving results

Total time: 500.9198181629181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431302
Iteration 2/25 | Loss: 0.00119591
Iteration 3/25 | Loss: 0.00113576
Iteration 4/25 | Loss: 0.00112726
Iteration 5/25 | Loss: 0.00112457
Iteration 6/25 | Loss: 0.00112455
Iteration 7/25 | Loss: 0.00112455
Iteration 8/25 | Loss: 0.00112455
Iteration 9/25 | Loss: 0.00112455
Iteration 10/25 | Loss: 0.00112455
Iteration 11/25 | Loss: 0.00112451
Iteration 12/25 | Loss: 0.00112451
Iteration 13/25 | Loss: 0.00112451
Iteration 14/25 | Loss: 0.00112451
Iteration 15/25 | Loss: 0.00112451
Iteration 16/25 | Loss: 0.00112451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011245065834373236, 0.0011245065834373236, 0.0011245065834373236, 0.0011245065834373236, 0.0011245065834373236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011245065834373236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32374001
Iteration 2/25 | Loss: 0.00077024
Iteration 3/25 | Loss: 0.00077024
Iteration 4/25 | Loss: 0.00077024
Iteration 5/25 | Loss: 0.00077024
Iteration 6/25 | Loss: 0.00077024
Iteration 7/25 | Loss: 0.00077024
Iteration 8/25 | Loss: 0.00077024
Iteration 9/25 | Loss: 0.00077024
Iteration 10/25 | Loss: 0.00077024
Iteration 11/25 | Loss: 0.00077023
Iteration 12/25 | Loss: 0.00077023
Iteration 13/25 | Loss: 0.00077023
Iteration 14/25 | Loss: 0.00077023
Iteration 15/25 | Loss: 0.00077023
Iteration 16/25 | Loss: 0.00077023
Iteration 17/25 | Loss: 0.00077023
Iteration 18/25 | Loss: 0.00077023
Iteration 19/25 | Loss: 0.00077023
Iteration 20/25 | Loss: 0.00077023
Iteration 21/25 | Loss: 0.00077023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000770234502851963, 0.000770234502851963, 0.000770234502851963, 0.000770234502851963, 0.000770234502851963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000770234502851963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077023
Iteration 2/1000 | Loss: 0.00002148
Iteration 3/1000 | Loss: 0.00001657
Iteration 4/1000 | Loss: 0.00001529
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001349
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001304
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001277
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001266
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001252
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001248
Iteration 68/1000 | Loss: 0.00001248
Iteration 69/1000 | Loss: 0.00001248
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001247
Iteration 72/1000 | Loss: 0.00001247
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001246
Iteration 75/1000 | Loss: 0.00001246
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001243
Iteration 90/1000 | Loss: 0.00001243
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001240
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001235
Iteration 120/1000 | Loss: 0.00001235
Iteration 121/1000 | Loss: 0.00001235
Iteration 122/1000 | Loss: 0.00001234
Iteration 123/1000 | Loss: 0.00001234
Iteration 124/1000 | Loss: 0.00001234
Iteration 125/1000 | Loss: 0.00001234
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001233
Iteration 129/1000 | Loss: 0.00001233
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001232
Iteration 133/1000 | Loss: 0.00001232
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Iteration 137/1000 | Loss: 0.00001232
Iteration 138/1000 | Loss: 0.00001232
Iteration 139/1000 | Loss: 0.00001232
Iteration 140/1000 | Loss: 0.00001231
Iteration 141/1000 | Loss: 0.00001231
Iteration 142/1000 | Loss: 0.00001231
Iteration 143/1000 | Loss: 0.00001231
Iteration 144/1000 | Loss: 0.00001231
Iteration 145/1000 | Loss: 0.00001231
Iteration 146/1000 | Loss: 0.00001231
Iteration 147/1000 | Loss: 0.00001231
Iteration 148/1000 | Loss: 0.00001231
Iteration 149/1000 | Loss: 0.00001231
Iteration 150/1000 | Loss: 0.00001231
Iteration 151/1000 | Loss: 0.00001231
Iteration 152/1000 | Loss: 0.00001231
Iteration 153/1000 | Loss: 0.00001231
Iteration 154/1000 | Loss: 0.00001231
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001231
Iteration 157/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2307612450968008e-05, 1.2307612450968008e-05, 1.2307612450968008e-05, 1.2307612450968008e-05, 1.2307612450968008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2307612450968008e-05

Optimization complete. Final v2v error: 2.9878711700439453 mm

Highest mean error: 3.2719132900238037 mm for frame 110

Lowest mean error: 2.82932186126709 mm for frame 3

Saving results

Total time: 40.02916407585144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784181
Iteration 2/25 | Loss: 0.00142538
Iteration 3/25 | Loss: 0.00119995
Iteration 4/25 | Loss: 0.00117690
Iteration 5/25 | Loss: 0.00117221
Iteration 6/25 | Loss: 0.00117081
Iteration 7/25 | Loss: 0.00117040
Iteration 8/25 | Loss: 0.00117028
Iteration 9/25 | Loss: 0.00117028
Iteration 10/25 | Loss: 0.00117028
Iteration 11/25 | Loss: 0.00117028
Iteration 12/25 | Loss: 0.00117028
Iteration 13/25 | Loss: 0.00117028
Iteration 14/25 | Loss: 0.00117028
Iteration 15/25 | Loss: 0.00117028
Iteration 16/25 | Loss: 0.00117028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011702801566570997, 0.0011702801566570997, 0.0011702801566570997, 0.0011702801566570997, 0.0011702801566570997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011702801566570997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60264087
Iteration 2/25 | Loss: 0.00092078
Iteration 3/25 | Loss: 0.00092078
Iteration 4/25 | Loss: 0.00092078
Iteration 5/25 | Loss: 0.00092078
Iteration 6/25 | Loss: 0.00092077
Iteration 7/25 | Loss: 0.00092077
Iteration 8/25 | Loss: 0.00092077
Iteration 9/25 | Loss: 0.00092077
Iteration 10/25 | Loss: 0.00092077
Iteration 11/25 | Loss: 0.00092077
Iteration 12/25 | Loss: 0.00092077
Iteration 13/25 | Loss: 0.00092077
Iteration 14/25 | Loss: 0.00092077
Iteration 15/25 | Loss: 0.00092077
Iteration 16/25 | Loss: 0.00092077
Iteration 17/25 | Loss: 0.00092077
Iteration 18/25 | Loss: 0.00092077
Iteration 19/25 | Loss: 0.00092077
Iteration 20/25 | Loss: 0.00092077
Iteration 21/25 | Loss: 0.00092077
Iteration 22/25 | Loss: 0.00092077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009207730181515217, 0.0009207730181515217, 0.0009207730181515217, 0.0009207730181515217, 0.0009207730181515217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009207730181515217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092077
Iteration 2/1000 | Loss: 0.00015535
Iteration 3/1000 | Loss: 0.00002442
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001892
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00015725
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00012130
Iteration 10/1000 | Loss: 0.00016111
Iteration 11/1000 | Loss: 0.00002579
Iteration 12/1000 | Loss: 0.00002125
Iteration 13/1000 | Loss: 0.00001982
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00055156
Iteration 16/1000 | Loss: 0.00003078
Iteration 17/1000 | Loss: 0.00002214
Iteration 18/1000 | Loss: 0.00001649
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00020110
Iteration 23/1000 | Loss: 0.00008604
Iteration 24/1000 | Loss: 0.00001705
Iteration 25/1000 | Loss: 0.00001605
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001541
Iteration 28/1000 | Loss: 0.00019970
Iteration 29/1000 | Loss: 0.00008063
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001461
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001457
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001455
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Iteration 75/1000 | Loss: 0.00001448
Iteration 76/1000 | Loss: 0.00001448
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001436
Iteration 129/1000 | Loss: 0.00001436
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001436
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001435
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001434
Iteration 152/1000 | Loss: 0.00001434
Iteration 153/1000 | Loss: 0.00001434
Iteration 154/1000 | Loss: 0.00001434
Iteration 155/1000 | Loss: 0.00001434
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001434
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.4341893802338745e-05, 1.4341893802338745e-05, 1.4341893802338745e-05, 1.4341893802338745e-05, 1.4341893802338745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4341893802338745e-05

Optimization complete. Final v2v error: 3.1301159858703613 mm

Highest mean error: 5.065223693847656 mm for frame 56

Lowest mean error: 2.7839736938476562 mm for frame 160

Saving results

Total time: 75.35590147972107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793567
Iteration 2/25 | Loss: 0.00130031
Iteration 3/25 | Loss: 0.00117094
Iteration 4/25 | Loss: 0.00115623
Iteration 5/25 | Loss: 0.00115083
Iteration 6/25 | Loss: 0.00114948
Iteration 7/25 | Loss: 0.00114948
Iteration 8/25 | Loss: 0.00114948
Iteration 9/25 | Loss: 0.00114948
Iteration 10/25 | Loss: 0.00114948
Iteration 11/25 | Loss: 0.00114948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011494827922433615, 0.0011494827922433615, 0.0011494827922433615, 0.0011494827922433615, 0.0011494827922433615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011494827922433615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51766968
Iteration 2/25 | Loss: 0.00096965
Iteration 3/25 | Loss: 0.00096956
Iteration 4/25 | Loss: 0.00096956
Iteration 5/25 | Loss: 0.00096956
Iteration 6/25 | Loss: 0.00096956
Iteration 7/25 | Loss: 0.00096956
Iteration 8/25 | Loss: 0.00096956
Iteration 9/25 | Loss: 0.00096956
Iteration 10/25 | Loss: 0.00096956
Iteration 11/25 | Loss: 0.00096956
Iteration 12/25 | Loss: 0.00096956
Iteration 13/25 | Loss: 0.00096956
Iteration 14/25 | Loss: 0.00096956
Iteration 15/25 | Loss: 0.00096956
Iteration 16/25 | Loss: 0.00096956
Iteration 17/25 | Loss: 0.00096956
Iteration 18/25 | Loss: 0.00096956
Iteration 19/25 | Loss: 0.00096956
Iteration 20/25 | Loss: 0.00096956
Iteration 21/25 | Loss: 0.00096956
Iteration 22/25 | Loss: 0.00096956
Iteration 23/25 | Loss: 0.00096956
Iteration 24/25 | Loss: 0.00096956
Iteration 25/25 | Loss: 0.00096956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096956
Iteration 2/1000 | Loss: 0.00004768
Iteration 3/1000 | Loss: 0.00003325
Iteration 4/1000 | Loss: 0.00002550
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002041
Iteration 8/1000 | Loss: 0.00001939
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001860
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001818
Iteration 14/1000 | Loss: 0.00001811
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001808
Iteration 17/1000 | Loss: 0.00001807
Iteration 18/1000 | Loss: 0.00001803
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001790
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001781
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001780
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001777
Iteration 36/1000 | Loss: 0.00001777
Iteration 37/1000 | Loss: 0.00001777
Iteration 38/1000 | Loss: 0.00001776
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001772
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001769
Iteration 50/1000 | Loss: 0.00001769
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001767
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001766
Iteration 59/1000 | Loss: 0.00001766
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001760
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001759
Iteration 90/1000 | Loss: 0.00001759
Iteration 91/1000 | Loss: 0.00001759
Iteration 92/1000 | Loss: 0.00001759
Iteration 93/1000 | Loss: 0.00001759
Iteration 94/1000 | Loss: 0.00001759
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001758
Iteration 100/1000 | Loss: 0.00001758
Iteration 101/1000 | Loss: 0.00001758
Iteration 102/1000 | Loss: 0.00001758
Iteration 103/1000 | Loss: 0.00001758
Iteration 104/1000 | Loss: 0.00001758
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001757
Iteration 107/1000 | Loss: 0.00001757
Iteration 108/1000 | Loss: 0.00001757
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001757
Iteration 111/1000 | Loss: 0.00001757
Iteration 112/1000 | Loss: 0.00001756
Iteration 113/1000 | Loss: 0.00001756
Iteration 114/1000 | Loss: 0.00001756
Iteration 115/1000 | Loss: 0.00001756
Iteration 116/1000 | Loss: 0.00001756
Iteration 117/1000 | Loss: 0.00001756
Iteration 118/1000 | Loss: 0.00001756
Iteration 119/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.7564167137607e-05, 1.7564167137607e-05, 1.7564167137607e-05, 1.7564167137607e-05, 1.7564167137607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7564167137607e-05

Optimization complete. Final v2v error: 3.608818292617798 mm

Highest mean error: 3.8815488815307617 mm for frame 125

Lowest mean error: 3.2069480419158936 mm for frame 182

Saving results

Total time: 39.235403537750244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848780
Iteration 2/25 | Loss: 0.00149543
Iteration 3/25 | Loss: 0.00125797
Iteration 4/25 | Loss: 0.00122552
Iteration 5/25 | Loss: 0.00121918
Iteration 6/25 | Loss: 0.00121830
Iteration 7/25 | Loss: 0.00121829
Iteration 8/25 | Loss: 0.00121829
Iteration 9/25 | Loss: 0.00121829
Iteration 10/25 | Loss: 0.00121829
Iteration 11/25 | Loss: 0.00121829
Iteration 12/25 | Loss: 0.00121829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012182852951809764, 0.0012182852951809764, 0.0012182852951809764, 0.0012182852951809764, 0.0012182852951809764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012182852951809764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25905037
Iteration 2/25 | Loss: 0.00059398
Iteration 3/25 | Loss: 0.00059395
Iteration 4/25 | Loss: 0.00059394
Iteration 5/25 | Loss: 0.00059394
Iteration 6/25 | Loss: 0.00059394
Iteration 7/25 | Loss: 0.00059394
Iteration 8/25 | Loss: 0.00059394
Iteration 9/25 | Loss: 0.00059394
Iteration 10/25 | Loss: 0.00059394
Iteration 11/25 | Loss: 0.00059394
Iteration 12/25 | Loss: 0.00059394
Iteration 13/25 | Loss: 0.00059394
Iteration 14/25 | Loss: 0.00059394
Iteration 15/25 | Loss: 0.00059394
Iteration 16/25 | Loss: 0.00059394
Iteration 17/25 | Loss: 0.00059394
Iteration 18/25 | Loss: 0.00059394
Iteration 19/25 | Loss: 0.00059394
Iteration 20/25 | Loss: 0.00059394
Iteration 21/25 | Loss: 0.00059394
Iteration 22/25 | Loss: 0.00059394
Iteration 23/25 | Loss: 0.00059394
Iteration 24/25 | Loss: 0.00059394
Iteration 25/25 | Loss: 0.00059394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059394
Iteration 2/1000 | Loss: 0.00006133
Iteration 3/1000 | Loss: 0.00004229
Iteration 4/1000 | Loss: 0.00003465
Iteration 5/1000 | Loss: 0.00003130
Iteration 6/1000 | Loss: 0.00002990
Iteration 7/1000 | Loss: 0.00002886
Iteration 8/1000 | Loss: 0.00002814
Iteration 9/1000 | Loss: 0.00002773
Iteration 10/1000 | Loss: 0.00002739
Iteration 11/1000 | Loss: 0.00002712
Iteration 12/1000 | Loss: 0.00002678
Iteration 13/1000 | Loss: 0.00002652
Iteration 14/1000 | Loss: 0.00002628
Iteration 15/1000 | Loss: 0.00002626
Iteration 16/1000 | Loss: 0.00002624
Iteration 17/1000 | Loss: 0.00002621
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00002598
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002587
Iteration 22/1000 | Loss: 0.00002580
Iteration 23/1000 | Loss: 0.00002576
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002568
Iteration 26/1000 | Loss: 0.00002567
Iteration 27/1000 | Loss: 0.00002563
Iteration 28/1000 | Loss: 0.00002561
Iteration 29/1000 | Loss: 0.00002557
Iteration 30/1000 | Loss: 0.00002557
Iteration 31/1000 | Loss: 0.00002555
Iteration 32/1000 | Loss: 0.00002553
Iteration 33/1000 | Loss: 0.00002553
Iteration 34/1000 | Loss: 0.00002552
Iteration 35/1000 | Loss: 0.00002550
Iteration 36/1000 | Loss: 0.00002550
Iteration 37/1000 | Loss: 0.00002550
Iteration 38/1000 | Loss: 0.00002550
Iteration 39/1000 | Loss: 0.00002550
Iteration 40/1000 | Loss: 0.00002549
Iteration 41/1000 | Loss: 0.00002549
Iteration 42/1000 | Loss: 0.00002548
Iteration 43/1000 | Loss: 0.00002548
Iteration 44/1000 | Loss: 0.00002548
Iteration 45/1000 | Loss: 0.00002548
Iteration 46/1000 | Loss: 0.00002548
Iteration 47/1000 | Loss: 0.00002548
Iteration 48/1000 | Loss: 0.00002548
Iteration 49/1000 | Loss: 0.00002548
Iteration 50/1000 | Loss: 0.00002548
Iteration 51/1000 | Loss: 0.00002548
Iteration 52/1000 | Loss: 0.00002548
Iteration 53/1000 | Loss: 0.00002548
Iteration 54/1000 | Loss: 0.00002548
Iteration 55/1000 | Loss: 0.00002548
Iteration 56/1000 | Loss: 0.00002548
Iteration 57/1000 | Loss: 0.00002548
Iteration 58/1000 | Loss: 0.00002548
Iteration 59/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [2.5479665055172518e-05, 2.5479665055172518e-05, 2.5479665055172518e-05, 2.5479665055172518e-05, 2.5479665055172518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5479665055172518e-05

Optimization complete. Final v2v error: 4.17362117767334 mm

Highest mean error: 4.520508289337158 mm for frame 3

Lowest mean error: 3.9685027599334717 mm for frame 137

Saving results

Total time: 38.359851121902466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804361
Iteration 2/25 | Loss: 0.00146842
Iteration 3/25 | Loss: 0.00117954
Iteration 4/25 | Loss: 0.00114862
Iteration 5/25 | Loss: 0.00114511
Iteration 6/25 | Loss: 0.00114436
Iteration 7/25 | Loss: 0.00114416
Iteration 8/25 | Loss: 0.00114416
Iteration 9/25 | Loss: 0.00114416
Iteration 10/25 | Loss: 0.00114416
Iteration 11/25 | Loss: 0.00114416
Iteration 12/25 | Loss: 0.00114416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011441624956205487, 0.0011441624956205487, 0.0011441624956205487, 0.0011441624956205487, 0.0011441624956205487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011441624956205487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36301899
Iteration 2/25 | Loss: 0.00079776
Iteration 3/25 | Loss: 0.00079776
Iteration 4/25 | Loss: 0.00079776
Iteration 5/25 | Loss: 0.00079776
Iteration 6/25 | Loss: 0.00079776
Iteration 7/25 | Loss: 0.00079776
Iteration 8/25 | Loss: 0.00079776
Iteration 9/25 | Loss: 0.00079776
Iteration 10/25 | Loss: 0.00079776
Iteration 11/25 | Loss: 0.00079776
Iteration 12/25 | Loss: 0.00079776
Iteration 13/25 | Loss: 0.00079776
Iteration 14/25 | Loss: 0.00079776
Iteration 15/25 | Loss: 0.00079776
Iteration 16/25 | Loss: 0.00079776
Iteration 17/25 | Loss: 0.00079776
Iteration 18/25 | Loss: 0.00079776
Iteration 19/25 | Loss: 0.00079776
Iteration 20/25 | Loss: 0.00079776
Iteration 21/25 | Loss: 0.00079776
Iteration 22/25 | Loss: 0.00079776
Iteration 23/25 | Loss: 0.00079776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000797759392298758, 0.000797759392298758, 0.000797759392298758, 0.000797759392298758, 0.000797759392298758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000797759392298758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079776
Iteration 2/1000 | Loss: 0.00002466
Iteration 3/1000 | Loss: 0.00001672
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001229
Iteration 7/1000 | Loss: 0.00001181
Iteration 8/1000 | Loss: 0.00001141
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001090
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001073
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001070
Iteration 18/1000 | Loss: 0.00001068
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001066
Iteration 22/1000 | Loss: 0.00001065
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001061
Iteration 27/1000 | Loss: 0.00001061
Iteration 28/1000 | Loss: 0.00001060
Iteration 29/1000 | Loss: 0.00001059
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001058
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001057
Iteration 35/1000 | Loss: 0.00001056
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001053
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001053
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001051
Iteration 42/1000 | Loss: 0.00001050
Iteration 43/1000 | Loss: 0.00001050
Iteration 44/1000 | Loss: 0.00001050
Iteration 45/1000 | Loss: 0.00001050
Iteration 46/1000 | Loss: 0.00001050
Iteration 47/1000 | Loss: 0.00001050
Iteration 48/1000 | Loss: 0.00001049
Iteration 49/1000 | Loss: 0.00001049
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001049
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001047
Iteration 57/1000 | Loss: 0.00001047
Iteration 58/1000 | Loss: 0.00001047
Iteration 59/1000 | Loss: 0.00001047
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001045
Iteration 74/1000 | Loss: 0.00001045
Iteration 75/1000 | Loss: 0.00001045
Iteration 76/1000 | Loss: 0.00001045
Iteration 77/1000 | Loss: 0.00001045
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001044
Iteration 81/1000 | Loss: 0.00001044
Iteration 82/1000 | Loss: 0.00001044
Iteration 83/1000 | Loss: 0.00001044
Iteration 84/1000 | Loss: 0.00001044
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001043
Iteration 91/1000 | Loss: 0.00001043
Iteration 92/1000 | Loss: 0.00001043
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001042
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001042
Iteration 100/1000 | Loss: 0.00001041
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001041
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001041
Iteration 108/1000 | Loss: 0.00001041
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001041
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001039
Iteration 116/1000 | Loss: 0.00001039
Iteration 117/1000 | Loss: 0.00001039
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001038
Iteration 121/1000 | Loss: 0.00001038
Iteration 122/1000 | Loss: 0.00001038
Iteration 123/1000 | Loss: 0.00001038
Iteration 124/1000 | Loss: 0.00001038
Iteration 125/1000 | Loss: 0.00001037
Iteration 126/1000 | Loss: 0.00001037
Iteration 127/1000 | Loss: 0.00001037
Iteration 128/1000 | Loss: 0.00001037
Iteration 129/1000 | Loss: 0.00001037
Iteration 130/1000 | Loss: 0.00001036
Iteration 131/1000 | Loss: 0.00001036
Iteration 132/1000 | Loss: 0.00001036
Iteration 133/1000 | Loss: 0.00001036
Iteration 134/1000 | Loss: 0.00001035
Iteration 135/1000 | Loss: 0.00001035
Iteration 136/1000 | Loss: 0.00001035
Iteration 137/1000 | Loss: 0.00001035
Iteration 138/1000 | Loss: 0.00001035
Iteration 139/1000 | Loss: 0.00001035
Iteration 140/1000 | Loss: 0.00001035
Iteration 141/1000 | Loss: 0.00001035
Iteration 142/1000 | Loss: 0.00001034
Iteration 143/1000 | Loss: 0.00001034
Iteration 144/1000 | Loss: 0.00001034
Iteration 145/1000 | Loss: 0.00001034
Iteration 146/1000 | Loss: 0.00001034
Iteration 147/1000 | Loss: 0.00001034
Iteration 148/1000 | Loss: 0.00001034
Iteration 149/1000 | Loss: 0.00001034
Iteration 150/1000 | Loss: 0.00001034
Iteration 151/1000 | Loss: 0.00001033
Iteration 152/1000 | Loss: 0.00001033
Iteration 153/1000 | Loss: 0.00001033
Iteration 154/1000 | Loss: 0.00001033
Iteration 155/1000 | Loss: 0.00001033
Iteration 156/1000 | Loss: 0.00001033
Iteration 157/1000 | Loss: 0.00001033
Iteration 158/1000 | Loss: 0.00001033
Iteration 159/1000 | Loss: 0.00001033
Iteration 160/1000 | Loss: 0.00001033
Iteration 161/1000 | Loss: 0.00001033
Iteration 162/1000 | Loss: 0.00001032
Iteration 163/1000 | Loss: 0.00001032
Iteration 164/1000 | Loss: 0.00001032
Iteration 165/1000 | Loss: 0.00001032
Iteration 166/1000 | Loss: 0.00001032
Iteration 167/1000 | Loss: 0.00001032
Iteration 168/1000 | Loss: 0.00001032
Iteration 169/1000 | Loss: 0.00001032
Iteration 170/1000 | Loss: 0.00001032
Iteration 171/1000 | Loss: 0.00001032
Iteration 172/1000 | Loss: 0.00001032
Iteration 173/1000 | Loss: 0.00001032
Iteration 174/1000 | Loss: 0.00001032
Iteration 175/1000 | Loss: 0.00001032
Iteration 176/1000 | Loss: 0.00001031
Iteration 177/1000 | Loss: 0.00001031
Iteration 178/1000 | Loss: 0.00001031
Iteration 179/1000 | Loss: 0.00001031
Iteration 180/1000 | Loss: 0.00001031
Iteration 181/1000 | Loss: 0.00001031
Iteration 182/1000 | Loss: 0.00001031
Iteration 183/1000 | Loss: 0.00001031
Iteration 184/1000 | Loss: 0.00001031
Iteration 185/1000 | Loss: 0.00001031
Iteration 186/1000 | Loss: 0.00001031
Iteration 187/1000 | Loss: 0.00001031
Iteration 188/1000 | Loss: 0.00001030
Iteration 189/1000 | Loss: 0.00001030
Iteration 190/1000 | Loss: 0.00001030
Iteration 191/1000 | Loss: 0.00001030
Iteration 192/1000 | Loss: 0.00001030
Iteration 193/1000 | Loss: 0.00001030
Iteration 194/1000 | Loss: 0.00001030
Iteration 195/1000 | Loss: 0.00001030
Iteration 196/1000 | Loss: 0.00001030
Iteration 197/1000 | Loss: 0.00001029
Iteration 198/1000 | Loss: 0.00001029
Iteration 199/1000 | Loss: 0.00001029
Iteration 200/1000 | Loss: 0.00001028
Iteration 201/1000 | Loss: 0.00001028
Iteration 202/1000 | Loss: 0.00001028
Iteration 203/1000 | Loss: 0.00001028
Iteration 204/1000 | Loss: 0.00001028
Iteration 205/1000 | Loss: 0.00001028
Iteration 206/1000 | Loss: 0.00001028
Iteration 207/1000 | Loss: 0.00001028
Iteration 208/1000 | Loss: 0.00001028
Iteration 209/1000 | Loss: 0.00001028
Iteration 210/1000 | Loss: 0.00001027
Iteration 211/1000 | Loss: 0.00001027
Iteration 212/1000 | Loss: 0.00001027
Iteration 213/1000 | Loss: 0.00001027
Iteration 214/1000 | Loss: 0.00001026
Iteration 215/1000 | Loss: 0.00001026
Iteration 216/1000 | Loss: 0.00001026
Iteration 217/1000 | Loss: 0.00001026
Iteration 218/1000 | Loss: 0.00001026
Iteration 219/1000 | Loss: 0.00001026
Iteration 220/1000 | Loss: 0.00001026
Iteration 221/1000 | Loss: 0.00001026
Iteration 222/1000 | Loss: 0.00001026
Iteration 223/1000 | Loss: 0.00001026
Iteration 224/1000 | Loss: 0.00001026
Iteration 225/1000 | Loss: 0.00001026
Iteration 226/1000 | Loss: 0.00001026
Iteration 227/1000 | Loss: 0.00001025
Iteration 228/1000 | Loss: 0.00001025
Iteration 229/1000 | Loss: 0.00001025
Iteration 230/1000 | Loss: 0.00001025
Iteration 231/1000 | Loss: 0.00001025
Iteration 232/1000 | Loss: 0.00001025
Iteration 233/1000 | Loss: 0.00001025
Iteration 234/1000 | Loss: 0.00001025
Iteration 235/1000 | Loss: 0.00001025
Iteration 236/1000 | Loss: 0.00001024
Iteration 237/1000 | Loss: 0.00001024
Iteration 238/1000 | Loss: 0.00001024
Iteration 239/1000 | Loss: 0.00001024
Iteration 240/1000 | Loss: 0.00001024
Iteration 241/1000 | Loss: 0.00001024
Iteration 242/1000 | Loss: 0.00001024
Iteration 243/1000 | Loss: 0.00001024
Iteration 244/1000 | Loss: 0.00001024
Iteration 245/1000 | Loss: 0.00001024
Iteration 246/1000 | Loss: 0.00001024
Iteration 247/1000 | Loss: 0.00001024
Iteration 248/1000 | Loss: 0.00001024
Iteration 249/1000 | Loss: 0.00001024
Iteration 250/1000 | Loss: 0.00001024
Iteration 251/1000 | Loss: 0.00001024
Iteration 252/1000 | Loss: 0.00001024
Iteration 253/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.0236454727419186e-05, 1.0236454727419186e-05, 1.0236454727419186e-05, 1.0236454727419186e-05, 1.0236454727419186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0236454727419186e-05

Optimization complete. Final v2v error: 2.755904197692871 mm

Highest mean error: 3.03096342086792 mm for frame 62

Lowest mean error: 2.640655279159546 mm for frame 25

Saving results

Total time: 44.35653829574585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770891
Iteration 2/25 | Loss: 0.00186118
Iteration 3/25 | Loss: 0.00139801
Iteration 4/25 | Loss: 0.00131282
Iteration 5/25 | Loss: 0.00130781
Iteration 6/25 | Loss: 0.00126340
Iteration 7/25 | Loss: 0.00125032
Iteration 8/25 | Loss: 0.00124776
Iteration 9/25 | Loss: 0.00124732
Iteration 10/25 | Loss: 0.00124722
Iteration 11/25 | Loss: 0.00124722
Iteration 12/25 | Loss: 0.00124722
Iteration 13/25 | Loss: 0.00124722
Iteration 14/25 | Loss: 0.00124722
Iteration 15/25 | Loss: 0.00124722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012472209054976702, 0.0012472209054976702, 0.0012472209054976702, 0.0012472209054976702, 0.0012472209054976702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012472209054976702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32371759
Iteration 2/25 | Loss: 0.00070191
Iteration 3/25 | Loss: 0.00070189
Iteration 4/25 | Loss: 0.00070189
Iteration 5/25 | Loss: 0.00070189
Iteration 6/25 | Loss: 0.00070189
Iteration 7/25 | Loss: 0.00070189
Iteration 8/25 | Loss: 0.00070189
Iteration 9/25 | Loss: 0.00070189
Iteration 10/25 | Loss: 0.00070189
Iteration 11/25 | Loss: 0.00070189
Iteration 12/25 | Loss: 0.00070189
Iteration 13/25 | Loss: 0.00070189
Iteration 14/25 | Loss: 0.00070189
Iteration 15/25 | Loss: 0.00070189
Iteration 16/25 | Loss: 0.00070189
Iteration 17/25 | Loss: 0.00070189
Iteration 18/25 | Loss: 0.00070189
Iteration 19/25 | Loss: 0.00070189
Iteration 20/25 | Loss: 0.00070189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007018910255283117, 0.0007018910255283117, 0.0007018910255283117, 0.0007018910255283117, 0.0007018910255283117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007018910255283117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070189
Iteration 2/1000 | Loss: 0.00003635
Iteration 3/1000 | Loss: 0.00002200
Iteration 4/1000 | Loss: 0.00002009
Iteration 5/1000 | Loss: 0.00001892
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001799
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001714
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001702
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001695
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001689
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001686
Iteration 55/1000 | Loss: 0.00001686
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001686
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001686
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001682
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001681
Iteration 79/1000 | Loss: 0.00001681
Iteration 80/1000 | Loss: 0.00001681
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001680
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001679
Iteration 89/1000 | Loss: 0.00001679
Iteration 90/1000 | Loss: 0.00001679
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001678
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001676
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001676
Iteration 114/1000 | Loss: 0.00001676
Iteration 115/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.6757467165007256e-05, 1.6757467165007256e-05, 1.6757467165007256e-05, 1.6757467165007256e-05, 1.6757467165007256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6757467165007256e-05

Optimization complete. Final v2v error: 3.438321352005005 mm

Highest mean error: 3.9647998809814453 mm for frame 181

Lowest mean error: 3.2606797218322754 mm for frame 127

Saving results

Total time: 46.417686223983765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029169
Iteration 2/25 | Loss: 0.00402839
Iteration 3/25 | Loss: 0.00334968
Iteration 4/25 | Loss: 0.00329534
Iteration 5/25 | Loss: 0.00356202
Iteration 6/25 | Loss: 0.00299986
Iteration 7/25 | Loss: 0.00274968
Iteration 8/25 | Loss: 0.00259905
Iteration 9/25 | Loss: 0.00246206
Iteration 10/25 | Loss: 0.00231072
Iteration 11/25 | Loss: 0.00220805
Iteration 12/25 | Loss: 0.00221408
Iteration 13/25 | Loss: 0.00220863
Iteration 14/25 | Loss: 0.00204757
Iteration 15/25 | Loss: 0.00197152
Iteration 16/25 | Loss: 0.00191638
Iteration 17/25 | Loss: 0.00191608
Iteration 18/25 | Loss: 0.00180367
Iteration 19/25 | Loss: 0.00181023
Iteration 20/25 | Loss: 0.00180659
Iteration 21/25 | Loss: 0.00179752
Iteration 22/25 | Loss: 0.00179466
Iteration 23/25 | Loss: 0.00179567
Iteration 24/25 | Loss: 0.00178243
Iteration 25/25 | Loss: 0.00176991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08395374
Iteration 2/25 | Loss: 0.00192738
Iteration 3/25 | Loss: 0.00192738
Iteration 4/25 | Loss: 0.00192738
Iteration 5/25 | Loss: 0.00192737
Iteration 6/25 | Loss: 0.00192737
Iteration 7/25 | Loss: 0.00192737
Iteration 8/25 | Loss: 0.00192737
Iteration 9/25 | Loss: 0.00192737
Iteration 10/25 | Loss: 0.00192737
Iteration 11/25 | Loss: 0.00192737
Iteration 12/25 | Loss: 0.00192737
Iteration 13/25 | Loss: 0.00192737
Iteration 14/25 | Loss: 0.00192737
Iteration 15/25 | Loss: 0.00192737
Iteration 16/25 | Loss: 0.00192737
Iteration 17/25 | Loss: 0.00192737
Iteration 18/25 | Loss: 0.00192737
Iteration 19/25 | Loss: 0.00192737
Iteration 20/25 | Loss: 0.00192737
Iteration 21/25 | Loss: 0.00192737
Iteration 22/25 | Loss: 0.00192737
Iteration 23/25 | Loss: 0.00192737
Iteration 24/25 | Loss: 0.00192737
Iteration 25/25 | Loss: 0.00192737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192737
Iteration 2/1000 | Loss: 0.00058942
Iteration 3/1000 | Loss: 0.00017917
Iteration 4/1000 | Loss: 0.00019060
Iteration 5/1000 | Loss: 0.00016566
Iteration 6/1000 | Loss: 0.00018374
Iteration 7/1000 | Loss: 0.00017056
Iteration 8/1000 | Loss: 0.00062352
Iteration 9/1000 | Loss: 0.00035278
Iteration 10/1000 | Loss: 0.00059342
Iteration 11/1000 | Loss: 0.00025812
Iteration 12/1000 | Loss: 0.00019731
Iteration 13/1000 | Loss: 0.00017470
Iteration 14/1000 | Loss: 0.00017373
Iteration 15/1000 | Loss: 0.00044638
Iteration 16/1000 | Loss: 0.00026038
Iteration 17/1000 | Loss: 0.00025152
Iteration 18/1000 | Loss: 0.00016059
Iteration 19/1000 | Loss: 0.00016101
Iteration 20/1000 | Loss: 0.00016115
Iteration 21/1000 | Loss: 0.00016241
Iteration 22/1000 | Loss: 0.00019596
Iteration 23/1000 | Loss: 0.00016880
Iteration 24/1000 | Loss: 0.00016719
Iteration 25/1000 | Loss: 0.00015248
Iteration 26/1000 | Loss: 0.00017516
Iteration 27/1000 | Loss: 0.00017404
Iteration 28/1000 | Loss: 0.00016495
Iteration 29/1000 | Loss: 0.00039682
Iteration 30/1000 | Loss: 0.00029027
Iteration 31/1000 | Loss: 0.00025922
Iteration 32/1000 | Loss: 0.00018909
Iteration 33/1000 | Loss: 0.00018827
Iteration 34/1000 | Loss: 0.00042645
Iteration 35/1000 | Loss: 0.00287872
Iteration 36/1000 | Loss: 0.00371446
Iteration 37/1000 | Loss: 0.00132406
Iteration 38/1000 | Loss: 0.00133192
Iteration 39/1000 | Loss: 0.00152777
Iteration 40/1000 | Loss: 0.00033730
Iteration 41/1000 | Loss: 0.00013390
Iteration 42/1000 | Loss: 0.00040025
Iteration 43/1000 | Loss: 0.00039487
Iteration 44/1000 | Loss: 0.00422724
Iteration 45/1000 | Loss: 0.00263635
Iteration 46/1000 | Loss: 0.00052843
Iteration 47/1000 | Loss: 0.00048431
Iteration 48/1000 | Loss: 0.00031857
Iteration 49/1000 | Loss: 0.00031756
Iteration 50/1000 | Loss: 0.00016909
Iteration 51/1000 | Loss: 0.00021493
Iteration 52/1000 | Loss: 0.00068462
Iteration 53/1000 | Loss: 0.00025383
Iteration 54/1000 | Loss: 0.00020752
Iteration 55/1000 | Loss: 0.00008274
Iteration 56/1000 | Loss: 0.00007424
Iteration 57/1000 | Loss: 0.00006937
Iteration 58/1000 | Loss: 0.00081607
Iteration 59/1000 | Loss: 0.00051583
Iteration 60/1000 | Loss: 0.00018031
Iteration 61/1000 | Loss: 0.00011331
Iteration 62/1000 | Loss: 0.00011126
Iteration 63/1000 | Loss: 0.00008382
Iteration 64/1000 | Loss: 0.00006618
Iteration 65/1000 | Loss: 0.00006189
Iteration 66/1000 | Loss: 0.00005870
Iteration 67/1000 | Loss: 0.00005492
Iteration 68/1000 | Loss: 0.00005259
Iteration 69/1000 | Loss: 0.00005112
Iteration 70/1000 | Loss: 0.00004964
Iteration 71/1000 | Loss: 0.00029479
Iteration 72/1000 | Loss: 0.00030204
Iteration 73/1000 | Loss: 0.00006947
Iteration 74/1000 | Loss: 0.00024233
Iteration 75/1000 | Loss: 0.00037473
Iteration 76/1000 | Loss: 0.00043275
Iteration 77/1000 | Loss: 0.00015259
Iteration 78/1000 | Loss: 0.00008926
Iteration 79/1000 | Loss: 0.00006670
Iteration 80/1000 | Loss: 0.00006129
Iteration 81/1000 | Loss: 0.00005746
Iteration 82/1000 | Loss: 0.00005279
Iteration 83/1000 | Loss: 0.00025936
Iteration 84/1000 | Loss: 0.00024471
Iteration 85/1000 | Loss: 0.00008464
Iteration 86/1000 | Loss: 0.00027561
Iteration 87/1000 | Loss: 0.00007460
Iteration 88/1000 | Loss: 0.00005475
Iteration 89/1000 | Loss: 0.00005150
Iteration 90/1000 | Loss: 0.00006124
Iteration 91/1000 | Loss: 0.00005556
Iteration 92/1000 | Loss: 0.00005553
Iteration 93/1000 | Loss: 0.00027470
Iteration 94/1000 | Loss: 0.00017945
Iteration 95/1000 | Loss: 0.00006395
Iteration 96/1000 | Loss: 0.00011682
Iteration 97/1000 | Loss: 0.00016656
Iteration 98/1000 | Loss: 0.00019031
Iteration 99/1000 | Loss: 0.00016327
Iteration 100/1000 | Loss: 0.00005726
Iteration 101/1000 | Loss: 0.00005217
Iteration 102/1000 | Loss: 0.00004902
Iteration 103/1000 | Loss: 0.00004767
Iteration 104/1000 | Loss: 0.00004674
Iteration 105/1000 | Loss: 0.00004615
Iteration 106/1000 | Loss: 0.00004553
Iteration 107/1000 | Loss: 0.00004490
Iteration 108/1000 | Loss: 0.00004435
Iteration 109/1000 | Loss: 0.00004403
Iteration 110/1000 | Loss: 0.00019464
Iteration 111/1000 | Loss: 0.00004959
Iteration 112/1000 | Loss: 0.00004669
Iteration 113/1000 | Loss: 0.00007888
Iteration 114/1000 | Loss: 0.00006850
Iteration 115/1000 | Loss: 0.00005351
Iteration 116/1000 | Loss: 0.00007563
Iteration 117/1000 | Loss: 0.00005843
Iteration 118/1000 | Loss: 0.00004912
Iteration 119/1000 | Loss: 0.00004719
Iteration 120/1000 | Loss: 0.00004630
Iteration 121/1000 | Loss: 0.00004537
Iteration 122/1000 | Loss: 0.00004455
Iteration 123/1000 | Loss: 0.00004415
Iteration 124/1000 | Loss: 0.00004385
Iteration 125/1000 | Loss: 0.00004361
Iteration 126/1000 | Loss: 0.00004358
Iteration 127/1000 | Loss: 0.00004357
Iteration 128/1000 | Loss: 0.00004357
Iteration 129/1000 | Loss: 0.00004355
Iteration 130/1000 | Loss: 0.00004354
Iteration 131/1000 | Loss: 0.00004354
Iteration 132/1000 | Loss: 0.00004352
Iteration 133/1000 | Loss: 0.00004351
Iteration 134/1000 | Loss: 0.00004347
Iteration 135/1000 | Loss: 0.00004347
Iteration 136/1000 | Loss: 0.00004347
Iteration 137/1000 | Loss: 0.00004346
Iteration 138/1000 | Loss: 0.00004346
Iteration 139/1000 | Loss: 0.00004346
Iteration 140/1000 | Loss: 0.00004342
Iteration 141/1000 | Loss: 0.00004341
Iteration 142/1000 | Loss: 0.00004341
Iteration 143/1000 | Loss: 0.00004340
Iteration 144/1000 | Loss: 0.00004340
Iteration 145/1000 | Loss: 0.00004340
Iteration 146/1000 | Loss: 0.00004340
Iteration 147/1000 | Loss: 0.00004339
Iteration 148/1000 | Loss: 0.00004339
Iteration 149/1000 | Loss: 0.00004339
Iteration 150/1000 | Loss: 0.00004339
Iteration 151/1000 | Loss: 0.00004339
Iteration 152/1000 | Loss: 0.00004338
Iteration 153/1000 | Loss: 0.00004338
Iteration 154/1000 | Loss: 0.00004338
Iteration 155/1000 | Loss: 0.00004337
Iteration 156/1000 | Loss: 0.00004337
Iteration 157/1000 | Loss: 0.00004337
Iteration 158/1000 | Loss: 0.00004337
Iteration 159/1000 | Loss: 0.00004336
Iteration 160/1000 | Loss: 0.00004336
Iteration 161/1000 | Loss: 0.00004335
Iteration 162/1000 | Loss: 0.00004335
Iteration 163/1000 | Loss: 0.00004335
Iteration 164/1000 | Loss: 0.00004335
Iteration 165/1000 | Loss: 0.00004335
Iteration 166/1000 | Loss: 0.00004334
Iteration 167/1000 | Loss: 0.00004334
Iteration 168/1000 | Loss: 0.00004334
Iteration 169/1000 | Loss: 0.00004334
Iteration 170/1000 | Loss: 0.00004334
Iteration 171/1000 | Loss: 0.00004333
Iteration 172/1000 | Loss: 0.00004333
Iteration 173/1000 | Loss: 0.00004333
Iteration 174/1000 | Loss: 0.00004333
Iteration 175/1000 | Loss: 0.00004333
Iteration 176/1000 | Loss: 0.00004333
Iteration 177/1000 | Loss: 0.00004333
Iteration 178/1000 | Loss: 0.00004333
Iteration 179/1000 | Loss: 0.00004332
Iteration 180/1000 | Loss: 0.00004332
Iteration 181/1000 | Loss: 0.00004332
Iteration 182/1000 | Loss: 0.00004332
Iteration 183/1000 | Loss: 0.00004332
Iteration 184/1000 | Loss: 0.00004332
Iteration 185/1000 | Loss: 0.00004331
Iteration 186/1000 | Loss: 0.00004331
Iteration 187/1000 | Loss: 0.00004331
Iteration 188/1000 | Loss: 0.00004331
Iteration 189/1000 | Loss: 0.00004331
Iteration 190/1000 | Loss: 0.00004331
Iteration 191/1000 | Loss: 0.00004331
Iteration 192/1000 | Loss: 0.00004331
Iteration 193/1000 | Loss: 0.00004331
Iteration 194/1000 | Loss: 0.00004331
Iteration 195/1000 | Loss: 0.00004331
Iteration 196/1000 | Loss: 0.00004331
Iteration 197/1000 | Loss: 0.00004330
Iteration 198/1000 | Loss: 0.00004330
Iteration 199/1000 | Loss: 0.00004330
Iteration 200/1000 | Loss: 0.00004330
Iteration 201/1000 | Loss: 0.00004329
Iteration 202/1000 | Loss: 0.00004329
Iteration 203/1000 | Loss: 0.00004329
Iteration 204/1000 | Loss: 0.00004329
Iteration 205/1000 | Loss: 0.00004329
Iteration 206/1000 | Loss: 0.00004329
Iteration 207/1000 | Loss: 0.00004329
Iteration 208/1000 | Loss: 0.00004329
Iteration 209/1000 | Loss: 0.00004329
Iteration 210/1000 | Loss: 0.00004329
Iteration 211/1000 | Loss: 0.00004329
Iteration 212/1000 | Loss: 0.00004329
Iteration 213/1000 | Loss: 0.00004329
Iteration 214/1000 | Loss: 0.00004329
Iteration 215/1000 | Loss: 0.00004329
Iteration 216/1000 | Loss: 0.00004328
Iteration 217/1000 | Loss: 0.00004328
Iteration 218/1000 | Loss: 0.00004328
Iteration 219/1000 | Loss: 0.00004327
Iteration 220/1000 | Loss: 0.00004327
Iteration 221/1000 | Loss: 0.00004326
Iteration 222/1000 | Loss: 0.00004326
Iteration 223/1000 | Loss: 0.00004326
Iteration 224/1000 | Loss: 0.00004326
Iteration 225/1000 | Loss: 0.00004326
Iteration 226/1000 | Loss: 0.00004326
Iteration 227/1000 | Loss: 0.00004325
Iteration 228/1000 | Loss: 0.00004324
Iteration 229/1000 | Loss: 0.00004324
Iteration 230/1000 | Loss: 0.00004323
Iteration 231/1000 | Loss: 0.00004323
Iteration 232/1000 | Loss: 0.00004323
Iteration 233/1000 | Loss: 0.00004322
Iteration 234/1000 | Loss: 0.00004317
Iteration 235/1000 | Loss: 0.00004317
Iteration 236/1000 | Loss: 0.00004317
Iteration 237/1000 | Loss: 0.00004316
Iteration 238/1000 | Loss: 0.00004316
Iteration 239/1000 | Loss: 0.00004316
Iteration 240/1000 | Loss: 0.00004315
Iteration 241/1000 | Loss: 0.00004315
Iteration 242/1000 | Loss: 0.00004315
Iteration 243/1000 | Loss: 0.00004315
Iteration 244/1000 | Loss: 0.00004314
Iteration 245/1000 | Loss: 0.00004314
Iteration 246/1000 | Loss: 0.00004314
Iteration 247/1000 | Loss: 0.00004314
Iteration 248/1000 | Loss: 0.00004314
Iteration 249/1000 | Loss: 0.00004314
Iteration 250/1000 | Loss: 0.00004314
Iteration 251/1000 | Loss: 0.00004313
Iteration 252/1000 | Loss: 0.00004313
Iteration 253/1000 | Loss: 0.00004313
Iteration 254/1000 | Loss: 0.00004313
Iteration 255/1000 | Loss: 0.00004313
Iteration 256/1000 | Loss: 0.00004313
Iteration 257/1000 | Loss: 0.00004313
Iteration 258/1000 | Loss: 0.00004312
Iteration 259/1000 | Loss: 0.00004312
Iteration 260/1000 | Loss: 0.00004312
Iteration 261/1000 | Loss: 0.00004312
Iteration 262/1000 | Loss: 0.00004312
Iteration 263/1000 | Loss: 0.00004312
Iteration 264/1000 | Loss: 0.00004311
Iteration 265/1000 | Loss: 0.00004311
Iteration 266/1000 | Loss: 0.00004311
Iteration 267/1000 | Loss: 0.00004311
Iteration 268/1000 | Loss: 0.00004311
Iteration 269/1000 | Loss: 0.00004311
Iteration 270/1000 | Loss: 0.00004311
Iteration 271/1000 | Loss: 0.00004311
Iteration 272/1000 | Loss: 0.00004311
Iteration 273/1000 | Loss: 0.00004311
Iteration 274/1000 | Loss: 0.00004311
Iteration 275/1000 | Loss: 0.00004311
Iteration 276/1000 | Loss: 0.00004311
Iteration 277/1000 | Loss: 0.00004311
Iteration 278/1000 | Loss: 0.00004311
Iteration 279/1000 | Loss: 0.00004311
Iteration 280/1000 | Loss: 0.00004310
Iteration 281/1000 | Loss: 0.00004310
Iteration 282/1000 | Loss: 0.00004310
Iteration 283/1000 | Loss: 0.00004310
Iteration 284/1000 | Loss: 0.00004310
Iteration 285/1000 | Loss: 0.00004310
Iteration 286/1000 | Loss: 0.00004310
Iteration 287/1000 | Loss: 0.00004310
Iteration 288/1000 | Loss: 0.00004310
Iteration 289/1000 | Loss: 0.00004310
Iteration 290/1000 | Loss: 0.00004310
Iteration 291/1000 | Loss: 0.00004310
Iteration 292/1000 | Loss: 0.00004310
Iteration 293/1000 | Loss: 0.00004310
Iteration 294/1000 | Loss: 0.00004310
Iteration 295/1000 | Loss: 0.00004310
Iteration 296/1000 | Loss: 0.00004310
Iteration 297/1000 | Loss: 0.00004310
Iteration 298/1000 | Loss: 0.00004310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [4.309534779167734e-05, 4.309534779167734e-05, 4.309534779167734e-05, 4.309534779167734e-05, 4.309534779167734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.309534779167734e-05

Optimization complete. Final v2v error: 4.779457092285156 mm

Highest mean error: 10.471006393432617 mm for frame 50

Lowest mean error: 4.116424560546875 mm for frame 163

Saving results

Total time: 244.2110776901245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461634
Iteration 2/25 | Loss: 0.00122529
Iteration 3/25 | Loss: 0.00114806
Iteration 4/25 | Loss: 0.00114189
Iteration 5/25 | Loss: 0.00114033
Iteration 6/25 | Loss: 0.00114026
Iteration 7/25 | Loss: 0.00114026
Iteration 8/25 | Loss: 0.00114026
Iteration 9/25 | Loss: 0.00114026
Iteration 10/25 | Loss: 0.00114026
Iteration 11/25 | Loss: 0.00114026
Iteration 12/25 | Loss: 0.00114026
Iteration 13/25 | Loss: 0.00114026
Iteration 14/25 | Loss: 0.00114026
Iteration 15/25 | Loss: 0.00114026
Iteration 16/25 | Loss: 0.00114026
Iteration 17/25 | Loss: 0.00114026
Iteration 18/25 | Loss: 0.00114026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011402606032788754, 0.0011402606032788754, 0.0011402606032788754, 0.0011402606032788754, 0.0011402606032788754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011402606032788754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36030149
Iteration 2/25 | Loss: 0.00092257
Iteration 3/25 | Loss: 0.00092256
Iteration 4/25 | Loss: 0.00092256
Iteration 5/25 | Loss: 0.00092256
Iteration 6/25 | Loss: 0.00092256
Iteration 7/25 | Loss: 0.00092256
Iteration 8/25 | Loss: 0.00092255
Iteration 9/25 | Loss: 0.00092255
Iteration 10/25 | Loss: 0.00092255
Iteration 11/25 | Loss: 0.00092255
Iteration 12/25 | Loss: 0.00092255
Iteration 13/25 | Loss: 0.00092255
Iteration 14/25 | Loss: 0.00092255
Iteration 15/25 | Loss: 0.00092255
Iteration 16/25 | Loss: 0.00092255
Iteration 17/25 | Loss: 0.00092255
Iteration 18/25 | Loss: 0.00092255
Iteration 19/25 | Loss: 0.00092255
Iteration 20/25 | Loss: 0.00092255
Iteration 21/25 | Loss: 0.00092255
Iteration 22/25 | Loss: 0.00092255
Iteration 23/25 | Loss: 0.00092255
Iteration 24/25 | Loss: 0.00092255
Iteration 25/25 | Loss: 0.00092255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092255
Iteration 2/1000 | Loss: 0.00002701
Iteration 3/1000 | Loss: 0.00001725
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001255
Iteration 6/1000 | Loss: 0.00001176
Iteration 7/1000 | Loss: 0.00001131
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001085
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001077
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001075
Iteration 15/1000 | Loss: 0.00001070
Iteration 16/1000 | Loss: 0.00001063
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001058
Iteration 19/1000 | Loss: 0.00001057
Iteration 20/1000 | Loss: 0.00001056
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001053
Iteration 23/1000 | Loss: 0.00001053
Iteration 24/1000 | Loss: 0.00001052
Iteration 25/1000 | Loss: 0.00001052
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00001041
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001036
Iteration 44/1000 | Loss: 0.00001036
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001030
Iteration 54/1000 | Loss: 0.00001030
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001029
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001028
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001028
Iteration 63/1000 | Loss: 0.00001027
Iteration 64/1000 | Loss: 0.00001027
Iteration 65/1000 | Loss: 0.00001027
Iteration 66/1000 | Loss: 0.00001027
Iteration 67/1000 | Loss: 0.00001026
Iteration 68/1000 | Loss: 0.00001026
Iteration 69/1000 | Loss: 0.00001026
Iteration 70/1000 | Loss: 0.00001026
Iteration 71/1000 | Loss: 0.00001025
Iteration 72/1000 | Loss: 0.00001025
Iteration 73/1000 | Loss: 0.00001025
Iteration 74/1000 | Loss: 0.00001025
Iteration 75/1000 | Loss: 0.00001025
Iteration 76/1000 | Loss: 0.00001025
Iteration 77/1000 | Loss: 0.00001025
Iteration 78/1000 | Loss: 0.00001024
Iteration 79/1000 | Loss: 0.00001024
Iteration 80/1000 | Loss: 0.00001024
Iteration 81/1000 | Loss: 0.00001024
Iteration 82/1000 | Loss: 0.00001024
Iteration 83/1000 | Loss: 0.00001023
Iteration 84/1000 | Loss: 0.00001023
Iteration 85/1000 | Loss: 0.00001023
Iteration 86/1000 | Loss: 0.00001023
Iteration 87/1000 | Loss: 0.00001023
Iteration 88/1000 | Loss: 0.00001023
Iteration 89/1000 | Loss: 0.00001022
Iteration 90/1000 | Loss: 0.00001022
Iteration 91/1000 | Loss: 0.00001022
Iteration 92/1000 | Loss: 0.00001022
Iteration 93/1000 | Loss: 0.00001022
Iteration 94/1000 | Loss: 0.00001022
Iteration 95/1000 | Loss: 0.00001021
Iteration 96/1000 | Loss: 0.00001021
Iteration 97/1000 | Loss: 0.00001021
Iteration 98/1000 | Loss: 0.00001021
Iteration 99/1000 | Loss: 0.00001021
Iteration 100/1000 | Loss: 0.00001020
Iteration 101/1000 | Loss: 0.00001020
Iteration 102/1000 | Loss: 0.00001020
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001020
Iteration 105/1000 | Loss: 0.00001020
Iteration 106/1000 | Loss: 0.00001020
Iteration 107/1000 | Loss: 0.00001020
Iteration 108/1000 | Loss: 0.00001020
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001018
Iteration 114/1000 | Loss: 0.00001018
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001018
Iteration 121/1000 | Loss: 0.00001017
Iteration 122/1000 | Loss: 0.00001017
Iteration 123/1000 | Loss: 0.00001017
Iteration 124/1000 | Loss: 0.00001016
Iteration 125/1000 | Loss: 0.00001016
Iteration 126/1000 | Loss: 0.00001016
Iteration 127/1000 | Loss: 0.00001015
Iteration 128/1000 | Loss: 0.00001015
Iteration 129/1000 | Loss: 0.00001015
Iteration 130/1000 | Loss: 0.00001015
Iteration 131/1000 | Loss: 0.00001014
Iteration 132/1000 | Loss: 0.00001014
Iteration 133/1000 | Loss: 0.00001014
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001013
Iteration 137/1000 | Loss: 0.00001013
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001012
Iteration 140/1000 | Loss: 0.00001012
Iteration 141/1000 | Loss: 0.00001012
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001012
Iteration 146/1000 | Loss: 0.00001012
Iteration 147/1000 | Loss: 0.00001012
Iteration 148/1000 | Loss: 0.00001012
Iteration 149/1000 | Loss: 0.00001012
Iteration 150/1000 | Loss: 0.00001012
Iteration 151/1000 | Loss: 0.00001012
Iteration 152/1000 | Loss: 0.00001011
Iteration 153/1000 | Loss: 0.00001011
Iteration 154/1000 | Loss: 0.00001011
Iteration 155/1000 | Loss: 0.00001011
Iteration 156/1000 | Loss: 0.00001011
Iteration 157/1000 | Loss: 0.00001011
Iteration 158/1000 | Loss: 0.00001011
Iteration 159/1000 | Loss: 0.00001010
Iteration 160/1000 | Loss: 0.00001010
Iteration 161/1000 | Loss: 0.00001010
Iteration 162/1000 | Loss: 0.00001009
Iteration 163/1000 | Loss: 0.00001009
Iteration 164/1000 | Loss: 0.00001009
Iteration 165/1000 | Loss: 0.00001009
Iteration 166/1000 | Loss: 0.00001009
Iteration 167/1000 | Loss: 0.00001009
Iteration 168/1000 | Loss: 0.00001009
Iteration 169/1000 | Loss: 0.00001009
Iteration 170/1000 | Loss: 0.00001009
Iteration 171/1000 | Loss: 0.00001009
Iteration 172/1000 | Loss: 0.00001009
Iteration 173/1000 | Loss: 0.00001009
Iteration 174/1000 | Loss: 0.00001009
Iteration 175/1000 | Loss: 0.00001009
Iteration 176/1000 | Loss: 0.00001009
Iteration 177/1000 | Loss: 0.00001008
Iteration 178/1000 | Loss: 0.00001008
Iteration 179/1000 | Loss: 0.00001008
Iteration 180/1000 | Loss: 0.00001008
Iteration 181/1000 | Loss: 0.00001008
Iteration 182/1000 | Loss: 0.00001008
Iteration 183/1000 | Loss: 0.00001008
Iteration 184/1000 | Loss: 0.00001008
Iteration 185/1000 | Loss: 0.00001008
Iteration 186/1000 | Loss: 0.00001007
Iteration 187/1000 | Loss: 0.00001007
Iteration 188/1000 | Loss: 0.00001007
Iteration 189/1000 | Loss: 0.00001007
Iteration 190/1000 | Loss: 0.00001007
Iteration 191/1000 | Loss: 0.00001007
Iteration 192/1000 | Loss: 0.00001006
Iteration 193/1000 | Loss: 0.00001006
Iteration 194/1000 | Loss: 0.00001006
Iteration 195/1000 | Loss: 0.00001006
Iteration 196/1000 | Loss: 0.00001006
Iteration 197/1000 | Loss: 0.00001005
Iteration 198/1000 | Loss: 0.00001005
Iteration 199/1000 | Loss: 0.00001005
Iteration 200/1000 | Loss: 0.00001005
Iteration 201/1000 | Loss: 0.00001004
Iteration 202/1000 | Loss: 0.00001004
Iteration 203/1000 | Loss: 0.00001004
Iteration 204/1000 | Loss: 0.00001004
Iteration 205/1000 | Loss: 0.00001004
Iteration 206/1000 | Loss: 0.00001003
Iteration 207/1000 | Loss: 0.00001003
Iteration 208/1000 | Loss: 0.00001002
Iteration 209/1000 | Loss: 0.00001002
Iteration 210/1000 | Loss: 0.00001002
Iteration 211/1000 | Loss: 0.00001002
Iteration 212/1000 | Loss: 0.00001002
Iteration 213/1000 | Loss: 0.00001002
Iteration 214/1000 | Loss: 0.00001002
Iteration 215/1000 | Loss: 0.00001002
Iteration 216/1000 | Loss: 0.00001002
Iteration 217/1000 | Loss: 0.00001002
Iteration 218/1000 | Loss: 0.00001002
Iteration 219/1000 | Loss: 0.00001002
Iteration 220/1000 | Loss: 0.00001001
Iteration 221/1000 | Loss: 0.00001001
Iteration 222/1000 | Loss: 0.00001001
Iteration 223/1000 | Loss: 0.00001001
Iteration 224/1000 | Loss: 0.00001001
Iteration 225/1000 | Loss: 0.00001001
Iteration 226/1000 | Loss: 0.00001001
Iteration 227/1000 | Loss: 0.00001001
Iteration 228/1000 | Loss: 0.00001001
Iteration 229/1000 | Loss: 0.00001001
Iteration 230/1000 | Loss: 0.00001001
Iteration 231/1000 | Loss: 0.00001001
Iteration 232/1000 | Loss: 0.00001000
Iteration 233/1000 | Loss: 0.00001000
Iteration 234/1000 | Loss: 0.00001000
Iteration 235/1000 | Loss: 0.00001000
Iteration 236/1000 | Loss: 0.00001000
Iteration 237/1000 | Loss: 0.00001000
Iteration 238/1000 | Loss: 0.00001000
Iteration 239/1000 | Loss: 0.00001000
Iteration 240/1000 | Loss: 0.00001000
Iteration 241/1000 | Loss: 0.00001000
Iteration 242/1000 | Loss: 0.00001000
Iteration 243/1000 | Loss: 0.00001000
Iteration 244/1000 | Loss: 0.00001000
Iteration 245/1000 | Loss: 0.00001000
Iteration 246/1000 | Loss: 0.00001000
Iteration 247/1000 | Loss: 0.00001000
Iteration 248/1000 | Loss: 0.00000999
Iteration 249/1000 | Loss: 0.00000999
Iteration 250/1000 | Loss: 0.00000999
Iteration 251/1000 | Loss: 0.00000999
Iteration 252/1000 | Loss: 0.00000999
Iteration 253/1000 | Loss: 0.00000999
Iteration 254/1000 | Loss: 0.00000999
Iteration 255/1000 | Loss: 0.00000999
Iteration 256/1000 | Loss: 0.00000999
Iteration 257/1000 | Loss: 0.00000999
Iteration 258/1000 | Loss: 0.00000999
Iteration 259/1000 | Loss: 0.00000999
Iteration 260/1000 | Loss: 0.00000999
Iteration 261/1000 | Loss: 0.00000999
Iteration 262/1000 | Loss: 0.00000999
Iteration 263/1000 | Loss: 0.00000999
Iteration 264/1000 | Loss: 0.00000999
Iteration 265/1000 | Loss: 0.00000999
Iteration 266/1000 | Loss: 0.00000999
Iteration 267/1000 | Loss: 0.00000999
Iteration 268/1000 | Loss: 0.00000999
Iteration 269/1000 | Loss: 0.00000999
Iteration 270/1000 | Loss: 0.00000999
Iteration 271/1000 | Loss: 0.00000999
Iteration 272/1000 | Loss: 0.00000999
Iteration 273/1000 | Loss: 0.00000999
Iteration 274/1000 | Loss: 0.00000999
Iteration 275/1000 | Loss: 0.00000999
Iteration 276/1000 | Loss: 0.00000999
Iteration 277/1000 | Loss: 0.00000999
Iteration 278/1000 | Loss: 0.00000999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [9.989364116336219e-06, 9.989364116336219e-06, 9.989364116336219e-06, 9.989364116336219e-06, 9.989364116336219e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.989364116336219e-06

Optimization complete. Final v2v error: 2.6066362857818604 mm

Highest mean error: 3.165210723876953 mm for frame 70

Lowest mean error: 2.4251322746276855 mm for frame 116

Saving results

Total time: 42.3435001373291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054705
Iteration 2/25 | Loss: 0.01054704
Iteration 3/25 | Loss: 0.00266531
Iteration 4/25 | Loss: 0.00194816
Iteration 5/25 | Loss: 0.00241034
Iteration 6/25 | Loss: 0.00158197
Iteration 7/25 | Loss: 0.00142283
Iteration 8/25 | Loss: 0.00136579
Iteration 9/25 | Loss: 0.00135742
Iteration 10/25 | Loss: 0.00135546
Iteration 11/25 | Loss: 0.00135381
Iteration 12/25 | Loss: 0.00135347
Iteration 13/25 | Loss: 0.00135329
Iteration 14/25 | Loss: 0.00135319
Iteration 15/25 | Loss: 0.00135311
Iteration 16/25 | Loss: 0.00135310
Iteration 17/25 | Loss: 0.00135310
Iteration 18/25 | Loss: 0.00135308
Iteration 19/25 | Loss: 0.00135296
Iteration 20/25 | Loss: 0.00135336
Iteration 21/25 | Loss: 0.00135226
Iteration 22/25 | Loss: 0.00135187
Iteration 23/25 | Loss: 0.00135183
Iteration 24/25 | Loss: 0.00135182
Iteration 25/25 | Loss: 0.00135182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19789052
Iteration 2/25 | Loss: 0.00074835
Iteration 3/25 | Loss: 0.00074835
Iteration 4/25 | Loss: 0.00074835
Iteration 5/25 | Loss: 0.00074835
Iteration 6/25 | Loss: 0.00074835
Iteration 7/25 | Loss: 0.00074835
Iteration 8/25 | Loss: 0.00074835
Iteration 9/25 | Loss: 0.00074835
Iteration 10/25 | Loss: 0.00074835
Iteration 11/25 | Loss: 0.00074835
Iteration 12/25 | Loss: 0.00074835
Iteration 13/25 | Loss: 0.00074835
Iteration 14/25 | Loss: 0.00074835
Iteration 15/25 | Loss: 0.00074835
Iteration 16/25 | Loss: 0.00074835
Iteration 17/25 | Loss: 0.00074835
Iteration 18/25 | Loss: 0.00074835
Iteration 19/25 | Loss: 0.00074835
Iteration 20/25 | Loss: 0.00074835
Iteration 21/25 | Loss: 0.00074835
Iteration 22/25 | Loss: 0.00074835
Iteration 23/25 | Loss: 0.00074835
Iteration 24/25 | Loss: 0.00074835
Iteration 25/25 | Loss: 0.00074835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074835
Iteration 2/1000 | Loss: 0.00004458
Iteration 3/1000 | Loss: 0.00003113
Iteration 4/1000 | Loss: 0.00002778
Iteration 5/1000 | Loss: 0.00002684
Iteration 6/1000 | Loss: 0.00002605
Iteration 7/1000 | Loss: 0.00002558
Iteration 8/1000 | Loss: 0.00002552
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002509
Iteration 11/1000 | Loss: 0.00002507
Iteration 12/1000 | Loss: 0.00002501
Iteration 13/1000 | Loss: 0.00002499
Iteration 14/1000 | Loss: 0.00002491
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00002488
Iteration 17/1000 | Loss: 0.00002487
Iteration 18/1000 | Loss: 0.00002480
Iteration 19/1000 | Loss: 0.00002480
Iteration 20/1000 | Loss: 0.00002480
Iteration 21/1000 | Loss: 0.00002480
Iteration 22/1000 | Loss: 0.00002480
Iteration 23/1000 | Loss: 0.00002480
Iteration 24/1000 | Loss: 0.00002479
Iteration 25/1000 | Loss: 0.00002479
Iteration 26/1000 | Loss: 0.00002479
Iteration 27/1000 | Loss: 0.00002479
Iteration 28/1000 | Loss: 0.00002479
Iteration 29/1000 | Loss: 0.00002478
Iteration 30/1000 | Loss: 0.00002477
Iteration 31/1000 | Loss: 0.00002477
Iteration 32/1000 | Loss: 0.00002477
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002476
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002475
Iteration 37/1000 | Loss: 0.00002475
Iteration 38/1000 | Loss: 0.00002475
Iteration 39/1000 | Loss: 0.00002475
Iteration 40/1000 | Loss: 0.00002475
Iteration 41/1000 | Loss: 0.00002475
Iteration 42/1000 | Loss: 0.00002475
Iteration 43/1000 | Loss: 0.00002474
Iteration 44/1000 | Loss: 0.00002474
Iteration 45/1000 | Loss: 0.00002474
Iteration 46/1000 | Loss: 0.00002474
Iteration 47/1000 | Loss: 0.00002473
Iteration 48/1000 | Loss: 0.00002473
Iteration 49/1000 | Loss: 0.00002472
Iteration 50/1000 | Loss: 0.00002472
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002472
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002471
Iteration 55/1000 | Loss: 0.00002471
Iteration 56/1000 | Loss: 0.00002471
Iteration 57/1000 | Loss: 0.00002471
Iteration 58/1000 | Loss: 0.00002471
Iteration 59/1000 | Loss: 0.00002471
Iteration 60/1000 | Loss: 0.00002471
Iteration 61/1000 | Loss: 0.00002471
Iteration 62/1000 | Loss: 0.00002471
Iteration 63/1000 | Loss: 0.00002471
Iteration 64/1000 | Loss: 0.00002471
Iteration 65/1000 | Loss: 0.00002470
Iteration 66/1000 | Loss: 0.00002470
Iteration 67/1000 | Loss: 0.00002470
Iteration 68/1000 | Loss: 0.00002470
Iteration 69/1000 | Loss: 0.00002470
Iteration 70/1000 | Loss: 0.00002470
Iteration 71/1000 | Loss: 0.00002469
Iteration 72/1000 | Loss: 0.00002469
Iteration 73/1000 | Loss: 0.00002469
Iteration 74/1000 | Loss: 0.00002469
Iteration 75/1000 | Loss: 0.00002469
Iteration 76/1000 | Loss: 0.00002469
Iteration 77/1000 | Loss: 0.00002469
Iteration 78/1000 | Loss: 0.00002469
Iteration 79/1000 | Loss: 0.00002469
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002468
Iteration 87/1000 | Loss: 0.00002468
Iteration 88/1000 | Loss: 0.00002468
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002468
Iteration 92/1000 | Loss: 0.00002467
Iteration 93/1000 | Loss: 0.00002467
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002467
Iteration 97/1000 | Loss: 0.00002467
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002467
Iteration 100/1000 | Loss: 0.00002466
Iteration 101/1000 | Loss: 0.00002466
Iteration 102/1000 | Loss: 0.00002466
Iteration 103/1000 | Loss: 0.00002466
Iteration 104/1000 | Loss: 0.00002466
Iteration 105/1000 | Loss: 0.00002466
Iteration 106/1000 | Loss: 0.00002466
Iteration 107/1000 | Loss: 0.00002466
Iteration 108/1000 | Loss: 0.00002465
Iteration 109/1000 | Loss: 0.00002465
Iteration 110/1000 | Loss: 0.00002465
Iteration 111/1000 | Loss: 0.00002465
Iteration 112/1000 | Loss: 0.00002465
Iteration 113/1000 | Loss: 0.00002465
Iteration 114/1000 | Loss: 0.00002465
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002465
Iteration 119/1000 | Loss: 0.00002465
Iteration 120/1000 | Loss: 0.00002465
Iteration 121/1000 | Loss: 0.00002465
Iteration 122/1000 | Loss: 0.00002465
Iteration 123/1000 | Loss: 0.00002464
Iteration 124/1000 | Loss: 0.00002464
Iteration 125/1000 | Loss: 0.00002464
Iteration 126/1000 | Loss: 0.00002464
Iteration 127/1000 | Loss: 0.00002464
Iteration 128/1000 | Loss: 0.00002464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.464483623043634e-05, 2.464483623043634e-05, 2.464483623043634e-05, 2.464483623043634e-05, 2.464483623043634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.464483623043634e-05

Optimization complete. Final v2v error: 4.05595064163208 mm

Highest mean error: 4.502772808074951 mm for frame 239

Lowest mean error: 3.720818281173706 mm for frame 17

Saving results

Total time: 64.06754040718079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934120
Iteration 2/25 | Loss: 0.00191941
Iteration 3/25 | Loss: 0.00186221
Iteration 4/25 | Loss: 0.00158648
Iteration 5/25 | Loss: 0.00152433
Iteration 6/25 | Loss: 0.00135789
Iteration 7/25 | Loss: 0.00132877
Iteration 8/25 | Loss: 0.00132619
Iteration 9/25 | Loss: 0.00132589
Iteration 10/25 | Loss: 0.00132579
Iteration 11/25 | Loss: 0.00132579
Iteration 12/25 | Loss: 0.00132579
Iteration 13/25 | Loss: 0.00132579
Iteration 14/25 | Loss: 0.00132579
Iteration 15/25 | Loss: 0.00132579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013257937971502542, 0.0013257937971502542, 0.0013257937971502542, 0.0013257937971502542, 0.0013257937971502542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013257937971502542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25484848
Iteration 2/25 | Loss: 0.00075169
Iteration 3/25 | Loss: 0.00075169
Iteration 4/25 | Loss: 0.00075169
Iteration 5/25 | Loss: 0.00075169
Iteration 6/25 | Loss: 0.00075169
Iteration 7/25 | Loss: 0.00075169
Iteration 8/25 | Loss: 0.00075169
Iteration 9/25 | Loss: 0.00075169
Iteration 10/25 | Loss: 0.00075169
Iteration 11/25 | Loss: 0.00075169
Iteration 12/25 | Loss: 0.00075169
Iteration 13/25 | Loss: 0.00075169
Iteration 14/25 | Loss: 0.00075169
Iteration 15/25 | Loss: 0.00075169
Iteration 16/25 | Loss: 0.00075169
Iteration 17/25 | Loss: 0.00075169
Iteration 18/25 | Loss: 0.00075169
Iteration 19/25 | Loss: 0.00075169
Iteration 20/25 | Loss: 0.00075169
Iteration 21/25 | Loss: 0.00075169
Iteration 22/25 | Loss: 0.00075169
Iteration 23/25 | Loss: 0.00075169
Iteration 24/25 | Loss: 0.00075169
Iteration 25/25 | Loss: 0.00075169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007516889600083232, 0.0007516889600083232, 0.0007516889600083232, 0.0007516889600083232, 0.0007516889600083232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007516889600083232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075169
Iteration 2/1000 | Loss: 0.00006315
Iteration 3/1000 | Loss: 0.00003910
Iteration 4/1000 | Loss: 0.00003462
Iteration 5/1000 | Loss: 0.00039706
Iteration 6/1000 | Loss: 0.00003247
Iteration 7/1000 | Loss: 0.00003147
Iteration 8/1000 | Loss: 0.00032002
Iteration 9/1000 | Loss: 0.00005367
Iteration 10/1000 | Loss: 0.00006523
Iteration 11/1000 | Loss: 0.00003042
Iteration 12/1000 | Loss: 0.00003004
Iteration 13/1000 | Loss: 0.00002971
Iteration 14/1000 | Loss: 0.00002953
Iteration 15/1000 | Loss: 0.00002938
Iteration 16/1000 | Loss: 0.00039740
Iteration 17/1000 | Loss: 0.00002958
Iteration 18/1000 | Loss: 0.00002927
Iteration 19/1000 | Loss: 0.00002911
Iteration 20/1000 | Loss: 0.00002898
Iteration 21/1000 | Loss: 0.00002894
Iteration 22/1000 | Loss: 0.00002891
Iteration 23/1000 | Loss: 0.00002891
Iteration 24/1000 | Loss: 0.00002891
Iteration 25/1000 | Loss: 0.00002891
Iteration 26/1000 | Loss: 0.00002891
Iteration 27/1000 | Loss: 0.00002891
Iteration 28/1000 | Loss: 0.00002891
Iteration 29/1000 | Loss: 0.00002891
Iteration 30/1000 | Loss: 0.00002890
Iteration 31/1000 | Loss: 0.00002890
Iteration 32/1000 | Loss: 0.00002890
Iteration 33/1000 | Loss: 0.00002890
Iteration 34/1000 | Loss: 0.00002890
Iteration 35/1000 | Loss: 0.00002890
Iteration 36/1000 | Loss: 0.00002889
Iteration 37/1000 | Loss: 0.00002889
Iteration 38/1000 | Loss: 0.00002889
Iteration 39/1000 | Loss: 0.00002889
Iteration 40/1000 | Loss: 0.00002889
Iteration 41/1000 | Loss: 0.00002888
Iteration 42/1000 | Loss: 0.00002888
Iteration 43/1000 | Loss: 0.00002887
Iteration 44/1000 | Loss: 0.00002886
Iteration 45/1000 | Loss: 0.00002885
Iteration 46/1000 | Loss: 0.00002883
Iteration 47/1000 | Loss: 0.00002883
Iteration 48/1000 | Loss: 0.00002883
Iteration 49/1000 | Loss: 0.00002882
Iteration 50/1000 | Loss: 0.00002882
Iteration 51/1000 | Loss: 0.00002881
Iteration 52/1000 | Loss: 0.00002881
Iteration 53/1000 | Loss: 0.00002880
Iteration 54/1000 | Loss: 0.00002880
Iteration 55/1000 | Loss: 0.00002880
Iteration 56/1000 | Loss: 0.00002880
Iteration 57/1000 | Loss: 0.00002879
Iteration 58/1000 | Loss: 0.00002879
Iteration 59/1000 | Loss: 0.00002879
Iteration 60/1000 | Loss: 0.00002878
Iteration 61/1000 | Loss: 0.00002878
Iteration 62/1000 | Loss: 0.00002877
Iteration 63/1000 | Loss: 0.00002877
Iteration 64/1000 | Loss: 0.00002877
Iteration 65/1000 | Loss: 0.00002877
Iteration 66/1000 | Loss: 0.00002877
Iteration 67/1000 | Loss: 0.00002876
Iteration 68/1000 | Loss: 0.00002876
Iteration 69/1000 | Loss: 0.00002876
Iteration 70/1000 | Loss: 0.00002876
Iteration 71/1000 | Loss: 0.00002875
Iteration 72/1000 | Loss: 0.00002875
Iteration 73/1000 | Loss: 0.00002874
Iteration 74/1000 | Loss: 0.00002874
Iteration 75/1000 | Loss: 0.00002874
Iteration 76/1000 | Loss: 0.00002874
Iteration 77/1000 | Loss: 0.00002874
Iteration 78/1000 | Loss: 0.00002874
Iteration 79/1000 | Loss: 0.00002874
Iteration 80/1000 | Loss: 0.00002874
Iteration 81/1000 | Loss: 0.00002874
Iteration 82/1000 | Loss: 0.00002874
Iteration 83/1000 | Loss: 0.00002873
Iteration 84/1000 | Loss: 0.00002873
Iteration 85/1000 | Loss: 0.00002873
Iteration 86/1000 | Loss: 0.00002873
Iteration 87/1000 | Loss: 0.00002873
Iteration 88/1000 | Loss: 0.00002873
Iteration 89/1000 | Loss: 0.00002872
Iteration 90/1000 | Loss: 0.00002872
Iteration 91/1000 | Loss: 0.00002872
Iteration 92/1000 | Loss: 0.00002872
Iteration 93/1000 | Loss: 0.00002872
Iteration 94/1000 | Loss: 0.00002872
Iteration 95/1000 | Loss: 0.00002872
Iteration 96/1000 | Loss: 0.00002872
Iteration 97/1000 | Loss: 0.00002871
Iteration 98/1000 | Loss: 0.00002871
Iteration 99/1000 | Loss: 0.00002871
Iteration 100/1000 | Loss: 0.00002871
Iteration 101/1000 | Loss: 0.00002871
Iteration 102/1000 | Loss: 0.00002871
Iteration 103/1000 | Loss: 0.00002871
Iteration 104/1000 | Loss: 0.00002871
Iteration 105/1000 | Loss: 0.00002871
Iteration 106/1000 | Loss: 0.00002870
Iteration 107/1000 | Loss: 0.00002870
Iteration 108/1000 | Loss: 0.00002870
Iteration 109/1000 | Loss: 0.00002870
Iteration 110/1000 | Loss: 0.00002870
Iteration 111/1000 | Loss: 0.00002870
Iteration 112/1000 | Loss: 0.00002870
Iteration 113/1000 | Loss: 0.00002870
Iteration 114/1000 | Loss: 0.00002870
Iteration 115/1000 | Loss: 0.00002870
Iteration 116/1000 | Loss: 0.00002869
Iteration 117/1000 | Loss: 0.00002869
Iteration 118/1000 | Loss: 0.00002869
Iteration 119/1000 | Loss: 0.00002869
Iteration 120/1000 | Loss: 0.00002869
Iteration 121/1000 | Loss: 0.00002869
Iteration 122/1000 | Loss: 0.00002868
Iteration 123/1000 | Loss: 0.00002868
Iteration 124/1000 | Loss: 0.00002868
Iteration 125/1000 | Loss: 0.00002868
Iteration 126/1000 | Loss: 0.00002868
Iteration 127/1000 | Loss: 0.00002868
Iteration 128/1000 | Loss: 0.00002868
Iteration 129/1000 | Loss: 0.00002868
Iteration 130/1000 | Loss: 0.00002868
Iteration 131/1000 | Loss: 0.00002868
Iteration 132/1000 | Loss: 0.00002868
Iteration 133/1000 | Loss: 0.00002868
Iteration 134/1000 | Loss: 0.00002868
Iteration 135/1000 | Loss: 0.00002868
Iteration 136/1000 | Loss: 0.00002868
Iteration 137/1000 | Loss: 0.00002868
Iteration 138/1000 | Loss: 0.00002868
Iteration 139/1000 | Loss: 0.00002868
Iteration 140/1000 | Loss: 0.00002868
Iteration 141/1000 | Loss: 0.00002868
Iteration 142/1000 | Loss: 0.00002868
Iteration 143/1000 | Loss: 0.00002868
Iteration 144/1000 | Loss: 0.00002868
Iteration 145/1000 | Loss: 0.00002868
Iteration 146/1000 | Loss: 0.00002868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.867864168365486e-05, 2.867864168365486e-05, 2.867864168365486e-05, 2.867864168365486e-05, 2.867864168365486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.867864168365486e-05

Optimization complete. Final v2v error: 4.55320930480957 mm

Highest mean error: 4.970075607299805 mm for frame 144

Lowest mean error: 4.119472503662109 mm for frame 204

Saving results

Total time: 58.79265284538269
