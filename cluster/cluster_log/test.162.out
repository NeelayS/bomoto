Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=162, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9072-9127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969768
Iteration 2/25 | Loss: 0.00320328
Iteration 3/25 | Loss: 0.00241428
Iteration 4/25 | Loss: 0.00185611
Iteration 5/25 | Loss: 0.00170467
Iteration 6/25 | Loss: 0.00159363
Iteration 7/25 | Loss: 0.00155140
Iteration 8/25 | Loss: 0.00154721
Iteration 9/25 | Loss: 0.00143656
Iteration 10/25 | Loss: 0.00141396
Iteration 11/25 | Loss: 0.00139017
Iteration 12/25 | Loss: 0.00134660
Iteration 13/25 | Loss: 0.00132093
Iteration 14/25 | Loss: 0.00131602
Iteration 15/25 | Loss: 0.00129882
Iteration 16/25 | Loss: 0.00129974
Iteration 17/25 | Loss: 0.00128648
Iteration 18/25 | Loss: 0.00128418
Iteration 19/25 | Loss: 0.00128350
Iteration 20/25 | Loss: 0.00128322
Iteration 21/25 | Loss: 0.00128303
Iteration 22/25 | Loss: 0.00128614
Iteration 23/25 | Loss: 0.00128378
Iteration 24/25 | Loss: 0.00128445
Iteration 25/25 | Loss: 0.00128328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33639920
Iteration 2/25 | Loss: 0.00182861
Iteration 3/25 | Loss: 0.00122508
Iteration 4/25 | Loss: 0.00122508
Iteration 5/25 | Loss: 0.00122508
Iteration 6/25 | Loss: 0.00122508
Iteration 7/25 | Loss: 0.00122508
Iteration 8/25 | Loss: 0.00122508
Iteration 9/25 | Loss: 0.00122508
Iteration 10/25 | Loss: 0.00122508
Iteration 11/25 | Loss: 0.00122508
Iteration 12/25 | Loss: 0.00122508
Iteration 13/25 | Loss: 0.00122508
Iteration 14/25 | Loss: 0.00122508
Iteration 15/25 | Loss: 0.00122508
Iteration 16/25 | Loss: 0.00122508
Iteration 17/25 | Loss: 0.00122508
Iteration 18/25 | Loss: 0.00122508
Iteration 19/25 | Loss: 0.00122508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012250794097781181, 0.0012250794097781181, 0.0012250794097781181, 0.0012250794097781181, 0.0012250794097781181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012250794097781181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122508
Iteration 2/1000 | Loss: 0.00065968
Iteration 3/1000 | Loss: 0.00072737
Iteration 4/1000 | Loss: 0.00058258
Iteration 5/1000 | Loss: 0.00009778
Iteration 6/1000 | Loss: 0.00026205
Iteration 7/1000 | Loss: 0.00141955
Iteration 8/1000 | Loss: 0.00075889
Iteration 9/1000 | Loss: 0.00118791
Iteration 10/1000 | Loss: 0.00111120
Iteration 11/1000 | Loss: 0.00008718
Iteration 12/1000 | Loss: 0.00011261
Iteration 13/1000 | Loss: 0.00007155
Iteration 14/1000 | Loss: 0.00041305
Iteration 15/1000 | Loss: 0.00041295
Iteration 16/1000 | Loss: 0.00021984
Iteration 17/1000 | Loss: 0.00008045
Iteration 18/1000 | Loss: 0.00024884
Iteration 19/1000 | Loss: 0.00128889
Iteration 20/1000 | Loss: 0.00140721
Iteration 21/1000 | Loss: 0.00145849
Iteration 22/1000 | Loss: 0.00050281
Iteration 23/1000 | Loss: 0.00071424
Iteration 24/1000 | Loss: 0.00267940
Iteration 25/1000 | Loss: 0.00317360
Iteration 26/1000 | Loss: 0.00129756
Iteration 27/1000 | Loss: 0.00010875
Iteration 28/1000 | Loss: 0.00008321
Iteration 29/1000 | Loss: 0.00107631
Iteration 30/1000 | Loss: 0.00049000
Iteration 31/1000 | Loss: 0.00006170
Iteration 32/1000 | Loss: 0.00041543
Iteration 33/1000 | Loss: 0.00005157
Iteration 34/1000 | Loss: 0.00003162
Iteration 35/1000 | Loss: 0.00021532
Iteration 36/1000 | Loss: 0.00091541
Iteration 37/1000 | Loss: 0.00005710
Iteration 38/1000 | Loss: 0.00009594
Iteration 39/1000 | Loss: 0.00002333
Iteration 40/1000 | Loss: 0.00036823
Iteration 41/1000 | Loss: 0.00002553
Iteration 42/1000 | Loss: 0.00002078
Iteration 43/1000 | Loss: 0.00020035
Iteration 44/1000 | Loss: 0.00080776
Iteration 45/1000 | Loss: 0.00003753
Iteration 46/1000 | Loss: 0.00005693
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001779
Iteration 49/1000 | Loss: 0.00017129
Iteration 50/1000 | Loss: 0.00004794
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00031162
Iteration 54/1000 | Loss: 0.00004659
Iteration 55/1000 | Loss: 0.00020276
Iteration 56/1000 | Loss: 0.00004915
Iteration 57/1000 | Loss: 0.00014749
Iteration 58/1000 | Loss: 0.00004957
Iteration 59/1000 | Loss: 0.00042117
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00012624
Iteration 65/1000 | Loss: 0.00002974
Iteration 66/1000 | Loss: 0.00001617
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001540
Iteration 89/1000 | Loss: 0.00001540
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001539
Iteration 96/1000 | Loss: 0.00001539
Iteration 97/1000 | Loss: 0.00001539
Iteration 98/1000 | Loss: 0.00001539
Iteration 99/1000 | Loss: 0.00001539
Iteration 100/1000 | Loss: 0.00001539
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001539
Iteration 103/1000 | Loss: 0.00001539
Iteration 104/1000 | Loss: 0.00001538
Iteration 105/1000 | Loss: 0.00001538
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001538
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001538
Iteration 111/1000 | Loss: 0.00001537
Iteration 112/1000 | Loss: 0.00001537
Iteration 113/1000 | Loss: 0.00001537
Iteration 114/1000 | Loss: 0.00001537
Iteration 115/1000 | Loss: 0.00001537
Iteration 116/1000 | Loss: 0.00001537
Iteration 117/1000 | Loss: 0.00001537
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001536
Iteration 121/1000 | Loss: 0.00001535
Iteration 122/1000 | Loss: 0.00001535
Iteration 123/1000 | Loss: 0.00001534
Iteration 124/1000 | Loss: 0.00001534
Iteration 125/1000 | Loss: 0.00001534
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001532
Iteration 128/1000 | Loss: 0.00001532
Iteration 129/1000 | Loss: 0.00001531
Iteration 130/1000 | Loss: 0.00001531
Iteration 131/1000 | Loss: 0.00001531
Iteration 132/1000 | Loss: 0.00001531
Iteration 133/1000 | Loss: 0.00001531
Iteration 134/1000 | Loss: 0.00001531
Iteration 135/1000 | Loss: 0.00001531
Iteration 136/1000 | Loss: 0.00001531
Iteration 137/1000 | Loss: 0.00001530
Iteration 138/1000 | Loss: 0.00001530
Iteration 139/1000 | Loss: 0.00001530
Iteration 140/1000 | Loss: 0.00001530
Iteration 141/1000 | Loss: 0.00001530
Iteration 142/1000 | Loss: 0.00001529
Iteration 143/1000 | Loss: 0.00001529
Iteration 144/1000 | Loss: 0.00001529
Iteration 145/1000 | Loss: 0.00001529
Iteration 146/1000 | Loss: 0.00001529
Iteration 147/1000 | Loss: 0.00001529
Iteration 148/1000 | Loss: 0.00001529
Iteration 149/1000 | Loss: 0.00001529
Iteration 150/1000 | Loss: 0.00001529
Iteration 151/1000 | Loss: 0.00001528
Iteration 152/1000 | Loss: 0.00001528
Iteration 153/1000 | Loss: 0.00001528
Iteration 154/1000 | Loss: 0.00001528
Iteration 155/1000 | Loss: 0.00001528
Iteration 156/1000 | Loss: 0.00001528
Iteration 157/1000 | Loss: 0.00001528
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001527
Iteration 161/1000 | Loss: 0.00001527
Iteration 162/1000 | Loss: 0.00001526
Iteration 163/1000 | Loss: 0.00001526
Iteration 164/1000 | Loss: 0.00001526
Iteration 165/1000 | Loss: 0.00001526
Iteration 166/1000 | Loss: 0.00001526
Iteration 167/1000 | Loss: 0.00001526
Iteration 168/1000 | Loss: 0.00001526
Iteration 169/1000 | Loss: 0.00001526
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001525
Iteration 175/1000 | Loss: 0.00001525
Iteration 176/1000 | Loss: 0.00001525
Iteration 177/1000 | Loss: 0.00001525
Iteration 178/1000 | Loss: 0.00001525
Iteration 179/1000 | Loss: 0.00001525
Iteration 180/1000 | Loss: 0.00001525
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001525
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001525
Iteration 185/1000 | Loss: 0.00001525
Iteration 186/1000 | Loss: 0.00001525
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001525
Iteration 189/1000 | Loss: 0.00001525
Iteration 190/1000 | Loss: 0.00001525
Iteration 191/1000 | Loss: 0.00001525
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001525
Iteration 195/1000 | Loss: 0.00001525
Iteration 196/1000 | Loss: 0.00001525
Iteration 197/1000 | Loss: 0.00001525
Iteration 198/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.5253106539603323e-05, 1.5253106539603323e-05, 1.5253106539603323e-05, 1.5253106539603323e-05, 1.5253106539603323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5253106539603323e-05

Optimization complete. Final v2v error: 3.16829252243042 mm

Highest mean error: 10.32451057434082 mm for frame 152

Lowest mean error: 2.868785858154297 mm for frame 110

Saving results

Total time: 177.4297478199005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016590
Iteration 2/25 | Loss: 0.00233921
Iteration 3/25 | Loss: 0.00301429
Iteration 4/25 | Loss: 0.00211956
Iteration 5/25 | Loss: 0.00186254
Iteration 6/25 | Loss: 0.00147698
Iteration 7/25 | Loss: 0.00132832
Iteration 8/25 | Loss: 0.00130429
Iteration 9/25 | Loss: 0.00126523
Iteration 10/25 | Loss: 0.00121175
Iteration 11/25 | Loss: 0.00120404
Iteration 12/25 | Loss: 0.00119749
Iteration 13/25 | Loss: 0.00117350
Iteration 14/25 | Loss: 0.00116673
Iteration 15/25 | Loss: 0.00116210
Iteration 16/25 | Loss: 0.00115945
Iteration 17/25 | Loss: 0.00115878
Iteration 18/25 | Loss: 0.00115843
Iteration 19/25 | Loss: 0.00115829
Iteration 20/25 | Loss: 0.00117819
Iteration 21/25 | Loss: 0.00115448
Iteration 22/25 | Loss: 0.00115255
Iteration 23/25 | Loss: 0.00115175
Iteration 24/25 | Loss: 0.00115109
Iteration 25/25 | Loss: 0.00115094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41931248
Iteration 2/25 | Loss: 0.00108333
Iteration 3/25 | Loss: 0.00108333
Iteration 4/25 | Loss: 0.00091206
Iteration 5/25 | Loss: 0.00091206
Iteration 6/25 | Loss: 0.00091206
Iteration 7/25 | Loss: 0.00091206
Iteration 8/25 | Loss: 0.00091206
Iteration 9/25 | Loss: 0.00091205
Iteration 10/25 | Loss: 0.00091205
Iteration 11/25 | Loss: 0.00091205
Iteration 12/25 | Loss: 0.00091205
Iteration 13/25 | Loss: 0.00091205
Iteration 14/25 | Loss: 0.00091205
Iteration 15/25 | Loss: 0.00091205
Iteration 16/25 | Loss: 0.00091205
Iteration 17/25 | Loss: 0.00091205
Iteration 18/25 | Loss: 0.00091205
Iteration 19/25 | Loss: 0.00091205
Iteration 20/25 | Loss: 0.00091205
Iteration 21/25 | Loss: 0.00091205
Iteration 22/25 | Loss: 0.00091205
Iteration 23/25 | Loss: 0.00091205
Iteration 24/25 | Loss: 0.00091205
Iteration 25/25 | Loss: 0.00091205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091205
Iteration 2/1000 | Loss: 0.00003995
Iteration 3/1000 | Loss: 0.00157069
Iteration 4/1000 | Loss: 0.00259436
Iteration 5/1000 | Loss: 0.00211135
Iteration 6/1000 | Loss: 0.00146768
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00029303
Iteration 9/1000 | Loss: 0.00020463
Iteration 10/1000 | Loss: 0.00033469
Iteration 11/1000 | Loss: 0.00002518
Iteration 12/1000 | Loss: 0.00002092
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001910
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001793
Iteration 18/1000 | Loss: 0.00001771
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001754
Iteration 21/1000 | Loss: 0.00001746
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001703
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001702
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001699
Iteration 151/1000 | Loss: 0.00001699
Iteration 152/1000 | Loss: 0.00001699
Iteration 153/1000 | Loss: 0.00001699
Iteration 154/1000 | Loss: 0.00001699
Iteration 155/1000 | Loss: 0.00001699
Iteration 156/1000 | Loss: 0.00001699
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001698
Iteration 160/1000 | Loss: 0.00001698
Iteration 161/1000 | Loss: 0.00001698
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001698
Iteration 164/1000 | Loss: 0.00001698
Iteration 165/1000 | Loss: 0.00001698
Iteration 166/1000 | Loss: 0.00001698
Iteration 167/1000 | Loss: 0.00001698
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001698
Iteration 170/1000 | Loss: 0.00001698
Iteration 171/1000 | Loss: 0.00001698
Iteration 172/1000 | Loss: 0.00001698
Iteration 173/1000 | Loss: 0.00001697
Iteration 174/1000 | Loss: 0.00001697
Iteration 175/1000 | Loss: 0.00001697
Iteration 176/1000 | Loss: 0.00001697
Iteration 177/1000 | Loss: 0.00001697
Iteration 178/1000 | Loss: 0.00001697
Iteration 179/1000 | Loss: 0.00001697
Iteration 180/1000 | Loss: 0.00001696
Iteration 181/1000 | Loss: 0.00001696
Iteration 182/1000 | Loss: 0.00001696
Iteration 183/1000 | Loss: 0.00001696
Iteration 184/1000 | Loss: 0.00001696
Iteration 185/1000 | Loss: 0.00001696
Iteration 186/1000 | Loss: 0.00001695
Iteration 187/1000 | Loss: 0.00001695
Iteration 188/1000 | Loss: 0.00001695
Iteration 189/1000 | Loss: 0.00001695
Iteration 190/1000 | Loss: 0.00001695
Iteration 191/1000 | Loss: 0.00001695
Iteration 192/1000 | Loss: 0.00001695
Iteration 193/1000 | Loss: 0.00001695
Iteration 194/1000 | Loss: 0.00001695
Iteration 195/1000 | Loss: 0.00001695
Iteration 196/1000 | Loss: 0.00001695
Iteration 197/1000 | Loss: 0.00001695
Iteration 198/1000 | Loss: 0.00001694
Iteration 199/1000 | Loss: 0.00001694
Iteration 200/1000 | Loss: 0.00001694
Iteration 201/1000 | Loss: 0.00001694
Iteration 202/1000 | Loss: 0.00001694
Iteration 203/1000 | Loss: 0.00001694
Iteration 204/1000 | Loss: 0.00001694
Iteration 205/1000 | Loss: 0.00001694
Iteration 206/1000 | Loss: 0.00001694
Iteration 207/1000 | Loss: 0.00001694
Iteration 208/1000 | Loss: 0.00001694
Iteration 209/1000 | Loss: 0.00001694
Iteration 210/1000 | Loss: 0.00001694
Iteration 211/1000 | Loss: 0.00001694
Iteration 212/1000 | Loss: 0.00001694
Iteration 213/1000 | Loss: 0.00001694
Iteration 214/1000 | Loss: 0.00001694
Iteration 215/1000 | Loss: 0.00001694
Iteration 216/1000 | Loss: 0.00001694
Iteration 217/1000 | Loss: 0.00001694
Iteration 218/1000 | Loss: 0.00001694
Iteration 219/1000 | Loss: 0.00001694
Iteration 220/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.6944328308454715e-05, 1.6944328308454715e-05, 1.6944328308454715e-05, 1.6944328308454715e-05, 1.6944328308454715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6944328308454715e-05

Optimization complete. Final v2v error: 3.4329583644866943 mm

Highest mean error: 4.797293186187744 mm for frame 107

Lowest mean error: 2.823659896850586 mm for frame 168

Saving results

Total time: 102.30421113967896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894956
Iteration 2/25 | Loss: 0.00138619
Iteration 3/25 | Loss: 0.00120513
Iteration 4/25 | Loss: 0.00118769
Iteration 5/25 | Loss: 0.00118289
Iteration 6/25 | Loss: 0.00118199
Iteration 7/25 | Loss: 0.00118199
Iteration 8/25 | Loss: 0.00118199
Iteration 9/25 | Loss: 0.00118199
Iteration 10/25 | Loss: 0.00118199
Iteration 11/25 | Loss: 0.00118199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011819908395409584, 0.0011819908395409584, 0.0011819908395409584, 0.0011819908395409584, 0.0011819908395409584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819908395409584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71035510
Iteration 2/25 | Loss: 0.00073435
Iteration 3/25 | Loss: 0.00073435
Iteration 4/25 | Loss: 0.00073435
Iteration 5/25 | Loss: 0.00073435
Iteration 6/25 | Loss: 0.00073435
Iteration 7/25 | Loss: 0.00073435
Iteration 8/25 | Loss: 0.00073435
Iteration 9/25 | Loss: 0.00073435
Iteration 10/25 | Loss: 0.00073435
Iteration 11/25 | Loss: 0.00073435
Iteration 12/25 | Loss: 0.00073435
Iteration 13/25 | Loss: 0.00073435
Iteration 14/25 | Loss: 0.00073435
Iteration 15/25 | Loss: 0.00073435
Iteration 16/25 | Loss: 0.00073435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007343473262153566, 0.0007343473262153566, 0.0007343473262153566, 0.0007343473262153566, 0.0007343473262153566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007343473262153566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073435
Iteration 2/1000 | Loss: 0.00004697
Iteration 3/1000 | Loss: 0.00003123
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002515
Iteration 6/1000 | Loss: 0.00002405
Iteration 7/1000 | Loss: 0.00002331
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002224
Iteration 10/1000 | Loss: 0.00002190
Iteration 11/1000 | Loss: 0.00002143
Iteration 12/1000 | Loss: 0.00002112
Iteration 13/1000 | Loss: 0.00002068
Iteration 14/1000 | Loss: 0.00002022
Iteration 15/1000 | Loss: 0.00001996
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001955
Iteration 18/1000 | Loss: 0.00001939
Iteration 19/1000 | Loss: 0.00001936
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001924
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001920
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001916
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001913
Iteration 38/1000 | Loss: 0.00001913
Iteration 39/1000 | Loss: 0.00001913
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001910
Iteration 48/1000 | Loss: 0.00001910
Iteration 49/1000 | Loss: 0.00001910
Iteration 50/1000 | Loss: 0.00001910
Iteration 51/1000 | Loss: 0.00001910
Iteration 52/1000 | Loss: 0.00001910
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001907
Iteration 55/1000 | Loss: 0.00001907
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001900
Iteration 60/1000 | Loss: 0.00001900
Iteration 61/1000 | Loss: 0.00001900
Iteration 62/1000 | Loss: 0.00001900
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001897
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001891
Iteration 77/1000 | Loss: 0.00001890
Iteration 78/1000 | Loss: 0.00001889
Iteration 79/1000 | Loss: 0.00001889
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00001888
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001883
Iteration 96/1000 | Loss: 0.00001883
Iteration 97/1000 | Loss: 0.00001883
Iteration 98/1000 | Loss: 0.00001882
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001880
Iteration 107/1000 | Loss: 0.00001880
Iteration 108/1000 | Loss: 0.00001880
Iteration 109/1000 | Loss: 0.00001880
Iteration 110/1000 | Loss: 0.00001880
Iteration 111/1000 | Loss: 0.00001880
Iteration 112/1000 | Loss: 0.00001880
Iteration 113/1000 | Loss: 0.00001880
Iteration 114/1000 | Loss: 0.00001880
Iteration 115/1000 | Loss: 0.00001880
Iteration 116/1000 | Loss: 0.00001879
Iteration 117/1000 | Loss: 0.00001879
Iteration 118/1000 | Loss: 0.00001879
Iteration 119/1000 | Loss: 0.00001879
Iteration 120/1000 | Loss: 0.00001879
Iteration 121/1000 | Loss: 0.00001879
Iteration 122/1000 | Loss: 0.00001879
Iteration 123/1000 | Loss: 0.00001879
Iteration 124/1000 | Loss: 0.00001879
Iteration 125/1000 | Loss: 0.00001879
Iteration 126/1000 | Loss: 0.00001879
Iteration 127/1000 | Loss: 0.00001879
Iteration 128/1000 | Loss: 0.00001879
Iteration 129/1000 | Loss: 0.00001879
Iteration 130/1000 | Loss: 0.00001879
Iteration 131/1000 | Loss: 0.00001879
Iteration 132/1000 | Loss: 0.00001879
Iteration 133/1000 | Loss: 0.00001879
Iteration 134/1000 | Loss: 0.00001879
Iteration 135/1000 | Loss: 0.00001879
Iteration 136/1000 | Loss: 0.00001879
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.8787008229992352e-05, 1.8787008229992352e-05, 1.8787008229992352e-05, 1.8787008229992352e-05, 1.8787008229992352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8787008229992352e-05

Optimization complete. Final v2v error: 3.519580841064453 mm

Highest mean error: 3.7309279441833496 mm for frame 58

Lowest mean error: 3.2469122409820557 mm for frame 162

Saving results

Total time: 47.61307764053345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830325
Iteration 2/25 | Loss: 0.00137186
Iteration 3/25 | Loss: 0.00115242
Iteration 4/25 | Loss: 0.00111581
Iteration 5/25 | Loss: 0.00110880
Iteration 6/25 | Loss: 0.00110810
Iteration 7/25 | Loss: 0.00110810
Iteration 8/25 | Loss: 0.00110810
Iteration 9/25 | Loss: 0.00110810
Iteration 10/25 | Loss: 0.00110810
Iteration 11/25 | Loss: 0.00110810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011081049451604486, 0.0011081049451604486, 0.0011081049451604486, 0.0011081049451604486, 0.0011081049451604486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011081049451604486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96680087
Iteration 2/25 | Loss: 0.00045652
Iteration 3/25 | Loss: 0.00045652
Iteration 4/25 | Loss: 0.00045651
Iteration 5/25 | Loss: 0.00045651
Iteration 6/25 | Loss: 0.00045651
Iteration 7/25 | Loss: 0.00045651
Iteration 8/25 | Loss: 0.00045651
Iteration 9/25 | Loss: 0.00045651
Iteration 10/25 | Loss: 0.00045651
Iteration 11/25 | Loss: 0.00045651
Iteration 12/25 | Loss: 0.00045651
Iteration 13/25 | Loss: 0.00045651
Iteration 14/25 | Loss: 0.00045651
Iteration 15/25 | Loss: 0.00045651
Iteration 16/25 | Loss: 0.00045651
Iteration 17/25 | Loss: 0.00045651
Iteration 18/25 | Loss: 0.00045651
Iteration 19/25 | Loss: 0.00045651
Iteration 20/25 | Loss: 0.00045651
Iteration 21/25 | Loss: 0.00045651
Iteration 22/25 | Loss: 0.00045651
Iteration 23/25 | Loss: 0.00045651
Iteration 24/25 | Loss: 0.00045651
Iteration 25/25 | Loss: 0.00045651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045651
Iteration 2/1000 | Loss: 0.00003882
Iteration 3/1000 | Loss: 0.00002850
Iteration 4/1000 | Loss: 0.00002552
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002201
Iteration 8/1000 | Loss: 0.00002135
Iteration 9/1000 | Loss: 0.00002098
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00002039
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00002015
Iteration 15/1000 | Loss: 0.00002014
Iteration 16/1000 | Loss: 0.00002012
Iteration 17/1000 | Loss: 0.00002000
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001991
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001984
Iteration 25/1000 | Loss: 0.00001984
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001983
Iteration 28/1000 | Loss: 0.00001983
Iteration 29/1000 | Loss: 0.00001982
Iteration 30/1000 | Loss: 0.00001982
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001980
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001978
Iteration 43/1000 | Loss: 0.00001978
Iteration 44/1000 | Loss: 0.00001978
Iteration 45/1000 | Loss: 0.00001977
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001976
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001975
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001974
Iteration 53/1000 | Loss: 0.00001974
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001973
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001973
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001972
Iteration 74/1000 | Loss: 0.00001972
Iteration 75/1000 | Loss: 0.00001972
Iteration 76/1000 | Loss: 0.00001972
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001971
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001969
Iteration 88/1000 | Loss: 0.00001969
Iteration 89/1000 | Loss: 0.00001969
Iteration 90/1000 | Loss: 0.00001969
Iteration 91/1000 | Loss: 0.00001969
Iteration 92/1000 | Loss: 0.00001969
Iteration 93/1000 | Loss: 0.00001969
Iteration 94/1000 | Loss: 0.00001969
Iteration 95/1000 | Loss: 0.00001969
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001968
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001967
Iteration 104/1000 | Loss: 0.00001967
Iteration 105/1000 | Loss: 0.00001967
Iteration 106/1000 | Loss: 0.00001967
Iteration 107/1000 | Loss: 0.00001967
Iteration 108/1000 | Loss: 0.00001967
Iteration 109/1000 | Loss: 0.00001967
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001967
Iteration 115/1000 | Loss: 0.00001967
Iteration 116/1000 | Loss: 0.00001967
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001966
Iteration 122/1000 | Loss: 0.00001966
Iteration 123/1000 | Loss: 0.00001966
Iteration 124/1000 | Loss: 0.00001966
Iteration 125/1000 | Loss: 0.00001965
Iteration 126/1000 | Loss: 0.00001965
Iteration 127/1000 | Loss: 0.00001965
Iteration 128/1000 | Loss: 0.00001965
Iteration 129/1000 | Loss: 0.00001965
Iteration 130/1000 | Loss: 0.00001965
Iteration 131/1000 | Loss: 0.00001965
Iteration 132/1000 | Loss: 0.00001965
Iteration 133/1000 | Loss: 0.00001964
Iteration 134/1000 | Loss: 0.00001964
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001963
Iteration 138/1000 | Loss: 0.00001962
Iteration 139/1000 | Loss: 0.00001962
Iteration 140/1000 | Loss: 0.00001962
Iteration 141/1000 | Loss: 0.00001962
Iteration 142/1000 | Loss: 0.00001962
Iteration 143/1000 | Loss: 0.00001961
Iteration 144/1000 | Loss: 0.00001961
Iteration 145/1000 | Loss: 0.00001960
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001960
Iteration 148/1000 | Loss: 0.00001960
Iteration 149/1000 | Loss: 0.00001960
Iteration 150/1000 | Loss: 0.00001960
Iteration 151/1000 | Loss: 0.00001960
Iteration 152/1000 | Loss: 0.00001960
Iteration 153/1000 | Loss: 0.00001960
Iteration 154/1000 | Loss: 0.00001959
Iteration 155/1000 | Loss: 0.00001959
Iteration 156/1000 | Loss: 0.00001959
Iteration 157/1000 | Loss: 0.00001959
Iteration 158/1000 | Loss: 0.00001959
Iteration 159/1000 | Loss: 0.00001959
Iteration 160/1000 | Loss: 0.00001958
Iteration 161/1000 | Loss: 0.00001958
Iteration 162/1000 | Loss: 0.00001958
Iteration 163/1000 | Loss: 0.00001958
Iteration 164/1000 | Loss: 0.00001957
Iteration 165/1000 | Loss: 0.00001957
Iteration 166/1000 | Loss: 0.00001957
Iteration 167/1000 | Loss: 0.00001957
Iteration 168/1000 | Loss: 0.00001957
Iteration 169/1000 | Loss: 0.00001957
Iteration 170/1000 | Loss: 0.00001957
Iteration 171/1000 | Loss: 0.00001957
Iteration 172/1000 | Loss: 0.00001957
Iteration 173/1000 | Loss: 0.00001957
Iteration 174/1000 | Loss: 0.00001957
Iteration 175/1000 | Loss: 0.00001957
Iteration 176/1000 | Loss: 0.00001957
Iteration 177/1000 | Loss: 0.00001957
Iteration 178/1000 | Loss: 0.00001957
Iteration 179/1000 | Loss: 0.00001957
Iteration 180/1000 | Loss: 0.00001957
Iteration 181/1000 | Loss: 0.00001957
Iteration 182/1000 | Loss: 0.00001957
Iteration 183/1000 | Loss: 0.00001957
Iteration 184/1000 | Loss: 0.00001957
Iteration 185/1000 | Loss: 0.00001957
Iteration 186/1000 | Loss: 0.00001957
Iteration 187/1000 | Loss: 0.00001957
Iteration 188/1000 | Loss: 0.00001957
Iteration 189/1000 | Loss: 0.00001957
Iteration 190/1000 | Loss: 0.00001957
Iteration 191/1000 | Loss: 0.00001957
Iteration 192/1000 | Loss: 0.00001957
Iteration 193/1000 | Loss: 0.00001957
Iteration 194/1000 | Loss: 0.00001957
Iteration 195/1000 | Loss: 0.00001957
Iteration 196/1000 | Loss: 0.00001957
Iteration 197/1000 | Loss: 0.00001957
Iteration 198/1000 | Loss: 0.00001957
Iteration 199/1000 | Loss: 0.00001957
Iteration 200/1000 | Loss: 0.00001957
Iteration 201/1000 | Loss: 0.00001957
Iteration 202/1000 | Loss: 0.00001957
Iteration 203/1000 | Loss: 0.00001957
Iteration 204/1000 | Loss: 0.00001957
Iteration 205/1000 | Loss: 0.00001957
Iteration 206/1000 | Loss: 0.00001957
Iteration 207/1000 | Loss: 0.00001957
Iteration 208/1000 | Loss: 0.00001957
Iteration 209/1000 | Loss: 0.00001957
Iteration 210/1000 | Loss: 0.00001957
Iteration 211/1000 | Loss: 0.00001957
Iteration 212/1000 | Loss: 0.00001957
Iteration 213/1000 | Loss: 0.00001957
Iteration 214/1000 | Loss: 0.00001957
Iteration 215/1000 | Loss: 0.00001957
Iteration 216/1000 | Loss: 0.00001957
Iteration 217/1000 | Loss: 0.00001957
Iteration 218/1000 | Loss: 0.00001957
Iteration 219/1000 | Loss: 0.00001957
Iteration 220/1000 | Loss: 0.00001957
Iteration 221/1000 | Loss: 0.00001957
Iteration 222/1000 | Loss: 0.00001957
Iteration 223/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.9565404727472924e-05, 1.9565404727472924e-05, 1.9565404727472924e-05, 1.9565404727472924e-05, 1.9565404727472924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9565404727472924e-05

Optimization complete. Final v2v error: 3.7860560417175293 mm

Highest mean error: 4.078996181488037 mm for frame 149

Lowest mean error: 3.6122848987579346 mm for frame 26

Saving results

Total time: 42.16995286941528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959152
Iteration 2/25 | Loss: 0.00198234
Iteration 3/25 | Loss: 0.00142740
Iteration 4/25 | Loss: 0.00138710
Iteration 5/25 | Loss: 0.00138301
Iteration 6/25 | Loss: 0.00138225
Iteration 7/25 | Loss: 0.00138225
Iteration 8/25 | Loss: 0.00138225
Iteration 9/25 | Loss: 0.00138225
Iteration 10/25 | Loss: 0.00138225
Iteration 11/25 | Loss: 0.00138225
Iteration 12/25 | Loss: 0.00138225
Iteration 13/25 | Loss: 0.00138225
Iteration 14/25 | Loss: 0.00138225
Iteration 15/25 | Loss: 0.00138225
Iteration 16/25 | Loss: 0.00138225
Iteration 17/25 | Loss: 0.00138225
Iteration 18/25 | Loss: 0.00138225
Iteration 19/25 | Loss: 0.00138225
Iteration 20/25 | Loss: 0.00138225
Iteration 21/25 | Loss: 0.00138225
Iteration 22/25 | Loss: 0.00138225
Iteration 23/25 | Loss: 0.00138225
Iteration 24/25 | Loss: 0.00138225
Iteration 25/25 | Loss: 0.00138225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43489927
Iteration 2/25 | Loss: 0.00054129
Iteration 3/25 | Loss: 0.00054129
Iteration 4/25 | Loss: 0.00054129
Iteration 5/25 | Loss: 0.00054129
Iteration 6/25 | Loss: 0.00054129
Iteration 7/25 | Loss: 0.00054128
Iteration 8/25 | Loss: 0.00054128
Iteration 9/25 | Loss: 0.00054128
Iteration 10/25 | Loss: 0.00054128
Iteration 11/25 | Loss: 0.00054128
Iteration 12/25 | Loss: 0.00054128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005412843311205506, 0.0005412843311205506, 0.0005412843311205506, 0.0005412843311205506, 0.0005412843311205506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005412843311205506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054128
Iteration 2/1000 | Loss: 0.00005459
Iteration 3/1000 | Loss: 0.00003788
Iteration 4/1000 | Loss: 0.00003447
Iteration 5/1000 | Loss: 0.00003332
Iteration 6/1000 | Loss: 0.00003264
Iteration 7/1000 | Loss: 0.00003217
Iteration 8/1000 | Loss: 0.00003153
Iteration 9/1000 | Loss: 0.00003118
Iteration 10/1000 | Loss: 0.00003073
Iteration 11/1000 | Loss: 0.00003034
Iteration 12/1000 | Loss: 0.00003008
Iteration 13/1000 | Loss: 0.00002993
Iteration 14/1000 | Loss: 0.00002974
Iteration 15/1000 | Loss: 0.00002969
Iteration 16/1000 | Loss: 0.00002954
Iteration 17/1000 | Loss: 0.00002945
Iteration 18/1000 | Loss: 0.00002936
Iteration 19/1000 | Loss: 0.00002936
Iteration 20/1000 | Loss: 0.00002934
Iteration 21/1000 | Loss: 0.00002922
Iteration 22/1000 | Loss: 0.00002920
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002901
Iteration 25/1000 | Loss: 0.00002900
Iteration 26/1000 | Loss: 0.00002900
Iteration 27/1000 | Loss: 0.00002900
Iteration 28/1000 | Loss: 0.00002900
Iteration 29/1000 | Loss: 0.00002900
Iteration 30/1000 | Loss: 0.00002900
Iteration 31/1000 | Loss: 0.00002898
Iteration 32/1000 | Loss: 0.00002897
Iteration 33/1000 | Loss: 0.00002893
Iteration 34/1000 | Loss: 0.00002892
Iteration 35/1000 | Loss: 0.00002880
Iteration 36/1000 | Loss: 0.00002873
Iteration 37/1000 | Loss: 0.00002873
Iteration 38/1000 | Loss: 0.00002861
Iteration 39/1000 | Loss: 0.00002859
Iteration 40/1000 | Loss: 0.00002859
Iteration 41/1000 | Loss: 0.00002858
Iteration 42/1000 | Loss: 0.00002858
Iteration 43/1000 | Loss: 0.00002854
Iteration 44/1000 | Loss: 0.00002844
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00002843
Iteration 47/1000 | Loss: 0.00002842
Iteration 48/1000 | Loss: 0.00002842
Iteration 49/1000 | Loss: 0.00002842
Iteration 50/1000 | Loss: 0.00002842
Iteration 51/1000 | Loss: 0.00002842
Iteration 52/1000 | Loss: 0.00002842
Iteration 53/1000 | Loss: 0.00002841
Iteration 54/1000 | Loss: 0.00002841
Iteration 55/1000 | Loss: 0.00002841
Iteration 56/1000 | Loss: 0.00002841
Iteration 57/1000 | Loss: 0.00002841
Iteration 58/1000 | Loss: 0.00002839
Iteration 59/1000 | Loss: 0.00002839
Iteration 60/1000 | Loss: 0.00002839
Iteration 61/1000 | Loss: 0.00002839
Iteration 62/1000 | Loss: 0.00002839
Iteration 63/1000 | Loss: 0.00002839
Iteration 64/1000 | Loss: 0.00002839
Iteration 65/1000 | Loss: 0.00002839
Iteration 66/1000 | Loss: 0.00002839
Iteration 67/1000 | Loss: 0.00002839
Iteration 68/1000 | Loss: 0.00002839
Iteration 69/1000 | Loss: 0.00002839
Iteration 70/1000 | Loss: 0.00002839
Iteration 71/1000 | Loss: 0.00002839
Iteration 72/1000 | Loss: 0.00002839
Iteration 73/1000 | Loss: 0.00002838
Iteration 74/1000 | Loss: 0.00002838
Iteration 75/1000 | Loss: 0.00002838
Iteration 76/1000 | Loss: 0.00002838
Iteration 77/1000 | Loss: 0.00002838
Iteration 78/1000 | Loss: 0.00002837
Iteration 79/1000 | Loss: 0.00002837
Iteration 80/1000 | Loss: 0.00002837
Iteration 81/1000 | Loss: 0.00002837
Iteration 82/1000 | Loss: 0.00002837
Iteration 83/1000 | Loss: 0.00002837
Iteration 84/1000 | Loss: 0.00002837
Iteration 85/1000 | Loss: 0.00002837
Iteration 86/1000 | Loss: 0.00002837
Iteration 87/1000 | Loss: 0.00002836
Iteration 88/1000 | Loss: 0.00002836
Iteration 89/1000 | Loss: 0.00002836
Iteration 90/1000 | Loss: 0.00002836
Iteration 91/1000 | Loss: 0.00002836
Iteration 92/1000 | Loss: 0.00002836
Iteration 93/1000 | Loss: 0.00002836
Iteration 94/1000 | Loss: 0.00002836
Iteration 95/1000 | Loss: 0.00002836
Iteration 96/1000 | Loss: 0.00002836
Iteration 97/1000 | Loss: 0.00002836
Iteration 98/1000 | Loss: 0.00002835
Iteration 99/1000 | Loss: 0.00002835
Iteration 100/1000 | Loss: 0.00002835
Iteration 101/1000 | Loss: 0.00002835
Iteration 102/1000 | Loss: 0.00002834
Iteration 103/1000 | Loss: 0.00002834
Iteration 104/1000 | Loss: 0.00002834
Iteration 105/1000 | Loss: 0.00002834
Iteration 106/1000 | Loss: 0.00002834
Iteration 107/1000 | Loss: 0.00002834
Iteration 108/1000 | Loss: 0.00002834
Iteration 109/1000 | Loss: 0.00002834
Iteration 110/1000 | Loss: 0.00002834
Iteration 111/1000 | Loss: 0.00002834
Iteration 112/1000 | Loss: 0.00002834
Iteration 113/1000 | Loss: 0.00002834
Iteration 114/1000 | Loss: 0.00002834
Iteration 115/1000 | Loss: 0.00002834
Iteration 116/1000 | Loss: 0.00002834
Iteration 117/1000 | Loss: 0.00002834
Iteration 118/1000 | Loss: 0.00002834
Iteration 119/1000 | Loss: 0.00002834
Iteration 120/1000 | Loss: 0.00002834
Iteration 121/1000 | Loss: 0.00002834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.833904727594927e-05, 2.833904727594927e-05, 2.833904727594927e-05, 2.833904727594927e-05, 2.833904727594927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.833904727594927e-05

Optimization complete. Final v2v error: 4.417391300201416 mm

Highest mean error: 4.598597526550293 mm for frame 135

Lowest mean error: 4.060563564300537 mm for frame 13

Saving results

Total time: 49.652671337127686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822880
Iteration 2/25 | Loss: 0.00147224
Iteration 3/25 | Loss: 0.00124614
Iteration 4/25 | Loss: 0.00122364
Iteration 5/25 | Loss: 0.00122113
Iteration 6/25 | Loss: 0.00122113
Iteration 7/25 | Loss: 0.00122113
Iteration 8/25 | Loss: 0.00122113
Iteration 9/25 | Loss: 0.00122113
Iteration 10/25 | Loss: 0.00122113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012211287394165993, 0.0012211287394165993, 0.0012211287394165993, 0.0012211287394165993, 0.0012211287394165993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012211287394165993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97754198
Iteration 2/25 | Loss: 0.00053263
Iteration 3/25 | Loss: 0.00053263
Iteration 4/25 | Loss: 0.00053263
Iteration 5/25 | Loss: 0.00053263
Iteration 6/25 | Loss: 0.00053262
Iteration 7/25 | Loss: 0.00053262
Iteration 8/25 | Loss: 0.00053262
Iteration 9/25 | Loss: 0.00053262
Iteration 10/25 | Loss: 0.00053262
Iteration 11/25 | Loss: 0.00053262
Iteration 12/25 | Loss: 0.00053262
Iteration 13/25 | Loss: 0.00053262
Iteration 14/25 | Loss: 0.00053262
Iteration 15/25 | Loss: 0.00053262
Iteration 16/25 | Loss: 0.00053262
Iteration 17/25 | Loss: 0.00053262
Iteration 18/25 | Loss: 0.00053262
Iteration 19/25 | Loss: 0.00053262
Iteration 20/25 | Loss: 0.00053262
Iteration 21/25 | Loss: 0.00053262
Iteration 22/25 | Loss: 0.00053262
Iteration 23/25 | Loss: 0.00053262
Iteration 24/25 | Loss: 0.00053262
Iteration 25/25 | Loss: 0.00053262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053262
Iteration 2/1000 | Loss: 0.00004022
Iteration 3/1000 | Loss: 0.00002968
Iteration 4/1000 | Loss: 0.00002669
Iteration 5/1000 | Loss: 0.00002532
Iteration 6/1000 | Loss: 0.00002441
Iteration 7/1000 | Loss: 0.00002383
Iteration 8/1000 | Loss: 0.00002341
Iteration 9/1000 | Loss: 0.00002303
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00002249
Iteration 12/1000 | Loss: 0.00002210
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002182
Iteration 15/1000 | Loss: 0.00002172
Iteration 16/1000 | Loss: 0.00002164
Iteration 17/1000 | Loss: 0.00002161
Iteration 18/1000 | Loss: 0.00002150
Iteration 19/1000 | Loss: 0.00002146
Iteration 20/1000 | Loss: 0.00002138
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00002137
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002133
Iteration 25/1000 | Loss: 0.00002131
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002129
Iteration 31/1000 | Loss: 0.00002129
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002125
Iteration 35/1000 | Loss: 0.00002123
Iteration 36/1000 | Loss: 0.00002123
Iteration 37/1000 | Loss: 0.00002122
Iteration 38/1000 | Loss: 0.00002122
Iteration 39/1000 | Loss: 0.00002121
Iteration 40/1000 | Loss: 0.00002120
Iteration 41/1000 | Loss: 0.00002120
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002118
Iteration 46/1000 | Loss: 0.00002118
Iteration 47/1000 | Loss: 0.00002118
Iteration 48/1000 | Loss: 0.00002118
Iteration 49/1000 | Loss: 0.00002118
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00002118
Iteration 52/1000 | Loss: 0.00002118
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002117
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002116
Iteration 71/1000 | Loss: 0.00002116
Iteration 72/1000 | Loss: 0.00002116
Iteration 73/1000 | Loss: 0.00002116
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002115
Iteration 76/1000 | Loss: 0.00002115
Iteration 77/1000 | Loss: 0.00002115
Iteration 78/1000 | Loss: 0.00002115
Iteration 79/1000 | Loss: 0.00002115
Iteration 80/1000 | Loss: 0.00002115
Iteration 81/1000 | Loss: 0.00002115
Iteration 82/1000 | Loss: 0.00002115
Iteration 83/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.1152431145310402e-05, 2.1152431145310402e-05, 2.1152431145310402e-05, 2.1152431145310402e-05, 2.1152431145310402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1152431145310402e-05

Optimization complete. Final v2v error: 3.8600857257843018 mm

Highest mean error: 3.888514280319214 mm for frame 0

Lowest mean error: 3.8382275104522705 mm for frame 119

Saving results

Total time: 37.18104577064514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00674891
Iteration 2/25 | Loss: 0.00187970
Iteration 3/25 | Loss: 0.00157448
Iteration 4/25 | Loss: 0.00121567
Iteration 5/25 | Loss: 0.00119628
Iteration 6/25 | Loss: 0.00118911
Iteration 7/25 | Loss: 0.00119770
Iteration 8/25 | Loss: 0.00117560
Iteration 9/25 | Loss: 0.00117389
Iteration 10/25 | Loss: 0.00117369
Iteration 11/25 | Loss: 0.00117354
Iteration 12/25 | Loss: 0.00117331
Iteration 13/25 | Loss: 0.00117306
Iteration 14/25 | Loss: 0.00117294
Iteration 15/25 | Loss: 0.00117291
Iteration 16/25 | Loss: 0.00117290
Iteration 17/25 | Loss: 0.00117290
Iteration 18/25 | Loss: 0.00117290
Iteration 19/25 | Loss: 0.00117289
Iteration 20/25 | Loss: 0.00117289
Iteration 21/25 | Loss: 0.00117289
Iteration 22/25 | Loss: 0.00117289
Iteration 23/25 | Loss: 0.00117289
Iteration 24/25 | Loss: 0.00117289
Iteration 25/25 | Loss: 0.00117289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11853099
Iteration 2/25 | Loss: 0.00143169
Iteration 3/25 | Loss: 0.00143169
Iteration 4/25 | Loss: 0.00118831
Iteration 5/25 | Loss: 0.00118831
Iteration 6/25 | Loss: 0.00118831
Iteration 7/25 | Loss: 0.00118830
Iteration 8/25 | Loss: 0.00118830
Iteration 9/25 | Loss: 0.00118830
Iteration 10/25 | Loss: 0.00118830
Iteration 11/25 | Loss: 0.00118830
Iteration 12/25 | Loss: 0.00118830
Iteration 13/25 | Loss: 0.00118830
Iteration 14/25 | Loss: 0.00118830
Iteration 15/25 | Loss: 0.00118830
Iteration 16/25 | Loss: 0.00118830
Iteration 17/25 | Loss: 0.00118830
Iteration 18/25 | Loss: 0.00118830
Iteration 19/25 | Loss: 0.00118830
Iteration 20/25 | Loss: 0.00118830
Iteration 21/25 | Loss: 0.00118830
Iteration 22/25 | Loss: 0.00118830
Iteration 23/25 | Loss: 0.00118830
Iteration 24/25 | Loss: 0.00118830
Iteration 25/25 | Loss: 0.00118830

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118830
Iteration 2/1000 | Loss: 0.00004525
Iteration 3/1000 | Loss: 0.00003567
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00002489
Iteration 6/1000 | Loss: 0.00002352
Iteration 7/1000 | Loss: 0.00002251
Iteration 8/1000 | Loss: 0.00002191
Iteration 9/1000 | Loss: 0.00002145
Iteration 10/1000 | Loss: 0.00002105
Iteration 11/1000 | Loss: 0.00002071
Iteration 12/1000 | Loss: 0.00039202
Iteration 13/1000 | Loss: 0.00002393
Iteration 14/1000 | Loss: 0.00002124
Iteration 15/1000 | Loss: 0.00002024
Iteration 16/1000 | Loss: 0.00001959
Iteration 17/1000 | Loss: 0.00001905
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001847
Iteration 25/1000 | Loss: 0.00001846
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001840
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001831
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001829
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001822
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001818
Iteration 59/1000 | Loss: 0.00001818
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001817
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001815
Iteration 66/1000 | Loss: 0.00001815
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001813
Iteration 76/1000 | Loss: 0.00001813
Iteration 77/1000 | Loss: 0.00001813
Iteration 78/1000 | Loss: 0.00001812
Iteration 79/1000 | Loss: 0.00001812
Iteration 80/1000 | Loss: 0.00001812
Iteration 81/1000 | Loss: 0.00001812
Iteration 82/1000 | Loss: 0.00001811
Iteration 83/1000 | Loss: 0.00001811
Iteration 84/1000 | Loss: 0.00001811
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001810
Iteration 89/1000 | Loss: 0.00001810
Iteration 90/1000 | Loss: 0.00001809
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00001809
Iteration 93/1000 | Loss: 0.00001808
Iteration 94/1000 | Loss: 0.00001808
Iteration 95/1000 | Loss: 0.00001808
Iteration 96/1000 | Loss: 0.00001808
Iteration 97/1000 | Loss: 0.00001808
Iteration 98/1000 | Loss: 0.00001808
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001807
Iteration 104/1000 | Loss: 0.00001807
Iteration 105/1000 | Loss: 0.00001807
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001804
Iteration 114/1000 | Loss: 0.00001804
Iteration 115/1000 | Loss: 0.00001804
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001803
Iteration 118/1000 | Loss: 0.00001803
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001802
Iteration 126/1000 | Loss: 0.00001802
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Iteration 136/1000 | Loss: 0.00001801
Iteration 137/1000 | Loss: 0.00001801
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001800
Iteration 143/1000 | Loss: 0.00001800
Iteration 144/1000 | Loss: 0.00001800
Iteration 145/1000 | Loss: 0.00001800
Iteration 146/1000 | Loss: 0.00001800
Iteration 147/1000 | Loss: 0.00001800
Iteration 148/1000 | Loss: 0.00001800
Iteration 149/1000 | Loss: 0.00001800
Iteration 150/1000 | Loss: 0.00001800
Iteration 151/1000 | Loss: 0.00001800
Iteration 152/1000 | Loss: 0.00001800
Iteration 153/1000 | Loss: 0.00001800
Iteration 154/1000 | Loss: 0.00001800
Iteration 155/1000 | Loss: 0.00001800
Iteration 156/1000 | Loss: 0.00001800
Iteration 157/1000 | Loss: 0.00001800
Iteration 158/1000 | Loss: 0.00001800
Iteration 159/1000 | Loss: 0.00001800
Iteration 160/1000 | Loss: 0.00001800
Iteration 161/1000 | Loss: 0.00001800
Iteration 162/1000 | Loss: 0.00001800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.8000697309616953e-05, 1.8000697309616953e-05, 1.8000697309616953e-05, 1.8000697309616953e-05, 1.8000697309616953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8000697309616953e-05

Optimization complete. Final v2v error: 3.450592041015625 mm

Highest mean error: 5.344959735870361 mm for frame 12

Lowest mean error: 2.485884189605713 mm for frame 115

Saving results

Total time: 78.49165916442871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830499
Iteration 2/25 | Loss: 0.00117109
Iteration 3/25 | Loss: 0.00109459
Iteration 4/25 | Loss: 0.00108246
Iteration 5/25 | Loss: 0.00107942
Iteration 6/25 | Loss: 0.00107942
Iteration 7/25 | Loss: 0.00107942
Iteration 8/25 | Loss: 0.00107942
Iteration 9/25 | Loss: 0.00107942
Iteration 10/25 | Loss: 0.00107942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010794239351525903, 0.0010794239351525903, 0.0010794239351525903, 0.0010794239351525903, 0.0010794239351525903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010794239351525903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36634958
Iteration 2/25 | Loss: 0.00079268
Iteration 3/25 | Loss: 0.00079267
Iteration 4/25 | Loss: 0.00079267
Iteration 5/25 | Loss: 0.00079267
Iteration 6/25 | Loss: 0.00079267
Iteration 7/25 | Loss: 0.00079267
Iteration 8/25 | Loss: 0.00079267
Iteration 9/25 | Loss: 0.00079267
Iteration 10/25 | Loss: 0.00079267
Iteration 11/25 | Loss: 0.00079267
Iteration 12/25 | Loss: 0.00079267
Iteration 13/25 | Loss: 0.00079267
Iteration 14/25 | Loss: 0.00079267
Iteration 15/25 | Loss: 0.00079267
Iteration 16/25 | Loss: 0.00079267
Iteration 17/25 | Loss: 0.00079267
Iteration 18/25 | Loss: 0.00079267
Iteration 19/25 | Loss: 0.00079267
Iteration 20/25 | Loss: 0.00079267
Iteration 21/25 | Loss: 0.00079267
Iteration 22/25 | Loss: 0.00079267
Iteration 23/25 | Loss: 0.00079267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007926676771603525, 0.0007926676771603525, 0.0007926676771603525, 0.0007926676771603525, 0.0007926676771603525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007926676771603525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079267
Iteration 2/1000 | Loss: 0.00001990
Iteration 3/1000 | Loss: 0.00001511
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001318
Iteration 6/1000 | Loss: 0.00001270
Iteration 7/1000 | Loss: 0.00001236
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001144
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001144
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001140
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001135
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001121
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001117
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001117
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001100
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001098
Iteration 93/1000 | Loss: 0.00001098
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001097
Iteration 96/1000 | Loss: 0.00001097
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001094
Iteration 107/1000 | Loss: 0.00001094
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001087
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001086
Iteration 145/1000 | Loss: 0.00001086
Iteration 146/1000 | Loss: 0.00001086
Iteration 147/1000 | Loss: 0.00001086
Iteration 148/1000 | Loss: 0.00001086
Iteration 149/1000 | Loss: 0.00001086
Iteration 150/1000 | Loss: 0.00001086
Iteration 151/1000 | Loss: 0.00001086
Iteration 152/1000 | Loss: 0.00001086
Iteration 153/1000 | Loss: 0.00001086
Iteration 154/1000 | Loss: 0.00001085
Iteration 155/1000 | Loss: 0.00001085
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001085
Iteration 158/1000 | Loss: 0.00001085
Iteration 159/1000 | Loss: 0.00001085
Iteration 160/1000 | Loss: 0.00001085
Iteration 161/1000 | Loss: 0.00001085
Iteration 162/1000 | Loss: 0.00001085
Iteration 163/1000 | Loss: 0.00001085
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001084
Iteration 166/1000 | Loss: 0.00001084
Iteration 167/1000 | Loss: 0.00001084
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001083
Iteration 170/1000 | Loss: 0.00001083
Iteration 171/1000 | Loss: 0.00001083
Iteration 172/1000 | Loss: 0.00001083
Iteration 173/1000 | Loss: 0.00001083
Iteration 174/1000 | Loss: 0.00001083
Iteration 175/1000 | Loss: 0.00001083
Iteration 176/1000 | Loss: 0.00001083
Iteration 177/1000 | Loss: 0.00001083
Iteration 178/1000 | Loss: 0.00001083
Iteration 179/1000 | Loss: 0.00001083
Iteration 180/1000 | Loss: 0.00001083
Iteration 181/1000 | Loss: 0.00001083
Iteration 182/1000 | Loss: 0.00001083
Iteration 183/1000 | Loss: 0.00001083
Iteration 184/1000 | Loss: 0.00001083
Iteration 185/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.0828285667230375e-05, 1.0828285667230375e-05, 1.0828285667230375e-05, 1.0828285667230375e-05, 1.0828285667230375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0828285667230375e-05

Optimization complete. Final v2v error: 2.799347400665283 mm

Highest mean error: 3.215649366378784 mm for frame 97

Lowest mean error: 2.6643686294555664 mm for frame 143

Saving results

Total time: 42.80977153778076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986732
Iteration 2/25 | Loss: 0.00212275
Iteration 3/25 | Loss: 0.00155278
Iteration 4/25 | Loss: 0.00141804
Iteration 5/25 | Loss: 0.00156069
Iteration 6/25 | Loss: 0.00139686
Iteration 7/25 | Loss: 0.00129010
Iteration 8/25 | Loss: 0.00124299
Iteration 9/25 | Loss: 0.00121346
Iteration 10/25 | Loss: 0.00119723
Iteration 11/25 | Loss: 0.00119637
Iteration 12/25 | Loss: 0.00120047
Iteration 13/25 | Loss: 0.00119320
Iteration 14/25 | Loss: 0.00117883
Iteration 15/25 | Loss: 0.00118196
Iteration 16/25 | Loss: 0.00116928
Iteration 17/25 | Loss: 0.00115953
Iteration 18/25 | Loss: 0.00116203
Iteration 19/25 | Loss: 0.00115380
Iteration 20/25 | Loss: 0.00115519
Iteration 21/25 | Loss: 0.00116290
Iteration 22/25 | Loss: 0.00115717
Iteration 23/25 | Loss: 0.00115939
Iteration 24/25 | Loss: 0.00115045
Iteration 25/25 | Loss: 0.00114696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35988462
Iteration 2/25 | Loss: 0.00107911
Iteration 3/25 | Loss: 0.00107910
Iteration 4/25 | Loss: 0.00101569
Iteration 5/25 | Loss: 0.00101567
Iteration 6/25 | Loss: 0.00101567
Iteration 7/25 | Loss: 0.00101567
Iteration 8/25 | Loss: 0.00101567
Iteration 9/25 | Loss: 0.00101567
Iteration 10/25 | Loss: 0.00101567
Iteration 11/25 | Loss: 0.00101567
Iteration 12/25 | Loss: 0.00101567
Iteration 13/25 | Loss: 0.00101567
Iteration 14/25 | Loss: 0.00101567
Iteration 15/25 | Loss: 0.00101567
Iteration 16/25 | Loss: 0.00101567
Iteration 17/25 | Loss: 0.00101567
Iteration 18/25 | Loss: 0.00101567
Iteration 19/25 | Loss: 0.00101567
Iteration 20/25 | Loss: 0.00101567
Iteration 21/25 | Loss: 0.00101567
Iteration 22/25 | Loss: 0.00101567
Iteration 23/25 | Loss: 0.00101567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001015666639432311, 0.001015666639432311, 0.001015666639432311, 0.001015666639432311, 0.001015666639432311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001015666639432311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101567
Iteration 2/1000 | Loss: 0.00009389
Iteration 3/1000 | Loss: 0.00021771
Iteration 4/1000 | Loss: 0.00010997
Iteration 5/1000 | Loss: 0.00010502
Iteration 6/1000 | Loss: 0.00011732
Iteration 7/1000 | Loss: 0.00007102
Iteration 8/1000 | Loss: 0.00099737
Iteration 9/1000 | Loss: 0.00046811
Iteration 10/1000 | Loss: 0.00056801
Iteration 11/1000 | Loss: 0.00005730
Iteration 12/1000 | Loss: 0.00003633
Iteration 13/1000 | Loss: 0.00003236
Iteration 14/1000 | Loss: 0.00002663
Iteration 15/1000 | Loss: 0.00002430
Iteration 16/1000 | Loss: 0.00007112
Iteration 17/1000 | Loss: 0.00002258
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002116
Iteration 20/1000 | Loss: 0.00002053
Iteration 21/1000 | Loss: 0.00031580
Iteration 22/1000 | Loss: 0.00013546
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002901
Iteration 25/1000 | Loss: 0.00004835
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00002722
Iteration 28/1000 | Loss: 0.00001458
Iteration 29/1000 | Loss: 0.00004290
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001386
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001287
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001281
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001277
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001275
Iteration 59/1000 | Loss: 0.00001275
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001271
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001271
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001271
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.2711852832580917e-05, 1.2711852832580917e-05, 1.2711852832580917e-05, 1.2711852832580917e-05, 1.2711852832580917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2711852832580917e-05

Optimization complete. Final v2v error: 3.0652406215667725 mm

Highest mean error: 3.899846315383911 mm for frame 71

Lowest mean error: 2.6636404991149902 mm for frame 125

Saving results

Total time: 103.67990350723267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418346
Iteration 2/25 | Loss: 0.00116683
Iteration 3/25 | Loss: 0.00110471
Iteration 4/25 | Loss: 0.00108955
Iteration 5/25 | Loss: 0.00108509
Iteration 6/25 | Loss: 0.00108424
Iteration 7/25 | Loss: 0.00108424
Iteration 8/25 | Loss: 0.00108424
Iteration 9/25 | Loss: 0.00108424
Iteration 10/25 | Loss: 0.00108424
Iteration 11/25 | Loss: 0.00108424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010842422489076853, 0.0010842422489076853, 0.0010842422489076853, 0.0010842422489076853, 0.0010842422489076853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010842422489076853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52607119
Iteration 2/25 | Loss: 0.00081991
Iteration 3/25 | Loss: 0.00081991
Iteration 4/25 | Loss: 0.00081991
Iteration 5/25 | Loss: 0.00081991
Iteration 6/25 | Loss: 0.00081991
Iteration 7/25 | Loss: 0.00081991
Iteration 8/25 | Loss: 0.00081991
Iteration 9/25 | Loss: 0.00081991
Iteration 10/25 | Loss: 0.00081991
Iteration 11/25 | Loss: 0.00081991
Iteration 12/25 | Loss: 0.00081991
Iteration 13/25 | Loss: 0.00081991
Iteration 14/25 | Loss: 0.00081991
Iteration 15/25 | Loss: 0.00081991
Iteration 16/25 | Loss: 0.00081991
Iteration 17/25 | Loss: 0.00081991
Iteration 18/25 | Loss: 0.00081991
Iteration 19/25 | Loss: 0.00081991
Iteration 20/25 | Loss: 0.00081991
Iteration 21/25 | Loss: 0.00081991
Iteration 22/25 | Loss: 0.00081991
Iteration 23/25 | Loss: 0.00081991
Iteration 24/25 | Loss: 0.00081991
Iteration 25/25 | Loss: 0.00081991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081991
Iteration 2/1000 | Loss: 0.00002583
Iteration 3/1000 | Loss: 0.00001758
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001336
Iteration 8/1000 | Loss: 0.00001293
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001211
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001204
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001194
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001184
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001182
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001156
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001152
Iteration 134/1000 | Loss: 0.00001152
Iteration 135/1000 | Loss: 0.00001152
Iteration 136/1000 | Loss: 0.00001152
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001150
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001149
Iteration 147/1000 | Loss: 0.00001149
Iteration 148/1000 | Loss: 0.00001149
Iteration 149/1000 | Loss: 0.00001149
Iteration 150/1000 | Loss: 0.00001148
Iteration 151/1000 | Loss: 0.00001148
Iteration 152/1000 | Loss: 0.00001148
Iteration 153/1000 | Loss: 0.00001148
Iteration 154/1000 | Loss: 0.00001148
Iteration 155/1000 | Loss: 0.00001148
Iteration 156/1000 | Loss: 0.00001148
Iteration 157/1000 | Loss: 0.00001148
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001148
Iteration 164/1000 | Loss: 0.00001148
Iteration 165/1000 | Loss: 0.00001148
Iteration 166/1000 | Loss: 0.00001148
Iteration 167/1000 | Loss: 0.00001148
Iteration 168/1000 | Loss: 0.00001147
Iteration 169/1000 | Loss: 0.00001147
Iteration 170/1000 | Loss: 0.00001147
Iteration 171/1000 | Loss: 0.00001147
Iteration 172/1000 | Loss: 0.00001147
Iteration 173/1000 | Loss: 0.00001147
Iteration 174/1000 | Loss: 0.00001147
Iteration 175/1000 | Loss: 0.00001147
Iteration 176/1000 | Loss: 0.00001147
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001146
Iteration 183/1000 | Loss: 0.00001146
Iteration 184/1000 | Loss: 0.00001146
Iteration 185/1000 | Loss: 0.00001146
Iteration 186/1000 | Loss: 0.00001146
Iteration 187/1000 | Loss: 0.00001146
Iteration 188/1000 | Loss: 0.00001146
Iteration 189/1000 | Loss: 0.00001146
Iteration 190/1000 | Loss: 0.00001146
Iteration 191/1000 | Loss: 0.00001146
Iteration 192/1000 | Loss: 0.00001146
Iteration 193/1000 | Loss: 0.00001146
Iteration 194/1000 | Loss: 0.00001146
Iteration 195/1000 | Loss: 0.00001146
Iteration 196/1000 | Loss: 0.00001146
Iteration 197/1000 | Loss: 0.00001146
Iteration 198/1000 | Loss: 0.00001146
Iteration 199/1000 | Loss: 0.00001146
Iteration 200/1000 | Loss: 0.00001146
Iteration 201/1000 | Loss: 0.00001146
Iteration 202/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.1458109838713426e-05, 1.1458109838713426e-05, 1.1458109838713426e-05, 1.1458109838713426e-05, 1.1458109838713426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1458109838713426e-05

Optimization complete. Final v2v error: 2.9251723289489746 mm

Highest mean error: 3.323371648788452 mm for frame 89

Lowest mean error: 2.7942955493927 mm for frame 123

Saving results

Total time: 41.3442599773407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569199
Iteration 2/25 | Loss: 0.00127625
Iteration 3/25 | Loss: 0.00114579
Iteration 4/25 | Loss: 0.00113114
Iteration 5/25 | Loss: 0.00112702
Iteration 6/25 | Loss: 0.00112611
Iteration 7/25 | Loss: 0.00112611
Iteration 8/25 | Loss: 0.00112611
Iteration 9/25 | Loss: 0.00112611
Iteration 10/25 | Loss: 0.00112611
Iteration 11/25 | Loss: 0.00112611
Iteration 12/25 | Loss: 0.00112611
Iteration 13/25 | Loss: 0.00112611
Iteration 14/25 | Loss: 0.00112611
Iteration 15/25 | Loss: 0.00112611
Iteration 16/25 | Loss: 0.00112611
Iteration 17/25 | Loss: 0.00112611
Iteration 18/25 | Loss: 0.00112611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011261055478826165, 0.0011261055478826165, 0.0011261055478826165, 0.0011261055478826165, 0.0011261055478826165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011261055478826165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93087482
Iteration 2/25 | Loss: 0.00077601
Iteration 3/25 | Loss: 0.00077600
Iteration 4/25 | Loss: 0.00077600
Iteration 5/25 | Loss: 0.00077600
Iteration 6/25 | Loss: 0.00077600
Iteration 7/25 | Loss: 0.00077600
Iteration 8/25 | Loss: 0.00077600
Iteration 9/25 | Loss: 0.00077600
Iteration 10/25 | Loss: 0.00077600
Iteration 11/25 | Loss: 0.00077600
Iteration 12/25 | Loss: 0.00077600
Iteration 13/25 | Loss: 0.00077600
Iteration 14/25 | Loss: 0.00077600
Iteration 15/25 | Loss: 0.00077600
Iteration 16/25 | Loss: 0.00077600
Iteration 17/25 | Loss: 0.00077600
Iteration 18/25 | Loss: 0.00077599
Iteration 19/25 | Loss: 0.00077599
Iteration 20/25 | Loss: 0.00077599
Iteration 21/25 | Loss: 0.00077599
Iteration 22/25 | Loss: 0.00077599
Iteration 23/25 | Loss: 0.00077599
Iteration 24/25 | Loss: 0.00077599
Iteration 25/25 | Loss: 0.00077599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007759949075989425, 0.0007759949075989425, 0.0007759949075989425, 0.0007759949075989425, 0.0007759949075989425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007759949075989425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077599
Iteration 2/1000 | Loss: 0.00003159
Iteration 3/1000 | Loss: 0.00001786
Iteration 4/1000 | Loss: 0.00001483
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001333
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001280
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001222
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001215
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001213
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001213
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001212
Iteration 94/1000 | Loss: 0.00001212
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001212
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001211
Iteration 107/1000 | Loss: 0.00001211
Iteration 108/1000 | Loss: 0.00001211
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001211
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001211
Iteration 122/1000 | Loss: 0.00001211
Iteration 123/1000 | Loss: 0.00001211
Iteration 124/1000 | Loss: 0.00001211
Iteration 125/1000 | Loss: 0.00001211
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001211
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2105852874810807e-05, 1.2105852874810807e-05, 1.2105852874810807e-05, 1.2105852874810807e-05, 1.2105852874810807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2105852874810807e-05

Optimization complete. Final v2v error: 2.958556652069092 mm

Highest mean error: 3.1928887367248535 mm for frame 99

Lowest mean error: 2.7325496673583984 mm for frame 41

Saving results

Total time: 35.49949765205383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757430
Iteration 2/25 | Loss: 0.00132452
Iteration 3/25 | Loss: 0.00118391
Iteration 4/25 | Loss: 0.00116459
Iteration 5/25 | Loss: 0.00115813
Iteration 6/25 | Loss: 0.00115662
Iteration 7/25 | Loss: 0.00115648
Iteration 8/25 | Loss: 0.00115648
Iteration 9/25 | Loss: 0.00115648
Iteration 10/25 | Loss: 0.00115648
Iteration 11/25 | Loss: 0.00115648
Iteration 12/25 | Loss: 0.00115648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011564773740246892, 0.0011564773740246892, 0.0011564773740246892, 0.0011564773740246892, 0.0011564773740246892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011564773740246892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63233757
Iteration 2/25 | Loss: 0.00084841
Iteration 3/25 | Loss: 0.00084838
Iteration 4/25 | Loss: 0.00084838
Iteration 5/25 | Loss: 0.00084838
Iteration 6/25 | Loss: 0.00084838
Iteration 7/25 | Loss: 0.00084838
Iteration 8/25 | Loss: 0.00084838
Iteration 9/25 | Loss: 0.00084838
Iteration 10/25 | Loss: 0.00084838
Iteration 11/25 | Loss: 0.00084838
Iteration 12/25 | Loss: 0.00084838
Iteration 13/25 | Loss: 0.00084838
Iteration 14/25 | Loss: 0.00084838
Iteration 15/25 | Loss: 0.00084838
Iteration 16/25 | Loss: 0.00084838
Iteration 17/25 | Loss: 0.00084838
Iteration 18/25 | Loss: 0.00084838
Iteration 19/25 | Loss: 0.00084838
Iteration 20/25 | Loss: 0.00084838
Iteration 21/25 | Loss: 0.00084838
Iteration 22/25 | Loss: 0.00084838
Iteration 23/25 | Loss: 0.00084838
Iteration 24/25 | Loss: 0.00084838
Iteration 25/25 | Loss: 0.00084838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084838
Iteration 2/1000 | Loss: 0.00005196
Iteration 3/1000 | Loss: 0.00002798
Iteration 4/1000 | Loss: 0.00002026
Iteration 5/1000 | Loss: 0.00001835
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001691
Iteration 8/1000 | Loss: 0.00001656
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001605
Iteration 11/1000 | Loss: 0.00001582
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001536
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001530
Iteration 20/1000 | Loss: 0.00001529
Iteration 21/1000 | Loss: 0.00001529
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001528
Iteration 24/1000 | Loss: 0.00001527
Iteration 25/1000 | Loss: 0.00001526
Iteration 26/1000 | Loss: 0.00001525
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001522
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001515
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001512
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001500
Iteration 85/1000 | Loss: 0.00001500
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001497
Iteration 105/1000 | Loss: 0.00001497
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001496
Iteration 111/1000 | Loss: 0.00001496
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001496
Iteration 116/1000 | Loss: 0.00001496
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001495
Iteration 121/1000 | Loss: 0.00001495
Iteration 122/1000 | Loss: 0.00001495
Iteration 123/1000 | Loss: 0.00001495
Iteration 124/1000 | Loss: 0.00001495
Iteration 125/1000 | Loss: 0.00001495
Iteration 126/1000 | Loss: 0.00001494
Iteration 127/1000 | Loss: 0.00001494
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001494
Iteration 130/1000 | Loss: 0.00001494
Iteration 131/1000 | Loss: 0.00001494
Iteration 132/1000 | Loss: 0.00001494
Iteration 133/1000 | Loss: 0.00001494
Iteration 134/1000 | Loss: 0.00001494
Iteration 135/1000 | Loss: 0.00001494
Iteration 136/1000 | Loss: 0.00001494
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00001493
Iteration 140/1000 | Loss: 0.00001493
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001493
Iteration 143/1000 | Loss: 0.00001492
Iteration 144/1000 | Loss: 0.00001492
Iteration 145/1000 | Loss: 0.00001492
Iteration 146/1000 | Loss: 0.00001492
Iteration 147/1000 | Loss: 0.00001492
Iteration 148/1000 | Loss: 0.00001492
Iteration 149/1000 | Loss: 0.00001492
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001492
Iteration 153/1000 | Loss: 0.00001492
Iteration 154/1000 | Loss: 0.00001492
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001491
Iteration 162/1000 | Loss: 0.00001491
Iteration 163/1000 | Loss: 0.00001491
Iteration 164/1000 | Loss: 0.00001491
Iteration 165/1000 | Loss: 0.00001491
Iteration 166/1000 | Loss: 0.00001491
Iteration 167/1000 | Loss: 0.00001491
Iteration 168/1000 | Loss: 0.00001491
Iteration 169/1000 | Loss: 0.00001491
Iteration 170/1000 | Loss: 0.00001491
Iteration 171/1000 | Loss: 0.00001491
Iteration 172/1000 | Loss: 0.00001491
Iteration 173/1000 | Loss: 0.00001491
Iteration 174/1000 | Loss: 0.00001491
Iteration 175/1000 | Loss: 0.00001491
Iteration 176/1000 | Loss: 0.00001491
Iteration 177/1000 | Loss: 0.00001491
Iteration 178/1000 | Loss: 0.00001491
Iteration 179/1000 | Loss: 0.00001491
Iteration 180/1000 | Loss: 0.00001491
Iteration 181/1000 | Loss: 0.00001491
Iteration 182/1000 | Loss: 0.00001491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.4914001440047286e-05, 1.4914001440047286e-05, 1.4914001440047286e-05, 1.4914001440047286e-05, 1.4914001440047286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4914001440047286e-05

Optimization complete. Final v2v error: 3.30283784866333 mm

Highest mean error: 3.6917529106140137 mm for frame 48

Lowest mean error: 2.8247809410095215 mm for frame 6

Saving results

Total time: 41.124972343444824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758309
Iteration 2/25 | Loss: 0.00136501
Iteration 3/25 | Loss: 0.00109779
Iteration 4/25 | Loss: 0.00107968
Iteration 5/25 | Loss: 0.00107668
Iteration 6/25 | Loss: 0.00107623
Iteration 7/25 | Loss: 0.00107623
Iteration 8/25 | Loss: 0.00107623
Iteration 9/25 | Loss: 0.00107623
Iteration 10/25 | Loss: 0.00107623
Iteration 11/25 | Loss: 0.00107623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010762298479676247, 0.0010762298479676247, 0.0010762298479676247, 0.0010762298479676247, 0.0010762298479676247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010762298479676247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33941329
Iteration 2/25 | Loss: 0.00060149
Iteration 3/25 | Loss: 0.00060149
Iteration 4/25 | Loss: 0.00060148
Iteration 5/25 | Loss: 0.00060148
Iteration 6/25 | Loss: 0.00060148
Iteration 7/25 | Loss: 0.00060148
Iteration 8/25 | Loss: 0.00060148
Iteration 9/25 | Loss: 0.00060148
Iteration 10/25 | Loss: 0.00060148
Iteration 11/25 | Loss: 0.00060148
Iteration 12/25 | Loss: 0.00060148
Iteration 13/25 | Loss: 0.00060148
Iteration 14/25 | Loss: 0.00060148
Iteration 15/25 | Loss: 0.00060148
Iteration 16/25 | Loss: 0.00060148
Iteration 17/25 | Loss: 0.00060148
Iteration 18/25 | Loss: 0.00060148
Iteration 19/25 | Loss: 0.00060148
Iteration 20/25 | Loss: 0.00060148
Iteration 21/25 | Loss: 0.00060148
Iteration 22/25 | Loss: 0.00060148
Iteration 23/25 | Loss: 0.00060148
Iteration 24/25 | Loss: 0.00060148
Iteration 25/25 | Loss: 0.00060148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060148
Iteration 2/1000 | Loss: 0.00002537
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001636
Iteration 5/1000 | Loss: 0.00001510
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001289
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001213
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001204
Iteration 41/1000 | Loss: 0.00001204
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001177
Iteration 119/1000 | Loss: 0.00001177
Iteration 120/1000 | Loss: 0.00001177
Iteration 121/1000 | Loss: 0.00001177
Iteration 122/1000 | Loss: 0.00001177
Iteration 123/1000 | Loss: 0.00001176
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001176
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001175
Iteration 131/1000 | Loss: 0.00001175
Iteration 132/1000 | Loss: 0.00001175
Iteration 133/1000 | Loss: 0.00001175
Iteration 134/1000 | Loss: 0.00001175
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001174
Iteration 137/1000 | Loss: 0.00001174
Iteration 138/1000 | Loss: 0.00001174
Iteration 139/1000 | Loss: 0.00001174
Iteration 140/1000 | Loss: 0.00001174
Iteration 141/1000 | Loss: 0.00001173
Iteration 142/1000 | Loss: 0.00001173
Iteration 143/1000 | Loss: 0.00001173
Iteration 144/1000 | Loss: 0.00001173
Iteration 145/1000 | Loss: 0.00001173
Iteration 146/1000 | Loss: 0.00001173
Iteration 147/1000 | Loss: 0.00001173
Iteration 148/1000 | Loss: 0.00001172
Iteration 149/1000 | Loss: 0.00001172
Iteration 150/1000 | Loss: 0.00001172
Iteration 151/1000 | Loss: 0.00001172
Iteration 152/1000 | Loss: 0.00001172
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001170
Iteration 161/1000 | Loss: 0.00001170
Iteration 162/1000 | Loss: 0.00001170
Iteration 163/1000 | Loss: 0.00001170
Iteration 164/1000 | Loss: 0.00001170
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001169
Iteration 168/1000 | Loss: 0.00001169
Iteration 169/1000 | Loss: 0.00001169
Iteration 170/1000 | Loss: 0.00001169
Iteration 171/1000 | Loss: 0.00001169
Iteration 172/1000 | Loss: 0.00001169
Iteration 173/1000 | Loss: 0.00001169
Iteration 174/1000 | Loss: 0.00001168
Iteration 175/1000 | Loss: 0.00001168
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001167
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001165
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001164
Iteration 187/1000 | Loss: 0.00001164
Iteration 188/1000 | Loss: 0.00001164
Iteration 189/1000 | Loss: 0.00001164
Iteration 190/1000 | Loss: 0.00001164
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Iteration 196/1000 | Loss: 0.00001163
Iteration 197/1000 | Loss: 0.00001163
Iteration 198/1000 | Loss: 0.00001162
Iteration 199/1000 | Loss: 0.00001162
Iteration 200/1000 | Loss: 0.00001162
Iteration 201/1000 | Loss: 0.00001162
Iteration 202/1000 | Loss: 0.00001162
Iteration 203/1000 | Loss: 0.00001162
Iteration 204/1000 | Loss: 0.00001162
Iteration 205/1000 | Loss: 0.00001162
Iteration 206/1000 | Loss: 0.00001162
Iteration 207/1000 | Loss: 0.00001162
Iteration 208/1000 | Loss: 0.00001162
Iteration 209/1000 | Loss: 0.00001162
Iteration 210/1000 | Loss: 0.00001162
Iteration 211/1000 | Loss: 0.00001162
Iteration 212/1000 | Loss: 0.00001162
Iteration 213/1000 | Loss: 0.00001162
Iteration 214/1000 | Loss: 0.00001161
Iteration 215/1000 | Loss: 0.00001161
Iteration 216/1000 | Loss: 0.00001161
Iteration 217/1000 | Loss: 0.00001161
Iteration 218/1000 | Loss: 0.00001161
Iteration 219/1000 | Loss: 0.00001161
Iteration 220/1000 | Loss: 0.00001161
Iteration 221/1000 | Loss: 0.00001160
Iteration 222/1000 | Loss: 0.00001160
Iteration 223/1000 | Loss: 0.00001160
Iteration 224/1000 | Loss: 0.00001160
Iteration 225/1000 | Loss: 0.00001160
Iteration 226/1000 | Loss: 0.00001160
Iteration 227/1000 | Loss: 0.00001160
Iteration 228/1000 | Loss: 0.00001160
Iteration 229/1000 | Loss: 0.00001160
Iteration 230/1000 | Loss: 0.00001160
Iteration 231/1000 | Loss: 0.00001160
Iteration 232/1000 | Loss: 0.00001159
Iteration 233/1000 | Loss: 0.00001159
Iteration 234/1000 | Loss: 0.00001159
Iteration 235/1000 | Loss: 0.00001159
Iteration 236/1000 | Loss: 0.00001159
Iteration 237/1000 | Loss: 0.00001159
Iteration 238/1000 | Loss: 0.00001159
Iteration 239/1000 | Loss: 0.00001159
Iteration 240/1000 | Loss: 0.00001159
Iteration 241/1000 | Loss: 0.00001159
Iteration 242/1000 | Loss: 0.00001158
Iteration 243/1000 | Loss: 0.00001158
Iteration 244/1000 | Loss: 0.00001158
Iteration 245/1000 | Loss: 0.00001158
Iteration 246/1000 | Loss: 0.00001158
Iteration 247/1000 | Loss: 0.00001158
Iteration 248/1000 | Loss: 0.00001158
Iteration 249/1000 | Loss: 0.00001158
Iteration 250/1000 | Loss: 0.00001158
Iteration 251/1000 | Loss: 0.00001158
Iteration 252/1000 | Loss: 0.00001158
Iteration 253/1000 | Loss: 0.00001157
Iteration 254/1000 | Loss: 0.00001157
Iteration 255/1000 | Loss: 0.00001157
Iteration 256/1000 | Loss: 0.00001157
Iteration 257/1000 | Loss: 0.00001157
Iteration 258/1000 | Loss: 0.00001157
Iteration 259/1000 | Loss: 0.00001157
Iteration 260/1000 | Loss: 0.00001157
Iteration 261/1000 | Loss: 0.00001157
Iteration 262/1000 | Loss: 0.00001157
Iteration 263/1000 | Loss: 0.00001157
Iteration 264/1000 | Loss: 0.00001157
Iteration 265/1000 | Loss: 0.00001157
Iteration 266/1000 | Loss: 0.00001157
Iteration 267/1000 | Loss: 0.00001157
Iteration 268/1000 | Loss: 0.00001157
Iteration 269/1000 | Loss: 0.00001157
Iteration 270/1000 | Loss: 0.00001156
Iteration 271/1000 | Loss: 0.00001156
Iteration 272/1000 | Loss: 0.00001156
Iteration 273/1000 | Loss: 0.00001156
Iteration 274/1000 | Loss: 0.00001156
Iteration 275/1000 | Loss: 0.00001156
Iteration 276/1000 | Loss: 0.00001156
Iteration 277/1000 | Loss: 0.00001156
Iteration 278/1000 | Loss: 0.00001156
Iteration 279/1000 | Loss: 0.00001156
Iteration 280/1000 | Loss: 0.00001156
Iteration 281/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.1564929991436657e-05, 1.1564929991436657e-05, 1.1564929991436657e-05, 1.1564929991436657e-05, 1.1564929991436657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1564929991436657e-05

Optimization complete. Final v2v error: 2.8293404579162598 mm

Highest mean error: 4.842372894287109 mm for frame 0

Lowest mean error: 2.4369492530822754 mm for frame 164

Saving results

Total time: 56.887526512145996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574533
Iteration 2/25 | Loss: 0.00137196
Iteration 3/25 | Loss: 0.00122192
Iteration 4/25 | Loss: 0.00118673
Iteration 5/25 | Loss: 0.00117594
Iteration 6/25 | Loss: 0.00117327
Iteration 7/25 | Loss: 0.00117438
Iteration 8/25 | Loss: 0.00117203
Iteration 9/25 | Loss: 0.00117081
Iteration 10/25 | Loss: 0.00116919
Iteration 11/25 | Loss: 0.00117468
Iteration 12/25 | Loss: 0.00116940
Iteration 13/25 | Loss: 0.00116856
Iteration 14/25 | Loss: 0.00116803
Iteration 15/25 | Loss: 0.00116714
Iteration 16/25 | Loss: 0.00116682
Iteration 17/25 | Loss: 0.00116659
Iteration 18/25 | Loss: 0.00117060
Iteration 19/25 | Loss: 0.00116868
Iteration 20/25 | Loss: 0.00116712
Iteration 21/25 | Loss: 0.00116393
Iteration 22/25 | Loss: 0.00116303
Iteration 23/25 | Loss: 0.00116292
Iteration 24/25 | Loss: 0.00116292
Iteration 25/25 | Loss: 0.00116292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16135287
Iteration 2/25 | Loss: 0.00082403
Iteration 3/25 | Loss: 0.00082402
Iteration 4/25 | Loss: 0.00082402
Iteration 5/25 | Loss: 0.00082402
Iteration 6/25 | Loss: 0.00082401
Iteration 7/25 | Loss: 0.00082401
Iteration 8/25 | Loss: 0.00082401
Iteration 9/25 | Loss: 0.00082401
Iteration 10/25 | Loss: 0.00082401
Iteration 11/25 | Loss: 0.00082401
Iteration 12/25 | Loss: 0.00082401
Iteration 13/25 | Loss: 0.00082401
Iteration 14/25 | Loss: 0.00082401
Iteration 15/25 | Loss: 0.00082401
Iteration 16/25 | Loss: 0.00082401
Iteration 17/25 | Loss: 0.00082401
Iteration 18/25 | Loss: 0.00082401
Iteration 19/25 | Loss: 0.00082401
Iteration 20/25 | Loss: 0.00082401
Iteration 21/25 | Loss: 0.00082401
Iteration 22/25 | Loss: 0.00082401
Iteration 23/25 | Loss: 0.00082401
Iteration 24/25 | Loss: 0.00082401
Iteration 25/25 | Loss: 0.00082401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082401
Iteration 2/1000 | Loss: 0.00004876
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00002622
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002420
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002324
Iteration 9/1000 | Loss: 0.00002279
Iteration 10/1000 | Loss: 0.00002255
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002207
Iteration 13/1000 | Loss: 0.00002188
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002185
Iteration 17/1000 | Loss: 0.00002168
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002165
Iteration 20/1000 | Loss: 0.00002164
Iteration 21/1000 | Loss: 0.00002164
Iteration 22/1000 | Loss: 0.00002160
Iteration 23/1000 | Loss: 0.00002160
Iteration 24/1000 | Loss: 0.00002155
Iteration 25/1000 | Loss: 0.00002154
Iteration 26/1000 | Loss: 0.00002151
Iteration 27/1000 | Loss: 0.00002147
Iteration 28/1000 | Loss: 0.00002146
Iteration 29/1000 | Loss: 0.00002146
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002143
Iteration 33/1000 | Loss: 0.00002143
Iteration 34/1000 | Loss: 0.00002143
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002142
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002141
Iteration 41/1000 | Loss: 0.00002141
Iteration 42/1000 | Loss: 0.00002140
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00002140
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002138
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002136
Iteration 57/1000 | Loss: 0.00002136
Iteration 58/1000 | Loss: 0.00002136
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002133
Iteration 68/1000 | Loss: 0.00002133
Iteration 69/1000 | Loss: 0.00002133
Iteration 70/1000 | Loss: 0.00002132
Iteration 71/1000 | Loss: 0.00002132
Iteration 72/1000 | Loss: 0.00002132
Iteration 73/1000 | Loss: 0.00002131
Iteration 74/1000 | Loss: 0.00002130
Iteration 75/1000 | Loss: 0.00002130
Iteration 76/1000 | Loss: 0.00002130
Iteration 77/1000 | Loss: 0.00002130
Iteration 78/1000 | Loss: 0.00002130
Iteration 79/1000 | Loss: 0.00002130
Iteration 80/1000 | Loss: 0.00002130
Iteration 81/1000 | Loss: 0.00002130
Iteration 82/1000 | Loss: 0.00002130
Iteration 83/1000 | Loss: 0.00002129
Iteration 84/1000 | Loss: 0.00002129
Iteration 85/1000 | Loss: 0.00002129
Iteration 86/1000 | Loss: 0.00002129
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002129
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002128
Iteration 91/1000 | Loss: 0.00002128
Iteration 92/1000 | Loss: 0.00002128
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002128
Iteration 96/1000 | Loss: 0.00002128
Iteration 97/1000 | Loss: 0.00002128
Iteration 98/1000 | Loss: 0.00002127
Iteration 99/1000 | Loss: 0.00002126
Iteration 100/1000 | Loss: 0.00002126
Iteration 101/1000 | Loss: 0.00002126
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002125
Iteration 104/1000 | Loss: 0.00002125
Iteration 105/1000 | Loss: 0.00002125
Iteration 106/1000 | Loss: 0.00002125
Iteration 107/1000 | Loss: 0.00002125
Iteration 108/1000 | Loss: 0.00002125
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002124
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002123
Iteration 117/1000 | Loss: 0.00002123
Iteration 118/1000 | Loss: 0.00002123
Iteration 119/1000 | Loss: 0.00002122
Iteration 120/1000 | Loss: 0.00002122
Iteration 121/1000 | Loss: 0.00002122
Iteration 122/1000 | Loss: 0.00002122
Iteration 123/1000 | Loss: 0.00002121
Iteration 124/1000 | Loss: 0.00002121
Iteration 125/1000 | Loss: 0.00002121
Iteration 126/1000 | Loss: 0.00002121
Iteration 127/1000 | Loss: 0.00002121
Iteration 128/1000 | Loss: 0.00002120
Iteration 129/1000 | Loss: 0.00002120
Iteration 130/1000 | Loss: 0.00002120
Iteration 131/1000 | Loss: 0.00002120
Iteration 132/1000 | Loss: 0.00002120
Iteration 133/1000 | Loss: 0.00002120
Iteration 134/1000 | Loss: 0.00002120
Iteration 135/1000 | Loss: 0.00002120
Iteration 136/1000 | Loss: 0.00002120
Iteration 137/1000 | Loss: 0.00002120
Iteration 138/1000 | Loss: 0.00002120
Iteration 139/1000 | Loss: 0.00002120
Iteration 140/1000 | Loss: 0.00002119
Iteration 141/1000 | Loss: 0.00002119
Iteration 142/1000 | Loss: 0.00002119
Iteration 143/1000 | Loss: 0.00002119
Iteration 144/1000 | Loss: 0.00002119
Iteration 145/1000 | Loss: 0.00002119
Iteration 146/1000 | Loss: 0.00002119
Iteration 147/1000 | Loss: 0.00002119
Iteration 148/1000 | Loss: 0.00002119
Iteration 149/1000 | Loss: 0.00002119
Iteration 150/1000 | Loss: 0.00002119
Iteration 151/1000 | Loss: 0.00002119
Iteration 152/1000 | Loss: 0.00002119
Iteration 153/1000 | Loss: 0.00002119
Iteration 154/1000 | Loss: 0.00002118
Iteration 155/1000 | Loss: 0.00002118
Iteration 156/1000 | Loss: 0.00002118
Iteration 157/1000 | Loss: 0.00002118
Iteration 158/1000 | Loss: 0.00002118
Iteration 159/1000 | Loss: 0.00002118
Iteration 160/1000 | Loss: 0.00002118
Iteration 161/1000 | Loss: 0.00002118
Iteration 162/1000 | Loss: 0.00002118
Iteration 163/1000 | Loss: 0.00002118
Iteration 164/1000 | Loss: 0.00002117
Iteration 165/1000 | Loss: 0.00002117
Iteration 166/1000 | Loss: 0.00002117
Iteration 167/1000 | Loss: 0.00002117
Iteration 168/1000 | Loss: 0.00002117
Iteration 169/1000 | Loss: 0.00002117
Iteration 170/1000 | Loss: 0.00002117
Iteration 171/1000 | Loss: 0.00002117
Iteration 172/1000 | Loss: 0.00002117
Iteration 173/1000 | Loss: 0.00002117
Iteration 174/1000 | Loss: 0.00002117
Iteration 175/1000 | Loss: 0.00002117
Iteration 176/1000 | Loss: 0.00002116
Iteration 177/1000 | Loss: 0.00002116
Iteration 178/1000 | Loss: 0.00002116
Iteration 179/1000 | Loss: 0.00002116
Iteration 180/1000 | Loss: 0.00002116
Iteration 181/1000 | Loss: 0.00002116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.116484938596841e-05, 2.116484938596841e-05, 2.116484938596841e-05, 2.116484938596841e-05, 2.116484938596841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.116484938596841e-05

Optimization complete. Final v2v error: 3.7389934062957764 mm

Highest mean error: 5.85428524017334 mm for frame 114

Lowest mean error: 3.0978543758392334 mm for frame 0

Saving results

Total time: 75.82602977752686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046482
Iteration 2/25 | Loss: 0.01046482
Iteration 3/25 | Loss: 0.01046481
Iteration 4/25 | Loss: 0.01046481
Iteration 5/25 | Loss: 0.00182654
Iteration 6/25 | Loss: 0.00121228
Iteration 7/25 | Loss: 0.00114445
Iteration 8/25 | Loss: 0.00113936
Iteration 9/25 | Loss: 0.00108765
Iteration 10/25 | Loss: 0.00109586
Iteration 11/25 | Loss: 0.00105456
Iteration 12/25 | Loss: 0.00104916
Iteration 13/25 | Loss: 0.00104905
Iteration 14/25 | Loss: 0.00104644
Iteration 15/25 | Loss: 0.00104660
Iteration 16/25 | Loss: 0.00104581
Iteration 17/25 | Loss: 0.00104635
Iteration 18/25 | Loss: 0.00104582
Iteration 19/25 | Loss: 0.00104578
Iteration 20/25 | Loss: 0.00104578
Iteration 21/25 | Loss: 0.00104578
Iteration 22/25 | Loss: 0.00104578
Iteration 23/25 | Loss: 0.00104578
Iteration 24/25 | Loss: 0.00104578
Iteration 25/25 | Loss: 0.00104578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41734636
Iteration 2/25 | Loss: 0.00108555
Iteration 3/25 | Loss: 0.00106172
Iteration 4/25 | Loss: 0.00106172
Iteration 5/25 | Loss: 0.00106172
Iteration 6/25 | Loss: 0.00106172
Iteration 7/25 | Loss: 0.00106172
Iteration 8/25 | Loss: 0.00106172
Iteration 9/25 | Loss: 0.00106172
Iteration 10/25 | Loss: 0.00106172
Iteration 11/25 | Loss: 0.00106172
Iteration 12/25 | Loss: 0.00106172
Iteration 13/25 | Loss: 0.00106172
Iteration 14/25 | Loss: 0.00106172
Iteration 15/25 | Loss: 0.00106172
Iteration 16/25 | Loss: 0.00106172
Iteration 17/25 | Loss: 0.00106172
Iteration 18/25 | Loss: 0.00106172
Iteration 19/25 | Loss: 0.00106172
Iteration 20/25 | Loss: 0.00106172
Iteration 21/25 | Loss: 0.00106172
Iteration 22/25 | Loss: 0.00106172
Iteration 23/25 | Loss: 0.00106172
Iteration 24/25 | Loss: 0.00106172
Iteration 25/25 | Loss: 0.00106172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106172
Iteration 2/1000 | Loss: 0.00005475
Iteration 3/1000 | Loss: 0.00006532
Iteration 4/1000 | Loss: 0.00003388
Iteration 5/1000 | Loss: 0.00008794
Iteration 6/1000 | Loss: 0.00005126
Iteration 7/1000 | Loss: 0.00005791
Iteration 8/1000 | Loss: 0.00004203
Iteration 9/1000 | Loss: 0.00004660
Iteration 10/1000 | Loss: 0.00000952
Iteration 11/1000 | Loss: 0.00002275
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00002017
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000871
Iteration 17/1000 | Loss: 0.00000858
Iteration 18/1000 | Loss: 0.00000837
Iteration 19/1000 | Loss: 0.00000849
Iteration 20/1000 | Loss: 0.00001041
Iteration 21/1000 | Loss: 0.00001964
Iteration 22/1000 | Loss: 0.00000818
Iteration 23/1000 | Loss: 0.00000814
Iteration 24/1000 | Loss: 0.00000813
Iteration 25/1000 | Loss: 0.00000813
Iteration 26/1000 | Loss: 0.00000813
Iteration 27/1000 | Loss: 0.00000813
Iteration 28/1000 | Loss: 0.00000813
Iteration 29/1000 | Loss: 0.00000813
Iteration 30/1000 | Loss: 0.00000813
Iteration 31/1000 | Loss: 0.00000813
Iteration 32/1000 | Loss: 0.00000812
Iteration 33/1000 | Loss: 0.00000811
Iteration 34/1000 | Loss: 0.00000811
Iteration 35/1000 | Loss: 0.00000811
Iteration 36/1000 | Loss: 0.00000811
Iteration 37/1000 | Loss: 0.00000810
Iteration 38/1000 | Loss: 0.00000810
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00000824
Iteration 44/1000 | Loss: 0.00000802
Iteration 45/1000 | Loss: 0.00000801
Iteration 46/1000 | Loss: 0.00000801
Iteration 47/1000 | Loss: 0.00000801
Iteration 48/1000 | Loss: 0.00000801
Iteration 49/1000 | Loss: 0.00000801
Iteration 50/1000 | Loss: 0.00000801
Iteration 51/1000 | Loss: 0.00000801
Iteration 52/1000 | Loss: 0.00000801
Iteration 53/1000 | Loss: 0.00000801
Iteration 54/1000 | Loss: 0.00000801
Iteration 55/1000 | Loss: 0.00000801
Iteration 56/1000 | Loss: 0.00000800
Iteration 57/1000 | Loss: 0.00000800
Iteration 58/1000 | Loss: 0.00000800
Iteration 59/1000 | Loss: 0.00000800
Iteration 60/1000 | Loss: 0.00000800
Iteration 61/1000 | Loss: 0.00000800
Iteration 62/1000 | Loss: 0.00000799
Iteration 63/1000 | Loss: 0.00000799
Iteration 64/1000 | Loss: 0.00000799
Iteration 65/1000 | Loss: 0.00000799
Iteration 66/1000 | Loss: 0.00000799
Iteration 67/1000 | Loss: 0.00000799
Iteration 68/1000 | Loss: 0.00000799
Iteration 69/1000 | Loss: 0.00000799
Iteration 70/1000 | Loss: 0.00000799
Iteration 71/1000 | Loss: 0.00000799
Iteration 72/1000 | Loss: 0.00000799
Iteration 73/1000 | Loss: 0.00000799
Iteration 74/1000 | Loss: 0.00000799
Iteration 75/1000 | Loss: 0.00000799
Iteration 76/1000 | Loss: 0.00000799
Iteration 77/1000 | Loss: 0.00000799
Iteration 78/1000 | Loss: 0.00000799
Iteration 79/1000 | Loss: 0.00000799
Iteration 80/1000 | Loss: 0.00000799
Iteration 81/1000 | Loss: 0.00000799
Iteration 82/1000 | Loss: 0.00000799
Iteration 83/1000 | Loss: 0.00000798
Iteration 84/1000 | Loss: 0.00000806
Iteration 85/1000 | Loss: 0.00000805
Iteration 86/1000 | Loss: 0.00000805
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00003385
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00000833
Iteration 92/1000 | Loss: 0.00001084
Iteration 93/1000 | Loss: 0.00002510
Iteration 94/1000 | Loss: 0.00000948
Iteration 95/1000 | Loss: 0.00000794
Iteration 96/1000 | Loss: 0.00001795
Iteration 97/1000 | Loss: 0.00000862
Iteration 98/1000 | Loss: 0.00000789
Iteration 99/1000 | Loss: 0.00000787
Iteration 100/1000 | Loss: 0.00000787
Iteration 101/1000 | Loss: 0.00000787
Iteration 102/1000 | Loss: 0.00000787
Iteration 103/1000 | Loss: 0.00000787
Iteration 104/1000 | Loss: 0.00000787
Iteration 105/1000 | Loss: 0.00000787
Iteration 106/1000 | Loss: 0.00000787
Iteration 107/1000 | Loss: 0.00000787
Iteration 108/1000 | Loss: 0.00000787
Iteration 109/1000 | Loss: 0.00000787
Iteration 110/1000 | Loss: 0.00000787
Iteration 111/1000 | Loss: 0.00000787
Iteration 112/1000 | Loss: 0.00000787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [7.869577530073002e-06, 7.869577530073002e-06, 7.869577530073002e-06, 7.869577530073002e-06, 7.869577530073002e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.869577530073002e-06

Optimization complete. Final v2v error: 2.4500601291656494 mm

Highest mean error: 2.652275323867798 mm for frame 13

Lowest mean error: 2.318058490753174 mm for frame 84

Saving results

Total time: 74.36461114883423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592797
Iteration 2/25 | Loss: 0.00143271
Iteration 3/25 | Loss: 0.00120665
Iteration 4/25 | Loss: 0.00118151
Iteration 5/25 | Loss: 0.00117790
Iteration 6/25 | Loss: 0.00117679
Iteration 7/25 | Loss: 0.00117654
Iteration 8/25 | Loss: 0.00117654
Iteration 9/25 | Loss: 0.00117654
Iteration 10/25 | Loss: 0.00117654
Iteration 11/25 | Loss: 0.00117654
Iteration 12/25 | Loss: 0.00117654
Iteration 13/25 | Loss: 0.00117654
Iteration 14/25 | Loss: 0.00117654
Iteration 15/25 | Loss: 0.00117654
Iteration 16/25 | Loss: 0.00117654
Iteration 17/25 | Loss: 0.00117654
Iteration 18/25 | Loss: 0.00117654
Iteration 19/25 | Loss: 0.00117654
Iteration 20/25 | Loss: 0.00117654
Iteration 21/25 | Loss: 0.00117654
Iteration 22/25 | Loss: 0.00117654
Iteration 23/25 | Loss: 0.00117654
Iteration 24/25 | Loss: 0.00117654
Iteration 25/25 | Loss: 0.00117654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17609203
Iteration 2/25 | Loss: 0.00068236
Iteration 3/25 | Loss: 0.00068236
Iteration 4/25 | Loss: 0.00068236
Iteration 5/25 | Loss: 0.00068236
Iteration 6/25 | Loss: 0.00068236
Iteration 7/25 | Loss: 0.00068236
Iteration 8/25 | Loss: 0.00068236
Iteration 9/25 | Loss: 0.00068236
Iteration 10/25 | Loss: 0.00068236
Iteration 11/25 | Loss: 0.00068236
Iteration 12/25 | Loss: 0.00068236
Iteration 13/25 | Loss: 0.00068236
Iteration 14/25 | Loss: 0.00068236
Iteration 15/25 | Loss: 0.00068236
Iteration 16/25 | Loss: 0.00068236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006823607254773378, 0.0006823607254773378, 0.0006823607254773378, 0.0006823607254773378, 0.0006823607254773378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006823607254773378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068236
Iteration 2/1000 | Loss: 0.00004140
Iteration 3/1000 | Loss: 0.00002411
Iteration 4/1000 | Loss: 0.00002030
Iteration 5/1000 | Loss: 0.00001914
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001807
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001697
Iteration 14/1000 | Loss: 0.00001697
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001690
Iteration 17/1000 | Loss: 0.00001690
Iteration 18/1000 | Loss: 0.00001690
Iteration 19/1000 | Loss: 0.00001690
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001688
Iteration 22/1000 | Loss: 0.00001688
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001678
Iteration 32/1000 | Loss: 0.00001676
Iteration 33/1000 | Loss: 0.00001676
Iteration 34/1000 | Loss: 0.00001672
Iteration 35/1000 | Loss: 0.00001672
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001668
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001666
Iteration 48/1000 | Loss: 0.00001666
Iteration 49/1000 | Loss: 0.00001665
Iteration 50/1000 | Loss: 0.00001665
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001663
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001659
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001655
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001654
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001653
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001652
Iteration 106/1000 | Loss: 0.00001652
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001652
Iteration 110/1000 | Loss: 0.00001652
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.648967736400664e-05, 1.648967736400664e-05, 1.648967736400664e-05, 1.648967736400664e-05, 1.648967736400664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.648967736400664e-05

Optimization complete. Final v2v error: 3.320904493331909 mm

Highest mean error: 5.117897033691406 mm for frame 58

Lowest mean error: 2.8726329803466797 mm for frame 133

Saving results

Total time: 38.39097332954407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395625
Iteration 2/25 | Loss: 0.00127995
Iteration 3/25 | Loss: 0.00114596
Iteration 4/25 | Loss: 0.00111639
Iteration 5/25 | Loss: 0.00110709
Iteration 6/25 | Loss: 0.00110331
Iteration 7/25 | Loss: 0.00111683
Iteration 8/25 | Loss: 0.00109416
Iteration 9/25 | Loss: 0.00108774
Iteration 10/25 | Loss: 0.00108451
Iteration 11/25 | Loss: 0.00108386
Iteration 12/25 | Loss: 0.00108326
Iteration 13/25 | Loss: 0.00108320
Iteration 14/25 | Loss: 0.00108320
Iteration 15/25 | Loss: 0.00108320
Iteration 16/25 | Loss: 0.00108320
Iteration 17/25 | Loss: 0.00108320
Iteration 18/25 | Loss: 0.00108320
Iteration 19/25 | Loss: 0.00108320
Iteration 20/25 | Loss: 0.00108320
Iteration 21/25 | Loss: 0.00108320
Iteration 22/25 | Loss: 0.00108320
Iteration 23/25 | Loss: 0.00108320
Iteration 24/25 | Loss: 0.00108320
Iteration 25/25 | Loss: 0.00108320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34150088
Iteration 2/25 | Loss: 0.00070799
Iteration 3/25 | Loss: 0.00070799
Iteration 4/25 | Loss: 0.00070798
Iteration 5/25 | Loss: 0.00070798
Iteration 6/25 | Loss: 0.00070798
Iteration 7/25 | Loss: 0.00070798
Iteration 8/25 | Loss: 0.00070798
Iteration 9/25 | Loss: 0.00070798
Iteration 10/25 | Loss: 0.00070798
Iteration 11/25 | Loss: 0.00070798
Iteration 12/25 | Loss: 0.00070798
Iteration 13/25 | Loss: 0.00070798
Iteration 14/25 | Loss: 0.00070798
Iteration 15/25 | Loss: 0.00070798
Iteration 16/25 | Loss: 0.00070798
Iteration 17/25 | Loss: 0.00070798
Iteration 18/25 | Loss: 0.00070798
Iteration 19/25 | Loss: 0.00070798
Iteration 20/25 | Loss: 0.00070798
Iteration 21/25 | Loss: 0.00070798
Iteration 22/25 | Loss: 0.00070798
Iteration 23/25 | Loss: 0.00070798
Iteration 24/25 | Loss: 0.00070798
Iteration 25/25 | Loss: 0.00070798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070798
Iteration 2/1000 | Loss: 0.00001855
Iteration 3/1000 | Loss: 0.00001438
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001255
Iteration 6/1000 | Loss: 0.00001196
Iteration 7/1000 | Loss: 0.00001180
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001097
Iteration 11/1000 | Loss: 0.00001094
Iteration 12/1000 | Loss: 0.00001092
Iteration 13/1000 | Loss: 0.00001091
Iteration 14/1000 | Loss: 0.00001090
Iteration 15/1000 | Loss: 0.00001090
Iteration 16/1000 | Loss: 0.00001088
Iteration 17/1000 | Loss: 0.00001087
Iteration 18/1000 | Loss: 0.00001086
Iteration 19/1000 | Loss: 0.00001086
Iteration 20/1000 | Loss: 0.00001085
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001079
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001075
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001074
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001073
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001072
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001071
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001070
Iteration 40/1000 | Loss: 0.00001070
Iteration 41/1000 | Loss: 0.00001070
Iteration 42/1000 | Loss: 0.00001069
Iteration 43/1000 | Loss: 0.00001069
Iteration 44/1000 | Loss: 0.00001069
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001066
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001065
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001065
Iteration 58/1000 | Loss: 0.00001065
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001064
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001058
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001057
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001057
Iteration 77/1000 | Loss: 0.00001056
Iteration 78/1000 | Loss: 0.00001056
Iteration 79/1000 | Loss: 0.00001055
Iteration 80/1000 | Loss: 0.00001055
Iteration 81/1000 | Loss: 0.00001054
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001052
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001052
Iteration 96/1000 | Loss: 0.00001052
Iteration 97/1000 | Loss: 0.00001051
Iteration 98/1000 | Loss: 0.00001051
Iteration 99/1000 | Loss: 0.00001051
Iteration 100/1000 | Loss: 0.00001050
Iteration 101/1000 | Loss: 0.00001050
Iteration 102/1000 | Loss: 0.00001050
Iteration 103/1000 | Loss: 0.00001050
Iteration 104/1000 | Loss: 0.00001050
Iteration 105/1000 | Loss: 0.00001050
Iteration 106/1000 | Loss: 0.00001050
Iteration 107/1000 | Loss: 0.00001050
Iteration 108/1000 | Loss: 0.00001049
Iteration 109/1000 | Loss: 0.00001049
Iteration 110/1000 | Loss: 0.00001049
Iteration 111/1000 | Loss: 0.00001049
Iteration 112/1000 | Loss: 0.00001049
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001049
Iteration 115/1000 | Loss: 0.00001048
Iteration 116/1000 | Loss: 0.00001048
Iteration 117/1000 | Loss: 0.00001048
Iteration 118/1000 | Loss: 0.00001048
Iteration 119/1000 | Loss: 0.00001048
Iteration 120/1000 | Loss: 0.00001048
Iteration 121/1000 | Loss: 0.00001048
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001047
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001047
Iteration 127/1000 | Loss: 0.00001046
Iteration 128/1000 | Loss: 0.00001046
Iteration 129/1000 | Loss: 0.00001046
Iteration 130/1000 | Loss: 0.00001045
Iteration 131/1000 | Loss: 0.00001045
Iteration 132/1000 | Loss: 0.00001045
Iteration 133/1000 | Loss: 0.00001044
Iteration 134/1000 | Loss: 0.00001044
Iteration 135/1000 | Loss: 0.00001044
Iteration 136/1000 | Loss: 0.00001044
Iteration 137/1000 | Loss: 0.00001043
Iteration 138/1000 | Loss: 0.00001043
Iteration 139/1000 | Loss: 0.00001043
Iteration 140/1000 | Loss: 0.00001043
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001042
Iteration 147/1000 | Loss: 0.00001042
Iteration 148/1000 | Loss: 0.00001042
Iteration 149/1000 | Loss: 0.00001042
Iteration 150/1000 | Loss: 0.00001042
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001040
Iteration 160/1000 | Loss: 0.00001040
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001038
Iteration 166/1000 | Loss: 0.00001037
Iteration 167/1000 | Loss: 0.00001037
Iteration 168/1000 | Loss: 0.00001036
Iteration 169/1000 | Loss: 0.00001036
Iteration 170/1000 | Loss: 0.00001036
Iteration 171/1000 | Loss: 0.00001036
Iteration 172/1000 | Loss: 0.00001036
Iteration 173/1000 | Loss: 0.00001036
Iteration 174/1000 | Loss: 0.00001036
Iteration 175/1000 | Loss: 0.00001036
Iteration 176/1000 | Loss: 0.00001036
Iteration 177/1000 | Loss: 0.00001036
Iteration 178/1000 | Loss: 0.00001036
Iteration 179/1000 | Loss: 0.00001036
Iteration 180/1000 | Loss: 0.00001036
Iteration 181/1000 | Loss: 0.00001036
Iteration 182/1000 | Loss: 0.00001035
Iteration 183/1000 | Loss: 0.00001035
Iteration 184/1000 | Loss: 0.00001035
Iteration 185/1000 | Loss: 0.00001035
Iteration 186/1000 | Loss: 0.00001035
Iteration 187/1000 | Loss: 0.00001035
Iteration 188/1000 | Loss: 0.00001034
Iteration 189/1000 | Loss: 0.00001034
Iteration 190/1000 | Loss: 0.00001034
Iteration 191/1000 | Loss: 0.00001034
Iteration 192/1000 | Loss: 0.00001034
Iteration 193/1000 | Loss: 0.00001034
Iteration 194/1000 | Loss: 0.00001034
Iteration 195/1000 | Loss: 0.00001034
Iteration 196/1000 | Loss: 0.00001034
Iteration 197/1000 | Loss: 0.00001034
Iteration 198/1000 | Loss: 0.00001034
Iteration 199/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.0343396752432454e-05, 1.0343396752432454e-05, 1.0343396752432454e-05, 1.0343396752432454e-05, 1.0343396752432454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0343396752432454e-05

Optimization complete. Final v2v error: 2.7717366218566895 mm

Highest mean error: 3.1704914569854736 mm for frame 133

Lowest mean error: 2.559612512588501 mm for frame 233

Saving results

Total time: 60.12037491798401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981392
Iteration 2/25 | Loss: 0.00331020
Iteration 3/25 | Loss: 0.00222399
Iteration 4/25 | Loss: 0.00196528
Iteration 5/25 | Loss: 0.00190842
Iteration 6/25 | Loss: 0.00173983
Iteration 7/25 | Loss: 0.00161666
Iteration 8/25 | Loss: 0.00154066
Iteration 9/25 | Loss: 0.00148548
Iteration 10/25 | Loss: 0.00143260
Iteration 11/25 | Loss: 0.00143985
Iteration 12/25 | Loss: 0.00142930
Iteration 13/25 | Loss: 0.00141041
Iteration 14/25 | Loss: 0.00140934
Iteration 15/25 | Loss: 0.00140983
Iteration 16/25 | Loss: 0.00140387
Iteration 17/25 | Loss: 0.00140252
Iteration 18/25 | Loss: 0.00140241
Iteration 19/25 | Loss: 0.00140240
Iteration 20/25 | Loss: 0.00140240
Iteration 21/25 | Loss: 0.00140240
Iteration 22/25 | Loss: 0.00140240
Iteration 23/25 | Loss: 0.00140240
Iteration 24/25 | Loss: 0.00140240
Iteration 25/25 | Loss: 0.00140240

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33794141
Iteration 2/25 | Loss: 0.00159903
Iteration 3/25 | Loss: 0.00117584
Iteration 4/25 | Loss: 0.00117583
Iteration 5/25 | Loss: 0.00117583
Iteration 6/25 | Loss: 0.00117583
Iteration 7/25 | Loss: 0.00117583
Iteration 8/25 | Loss: 0.00117583
Iteration 9/25 | Loss: 0.00117583
Iteration 10/25 | Loss: 0.00117583
Iteration 11/25 | Loss: 0.00117583
Iteration 12/25 | Loss: 0.00117583
Iteration 13/25 | Loss: 0.00117583
Iteration 14/25 | Loss: 0.00117583
Iteration 15/25 | Loss: 0.00117583
Iteration 16/25 | Loss: 0.00117583
Iteration 17/25 | Loss: 0.00117583
Iteration 18/25 | Loss: 0.00117583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011758318869397044, 0.0011758318869397044, 0.0011758318869397044, 0.0011758318869397044, 0.0011758318869397044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011758318869397044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117583
Iteration 2/1000 | Loss: 0.00165615
Iteration 3/1000 | Loss: 0.00120151
Iteration 4/1000 | Loss: 0.00016377
Iteration 5/1000 | Loss: 0.00031449
Iteration 6/1000 | Loss: 0.00040927
Iteration 7/1000 | Loss: 0.00014173
Iteration 8/1000 | Loss: 0.00011181
Iteration 9/1000 | Loss: 0.00035971
Iteration 10/1000 | Loss: 0.00033659
Iteration 11/1000 | Loss: 0.00064819
Iteration 12/1000 | Loss: 0.00008971
Iteration 13/1000 | Loss: 0.00022625
Iteration 14/1000 | Loss: 0.00009045
Iteration 15/1000 | Loss: 0.00014750
Iteration 16/1000 | Loss: 0.00035835
Iteration 17/1000 | Loss: 0.00012548
Iteration 18/1000 | Loss: 0.00008475
Iteration 19/1000 | Loss: 0.00014035
Iteration 20/1000 | Loss: 0.00011636
Iteration 21/1000 | Loss: 0.00013035
Iteration 22/1000 | Loss: 0.00009805
Iteration 23/1000 | Loss: 0.00008235
Iteration 24/1000 | Loss: 0.00007922
Iteration 25/1000 | Loss: 0.00007762
Iteration 26/1000 | Loss: 0.00039878
Iteration 27/1000 | Loss: 0.00008180
Iteration 28/1000 | Loss: 0.00007664
Iteration 29/1000 | Loss: 0.00023629
Iteration 30/1000 | Loss: 0.00048805
Iteration 31/1000 | Loss: 0.00007466
Iteration 32/1000 | Loss: 0.00023464
Iteration 33/1000 | Loss: 0.00007553
Iteration 34/1000 | Loss: 0.00007340
Iteration 35/1000 | Loss: 0.00099573
Iteration 36/1000 | Loss: 0.00243533
Iteration 37/1000 | Loss: 0.00362902
Iteration 38/1000 | Loss: 0.00103549
Iteration 39/1000 | Loss: 0.00142767
Iteration 40/1000 | Loss: 0.00039712
Iteration 41/1000 | Loss: 0.00014488
Iteration 42/1000 | Loss: 0.00025522
Iteration 43/1000 | Loss: 0.00030573
Iteration 44/1000 | Loss: 0.00008745
Iteration 45/1000 | Loss: 0.00229038
Iteration 46/1000 | Loss: 0.00148835
Iteration 47/1000 | Loss: 0.00014390
Iteration 48/1000 | Loss: 0.00052115
Iteration 49/1000 | Loss: 0.00045327
Iteration 50/1000 | Loss: 0.00007708
Iteration 51/1000 | Loss: 0.00027577
Iteration 52/1000 | Loss: 0.00021625
Iteration 53/1000 | Loss: 0.00005981
Iteration 54/1000 | Loss: 0.00005480
Iteration 55/1000 | Loss: 0.00005166
Iteration 56/1000 | Loss: 0.00045185
Iteration 57/1000 | Loss: 0.00144141
Iteration 58/1000 | Loss: 0.00004915
Iteration 59/1000 | Loss: 0.00027497
Iteration 60/1000 | Loss: 0.00004552
Iteration 61/1000 | Loss: 0.00026007
Iteration 62/1000 | Loss: 0.00004406
Iteration 63/1000 | Loss: 0.00004271
Iteration 64/1000 | Loss: 0.00004189
Iteration 65/1000 | Loss: 0.00004115
Iteration 66/1000 | Loss: 0.00004065
Iteration 67/1000 | Loss: 0.00029822
Iteration 68/1000 | Loss: 0.00004096
Iteration 69/1000 | Loss: 0.00004017
Iteration 70/1000 | Loss: 0.00003998
Iteration 71/1000 | Loss: 0.00003986
Iteration 72/1000 | Loss: 0.00003986
Iteration 73/1000 | Loss: 0.00003986
Iteration 74/1000 | Loss: 0.00003985
Iteration 75/1000 | Loss: 0.00003985
Iteration 76/1000 | Loss: 0.00003984
Iteration 77/1000 | Loss: 0.00003982
Iteration 78/1000 | Loss: 0.00003978
Iteration 79/1000 | Loss: 0.00003977
Iteration 80/1000 | Loss: 0.00003976
Iteration 81/1000 | Loss: 0.00003976
Iteration 82/1000 | Loss: 0.00003976
Iteration 83/1000 | Loss: 0.00003974
Iteration 84/1000 | Loss: 0.00003971
Iteration 85/1000 | Loss: 0.00003970
Iteration 86/1000 | Loss: 0.00003969
Iteration 87/1000 | Loss: 0.00003969
Iteration 88/1000 | Loss: 0.00003968
Iteration 89/1000 | Loss: 0.00003968
Iteration 90/1000 | Loss: 0.00003967
Iteration 91/1000 | Loss: 0.00003967
Iteration 92/1000 | Loss: 0.00003967
Iteration 93/1000 | Loss: 0.00003966
Iteration 94/1000 | Loss: 0.00003966
Iteration 95/1000 | Loss: 0.00003965
Iteration 96/1000 | Loss: 0.00003965
Iteration 97/1000 | Loss: 0.00003964
Iteration 98/1000 | Loss: 0.00003964
Iteration 99/1000 | Loss: 0.00003964
Iteration 100/1000 | Loss: 0.00003963
Iteration 101/1000 | Loss: 0.00003963
Iteration 102/1000 | Loss: 0.00003963
Iteration 103/1000 | Loss: 0.00003963
Iteration 104/1000 | Loss: 0.00003963
Iteration 105/1000 | Loss: 0.00003963
Iteration 106/1000 | Loss: 0.00003963
Iteration 107/1000 | Loss: 0.00003963
Iteration 108/1000 | Loss: 0.00003962
Iteration 109/1000 | Loss: 0.00003962
Iteration 110/1000 | Loss: 0.00003961
Iteration 111/1000 | Loss: 0.00003961
Iteration 112/1000 | Loss: 0.00003961
Iteration 113/1000 | Loss: 0.00003960
Iteration 114/1000 | Loss: 0.00003960
Iteration 115/1000 | Loss: 0.00003960
Iteration 116/1000 | Loss: 0.00003960
Iteration 117/1000 | Loss: 0.00003960
Iteration 118/1000 | Loss: 0.00003959
Iteration 119/1000 | Loss: 0.00003959
Iteration 120/1000 | Loss: 0.00003959
Iteration 121/1000 | Loss: 0.00003959
Iteration 122/1000 | Loss: 0.00003959
Iteration 123/1000 | Loss: 0.00003959
Iteration 124/1000 | Loss: 0.00003959
Iteration 125/1000 | Loss: 0.00003959
Iteration 126/1000 | Loss: 0.00003959
Iteration 127/1000 | Loss: 0.00003959
Iteration 128/1000 | Loss: 0.00003959
Iteration 129/1000 | Loss: 0.00003959
Iteration 130/1000 | Loss: 0.00003958
Iteration 131/1000 | Loss: 0.00003958
Iteration 132/1000 | Loss: 0.00003958
Iteration 133/1000 | Loss: 0.00003958
Iteration 134/1000 | Loss: 0.00003958
Iteration 135/1000 | Loss: 0.00003958
Iteration 136/1000 | Loss: 0.00003958
Iteration 137/1000 | Loss: 0.00003958
Iteration 138/1000 | Loss: 0.00003958
Iteration 139/1000 | Loss: 0.00003958
Iteration 140/1000 | Loss: 0.00003957
Iteration 141/1000 | Loss: 0.00003957
Iteration 142/1000 | Loss: 0.00003957
Iteration 143/1000 | Loss: 0.00003957
Iteration 144/1000 | Loss: 0.00003957
Iteration 145/1000 | Loss: 0.00003957
Iteration 146/1000 | Loss: 0.00003957
Iteration 147/1000 | Loss: 0.00003957
Iteration 148/1000 | Loss: 0.00003957
Iteration 149/1000 | Loss: 0.00003957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.9572205423610285e-05, 3.9572205423610285e-05, 3.9572205423610285e-05, 3.9572205423610285e-05, 3.9572205423610285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9572205423610285e-05

Optimization complete. Final v2v error: 4.746151447296143 mm

Highest mean error: 11.262848854064941 mm for frame 120

Lowest mean error: 4.426311492919922 mm for frame 51

Saving results

Total time: 158.68338227272034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826379
Iteration 2/25 | Loss: 0.00164616
Iteration 3/25 | Loss: 0.00130240
Iteration 4/25 | Loss: 0.00123827
Iteration 5/25 | Loss: 0.00122150
Iteration 6/25 | Loss: 0.00123556
Iteration 7/25 | Loss: 0.00125839
Iteration 8/25 | Loss: 0.00118804
Iteration 9/25 | Loss: 0.00116021
Iteration 10/25 | Loss: 0.00116026
Iteration 11/25 | Loss: 0.00115755
Iteration 12/25 | Loss: 0.00114573
Iteration 13/25 | Loss: 0.00114284
Iteration 14/25 | Loss: 0.00113746
Iteration 15/25 | Loss: 0.00113440
Iteration 16/25 | Loss: 0.00113461
Iteration 17/25 | Loss: 0.00113383
Iteration 18/25 | Loss: 0.00113344
Iteration 19/25 | Loss: 0.00113274
Iteration 20/25 | Loss: 0.00113341
Iteration 21/25 | Loss: 0.00113619
Iteration 22/25 | Loss: 0.00113478
Iteration 23/25 | Loss: 0.00113350
Iteration 24/25 | Loss: 0.00113177
Iteration 25/25 | Loss: 0.00113273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.23185730
Iteration 2/25 | Loss: 0.00082388
Iteration 3/25 | Loss: 0.00082385
Iteration 4/25 | Loss: 0.00082385
Iteration 5/25 | Loss: 0.00082385
Iteration 6/25 | Loss: 0.00082385
Iteration 7/25 | Loss: 0.00082385
Iteration 8/25 | Loss: 0.00082385
Iteration 9/25 | Loss: 0.00082385
Iteration 10/25 | Loss: 0.00082385
Iteration 11/25 | Loss: 0.00082385
Iteration 12/25 | Loss: 0.00082385
Iteration 13/25 | Loss: 0.00082385
Iteration 14/25 | Loss: 0.00082385
Iteration 15/25 | Loss: 0.00082385
Iteration 16/25 | Loss: 0.00082385
Iteration 17/25 | Loss: 0.00082385
Iteration 18/25 | Loss: 0.00082385
Iteration 19/25 | Loss: 0.00082385
Iteration 20/25 | Loss: 0.00082385
Iteration 21/25 | Loss: 0.00082385
Iteration 22/25 | Loss: 0.00082385
Iteration 23/25 | Loss: 0.00082385
Iteration 24/25 | Loss: 0.00082385
Iteration 25/25 | Loss: 0.00082385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082385
Iteration 2/1000 | Loss: 0.00002813
Iteration 3/1000 | Loss: 0.00038701
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00002946
Iteration 7/1000 | Loss: 0.00054458
Iteration 8/1000 | Loss: 0.00140657
Iteration 9/1000 | Loss: 0.00093896
Iteration 10/1000 | Loss: 0.00007780
Iteration 11/1000 | Loss: 0.00032370
Iteration 12/1000 | Loss: 0.00007747
Iteration 13/1000 | Loss: 0.00002177
Iteration 14/1000 | Loss: 0.00003438
Iteration 15/1000 | Loss: 0.00002221
Iteration 16/1000 | Loss: 0.00003447
Iteration 17/1000 | Loss: 0.00018900
Iteration 18/1000 | Loss: 0.00003508
Iteration 19/1000 | Loss: 0.00002470
Iteration 20/1000 | Loss: 0.00011565
Iteration 21/1000 | Loss: 0.00007491
Iteration 22/1000 | Loss: 0.00022089
Iteration 23/1000 | Loss: 0.00015634
Iteration 24/1000 | Loss: 0.00012977
Iteration 25/1000 | Loss: 0.00001862
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001480
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00025397
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00026927
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001208
Iteration 44/1000 | Loss: 0.00001207
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001173
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001168
Iteration 121/1000 | Loss: 0.00001168
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001167
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001166
Iteration 143/1000 | Loss: 0.00001166
Iteration 144/1000 | Loss: 0.00001166
Iteration 145/1000 | Loss: 0.00001166
Iteration 146/1000 | Loss: 0.00001166
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001165
Iteration 153/1000 | Loss: 0.00001165
Iteration 154/1000 | Loss: 0.00001165
Iteration 155/1000 | Loss: 0.00001165
Iteration 156/1000 | Loss: 0.00001165
Iteration 157/1000 | Loss: 0.00001165
Iteration 158/1000 | Loss: 0.00001165
Iteration 159/1000 | Loss: 0.00001165
Iteration 160/1000 | Loss: 0.00001165
Iteration 161/1000 | Loss: 0.00001164
Iteration 162/1000 | Loss: 0.00001164
Iteration 163/1000 | Loss: 0.00001164
Iteration 164/1000 | Loss: 0.00001164
Iteration 165/1000 | Loss: 0.00001164
Iteration 166/1000 | Loss: 0.00001164
Iteration 167/1000 | Loss: 0.00001164
Iteration 168/1000 | Loss: 0.00001164
Iteration 169/1000 | Loss: 0.00001164
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001163
Iteration 182/1000 | Loss: 0.00001163
Iteration 183/1000 | Loss: 0.00001163
Iteration 184/1000 | Loss: 0.00001163
Iteration 185/1000 | Loss: 0.00001163
Iteration 186/1000 | Loss: 0.00001163
Iteration 187/1000 | Loss: 0.00001163
Iteration 188/1000 | Loss: 0.00001163
Iteration 189/1000 | Loss: 0.00001163
Iteration 190/1000 | Loss: 0.00001163
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Iteration 196/1000 | Loss: 0.00001163
Iteration 197/1000 | Loss: 0.00001163
Iteration 198/1000 | Loss: 0.00001163
Iteration 199/1000 | Loss: 0.00001163
Iteration 200/1000 | Loss: 0.00001163
Iteration 201/1000 | Loss: 0.00001163
Iteration 202/1000 | Loss: 0.00001163
Iteration 203/1000 | Loss: 0.00001163
Iteration 204/1000 | Loss: 0.00001163
Iteration 205/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1627739695541095e-05, 1.1627739695541095e-05, 1.1627739695541095e-05, 1.1627739695541095e-05, 1.1627739695541095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1627739695541095e-05

Optimization complete. Final v2v error: 2.902087688446045 mm

Highest mean error: 3.7532989978790283 mm for frame 166

Lowest mean error: 2.454343318939209 mm for frame 226

Saving results

Total time: 129.3532054424286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598608
Iteration 2/25 | Loss: 0.00116012
Iteration 3/25 | Loss: 0.00108716
Iteration 4/25 | Loss: 0.00107544
Iteration 5/25 | Loss: 0.00107077
Iteration 6/25 | Loss: 0.00106951
Iteration 7/25 | Loss: 0.00106951
Iteration 8/25 | Loss: 0.00106951
Iteration 9/25 | Loss: 0.00106951
Iteration 10/25 | Loss: 0.00106951
Iteration 11/25 | Loss: 0.00106951
Iteration 12/25 | Loss: 0.00106951
Iteration 13/25 | Loss: 0.00106951
Iteration 14/25 | Loss: 0.00106951
Iteration 15/25 | Loss: 0.00106951
Iteration 16/25 | Loss: 0.00106951
Iteration 17/25 | Loss: 0.00106951
Iteration 18/25 | Loss: 0.00106951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010695068631321192, 0.0010695068631321192, 0.0010695068631321192, 0.0010695068631321192, 0.0010695068631321192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010695068631321192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89397788
Iteration 2/25 | Loss: 0.00087145
Iteration 3/25 | Loss: 0.00087145
Iteration 4/25 | Loss: 0.00087145
Iteration 5/25 | Loss: 0.00087145
Iteration 6/25 | Loss: 0.00087145
Iteration 7/25 | Loss: 0.00087145
Iteration 8/25 | Loss: 0.00087145
Iteration 9/25 | Loss: 0.00087145
Iteration 10/25 | Loss: 0.00087145
Iteration 11/25 | Loss: 0.00087145
Iteration 12/25 | Loss: 0.00087145
Iteration 13/25 | Loss: 0.00087145
Iteration 14/25 | Loss: 0.00087145
Iteration 15/25 | Loss: 0.00087145
Iteration 16/25 | Loss: 0.00087145
Iteration 17/25 | Loss: 0.00087145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008714469731785357, 0.0008714469731785357, 0.0008714469731785357, 0.0008714469731785357, 0.0008714469731785357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008714469731785357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087145
Iteration 2/1000 | Loss: 0.00001815
Iteration 3/1000 | Loss: 0.00001255
Iteration 4/1000 | Loss: 0.00001147
Iteration 5/1000 | Loss: 0.00001110
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001038
Iteration 8/1000 | Loss: 0.00001033
Iteration 9/1000 | Loss: 0.00001031
Iteration 10/1000 | Loss: 0.00001011
Iteration 11/1000 | Loss: 0.00001003
Iteration 12/1000 | Loss: 0.00000987
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000978
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000963
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000947
Iteration 24/1000 | Loss: 0.00000947
Iteration 25/1000 | Loss: 0.00000946
Iteration 26/1000 | Loss: 0.00000946
Iteration 27/1000 | Loss: 0.00000946
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000943
Iteration 30/1000 | Loss: 0.00000942
Iteration 31/1000 | Loss: 0.00000941
Iteration 32/1000 | Loss: 0.00000941
Iteration 33/1000 | Loss: 0.00000941
Iteration 34/1000 | Loss: 0.00000940
Iteration 35/1000 | Loss: 0.00000940
Iteration 36/1000 | Loss: 0.00000939
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000936
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000936
Iteration 48/1000 | Loss: 0.00000936
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000935
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000934
Iteration 53/1000 | Loss: 0.00000934
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000933
Iteration 56/1000 | Loss: 0.00000933
Iteration 57/1000 | Loss: 0.00000932
Iteration 58/1000 | Loss: 0.00000932
Iteration 59/1000 | Loss: 0.00000931
Iteration 60/1000 | Loss: 0.00000931
Iteration 61/1000 | Loss: 0.00000931
Iteration 62/1000 | Loss: 0.00000930
Iteration 63/1000 | Loss: 0.00000929
Iteration 64/1000 | Loss: 0.00000929
Iteration 65/1000 | Loss: 0.00000928
Iteration 66/1000 | Loss: 0.00000928
Iteration 67/1000 | Loss: 0.00000928
Iteration 68/1000 | Loss: 0.00000927
Iteration 69/1000 | Loss: 0.00000927
Iteration 70/1000 | Loss: 0.00000927
Iteration 71/1000 | Loss: 0.00000927
Iteration 72/1000 | Loss: 0.00000927
Iteration 73/1000 | Loss: 0.00000927
Iteration 74/1000 | Loss: 0.00000927
Iteration 75/1000 | Loss: 0.00000927
Iteration 76/1000 | Loss: 0.00000927
Iteration 77/1000 | Loss: 0.00000926
Iteration 78/1000 | Loss: 0.00000926
Iteration 79/1000 | Loss: 0.00000926
Iteration 80/1000 | Loss: 0.00000925
Iteration 81/1000 | Loss: 0.00000924
Iteration 82/1000 | Loss: 0.00000924
Iteration 83/1000 | Loss: 0.00000924
Iteration 84/1000 | Loss: 0.00000924
Iteration 85/1000 | Loss: 0.00000923
Iteration 86/1000 | Loss: 0.00000923
Iteration 87/1000 | Loss: 0.00000923
Iteration 88/1000 | Loss: 0.00000923
Iteration 89/1000 | Loss: 0.00000923
Iteration 90/1000 | Loss: 0.00000923
Iteration 91/1000 | Loss: 0.00000922
Iteration 92/1000 | Loss: 0.00000922
Iteration 93/1000 | Loss: 0.00000922
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000920
Iteration 100/1000 | Loss: 0.00000920
Iteration 101/1000 | Loss: 0.00000920
Iteration 102/1000 | Loss: 0.00000920
Iteration 103/1000 | Loss: 0.00000920
Iteration 104/1000 | Loss: 0.00000920
Iteration 105/1000 | Loss: 0.00000920
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000919
Iteration 110/1000 | Loss: 0.00000919
Iteration 111/1000 | Loss: 0.00000919
Iteration 112/1000 | Loss: 0.00000919
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000918
Iteration 117/1000 | Loss: 0.00000918
Iteration 118/1000 | Loss: 0.00000918
Iteration 119/1000 | Loss: 0.00000918
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000918
Iteration 122/1000 | Loss: 0.00000918
Iteration 123/1000 | Loss: 0.00000918
Iteration 124/1000 | Loss: 0.00000918
Iteration 125/1000 | Loss: 0.00000918
Iteration 126/1000 | Loss: 0.00000918
Iteration 127/1000 | Loss: 0.00000917
Iteration 128/1000 | Loss: 0.00000917
Iteration 129/1000 | Loss: 0.00000917
Iteration 130/1000 | Loss: 0.00000917
Iteration 131/1000 | Loss: 0.00000916
Iteration 132/1000 | Loss: 0.00000916
Iteration 133/1000 | Loss: 0.00000916
Iteration 134/1000 | Loss: 0.00000916
Iteration 135/1000 | Loss: 0.00000916
Iteration 136/1000 | Loss: 0.00000915
Iteration 137/1000 | Loss: 0.00000915
Iteration 138/1000 | Loss: 0.00000915
Iteration 139/1000 | Loss: 0.00000915
Iteration 140/1000 | Loss: 0.00000915
Iteration 141/1000 | Loss: 0.00000915
Iteration 142/1000 | Loss: 0.00000915
Iteration 143/1000 | Loss: 0.00000915
Iteration 144/1000 | Loss: 0.00000915
Iteration 145/1000 | Loss: 0.00000915
Iteration 146/1000 | Loss: 0.00000915
Iteration 147/1000 | Loss: 0.00000915
Iteration 148/1000 | Loss: 0.00000915
Iteration 149/1000 | Loss: 0.00000915
Iteration 150/1000 | Loss: 0.00000915
Iteration 151/1000 | Loss: 0.00000915
Iteration 152/1000 | Loss: 0.00000915
Iteration 153/1000 | Loss: 0.00000915
Iteration 154/1000 | Loss: 0.00000915
Iteration 155/1000 | Loss: 0.00000915
Iteration 156/1000 | Loss: 0.00000915
Iteration 157/1000 | Loss: 0.00000915
Iteration 158/1000 | Loss: 0.00000915
Iteration 159/1000 | Loss: 0.00000915
Iteration 160/1000 | Loss: 0.00000915
Iteration 161/1000 | Loss: 0.00000915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [9.149176548817195e-06, 9.149176548817195e-06, 9.149176548817195e-06, 9.149176548817195e-06, 9.149176548817195e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.149176548817195e-06

Optimization complete. Final v2v error: 2.6045451164245605 mm

Highest mean error: 2.812466621398926 mm for frame 30

Lowest mean error: 2.4593546390533447 mm for frame 122

Saving results

Total time: 37.52434039115906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400951
Iteration 2/25 | Loss: 0.00119448
Iteration 3/25 | Loss: 0.00108937
Iteration 4/25 | Loss: 0.00107565
Iteration 5/25 | Loss: 0.00107321
Iteration 6/25 | Loss: 0.00107315
Iteration 7/25 | Loss: 0.00107315
Iteration 8/25 | Loss: 0.00107315
Iteration 9/25 | Loss: 0.00107315
Iteration 10/25 | Loss: 0.00107315
Iteration 11/25 | Loss: 0.00107315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010731486836448312, 0.0010731486836448312, 0.0010731486836448312, 0.0010731486836448312, 0.0010731486836448312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010731486836448312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33850515
Iteration 2/25 | Loss: 0.00071114
Iteration 3/25 | Loss: 0.00071114
Iteration 4/25 | Loss: 0.00071114
Iteration 5/25 | Loss: 0.00071114
Iteration 6/25 | Loss: 0.00071114
Iteration 7/25 | Loss: 0.00071113
Iteration 8/25 | Loss: 0.00071113
Iteration 9/25 | Loss: 0.00071113
Iteration 10/25 | Loss: 0.00071113
Iteration 11/25 | Loss: 0.00071113
Iteration 12/25 | Loss: 0.00071113
Iteration 13/25 | Loss: 0.00071113
Iteration 14/25 | Loss: 0.00071113
Iteration 15/25 | Loss: 0.00071113
Iteration 16/25 | Loss: 0.00071113
Iteration 17/25 | Loss: 0.00071113
Iteration 18/25 | Loss: 0.00071113
Iteration 19/25 | Loss: 0.00071113
Iteration 20/25 | Loss: 0.00071113
Iteration 21/25 | Loss: 0.00071113
Iteration 22/25 | Loss: 0.00071113
Iteration 23/25 | Loss: 0.00071113
Iteration 24/25 | Loss: 0.00071113
Iteration 25/25 | Loss: 0.00071113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071113
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001814
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001420
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001296
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001200
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001195
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001176
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001165
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001159
Iteration 64/1000 | Loss: 0.00001159
Iteration 65/1000 | Loss: 0.00001159
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001150
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001148
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001147
Iteration 129/1000 | Loss: 0.00001147
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001145
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Iteration 167/1000 | Loss: 0.00001142
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001142
Iteration 176/1000 | Loss: 0.00001142
Iteration 177/1000 | Loss: 0.00001142
Iteration 178/1000 | Loss: 0.00001142
Iteration 179/1000 | Loss: 0.00001142
Iteration 180/1000 | Loss: 0.00001142
Iteration 181/1000 | Loss: 0.00001142
Iteration 182/1000 | Loss: 0.00001142
Iteration 183/1000 | Loss: 0.00001142
Iteration 184/1000 | Loss: 0.00001142
Iteration 185/1000 | Loss: 0.00001142
Iteration 186/1000 | Loss: 0.00001142
Iteration 187/1000 | Loss: 0.00001142
Iteration 188/1000 | Loss: 0.00001142
Iteration 189/1000 | Loss: 0.00001142
Iteration 190/1000 | Loss: 0.00001142
Iteration 191/1000 | Loss: 0.00001142
Iteration 192/1000 | Loss: 0.00001142
Iteration 193/1000 | Loss: 0.00001142
Iteration 194/1000 | Loss: 0.00001142
Iteration 195/1000 | Loss: 0.00001142
Iteration 196/1000 | Loss: 0.00001142
Iteration 197/1000 | Loss: 0.00001142
Iteration 198/1000 | Loss: 0.00001142
Iteration 199/1000 | Loss: 0.00001142
Iteration 200/1000 | Loss: 0.00001142
Iteration 201/1000 | Loss: 0.00001142
Iteration 202/1000 | Loss: 0.00001142
Iteration 203/1000 | Loss: 0.00001142
Iteration 204/1000 | Loss: 0.00001142
Iteration 205/1000 | Loss: 0.00001142
Iteration 206/1000 | Loss: 0.00001142
Iteration 207/1000 | Loss: 0.00001142
Iteration 208/1000 | Loss: 0.00001142
Iteration 209/1000 | Loss: 0.00001142
Iteration 210/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1417916539357975e-05, 1.1417916539357975e-05, 1.1417916539357975e-05, 1.1417916539357975e-05, 1.1417916539357975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1417916539357975e-05

Optimization complete. Final v2v error: 2.8574461936950684 mm

Highest mean error: 3.4568545818328857 mm for frame 97

Lowest mean error: 2.4944584369659424 mm for frame 18

Saving results

Total time: 44.7575421333313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927248
Iteration 2/25 | Loss: 0.00160089
Iteration 3/25 | Loss: 0.00140448
Iteration 4/25 | Loss: 0.00136792
Iteration 5/25 | Loss: 0.00135927
Iteration 6/25 | Loss: 0.00134751
Iteration 7/25 | Loss: 0.00134188
Iteration 8/25 | Loss: 0.00133956
Iteration 9/25 | Loss: 0.00134118
Iteration 10/25 | Loss: 0.00133416
Iteration 11/25 | Loss: 0.00132818
Iteration 12/25 | Loss: 0.00132667
Iteration 13/25 | Loss: 0.00132622
Iteration 14/25 | Loss: 0.00132579
Iteration 15/25 | Loss: 0.00132809
Iteration 16/25 | Loss: 0.00132755
Iteration 17/25 | Loss: 0.00132190
Iteration 18/25 | Loss: 0.00132007
Iteration 19/25 | Loss: 0.00131977
Iteration 20/25 | Loss: 0.00131975
Iteration 21/25 | Loss: 0.00131975
Iteration 22/25 | Loss: 0.00131974
Iteration 23/25 | Loss: 0.00131974
Iteration 24/25 | Loss: 0.00131974
Iteration 25/25 | Loss: 0.00131974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34417522
Iteration 2/25 | Loss: 0.00179046
Iteration 3/25 | Loss: 0.00179046
Iteration 4/25 | Loss: 0.00179046
Iteration 5/25 | Loss: 0.00179045
Iteration 6/25 | Loss: 0.00179045
Iteration 7/25 | Loss: 0.00179045
Iteration 8/25 | Loss: 0.00179045
Iteration 9/25 | Loss: 0.00179045
Iteration 10/25 | Loss: 0.00179045
Iteration 11/25 | Loss: 0.00179045
Iteration 12/25 | Loss: 0.00179045
Iteration 13/25 | Loss: 0.00179045
Iteration 14/25 | Loss: 0.00179045
Iteration 15/25 | Loss: 0.00179045
Iteration 16/25 | Loss: 0.00179045
Iteration 17/25 | Loss: 0.00179045
Iteration 18/25 | Loss: 0.00179045
Iteration 19/25 | Loss: 0.00179045
Iteration 20/25 | Loss: 0.00179045
Iteration 21/25 | Loss: 0.00179045
Iteration 22/25 | Loss: 0.00179045
Iteration 23/25 | Loss: 0.00179045
Iteration 24/25 | Loss: 0.00179045
Iteration 25/25 | Loss: 0.00179045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179045
Iteration 2/1000 | Loss: 0.00076990
Iteration 3/1000 | Loss: 0.00012644
Iteration 4/1000 | Loss: 0.00012360
Iteration 5/1000 | Loss: 0.00009352
Iteration 6/1000 | Loss: 0.00008464
Iteration 7/1000 | Loss: 0.00007767
Iteration 8/1000 | Loss: 0.00007214
Iteration 9/1000 | Loss: 0.00006926
Iteration 10/1000 | Loss: 0.00006691
Iteration 11/1000 | Loss: 0.00016125
Iteration 12/1000 | Loss: 0.00011570
Iteration 13/1000 | Loss: 0.00008192
Iteration 14/1000 | Loss: 0.00007332
Iteration 15/1000 | Loss: 0.00006333
Iteration 16/1000 | Loss: 0.00006170
Iteration 17/1000 | Loss: 0.00006013
Iteration 18/1000 | Loss: 0.00045779
Iteration 19/1000 | Loss: 0.00067571
Iteration 20/1000 | Loss: 0.00024170
Iteration 21/1000 | Loss: 0.00013522
Iteration 22/1000 | Loss: 0.00006256
Iteration 23/1000 | Loss: 0.00005785
Iteration 24/1000 | Loss: 0.00005703
Iteration 25/1000 | Loss: 0.00028868
Iteration 26/1000 | Loss: 0.00013180
Iteration 27/1000 | Loss: 0.00005847
Iteration 28/1000 | Loss: 0.00005623
Iteration 29/1000 | Loss: 0.00206134
Iteration 30/1000 | Loss: 0.00299989
Iteration 31/1000 | Loss: 0.00123472
Iteration 32/1000 | Loss: 0.00011465
Iteration 33/1000 | Loss: 0.00009078
Iteration 34/1000 | Loss: 0.00029472
Iteration 35/1000 | Loss: 0.00006872
Iteration 36/1000 | Loss: 0.00005620
Iteration 37/1000 | Loss: 0.00004598
Iteration 38/1000 | Loss: 0.00003991
Iteration 39/1000 | Loss: 0.00003546
Iteration 40/1000 | Loss: 0.00003257
Iteration 41/1000 | Loss: 0.00020944
Iteration 42/1000 | Loss: 0.00003379
Iteration 43/1000 | Loss: 0.00003045
Iteration 44/1000 | Loss: 0.00002875
Iteration 45/1000 | Loss: 0.00002763
Iteration 46/1000 | Loss: 0.00002692
Iteration 47/1000 | Loss: 0.00002629
Iteration 48/1000 | Loss: 0.00002574
Iteration 49/1000 | Loss: 0.00002525
Iteration 50/1000 | Loss: 0.00002494
Iteration 51/1000 | Loss: 0.00002470
Iteration 52/1000 | Loss: 0.00002455
Iteration 53/1000 | Loss: 0.00002442
Iteration 54/1000 | Loss: 0.00002435
Iteration 55/1000 | Loss: 0.00002435
Iteration 56/1000 | Loss: 0.00002434
Iteration 57/1000 | Loss: 0.00002434
Iteration 58/1000 | Loss: 0.00002433
Iteration 59/1000 | Loss: 0.00002433
Iteration 60/1000 | Loss: 0.00002432
Iteration 61/1000 | Loss: 0.00002432
Iteration 62/1000 | Loss: 0.00002431
Iteration 63/1000 | Loss: 0.00002431
Iteration 64/1000 | Loss: 0.00002431
Iteration 65/1000 | Loss: 0.00002426
Iteration 66/1000 | Loss: 0.00002425
Iteration 67/1000 | Loss: 0.00002423
Iteration 68/1000 | Loss: 0.00002423
Iteration 69/1000 | Loss: 0.00002416
Iteration 70/1000 | Loss: 0.00002414
Iteration 71/1000 | Loss: 0.00002414
Iteration 72/1000 | Loss: 0.00002414
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002413
Iteration 76/1000 | Loss: 0.00002413
Iteration 77/1000 | Loss: 0.00002412
Iteration 78/1000 | Loss: 0.00002412
Iteration 79/1000 | Loss: 0.00002411
Iteration 80/1000 | Loss: 0.00002411
Iteration 81/1000 | Loss: 0.00002411
Iteration 82/1000 | Loss: 0.00002410
Iteration 83/1000 | Loss: 0.00002410
Iteration 84/1000 | Loss: 0.00002410
Iteration 85/1000 | Loss: 0.00002410
Iteration 86/1000 | Loss: 0.00002410
Iteration 87/1000 | Loss: 0.00002409
Iteration 88/1000 | Loss: 0.00002409
Iteration 89/1000 | Loss: 0.00002409
Iteration 90/1000 | Loss: 0.00002409
Iteration 91/1000 | Loss: 0.00002408
Iteration 92/1000 | Loss: 0.00002408
Iteration 93/1000 | Loss: 0.00002408
Iteration 94/1000 | Loss: 0.00002408
Iteration 95/1000 | Loss: 0.00002408
Iteration 96/1000 | Loss: 0.00002407
Iteration 97/1000 | Loss: 0.00002407
Iteration 98/1000 | Loss: 0.00002407
Iteration 99/1000 | Loss: 0.00002407
Iteration 100/1000 | Loss: 0.00002407
Iteration 101/1000 | Loss: 0.00002407
Iteration 102/1000 | Loss: 0.00002407
Iteration 103/1000 | Loss: 0.00002407
Iteration 104/1000 | Loss: 0.00002407
Iteration 105/1000 | Loss: 0.00002406
Iteration 106/1000 | Loss: 0.00002406
Iteration 107/1000 | Loss: 0.00002406
Iteration 108/1000 | Loss: 0.00002406
Iteration 109/1000 | Loss: 0.00002406
Iteration 110/1000 | Loss: 0.00002406
Iteration 111/1000 | Loss: 0.00002406
Iteration 112/1000 | Loss: 0.00002406
Iteration 113/1000 | Loss: 0.00002406
Iteration 114/1000 | Loss: 0.00002405
Iteration 115/1000 | Loss: 0.00002405
Iteration 116/1000 | Loss: 0.00002405
Iteration 117/1000 | Loss: 0.00002405
Iteration 118/1000 | Loss: 0.00002405
Iteration 119/1000 | Loss: 0.00002405
Iteration 120/1000 | Loss: 0.00002405
Iteration 121/1000 | Loss: 0.00002405
Iteration 122/1000 | Loss: 0.00002405
Iteration 123/1000 | Loss: 0.00002405
Iteration 124/1000 | Loss: 0.00002405
Iteration 125/1000 | Loss: 0.00002405
Iteration 126/1000 | Loss: 0.00002404
Iteration 127/1000 | Loss: 0.00002404
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002404
Iteration 130/1000 | Loss: 0.00002404
Iteration 131/1000 | Loss: 0.00002404
Iteration 132/1000 | Loss: 0.00002404
Iteration 133/1000 | Loss: 0.00002404
Iteration 134/1000 | Loss: 0.00002404
Iteration 135/1000 | Loss: 0.00002404
Iteration 136/1000 | Loss: 0.00002404
Iteration 137/1000 | Loss: 0.00002404
Iteration 138/1000 | Loss: 0.00002404
Iteration 139/1000 | Loss: 0.00002404
Iteration 140/1000 | Loss: 0.00002404
Iteration 141/1000 | Loss: 0.00002404
Iteration 142/1000 | Loss: 0.00002403
Iteration 143/1000 | Loss: 0.00002403
Iteration 144/1000 | Loss: 0.00002403
Iteration 145/1000 | Loss: 0.00002403
Iteration 146/1000 | Loss: 0.00002403
Iteration 147/1000 | Loss: 0.00002403
Iteration 148/1000 | Loss: 0.00002403
Iteration 149/1000 | Loss: 0.00002403
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002403
Iteration 152/1000 | Loss: 0.00002403
Iteration 153/1000 | Loss: 0.00002403
Iteration 154/1000 | Loss: 0.00002403
Iteration 155/1000 | Loss: 0.00002403
Iteration 156/1000 | Loss: 0.00002402
Iteration 157/1000 | Loss: 0.00002402
Iteration 158/1000 | Loss: 0.00002402
Iteration 159/1000 | Loss: 0.00002402
Iteration 160/1000 | Loss: 0.00002402
Iteration 161/1000 | Loss: 0.00002402
Iteration 162/1000 | Loss: 0.00002402
Iteration 163/1000 | Loss: 0.00002402
Iteration 164/1000 | Loss: 0.00002402
Iteration 165/1000 | Loss: 0.00002402
Iteration 166/1000 | Loss: 0.00002402
Iteration 167/1000 | Loss: 0.00002402
Iteration 168/1000 | Loss: 0.00002402
Iteration 169/1000 | Loss: 0.00002402
Iteration 170/1000 | Loss: 0.00002402
Iteration 171/1000 | Loss: 0.00002402
Iteration 172/1000 | Loss: 0.00002402
Iteration 173/1000 | Loss: 0.00002402
Iteration 174/1000 | Loss: 0.00002402
Iteration 175/1000 | Loss: 0.00002402
Iteration 176/1000 | Loss: 0.00002402
Iteration 177/1000 | Loss: 0.00002402
Iteration 178/1000 | Loss: 0.00002401
Iteration 179/1000 | Loss: 0.00002401
Iteration 180/1000 | Loss: 0.00002401
Iteration 181/1000 | Loss: 0.00002401
Iteration 182/1000 | Loss: 0.00002401
Iteration 183/1000 | Loss: 0.00002401
Iteration 184/1000 | Loss: 0.00002401
Iteration 185/1000 | Loss: 0.00002401
Iteration 186/1000 | Loss: 0.00002401
Iteration 187/1000 | Loss: 0.00002401
Iteration 188/1000 | Loss: 0.00002401
Iteration 189/1000 | Loss: 0.00002401
Iteration 190/1000 | Loss: 0.00002400
Iteration 191/1000 | Loss: 0.00002400
Iteration 192/1000 | Loss: 0.00002400
Iteration 193/1000 | Loss: 0.00002400
Iteration 194/1000 | Loss: 0.00002400
Iteration 195/1000 | Loss: 0.00002400
Iteration 196/1000 | Loss: 0.00002400
Iteration 197/1000 | Loss: 0.00002400
Iteration 198/1000 | Loss: 0.00002400
Iteration 199/1000 | Loss: 0.00002400
Iteration 200/1000 | Loss: 0.00002400
Iteration 201/1000 | Loss: 0.00002400
Iteration 202/1000 | Loss: 0.00002400
Iteration 203/1000 | Loss: 0.00002399
Iteration 204/1000 | Loss: 0.00002399
Iteration 205/1000 | Loss: 0.00002399
Iteration 206/1000 | Loss: 0.00002399
Iteration 207/1000 | Loss: 0.00002399
Iteration 208/1000 | Loss: 0.00002399
Iteration 209/1000 | Loss: 0.00002399
Iteration 210/1000 | Loss: 0.00002399
Iteration 211/1000 | Loss: 0.00002399
Iteration 212/1000 | Loss: 0.00002399
Iteration 213/1000 | Loss: 0.00002399
Iteration 214/1000 | Loss: 0.00002399
Iteration 215/1000 | Loss: 0.00002399
Iteration 216/1000 | Loss: 0.00002399
Iteration 217/1000 | Loss: 0.00002399
Iteration 218/1000 | Loss: 0.00002399
Iteration 219/1000 | Loss: 0.00002399
Iteration 220/1000 | Loss: 0.00002398
Iteration 221/1000 | Loss: 0.00002398
Iteration 222/1000 | Loss: 0.00002398
Iteration 223/1000 | Loss: 0.00002398
Iteration 224/1000 | Loss: 0.00002398
Iteration 225/1000 | Loss: 0.00002398
Iteration 226/1000 | Loss: 0.00002398
Iteration 227/1000 | Loss: 0.00002398
Iteration 228/1000 | Loss: 0.00002398
Iteration 229/1000 | Loss: 0.00002398
Iteration 230/1000 | Loss: 0.00002398
Iteration 231/1000 | Loss: 0.00002398
Iteration 232/1000 | Loss: 0.00002398
Iteration 233/1000 | Loss: 0.00002398
Iteration 234/1000 | Loss: 0.00002398
Iteration 235/1000 | Loss: 0.00002398
Iteration 236/1000 | Loss: 0.00002398
Iteration 237/1000 | Loss: 0.00002398
Iteration 238/1000 | Loss: 0.00002398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.398126889602281e-05, 2.398126889602281e-05, 2.398126889602281e-05, 2.398126889602281e-05, 2.398126889602281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.398126889602281e-05

Optimization complete. Final v2v error: 4.072031497955322 mm

Highest mean error: 4.896138668060303 mm for frame 121

Lowest mean error: 3.267997980117798 mm for frame 203

Saving results

Total time: 140.01640915870667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498523
Iteration 2/25 | Loss: 0.00124691
Iteration 3/25 | Loss: 0.00115087
Iteration 4/25 | Loss: 0.00114048
Iteration 5/25 | Loss: 0.00113816
Iteration 6/25 | Loss: 0.00113816
Iteration 7/25 | Loss: 0.00113816
Iteration 8/25 | Loss: 0.00113816
Iteration 9/25 | Loss: 0.00113816
Iteration 10/25 | Loss: 0.00113816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011381577933207154, 0.0011381577933207154, 0.0011381577933207154, 0.0011381577933207154, 0.0011381577933207154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011381577933207154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34884334
Iteration 2/25 | Loss: 0.00068117
Iteration 3/25 | Loss: 0.00068115
Iteration 4/25 | Loss: 0.00068115
Iteration 5/25 | Loss: 0.00068115
Iteration 6/25 | Loss: 0.00068115
Iteration 7/25 | Loss: 0.00068115
Iteration 8/25 | Loss: 0.00068115
Iteration 9/25 | Loss: 0.00068115
Iteration 10/25 | Loss: 0.00068115
Iteration 11/25 | Loss: 0.00068115
Iteration 12/25 | Loss: 0.00068115
Iteration 13/25 | Loss: 0.00068115
Iteration 14/25 | Loss: 0.00068115
Iteration 15/25 | Loss: 0.00068115
Iteration 16/25 | Loss: 0.00068115
Iteration 17/25 | Loss: 0.00068115
Iteration 18/25 | Loss: 0.00068115
Iteration 19/25 | Loss: 0.00068115
Iteration 20/25 | Loss: 0.00068115
Iteration 21/25 | Loss: 0.00068115
Iteration 22/25 | Loss: 0.00068115
Iteration 23/25 | Loss: 0.00068115
Iteration 24/25 | Loss: 0.00068115
Iteration 25/25 | Loss: 0.00068115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068115
Iteration 2/1000 | Loss: 0.00002137
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001431
Iteration 5/1000 | Loss: 0.00001372
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001244
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001238
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001233
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001216
Iteration 29/1000 | Loss: 0.00001216
Iteration 30/1000 | Loss: 0.00001215
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001204
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001188
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001175
Iteration 130/1000 | Loss: 0.00001175
Iteration 131/1000 | Loss: 0.00001174
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001174
Iteration 135/1000 | Loss: 0.00001173
Iteration 136/1000 | Loss: 0.00001173
Iteration 137/1000 | Loss: 0.00001173
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001171
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1694062777678482e-05, 1.1694062777678482e-05, 1.1694062777678482e-05, 1.1694062777678482e-05, 1.1694062777678482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1694062777678482e-05

Optimization complete. Final v2v error: 2.874420404434204 mm

Highest mean error: 3.0836892127990723 mm for frame 65

Lowest mean error: 2.575974702835083 mm for frame 223

Saving results

Total time: 42.772722482681274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666804
Iteration 2/25 | Loss: 0.00147839
Iteration 3/25 | Loss: 0.00123379
Iteration 4/25 | Loss: 0.00121217
Iteration 5/25 | Loss: 0.00121229
Iteration 6/25 | Loss: 0.00120865
Iteration 7/25 | Loss: 0.00120731
Iteration 8/25 | Loss: 0.00120714
Iteration 9/25 | Loss: 0.00120710
Iteration 10/25 | Loss: 0.00120710
Iteration 11/25 | Loss: 0.00120710
Iteration 12/25 | Loss: 0.00120710
Iteration 13/25 | Loss: 0.00120710
Iteration 14/25 | Loss: 0.00120710
Iteration 15/25 | Loss: 0.00120710
Iteration 16/25 | Loss: 0.00120710
Iteration 17/25 | Loss: 0.00120710
Iteration 18/25 | Loss: 0.00120710
Iteration 19/25 | Loss: 0.00120710
Iteration 20/25 | Loss: 0.00120710
Iteration 21/25 | Loss: 0.00120710
Iteration 22/25 | Loss: 0.00120710
Iteration 23/25 | Loss: 0.00120710
Iteration 24/25 | Loss: 0.00120710
Iteration 25/25 | Loss: 0.00120710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70083284
Iteration 2/25 | Loss: 0.00092958
Iteration 3/25 | Loss: 0.00092950
Iteration 4/25 | Loss: 0.00092950
Iteration 5/25 | Loss: 0.00092950
Iteration 6/25 | Loss: 0.00092950
Iteration 7/25 | Loss: 0.00092950
Iteration 8/25 | Loss: 0.00092950
Iteration 9/25 | Loss: 0.00092950
Iteration 10/25 | Loss: 0.00092950
Iteration 11/25 | Loss: 0.00092950
Iteration 12/25 | Loss: 0.00092950
Iteration 13/25 | Loss: 0.00092950
Iteration 14/25 | Loss: 0.00092950
Iteration 15/25 | Loss: 0.00092950
Iteration 16/25 | Loss: 0.00092950
Iteration 17/25 | Loss: 0.00092950
Iteration 18/25 | Loss: 0.00092950
Iteration 19/25 | Loss: 0.00092950
Iteration 20/25 | Loss: 0.00092950
Iteration 21/25 | Loss: 0.00092950
Iteration 22/25 | Loss: 0.00092950
Iteration 23/25 | Loss: 0.00092950
Iteration 24/25 | Loss: 0.00092950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009294953779317439, 0.0009294953779317439, 0.0009294953779317439, 0.0009294953779317439, 0.0009294953779317439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009294953779317439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092950
Iteration 2/1000 | Loss: 0.00003069
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001618
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001600
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001591
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001589
Iteration 29/1000 | Loss: 0.00001589
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001584
Iteration 34/1000 | Loss: 0.00001584
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001581
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001581
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001579
Iteration 46/1000 | Loss: 0.00001579
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001578
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001578
Iteration 54/1000 | Loss: 0.00001578
Iteration 55/1000 | Loss: 0.00001578
Iteration 56/1000 | Loss: 0.00001578
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001577
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001568
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001567
Iteration 119/1000 | Loss: 0.00001567
Iteration 120/1000 | Loss: 0.00001567
Iteration 121/1000 | Loss: 0.00001567
Iteration 122/1000 | Loss: 0.00001567
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001566
Iteration 127/1000 | Loss: 0.00001566
Iteration 128/1000 | Loss: 0.00001566
Iteration 129/1000 | Loss: 0.00001566
Iteration 130/1000 | Loss: 0.00001566
Iteration 131/1000 | Loss: 0.00001566
Iteration 132/1000 | Loss: 0.00001566
Iteration 133/1000 | Loss: 0.00001566
Iteration 134/1000 | Loss: 0.00001566
Iteration 135/1000 | Loss: 0.00001566
Iteration 136/1000 | Loss: 0.00001566
Iteration 137/1000 | Loss: 0.00001566
Iteration 138/1000 | Loss: 0.00001566
Iteration 139/1000 | Loss: 0.00001565
Iteration 140/1000 | Loss: 0.00001565
Iteration 141/1000 | Loss: 0.00001565
Iteration 142/1000 | Loss: 0.00001565
Iteration 143/1000 | Loss: 0.00001565
Iteration 144/1000 | Loss: 0.00001565
Iteration 145/1000 | Loss: 0.00001565
Iteration 146/1000 | Loss: 0.00001565
Iteration 147/1000 | Loss: 0.00001565
Iteration 148/1000 | Loss: 0.00001565
Iteration 149/1000 | Loss: 0.00001565
Iteration 150/1000 | Loss: 0.00001565
Iteration 151/1000 | Loss: 0.00001565
Iteration 152/1000 | Loss: 0.00001565
Iteration 153/1000 | Loss: 0.00001565
Iteration 154/1000 | Loss: 0.00001565
Iteration 155/1000 | Loss: 0.00001565
Iteration 156/1000 | Loss: 0.00001565
Iteration 157/1000 | Loss: 0.00001565
Iteration 158/1000 | Loss: 0.00001565
Iteration 159/1000 | Loss: 0.00001565
Iteration 160/1000 | Loss: 0.00001565
Iteration 161/1000 | Loss: 0.00001565
Iteration 162/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.5650730347260833e-05, 1.5650730347260833e-05, 1.5650730347260833e-05, 1.5650730347260833e-05, 1.5650730347260833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5650730347260833e-05

Optimization complete. Final v2v error: 3.242465019226074 mm

Highest mean error: 3.7040271759033203 mm for frame 85

Lowest mean error: 2.7989468574523926 mm for frame 157

Saving results

Total time: 50.528857946395874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063270
Iteration 2/25 | Loss: 0.00150586
Iteration 3/25 | Loss: 0.00125781
Iteration 4/25 | Loss: 0.00114405
Iteration 5/25 | Loss: 0.00112942
Iteration 6/25 | Loss: 0.00112516
Iteration 7/25 | Loss: 0.00112484
Iteration 8/25 | Loss: 0.00112470
Iteration 9/25 | Loss: 0.00112876
Iteration 10/25 | Loss: 0.00112529
Iteration 11/25 | Loss: 0.00112285
Iteration 12/25 | Loss: 0.00112199
Iteration 13/25 | Loss: 0.00112168
Iteration 14/25 | Loss: 0.00112148
Iteration 15/25 | Loss: 0.00112148
Iteration 16/25 | Loss: 0.00112147
Iteration 17/25 | Loss: 0.00112147
Iteration 18/25 | Loss: 0.00112147
Iteration 19/25 | Loss: 0.00112147
Iteration 20/25 | Loss: 0.00112147
Iteration 21/25 | Loss: 0.00112147
Iteration 22/25 | Loss: 0.00112147
Iteration 23/25 | Loss: 0.00112147
Iteration 24/25 | Loss: 0.00112147
Iteration 25/25 | Loss: 0.00112147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.42877293
Iteration 2/25 | Loss: 0.00088569
Iteration 3/25 | Loss: 0.00088569
Iteration 4/25 | Loss: 0.00088569
Iteration 5/25 | Loss: 0.00088569
Iteration 6/25 | Loss: 0.00088569
Iteration 7/25 | Loss: 0.00088569
Iteration 8/25 | Loss: 0.00088569
Iteration 9/25 | Loss: 0.00088569
Iteration 10/25 | Loss: 0.00088569
Iteration 11/25 | Loss: 0.00088569
Iteration 12/25 | Loss: 0.00088568
Iteration 13/25 | Loss: 0.00088568
Iteration 14/25 | Loss: 0.00088568
Iteration 15/25 | Loss: 0.00088568
Iteration 16/25 | Loss: 0.00088568
Iteration 17/25 | Loss: 0.00088568
Iteration 18/25 | Loss: 0.00088568
Iteration 19/25 | Loss: 0.00088568
Iteration 20/25 | Loss: 0.00088568
Iteration 21/25 | Loss: 0.00088568
Iteration 22/25 | Loss: 0.00088568
Iteration 23/25 | Loss: 0.00088568
Iteration 24/25 | Loss: 0.00088568
Iteration 25/25 | Loss: 0.00088568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088568
Iteration 2/1000 | Loss: 0.00002134
Iteration 3/1000 | Loss: 0.00007501
Iteration 4/1000 | Loss: 0.00012720
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00016058
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00020031
Iteration 13/1000 | Loss: 0.00005055
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00006127
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001064
Iteration 21/1000 | Loss: 0.00001060
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00003293
Iteration 25/1000 | Loss: 0.00001042
Iteration 26/1000 | Loss: 0.00001038
Iteration 27/1000 | Loss: 0.00001038
Iteration 28/1000 | Loss: 0.00001038
Iteration 29/1000 | Loss: 0.00001038
Iteration 30/1000 | Loss: 0.00001038
Iteration 31/1000 | Loss: 0.00001037
Iteration 32/1000 | Loss: 0.00001037
Iteration 33/1000 | Loss: 0.00001037
Iteration 34/1000 | Loss: 0.00001037
Iteration 35/1000 | Loss: 0.00001037
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001037
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001036
Iteration 40/1000 | Loss: 0.00001036
Iteration 41/1000 | Loss: 0.00001035
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001033
Iteration 46/1000 | Loss: 0.00001032
Iteration 47/1000 | Loss: 0.00001032
Iteration 48/1000 | Loss: 0.00001031
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001030
Iteration 54/1000 | Loss: 0.00001030
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001029
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00008000
Iteration 61/1000 | Loss: 0.00001035
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001025
Iteration 67/1000 | Loss: 0.00001025
Iteration 68/1000 | Loss: 0.00001025
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00004352
Iteration 80/1000 | Loss: 0.00001403
Iteration 81/1000 | Loss: 0.00001032
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00006858
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00002995
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001246
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001026
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001025
Iteration 94/1000 | Loss: 0.00001024
Iteration 95/1000 | Loss: 0.00001024
Iteration 96/1000 | Loss: 0.00001024
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001023
Iteration 100/1000 | Loss: 0.00001023
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001022
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001020
Iteration 115/1000 | Loss: 0.00001020
Iteration 116/1000 | Loss: 0.00001020
Iteration 117/1000 | Loss: 0.00001020
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001018
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001017
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001017
Iteration 135/1000 | Loss: 0.00001017
Iteration 136/1000 | Loss: 0.00001017
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001016
Iteration 140/1000 | Loss: 0.00001016
Iteration 141/1000 | Loss: 0.00001016
Iteration 142/1000 | Loss: 0.00001016
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001016
Iteration 149/1000 | Loss: 0.00001016
Iteration 150/1000 | Loss: 0.00001016
Iteration 151/1000 | Loss: 0.00001016
Iteration 152/1000 | Loss: 0.00001016
Iteration 153/1000 | Loss: 0.00001016
Iteration 154/1000 | Loss: 0.00001016
Iteration 155/1000 | Loss: 0.00001016
Iteration 156/1000 | Loss: 0.00001016
Iteration 157/1000 | Loss: 0.00001016
Iteration 158/1000 | Loss: 0.00001016
Iteration 159/1000 | Loss: 0.00001016
Iteration 160/1000 | Loss: 0.00001016
Iteration 161/1000 | Loss: 0.00001016
Iteration 162/1000 | Loss: 0.00001016
Iteration 163/1000 | Loss: 0.00001016
Iteration 164/1000 | Loss: 0.00001016
Iteration 165/1000 | Loss: 0.00001016
Iteration 166/1000 | Loss: 0.00001016
Iteration 167/1000 | Loss: 0.00001016
Iteration 168/1000 | Loss: 0.00001016
Iteration 169/1000 | Loss: 0.00001016
Iteration 170/1000 | Loss: 0.00001016
Iteration 171/1000 | Loss: 0.00001016
Iteration 172/1000 | Loss: 0.00001016
Iteration 173/1000 | Loss: 0.00001016
Iteration 174/1000 | Loss: 0.00001016
Iteration 175/1000 | Loss: 0.00001016
Iteration 176/1000 | Loss: 0.00001016
Iteration 177/1000 | Loss: 0.00001016
Iteration 178/1000 | Loss: 0.00001016
Iteration 179/1000 | Loss: 0.00001016
Iteration 180/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.0157526958209928e-05, 1.0157526958209928e-05, 1.0157526958209928e-05, 1.0157526958209928e-05, 1.0157526958209928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0157526958209928e-05

Optimization complete. Final v2v error: 2.7001397609710693 mm

Highest mean error: 3.0084521770477295 mm for frame 177

Lowest mean error: 2.5155436992645264 mm for frame 159

Saving results

Total time: 87.3621129989624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970766
Iteration 2/25 | Loss: 0.00192727
Iteration 3/25 | Loss: 0.00129028
Iteration 4/25 | Loss: 0.00123316
Iteration 5/25 | Loss: 0.00125107
Iteration 6/25 | Loss: 0.00122130
Iteration 7/25 | Loss: 0.00118684
Iteration 8/25 | Loss: 0.00117042
Iteration 9/25 | Loss: 0.00116657
Iteration 10/25 | Loss: 0.00116005
Iteration 11/25 | Loss: 0.00115888
Iteration 12/25 | Loss: 0.00113958
Iteration 13/25 | Loss: 0.00112527
Iteration 14/25 | Loss: 0.00111895
Iteration 15/25 | Loss: 0.00111832
Iteration 16/25 | Loss: 0.00111508
Iteration 17/25 | Loss: 0.00111767
Iteration 18/25 | Loss: 0.00111184
Iteration 19/25 | Loss: 0.00110745
Iteration 20/25 | Loss: 0.00110787
Iteration 21/25 | Loss: 0.00110842
Iteration 22/25 | Loss: 0.00110790
Iteration 23/25 | Loss: 0.00110699
Iteration 24/25 | Loss: 0.00110690
Iteration 25/25 | Loss: 0.00110690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45637703
Iteration 2/25 | Loss: 0.00112155
Iteration 3/25 | Loss: 0.00112155
Iteration 4/25 | Loss: 0.00111226
Iteration 5/25 | Loss: 0.00111225
Iteration 6/25 | Loss: 0.00111225
Iteration 7/25 | Loss: 0.00111225
Iteration 8/25 | Loss: 0.00111225
Iteration 9/25 | Loss: 0.00111225
Iteration 10/25 | Loss: 0.00111225
Iteration 11/25 | Loss: 0.00111225
Iteration 12/25 | Loss: 0.00111225
Iteration 13/25 | Loss: 0.00111225
Iteration 14/25 | Loss: 0.00111225
Iteration 15/25 | Loss: 0.00111225
Iteration 16/25 | Loss: 0.00111225
Iteration 17/25 | Loss: 0.00111225
Iteration 18/25 | Loss: 0.00111225
Iteration 19/25 | Loss: 0.00111225
Iteration 20/25 | Loss: 0.00111225
Iteration 21/25 | Loss: 0.00111225
Iteration 22/25 | Loss: 0.00111225
Iteration 23/25 | Loss: 0.00111225
Iteration 24/25 | Loss: 0.00111225
Iteration 25/25 | Loss: 0.00111225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111225
Iteration 2/1000 | Loss: 0.00005198
Iteration 3/1000 | Loss: 0.00005065
Iteration 4/1000 | Loss: 0.00002607
Iteration 5/1000 | Loss: 0.00007498
Iteration 6/1000 | Loss: 0.00002249
Iteration 7/1000 | Loss: 0.00005539
Iteration 8/1000 | Loss: 0.00014038
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002771
Iteration 11/1000 | Loss: 0.00004180
Iteration 12/1000 | Loss: 0.00001983
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00004731
Iteration 16/1000 | Loss: 0.00016105
Iteration 17/1000 | Loss: 0.00011775
Iteration 18/1000 | Loss: 0.00029950
Iteration 19/1000 | Loss: 0.00005192
Iteration 20/1000 | Loss: 0.00003235
Iteration 21/1000 | Loss: 0.00012098
Iteration 22/1000 | Loss: 0.00075905
Iteration 23/1000 | Loss: 0.00004766
Iteration 24/1000 | Loss: 0.00005257
Iteration 25/1000 | Loss: 0.00002381
Iteration 26/1000 | Loss: 0.00009732
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002616
Iteration 29/1000 | Loss: 0.00004765
Iteration 30/1000 | Loss: 0.00002387
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00002256
Iteration 33/1000 | Loss: 0.00002657
Iteration 34/1000 | Loss: 0.00004544
Iteration 35/1000 | Loss: 0.00004780
Iteration 36/1000 | Loss: 0.00005094
Iteration 37/1000 | Loss: 0.00004360
Iteration 38/1000 | Loss: 0.00014862
Iteration 39/1000 | Loss: 0.00004312
Iteration 40/1000 | Loss: 0.00041763
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002679
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00002955
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00002925
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00003341
Iteration 70/1000 | Loss: 0.00003239
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00003334
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001198
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00002950
Iteration 115/1000 | Loss: 0.00003229
Iteration 116/1000 | Loss: 0.00001453
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001190
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001189
Iteration 157/1000 | Loss: 0.00001189
Iteration 158/1000 | Loss: 0.00001189
Iteration 159/1000 | Loss: 0.00001189
Iteration 160/1000 | Loss: 0.00001189
Iteration 161/1000 | Loss: 0.00001189
Iteration 162/1000 | Loss: 0.00001189
Iteration 163/1000 | Loss: 0.00001189
Iteration 164/1000 | Loss: 0.00001189
Iteration 165/1000 | Loss: 0.00001189
Iteration 166/1000 | Loss: 0.00001189
Iteration 167/1000 | Loss: 0.00001189
Iteration 168/1000 | Loss: 0.00001189
Iteration 169/1000 | Loss: 0.00001189
Iteration 170/1000 | Loss: 0.00001189
Iteration 171/1000 | Loss: 0.00001189
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1890752830368001e-05, 1.1890752830368001e-05, 1.1890752830368001e-05, 1.1890752830368001e-05, 1.1890752830368001e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1890752830368001e-05

Optimization complete. Final v2v error: 2.9257121086120605 mm

Highest mean error: 4.5341339111328125 mm for frame 53

Lowest mean error: 2.317378520965576 mm for frame 97

Saving results

Total time: 134.28302669525146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712840
Iteration 2/25 | Loss: 0.00156612
Iteration 3/25 | Loss: 0.00128779
Iteration 4/25 | Loss: 0.00124861
Iteration 5/25 | Loss: 0.00123551
Iteration 6/25 | Loss: 0.00123195
Iteration 7/25 | Loss: 0.00123020
Iteration 8/25 | Loss: 0.00122903
Iteration 9/25 | Loss: 0.00123133
Iteration 10/25 | Loss: 0.00123130
Iteration 11/25 | Loss: 0.00122485
Iteration 12/25 | Loss: 0.00122274
Iteration 13/25 | Loss: 0.00122221
Iteration 14/25 | Loss: 0.00122198
Iteration 15/25 | Loss: 0.00122190
Iteration 16/25 | Loss: 0.00122187
Iteration 17/25 | Loss: 0.00122187
Iteration 18/25 | Loss: 0.00122186
Iteration 19/25 | Loss: 0.00122186
Iteration 20/25 | Loss: 0.00122186
Iteration 21/25 | Loss: 0.00122186
Iteration 22/25 | Loss: 0.00122186
Iteration 23/25 | Loss: 0.00122186
Iteration 24/25 | Loss: 0.00122186
Iteration 25/25 | Loss: 0.00122186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25698543
Iteration 2/25 | Loss: 0.00109602
Iteration 3/25 | Loss: 0.00109598
Iteration 4/25 | Loss: 0.00109598
Iteration 5/25 | Loss: 0.00109598
Iteration 6/25 | Loss: 0.00109598
Iteration 7/25 | Loss: 0.00109598
Iteration 8/25 | Loss: 0.00109598
Iteration 9/25 | Loss: 0.00109598
Iteration 10/25 | Loss: 0.00109598
Iteration 11/25 | Loss: 0.00109598
Iteration 12/25 | Loss: 0.00109598
Iteration 13/25 | Loss: 0.00109598
Iteration 14/25 | Loss: 0.00109598
Iteration 15/25 | Loss: 0.00109598
Iteration 16/25 | Loss: 0.00109598
Iteration 17/25 | Loss: 0.00109598
Iteration 18/25 | Loss: 0.00109598
Iteration 19/25 | Loss: 0.00109598
Iteration 20/25 | Loss: 0.00109598
Iteration 21/25 | Loss: 0.00109598
Iteration 22/25 | Loss: 0.00109598
Iteration 23/25 | Loss: 0.00109598
Iteration 24/25 | Loss: 0.00109598
Iteration 25/25 | Loss: 0.00109598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109598
Iteration 2/1000 | Loss: 0.00007006
Iteration 3/1000 | Loss: 0.00005246
Iteration 4/1000 | Loss: 0.00004835
Iteration 5/1000 | Loss: 0.00004598
Iteration 6/1000 | Loss: 0.00004442
Iteration 7/1000 | Loss: 0.00004366
Iteration 8/1000 | Loss: 0.00004309
Iteration 9/1000 | Loss: 0.00004267
Iteration 10/1000 | Loss: 0.00004220
Iteration 11/1000 | Loss: 0.00004184
Iteration 12/1000 | Loss: 0.00004158
Iteration 13/1000 | Loss: 0.00004123
Iteration 14/1000 | Loss: 0.00004100
Iteration 15/1000 | Loss: 0.00004080
Iteration 16/1000 | Loss: 0.00004076
Iteration 17/1000 | Loss: 0.00004058
Iteration 18/1000 | Loss: 0.00004045
Iteration 19/1000 | Loss: 0.00004045
Iteration 20/1000 | Loss: 0.00004042
Iteration 21/1000 | Loss: 0.00004039
Iteration 22/1000 | Loss: 0.00004028
Iteration 23/1000 | Loss: 0.00004025
Iteration 24/1000 | Loss: 0.00004024
Iteration 25/1000 | Loss: 0.00004024
Iteration 26/1000 | Loss: 0.00004023
Iteration 27/1000 | Loss: 0.00004022
Iteration 28/1000 | Loss: 0.00004021
Iteration 29/1000 | Loss: 0.00004019
Iteration 30/1000 | Loss: 0.00004018
Iteration 31/1000 | Loss: 0.00004016
Iteration 32/1000 | Loss: 0.00004010
Iteration 33/1000 | Loss: 0.00004010
Iteration 34/1000 | Loss: 0.00004010
Iteration 35/1000 | Loss: 0.00004010
Iteration 36/1000 | Loss: 0.00004010
Iteration 37/1000 | Loss: 0.00004010
Iteration 38/1000 | Loss: 0.00004010
Iteration 39/1000 | Loss: 0.00004009
Iteration 40/1000 | Loss: 0.00004008
Iteration 41/1000 | Loss: 0.00004007
Iteration 42/1000 | Loss: 0.00004004
Iteration 43/1000 | Loss: 0.00004003
Iteration 44/1000 | Loss: 0.00004001
Iteration 45/1000 | Loss: 0.00004001
Iteration 46/1000 | Loss: 0.00004001
Iteration 47/1000 | Loss: 0.00004001
Iteration 48/1000 | Loss: 0.00004001
Iteration 49/1000 | Loss: 0.00004001
Iteration 50/1000 | Loss: 0.00004001
Iteration 51/1000 | Loss: 0.00004000
Iteration 52/1000 | Loss: 0.00003999
Iteration 53/1000 | Loss: 0.00003996
Iteration 54/1000 | Loss: 0.00003993
Iteration 55/1000 | Loss: 0.00003992
Iteration 56/1000 | Loss: 0.00003992
Iteration 57/1000 | Loss: 0.00003992
Iteration 58/1000 | Loss: 0.00003991
Iteration 59/1000 | Loss: 0.00003991
Iteration 60/1000 | Loss: 0.00003991
Iteration 61/1000 | Loss: 0.00003991
Iteration 62/1000 | Loss: 0.00003990
Iteration 63/1000 | Loss: 0.00003990
Iteration 64/1000 | Loss: 0.00003990
Iteration 65/1000 | Loss: 0.00003989
Iteration 66/1000 | Loss: 0.00003989
Iteration 67/1000 | Loss: 0.00003989
Iteration 68/1000 | Loss: 0.00003989
Iteration 69/1000 | Loss: 0.00003988
Iteration 70/1000 | Loss: 0.00003988
Iteration 71/1000 | Loss: 0.00003988
Iteration 72/1000 | Loss: 0.00003988
Iteration 73/1000 | Loss: 0.00003987
Iteration 74/1000 | Loss: 0.00003987
Iteration 75/1000 | Loss: 0.00003987
Iteration 76/1000 | Loss: 0.00003987
Iteration 77/1000 | Loss: 0.00003987
Iteration 78/1000 | Loss: 0.00003986
Iteration 79/1000 | Loss: 0.00003986
Iteration 80/1000 | Loss: 0.00003985
Iteration 81/1000 | Loss: 0.00003985
Iteration 82/1000 | Loss: 0.00003985
Iteration 83/1000 | Loss: 0.00003984
Iteration 84/1000 | Loss: 0.00003984
Iteration 85/1000 | Loss: 0.00003983
Iteration 86/1000 | Loss: 0.00003983
Iteration 87/1000 | Loss: 0.00003982
Iteration 88/1000 | Loss: 0.00003982
Iteration 89/1000 | Loss: 0.00003982
Iteration 90/1000 | Loss: 0.00003982
Iteration 91/1000 | Loss: 0.00003982
Iteration 92/1000 | Loss: 0.00003982
Iteration 93/1000 | Loss: 0.00003982
Iteration 94/1000 | Loss: 0.00003982
Iteration 95/1000 | Loss: 0.00003982
Iteration 96/1000 | Loss: 0.00003982
Iteration 97/1000 | Loss: 0.00003981
Iteration 98/1000 | Loss: 0.00003981
Iteration 99/1000 | Loss: 0.00003981
Iteration 100/1000 | Loss: 0.00003981
Iteration 101/1000 | Loss: 0.00003981
Iteration 102/1000 | Loss: 0.00003980
Iteration 103/1000 | Loss: 0.00003979
Iteration 104/1000 | Loss: 0.00003979
Iteration 105/1000 | Loss: 0.00003979
Iteration 106/1000 | Loss: 0.00003978
Iteration 107/1000 | Loss: 0.00003978
Iteration 108/1000 | Loss: 0.00003978
Iteration 109/1000 | Loss: 0.00003978
Iteration 110/1000 | Loss: 0.00003978
Iteration 111/1000 | Loss: 0.00003977
Iteration 112/1000 | Loss: 0.00003977
Iteration 113/1000 | Loss: 0.00003976
Iteration 114/1000 | Loss: 0.00003976
Iteration 115/1000 | Loss: 0.00003975
Iteration 116/1000 | Loss: 0.00003975
Iteration 117/1000 | Loss: 0.00003975
Iteration 118/1000 | Loss: 0.00003975
Iteration 119/1000 | Loss: 0.00003975
Iteration 120/1000 | Loss: 0.00003975
Iteration 121/1000 | Loss: 0.00003974
Iteration 122/1000 | Loss: 0.00003974
Iteration 123/1000 | Loss: 0.00003974
Iteration 124/1000 | Loss: 0.00003974
Iteration 125/1000 | Loss: 0.00003973
Iteration 126/1000 | Loss: 0.00003973
Iteration 127/1000 | Loss: 0.00003972
Iteration 128/1000 | Loss: 0.00003972
Iteration 129/1000 | Loss: 0.00003972
Iteration 130/1000 | Loss: 0.00003971
Iteration 131/1000 | Loss: 0.00003971
Iteration 132/1000 | Loss: 0.00003970
Iteration 133/1000 | Loss: 0.00003969
Iteration 134/1000 | Loss: 0.00003969
Iteration 135/1000 | Loss: 0.00003969
Iteration 136/1000 | Loss: 0.00003968
Iteration 137/1000 | Loss: 0.00003968
Iteration 138/1000 | Loss: 0.00003967
Iteration 139/1000 | Loss: 0.00003967
Iteration 140/1000 | Loss: 0.00003967
Iteration 141/1000 | Loss: 0.00003966
Iteration 142/1000 | Loss: 0.00003965
Iteration 143/1000 | Loss: 0.00003965
Iteration 144/1000 | Loss: 0.00003965
Iteration 145/1000 | Loss: 0.00003965
Iteration 146/1000 | Loss: 0.00003965
Iteration 147/1000 | Loss: 0.00003965
Iteration 148/1000 | Loss: 0.00003965
Iteration 149/1000 | Loss: 0.00003964
Iteration 150/1000 | Loss: 0.00003964
Iteration 151/1000 | Loss: 0.00003964
Iteration 152/1000 | Loss: 0.00003964
Iteration 153/1000 | Loss: 0.00003964
Iteration 154/1000 | Loss: 0.00003964
Iteration 155/1000 | Loss: 0.00003964
Iteration 156/1000 | Loss: 0.00003964
Iteration 157/1000 | Loss: 0.00003963
Iteration 158/1000 | Loss: 0.00003963
Iteration 159/1000 | Loss: 0.00003963
Iteration 160/1000 | Loss: 0.00003962
Iteration 161/1000 | Loss: 0.00003962
Iteration 162/1000 | Loss: 0.00003962
Iteration 163/1000 | Loss: 0.00003961
Iteration 164/1000 | Loss: 0.00003961
Iteration 165/1000 | Loss: 0.00003961
Iteration 166/1000 | Loss: 0.00003960
Iteration 167/1000 | Loss: 0.00003959
Iteration 168/1000 | Loss: 0.00003959
Iteration 169/1000 | Loss: 0.00003959
Iteration 170/1000 | Loss: 0.00003959
Iteration 171/1000 | Loss: 0.00003959
Iteration 172/1000 | Loss: 0.00003959
Iteration 173/1000 | Loss: 0.00003959
Iteration 174/1000 | Loss: 0.00003959
Iteration 175/1000 | Loss: 0.00003959
Iteration 176/1000 | Loss: 0.00003959
Iteration 177/1000 | Loss: 0.00003958
Iteration 178/1000 | Loss: 0.00003958
Iteration 179/1000 | Loss: 0.00003958
Iteration 180/1000 | Loss: 0.00003958
Iteration 181/1000 | Loss: 0.00003957
Iteration 182/1000 | Loss: 0.00003957
Iteration 183/1000 | Loss: 0.00003956
Iteration 184/1000 | Loss: 0.00003956
Iteration 185/1000 | Loss: 0.00003956
Iteration 186/1000 | Loss: 0.00003955
Iteration 187/1000 | Loss: 0.00003955
Iteration 188/1000 | Loss: 0.00003955
Iteration 189/1000 | Loss: 0.00003954
Iteration 190/1000 | Loss: 0.00003954
Iteration 191/1000 | Loss: 0.00003954
Iteration 192/1000 | Loss: 0.00003954
Iteration 193/1000 | Loss: 0.00003953
Iteration 194/1000 | Loss: 0.00003953
Iteration 195/1000 | Loss: 0.00003953
Iteration 196/1000 | Loss: 0.00003952
Iteration 197/1000 | Loss: 0.00003952
Iteration 198/1000 | Loss: 0.00003951
Iteration 199/1000 | Loss: 0.00003951
Iteration 200/1000 | Loss: 0.00003951
Iteration 201/1000 | Loss: 0.00003951
Iteration 202/1000 | Loss: 0.00003951
Iteration 203/1000 | Loss: 0.00003950
Iteration 204/1000 | Loss: 0.00003950
Iteration 205/1000 | Loss: 0.00003950
Iteration 206/1000 | Loss: 0.00003949
Iteration 207/1000 | Loss: 0.00003948
Iteration 208/1000 | Loss: 0.00003948
Iteration 209/1000 | Loss: 0.00003946
Iteration 210/1000 | Loss: 0.00003946
Iteration 211/1000 | Loss: 0.00003945
Iteration 212/1000 | Loss: 0.00003945
Iteration 213/1000 | Loss: 0.00003945
Iteration 214/1000 | Loss: 0.00003944
Iteration 215/1000 | Loss: 0.00003944
Iteration 216/1000 | Loss: 0.00003944
Iteration 217/1000 | Loss: 0.00003944
Iteration 218/1000 | Loss: 0.00003944
Iteration 219/1000 | Loss: 0.00003944
Iteration 220/1000 | Loss: 0.00003943
Iteration 221/1000 | Loss: 0.00003943
Iteration 222/1000 | Loss: 0.00003943
Iteration 223/1000 | Loss: 0.00003943
Iteration 224/1000 | Loss: 0.00003942
Iteration 225/1000 | Loss: 0.00003942
Iteration 226/1000 | Loss: 0.00003942
Iteration 227/1000 | Loss: 0.00003942
Iteration 228/1000 | Loss: 0.00003942
Iteration 229/1000 | Loss: 0.00003941
Iteration 230/1000 | Loss: 0.00003941
Iteration 231/1000 | Loss: 0.00003941
Iteration 232/1000 | Loss: 0.00003941
Iteration 233/1000 | Loss: 0.00003941
Iteration 234/1000 | Loss: 0.00003941
Iteration 235/1000 | Loss: 0.00003941
Iteration 236/1000 | Loss: 0.00003941
Iteration 237/1000 | Loss: 0.00003941
Iteration 238/1000 | Loss: 0.00003941
Iteration 239/1000 | Loss: 0.00003941
Iteration 240/1000 | Loss: 0.00003940
Iteration 241/1000 | Loss: 0.00003940
Iteration 242/1000 | Loss: 0.00003940
Iteration 243/1000 | Loss: 0.00003940
Iteration 244/1000 | Loss: 0.00003940
Iteration 245/1000 | Loss: 0.00003940
Iteration 246/1000 | Loss: 0.00003940
Iteration 247/1000 | Loss: 0.00003940
Iteration 248/1000 | Loss: 0.00003940
Iteration 249/1000 | Loss: 0.00003940
Iteration 250/1000 | Loss: 0.00003940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [3.94024136767257e-05, 3.94024136767257e-05, 3.94024136767257e-05, 3.94024136767257e-05, 3.94024136767257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.94024136767257e-05

Optimization complete. Final v2v error: 3.8844969272613525 mm

Highest mean error: 11.335920333862305 mm for frame 141

Lowest mean error: 2.8575992584228516 mm for frame 36

Saving results

Total time: 87.00726437568665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809705
Iteration 2/25 | Loss: 0.00133205
Iteration 3/25 | Loss: 0.00118874
Iteration 4/25 | Loss: 0.00117058
Iteration 5/25 | Loss: 0.00116615
Iteration 6/25 | Loss: 0.00116501
Iteration 7/25 | Loss: 0.00116470
Iteration 8/25 | Loss: 0.00116470
Iteration 9/25 | Loss: 0.00116470
Iteration 10/25 | Loss: 0.00116470
Iteration 11/25 | Loss: 0.00116470
Iteration 12/25 | Loss: 0.00116470
Iteration 13/25 | Loss: 0.00116470
Iteration 14/25 | Loss: 0.00116470
Iteration 15/25 | Loss: 0.00116470
Iteration 16/25 | Loss: 0.00116470
Iteration 17/25 | Loss: 0.00116470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011647017672657967, 0.0011647017672657967, 0.0011647017672657967, 0.0011647017672657967, 0.0011647017672657967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011647017672657967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34977019
Iteration 2/25 | Loss: 0.00194246
Iteration 3/25 | Loss: 0.00194245
Iteration 4/25 | Loss: 0.00194245
Iteration 5/25 | Loss: 0.00194245
Iteration 6/25 | Loss: 0.00194245
Iteration 7/25 | Loss: 0.00194245
Iteration 8/25 | Loss: 0.00194245
Iteration 9/25 | Loss: 0.00194245
Iteration 10/25 | Loss: 0.00194245
Iteration 11/25 | Loss: 0.00194245
Iteration 12/25 | Loss: 0.00194245
Iteration 13/25 | Loss: 0.00194245
Iteration 14/25 | Loss: 0.00194245
Iteration 15/25 | Loss: 0.00194245
Iteration 16/25 | Loss: 0.00194245
Iteration 17/25 | Loss: 0.00194245
Iteration 18/25 | Loss: 0.00194245
Iteration 19/25 | Loss: 0.00194245
Iteration 20/25 | Loss: 0.00194245
Iteration 21/25 | Loss: 0.00194245
Iteration 22/25 | Loss: 0.00194245
Iteration 23/25 | Loss: 0.00194245
Iteration 24/25 | Loss: 0.00194245
Iteration 25/25 | Loss: 0.00194245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194245
Iteration 2/1000 | Loss: 0.00038677
Iteration 3/1000 | Loss: 0.00010758
Iteration 4/1000 | Loss: 0.00020354
Iteration 5/1000 | Loss: 0.00021133
Iteration 6/1000 | Loss: 0.00018367
Iteration 7/1000 | Loss: 0.00016256
Iteration 8/1000 | Loss: 0.00017202
Iteration 9/1000 | Loss: 0.00006480
Iteration 10/1000 | Loss: 0.00006177
Iteration 11/1000 | Loss: 0.00005926
Iteration 12/1000 | Loss: 0.00005743
Iteration 13/1000 | Loss: 0.00014912
Iteration 14/1000 | Loss: 0.00013052
Iteration 15/1000 | Loss: 0.00013252
Iteration 16/1000 | Loss: 0.00012652
Iteration 17/1000 | Loss: 0.00006308
Iteration 18/1000 | Loss: 0.00005741
Iteration 19/1000 | Loss: 0.00005544
Iteration 20/1000 | Loss: 0.00012903
Iteration 21/1000 | Loss: 0.00005681
Iteration 22/1000 | Loss: 0.00006091
Iteration 23/1000 | Loss: 0.00011266
Iteration 24/1000 | Loss: 0.00016669
Iteration 25/1000 | Loss: 0.00011650
Iteration 26/1000 | Loss: 0.00009376
Iteration 27/1000 | Loss: 0.00006022
Iteration 28/1000 | Loss: 0.00007078
Iteration 29/1000 | Loss: 0.00006369
Iteration 30/1000 | Loss: 0.00007736
Iteration 31/1000 | Loss: 0.00006599
Iteration 32/1000 | Loss: 0.00005561
Iteration 33/1000 | Loss: 0.00008075
Iteration 34/1000 | Loss: 0.00008459
Iteration 35/1000 | Loss: 0.00007207
Iteration 36/1000 | Loss: 0.00008705
Iteration 37/1000 | Loss: 0.00014814
Iteration 38/1000 | Loss: 0.00012330
Iteration 39/1000 | Loss: 0.00017362
Iteration 40/1000 | Loss: 0.00014697
Iteration 41/1000 | Loss: 0.00017310
Iteration 42/1000 | Loss: 0.00006001
Iteration 43/1000 | Loss: 0.00005677
Iteration 44/1000 | Loss: 0.00015895
Iteration 45/1000 | Loss: 0.00010427
Iteration 46/1000 | Loss: 0.00019313
Iteration 47/1000 | Loss: 0.00011303
Iteration 48/1000 | Loss: 0.00016400
Iteration 49/1000 | Loss: 0.00016066
Iteration 50/1000 | Loss: 0.00020161
Iteration 51/1000 | Loss: 0.00012249
Iteration 52/1000 | Loss: 0.00008404
Iteration 53/1000 | Loss: 0.00010905
Iteration 54/1000 | Loss: 0.00015258
Iteration 55/1000 | Loss: 0.00017963
Iteration 56/1000 | Loss: 0.00015050
Iteration 57/1000 | Loss: 0.00013321
Iteration 58/1000 | Loss: 0.00015508
Iteration 59/1000 | Loss: 0.00022599
Iteration 60/1000 | Loss: 0.00019778
Iteration 61/1000 | Loss: 0.00014705
Iteration 62/1000 | Loss: 0.00018592
Iteration 63/1000 | Loss: 0.00005549
Iteration 64/1000 | Loss: 0.00005415
Iteration 65/1000 | Loss: 0.00005352
Iteration 66/1000 | Loss: 0.00005276
Iteration 67/1000 | Loss: 0.00005223
Iteration 68/1000 | Loss: 0.00005169
Iteration 69/1000 | Loss: 0.00005116
Iteration 70/1000 | Loss: 0.00005102
Iteration 71/1000 | Loss: 0.00005080
Iteration 72/1000 | Loss: 0.00005047
Iteration 73/1000 | Loss: 0.00005037
Iteration 74/1000 | Loss: 0.00005006
Iteration 75/1000 | Loss: 0.00004977
Iteration 76/1000 | Loss: 0.00004940
Iteration 77/1000 | Loss: 0.00004904
Iteration 78/1000 | Loss: 0.00004863
Iteration 79/1000 | Loss: 0.00004837
Iteration 80/1000 | Loss: 0.00004807
Iteration 81/1000 | Loss: 0.00004770
Iteration 82/1000 | Loss: 0.00004744
Iteration 83/1000 | Loss: 0.00004720
Iteration 84/1000 | Loss: 0.00004694
Iteration 85/1000 | Loss: 0.00004665
Iteration 86/1000 | Loss: 0.00004655
Iteration 87/1000 | Loss: 0.00004642
Iteration 88/1000 | Loss: 0.00004640
Iteration 89/1000 | Loss: 0.00004639
Iteration 90/1000 | Loss: 0.00004624
Iteration 91/1000 | Loss: 0.00004616
Iteration 92/1000 | Loss: 0.00004611
Iteration 93/1000 | Loss: 0.00004595
Iteration 94/1000 | Loss: 0.00004576
Iteration 95/1000 | Loss: 0.00004571
Iteration 96/1000 | Loss: 0.00004568
Iteration 97/1000 | Loss: 0.00004565
Iteration 98/1000 | Loss: 0.00004564
Iteration 99/1000 | Loss: 0.00004563
Iteration 100/1000 | Loss: 0.00004563
Iteration 101/1000 | Loss: 0.00004555
Iteration 102/1000 | Loss: 0.00004553
Iteration 103/1000 | Loss: 0.00004553
Iteration 104/1000 | Loss: 0.00004552
Iteration 105/1000 | Loss: 0.00004552
Iteration 106/1000 | Loss: 0.00004552
Iteration 107/1000 | Loss: 0.00004551
Iteration 108/1000 | Loss: 0.00004551
Iteration 109/1000 | Loss: 0.00004551
Iteration 110/1000 | Loss: 0.00004550
Iteration 111/1000 | Loss: 0.00004550
Iteration 112/1000 | Loss: 0.00004549
Iteration 113/1000 | Loss: 0.00004548
Iteration 114/1000 | Loss: 0.00004548
Iteration 115/1000 | Loss: 0.00004548
Iteration 116/1000 | Loss: 0.00004547
Iteration 117/1000 | Loss: 0.00004547
Iteration 118/1000 | Loss: 0.00004546
Iteration 119/1000 | Loss: 0.00004545
Iteration 120/1000 | Loss: 0.00004545
Iteration 121/1000 | Loss: 0.00004544
Iteration 122/1000 | Loss: 0.00004544
Iteration 123/1000 | Loss: 0.00004543
Iteration 124/1000 | Loss: 0.00004539
Iteration 125/1000 | Loss: 0.00004539
Iteration 126/1000 | Loss: 0.00004538
Iteration 127/1000 | Loss: 0.00004538
Iteration 128/1000 | Loss: 0.00004537
Iteration 129/1000 | Loss: 0.00004537
Iteration 130/1000 | Loss: 0.00004536
Iteration 131/1000 | Loss: 0.00004535
Iteration 132/1000 | Loss: 0.00004535
Iteration 133/1000 | Loss: 0.00004535
Iteration 134/1000 | Loss: 0.00004534
Iteration 135/1000 | Loss: 0.00004534
Iteration 136/1000 | Loss: 0.00004534
Iteration 137/1000 | Loss: 0.00004534
Iteration 138/1000 | Loss: 0.00004534
Iteration 139/1000 | Loss: 0.00004534
Iteration 140/1000 | Loss: 0.00004534
Iteration 141/1000 | Loss: 0.00004533
Iteration 142/1000 | Loss: 0.00004533
Iteration 143/1000 | Loss: 0.00004532
Iteration 144/1000 | Loss: 0.00004531
Iteration 145/1000 | Loss: 0.00004531
Iteration 146/1000 | Loss: 0.00004531
Iteration 147/1000 | Loss: 0.00004529
Iteration 148/1000 | Loss: 0.00004529
Iteration 149/1000 | Loss: 0.00004529
Iteration 150/1000 | Loss: 0.00004529
Iteration 151/1000 | Loss: 0.00004528
Iteration 152/1000 | Loss: 0.00004528
Iteration 153/1000 | Loss: 0.00004526
Iteration 154/1000 | Loss: 0.00004525
Iteration 155/1000 | Loss: 0.00004524
Iteration 156/1000 | Loss: 0.00004524
Iteration 157/1000 | Loss: 0.00004524
Iteration 158/1000 | Loss: 0.00004524
Iteration 159/1000 | Loss: 0.00004524
Iteration 160/1000 | Loss: 0.00004524
Iteration 161/1000 | Loss: 0.00004524
Iteration 162/1000 | Loss: 0.00004524
Iteration 163/1000 | Loss: 0.00004524
Iteration 164/1000 | Loss: 0.00004523
Iteration 165/1000 | Loss: 0.00004523
Iteration 166/1000 | Loss: 0.00004523
Iteration 167/1000 | Loss: 0.00004523
Iteration 168/1000 | Loss: 0.00004520
Iteration 169/1000 | Loss: 0.00004520
Iteration 170/1000 | Loss: 0.00004517
Iteration 171/1000 | Loss: 0.00004517
Iteration 172/1000 | Loss: 0.00004516
Iteration 173/1000 | Loss: 0.00004516
Iteration 174/1000 | Loss: 0.00004515
Iteration 175/1000 | Loss: 0.00004515
Iteration 176/1000 | Loss: 0.00004515
Iteration 177/1000 | Loss: 0.00004514
Iteration 178/1000 | Loss: 0.00004513
Iteration 179/1000 | Loss: 0.00004513
Iteration 180/1000 | Loss: 0.00004513
Iteration 181/1000 | Loss: 0.00004512
Iteration 182/1000 | Loss: 0.00004512
Iteration 183/1000 | Loss: 0.00004511
Iteration 184/1000 | Loss: 0.00004511
Iteration 185/1000 | Loss: 0.00004510
Iteration 186/1000 | Loss: 0.00004509
Iteration 187/1000 | Loss: 0.00004508
Iteration 188/1000 | Loss: 0.00004508
Iteration 189/1000 | Loss: 0.00004508
Iteration 190/1000 | Loss: 0.00004508
Iteration 191/1000 | Loss: 0.00004507
Iteration 192/1000 | Loss: 0.00004506
Iteration 193/1000 | Loss: 0.00004506
Iteration 194/1000 | Loss: 0.00004506
Iteration 195/1000 | Loss: 0.00004506
Iteration 196/1000 | Loss: 0.00004506
Iteration 197/1000 | Loss: 0.00004505
Iteration 198/1000 | Loss: 0.00004505
Iteration 199/1000 | Loss: 0.00004504
Iteration 200/1000 | Loss: 0.00004504
Iteration 201/1000 | Loss: 0.00004503
Iteration 202/1000 | Loss: 0.00004503
Iteration 203/1000 | Loss: 0.00004503
Iteration 204/1000 | Loss: 0.00004503
Iteration 205/1000 | Loss: 0.00004503
Iteration 206/1000 | Loss: 0.00004503
Iteration 207/1000 | Loss: 0.00004503
Iteration 208/1000 | Loss: 0.00004502
Iteration 209/1000 | Loss: 0.00004502
Iteration 210/1000 | Loss: 0.00004502
Iteration 211/1000 | Loss: 0.00004502
Iteration 212/1000 | Loss: 0.00004502
Iteration 213/1000 | Loss: 0.00004502
Iteration 214/1000 | Loss: 0.00004502
Iteration 215/1000 | Loss: 0.00004502
Iteration 216/1000 | Loss: 0.00004502
Iteration 217/1000 | Loss: 0.00004502
Iteration 218/1000 | Loss: 0.00004502
Iteration 219/1000 | Loss: 0.00004501
Iteration 220/1000 | Loss: 0.00004501
Iteration 221/1000 | Loss: 0.00004501
Iteration 222/1000 | Loss: 0.00004501
Iteration 223/1000 | Loss: 0.00004501
Iteration 224/1000 | Loss: 0.00004500
Iteration 225/1000 | Loss: 0.00004500
Iteration 226/1000 | Loss: 0.00004500
Iteration 227/1000 | Loss: 0.00004499
Iteration 228/1000 | Loss: 0.00004499
Iteration 229/1000 | Loss: 0.00004499
Iteration 230/1000 | Loss: 0.00004499
Iteration 231/1000 | Loss: 0.00004499
Iteration 232/1000 | Loss: 0.00004499
Iteration 233/1000 | Loss: 0.00004498
Iteration 234/1000 | Loss: 0.00004498
Iteration 235/1000 | Loss: 0.00004498
Iteration 236/1000 | Loss: 0.00004498
Iteration 237/1000 | Loss: 0.00004496
Iteration 238/1000 | Loss: 0.00004496
Iteration 239/1000 | Loss: 0.00004496
Iteration 240/1000 | Loss: 0.00004495
Iteration 241/1000 | Loss: 0.00004495
Iteration 242/1000 | Loss: 0.00004495
Iteration 243/1000 | Loss: 0.00004494
Iteration 244/1000 | Loss: 0.00004494
Iteration 245/1000 | Loss: 0.00004493
Iteration 246/1000 | Loss: 0.00004492
Iteration 247/1000 | Loss: 0.00004491
Iteration 248/1000 | Loss: 0.00004491
Iteration 249/1000 | Loss: 0.00004490
Iteration 250/1000 | Loss: 0.00004490
Iteration 251/1000 | Loss: 0.00004489
Iteration 252/1000 | Loss: 0.00004489
Iteration 253/1000 | Loss: 0.00004489
Iteration 254/1000 | Loss: 0.00004488
Iteration 255/1000 | Loss: 0.00004488
Iteration 256/1000 | Loss: 0.00004487
Iteration 257/1000 | Loss: 0.00004487
Iteration 258/1000 | Loss: 0.00004487
Iteration 259/1000 | Loss: 0.00004486
Iteration 260/1000 | Loss: 0.00004486
Iteration 261/1000 | Loss: 0.00004486
Iteration 262/1000 | Loss: 0.00004486
Iteration 263/1000 | Loss: 0.00004486
Iteration 264/1000 | Loss: 0.00004486
Iteration 265/1000 | Loss: 0.00004486
Iteration 266/1000 | Loss: 0.00004486
Iteration 267/1000 | Loss: 0.00004486
Iteration 268/1000 | Loss: 0.00004485
Iteration 269/1000 | Loss: 0.00004485
Iteration 270/1000 | Loss: 0.00004484
Iteration 271/1000 | Loss: 0.00004484
Iteration 272/1000 | Loss: 0.00004483
Iteration 273/1000 | Loss: 0.00004483
Iteration 274/1000 | Loss: 0.00004483
Iteration 275/1000 | Loss: 0.00004482
Iteration 276/1000 | Loss: 0.00004482
Iteration 277/1000 | Loss: 0.00004482
Iteration 278/1000 | Loss: 0.00004482
Iteration 279/1000 | Loss: 0.00004482
Iteration 280/1000 | Loss: 0.00004482
Iteration 281/1000 | Loss: 0.00004481
Iteration 282/1000 | Loss: 0.00004481
Iteration 283/1000 | Loss: 0.00004481
Iteration 284/1000 | Loss: 0.00004480
Iteration 285/1000 | Loss: 0.00004480
Iteration 286/1000 | Loss: 0.00004480
Iteration 287/1000 | Loss: 0.00004480
Iteration 288/1000 | Loss: 0.00004480
Iteration 289/1000 | Loss: 0.00004480
Iteration 290/1000 | Loss: 0.00004479
Iteration 291/1000 | Loss: 0.00004479
Iteration 292/1000 | Loss: 0.00004479
Iteration 293/1000 | Loss: 0.00004479
Iteration 294/1000 | Loss: 0.00004479
Iteration 295/1000 | Loss: 0.00004479
Iteration 296/1000 | Loss: 0.00004479
Iteration 297/1000 | Loss: 0.00004478
Iteration 298/1000 | Loss: 0.00004478
Iteration 299/1000 | Loss: 0.00004478
Iteration 300/1000 | Loss: 0.00004478
Iteration 301/1000 | Loss: 0.00004477
Iteration 302/1000 | Loss: 0.00004477
Iteration 303/1000 | Loss: 0.00004477
Iteration 304/1000 | Loss: 0.00004476
Iteration 305/1000 | Loss: 0.00004476
Iteration 306/1000 | Loss: 0.00004476
Iteration 307/1000 | Loss: 0.00004476
Iteration 308/1000 | Loss: 0.00004476
Iteration 309/1000 | Loss: 0.00004476
Iteration 310/1000 | Loss: 0.00004476
Iteration 311/1000 | Loss: 0.00004476
Iteration 312/1000 | Loss: 0.00004475
Iteration 313/1000 | Loss: 0.00004475
Iteration 314/1000 | Loss: 0.00004475
Iteration 315/1000 | Loss: 0.00004475
Iteration 316/1000 | Loss: 0.00004475
Iteration 317/1000 | Loss: 0.00004475
Iteration 318/1000 | Loss: 0.00004475
Iteration 319/1000 | Loss: 0.00004474
Iteration 320/1000 | Loss: 0.00004474
Iteration 321/1000 | Loss: 0.00004474
Iteration 322/1000 | Loss: 0.00004474
Iteration 323/1000 | Loss: 0.00004473
Iteration 324/1000 | Loss: 0.00004473
Iteration 325/1000 | Loss: 0.00004473
Iteration 326/1000 | Loss: 0.00004473
Iteration 327/1000 | Loss: 0.00004473
Iteration 328/1000 | Loss: 0.00004473
Iteration 329/1000 | Loss: 0.00004472
Iteration 330/1000 | Loss: 0.00004472
Iteration 331/1000 | Loss: 0.00004472
Iteration 332/1000 | Loss: 0.00004471
Iteration 333/1000 | Loss: 0.00004471
Iteration 334/1000 | Loss: 0.00004471
Iteration 335/1000 | Loss: 0.00004470
Iteration 336/1000 | Loss: 0.00004469
Iteration 337/1000 | Loss: 0.00004469
Iteration 338/1000 | Loss: 0.00004468
Iteration 339/1000 | Loss: 0.00004468
Iteration 340/1000 | Loss: 0.00004467
Iteration 341/1000 | Loss: 0.00004467
Iteration 342/1000 | Loss: 0.00004466
Iteration 343/1000 | Loss: 0.00004466
Iteration 344/1000 | Loss: 0.00004466
Iteration 345/1000 | Loss: 0.00004465
Iteration 346/1000 | Loss: 0.00004465
Iteration 347/1000 | Loss: 0.00004465
Iteration 348/1000 | Loss: 0.00004464
Iteration 349/1000 | Loss: 0.00004464
Iteration 350/1000 | Loss: 0.00004464
Iteration 351/1000 | Loss: 0.00004464
Iteration 352/1000 | Loss: 0.00004463
Iteration 353/1000 | Loss: 0.00004463
Iteration 354/1000 | Loss: 0.00004463
Iteration 355/1000 | Loss: 0.00004463
Iteration 356/1000 | Loss: 0.00004462
Iteration 357/1000 | Loss: 0.00004462
Iteration 358/1000 | Loss: 0.00004462
Iteration 359/1000 | Loss: 0.00004462
Iteration 360/1000 | Loss: 0.00004462
Iteration 361/1000 | Loss: 0.00004462
Iteration 362/1000 | Loss: 0.00004462
Iteration 363/1000 | Loss: 0.00004462
Iteration 364/1000 | Loss: 0.00004461
Iteration 365/1000 | Loss: 0.00004461
Iteration 366/1000 | Loss: 0.00004461
Iteration 367/1000 | Loss: 0.00004461
Iteration 368/1000 | Loss: 0.00004461
Iteration 369/1000 | Loss: 0.00004461
Iteration 370/1000 | Loss: 0.00004461
Iteration 371/1000 | Loss: 0.00004461
Iteration 372/1000 | Loss: 0.00004461
Iteration 373/1000 | Loss: 0.00004460
Iteration 374/1000 | Loss: 0.00004460
Iteration 375/1000 | Loss: 0.00004460
Iteration 376/1000 | Loss: 0.00004460
Iteration 377/1000 | Loss: 0.00004460
Iteration 378/1000 | Loss: 0.00004460
Iteration 379/1000 | Loss: 0.00004460
Iteration 380/1000 | Loss: 0.00004460
Iteration 381/1000 | Loss: 0.00004459
Iteration 382/1000 | Loss: 0.00004459
Iteration 383/1000 | Loss: 0.00004459
Iteration 384/1000 | Loss: 0.00004459
Iteration 385/1000 | Loss: 0.00004459
Iteration 386/1000 | Loss: 0.00004459
Iteration 387/1000 | Loss: 0.00004459
Iteration 388/1000 | Loss: 0.00004459
Iteration 389/1000 | Loss: 0.00004459
Iteration 390/1000 | Loss: 0.00004459
Iteration 391/1000 | Loss: 0.00004459
Iteration 392/1000 | Loss: 0.00004459
Iteration 393/1000 | Loss: 0.00004458
Iteration 394/1000 | Loss: 0.00004458
Iteration 395/1000 | Loss: 0.00004458
Iteration 396/1000 | Loss: 0.00004458
Iteration 397/1000 | Loss: 0.00004458
Iteration 398/1000 | Loss: 0.00004458
Iteration 399/1000 | Loss: 0.00004458
Iteration 400/1000 | Loss: 0.00004458
Iteration 401/1000 | Loss: 0.00004458
Iteration 402/1000 | Loss: 0.00004458
Iteration 403/1000 | Loss: 0.00004457
Iteration 404/1000 | Loss: 0.00004457
Iteration 405/1000 | Loss: 0.00004457
Iteration 406/1000 | Loss: 0.00004457
Iteration 407/1000 | Loss: 0.00004457
Iteration 408/1000 | Loss: 0.00004457
Iteration 409/1000 | Loss: 0.00004456
Iteration 410/1000 | Loss: 0.00004456
Iteration 411/1000 | Loss: 0.00004456
Iteration 412/1000 | Loss: 0.00004456
Iteration 413/1000 | Loss: 0.00004456
Iteration 414/1000 | Loss: 0.00004456
Iteration 415/1000 | Loss: 0.00004456
Iteration 416/1000 | Loss: 0.00004456
Iteration 417/1000 | Loss: 0.00004456
Iteration 418/1000 | Loss: 0.00004455
Iteration 419/1000 | Loss: 0.00004455
Iteration 420/1000 | Loss: 0.00004455
Iteration 421/1000 | Loss: 0.00004455
Iteration 422/1000 | Loss: 0.00004455
Iteration 423/1000 | Loss: 0.00004455
Iteration 424/1000 | Loss: 0.00004455
Iteration 425/1000 | Loss: 0.00004455
Iteration 426/1000 | Loss: 0.00004455
Iteration 427/1000 | Loss: 0.00004455
Iteration 428/1000 | Loss: 0.00004455
Iteration 429/1000 | Loss: 0.00004455
Iteration 430/1000 | Loss: 0.00004455
Iteration 431/1000 | Loss: 0.00004455
Iteration 432/1000 | Loss: 0.00004454
Iteration 433/1000 | Loss: 0.00004454
Iteration 434/1000 | Loss: 0.00004454
Iteration 435/1000 | Loss: 0.00004454
Iteration 436/1000 | Loss: 0.00004454
Iteration 437/1000 | Loss: 0.00004454
Iteration 438/1000 | Loss: 0.00004454
Iteration 439/1000 | Loss: 0.00004454
Iteration 440/1000 | Loss: 0.00004454
Iteration 441/1000 | Loss: 0.00004454
Iteration 442/1000 | Loss: 0.00004454
Iteration 443/1000 | Loss: 0.00004454
Iteration 444/1000 | Loss: 0.00004454
Iteration 445/1000 | Loss: 0.00004453
Iteration 446/1000 | Loss: 0.00004453
Iteration 447/1000 | Loss: 0.00004453
Iteration 448/1000 | Loss: 0.00004453
Iteration 449/1000 | Loss: 0.00004453
Iteration 450/1000 | Loss: 0.00004453
Iteration 451/1000 | Loss: 0.00004453
Iteration 452/1000 | Loss: 0.00004453
Iteration 453/1000 | Loss: 0.00004453
Iteration 454/1000 | Loss: 0.00004453
Iteration 455/1000 | Loss: 0.00004453
Iteration 456/1000 | Loss: 0.00004453
Iteration 457/1000 | Loss: 0.00004453
Iteration 458/1000 | Loss: 0.00004453
Iteration 459/1000 | Loss: 0.00004452
Iteration 460/1000 | Loss: 0.00004452
Iteration 461/1000 | Loss: 0.00004452
Iteration 462/1000 | Loss: 0.00004452
Iteration 463/1000 | Loss: 0.00004452
Iteration 464/1000 | Loss: 0.00004452
Iteration 465/1000 | Loss: 0.00004452
Iteration 466/1000 | Loss: 0.00004452
Iteration 467/1000 | Loss: 0.00004452
Iteration 468/1000 | Loss: 0.00004452
Iteration 469/1000 | Loss: 0.00004452
Iteration 470/1000 | Loss: 0.00004452
Iteration 471/1000 | Loss: 0.00004452
Iteration 472/1000 | Loss: 0.00004452
Iteration 473/1000 | Loss: 0.00004452
Iteration 474/1000 | Loss: 0.00004452
Iteration 475/1000 | Loss: 0.00004452
Iteration 476/1000 | Loss: 0.00004452
Iteration 477/1000 | Loss: 0.00004452
Iteration 478/1000 | Loss: 0.00004452
Iteration 479/1000 | Loss: 0.00004452
Iteration 480/1000 | Loss: 0.00004452
Iteration 481/1000 | Loss: 0.00004452
Iteration 482/1000 | Loss: 0.00004452
Iteration 483/1000 | Loss: 0.00004452
Iteration 484/1000 | Loss: 0.00004452
Iteration 485/1000 | Loss: 0.00004452
Iteration 486/1000 | Loss: 0.00004452
Iteration 487/1000 | Loss: 0.00004452
Iteration 488/1000 | Loss: 0.00004452
Iteration 489/1000 | Loss: 0.00004452
Iteration 490/1000 | Loss: 0.00004452
Iteration 491/1000 | Loss: 0.00004452
Iteration 492/1000 | Loss: 0.00004452
Iteration 493/1000 | Loss: 0.00004452
Iteration 494/1000 | Loss: 0.00004452
Iteration 495/1000 | Loss: 0.00004452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 495. Stopping optimization.
Last 5 losses: [4.451678614714183e-05, 4.451678614714183e-05, 4.451678614714183e-05, 4.451678614714183e-05, 4.451678614714183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.451678614714183e-05

Optimization complete. Final v2v error: 3.2845561504364014 mm

Highest mean error: 11.067914009094238 mm for frame 86

Lowest mean error: 2.219162940979004 mm for frame 31

Saving results

Total time: 184.7319037914276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785498
Iteration 2/25 | Loss: 0.00138181
Iteration 3/25 | Loss: 0.00115194
Iteration 4/25 | Loss: 0.00113312
Iteration 5/25 | Loss: 0.00112969
Iteration 6/25 | Loss: 0.00112849
Iteration 7/25 | Loss: 0.00112813
Iteration 8/25 | Loss: 0.00112813
Iteration 9/25 | Loss: 0.00112813
Iteration 10/25 | Loss: 0.00112813
Iteration 11/25 | Loss: 0.00112813
Iteration 12/25 | Loss: 0.00112813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011281331535428762, 0.0011281331535428762, 0.0011281331535428762, 0.0011281331535428762, 0.0011281331535428762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011281331535428762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58961749
Iteration 2/25 | Loss: 0.00091903
Iteration 3/25 | Loss: 0.00091903
Iteration 4/25 | Loss: 0.00091903
Iteration 5/25 | Loss: 0.00091903
Iteration 6/25 | Loss: 0.00091903
Iteration 7/25 | Loss: 0.00091902
Iteration 8/25 | Loss: 0.00091902
Iteration 9/25 | Loss: 0.00091902
Iteration 10/25 | Loss: 0.00091902
Iteration 11/25 | Loss: 0.00091902
Iteration 12/25 | Loss: 0.00091902
Iteration 13/25 | Loss: 0.00091902
Iteration 14/25 | Loss: 0.00091902
Iteration 15/25 | Loss: 0.00091902
Iteration 16/25 | Loss: 0.00091902
Iteration 17/25 | Loss: 0.00091902
Iteration 18/25 | Loss: 0.00091902
Iteration 19/25 | Loss: 0.00091902
Iteration 20/25 | Loss: 0.00091902
Iteration 21/25 | Loss: 0.00091902
Iteration 22/25 | Loss: 0.00091902
Iteration 23/25 | Loss: 0.00091902
Iteration 24/25 | Loss: 0.00091902
Iteration 25/25 | Loss: 0.00091902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091902
Iteration 2/1000 | Loss: 0.00002468
Iteration 3/1000 | Loss: 0.00001907
Iteration 4/1000 | Loss: 0.00015471
Iteration 5/1000 | Loss: 0.00012175
Iteration 6/1000 | Loss: 0.00011240
Iteration 7/1000 | Loss: 0.00002811
Iteration 8/1000 | Loss: 0.00007830
Iteration 9/1000 | Loss: 0.00010525
Iteration 10/1000 | Loss: 0.00008482
Iteration 11/1000 | Loss: 0.00012689
Iteration 12/1000 | Loss: 0.00002718
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001565
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001489
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001393
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001386
Iteration 40/1000 | Loss: 0.00001385
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001383
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001382
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001381
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001364
Iteration 81/1000 | Loss: 0.00001364
Iteration 82/1000 | Loss: 0.00001364
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001363
Iteration 86/1000 | Loss: 0.00001363
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001358
Iteration 120/1000 | Loss: 0.00001358
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001357
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001357
Iteration 129/1000 | Loss: 0.00001357
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001357
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3568480426329188e-05, 1.3568480426329188e-05, 1.3568480426329188e-05, 1.3568480426329188e-05, 1.3568480426329188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3568480426329188e-05

Optimization complete. Final v2v error: 3.048499822616577 mm

Highest mean error: 4.958840370178223 mm for frame 56

Lowest mean error: 2.6971166133880615 mm for frame 160

Saving results

Total time: 60.05960464477539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799313
Iteration 2/25 | Loss: 0.00119235
Iteration 3/25 | Loss: 0.00107853
Iteration 4/25 | Loss: 0.00106692
Iteration 5/25 | Loss: 0.00106460
Iteration 6/25 | Loss: 0.00106460
Iteration 7/25 | Loss: 0.00106460
Iteration 8/25 | Loss: 0.00106460
Iteration 9/25 | Loss: 0.00106460
Iteration 10/25 | Loss: 0.00106460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010645983275026083, 0.0010645983275026083, 0.0010645983275026083, 0.0010645983275026083, 0.0010645983275026083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010645983275026083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35434890
Iteration 2/25 | Loss: 0.00081265
Iteration 3/25 | Loss: 0.00081265
Iteration 4/25 | Loss: 0.00081265
Iteration 5/25 | Loss: 0.00081264
Iteration 6/25 | Loss: 0.00081264
Iteration 7/25 | Loss: 0.00081264
Iteration 8/25 | Loss: 0.00081264
Iteration 9/25 | Loss: 0.00081264
Iteration 10/25 | Loss: 0.00081264
Iteration 11/25 | Loss: 0.00081264
Iteration 12/25 | Loss: 0.00081264
Iteration 13/25 | Loss: 0.00081264
Iteration 14/25 | Loss: 0.00081264
Iteration 15/25 | Loss: 0.00081264
Iteration 16/25 | Loss: 0.00081264
Iteration 17/25 | Loss: 0.00081264
Iteration 18/25 | Loss: 0.00081264
Iteration 19/25 | Loss: 0.00081264
Iteration 20/25 | Loss: 0.00081264
Iteration 21/25 | Loss: 0.00081264
Iteration 22/25 | Loss: 0.00081264
Iteration 23/25 | Loss: 0.00081264
Iteration 24/25 | Loss: 0.00081264
Iteration 25/25 | Loss: 0.00081264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081264
Iteration 2/1000 | Loss: 0.00002082
Iteration 3/1000 | Loss: 0.00001276
Iteration 4/1000 | Loss: 0.00001125
Iteration 5/1000 | Loss: 0.00001053
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000961
Iteration 8/1000 | Loss: 0.00000954
Iteration 9/1000 | Loss: 0.00000940
Iteration 10/1000 | Loss: 0.00000937
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000901
Iteration 13/1000 | Loss: 0.00000896
Iteration 14/1000 | Loss: 0.00000895
Iteration 15/1000 | Loss: 0.00000890
Iteration 16/1000 | Loss: 0.00000883
Iteration 17/1000 | Loss: 0.00000881
Iteration 18/1000 | Loss: 0.00000881
Iteration 19/1000 | Loss: 0.00000880
Iteration 20/1000 | Loss: 0.00000879
Iteration 21/1000 | Loss: 0.00000878
Iteration 22/1000 | Loss: 0.00000878
Iteration 23/1000 | Loss: 0.00000877
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000876
Iteration 27/1000 | Loss: 0.00000876
Iteration 28/1000 | Loss: 0.00000876
Iteration 29/1000 | Loss: 0.00000874
Iteration 30/1000 | Loss: 0.00000873
Iteration 31/1000 | Loss: 0.00000872
Iteration 32/1000 | Loss: 0.00000872
Iteration 33/1000 | Loss: 0.00000871
Iteration 34/1000 | Loss: 0.00000870
Iteration 35/1000 | Loss: 0.00000868
Iteration 36/1000 | Loss: 0.00000868
Iteration 37/1000 | Loss: 0.00000867
Iteration 38/1000 | Loss: 0.00000867
Iteration 39/1000 | Loss: 0.00000866
Iteration 40/1000 | Loss: 0.00000866
Iteration 41/1000 | Loss: 0.00000865
Iteration 42/1000 | Loss: 0.00000864
Iteration 43/1000 | Loss: 0.00000864
Iteration 44/1000 | Loss: 0.00000863
Iteration 45/1000 | Loss: 0.00000863
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000862
Iteration 48/1000 | Loss: 0.00000862
Iteration 49/1000 | Loss: 0.00000862
Iteration 50/1000 | Loss: 0.00000861
Iteration 51/1000 | Loss: 0.00000861
Iteration 52/1000 | Loss: 0.00000861
Iteration 53/1000 | Loss: 0.00000861
Iteration 54/1000 | Loss: 0.00000861
Iteration 55/1000 | Loss: 0.00000860
Iteration 56/1000 | Loss: 0.00000860
Iteration 57/1000 | Loss: 0.00000860
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000860
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000858
Iteration 66/1000 | Loss: 0.00000858
Iteration 67/1000 | Loss: 0.00000858
Iteration 68/1000 | Loss: 0.00000858
Iteration 69/1000 | Loss: 0.00000857
Iteration 70/1000 | Loss: 0.00000857
Iteration 71/1000 | Loss: 0.00000856
Iteration 72/1000 | Loss: 0.00000855
Iteration 73/1000 | Loss: 0.00000854
Iteration 74/1000 | Loss: 0.00000854
Iteration 75/1000 | Loss: 0.00000853
Iteration 76/1000 | Loss: 0.00000853
Iteration 77/1000 | Loss: 0.00000852
Iteration 78/1000 | Loss: 0.00000851
Iteration 79/1000 | Loss: 0.00000850
Iteration 80/1000 | Loss: 0.00000850
Iteration 81/1000 | Loss: 0.00000850
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000849
Iteration 84/1000 | Loss: 0.00000849
Iteration 85/1000 | Loss: 0.00000848
Iteration 86/1000 | Loss: 0.00000848
Iteration 87/1000 | Loss: 0.00000848
Iteration 88/1000 | Loss: 0.00000848
Iteration 89/1000 | Loss: 0.00000848
Iteration 90/1000 | Loss: 0.00000848
Iteration 91/1000 | Loss: 0.00000848
Iteration 92/1000 | Loss: 0.00000847
Iteration 93/1000 | Loss: 0.00000847
Iteration 94/1000 | Loss: 0.00000847
Iteration 95/1000 | Loss: 0.00000847
Iteration 96/1000 | Loss: 0.00000847
Iteration 97/1000 | Loss: 0.00000846
Iteration 98/1000 | Loss: 0.00000846
Iteration 99/1000 | Loss: 0.00000845
Iteration 100/1000 | Loss: 0.00000845
Iteration 101/1000 | Loss: 0.00000845
Iteration 102/1000 | Loss: 0.00000845
Iteration 103/1000 | Loss: 0.00000844
Iteration 104/1000 | Loss: 0.00000844
Iteration 105/1000 | Loss: 0.00000844
Iteration 106/1000 | Loss: 0.00000844
Iteration 107/1000 | Loss: 0.00000844
Iteration 108/1000 | Loss: 0.00000844
Iteration 109/1000 | Loss: 0.00000843
Iteration 110/1000 | Loss: 0.00000843
Iteration 111/1000 | Loss: 0.00000843
Iteration 112/1000 | Loss: 0.00000843
Iteration 113/1000 | Loss: 0.00000842
Iteration 114/1000 | Loss: 0.00000842
Iteration 115/1000 | Loss: 0.00000842
Iteration 116/1000 | Loss: 0.00000841
Iteration 117/1000 | Loss: 0.00000841
Iteration 118/1000 | Loss: 0.00000841
Iteration 119/1000 | Loss: 0.00000841
Iteration 120/1000 | Loss: 0.00000841
Iteration 121/1000 | Loss: 0.00000841
Iteration 122/1000 | Loss: 0.00000840
Iteration 123/1000 | Loss: 0.00000840
Iteration 124/1000 | Loss: 0.00000840
Iteration 125/1000 | Loss: 0.00000840
Iteration 126/1000 | Loss: 0.00000840
Iteration 127/1000 | Loss: 0.00000840
Iteration 128/1000 | Loss: 0.00000840
Iteration 129/1000 | Loss: 0.00000840
Iteration 130/1000 | Loss: 0.00000840
Iteration 131/1000 | Loss: 0.00000840
Iteration 132/1000 | Loss: 0.00000840
Iteration 133/1000 | Loss: 0.00000840
Iteration 134/1000 | Loss: 0.00000840
Iteration 135/1000 | Loss: 0.00000840
Iteration 136/1000 | Loss: 0.00000840
Iteration 137/1000 | Loss: 0.00000840
Iteration 138/1000 | Loss: 0.00000840
Iteration 139/1000 | Loss: 0.00000840
Iteration 140/1000 | Loss: 0.00000840
Iteration 141/1000 | Loss: 0.00000840
Iteration 142/1000 | Loss: 0.00000840
Iteration 143/1000 | Loss: 0.00000840
Iteration 144/1000 | Loss: 0.00000840
Iteration 145/1000 | Loss: 0.00000840
Iteration 146/1000 | Loss: 0.00000840
Iteration 147/1000 | Loss: 0.00000840
Iteration 148/1000 | Loss: 0.00000840
Iteration 149/1000 | Loss: 0.00000840
Iteration 150/1000 | Loss: 0.00000840
Iteration 151/1000 | Loss: 0.00000840
Iteration 152/1000 | Loss: 0.00000840
Iteration 153/1000 | Loss: 0.00000840
Iteration 154/1000 | Loss: 0.00000840
Iteration 155/1000 | Loss: 0.00000840
Iteration 156/1000 | Loss: 0.00000840
Iteration 157/1000 | Loss: 0.00000840
Iteration 158/1000 | Loss: 0.00000840
Iteration 159/1000 | Loss: 0.00000840
Iteration 160/1000 | Loss: 0.00000840
Iteration 161/1000 | Loss: 0.00000840
Iteration 162/1000 | Loss: 0.00000840
Iteration 163/1000 | Loss: 0.00000840
Iteration 164/1000 | Loss: 0.00000840
Iteration 165/1000 | Loss: 0.00000840
Iteration 166/1000 | Loss: 0.00000840
Iteration 167/1000 | Loss: 0.00000840
Iteration 168/1000 | Loss: 0.00000840
Iteration 169/1000 | Loss: 0.00000840
Iteration 170/1000 | Loss: 0.00000840
Iteration 171/1000 | Loss: 0.00000840
Iteration 172/1000 | Loss: 0.00000840
Iteration 173/1000 | Loss: 0.00000840
Iteration 174/1000 | Loss: 0.00000840
Iteration 175/1000 | Loss: 0.00000840
Iteration 176/1000 | Loss: 0.00000840
Iteration 177/1000 | Loss: 0.00000840
Iteration 178/1000 | Loss: 0.00000840
Iteration 179/1000 | Loss: 0.00000840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [8.401585546380375e-06, 8.401585546380375e-06, 8.401585546380375e-06, 8.401585546380375e-06, 8.401585546380375e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.401585546380375e-06

Optimization complete. Final v2v error: 2.4755170345306396 mm

Highest mean error: 2.686119794845581 mm for frame 29

Lowest mean error: 2.3713033199310303 mm for frame 146

Saving results

Total time: 36.28971242904663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825679
Iteration 2/25 | Loss: 0.00114802
Iteration 3/25 | Loss: 0.00107450
Iteration 4/25 | Loss: 0.00106962
Iteration 5/25 | Loss: 0.00106834
Iteration 6/25 | Loss: 0.00106834
Iteration 7/25 | Loss: 0.00106834
Iteration 8/25 | Loss: 0.00106834
Iteration 9/25 | Loss: 0.00106834
Iteration 10/25 | Loss: 0.00106834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001068337238393724, 0.001068337238393724, 0.001068337238393724, 0.001068337238393724, 0.001068337238393724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001068337238393724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34723032
Iteration 2/25 | Loss: 0.00070181
Iteration 3/25 | Loss: 0.00070179
Iteration 4/25 | Loss: 0.00070179
Iteration 5/25 | Loss: 0.00070179
Iteration 6/25 | Loss: 0.00070179
Iteration 7/25 | Loss: 0.00070179
Iteration 8/25 | Loss: 0.00070179
Iteration 9/25 | Loss: 0.00070179
Iteration 10/25 | Loss: 0.00070179
Iteration 11/25 | Loss: 0.00070179
Iteration 12/25 | Loss: 0.00070179
Iteration 13/25 | Loss: 0.00070179
Iteration 14/25 | Loss: 0.00070179
Iteration 15/25 | Loss: 0.00070179
Iteration 16/25 | Loss: 0.00070179
Iteration 17/25 | Loss: 0.00070179
Iteration 18/25 | Loss: 0.00070179
Iteration 19/25 | Loss: 0.00070179
Iteration 20/25 | Loss: 0.00070179
Iteration 21/25 | Loss: 0.00070179
Iteration 22/25 | Loss: 0.00070179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007017854368314147, 0.0007017854368314147, 0.0007017854368314147, 0.0007017854368314147, 0.0007017854368314147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007017854368314147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070179
Iteration 2/1000 | Loss: 0.00001868
Iteration 3/1000 | Loss: 0.00001214
Iteration 4/1000 | Loss: 0.00001069
Iteration 5/1000 | Loss: 0.00000979
Iteration 6/1000 | Loss: 0.00000931
Iteration 7/1000 | Loss: 0.00000889
Iteration 8/1000 | Loss: 0.00000871
Iteration 9/1000 | Loss: 0.00000864
Iteration 10/1000 | Loss: 0.00000855
Iteration 11/1000 | Loss: 0.00000845
Iteration 12/1000 | Loss: 0.00000834
Iteration 13/1000 | Loss: 0.00000832
Iteration 14/1000 | Loss: 0.00000826
Iteration 15/1000 | Loss: 0.00000824
Iteration 16/1000 | Loss: 0.00000820
Iteration 17/1000 | Loss: 0.00000816
Iteration 18/1000 | Loss: 0.00000816
Iteration 19/1000 | Loss: 0.00000815
Iteration 20/1000 | Loss: 0.00000815
Iteration 21/1000 | Loss: 0.00000814
Iteration 22/1000 | Loss: 0.00000814
Iteration 23/1000 | Loss: 0.00000813
Iteration 24/1000 | Loss: 0.00000813
Iteration 25/1000 | Loss: 0.00000812
Iteration 26/1000 | Loss: 0.00000811
Iteration 27/1000 | Loss: 0.00000810
Iteration 28/1000 | Loss: 0.00000810
Iteration 29/1000 | Loss: 0.00000809
Iteration 30/1000 | Loss: 0.00000804
Iteration 31/1000 | Loss: 0.00000803
Iteration 32/1000 | Loss: 0.00000803
Iteration 33/1000 | Loss: 0.00000802
Iteration 34/1000 | Loss: 0.00000801
Iteration 35/1000 | Loss: 0.00000800
Iteration 36/1000 | Loss: 0.00000799
Iteration 37/1000 | Loss: 0.00000799
Iteration 38/1000 | Loss: 0.00000799
Iteration 39/1000 | Loss: 0.00000799
Iteration 40/1000 | Loss: 0.00000799
Iteration 41/1000 | Loss: 0.00000799
Iteration 42/1000 | Loss: 0.00000799
Iteration 43/1000 | Loss: 0.00000798
Iteration 44/1000 | Loss: 0.00000798
Iteration 45/1000 | Loss: 0.00000797
Iteration 46/1000 | Loss: 0.00000797
Iteration 47/1000 | Loss: 0.00000797
Iteration 48/1000 | Loss: 0.00000797
Iteration 49/1000 | Loss: 0.00000797
Iteration 50/1000 | Loss: 0.00000797
Iteration 51/1000 | Loss: 0.00000797
Iteration 52/1000 | Loss: 0.00000796
Iteration 53/1000 | Loss: 0.00000796
Iteration 54/1000 | Loss: 0.00000796
Iteration 55/1000 | Loss: 0.00000796
Iteration 56/1000 | Loss: 0.00000796
Iteration 57/1000 | Loss: 0.00000796
Iteration 58/1000 | Loss: 0.00000796
Iteration 59/1000 | Loss: 0.00000796
Iteration 60/1000 | Loss: 0.00000795
Iteration 61/1000 | Loss: 0.00000795
Iteration 62/1000 | Loss: 0.00000795
Iteration 63/1000 | Loss: 0.00000795
Iteration 64/1000 | Loss: 0.00000795
Iteration 65/1000 | Loss: 0.00000795
Iteration 66/1000 | Loss: 0.00000795
Iteration 67/1000 | Loss: 0.00000795
Iteration 68/1000 | Loss: 0.00000794
Iteration 69/1000 | Loss: 0.00000794
Iteration 70/1000 | Loss: 0.00000794
Iteration 71/1000 | Loss: 0.00000794
Iteration 72/1000 | Loss: 0.00000794
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000794
Iteration 75/1000 | Loss: 0.00000794
Iteration 76/1000 | Loss: 0.00000794
Iteration 77/1000 | Loss: 0.00000794
Iteration 78/1000 | Loss: 0.00000793
Iteration 79/1000 | Loss: 0.00000793
Iteration 80/1000 | Loss: 0.00000793
Iteration 81/1000 | Loss: 0.00000793
Iteration 82/1000 | Loss: 0.00000793
Iteration 83/1000 | Loss: 0.00000793
Iteration 84/1000 | Loss: 0.00000793
Iteration 85/1000 | Loss: 0.00000793
Iteration 86/1000 | Loss: 0.00000793
Iteration 87/1000 | Loss: 0.00000793
Iteration 88/1000 | Loss: 0.00000793
Iteration 89/1000 | Loss: 0.00000792
Iteration 90/1000 | Loss: 0.00000792
Iteration 91/1000 | Loss: 0.00000792
Iteration 92/1000 | Loss: 0.00000792
Iteration 93/1000 | Loss: 0.00000792
Iteration 94/1000 | Loss: 0.00000792
Iteration 95/1000 | Loss: 0.00000791
Iteration 96/1000 | Loss: 0.00000791
Iteration 97/1000 | Loss: 0.00000791
Iteration 98/1000 | Loss: 0.00000791
Iteration 99/1000 | Loss: 0.00000791
Iteration 100/1000 | Loss: 0.00000790
Iteration 101/1000 | Loss: 0.00000790
Iteration 102/1000 | Loss: 0.00000790
Iteration 103/1000 | Loss: 0.00000789
Iteration 104/1000 | Loss: 0.00000789
Iteration 105/1000 | Loss: 0.00000789
Iteration 106/1000 | Loss: 0.00000789
Iteration 107/1000 | Loss: 0.00000789
Iteration 108/1000 | Loss: 0.00000789
Iteration 109/1000 | Loss: 0.00000789
Iteration 110/1000 | Loss: 0.00000789
Iteration 111/1000 | Loss: 0.00000789
Iteration 112/1000 | Loss: 0.00000789
Iteration 113/1000 | Loss: 0.00000789
Iteration 114/1000 | Loss: 0.00000789
Iteration 115/1000 | Loss: 0.00000789
Iteration 116/1000 | Loss: 0.00000789
Iteration 117/1000 | Loss: 0.00000789
Iteration 118/1000 | Loss: 0.00000789
Iteration 119/1000 | Loss: 0.00000789
Iteration 120/1000 | Loss: 0.00000789
Iteration 121/1000 | Loss: 0.00000789
Iteration 122/1000 | Loss: 0.00000789
Iteration 123/1000 | Loss: 0.00000789
Iteration 124/1000 | Loss: 0.00000789
Iteration 125/1000 | Loss: 0.00000789
Iteration 126/1000 | Loss: 0.00000789
Iteration 127/1000 | Loss: 0.00000789
Iteration 128/1000 | Loss: 0.00000789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [7.886178536864463e-06, 7.886178536864463e-06, 7.886178536864463e-06, 7.886178536864463e-06, 7.886178536864463e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.886178536864463e-06

Optimization complete. Final v2v error: 2.43176007270813 mm

Highest mean error: 2.6124322414398193 mm for frame 4

Lowest mean error: 2.328368902206421 mm for frame 119

Saving results

Total time: 32.86889100074768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835759
Iteration 2/25 | Loss: 0.00126431
Iteration 3/25 | Loss: 0.00118858
Iteration 4/25 | Loss: 0.00118223
Iteration 5/25 | Loss: 0.00118025
Iteration 6/25 | Loss: 0.00117984
Iteration 7/25 | Loss: 0.00117984
Iteration 8/25 | Loss: 0.00117984
Iteration 9/25 | Loss: 0.00117984
Iteration 10/25 | Loss: 0.00117984
Iteration 11/25 | Loss: 0.00117984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011798359919339418, 0.0011798359919339418, 0.0011798359919339418, 0.0011798359919339418, 0.0011798359919339418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011798359919339418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32676518
Iteration 2/25 | Loss: 0.00097169
Iteration 3/25 | Loss: 0.00097169
Iteration 4/25 | Loss: 0.00097169
Iteration 5/25 | Loss: 0.00097169
Iteration 6/25 | Loss: 0.00097169
Iteration 7/25 | Loss: 0.00097169
Iteration 8/25 | Loss: 0.00097169
Iteration 9/25 | Loss: 0.00097169
Iteration 10/25 | Loss: 0.00097169
Iteration 11/25 | Loss: 0.00097169
Iteration 12/25 | Loss: 0.00097169
Iteration 13/25 | Loss: 0.00097169
Iteration 14/25 | Loss: 0.00097169
Iteration 15/25 | Loss: 0.00097169
Iteration 16/25 | Loss: 0.00097169
Iteration 17/25 | Loss: 0.00097169
Iteration 18/25 | Loss: 0.00097169
Iteration 19/25 | Loss: 0.00097169
Iteration 20/25 | Loss: 0.00097169
Iteration 21/25 | Loss: 0.00097169
Iteration 22/25 | Loss: 0.00097169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009716892382130027, 0.0009716892382130027, 0.0009716892382130027, 0.0009716892382130027, 0.0009716892382130027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009716892382130027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097169
Iteration 2/1000 | Loss: 0.00002818
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001758
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001745
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001743
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001722
Iteration 42/1000 | Loss: 0.00001722
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001718
Iteration 63/1000 | Loss: 0.00001718
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001717
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001717
Iteration 73/1000 | Loss: 0.00001717
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001713
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001710
Iteration 99/1000 | Loss: 0.00001710
Iteration 100/1000 | Loss: 0.00001710
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001707
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001706
Iteration 117/1000 | Loss: 0.00001706
Iteration 118/1000 | Loss: 0.00001706
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001703
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001702
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001699
Iteration 152/1000 | Loss: 0.00001699
Iteration 153/1000 | Loss: 0.00001699
Iteration 154/1000 | Loss: 0.00001699
Iteration 155/1000 | Loss: 0.00001699
Iteration 156/1000 | Loss: 0.00001699
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001698
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001698
Iteration 164/1000 | Loss: 0.00001698
Iteration 165/1000 | Loss: 0.00001698
Iteration 166/1000 | Loss: 0.00001698
Iteration 167/1000 | Loss: 0.00001698
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001697
Iteration 170/1000 | Loss: 0.00001697
Iteration 171/1000 | Loss: 0.00001697
Iteration 172/1000 | Loss: 0.00001697
Iteration 173/1000 | Loss: 0.00001697
Iteration 174/1000 | Loss: 0.00001697
Iteration 175/1000 | Loss: 0.00001697
Iteration 176/1000 | Loss: 0.00001697
Iteration 177/1000 | Loss: 0.00001697
Iteration 178/1000 | Loss: 0.00001697
Iteration 179/1000 | Loss: 0.00001697
Iteration 180/1000 | Loss: 0.00001697
Iteration 181/1000 | Loss: 0.00001697
Iteration 182/1000 | Loss: 0.00001697
Iteration 183/1000 | Loss: 0.00001697
Iteration 184/1000 | Loss: 0.00001696
Iteration 185/1000 | Loss: 0.00001696
Iteration 186/1000 | Loss: 0.00001696
Iteration 187/1000 | Loss: 0.00001696
Iteration 188/1000 | Loss: 0.00001696
Iteration 189/1000 | Loss: 0.00001696
Iteration 190/1000 | Loss: 0.00001696
Iteration 191/1000 | Loss: 0.00001696
Iteration 192/1000 | Loss: 0.00001696
Iteration 193/1000 | Loss: 0.00001696
Iteration 194/1000 | Loss: 0.00001696
Iteration 195/1000 | Loss: 0.00001696
Iteration 196/1000 | Loss: 0.00001695
Iteration 197/1000 | Loss: 0.00001695
Iteration 198/1000 | Loss: 0.00001695
Iteration 199/1000 | Loss: 0.00001695
Iteration 200/1000 | Loss: 0.00001695
Iteration 201/1000 | Loss: 0.00001695
Iteration 202/1000 | Loss: 0.00001695
Iteration 203/1000 | Loss: 0.00001695
Iteration 204/1000 | Loss: 0.00001695
Iteration 205/1000 | Loss: 0.00001695
Iteration 206/1000 | Loss: 0.00001695
Iteration 207/1000 | Loss: 0.00001695
Iteration 208/1000 | Loss: 0.00001695
Iteration 209/1000 | Loss: 0.00001695
Iteration 210/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.695055834716186e-05, 1.695055834716186e-05, 1.695055834716186e-05, 1.695055834716186e-05, 1.695055834716186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.695055834716186e-05

Optimization complete. Final v2v error: 3.4518611431121826 mm

Highest mean error: 3.694662570953369 mm for frame 85

Lowest mean error: 3.1081061363220215 mm for frame 20

Saving results

Total time: 39.11704659461975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741506
Iteration 2/25 | Loss: 0.00154755
Iteration 3/25 | Loss: 0.00135883
Iteration 4/25 | Loss: 0.00132464
Iteration 5/25 | Loss: 0.00131019
Iteration 6/25 | Loss: 0.00132169
Iteration 7/25 | Loss: 0.00130344
Iteration 8/25 | Loss: 0.00129263
Iteration 9/25 | Loss: 0.00128884
Iteration 10/25 | Loss: 0.00128788
Iteration 11/25 | Loss: 0.00128727
Iteration 12/25 | Loss: 0.00128703
Iteration 13/25 | Loss: 0.00128694
Iteration 14/25 | Loss: 0.00128684
Iteration 15/25 | Loss: 0.00128683
Iteration 16/25 | Loss: 0.00128683
Iteration 17/25 | Loss: 0.00128683
Iteration 18/25 | Loss: 0.00128683
Iteration 19/25 | Loss: 0.00128683
Iteration 20/25 | Loss: 0.00128683
Iteration 21/25 | Loss: 0.00128683
Iteration 22/25 | Loss: 0.00128683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012868278427049518, 0.0012868278427049518, 0.0012868278427049518, 0.0012868278427049518, 0.0012868278427049518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012868278427049518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45457006
Iteration 2/25 | Loss: 0.00178707
Iteration 3/25 | Loss: 0.00178707
Iteration 4/25 | Loss: 0.00178707
Iteration 5/25 | Loss: 0.00178707
Iteration 6/25 | Loss: 0.00178707
Iteration 7/25 | Loss: 0.00178707
Iteration 8/25 | Loss: 0.00178707
Iteration 9/25 | Loss: 0.00178707
Iteration 10/25 | Loss: 0.00178707
Iteration 11/25 | Loss: 0.00178707
Iteration 12/25 | Loss: 0.00178707
Iteration 13/25 | Loss: 0.00178707
Iteration 14/25 | Loss: 0.00178707
Iteration 15/25 | Loss: 0.00178706
Iteration 16/25 | Loss: 0.00178706
Iteration 17/25 | Loss: 0.00178706
Iteration 18/25 | Loss: 0.00178706
Iteration 19/25 | Loss: 0.00178706
Iteration 20/25 | Loss: 0.00178706
Iteration 21/25 | Loss: 0.00178706
Iteration 22/25 | Loss: 0.00178706
Iteration 23/25 | Loss: 0.00178706
Iteration 24/25 | Loss: 0.00178706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017870647134259343, 0.0017870647134259343, 0.0017870647134259343, 0.0017870647134259343, 0.0017870647134259343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017870647134259343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178706
Iteration 2/1000 | Loss: 0.00019244
Iteration 3/1000 | Loss: 0.00048830
Iteration 4/1000 | Loss: 0.00020337
Iteration 5/1000 | Loss: 0.00005209
Iteration 6/1000 | Loss: 0.00003700
Iteration 7/1000 | Loss: 0.00003097
Iteration 8/1000 | Loss: 0.00002776
Iteration 9/1000 | Loss: 0.00002630
Iteration 10/1000 | Loss: 0.00002557
Iteration 11/1000 | Loss: 0.00002501
Iteration 12/1000 | Loss: 0.00002461
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002385
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002312
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002274
Iteration 20/1000 | Loss: 0.00002257
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00002249
Iteration 24/1000 | Loss: 0.00002249
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002246
Iteration 27/1000 | Loss: 0.00002241
Iteration 28/1000 | Loss: 0.00002239
Iteration 29/1000 | Loss: 0.00002238
Iteration 30/1000 | Loss: 0.00002230
Iteration 31/1000 | Loss: 0.00002229
Iteration 32/1000 | Loss: 0.00002229
Iteration 33/1000 | Loss: 0.00002227
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00002226
Iteration 36/1000 | Loss: 0.00002225
Iteration 37/1000 | Loss: 0.00002225
Iteration 38/1000 | Loss: 0.00002224
Iteration 39/1000 | Loss: 0.00002224
Iteration 40/1000 | Loss: 0.00002223
Iteration 41/1000 | Loss: 0.00002221
Iteration 42/1000 | Loss: 0.00002221
Iteration 43/1000 | Loss: 0.00002219
Iteration 44/1000 | Loss: 0.00002218
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002216
Iteration 48/1000 | Loss: 0.00002216
Iteration 49/1000 | Loss: 0.00002215
Iteration 50/1000 | Loss: 0.00002215
Iteration 51/1000 | Loss: 0.00002214
Iteration 52/1000 | Loss: 0.00002214
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002213
Iteration 59/1000 | Loss: 0.00002213
Iteration 60/1000 | Loss: 0.00002213
Iteration 61/1000 | Loss: 0.00002212
Iteration 62/1000 | Loss: 0.00002212
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002210
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002208
Iteration 73/1000 | Loss: 0.00002208
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002207
Iteration 77/1000 | Loss: 0.00002207
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002206
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002205
Iteration 83/1000 | Loss: 0.00002205
Iteration 84/1000 | Loss: 0.00002205
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002203
Iteration 89/1000 | Loss: 0.00002203
Iteration 90/1000 | Loss: 0.00002202
Iteration 91/1000 | Loss: 0.00002202
Iteration 92/1000 | Loss: 0.00002202
Iteration 93/1000 | Loss: 0.00002202
Iteration 94/1000 | Loss: 0.00002201
Iteration 95/1000 | Loss: 0.00002201
Iteration 96/1000 | Loss: 0.00002201
Iteration 97/1000 | Loss: 0.00002201
Iteration 98/1000 | Loss: 0.00002201
Iteration 99/1000 | Loss: 0.00002201
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002200
Iteration 103/1000 | Loss: 0.00002200
Iteration 104/1000 | Loss: 0.00002200
Iteration 105/1000 | Loss: 0.00002199
Iteration 106/1000 | Loss: 0.00002199
Iteration 107/1000 | Loss: 0.00002199
Iteration 108/1000 | Loss: 0.00002199
Iteration 109/1000 | Loss: 0.00002199
Iteration 110/1000 | Loss: 0.00002199
Iteration 111/1000 | Loss: 0.00002198
Iteration 112/1000 | Loss: 0.00002198
Iteration 113/1000 | Loss: 0.00002198
Iteration 114/1000 | Loss: 0.00002197
Iteration 115/1000 | Loss: 0.00002197
Iteration 116/1000 | Loss: 0.00002197
Iteration 117/1000 | Loss: 0.00002197
Iteration 118/1000 | Loss: 0.00002196
Iteration 119/1000 | Loss: 0.00002196
Iteration 120/1000 | Loss: 0.00002196
Iteration 121/1000 | Loss: 0.00002196
Iteration 122/1000 | Loss: 0.00002195
Iteration 123/1000 | Loss: 0.00002195
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002195
Iteration 126/1000 | Loss: 0.00002194
Iteration 127/1000 | Loss: 0.00002194
Iteration 128/1000 | Loss: 0.00002194
Iteration 129/1000 | Loss: 0.00002194
Iteration 130/1000 | Loss: 0.00002194
Iteration 131/1000 | Loss: 0.00002194
Iteration 132/1000 | Loss: 0.00002194
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002194
Iteration 135/1000 | Loss: 0.00002193
Iteration 136/1000 | Loss: 0.00002193
Iteration 137/1000 | Loss: 0.00002193
Iteration 138/1000 | Loss: 0.00002193
Iteration 139/1000 | Loss: 0.00002193
Iteration 140/1000 | Loss: 0.00002193
Iteration 141/1000 | Loss: 0.00002193
Iteration 142/1000 | Loss: 0.00002193
Iteration 143/1000 | Loss: 0.00002193
Iteration 144/1000 | Loss: 0.00002193
Iteration 145/1000 | Loss: 0.00002193
Iteration 146/1000 | Loss: 0.00002192
Iteration 147/1000 | Loss: 0.00002192
Iteration 148/1000 | Loss: 0.00002192
Iteration 149/1000 | Loss: 0.00002192
Iteration 150/1000 | Loss: 0.00002192
Iteration 151/1000 | Loss: 0.00002192
Iteration 152/1000 | Loss: 0.00002192
Iteration 153/1000 | Loss: 0.00002192
Iteration 154/1000 | Loss: 0.00002192
Iteration 155/1000 | Loss: 0.00002192
Iteration 156/1000 | Loss: 0.00002192
Iteration 157/1000 | Loss: 0.00002192
Iteration 158/1000 | Loss: 0.00002192
Iteration 159/1000 | Loss: 0.00002192
Iteration 160/1000 | Loss: 0.00002192
Iteration 161/1000 | Loss: 0.00002192
Iteration 162/1000 | Loss: 0.00002192
Iteration 163/1000 | Loss: 0.00002191
Iteration 164/1000 | Loss: 0.00002191
Iteration 165/1000 | Loss: 0.00002191
Iteration 166/1000 | Loss: 0.00002191
Iteration 167/1000 | Loss: 0.00002191
Iteration 168/1000 | Loss: 0.00002191
Iteration 169/1000 | Loss: 0.00002191
Iteration 170/1000 | Loss: 0.00002191
Iteration 171/1000 | Loss: 0.00002190
Iteration 172/1000 | Loss: 0.00002190
Iteration 173/1000 | Loss: 0.00002190
Iteration 174/1000 | Loss: 0.00002190
Iteration 175/1000 | Loss: 0.00002190
Iteration 176/1000 | Loss: 0.00002190
Iteration 177/1000 | Loss: 0.00002189
Iteration 178/1000 | Loss: 0.00002189
Iteration 179/1000 | Loss: 0.00002189
Iteration 180/1000 | Loss: 0.00002189
Iteration 181/1000 | Loss: 0.00002189
Iteration 182/1000 | Loss: 0.00002189
Iteration 183/1000 | Loss: 0.00002189
Iteration 184/1000 | Loss: 0.00002189
Iteration 185/1000 | Loss: 0.00002189
Iteration 186/1000 | Loss: 0.00002189
Iteration 187/1000 | Loss: 0.00002189
Iteration 188/1000 | Loss: 0.00002189
Iteration 189/1000 | Loss: 0.00002189
Iteration 190/1000 | Loss: 0.00002188
Iteration 191/1000 | Loss: 0.00002188
Iteration 192/1000 | Loss: 0.00002188
Iteration 193/1000 | Loss: 0.00002188
Iteration 194/1000 | Loss: 0.00002188
Iteration 195/1000 | Loss: 0.00002188
Iteration 196/1000 | Loss: 0.00002188
Iteration 197/1000 | Loss: 0.00002188
Iteration 198/1000 | Loss: 0.00002188
Iteration 199/1000 | Loss: 0.00002188
Iteration 200/1000 | Loss: 0.00002188
Iteration 201/1000 | Loss: 0.00002188
Iteration 202/1000 | Loss: 0.00002188
Iteration 203/1000 | Loss: 0.00002188
Iteration 204/1000 | Loss: 0.00002187
Iteration 205/1000 | Loss: 0.00002187
Iteration 206/1000 | Loss: 0.00002187
Iteration 207/1000 | Loss: 0.00002187
Iteration 208/1000 | Loss: 0.00002187
Iteration 209/1000 | Loss: 0.00002187
Iteration 210/1000 | Loss: 0.00002187
Iteration 211/1000 | Loss: 0.00002187
Iteration 212/1000 | Loss: 0.00002187
Iteration 213/1000 | Loss: 0.00002187
Iteration 214/1000 | Loss: 0.00002187
Iteration 215/1000 | Loss: 0.00002187
Iteration 216/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.1873536752536893e-05, 2.1873536752536893e-05, 2.1873536752536893e-05, 2.1873536752536893e-05, 2.1873536752536893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1873536752536893e-05

Optimization complete. Final v2v error: 3.7402942180633545 mm

Highest mean error: 6.800637245178223 mm for frame 100

Lowest mean error: 2.6060450077056885 mm for frame 195

Saving results

Total time: 79.24400305747986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977946
Iteration 2/25 | Loss: 0.00154983
Iteration 3/25 | Loss: 0.00133571
Iteration 4/25 | Loss: 0.00137730
Iteration 5/25 | Loss: 0.00130805
Iteration 6/25 | Loss: 0.00128503
Iteration 7/25 | Loss: 0.00126167
Iteration 8/25 | Loss: 0.00125593
Iteration 9/25 | Loss: 0.00124487
Iteration 10/25 | Loss: 0.00123752
Iteration 11/25 | Loss: 0.00123317
Iteration 12/25 | Loss: 0.00122846
Iteration 13/25 | Loss: 0.00122697
Iteration 14/25 | Loss: 0.00122657
Iteration 15/25 | Loss: 0.00122472
Iteration 16/25 | Loss: 0.00122857
Iteration 17/25 | Loss: 0.00122909
Iteration 18/25 | Loss: 0.00122812
Iteration 19/25 | Loss: 0.00122967
Iteration 20/25 | Loss: 0.00122543
Iteration 21/25 | Loss: 0.00122452
Iteration 22/25 | Loss: 0.00122453
Iteration 23/25 | Loss: 0.00122473
Iteration 24/25 | Loss: 0.00122458
Iteration 25/25 | Loss: 0.00122530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27022719
Iteration 2/25 | Loss: 0.00118084
Iteration 3/25 | Loss: 0.00118081
Iteration 4/25 | Loss: 0.00118081
Iteration 5/25 | Loss: 0.00118081
Iteration 6/25 | Loss: 0.00118081
Iteration 7/25 | Loss: 0.00118081
Iteration 8/25 | Loss: 0.00118081
Iteration 9/25 | Loss: 0.00118081
Iteration 10/25 | Loss: 0.00118081
Iteration 11/25 | Loss: 0.00118081
Iteration 12/25 | Loss: 0.00118081
Iteration 13/25 | Loss: 0.00118081
Iteration 14/25 | Loss: 0.00118081
Iteration 15/25 | Loss: 0.00118081
Iteration 16/25 | Loss: 0.00118081
Iteration 17/25 | Loss: 0.00118081
Iteration 18/25 | Loss: 0.00118081
Iteration 19/25 | Loss: 0.00118081
Iteration 20/25 | Loss: 0.00118081
Iteration 21/25 | Loss: 0.00118081
Iteration 22/25 | Loss: 0.00118081
Iteration 23/25 | Loss: 0.00118081
Iteration 24/25 | Loss: 0.00118081
Iteration 25/25 | Loss: 0.00118081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118081
Iteration 2/1000 | Loss: 0.00005688
Iteration 3/1000 | Loss: 0.00092086
Iteration 4/1000 | Loss: 0.00004587
Iteration 5/1000 | Loss: 0.00003539
Iteration 6/1000 | Loss: 0.00004066
Iteration 7/1000 | Loss: 0.00003608
Iteration 8/1000 | Loss: 0.00019030
Iteration 9/1000 | Loss: 0.00013433
Iteration 10/1000 | Loss: 0.00004276
Iteration 11/1000 | Loss: 0.00103886
Iteration 12/1000 | Loss: 0.00210170
Iteration 13/1000 | Loss: 0.00149704
Iteration 14/1000 | Loss: 0.00099632
Iteration 15/1000 | Loss: 0.00063960
Iteration 16/1000 | Loss: 0.00096074
Iteration 17/1000 | Loss: 0.00092240
Iteration 18/1000 | Loss: 0.00059578
Iteration 19/1000 | Loss: 0.00014531
Iteration 20/1000 | Loss: 0.00211555
Iteration 21/1000 | Loss: 0.00006832
Iteration 22/1000 | Loss: 0.00003805
Iteration 23/1000 | Loss: 0.00003511
Iteration 24/1000 | Loss: 0.00004338
Iteration 25/1000 | Loss: 0.00002845
Iteration 26/1000 | Loss: 0.00003169
Iteration 27/1000 | Loss: 0.00002787
Iteration 28/1000 | Loss: 0.00003093
Iteration 29/1000 | Loss: 0.00002865
Iteration 30/1000 | Loss: 0.00003195
Iteration 31/1000 | Loss: 0.00003740
Iteration 32/1000 | Loss: 0.00004055
Iteration 33/1000 | Loss: 0.00003847
Iteration 34/1000 | Loss: 0.00003720
Iteration 35/1000 | Loss: 0.00003649
Iteration 36/1000 | Loss: 0.00003763
Iteration 37/1000 | Loss: 0.00003734
Iteration 38/1000 | Loss: 0.00003758
Iteration 39/1000 | Loss: 0.00003085
Iteration 40/1000 | Loss: 0.00004172
Iteration 41/1000 | Loss: 0.00003291
Iteration 42/1000 | Loss: 0.00003438
Iteration 43/1000 | Loss: 0.00004145
Iteration 44/1000 | Loss: 0.00003610
Iteration 45/1000 | Loss: 0.00003686
Iteration 46/1000 | Loss: 0.00003573
Iteration 47/1000 | Loss: 0.00003930
Iteration 48/1000 | Loss: 0.00003415
Iteration 49/1000 | Loss: 0.00004359
Iteration 50/1000 | Loss: 0.00003655
Iteration 51/1000 | Loss: 0.00003245
Iteration 52/1000 | Loss: 0.00002901
Iteration 53/1000 | Loss: 0.00003650
Iteration 54/1000 | Loss: 0.00003552
Iteration 55/1000 | Loss: 0.00003617
Iteration 56/1000 | Loss: 0.00003646
Iteration 57/1000 | Loss: 0.00004373
Iteration 58/1000 | Loss: 0.00004264
Iteration 59/1000 | Loss: 0.00003661
Iteration 60/1000 | Loss: 0.00004058
Iteration 61/1000 | Loss: 0.00004035
Iteration 62/1000 | Loss: 0.00003438
Iteration 63/1000 | Loss: 0.00003080
Iteration 64/1000 | Loss: 0.00003680
Iteration 65/1000 | Loss: 0.00004038
Iteration 66/1000 | Loss: 0.00003998
Iteration 67/1000 | Loss: 0.00003563
Iteration 68/1000 | Loss: 0.00003663
Iteration 69/1000 | Loss: 0.00003586
Iteration 70/1000 | Loss: 0.00003558
Iteration 71/1000 | Loss: 0.00003536
Iteration 72/1000 | Loss: 0.00003446
Iteration 73/1000 | Loss: 0.00004283
Iteration 74/1000 | Loss: 0.00004058
Iteration 75/1000 | Loss: 0.00003542
Iteration 76/1000 | Loss: 0.00003859
Iteration 77/1000 | Loss: 0.00003362
Iteration 78/1000 | Loss: 0.00003617
Iteration 79/1000 | Loss: 0.00003401
Iteration 80/1000 | Loss: 0.00003381
Iteration 81/1000 | Loss: 0.00004434
Iteration 82/1000 | Loss: 0.00003341
Iteration 83/1000 | Loss: 0.00003401
Iteration 84/1000 | Loss: 0.00002915
Iteration 85/1000 | Loss: 0.00004124
Iteration 86/1000 | Loss: 0.00003108
Iteration 87/1000 | Loss: 0.00003709
Iteration 88/1000 | Loss: 0.00004300
Iteration 89/1000 | Loss: 0.00003699
Iteration 90/1000 | Loss: 0.00004305
Iteration 91/1000 | Loss: 0.00003040
Iteration 92/1000 | Loss: 0.00003406
Iteration 93/1000 | Loss: 0.00004515
Iteration 94/1000 | Loss: 0.00003203
Iteration 95/1000 | Loss: 0.00003646
Iteration 96/1000 | Loss: 0.00003963
Iteration 97/1000 | Loss: 0.00003544
Iteration 98/1000 | Loss: 0.00003287
Iteration 99/1000 | Loss: 0.00003723
Iteration 100/1000 | Loss: 0.00004553
Iteration 101/1000 | Loss: 0.00004076
Iteration 102/1000 | Loss: 0.00003316
Iteration 103/1000 | Loss: 0.00003811
Iteration 104/1000 | Loss: 0.00003451
Iteration 105/1000 | Loss: 0.00003437
Iteration 106/1000 | Loss: 0.00003316
Iteration 107/1000 | Loss: 0.00004842
Iteration 108/1000 | Loss: 0.00003946
Iteration 109/1000 | Loss: 0.00002990
Iteration 110/1000 | Loss: 0.00004347
Iteration 111/1000 | Loss: 0.00003311
Iteration 112/1000 | Loss: 0.00003536
Iteration 113/1000 | Loss: 0.00003488
Iteration 114/1000 | Loss: 0.00003792
Iteration 115/1000 | Loss: 0.00003407
Iteration 116/1000 | Loss: 0.00004324
Iteration 117/1000 | Loss: 0.00003590
Iteration 118/1000 | Loss: 0.00003912
Iteration 119/1000 | Loss: 0.00002805
Iteration 120/1000 | Loss: 0.00003358
Iteration 121/1000 | Loss: 0.00002784
Iteration 122/1000 | Loss: 0.00003436
Iteration 123/1000 | Loss: 0.00002851
Iteration 124/1000 | Loss: 0.00003432
Iteration 125/1000 | Loss: 0.00002858
Iteration 126/1000 | Loss: 0.00003354
Iteration 127/1000 | Loss: 0.00002572
Iteration 128/1000 | Loss: 0.00002906
Iteration 129/1000 | Loss: 0.00003506
Iteration 130/1000 | Loss: 0.00003172
Iteration 131/1000 | Loss: 0.00002768
Iteration 132/1000 | Loss: 0.00003189
Iteration 133/1000 | Loss: 0.00003179
Iteration 134/1000 | Loss: 0.00004150
Iteration 135/1000 | Loss: 0.00011958
Iteration 136/1000 | Loss: 0.00004322
Iteration 137/1000 | Loss: 0.00002676
Iteration 138/1000 | Loss: 0.00002330
Iteration 139/1000 | Loss: 0.00002207
Iteration 140/1000 | Loss: 0.00002138
Iteration 141/1000 | Loss: 0.00002101
Iteration 142/1000 | Loss: 0.00002072
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002053
Iteration 145/1000 | Loss: 0.00002048
Iteration 146/1000 | Loss: 0.00002046
Iteration 147/1000 | Loss: 0.00002046
Iteration 148/1000 | Loss: 0.00002032
Iteration 149/1000 | Loss: 0.00002027
Iteration 150/1000 | Loss: 0.00002027
Iteration 151/1000 | Loss: 0.00002022
Iteration 152/1000 | Loss: 0.00002017
Iteration 153/1000 | Loss: 0.00002013
Iteration 154/1000 | Loss: 0.00002008
Iteration 155/1000 | Loss: 0.00002003
Iteration 156/1000 | Loss: 0.00002003
Iteration 157/1000 | Loss: 0.00002001
Iteration 158/1000 | Loss: 0.00002001
Iteration 159/1000 | Loss: 0.00001998
Iteration 160/1000 | Loss: 0.00001995
Iteration 161/1000 | Loss: 0.00001994
Iteration 162/1000 | Loss: 0.00001994
Iteration 163/1000 | Loss: 0.00001993
Iteration 164/1000 | Loss: 0.00001992
Iteration 165/1000 | Loss: 0.00001991
Iteration 166/1000 | Loss: 0.00001990
Iteration 167/1000 | Loss: 0.00001990
Iteration 168/1000 | Loss: 0.00001990
Iteration 169/1000 | Loss: 0.00001990
Iteration 170/1000 | Loss: 0.00001990
Iteration 171/1000 | Loss: 0.00001990
Iteration 172/1000 | Loss: 0.00001990
Iteration 173/1000 | Loss: 0.00001989
Iteration 174/1000 | Loss: 0.00001989
Iteration 175/1000 | Loss: 0.00001989
Iteration 176/1000 | Loss: 0.00001988
Iteration 177/1000 | Loss: 0.00001987
Iteration 178/1000 | Loss: 0.00001986
Iteration 179/1000 | Loss: 0.00001986
Iteration 180/1000 | Loss: 0.00001986
Iteration 181/1000 | Loss: 0.00001986
Iteration 182/1000 | Loss: 0.00001985
Iteration 183/1000 | Loss: 0.00001985
Iteration 184/1000 | Loss: 0.00001985
Iteration 185/1000 | Loss: 0.00001985
Iteration 186/1000 | Loss: 0.00001984
Iteration 187/1000 | Loss: 0.00001984
Iteration 188/1000 | Loss: 0.00001984
Iteration 189/1000 | Loss: 0.00001984
Iteration 190/1000 | Loss: 0.00001984
Iteration 191/1000 | Loss: 0.00001984
Iteration 192/1000 | Loss: 0.00001983
Iteration 193/1000 | Loss: 0.00001983
Iteration 194/1000 | Loss: 0.00001983
Iteration 195/1000 | Loss: 0.00001983
Iteration 196/1000 | Loss: 0.00001982
Iteration 197/1000 | Loss: 0.00001982
Iteration 198/1000 | Loss: 0.00001982
Iteration 199/1000 | Loss: 0.00001982
Iteration 200/1000 | Loss: 0.00001981
Iteration 201/1000 | Loss: 0.00001981
Iteration 202/1000 | Loss: 0.00001981
Iteration 203/1000 | Loss: 0.00001981
Iteration 204/1000 | Loss: 0.00001980
Iteration 205/1000 | Loss: 0.00001980
Iteration 206/1000 | Loss: 0.00001980
Iteration 207/1000 | Loss: 0.00001980
Iteration 208/1000 | Loss: 0.00001980
Iteration 209/1000 | Loss: 0.00001980
Iteration 210/1000 | Loss: 0.00001980
Iteration 211/1000 | Loss: 0.00001980
Iteration 212/1000 | Loss: 0.00001980
Iteration 213/1000 | Loss: 0.00001979
Iteration 214/1000 | Loss: 0.00001979
Iteration 215/1000 | Loss: 0.00001979
Iteration 216/1000 | Loss: 0.00001979
Iteration 217/1000 | Loss: 0.00001979
Iteration 218/1000 | Loss: 0.00001979
Iteration 219/1000 | Loss: 0.00001978
Iteration 220/1000 | Loss: 0.00001978
Iteration 221/1000 | Loss: 0.00001978
Iteration 222/1000 | Loss: 0.00001978
Iteration 223/1000 | Loss: 0.00001978
Iteration 224/1000 | Loss: 0.00001978
Iteration 225/1000 | Loss: 0.00001978
Iteration 226/1000 | Loss: 0.00001978
Iteration 227/1000 | Loss: 0.00001978
Iteration 228/1000 | Loss: 0.00001978
Iteration 229/1000 | Loss: 0.00001977
Iteration 230/1000 | Loss: 0.00001977
Iteration 231/1000 | Loss: 0.00001977
Iteration 232/1000 | Loss: 0.00001977
Iteration 233/1000 | Loss: 0.00001977
Iteration 234/1000 | Loss: 0.00001977
Iteration 235/1000 | Loss: 0.00001977
Iteration 236/1000 | Loss: 0.00001977
Iteration 237/1000 | Loss: 0.00001977
Iteration 238/1000 | Loss: 0.00001977
Iteration 239/1000 | Loss: 0.00001977
Iteration 240/1000 | Loss: 0.00001977
Iteration 241/1000 | Loss: 0.00001976
Iteration 242/1000 | Loss: 0.00001976
Iteration 243/1000 | Loss: 0.00001976
Iteration 244/1000 | Loss: 0.00001976
Iteration 245/1000 | Loss: 0.00001976
Iteration 246/1000 | Loss: 0.00001976
Iteration 247/1000 | Loss: 0.00001976
Iteration 248/1000 | Loss: 0.00001976
Iteration 249/1000 | Loss: 0.00001976
Iteration 250/1000 | Loss: 0.00001976
Iteration 251/1000 | Loss: 0.00001976
Iteration 252/1000 | Loss: 0.00001976
Iteration 253/1000 | Loss: 0.00001976
Iteration 254/1000 | Loss: 0.00001976
Iteration 255/1000 | Loss: 0.00001976
Iteration 256/1000 | Loss: 0.00001976
Iteration 257/1000 | Loss: 0.00001976
Iteration 258/1000 | Loss: 0.00001975
Iteration 259/1000 | Loss: 0.00001975
Iteration 260/1000 | Loss: 0.00001975
Iteration 261/1000 | Loss: 0.00001975
Iteration 262/1000 | Loss: 0.00001975
Iteration 263/1000 | Loss: 0.00001975
Iteration 264/1000 | Loss: 0.00001975
Iteration 265/1000 | Loss: 0.00001975
Iteration 266/1000 | Loss: 0.00001975
Iteration 267/1000 | Loss: 0.00001975
Iteration 268/1000 | Loss: 0.00001975
Iteration 269/1000 | Loss: 0.00001975
Iteration 270/1000 | Loss: 0.00001975
Iteration 271/1000 | Loss: 0.00001974
Iteration 272/1000 | Loss: 0.00001974
Iteration 273/1000 | Loss: 0.00001974
Iteration 274/1000 | Loss: 0.00001974
Iteration 275/1000 | Loss: 0.00001974
Iteration 276/1000 | Loss: 0.00001974
Iteration 277/1000 | Loss: 0.00001974
Iteration 278/1000 | Loss: 0.00001974
Iteration 279/1000 | Loss: 0.00001974
Iteration 280/1000 | Loss: 0.00001974
Iteration 281/1000 | Loss: 0.00001974
Iteration 282/1000 | Loss: 0.00001973
Iteration 283/1000 | Loss: 0.00001973
Iteration 284/1000 | Loss: 0.00001973
Iteration 285/1000 | Loss: 0.00001973
Iteration 286/1000 | Loss: 0.00001973
Iteration 287/1000 | Loss: 0.00001973
Iteration 288/1000 | Loss: 0.00001973
Iteration 289/1000 | Loss: 0.00001973
Iteration 290/1000 | Loss: 0.00001973
Iteration 291/1000 | Loss: 0.00001973
Iteration 292/1000 | Loss: 0.00001973
Iteration 293/1000 | Loss: 0.00001973
Iteration 294/1000 | Loss: 0.00001973
Iteration 295/1000 | Loss: 0.00001973
Iteration 296/1000 | Loss: 0.00001973
Iteration 297/1000 | Loss: 0.00001973
Iteration 298/1000 | Loss: 0.00001973
Iteration 299/1000 | Loss: 0.00001973
Iteration 300/1000 | Loss: 0.00001973
Iteration 301/1000 | Loss: 0.00001973
Iteration 302/1000 | Loss: 0.00001973
Iteration 303/1000 | Loss: 0.00001973
Iteration 304/1000 | Loss: 0.00001973
Iteration 305/1000 | Loss: 0.00001973
Iteration 306/1000 | Loss: 0.00001973
Iteration 307/1000 | Loss: 0.00001973
Iteration 308/1000 | Loss: 0.00001973
Iteration 309/1000 | Loss: 0.00001973
Iteration 310/1000 | Loss: 0.00001973
Iteration 311/1000 | Loss: 0.00001973
Iteration 312/1000 | Loss: 0.00001973
Iteration 313/1000 | Loss: 0.00001973
Iteration 314/1000 | Loss: 0.00001973
Iteration 315/1000 | Loss: 0.00001973
Iteration 316/1000 | Loss: 0.00001973
Iteration 317/1000 | Loss: 0.00001973
Iteration 318/1000 | Loss: 0.00001973
Iteration 319/1000 | Loss: 0.00001973
Iteration 320/1000 | Loss: 0.00001973
Iteration 321/1000 | Loss: 0.00001973
Iteration 322/1000 | Loss: 0.00001973
Iteration 323/1000 | Loss: 0.00001973
Iteration 324/1000 | Loss: 0.00001973
Iteration 325/1000 | Loss: 0.00001973
Iteration 326/1000 | Loss: 0.00001973
Iteration 327/1000 | Loss: 0.00001973
Iteration 328/1000 | Loss: 0.00001973
Iteration 329/1000 | Loss: 0.00001973
Iteration 330/1000 | Loss: 0.00001973
Iteration 331/1000 | Loss: 0.00001973
Iteration 332/1000 | Loss: 0.00001973
Iteration 333/1000 | Loss: 0.00001973
Iteration 334/1000 | Loss: 0.00001973
Iteration 335/1000 | Loss: 0.00001973
Iteration 336/1000 | Loss: 0.00001973
Iteration 337/1000 | Loss: 0.00001973
Iteration 338/1000 | Loss: 0.00001973
Iteration 339/1000 | Loss: 0.00001973
Iteration 340/1000 | Loss: 0.00001973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [1.9728757251868956e-05, 1.9728757251868956e-05, 1.9728757251868956e-05, 1.9728757251868956e-05, 1.9728757251868956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9728757251868956e-05

Optimization complete. Final v2v error: 3.608506679534912 mm

Highest mean error: 4.598174571990967 mm for frame 78

Lowest mean error: 2.662663459777832 mm for frame 168

Saving results

Total time: 286.04930353164673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806087
Iteration 2/25 | Loss: 0.00122706
Iteration 3/25 | Loss: 0.00113309
Iteration 4/25 | Loss: 0.00111850
Iteration 5/25 | Loss: 0.00111461
Iteration 6/25 | Loss: 0.00111461
Iteration 7/25 | Loss: 0.00111461
Iteration 8/25 | Loss: 0.00111461
Iteration 9/25 | Loss: 0.00111461
Iteration 10/25 | Loss: 0.00111461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011146070901304483, 0.0011146070901304483, 0.0011146070901304483, 0.0011146070901304483, 0.0011146070901304483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011146070901304483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06638575
Iteration 2/25 | Loss: 0.00082955
Iteration 3/25 | Loss: 0.00082955
Iteration 4/25 | Loss: 0.00082955
Iteration 5/25 | Loss: 0.00082955
Iteration 6/25 | Loss: 0.00082955
Iteration 7/25 | Loss: 0.00082955
Iteration 8/25 | Loss: 0.00082955
Iteration 9/25 | Loss: 0.00082955
Iteration 10/25 | Loss: 0.00082955
Iteration 11/25 | Loss: 0.00082955
Iteration 12/25 | Loss: 0.00082955
Iteration 13/25 | Loss: 0.00082955
Iteration 14/25 | Loss: 0.00082955
Iteration 15/25 | Loss: 0.00082955
Iteration 16/25 | Loss: 0.00082955
Iteration 17/25 | Loss: 0.00082955
Iteration 18/25 | Loss: 0.00082955
Iteration 19/25 | Loss: 0.00082955
Iteration 20/25 | Loss: 0.00082955
Iteration 21/25 | Loss: 0.00082955
Iteration 22/25 | Loss: 0.00082955
Iteration 23/25 | Loss: 0.00082955
Iteration 24/25 | Loss: 0.00082955
Iteration 25/25 | Loss: 0.00082955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082955
Iteration 2/1000 | Loss: 0.00002576
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001760
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001645
Iteration 8/1000 | Loss: 0.00001597
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001501
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001459
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001458
Iteration 27/1000 | Loss: 0.00001457
Iteration 28/1000 | Loss: 0.00001456
Iteration 29/1000 | Loss: 0.00001456
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001455
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001454
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001440
Iteration 57/1000 | Loss: 0.00001440
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001437
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001431
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001425
Iteration 85/1000 | Loss: 0.00001425
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001424
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001422
Iteration 96/1000 | Loss: 0.00001421
Iteration 97/1000 | Loss: 0.00001421
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001417
Iteration 112/1000 | Loss: 0.00001417
Iteration 113/1000 | Loss: 0.00001417
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001416
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001415
Iteration 123/1000 | Loss: 0.00001415
Iteration 124/1000 | Loss: 0.00001415
Iteration 125/1000 | Loss: 0.00001415
Iteration 126/1000 | Loss: 0.00001415
Iteration 127/1000 | Loss: 0.00001415
Iteration 128/1000 | Loss: 0.00001415
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001414
Iteration 131/1000 | Loss: 0.00001414
Iteration 132/1000 | Loss: 0.00001414
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00001414
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001414
Iteration 140/1000 | Loss: 0.00001414
Iteration 141/1000 | Loss: 0.00001414
Iteration 142/1000 | Loss: 0.00001414
Iteration 143/1000 | Loss: 0.00001414
Iteration 144/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.4136334357317537e-05, 1.4136334357317537e-05, 1.4136334357317537e-05, 1.4136334357317537e-05, 1.4136334357317537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4136334357317537e-05

Optimization complete. Final v2v error: 3.1855900287628174 mm

Highest mean error: 3.439185619354248 mm for frame 129

Lowest mean error: 2.9402825832366943 mm for frame 203

Saving results

Total time: 43.88069486618042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056886
Iteration 2/25 | Loss: 0.01056886
Iteration 3/25 | Loss: 0.01056886
Iteration 4/25 | Loss: 0.01056886
Iteration 5/25 | Loss: 0.01056886
Iteration 6/25 | Loss: 0.01056886
Iteration 7/25 | Loss: 0.01056886
Iteration 8/25 | Loss: 0.01056886
Iteration 9/25 | Loss: 0.01056886
Iteration 10/25 | Loss: 0.01056886
Iteration 11/25 | Loss: 0.01056886
Iteration 12/25 | Loss: 0.01056886
Iteration 13/25 | Loss: 0.01056886
Iteration 14/25 | Loss: 0.01056886
Iteration 15/25 | Loss: 0.01056885
Iteration 16/25 | Loss: 0.01056885
Iteration 17/25 | Loss: 0.01056885
Iteration 18/25 | Loss: 0.01056885
Iteration 19/25 | Loss: 0.01056885
Iteration 20/25 | Loss: 0.01056885
Iteration 21/25 | Loss: 0.01056885
Iteration 22/25 | Loss: 0.01056885
Iteration 23/25 | Loss: 0.01056885
Iteration 24/25 | Loss: 0.01056885
Iteration 25/25 | Loss: 0.01056885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59884930
Iteration 2/25 | Loss: 0.08293544
Iteration 3/25 | Loss: 0.08292691
Iteration 4/25 | Loss: 0.08292691
Iteration 5/25 | Loss: 0.08292691
Iteration 6/25 | Loss: 0.08292691
Iteration 7/25 | Loss: 0.08292689
Iteration 8/25 | Loss: 0.08292689
Iteration 9/25 | Loss: 0.08292689
Iteration 10/25 | Loss: 0.08292689
Iteration 11/25 | Loss: 0.08292689
Iteration 12/25 | Loss: 0.08292689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08292689174413681, 0.08292689174413681, 0.08292689174413681, 0.08292689174413681, 0.08292689174413681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08292689174413681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08292689
Iteration 2/1000 | Loss: 0.00046432
Iteration 3/1000 | Loss: 0.00014892
Iteration 4/1000 | Loss: 0.00006128
Iteration 5/1000 | Loss: 0.00003484
Iteration 6/1000 | Loss: 0.00002674
Iteration 7/1000 | Loss: 0.00002243
Iteration 8/1000 | Loss: 0.00002014
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001072
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000967
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000947
Iteration 31/1000 | Loss: 0.00000939
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000931
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000930
Iteration 36/1000 | Loss: 0.00000929
Iteration 37/1000 | Loss: 0.00000928
Iteration 38/1000 | Loss: 0.00000928
Iteration 39/1000 | Loss: 0.00000926
Iteration 40/1000 | Loss: 0.00000926
Iteration 41/1000 | Loss: 0.00000925
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000923
Iteration 44/1000 | Loss: 0.00000923
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000923
Iteration 47/1000 | Loss: 0.00000921
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000920
Iteration 51/1000 | Loss: 0.00000920
Iteration 52/1000 | Loss: 0.00000919
Iteration 53/1000 | Loss: 0.00000919
Iteration 54/1000 | Loss: 0.00000918
Iteration 55/1000 | Loss: 0.00000918
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000917
Iteration 58/1000 | Loss: 0.00000916
Iteration 59/1000 | Loss: 0.00000916
Iteration 60/1000 | Loss: 0.00000916
Iteration 61/1000 | Loss: 0.00000916
Iteration 62/1000 | Loss: 0.00000916
Iteration 63/1000 | Loss: 0.00000915
Iteration 64/1000 | Loss: 0.00000915
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000915
Iteration 67/1000 | Loss: 0.00000915
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000912
Iteration 73/1000 | Loss: 0.00000912
Iteration 74/1000 | Loss: 0.00000911
Iteration 75/1000 | Loss: 0.00000911
Iteration 76/1000 | Loss: 0.00000911
Iteration 77/1000 | Loss: 0.00000911
Iteration 78/1000 | Loss: 0.00000911
Iteration 79/1000 | Loss: 0.00000911
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000909
Iteration 82/1000 | Loss: 0.00000909
Iteration 83/1000 | Loss: 0.00000909
Iteration 84/1000 | Loss: 0.00000909
Iteration 85/1000 | Loss: 0.00000909
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000909
Iteration 88/1000 | Loss: 0.00000909
Iteration 89/1000 | Loss: 0.00000909
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000908
Iteration 93/1000 | Loss: 0.00000908
Iteration 94/1000 | Loss: 0.00000908
Iteration 95/1000 | Loss: 0.00000908
Iteration 96/1000 | Loss: 0.00000908
Iteration 97/1000 | Loss: 0.00000907
Iteration 98/1000 | Loss: 0.00000907
Iteration 99/1000 | Loss: 0.00000907
Iteration 100/1000 | Loss: 0.00000907
Iteration 101/1000 | Loss: 0.00000907
Iteration 102/1000 | Loss: 0.00000906
Iteration 103/1000 | Loss: 0.00000906
Iteration 104/1000 | Loss: 0.00000906
Iteration 105/1000 | Loss: 0.00000905
Iteration 106/1000 | Loss: 0.00000905
Iteration 107/1000 | Loss: 0.00000905
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000905
Iteration 110/1000 | Loss: 0.00000905
Iteration 111/1000 | Loss: 0.00000905
Iteration 112/1000 | Loss: 0.00000905
Iteration 113/1000 | Loss: 0.00000905
Iteration 114/1000 | Loss: 0.00000905
Iteration 115/1000 | Loss: 0.00000904
Iteration 116/1000 | Loss: 0.00000904
Iteration 117/1000 | Loss: 0.00000904
Iteration 118/1000 | Loss: 0.00000904
Iteration 119/1000 | Loss: 0.00000904
Iteration 120/1000 | Loss: 0.00000904
Iteration 121/1000 | Loss: 0.00000904
Iteration 122/1000 | Loss: 0.00000904
Iteration 123/1000 | Loss: 0.00000904
Iteration 124/1000 | Loss: 0.00000904
Iteration 125/1000 | Loss: 0.00000904
Iteration 126/1000 | Loss: 0.00000904
Iteration 127/1000 | Loss: 0.00000904
Iteration 128/1000 | Loss: 0.00000904
Iteration 129/1000 | Loss: 0.00000903
Iteration 130/1000 | Loss: 0.00000903
Iteration 131/1000 | Loss: 0.00000903
Iteration 132/1000 | Loss: 0.00000903
Iteration 133/1000 | Loss: 0.00000903
Iteration 134/1000 | Loss: 0.00000903
Iteration 135/1000 | Loss: 0.00000903
Iteration 136/1000 | Loss: 0.00000903
Iteration 137/1000 | Loss: 0.00000903
Iteration 138/1000 | Loss: 0.00000903
Iteration 139/1000 | Loss: 0.00000903
Iteration 140/1000 | Loss: 0.00000903
Iteration 141/1000 | Loss: 0.00000903
Iteration 142/1000 | Loss: 0.00000903
Iteration 143/1000 | Loss: 0.00000903
Iteration 144/1000 | Loss: 0.00000902
Iteration 145/1000 | Loss: 0.00000902
Iteration 146/1000 | Loss: 0.00000902
Iteration 147/1000 | Loss: 0.00000902
Iteration 148/1000 | Loss: 0.00000902
Iteration 149/1000 | Loss: 0.00000902
Iteration 150/1000 | Loss: 0.00000902
Iteration 151/1000 | Loss: 0.00000902
Iteration 152/1000 | Loss: 0.00000902
Iteration 153/1000 | Loss: 0.00000902
Iteration 154/1000 | Loss: 0.00000902
Iteration 155/1000 | Loss: 0.00000902
Iteration 156/1000 | Loss: 0.00000902
Iteration 157/1000 | Loss: 0.00000901
Iteration 158/1000 | Loss: 0.00000901
Iteration 159/1000 | Loss: 0.00000901
Iteration 160/1000 | Loss: 0.00000901
Iteration 161/1000 | Loss: 0.00000901
Iteration 162/1000 | Loss: 0.00000901
Iteration 163/1000 | Loss: 0.00000901
Iteration 164/1000 | Loss: 0.00000901
Iteration 165/1000 | Loss: 0.00000901
Iteration 166/1000 | Loss: 0.00000901
Iteration 167/1000 | Loss: 0.00000901
Iteration 168/1000 | Loss: 0.00000901
Iteration 169/1000 | Loss: 0.00000901
Iteration 170/1000 | Loss: 0.00000901
Iteration 171/1000 | Loss: 0.00000901
Iteration 172/1000 | Loss: 0.00000901
Iteration 173/1000 | Loss: 0.00000901
Iteration 174/1000 | Loss: 0.00000901
Iteration 175/1000 | Loss: 0.00000900
Iteration 176/1000 | Loss: 0.00000900
Iteration 177/1000 | Loss: 0.00000900
Iteration 178/1000 | Loss: 0.00000900
Iteration 179/1000 | Loss: 0.00000900
Iteration 180/1000 | Loss: 0.00000900
Iteration 181/1000 | Loss: 0.00000900
Iteration 182/1000 | Loss: 0.00000900
Iteration 183/1000 | Loss: 0.00000900
Iteration 184/1000 | Loss: 0.00000900
Iteration 185/1000 | Loss: 0.00000900
Iteration 186/1000 | Loss: 0.00000900
Iteration 187/1000 | Loss: 0.00000900
Iteration 188/1000 | Loss: 0.00000900
Iteration 189/1000 | Loss: 0.00000900
Iteration 190/1000 | Loss: 0.00000900
Iteration 191/1000 | Loss: 0.00000899
Iteration 192/1000 | Loss: 0.00000899
Iteration 193/1000 | Loss: 0.00000899
Iteration 194/1000 | Loss: 0.00000899
Iteration 195/1000 | Loss: 0.00000899
Iteration 196/1000 | Loss: 0.00000899
Iteration 197/1000 | Loss: 0.00000899
Iteration 198/1000 | Loss: 0.00000899
Iteration 199/1000 | Loss: 0.00000899
Iteration 200/1000 | Loss: 0.00000899
Iteration 201/1000 | Loss: 0.00000899
Iteration 202/1000 | Loss: 0.00000899
Iteration 203/1000 | Loss: 0.00000899
Iteration 204/1000 | Loss: 0.00000899
Iteration 205/1000 | Loss: 0.00000899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [8.990091373561881e-06, 8.990091373561881e-06, 8.990091373561881e-06, 8.990091373561881e-06, 8.990091373561881e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.990091373561881e-06

Optimization complete. Final v2v error: 2.5834603309631348 mm

Highest mean error: 2.768501043319702 mm for frame 158

Lowest mean error: 2.379793882369995 mm for frame 188

Saving results

Total time: 57.952120304107666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803589
Iteration 2/25 | Loss: 0.00147077
Iteration 3/25 | Loss: 0.00123129
Iteration 4/25 | Loss: 0.00120188
Iteration 5/25 | Loss: 0.00119486
Iteration 6/25 | Loss: 0.00119360
Iteration 7/25 | Loss: 0.00119359
Iteration 8/25 | Loss: 0.00119360
Iteration 9/25 | Loss: 0.00119360
Iteration 10/25 | Loss: 0.00119360
Iteration 11/25 | Loss: 0.00119359
Iteration 12/25 | Loss: 0.00119359
Iteration 13/25 | Loss: 0.00119360
Iteration 14/25 | Loss: 0.00119360
Iteration 15/25 | Loss: 0.00119359
Iteration 16/25 | Loss: 0.00119359
Iteration 17/25 | Loss: 0.00119360
Iteration 18/25 | Loss: 0.00119360
Iteration 19/25 | Loss: 0.00119359
Iteration 20/25 | Loss: 0.00119360
Iteration 21/25 | Loss: 0.00119360
Iteration 22/25 | Loss: 0.00119360
Iteration 23/25 | Loss: 0.00119359
Iteration 24/25 | Loss: 0.00119360
Iteration 25/25 | Loss: 0.00119359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05439055
Iteration 2/25 | Loss: 0.00084289
Iteration 3/25 | Loss: 0.00084289
Iteration 4/25 | Loss: 0.00084289
Iteration 5/25 | Loss: 0.00084289
Iteration 6/25 | Loss: 0.00084289
Iteration 7/25 | Loss: 0.00084289
Iteration 8/25 | Loss: 0.00084289
Iteration 9/25 | Loss: 0.00084289
Iteration 10/25 | Loss: 0.00084289
Iteration 11/25 | Loss: 0.00084289
Iteration 12/25 | Loss: 0.00084289
Iteration 13/25 | Loss: 0.00084289
Iteration 14/25 | Loss: 0.00084289
Iteration 15/25 | Loss: 0.00084289
Iteration 16/25 | Loss: 0.00084289
Iteration 17/25 | Loss: 0.00084289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008428880828432739, 0.0008428880828432739, 0.0008428880828432739, 0.0008428880828432739, 0.0008428880828432739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008428880828432739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084289
Iteration 2/1000 | Loss: 0.00007573
Iteration 3/1000 | Loss: 0.00004903
Iteration 4/1000 | Loss: 0.00003823
Iteration 5/1000 | Loss: 0.00003453
Iteration 6/1000 | Loss: 0.00003291
Iteration 7/1000 | Loss: 0.00003142
Iteration 8/1000 | Loss: 0.00003056
Iteration 9/1000 | Loss: 0.00002974
Iteration 10/1000 | Loss: 0.00002922
Iteration 11/1000 | Loss: 0.00002876
Iteration 12/1000 | Loss: 0.00002838
Iteration 13/1000 | Loss: 0.00002813
Iteration 14/1000 | Loss: 0.00002809
Iteration 15/1000 | Loss: 0.00002786
Iteration 16/1000 | Loss: 0.00002760
Iteration 17/1000 | Loss: 0.00002740
Iteration 18/1000 | Loss: 0.00002726
Iteration 19/1000 | Loss: 0.00002709
Iteration 20/1000 | Loss: 0.00002704
Iteration 21/1000 | Loss: 0.00002700
Iteration 22/1000 | Loss: 0.00002697
Iteration 23/1000 | Loss: 0.00002696
Iteration 24/1000 | Loss: 0.00002696
Iteration 25/1000 | Loss: 0.00002694
Iteration 26/1000 | Loss: 0.00002693
Iteration 27/1000 | Loss: 0.00002692
Iteration 28/1000 | Loss: 0.00002692
Iteration 29/1000 | Loss: 0.00002692
Iteration 30/1000 | Loss: 0.00002691
Iteration 31/1000 | Loss: 0.00002691
Iteration 32/1000 | Loss: 0.00002690
Iteration 33/1000 | Loss: 0.00002690
Iteration 34/1000 | Loss: 0.00002689
Iteration 35/1000 | Loss: 0.00002686
Iteration 36/1000 | Loss: 0.00002683
Iteration 37/1000 | Loss: 0.00002679
Iteration 38/1000 | Loss: 0.00002679
Iteration 39/1000 | Loss: 0.00002678
Iteration 40/1000 | Loss: 0.00002678
Iteration 41/1000 | Loss: 0.00002678
Iteration 42/1000 | Loss: 0.00002678
Iteration 43/1000 | Loss: 0.00002678
Iteration 44/1000 | Loss: 0.00002678
Iteration 45/1000 | Loss: 0.00002678
Iteration 46/1000 | Loss: 0.00002678
Iteration 47/1000 | Loss: 0.00002678
Iteration 48/1000 | Loss: 0.00002678
Iteration 49/1000 | Loss: 0.00002678
Iteration 50/1000 | Loss: 0.00002677
Iteration 51/1000 | Loss: 0.00002677
Iteration 52/1000 | Loss: 0.00002677
Iteration 53/1000 | Loss: 0.00002677
Iteration 54/1000 | Loss: 0.00002677
Iteration 55/1000 | Loss: 0.00002677
Iteration 56/1000 | Loss: 0.00002677
Iteration 57/1000 | Loss: 0.00002673
Iteration 58/1000 | Loss: 0.00002673
Iteration 59/1000 | Loss: 0.00002673
Iteration 60/1000 | Loss: 0.00002673
Iteration 61/1000 | Loss: 0.00002673
Iteration 62/1000 | Loss: 0.00002673
Iteration 63/1000 | Loss: 0.00002673
Iteration 64/1000 | Loss: 0.00002672
Iteration 65/1000 | Loss: 0.00002672
Iteration 66/1000 | Loss: 0.00002672
Iteration 67/1000 | Loss: 0.00002672
Iteration 68/1000 | Loss: 0.00002671
Iteration 69/1000 | Loss: 0.00002671
Iteration 70/1000 | Loss: 0.00002670
Iteration 71/1000 | Loss: 0.00002670
Iteration 72/1000 | Loss: 0.00002669
Iteration 73/1000 | Loss: 0.00002669
Iteration 74/1000 | Loss: 0.00002669
Iteration 75/1000 | Loss: 0.00002669
Iteration 76/1000 | Loss: 0.00002669
Iteration 77/1000 | Loss: 0.00002669
Iteration 78/1000 | Loss: 0.00002669
Iteration 79/1000 | Loss: 0.00002669
Iteration 80/1000 | Loss: 0.00002668
Iteration 81/1000 | Loss: 0.00002668
Iteration 82/1000 | Loss: 0.00002668
Iteration 83/1000 | Loss: 0.00002667
Iteration 84/1000 | Loss: 0.00002667
Iteration 85/1000 | Loss: 0.00002666
Iteration 86/1000 | Loss: 0.00002666
Iteration 87/1000 | Loss: 0.00002663
Iteration 88/1000 | Loss: 0.00002663
Iteration 89/1000 | Loss: 0.00002663
Iteration 90/1000 | Loss: 0.00002663
Iteration 91/1000 | Loss: 0.00002662
Iteration 92/1000 | Loss: 0.00002662
Iteration 93/1000 | Loss: 0.00002662
Iteration 94/1000 | Loss: 0.00002662
Iteration 95/1000 | Loss: 0.00002661
Iteration 96/1000 | Loss: 0.00002661
Iteration 97/1000 | Loss: 0.00002661
Iteration 98/1000 | Loss: 0.00002661
Iteration 99/1000 | Loss: 0.00002660
Iteration 100/1000 | Loss: 0.00002660
Iteration 101/1000 | Loss: 0.00002660
Iteration 102/1000 | Loss: 0.00002660
Iteration 103/1000 | Loss: 0.00002660
Iteration 104/1000 | Loss: 0.00002660
Iteration 105/1000 | Loss: 0.00002660
Iteration 106/1000 | Loss: 0.00002660
Iteration 107/1000 | Loss: 0.00002660
Iteration 108/1000 | Loss: 0.00002660
Iteration 109/1000 | Loss: 0.00002660
Iteration 110/1000 | Loss: 0.00002660
Iteration 111/1000 | Loss: 0.00002660
Iteration 112/1000 | Loss: 0.00002659
Iteration 113/1000 | Loss: 0.00002659
Iteration 114/1000 | Loss: 0.00002659
Iteration 115/1000 | Loss: 0.00002659
Iteration 116/1000 | Loss: 0.00002659
Iteration 117/1000 | Loss: 0.00002659
Iteration 118/1000 | Loss: 0.00002659
Iteration 119/1000 | Loss: 0.00002659
Iteration 120/1000 | Loss: 0.00002658
Iteration 121/1000 | Loss: 0.00002658
Iteration 122/1000 | Loss: 0.00002658
Iteration 123/1000 | Loss: 0.00002658
Iteration 124/1000 | Loss: 0.00002658
Iteration 125/1000 | Loss: 0.00002658
Iteration 126/1000 | Loss: 0.00002658
Iteration 127/1000 | Loss: 0.00002658
Iteration 128/1000 | Loss: 0.00002658
Iteration 129/1000 | Loss: 0.00002657
Iteration 130/1000 | Loss: 0.00002657
Iteration 131/1000 | Loss: 0.00002657
Iteration 132/1000 | Loss: 0.00002656
Iteration 133/1000 | Loss: 0.00002656
Iteration 134/1000 | Loss: 0.00002656
Iteration 135/1000 | Loss: 0.00002656
Iteration 136/1000 | Loss: 0.00002656
Iteration 137/1000 | Loss: 0.00002656
Iteration 138/1000 | Loss: 0.00002655
Iteration 139/1000 | Loss: 0.00002655
Iteration 140/1000 | Loss: 0.00002655
Iteration 141/1000 | Loss: 0.00002655
Iteration 142/1000 | Loss: 0.00002655
Iteration 143/1000 | Loss: 0.00002654
Iteration 144/1000 | Loss: 0.00002654
Iteration 145/1000 | Loss: 0.00002654
Iteration 146/1000 | Loss: 0.00002654
Iteration 147/1000 | Loss: 0.00002653
Iteration 148/1000 | Loss: 0.00002653
Iteration 149/1000 | Loss: 0.00002653
Iteration 150/1000 | Loss: 0.00002653
Iteration 151/1000 | Loss: 0.00002653
Iteration 152/1000 | Loss: 0.00002653
Iteration 153/1000 | Loss: 0.00002653
Iteration 154/1000 | Loss: 0.00002653
Iteration 155/1000 | Loss: 0.00002653
Iteration 156/1000 | Loss: 0.00002653
Iteration 157/1000 | Loss: 0.00002653
Iteration 158/1000 | Loss: 0.00002653
Iteration 159/1000 | Loss: 0.00002653
Iteration 160/1000 | Loss: 0.00002652
Iteration 161/1000 | Loss: 0.00002652
Iteration 162/1000 | Loss: 0.00002652
Iteration 163/1000 | Loss: 0.00002652
Iteration 164/1000 | Loss: 0.00002651
Iteration 165/1000 | Loss: 0.00002651
Iteration 166/1000 | Loss: 0.00002651
Iteration 167/1000 | Loss: 0.00002651
Iteration 168/1000 | Loss: 0.00002651
Iteration 169/1000 | Loss: 0.00002651
Iteration 170/1000 | Loss: 0.00002650
Iteration 171/1000 | Loss: 0.00002650
Iteration 172/1000 | Loss: 0.00002650
Iteration 173/1000 | Loss: 0.00002650
Iteration 174/1000 | Loss: 0.00002650
Iteration 175/1000 | Loss: 0.00002650
Iteration 176/1000 | Loss: 0.00002650
Iteration 177/1000 | Loss: 0.00002650
Iteration 178/1000 | Loss: 0.00002650
Iteration 179/1000 | Loss: 0.00002650
Iteration 180/1000 | Loss: 0.00002650
Iteration 181/1000 | Loss: 0.00002650
Iteration 182/1000 | Loss: 0.00002650
Iteration 183/1000 | Loss: 0.00002650
Iteration 184/1000 | Loss: 0.00002650
Iteration 185/1000 | Loss: 0.00002650
Iteration 186/1000 | Loss: 0.00002650
Iteration 187/1000 | Loss: 0.00002650
Iteration 188/1000 | Loss: 0.00002650
Iteration 189/1000 | Loss: 0.00002650
Iteration 190/1000 | Loss: 0.00002649
Iteration 191/1000 | Loss: 0.00002649
Iteration 192/1000 | Loss: 0.00002649
Iteration 193/1000 | Loss: 0.00002649
Iteration 194/1000 | Loss: 0.00002649
Iteration 195/1000 | Loss: 0.00002649
Iteration 196/1000 | Loss: 0.00002649
Iteration 197/1000 | Loss: 0.00002649
Iteration 198/1000 | Loss: 0.00002649
Iteration 199/1000 | Loss: 0.00002649
Iteration 200/1000 | Loss: 0.00002649
Iteration 201/1000 | Loss: 0.00002649
Iteration 202/1000 | Loss: 0.00002649
Iteration 203/1000 | Loss: 0.00002649
Iteration 204/1000 | Loss: 0.00002649
Iteration 205/1000 | Loss: 0.00002649
Iteration 206/1000 | Loss: 0.00002649
Iteration 207/1000 | Loss: 0.00002649
Iteration 208/1000 | Loss: 0.00002649
Iteration 209/1000 | Loss: 0.00002649
Iteration 210/1000 | Loss: 0.00002649
Iteration 211/1000 | Loss: 0.00002649
Iteration 212/1000 | Loss: 0.00002649
Iteration 213/1000 | Loss: 0.00002649
Iteration 214/1000 | Loss: 0.00002649
Iteration 215/1000 | Loss: 0.00002649
Iteration 216/1000 | Loss: 0.00002649
Iteration 217/1000 | Loss: 0.00002649
Iteration 218/1000 | Loss: 0.00002649
Iteration 219/1000 | Loss: 0.00002649
Iteration 220/1000 | Loss: 0.00002649
Iteration 221/1000 | Loss: 0.00002649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.6487203285796568e-05, 2.6487203285796568e-05, 2.6487203285796568e-05, 2.6487203285796568e-05, 2.6487203285796568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6487203285796568e-05

Optimization complete. Final v2v error: 4.133883476257324 mm

Highest mean error: 5.453742504119873 mm for frame 35

Lowest mean error: 2.994581460952759 mm for frame 0

Saving results

Total time: 58.09056353569031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011950
Iteration 2/25 | Loss: 0.00180130
Iteration 3/25 | Loss: 0.00132787
Iteration 4/25 | Loss: 0.00125681
Iteration 5/25 | Loss: 0.00120792
Iteration 6/25 | Loss: 0.00119428
Iteration 7/25 | Loss: 0.00119089
Iteration 8/25 | Loss: 0.00119149
Iteration 9/25 | Loss: 0.00119019
Iteration 10/25 | Loss: 0.00118975
Iteration 11/25 | Loss: 0.00118975
Iteration 12/25 | Loss: 0.00118975
Iteration 13/25 | Loss: 0.00118975
Iteration 14/25 | Loss: 0.00118975
Iteration 15/25 | Loss: 0.00118975
Iteration 16/25 | Loss: 0.00118975
Iteration 17/25 | Loss: 0.00118974
Iteration 18/25 | Loss: 0.00118974
Iteration 19/25 | Loss: 0.00118974
Iteration 20/25 | Loss: 0.00118974
Iteration 21/25 | Loss: 0.00118974
Iteration 22/25 | Loss: 0.00118974
Iteration 23/25 | Loss: 0.00118974
Iteration 24/25 | Loss: 0.00118974
Iteration 25/25 | Loss: 0.00118974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32985473
Iteration 2/25 | Loss: 0.00079101
Iteration 3/25 | Loss: 0.00079101
Iteration 4/25 | Loss: 0.00079101
Iteration 5/25 | Loss: 0.00079101
Iteration 6/25 | Loss: 0.00079101
Iteration 7/25 | Loss: 0.00079101
Iteration 8/25 | Loss: 0.00079101
Iteration 9/25 | Loss: 0.00079101
Iteration 10/25 | Loss: 0.00079101
Iteration 11/25 | Loss: 0.00079101
Iteration 12/25 | Loss: 0.00079101
Iteration 13/25 | Loss: 0.00079101
Iteration 14/25 | Loss: 0.00079101
Iteration 15/25 | Loss: 0.00079101
Iteration 16/25 | Loss: 0.00079101
Iteration 17/25 | Loss: 0.00079101
Iteration 18/25 | Loss: 0.00079101
Iteration 19/25 | Loss: 0.00079101
Iteration 20/25 | Loss: 0.00079101
Iteration 21/25 | Loss: 0.00079101
Iteration 22/25 | Loss: 0.00079101
Iteration 23/25 | Loss: 0.00079101
Iteration 24/25 | Loss: 0.00079101
Iteration 25/25 | Loss: 0.00079101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079101
Iteration 2/1000 | Loss: 0.00003772
Iteration 3/1000 | Loss: 0.00002738
Iteration 4/1000 | Loss: 0.00002497
Iteration 5/1000 | Loss: 0.00002496
Iteration 6/1000 | Loss: 0.00002349
Iteration 7/1000 | Loss: 0.00002311
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002205
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002184
Iteration 14/1000 | Loss: 0.00002183
Iteration 15/1000 | Loss: 0.00002183
Iteration 16/1000 | Loss: 0.00002182
Iteration 17/1000 | Loss: 0.00015215
Iteration 18/1000 | Loss: 0.00002268
Iteration 19/1000 | Loss: 0.00002133
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00002047
Iteration 22/1000 | Loss: 0.00002039
Iteration 23/1000 | Loss: 0.00002039
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002032
Iteration 26/1000 | Loss: 0.00002032
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002032
Iteration 29/1000 | Loss: 0.00002032
Iteration 30/1000 | Loss: 0.00002032
Iteration 31/1000 | Loss: 0.00002031
Iteration 32/1000 | Loss: 0.00002031
Iteration 33/1000 | Loss: 0.00002030
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00002030
Iteration 36/1000 | Loss: 0.00002030
Iteration 37/1000 | Loss: 0.00002030
Iteration 38/1000 | Loss: 0.00002030
Iteration 39/1000 | Loss: 0.00002030
Iteration 40/1000 | Loss: 0.00002030
Iteration 41/1000 | Loss: 0.00002030
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00002029
Iteration 44/1000 | Loss: 0.00002029
Iteration 45/1000 | Loss: 0.00002029
Iteration 46/1000 | Loss: 0.00002029
Iteration 47/1000 | Loss: 0.00002028
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00002028
Iteration 50/1000 | Loss: 0.00002028
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002028
Iteration 53/1000 | Loss: 0.00002028
Iteration 54/1000 | Loss: 0.00002028
Iteration 55/1000 | Loss: 0.00002028
Iteration 56/1000 | Loss: 0.00002028
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002027
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002027
Iteration 73/1000 | Loss: 0.00002027
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002027
Iteration 82/1000 | Loss: 0.00002027
Iteration 83/1000 | Loss: 0.00002027
Iteration 84/1000 | Loss: 0.00002027
Iteration 85/1000 | Loss: 0.00002027
Iteration 86/1000 | Loss: 0.00002027
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002027
Iteration 95/1000 | Loss: 0.00002027
Iteration 96/1000 | Loss: 0.00002027
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002027
Iteration 99/1000 | Loss: 0.00002027
Iteration 100/1000 | Loss: 0.00002027
Iteration 101/1000 | Loss: 0.00002027
Iteration 102/1000 | Loss: 0.00002027
Iteration 103/1000 | Loss: 0.00002027
Iteration 104/1000 | Loss: 0.00002027
Iteration 105/1000 | Loss: 0.00002027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.02659830392804e-05, 2.02659830392804e-05, 2.02659830392804e-05, 2.02659830392804e-05, 2.02659830392804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.02659830392804e-05

Optimization complete. Final v2v error: 3.783734083175659 mm

Highest mean error: 4.60653829574585 mm for frame 103

Lowest mean error: 3.618415594100952 mm for frame 96

Saving results

Total time: 46.277883529663086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652490
Iteration 2/25 | Loss: 0.00120616
Iteration 3/25 | Loss: 0.00113376
Iteration 4/25 | Loss: 0.00112425
Iteration 5/25 | Loss: 0.00112169
Iteration 6/25 | Loss: 0.00112145
Iteration 7/25 | Loss: 0.00112145
Iteration 8/25 | Loss: 0.00112145
Iteration 9/25 | Loss: 0.00112145
Iteration 10/25 | Loss: 0.00112145
Iteration 11/25 | Loss: 0.00112145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011214526602998376, 0.0011214526602998376, 0.0011214526602998376, 0.0011214526602998376, 0.0011214526602998376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011214526602998376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06348610
Iteration 2/25 | Loss: 0.00081762
Iteration 3/25 | Loss: 0.00081762
Iteration 4/25 | Loss: 0.00081762
Iteration 5/25 | Loss: 0.00081762
Iteration 6/25 | Loss: 0.00081762
Iteration 7/25 | Loss: 0.00081762
Iteration 8/25 | Loss: 0.00081762
Iteration 9/25 | Loss: 0.00081762
Iteration 10/25 | Loss: 0.00081762
Iteration 11/25 | Loss: 0.00081762
Iteration 12/25 | Loss: 0.00081762
Iteration 13/25 | Loss: 0.00081762
Iteration 14/25 | Loss: 0.00081762
Iteration 15/25 | Loss: 0.00081762
Iteration 16/25 | Loss: 0.00081762
Iteration 17/25 | Loss: 0.00081762
Iteration 18/25 | Loss: 0.00081762
Iteration 19/25 | Loss: 0.00081762
Iteration 20/25 | Loss: 0.00081762
Iteration 21/25 | Loss: 0.00081762
Iteration 22/25 | Loss: 0.00081762
Iteration 23/25 | Loss: 0.00081762
Iteration 24/25 | Loss: 0.00081762
Iteration 25/25 | Loss: 0.00081762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008176208939403296, 0.0008176208939403296, 0.0008176208939403296, 0.0008176208939403296, 0.0008176208939403296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008176208939403296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081762
Iteration 2/1000 | Loss: 0.00002841
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001582
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001486
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001434
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001431
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001425
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001422
Iteration 60/1000 | Loss: 0.00001422
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001421
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001417
Iteration 81/1000 | Loss: 0.00001417
Iteration 82/1000 | Loss: 0.00001417
Iteration 83/1000 | Loss: 0.00001417
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001416
Iteration 88/1000 | Loss: 0.00001416
Iteration 89/1000 | Loss: 0.00001416
Iteration 90/1000 | Loss: 0.00001416
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001415
Iteration 93/1000 | Loss: 0.00001415
Iteration 94/1000 | Loss: 0.00001415
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001413
Iteration 99/1000 | Loss: 0.00001413
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001413
Iteration 102/1000 | Loss: 0.00001413
Iteration 103/1000 | Loss: 0.00001412
Iteration 104/1000 | Loss: 0.00001412
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001411
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001409
Iteration 119/1000 | Loss: 0.00001409
Iteration 120/1000 | Loss: 0.00001409
Iteration 121/1000 | Loss: 0.00001409
Iteration 122/1000 | Loss: 0.00001409
Iteration 123/1000 | Loss: 0.00001408
Iteration 124/1000 | Loss: 0.00001408
Iteration 125/1000 | Loss: 0.00001408
Iteration 126/1000 | Loss: 0.00001408
Iteration 127/1000 | Loss: 0.00001408
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001407
Iteration 130/1000 | Loss: 0.00001407
Iteration 131/1000 | Loss: 0.00001407
Iteration 132/1000 | Loss: 0.00001407
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001407
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001406
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001406
Iteration 140/1000 | Loss: 0.00001406
Iteration 141/1000 | Loss: 0.00001406
Iteration 142/1000 | Loss: 0.00001406
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001406
Iteration 150/1000 | Loss: 0.00001406
Iteration 151/1000 | Loss: 0.00001406
Iteration 152/1000 | Loss: 0.00001406
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Iteration 168/1000 | Loss: 0.00001406
Iteration 169/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.405550392519217e-05, 1.405550392519217e-05, 1.405550392519217e-05, 1.405550392519217e-05, 1.405550392519217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.405550392519217e-05

Optimization complete. Final v2v error: 3.2243552207946777 mm

Highest mean error: 3.5989739894866943 mm for frame 52

Lowest mean error: 3.0112192630767822 mm for frame 153

Saving results

Total time: 39.21954846382141
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843394
Iteration 2/25 | Loss: 0.00139712
Iteration 3/25 | Loss: 0.00121520
Iteration 4/25 | Loss: 0.00118707
Iteration 5/25 | Loss: 0.00117847
Iteration 6/25 | Loss: 0.00117640
Iteration 7/25 | Loss: 0.00117640
Iteration 8/25 | Loss: 0.00117640
Iteration 9/25 | Loss: 0.00117640
Iteration 10/25 | Loss: 0.00117640
Iteration 11/25 | Loss: 0.00117640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011763987131416798, 0.0011763987131416798, 0.0011763987131416798, 0.0011763987131416798, 0.0011763987131416798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011763987131416798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.96274900
Iteration 2/25 | Loss: 0.00104400
Iteration 3/25 | Loss: 0.00104380
Iteration 4/25 | Loss: 0.00104380
Iteration 5/25 | Loss: 0.00104380
Iteration 6/25 | Loss: 0.00104380
Iteration 7/25 | Loss: 0.00104380
Iteration 8/25 | Loss: 0.00104380
Iteration 9/25 | Loss: 0.00104380
Iteration 10/25 | Loss: 0.00104380
Iteration 11/25 | Loss: 0.00104380
Iteration 12/25 | Loss: 0.00104380
Iteration 13/25 | Loss: 0.00104380
Iteration 14/25 | Loss: 0.00104380
Iteration 15/25 | Loss: 0.00104380
Iteration 16/25 | Loss: 0.00104380
Iteration 17/25 | Loss: 0.00104380
Iteration 18/25 | Loss: 0.00104380
Iteration 19/25 | Loss: 0.00104380
Iteration 20/25 | Loss: 0.00104380
Iteration 21/25 | Loss: 0.00104380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010437958408147097, 0.0010437958408147097, 0.0010437958408147097, 0.0010437958408147097, 0.0010437958408147097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010437958408147097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104380
Iteration 2/1000 | Loss: 0.00005882
Iteration 3/1000 | Loss: 0.00003972
Iteration 4/1000 | Loss: 0.00003406
Iteration 5/1000 | Loss: 0.00003171
Iteration 6/1000 | Loss: 0.00003028
Iteration 7/1000 | Loss: 0.00002916
Iteration 8/1000 | Loss: 0.00002831
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002720
Iteration 11/1000 | Loss: 0.00002683
Iteration 12/1000 | Loss: 0.00002648
Iteration 13/1000 | Loss: 0.00002625
Iteration 14/1000 | Loss: 0.00002601
Iteration 15/1000 | Loss: 0.00002583
Iteration 16/1000 | Loss: 0.00002578
Iteration 17/1000 | Loss: 0.00002572
Iteration 18/1000 | Loss: 0.00002560
Iteration 19/1000 | Loss: 0.00002558
Iteration 20/1000 | Loss: 0.00002555
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00002554
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002553
Iteration 25/1000 | Loss: 0.00002552
Iteration 26/1000 | Loss: 0.00002552
Iteration 27/1000 | Loss: 0.00002551
Iteration 28/1000 | Loss: 0.00002549
Iteration 29/1000 | Loss: 0.00002547
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002545
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002544
Iteration 34/1000 | Loss: 0.00002544
Iteration 35/1000 | Loss: 0.00002544
Iteration 36/1000 | Loss: 0.00002543
Iteration 37/1000 | Loss: 0.00002543
Iteration 38/1000 | Loss: 0.00002543
Iteration 39/1000 | Loss: 0.00002543
Iteration 40/1000 | Loss: 0.00002543
Iteration 41/1000 | Loss: 0.00002542
Iteration 42/1000 | Loss: 0.00002542
Iteration 43/1000 | Loss: 0.00002542
Iteration 44/1000 | Loss: 0.00002542
Iteration 45/1000 | Loss: 0.00002541
Iteration 46/1000 | Loss: 0.00002541
Iteration 47/1000 | Loss: 0.00002541
Iteration 48/1000 | Loss: 0.00002541
Iteration 49/1000 | Loss: 0.00002540
Iteration 50/1000 | Loss: 0.00002540
Iteration 51/1000 | Loss: 0.00002539
Iteration 52/1000 | Loss: 0.00002539
Iteration 53/1000 | Loss: 0.00002539
Iteration 54/1000 | Loss: 0.00002538
Iteration 55/1000 | Loss: 0.00002538
Iteration 56/1000 | Loss: 0.00002538
Iteration 57/1000 | Loss: 0.00002537
Iteration 58/1000 | Loss: 0.00002537
Iteration 59/1000 | Loss: 0.00002536
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002535
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002535
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002533
Iteration 70/1000 | Loss: 0.00002533
Iteration 71/1000 | Loss: 0.00002532
Iteration 72/1000 | Loss: 0.00002532
Iteration 73/1000 | Loss: 0.00002532
Iteration 74/1000 | Loss: 0.00002531
Iteration 75/1000 | Loss: 0.00002531
Iteration 76/1000 | Loss: 0.00002531
Iteration 77/1000 | Loss: 0.00002530
Iteration 78/1000 | Loss: 0.00002530
Iteration 79/1000 | Loss: 0.00002530
Iteration 80/1000 | Loss: 0.00002530
Iteration 81/1000 | Loss: 0.00002529
Iteration 82/1000 | Loss: 0.00002529
Iteration 83/1000 | Loss: 0.00002529
Iteration 84/1000 | Loss: 0.00002528
Iteration 85/1000 | Loss: 0.00002528
Iteration 86/1000 | Loss: 0.00002528
Iteration 87/1000 | Loss: 0.00002528
Iteration 88/1000 | Loss: 0.00002528
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002527
Iteration 91/1000 | Loss: 0.00002527
Iteration 92/1000 | Loss: 0.00002527
Iteration 93/1000 | Loss: 0.00002527
Iteration 94/1000 | Loss: 0.00002527
Iteration 95/1000 | Loss: 0.00002526
Iteration 96/1000 | Loss: 0.00002526
Iteration 97/1000 | Loss: 0.00002526
Iteration 98/1000 | Loss: 0.00002525
Iteration 99/1000 | Loss: 0.00002525
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002524
Iteration 102/1000 | Loss: 0.00002524
Iteration 103/1000 | Loss: 0.00002523
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002523
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002522
Iteration 109/1000 | Loss: 0.00002521
Iteration 110/1000 | Loss: 0.00002521
Iteration 111/1000 | Loss: 0.00002521
Iteration 112/1000 | Loss: 0.00002520
Iteration 113/1000 | Loss: 0.00002520
Iteration 114/1000 | Loss: 0.00002520
Iteration 115/1000 | Loss: 0.00002520
Iteration 116/1000 | Loss: 0.00002519
Iteration 117/1000 | Loss: 0.00002519
Iteration 118/1000 | Loss: 0.00002519
Iteration 119/1000 | Loss: 0.00002518
Iteration 120/1000 | Loss: 0.00002518
Iteration 121/1000 | Loss: 0.00002517
Iteration 122/1000 | Loss: 0.00002517
Iteration 123/1000 | Loss: 0.00002517
Iteration 124/1000 | Loss: 0.00002516
Iteration 125/1000 | Loss: 0.00002516
Iteration 126/1000 | Loss: 0.00002516
Iteration 127/1000 | Loss: 0.00002516
Iteration 128/1000 | Loss: 0.00002515
Iteration 129/1000 | Loss: 0.00002515
Iteration 130/1000 | Loss: 0.00002515
Iteration 131/1000 | Loss: 0.00002515
Iteration 132/1000 | Loss: 0.00002515
Iteration 133/1000 | Loss: 0.00002514
Iteration 134/1000 | Loss: 0.00002514
Iteration 135/1000 | Loss: 0.00002514
Iteration 136/1000 | Loss: 0.00002513
Iteration 137/1000 | Loss: 0.00002513
Iteration 138/1000 | Loss: 0.00002513
Iteration 139/1000 | Loss: 0.00002512
Iteration 140/1000 | Loss: 0.00002512
Iteration 141/1000 | Loss: 0.00002512
Iteration 142/1000 | Loss: 0.00002512
Iteration 143/1000 | Loss: 0.00002511
Iteration 144/1000 | Loss: 0.00002511
Iteration 145/1000 | Loss: 0.00002511
Iteration 146/1000 | Loss: 0.00002511
Iteration 147/1000 | Loss: 0.00002511
Iteration 148/1000 | Loss: 0.00002510
Iteration 149/1000 | Loss: 0.00002510
Iteration 150/1000 | Loss: 0.00002510
Iteration 151/1000 | Loss: 0.00002510
Iteration 152/1000 | Loss: 0.00002510
Iteration 153/1000 | Loss: 0.00002510
Iteration 154/1000 | Loss: 0.00002509
Iteration 155/1000 | Loss: 0.00002509
Iteration 156/1000 | Loss: 0.00002509
Iteration 157/1000 | Loss: 0.00002509
Iteration 158/1000 | Loss: 0.00002509
Iteration 159/1000 | Loss: 0.00002509
Iteration 160/1000 | Loss: 0.00002509
Iteration 161/1000 | Loss: 0.00002509
Iteration 162/1000 | Loss: 0.00002509
Iteration 163/1000 | Loss: 0.00002508
Iteration 164/1000 | Loss: 0.00002508
Iteration 165/1000 | Loss: 0.00002508
Iteration 166/1000 | Loss: 0.00002508
Iteration 167/1000 | Loss: 0.00002508
Iteration 168/1000 | Loss: 0.00002508
Iteration 169/1000 | Loss: 0.00002507
Iteration 170/1000 | Loss: 0.00002507
Iteration 171/1000 | Loss: 0.00002507
Iteration 172/1000 | Loss: 0.00002507
Iteration 173/1000 | Loss: 0.00002507
Iteration 174/1000 | Loss: 0.00002507
Iteration 175/1000 | Loss: 0.00002507
Iteration 176/1000 | Loss: 0.00002506
Iteration 177/1000 | Loss: 0.00002506
Iteration 178/1000 | Loss: 0.00002506
Iteration 179/1000 | Loss: 0.00002506
Iteration 180/1000 | Loss: 0.00002505
Iteration 181/1000 | Loss: 0.00002505
Iteration 182/1000 | Loss: 0.00002505
Iteration 183/1000 | Loss: 0.00002505
Iteration 184/1000 | Loss: 0.00002505
Iteration 185/1000 | Loss: 0.00002505
Iteration 186/1000 | Loss: 0.00002504
Iteration 187/1000 | Loss: 0.00002504
Iteration 188/1000 | Loss: 0.00002504
Iteration 189/1000 | Loss: 0.00002504
Iteration 190/1000 | Loss: 0.00002504
Iteration 191/1000 | Loss: 0.00002504
Iteration 192/1000 | Loss: 0.00002504
Iteration 193/1000 | Loss: 0.00002504
Iteration 194/1000 | Loss: 0.00002503
Iteration 195/1000 | Loss: 0.00002503
Iteration 196/1000 | Loss: 0.00002503
Iteration 197/1000 | Loss: 0.00002503
Iteration 198/1000 | Loss: 0.00002503
Iteration 199/1000 | Loss: 0.00002503
Iteration 200/1000 | Loss: 0.00002503
Iteration 201/1000 | Loss: 0.00002503
Iteration 202/1000 | Loss: 0.00002503
Iteration 203/1000 | Loss: 0.00002503
Iteration 204/1000 | Loss: 0.00002503
Iteration 205/1000 | Loss: 0.00002503
Iteration 206/1000 | Loss: 0.00002503
Iteration 207/1000 | Loss: 0.00002503
Iteration 208/1000 | Loss: 0.00002502
Iteration 209/1000 | Loss: 0.00002502
Iteration 210/1000 | Loss: 0.00002502
Iteration 211/1000 | Loss: 0.00002502
Iteration 212/1000 | Loss: 0.00002502
Iteration 213/1000 | Loss: 0.00002501
Iteration 214/1000 | Loss: 0.00002501
Iteration 215/1000 | Loss: 0.00002501
Iteration 216/1000 | Loss: 0.00002501
Iteration 217/1000 | Loss: 0.00002501
Iteration 218/1000 | Loss: 0.00002501
Iteration 219/1000 | Loss: 0.00002501
Iteration 220/1000 | Loss: 0.00002501
Iteration 221/1000 | Loss: 0.00002501
Iteration 222/1000 | Loss: 0.00002501
Iteration 223/1000 | Loss: 0.00002501
Iteration 224/1000 | Loss: 0.00002501
Iteration 225/1000 | Loss: 0.00002501
Iteration 226/1000 | Loss: 0.00002501
Iteration 227/1000 | Loss: 0.00002501
Iteration 228/1000 | Loss: 0.00002501
Iteration 229/1000 | Loss: 0.00002500
Iteration 230/1000 | Loss: 0.00002500
Iteration 231/1000 | Loss: 0.00002500
Iteration 232/1000 | Loss: 0.00002500
Iteration 233/1000 | Loss: 0.00002500
Iteration 234/1000 | Loss: 0.00002500
Iteration 235/1000 | Loss: 0.00002500
Iteration 236/1000 | Loss: 0.00002500
Iteration 237/1000 | Loss: 0.00002500
Iteration 238/1000 | Loss: 0.00002500
Iteration 239/1000 | Loss: 0.00002500
Iteration 240/1000 | Loss: 0.00002500
Iteration 241/1000 | Loss: 0.00002500
Iteration 242/1000 | Loss: 0.00002500
Iteration 243/1000 | Loss: 0.00002499
Iteration 244/1000 | Loss: 0.00002499
Iteration 245/1000 | Loss: 0.00002499
Iteration 246/1000 | Loss: 0.00002499
Iteration 247/1000 | Loss: 0.00002499
Iteration 248/1000 | Loss: 0.00002499
Iteration 249/1000 | Loss: 0.00002499
Iteration 250/1000 | Loss: 0.00002499
Iteration 251/1000 | Loss: 0.00002499
Iteration 252/1000 | Loss: 0.00002499
Iteration 253/1000 | Loss: 0.00002499
Iteration 254/1000 | Loss: 0.00002499
Iteration 255/1000 | Loss: 0.00002499
Iteration 256/1000 | Loss: 0.00002499
Iteration 257/1000 | Loss: 0.00002499
Iteration 258/1000 | Loss: 0.00002499
Iteration 259/1000 | Loss: 0.00002499
Iteration 260/1000 | Loss: 0.00002499
Iteration 261/1000 | Loss: 0.00002499
Iteration 262/1000 | Loss: 0.00002499
Iteration 263/1000 | Loss: 0.00002499
Iteration 264/1000 | Loss: 0.00002499
Iteration 265/1000 | Loss: 0.00002499
Iteration 266/1000 | Loss: 0.00002498
Iteration 267/1000 | Loss: 0.00002498
Iteration 268/1000 | Loss: 0.00002498
Iteration 269/1000 | Loss: 0.00002498
Iteration 270/1000 | Loss: 0.00002498
Iteration 271/1000 | Loss: 0.00002498
Iteration 272/1000 | Loss: 0.00002498
Iteration 273/1000 | Loss: 0.00002498
Iteration 274/1000 | Loss: 0.00002498
Iteration 275/1000 | Loss: 0.00002498
Iteration 276/1000 | Loss: 0.00002498
Iteration 277/1000 | Loss: 0.00002498
Iteration 278/1000 | Loss: 0.00002498
Iteration 279/1000 | Loss: 0.00002498
Iteration 280/1000 | Loss: 0.00002498
Iteration 281/1000 | Loss: 0.00002498
Iteration 282/1000 | Loss: 0.00002498
Iteration 283/1000 | Loss: 0.00002498
Iteration 284/1000 | Loss: 0.00002498
Iteration 285/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.498354842828121e-05, 2.498354842828121e-05, 2.498354842828121e-05, 2.498354842828121e-05, 2.498354842828121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.498354842828121e-05

Optimization complete. Final v2v error: 4.105321407318115 mm

Highest mean error: 6.404265403747559 mm for frame 130

Lowest mean error: 2.930434465408325 mm for frame 153

Saving results

Total time: 60.66179084777832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484480
Iteration 2/25 | Loss: 0.00117644
Iteration 3/25 | Loss: 0.00109638
Iteration 4/25 | Loss: 0.00108202
Iteration 5/25 | Loss: 0.00107674
Iteration 6/25 | Loss: 0.00107605
Iteration 7/25 | Loss: 0.00107605
Iteration 8/25 | Loss: 0.00107605
Iteration 9/25 | Loss: 0.00107605
Iteration 10/25 | Loss: 0.00107605
Iteration 11/25 | Loss: 0.00107605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010760468430817127, 0.0010760468430817127, 0.0010760468430817127, 0.0010760468430817127, 0.0010760468430817127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010760468430817127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46403003
Iteration 2/25 | Loss: 0.00080568
Iteration 3/25 | Loss: 0.00080568
Iteration 4/25 | Loss: 0.00080568
Iteration 5/25 | Loss: 0.00080568
Iteration 6/25 | Loss: 0.00080568
Iteration 7/25 | Loss: 0.00080568
Iteration 8/25 | Loss: 0.00080568
Iteration 9/25 | Loss: 0.00080568
Iteration 10/25 | Loss: 0.00080568
Iteration 11/25 | Loss: 0.00080568
Iteration 12/25 | Loss: 0.00080568
Iteration 13/25 | Loss: 0.00080568
Iteration 14/25 | Loss: 0.00080568
Iteration 15/25 | Loss: 0.00080568
Iteration 16/25 | Loss: 0.00080568
Iteration 17/25 | Loss: 0.00080568
Iteration 18/25 | Loss: 0.00080568
Iteration 19/25 | Loss: 0.00080568
Iteration 20/25 | Loss: 0.00080568
Iteration 21/25 | Loss: 0.00080568
Iteration 22/25 | Loss: 0.00080568
Iteration 23/25 | Loss: 0.00080568
Iteration 24/25 | Loss: 0.00080568
Iteration 25/25 | Loss: 0.00080568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080568
Iteration 2/1000 | Loss: 0.00002002
Iteration 3/1000 | Loss: 0.00001485
Iteration 4/1000 | Loss: 0.00001351
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001243
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001182
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001144
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001092
Iteration 30/1000 | Loss: 0.00001091
Iteration 31/1000 | Loss: 0.00001091
Iteration 32/1000 | Loss: 0.00001090
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001081
Iteration 39/1000 | Loss: 0.00001081
Iteration 40/1000 | Loss: 0.00001080
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001077
Iteration 44/1000 | Loss: 0.00001072
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001062
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001062
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001059
Iteration 66/1000 | Loss: 0.00001059
Iteration 67/1000 | Loss: 0.00001059
Iteration 68/1000 | Loss: 0.00001058
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001058
Iteration 72/1000 | Loss: 0.00001058
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001057
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001056
Iteration 81/1000 | Loss: 0.00001056
Iteration 82/1000 | Loss: 0.00001055
Iteration 83/1000 | Loss: 0.00001055
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001055
Iteration 86/1000 | Loss: 0.00001055
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001054
Iteration 89/1000 | Loss: 0.00001054
Iteration 90/1000 | Loss: 0.00001054
Iteration 91/1000 | Loss: 0.00001054
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001053
Iteration 94/1000 | Loss: 0.00001053
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001052
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001051
Iteration 100/1000 | Loss: 0.00001051
Iteration 101/1000 | Loss: 0.00001051
Iteration 102/1000 | Loss: 0.00001050
Iteration 103/1000 | Loss: 0.00001049
Iteration 104/1000 | Loss: 0.00001049
Iteration 105/1000 | Loss: 0.00001049
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001048
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001047
Iteration 110/1000 | Loss: 0.00001047
Iteration 111/1000 | Loss: 0.00001047
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001046
Iteration 118/1000 | Loss: 0.00001046
Iteration 119/1000 | Loss: 0.00001046
Iteration 120/1000 | Loss: 0.00001046
Iteration 121/1000 | Loss: 0.00001045
Iteration 122/1000 | Loss: 0.00001045
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001045
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00001045
Iteration 127/1000 | Loss: 0.00001045
Iteration 128/1000 | Loss: 0.00001045
Iteration 129/1000 | Loss: 0.00001044
Iteration 130/1000 | Loss: 0.00001044
Iteration 131/1000 | Loss: 0.00001044
Iteration 132/1000 | Loss: 0.00001044
Iteration 133/1000 | Loss: 0.00001044
Iteration 134/1000 | Loss: 0.00001044
Iteration 135/1000 | Loss: 0.00001044
Iteration 136/1000 | Loss: 0.00001044
Iteration 137/1000 | Loss: 0.00001044
Iteration 138/1000 | Loss: 0.00001044
Iteration 139/1000 | Loss: 0.00001044
Iteration 140/1000 | Loss: 0.00001044
Iteration 141/1000 | Loss: 0.00001044
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001042
Iteration 147/1000 | Loss: 0.00001042
Iteration 148/1000 | Loss: 0.00001042
Iteration 149/1000 | Loss: 0.00001042
Iteration 150/1000 | Loss: 0.00001042
Iteration 151/1000 | Loss: 0.00001042
Iteration 152/1000 | Loss: 0.00001042
Iteration 153/1000 | Loss: 0.00001042
Iteration 154/1000 | Loss: 0.00001042
Iteration 155/1000 | Loss: 0.00001042
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001041
Iteration 160/1000 | Loss: 0.00001041
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001040
Iteration 163/1000 | Loss: 0.00001040
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001040
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001039
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001038
Iteration 177/1000 | Loss: 0.00001038
Iteration 178/1000 | Loss: 0.00001038
Iteration 179/1000 | Loss: 0.00001038
Iteration 180/1000 | Loss: 0.00001038
Iteration 181/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.03832280728966e-05, 1.03832280728966e-05, 1.03832280728966e-05, 1.03832280728966e-05, 1.03832280728966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.03832280728966e-05

Optimization complete. Final v2v error: 2.8075332641601562 mm

Highest mean error: 3.033121347427368 mm for frame 169

Lowest mean error: 2.713280439376831 mm for frame 151

Saving results

Total time: 43.423569440841675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773429
Iteration 2/25 | Loss: 0.00127696
Iteration 3/25 | Loss: 0.00110539
Iteration 4/25 | Loss: 0.00109030
Iteration 5/25 | Loss: 0.00108755
Iteration 6/25 | Loss: 0.00108719
Iteration 7/25 | Loss: 0.00108719
Iteration 8/25 | Loss: 0.00108719
Iteration 9/25 | Loss: 0.00108719
Iteration 10/25 | Loss: 0.00108719
Iteration 11/25 | Loss: 0.00108719
Iteration 12/25 | Loss: 0.00108719
Iteration 13/25 | Loss: 0.00108719
Iteration 14/25 | Loss: 0.00108719
Iteration 15/25 | Loss: 0.00108719
Iteration 16/25 | Loss: 0.00108719
Iteration 17/25 | Loss: 0.00108719
Iteration 18/25 | Loss: 0.00108719
Iteration 19/25 | Loss: 0.00108719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010871869744732976, 0.0010871869744732976, 0.0010871869744732976, 0.0010871869744732976, 0.0010871869744732976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010871869744732976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35709751
Iteration 2/25 | Loss: 0.00085155
Iteration 3/25 | Loss: 0.00085155
Iteration 4/25 | Loss: 0.00085155
Iteration 5/25 | Loss: 0.00085155
Iteration 6/25 | Loss: 0.00085155
Iteration 7/25 | Loss: 0.00085154
Iteration 8/25 | Loss: 0.00085154
Iteration 9/25 | Loss: 0.00085154
Iteration 10/25 | Loss: 0.00085154
Iteration 11/25 | Loss: 0.00085154
Iteration 12/25 | Loss: 0.00085154
Iteration 13/25 | Loss: 0.00085154
Iteration 14/25 | Loss: 0.00085154
Iteration 15/25 | Loss: 0.00085154
Iteration 16/25 | Loss: 0.00085154
Iteration 17/25 | Loss: 0.00085154
Iteration 18/25 | Loss: 0.00085154
Iteration 19/25 | Loss: 0.00085154
Iteration 20/25 | Loss: 0.00085154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008515435038134456, 0.0008515435038134456, 0.0008515435038134456, 0.0008515435038134456, 0.0008515435038134456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008515435038134456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085154
Iteration 2/1000 | Loss: 0.00002136
Iteration 3/1000 | Loss: 0.00001340
Iteration 4/1000 | Loss: 0.00001196
Iteration 5/1000 | Loss: 0.00001113
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001018
Iteration 8/1000 | Loss: 0.00000997
Iteration 9/1000 | Loss: 0.00000972
Iteration 10/1000 | Loss: 0.00000945
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000940
Iteration 13/1000 | Loss: 0.00000938
Iteration 14/1000 | Loss: 0.00000937
Iteration 15/1000 | Loss: 0.00000937
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000936
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000935
Iteration 20/1000 | Loss: 0.00000935
Iteration 21/1000 | Loss: 0.00000934
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000929
Iteration 25/1000 | Loss: 0.00000929
Iteration 26/1000 | Loss: 0.00000928
Iteration 27/1000 | Loss: 0.00000922
Iteration 28/1000 | Loss: 0.00000921
Iteration 29/1000 | Loss: 0.00000921
Iteration 30/1000 | Loss: 0.00000920
Iteration 31/1000 | Loss: 0.00000919
Iteration 32/1000 | Loss: 0.00000916
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000916
Iteration 36/1000 | Loss: 0.00000916
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000910
Iteration 39/1000 | Loss: 0.00000910
Iteration 40/1000 | Loss: 0.00000910
Iteration 41/1000 | Loss: 0.00000910
Iteration 42/1000 | Loss: 0.00000909
Iteration 43/1000 | Loss: 0.00000909
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000908
Iteration 46/1000 | Loss: 0.00000908
Iteration 47/1000 | Loss: 0.00000908
Iteration 48/1000 | Loss: 0.00000907
Iteration 49/1000 | Loss: 0.00000907
Iteration 50/1000 | Loss: 0.00000907
Iteration 51/1000 | Loss: 0.00000907
Iteration 52/1000 | Loss: 0.00000907
Iteration 53/1000 | Loss: 0.00000906
Iteration 54/1000 | Loss: 0.00000906
Iteration 55/1000 | Loss: 0.00000906
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000904
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000903
Iteration 61/1000 | Loss: 0.00000903
Iteration 62/1000 | Loss: 0.00000903
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000902
Iteration 66/1000 | Loss: 0.00000902
Iteration 67/1000 | Loss: 0.00000902
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000902
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000902
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000901
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000901
Iteration 78/1000 | Loss: 0.00000901
Iteration 79/1000 | Loss: 0.00000901
Iteration 80/1000 | Loss: 0.00000901
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000900
Iteration 83/1000 | Loss: 0.00000900
Iteration 84/1000 | Loss: 0.00000900
Iteration 85/1000 | Loss: 0.00000900
Iteration 86/1000 | Loss: 0.00000899
Iteration 87/1000 | Loss: 0.00000899
Iteration 88/1000 | Loss: 0.00000899
Iteration 89/1000 | Loss: 0.00000899
Iteration 90/1000 | Loss: 0.00000899
Iteration 91/1000 | Loss: 0.00000899
Iteration 92/1000 | Loss: 0.00000899
Iteration 93/1000 | Loss: 0.00000899
Iteration 94/1000 | Loss: 0.00000898
Iteration 95/1000 | Loss: 0.00000898
Iteration 96/1000 | Loss: 0.00000897
Iteration 97/1000 | Loss: 0.00000897
Iteration 98/1000 | Loss: 0.00000897
Iteration 99/1000 | Loss: 0.00000896
Iteration 100/1000 | Loss: 0.00000896
Iteration 101/1000 | Loss: 0.00000896
Iteration 102/1000 | Loss: 0.00000895
Iteration 103/1000 | Loss: 0.00000895
Iteration 104/1000 | Loss: 0.00000894
Iteration 105/1000 | Loss: 0.00000894
Iteration 106/1000 | Loss: 0.00000894
Iteration 107/1000 | Loss: 0.00000894
Iteration 108/1000 | Loss: 0.00000894
Iteration 109/1000 | Loss: 0.00000894
Iteration 110/1000 | Loss: 0.00000894
Iteration 111/1000 | Loss: 0.00000894
Iteration 112/1000 | Loss: 0.00000894
Iteration 113/1000 | Loss: 0.00000894
Iteration 114/1000 | Loss: 0.00000894
Iteration 115/1000 | Loss: 0.00000894
Iteration 116/1000 | Loss: 0.00000894
Iteration 117/1000 | Loss: 0.00000893
Iteration 118/1000 | Loss: 0.00000893
Iteration 119/1000 | Loss: 0.00000893
Iteration 120/1000 | Loss: 0.00000893
Iteration 121/1000 | Loss: 0.00000893
Iteration 122/1000 | Loss: 0.00000893
Iteration 123/1000 | Loss: 0.00000893
Iteration 124/1000 | Loss: 0.00000893
Iteration 125/1000 | Loss: 0.00000893
Iteration 126/1000 | Loss: 0.00000893
Iteration 127/1000 | Loss: 0.00000893
Iteration 128/1000 | Loss: 0.00000893
Iteration 129/1000 | Loss: 0.00000892
Iteration 130/1000 | Loss: 0.00000892
Iteration 131/1000 | Loss: 0.00000892
Iteration 132/1000 | Loss: 0.00000892
Iteration 133/1000 | Loss: 0.00000892
Iteration 134/1000 | Loss: 0.00000892
Iteration 135/1000 | Loss: 0.00000891
Iteration 136/1000 | Loss: 0.00000891
Iteration 137/1000 | Loss: 0.00000891
Iteration 138/1000 | Loss: 0.00000891
Iteration 139/1000 | Loss: 0.00000891
Iteration 140/1000 | Loss: 0.00000891
Iteration 141/1000 | Loss: 0.00000891
Iteration 142/1000 | Loss: 0.00000891
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000890
Iteration 147/1000 | Loss: 0.00000890
Iteration 148/1000 | Loss: 0.00000890
Iteration 149/1000 | Loss: 0.00000890
Iteration 150/1000 | Loss: 0.00000890
Iteration 151/1000 | Loss: 0.00000889
Iteration 152/1000 | Loss: 0.00000889
Iteration 153/1000 | Loss: 0.00000889
Iteration 154/1000 | Loss: 0.00000888
Iteration 155/1000 | Loss: 0.00000888
Iteration 156/1000 | Loss: 0.00000888
Iteration 157/1000 | Loss: 0.00000888
Iteration 158/1000 | Loss: 0.00000888
Iteration 159/1000 | Loss: 0.00000888
Iteration 160/1000 | Loss: 0.00000888
Iteration 161/1000 | Loss: 0.00000888
Iteration 162/1000 | Loss: 0.00000888
Iteration 163/1000 | Loss: 0.00000887
Iteration 164/1000 | Loss: 0.00000887
Iteration 165/1000 | Loss: 0.00000887
Iteration 166/1000 | Loss: 0.00000887
Iteration 167/1000 | Loss: 0.00000887
Iteration 168/1000 | Loss: 0.00000887
Iteration 169/1000 | Loss: 0.00000887
Iteration 170/1000 | Loss: 0.00000886
Iteration 171/1000 | Loss: 0.00000886
Iteration 172/1000 | Loss: 0.00000886
Iteration 173/1000 | Loss: 0.00000886
Iteration 174/1000 | Loss: 0.00000886
Iteration 175/1000 | Loss: 0.00000886
Iteration 176/1000 | Loss: 0.00000886
Iteration 177/1000 | Loss: 0.00000886
Iteration 178/1000 | Loss: 0.00000886
Iteration 179/1000 | Loss: 0.00000886
Iteration 180/1000 | Loss: 0.00000886
Iteration 181/1000 | Loss: 0.00000886
Iteration 182/1000 | Loss: 0.00000886
Iteration 183/1000 | Loss: 0.00000885
Iteration 184/1000 | Loss: 0.00000885
Iteration 185/1000 | Loss: 0.00000885
Iteration 186/1000 | Loss: 0.00000885
Iteration 187/1000 | Loss: 0.00000885
Iteration 188/1000 | Loss: 0.00000885
Iteration 189/1000 | Loss: 0.00000885
Iteration 190/1000 | Loss: 0.00000885
Iteration 191/1000 | Loss: 0.00000885
Iteration 192/1000 | Loss: 0.00000885
Iteration 193/1000 | Loss: 0.00000885
Iteration 194/1000 | Loss: 0.00000885
Iteration 195/1000 | Loss: 0.00000885
Iteration 196/1000 | Loss: 0.00000885
Iteration 197/1000 | Loss: 0.00000885
Iteration 198/1000 | Loss: 0.00000885
Iteration 199/1000 | Loss: 0.00000885
Iteration 200/1000 | Loss: 0.00000884
Iteration 201/1000 | Loss: 0.00000884
Iteration 202/1000 | Loss: 0.00000884
Iteration 203/1000 | Loss: 0.00000884
Iteration 204/1000 | Loss: 0.00000884
Iteration 205/1000 | Loss: 0.00000884
Iteration 206/1000 | Loss: 0.00000884
Iteration 207/1000 | Loss: 0.00000884
Iteration 208/1000 | Loss: 0.00000884
Iteration 209/1000 | Loss: 0.00000884
Iteration 210/1000 | Loss: 0.00000884
Iteration 211/1000 | Loss: 0.00000884
Iteration 212/1000 | Loss: 0.00000884
Iteration 213/1000 | Loss: 0.00000884
Iteration 214/1000 | Loss: 0.00000884
Iteration 215/1000 | Loss: 0.00000884
Iteration 216/1000 | Loss: 0.00000884
Iteration 217/1000 | Loss: 0.00000884
Iteration 218/1000 | Loss: 0.00000884
Iteration 219/1000 | Loss: 0.00000884
Iteration 220/1000 | Loss: 0.00000884
Iteration 221/1000 | Loss: 0.00000884
Iteration 222/1000 | Loss: 0.00000884
Iteration 223/1000 | Loss: 0.00000883
Iteration 224/1000 | Loss: 0.00000883
Iteration 225/1000 | Loss: 0.00000883
Iteration 226/1000 | Loss: 0.00000883
Iteration 227/1000 | Loss: 0.00000883
Iteration 228/1000 | Loss: 0.00000883
Iteration 229/1000 | Loss: 0.00000883
Iteration 230/1000 | Loss: 0.00000883
Iteration 231/1000 | Loss: 0.00000883
Iteration 232/1000 | Loss: 0.00000883
Iteration 233/1000 | Loss: 0.00000883
Iteration 234/1000 | Loss: 0.00000883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [8.833927495288663e-06, 8.833927495288663e-06, 8.833927495288663e-06, 8.833927495288663e-06, 8.833927495288663e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.833927495288663e-06

Optimization complete. Final v2v error: 2.5573933124542236 mm

Highest mean error: 2.9638006687164307 mm for frame 85

Lowest mean error: 2.431980848312378 mm for frame 65

Saving results

Total time: 40.781085729599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964621
Iteration 2/25 | Loss: 0.00342310
Iteration 3/25 | Loss: 0.00211117
Iteration 4/25 | Loss: 0.00192320
Iteration 5/25 | Loss: 0.00175236
Iteration 6/25 | Loss: 0.00187377
Iteration 7/25 | Loss: 0.00175482
Iteration 8/25 | Loss: 0.00166408
Iteration 9/25 | Loss: 0.00154785
Iteration 10/25 | Loss: 0.00151466
Iteration 11/25 | Loss: 0.00146926
Iteration 12/25 | Loss: 0.00145516
Iteration 13/25 | Loss: 0.00145051
Iteration 14/25 | Loss: 0.00141964
Iteration 15/25 | Loss: 0.00140511
Iteration 16/25 | Loss: 0.00139330
Iteration 17/25 | Loss: 0.00139172
Iteration 18/25 | Loss: 0.00139450
Iteration 19/25 | Loss: 0.00138958
Iteration 20/25 | Loss: 0.00138559
Iteration 21/25 | Loss: 0.00138442
Iteration 22/25 | Loss: 0.00138380
Iteration 23/25 | Loss: 0.00138396
Iteration 24/25 | Loss: 0.00138328
Iteration 25/25 | Loss: 0.00138367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34094584
Iteration 2/25 | Loss: 0.00281638
Iteration 3/25 | Loss: 0.00266334
Iteration 4/25 | Loss: 0.00266334
Iteration 5/25 | Loss: 0.00266334
Iteration 6/25 | Loss: 0.00266334
Iteration 7/25 | Loss: 0.00266334
Iteration 8/25 | Loss: 0.00266334
Iteration 9/25 | Loss: 0.00266334
Iteration 10/25 | Loss: 0.00266334
Iteration 11/25 | Loss: 0.00266334
Iteration 12/25 | Loss: 0.00266333
Iteration 13/25 | Loss: 0.00266333
Iteration 14/25 | Loss: 0.00266333
Iteration 15/25 | Loss: 0.00266333
Iteration 16/25 | Loss: 0.00266333
Iteration 17/25 | Loss: 0.00266333
Iteration 18/25 | Loss: 0.00266333
Iteration 19/25 | Loss: 0.00266333
Iteration 20/25 | Loss: 0.00266333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0026633343659341335, 0.0026633343659341335, 0.0026633343659341335, 0.0026633343659341335, 0.0026633343659341335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026633343659341335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266333
Iteration 2/1000 | Loss: 0.00061495
Iteration 3/1000 | Loss: 0.00038115
Iteration 4/1000 | Loss: 0.00060601
Iteration 5/1000 | Loss: 0.00048123
Iteration 6/1000 | Loss: 0.00106506
Iteration 7/1000 | Loss: 0.00168684
Iteration 8/1000 | Loss: 0.00156110
Iteration 9/1000 | Loss: 0.00072548
Iteration 10/1000 | Loss: 0.00027130
Iteration 11/1000 | Loss: 0.00031617
Iteration 12/1000 | Loss: 0.00037873
Iteration 13/1000 | Loss: 0.00027232
Iteration 14/1000 | Loss: 0.00025846
Iteration 15/1000 | Loss: 0.00071909
Iteration 16/1000 | Loss: 0.00073300
Iteration 17/1000 | Loss: 0.00026764
Iteration 18/1000 | Loss: 0.00013778
Iteration 19/1000 | Loss: 0.00032243
Iteration 20/1000 | Loss: 0.00035985
Iteration 21/1000 | Loss: 0.00065686
Iteration 22/1000 | Loss: 0.00020636
Iteration 23/1000 | Loss: 0.00058862
Iteration 24/1000 | Loss: 0.00013866
Iteration 25/1000 | Loss: 0.00010692
Iteration 26/1000 | Loss: 0.00079621
Iteration 27/1000 | Loss: 0.00592441
Iteration 28/1000 | Loss: 0.00450716
Iteration 29/1000 | Loss: 0.00152568
Iteration 30/1000 | Loss: 0.00412098
Iteration 31/1000 | Loss: 0.00094712
Iteration 32/1000 | Loss: 0.00112175
Iteration 33/1000 | Loss: 0.00090577
Iteration 34/1000 | Loss: 0.00023184
Iteration 35/1000 | Loss: 0.00025291
Iteration 36/1000 | Loss: 0.00010871
Iteration 37/1000 | Loss: 0.00015018
Iteration 38/1000 | Loss: 0.00014141
Iteration 39/1000 | Loss: 0.00029995
Iteration 40/1000 | Loss: 0.00005884
Iteration 41/1000 | Loss: 0.00009773
Iteration 42/1000 | Loss: 0.00055854
Iteration 43/1000 | Loss: 0.00023277
Iteration 44/1000 | Loss: 0.00005670
Iteration 45/1000 | Loss: 0.00022971
Iteration 46/1000 | Loss: 0.00003641
Iteration 47/1000 | Loss: 0.00034164
Iteration 48/1000 | Loss: 0.00027876
Iteration 49/1000 | Loss: 0.00025222
Iteration 50/1000 | Loss: 0.00016540
Iteration 51/1000 | Loss: 0.00008037
Iteration 52/1000 | Loss: 0.00003048
Iteration 53/1000 | Loss: 0.00006327
Iteration 54/1000 | Loss: 0.00007701
Iteration 55/1000 | Loss: 0.00004619
Iteration 56/1000 | Loss: 0.00006901
Iteration 57/1000 | Loss: 0.00002245
Iteration 58/1000 | Loss: 0.00005343
Iteration 59/1000 | Loss: 0.00002490
Iteration 60/1000 | Loss: 0.00003716
Iteration 61/1000 | Loss: 0.00001940
Iteration 62/1000 | Loss: 0.00031196
Iteration 63/1000 | Loss: 0.00002193
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00001494
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00003002
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00002230
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001302
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001296
Iteration 94/1000 | Loss: 0.00001296
Iteration 95/1000 | Loss: 0.00001296
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001292
Iteration 122/1000 | Loss: 0.00001292
Iteration 123/1000 | Loss: 0.00001292
Iteration 124/1000 | Loss: 0.00001292
Iteration 125/1000 | Loss: 0.00001292
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001292
Iteration 134/1000 | Loss: 0.00001292
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001291
Iteration 152/1000 | Loss: 0.00001291
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001290
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001289
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00002089
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001287
Iteration 166/1000 | Loss: 0.00001287
Iteration 167/1000 | Loss: 0.00001287
Iteration 168/1000 | Loss: 0.00001287
Iteration 169/1000 | Loss: 0.00001287
Iteration 170/1000 | Loss: 0.00001287
Iteration 171/1000 | Loss: 0.00001287
Iteration 172/1000 | Loss: 0.00001286
Iteration 173/1000 | Loss: 0.00001286
Iteration 174/1000 | Loss: 0.00001286
Iteration 175/1000 | Loss: 0.00001286
Iteration 176/1000 | Loss: 0.00001286
Iteration 177/1000 | Loss: 0.00001286
Iteration 178/1000 | Loss: 0.00001285
Iteration 179/1000 | Loss: 0.00001285
Iteration 180/1000 | Loss: 0.00001285
Iteration 181/1000 | Loss: 0.00001285
Iteration 182/1000 | Loss: 0.00001285
Iteration 183/1000 | Loss: 0.00001285
Iteration 184/1000 | Loss: 0.00001285
Iteration 185/1000 | Loss: 0.00001285
Iteration 186/1000 | Loss: 0.00001285
Iteration 187/1000 | Loss: 0.00001285
Iteration 188/1000 | Loss: 0.00001285
Iteration 189/1000 | Loss: 0.00001285
Iteration 190/1000 | Loss: 0.00001285
Iteration 191/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.2851190149376635e-05, 1.2851190149376635e-05, 1.2851190149376635e-05, 1.2851190149376635e-05, 1.2851190149376635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2851190149376635e-05

Optimization complete. Final v2v error: 3.029478073120117 mm

Highest mean error: 3.826133966445923 mm for frame 225

Lowest mean error: 2.6178367137908936 mm for frame 234

Saving results

Total time: 183.83570003509521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536201
Iteration 2/25 | Loss: 0.00125021
Iteration 3/25 | Loss: 0.00115880
Iteration 4/25 | Loss: 0.00114510
Iteration 5/25 | Loss: 0.00113956
Iteration 6/25 | Loss: 0.00113851
Iteration 7/25 | Loss: 0.00113851
Iteration 8/25 | Loss: 0.00113851
Iteration 9/25 | Loss: 0.00113851
Iteration 10/25 | Loss: 0.00113851
Iteration 11/25 | Loss: 0.00113851
Iteration 12/25 | Loss: 0.00113851
Iteration 13/25 | Loss: 0.00113851
Iteration 14/25 | Loss: 0.00113851
Iteration 15/25 | Loss: 0.00113851
Iteration 16/25 | Loss: 0.00113851
Iteration 17/25 | Loss: 0.00113851
Iteration 18/25 | Loss: 0.00113851
Iteration 19/25 | Loss: 0.00113851
Iteration 20/25 | Loss: 0.00113851
Iteration 21/25 | Loss: 0.00113851
Iteration 22/25 | Loss: 0.00113851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011385066900402308, 0.0011385066900402308, 0.0011385066900402308, 0.0011385066900402308, 0.0011385066900402308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011385066900402308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75175208
Iteration 2/25 | Loss: 0.00075647
Iteration 3/25 | Loss: 0.00075647
Iteration 4/25 | Loss: 0.00075647
Iteration 5/25 | Loss: 0.00075647
Iteration 6/25 | Loss: 0.00075647
Iteration 7/25 | Loss: 0.00075647
Iteration 8/25 | Loss: 0.00075647
Iteration 9/25 | Loss: 0.00075647
Iteration 10/25 | Loss: 0.00075647
Iteration 11/25 | Loss: 0.00075647
Iteration 12/25 | Loss: 0.00075647
Iteration 13/25 | Loss: 0.00075647
Iteration 14/25 | Loss: 0.00075647
Iteration 15/25 | Loss: 0.00075647
Iteration 16/25 | Loss: 0.00075647
Iteration 17/25 | Loss: 0.00075647
Iteration 18/25 | Loss: 0.00075647
Iteration 19/25 | Loss: 0.00075647
Iteration 20/25 | Loss: 0.00075647
Iteration 21/25 | Loss: 0.00075647
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007564672851003706, 0.0007564672851003706, 0.0007564672851003706, 0.0007564672851003706, 0.0007564672851003706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007564672851003706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075647
Iteration 2/1000 | Loss: 0.00002995
Iteration 3/1000 | Loss: 0.00002280
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00002025
Iteration 6/1000 | Loss: 0.00001976
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001926
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001740
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001723
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001722
Iteration 26/1000 | Loss: 0.00001722
Iteration 27/1000 | Loss: 0.00001722
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001711
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001709
Iteration 36/1000 | Loss: 0.00001709
Iteration 37/1000 | Loss: 0.00001709
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001706
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001706
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001703
Iteration 62/1000 | Loss: 0.00001703
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001701
Iteration 72/1000 | Loss: 0.00001701
Iteration 73/1000 | Loss: 0.00001701
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001699
Iteration 80/1000 | Loss: 0.00001699
Iteration 81/1000 | Loss: 0.00001699
Iteration 82/1000 | Loss: 0.00001699
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001695
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001694
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001693
Iteration 108/1000 | Loss: 0.00001693
Iteration 109/1000 | Loss: 0.00001693
Iteration 110/1000 | Loss: 0.00001693
Iteration 111/1000 | Loss: 0.00001693
Iteration 112/1000 | Loss: 0.00001693
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001693
Iteration 115/1000 | Loss: 0.00001693
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001692
Iteration 118/1000 | Loss: 0.00001692
Iteration 119/1000 | Loss: 0.00001692
Iteration 120/1000 | Loss: 0.00001692
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001691
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001691
Iteration 127/1000 | Loss: 0.00001691
Iteration 128/1000 | Loss: 0.00001691
Iteration 129/1000 | Loss: 0.00001690
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001690
Iteration 132/1000 | Loss: 0.00001690
Iteration 133/1000 | Loss: 0.00001690
Iteration 134/1000 | Loss: 0.00001690
Iteration 135/1000 | Loss: 0.00001690
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001689
Iteration 138/1000 | Loss: 0.00001689
Iteration 139/1000 | Loss: 0.00001689
Iteration 140/1000 | Loss: 0.00001689
Iteration 141/1000 | Loss: 0.00001689
Iteration 142/1000 | Loss: 0.00001689
Iteration 143/1000 | Loss: 0.00001689
Iteration 144/1000 | Loss: 0.00001689
Iteration 145/1000 | Loss: 0.00001689
Iteration 146/1000 | Loss: 0.00001688
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001688
Iteration 154/1000 | Loss: 0.00001688
Iteration 155/1000 | Loss: 0.00001687
Iteration 156/1000 | Loss: 0.00001687
Iteration 157/1000 | Loss: 0.00001687
Iteration 158/1000 | Loss: 0.00001687
Iteration 159/1000 | Loss: 0.00001687
Iteration 160/1000 | Loss: 0.00001687
Iteration 161/1000 | Loss: 0.00001687
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001687
Iteration 164/1000 | Loss: 0.00001687
Iteration 165/1000 | Loss: 0.00001687
Iteration 166/1000 | Loss: 0.00001686
Iteration 167/1000 | Loss: 0.00001686
Iteration 168/1000 | Loss: 0.00001686
Iteration 169/1000 | Loss: 0.00001686
Iteration 170/1000 | Loss: 0.00001686
Iteration 171/1000 | Loss: 0.00001686
Iteration 172/1000 | Loss: 0.00001685
Iteration 173/1000 | Loss: 0.00001685
Iteration 174/1000 | Loss: 0.00001685
Iteration 175/1000 | Loss: 0.00001685
Iteration 176/1000 | Loss: 0.00001685
Iteration 177/1000 | Loss: 0.00001685
Iteration 178/1000 | Loss: 0.00001685
Iteration 179/1000 | Loss: 0.00001685
Iteration 180/1000 | Loss: 0.00001685
Iteration 181/1000 | Loss: 0.00001685
Iteration 182/1000 | Loss: 0.00001685
Iteration 183/1000 | Loss: 0.00001685
Iteration 184/1000 | Loss: 0.00001685
Iteration 185/1000 | Loss: 0.00001685
Iteration 186/1000 | Loss: 0.00001685
Iteration 187/1000 | Loss: 0.00001685
Iteration 188/1000 | Loss: 0.00001685
Iteration 189/1000 | Loss: 0.00001685
Iteration 190/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.6851799955475144e-05, 1.6851799955475144e-05, 1.6851799955475144e-05, 1.6851799955475144e-05, 1.6851799955475144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6851799955475144e-05

Optimization complete. Final v2v error: 3.4633307456970215 mm

Highest mean error: 3.530473232269287 mm for frame 165

Lowest mean error: 3.3914835453033447 mm for frame 20

Saving results

Total time: 47.22833275794983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409811
Iteration 2/25 | Loss: 0.00127585
Iteration 3/25 | Loss: 0.00112639
Iteration 4/25 | Loss: 0.00110634
Iteration 5/25 | Loss: 0.00110287
Iteration 6/25 | Loss: 0.00110251
Iteration 7/25 | Loss: 0.00110251
Iteration 8/25 | Loss: 0.00110251
Iteration 9/25 | Loss: 0.00110251
Iteration 10/25 | Loss: 0.00110251
Iteration 11/25 | Loss: 0.00110251
Iteration 12/25 | Loss: 0.00110251
Iteration 13/25 | Loss: 0.00110251
Iteration 14/25 | Loss: 0.00110251
Iteration 15/25 | Loss: 0.00110251
Iteration 16/25 | Loss: 0.00110251
Iteration 17/25 | Loss: 0.00110251
Iteration 18/25 | Loss: 0.00110251
Iteration 19/25 | Loss: 0.00110251
Iteration 20/25 | Loss: 0.00110251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011025097919628024, 0.0011025097919628024, 0.0011025097919628024, 0.0011025097919628024, 0.0011025097919628024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011025097919628024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34479380
Iteration 2/25 | Loss: 0.00073801
Iteration 3/25 | Loss: 0.00073801
Iteration 4/25 | Loss: 0.00073801
Iteration 5/25 | Loss: 0.00073801
Iteration 6/25 | Loss: 0.00073801
Iteration 7/25 | Loss: 0.00073801
Iteration 8/25 | Loss: 0.00073801
Iteration 9/25 | Loss: 0.00073801
Iteration 10/25 | Loss: 0.00073801
Iteration 11/25 | Loss: 0.00073801
Iteration 12/25 | Loss: 0.00073801
Iteration 13/25 | Loss: 0.00073801
Iteration 14/25 | Loss: 0.00073801
Iteration 15/25 | Loss: 0.00073801
Iteration 16/25 | Loss: 0.00073801
Iteration 17/25 | Loss: 0.00073801
Iteration 18/25 | Loss: 0.00073801
Iteration 19/25 | Loss: 0.00073801
Iteration 20/25 | Loss: 0.00073801
Iteration 21/25 | Loss: 0.00073801
Iteration 22/25 | Loss: 0.00073801
Iteration 23/25 | Loss: 0.00073801
Iteration 24/25 | Loss: 0.00073801
Iteration 25/25 | Loss: 0.00073801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073801
Iteration 2/1000 | Loss: 0.00002406
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001229
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001211
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001210
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001201
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001198
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001196
Iteration 91/1000 | Loss: 0.00001196
Iteration 92/1000 | Loss: 0.00001196
Iteration 93/1000 | Loss: 0.00001196
Iteration 94/1000 | Loss: 0.00001196
Iteration 95/1000 | Loss: 0.00001196
Iteration 96/1000 | Loss: 0.00001196
Iteration 97/1000 | Loss: 0.00001195
Iteration 98/1000 | Loss: 0.00001195
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001195
Iteration 101/1000 | Loss: 0.00001195
Iteration 102/1000 | Loss: 0.00001195
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001194
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001193
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001189
Iteration 160/1000 | Loss: 0.00001189
Iteration 161/1000 | Loss: 0.00001189
Iteration 162/1000 | Loss: 0.00001189
Iteration 163/1000 | Loss: 0.00001189
Iteration 164/1000 | Loss: 0.00001189
Iteration 165/1000 | Loss: 0.00001189
Iteration 166/1000 | Loss: 0.00001189
Iteration 167/1000 | Loss: 0.00001189
Iteration 168/1000 | Loss: 0.00001189
Iteration 169/1000 | Loss: 0.00001189
Iteration 170/1000 | Loss: 0.00001189
Iteration 171/1000 | Loss: 0.00001189
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001189
Iteration 191/1000 | Loss: 0.00001189
Iteration 192/1000 | Loss: 0.00001189
Iteration 193/1000 | Loss: 0.00001189
Iteration 194/1000 | Loss: 0.00001189
Iteration 195/1000 | Loss: 0.00001189
Iteration 196/1000 | Loss: 0.00001189
Iteration 197/1000 | Loss: 0.00001189
Iteration 198/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.1887211257999297e-05, 1.1887211257999297e-05, 1.1887211257999297e-05, 1.1887211257999297e-05, 1.1887211257999297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1887211257999297e-05

Optimization complete. Final v2v error: 2.9506771564483643 mm

Highest mean error: 3.558793067932129 mm for frame 4

Lowest mean error: 2.667557954788208 mm for frame 194

Saving results

Total time: 43.13496136665344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525750
Iteration 2/25 | Loss: 0.00130607
Iteration 3/25 | Loss: 0.00118220
Iteration 4/25 | Loss: 0.00113930
Iteration 5/25 | Loss: 0.00112166
Iteration 6/25 | Loss: 0.00114356
Iteration 7/25 | Loss: 0.00112094
Iteration 8/25 | Loss: 0.00111499
Iteration 9/25 | Loss: 0.00109940
Iteration 10/25 | Loss: 0.00110063
Iteration 11/25 | Loss: 0.00109529
Iteration 12/25 | Loss: 0.00109380
Iteration 13/25 | Loss: 0.00109360
Iteration 14/25 | Loss: 0.00109386
Iteration 15/25 | Loss: 0.00109263
Iteration 16/25 | Loss: 0.00109154
Iteration 17/25 | Loss: 0.00109555
Iteration 18/25 | Loss: 0.00109527
Iteration 19/25 | Loss: 0.00109424
Iteration 20/25 | Loss: 0.00109452
Iteration 21/25 | Loss: 0.00109215
Iteration 22/25 | Loss: 0.00108885
Iteration 23/25 | Loss: 0.00108894
Iteration 24/25 | Loss: 0.00108800
Iteration 25/25 | Loss: 0.00108810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40887213
Iteration 2/25 | Loss: 0.00094808
Iteration 3/25 | Loss: 0.00094807
Iteration 4/25 | Loss: 0.00094807
Iteration 5/25 | Loss: 0.00094807
Iteration 6/25 | Loss: 0.00094807
Iteration 7/25 | Loss: 0.00094807
Iteration 8/25 | Loss: 0.00094807
Iteration 9/25 | Loss: 0.00094807
Iteration 10/25 | Loss: 0.00094807
Iteration 11/25 | Loss: 0.00094807
Iteration 12/25 | Loss: 0.00094807
Iteration 13/25 | Loss: 0.00094807
Iteration 14/25 | Loss: 0.00094807
Iteration 15/25 | Loss: 0.00094807
Iteration 16/25 | Loss: 0.00094807
Iteration 17/25 | Loss: 0.00094807
Iteration 18/25 | Loss: 0.00094807
Iteration 19/25 | Loss: 0.00094807
Iteration 20/25 | Loss: 0.00094807
Iteration 21/25 | Loss: 0.00094807
Iteration 22/25 | Loss: 0.00094807
Iteration 23/25 | Loss: 0.00094807
Iteration 24/25 | Loss: 0.00094807
Iteration 25/25 | Loss: 0.00094807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094807
Iteration 2/1000 | Loss: 0.00003101
Iteration 3/1000 | Loss: 0.00004493
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002072
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001524
Iteration 8/1000 | Loss: 0.00002727
Iteration 9/1000 | Loss: 0.00003562
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001324
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001079
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001049
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000988
Iteration 24/1000 | Loss: 0.00000986
Iteration 25/1000 | Loss: 0.00000985
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000982
Iteration 28/1000 | Loss: 0.00000982
Iteration 29/1000 | Loss: 0.00000982
Iteration 30/1000 | Loss: 0.00000981
Iteration 31/1000 | Loss: 0.00000981
Iteration 32/1000 | Loss: 0.00000981
Iteration 33/1000 | Loss: 0.00000980
Iteration 34/1000 | Loss: 0.00000980
Iteration 35/1000 | Loss: 0.00000980
Iteration 36/1000 | Loss: 0.00000979
Iteration 37/1000 | Loss: 0.00000979
Iteration 38/1000 | Loss: 0.00000978
Iteration 39/1000 | Loss: 0.00000978
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000977
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000976
Iteration 46/1000 | Loss: 0.00000976
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000974
Iteration 60/1000 | Loss: 0.00000974
Iteration 61/1000 | Loss: 0.00000974
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000973
Iteration 64/1000 | Loss: 0.00000973
Iteration 65/1000 | Loss: 0.00000973
Iteration 66/1000 | Loss: 0.00000973
Iteration 67/1000 | Loss: 0.00000973
Iteration 68/1000 | Loss: 0.00000973
Iteration 69/1000 | Loss: 0.00000973
Iteration 70/1000 | Loss: 0.00000973
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000972
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000971
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000971
Iteration 80/1000 | Loss: 0.00000970
Iteration 81/1000 | Loss: 0.00000970
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000969
Iteration 85/1000 | Loss: 0.00000969
Iteration 86/1000 | Loss: 0.00000969
Iteration 87/1000 | Loss: 0.00000969
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000969
Iteration 92/1000 | Loss: 0.00000969
Iteration 93/1000 | Loss: 0.00000969
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000968
Iteration 96/1000 | Loss: 0.00000968
Iteration 97/1000 | Loss: 0.00000968
Iteration 98/1000 | Loss: 0.00000968
Iteration 99/1000 | Loss: 0.00000968
Iteration 100/1000 | Loss: 0.00000968
Iteration 101/1000 | Loss: 0.00000968
Iteration 102/1000 | Loss: 0.00000968
Iteration 103/1000 | Loss: 0.00000967
Iteration 104/1000 | Loss: 0.00000967
Iteration 105/1000 | Loss: 0.00000967
Iteration 106/1000 | Loss: 0.00000967
Iteration 107/1000 | Loss: 0.00000967
Iteration 108/1000 | Loss: 0.00000967
Iteration 109/1000 | Loss: 0.00000967
Iteration 110/1000 | Loss: 0.00000967
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000967
Iteration 113/1000 | Loss: 0.00000966
Iteration 114/1000 | Loss: 0.00000966
Iteration 115/1000 | Loss: 0.00000966
Iteration 116/1000 | Loss: 0.00000965
Iteration 117/1000 | Loss: 0.00000965
Iteration 118/1000 | Loss: 0.00000965
Iteration 119/1000 | Loss: 0.00000965
Iteration 120/1000 | Loss: 0.00000965
Iteration 121/1000 | Loss: 0.00000965
Iteration 122/1000 | Loss: 0.00000965
Iteration 123/1000 | Loss: 0.00000964
Iteration 124/1000 | Loss: 0.00000964
Iteration 125/1000 | Loss: 0.00000964
Iteration 126/1000 | Loss: 0.00000964
Iteration 127/1000 | Loss: 0.00000964
Iteration 128/1000 | Loss: 0.00000964
Iteration 129/1000 | Loss: 0.00000964
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000963
Iteration 133/1000 | Loss: 0.00000963
Iteration 134/1000 | Loss: 0.00000963
Iteration 135/1000 | Loss: 0.00000963
Iteration 136/1000 | Loss: 0.00000963
Iteration 137/1000 | Loss: 0.00000963
Iteration 138/1000 | Loss: 0.00000963
Iteration 139/1000 | Loss: 0.00000963
Iteration 140/1000 | Loss: 0.00000962
Iteration 141/1000 | Loss: 0.00000962
Iteration 142/1000 | Loss: 0.00000962
Iteration 143/1000 | Loss: 0.00000962
Iteration 144/1000 | Loss: 0.00000962
Iteration 145/1000 | Loss: 0.00000962
Iteration 146/1000 | Loss: 0.00000962
Iteration 147/1000 | Loss: 0.00000962
Iteration 148/1000 | Loss: 0.00000962
Iteration 149/1000 | Loss: 0.00000962
Iteration 150/1000 | Loss: 0.00000961
Iteration 151/1000 | Loss: 0.00000961
Iteration 152/1000 | Loss: 0.00000961
Iteration 153/1000 | Loss: 0.00000961
Iteration 154/1000 | Loss: 0.00000961
Iteration 155/1000 | Loss: 0.00000961
Iteration 156/1000 | Loss: 0.00000961
Iteration 157/1000 | Loss: 0.00000961
Iteration 158/1000 | Loss: 0.00000961
Iteration 159/1000 | Loss: 0.00000961
Iteration 160/1000 | Loss: 0.00000961
Iteration 161/1000 | Loss: 0.00000961
Iteration 162/1000 | Loss: 0.00000961
Iteration 163/1000 | Loss: 0.00000961
Iteration 164/1000 | Loss: 0.00000960
Iteration 165/1000 | Loss: 0.00000960
Iteration 166/1000 | Loss: 0.00000960
Iteration 167/1000 | Loss: 0.00000960
Iteration 168/1000 | Loss: 0.00000960
Iteration 169/1000 | Loss: 0.00000960
Iteration 170/1000 | Loss: 0.00000960
Iteration 171/1000 | Loss: 0.00000960
Iteration 172/1000 | Loss: 0.00000960
Iteration 173/1000 | Loss: 0.00000960
Iteration 174/1000 | Loss: 0.00000959
Iteration 175/1000 | Loss: 0.00000959
Iteration 176/1000 | Loss: 0.00000959
Iteration 177/1000 | Loss: 0.00000959
Iteration 178/1000 | Loss: 0.00000959
Iteration 179/1000 | Loss: 0.00000959
Iteration 180/1000 | Loss: 0.00000959
Iteration 181/1000 | Loss: 0.00000959
Iteration 182/1000 | Loss: 0.00000959
Iteration 183/1000 | Loss: 0.00000959
Iteration 184/1000 | Loss: 0.00000959
Iteration 185/1000 | Loss: 0.00000958
Iteration 186/1000 | Loss: 0.00000958
Iteration 187/1000 | Loss: 0.00000958
Iteration 188/1000 | Loss: 0.00000958
Iteration 189/1000 | Loss: 0.00000958
Iteration 190/1000 | Loss: 0.00000958
Iteration 191/1000 | Loss: 0.00000958
Iteration 192/1000 | Loss: 0.00000958
Iteration 193/1000 | Loss: 0.00000958
Iteration 194/1000 | Loss: 0.00000958
Iteration 195/1000 | Loss: 0.00000958
Iteration 196/1000 | Loss: 0.00000958
Iteration 197/1000 | Loss: 0.00000957
Iteration 198/1000 | Loss: 0.00000957
Iteration 199/1000 | Loss: 0.00000957
Iteration 200/1000 | Loss: 0.00000957
Iteration 201/1000 | Loss: 0.00000957
Iteration 202/1000 | Loss: 0.00000957
Iteration 203/1000 | Loss: 0.00000957
Iteration 204/1000 | Loss: 0.00000957
Iteration 205/1000 | Loss: 0.00000957
Iteration 206/1000 | Loss: 0.00000957
Iteration 207/1000 | Loss: 0.00000957
Iteration 208/1000 | Loss: 0.00000957
Iteration 209/1000 | Loss: 0.00000957
Iteration 210/1000 | Loss: 0.00000957
Iteration 211/1000 | Loss: 0.00000957
Iteration 212/1000 | Loss: 0.00000957
Iteration 213/1000 | Loss: 0.00000957
Iteration 214/1000 | Loss: 0.00000957
Iteration 215/1000 | Loss: 0.00000957
Iteration 216/1000 | Loss: 0.00000957
Iteration 217/1000 | Loss: 0.00000957
Iteration 218/1000 | Loss: 0.00000957
Iteration 219/1000 | Loss: 0.00000957
Iteration 220/1000 | Loss: 0.00000956
Iteration 221/1000 | Loss: 0.00000956
Iteration 222/1000 | Loss: 0.00000956
Iteration 223/1000 | Loss: 0.00000956
Iteration 224/1000 | Loss: 0.00000956
Iteration 225/1000 | Loss: 0.00000956
Iteration 226/1000 | Loss: 0.00000956
Iteration 227/1000 | Loss: 0.00000956
Iteration 228/1000 | Loss: 0.00000956
Iteration 229/1000 | Loss: 0.00000956
Iteration 230/1000 | Loss: 0.00000956
Iteration 231/1000 | Loss: 0.00000956
Iteration 232/1000 | Loss: 0.00000955
Iteration 233/1000 | Loss: 0.00000955
Iteration 234/1000 | Loss: 0.00000955
Iteration 235/1000 | Loss: 0.00000955
Iteration 236/1000 | Loss: 0.00000955
Iteration 237/1000 | Loss: 0.00000955
Iteration 238/1000 | Loss: 0.00000955
Iteration 239/1000 | Loss: 0.00000955
Iteration 240/1000 | Loss: 0.00000955
Iteration 241/1000 | Loss: 0.00000955
Iteration 242/1000 | Loss: 0.00000955
Iteration 243/1000 | Loss: 0.00000954
Iteration 244/1000 | Loss: 0.00000954
Iteration 245/1000 | Loss: 0.00000954
Iteration 246/1000 | Loss: 0.00000954
Iteration 247/1000 | Loss: 0.00000954
Iteration 248/1000 | Loss: 0.00000954
Iteration 249/1000 | Loss: 0.00000954
Iteration 250/1000 | Loss: 0.00000954
Iteration 251/1000 | Loss: 0.00000954
Iteration 252/1000 | Loss: 0.00000954
Iteration 253/1000 | Loss: 0.00000954
Iteration 254/1000 | Loss: 0.00000954
Iteration 255/1000 | Loss: 0.00000954
Iteration 256/1000 | Loss: 0.00000954
Iteration 257/1000 | Loss: 0.00000954
Iteration 258/1000 | Loss: 0.00000954
Iteration 259/1000 | Loss: 0.00000954
Iteration 260/1000 | Loss: 0.00000954
Iteration 261/1000 | Loss: 0.00000954
Iteration 262/1000 | Loss: 0.00000953
Iteration 263/1000 | Loss: 0.00000953
Iteration 264/1000 | Loss: 0.00000953
Iteration 265/1000 | Loss: 0.00000953
Iteration 266/1000 | Loss: 0.00000953
Iteration 267/1000 | Loss: 0.00000953
Iteration 268/1000 | Loss: 0.00000953
Iteration 269/1000 | Loss: 0.00000953
Iteration 270/1000 | Loss: 0.00000953
Iteration 271/1000 | Loss: 0.00000953
Iteration 272/1000 | Loss: 0.00000953
Iteration 273/1000 | Loss: 0.00000953
Iteration 274/1000 | Loss: 0.00000953
Iteration 275/1000 | Loss: 0.00000953
Iteration 276/1000 | Loss: 0.00000953
Iteration 277/1000 | Loss: 0.00000953
Iteration 278/1000 | Loss: 0.00000953
Iteration 279/1000 | Loss: 0.00000953
Iteration 280/1000 | Loss: 0.00000953
Iteration 281/1000 | Loss: 0.00000953
Iteration 282/1000 | Loss: 0.00000952
Iteration 283/1000 | Loss: 0.00000952
Iteration 284/1000 | Loss: 0.00000952
Iteration 285/1000 | Loss: 0.00000952
Iteration 286/1000 | Loss: 0.00000952
Iteration 287/1000 | Loss: 0.00000952
Iteration 288/1000 | Loss: 0.00000952
Iteration 289/1000 | Loss: 0.00000952
Iteration 290/1000 | Loss: 0.00000952
Iteration 291/1000 | Loss: 0.00000952
Iteration 292/1000 | Loss: 0.00000952
Iteration 293/1000 | Loss: 0.00000952
Iteration 294/1000 | Loss: 0.00000952
Iteration 295/1000 | Loss: 0.00000952
Iteration 296/1000 | Loss: 0.00000952
Iteration 297/1000 | Loss: 0.00000952
Iteration 298/1000 | Loss: 0.00000952
Iteration 299/1000 | Loss: 0.00000952
Iteration 300/1000 | Loss: 0.00000952
Iteration 301/1000 | Loss: 0.00000952
Iteration 302/1000 | Loss: 0.00000952
Iteration 303/1000 | Loss: 0.00000952
Iteration 304/1000 | Loss: 0.00000951
Iteration 305/1000 | Loss: 0.00000951
Iteration 306/1000 | Loss: 0.00000951
Iteration 307/1000 | Loss: 0.00000951
Iteration 308/1000 | Loss: 0.00000951
Iteration 309/1000 | Loss: 0.00000951
Iteration 310/1000 | Loss: 0.00000951
Iteration 311/1000 | Loss: 0.00000951
Iteration 312/1000 | Loss: 0.00000951
Iteration 313/1000 | Loss: 0.00000951
Iteration 314/1000 | Loss: 0.00000951
Iteration 315/1000 | Loss: 0.00000951
Iteration 316/1000 | Loss: 0.00000951
Iteration 317/1000 | Loss: 0.00000951
Iteration 318/1000 | Loss: 0.00000951
Iteration 319/1000 | Loss: 0.00000951
Iteration 320/1000 | Loss: 0.00000951
Iteration 321/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [9.510845302429516e-06, 9.510845302429516e-06, 9.510845302429516e-06, 9.510845302429516e-06, 9.510845302429516e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.510845302429516e-06

Optimization complete. Final v2v error: 2.6443591117858887 mm

Highest mean error: 3.503871202468872 mm for frame 83

Lowest mean error: 2.350463628768921 mm for frame 120

Saving results

Total time: 95.27091574668884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906650
Iteration 2/25 | Loss: 0.00243807
Iteration 3/25 | Loss: 0.00178756
Iteration 4/25 | Loss: 0.00177619
Iteration 5/25 | Loss: 0.00162058
Iteration 6/25 | Loss: 0.00154374
Iteration 7/25 | Loss: 0.00143814
Iteration 8/25 | Loss: 0.00140354
Iteration 9/25 | Loss: 0.00137207
Iteration 10/25 | Loss: 0.00135601
Iteration 11/25 | Loss: 0.00135885
Iteration 12/25 | Loss: 0.00133374
Iteration 13/25 | Loss: 0.00132534
Iteration 14/25 | Loss: 0.00133872
Iteration 15/25 | Loss: 0.00129750
Iteration 16/25 | Loss: 0.00128851
Iteration 17/25 | Loss: 0.00124765
Iteration 18/25 | Loss: 0.00123840
Iteration 19/25 | Loss: 0.00124058
Iteration 20/25 | Loss: 0.00121903
Iteration 21/25 | Loss: 0.00121750
Iteration 22/25 | Loss: 0.00121729
Iteration 23/25 | Loss: 0.00121715
Iteration 24/25 | Loss: 0.00121713
Iteration 25/25 | Loss: 0.00121709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31278634
Iteration 2/25 | Loss: 0.00087584
Iteration 3/25 | Loss: 0.00077273
Iteration 4/25 | Loss: 0.00077273
Iteration 5/25 | Loss: 0.00077273
Iteration 6/25 | Loss: 0.00077273
Iteration 7/25 | Loss: 0.00077273
Iteration 8/25 | Loss: 0.00077273
Iteration 9/25 | Loss: 0.00077273
Iteration 10/25 | Loss: 0.00077273
Iteration 11/25 | Loss: 0.00077273
Iteration 12/25 | Loss: 0.00077273
Iteration 13/25 | Loss: 0.00077273
Iteration 14/25 | Loss: 0.00077273
Iteration 15/25 | Loss: 0.00077273
Iteration 16/25 | Loss: 0.00077273
Iteration 17/25 | Loss: 0.00077273
Iteration 18/25 | Loss: 0.00077273
Iteration 19/25 | Loss: 0.00077272
Iteration 20/25 | Loss: 0.00077273
Iteration 21/25 | Loss: 0.00077272
Iteration 22/25 | Loss: 0.00077273
Iteration 23/25 | Loss: 0.00077272
Iteration 24/25 | Loss: 0.00077272
Iteration 25/25 | Loss: 0.00077273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077273
Iteration 2/1000 | Loss: 0.00046020
Iteration 3/1000 | Loss: 0.00005172
Iteration 4/1000 | Loss: 0.00003078
Iteration 5/1000 | Loss: 0.00002721
Iteration 6/1000 | Loss: 0.00002464
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00002272
Iteration 9/1000 | Loss: 0.00002206
Iteration 10/1000 | Loss: 0.00010693
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002182
Iteration 13/1000 | Loss: 0.00002084
Iteration 14/1000 | Loss: 0.00002022
Iteration 15/1000 | Loss: 0.00001989
Iteration 16/1000 | Loss: 0.00001957
Iteration 17/1000 | Loss: 0.00001945
Iteration 18/1000 | Loss: 0.00001943
Iteration 19/1000 | Loss: 0.00010806
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001905
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001880
Iteration 26/1000 | Loss: 0.00010026
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001862
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001856
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001851
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001848
Iteration 47/1000 | Loss: 0.00001848
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.842015262809582e-05, 1.842015262809582e-05, 1.842015262809582e-05, 1.842015262809582e-05, 1.842015262809582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.842015262809582e-05

Optimization complete. Final v2v error: 3.5633621215820312 mm

Highest mean error: 5.1533522605896 mm for frame 146

Lowest mean error: 3.338515043258667 mm for frame 29

Saving results

Total time: 91.57249760627747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989430
Iteration 2/25 | Loss: 0.00989430
Iteration 3/25 | Loss: 0.00989430
Iteration 4/25 | Loss: 0.00989430
Iteration 5/25 | Loss: 0.00989430
Iteration 6/25 | Loss: 0.00989430
Iteration 7/25 | Loss: 0.00989430
Iteration 8/25 | Loss: 0.00989430
Iteration 9/25 | Loss: 0.00989430
Iteration 10/25 | Loss: 0.00989429
Iteration 11/25 | Loss: 0.00989429
Iteration 12/25 | Loss: 0.00989429
Iteration 13/25 | Loss: 0.00989429
Iteration 14/25 | Loss: 0.00989429
Iteration 15/25 | Loss: 0.00989429
Iteration 16/25 | Loss: 0.00989429
Iteration 17/25 | Loss: 0.00989429
Iteration 18/25 | Loss: 0.00989428
Iteration 19/25 | Loss: 0.00989428
Iteration 20/25 | Loss: 0.00989428
Iteration 21/25 | Loss: 0.00989428
Iteration 22/25 | Loss: 0.00989428
Iteration 23/25 | Loss: 0.00989428
Iteration 24/25 | Loss: 0.00989428
Iteration 25/25 | Loss: 0.00989428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43311620
Iteration 2/25 | Loss: 0.18366952
Iteration 3/25 | Loss: 0.18366504
Iteration 4/25 | Loss: 0.18366499
Iteration 5/25 | Loss: 0.18366499
Iteration 6/25 | Loss: 0.18366495
Iteration 7/25 | Loss: 0.18366495
Iteration 8/25 | Loss: 0.18366495
Iteration 9/25 | Loss: 0.18366495
Iteration 10/25 | Loss: 0.18366493
Iteration 11/25 | Loss: 0.18366493
Iteration 12/25 | Loss: 0.18366493
Iteration 13/25 | Loss: 0.18366493
Iteration 14/25 | Loss: 0.18366493
Iteration 15/25 | Loss: 0.18366493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.183664932847023, 0.183664932847023, 0.183664932847023, 0.183664932847023, 0.183664932847023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.183664932847023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18366493
Iteration 2/1000 | Loss: 0.00587680
Iteration 3/1000 | Loss: 0.00242690
Iteration 4/1000 | Loss: 0.00218595
Iteration 5/1000 | Loss: 0.00064741
Iteration 6/1000 | Loss: 0.00275967
Iteration 7/1000 | Loss: 0.00038573
Iteration 8/1000 | Loss: 0.00106918
Iteration 9/1000 | Loss: 0.00015822
Iteration 10/1000 | Loss: 0.00052979
Iteration 11/1000 | Loss: 0.00045523
Iteration 12/1000 | Loss: 0.00028730
Iteration 13/1000 | Loss: 0.00006913
Iteration 14/1000 | Loss: 0.00012182
Iteration 15/1000 | Loss: 0.00010287
Iteration 16/1000 | Loss: 0.00004604
Iteration 17/1000 | Loss: 0.00009784
Iteration 18/1000 | Loss: 0.00003126
Iteration 19/1000 | Loss: 0.00012202
Iteration 20/1000 | Loss: 0.00002776
Iteration 21/1000 | Loss: 0.00007456
Iteration 22/1000 | Loss: 0.00012009
Iteration 23/1000 | Loss: 0.00003629
Iteration 24/1000 | Loss: 0.00002803
Iteration 25/1000 | Loss: 0.00002669
Iteration 26/1000 | Loss: 0.00002272
Iteration 27/1000 | Loss: 0.00002178
Iteration 28/1000 | Loss: 0.00002102
Iteration 29/1000 | Loss: 0.00002033
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001844
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001840
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001835
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001834
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001834
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001822
Iteration 86/1000 | Loss: 0.00001822
Iteration 87/1000 | Loss: 0.00001822
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001819
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001818
Iteration 107/1000 | Loss: 0.00001818
Iteration 108/1000 | Loss: 0.00001818
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001818
Iteration 114/1000 | Loss: 0.00001818
Iteration 115/1000 | Loss: 0.00001818
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001817
Iteration 118/1000 | Loss: 0.00001817
Iteration 119/1000 | Loss: 0.00001817
Iteration 120/1000 | Loss: 0.00001817
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001816
Iteration 123/1000 | Loss: 0.00001816
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001815
Iteration 127/1000 | Loss: 0.00001815
Iteration 128/1000 | Loss: 0.00001815
Iteration 129/1000 | Loss: 0.00001814
Iteration 130/1000 | Loss: 0.00001814
Iteration 131/1000 | Loss: 0.00001814
Iteration 132/1000 | Loss: 0.00001814
Iteration 133/1000 | Loss: 0.00001814
Iteration 134/1000 | Loss: 0.00001814
Iteration 135/1000 | Loss: 0.00001814
Iteration 136/1000 | Loss: 0.00001814
Iteration 137/1000 | Loss: 0.00001814
Iteration 138/1000 | Loss: 0.00001814
Iteration 139/1000 | Loss: 0.00001814
Iteration 140/1000 | Loss: 0.00001814
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001813
Iteration 143/1000 | Loss: 0.00001813
Iteration 144/1000 | Loss: 0.00001813
Iteration 145/1000 | Loss: 0.00001812
Iteration 146/1000 | Loss: 0.00001812
Iteration 147/1000 | Loss: 0.00001812
Iteration 148/1000 | Loss: 0.00001811
Iteration 149/1000 | Loss: 0.00001811
Iteration 150/1000 | Loss: 0.00001811
Iteration 151/1000 | Loss: 0.00001810
Iteration 152/1000 | Loss: 0.00001810
Iteration 153/1000 | Loss: 0.00001810
Iteration 154/1000 | Loss: 0.00001810
Iteration 155/1000 | Loss: 0.00001810
Iteration 156/1000 | Loss: 0.00001810
Iteration 157/1000 | Loss: 0.00001810
Iteration 158/1000 | Loss: 0.00001810
Iteration 159/1000 | Loss: 0.00001810
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001809
Iteration 164/1000 | Loss: 0.00001809
Iteration 165/1000 | Loss: 0.00001809
Iteration 166/1000 | Loss: 0.00001809
Iteration 167/1000 | Loss: 0.00001809
Iteration 168/1000 | Loss: 0.00001809
Iteration 169/1000 | Loss: 0.00001809
Iteration 170/1000 | Loss: 0.00001809
Iteration 171/1000 | Loss: 0.00001809
Iteration 172/1000 | Loss: 0.00001809
Iteration 173/1000 | Loss: 0.00001809
Iteration 174/1000 | Loss: 0.00001808
Iteration 175/1000 | Loss: 0.00001808
Iteration 176/1000 | Loss: 0.00001808
Iteration 177/1000 | Loss: 0.00001808
Iteration 178/1000 | Loss: 0.00001808
Iteration 179/1000 | Loss: 0.00001808
Iteration 180/1000 | Loss: 0.00001808
Iteration 181/1000 | Loss: 0.00001808
Iteration 182/1000 | Loss: 0.00001808
Iteration 183/1000 | Loss: 0.00001808
Iteration 184/1000 | Loss: 0.00001808
Iteration 185/1000 | Loss: 0.00001807
Iteration 186/1000 | Loss: 0.00001807
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001807
Iteration 189/1000 | Loss: 0.00001807
Iteration 190/1000 | Loss: 0.00001807
Iteration 191/1000 | Loss: 0.00001807
Iteration 192/1000 | Loss: 0.00001807
Iteration 193/1000 | Loss: 0.00001807
Iteration 194/1000 | Loss: 0.00001807
Iteration 195/1000 | Loss: 0.00001807
Iteration 196/1000 | Loss: 0.00001807
Iteration 197/1000 | Loss: 0.00001806
Iteration 198/1000 | Loss: 0.00001806
Iteration 199/1000 | Loss: 0.00001806
Iteration 200/1000 | Loss: 0.00001806
Iteration 201/1000 | Loss: 0.00001806
Iteration 202/1000 | Loss: 0.00001806
Iteration 203/1000 | Loss: 0.00001806
Iteration 204/1000 | Loss: 0.00001806
Iteration 205/1000 | Loss: 0.00001806
Iteration 206/1000 | Loss: 0.00001806
Iteration 207/1000 | Loss: 0.00001806
Iteration 208/1000 | Loss: 0.00001806
Iteration 209/1000 | Loss: 0.00001806
Iteration 210/1000 | Loss: 0.00001806
Iteration 211/1000 | Loss: 0.00001806
Iteration 212/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.8061504306388088e-05, 1.8061504306388088e-05, 1.8061504306388088e-05, 1.8061504306388088e-05, 1.8061504306388088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8061504306388088e-05

Optimization complete. Final v2v error: 3.6310160160064697 mm

Highest mean error: 4.0685200691223145 mm for frame 18

Lowest mean error: 3.3537609577178955 mm for frame 129

Saving results

Total time: 77.8892011642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399655
Iteration 2/25 | Loss: 0.00114166
Iteration 3/25 | Loss: 0.00107023
Iteration 4/25 | Loss: 0.00105828
Iteration 5/25 | Loss: 0.00105442
Iteration 6/25 | Loss: 0.00105369
Iteration 7/25 | Loss: 0.00105369
Iteration 8/25 | Loss: 0.00105369
Iteration 9/25 | Loss: 0.00105369
Iteration 10/25 | Loss: 0.00105369
Iteration 11/25 | Loss: 0.00105369
Iteration 12/25 | Loss: 0.00105369
Iteration 13/25 | Loss: 0.00105369
Iteration 14/25 | Loss: 0.00105369
Iteration 15/25 | Loss: 0.00105369
Iteration 16/25 | Loss: 0.00105369
Iteration 17/25 | Loss: 0.00105369
Iteration 18/25 | Loss: 0.00105369
Iteration 19/25 | Loss: 0.00105369
Iteration 20/25 | Loss: 0.00105369
Iteration 21/25 | Loss: 0.00105369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010536863701418042, 0.0010536863701418042, 0.0010536863701418042, 0.0010536863701418042, 0.0010536863701418042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010536863701418042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73531270
Iteration 2/25 | Loss: 0.00078281
Iteration 3/25 | Loss: 0.00078281
Iteration 4/25 | Loss: 0.00078281
Iteration 5/25 | Loss: 0.00078281
Iteration 6/25 | Loss: 0.00078281
Iteration 7/25 | Loss: 0.00078281
Iteration 8/25 | Loss: 0.00078281
Iteration 9/25 | Loss: 0.00078281
Iteration 10/25 | Loss: 0.00078281
Iteration 11/25 | Loss: 0.00078281
Iteration 12/25 | Loss: 0.00078281
Iteration 13/25 | Loss: 0.00078281
Iteration 14/25 | Loss: 0.00078281
Iteration 15/25 | Loss: 0.00078281
Iteration 16/25 | Loss: 0.00078281
Iteration 17/25 | Loss: 0.00078281
Iteration 18/25 | Loss: 0.00078281
Iteration 19/25 | Loss: 0.00078281
Iteration 20/25 | Loss: 0.00078281
Iteration 21/25 | Loss: 0.00078281
Iteration 22/25 | Loss: 0.00078281
Iteration 23/25 | Loss: 0.00078281
Iteration 24/25 | Loss: 0.00078281
Iteration 25/25 | Loss: 0.00078281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078281
Iteration 2/1000 | Loss: 0.00001748
Iteration 3/1000 | Loss: 0.00001288
Iteration 4/1000 | Loss: 0.00001181
Iteration 5/1000 | Loss: 0.00001121
Iteration 6/1000 | Loss: 0.00001077
Iteration 7/1000 | Loss: 0.00001047
Iteration 8/1000 | Loss: 0.00001026
Iteration 9/1000 | Loss: 0.00001001
Iteration 10/1000 | Loss: 0.00001001
Iteration 11/1000 | Loss: 0.00000993
Iteration 12/1000 | Loss: 0.00000990
Iteration 13/1000 | Loss: 0.00000976
Iteration 14/1000 | Loss: 0.00000975
Iteration 15/1000 | Loss: 0.00000974
Iteration 16/1000 | Loss: 0.00000973
Iteration 17/1000 | Loss: 0.00000967
Iteration 18/1000 | Loss: 0.00000964
Iteration 19/1000 | Loss: 0.00000964
Iteration 20/1000 | Loss: 0.00000964
Iteration 21/1000 | Loss: 0.00000962
Iteration 22/1000 | Loss: 0.00000961
Iteration 23/1000 | Loss: 0.00000957
Iteration 24/1000 | Loss: 0.00000957
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000954
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000951
Iteration 30/1000 | Loss: 0.00000950
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000949
Iteration 33/1000 | Loss: 0.00000949
Iteration 34/1000 | Loss: 0.00000949
Iteration 35/1000 | Loss: 0.00000948
Iteration 36/1000 | Loss: 0.00000948
Iteration 37/1000 | Loss: 0.00000948
Iteration 38/1000 | Loss: 0.00000948
Iteration 39/1000 | Loss: 0.00000948
Iteration 40/1000 | Loss: 0.00000948
Iteration 41/1000 | Loss: 0.00000948
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000945
Iteration 44/1000 | Loss: 0.00000944
Iteration 45/1000 | Loss: 0.00000943
Iteration 46/1000 | Loss: 0.00000943
Iteration 47/1000 | Loss: 0.00000941
Iteration 48/1000 | Loss: 0.00000940
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000940
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00000939
Iteration 53/1000 | Loss: 0.00000939
Iteration 54/1000 | Loss: 0.00000939
Iteration 55/1000 | Loss: 0.00000938
Iteration 56/1000 | Loss: 0.00000938
Iteration 57/1000 | Loss: 0.00000937
Iteration 58/1000 | Loss: 0.00000936
Iteration 59/1000 | Loss: 0.00000936
Iteration 60/1000 | Loss: 0.00000936
Iteration 61/1000 | Loss: 0.00000935
Iteration 62/1000 | Loss: 0.00000935
Iteration 63/1000 | Loss: 0.00000934
Iteration 64/1000 | Loss: 0.00000933
Iteration 65/1000 | Loss: 0.00000933
Iteration 66/1000 | Loss: 0.00000932
Iteration 67/1000 | Loss: 0.00000931
Iteration 68/1000 | Loss: 0.00000930
Iteration 69/1000 | Loss: 0.00000929
Iteration 70/1000 | Loss: 0.00000929
Iteration 71/1000 | Loss: 0.00000929
Iteration 72/1000 | Loss: 0.00000929
Iteration 73/1000 | Loss: 0.00000929
Iteration 74/1000 | Loss: 0.00000929
Iteration 75/1000 | Loss: 0.00000929
Iteration 76/1000 | Loss: 0.00000929
Iteration 77/1000 | Loss: 0.00000929
Iteration 78/1000 | Loss: 0.00000929
Iteration 79/1000 | Loss: 0.00000929
Iteration 80/1000 | Loss: 0.00000929
Iteration 81/1000 | Loss: 0.00000929
Iteration 82/1000 | Loss: 0.00000928
Iteration 83/1000 | Loss: 0.00000928
Iteration 84/1000 | Loss: 0.00000927
Iteration 85/1000 | Loss: 0.00000927
Iteration 86/1000 | Loss: 0.00000926
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000925
Iteration 91/1000 | Loss: 0.00000925
Iteration 92/1000 | Loss: 0.00000925
Iteration 93/1000 | Loss: 0.00000925
Iteration 94/1000 | Loss: 0.00000925
Iteration 95/1000 | Loss: 0.00000925
Iteration 96/1000 | Loss: 0.00000925
Iteration 97/1000 | Loss: 0.00000925
Iteration 98/1000 | Loss: 0.00000924
Iteration 99/1000 | Loss: 0.00000924
Iteration 100/1000 | Loss: 0.00000924
Iteration 101/1000 | Loss: 0.00000924
Iteration 102/1000 | Loss: 0.00000923
Iteration 103/1000 | Loss: 0.00000923
Iteration 104/1000 | Loss: 0.00000923
Iteration 105/1000 | Loss: 0.00000923
Iteration 106/1000 | Loss: 0.00000922
Iteration 107/1000 | Loss: 0.00000922
Iteration 108/1000 | Loss: 0.00000922
Iteration 109/1000 | Loss: 0.00000922
Iteration 110/1000 | Loss: 0.00000922
Iteration 111/1000 | Loss: 0.00000921
Iteration 112/1000 | Loss: 0.00000921
Iteration 113/1000 | Loss: 0.00000921
Iteration 114/1000 | Loss: 0.00000921
Iteration 115/1000 | Loss: 0.00000920
Iteration 116/1000 | Loss: 0.00000920
Iteration 117/1000 | Loss: 0.00000920
Iteration 118/1000 | Loss: 0.00000919
Iteration 119/1000 | Loss: 0.00000919
Iteration 120/1000 | Loss: 0.00000919
Iteration 121/1000 | Loss: 0.00000919
Iteration 122/1000 | Loss: 0.00000919
Iteration 123/1000 | Loss: 0.00000918
Iteration 124/1000 | Loss: 0.00000918
Iteration 125/1000 | Loss: 0.00000918
Iteration 126/1000 | Loss: 0.00000918
Iteration 127/1000 | Loss: 0.00000917
Iteration 128/1000 | Loss: 0.00000917
Iteration 129/1000 | Loss: 0.00000917
Iteration 130/1000 | Loss: 0.00000917
Iteration 131/1000 | Loss: 0.00000917
Iteration 132/1000 | Loss: 0.00000917
Iteration 133/1000 | Loss: 0.00000916
Iteration 134/1000 | Loss: 0.00000916
Iteration 135/1000 | Loss: 0.00000916
Iteration 136/1000 | Loss: 0.00000916
Iteration 137/1000 | Loss: 0.00000916
Iteration 138/1000 | Loss: 0.00000916
Iteration 139/1000 | Loss: 0.00000916
Iteration 140/1000 | Loss: 0.00000915
Iteration 141/1000 | Loss: 0.00000915
Iteration 142/1000 | Loss: 0.00000915
Iteration 143/1000 | Loss: 0.00000915
Iteration 144/1000 | Loss: 0.00000915
Iteration 145/1000 | Loss: 0.00000915
Iteration 146/1000 | Loss: 0.00000915
Iteration 147/1000 | Loss: 0.00000915
Iteration 148/1000 | Loss: 0.00000915
Iteration 149/1000 | Loss: 0.00000915
Iteration 150/1000 | Loss: 0.00000915
Iteration 151/1000 | Loss: 0.00000915
Iteration 152/1000 | Loss: 0.00000915
Iteration 153/1000 | Loss: 0.00000915
Iteration 154/1000 | Loss: 0.00000914
Iteration 155/1000 | Loss: 0.00000914
Iteration 156/1000 | Loss: 0.00000914
Iteration 157/1000 | Loss: 0.00000914
Iteration 158/1000 | Loss: 0.00000914
Iteration 159/1000 | Loss: 0.00000914
Iteration 160/1000 | Loss: 0.00000914
Iteration 161/1000 | Loss: 0.00000914
Iteration 162/1000 | Loss: 0.00000914
Iteration 163/1000 | Loss: 0.00000914
Iteration 164/1000 | Loss: 0.00000914
Iteration 165/1000 | Loss: 0.00000914
Iteration 166/1000 | Loss: 0.00000914
Iteration 167/1000 | Loss: 0.00000914
Iteration 168/1000 | Loss: 0.00000914
Iteration 169/1000 | Loss: 0.00000914
Iteration 170/1000 | Loss: 0.00000913
Iteration 171/1000 | Loss: 0.00000913
Iteration 172/1000 | Loss: 0.00000913
Iteration 173/1000 | Loss: 0.00000913
Iteration 174/1000 | Loss: 0.00000913
Iteration 175/1000 | Loss: 0.00000913
Iteration 176/1000 | Loss: 0.00000913
Iteration 177/1000 | Loss: 0.00000913
Iteration 178/1000 | Loss: 0.00000913
Iteration 179/1000 | Loss: 0.00000913
Iteration 180/1000 | Loss: 0.00000913
Iteration 181/1000 | Loss: 0.00000913
Iteration 182/1000 | Loss: 0.00000913
Iteration 183/1000 | Loss: 0.00000913
Iteration 184/1000 | Loss: 0.00000913
Iteration 185/1000 | Loss: 0.00000912
Iteration 186/1000 | Loss: 0.00000912
Iteration 187/1000 | Loss: 0.00000912
Iteration 188/1000 | Loss: 0.00000912
Iteration 189/1000 | Loss: 0.00000912
Iteration 190/1000 | Loss: 0.00000912
Iteration 191/1000 | Loss: 0.00000912
Iteration 192/1000 | Loss: 0.00000912
Iteration 193/1000 | Loss: 0.00000912
Iteration 194/1000 | Loss: 0.00000912
Iteration 195/1000 | Loss: 0.00000912
Iteration 196/1000 | Loss: 0.00000912
Iteration 197/1000 | Loss: 0.00000911
Iteration 198/1000 | Loss: 0.00000911
Iteration 199/1000 | Loss: 0.00000911
Iteration 200/1000 | Loss: 0.00000911
Iteration 201/1000 | Loss: 0.00000911
Iteration 202/1000 | Loss: 0.00000911
Iteration 203/1000 | Loss: 0.00000911
Iteration 204/1000 | Loss: 0.00000911
Iteration 205/1000 | Loss: 0.00000911
Iteration 206/1000 | Loss: 0.00000911
Iteration 207/1000 | Loss: 0.00000911
Iteration 208/1000 | Loss: 0.00000911
Iteration 209/1000 | Loss: 0.00000911
Iteration 210/1000 | Loss: 0.00000911
Iteration 211/1000 | Loss: 0.00000911
Iteration 212/1000 | Loss: 0.00000911
Iteration 213/1000 | Loss: 0.00000911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [9.114657586906105e-06, 9.114657586906105e-06, 9.114657586906105e-06, 9.114657586906105e-06, 9.114657586906105e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.114657586906105e-06

Optimization complete. Final v2v error: 2.6242921352386475 mm

Highest mean error: 2.9083666801452637 mm for frame 124

Lowest mean error: 2.479969024658203 mm for frame 183

Saving results

Total time: 41.86873507499695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346193
Iteration 2/25 | Loss: 0.00111501
Iteration 3/25 | Loss: 0.00105509
Iteration 4/25 | Loss: 0.00104555
Iteration 5/25 | Loss: 0.00104314
Iteration 6/25 | Loss: 0.00104297
Iteration 7/25 | Loss: 0.00104297
Iteration 8/25 | Loss: 0.00104297
Iteration 9/25 | Loss: 0.00104297
Iteration 10/25 | Loss: 0.00104297
Iteration 11/25 | Loss: 0.00104297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010429713875055313, 0.0010429713875055313, 0.0010429713875055313, 0.0010429713875055313, 0.0010429713875055313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010429713875055313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35745490
Iteration 2/25 | Loss: 0.00098963
Iteration 3/25 | Loss: 0.00098963
Iteration 4/25 | Loss: 0.00098962
Iteration 5/25 | Loss: 0.00098962
Iteration 6/25 | Loss: 0.00098962
Iteration 7/25 | Loss: 0.00098962
Iteration 8/25 | Loss: 0.00098962
Iteration 9/25 | Loss: 0.00098962
Iteration 10/25 | Loss: 0.00098962
Iteration 11/25 | Loss: 0.00098962
Iteration 12/25 | Loss: 0.00098962
Iteration 13/25 | Loss: 0.00098962
Iteration 14/25 | Loss: 0.00098962
Iteration 15/25 | Loss: 0.00098962
Iteration 16/25 | Loss: 0.00098962
Iteration 17/25 | Loss: 0.00098962
Iteration 18/25 | Loss: 0.00098962
Iteration 19/25 | Loss: 0.00098962
Iteration 20/25 | Loss: 0.00098962
Iteration 21/25 | Loss: 0.00098962
Iteration 22/25 | Loss: 0.00098962
Iteration 23/25 | Loss: 0.00098962
Iteration 24/25 | Loss: 0.00098962
Iteration 25/25 | Loss: 0.00098962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098962
Iteration 2/1000 | Loss: 0.00002703
Iteration 3/1000 | Loss: 0.00001728
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001304
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001181
Iteration 8/1000 | Loss: 0.00001144
Iteration 9/1000 | Loss: 0.00001121
Iteration 10/1000 | Loss: 0.00001108
Iteration 11/1000 | Loss: 0.00001099
Iteration 12/1000 | Loss: 0.00001097
Iteration 13/1000 | Loss: 0.00001094
Iteration 14/1000 | Loss: 0.00001092
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001091
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001090
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001084
Iteration 21/1000 | Loss: 0.00001083
Iteration 22/1000 | Loss: 0.00001080
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001078
Iteration 25/1000 | Loss: 0.00001075
Iteration 26/1000 | Loss: 0.00001075
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001067
Iteration 29/1000 | Loss: 0.00001065
Iteration 30/1000 | Loss: 0.00001065
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001064
Iteration 34/1000 | Loss: 0.00001062
Iteration 35/1000 | Loss: 0.00001060
Iteration 36/1000 | Loss: 0.00001060
Iteration 37/1000 | Loss: 0.00001059
Iteration 38/1000 | Loss: 0.00001059
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001057
Iteration 44/1000 | Loss: 0.00001057
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001056
Iteration 56/1000 | Loss: 0.00001056
Iteration 57/1000 | Loss: 0.00001056
Iteration 58/1000 | Loss: 0.00001056
Iteration 59/1000 | Loss: 0.00001056
Iteration 60/1000 | Loss: 0.00001056
Iteration 61/1000 | Loss: 0.00001056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.0555876542639453e-05, 1.0555876542639453e-05, 1.0555876542639453e-05, 1.0555876542639453e-05, 1.0555876542639453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0555876542639453e-05

Optimization complete. Final v2v error: 2.760073184967041 mm

Highest mean error: 3.6264185905456543 mm for frame 10

Lowest mean error: 2.2663183212280273 mm for frame 132

Saving results

Total time: 29.561635494232178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489916
Iteration 2/25 | Loss: 0.00121326
Iteration 3/25 | Loss: 0.00112919
Iteration 4/25 | Loss: 0.00111140
Iteration 5/25 | Loss: 0.00110873
Iteration 6/25 | Loss: 0.00110873
Iteration 7/25 | Loss: 0.00110872
Iteration 8/25 | Loss: 0.00110873
Iteration 9/25 | Loss: 0.00110872
Iteration 10/25 | Loss: 0.00110873
Iteration 11/25 | Loss: 0.00110873
Iteration 12/25 | Loss: 0.00110872
Iteration 13/25 | Loss: 0.00110873
Iteration 14/25 | Loss: 0.00110872
Iteration 15/25 | Loss: 0.00110873
Iteration 16/25 | Loss: 0.00110873
Iteration 17/25 | Loss: 0.00110873
Iteration 18/25 | Loss: 0.00110873
Iteration 19/25 | Loss: 0.00110873
Iteration 20/25 | Loss: 0.00110872
Iteration 21/25 | Loss: 0.00110873
Iteration 22/25 | Loss: 0.00110873
Iteration 23/25 | Loss: 0.00110873
Iteration 24/25 | Loss: 0.00110873
Iteration 25/25 | Loss: 0.00110873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60698128
Iteration 2/25 | Loss: 0.00086954
Iteration 3/25 | Loss: 0.00086954
Iteration 4/25 | Loss: 0.00086953
Iteration 5/25 | Loss: 0.00086953
Iteration 6/25 | Loss: 0.00086953
Iteration 7/25 | Loss: 0.00086953
Iteration 8/25 | Loss: 0.00086953
Iteration 9/25 | Loss: 0.00086953
Iteration 10/25 | Loss: 0.00086953
Iteration 11/25 | Loss: 0.00086953
Iteration 12/25 | Loss: 0.00086953
Iteration 13/25 | Loss: 0.00086953
Iteration 14/25 | Loss: 0.00086953
Iteration 15/25 | Loss: 0.00086953
Iteration 16/25 | Loss: 0.00086953
Iteration 17/25 | Loss: 0.00086953
Iteration 18/25 | Loss: 0.00086953
Iteration 19/25 | Loss: 0.00086953
Iteration 20/25 | Loss: 0.00086953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008695321739651263, 0.0008695321739651263, 0.0008695321739651263, 0.0008695321739651263, 0.0008695321739651263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008695321739651263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086953
Iteration 2/1000 | Loss: 0.00002409
Iteration 3/1000 | Loss: 0.00001756
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001355
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001286
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001282
Iteration 22/1000 | Loss: 0.00001282
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001276
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001272
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001270
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001270
Iteration 51/1000 | Loss: 0.00001270
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001269
Iteration 56/1000 | Loss: 0.00001269
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001266
Iteration 66/1000 | Loss: 0.00001266
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001266
Iteration 69/1000 | Loss: 0.00001266
Iteration 70/1000 | Loss: 0.00001266
Iteration 71/1000 | Loss: 0.00001265
Iteration 72/1000 | Loss: 0.00001265
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001264
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001262
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001260
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001257
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001252
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001252
Iteration 140/1000 | Loss: 0.00001252
Iteration 141/1000 | Loss: 0.00001252
Iteration 142/1000 | Loss: 0.00001252
Iteration 143/1000 | Loss: 0.00001252
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001252
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001252
Iteration 149/1000 | Loss: 0.00001251
Iteration 150/1000 | Loss: 0.00001251
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001251
Iteration 154/1000 | Loss: 0.00001251
Iteration 155/1000 | Loss: 0.00001251
Iteration 156/1000 | Loss: 0.00001251
Iteration 157/1000 | Loss: 0.00001251
Iteration 158/1000 | Loss: 0.00001251
Iteration 159/1000 | Loss: 0.00001251
Iteration 160/1000 | Loss: 0.00001251
Iteration 161/1000 | Loss: 0.00001251
Iteration 162/1000 | Loss: 0.00001251
Iteration 163/1000 | Loss: 0.00001251
Iteration 164/1000 | Loss: 0.00001251
Iteration 165/1000 | Loss: 0.00001251
Iteration 166/1000 | Loss: 0.00001251
Iteration 167/1000 | Loss: 0.00001251
Iteration 168/1000 | Loss: 0.00001251
Iteration 169/1000 | Loss: 0.00001251
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001251
Iteration 174/1000 | Loss: 0.00001251
Iteration 175/1000 | Loss: 0.00001251
Iteration 176/1000 | Loss: 0.00001251
Iteration 177/1000 | Loss: 0.00001251
Iteration 178/1000 | Loss: 0.00001251
Iteration 179/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.2509288353612646e-05, 1.2509288353612646e-05, 1.2509288353612646e-05, 1.2509288353612646e-05, 1.2509288353612646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2509288353612646e-05

Optimization complete. Final v2v error: 2.976869821548462 mm

Highest mean error: 3.217284917831421 mm for frame 266

Lowest mean error: 2.770071029663086 mm for frame 14

Saving results

Total time: 44.71282124519348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531229
Iteration 2/25 | Loss: 0.00142107
Iteration 3/25 | Loss: 0.00121859
Iteration 4/25 | Loss: 0.00120263
Iteration 5/25 | Loss: 0.00119650
Iteration 6/25 | Loss: 0.00119492
Iteration 7/25 | Loss: 0.00119492
Iteration 8/25 | Loss: 0.00119492
Iteration 9/25 | Loss: 0.00119492
Iteration 10/25 | Loss: 0.00119492
Iteration 11/25 | Loss: 0.00119492
Iteration 12/25 | Loss: 0.00119492
Iteration 13/25 | Loss: 0.00119492
Iteration 14/25 | Loss: 0.00119492
Iteration 15/25 | Loss: 0.00119492
Iteration 16/25 | Loss: 0.00119492
Iteration 17/25 | Loss: 0.00119492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011949194595217705, 0.0011949194595217705, 0.0011949194595217705, 0.0011949194595217705, 0.0011949194595217705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011949194595217705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90285796
Iteration 2/25 | Loss: 0.00098717
Iteration 3/25 | Loss: 0.00098717
Iteration 4/25 | Loss: 0.00098717
Iteration 5/25 | Loss: 0.00098717
Iteration 6/25 | Loss: 0.00098717
Iteration 7/25 | Loss: 0.00098717
Iteration 8/25 | Loss: 0.00098717
Iteration 9/25 | Loss: 0.00098717
Iteration 10/25 | Loss: 0.00098717
Iteration 11/25 | Loss: 0.00098717
Iteration 12/25 | Loss: 0.00098717
Iteration 13/25 | Loss: 0.00098717
Iteration 14/25 | Loss: 0.00098717
Iteration 15/25 | Loss: 0.00098717
Iteration 16/25 | Loss: 0.00098717
Iteration 17/25 | Loss: 0.00098717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009871692163869739, 0.0009871692163869739, 0.0009871692163869739, 0.0009871692163869739, 0.0009871692163869739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009871692163869739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098717
Iteration 2/1000 | Loss: 0.00007459
Iteration 3/1000 | Loss: 0.00004099
Iteration 4/1000 | Loss: 0.00003317
Iteration 5/1000 | Loss: 0.00003057
Iteration 6/1000 | Loss: 0.00002887
Iteration 7/1000 | Loss: 0.00002805
Iteration 8/1000 | Loss: 0.00002726
Iteration 9/1000 | Loss: 0.00002667
Iteration 10/1000 | Loss: 0.00002619
Iteration 11/1000 | Loss: 0.00002575
Iteration 12/1000 | Loss: 0.00002544
Iteration 13/1000 | Loss: 0.00002519
Iteration 14/1000 | Loss: 0.00002496
Iteration 15/1000 | Loss: 0.00002475
Iteration 16/1000 | Loss: 0.00002458
Iteration 17/1000 | Loss: 0.00002444
Iteration 18/1000 | Loss: 0.00002430
Iteration 19/1000 | Loss: 0.00002424
Iteration 20/1000 | Loss: 0.00002424
Iteration 21/1000 | Loss: 0.00002423
Iteration 22/1000 | Loss: 0.00002423
Iteration 23/1000 | Loss: 0.00002423
Iteration 24/1000 | Loss: 0.00002423
Iteration 25/1000 | Loss: 0.00002423
Iteration 26/1000 | Loss: 0.00002423
Iteration 27/1000 | Loss: 0.00002422
Iteration 28/1000 | Loss: 0.00002421
Iteration 29/1000 | Loss: 0.00002420
Iteration 30/1000 | Loss: 0.00002420
Iteration 31/1000 | Loss: 0.00002420
Iteration 32/1000 | Loss: 0.00002419
Iteration 33/1000 | Loss: 0.00002419
Iteration 34/1000 | Loss: 0.00002419
Iteration 35/1000 | Loss: 0.00002419
Iteration 36/1000 | Loss: 0.00002419
Iteration 37/1000 | Loss: 0.00002419
Iteration 38/1000 | Loss: 0.00002418
Iteration 39/1000 | Loss: 0.00002418
Iteration 40/1000 | Loss: 0.00002418
Iteration 41/1000 | Loss: 0.00002418
Iteration 42/1000 | Loss: 0.00002418
Iteration 43/1000 | Loss: 0.00002418
Iteration 44/1000 | Loss: 0.00002418
Iteration 45/1000 | Loss: 0.00002418
Iteration 46/1000 | Loss: 0.00002418
Iteration 47/1000 | Loss: 0.00002418
Iteration 48/1000 | Loss: 0.00002417
Iteration 49/1000 | Loss: 0.00002416
Iteration 50/1000 | Loss: 0.00002415
Iteration 51/1000 | Loss: 0.00002414
Iteration 52/1000 | Loss: 0.00002413
Iteration 53/1000 | Loss: 0.00002410
Iteration 54/1000 | Loss: 0.00002409
Iteration 55/1000 | Loss: 0.00002409
Iteration 56/1000 | Loss: 0.00002408
Iteration 57/1000 | Loss: 0.00002407
Iteration 58/1000 | Loss: 0.00002407
Iteration 59/1000 | Loss: 0.00002406
Iteration 60/1000 | Loss: 0.00002406
Iteration 61/1000 | Loss: 0.00002406
Iteration 62/1000 | Loss: 0.00002406
Iteration 63/1000 | Loss: 0.00002405
Iteration 64/1000 | Loss: 0.00002405
Iteration 65/1000 | Loss: 0.00002405
Iteration 66/1000 | Loss: 0.00002405
Iteration 67/1000 | Loss: 0.00002405
Iteration 68/1000 | Loss: 0.00002405
Iteration 69/1000 | Loss: 0.00002405
Iteration 70/1000 | Loss: 0.00002405
Iteration 71/1000 | Loss: 0.00002405
Iteration 72/1000 | Loss: 0.00002404
Iteration 73/1000 | Loss: 0.00002404
Iteration 74/1000 | Loss: 0.00002404
Iteration 75/1000 | Loss: 0.00002404
Iteration 76/1000 | Loss: 0.00002404
Iteration 77/1000 | Loss: 0.00002404
Iteration 78/1000 | Loss: 0.00002404
Iteration 79/1000 | Loss: 0.00002403
Iteration 80/1000 | Loss: 0.00002403
Iteration 81/1000 | Loss: 0.00002403
Iteration 82/1000 | Loss: 0.00002403
Iteration 83/1000 | Loss: 0.00002403
Iteration 84/1000 | Loss: 0.00002403
Iteration 85/1000 | Loss: 0.00002403
Iteration 86/1000 | Loss: 0.00002403
Iteration 87/1000 | Loss: 0.00002403
Iteration 88/1000 | Loss: 0.00002403
Iteration 89/1000 | Loss: 0.00002403
Iteration 90/1000 | Loss: 0.00002403
Iteration 91/1000 | Loss: 0.00002402
Iteration 92/1000 | Loss: 0.00002402
Iteration 93/1000 | Loss: 0.00002402
Iteration 94/1000 | Loss: 0.00002402
Iteration 95/1000 | Loss: 0.00002402
Iteration 96/1000 | Loss: 0.00002401
Iteration 97/1000 | Loss: 0.00002401
Iteration 98/1000 | Loss: 0.00002401
Iteration 99/1000 | Loss: 0.00002401
Iteration 100/1000 | Loss: 0.00002401
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00002400
Iteration 103/1000 | Loss: 0.00002400
Iteration 104/1000 | Loss: 0.00002400
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002399
Iteration 107/1000 | Loss: 0.00002398
Iteration 108/1000 | Loss: 0.00002398
Iteration 109/1000 | Loss: 0.00002398
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002397
Iteration 112/1000 | Loss: 0.00002397
Iteration 113/1000 | Loss: 0.00002397
Iteration 114/1000 | Loss: 0.00002397
Iteration 115/1000 | Loss: 0.00002397
Iteration 116/1000 | Loss: 0.00002396
Iteration 117/1000 | Loss: 0.00002396
Iteration 118/1000 | Loss: 0.00002396
Iteration 119/1000 | Loss: 0.00002396
Iteration 120/1000 | Loss: 0.00002396
Iteration 121/1000 | Loss: 0.00002395
Iteration 122/1000 | Loss: 0.00002395
Iteration 123/1000 | Loss: 0.00002395
Iteration 124/1000 | Loss: 0.00002395
Iteration 125/1000 | Loss: 0.00002395
Iteration 126/1000 | Loss: 0.00002395
Iteration 127/1000 | Loss: 0.00002395
Iteration 128/1000 | Loss: 0.00002394
Iteration 129/1000 | Loss: 0.00002394
Iteration 130/1000 | Loss: 0.00002394
Iteration 131/1000 | Loss: 0.00002394
Iteration 132/1000 | Loss: 0.00002394
Iteration 133/1000 | Loss: 0.00002394
Iteration 134/1000 | Loss: 0.00002394
Iteration 135/1000 | Loss: 0.00002394
Iteration 136/1000 | Loss: 0.00002393
Iteration 137/1000 | Loss: 0.00002393
Iteration 138/1000 | Loss: 0.00002393
Iteration 139/1000 | Loss: 0.00002393
Iteration 140/1000 | Loss: 0.00002393
Iteration 141/1000 | Loss: 0.00002393
Iteration 142/1000 | Loss: 0.00002392
Iteration 143/1000 | Loss: 0.00002392
Iteration 144/1000 | Loss: 0.00002392
Iteration 145/1000 | Loss: 0.00002392
Iteration 146/1000 | Loss: 0.00002392
Iteration 147/1000 | Loss: 0.00002392
Iteration 148/1000 | Loss: 0.00002392
Iteration 149/1000 | Loss: 0.00002392
Iteration 150/1000 | Loss: 0.00002392
Iteration 151/1000 | Loss: 0.00002392
Iteration 152/1000 | Loss: 0.00002392
Iteration 153/1000 | Loss: 0.00002392
Iteration 154/1000 | Loss: 0.00002392
Iteration 155/1000 | Loss: 0.00002391
Iteration 156/1000 | Loss: 0.00002391
Iteration 157/1000 | Loss: 0.00002391
Iteration 158/1000 | Loss: 0.00002391
Iteration 159/1000 | Loss: 0.00002391
Iteration 160/1000 | Loss: 0.00002391
Iteration 161/1000 | Loss: 0.00002391
Iteration 162/1000 | Loss: 0.00002391
Iteration 163/1000 | Loss: 0.00002391
Iteration 164/1000 | Loss: 0.00002391
Iteration 165/1000 | Loss: 0.00002391
Iteration 166/1000 | Loss: 0.00002391
Iteration 167/1000 | Loss: 0.00002391
Iteration 168/1000 | Loss: 0.00002391
Iteration 169/1000 | Loss: 0.00002391
Iteration 170/1000 | Loss: 0.00002391
Iteration 171/1000 | Loss: 0.00002391
Iteration 172/1000 | Loss: 0.00002390
Iteration 173/1000 | Loss: 0.00002390
Iteration 174/1000 | Loss: 0.00002390
Iteration 175/1000 | Loss: 0.00002390
Iteration 176/1000 | Loss: 0.00002390
Iteration 177/1000 | Loss: 0.00002390
Iteration 178/1000 | Loss: 0.00002390
Iteration 179/1000 | Loss: 0.00002390
Iteration 180/1000 | Loss: 0.00002390
Iteration 181/1000 | Loss: 0.00002390
Iteration 182/1000 | Loss: 0.00002390
Iteration 183/1000 | Loss: 0.00002390
Iteration 184/1000 | Loss: 0.00002390
Iteration 185/1000 | Loss: 0.00002390
Iteration 186/1000 | Loss: 0.00002390
Iteration 187/1000 | Loss: 0.00002390
Iteration 188/1000 | Loss: 0.00002390
Iteration 189/1000 | Loss: 0.00002390
Iteration 190/1000 | Loss: 0.00002390
Iteration 191/1000 | Loss: 0.00002390
Iteration 192/1000 | Loss: 0.00002390
Iteration 193/1000 | Loss: 0.00002390
Iteration 194/1000 | Loss: 0.00002390
Iteration 195/1000 | Loss: 0.00002390
Iteration 196/1000 | Loss: 0.00002390
Iteration 197/1000 | Loss: 0.00002390
Iteration 198/1000 | Loss: 0.00002390
Iteration 199/1000 | Loss: 0.00002390
Iteration 200/1000 | Loss: 0.00002390
Iteration 201/1000 | Loss: 0.00002390
Iteration 202/1000 | Loss: 0.00002390
Iteration 203/1000 | Loss: 0.00002390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.390158988418989e-05, 2.390158988418989e-05, 2.390158988418989e-05, 2.390158988418989e-05, 2.390158988418989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.390158988418989e-05

Optimization complete. Final v2v error: 4.011419773101807 mm

Highest mean error: 5.108578205108643 mm for frame 19

Lowest mean error: 3.3982186317443848 mm for frame 46

Saving results

Total time: 50.34859609603882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055633
Iteration 2/25 | Loss: 0.01055633
Iteration 3/25 | Loss: 0.01055633
Iteration 4/25 | Loss: 0.00250088
Iteration 5/25 | Loss: 0.00172700
Iteration 6/25 | Loss: 0.00171402
Iteration 7/25 | Loss: 0.00165767
Iteration 8/25 | Loss: 0.00153236
Iteration 9/25 | Loss: 0.00147266
Iteration 10/25 | Loss: 0.00141681
Iteration 11/25 | Loss: 0.00138324
Iteration 12/25 | Loss: 0.00135430
Iteration 13/25 | Loss: 0.00128074
Iteration 14/25 | Loss: 0.00125487
Iteration 15/25 | Loss: 0.00121060
Iteration 16/25 | Loss: 0.00119195
Iteration 17/25 | Loss: 0.00118771
Iteration 18/25 | Loss: 0.00120760
Iteration 19/25 | Loss: 0.00118096
Iteration 20/25 | Loss: 0.00116832
Iteration 21/25 | Loss: 0.00113669
Iteration 22/25 | Loss: 0.00112248
Iteration 23/25 | Loss: 0.00111219
Iteration 24/25 | Loss: 0.00110334
Iteration 25/25 | Loss: 0.00109884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44699442
Iteration 2/25 | Loss: 0.00182552
Iteration 3/25 | Loss: 0.00128474
Iteration 4/25 | Loss: 0.00128474
Iteration 5/25 | Loss: 0.00128474
Iteration 6/25 | Loss: 0.00128474
Iteration 7/25 | Loss: 0.00128474
Iteration 8/25 | Loss: 0.00128474
Iteration 9/25 | Loss: 0.00128474
Iteration 10/25 | Loss: 0.00128474
Iteration 11/25 | Loss: 0.00128474
Iteration 12/25 | Loss: 0.00128474
Iteration 13/25 | Loss: 0.00128474
Iteration 14/25 | Loss: 0.00128474
Iteration 15/25 | Loss: 0.00128474
Iteration 16/25 | Loss: 0.00128474
Iteration 17/25 | Loss: 0.00128474
Iteration 18/25 | Loss: 0.00128474
Iteration 19/25 | Loss: 0.00128474
Iteration 20/25 | Loss: 0.00128474
Iteration 21/25 | Loss: 0.00128474
Iteration 22/25 | Loss: 0.00128474
Iteration 23/25 | Loss: 0.00128474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001284737722016871, 0.001284737722016871, 0.001284737722016871, 0.001284737722016871, 0.001284737722016871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001284737722016871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128474
Iteration 2/1000 | Loss: 0.00048109
Iteration 3/1000 | Loss: 0.00110477
Iteration 4/1000 | Loss: 0.00181194
Iteration 5/1000 | Loss: 0.00076667
Iteration 6/1000 | Loss: 0.00066453
Iteration 7/1000 | Loss: 0.00162753
Iteration 8/1000 | Loss: 0.00129725
Iteration 9/1000 | Loss: 0.00065982
Iteration 10/1000 | Loss: 0.00050943
Iteration 11/1000 | Loss: 0.00107615
Iteration 12/1000 | Loss: 0.00049388
Iteration 13/1000 | Loss: 0.00041583
Iteration 14/1000 | Loss: 0.00122787
Iteration 15/1000 | Loss: 0.00072908
Iteration 16/1000 | Loss: 0.00098838
Iteration 17/1000 | Loss: 0.00087197
Iteration 18/1000 | Loss: 0.00051557
Iteration 19/1000 | Loss: 0.00071589
Iteration 20/1000 | Loss: 0.00066907
Iteration 21/1000 | Loss: 0.00092779
Iteration 22/1000 | Loss: 0.00087302
Iteration 23/1000 | Loss: 0.00105596
Iteration 24/1000 | Loss: 0.00096348
Iteration 25/1000 | Loss: 0.00062707
Iteration 26/1000 | Loss: 0.00102190
Iteration 27/1000 | Loss: 0.00069809
Iteration 28/1000 | Loss: 0.00076065
Iteration 29/1000 | Loss: 0.00061738
Iteration 30/1000 | Loss: 0.00048836
Iteration 31/1000 | Loss: 0.00055810
Iteration 32/1000 | Loss: 0.00023324
Iteration 33/1000 | Loss: 0.00049238
Iteration 34/1000 | Loss: 0.00048212
Iteration 35/1000 | Loss: 0.00056953
Iteration 36/1000 | Loss: 0.00069078
Iteration 37/1000 | Loss: 0.00055021
Iteration 38/1000 | Loss: 0.00067849
Iteration 39/1000 | Loss: 0.00063917
Iteration 40/1000 | Loss: 0.00045709
Iteration 41/1000 | Loss: 0.00079247
Iteration 42/1000 | Loss: 0.00063374
Iteration 43/1000 | Loss: 0.00032399
Iteration 44/1000 | Loss: 0.00035012
Iteration 45/1000 | Loss: 0.00045011
Iteration 46/1000 | Loss: 0.00033772
Iteration 47/1000 | Loss: 0.00035474
Iteration 48/1000 | Loss: 0.00030966
Iteration 49/1000 | Loss: 0.00027070
Iteration 50/1000 | Loss: 0.00030536
Iteration 51/1000 | Loss: 0.00086359
Iteration 52/1000 | Loss: 0.00048825
Iteration 53/1000 | Loss: 0.00021368
Iteration 54/1000 | Loss: 0.00023010
Iteration 55/1000 | Loss: 0.00028508
Iteration 56/1000 | Loss: 0.00030953
Iteration 57/1000 | Loss: 0.00023907
Iteration 58/1000 | Loss: 0.00031645
Iteration 59/1000 | Loss: 0.00032320
Iteration 60/1000 | Loss: 0.00024773
Iteration 61/1000 | Loss: 0.00031644
Iteration 62/1000 | Loss: 0.00030526
Iteration 63/1000 | Loss: 0.00037403
Iteration 64/1000 | Loss: 0.00034393
Iteration 65/1000 | Loss: 0.00037259
Iteration 66/1000 | Loss: 0.00065440
Iteration 67/1000 | Loss: 0.00122025
Iteration 68/1000 | Loss: 0.00052783
Iteration 69/1000 | Loss: 0.00037333
Iteration 70/1000 | Loss: 0.00044395
Iteration 71/1000 | Loss: 0.00042349
Iteration 72/1000 | Loss: 0.00033133
Iteration 73/1000 | Loss: 0.00047364
Iteration 74/1000 | Loss: 0.00115047
Iteration 75/1000 | Loss: 0.00036778
Iteration 76/1000 | Loss: 0.00026083
Iteration 77/1000 | Loss: 0.00024144
Iteration 78/1000 | Loss: 0.00040340
Iteration 79/1000 | Loss: 0.00031317
Iteration 80/1000 | Loss: 0.00032748
Iteration 81/1000 | Loss: 0.00046456
Iteration 82/1000 | Loss: 0.00044667
Iteration 83/1000 | Loss: 0.00027920
Iteration 84/1000 | Loss: 0.00029971
Iteration 85/1000 | Loss: 0.00024324
Iteration 86/1000 | Loss: 0.00026287
Iteration 87/1000 | Loss: 0.00037521
Iteration 88/1000 | Loss: 0.00031286
Iteration 89/1000 | Loss: 0.00029606
Iteration 90/1000 | Loss: 0.00035209
Iteration 91/1000 | Loss: 0.00030209
Iteration 92/1000 | Loss: 0.00062189
Iteration 93/1000 | Loss: 0.00031034
Iteration 94/1000 | Loss: 0.00030259
Iteration 95/1000 | Loss: 0.00029530
Iteration 96/1000 | Loss: 0.00026125
Iteration 97/1000 | Loss: 0.00034527
Iteration 98/1000 | Loss: 0.00030295
Iteration 99/1000 | Loss: 0.00031363
Iteration 100/1000 | Loss: 0.00029214
Iteration 101/1000 | Loss: 0.00030281
Iteration 102/1000 | Loss: 0.00024733
Iteration 103/1000 | Loss: 0.00028448
Iteration 104/1000 | Loss: 0.00025501
Iteration 105/1000 | Loss: 0.00027057
Iteration 106/1000 | Loss: 0.00031015
Iteration 107/1000 | Loss: 0.00029114
Iteration 108/1000 | Loss: 0.00054737
Iteration 109/1000 | Loss: 0.00024428
Iteration 110/1000 | Loss: 0.00028105
Iteration 111/1000 | Loss: 0.00023485
Iteration 112/1000 | Loss: 0.00030048
Iteration 113/1000 | Loss: 0.00034907
Iteration 114/1000 | Loss: 0.00032550
Iteration 115/1000 | Loss: 0.00032091
Iteration 116/1000 | Loss: 0.00025145
Iteration 117/1000 | Loss: 0.00005284
Iteration 118/1000 | Loss: 0.00004126
Iteration 119/1000 | Loss: 0.00012426
Iteration 120/1000 | Loss: 0.00007225
Iteration 121/1000 | Loss: 0.00019293
Iteration 122/1000 | Loss: 0.00013603
Iteration 123/1000 | Loss: 0.00002674
Iteration 124/1000 | Loss: 0.00003787
Iteration 125/1000 | Loss: 0.00003324
Iteration 126/1000 | Loss: 0.00011912
Iteration 127/1000 | Loss: 0.00002680
Iteration 128/1000 | Loss: 0.00008190
Iteration 129/1000 | Loss: 0.00038730
Iteration 130/1000 | Loss: 0.00014025
Iteration 131/1000 | Loss: 0.00012968
Iteration 132/1000 | Loss: 0.00002107
Iteration 133/1000 | Loss: 0.00002061
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00005668
Iteration 136/1000 | Loss: 0.00010534
Iteration 137/1000 | Loss: 0.00002996
Iteration 138/1000 | Loss: 0.00002548
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001943
Iteration 141/1000 | Loss: 0.00040071
Iteration 142/1000 | Loss: 0.00005696
Iteration 143/1000 | Loss: 0.00002007
Iteration 144/1000 | Loss: 0.00002826
Iteration 145/1000 | Loss: 0.00001899
Iteration 146/1000 | Loss: 0.00001897
Iteration 147/1000 | Loss: 0.00002462
Iteration 148/1000 | Loss: 0.00008014
Iteration 149/1000 | Loss: 0.00002529
Iteration 150/1000 | Loss: 0.00003921
Iteration 151/1000 | Loss: 0.00002925
Iteration 152/1000 | Loss: 0.00003673
Iteration 153/1000 | Loss: 0.00014063
Iteration 154/1000 | Loss: 0.00003076
Iteration 155/1000 | Loss: 0.00001849
Iteration 156/1000 | Loss: 0.00002196
Iteration 157/1000 | Loss: 0.00001831
Iteration 158/1000 | Loss: 0.00001831
Iteration 159/1000 | Loss: 0.00001831
Iteration 160/1000 | Loss: 0.00002187
Iteration 161/1000 | Loss: 0.00003142
Iteration 162/1000 | Loss: 0.00006082
Iteration 163/1000 | Loss: 0.00001891
Iteration 164/1000 | Loss: 0.00001828
Iteration 165/1000 | Loss: 0.00001827
Iteration 166/1000 | Loss: 0.00001872
Iteration 167/1000 | Loss: 0.00001819
Iteration 168/1000 | Loss: 0.00001828
Iteration 169/1000 | Loss: 0.00001823
Iteration 170/1000 | Loss: 0.00005766
Iteration 171/1000 | Loss: 0.00001826
Iteration 172/1000 | Loss: 0.00001866
Iteration 173/1000 | Loss: 0.00001804
Iteration 174/1000 | Loss: 0.00001802
Iteration 175/1000 | Loss: 0.00001801
Iteration 176/1000 | Loss: 0.00001801
Iteration 177/1000 | Loss: 0.00001801
Iteration 178/1000 | Loss: 0.00001801
Iteration 179/1000 | Loss: 0.00001801
Iteration 180/1000 | Loss: 0.00001801
Iteration 181/1000 | Loss: 0.00001801
Iteration 182/1000 | Loss: 0.00001801
Iteration 183/1000 | Loss: 0.00001800
Iteration 184/1000 | Loss: 0.00001800
Iteration 185/1000 | Loss: 0.00001800
Iteration 186/1000 | Loss: 0.00001800
Iteration 187/1000 | Loss: 0.00001800
Iteration 188/1000 | Loss: 0.00001800
Iteration 189/1000 | Loss: 0.00003265
Iteration 190/1000 | Loss: 0.00005329
Iteration 191/1000 | Loss: 0.00001824
Iteration 192/1000 | Loss: 0.00001808
Iteration 193/1000 | Loss: 0.00001931
Iteration 194/1000 | Loss: 0.00001797
Iteration 195/1000 | Loss: 0.00001796
Iteration 196/1000 | Loss: 0.00001794
Iteration 197/1000 | Loss: 0.00007147
Iteration 198/1000 | Loss: 0.00003052
Iteration 199/1000 | Loss: 0.00001912
Iteration 200/1000 | Loss: 0.00004883
Iteration 201/1000 | Loss: 0.00003304
Iteration 202/1000 | Loss: 0.00002024
Iteration 203/1000 | Loss: 0.00008261
Iteration 204/1000 | Loss: 0.00001793
Iteration 205/1000 | Loss: 0.00001790
Iteration 206/1000 | Loss: 0.00001790
Iteration 207/1000 | Loss: 0.00001790
Iteration 208/1000 | Loss: 0.00001790
Iteration 209/1000 | Loss: 0.00001790
Iteration 210/1000 | Loss: 0.00001790
Iteration 211/1000 | Loss: 0.00001790
Iteration 212/1000 | Loss: 0.00001790
Iteration 213/1000 | Loss: 0.00001790
Iteration 214/1000 | Loss: 0.00001790
Iteration 215/1000 | Loss: 0.00001790
Iteration 216/1000 | Loss: 0.00001789
Iteration 217/1000 | Loss: 0.00001789
Iteration 218/1000 | Loss: 0.00001789
Iteration 219/1000 | Loss: 0.00001789
Iteration 220/1000 | Loss: 0.00001789
Iteration 221/1000 | Loss: 0.00001788
Iteration 222/1000 | Loss: 0.00001788
Iteration 223/1000 | Loss: 0.00001787
Iteration 224/1000 | Loss: 0.00001787
Iteration 225/1000 | Loss: 0.00001787
Iteration 226/1000 | Loss: 0.00001787
Iteration 227/1000 | Loss: 0.00001787
Iteration 228/1000 | Loss: 0.00002757
Iteration 229/1000 | Loss: 0.00035548
Iteration 230/1000 | Loss: 0.00003657
Iteration 231/1000 | Loss: 0.00003292
Iteration 232/1000 | Loss: 0.00001995
Iteration 233/1000 | Loss: 0.00002051
Iteration 234/1000 | Loss: 0.00004357
Iteration 235/1000 | Loss: 0.00002276
Iteration 236/1000 | Loss: 0.00001737
Iteration 237/1000 | Loss: 0.00002125
Iteration 238/1000 | Loss: 0.00001702
Iteration 239/1000 | Loss: 0.00009173
Iteration 240/1000 | Loss: 0.00023538
Iteration 241/1000 | Loss: 0.00005323
Iteration 242/1000 | Loss: 0.00001634
Iteration 243/1000 | Loss: 0.00002090
Iteration 244/1000 | Loss: 0.00001594
Iteration 245/1000 | Loss: 0.00001594
Iteration 246/1000 | Loss: 0.00001594
Iteration 247/1000 | Loss: 0.00001594
Iteration 248/1000 | Loss: 0.00001594
Iteration 249/1000 | Loss: 0.00001594
Iteration 250/1000 | Loss: 0.00001594
Iteration 251/1000 | Loss: 0.00001594
Iteration 252/1000 | Loss: 0.00001594
Iteration 253/1000 | Loss: 0.00001594
Iteration 254/1000 | Loss: 0.00001594
Iteration 255/1000 | Loss: 0.00001594
Iteration 256/1000 | Loss: 0.00001594
Iteration 257/1000 | Loss: 0.00001594
Iteration 258/1000 | Loss: 0.00001594
Iteration 259/1000 | Loss: 0.00001594
Iteration 260/1000 | Loss: 0.00001594
Iteration 261/1000 | Loss: 0.00001594
Iteration 262/1000 | Loss: 0.00001594
Iteration 263/1000 | Loss: 0.00001594
Iteration 264/1000 | Loss: 0.00001594
Iteration 265/1000 | Loss: 0.00001594
Iteration 266/1000 | Loss: 0.00001594
Iteration 267/1000 | Loss: 0.00001594
Iteration 268/1000 | Loss: 0.00001594
Iteration 269/1000 | Loss: 0.00001594
Iteration 270/1000 | Loss: 0.00001594
Iteration 271/1000 | Loss: 0.00001594
Iteration 272/1000 | Loss: 0.00001594
Iteration 273/1000 | Loss: 0.00001594
Iteration 274/1000 | Loss: 0.00001594
Iteration 275/1000 | Loss: 0.00001594
Iteration 276/1000 | Loss: 0.00001594
Iteration 277/1000 | Loss: 0.00001594
Iteration 278/1000 | Loss: 0.00001594
Iteration 279/1000 | Loss: 0.00001594
Iteration 280/1000 | Loss: 0.00001594
Iteration 281/1000 | Loss: 0.00001594
Iteration 282/1000 | Loss: 0.00001594
Iteration 283/1000 | Loss: 0.00001594
Iteration 284/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.5937312127789482e-05, 1.5937312127789482e-05, 1.5937312127789482e-05, 1.5937312127789482e-05, 1.5937312127789482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5937312127789482e-05

Optimization complete. Final v2v error: 3.368809938430786 mm

Highest mean error: 6.703126430511475 mm for frame 174

Lowest mean error: 2.616321563720703 mm for frame 238

Saving results

Total time: 364.92175340652466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882128
Iteration 2/25 | Loss: 0.00119484
Iteration 3/25 | Loss: 0.00101324
Iteration 4/25 | Loss: 0.00099539
Iteration 5/25 | Loss: 0.00099053
Iteration 6/25 | Loss: 0.00098898
Iteration 7/25 | Loss: 0.00098860
Iteration 8/25 | Loss: 0.00098860
Iteration 9/25 | Loss: 0.00098860
Iteration 10/25 | Loss: 0.00098860
Iteration 11/25 | Loss: 0.00098860
Iteration 12/25 | Loss: 0.00098860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009886017069220543, 0.0009886017069220543, 0.0009886017069220543, 0.0009886017069220543, 0.0009886017069220543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009886017069220543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48132360
Iteration 2/25 | Loss: 0.00087785
Iteration 3/25 | Loss: 0.00087785
Iteration 4/25 | Loss: 0.00087785
Iteration 5/25 | Loss: 0.00087785
Iteration 6/25 | Loss: 0.00087785
Iteration 7/25 | Loss: 0.00087785
Iteration 8/25 | Loss: 0.00087785
Iteration 9/25 | Loss: 0.00087785
Iteration 10/25 | Loss: 0.00087785
Iteration 11/25 | Loss: 0.00087785
Iteration 12/25 | Loss: 0.00087785
Iteration 13/25 | Loss: 0.00087785
Iteration 14/25 | Loss: 0.00087785
Iteration 15/25 | Loss: 0.00087785
Iteration 16/25 | Loss: 0.00087785
Iteration 17/25 | Loss: 0.00087785
Iteration 18/25 | Loss: 0.00087785
Iteration 19/25 | Loss: 0.00087785
Iteration 20/25 | Loss: 0.00087785
Iteration 21/25 | Loss: 0.00087785
Iteration 22/25 | Loss: 0.00087785
Iteration 23/25 | Loss: 0.00087785
Iteration 24/25 | Loss: 0.00087785
Iteration 25/25 | Loss: 0.00087785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087785
Iteration 2/1000 | Loss: 0.00002923
Iteration 3/1000 | Loss: 0.00002442
Iteration 4/1000 | Loss: 0.00002219
Iteration 5/1000 | Loss: 0.00002129
Iteration 6/1000 | Loss: 0.00002070
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00002006
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00002002
Iteration 11/1000 | Loss: 0.00002002
Iteration 12/1000 | Loss: 0.00002000
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001991
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001985
Iteration 20/1000 | Loss: 0.00001985
Iteration 21/1000 | Loss: 0.00001978
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001973
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001972
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001971
Iteration 30/1000 | Loss: 0.00001970
Iteration 31/1000 | Loss: 0.00001970
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001968
Iteration 37/1000 | Loss: 0.00001968
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001964
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001963
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001961
Iteration 53/1000 | Loss: 0.00001961
Iteration 54/1000 | Loss: 0.00001961
Iteration 55/1000 | Loss: 0.00001961
Iteration 56/1000 | Loss: 0.00001961
Iteration 57/1000 | Loss: 0.00001960
Iteration 58/1000 | Loss: 0.00001960
Iteration 59/1000 | Loss: 0.00001960
Iteration 60/1000 | Loss: 0.00001960
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00001958
Iteration 72/1000 | Loss: 0.00001958
Iteration 73/1000 | Loss: 0.00001958
Iteration 74/1000 | Loss: 0.00001958
Iteration 75/1000 | Loss: 0.00001958
Iteration 76/1000 | Loss: 0.00001958
Iteration 77/1000 | Loss: 0.00001958
Iteration 78/1000 | Loss: 0.00001958
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001958
Iteration 88/1000 | Loss: 0.00001958
Iteration 89/1000 | Loss: 0.00001958
Iteration 90/1000 | Loss: 0.00001958
Iteration 91/1000 | Loss: 0.00001958
Iteration 92/1000 | Loss: 0.00001958
Iteration 93/1000 | Loss: 0.00001958
Iteration 94/1000 | Loss: 0.00001958
Iteration 95/1000 | Loss: 0.00001958
Iteration 96/1000 | Loss: 0.00001958
Iteration 97/1000 | Loss: 0.00001958
Iteration 98/1000 | Loss: 0.00001958
Iteration 99/1000 | Loss: 0.00001958
Iteration 100/1000 | Loss: 0.00001958
Iteration 101/1000 | Loss: 0.00001958
Iteration 102/1000 | Loss: 0.00001958
Iteration 103/1000 | Loss: 0.00001958
Iteration 104/1000 | Loss: 0.00001958
Iteration 105/1000 | Loss: 0.00001958
Iteration 106/1000 | Loss: 0.00001958
Iteration 107/1000 | Loss: 0.00001958
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001958
Iteration 112/1000 | Loss: 0.00001958
Iteration 113/1000 | Loss: 0.00001958
Iteration 114/1000 | Loss: 0.00001958
Iteration 115/1000 | Loss: 0.00001958
Iteration 116/1000 | Loss: 0.00001958
Iteration 117/1000 | Loss: 0.00001958
Iteration 118/1000 | Loss: 0.00001958
Iteration 119/1000 | Loss: 0.00001958
Iteration 120/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.9582586901378818e-05, 1.9582586901378818e-05, 1.9582586901378818e-05, 1.9582586901378818e-05, 1.9582586901378818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9582586901378818e-05

Optimization complete. Final v2v error: 3.8388357162475586 mm

Highest mean error: 4.021881103515625 mm for frame 101

Lowest mean error: 3.211097478866577 mm for frame 154

Saving results

Total time: 31.416176557540894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143413
Iteration 2/25 | Loss: 0.00271886
Iteration 3/25 | Loss: 0.00125131
Iteration 4/25 | Loss: 0.00110269
Iteration 5/25 | Loss: 0.00104547
Iteration 6/25 | Loss: 0.00098614
Iteration 7/25 | Loss: 0.00095916
Iteration 8/25 | Loss: 0.00094955
Iteration 9/25 | Loss: 0.00093697
Iteration 10/25 | Loss: 0.00093622
Iteration 11/25 | Loss: 0.00091304
Iteration 12/25 | Loss: 0.00090544
Iteration 13/25 | Loss: 0.00089701
Iteration 14/25 | Loss: 0.00089444
Iteration 15/25 | Loss: 0.00089701
Iteration 16/25 | Loss: 0.00089325
Iteration 17/25 | Loss: 0.00089651
Iteration 18/25 | Loss: 0.00089299
Iteration 19/25 | Loss: 0.00089296
Iteration 20/25 | Loss: 0.00089295
Iteration 21/25 | Loss: 0.00089295
Iteration 22/25 | Loss: 0.00089295
Iteration 23/25 | Loss: 0.00089478
Iteration 24/25 | Loss: 0.00089294
Iteration 25/25 | Loss: 0.00089294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53171241
Iteration 2/25 | Loss: 0.00096574
Iteration 3/25 | Loss: 0.00091159
Iteration 4/25 | Loss: 0.00091157
Iteration 5/25 | Loss: 0.00091157
Iteration 6/25 | Loss: 0.00091157
Iteration 7/25 | Loss: 0.00091157
Iteration 8/25 | Loss: 0.00091157
Iteration 9/25 | Loss: 0.00091157
Iteration 10/25 | Loss: 0.00091157
Iteration 11/25 | Loss: 0.00091157
Iteration 12/25 | Loss: 0.00091157
Iteration 13/25 | Loss: 0.00091157
Iteration 14/25 | Loss: 0.00091157
Iteration 15/25 | Loss: 0.00091157
Iteration 16/25 | Loss: 0.00091157
Iteration 17/25 | Loss: 0.00091157
Iteration 18/25 | Loss: 0.00091157
Iteration 19/25 | Loss: 0.00091157
Iteration 20/25 | Loss: 0.00091157
Iteration 21/25 | Loss: 0.00091157
Iteration 22/25 | Loss: 0.00091157
Iteration 23/25 | Loss: 0.00091157
Iteration 24/25 | Loss: 0.00091157
Iteration 25/25 | Loss: 0.00091157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009115681750699878, 0.0009115681750699878, 0.0009115681750699878, 0.0009115681750699878, 0.0009115681750699878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009115681750699878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091157
Iteration 2/1000 | Loss: 0.00011693
Iteration 3/1000 | Loss: 0.00008063
Iteration 4/1000 | Loss: 0.00002048
Iteration 5/1000 | Loss: 0.00004487
Iteration 6/1000 | Loss: 0.00091779
Iteration 7/1000 | Loss: 0.00007055
Iteration 8/1000 | Loss: 0.00003392
Iteration 9/1000 | Loss: 0.00003905
Iteration 10/1000 | Loss: 0.00001691
Iteration 11/1000 | Loss: 0.00002269
Iteration 12/1000 | Loss: 0.00002379
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00002406
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001493
Iteration 20/1000 | Loss: 0.00003125
Iteration 21/1000 | Loss: 0.00004534
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00003205
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00003337
Iteration 27/1000 | Loss: 0.00001841
Iteration 28/1000 | Loss: 0.00005592
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00005595
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00002340
Iteration 33/1000 | Loss: 0.00005128
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001431
Iteration 89/1000 | Loss: 0.00001431
Iteration 90/1000 | Loss: 0.00001431
Iteration 91/1000 | Loss: 0.00001431
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001430
Iteration 94/1000 | Loss: 0.00001430
Iteration 95/1000 | Loss: 0.00001430
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001429
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001429
Iteration 117/1000 | Loss: 0.00001429
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001429
Iteration 124/1000 | Loss: 0.00001429
Iteration 125/1000 | Loss: 0.00001429
Iteration 126/1000 | Loss: 0.00001429
Iteration 127/1000 | Loss: 0.00001429
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.4285630641097669e-05, 1.4285630641097669e-05, 1.4285630641097669e-05, 1.4285630641097669e-05, 1.4285630641097669e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4285630641097669e-05

Optimization complete. Final v2v error: 3.1468794345855713 mm

Highest mean error: 10.22161865234375 mm for frame 123

Lowest mean error: 2.8402984142303467 mm for frame 37

Saving results

Total time: 82.20288681983948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772054
Iteration 2/25 | Loss: 0.00120586
Iteration 3/25 | Loss: 0.00108418
Iteration 4/25 | Loss: 0.00107090
Iteration 5/25 | Loss: 0.00106575
Iteration 6/25 | Loss: 0.00106472
Iteration 7/25 | Loss: 0.00106472
Iteration 8/25 | Loss: 0.00106472
Iteration 9/25 | Loss: 0.00106472
Iteration 10/25 | Loss: 0.00106472
Iteration 11/25 | Loss: 0.00106472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010647218441590667, 0.0010647218441590667, 0.0010647218441590667, 0.0010647218441590667, 0.0010647218441590667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010647218441590667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48263276
Iteration 2/25 | Loss: 0.00095992
Iteration 3/25 | Loss: 0.00095991
Iteration 4/25 | Loss: 0.00095991
Iteration 5/25 | Loss: 0.00095991
Iteration 6/25 | Loss: 0.00095991
Iteration 7/25 | Loss: 0.00095991
Iteration 8/25 | Loss: 0.00095991
Iteration 9/25 | Loss: 0.00095991
Iteration 10/25 | Loss: 0.00095991
Iteration 11/25 | Loss: 0.00095991
Iteration 12/25 | Loss: 0.00095991
Iteration 13/25 | Loss: 0.00095991
Iteration 14/25 | Loss: 0.00095991
Iteration 15/25 | Loss: 0.00095991
Iteration 16/25 | Loss: 0.00095991
Iteration 17/25 | Loss: 0.00095991
Iteration 18/25 | Loss: 0.00095991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009599124896340072, 0.0009599124896340072, 0.0009599124896340072, 0.0009599124896340072, 0.0009599124896340072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009599124896340072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095991
Iteration 2/1000 | Loss: 0.00004301
Iteration 3/1000 | Loss: 0.00003263
Iteration 4/1000 | Loss: 0.00002970
Iteration 5/1000 | Loss: 0.00002812
Iteration 6/1000 | Loss: 0.00002730
Iteration 7/1000 | Loss: 0.00002674
Iteration 8/1000 | Loss: 0.00002618
Iteration 9/1000 | Loss: 0.00002577
Iteration 10/1000 | Loss: 0.00002539
Iteration 11/1000 | Loss: 0.00002518
Iteration 12/1000 | Loss: 0.00002503
Iteration 13/1000 | Loss: 0.00002500
Iteration 14/1000 | Loss: 0.00002498
Iteration 15/1000 | Loss: 0.00002497
Iteration 16/1000 | Loss: 0.00002496
Iteration 17/1000 | Loss: 0.00002495
Iteration 18/1000 | Loss: 0.00002494
Iteration 19/1000 | Loss: 0.00002492
Iteration 20/1000 | Loss: 0.00002490
Iteration 21/1000 | Loss: 0.00002485
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002482
Iteration 26/1000 | Loss: 0.00002480
Iteration 27/1000 | Loss: 0.00002480
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002479
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002479
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002479
Iteration 37/1000 | Loss: 0.00002479
Iteration 38/1000 | Loss: 0.00002479
Iteration 39/1000 | Loss: 0.00002478
Iteration 40/1000 | Loss: 0.00002478
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002478
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002478
Iteration 49/1000 | Loss: 0.00002478
Iteration 50/1000 | Loss: 0.00002478
Iteration 51/1000 | Loss: 0.00002477
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00002477
Iteration 54/1000 | Loss: 0.00002477
Iteration 55/1000 | Loss: 0.00002476
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002476
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00002475
Iteration 61/1000 | Loss: 0.00002475
Iteration 62/1000 | Loss: 0.00002475
Iteration 63/1000 | Loss: 0.00002475
Iteration 64/1000 | Loss: 0.00002475
Iteration 65/1000 | Loss: 0.00002475
Iteration 66/1000 | Loss: 0.00002475
Iteration 67/1000 | Loss: 0.00002474
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002474
Iteration 74/1000 | Loss: 0.00002474
Iteration 75/1000 | Loss: 0.00002474
Iteration 76/1000 | Loss: 0.00002474
Iteration 77/1000 | Loss: 0.00002474
Iteration 78/1000 | Loss: 0.00002474
Iteration 79/1000 | Loss: 0.00002474
Iteration 80/1000 | Loss: 0.00002473
Iteration 81/1000 | Loss: 0.00002473
Iteration 82/1000 | Loss: 0.00002473
Iteration 83/1000 | Loss: 0.00002473
Iteration 84/1000 | Loss: 0.00002473
Iteration 85/1000 | Loss: 0.00002473
Iteration 86/1000 | Loss: 0.00002473
Iteration 87/1000 | Loss: 0.00002473
Iteration 88/1000 | Loss: 0.00002473
Iteration 89/1000 | Loss: 0.00002473
Iteration 90/1000 | Loss: 0.00002473
Iteration 91/1000 | Loss: 0.00002473
Iteration 92/1000 | Loss: 0.00002472
Iteration 93/1000 | Loss: 0.00002472
Iteration 94/1000 | Loss: 0.00002472
Iteration 95/1000 | Loss: 0.00002472
Iteration 96/1000 | Loss: 0.00002472
Iteration 97/1000 | Loss: 0.00002472
Iteration 98/1000 | Loss: 0.00002472
Iteration 99/1000 | Loss: 0.00002471
Iteration 100/1000 | Loss: 0.00002471
Iteration 101/1000 | Loss: 0.00002471
Iteration 102/1000 | Loss: 0.00002471
Iteration 103/1000 | Loss: 0.00002471
Iteration 104/1000 | Loss: 0.00002471
Iteration 105/1000 | Loss: 0.00002471
Iteration 106/1000 | Loss: 0.00002471
Iteration 107/1000 | Loss: 0.00002471
Iteration 108/1000 | Loss: 0.00002471
Iteration 109/1000 | Loss: 0.00002471
Iteration 110/1000 | Loss: 0.00002471
Iteration 111/1000 | Loss: 0.00002471
Iteration 112/1000 | Loss: 0.00002471
Iteration 113/1000 | Loss: 0.00002471
Iteration 114/1000 | Loss: 0.00002471
Iteration 115/1000 | Loss: 0.00002471
Iteration 116/1000 | Loss: 0.00002471
Iteration 117/1000 | Loss: 0.00002471
Iteration 118/1000 | Loss: 0.00002471
Iteration 119/1000 | Loss: 0.00002471
Iteration 120/1000 | Loss: 0.00002471
Iteration 121/1000 | Loss: 0.00002471
Iteration 122/1000 | Loss: 0.00002471
Iteration 123/1000 | Loss: 0.00002471
Iteration 124/1000 | Loss: 0.00002471
Iteration 125/1000 | Loss: 0.00002471
Iteration 126/1000 | Loss: 0.00002471
Iteration 127/1000 | Loss: 0.00002471
Iteration 128/1000 | Loss: 0.00002471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.4711054720683023e-05, 2.4711054720683023e-05, 2.4711054720683023e-05, 2.4711054720683023e-05, 2.4711054720683023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4711054720683023e-05

Optimization complete. Final v2v error: 4.306558132171631 mm

Highest mean error: 5.352528095245361 mm for frame 210

Lowest mean error: 3.6581530570983887 mm for frame 15

Saving results

Total time: 38.89485955238342
