Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=88, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4928-4983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01198727
Iteration 2/25 | Loss: 0.00400445
Iteration 3/25 | Loss: 0.00300267
Iteration 4/25 | Loss: 0.00253838
Iteration 5/25 | Loss: 0.00217409
Iteration 6/25 | Loss: 0.00202394
Iteration 7/25 | Loss: 0.00196648
Iteration 8/25 | Loss: 0.00191986
Iteration 9/25 | Loss: 0.00188284
Iteration 10/25 | Loss: 0.00188225
Iteration 11/25 | Loss: 0.00187388
Iteration 12/25 | Loss: 0.00185861
Iteration 13/25 | Loss: 0.00185047
Iteration 14/25 | Loss: 0.00183535
Iteration 15/25 | Loss: 0.00183254
Iteration 16/25 | Loss: 0.00182824
Iteration 17/25 | Loss: 0.00181411
Iteration 18/25 | Loss: 0.00180307
Iteration 19/25 | Loss: 0.00180476
Iteration 20/25 | Loss: 0.00179806
Iteration 21/25 | Loss: 0.00179930
Iteration 22/25 | Loss: 0.00180109
Iteration 23/25 | Loss: 0.00179875
Iteration 24/25 | Loss: 0.00180161
Iteration 25/25 | Loss: 0.00180296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59964782
Iteration 2/25 | Loss: 0.00536149
Iteration 3/25 | Loss: 0.00476858
Iteration 4/25 | Loss: 0.00476858
Iteration 5/25 | Loss: 0.00476857
Iteration 6/25 | Loss: 0.00476857
Iteration 7/25 | Loss: 0.00476857
Iteration 8/25 | Loss: 0.00476857
Iteration 9/25 | Loss: 0.00476857
Iteration 10/25 | Loss: 0.00476857
Iteration 11/25 | Loss: 0.00476857
Iteration 12/25 | Loss: 0.00476857
Iteration 13/25 | Loss: 0.00476857
Iteration 14/25 | Loss: 0.00476857
Iteration 15/25 | Loss: 0.00476857
Iteration 16/25 | Loss: 0.00476857
Iteration 17/25 | Loss: 0.00476857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00476857228204608, 0.00476857228204608, 0.00476857228204608, 0.00476857228204608, 0.00476857228204608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00476857228204608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00476857
Iteration 2/1000 | Loss: 0.00115949
Iteration 3/1000 | Loss: 0.00100080
Iteration 4/1000 | Loss: 0.00075634
Iteration 5/1000 | Loss: 0.00119460
Iteration 6/1000 | Loss: 0.00147840
Iteration 7/1000 | Loss: 0.00027032
Iteration 8/1000 | Loss: 0.00041574
Iteration 9/1000 | Loss: 0.00089544
Iteration 10/1000 | Loss: 0.00052056
Iteration 11/1000 | Loss: 0.00065400
Iteration 12/1000 | Loss: 0.00149350
Iteration 13/1000 | Loss: 0.00048645
Iteration 14/1000 | Loss: 0.00022297
Iteration 15/1000 | Loss: 0.00045553
Iteration 16/1000 | Loss: 0.00061929
Iteration 17/1000 | Loss: 0.00051330
Iteration 18/1000 | Loss: 0.00118560
Iteration 19/1000 | Loss: 0.00034651
Iteration 20/1000 | Loss: 0.00042363
Iteration 21/1000 | Loss: 0.00035380
Iteration 22/1000 | Loss: 0.00047335
Iteration 23/1000 | Loss: 0.00038610
Iteration 24/1000 | Loss: 0.00120138
Iteration 25/1000 | Loss: 0.00155198
Iteration 26/1000 | Loss: 0.00073666
Iteration 27/1000 | Loss: 0.00093675
Iteration 28/1000 | Loss: 0.00095141
Iteration 29/1000 | Loss: 0.00084165
Iteration 30/1000 | Loss: 0.00106003
Iteration 31/1000 | Loss: 0.00076954
Iteration 32/1000 | Loss: 0.00057168
Iteration 33/1000 | Loss: 0.00038142
Iteration 34/1000 | Loss: 0.00042206
Iteration 35/1000 | Loss: 0.00032739
Iteration 36/1000 | Loss: 0.00073040
Iteration 37/1000 | Loss: 0.00076073
Iteration 38/1000 | Loss: 0.00057746
Iteration 39/1000 | Loss: 0.00015421
Iteration 40/1000 | Loss: 0.00015659
Iteration 41/1000 | Loss: 0.00040423
Iteration 42/1000 | Loss: 0.00096426
Iteration 43/1000 | Loss: 0.00021481
Iteration 44/1000 | Loss: 0.00083185
Iteration 45/1000 | Loss: 0.00061677
Iteration 46/1000 | Loss: 0.00023441
Iteration 47/1000 | Loss: 0.00020767
Iteration 48/1000 | Loss: 0.00103488
Iteration 49/1000 | Loss: 0.00033050
Iteration 50/1000 | Loss: 0.00082601
Iteration 51/1000 | Loss: 0.00033552
Iteration 52/1000 | Loss: 0.00030398
Iteration 53/1000 | Loss: 0.00041136
Iteration 54/1000 | Loss: 0.00118123
Iteration 55/1000 | Loss: 0.00084157
Iteration 56/1000 | Loss: 0.00101713
Iteration 57/1000 | Loss: 0.00048233
Iteration 58/1000 | Loss: 0.00063120
Iteration 59/1000 | Loss: 0.00031754
Iteration 60/1000 | Loss: 0.00043634
Iteration 61/1000 | Loss: 0.00033872
Iteration 62/1000 | Loss: 0.00031634
Iteration 63/1000 | Loss: 0.00155713
Iteration 64/1000 | Loss: 0.00137495
Iteration 65/1000 | Loss: 0.00060380
Iteration 66/1000 | Loss: 0.00035857
Iteration 67/1000 | Loss: 0.00032082
Iteration 68/1000 | Loss: 0.00019975
Iteration 69/1000 | Loss: 0.00070126
Iteration 70/1000 | Loss: 0.00117061
Iteration 71/1000 | Loss: 0.00079678
Iteration 72/1000 | Loss: 0.00056663
Iteration 73/1000 | Loss: 0.00037610
Iteration 74/1000 | Loss: 0.00017198
Iteration 75/1000 | Loss: 0.00029217
Iteration 76/1000 | Loss: 0.00027520
Iteration 77/1000 | Loss: 0.00066854
Iteration 78/1000 | Loss: 0.00019162
Iteration 79/1000 | Loss: 0.00071431
Iteration 80/1000 | Loss: 0.00038722
Iteration 81/1000 | Loss: 0.00090198
Iteration 82/1000 | Loss: 0.00048988
Iteration 83/1000 | Loss: 0.00015183
Iteration 84/1000 | Loss: 0.00041368
Iteration 85/1000 | Loss: 0.00060358
Iteration 86/1000 | Loss: 0.00042330
Iteration 87/1000 | Loss: 0.00049565
Iteration 88/1000 | Loss: 0.00045335
Iteration 89/1000 | Loss: 0.00039936
Iteration 90/1000 | Loss: 0.00033016
Iteration 91/1000 | Loss: 0.00028400
Iteration 92/1000 | Loss: 0.00041755
Iteration 93/1000 | Loss: 0.00043298
Iteration 94/1000 | Loss: 0.00111007
Iteration 95/1000 | Loss: 0.00019953
Iteration 96/1000 | Loss: 0.00013342
Iteration 97/1000 | Loss: 0.00068346
Iteration 98/1000 | Loss: 0.00032239
Iteration 99/1000 | Loss: 0.00093844
Iteration 100/1000 | Loss: 0.00018502
Iteration 101/1000 | Loss: 0.00019936
Iteration 102/1000 | Loss: 0.00025023
Iteration 103/1000 | Loss: 0.00012797
Iteration 104/1000 | Loss: 0.00012454
Iteration 105/1000 | Loss: 0.00012070
Iteration 106/1000 | Loss: 0.00034075
Iteration 107/1000 | Loss: 0.00030898
Iteration 108/1000 | Loss: 0.00013281
Iteration 109/1000 | Loss: 0.00011862
Iteration 110/1000 | Loss: 0.00017902
Iteration 111/1000 | Loss: 0.00016751
Iteration 112/1000 | Loss: 0.00011617
Iteration 113/1000 | Loss: 0.00011194
Iteration 114/1000 | Loss: 0.00010668
Iteration 115/1000 | Loss: 0.00010117
Iteration 116/1000 | Loss: 0.00061921
Iteration 117/1000 | Loss: 0.00061725
Iteration 118/1000 | Loss: 0.00045119
Iteration 119/1000 | Loss: 0.00029977
Iteration 120/1000 | Loss: 0.00019979
Iteration 121/1000 | Loss: 0.00011381
Iteration 122/1000 | Loss: 0.00021138
Iteration 123/1000 | Loss: 0.00017986
Iteration 124/1000 | Loss: 0.00025646
Iteration 125/1000 | Loss: 0.00022821
Iteration 126/1000 | Loss: 0.00017418
Iteration 127/1000 | Loss: 0.00022107
Iteration 128/1000 | Loss: 0.00019203
Iteration 129/1000 | Loss: 0.00037821
Iteration 130/1000 | Loss: 0.00029132
Iteration 131/1000 | Loss: 0.00037972
Iteration 132/1000 | Loss: 0.00012948
Iteration 133/1000 | Loss: 0.00012138
Iteration 134/1000 | Loss: 0.00011812
Iteration 135/1000 | Loss: 0.00012537
Iteration 136/1000 | Loss: 0.00012971
Iteration 137/1000 | Loss: 0.00017377
Iteration 138/1000 | Loss: 0.00012695
Iteration 139/1000 | Loss: 0.00016379
Iteration 140/1000 | Loss: 0.00013690
Iteration 141/1000 | Loss: 0.00016046
Iteration 142/1000 | Loss: 0.00016774
Iteration 143/1000 | Loss: 0.00022652
Iteration 144/1000 | Loss: 0.00013280
Iteration 145/1000 | Loss: 0.00025408
Iteration 146/1000 | Loss: 0.00021007
Iteration 147/1000 | Loss: 0.00062080
Iteration 148/1000 | Loss: 0.00025795
Iteration 149/1000 | Loss: 0.00026361
Iteration 150/1000 | Loss: 0.00027946
Iteration 151/1000 | Loss: 0.00124570
Iteration 152/1000 | Loss: 0.00037315
Iteration 153/1000 | Loss: 0.00067855
Iteration 154/1000 | Loss: 0.00040642
Iteration 155/1000 | Loss: 0.00029216
Iteration 156/1000 | Loss: 0.00039590
Iteration 157/1000 | Loss: 0.00049807
Iteration 158/1000 | Loss: 0.00028423
Iteration 159/1000 | Loss: 0.00035291
Iteration 160/1000 | Loss: 0.00030292
Iteration 161/1000 | Loss: 0.00011897
Iteration 162/1000 | Loss: 0.00012030
Iteration 163/1000 | Loss: 0.00011807
Iteration 164/1000 | Loss: 0.00011150
Iteration 165/1000 | Loss: 0.00029395
Iteration 166/1000 | Loss: 0.00088092
Iteration 167/1000 | Loss: 0.00037670
Iteration 168/1000 | Loss: 0.00086075
Iteration 169/1000 | Loss: 0.00028028
Iteration 170/1000 | Loss: 0.00016036
Iteration 171/1000 | Loss: 0.00024349
Iteration 172/1000 | Loss: 0.00130937
Iteration 173/1000 | Loss: 0.00080703
Iteration 174/1000 | Loss: 0.00017236
Iteration 175/1000 | Loss: 0.00036663
Iteration 176/1000 | Loss: 0.00034232
Iteration 177/1000 | Loss: 0.00041026
Iteration 178/1000 | Loss: 0.00054461
Iteration 179/1000 | Loss: 0.00033552
Iteration 180/1000 | Loss: 0.00028944
Iteration 181/1000 | Loss: 0.00033028
Iteration 182/1000 | Loss: 0.00011500
Iteration 183/1000 | Loss: 0.00098343
Iteration 184/1000 | Loss: 0.00062216
Iteration 185/1000 | Loss: 0.00088674
Iteration 186/1000 | Loss: 0.00059417
Iteration 187/1000 | Loss: 0.00010178
Iteration 188/1000 | Loss: 0.00011422
Iteration 189/1000 | Loss: 0.00027926
Iteration 190/1000 | Loss: 0.00028786
Iteration 191/1000 | Loss: 0.00026340
Iteration 192/1000 | Loss: 0.00027318
Iteration 193/1000 | Loss: 0.00027324
Iteration 194/1000 | Loss: 0.00027118
Iteration 195/1000 | Loss: 0.00023502
Iteration 196/1000 | Loss: 0.00020471
Iteration 197/1000 | Loss: 0.00012141
Iteration 198/1000 | Loss: 0.00025746
Iteration 199/1000 | Loss: 0.00034206
Iteration 200/1000 | Loss: 0.00013587
Iteration 201/1000 | Loss: 0.00019324
Iteration 202/1000 | Loss: 0.00018585
Iteration 203/1000 | Loss: 0.00017954
Iteration 204/1000 | Loss: 0.00019428
Iteration 205/1000 | Loss: 0.00017382
Iteration 206/1000 | Loss: 0.00018952
Iteration 207/1000 | Loss: 0.00022943
Iteration 208/1000 | Loss: 0.00022725
Iteration 209/1000 | Loss: 0.00023784
Iteration 210/1000 | Loss: 0.00020667
Iteration 211/1000 | Loss: 0.00017236
Iteration 212/1000 | Loss: 0.00011724
Iteration 213/1000 | Loss: 0.00017208
Iteration 214/1000 | Loss: 0.00019255
Iteration 215/1000 | Loss: 0.00012034
Iteration 216/1000 | Loss: 0.00023456
Iteration 217/1000 | Loss: 0.00023665
Iteration 218/1000 | Loss: 0.00021794
Iteration 219/1000 | Loss: 0.00012727
Iteration 220/1000 | Loss: 0.00022451
Iteration 221/1000 | Loss: 0.00014620
Iteration 222/1000 | Loss: 0.00010741
Iteration 223/1000 | Loss: 0.00011226
Iteration 224/1000 | Loss: 0.00011159
Iteration 225/1000 | Loss: 0.00011121
Iteration 226/1000 | Loss: 0.00011084
Iteration 227/1000 | Loss: 0.00010850
Iteration 228/1000 | Loss: 0.00030355
Iteration 229/1000 | Loss: 0.00018892
Iteration 230/1000 | Loss: 0.00032388
Iteration 231/1000 | Loss: 0.00037210
Iteration 232/1000 | Loss: 0.00010542
Iteration 233/1000 | Loss: 0.00024372
Iteration 234/1000 | Loss: 0.00023005
Iteration 235/1000 | Loss: 0.00010939
Iteration 236/1000 | Loss: 0.00021772
Iteration 237/1000 | Loss: 0.00025369
Iteration 238/1000 | Loss: 0.00022379
Iteration 239/1000 | Loss: 0.00021053
Iteration 240/1000 | Loss: 0.00021065
Iteration 241/1000 | Loss: 0.00022868
Iteration 242/1000 | Loss: 0.00019682
Iteration 243/1000 | Loss: 0.00022487
Iteration 244/1000 | Loss: 0.00018107
Iteration 245/1000 | Loss: 0.00021791
Iteration 246/1000 | Loss: 0.00018797
Iteration 247/1000 | Loss: 0.00021345
Iteration 248/1000 | Loss: 0.00019691
Iteration 249/1000 | Loss: 0.00024469
Iteration 250/1000 | Loss: 0.00038341
Iteration 251/1000 | Loss: 0.00022622
Iteration 252/1000 | Loss: 0.00016830
Iteration 253/1000 | Loss: 0.00011809
Iteration 254/1000 | Loss: 0.00015438
Iteration 255/1000 | Loss: 0.00015671
Iteration 256/1000 | Loss: 0.00093158
Iteration 257/1000 | Loss: 0.00035959
Iteration 258/1000 | Loss: 0.00062995
Iteration 259/1000 | Loss: 0.00068543
Iteration 260/1000 | Loss: 0.00017924
Iteration 261/1000 | Loss: 0.00024460
Iteration 262/1000 | Loss: 0.00010933
Iteration 263/1000 | Loss: 0.00013804
Iteration 264/1000 | Loss: 0.00020360
Iteration 265/1000 | Loss: 0.00022189
Iteration 266/1000 | Loss: 0.00019471
Iteration 267/1000 | Loss: 0.00012743
Iteration 268/1000 | Loss: 0.00020560
Iteration 269/1000 | Loss: 0.00037643
Iteration 270/1000 | Loss: 0.00028035
Iteration 271/1000 | Loss: 0.00011010
Iteration 272/1000 | Loss: 0.00010056
Iteration 273/1000 | Loss: 0.00010226
Iteration 274/1000 | Loss: 0.00009601
Iteration 275/1000 | Loss: 0.00031533
Iteration 276/1000 | Loss: 0.00028861
Iteration 277/1000 | Loss: 0.00010087
Iteration 278/1000 | Loss: 0.00009156
Iteration 279/1000 | Loss: 0.00008806
Iteration 280/1000 | Loss: 0.00008561
Iteration 281/1000 | Loss: 0.00179637
Iteration 282/1000 | Loss: 0.00167798
Iteration 283/1000 | Loss: 0.00130145
Iteration 284/1000 | Loss: 0.00011490
Iteration 285/1000 | Loss: 0.00054212
Iteration 286/1000 | Loss: 0.00009033
Iteration 287/1000 | Loss: 0.00009632
Iteration 288/1000 | Loss: 0.00008558
Iteration 289/1000 | Loss: 0.00142258
Iteration 290/1000 | Loss: 0.00199919
Iteration 291/1000 | Loss: 0.00045059
Iteration 292/1000 | Loss: 0.00011153
Iteration 293/1000 | Loss: 0.00009241
Iteration 294/1000 | Loss: 0.00008568
Iteration 295/1000 | Loss: 0.00026159
Iteration 296/1000 | Loss: 0.00014507
Iteration 297/1000 | Loss: 0.00008939
Iteration 298/1000 | Loss: 0.00020090
Iteration 299/1000 | Loss: 0.00013193
Iteration 300/1000 | Loss: 0.00027881
Iteration 301/1000 | Loss: 0.00021782
Iteration 302/1000 | Loss: 0.00024052
Iteration 303/1000 | Loss: 0.00014910
Iteration 304/1000 | Loss: 0.00008190
Iteration 305/1000 | Loss: 0.00008028
Iteration 306/1000 | Loss: 0.00020253
Iteration 307/1000 | Loss: 0.00015918
Iteration 308/1000 | Loss: 0.00017889
Iteration 309/1000 | Loss: 0.00008008
Iteration 310/1000 | Loss: 0.00007721
Iteration 311/1000 | Loss: 0.00007606
Iteration 312/1000 | Loss: 0.00007539
Iteration 313/1000 | Loss: 0.00048211
Iteration 314/1000 | Loss: 0.00007636
Iteration 315/1000 | Loss: 0.00007340
Iteration 316/1000 | Loss: 0.00007178
Iteration 317/1000 | Loss: 0.00007042
Iteration 318/1000 | Loss: 0.00006935
Iteration 319/1000 | Loss: 0.00006853
Iteration 320/1000 | Loss: 0.00006787
Iteration 321/1000 | Loss: 0.00006742
Iteration 322/1000 | Loss: 0.00006697
Iteration 323/1000 | Loss: 0.00006655
Iteration 324/1000 | Loss: 0.00091764
Iteration 325/1000 | Loss: 0.00008489
Iteration 326/1000 | Loss: 0.00007406
Iteration 327/1000 | Loss: 0.00006972
Iteration 328/1000 | Loss: 0.00006686
Iteration 329/1000 | Loss: 0.00007523
Iteration 330/1000 | Loss: 0.00006738
Iteration 331/1000 | Loss: 0.00006580
Iteration 332/1000 | Loss: 0.00006529
Iteration 333/1000 | Loss: 0.00006458
Iteration 334/1000 | Loss: 0.00006411
Iteration 335/1000 | Loss: 0.00006381
Iteration 336/1000 | Loss: 0.00006334
Iteration 337/1000 | Loss: 0.00006300
Iteration 338/1000 | Loss: 0.00006272
Iteration 339/1000 | Loss: 0.00006241
Iteration 340/1000 | Loss: 0.00006218
Iteration 341/1000 | Loss: 0.00006216
Iteration 342/1000 | Loss: 0.00006210
Iteration 343/1000 | Loss: 0.00006196
Iteration 344/1000 | Loss: 0.00006186
Iteration 345/1000 | Loss: 0.00006186
Iteration 346/1000 | Loss: 0.00006173
Iteration 347/1000 | Loss: 0.00006172
Iteration 348/1000 | Loss: 0.00006172
Iteration 349/1000 | Loss: 0.00006168
Iteration 350/1000 | Loss: 0.00006168
Iteration 351/1000 | Loss: 0.00006167
Iteration 352/1000 | Loss: 0.00006166
Iteration 353/1000 | Loss: 0.00006162
Iteration 354/1000 | Loss: 0.00006156
Iteration 355/1000 | Loss: 0.00006141
Iteration 356/1000 | Loss: 0.00089923
Iteration 357/1000 | Loss: 0.00032173
Iteration 358/1000 | Loss: 0.00006272
Iteration 359/1000 | Loss: 0.00006180
Iteration 360/1000 | Loss: 0.00058200
Iteration 361/1000 | Loss: 0.00010540
Iteration 362/1000 | Loss: 0.00006237
Iteration 363/1000 | Loss: 0.00046715
Iteration 364/1000 | Loss: 0.00013476
Iteration 365/1000 | Loss: 0.00006231
Iteration 366/1000 | Loss: 0.00006170
Iteration 367/1000 | Loss: 0.00056586
Iteration 368/1000 | Loss: 0.00006705
Iteration 369/1000 | Loss: 0.00006334
Iteration 370/1000 | Loss: 0.00006094
Iteration 371/1000 | Loss: 0.00091163
Iteration 372/1000 | Loss: 0.00071766
Iteration 373/1000 | Loss: 0.00008908
Iteration 374/1000 | Loss: 0.00006228
Iteration 375/1000 | Loss: 0.00006041
Iteration 376/1000 | Loss: 0.00005974
Iteration 377/1000 | Loss: 0.00005948
Iteration 378/1000 | Loss: 0.00094548
Iteration 379/1000 | Loss: 0.00011930
Iteration 380/1000 | Loss: 0.00007441
Iteration 381/1000 | Loss: 0.00007015
Iteration 382/1000 | Loss: 0.00023196
Iteration 383/1000 | Loss: 0.00024178
Iteration 384/1000 | Loss: 0.00019948
Iteration 385/1000 | Loss: 0.00006607
Iteration 386/1000 | Loss: 0.00006294
Iteration 387/1000 | Loss: 0.00006180
Iteration 388/1000 | Loss: 0.00006109
Iteration 389/1000 | Loss: 0.00006062
Iteration 390/1000 | Loss: 0.00006004
Iteration 391/1000 | Loss: 0.00005982
Iteration 392/1000 | Loss: 0.00005972
Iteration 393/1000 | Loss: 0.00005970
Iteration 394/1000 | Loss: 0.00005969
Iteration 395/1000 | Loss: 0.00005969
Iteration 396/1000 | Loss: 0.00005968
Iteration 397/1000 | Loss: 0.00005968
Iteration 398/1000 | Loss: 0.00005965
Iteration 399/1000 | Loss: 0.00005960
Iteration 400/1000 | Loss: 0.00005956
Iteration 401/1000 | Loss: 0.00005956
Iteration 402/1000 | Loss: 0.00005955
Iteration 403/1000 | Loss: 0.00005955
Iteration 404/1000 | Loss: 0.00005948
Iteration 405/1000 | Loss: 0.00005947
Iteration 406/1000 | Loss: 0.00037298
Iteration 407/1000 | Loss: 0.00025168
Iteration 408/1000 | Loss: 0.00006985
Iteration 409/1000 | Loss: 0.00006046
Iteration 410/1000 | Loss: 0.00032846
Iteration 411/1000 | Loss: 0.00018990
Iteration 412/1000 | Loss: 0.00007817
Iteration 413/1000 | Loss: 0.00006340
Iteration 414/1000 | Loss: 0.00006176
Iteration 415/1000 | Loss: 0.00005987
Iteration 416/1000 | Loss: 0.00005888
Iteration 417/1000 | Loss: 0.00005816
Iteration 418/1000 | Loss: 0.00005789
Iteration 419/1000 | Loss: 0.00005767
Iteration 420/1000 | Loss: 0.00005748
Iteration 421/1000 | Loss: 0.00005747
Iteration 422/1000 | Loss: 0.00005738
Iteration 423/1000 | Loss: 0.00005737
Iteration 424/1000 | Loss: 0.00005736
Iteration 425/1000 | Loss: 0.00005731
Iteration 426/1000 | Loss: 0.00005726
Iteration 427/1000 | Loss: 0.00005725
Iteration 428/1000 | Loss: 0.00005722
Iteration 429/1000 | Loss: 0.00005722
Iteration 430/1000 | Loss: 0.00005721
Iteration 431/1000 | Loss: 0.00005721
Iteration 432/1000 | Loss: 0.00005720
Iteration 433/1000 | Loss: 0.00005719
Iteration 434/1000 | Loss: 0.00005719
Iteration 435/1000 | Loss: 0.00005719
Iteration 436/1000 | Loss: 0.00005719
Iteration 437/1000 | Loss: 0.00005718
Iteration 438/1000 | Loss: 0.00005717
Iteration 439/1000 | Loss: 0.00005717
Iteration 440/1000 | Loss: 0.00005716
Iteration 441/1000 | Loss: 0.00005714
Iteration 442/1000 | Loss: 0.00005714
Iteration 443/1000 | Loss: 0.00005714
Iteration 444/1000 | Loss: 0.00005713
Iteration 445/1000 | Loss: 0.00005713
Iteration 446/1000 | Loss: 0.00005712
Iteration 447/1000 | Loss: 0.00005712
Iteration 448/1000 | Loss: 0.00005712
Iteration 449/1000 | Loss: 0.00005711
Iteration 450/1000 | Loss: 0.00005711
Iteration 451/1000 | Loss: 0.00005711
Iteration 452/1000 | Loss: 0.00005711
Iteration 453/1000 | Loss: 0.00005711
Iteration 454/1000 | Loss: 0.00005711
Iteration 455/1000 | Loss: 0.00005711
Iteration 456/1000 | Loss: 0.00005711
Iteration 457/1000 | Loss: 0.00005711
Iteration 458/1000 | Loss: 0.00005711
Iteration 459/1000 | Loss: 0.00005711
Iteration 460/1000 | Loss: 0.00005710
Iteration 461/1000 | Loss: 0.00005710
Iteration 462/1000 | Loss: 0.00005710
Iteration 463/1000 | Loss: 0.00005710
Iteration 464/1000 | Loss: 0.00005709
Iteration 465/1000 | Loss: 0.00005707
Iteration 466/1000 | Loss: 0.00005707
Iteration 467/1000 | Loss: 0.00005706
Iteration 468/1000 | Loss: 0.00005706
Iteration 469/1000 | Loss: 0.00005706
Iteration 470/1000 | Loss: 0.00005705
Iteration 471/1000 | Loss: 0.00005704
Iteration 472/1000 | Loss: 0.00005704
Iteration 473/1000 | Loss: 0.00005700
Iteration 474/1000 | Loss: 0.00005700
Iteration 475/1000 | Loss: 0.00005700
Iteration 476/1000 | Loss: 0.00005700
Iteration 477/1000 | Loss: 0.00005699
Iteration 478/1000 | Loss: 0.00005699
Iteration 479/1000 | Loss: 0.00005699
Iteration 480/1000 | Loss: 0.00005699
Iteration 481/1000 | Loss: 0.00005699
Iteration 482/1000 | Loss: 0.00005699
Iteration 483/1000 | Loss: 0.00005699
Iteration 484/1000 | Loss: 0.00005699
Iteration 485/1000 | Loss: 0.00005699
Iteration 486/1000 | Loss: 0.00005699
Iteration 487/1000 | Loss: 0.00005698
Iteration 488/1000 | Loss: 0.00005698
Iteration 489/1000 | Loss: 0.00005698
Iteration 490/1000 | Loss: 0.00005698
Iteration 491/1000 | Loss: 0.00005698
Iteration 492/1000 | Loss: 0.00005698
Iteration 493/1000 | Loss: 0.00005698
Iteration 494/1000 | Loss: 0.00005698
Iteration 495/1000 | Loss: 0.00005698
Iteration 496/1000 | Loss: 0.00005698
Iteration 497/1000 | Loss: 0.00005698
Iteration 498/1000 | Loss: 0.00005697
Iteration 499/1000 | Loss: 0.00005697
Iteration 500/1000 | Loss: 0.00005697
Iteration 501/1000 | Loss: 0.00005697
Iteration 502/1000 | Loss: 0.00005697
Iteration 503/1000 | Loss: 0.00005697
Iteration 504/1000 | Loss: 0.00005697
Iteration 505/1000 | Loss: 0.00005697
Iteration 506/1000 | Loss: 0.00005697
Iteration 507/1000 | Loss: 0.00005697
Iteration 508/1000 | Loss: 0.00005697
Iteration 509/1000 | Loss: 0.00005697
Iteration 510/1000 | Loss: 0.00005696
Iteration 511/1000 | Loss: 0.00005696
Iteration 512/1000 | Loss: 0.00005696
Iteration 513/1000 | Loss: 0.00005696
Iteration 514/1000 | Loss: 0.00005696
Iteration 515/1000 | Loss: 0.00005696
Iteration 516/1000 | Loss: 0.00005696
Iteration 517/1000 | Loss: 0.00005696
Iteration 518/1000 | Loss: 0.00005696
Iteration 519/1000 | Loss: 0.00005696
Iteration 520/1000 | Loss: 0.00005696
Iteration 521/1000 | Loss: 0.00005696
Iteration 522/1000 | Loss: 0.00005695
Iteration 523/1000 | Loss: 0.00005695
Iteration 524/1000 | Loss: 0.00005695
Iteration 525/1000 | Loss: 0.00005695
Iteration 526/1000 | Loss: 0.00005695
Iteration 527/1000 | Loss: 0.00005695
Iteration 528/1000 | Loss: 0.00005694
Iteration 529/1000 | Loss: 0.00005694
Iteration 530/1000 | Loss: 0.00005694
Iteration 531/1000 | Loss: 0.00005694
Iteration 532/1000 | Loss: 0.00005694
Iteration 533/1000 | Loss: 0.00005694
Iteration 534/1000 | Loss: 0.00005694
Iteration 535/1000 | Loss: 0.00005694
Iteration 536/1000 | Loss: 0.00005694
Iteration 537/1000 | Loss: 0.00005694
Iteration 538/1000 | Loss: 0.00005694
Iteration 539/1000 | Loss: 0.00005694
Iteration 540/1000 | Loss: 0.00005694
Iteration 541/1000 | Loss: 0.00005694
Iteration 542/1000 | Loss: 0.00005694
Iteration 543/1000 | Loss: 0.00005694
Iteration 544/1000 | Loss: 0.00005694
Iteration 545/1000 | Loss: 0.00005694
Iteration 546/1000 | Loss: 0.00005694
Iteration 547/1000 | Loss: 0.00005694
Iteration 548/1000 | Loss: 0.00005694
Iteration 549/1000 | Loss: 0.00005694
Iteration 550/1000 | Loss: 0.00005694
Iteration 551/1000 | Loss: 0.00005694
Iteration 552/1000 | Loss: 0.00005694
Iteration 553/1000 | Loss: 0.00005694
Iteration 554/1000 | Loss: 0.00005694
Iteration 555/1000 | Loss: 0.00005694
Iteration 556/1000 | Loss: 0.00005694
Iteration 557/1000 | Loss: 0.00005694
Iteration 558/1000 | Loss: 0.00005694
Iteration 559/1000 | Loss: 0.00005694
Iteration 560/1000 | Loss: 0.00005694
Iteration 561/1000 | Loss: 0.00005694
Iteration 562/1000 | Loss: 0.00005694
Iteration 563/1000 | Loss: 0.00005694
Iteration 564/1000 | Loss: 0.00005694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 564. Stopping optimization.
Last 5 losses: [5.693839557352476e-05, 5.693839557352476e-05, 5.693839557352476e-05, 5.693839557352476e-05, 5.693839557352476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.693839557352476e-05

Optimization complete. Final v2v error: 5.200735569000244 mm

Highest mean error: 11.990145683288574 mm for frame 75

Lowest mean error: 4.200582504272461 mm for frame 115

Saving results

Total time: 620.1944615840912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797183
Iteration 2/25 | Loss: 0.00141431
Iteration 3/25 | Loss: 0.00133376
Iteration 4/25 | Loss: 0.00131963
Iteration 5/25 | Loss: 0.00131534
Iteration 6/25 | Loss: 0.00131534
Iteration 7/25 | Loss: 0.00131534
Iteration 8/25 | Loss: 0.00131534
Iteration 9/25 | Loss: 0.00131534
Iteration 10/25 | Loss: 0.00131534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001315344707109034, 0.001315344707109034, 0.001315344707109034, 0.001315344707109034, 0.001315344707109034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001315344707109034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12198353
Iteration 2/25 | Loss: 0.00087896
Iteration 3/25 | Loss: 0.00087895
Iteration 4/25 | Loss: 0.00087895
Iteration 5/25 | Loss: 0.00087895
Iteration 6/25 | Loss: 0.00087895
Iteration 7/25 | Loss: 0.00087895
Iteration 8/25 | Loss: 0.00087895
Iteration 9/25 | Loss: 0.00087895
Iteration 10/25 | Loss: 0.00087895
Iteration 11/25 | Loss: 0.00087895
Iteration 12/25 | Loss: 0.00087895
Iteration 13/25 | Loss: 0.00087895
Iteration 14/25 | Loss: 0.00087895
Iteration 15/25 | Loss: 0.00087895
Iteration 16/25 | Loss: 0.00087895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008789518615230918, 0.0008789518615230918, 0.0008789518615230918, 0.0008789518615230918, 0.0008789518615230918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008789518615230918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087895
Iteration 2/1000 | Loss: 0.00003556
Iteration 3/1000 | Loss: 0.00002676
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002318
Iteration 6/1000 | Loss: 0.00002224
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002100
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00001991
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00001962
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001943
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001927
Iteration 18/1000 | Loss: 0.00001915
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001896
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001895
Iteration 42/1000 | Loss: 0.00001894
Iteration 43/1000 | Loss: 0.00001894
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001883
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001878
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00001876
Iteration 65/1000 | Loss: 0.00001876
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001874
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001874
Iteration 72/1000 | Loss: 0.00001873
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001873
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001872
Iteration 77/1000 | Loss: 0.00001872
Iteration 78/1000 | Loss: 0.00001871
Iteration 79/1000 | Loss: 0.00001871
Iteration 80/1000 | Loss: 0.00001871
Iteration 81/1000 | Loss: 0.00001870
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001870
Iteration 84/1000 | Loss: 0.00001870
Iteration 85/1000 | Loss: 0.00001870
Iteration 86/1000 | Loss: 0.00001870
Iteration 87/1000 | Loss: 0.00001870
Iteration 88/1000 | Loss: 0.00001870
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001869
Iteration 94/1000 | Loss: 0.00001869
Iteration 95/1000 | Loss: 0.00001869
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001868
Iteration 100/1000 | Loss: 0.00001868
Iteration 101/1000 | Loss: 0.00001868
Iteration 102/1000 | Loss: 0.00001868
Iteration 103/1000 | Loss: 0.00001867
Iteration 104/1000 | Loss: 0.00001867
Iteration 105/1000 | Loss: 0.00001867
Iteration 106/1000 | Loss: 0.00001867
Iteration 107/1000 | Loss: 0.00001867
Iteration 108/1000 | Loss: 0.00001867
Iteration 109/1000 | Loss: 0.00001867
Iteration 110/1000 | Loss: 0.00001867
Iteration 111/1000 | Loss: 0.00001866
Iteration 112/1000 | Loss: 0.00001866
Iteration 113/1000 | Loss: 0.00001866
Iteration 114/1000 | Loss: 0.00001866
Iteration 115/1000 | Loss: 0.00001866
Iteration 116/1000 | Loss: 0.00001866
Iteration 117/1000 | Loss: 0.00001866
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001865
Iteration 120/1000 | Loss: 0.00001865
Iteration 121/1000 | Loss: 0.00001865
Iteration 122/1000 | Loss: 0.00001865
Iteration 123/1000 | Loss: 0.00001865
Iteration 124/1000 | Loss: 0.00001865
Iteration 125/1000 | Loss: 0.00001865
Iteration 126/1000 | Loss: 0.00001865
Iteration 127/1000 | Loss: 0.00001865
Iteration 128/1000 | Loss: 0.00001865
Iteration 129/1000 | Loss: 0.00001865
Iteration 130/1000 | Loss: 0.00001865
Iteration 131/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.865241392806638e-05, 1.865241392806638e-05, 1.865241392806638e-05, 1.865241392806638e-05, 1.865241392806638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.865241392806638e-05

Optimization complete. Final v2v error: 3.6254923343658447 mm

Highest mean error: 3.9033937454223633 mm for frame 129

Lowest mean error: 3.4115066528320312 mm for frame 202

Saving results

Total time: 43.70130681991577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403514
Iteration 2/25 | Loss: 0.00144493
Iteration 3/25 | Loss: 0.00131745
Iteration 4/25 | Loss: 0.00129432
Iteration 5/25 | Loss: 0.00128894
Iteration 6/25 | Loss: 0.00128851
Iteration 7/25 | Loss: 0.00128851
Iteration 8/25 | Loss: 0.00128851
Iteration 9/25 | Loss: 0.00128851
Iteration 10/25 | Loss: 0.00128851
Iteration 11/25 | Loss: 0.00128851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001288512023165822, 0.001288512023165822, 0.001288512023165822, 0.001288512023165822, 0.001288512023165822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001288512023165822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40460324
Iteration 2/25 | Loss: 0.00089389
Iteration 3/25 | Loss: 0.00089389
Iteration 4/25 | Loss: 0.00089389
Iteration 5/25 | Loss: 0.00089389
Iteration 6/25 | Loss: 0.00089389
Iteration 7/25 | Loss: 0.00089389
Iteration 8/25 | Loss: 0.00089389
Iteration 9/25 | Loss: 0.00089389
Iteration 10/25 | Loss: 0.00089389
Iteration 11/25 | Loss: 0.00089389
Iteration 12/25 | Loss: 0.00089389
Iteration 13/25 | Loss: 0.00089389
Iteration 14/25 | Loss: 0.00089389
Iteration 15/25 | Loss: 0.00089389
Iteration 16/25 | Loss: 0.00089389
Iteration 17/25 | Loss: 0.00089389
Iteration 18/25 | Loss: 0.00089389
Iteration 19/25 | Loss: 0.00089389
Iteration 20/25 | Loss: 0.00089389
Iteration 21/25 | Loss: 0.00089389
Iteration 22/25 | Loss: 0.00089389
Iteration 23/25 | Loss: 0.00089389
Iteration 24/25 | Loss: 0.00089389
Iteration 25/25 | Loss: 0.00089389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089389
Iteration 2/1000 | Loss: 0.00004559
Iteration 3/1000 | Loss: 0.00003259
Iteration 4/1000 | Loss: 0.00002761
Iteration 5/1000 | Loss: 0.00002509
Iteration 6/1000 | Loss: 0.00002265
Iteration 7/1000 | Loss: 0.00002124
Iteration 8/1000 | Loss: 0.00002039
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001871
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001823
Iteration 14/1000 | Loss: 0.00001802
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001776
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001762
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001757
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001754
Iteration 35/1000 | Loss: 0.00001753
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001752
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001751
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001751
Iteration 42/1000 | Loss: 0.00001751
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001748
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001748
Iteration 54/1000 | Loss: 0.00001747
Iteration 55/1000 | Loss: 0.00001747
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001747
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001746
Iteration 62/1000 | Loss: 0.00001746
Iteration 63/1000 | Loss: 0.00001746
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001745
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001743
Iteration 79/1000 | Loss: 0.00001743
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001742
Iteration 89/1000 | Loss: 0.00001742
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001741
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001741
Iteration 94/1000 | Loss: 0.00001741
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001741
Iteration 97/1000 | Loss: 0.00001741
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001738
Iteration 112/1000 | Loss: 0.00001738
Iteration 113/1000 | Loss: 0.00001738
Iteration 114/1000 | Loss: 0.00001738
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001737
Iteration 118/1000 | Loss: 0.00001737
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001736
Iteration 122/1000 | Loss: 0.00001736
Iteration 123/1000 | Loss: 0.00001736
Iteration 124/1000 | Loss: 0.00001736
Iteration 125/1000 | Loss: 0.00001736
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001735
Iteration 128/1000 | Loss: 0.00001735
Iteration 129/1000 | Loss: 0.00001735
Iteration 130/1000 | Loss: 0.00001735
Iteration 131/1000 | Loss: 0.00001735
Iteration 132/1000 | Loss: 0.00001735
Iteration 133/1000 | Loss: 0.00001735
Iteration 134/1000 | Loss: 0.00001734
Iteration 135/1000 | Loss: 0.00001734
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001734
Iteration 141/1000 | Loss: 0.00001734
Iteration 142/1000 | Loss: 0.00001734
Iteration 143/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.7342912542517297e-05, 1.7342912542517297e-05, 1.7342912542517297e-05, 1.7342912542517297e-05, 1.7342912542517297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7342912542517297e-05

Optimization complete. Final v2v error: 3.557594060897827 mm

Highest mean error: 4.57664155960083 mm for frame 5

Lowest mean error: 2.9667770862579346 mm for frame 129

Saving results

Total time: 46.91964316368103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874150
Iteration 2/25 | Loss: 0.00171519
Iteration 3/25 | Loss: 0.00142441
Iteration 4/25 | Loss: 0.00140552
Iteration 5/25 | Loss: 0.00140251
Iteration 6/25 | Loss: 0.00139902
Iteration 7/25 | Loss: 0.00139785
Iteration 8/25 | Loss: 0.00139864
Iteration 9/25 | Loss: 0.00139276
Iteration 10/25 | Loss: 0.00139636
Iteration 11/25 | Loss: 0.00140031
Iteration 12/25 | Loss: 0.00139425
Iteration 13/25 | Loss: 0.00139452
Iteration 14/25 | Loss: 0.00139426
Iteration 15/25 | Loss: 0.00139449
Iteration 16/25 | Loss: 0.00139423
Iteration 17/25 | Loss: 0.00139442
Iteration 18/25 | Loss: 0.00139444
Iteration 19/25 | Loss: 0.00139512
Iteration 20/25 | Loss: 0.00139452
Iteration 21/25 | Loss: 0.00139453
Iteration 22/25 | Loss: 0.00139458
Iteration 23/25 | Loss: 0.00139407
Iteration 24/25 | Loss: 0.00139480
Iteration 25/25 | Loss: 0.00139498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.39066124
Iteration 2/25 | Loss: 0.00120099
Iteration 3/25 | Loss: 0.00093892
Iteration 4/25 | Loss: 0.00093892
Iteration 5/25 | Loss: 0.00093892
Iteration 6/25 | Loss: 0.00093892
Iteration 7/25 | Loss: 0.00093892
Iteration 8/25 | Loss: 0.00093892
Iteration 9/25 | Loss: 0.00093892
Iteration 10/25 | Loss: 0.00093892
Iteration 11/25 | Loss: 0.00093892
Iteration 12/25 | Loss: 0.00093891
Iteration 13/25 | Loss: 0.00093891
Iteration 14/25 | Loss: 0.00093891
Iteration 15/25 | Loss: 0.00093891
Iteration 16/25 | Loss: 0.00093891
Iteration 17/25 | Loss: 0.00093891
Iteration 18/25 | Loss: 0.00093891
Iteration 19/25 | Loss: 0.00093891
Iteration 20/25 | Loss: 0.00093891
Iteration 21/25 | Loss: 0.00093891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009389148326590657, 0.0009389148326590657, 0.0009389148326590657, 0.0009389148326590657, 0.0009389148326590657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009389148326590657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093891
Iteration 2/1000 | Loss: 0.00019452
Iteration 3/1000 | Loss: 0.00023526
Iteration 4/1000 | Loss: 0.00039060
Iteration 5/1000 | Loss: 0.00003744
Iteration 6/1000 | Loss: 0.00003819
Iteration 7/1000 | Loss: 0.00042251
Iteration 8/1000 | Loss: 0.00065011
Iteration 9/1000 | Loss: 0.00053858
Iteration 10/1000 | Loss: 0.00006283
Iteration 11/1000 | Loss: 0.00003494
Iteration 12/1000 | Loss: 0.00043722
Iteration 13/1000 | Loss: 0.00010991
Iteration 14/1000 | Loss: 0.00028468
Iteration 15/1000 | Loss: 0.00040895
Iteration 16/1000 | Loss: 0.00024165
Iteration 17/1000 | Loss: 0.00040757
Iteration 18/1000 | Loss: 0.00057129
Iteration 19/1000 | Loss: 0.00006879
Iteration 20/1000 | Loss: 0.00047180
Iteration 21/1000 | Loss: 0.00054536
Iteration 22/1000 | Loss: 0.00033128
Iteration 23/1000 | Loss: 0.00013035
Iteration 24/1000 | Loss: 0.00016701
Iteration 25/1000 | Loss: 0.00025704
Iteration 26/1000 | Loss: 0.00024726
Iteration 27/1000 | Loss: 0.00017610
Iteration 28/1000 | Loss: 0.00020270
Iteration 29/1000 | Loss: 0.00021432
Iteration 30/1000 | Loss: 0.00012794
Iteration 31/1000 | Loss: 0.00012930
Iteration 32/1000 | Loss: 0.00008791
Iteration 33/1000 | Loss: 0.00003315
Iteration 34/1000 | Loss: 0.00048912
Iteration 35/1000 | Loss: 0.00005707
Iteration 36/1000 | Loss: 0.00003259
Iteration 37/1000 | Loss: 0.00003080
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002884
Iteration 41/1000 | Loss: 0.00002862
Iteration 42/1000 | Loss: 0.00049464
Iteration 43/1000 | Loss: 0.00003268
Iteration 44/1000 | Loss: 0.00003038
Iteration 45/1000 | Loss: 0.00002708
Iteration 46/1000 | Loss: 0.00002636
Iteration 47/1000 | Loss: 0.00002566
Iteration 48/1000 | Loss: 0.00002532
Iteration 49/1000 | Loss: 0.00002527
Iteration 50/1000 | Loss: 0.00002517
Iteration 51/1000 | Loss: 0.00003197
Iteration 52/1000 | Loss: 0.00002652
Iteration 53/1000 | Loss: 0.00002518
Iteration 54/1000 | Loss: 0.00002488
Iteration 55/1000 | Loss: 0.00002469
Iteration 56/1000 | Loss: 0.00002469
Iteration 57/1000 | Loss: 0.00002463
Iteration 58/1000 | Loss: 0.00002463
Iteration 59/1000 | Loss: 0.00002462
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002461
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002461
Iteration 66/1000 | Loss: 0.00002461
Iteration 67/1000 | Loss: 0.00002461
Iteration 68/1000 | Loss: 0.00002460
Iteration 69/1000 | Loss: 0.00002460
Iteration 70/1000 | Loss: 0.00002460
Iteration 71/1000 | Loss: 0.00002460
Iteration 72/1000 | Loss: 0.00002460
Iteration 73/1000 | Loss: 0.00002460
Iteration 74/1000 | Loss: 0.00002460
Iteration 75/1000 | Loss: 0.00002460
Iteration 76/1000 | Loss: 0.00002460
Iteration 77/1000 | Loss: 0.00002459
Iteration 78/1000 | Loss: 0.00002459
Iteration 79/1000 | Loss: 0.00002459
Iteration 80/1000 | Loss: 0.00002459
Iteration 81/1000 | Loss: 0.00002459
Iteration 82/1000 | Loss: 0.00002459
Iteration 83/1000 | Loss: 0.00002459
Iteration 84/1000 | Loss: 0.00002459
Iteration 85/1000 | Loss: 0.00002459
Iteration 86/1000 | Loss: 0.00002459
Iteration 87/1000 | Loss: 0.00002459
Iteration 88/1000 | Loss: 0.00002459
Iteration 89/1000 | Loss: 0.00002459
Iteration 90/1000 | Loss: 0.00002459
Iteration 91/1000 | Loss: 0.00002459
Iteration 92/1000 | Loss: 0.00002459
Iteration 93/1000 | Loss: 0.00002459
Iteration 94/1000 | Loss: 0.00002459
Iteration 95/1000 | Loss: 0.00002459
Iteration 96/1000 | Loss: 0.00002459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.4588394808233716e-05, 2.4588394808233716e-05, 2.4588394808233716e-05, 2.4588394808233716e-05, 2.4588394808233716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4588394808233716e-05

Optimization complete. Final v2v error: 4.135439872741699 mm

Highest mean error: 5.627024173736572 mm for frame 112

Lowest mean error: 3.538957357406616 mm for frame 206

Saving results

Total time: 137.35294938087463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841315
Iteration 2/25 | Loss: 0.00234305
Iteration 3/25 | Loss: 0.00182008
Iteration 4/25 | Loss: 0.00177223
Iteration 5/25 | Loss: 0.00156653
Iteration 6/25 | Loss: 0.00151032
Iteration 7/25 | Loss: 0.00148482
Iteration 8/25 | Loss: 0.00146205
Iteration 9/25 | Loss: 0.00145519
Iteration 10/25 | Loss: 0.00144969
Iteration 11/25 | Loss: 0.00144128
Iteration 12/25 | Loss: 0.00143227
Iteration 13/25 | Loss: 0.00143053
Iteration 14/25 | Loss: 0.00142978
Iteration 15/25 | Loss: 0.00142957
Iteration 16/25 | Loss: 0.00142951
Iteration 17/25 | Loss: 0.00142951
Iteration 18/25 | Loss: 0.00142951
Iteration 19/25 | Loss: 0.00142950
Iteration 20/25 | Loss: 0.00142950
Iteration 21/25 | Loss: 0.00142950
Iteration 22/25 | Loss: 0.00142950
Iteration 23/25 | Loss: 0.00142950
Iteration 24/25 | Loss: 0.00142950
Iteration 25/25 | Loss: 0.00142950

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04662156
Iteration 2/25 | Loss: 0.00098982
Iteration 3/25 | Loss: 0.00098982
Iteration 4/25 | Loss: 0.00098982
Iteration 5/25 | Loss: 0.00098982
Iteration 6/25 | Loss: 0.00098982
Iteration 7/25 | Loss: 0.00098982
Iteration 8/25 | Loss: 0.00098982
Iteration 9/25 | Loss: 0.00098982
Iteration 10/25 | Loss: 0.00098982
Iteration 11/25 | Loss: 0.00098982
Iteration 12/25 | Loss: 0.00098982
Iteration 13/25 | Loss: 0.00098982
Iteration 14/25 | Loss: 0.00098982
Iteration 15/25 | Loss: 0.00098982
Iteration 16/25 | Loss: 0.00098982
Iteration 17/25 | Loss: 0.00098982
Iteration 18/25 | Loss: 0.00098982
Iteration 19/25 | Loss: 0.00098982
Iteration 20/25 | Loss: 0.00098982
Iteration 21/25 | Loss: 0.00098982
Iteration 22/25 | Loss: 0.00098982
Iteration 23/25 | Loss: 0.00098982
Iteration 24/25 | Loss: 0.00098982
Iteration 25/25 | Loss: 0.00098982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098982
Iteration 2/1000 | Loss: 0.00006120
Iteration 3/1000 | Loss: 0.00003960
Iteration 4/1000 | Loss: 0.00003026
Iteration 5/1000 | Loss: 0.00002865
Iteration 6/1000 | Loss: 0.00002757
Iteration 7/1000 | Loss: 0.00002665
Iteration 8/1000 | Loss: 0.00002607
Iteration 9/1000 | Loss: 0.00002562
Iteration 10/1000 | Loss: 0.00002523
Iteration 11/1000 | Loss: 0.00002494
Iteration 12/1000 | Loss: 0.00002474
Iteration 13/1000 | Loss: 0.00002473
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002454
Iteration 16/1000 | Loss: 0.00002436
Iteration 17/1000 | Loss: 0.00002428
Iteration 18/1000 | Loss: 0.00002425
Iteration 19/1000 | Loss: 0.00002424
Iteration 20/1000 | Loss: 0.00002424
Iteration 21/1000 | Loss: 0.00012222
Iteration 22/1000 | Loss: 0.00002586
Iteration 23/1000 | Loss: 0.00002442
Iteration 24/1000 | Loss: 0.00002365
Iteration 25/1000 | Loss: 0.00002351
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002341
Iteration 29/1000 | Loss: 0.00002338
Iteration 30/1000 | Loss: 0.00002334
Iteration 31/1000 | Loss: 0.00002331
Iteration 32/1000 | Loss: 0.00002330
Iteration 33/1000 | Loss: 0.00002330
Iteration 34/1000 | Loss: 0.00002329
Iteration 35/1000 | Loss: 0.00002329
Iteration 36/1000 | Loss: 0.00002329
Iteration 37/1000 | Loss: 0.00002325
Iteration 38/1000 | Loss: 0.00002325
Iteration 39/1000 | Loss: 0.00002324
Iteration 40/1000 | Loss: 0.00002324
Iteration 41/1000 | Loss: 0.00002324
Iteration 42/1000 | Loss: 0.00002323
Iteration 43/1000 | Loss: 0.00002323
Iteration 44/1000 | Loss: 0.00002323
Iteration 45/1000 | Loss: 0.00002323
Iteration 46/1000 | Loss: 0.00002322
Iteration 47/1000 | Loss: 0.00002322
Iteration 48/1000 | Loss: 0.00002322
Iteration 49/1000 | Loss: 0.00002322
Iteration 50/1000 | Loss: 0.00002322
Iteration 51/1000 | Loss: 0.00002322
Iteration 52/1000 | Loss: 0.00002322
Iteration 53/1000 | Loss: 0.00002321
Iteration 54/1000 | Loss: 0.00002321
Iteration 55/1000 | Loss: 0.00002321
Iteration 56/1000 | Loss: 0.00002321
Iteration 57/1000 | Loss: 0.00002321
Iteration 58/1000 | Loss: 0.00002321
Iteration 59/1000 | Loss: 0.00002320
Iteration 60/1000 | Loss: 0.00002320
Iteration 61/1000 | Loss: 0.00002320
Iteration 62/1000 | Loss: 0.00002320
Iteration 63/1000 | Loss: 0.00002320
Iteration 64/1000 | Loss: 0.00002319
Iteration 65/1000 | Loss: 0.00002319
Iteration 66/1000 | Loss: 0.00002319
Iteration 67/1000 | Loss: 0.00002319
Iteration 68/1000 | Loss: 0.00002319
Iteration 69/1000 | Loss: 0.00002319
Iteration 70/1000 | Loss: 0.00002318
Iteration 71/1000 | Loss: 0.00002318
Iteration 72/1000 | Loss: 0.00002318
Iteration 73/1000 | Loss: 0.00002318
Iteration 74/1000 | Loss: 0.00002317
Iteration 75/1000 | Loss: 0.00002317
Iteration 76/1000 | Loss: 0.00002317
Iteration 77/1000 | Loss: 0.00002317
Iteration 78/1000 | Loss: 0.00002317
Iteration 79/1000 | Loss: 0.00002316
Iteration 80/1000 | Loss: 0.00002316
Iteration 81/1000 | Loss: 0.00002316
Iteration 82/1000 | Loss: 0.00002315
Iteration 83/1000 | Loss: 0.00002315
Iteration 84/1000 | Loss: 0.00002315
Iteration 85/1000 | Loss: 0.00002315
Iteration 86/1000 | Loss: 0.00002315
Iteration 87/1000 | Loss: 0.00002315
Iteration 88/1000 | Loss: 0.00002315
Iteration 89/1000 | Loss: 0.00002314
Iteration 90/1000 | Loss: 0.00002314
Iteration 91/1000 | Loss: 0.00002314
Iteration 92/1000 | Loss: 0.00002314
Iteration 93/1000 | Loss: 0.00002314
Iteration 94/1000 | Loss: 0.00002314
Iteration 95/1000 | Loss: 0.00002313
Iteration 96/1000 | Loss: 0.00002313
Iteration 97/1000 | Loss: 0.00002313
Iteration 98/1000 | Loss: 0.00002313
Iteration 99/1000 | Loss: 0.00002313
Iteration 100/1000 | Loss: 0.00002313
Iteration 101/1000 | Loss: 0.00002313
Iteration 102/1000 | Loss: 0.00002312
Iteration 103/1000 | Loss: 0.00002312
Iteration 104/1000 | Loss: 0.00002312
Iteration 105/1000 | Loss: 0.00002312
Iteration 106/1000 | Loss: 0.00002311
Iteration 107/1000 | Loss: 0.00002311
Iteration 108/1000 | Loss: 0.00002311
Iteration 109/1000 | Loss: 0.00002311
Iteration 110/1000 | Loss: 0.00002311
Iteration 111/1000 | Loss: 0.00002311
Iteration 112/1000 | Loss: 0.00002311
Iteration 113/1000 | Loss: 0.00002311
Iteration 114/1000 | Loss: 0.00002311
Iteration 115/1000 | Loss: 0.00002311
Iteration 116/1000 | Loss: 0.00002311
Iteration 117/1000 | Loss: 0.00002311
Iteration 118/1000 | Loss: 0.00002311
Iteration 119/1000 | Loss: 0.00002311
Iteration 120/1000 | Loss: 0.00002311
Iteration 121/1000 | Loss: 0.00002311
Iteration 122/1000 | Loss: 0.00002311
Iteration 123/1000 | Loss: 0.00002311
Iteration 124/1000 | Loss: 0.00002311
Iteration 125/1000 | Loss: 0.00002311
Iteration 126/1000 | Loss: 0.00002311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.3107339075068012e-05, 2.3107339075068012e-05, 2.3107339075068012e-05, 2.3107339075068012e-05, 2.3107339075068012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3107339075068012e-05

Optimization complete. Final v2v error: 4.010296821594238 mm

Highest mean error: 4.833522796630859 mm for frame 151

Lowest mean error: 3.607307195663452 mm for frame 156

Saving results

Total time: 72.35609316825867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917234
Iteration 2/25 | Loss: 0.00166370
Iteration 3/25 | Loss: 0.00156867
Iteration 4/25 | Loss: 0.00140316
Iteration 5/25 | Loss: 0.00138202
Iteration 6/25 | Loss: 0.00137572
Iteration 7/25 | Loss: 0.00137895
Iteration 8/25 | Loss: 0.00136258
Iteration 9/25 | Loss: 0.00135792
Iteration 10/25 | Loss: 0.00135847
Iteration 11/25 | Loss: 0.00135375
Iteration 12/25 | Loss: 0.00135383
Iteration 13/25 | Loss: 0.00135168
Iteration 14/25 | Loss: 0.00135073
Iteration 15/25 | Loss: 0.00135017
Iteration 16/25 | Loss: 0.00134993
Iteration 17/25 | Loss: 0.00134987
Iteration 18/25 | Loss: 0.00134986
Iteration 19/25 | Loss: 0.00134986
Iteration 20/25 | Loss: 0.00134986
Iteration 21/25 | Loss: 0.00134983
Iteration 22/25 | Loss: 0.00134983
Iteration 23/25 | Loss: 0.00134982
Iteration 24/25 | Loss: 0.00134982
Iteration 25/25 | Loss: 0.00134982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51692843
Iteration 2/25 | Loss: 0.00102231
Iteration 3/25 | Loss: 0.00091611
Iteration 4/25 | Loss: 0.00091611
Iteration 5/25 | Loss: 0.00091611
Iteration 6/25 | Loss: 0.00091611
Iteration 7/25 | Loss: 0.00091611
Iteration 8/25 | Loss: 0.00091611
Iteration 9/25 | Loss: 0.00091611
Iteration 10/25 | Loss: 0.00091611
Iteration 11/25 | Loss: 0.00091611
Iteration 12/25 | Loss: 0.00091611
Iteration 13/25 | Loss: 0.00091611
Iteration 14/25 | Loss: 0.00091611
Iteration 15/25 | Loss: 0.00091611
Iteration 16/25 | Loss: 0.00091611
Iteration 17/25 | Loss: 0.00091611
Iteration 18/25 | Loss: 0.00091611
Iteration 19/25 | Loss: 0.00091611
Iteration 20/25 | Loss: 0.00091611
Iteration 21/25 | Loss: 0.00091611
Iteration 22/25 | Loss: 0.00091611
Iteration 23/25 | Loss: 0.00091611
Iteration 24/25 | Loss: 0.00091611
Iteration 25/25 | Loss: 0.00091611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091611
Iteration 2/1000 | Loss: 0.00014429
Iteration 3/1000 | Loss: 0.00012379
Iteration 4/1000 | Loss: 0.00004646
Iteration 5/1000 | Loss: 0.00004425
Iteration 6/1000 | Loss: 0.00011507
Iteration 7/1000 | Loss: 0.00002401
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002247
Iteration 10/1000 | Loss: 0.00002205
Iteration 11/1000 | Loss: 0.00004642
Iteration 12/1000 | Loss: 0.00002155
Iteration 13/1000 | Loss: 0.00002117
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00043718
Iteration 16/1000 | Loss: 0.00002786
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002623
Iteration 19/1000 | Loss: 0.00004545
Iteration 20/1000 | Loss: 0.00008206
Iteration 21/1000 | Loss: 0.00002036
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00002506
Iteration 30/1000 | Loss: 0.00003850
Iteration 31/1000 | Loss: 0.00002179
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001878
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001877
Iteration 42/1000 | Loss: 0.00001877
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001877
Iteration 48/1000 | Loss: 0.00001877
Iteration 49/1000 | Loss: 0.00001877
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001870
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001867
Iteration 60/1000 | Loss: 0.00001867
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001863
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001853
Iteration 69/1000 | Loss: 0.00001853
Iteration 70/1000 | Loss: 0.00001852
Iteration 71/1000 | Loss: 0.00001852
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001851
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001848
Iteration 82/1000 | Loss: 0.00001848
Iteration 83/1000 | Loss: 0.00006377
Iteration 84/1000 | Loss: 0.00001863
Iteration 85/1000 | Loss: 0.00001843
Iteration 86/1000 | Loss: 0.00001842
Iteration 87/1000 | Loss: 0.00001842
Iteration 88/1000 | Loss: 0.00001842
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001841
Iteration 99/1000 | Loss: 0.00001841
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001840
Iteration 103/1000 | Loss: 0.00001840
Iteration 104/1000 | Loss: 0.00001840
Iteration 105/1000 | Loss: 0.00001840
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001840
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001839
Iteration 112/1000 | Loss: 0.00001839
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001839
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001838
Iteration 118/1000 | Loss: 0.00001838
Iteration 119/1000 | Loss: 0.00001838
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001838
Iteration 128/1000 | Loss: 0.00001838
Iteration 129/1000 | Loss: 0.00001838
Iteration 130/1000 | Loss: 0.00001838
Iteration 131/1000 | Loss: 0.00001838
Iteration 132/1000 | Loss: 0.00001838
Iteration 133/1000 | Loss: 0.00001838
Iteration 134/1000 | Loss: 0.00001838
Iteration 135/1000 | Loss: 0.00001838
Iteration 136/1000 | Loss: 0.00001838
Iteration 137/1000 | Loss: 0.00001838
Iteration 138/1000 | Loss: 0.00001838
Iteration 139/1000 | Loss: 0.00001838
Iteration 140/1000 | Loss: 0.00001838
Iteration 141/1000 | Loss: 0.00001838
Iteration 142/1000 | Loss: 0.00001838
Iteration 143/1000 | Loss: 0.00001838
Iteration 144/1000 | Loss: 0.00001838
Iteration 145/1000 | Loss: 0.00001838
Iteration 146/1000 | Loss: 0.00001838
Iteration 147/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.8379299945081584e-05, 1.8379299945081584e-05, 1.8379299945081584e-05, 1.8379299945081584e-05, 1.8379299945081584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8379299945081584e-05

Optimization complete. Final v2v error: 3.610460042953491 mm

Highest mean error: 4.423327922821045 mm for frame 178

Lowest mean error: 3.025407314300537 mm for frame 200

Saving results

Total time: 86.05416440963745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953740
Iteration 2/25 | Loss: 0.00167427
Iteration 3/25 | Loss: 0.00145296
Iteration 4/25 | Loss: 0.00138497
Iteration 5/25 | Loss: 0.00138215
Iteration 6/25 | Loss: 0.00135512
Iteration 7/25 | Loss: 0.00133959
Iteration 8/25 | Loss: 0.00132905
Iteration 9/25 | Loss: 0.00132266
Iteration 10/25 | Loss: 0.00132264
Iteration 11/25 | Loss: 0.00132210
Iteration 12/25 | Loss: 0.00131760
Iteration 13/25 | Loss: 0.00131627
Iteration 14/25 | Loss: 0.00131590
Iteration 15/25 | Loss: 0.00131582
Iteration 16/25 | Loss: 0.00131574
Iteration 17/25 | Loss: 0.00131572
Iteration 18/25 | Loss: 0.00131571
Iteration 19/25 | Loss: 0.00131571
Iteration 20/25 | Loss: 0.00131571
Iteration 21/25 | Loss: 0.00131571
Iteration 22/25 | Loss: 0.00131571
Iteration 23/25 | Loss: 0.00131571
Iteration 24/25 | Loss: 0.00131571
Iteration 25/25 | Loss: 0.00131571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26241565
Iteration 2/25 | Loss: 0.00084506
Iteration 3/25 | Loss: 0.00084505
Iteration 4/25 | Loss: 0.00084505
Iteration 5/25 | Loss: 0.00084505
Iteration 6/25 | Loss: 0.00084504
Iteration 7/25 | Loss: 0.00084504
Iteration 8/25 | Loss: 0.00084504
Iteration 9/25 | Loss: 0.00084504
Iteration 10/25 | Loss: 0.00084504
Iteration 11/25 | Loss: 0.00084504
Iteration 12/25 | Loss: 0.00084504
Iteration 13/25 | Loss: 0.00084504
Iteration 14/25 | Loss: 0.00084504
Iteration 15/25 | Loss: 0.00084504
Iteration 16/25 | Loss: 0.00084504
Iteration 17/25 | Loss: 0.00084504
Iteration 18/25 | Loss: 0.00084504
Iteration 19/25 | Loss: 0.00084504
Iteration 20/25 | Loss: 0.00084504
Iteration 21/25 | Loss: 0.00084504
Iteration 22/25 | Loss: 0.00084504
Iteration 23/25 | Loss: 0.00084504
Iteration 24/25 | Loss: 0.00084504
Iteration 25/25 | Loss: 0.00084504
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000845042522996664, 0.000845042522996664, 0.000845042522996664, 0.000845042522996664, 0.000845042522996664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000845042522996664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084504
Iteration 2/1000 | Loss: 0.00004002
Iteration 3/1000 | Loss: 0.00002485
Iteration 4/1000 | Loss: 0.00002267
Iteration 5/1000 | Loss: 0.00002165
Iteration 6/1000 | Loss: 0.00002099
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00001997
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001968
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001934
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001914
Iteration 16/1000 | Loss: 0.00001908
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001907
Iteration 19/1000 | Loss: 0.00001906
Iteration 20/1000 | Loss: 0.00001899
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001894
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001875
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001873
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001873
Iteration 44/1000 | Loss: 0.00001873
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001872
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001871
Iteration 49/1000 | Loss: 0.00001871
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001865
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001865
Iteration 72/1000 | Loss: 0.00001865
Iteration 73/1000 | Loss: 0.00001865
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001864
Iteration 78/1000 | Loss: 0.00001864
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001863
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001861
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001855
Iteration 125/1000 | Loss: 0.00001855
Iteration 126/1000 | Loss: 0.00001855
Iteration 127/1000 | Loss: 0.00001855
Iteration 128/1000 | Loss: 0.00001855
Iteration 129/1000 | Loss: 0.00001854
Iteration 130/1000 | Loss: 0.00001854
Iteration 131/1000 | Loss: 0.00001854
Iteration 132/1000 | Loss: 0.00001854
Iteration 133/1000 | Loss: 0.00001854
Iteration 134/1000 | Loss: 0.00001854
Iteration 135/1000 | Loss: 0.00001854
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001853
Iteration 141/1000 | Loss: 0.00001853
Iteration 142/1000 | Loss: 0.00001853
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Iteration 149/1000 | Loss: 0.00001852
Iteration 150/1000 | Loss: 0.00001852
Iteration 151/1000 | Loss: 0.00001852
Iteration 152/1000 | Loss: 0.00001852
Iteration 153/1000 | Loss: 0.00001852
Iteration 154/1000 | Loss: 0.00001852
Iteration 155/1000 | Loss: 0.00001851
Iteration 156/1000 | Loss: 0.00001851
Iteration 157/1000 | Loss: 0.00001851
Iteration 158/1000 | Loss: 0.00001851
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.8511605958337896e-05, 1.8511605958337896e-05, 1.8511605958337896e-05, 1.8511605958337896e-05, 1.8511605958337896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8511605958337896e-05

Optimization complete. Final v2v error: 3.676693916320801 mm

Highest mean error: 4.226190567016602 mm for frame 175

Lowest mean error: 3.0610430240631104 mm for frame 61

Saving results

Total time: 69.35935497283936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870795
Iteration 2/25 | Loss: 0.00143425
Iteration 3/25 | Loss: 0.00139087
Iteration 4/25 | Loss: 0.00138460
Iteration 5/25 | Loss: 0.00138365
Iteration 6/25 | Loss: 0.00138365
Iteration 7/25 | Loss: 0.00138365
Iteration 8/25 | Loss: 0.00138365
Iteration 9/25 | Loss: 0.00138365
Iteration 10/25 | Loss: 0.00138365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013836504658684134, 0.0013836504658684134, 0.0013836504658684134, 0.0013836504658684134, 0.0013836504658684134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013836504658684134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34761930
Iteration 2/25 | Loss: 0.00106070
Iteration 3/25 | Loss: 0.00106070
Iteration 4/25 | Loss: 0.00106070
Iteration 5/25 | Loss: 0.00106070
Iteration 6/25 | Loss: 0.00106070
Iteration 7/25 | Loss: 0.00106070
Iteration 8/25 | Loss: 0.00106070
Iteration 9/25 | Loss: 0.00106070
Iteration 10/25 | Loss: 0.00106070
Iteration 11/25 | Loss: 0.00106070
Iteration 12/25 | Loss: 0.00106070
Iteration 13/25 | Loss: 0.00106070
Iteration 14/25 | Loss: 0.00106070
Iteration 15/25 | Loss: 0.00106070
Iteration 16/25 | Loss: 0.00106070
Iteration 17/25 | Loss: 0.00106070
Iteration 18/25 | Loss: 0.00106070
Iteration 19/25 | Loss: 0.00106070
Iteration 20/25 | Loss: 0.00106070
Iteration 21/25 | Loss: 0.00106070
Iteration 22/25 | Loss: 0.00106070
Iteration 23/25 | Loss: 0.00106070
Iteration 24/25 | Loss: 0.00106070
Iteration 25/25 | Loss: 0.00106070

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106070
Iteration 2/1000 | Loss: 0.00003450
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00002071
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001854
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001853
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001848
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001848
Iteration 35/1000 | Loss: 0.00001848
Iteration 36/1000 | Loss: 0.00001848
Iteration 37/1000 | Loss: 0.00001847
Iteration 38/1000 | Loss: 0.00001847
Iteration 39/1000 | Loss: 0.00001847
Iteration 40/1000 | Loss: 0.00001847
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001846
Iteration 45/1000 | Loss: 0.00001846
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001845
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001844
Iteration 57/1000 | Loss: 0.00001844
Iteration 58/1000 | Loss: 0.00001844
Iteration 59/1000 | Loss: 0.00001844
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001843
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001838
Iteration 82/1000 | Loss: 0.00001838
Iteration 83/1000 | Loss: 0.00001838
Iteration 84/1000 | Loss: 0.00001837
Iteration 85/1000 | Loss: 0.00001837
Iteration 86/1000 | Loss: 0.00001837
Iteration 87/1000 | Loss: 0.00001837
Iteration 88/1000 | Loss: 0.00001837
Iteration 89/1000 | Loss: 0.00001837
Iteration 90/1000 | Loss: 0.00001837
Iteration 91/1000 | Loss: 0.00001837
Iteration 92/1000 | Loss: 0.00001837
Iteration 93/1000 | Loss: 0.00001836
Iteration 94/1000 | Loss: 0.00001836
Iteration 95/1000 | Loss: 0.00001836
Iteration 96/1000 | Loss: 0.00001836
Iteration 97/1000 | Loss: 0.00001836
Iteration 98/1000 | Loss: 0.00001836
Iteration 99/1000 | Loss: 0.00001836
Iteration 100/1000 | Loss: 0.00001835
Iteration 101/1000 | Loss: 0.00001835
Iteration 102/1000 | Loss: 0.00001835
Iteration 103/1000 | Loss: 0.00001835
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00001834
Iteration 106/1000 | Loss: 0.00001834
Iteration 107/1000 | Loss: 0.00001833
Iteration 108/1000 | Loss: 0.00001833
Iteration 109/1000 | Loss: 0.00001833
Iteration 110/1000 | Loss: 0.00001832
Iteration 111/1000 | Loss: 0.00001832
Iteration 112/1000 | Loss: 0.00001832
Iteration 113/1000 | Loss: 0.00001831
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001830
Iteration 116/1000 | Loss: 0.00001829
Iteration 117/1000 | Loss: 0.00001829
Iteration 118/1000 | Loss: 0.00001829
Iteration 119/1000 | Loss: 0.00001829
Iteration 120/1000 | Loss: 0.00001829
Iteration 121/1000 | Loss: 0.00001829
Iteration 122/1000 | Loss: 0.00001828
Iteration 123/1000 | Loss: 0.00001828
Iteration 124/1000 | Loss: 0.00001828
Iteration 125/1000 | Loss: 0.00001828
Iteration 126/1000 | Loss: 0.00001828
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Iteration 130/1000 | Loss: 0.00001828
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001828
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001828
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001826
Iteration 138/1000 | Loss: 0.00001826
Iteration 139/1000 | Loss: 0.00001826
Iteration 140/1000 | Loss: 0.00001826
Iteration 141/1000 | Loss: 0.00001825
Iteration 142/1000 | Loss: 0.00001825
Iteration 143/1000 | Loss: 0.00001825
Iteration 144/1000 | Loss: 0.00001825
Iteration 145/1000 | Loss: 0.00001825
Iteration 146/1000 | Loss: 0.00001825
Iteration 147/1000 | Loss: 0.00001825
Iteration 148/1000 | Loss: 0.00001825
Iteration 149/1000 | Loss: 0.00001825
Iteration 150/1000 | Loss: 0.00001825
Iteration 151/1000 | Loss: 0.00001824
Iteration 152/1000 | Loss: 0.00001824
Iteration 153/1000 | Loss: 0.00001824
Iteration 154/1000 | Loss: 0.00001824
Iteration 155/1000 | Loss: 0.00001824
Iteration 156/1000 | Loss: 0.00001824
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001823
Iteration 160/1000 | Loss: 0.00001823
Iteration 161/1000 | Loss: 0.00001823
Iteration 162/1000 | Loss: 0.00001822
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001821
Iteration 167/1000 | Loss: 0.00001821
Iteration 168/1000 | Loss: 0.00001821
Iteration 169/1000 | Loss: 0.00001820
Iteration 170/1000 | Loss: 0.00001820
Iteration 171/1000 | Loss: 0.00001820
Iteration 172/1000 | Loss: 0.00001820
Iteration 173/1000 | Loss: 0.00001819
Iteration 174/1000 | Loss: 0.00001819
Iteration 175/1000 | Loss: 0.00001819
Iteration 176/1000 | Loss: 0.00001818
Iteration 177/1000 | Loss: 0.00001818
Iteration 178/1000 | Loss: 0.00001817
Iteration 179/1000 | Loss: 0.00001817
Iteration 180/1000 | Loss: 0.00001817
Iteration 181/1000 | Loss: 0.00001817
Iteration 182/1000 | Loss: 0.00001817
Iteration 183/1000 | Loss: 0.00001816
Iteration 184/1000 | Loss: 0.00001816
Iteration 185/1000 | Loss: 0.00001816
Iteration 186/1000 | Loss: 0.00001815
Iteration 187/1000 | Loss: 0.00001815
Iteration 188/1000 | Loss: 0.00001815
Iteration 189/1000 | Loss: 0.00001815
Iteration 190/1000 | Loss: 0.00001814
Iteration 191/1000 | Loss: 0.00001814
Iteration 192/1000 | Loss: 0.00001814
Iteration 193/1000 | Loss: 0.00001814
Iteration 194/1000 | Loss: 0.00001814
Iteration 195/1000 | Loss: 0.00001814
Iteration 196/1000 | Loss: 0.00001814
Iteration 197/1000 | Loss: 0.00001814
Iteration 198/1000 | Loss: 0.00001814
Iteration 199/1000 | Loss: 0.00001813
Iteration 200/1000 | Loss: 0.00001813
Iteration 201/1000 | Loss: 0.00001813
Iteration 202/1000 | Loss: 0.00001813
Iteration 203/1000 | Loss: 0.00001812
Iteration 204/1000 | Loss: 0.00001812
Iteration 205/1000 | Loss: 0.00001812
Iteration 206/1000 | Loss: 0.00001812
Iteration 207/1000 | Loss: 0.00001812
Iteration 208/1000 | Loss: 0.00001812
Iteration 209/1000 | Loss: 0.00001812
Iteration 210/1000 | Loss: 0.00001812
Iteration 211/1000 | Loss: 0.00001812
Iteration 212/1000 | Loss: 0.00001812
Iteration 213/1000 | Loss: 0.00001812
Iteration 214/1000 | Loss: 0.00001812
Iteration 215/1000 | Loss: 0.00001811
Iteration 216/1000 | Loss: 0.00001811
Iteration 217/1000 | Loss: 0.00001811
Iteration 218/1000 | Loss: 0.00001811
Iteration 219/1000 | Loss: 0.00001811
Iteration 220/1000 | Loss: 0.00001810
Iteration 221/1000 | Loss: 0.00001810
Iteration 222/1000 | Loss: 0.00001810
Iteration 223/1000 | Loss: 0.00001810
Iteration 224/1000 | Loss: 0.00001810
Iteration 225/1000 | Loss: 0.00001810
Iteration 226/1000 | Loss: 0.00001809
Iteration 227/1000 | Loss: 0.00001809
Iteration 228/1000 | Loss: 0.00001809
Iteration 229/1000 | Loss: 0.00001809
Iteration 230/1000 | Loss: 0.00001809
Iteration 231/1000 | Loss: 0.00001809
Iteration 232/1000 | Loss: 0.00001809
Iteration 233/1000 | Loss: 0.00001809
Iteration 234/1000 | Loss: 0.00001809
Iteration 235/1000 | Loss: 0.00001809
Iteration 236/1000 | Loss: 0.00001809
Iteration 237/1000 | Loss: 0.00001809
Iteration 238/1000 | Loss: 0.00001809
Iteration 239/1000 | Loss: 0.00001809
Iteration 240/1000 | Loss: 0.00001808
Iteration 241/1000 | Loss: 0.00001808
Iteration 242/1000 | Loss: 0.00001808
Iteration 243/1000 | Loss: 0.00001808
Iteration 244/1000 | Loss: 0.00001808
Iteration 245/1000 | Loss: 0.00001808
Iteration 246/1000 | Loss: 0.00001808
Iteration 247/1000 | Loss: 0.00001808
Iteration 248/1000 | Loss: 0.00001808
Iteration 249/1000 | Loss: 0.00001808
Iteration 250/1000 | Loss: 0.00001808
Iteration 251/1000 | Loss: 0.00001807
Iteration 252/1000 | Loss: 0.00001807
Iteration 253/1000 | Loss: 0.00001807
Iteration 254/1000 | Loss: 0.00001807
Iteration 255/1000 | Loss: 0.00001807
Iteration 256/1000 | Loss: 0.00001807
Iteration 257/1000 | Loss: 0.00001807
Iteration 258/1000 | Loss: 0.00001807
Iteration 259/1000 | Loss: 0.00001807
Iteration 260/1000 | Loss: 0.00001807
Iteration 261/1000 | Loss: 0.00001807
Iteration 262/1000 | Loss: 0.00001807
Iteration 263/1000 | Loss: 0.00001807
Iteration 264/1000 | Loss: 0.00001807
Iteration 265/1000 | Loss: 0.00001807
Iteration 266/1000 | Loss: 0.00001807
Iteration 267/1000 | Loss: 0.00001807
Iteration 268/1000 | Loss: 0.00001806
Iteration 269/1000 | Loss: 0.00001806
Iteration 270/1000 | Loss: 0.00001806
Iteration 271/1000 | Loss: 0.00001806
Iteration 272/1000 | Loss: 0.00001806
Iteration 273/1000 | Loss: 0.00001806
Iteration 274/1000 | Loss: 0.00001805
Iteration 275/1000 | Loss: 0.00001805
Iteration 276/1000 | Loss: 0.00001805
Iteration 277/1000 | Loss: 0.00001805
Iteration 278/1000 | Loss: 0.00001805
Iteration 279/1000 | Loss: 0.00001805
Iteration 280/1000 | Loss: 0.00001805
Iteration 281/1000 | Loss: 0.00001805
Iteration 282/1000 | Loss: 0.00001805
Iteration 283/1000 | Loss: 0.00001805
Iteration 284/1000 | Loss: 0.00001805
Iteration 285/1000 | Loss: 0.00001805
Iteration 286/1000 | Loss: 0.00001805
Iteration 287/1000 | Loss: 0.00001805
Iteration 288/1000 | Loss: 0.00001805
Iteration 289/1000 | Loss: 0.00001805
Iteration 290/1000 | Loss: 0.00001805
Iteration 291/1000 | Loss: 0.00001805
Iteration 292/1000 | Loss: 0.00001805
Iteration 293/1000 | Loss: 0.00001805
Iteration 294/1000 | Loss: 0.00001805
Iteration 295/1000 | Loss: 0.00001805
Iteration 296/1000 | Loss: 0.00001805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.804618841561023e-05, 1.804618841561023e-05, 1.804618841561023e-05, 1.804618841561023e-05, 1.804618841561023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.804618841561023e-05

Optimization complete. Final v2v error: 3.5597212314605713 mm

Highest mean error: 3.7183103561401367 mm for frame 16

Lowest mean error: 3.418304681777954 mm for frame 196

Saving results

Total time: 47.4835319519043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793993
Iteration 2/25 | Loss: 0.00165802
Iteration 3/25 | Loss: 0.00149484
Iteration 4/25 | Loss: 0.00136751
Iteration 5/25 | Loss: 0.00136824
Iteration 6/25 | Loss: 0.00137454
Iteration 7/25 | Loss: 0.00138193
Iteration 8/25 | Loss: 0.00134145
Iteration 9/25 | Loss: 0.00133965
Iteration 10/25 | Loss: 0.00135778
Iteration 11/25 | Loss: 0.00132350
Iteration 12/25 | Loss: 0.00132867
Iteration 13/25 | Loss: 0.00132479
Iteration 14/25 | Loss: 0.00132526
Iteration 15/25 | Loss: 0.00132185
Iteration 16/25 | Loss: 0.00132272
Iteration 17/25 | Loss: 0.00132175
Iteration 18/25 | Loss: 0.00132174
Iteration 19/25 | Loss: 0.00132174
Iteration 20/25 | Loss: 0.00132174
Iteration 21/25 | Loss: 0.00132174
Iteration 22/25 | Loss: 0.00132174
Iteration 23/25 | Loss: 0.00132174
Iteration 24/25 | Loss: 0.00132174
Iteration 25/25 | Loss: 0.00132174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87386417
Iteration 2/25 | Loss: 0.00120134
Iteration 3/25 | Loss: 0.00095228
Iteration 4/25 | Loss: 0.00095228
Iteration 5/25 | Loss: 0.00095228
Iteration 6/25 | Loss: 0.00095228
Iteration 7/25 | Loss: 0.00095228
Iteration 8/25 | Loss: 0.00095228
Iteration 9/25 | Loss: 0.00095227
Iteration 10/25 | Loss: 0.00095227
Iteration 11/25 | Loss: 0.00095227
Iteration 12/25 | Loss: 0.00095227
Iteration 13/25 | Loss: 0.00095227
Iteration 14/25 | Loss: 0.00095227
Iteration 15/25 | Loss: 0.00095227
Iteration 16/25 | Loss: 0.00095227
Iteration 17/25 | Loss: 0.00095227
Iteration 18/25 | Loss: 0.00095227
Iteration 19/25 | Loss: 0.00095227
Iteration 20/25 | Loss: 0.00095227
Iteration 21/25 | Loss: 0.00095227
Iteration 22/25 | Loss: 0.00095227
Iteration 23/25 | Loss: 0.00095227
Iteration 24/25 | Loss: 0.00095227
Iteration 25/25 | Loss: 0.00095227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095227
Iteration 2/1000 | Loss: 0.00008815
Iteration 3/1000 | Loss: 0.00028734
Iteration 4/1000 | Loss: 0.00002531
Iteration 5/1000 | Loss: 0.00012341
Iteration 6/1000 | Loss: 0.00024274
Iteration 7/1000 | Loss: 0.00002335
Iteration 8/1000 | Loss: 0.00002265
Iteration 9/1000 | Loss: 0.00007041
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002149
Iteration 12/1000 | Loss: 0.00008146
Iteration 13/1000 | Loss: 0.00003485
Iteration 14/1000 | Loss: 0.00002105
Iteration 15/1000 | Loss: 0.00004109
Iteration 16/1000 | Loss: 0.00026951
Iteration 17/1000 | Loss: 0.00003638
Iteration 18/1000 | Loss: 0.00002532
Iteration 19/1000 | Loss: 0.00002082
Iteration 20/1000 | Loss: 0.00002080
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00002047
Iteration 24/1000 | Loss: 0.00002035
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002022
Iteration 27/1000 | Loss: 0.00002011
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002003
Iteration 30/1000 | Loss: 0.00010936
Iteration 31/1000 | Loss: 0.00005079
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00008686
Iteration 34/1000 | Loss: 0.00004734
Iteration 35/1000 | Loss: 0.00006795
Iteration 36/1000 | Loss: 0.00002033
Iteration 37/1000 | Loss: 0.00003811
Iteration 38/1000 | Loss: 0.00002011
Iteration 39/1000 | Loss: 0.00002982
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001984
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001983
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001983
Iteration 66/1000 | Loss: 0.00001983
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001982
Iteration 73/1000 | Loss: 0.00001982
Iteration 74/1000 | Loss: 0.00001982
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001981
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001980
Iteration 81/1000 | Loss: 0.00001980
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001979
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001977
Iteration 93/1000 | Loss: 0.00001977
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00001976
Iteration 96/1000 | Loss: 0.00001976
Iteration 97/1000 | Loss: 0.00001976
Iteration 98/1000 | Loss: 0.00001976
Iteration 99/1000 | Loss: 0.00001976
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001976
Iteration 104/1000 | Loss: 0.00001976
Iteration 105/1000 | Loss: 0.00001976
Iteration 106/1000 | Loss: 0.00001976
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.9757699192268774e-05, 1.9757699192268774e-05, 1.9757699192268774e-05, 1.9757699192268774e-05, 1.9757699192268774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9757699192268774e-05

Optimization complete. Final v2v error: 3.778595209121704 mm

Highest mean error: 4.221649646759033 mm for frame 104

Lowest mean error: 3.461515426635742 mm for frame 133

Saving results

Total time: 82.40390658378601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895054
Iteration 2/25 | Loss: 0.00173235
Iteration 3/25 | Loss: 0.00144068
Iteration 4/25 | Loss: 0.00141674
Iteration 5/25 | Loss: 0.00140956
Iteration 6/25 | Loss: 0.00140762
Iteration 7/25 | Loss: 0.00140756
Iteration 8/25 | Loss: 0.00140756
Iteration 9/25 | Loss: 0.00140756
Iteration 10/25 | Loss: 0.00140756
Iteration 11/25 | Loss: 0.00140756
Iteration 12/25 | Loss: 0.00140756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001407560077495873, 0.001407560077495873, 0.001407560077495873, 0.001407560077495873, 0.001407560077495873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001407560077495873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09240162
Iteration 2/25 | Loss: 0.00089604
Iteration 3/25 | Loss: 0.00089602
Iteration 4/25 | Loss: 0.00089602
Iteration 5/25 | Loss: 0.00089602
Iteration 6/25 | Loss: 0.00089602
Iteration 7/25 | Loss: 0.00089602
Iteration 8/25 | Loss: 0.00089602
Iteration 9/25 | Loss: 0.00089602
Iteration 10/25 | Loss: 0.00089602
Iteration 11/25 | Loss: 0.00089602
Iteration 12/25 | Loss: 0.00089602
Iteration 13/25 | Loss: 0.00089602
Iteration 14/25 | Loss: 0.00089602
Iteration 15/25 | Loss: 0.00089602
Iteration 16/25 | Loss: 0.00089602
Iteration 17/25 | Loss: 0.00089602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008960198028944433, 0.0008960198028944433, 0.0008960198028944433, 0.0008960198028944433, 0.0008960198028944433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008960198028944433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089602
Iteration 2/1000 | Loss: 0.00006013
Iteration 3/1000 | Loss: 0.00004024
Iteration 4/1000 | Loss: 0.00003191
Iteration 5/1000 | Loss: 0.00002963
Iteration 6/1000 | Loss: 0.00002852
Iteration 7/1000 | Loss: 0.00002777
Iteration 8/1000 | Loss: 0.00002702
Iteration 9/1000 | Loss: 0.00002650
Iteration 10/1000 | Loss: 0.00002616
Iteration 11/1000 | Loss: 0.00002583
Iteration 12/1000 | Loss: 0.00002560
Iteration 13/1000 | Loss: 0.00002534
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002479
Iteration 17/1000 | Loss: 0.00002457
Iteration 18/1000 | Loss: 0.00002441
Iteration 19/1000 | Loss: 0.00002426
Iteration 20/1000 | Loss: 0.00002422
Iteration 21/1000 | Loss: 0.00002414
Iteration 22/1000 | Loss: 0.00002410
Iteration 23/1000 | Loss: 0.00002410
Iteration 24/1000 | Loss: 0.00002410
Iteration 25/1000 | Loss: 0.00002406
Iteration 26/1000 | Loss: 0.00002405
Iteration 27/1000 | Loss: 0.00002404
Iteration 28/1000 | Loss: 0.00002402
Iteration 29/1000 | Loss: 0.00002400
Iteration 30/1000 | Loss: 0.00002399
Iteration 31/1000 | Loss: 0.00002399
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002395
Iteration 34/1000 | Loss: 0.00002394
Iteration 35/1000 | Loss: 0.00002392
Iteration 36/1000 | Loss: 0.00002392
Iteration 37/1000 | Loss: 0.00002392
Iteration 38/1000 | Loss: 0.00002392
Iteration 39/1000 | Loss: 0.00002391
Iteration 40/1000 | Loss: 0.00002391
Iteration 41/1000 | Loss: 0.00002391
Iteration 42/1000 | Loss: 0.00002391
Iteration 43/1000 | Loss: 0.00002391
Iteration 44/1000 | Loss: 0.00002391
Iteration 45/1000 | Loss: 0.00002391
Iteration 46/1000 | Loss: 0.00002391
Iteration 47/1000 | Loss: 0.00002391
Iteration 48/1000 | Loss: 0.00002390
Iteration 49/1000 | Loss: 0.00002390
Iteration 50/1000 | Loss: 0.00002390
Iteration 51/1000 | Loss: 0.00002389
Iteration 52/1000 | Loss: 0.00002389
Iteration 53/1000 | Loss: 0.00002389
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002388
Iteration 56/1000 | Loss: 0.00002388
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002388
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002386
Iteration 64/1000 | Loss: 0.00002386
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002386
Iteration 69/1000 | Loss: 0.00002386
Iteration 70/1000 | Loss: 0.00002385
Iteration 71/1000 | Loss: 0.00002385
Iteration 72/1000 | Loss: 0.00002385
Iteration 73/1000 | Loss: 0.00002385
Iteration 74/1000 | Loss: 0.00002385
Iteration 75/1000 | Loss: 0.00002385
Iteration 76/1000 | Loss: 0.00002385
Iteration 77/1000 | Loss: 0.00002385
Iteration 78/1000 | Loss: 0.00002385
Iteration 79/1000 | Loss: 0.00002385
Iteration 80/1000 | Loss: 0.00002384
Iteration 81/1000 | Loss: 0.00002384
Iteration 82/1000 | Loss: 0.00002384
Iteration 83/1000 | Loss: 0.00002384
Iteration 84/1000 | Loss: 0.00002384
Iteration 85/1000 | Loss: 0.00002384
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002384
Iteration 88/1000 | Loss: 0.00002384
Iteration 89/1000 | Loss: 0.00002384
Iteration 90/1000 | Loss: 0.00002384
Iteration 91/1000 | Loss: 0.00002383
Iteration 92/1000 | Loss: 0.00002383
Iteration 93/1000 | Loss: 0.00002383
Iteration 94/1000 | Loss: 0.00002383
Iteration 95/1000 | Loss: 0.00002383
Iteration 96/1000 | Loss: 0.00002383
Iteration 97/1000 | Loss: 0.00002383
Iteration 98/1000 | Loss: 0.00002383
Iteration 99/1000 | Loss: 0.00002383
Iteration 100/1000 | Loss: 0.00002383
Iteration 101/1000 | Loss: 0.00002383
Iteration 102/1000 | Loss: 0.00002383
Iteration 103/1000 | Loss: 0.00002383
Iteration 104/1000 | Loss: 0.00002382
Iteration 105/1000 | Loss: 0.00002382
Iteration 106/1000 | Loss: 0.00002382
Iteration 107/1000 | Loss: 0.00002382
Iteration 108/1000 | Loss: 0.00002382
Iteration 109/1000 | Loss: 0.00002382
Iteration 110/1000 | Loss: 0.00002382
Iteration 111/1000 | Loss: 0.00002382
Iteration 112/1000 | Loss: 0.00002382
Iteration 113/1000 | Loss: 0.00002382
Iteration 114/1000 | Loss: 0.00002382
Iteration 115/1000 | Loss: 0.00002381
Iteration 116/1000 | Loss: 0.00002381
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002381
Iteration 119/1000 | Loss: 0.00002381
Iteration 120/1000 | Loss: 0.00002381
Iteration 121/1000 | Loss: 0.00002381
Iteration 122/1000 | Loss: 0.00002381
Iteration 123/1000 | Loss: 0.00002381
Iteration 124/1000 | Loss: 0.00002380
Iteration 125/1000 | Loss: 0.00002380
Iteration 126/1000 | Loss: 0.00002380
Iteration 127/1000 | Loss: 0.00002380
Iteration 128/1000 | Loss: 0.00002380
Iteration 129/1000 | Loss: 0.00002380
Iteration 130/1000 | Loss: 0.00002380
Iteration 131/1000 | Loss: 0.00002380
Iteration 132/1000 | Loss: 0.00002380
Iteration 133/1000 | Loss: 0.00002380
Iteration 134/1000 | Loss: 0.00002380
Iteration 135/1000 | Loss: 0.00002379
Iteration 136/1000 | Loss: 0.00002379
Iteration 137/1000 | Loss: 0.00002379
Iteration 138/1000 | Loss: 0.00002379
Iteration 139/1000 | Loss: 0.00002379
Iteration 140/1000 | Loss: 0.00002379
Iteration 141/1000 | Loss: 0.00002379
Iteration 142/1000 | Loss: 0.00002379
Iteration 143/1000 | Loss: 0.00002379
Iteration 144/1000 | Loss: 0.00002379
Iteration 145/1000 | Loss: 0.00002379
Iteration 146/1000 | Loss: 0.00002379
Iteration 147/1000 | Loss: 0.00002379
Iteration 148/1000 | Loss: 0.00002379
Iteration 149/1000 | Loss: 0.00002379
Iteration 150/1000 | Loss: 0.00002379
Iteration 151/1000 | Loss: 0.00002379
Iteration 152/1000 | Loss: 0.00002379
Iteration 153/1000 | Loss: 0.00002379
Iteration 154/1000 | Loss: 0.00002379
Iteration 155/1000 | Loss: 0.00002379
Iteration 156/1000 | Loss: 0.00002379
Iteration 157/1000 | Loss: 0.00002379
Iteration 158/1000 | Loss: 0.00002379
Iteration 159/1000 | Loss: 0.00002379
Iteration 160/1000 | Loss: 0.00002379
Iteration 161/1000 | Loss: 0.00002379
Iteration 162/1000 | Loss: 0.00002379
Iteration 163/1000 | Loss: 0.00002379
Iteration 164/1000 | Loss: 0.00002379
Iteration 165/1000 | Loss: 0.00002379
Iteration 166/1000 | Loss: 0.00002379
Iteration 167/1000 | Loss: 0.00002379
Iteration 168/1000 | Loss: 0.00002379
Iteration 169/1000 | Loss: 0.00002379
Iteration 170/1000 | Loss: 0.00002379
Iteration 171/1000 | Loss: 0.00002379
Iteration 172/1000 | Loss: 0.00002379
Iteration 173/1000 | Loss: 0.00002379
Iteration 174/1000 | Loss: 0.00002379
Iteration 175/1000 | Loss: 0.00002379
Iteration 176/1000 | Loss: 0.00002379
Iteration 177/1000 | Loss: 0.00002379
Iteration 178/1000 | Loss: 0.00002379
Iteration 179/1000 | Loss: 0.00002379
Iteration 180/1000 | Loss: 0.00002379
Iteration 181/1000 | Loss: 0.00002379
Iteration 182/1000 | Loss: 0.00002379
Iteration 183/1000 | Loss: 0.00002379
Iteration 184/1000 | Loss: 0.00002379
Iteration 185/1000 | Loss: 0.00002379
Iteration 186/1000 | Loss: 0.00002379
Iteration 187/1000 | Loss: 0.00002379
Iteration 188/1000 | Loss: 0.00002379
Iteration 189/1000 | Loss: 0.00002379
Iteration 190/1000 | Loss: 0.00002379
Iteration 191/1000 | Loss: 0.00002379
Iteration 192/1000 | Loss: 0.00002379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.3787702957633883e-05, 2.3787702957633883e-05, 2.3787702957633883e-05, 2.3787702957633883e-05, 2.3787702957633883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3787702957633883e-05

Optimization complete. Final v2v error: 4.053621292114258 mm

Highest mean error: 5.248908042907715 mm for frame 103

Lowest mean error: 3.291518449783325 mm for frame 122

Saving results

Total time: 48.36836528778076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804371
Iteration 2/25 | Loss: 0.00179527
Iteration 3/25 | Loss: 0.00154616
Iteration 4/25 | Loss: 0.00152604
Iteration 5/25 | Loss: 0.00152222
Iteration 6/25 | Loss: 0.00152172
Iteration 7/25 | Loss: 0.00152172
Iteration 8/25 | Loss: 0.00152172
Iteration 9/25 | Loss: 0.00152172
Iteration 10/25 | Loss: 0.00152172
Iteration 11/25 | Loss: 0.00152172
Iteration 12/25 | Loss: 0.00152172
Iteration 13/25 | Loss: 0.00152172
Iteration 14/25 | Loss: 0.00152172
Iteration 15/25 | Loss: 0.00152172
Iteration 16/25 | Loss: 0.00152172
Iteration 17/25 | Loss: 0.00152172
Iteration 18/25 | Loss: 0.00152172
Iteration 19/25 | Loss: 0.00152172
Iteration 20/25 | Loss: 0.00152172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015217215986922383, 0.0015217215986922383, 0.0015217215986922383, 0.0015217215986922383, 0.0015217215986922383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015217215986922383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.27168882
Iteration 2/25 | Loss: 0.00142909
Iteration 3/25 | Loss: 0.00142909
Iteration 4/25 | Loss: 0.00142909
Iteration 5/25 | Loss: 0.00142909
Iteration 6/25 | Loss: 0.00142909
Iteration 7/25 | Loss: 0.00142908
Iteration 8/25 | Loss: 0.00142908
Iteration 9/25 | Loss: 0.00142908
Iteration 10/25 | Loss: 0.00142908
Iteration 11/25 | Loss: 0.00142908
Iteration 12/25 | Loss: 0.00142908
Iteration 13/25 | Loss: 0.00142908
Iteration 14/25 | Loss: 0.00142908
Iteration 15/25 | Loss: 0.00142908
Iteration 16/25 | Loss: 0.00142908
Iteration 17/25 | Loss: 0.00142908
Iteration 18/25 | Loss: 0.00142908
Iteration 19/25 | Loss: 0.00142908
Iteration 20/25 | Loss: 0.00142908
Iteration 21/25 | Loss: 0.00142908
Iteration 22/25 | Loss: 0.00142908
Iteration 23/25 | Loss: 0.00142908
Iteration 24/25 | Loss: 0.00142908
Iteration 25/25 | Loss: 0.00142908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014290839899331331, 0.0014290839899331331, 0.0014290839899331331, 0.0014290839899331331, 0.0014290839899331331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014290839899331331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142908
Iteration 2/1000 | Loss: 0.00007160
Iteration 3/1000 | Loss: 0.00004660
Iteration 4/1000 | Loss: 0.00003537
Iteration 5/1000 | Loss: 0.00003239
Iteration 6/1000 | Loss: 0.00003034
Iteration 7/1000 | Loss: 0.00002922
Iteration 8/1000 | Loss: 0.00002830
Iteration 9/1000 | Loss: 0.00002770
Iteration 10/1000 | Loss: 0.00002741
Iteration 11/1000 | Loss: 0.00002714
Iteration 12/1000 | Loss: 0.00002696
Iteration 13/1000 | Loss: 0.00002673
Iteration 14/1000 | Loss: 0.00002654
Iteration 15/1000 | Loss: 0.00002643
Iteration 16/1000 | Loss: 0.00002637
Iteration 17/1000 | Loss: 0.00002637
Iteration 18/1000 | Loss: 0.00002635
Iteration 19/1000 | Loss: 0.00002634
Iteration 20/1000 | Loss: 0.00002634
Iteration 21/1000 | Loss: 0.00002633
Iteration 22/1000 | Loss: 0.00002633
Iteration 23/1000 | Loss: 0.00002631
Iteration 24/1000 | Loss: 0.00002631
Iteration 25/1000 | Loss: 0.00002631
Iteration 26/1000 | Loss: 0.00002631
Iteration 27/1000 | Loss: 0.00002631
Iteration 28/1000 | Loss: 0.00002631
Iteration 29/1000 | Loss: 0.00002631
Iteration 30/1000 | Loss: 0.00002631
Iteration 31/1000 | Loss: 0.00002631
Iteration 32/1000 | Loss: 0.00002630
Iteration 33/1000 | Loss: 0.00002630
Iteration 34/1000 | Loss: 0.00002630
Iteration 35/1000 | Loss: 0.00002630
Iteration 36/1000 | Loss: 0.00002630
Iteration 37/1000 | Loss: 0.00002629
Iteration 38/1000 | Loss: 0.00002628
Iteration 39/1000 | Loss: 0.00002628
Iteration 40/1000 | Loss: 0.00002628
Iteration 41/1000 | Loss: 0.00002628
Iteration 42/1000 | Loss: 0.00002628
Iteration 43/1000 | Loss: 0.00002628
Iteration 44/1000 | Loss: 0.00002628
Iteration 45/1000 | Loss: 0.00002628
Iteration 46/1000 | Loss: 0.00002628
Iteration 47/1000 | Loss: 0.00002628
Iteration 48/1000 | Loss: 0.00002628
Iteration 49/1000 | Loss: 0.00002628
Iteration 50/1000 | Loss: 0.00002628
Iteration 51/1000 | Loss: 0.00002627
Iteration 52/1000 | Loss: 0.00002627
Iteration 53/1000 | Loss: 0.00002627
Iteration 54/1000 | Loss: 0.00002626
Iteration 55/1000 | Loss: 0.00002626
Iteration 56/1000 | Loss: 0.00002626
Iteration 57/1000 | Loss: 0.00002626
Iteration 58/1000 | Loss: 0.00002626
Iteration 59/1000 | Loss: 0.00002626
Iteration 60/1000 | Loss: 0.00002626
Iteration 61/1000 | Loss: 0.00002625
Iteration 62/1000 | Loss: 0.00002625
Iteration 63/1000 | Loss: 0.00002625
Iteration 64/1000 | Loss: 0.00002625
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002624
Iteration 67/1000 | Loss: 0.00002624
Iteration 68/1000 | Loss: 0.00002624
Iteration 69/1000 | Loss: 0.00002624
Iteration 70/1000 | Loss: 0.00002624
Iteration 71/1000 | Loss: 0.00002624
Iteration 72/1000 | Loss: 0.00002624
Iteration 73/1000 | Loss: 0.00002623
Iteration 74/1000 | Loss: 0.00002623
Iteration 75/1000 | Loss: 0.00002623
Iteration 76/1000 | Loss: 0.00002623
Iteration 77/1000 | Loss: 0.00002623
Iteration 78/1000 | Loss: 0.00002623
Iteration 79/1000 | Loss: 0.00002622
Iteration 80/1000 | Loss: 0.00002622
Iteration 81/1000 | Loss: 0.00002622
Iteration 82/1000 | Loss: 0.00002622
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002621
Iteration 86/1000 | Loss: 0.00002621
Iteration 87/1000 | Loss: 0.00002620
Iteration 88/1000 | Loss: 0.00002620
Iteration 89/1000 | Loss: 0.00002620
Iteration 90/1000 | Loss: 0.00002620
Iteration 91/1000 | Loss: 0.00002619
Iteration 92/1000 | Loss: 0.00002619
Iteration 93/1000 | Loss: 0.00002619
Iteration 94/1000 | Loss: 0.00002619
Iteration 95/1000 | Loss: 0.00002619
Iteration 96/1000 | Loss: 0.00002618
Iteration 97/1000 | Loss: 0.00002618
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002617
Iteration 100/1000 | Loss: 0.00002617
Iteration 101/1000 | Loss: 0.00002616
Iteration 102/1000 | Loss: 0.00002616
Iteration 103/1000 | Loss: 0.00002616
Iteration 104/1000 | Loss: 0.00002616
Iteration 105/1000 | Loss: 0.00002615
Iteration 106/1000 | Loss: 0.00002615
Iteration 107/1000 | Loss: 0.00002615
Iteration 108/1000 | Loss: 0.00002615
Iteration 109/1000 | Loss: 0.00002615
Iteration 110/1000 | Loss: 0.00002615
Iteration 111/1000 | Loss: 0.00002614
Iteration 112/1000 | Loss: 0.00002614
Iteration 113/1000 | Loss: 0.00002613
Iteration 114/1000 | Loss: 0.00002613
Iteration 115/1000 | Loss: 0.00002613
Iteration 116/1000 | Loss: 0.00002613
Iteration 117/1000 | Loss: 0.00002613
Iteration 118/1000 | Loss: 0.00002613
Iteration 119/1000 | Loss: 0.00002613
Iteration 120/1000 | Loss: 0.00002613
Iteration 121/1000 | Loss: 0.00002612
Iteration 122/1000 | Loss: 0.00002612
Iteration 123/1000 | Loss: 0.00002612
Iteration 124/1000 | Loss: 0.00002612
Iteration 125/1000 | Loss: 0.00002611
Iteration 126/1000 | Loss: 0.00002611
Iteration 127/1000 | Loss: 0.00002611
Iteration 128/1000 | Loss: 0.00002611
Iteration 129/1000 | Loss: 0.00002611
Iteration 130/1000 | Loss: 0.00002610
Iteration 131/1000 | Loss: 0.00002610
Iteration 132/1000 | Loss: 0.00002610
Iteration 133/1000 | Loss: 0.00002610
Iteration 134/1000 | Loss: 0.00002609
Iteration 135/1000 | Loss: 0.00002609
Iteration 136/1000 | Loss: 0.00002609
Iteration 137/1000 | Loss: 0.00002609
Iteration 138/1000 | Loss: 0.00002609
Iteration 139/1000 | Loss: 0.00002609
Iteration 140/1000 | Loss: 0.00002608
Iteration 141/1000 | Loss: 0.00002608
Iteration 142/1000 | Loss: 0.00002608
Iteration 143/1000 | Loss: 0.00002608
Iteration 144/1000 | Loss: 0.00002608
Iteration 145/1000 | Loss: 0.00002608
Iteration 146/1000 | Loss: 0.00002608
Iteration 147/1000 | Loss: 0.00002608
Iteration 148/1000 | Loss: 0.00002608
Iteration 149/1000 | Loss: 0.00002608
Iteration 150/1000 | Loss: 0.00002608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.6079052986460738e-05, 2.6079052986460738e-05, 2.6079052986460738e-05, 2.6079052986460738e-05, 2.6079052986460738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6079052986460738e-05

Optimization complete. Final v2v error: 4.149662971496582 mm

Highest mean error: 4.850785732269287 mm for frame 5

Lowest mean error: 3.548771858215332 mm for frame 144

Saving results

Total time: 39.70586109161377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852600
Iteration 2/25 | Loss: 0.00139072
Iteration 3/25 | Loss: 0.00131675
Iteration 4/25 | Loss: 0.00130369
Iteration 5/25 | Loss: 0.00129919
Iteration 6/25 | Loss: 0.00129871
Iteration 7/25 | Loss: 0.00129871
Iteration 8/25 | Loss: 0.00129871
Iteration 9/25 | Loss: 0.00129871
Iteration 10/25 | Loss: 0.00129871
Iteration 11/25 | Loss: 0.00129871
Iteration 12/25 | Loss: 0.00129871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001298705697990954, 0.001298705697990954, 0.001298705697990954, 0.001298705697990954, 0.001298705697990954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001298705697990954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.53166151
Iteration 2/25 | Loss: 0.00085423
Iteration 3/25 | Loss: 0.00085423
Iteration 4/25 | Loss: 0.00085423
Iteration 5/25 | Loss: 0.00085423
Iteration 6/25 | Loss: 0.00085423
Iteration 7/25 | Loss: 0.00085423
Iteration 8/25 | Loss: 0.00085423
Iteration 9/25 | Loss: 0.00085423
Iteration 10/25 | Loss: 0.00085423
Iteration 11/25 | Loss: 0.00085423
Iteration 12/25 | Loss: 0.00085423
Iteration 13/25 | Loss: 0.00085423
Iteration 14/25 | Loss: 0.00085423
Iteration 15/25 | Loss: 0.00085423
Iteration 16/25 | Loss: 0.00085423
Iteration 17/25 | Loss: 0.00085423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008542274590581656, 0.0008542274590581656, 0.0008542274590581656, 0.0008542274590581656, 0.0008542274590581656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008542274590581656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085423
Iteration 2/1000 | Loss: 0.00002885
Iteration 3/1000 | Loss: 0.00002182
Iteration 4/1000 | Loss: 0.00002021
Iteration 5/1000 | Loss: 0.00001929
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001814
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001722
Iteration 11/1000 | Loss: 0.00001701
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001696
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001673
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001636
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001607
Iteration 34/1000 | Loss: 0.00001607
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001605
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001602
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001600
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001600
Iteration 52/1000 | Loss: 0.00001600
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.597472873982042e-05, 1.597472873982042e-05, 1.597472873982042e-05, 1.597472873982042e-05, 1.597472873982042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597472873982042e-05

Optimization complete. Final v2v error: 3.4249744415283203 mm

Highest mean error: 3.79976224899292 mm for frame 180

Lowest mean error: 3.1902594566345215 mm for frame 58

Saving results

Total time: 37.08467745780945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980555
Iteration 2/25 | Loss: 0.00260300
Iteration 3/25 | Loss: 0.00199058
Iteration 4/25 | Loss: 0.00188985
Iteration 5/25 | Loss: 0.00188941
Iteration 6/25 | Loss: 0.00193706
Iteration 7/25 | Loss: 0.00184170
Iteration 8/25 | Loss: 0.00172610
Iteration 9/25 | Loss: 0.00165059
Iteration 10/25 | Loss: 0.00162234
Iteration 11/25 | Loss: 0.00160443
Iteration 12/25 | Loss: 0.00160260
Iteration 13/25 | Loss: 0.00157967
Iteration 14/25 | Loss: 0.00157004
Iteration 15/25 | Loss: 0.00155536
Iteration 16/25 | Loss: 0.00154800
Iteration 17/25 | Loss: 0.00154799
Iteration 18/25 | Loss: 0.00154355
Iteration 19/25 | Loss: 0.00152998
Iteration 20/25 | Loss: 0.00152511
Iteration 21/25 | Loss: 0.00152298
Iteration 22/25 | Loss: 0.00152130
Iteration 23/25 | Loss: 0.00152219
Iteration 24/25 | Loss: 0.00152102
Iteration 25/25 | Loss: 0.00152084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41276443
Iteration 2/25 | Loss: 0.00285268
Iteration 3/25 | Loss: 0.00268466
Iteration 4/25 | Loss: 0.00268348
Iteration 5/25 | Loss: 0.00268348
Iteration 6/25 | Loss: 0.00268348
Iteration 7/25 | Loss: 0.00268348
Iteration 8/25 | Loss: 0.00268348
Iteration 9/25 | Loss: 0.00268347
Iteration 10/25 | Loss: 0.00268347
Iteration 11/25 | Loss: 0.00268347
Iteration 12/25 | Loss: 0.00268347
Iteration 13/25 | Loss: 0.00268347
Iteration 14/25 | Loss: 0.00268347
Iteration 15/25 | Loss: 0.00268347
Iteration 16/25 | Loss: 0.00268347
Iteration 17/25 | Loss: 0.00268347
Iteration 18/25 | Loss: 0.00268347
Iteration 19/25 | Loss: 0.00268347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002683473750948906, 0.002683473750948906, 0.002683473750948906, 0.002683473750948906, 0.002683473750948906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002683473750948906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268347
Iteration 2/1000 | Loss: 0.00125147
Iteration 3/1000 | Loss: 0.00088838
Iteration 4/1000 | Loss: 0.00056033
Iteration 5/1000 | Loss: 0.00026350
Iteration 6/1000 | Loss: 0.00039366
Iteration 7/1000 | Loss: 0.00088903
Iteration 8/1000 | Loss: 0.00133309
Iteration 9/1000 | Loss: 0.00053505
Iteration 10/1000 | Loss: 0.00063976
Iteration 11/1000 | Loss: 0.00098234
Iteration 12/1000 | Loss: 0.00020619
Iteration 13/1000 | Loss: 0.00158550
Iteration 14/1000 | Loss: 0.00091862
Iteration 15/1000 | Loss: 0.00062247
Iteration 16/1000 | Loss: 0.00040613
Iteration 17/1000 | Loss: 0.00026598
Iteration 18/1000 | Loss: 0.00036007
Iteration 19/1000 | Loss: 0.00023777
Iteration 20/1000 | Loss: 0.00076442
Iteration 21/1000 | Loss: 0.00091066
Iteration 22/1000 | Loss: 0.00017399
Iteration 23/1000 | Loss: 0.00030827
Iteration 24/1000 | Loss: 0.00022576
Iteration 25/1000 | Loss: 0.00020960
Iteration 26/1000 | Loss: 0.00059961
Iteration 27/1000 | Loss: 0.00065966
Iteration 28/1000 | Loss: 0.00110806
Iteration 29/1000 | Loss: 0.00081832
Iteration 30/1000 | Loss: 0.00032224
Iteration 31/1000 | Loss: 0.00053390
Iteration 32/1000 | Loss: 0.00018066
Iteration 33/1000 | Loss: 0.00026415
Iteration 34/1000 | Loss: 0.00027816
Iteration 35/1000 | Loss: 0.00024237
Iteration 36/1000 | Loss: 0.00013991
Iteration 37/1000 | Loss: 0.00041413
Iteration 38/1000 | Loss: 0.00111934
Iteration 39/1000 | Loss: 0.00090829
Iteration 40/1000 | Loss: 0.00131050
Iteration 41/1000 | Loss: 0.00073684
Iteration 42/1000 | Loss: 0.00025485
Iteration 43/1000 | Loss: 0.00030898
Iteration 44/1000 | Loss: 0.00062404
Iteration 45/1000 | Loss: 0.00068622
Iteration 46/1000 | Loss: 0.00036638
Iteration 47/1000 | Loss: 0.00040770
Iteration 48/1000 | Loss: 0.00037093
Iteration 49/1000 | Loss: 0.00095219
Iteration 50/1000 | Loss: 0.00074301
Iteration 51/1000 | Loss: 0.00029424
Iteration 52/1000 | Loss: 0.00012272
Iteration 53/1000 | Loss: 0.00013836
Iteration 54/1000 | Loss: 0.00013380
Iteration 55/1000 | Loss: 0.00012857
Iteration 56/1000 | Loss: 0.00010534
Iteration 57/1000 | Loss: 0.00011445
Iteration 58/1000 | Loss: 0.00021027
Iteration 59/1000 | Loss: 0.00017995
Iteration 60/1000 | Loss: 0.00019904
Iteration 61/1000 | Loss: 0.00015044
Iteration 62/1000 | Loss: 0.00012017
Iteration 63/1000 | Loss: 0.00010151
Iteration 64/1000 | Loss: 0.00012584
Iteration 65/1000 | Loss: 0.00012535
Iteration 66/1000 | Loss: 0.00012970
Iteration 67/1000 | Loss: 0.00011513
Iteration 68/1000 | Loss: 0.00100722
Iteration 69/1000 | Loss: 0.00385568
Iteration 70/1000 | Loss: 0.00066705
Iteration 71/1000 | Loss: 0.00066638
Iteration 72/1000 | Loss: 0.00040015
Iteration 73/1000 | Loss: 0.00032866
Iteration 74/1000 | Loss: 0.00043773
Iteration 75/1000 | Loss: 0.00036273
Iteration 76/1000 | Loss: 0.00024605
Iteration 77/1000 | Loss: 0.00023342
Iteration 78/1000 | Loss: 0.00021317
Iteration 79/1000 | Loss: 0.00048735
Iteration 80/1000 | Loss: 0.00009856
Iteration 81/1000 | Loss: 0.00055623
Iteration 82/1000 | Loss: 0.00119316
Iteration 83/1000 | Loss: 0.00071219
Iteration 84/1000 | Loss: 0.00025034
Iteration 85/1000 | Loss: 0.00015358
Iteration 86/1000 | Loss: 0.00015224
Iteration 87/1000 | Loss: 0.00012597
Iteration 88/1000 | Loss: 0.00011883
Iteration 89/1000 | Loss: 0.00015066
Iteration 90/1000 | Loss: 0.00063825
Iteration 91/1000 | Loss: 0.00005152
Iteration 92/1000 | Loss: 0.00013267
Iteration 93/1000 | Loss: 0.00005636
Iteration 94/1000 | Loss: 0.00005037
Iteration 95/1000 | Loss: 0.00005097
Iteration 96/1000 | Loss: 0.00043339
Iteration 97/1000 | Loss: 0.00004972
Iteration 98/1000 | Loss: 0.00004807
Iteration 99/1000 | Loss: 0.00012772
Iteration 100/1000 | Loss: 0.00009964
Iteration 101/1000 | Loss: 0.00010792
Iteration 102/1000 | Loss: 0.00008049
Iteration 103/1000 | Loss: 0.00013204
Iteration 104/1000 | Loss: 0.00013785
Iteration 105/1000 | Loss: 0.00014014
Iteration 106/1000 | Loss: 0.00030598
Iteration 107/1000 | Loss: 0.00012435
Iteration 108/1000 | Loss: 0.00020164
Iteration 109/1000 | Loss: 0.00019827
Iteration 110/1000 | Loss: 0.00011117
Iteration 111/1000 | Loss: 0.00015429
Iteration 112/1000 | Loss: 0.00012292
Iteration 113/1000 | Loss: 0.00047015
Iteration 114/1000 | Loss: 0.00053228
Iteration 115/1000 | Loss: 0.00131287
Iteration 116/1000 | Loss: 0.00005210
Iteration 117/1000 | Loss: 0.00018242
Iteration 118/1000 | Loss: 0.00006246
Iteration 119/1000 | Loss: 0.00004772
Iteration 120/1000 | Loss: 0.00004358
Iteration 121/1000 | Loss: 0.00004500
Iteration 122/1000 | Loss: 0.00020553
Iteration 123/1000 | Loss: 0.00016341
Iteration 124/1000 | Loss: 0.00005088
Iteration 125/1000 | Loss: 0.00018403
Iteration 126/1000 | Loss: 0.00018719
Iteration 127/1000 | Loss: 0.00006332
Iteration 128/1000 | Loss: 0.00004818
Iteration 129/1000 | Loss: 0.00043666
Iteration 130/1000 | Loss: 0.00004364
Iteration 131/1000 | Loss: 0.00004517
Iteration 132/1000 | Loss: 0.00004210
Iteration 133/1000 | Loss: 0.00004267
Iteration 134/1000 | Loss: 0.00003602
Iteration 135/1000 | Loss: 0.00004341
Iteration 136/1000 | Loss: 0.00003656
Iteration 137/1000 | Loss: 0.00004866
Iteration 138/1000 | Loss: 0.00047222
Iteration 139/1000 | Loss: 0.00003323
Iteration 140/1000 | Loss: 0.00003027
Iteration 141/1000 | Loss: 0.00002878
Iteration 142/1000 | Loss: 0.00002768
Iteration 143/1000 | Loss: 0.00002705
Iteration 144/1000 | Loss: 0.00049315
Iteration 145/1000 | Loss: 0.00002728
Iteration 146/1000 | Loss: 0.00002592
Iteration 147/1000 | Loss: 0.00002525
Iteration 148/1000 | Loss: 0.00002445
Iteration 149/1000 | Loss: 0.00002388
Iteration 150/1000 | Loss: 0.00002348
Iteration 151/1000 | Loss: 0.00002338
Iteration 152/1000 | Loss: 0.00002329
Iteration 153/1000 | Loss: 0.00002328
Iteration 154/1000 | Loss: 0.00002328
Iteration 155/1000 | Loss: 0.00002325
Iteration 156/1000 | Loss: 0.00002324
Iteration 157/1000 | Loss: 0.00002323
Iteration 158/1000 | Loss: 0.00002322
Iteration 159/1000 | Loss: 0.00002319
Iteration 160/1000 | Loss: 0.00002317
Iteration 161/1000 | Loss: 0.00039032
Iteration 162/1000 | Loss: 0.00002617
Iteration 163/1000 | Loss: 0.00002335
Iteration 164/1000 | Loss: 0.00002269
Iteration 165/1000 | Loss: 0.00016903
Iteration 166/1000 | Loss: 0.00002952
Iteration 167/1000 | Loss: 0.00002456
Iteration 168/1000 | Loss: 0.00002316
Iteration 169/1000 | Loss: 0.00002159
Iteration 170/1000 | Loss: 0.00002079
Iteration 171/1000 | Loss: 0.00002018
Iteration 172/1000 | Loss: 0.00019244
Iteration 173/1000 | Loss: 0.00037819
Iteration 174/1000 | Loss: 0.00003514
Iteration 175/1000 | Loss: 0.00009299
Iteration 176/1000 | Loss: 0.00002768
Iteration 177/1000 | Loss: 0.00002273
Iteration 178/1000 | Loss: 0.00002107
Iteration 179/1000 | Loss: 0.00001952
Iteration 180/1000 | Loss: 0.00004725
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001776
Iteration 183/1000 | Loss: 0.00001739
Iteration 184/1000 | Loss: 0.00004992
Iteration 185/1000 | Loss: 0.00001703
Iteration 186/1000 | Loss: 0.00001690
Iteration 187/1000 | Loss: 0.00001688
Iteration 188/1000 | Loss: 0.00001687
Iteration 189/1000 | Loss: 0.00001686
Iteration 190/1000 | Loss: 0.00004391
Iteration 191/1000 | Loss: 0.00001819
Iteration 192/1000 | Loss: 0.00001920
Iteration 193/1000 | Loss: 0.00001671
Iteration 194/1000 | Loss: 0.00001671
Iteration 195/1000 | Loss: 0.00001671
Iteration 196/1000 | Loss: 0.00001671
Iteration 197/1000 | Loss: 0.00001671
Iteration 198/1000 | Loss: 0.00001671
Iteration 199/1000 | Loss: 0.00001671
Iteration 200/1000 | Loss: 0.00001671
Iteration 201/1000 | Loss: 0.00001670
Iteration 202/1000 | Loss: 0.00001670
Iteration 203/1000 | Loss: 0.00001670
Iteration 204/1000 | Loss: 0.00001670
Iteration 205/1000 | Loss: 0.00001670
Iteration 206/1000 | Loss: 0.00001669
Iteration 207/1000 | Loss: 0.00001669
Iteration 208/1000 | Loss: 0.00001666
Iteration 209/1000 | Loss: 0.00001665
Iteration 210/1000 | Loss: 0.00001664
Iteration 211/1000 | Loss: 0.00001663
Iteration 212/1000 | Loss: 0.00001662
Iteration 213/1000 | Loss: 0.00001659
Iteration 214/1000 | Loss: 0.00001659
Iteration 215/1000 | Loss: 0.00001659
Iteration 216/1000 | Loss: 0.00001659
Iteration 217/1000 | Loss: 0.00001659
Iteration 218/1000 | Loss: 0.00001659
Iteration 219/1000 | Loss: 0.00001659
Iteration 220/1000 | Loss: 0.00001659
Iteration 221/1000 | Loss: 0.00001659
Iteration 222/1000 | Loss: 0.00001659
Iteration 223/1000 | Loss: 0.00001658
Iteration 224/1000 | Loss: 0.00001658
Iteration 225/1000 | Loss: 0.00001658
Iteration 226/1000 | Loss: 0.00001658
Iteration 227/1000 | Loss: 0.00003774
Iteration 228/1000 | Loss: 0.00001669
Iteration 229/1000 | Loss: 0.00001656
Iteration 230/1000 | Loss: 0.00001656
Iteration 231/1000 | Loss: 0.00001656
Iteration 232/1000 | Loss: 0.00001656
Iteration 233/1000 | Loss: 0.00001656
Iteration 234/1000 | Loss: 0.00001656
Iteration 235/1000 | Loss: 0.00001656
Iteration 236/1000 | Loss: 0.00001655
Iteration 237/1000 | Loss: 0.00001655
Iteration 238/1000 | Loss: 0.00001655
Iteration 239/1000 | Loss: 0.00001655
Iteration 240/1000 | Loss: 0.00001655
Iteration 241/1000 | Loss: 0.00001654
Iteration 242/1000 | Loss: 0.00001654
Iteration 243/1000 | Loss: 0.00001654
Iteration 244/1000 | Loss: 0.00001654
Iteration 245/1000 | Loss: 0.00001654
Iteration 246/1000 | Loss: 0.00001654
Iteration 247/1000 | Loss: 0.00001654
Iteration 248/1000 | Loss: 0.00001654
Iteration 249/1000 | Loss: 0.00001654
Iteration 250/1000 | Loss: 0.00001654
Iteration 251/1000 | Loss: 0.00001654
Iteration 252/1000 | Loss: 0.00001654
Iteration 253/1000 | Loss: 0.00001654
Iteration 254/1000 | Loss: 0.00001654
Iteration 255/1000 | Loss: 0.00001654
Iteration 256/1000 | Loss: 0.00001654
Iteration 257/1000 | Loss: 0.00001654
Iteration 258/1000 | Loss: 0.00001654
Iteration 259/1000 | Loss: 0.00001654
Iteration 260/1000 | Loss: 0.00001654
Iteration 261/1000 | Loss: 0.00001654
Iteration 262/1000 | Loss: 0.00001654
Iteration 263/1000 | Loss: 0.00001654
Iteration 264/1000 | Loss: 0.00001654
Iteration 265/1000 | Loss: 0.00001654
Iteration 266/1000 | Loss: 0.00001654
Iteration 267/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.6539308489882387e-05, 1.6539308489882387e-05, 1.6539308489882387e-05, 1.6539308489882387e-05, 1.6539308489882387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6539308489882387e-05

Optimization complete. Final v2v error: 3.295193910598755 mm

Highest mean error: 10.530550956726074 mm for frame 17

Lowest mean error: 2.7835514545440674 mm for frame 30

Saving results

Total time: 344.01831221580505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798986
Iteration 2/25 | Loss: 0.00153983
Iteration 3/25 | Loss: 0.00133747
Iteration 4/25 | Loss: 0.00131842
Iteration 5/25 | Loss: 0.00131488
Iteration 6/25 | Loss: 0.00131390
Iteration 7/25 | Loss: 0.00131352
Iteration 8/25 | Loss: 0.00131320
Iteration 9/25 | Loss: 0.00131492
Iteration 10/25 | Loss: 0.00131294
Iteration 11/25 | Loss: 0.00131348
Iteration 12/25 | Loss: 0.00131092
Iteration 13/25 | Loss: 0.00130891
Iteration 14/25 | Loss: 0.00130809
Iteration 15/25 | Loss: 0.00130729
Iteration 16/25 | Loss: 0.00130598
Iteration 17/25 | Loss: 0.00130557
Iteration 18/25 | Loss: 0.00130538
Iteration 19/25 | Loss: 0.00130529
Iteration 20/25 | Loss: 0.00130529
Iteration 21/25 | Loss: 0.00130528
Iteration 22/25 | Loss: 0.00130528
Iteration 23/25 | Loss: 0.00130528
Iteration 24/25 | Loss: 0.00130528
Iteration 25/25 | Loss: 0.00130528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39974439
Iteration 2/25 | Loss: 0.00086125
Iteration 3/25 | Loss: 0.00086125
Iteration 4/25 | Loss: 0.00086125
Iteration 5/25 | Loss: 0.00086125
Iteration 6/25 | Loss: 0.00086125
Iteration 7/25 | Loss: 0.00086125
Iteration 8/25 | Loss: 0.00086125
Iteration 9/25 | Loss: 0.00086125
Iteration 10/25 | Loss: 0.00086125
Iteration 11/25 | Loss: 0.00086125
Iteration 12/25 | Loss: 0.00086125
Iteration 13/25 | Loss: 0.00086125
Iteration 14/25 | Loss: 0.00086125
Iteration 15/25 | Loss: 0.00086125
Iteration 16/25 | Loss: 0.00086125
Iteration 17/25 | Loss: 0.00086125
Iteration 18/25 | Loss: 0.00086125
Iteration 19/25 | Loss: 0.00086125
Iteration 20/25 | Loss: 0.00086125
Iteration 21/25 | Loss: 0.00086125
Iteration 22/25 | Loss: 0.00086125
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008612484089098871, 0.0008612484089098871, 0.0008612484089098871, 0.0008612484089098871, 0.0008612484089098871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008612484089098871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086125
Iteration 2/1000 | Loss: 0.00003025
Iteration 3/1000 | Loss: 0.00002017
Iteration 4/1000 | Loss: 0.00001716
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001498
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001440
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001382
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001379
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001361
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001359
Iteration 38/1000 | Loss: 0.00001359
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.3587510693469085e-05, 1.3587510693469085e-05, 1.3587510693469085e-05, 1.3587510693469085e-05, 1.3587510693469085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3587510693469085e-05

Optimization complete. Final v2v error: 3.1866986751556396 mm

Highest mean error: 3.48351788520813 mm for frame 124

Lowest mean error: 3.0022480487823486 mm for frame 19

Saving results

Total time: 59.083139181137085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754753
Iteration 2/25 | Loss: 0.00160456
Iteration 3/25 | Loss: 0.00146921
Iteration 4/25 | Loss: 0.00142814
Iteration 5/25 | Loss: 0.00142296
Iteration 6/25 | Loss: 0.00142467
Iteration 7/25 | Loss: 0.00141375
Iteration 8/25 | Loss: 0.00141254
Iteration 9/25 | Loss: 0.00141233
Iteration 10/25 | Loss: 0.00141226
Iteration 11/25 | Loss: 0.00141225
Iteration 12/25 | Loss: 0.00141225
Iteration 13/25 | Loss: 0.00141225
Iteration 14/25 | Loss: 0.00141225
Iteration 15/25 | Loss: 0.00141225
Iteration 16/25 | Loss: 0.00141225
Iteration 17/25 | Loss: 0.00141225
Iteration 18/25 | Loss: 0.00141225
Iteration 19/25 | Loss: 0.00141225
Iteration 20/25 | Loss: 0.00141225
Iteration 21/25 | Loss: 0.00141225
Iteration 22/25 | Loss: 0.00141225
Iteration 23/25 | Loss: 0.00141224
Iteration 24/25 | Loss: 0.00141224
Iteration 25/25 | Loss: 0.00141224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87371540
Iteration 2/25 | Loss: 0.00101453
Iteration 3/25 | Loss: 0.00101453
Iteration 4/25 | Loss: 0.00101453
Iteration 5/25 | Loss: 0.00101453
Iteration 6/25 | Loss: 0.00101453
Iteration 7/25 | Loss: 0.00101453
Iteration 8/25 | Loss: 0.00101453
Iteration 9/25 | Loss: 0.00101453
Iteration 10/25 | Loss: 0.00101453
Iteration 11/25 | Loss: 0.00101453
Iteration 12/25 | Loss: 0.00101453
Iteration 13/25 | Loss: 0.00101453
Iteration 14/25 | Loss: 0.00101453
Iteration 15/25 | Loss: 0.00101453
Iteration 16/25 | Loss: 0.00101453
Iteration 17/25 | Loss: 0.00101453
Iteration 18/25 | Loss: 0.00101453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010145278647542, 0.0010145278647542, 0.0010145278647542, 0.0010145278647542, 0.0010145278647542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010145278647542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101453
Iteration 2/1000 | Loss: 0.00007583
Iteration 3/1000 | Loss: 0.00005508
Iteration 4/1000 | Loss: 0.00004772
Iteration 5/1000 | Loss: 0.00004476
Iteration 6/1000 | Loss: 0.00004242
Iteration 7/1000 | Loss: 0.00004062
Iteration 8/1000 | Loss: 0.00003921
Iteration 9/1000 | Loss: 0.00003833
Iteration 10/1000 | Loss: 0.00003752
Iteration 11/1000 | Loss: 0.00003684
Iteration 12/1000 | Loss: 0.00003637
Iteration 13/1000 | Loss: 0.00003597
Iteration 14/1000 | Loss: 0.00003564
Iteration 15/1000 | Loss: 0.00003535
Iteration 16/1000 | Loss: 0.00003512
Iteration 17/1000 | Loss: 0.00003504
Iteration 18/1000 | Loss: 0.00003498
Iteration 19/1000 | Loss: 0.00003492
Iteration 20/1000 | Loss: 0.00003476
Iteration 21/1000 | Loss: 0.00003473
Iteration 22/1000 | Loss: 0.00003468
Iteration 23/1000 | Loss: 0.00003463
Iteration 24/1000 | Loss: 0.00003459
Iteration 25/1000 | Loss: 0.00003456
Iteration 26/1000 | Loss: 0.00003455
Iteration 27/1000 | Loss: 0.00003451
Iteration 28/1000 | Loss: 0.00003451
Iteration 29/1000 | Loss: 0.00003449
Iteration 30/1000 | Loss: 0.00003446
Iteration 31/1000 | Loss: 0.00003445
Iteration 32/1000 | Loss: 0.00003444
Iteration 33/1000 | Loss: 0.00003444
Iteration 34/1000 | Loss: 0.00003443
Iteration 35/1000 | Loss: 0.00003443
Iteration 36/1000 | Loss: 0.00003443
Iteration 37/1000 | Loss: 0.00003442
Iteration 38/1000 | Loss: 0.00003441
Iteration 39/1000 | Loss: 0.00003441
Iteration 40/1000 | Loss: 0.00003441
Iteration 41/1000 | Loss: 0.00003440
Iteration 42/1000 | Loss: 0.00003440
Iteration 43/1000 | Loss: 0.00003440
Iteration 44/1000 | Loss: 0.00003439
Iteration 45/1000 | Loss: 0.00003439
Iteration 46/1000 | Loss: 0.00003439
Iteration 47/1000 | Loss: 0.00003439
Iteration 48/1000 | Loss: 0.00003438
Iteration 49/1000 | Loss: 0.00003438
Iteration 50/1000 | Loss: 0.00003438
Iteration 51/1000 | Loss: 0.00003438
Iteration 52/1000 | Loss: 0.00003437
Iteration 53/1000 | Loss: 0.00003437
Iteration 54/1000 | Loss: 0.00003437
Iteration 55/1000 | Loss: 0.00003437
Iteration 56/1000 | Loss: 0.00003436
Iteration 57/1000 | Loss: 0.00003436
Iteration 58/1000 | Loss: 0.00003436
Iteration 59/1000 | Loss: 0.00003436
Iteration 60/1000 | Loss: 0.00003436
Iteration 61/1000 | Loss: 0.00003436
Iteration 62/1000 | Loss: 0.00003435
Iteration 63/1000 | Loss: 0.00003435
Iteration 64/1000 | Loss: 0.00003435
Iteration 65/1000 | Loss: 0.00003435
Iteration 66/1000 | Loss: 0.00003435
Iteration 67/1000 | Loss: 0.00003434
Iteration 68/1000 | Loss: 0.00003434
Iteration 69/1000 | Loss: 0.00003434
Iteration 70/1000 | Loss: 0.00003434
Iteration 71/1000 | Loss: 0.00003433
Iteration 72/1000 | Loss: 0.00003433
Iteration 73/1000 | Loss: 0.00003433
Iteration 74/1000 | Loss: 0.00003433
Iteration 75/1000 | Loss: 0.00003433
Iteration 76/1000 | Loss: 0.00003432
Iteration 77/1000 | Loss: 0.00003432
Iteration 78/1000 | Loss: 0.00003432
Iteration 79/1000 | Loss: 0.00003432
Iteration 80/1000 | Loss: 0.00003431
Iteration 81/1000 | Loss: 0.00003431
Iteration 82/1000 | Loss: 0.00003431
Iteration 83/1000 | Loss: 0.00003431
Iteration 84/1000 | Loss: 0.00003431
Iteration 85/1000 | Loss: 0.00003431
Iteration 86/1000 | Loss: 0.00003431
Iteration 87/1000 | Loss: 0.00003430
Iteration 88/1000 | Loss: 0.00003430
Iteration 89/1000 | Loss: 0.00003430
Iteration 90/1000 | Loss: 0.00003430
Iteration 91/1000 | Loss: 0.00003430
Iteration 92/1000 | Loss: 0.00003430
Iteration 93/1000 | Loss: 0.00003430
Iteration 94/1000 | Loss: 0.00003429
Iteration 95/1000 | Loss: 0.00003429
Iteration 96/1000 | Loss: 0.00003429
Iteration 97/1000 | Loss: 0.00003429
Iteration 98/1000 | Loss: 0.00003429
Iteration 99/1000 | Loss: 0.00003429
Iteration 100/1000 | Loss: 0.00003428
Iteration 101/1000 | Loss: 0.00003428
Iteration 102/1000 | Loss: 0.00003428
Iteration 103/1000 | Loss: 0.00003428
Iteration 104/1000 | Loss: 0.00003428
Iteration 105/1000 | Loss: 0.00003427
Iteration 106/1000 | Loss: 0.00003427
Iteration 107/1000 | Loss: 0.00003427
Iteration 108/1000 | Loss: 0.00003427
Iteration 109/1000 | Loss: 0.00003427
Iteration 110/1000 | Loss: 0.00003427
Iteration 111/1000 | Loss: 0.00003427
Iteration 112/1000 | Loss: 0.00003426
Iteration 113/1000 | Loss: 0.00003426
Iteration 114/1000 | Loss: 0.00003426
Iteration 115/1000 | Loss: 0.00003425
Iteration 116/1000 | Loss: 0.00003425
Iteration 117/1000 | Loss: 0.00003425
Iteration 118/1000 | Loss: 0.00003425
Iteration 119/1000 | Loss: 0.00003425
Iteration 120/1000 | Loss: 0.00003425
Iteration 121/1000 | Loss: 0.00003425
Iteration 122/1000 | Loss: 0.00003425
Iteration 123/1000 | Loss: 0.00003424
Iteration 124/1000 | Loss: 0.00003424
Iteration 125/1000 | Loss: 0.00003424
Iteration 126/1000 | Loss: 0.00003424
Iteration 127/1000 | Loss: 0.00003424
Iteration 128/1000 | Loss: 0.00003424
Iteration 129/1000 | Loss: 0.00003424
Iteration 130/1000 | Loss: 0.00003424
Iteration 131/1000 | Loss: 0.00003424
Iteration 132/1000 | Loss: 0.00003424
Iteration 133/1000 | Loss: 0.00003424
Iteration 134/1000 | Loss: 0.00003424
Iteration 135/1000 | Loss: 0.00003423
Iteration 136/1000 | Loss: 0.00003423
Iteration 137/1000 | Loss: 0.00003423
Iteration 138/1000 | Loss: 0.00003423
Iteration 139/1000 | Loss: 0.00003422
Iteration 140/1000 | Loss: 0.00003422
Iteration 141/1000 | Loss: 0.00003422
Iteration 142/1000 | Loss: 0.00003422
Iteration 143/1000 | Loss: 0.00003422
Iteration 144/1000 | Loss: 0.00003422
Iteration 145/1000 | Loss: 0.00003422
Iteration 146/1000 | Loss: 0.00003422
Iteration 147/1000 | Loss: 0.00003422
Iteration 148/1000 | Loss: 0.00003422
Iteration 149/1000 | Loss: 0.00003422
Iteration 150/1000 | Loss: 0.00003422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.42185448971577e-05, 3.42185448971577e-05, 3.42185448971577e-05, 3.42185448971577e-05, 3.42185448971577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.42185448971577e-05

Optimization complete. Final v2v error: 4.804881572723389 mm

Highest mean error: 12.160346031188965 mm for frame 9

Lowest mean error: 3.6083903312683105 mm for frame 231

Saving results

Total time: 62.08072018623352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963973
Iteration 2/25 | Loss: 0.00332501
Iteration 3/25 | Loss: 0.00226236
Iteration 4/25 | Loss: 0.00205540
Iteration 5/25 | Loss: 0.00200304
Iteration 6/25 | Loss: 0.00204029
Iteration 7/25 | Loss: 0.00202274
Iteration 8/25 | Loss: 0.00189080
Iteration 9/25 | Loss: 0.00180184
Iteration 10/25 | Loss: 0.00178203
Iteration 11/25 | Loss: 0.00174187
Iteration 12/25 | Loss: 0.00170316
Iteration 13/25 | Loss: 0.00169391
Iteration 14/25 | Loss: 0.00169691
Iteration 15/25 | Loss: 0.00167390
Iteration 16/25 | Loss: 0.00166363
Iteration 17/25 | Loss: 0.00166153
Iteration 18/25 | Loss: 0.00165066
Iteration 19/25 | Loss: 0.00164444
Iteration 20/25 | Loss: 0.00164283
Iteration 21/25 | Loss: 0.00163403
Iteration 22/25 | Loss: 0.00163241
Iteration 23/25 | Loss: 0.00164347
Iteration 24/25 | Loss: 0.00162759
Iteration 25/25 | Loss: 0.00162786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47036552
Iteration 2/25 | Loss: 0.00446180
Iteration 3/25 | Loss: 0.00244897
Iteration 4/25 | Loss: 0.00244896
Iteration 5/25 | Loss: 0.00244896
Iteration 6/25 | Loss: 0.00244896
Iteration 7/25 | Loss: 0.00244896
Iteration 8/25 | Loss: 0.00244896
Iteration 9/25 | Loss: 0.00244896
Iteration 10/25 | Loss: 0.00244896
Iteration 11/25 | Loss: 0.00244896
Iteration 12/25 | Loss: 0.00244896
Iteration 13/25 | Loss: 0.00244896
Iteration 14/25 | Loss: 0.00244896
Iteration 15/25 | Loss: 0.00244896
Iteration 16/25 | Loss: 0.00244896
Iteration 17/25 | Loss: 0.00244896
Iteration 18/25 | Loss: 0.00244896
Iteration 19/25 | Loss: 0.00244896
Iteration 20/25 | Loss: 0.00244896
Iteration 21/25 | Loss: 0.00244896
Iteration 22/25 | Loss: 0.00244896
Iteration 23/25 | Loss: 0.00244896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0024489581119269133, 0.0024489581119269133, 0.0024489581119269133, 0.0024489581119269133, 0.0024489581119269133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024489581119269133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244896
Iteration 2/1000 | Loss: 0.00466247
Iteration 3/1000 | Loss: 0.00331860
Iteration 4/1000 | Loss: 0.00245399
Iteration 5/1000 | Loss: 0.00068530
Iteration 6/1000 | Loss: 0.00098270
Iteration 7/1000 | Loss: 0.00096665
Iteration 8/1000 | Loss: 0.00125144
Iteration 9/1000 | Loss: 0.00075477
Iteration 10/1000 | Loss: 0.00097486
Iteration 11/1000 | Loss: 0.00057442
Iteration 12/1000 | Loss: 0.00076011
Iteration 13/1000 | Loss: 0.00079702
Iteration 14/1000 | Loss: 0.00088454
Iteration 15/1000 | Loss: 0.00050503
Iteration 16/1000 | Loss: 0.00040128
Iteration 17/1000 | Loss: 0.00046309
Iteration 18/1000 | Loss: 0.00045437
Iteration 19/1000 | Loss: 0.00051023
Iteration 20/1000 | Loss: 0.00040222
Iteration 21/1000 | Loss: 0.00029492
Iteration 22/1000 | Loss: 0.00026919
Iteration 23/1000 | Loss: 0.00048919
Iteration 24/1000 | Loss: 0.00035451
Iteration 25/1000 | Loss: 0.00033003
Iteration 26/1000 | Loss: 0.00037252
Iteration 27/1000 | Loss: 0.00036279
Iteration 28/1000 | Loss: 0.00028967
Iteration 29/1000 | Loss: 0.00033442
Iteration 30/1000 | Loss: 0.00029162
Iteration 31/1000 | Loss: 0.00039264
Iteration 32/1000 | Loss: 0.00034127
Iteration 33/1000 | Loss: 0.00022254
Iteration 34/1000 | Loss: 0.00027431
Iteration 35/1000 | Loss: 0.00021470
Iteration 36/1000 | Loss: 0.00024631
Iteration 37/1000 | Loss: 0.00023946
Iteration 38/1000 | Loss: 0.00028175
Iteration 39/1000 | Loss: 0.00063113
Iteration 40/1000 | Loss: 0.00035743
Iteration 41/1000 | Loss: 0.00058316
Iteration 42/1000 | Loss: 0.00091697
Iteration 43/1000 | Loss: 0.00071680
Iteration 44/1000 | Loss: 0.00019723
Iteration 45/1000 | Loss: 0.00059160
Iteration 46/1000 | Loss: 0.00051182
Iteration 47/1000 | Loss: 0.00044795
Iteration 48/1000 | Loss: 0.00035810
Iteration 49/1000 | Loss: 0.00027523
Iteration 50/1000 | Loss: 0.00096496
Iteration 51/1000 | Loss: 0.00028329
Iteration 52/1000 | Loss: 0.00063608
Iteration 53/1000 | Loss: 0.00070060
Iteration 54/1000 | Loss: 0.00044719
Iteration 55/1000 | Loss: 0.00037740
Iteration 56/1000 | Loss: 0.00021273
Iteration 57/1000 | Loss: 0.00021181
Iteration 58/1000 | Loss: 0.00026055
Iteration 59/1000 | Loss: 0.00022398
Iteration 60/1000 | Loss: 0.00054095
Iteration 61/1000 | Loss: 0.00029247
Iteration 62/1000 | Loss: 0.00043801
Iteration 63/1000 | Loss: 0.00045516
Iteration 64/1000 | Loss: 0.00027292
Iteration 65/1000 | Loss: 0.00024037
Iteration 66/1000 | Loss: 0.00032180
Iteration 67/1000 | Loss: 0.00022247
Iteration 68/1000 | Loss: 0.00055277
Iteration 69/1000 | Loss: 0.00102904
Iteration 70/1000 | Loss: 0.00049514
Iteration 71/1000 | Loss: 0.00027791
Iteration 72/1000 | Loss: 0.00038050
Iteration 73/1000 | Loss: 0.00047365
Iteration 74/1000 | Loss: 0.00039575
Iteration 75/1000 | Loss: 0.00043896
Iteration 76/1000 | Loss: 0.00027246
Iteration 77/1000 | Loss: 0.00018414
Iteration 78/1000 | Loss: 0.00039846
Iteration 79/1000 | Loss: 0.00040488
Iteration 80/1000 | Loss: 0.00052083
Iteration 81/1000 | Loss: 0.00051595
Iteration 82/1000 | Loss: 0.00028687
Iteration 83/1000 | Loss: 0.00024268
Iteration 84/1000 | Loss: 0.00029775
Iteration 85/1000 | Loss: 0.00025424
Iteration 86/1000 | Loss: 0.00031801
Iteration 87/1000 | Loss: 0.00024501
Iteration 88/1000 | Loss: 0.00019149
Iteration 89/1000 | Loss: 0.00020612
Iteration 90/1000 | Loss: 0.00023329
Iteration 91/1000 | Loss: 0.00023705
Iteration 92/1000 | Loss: 0.00023462
Iteration 93/1000 | Loss: 0.00023884
Iteration 94/1000 | Loss: 0.00027702
Iteration 95/1000 | Loss: 0.00029697
Iteration 96/1000 | Loss: 0.00017044
Iteration 97/1000 | Loss: 0.00024981
Iteration 98/1000 | Loss: 0.00035324
Iteration 99/1000 | Loss: 0.00029120
Iteration 100/1000 | Loss: 0.00019823
Iteration 101/1000 | Loss: 0.00032308
Iteration 102/1000 | Loss: 0.00019987
Iteration 103/1000 | Loss: 0.00018419
Iteration 104/1000 | Loss: 0.00018029
Iteration 105/1000 | Loss: 0.00019623
Iteration 106/1000 | Loss: 0.00030584
Iteration 107/1000 | Loss: 0.00024452
Iteration 108/1000 | Loss: 0.00031554
Iteration 109/1000 | Loss: 0.00026794
Iteration 110/1000 | Loss: 0.00024917
Iteration 111/1000 | Loss: 0.00016836
Iteration 112/1000 | Loss: 0.00035312
Iteration 113/1000 | Loss: 0.00030278
Iteration 114/1000 | Loss: 0.00018951
Iteration 115/1000 | Loss: 0.00039844
Iteration 116/1000 | Loss: 0.00032612
Iteration 117/1000 | Loss: 0.00030840
Iteration 118/1000 | Loss: 0.00028366
Iteration 119/1000 | Loss: 0.00024737
Iteration 120/1000 | Loss: 0.00023408
Iteration 121/1000 | Loss: 0.00019514
Iteration 122/1000 | Loss: 0.00026129
Iteration 123/1000 | Loss: 0.00025848
Iteration 124/1000 | Loss: 0.00026494
Iteration 125/1000 | Loss: 0.00025734
Iteration 126/1000 | Loss: 0.00025939
Iteration 127/1000 | Loss: 0.00021938
Iteration 128/1000 | Loss: 0.00023335
Iteration 129/1000 | Loss: 0.00019413
Iteration 130/1000 | Loss: 0.00019531
Iteration 131/1000 | Loss: 0.00023225
Iteration 132/1000 | Loss: 0.00026836
Iteration 133/1000 | Loss: 0.00022159
Iteration 134/1000 | Loss: 0.00019976
Iteration 135/1000 | Loss: 0.00019886
Iteration 136/1000 | Loss: 0.00018669
Iteration 137/1000 | Loss: 0.00018902
Iteration 138/1000 | Loss: 0.00016854
Iteration 139/1000 | Loss: 0.00018552
Iteration 140/1000 | Loss: 0.00037165
Iteration 141/1000 | Loss: 0.00027571
Iteration 142/1000 | Loss: 0.00032220
Iteration 143/1000 | Loss: 0.00032620
Iteration 144/1000 | Loss: 0.00030959
Iteration 145/1000 | Loss: 0.00034633
Iteration 146/1000 | Loss: 0.00030325
Iteration 147/1000 | Loss: 0.00026838
Iteration 148/1000 | Loss: 0.00023951
Iteration 149/1000 | Loss: 0.00036110
Iteration 150/1000 | Loss: 0.00027127
Iteration 151/1000 | Loss: 0.00021307
Iteration 152/1000 | Loss: 0.00035711
Iteration 153/1000 | Loss: 0.00025992
Iteration 154/1000 | Loss: 0.00016019
Iteration 155/1000 | Loss: 0.00017348
Iteration 156/1000 | Loss: 0.00016354
Iteration 157/1000 | Loss: 0.00018404
Iteration 158/1000 | Loss: 0.00018008
Iteration 159/1000 | Loss: 0.00013483
Iteration 160/1000 | Loss: 0.00014753
Iteration 161/1000 | Loss: 0.00015957
Iteration 162/1000 | Loss: 0.00015977
Iteration 163/1000 | Loss: 0.00015908
Iteration 164/1000 | Loss: 0.00016164
Iteration 165/1000 | Loss: 0.00033185
Iteration 166/1000 | Loss: 0.00024913
Iteration 167/1000 | Loss: 0.00033547
Iteration 168/1000 | Loss: 0.00026619
Iteration 169/1000 | Loss: 0.00037509
Iteration 170/1000 | Loss: 0.00052360
Iteration 171/1000 | Loss: 0.00031871
Iteration 172/1000 | Loss: 0.00070975
Iteration 173/1000 | Loss: 0.00020448
Iteration 174/1000 | Loss: 0.00026782
Iteration 175/1000 | Loss: 0.00020162
Iteration 176/1000 | Loss: 0.00019214
Iteration 177/1000 | Loss: 0.00020300
Iteration 178/1000 | Loss: 0.00036892
Iteration 179/1000 | Loss: 0.00050404
Iteration 180/1000 | Loss: 0.00024067
Iteration 181/1000 | Loss: 0.00030521
Iteration 182/1000 | Loss: 0.00029460
Iteration 183/1000 | Loss: 0.00033754
Iteration 184/1000 | Loss: 0.00024473
Iteration 185/1000 | Loss: 0.00027032
Iteration 186/1000 | Loss: 0.00024668
Iteration 187/1000 | Loss: 0.00061804
Iteration 188/1000 | Loss: 0.00047072
Iteration 189/1000 | Loss: 0.00056102
Iteration 190/1000 | Loss: 0.00056459
Iteration 191/1000 | Loss: 0.00057610
Iteration 192/1000 | Loss: 0.00054371
Iteration 193/1000 | Loss: 0.00058566
Iteration 194/1000 | Loss: 0.00039854
Iteration 195/1000 | Loss: 0.00053103
Iteration 196/1000 | Loss: 0.00053306
Iteration 197/1000 | Loss: 0.00043166
Iteration 198/1000 | Loss: 0.00029919
Iteration 199/1000 | Loss: 0.00031263
Iteration 200/1000 | Loss: 0.00035993
Iteration 201/1000 | Loss: 0.00029562
Iteration 202/1000 | Loss: 0.00035703
Iteration 203/1000 | Loss: 0.00043089
Iteration 204/1000 | Loss: 0.00024499
Iteration 205/1000 | Loss: 0.00035972
Iteration 206/1000 | Loss: 0.00039135
Iteration 207/1000 | Loss: 0.00024108
Iteration 208/1000 | Loss: 0.00028430
Iteration 209/1000 | Loss: 0.00017751
Iteration 210/1000 | Loss: 0.00020158
Iteration 211/1000 | Loss: 0.00025623
Iteration 212/1000 | Loss: 0.00028018
Iteration 213/1000 | Loss: 0.00026669
Iteration 214/1000 | Loss: 0.00025234
Iteration 215/1000 | Loss: 0.00026581
Iteration 216/1000 | Loss: 0.00018223
Iteration 217/1000 | Loss: 0.00033373
Iteration 218/1000 | Loss: 0.00029621
Iteration 219/1000 | Loss: 0.00017622
Iteration 220/1000 | Loss: 0.00035574
Iteration 221/1000 | Loss: 0.00036576
Iteration 222/1000 | Loss: 0.00017122
Iteration 223/1000 | Loss: 0.00018774
Iteration 224/1000 | Loss: 0.00028598
Iteration 225/1000 | Loss: 0.00019191
Iteration 226/1000 | Loss: 0.00036979
Iteration 227/1000 | Loss: 0.00035114
Iteration 228/1000 | Loss: 0.00030234
Iteration 229/1000 | Loss: 0.00028364
Iteration 230/1000 | Loss: 0.00029493
Iteration 231/1000 | Loss: 0.00025510
Iteration 232/1000 | Loss: 0.00032914
Iteration 233/1000 | Loss: 0.00031561
Iteration 234/1000 | Loss: 0.00020050
Iteration 235/1000 | Loss: 0.00023370
Iteration 236/1000 | Loss: 0.00037123
Iteration 237/1000 | Loss: 0.00022901
Iteration 238/1000 | Loss: 0.00017169
Iteration 239/1000 | Loss: 0.00062474
Iteration 240/1000 | Loss: 0.00018602
Iteration 241/1000 | Loss: 0.00023541
Iteration 242/1000 | Loss: 0.00025902
Iteration 243/1000 | Loss: 0.00020576
Iteration 244/1000 | Loss: 0.00026877
Iteration 245/1000 | Loss: 0.00015913
Iteration 246/1000 | Loss: 0.00017748
Iteration 247/1000 | Loss: 0.00013520
Iteration 248/1000 | Loss: 0.00017745
Iteration 249/1000 | Loss: 0.00015469
Iteration 250/1000 | Loss: 0.00015908
Iteration 251/1000 | Loss: 0.00016162
Iteration 252/1000 | Loss: 0.00015796
Iteration 253/1000 | Loss: 0.00031225
Iteration 254/1000 | Loss: 0.00023933
Iteration 255/1000 | Loss: 0.00029902
Iteration 256/1000 | Loss: 0.00021484
Iteration 257/1000 | Loss: 0.00017690
Iteration 258/1000 | Loss: 0.00016975
Iteration 259/1000 | Loss: 0.00015970
Iteration 260/1000 | Loss: 0.00027231
Iteration 261/1000 | Loss: 0.00021163
Iteration 262/1000 | Loss: 0.00021015
Iteration 263/1000 | Loss: 0.00020647
Iteration 264/1000 | Loss: 0.00021454
Iteration 265/1000 | Loss: 0.00019331
Iteration 266/1000 | Loss: 0.00018546
Iteration 267/1000 | Loss: 0.00024131
Iteration 268/1000 | Loss: 0.00022622
Iteration 269/1000 | Loss: 0.00022002
Iteration 270/1000 | Loss: 0.00020526
Iteration 271/1000 | Loss: 0.00020931
Iteration 272/1000 | Loss: 0.00023705
Iteration 273/1000 | Loss: 0.00020156
Iteration 274/1000 | Loss: 0.00035822
Iteration 275/1000 | Loss: 0.00022582
Iteration 276/1000 | Loss: 0.00030379
Iteration 277/1000 | Loss: 0.00082668
Iteration 278/1000 | Loss: 0.00049929
Iteration 279/1000 | Loss: 0.00020674
Iteration 280/1000 | Loss: 0.00053607
Iteration 281/1000 | Loss: 0.00051087
Iteration 282/1000 | Loss: 0.00019186
Iteration 283/1000 | Loss: 0.00016285
Iteration 284/1000 | Loss: 0.00016059
Iteration 285/1000 | Loss: 0.00014341
Iteration 286/1000 | Loss: 0.00016008
Iteration 287/1000 | Loss: 0.00016147
Iteration 288/1000 | Loss: 0.00022352
Iteration 289/1000 | Loss: 0.00014148
Iteration 290/1000 | Loss: 0.00025150
Iteration 291/1000 | Loss: 0.00019304
Iteration 292/1000 | Loss: 0.00023783
Iteration 293/1000 | Loss: 0.00021809
Iteration 294/1000 | Loss: 0.00015514
Iteration 295/1000 | Loss: 0.00017963
Iteration 296/1000 | Loss: 0.00015574
Iteration 297/1000 | Loss: 0.00015768
Iteration 298/1000 | Loss: 0.00017997
Iteration 299/1000 | Loss: 0.00016189
Iteration 300/1000 | Loss: 0.00017505
Iteration 301/1000 | Loss: 0.00018682
Iteration 302/1000 | Loss: 0.00017741
Iteration 303/1000 | Loss: 0.00015488
Iteration 304/1000 | Loss: 0.00017055
Iteration 305/1000 | Loss: 0.00024941
Iteration 306/1000 | Loss: 0.00016124
Iteration 307/1000 | Loss: 0.00038265
Iteration 308/1000 | Loss: 0.00018415
Iteration 309/1000 | Loss: 0.00036424
Iteration 310/1000 | Loss: 0.00079946
Iteration 311/1000 | Loss: 0.00037797
Iteration 312/1000 | Loss: 0.00036025
Iteration 313/1000 | Loss: 0.00052311
Iteration 314/1000 | Loss: 0.00019301
Iteration 315/1000 | Loss: 0.00023048
Iteration 316/1000 | Loss: 0.00025911
Iteration 317/1000 | Loss: 0.00022492
Iteration 318/1000 | Loss: 0.00014009
Iteration 319/1000 | Loss: 0.00014743
Iteration 320/1000 | Loss: 0.00016790
Iteration 321/1000 | Loss: 0.00016121
Iteration 322/1000 | Loss: 0.00016101
Iteration 323/1000 | Loss: 0.00016494
Iteration 324/1000 | Loss: 0.00016815
Iteration 325/1000 | Loss: 0.00025688
Iteration 326/1000 | Loss: 0.00018234
Iteration 327/1000 | Loss: 0.00024225
Iteration 328/1000 | Loss: 0.00016437
Iteration 329/1000 | Loss: 0.00015356
Iteration 330/1000 | Loss: 0.00016536
Iteration 331/1000 | Loss: 0.00015461
Iteration 332/1000 | Loss: 0.00014687
Iteration 333/1000 | Loss: 0.00015304
Iteration 334/1000 | Loss: 0.00025908
Iteration 335/1000 | Loss: 0.00023982
Iteration 336/1000 | Loss: 0.00028316
Iteration 337/1000 | Loss: 0.00022948
Iteration 338/1000 | Loss: 0.00036794
Iteration 339/1000 | Loss: 0.00023292
Iteration 340/1000 | Loss: 0.00027540
Iteration 341/1000 | Loss: 0.00088848
Iteration 342/1000 | Loss: 0.00035472
Iteration 343/1000 | Loss: 0.00039270
Iteration 344/1000 | Loss: 0.00042499
Iteration 345/1000 | Loss: 0.00036579
Iteration 346/1000 | Loss: 0.00034707
Iteration 347/1000 | Loss: 0.00032822
Iteration 348/1000 | Loss: 0.00028104
Iteration 349/1000 | Loss: 0.00015094
Iteration 350/1000 | Loss: 0.00015245
Iteration 351/1000 | Loss: 0.00015801
Iteration 352/1000 | Loss: 0.00014346
Iteration 353/1000 | Loss: 0.00016696
Iteration 354/1000 | Loss: 0.00016588
Iteration 355/1000 | Loss: 0.00015387
Iteration 356/1000 | Loss: 0.00015913
Iteration 357/1000 | Loss: 0.00014800
Iteration 358/1000 | Loss: 0.00030042
Iteration 359/1000 | Loss: 0.00032604
Iteration 360/1000 | Loss: 0.00016981
Iteration 361/1000 | Loss: 0.00017482
Iteration 362/1000 | Loss: 0.00016897
Iteration 363/1000 | Loss: 0.00015952
Iteration 364/1000 | Loss: 0.00028551
Iteration 365/1000 | Loss: 0.00020162
Iteration 366/1000 | Loss: 0.00015272
Iteration 367/1000 | Loss: 0.00016977
Iteration 368/1000 | Loss: 0.00015765
Iteration 369/1000 | Loss: 0.00035526
Iteration 370/1000 | Loss: 0.00020550
Iteration 371/1000 | Loss: 0.00016586
Iteration 372/1000 | Loss: 0.00014420
Iteration 373/1000 | Loss: 0.00035512
Iteration 374/1000 | Loss: 0.00015766
Iteration 375/1000 | Loss: 0.00014091
Iteration 376/1000 | Loss: 0.00014574
Iteration 377/1000 | Loss: 0.00015999
Iteration 378/1000 | Loss: 0.00017000
Iteration 379/1000 | Loss: 0.00013457
Iteration 380/1000 | Loss: 0.00013547
Iteration 381/1000 | Loss: 0.00025838
Iteration 382/1000 | Loss: 0.00013979
Iteration 383/1000 | Loss: 0.00013492
Iteration 384/1000 | Loss: 0.00036654
Iteration 385/1000 | Loss: 0.00016797
Iteration 386/1000 | Loss: 0.00021719
Iteration 387/1000 | Loss: 0.00023063
Iteration 388/1000 | Loss: 0.00015116
Iteration 389/1000 | Loss: 0.00012710
Iteration 390/1000 | Loss: 0.00015996
Iteration 391/1000 | Loss: 0.00016404
Iteration 392/1000 | Loss: 0.00016508
Iteration 393/1000 | Loss: 0.00019097
Iteration 394/1000 | Loss: 0.00017125
Iteration 395/1000 | Loss: 0.00033094
Iteration 396/1000 | Loss: 0.00030986
Iteration 397/1000 | Loss: 0.00013642
Iteration 398/1000 | Loss: 0.00011671
Iteration 399/1000 | Loss: 0.00016439
Iteration 400/1000 | Loss: 0.00015098
Iteration 401/1000 | Loss: 0.00010624
Iteration 402/1000 | Loss: 0.00010421
Iteration 403/1000 | Loss: 0.00048243
Iteration 404/1000 | Loss: 0.00039963
Iteration 405/1000 | Loss: 0.00018491
Iteration 406/1000 | Loss: 0.00038409
Iteration 407/1000 | Loss: 0.00112987
Iteration 408/1000 | Loss: 0.00023564
Iteration 409/1000 | Loss: 0.00010282
Iteration 410/1000 | Loss: 0.00023071
Iteration 411/1000 | Loss: 0.00017882
Iteration 412/1000 | Loss: 0.00042327
Iteration 413/1000 | Loss: 0.00051249
Iteration 414/1000 | Loss: 0.00014572
Iteration 415/1000 | Loss: 0.00028227
Iteration 416/1000 | Loss: 0.00019497
Iteration 417/1000 | Loss: 0.00024398
Iteration 418/1000 | Loss: 0.00017410
Iteration 419/1000 | Loss: 0.00010736
Iteration 420/1000 | Loss: 0.00019574
Iteration 421/1000 | Loss: 0.00012679
Iteration 422/1000 | Loss: 0.00019001
Iteration 423/1000 | Loss: 0.00021804
Iteration 424/1000 | Loss: 0.00025401
Iteration 425/1000 | Loss: 0.00014676
Iteration 426/1000 | Loss: 0.00013152
Iteration 427/1000 | Loss: 0.00010912
Iteration 428/1000 | Loss: 0.00025668
Iteration 429/1000 | Loss: 0.00035198
Iteration 430/1000 | Loss: 0.00011404
Iteration 431/1000 | Loss: 0.00010060
Iteration 432/1000 | Loss: 0.00011027
Iteration 433/1000 | Loss: 0.00010245
Iteration 434/1000 | Loss: 0.00010938
Iteration 435/1000 | Loss: 0.00022376
Iteration 436/1000 | Loss: 0.00012360
Iteration 437/1000 | Loss: 0.00011321
Iteration 438/1000 | Loss: 0.00010679
Iteration 439/1000 | Loss: 0.00010554
Iteration 440/1000 | Loss: 0.00010568
Iteration 441/1000 | Loss: 0.00021328
Iteration 442/1000 | Loss: 0.00010058
Iteration 443/1000 | Loss: 0.00009598
Iteration 444/1000 | Loss: 0.00012529
Iteration 445/1000 | Loss: 0.00010066
Iteration 446/1000 | Loss: 0.00010553
Iteration 447/1000 | Loss: 0.00010351
Iteration 448/1000 | Loss: 0.00010539
Iteration 449/1000 | Loss: 0.00010449
Iteration 450/1000 | Loss: 0.00009743
Iteration 451/1000 | Loss: 0.00009834
Iteration 452/1000 | Loss: 0.00010587
Iteration 453/1000 | Loss: 0.00010855
Iteration 454/1000 | Loss: 0.00009679
Iteration 455/1000 | Loss: 0.00009421
Iteration 456/1000 | Loss: 0.00009319
Iteration 457/1000 | Loss: 0.00010481
Iteration 458/1000 | Loss: 0.00009490
Iteration 459/1000 | Loss: 0.00022364
Iteration 460/1000 | Loss: 0.00009604
Iteration 461/1000 | Loss: 0.00010949
Iteration 462/1000 | Loss: 0.00020381
Iteration 463/1000 | Loss: 0.00011505
Iteration 464/1000 | Loss: 0.00049770
Iteration 465/1000 | Loss: 0.00051164
Iteration 466/1000 | Loss: 0.00023909
Iteration 467/1000 | Loss: 0.00042406
Iteration 468/1000 | Loss: 0.00011002
Iteration 469/1000 | Loss: 0.00009709
Iteration 470/1000 | Loss: 0.00009516
Iteration 471/1000 | Loss: 0.00009373
Iteration 472/1000 | Loss: 0.00009214
Iteration 473/1000 | Loss: 0.00010618
Iteration 474/1000 | Loss: 0.00010232
Iteration 475/1000 | Loss: 0.00026570
Iteration 476/1000 | Loss: 0.00012249
Iteration 477/1000 | Loss: 0.00010263
Iteration 478/1000 | Loss: 0.00012042
Iteration 479/1000 | Loss: 0.00009423
Iteration 480/1000 | Loss: 0.00009203
Iteration 481/1000 | Loss: 0.00009058
Iteration 482/1000 | Loss: 0.00012836
Iteration 483/1000 | Loss: 0.00009920
Iteration 484/1000 | Loss: 0.00023779
Iteration 485/1000 | Loss: 0.00097012
Iteration 486/1000 | Loss: 0.00036204
Iteration 487/1000 | Loss: 0.00060660
Iteration 488/1000 | Loss: 0.00018864
Iteration 489/1000 | Loss: 0.00101165
Iteration 490/1000 | Loss: 0.00036918
Iteration 491/1000 | Loss: 0.00034529
Iteration 492/1000 | Loss: 0.00010320
Iteration 493/1000 | Loss: 0.00009947
Iteration 494/1000 | Loss: 0.00009718
Iteration 495/1000 | Loss: 0.00009536
Iteration 496/1000 | Loss: 0.00008781
Iteration 497/1000 | Loss: 0.00008680
Iteration 498/1000 | Loss: 0.00011256
Iteration 499/1000 | Loss: 0.00022789
Iteration 500/1000 | Loss: 0.00010302
Iteration 501/1000 | Loss: 0.00009954
Iteration 502/1000 | Loss: 0.00008730
Iteration 503/1000 | Loss: 0.00008656
Iteration 504/1000 | Loss: 0.00015446
Iteration 505/1000 | Loss: 0.00008571
Iteration 506/1000 | Loss: 0.00008495
Iteration 507/1000 | Loss: 0.00008444
Iteration 508/1000 | Loss: 0.00008416
Iteration 509/1000 | Loss: 0.00008402
Iteration 510/1000 | Loss: 0.00008390
Iteration 511/1000 | Loss: 0.00008389
Iteration 512/1000 | Loss: 0.00008367
Iteration 513/1000 | Loss: 0.00008348
Iteration 514/1000 | Loss: 0.00008347
Iteration 515/1000 | Loss: 0.00008333
Iteration 516/1000 | Loss: 0.00008333
Iteration 517/1000 | Loss: 0.00008332
Iteration 518/1000 | Loss: 0.00008331
Iteration 519/1000 | Loss: 0.00008331
Iteration 520/1000 | Loss: 0.00008329
Iteration 521/1000 | Loss: 0.00021677
Iteration 522/1000 | Loss: 0.00008352
Iteration 523/1000 | Loss: 0.00008259
Iteration 524/1000 | Loss: 0.00008209
Iteration 525/1000 | Loss: 0.00008157
Iteration 526/1000 | Loss: 0.00008131
Iteration 527/1000 | Loss: 0.00008116
Iteration 528/1000 | Loss: 0.00008108
Iteration 529/1000 | Loss: 0.00008100
Iteration 530/1000 | Loss: 0.00008096
Iteration 531/1000 | Loss: 0.00008090
Iteration 532/1000 | Loss: 0.00008084
Iteration 533/1000 | Loss: 0.00008084
Iteration 534/1000 | Loss: 0.00008083
Iteration 535/1000 | Loss: 0.00008082
Iteration 536/1000 | Loss: 0.00008082
Iteration 537/1000 | Loss: 0.00008081
Iteration 538/1000 | Loss: 0.00008068
Iteration 539/1000 | Loss: 0.00008065
Iteration 540/1000 | Loss: 0.00008056
Iteration 541/1000 | Loss: 0.00008040
Iteration 542/1000 | Loss: 0.00008023
Iteration 543/1000 | Loss: 0.00022490
Iteration 544/1000 | Loss: 0.00043349
Iteration 545/1000 | Loss: 0.00010480
Iteration 546/1000 | Loss: 0.00029070
Iteration 547/1000 | Loss: 0.00027305
Iteration 548/1000 | Loss: 0.00025399
Iteration 549/1000 | Loss: 0.00024926
Iteration 550/1000 | Loss: 0.00008384
Iteration 551/1000 | Loss: 0.00008091
Iteration 552/1000 | Loss: 0.00008040
Iteration 553/1000 | Loss: 0.00007998
Iteration 554/1000 | Loss: 0.00007969
Iteration 555/1000 | Loss: 0.00007951
Iteration 556/1000 | Loss: 0.00007946
Iteration 557/1000 | Loss: 0.00007946
Iteration 558/1000 | Loss: 0.00007942
Iteration 559/1000 | Loss: 0.00007942
Iteration 560/1000 | Loss: 0.00007930
Iteration 561/1000 | Loss: 0.00007928
Iteration 562/1000 | Loss: 0.00007924
Iteration 563/1000 | Loss: 0.00007923
Iteration 564/1000 | Loss: 0.00007923
Iteration 565/1000 | Loss: 0.00007922
Iteration 566/1000 | Loss: 0.00007922
Iteration 567/1000 | Loss: 0.00007922
Iteration 568/1000 | Loss: 0.00007921
Iteration 569/1000 | Loss: 0.00007921
Iteration 570/1000 | Loss: 0.00007921
Iteration 571/1000 | Loss: 0.00007921
Iteration 572/1000 | Loss: 0.00007920
Iteration 573/1000 | Loss: 0.00007920
Iteration 574/1000 | Loss: 0.00007920
Iteration 575/1000 | Loss: 0.00007920
Iteration 576/1000 | Loss: 0.00007920
Iteration 577/1000 | Loss: 0.00007920
Iteration 578/1000 | Loss: 0.00007920
Iteration 579/1000 | Loss: 0.00007920
Iteration 580/1000 | Loss: 0.00007920
Iteration 581/1000 | Loss: 0.00007919
Iteration 582/1000 | Loss: 0.00007918
Iteration 583/1000 | Loss: 0.00007918
Iteration 584/1000 | Loss: 0.00007918
Iteration 585/1000 | Loss: 0.00007918
Iteration 586/1000 | Loss: 0.00007918
Iteration 587/1000 | Loss: 0.00007918
Iteration 588/1000 | Loss: 0.00007918
Iteration 589/1000 | Loss: 0.00007918
Iteration 590/1000 | Loss: 0.00007918
Iteration 591/1000 | Loss: 0.00007918
Iteration 592/1000 | Loss: 0.00020667
Iteration 593/1000 | Loss: 0.00008654
Iteration 594/1000 | Loss: 0.00007923
Iteration 595/1000 | Loss: 0.00007919
Iteration 596/1000 | Loss: 0.00007918
Iteration 597/1000 | Loss: 0.00007917
Iteration 598/1000 | Loss: 0.00007917
Iteration 599/1000 | Loss: 0.00010881
Iteration 600/1000 | Loss: 0.00007923
Iteration 601/1000 | Loss: 0.00007918
Iteration 602/1000 | Loss: 0.00007917
Iteration 603/1000 | Loss: 0.00007917
Iteration 604/1000 | Loss: 0.00007917
Iteration 605/1000 | Loss: 0.00007917
Iteration 606/1000 | Loss: 0.00007917
Iteration 607/1000 | Loss: 0.00007917
Iteration 608/1000 | Loss: 0.00009701
Iteration 609/1000 | Loss: 0.00007916
Iteration 610/1000 | Loss: 0.00007916
Iteration 611/1000 | Loss: 0.00007916
Iteration 612/1000 | Loss: 0.00007916
Iteration 613/1000 | Loss: 0.00007916
Iteration 614/1000 | Loss: 0.00007916
Iteration 615/1000 | Loss: 0.00007916
Iteration 616/1000 | Loss: 0.00007916
Iteration 617/1000 | Loss: 0.00007916
Iteration 618/1000 | Loss: 0.00007916
Iteration 619/1000 | Loss: 0.00007915
Iteration 620/1000 | Loss: 0.00007915
Iteration 621/1000 | Loss: 0.00007915
Iteration 622/1000 | Loss: 0.00007914
Iteration 623/1000 | Loss: 0.00007914
Iteration 624/1000 | Loss: 0.00007913
Iteration 625/1000 | Loss: 0.00007913
Iteration 626/1000 | Loss: 0.00007913
Iteration 627/1000 | Loss: 0.00007913
Iteration 628/1000 | Loss: 0.00007912
Iteration 629/1000 | Loss: 0.00007912
Iteration 630/1000 | Loss: 0.00007912
Iteration 631/1000 | Loss: 0.00007912
Iteration 632/1000 | Loss: 0.00007912
Iteration 633/1000 | Loss: 0.00007912
Iteration 634/1000 | Loss: 0.00007912
Iteration 635/1000 | Loss: 0.00007912
Iteration 636/1000 | Loss: 0.00007912
Iteration 637/1000 | Loss: 0.00007912
Iteration 638/1000 | Loss: 0.00007912
Iteration 639/1000 | Loss: 0.00007912
Iteration 640/1000 | Loss: 0.00007912
Iteration 641/1000 | Loss: 0.00007912
Iteration 642/1000 | Loss: 0.00007912
Iteration 643/1000 | Loss: 0.00007912
Iteration 644/1000 | Loss: 0.00007912
Iteration 645/1000 | Loss: 0.00007912
Iteration 646/1000 | Loss: 0.00007912
Iteration 647/1000 | Loss: 0.00007911
Iteration 648/1000 | Loss: 0.00007911
Iteration 649/1000 | Loss: 0.00007911
Iteration 650/1000 | Loss: 0.00007911
Iteration 651/1000 | Loss: 0.00007911
Iteration 652/1000 | Loss: 0.00007911
Iteration 653/1000 | Loss: 0.00007910
Iteration 654/1000 | Loss: 0.00007910
Iteration 655/1000 | Loss: 0.00007910
Iteration 656/1000 | Loss: 0.00007910
Iteration 657/1000 | Loss: 0.00007910
Iteration 658/1000 | Loss: 0.00007910
Iteration 659/1000 | Loss: 0.00007910
Iteration 660/1000 | Loss: 0.00007910
Iteration 661/1000 | Loss: 0.00007910
Iteration 662/1000 | Loss: 0.00007910
Iteration 663/1000 | Loss: 0.00007909
Iteration 664/1000 | Loss: 0.00007909
Iteration 665/1000 | Loss: 0.00007909
Iteration 666/1000 | Loss: 0.00007909
Iteration 667/1000 | Loss: 0.00007909
Iteration 668/1000 | Loss: 0.00007909
Iteration 669/1000 | Loss: 0.00007909
Iteration 670/1000 | Loss: 0.00007909
Iteration 671/1000 | Loss: 0.00007909
Iteration 672/1000 | Loss: 0.00007909
Iteration 673/1000 | Loss: 0.00007909
Iteration 674/1000 | Loss: 0.00007909
Iteration 675/1000 | Loss: 0.00007909
Iteration 676/1000 | Loss: 0.00007909
Iteration 677/1000 | Loss: 0.00007909
Iteration 678/1000 | Loss: 0.00007909
Iteration 679/1000 | Loss: 0.00007909
Iteration 680/1000 | Loss: 0.00007909
Iteration 681/1000 | Loss: 0.00007909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 681. Stopping optimization.
Last 5 losses: [7.908890984253958e-05, 7.908890984253958e-05, 7.908890984253958e-05, 7.908890984253958e-05, 7.908890984253958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.908890984253958e-05

Optimization complete. Final v2v error: 5.476124286651611 mm

Highest mean error: 12.21367073059082 mm for frame 184

Lowest mean error: 3.452443838119507 mm for frame 103

Saving results

Total time: 922.708203792572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895435
Iteration 2/25 | Loss: 0.00172048
Iteration 3/25 | Loss: 0.00138766
Iteration 4/25 | Loss: 0.00136976
Iteration 5/25 | Loss: 0.00136428
Iteration 6/25 | Loss: 0.00136315
Iteration 7/25 | Loss: 0.00136315
Iteration 8/25 | Loss: 0.00136315
Iteration 9/25 | Loss: 0.00136315
Iteration 10/25 | Loss: 0.00136315
Iteration 11/25 | Loss: 0.00136315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013631463516503572, 0.0013631463516503572, 0.0013631463516503572, 0.0013631463516503572, 0.0013631463516503572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013631463516503572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12484908
Iteration 2/25 | Loss: 0.00095687
Iteration 3/25 | Loss: 0.00095686
Iteration 4/25 | Loss: 0.00095686
Iteration 5/25 | Loss: 0.00095686
Iteration 6/25 | Loss: 0.00095686
Iteration 7/25 | Loss: 0.00095686
Iteration 8/25 | Loss: 0.00095685
Iteration 9/25 | Loss: 0.00095685
Iteration 10/25 | Loss: 0.00095685
Iteration 11/25 | Loss: 0.00095685
Iteration 12/25 | Loss: 0.00095685
Iteration 13/25 | Loss: 0.00095685
Iteration 14/25 | Loss: 0.00095685
Iteration 15/25 | Loss: 0.00095685
Iteration 16/25 | Loss: 0.00095685
Iteration 17/25 | Loss: 0.00095685
Iteration 18/25 | Loss: 0.00095685
Iteration 19/25 | Loss: 0.00095685
Iteration 20/25 | Loss: 0.00095685
Iteration 21/25 | Loss: 0.00095685
Iteration 22/25 | Loss: 0.00095685
Iteration 23/25 | Loss: 0.00095685
Iteration 24/25 | Loss: 0.00095685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009568536188453436, 0.0009568536188453436, 0.0009568536188453436, 0.0009568536188453436, 0.0009568536188453436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009568536188453436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095685
Iteration 2/1000 | Loss: 0.00004087
Iteration 3/1000 | Loss: 0.00002765
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002299
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00001991
Iteration 11/1000 | Loss: 0.00001960
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001861
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001842
Iteration 18/1000 | Loss: 0.00001829
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001823
Iteration 21/1000 | Loss: 0.00001822
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001805
Iteration 26/1000 | Loss: 0.00001804
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001782
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001781
Iteration 69/1000 | Loss: 0.00001781
Iteration 70/1000 | Loss: 0.00001781
Iteration 71/1000 | Loss: 0.00001781
Iteration 72/1000 | Loss: 0.00001781
Iteration 73/1000 | Loss: 0.00001781
Iteration 74/1000 | Loss: 0.00001781
Iteration 75/1000 | Loss: 0.00001781
Iteration 76/1000 | Loss: 0.00001780
Iteration 77/1000 | Loss: 0.00001780
Iteration 78/1000 | Loss: 0.00001780
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001780
Iteration 84/1000 | Loss: 0.00001780
Iteration 85/1000 | Loss: 0.00001780
Iteration 86/1000 | Loss: 0.00001780
Iteration 87/1000 | Loss: 0.00001780
Iteration 88/1000 | Loss: 0.00001780
Iteration 89/1000 | Loss: 0.00001780
Iteration 90/1000 | Loss: 0.00001780
Iteration 91/1000 | Loss: 0.00001780
Iteration 92/1000 | Loss: 0.00001780
Iteration 93/1000 | Loss: 0.00001779
Iteration 94/1000 | Loss: 0.00001779
Iteration 95/1000 | Loss: 0.00001779
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001779
Iteration 99/1000 | Loss: 0.00001779
Iteration 100/1000 | Loss: 0.00001779
Iteration 101/1000 | Loss: 0.00001779
Iteration 102/1000 | Loss: 0.00001779
Iteration 103/1000 | Loss: 0.00001779
Iteration 104/1000 | Loss: 0.00001778
Iteration 105/1000 | Loss: 0.00001778
Iteration 106/1000 | Loss: 0.00001778
Iteration 107/1000 | Loss: 0.00001778
Iteration 108/1000 | Loss: 0.00001778
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001778
Iteration 112/1000 | Loss: 0.00001778
Iteration 113/1000 | Loss: 0.00001777
Iteration 114/1000 | Loss: 0.00001777
Iteration 115/1000 | Loss: 0.00001777
Iteration 116/1000 | Loss: 0.00001777
Iteration 117/1000 | Loss: 0.00001777
Iteration 118/1000 | Loss: 0.00001777
Iteration 119/1000 | Loss: 0.00001777
Iteration 120/1000 | Loss: 0.00001777
Iteration 121/1000 | Loss: 0.00001776
Iteration 122/1000 | Loss: 0.00001776
Iteration 123/1000 | Loss: 0.00001776
Iteration 124/1000 | Loss: 0.00001776
Iteration 125/1000 | Loss: 0.00001776
Iteration 126/1000 | Loss: 0.00001776
Iteration 127/1000 | Loss: 0.00001776
Iteration 128/1000 | Loss: 0.00001776
Iteration 129/1000 | Loss: 0.00001776
Iteration 130/1000 | Loss: 0.00001776
Iteration 131/1000 | Loss: 0.00001776
Iteration 132/1000 | Loss: 0.00001776
Iteration 133/1000 | Loss: 0.00001775
Iteration 134/1000 | Loss: 0.00001775
Iteration 135/1000 | Loss: 0.00001775
Iteration 136/1000 | Loss: 0.00001775
Iteration 137/1000 | Loss: 0.00001775
Iteration 138/1000 | Loss: 0.00001775
Iteration 139/1000 | Loss: 0.00001775
Iteration 140/1000 | Loss: 0.00001775
Iteration 141/1000 | Loss: 0.00001774
Iteration 142/1000 | Loss: 0.00001774
Iteration 143/1000 | Loss: 0.00001774
Iteration 144/1000 | Loss: 0.00001774
Iteration 145/1000 | Loss: 0.00001774
Iteration 146/1000 | Loss: 0.00001774
Iteration 147/1000 | Loss: 0.00001774
Iteration 148/1000 | Loss: 0.00001774
Iteration 149/1000 | Loss: 0.00001774
Iteration 150/1000 | Loss: 0.00001774
Iteration 151/1000 | Loss: 0.00001774
Iteration 152/1000 | Loss: 0.00001774
Iteration 153/1000 | Loss: 0.00001773
Iteration 154/1000 | Loss: 0.00001773
Iteration 155/1000 | Loss: 0.00001773
Iteration 156/1000 | Loss: 0.00001773
Iteration 157/1000 | Loss: 0.00001773
Iteration 158/1000 | Loss: 0.00001773
Iteration 159/1000 | Loss: 0.00001773
Iteration 160/1000 | Loss: 0.00001773
Iteration 161/1000 | Loss: 0.00001773
Iteration 162/1000 | Loss: 0.00001773
Iteration 163/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7732223568600602e-05, 1.7732223568600602e-05, 1.7732223568600602e-05, 1.7732223568600602e-05, 1.7732223568600602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7732223568600602e-05

Optimization complete. Final v2v error: 3.5151796340942383 mm

Highest mean error: 4.8184099197387695 mm for frame 125

Lowest mean error: 2.8740923404693604 mm for frame 168

Saving results

Total time: 45.91006016731262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973080
Iteration 2/25 | Loss: 0.00228948
Iteration 3/25 | Loss: 0.00162533
Iteration 4/25 | Loss: 0.00152288
Iteration 5/25 | Loss: 0.00147065
Iteration 6/25 | Loss: 0.00144292
Iteration 7/25 | Loss: 0.00143632
Iteration 8/25 | Loss: 0.00142386
Iteration 9/25 | Loss: 0.00141354
Iteration 10/25 | Loss: 0.00141535
Iteration 11/25 | Loss: 0.00140964
Iteration 12/25 | Loss: 0.00141349
Iteration 13/25 | Loss: 0.00140749
Iteration 14/25 | Loss: 0.00140317
Iteration 15/25 | Loss: 0.00139976
Iteration 16/25 | Loss: 0.00139670
Iteration 17/25 | Loss: 0.00139165
Iteration 18/25 | Loss: 0.00139086
Iteration 19/25 | Loss: 0.00139485
Iteration 20/25 | Loss: 0.00139268
Iteration 21/25 | Loss: 0.00139093
Iteration 22/25 | Loss: 0.00139064
Iteration 23/25 | Loss: 0.00139032
Iteration 24/25 | Loss: 0.00139299
Iteration 25/25 | Loss: 0.00138979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31338203
Iteration 2/25 | Loss: 0.00152173
Iteration 3/25 | Loss: 0.00120732
Iteration 4/25 | Loss: 0.00120732
Iteration 5/25 | Loss: 0.00120732
Iteration 6/25 | Loss: 0.00120732
Iteration 7/25 | Loss: 0.00120732
Iteration 8/25 | Loss: 0.00120732
Iteration 9/25 | Loss: 0.00120732
Iteration 10/25 | Loss: 0.00120732
Iteration 11/25 | Loss: 0.00120732
Iteration 12/25 | Loss: 0.00120732
Iteration 13/25 | Loss: 0.00120732
Iteration 14/25 | Loss: 0.00120732
Iteration 15/25 | Loss: 0.00120732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012073173420503736, 0.0012073173420503736, 0.0012073173420503736, 0.0012073173420503736, 0.0012073173420503736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012073173420503736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120732
Iteration 2/1000 | Loss: 0.00066939
Iteration 3/1000 | Loss: 0.00248144
Iteration 4/1000 | Loss: 0.00012558
Iteration 5/1000 | Loss: 0.00005316
Iteration 6/1000 | Loss: 0.00038795
Iteration 7/1000 | Loss: 0.00004646
Iteration 8/1000 | Loss: 0.00018443
Iteration 9/1000 | Loss: 0.00002980
Iteration 10/1000 | Loss: 0.00002698
Iteration 11/1000 | Loss: 0.00002490
Iteration 12/1000 | Loss: 0.00002340
Iteration 13/1000 | Loss: 0.00012083
Iteration 14/1000 | Loss: 0.00002890
Iteration 15/1000 | Loss: 0.00002456
Iteration 16/1000 | Loss: 0.00002188
Iteration 17/1000 | Loss: 0.00002145
Iteration 18/1000 | Loss: 0.00015741
Iteration 19/1000 | Loss: 0.00002099
Iteration 20/1000 | Loss: 0.00009974
Iteration 21/1000 | Loss: 0.00040136
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002048
Iteration 24/1000 | Loss: 0.00002045
Iteration 25/1000 | Loss: 0.00002045
Iteration 26/1000 | Loss: 0.00002043
Iteration 27/1000 | Loss: 0.00010969
Iteration 28/1000 | Loss: 0.00014526
Iteration 29/1000 | Loss: 0.00004312
Iteration 30/1000 | Loss: 0.00003010
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002077
Iteration 34/1000 | Loss: 0.00002036
Iteration 35/1000 | Loss: 0.00002034
Iteration 36/1000 | Loss: 0.00002032
Iteration 37/1000 | Loss: 0.00002031
Iteration 38/1000 | Loss: 0.00002031
Iteration 39/1000 | Loss: 0.00002031
Iteration 40/1000 | Loss: 0.00002031
Iteration 41/1000 | Loss: 0.00002030
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00002030
Iteration 44/1000 | Loss: 0.00002029
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002023
Iteration 47/1000 | Loss: 0.00002023
Iteration 48/1000 | Loss: 0.00002023
Iteration 49/1000 | Loss: 0.00002023
Iteration 50/1000 | Loss: 0.00002023
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002023
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002022
Iteration 59/1000 | Loss: 0.00002018
Iteration 60/1000 | Loss: 0.00002016
Iteration 61/1000 | Loss: 0.00002002
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002000
Iteration 64/1000 | Loss: 0.00002000
Iteration 65/1000 | Loss: 0.00001999
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001997
Iteration 70/1000 | Loss: 0.00001997
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001996
Iteration 74/1000 | Loss: 0.00001995
Iteration 75/1000 | Loss: 0.00001995
Iteration 76/1000 | Loss: 0.00001995
Iteration 77/1000 | Loss: 0.00001994
Iteration 78/1000 | Loss: 0.00001994
Iteration 79/1000 | Loss: 0.00001994
Iteration 80/1000 | Loss: 0.00001994
Iteration 81/1000 | Loss: 0.00001993
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001993
Iteration 84/1000 | Loss: 0.00001993
Iteration 85/1000 | Loss: 0.00001993
Iteration 86/1000 | Loss: 0.00001993
Iteration 87/1000 | Loss: 0.00001993
Iteration 88/1000 | Loss: 0.00001993
Iteration 89/1000 | Loss: 0.00001993
Iteration 90/1000 | Loss: 0.00001993
Iteration 91/1000 | Loss: 0.00001992
Iteration 92/1000 | Loss: 0.00001992
Iteration 93/1000 | Loss: 0.00001992
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001991
Iteration 97/1000 | Loss: 0.00001990
Iteration 98/1000 | Loss: 0.00001989
Iteration 99/1000 | Loss: 0.00001988
Iteration 100/1000 | Loss: 0.00001988
Iteration 101/1000 | Loss: 0.00001988
Iteration 102/1000 | Loss: 0.00001987
Iteration 103/1000 | Loss: 0.00001987
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001987
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001986
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001985
Iteration 113/1000 | Loss: 0.00001985
Iteration 114/1000 | Loss: 0.00001985
Iteration 115/1000 | Loss: 0.00001985
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001984
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001983
Iteration 131/1000 | Loss: 0.00001983
Iteration 132/1000 | Loss: 0.00001983
Iteration 133/1000 | Loss: 0.00001983
Iteration 134/1000 | Loss: 0.00001983
Iteration 135/1000 | Loss: 0.00001983
Iteration 136/1000 | Loss: 0.00001983
Iteration 137/1000 | Loss: 0.00001983
Iteration 138/1000 | Loss: 0.00001983
Iteration 139/1000 | Loss: 0.00001983
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001982
Iteration 143/1000 | Loss: 0.00001982
Iteration 144/1000 | Loss: 0.00001982
Iteration 145/1000 | Loss: 0.00001982
Iteration 146/1000 | Loss: 0.00001982
Iteration 147/1000 | Loss: 0.00001982
Iteration 148/1000 | Loss: 0.00001982
Iteration 149/1000 | Loss: 0.00001982
Iteration 150/1000 | Loss: 0.00001982
Iteration 151/1000 | Loss: 0.00001982
Iteration 152/1000 | Loss: 0.00001982
Iteration 153/1000 | Loss: 0.00001982
Iteration 154/1000 | Loss: 0.00001982
Iteration 155/1000 | Loss: 0.00001982
Iteration 156/1000 | Loss: 0.00001982
Iteration 157/1000 | Loss: 0.00001982
Iteration 158/1000 | Loss: 0.00001982
Iteration 159/1000 | Loss: 0.00001982
Iteration 160/1000 | Loss: 0.00001982
Iteration 161/1000 | Loss: 0.00001982
Iteration 162/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.9821494788629934e-05, 1.9821494788629934e-05, 1.9821494788629934e-05, 1.9821494788629934e-05, 1.9821494788629934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9821494788629934e-05

Optimization complete. Final v2v error: 3.736415147781372 mm

Highest mean error: 4.338860034942627 mm for frame 134

Lowest mean error: 3.2462661266326904 mm for frame 17

Saving results

Total time: 112.29518270492554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819965
Iteration 2/25 | Loss: 0.00133020
Iteration 3/25 | Loss: 0.00127932
Iteration 4/25 | Loss: 0.00127008
Iteration 5/25 | Loss: 0.00126759
Iteration 6/25 | Loss: 0.00126731
Iteration 7/25 | Loss: 0.00126731
Iteration 8/25 | Loss: 0.00126731
Iteration 9/25 | Loss: 0.00126731
Iteration 10/25 | Loss: 0.00126731
Iteration 11/25 | Loss: 0.00126731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001267311628907919, 0.001267311628907919, 0.001267311628907919, 0.001267311628907919, 0.001267311628907919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001267311628907919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.50671864
Iteration 2/25 | Loss: 0.00082543
Iteration 3/25 | Loss: 0.00082542
Iteration 4/25 | Loss: 0.00082542
Iteration 5/25 | Loss: 0.00082542
Iteration 6/25 | Loss: 0.00082542
Iteration 7/25 | Loss: 0.00082542
Iteration 8/25 | Loss: 0.00082542
Iteration 9/25 | Loss: 0.00082542
Iteration 10/25 | Loss: 0.00082542
Iteration 11/25 | Loss: 0.00082542
Iteration 12/25 | Loss: 0.00082542
Iteration 13/25 | Loss: 0.00082542
Iteration 14/25 | Loss: 0.00082542
Iteration 15/25 | Loss: 0.00082542
Iteration 16/25 | Loss: 0.00082542
Iteration 17/25 | Loss: 0.00082542
Iteration 18/25 | Loss: 0.00082542
Iteration 19/25 | Loss: 0.00082542
Iteration 20/25 | Loss: 0.00082542
Iteration 21/25 | Loss: 0.00082542
Iteration 22/25 | Loss: 0.00082542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000825415481813252, 0.000825415481813252, 0.000825415481813252, 0.000825415481813252, 0.000825415481813252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000825415481813252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082542
Iteration 2/1000 | Loss: 0.00002738
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001656
Iteration 8/1000 | Loss: 0.00001622
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001563
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001539
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001530
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001528
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001521
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001520
Iteration 39/1000 | Loss: 0.00001520
Iteration 40/1000 | Loss: 0.00001519
Iteration 41/1000 | Loss: 0.00001519
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001509
Iteration 59/1000 | Loss: 0.00001509
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001504
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001495
Iteration 97/1000 | Loss: 0.00001495
Iteration 98/1000 | Loss: 0.00001495
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001494
Iteration 102/1000 | Loss: 0.00001494
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001491
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001490
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001490
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001490
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001486
Iteration 129/1000 | Loss: 0.00001486
Iteration 130/1000 | Loss: 0.00001486
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001485
Iteration 134/1000 | Loss: 0.00001485
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001484
Iteration 137/1000 | Loss: 0.00001484
Iteration 138/1000 | Loss: 0.00001484
Iteration 139/1000 | Loss: 0.00001484
Iteration 140/1000 | Loss: 0.00001484
Iteration 141/1000 | Loss: 0.00001483
Iteration 142/1000 | Loss: 0.00001483
Iteration 143/1000 | Loss: 0.00001483
Iteration 144/1000 | Loss: 0.00001483
Iteration 145/1000 | Loss: 0.00001483
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001483
Iteration 148/1000 | Loss: 0.00001483
Iteration 149/1000 | Loss: 0.00001483
Iteration 150/1000 | Loss: 0.00001483
Iteration 151/1000 | Loss: 0.00001483
Iteration 152/1000 | Loss: 0.00001483
Iteration 153/1000 | Loss: 0.00001483
Iteration 154/1000 | Loss: 0.00001483
Iteration 155/1000 | Loss: 0.00001483
Iteration 156/1000 | Loss: 0.00001483
Iteration 157/1000 | Loss: 0.00001483
Iteration 158/1000 | Loss: 0.00001483
Iteration 159/1000 | Loss: 0.00001483
Iteration 160/1000 | Loss: 0.00001482
Iteration 161/1000 | Loss: 0.00001482
Iteration 162/1000 | Loss: 0.00001482
Iteration 163/1000 | Loss: 0.00001482
Iteration 164/1000 | Loss: 0.00001482
Iteration 165/1000 | Loss: 0.00001482
Iteration 166/1000 | Loss: 0.00001482
Iteration 167/1000 | Loss: 0.00001482
Iteration 168/1000 | Loss: 0.00001482
Iteration 169/1000 | Loss: 0.00001482
Iteration 170/1000 | Loss: 0.00001482
Iteration 171/1000 | Loss: 0.00001482
Iteration 172/1000 | Loss: 0.00001482
Iteration 173/1000 | Loss: 0.00001482
Iteration 174/1000 | Loss: 0.00001482
Iteration 175/1000 | Loss: 0.00001482
Iteration 176/1000 | Loss: 0.00001482
Iteration 177/1000 | Loss: 0.00001482
Iteration 178/1000 | Loss: 0.00001482
Iteration 179/1000 | Loss: 0.00001482
Iteration 180/1000 | Loss: 0.00001482
Iteration 181/1000 | Loss: 0.00001482
Iteration 182/1000 | Loss: 0.00001482
Iteration 183/1000 | Loss: 0.00001482
Iteration 184/1000 | Loss: 0.00001482
Iteration 185/1000 | Loss: 0.00001482
Iteration 186/1000 | Loss: 0.00001482
Iteration 187/1000 | Loss: 0.00001482
Iteration 188/1000 | Loss: 0.00001482
Iteration 189/1000 | Loss: 0.00001482
Iteration 190/1000 | Loss: 0.00001482
Iteration 191/1000 | Loss: 0.00001482
Iteration 192/1000 | Loss: 0.00001482
Iteration 193/1000 | Loss: 0.00001482
Iteration 194/1000 | Loss: 0.00001482
Iteration 195/1000 | Loss: 0.00001482
Iteration 196/1000 | Loss: 0.00001482
Iteration 197/1000 | Loss: 0.00001482
Iteration 198/1000 | Loss: 0.00001482
Iteration 199/1000 | Loss: 0.00001482
Iteration 200/1000 | Loss: 0.00001482
Iteration 201/1000 | Loss: 0.00001482
Iteration 202/1000 | Loss: 0.00001482
Iteration 203/1000 | Loss: 0.00001482
Iteration 204/1000 | Loss: 0.00001482
Iteration 205/1000 | Loss: 0.00001482
Iteration 206/1000 | Loss: 0.00001482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4821151125943288e-05, 1.4821151125943288e-05, 1.4821151125943288e-05, 1.4821151125943288e-05, 1.4821151125943288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4821151125943288e-05

Optimization complete. Final v2v error: 3.2578160762786865 mm

Highest mean error: 3.849745273590088 mm for frame 101

Lowest mean error: 2.9635181427001953 mm for frame 15

Saving results

Total time: 37.65771412849426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012109
Iteration 2/25 | Loss: 0.01012109
Iteration 3/25 | Loss: 0.01012109
Iteration 4/25 | Loss: 0.01012109
Iteration 5/25 | Loss: 0.01012108
Iteration 6/25 | Loss: 0.01012108
Iteration 7/25 | Loss: 0.01012108
Iteration 8/25 | Loss: 0.01012108
Iteration 9/25 | Loss: 0.00189059
Iteration 10/25 | Loss: 0.00140923
Iteration 11/25 | Loss: 0.00134925
Iteration 12/25 | Loss: 0.00133969
Iteration 13/25 | Loss: 0.00133888
Iteration 14/25 | Loss: 0.00133911
Iteration 15/25 | Loss: 0.00131953
Iteration 16/25 | Loss: 0.00131241
Iteration 17/25 | Loss: 0.00131140
Iteration 18/25 | Loss: 0.00130769
Iteration 19/25 | Loss: 0.00130312
Iteration 20/25 | Loss: 0.00130699
Iteration 21/25 | Loss: 0.00129596
Iteration 22/25 | Loss: 0.00129564
Iteration 23/25 | Loss: 0.00129448
Iteration 24/25 | Loss: 0.00129440
Iteration 25/25 | Loss: 0.00129439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41879821
Iteration 2/25 | Loss: 0.00099912
Iteration 3/25 | Loss: 0.00099912
Iteration 4/25 | Loss: 0.00099912
Iteration 5/25 | Loss: 0.00099912
Iteration 6/25 | Loss: 0.00099912
Iteration 7/25 | Loss: 0.00099912
Iteration 8/25 | Loss: 0.00099912
Iteration 9/25 | Loss: 0.00099912
Iteration 10/25 | Loss: 0.00099912
Iteration 11/25 | Loss: 0.00099912
Iteration 12/25 | Loss: 0.00099912
Iteration 13/25 | Loss: 0.00099912
Iteration 14/25 | Loss: 0.00099912
Iteration 15/25 | Loss: 0.00099912
Iteration 16/25 | Loss: 0.00099912
Iteration 17/25 | Loss: 0.00099912
Iteration 18/25 | Loss: 0.00099912
Iteration 19/25 | Loss: 0.00099912
Iteration 20/25 | Loss: 0.00099912
Iteration 21/25 | Loss: 0.00099912
Iteration 22/25 | Loss: 0.00099912
Iteration 23/25 | Loss: 0.00099912
Iteration 24/25 | Loss: 0.00099912
Iteration 25/25 | Loss: 0.00099912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009991176193580031, 0.0009991176193580031, 0.0009991176193580031, 0.0009991176193580031, 0.0009991176193580031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009991176193580031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099912
Iteration 2/1000 | Loss: 0.00003556
Iteration 3/1000 | Loss: 0.00002177
Iteration 4/1000 | Loss: 0.00011539
Iteration 5/1000 | Loss: 0.00002133
Iteration 6/1000 | Loss: 0.00031536
Iteration 7/1000 | Loss: 0.00001972
Iteration 8/1000 | Loss: 0.00011007
Iteration 9/1000 | Loss: 0.00006426
Iteration 10/1000 | Loss: 0.00063791
Iteration 11/1000 | Loss: 0.00002861
Iteration 12/1000 | Loss: 0.00003382
Iteration 13/1000 | Loss: 0.00001659
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00005431
Iteration 16/1000 | Loss: 0.00009157
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00004177
Iteration 19/1000 | Loss: 0.00002207
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001791
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00006768
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00003968
Iteration 33/1000 | Loss: 0.00052694
Iteration 34/1000 | Loss: 0.00131774
Iteration 35/1000 | Loss: 0.00006448
Iteration 36/1000 | Loss: 0.00009227
Iteration 37/1000 | Loss: 0.00010102
Iteration 38/1000 | Loss: 0.00005903
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00008905
Iteration 41/1000 | Loss: 0.00002735
Iteration 42/1000 | Loss: 0.00002380
Iteration 43/1000 | Loss: 0.00002681
Iteration 44/1000 | Loss: 0.00002781
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00001544
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001543
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001540
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001539
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001539
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00002798
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00003683
Iteration 73/1000 | Loss: 0.00006004
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001943
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001531
Iteration 110/1000 | Loss: 0.00001531
Iteration 111/1000 | Loss: 0.00001531
Iteration 112/1000 | Loss: 0.00001531
Iteration 113/1000 | Loss: 0.00001531
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001531
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001531
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001531
Iteration 125/1000 | Loss: 0.00001531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.530544977867976e-05, 1.530544977867976e-05, 1.530544977867976e-05, 1.530544977867976e-05, 1.530544977867976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530544977867976e-05

Optimization complete. Final v2v error: 3.336963653564453 mm

Highest mean error: 3.7597506046295166 mm for frame 22

Lowest mean error: 2.9995572566986084 mm for frame 35

Saving results

Total time: 109.12317943572998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399206
Iteration 2/25 | Loss: 0.00132980
Iteration 3/25 | Loss: 0.00126950
Iteration 4/25 | Loss: 0.00126425
Iteration 5/25 | Loss: 0.00126306
Iteration 6/25 | Loss: 0.00126306
Iteration 7/25 | Loss: 0.00126306
Iteration 8/25 | Loss: 0.00126306
Iteration 9/25 | Loss: 0.00126306
Iteration 10/25 | Loss: 0.00126306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012630649143829942, 0.0012630649143829942, 0.0012630649143829942, 0.0012630649143829942, 0.0012630649143829942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012630649143829942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39800513
Iteration 2/25 | Loss: 0.00092422
Iteration 3/25 | Loss: 0.00092421
Iteration 4/25 | Loss: 0.00092421
Iteration 5/25 | Loss: 0.00092421
Iteration 6/25 | Loss: 0.00092421
Iteration 7/25 | Loss: 0.00092421
Iteration 8/25 | Loss: 0.00092421
Iteration 9/25 | Loss: 0.00092421
Iteration 10/25 | Loss: 0.00092421
Iteration 11/25 | Loss: 0.00092421
Iteration 12/25 | Loss: 0.00092421
Iteration 13/25 | Loss: 0.00092421
Iteration 14/25 | Loss: 0.00092421
Iteration 15/25 | Loss: 0.00092421
Iteration 16/25 | Loss: 0.00092421
Iteration 17/25 | Loss: 0.00092421
Iteration 18/25 | Loss: 0.00092421
Iteration 19/25 | Loss: 0.00092421
Iteration 20/25 | Loss: 0.00092421
Iteration 21/25 | Loss: 0.00092421
Iteration 22/25 | Loss: 0.00092421
Iteration 23/25 | Loss: 0.00092421
Iteration 24/25 | Loss: 0.00092421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009242078522220254, 0.0009242078522220254, 0.0009242078522220254, 0.0009242078522220254, 0.0009242078522220254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009242078522220254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092421
Iteration 2/1000 | Loss: 0.00003074
Iteration 3/1000 | Loss: 0.00001933
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00001393
Iteration 6/1000 | Loss: 0.00001314
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001158
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001121
Iteration 32/1000 | Loss: 0.00001121
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001116
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001096
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001093
Iteration 98/1000 | Loss: 0.00001093
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001090
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001083
Iteration 118/1000 | Loss: 0.00001083
Iteration 119/1000 | Loss: 0.00001083
Iteration 120/1000 | Loss: 0.00001083
Iteration 121/1000 | Loss: 0.00001082
Iteration 122/1000 | Loss: 0.00001082
Iteration 123/1000 | Loss: 0.00001082
Iteration 124/1000 | Loss: 0.00001082
Iteration 125/1000 | Loss: 0.00001082
Iteration 126/1000 | Loss: 0.00001082
Iteration 127/1000 | Loss: 0.00001082
Iteration 128/1000 | Loss: 0.00001082
Iteration 129/1000 | Loss: 0.00001082
Iteration 130/1000 | Loss: 0.00001082
Iteration 131/1000 | Loss: 0.00001082
Iteration 132/1000 | Loss: 0.00001082
Iteration 133/1000 | Loss: 0.00001082
Iteration 134/1000 | Loss: 0.00001082
Iteration 135/1000 | Loss: 0.00001082
Iteration 136/1000 | Loss: 0.00001082
Iteration 137/1000 | Loss: 0.00001082
Iteration 138/1000 | Loss: 0.00001082
Iteration 139/1000 | Loss: 0.00001081
Iteration 140/1000 | Loss: 0.00001080
Iteration 141/1000 | Loss: 0.00001080
Iteration 142/1000 | Loss: 0.00001080
Iteration 143/1000 | Loss: 0.00001080
Iteration 144/1000 | Loss: 0.00001080
Iteration 145/1000 | Loss: 0.00001080
Iteration 146/1000 | Loss: 0.00001080
Iteration 147/1000 | Loss: 0.00001079
Iteration 148/1000 | Loss: 0.00001079
Iteration 149/1000 | Loss: 0.00001079
Iteration 150/1000 | Loss: 0.00001079
Iteration 151/1000 | Loss: 0.00001079
Iteration 152/1000 | Loss: 0.00001079
Iteration 153/1000 | Loss: 0.00001079
Iteration 154/1000 | Loss: 0.00001079
Iteration 155/1000 | Loss: 0.00001078
Iteration 156/1000 | Loss: 0.00001078
Iteration 157/1000 | Loss: 0.00001078
Iteration 158/1000 | Loss: 0.00001078
Iteration 159/1000 | Loss: 0.00001078
Iteration 160/1000 | Loss: 0.00001078
Iteration 161/1000 | Loss: 0.00001078
Iteration 162/1000 | Loss: 0.00001078
Iteration 163/1000 | Loss: 0.00001078
Iteration 164/1000 | Loss: 0.00001078
Iteration 165/1000 | Loss: 0.00001078
Iteration 166/1000 | Loss: 0.00001078
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001078
Iteration 169/1000 | Loss: 0.00001078
Iteration 170/1000 | Loss: 0.00001078
Iteration 171/1000 | Loss: 0.00001078
Iteration 172/1000 | Loss: 0.00001078
Iteration 173/1000 | Loss: 0.00001078
Iteration 174/1000 | Loss: 0.00001078
Iteration 175/1000 | Loss: 0.00001078
Iteration 176/1000 | Loss: 0.00001078
Iteration 177/1000 | Loss: 0.00001078
Iteration 178/1000 | Loss: 0.00001078
Iteration 179/1000 | Loss: 0.00001078
Iteration 180/1000 | Loss: 0.00001078
Iteration 181/1000 | Loss: 0.00001078
Iteration 182/1000 | Loss: 0.00001078
Iteration 183/1000 | Loss: 0.00001078
Iteration 184/1000 | Loss: 0.00001078
Iteration 185/1000 | Loss: 0.00001078
Iteration 186/1000 | Loss: 0.00001078
Iteration 187/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0775280316011049e-05, 1.0775280316011049e-05, 1.0775280316011049e-05, 1.0775280316011049e-05, 1.0775280316011049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0775280316011049e-05

Optimization complete. Final v2v error: 2.8288447856903076 mm

Highest mean error: 2.9135122299194336 mm for frame 73

Lowest mean error: 2.769214630126953 mm for frame 108

Saving results

Total time: 38.221245765686035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474632
Iteration 2/25 | Loss: 0.00134534
Iteration 3/25 | Loss: 0.00127018
Iteration 4/25 | Loss: 0.00125852
Iteration 5/25 | Loss: 0.00125476
Iteration 6/25 | Loss: 0.00125441
Iteration 7/25 | Loss: 0.00125441
Iteration 8/25 | Loss: 0.00125441
Iteration 9/25 | Loss: 0.00125441
Iteration 10/25 | Loss: 0.00125441
Iteration 11/25 | Loss: 0.00125441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012544083874672651, 0.0012544083874672651, 0.0012544083874672651, 0.0012544083874672651, 0.0012544083874672651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012544083874672651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51206732
Iteration 2/25 | Loss: 0.00080259
Iteration 3/25 | Loss: 0.00080259
Iteration 4/25 | Loss: 0.00080259
Iteration 5/25 | Loss: 0.00080259
Iteration 6/25 | Loss: 0.00080259
Iteration 7/25 | Loss: 0.00080259
Iteration 8/25 | Loss: 0.00080258
Iteration 9/25 | Loss: 0.00080258
Iteration 10/25 | Loss: 0.00080258
Iteration 11/25 | Loss: 0.00080258
Iteration 12/25 | Loss: 0.00080258
Iteration 13/25 | Loss: 0.00080258
Iteration 14/25 | Loss: 0.00080258
Iteration 15/25 | Loss: 0.00080258
Iteration 16/25 | Loss: 0.00080258
Iteration 17/25 | Loss: 0.00080258
Iteration 18/25 | Loss: 0.00080258
Iteration 19/25 | Loss: 0.00080258
Iteration 20/25 | Loss: 0.00080258
Iteration 21/25 | Loss: 0.00080258
Iteration 22/25 | Loss: 0.00080258
Iteration 23/25 | Loss: 0.00080258
Iteration 24/25 | Loss: 0.00080258
Iteration 25/25 | Loss: 0.00080258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000802583759650588, 0.000802583759650588, 0.000802583759650588, 0.000802583759650588, 0.000802583759650588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000802583759650588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080258
Iteration 2/1000 | Loss: 0.00002535
Iteration 3/1000 | Loss: 0.00001752
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001461
Iteration 7/1000 | Loss: 0.00001420
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001351
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001340
Iteration 15/1000 | Loss: 0.00001338
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001295
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001294
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001291
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001290
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001284
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001277
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001274
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001272
Iteration 54/1000 | Loss: 0.00001272
Iteration 55/1000 | Loss: 0.00001272
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001271
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001270
Iteration 61/1000 | Loss: 0.00001270
Iteration 62/1000 | Loss: 0.00001270
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001269
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001267
Iteration 86/1000 | Loss: 0.00001267
Iteration 87/1000 | Loss: 0.00001266
Iteration 88/1000 | Loss: 0.00001266
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001264
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001263
Iteration 95/1000 | Loss: 0.00001263
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001261
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001255
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001252
Iteration 141/1000 | Loss: 0.00001252
Iteration 142/1000 | Loss: 0.00001252
Iteration 143/1000 | Loss: 0.00001252
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001252
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001251
Iteration 149/1000 | Loss: 0.00001251
Iteration 150/1000 | Loss: 0.00001251
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001251
Iteration 154/1000 | Loss: 0.00001251
Iteration 155/1000 | Loss: 0.00001251
Iteration 156/1000 | Loss: 0.00001251
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001250
Iteration 160/1000 | Loss: 0.00001250
Iteration 161/1000 | Loss: 0.00001250
Iteration 162/1000 | Loss: 0.00001250
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Iteration 165/1000 | Loss: 0.00001249
Iteration 166/1000 | Loss: 0.00001249
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001249
Iteration 174/1000 | Loss: 0.00001249
Iteration 175/1000 | Loss: 0.00001249
Iteration 176/1000 | Loss: 0.00001249
Iteration 177/1000 | Loss: 0.00001249
Iteration 178/1000 | Loss: 0.00001249
Iteration 179/1000 | Loss: 0.00001249
Iteration 180/1000 | Loss: 0.00001249
Iteration 181/1000 | Loss: 0.00001249
Iteration 182/1000 | Loss: 0.00001249
Iteration 183/1000 | Loss: 0.00001249
Iteration 184/1000 | Loss: 0.00001249
Iteration 185/1000 | Loss: 0.00001249
Iteration 186/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2494479051383678e-05, 1.2494479051383678e-05, 1.2494479051383678e-05, 1.2494479051383678e-05, 1.2494479051383678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2494479051383678e-05

Optimization complete. Final v2v error: 3.0405306816101074 mm

Highest mean error: 3.3628523349761963 mm for frame 37

Lowest mean error: 2.895749807357788 mm for frame 74

Saving results

Total time: 39.75517702102661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442651
Iteration 2/25 | Loss: 0.00149184
Iteration 3/25 | Loss: 0.00134765
Iteration 4/25 | Loss: 0.00133739
Iteration 5/25 | Loss: 0.00133559
Iteration 6/25 | Loss: 0.00133536
Iteration 7/25 | Loss: 0.00133536
Iteration 8/25 | Loss: 0.00133536
Iteration 9/25 | Loss: 0.00133536
Iteration 10/25 | Loss: 0.00133536
Iteration 11/25 | Loss: 0.00133534
Iteration 12/25 | Loss: 0.00133534
Iteration 13/25 | Loss: 0.00133534
Iteration 14/25 | Loss: 0.00133534
Iteration 15/25 | Loss: 0.00133534
Iteration 16/25 | Loss: 0.00133534
Iteration 17/25 | Loss: 0.00133534
Iteration 18/25 | Loss: 0.00133534
Iteration 19/25 | Loss: 0.00133534
Iteration 20/25 | Loss: 0.00133534
Iteration 21/25 | Loss: 0.00133534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013353442773222923, 0.0013353442773222923, 0.0013353442773222923, 0.0013353442773222923, 0.0013353442773222923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013353442773222923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.30930662
Iteration 2/25 | Loss: 0.00086730
Iteration 3/25 | Loss: 0.00086730
Iteration 4/25 | Loss: 0.00086730
Iteration 5/25 | Loss: 0.00086730
Iteration 6/25 | Loss: 0.00086730
Iteration 7/25 | Loss: 0.00086730
Iteration 8/25 | Loss: 0.00086730
Iteration 9/25 | Loss: 0.00086730
Iteration 10/25 | Loss: 0.00086730
Iteration 11/25 | Loss: 0.00086730
Iteration 12/25 | Loss: 0.00086730
Iteration 13/25 | Loss: 0.00086730
Iteration 14/25 | Loss: 0.00086730
Iteration 15/25 | Loss: 0.00086730
Iteration 16/25 | Loss: 0.00086730
Iteration 17/25 | Loss: 0.00086729
Iteration 18/25 | Loss: 0.00086729
Iteration 19/25 | Loss: 0.00086729
Iteration 20/25 | Loss: 0.00086729
Iteration 21/25 | Loss: 0.00086729
Iteration 22/25 | Loss: 0.00086729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008672949625179172, 0.0008672949625179172, 0.0008672949625179172, 0.0008672949625179172, 0.0008672949625179172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008672949625179172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086729
Iteration 2/1000 | Loss: 0.00003725
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002169
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001817
Iteration 9/1000 | Loss: 0.00001769
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001716
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001670
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001665
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001664
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001651
Iteration 34/1000 | Loss: 0.00001651
Iteration 35/1000 | Loss: 0.00001649
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001638
Iteration 56/1000 | Loss: 0.00001636
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001635
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001633
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001632
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001632
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001629
Iteration 91/1000 | Loss: 0.00001629
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001626
Iteration 99/1000 | Loss: 0.00001626
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00001625
Iteration 103/1000 | Loss: 0.00001625
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001624
Iteration 106/1000 | Loss: 0.00001624
Iteration 107/1000 | Loss: 0.00001624
Iteration 108/1000 | Loss: 0.00001624
Iteration 109/1000 | Loss: 0.00001624
Iteration 110/1000 | Loss: 0.00001623
Iteration 111/1000 | Loss: 0.00001623
Iteration 112/1000 | Loss: 0.00001623
Iteration 113/1000 | Loss: 0.00001623
Iteration 114/1000 | Loss: 0.00001623
Iteration 115/1000 | Loss: 0.00001623
Iteration 116/1000 | Loss: 0.00001623
Iteration 117/1000 | Loss: 0.00001623
Iteration 118/1000 | Loss: 0.00001623
Iteration 119/1000 | Loss: 0.00001623
Iteration 120/1000 | Loss: 0.00001623
Iteration 121/1000 | Loss: 0.00001623
Iteration 122/1000 | Loss: 0.00001622
Iteration 123/1000 | Loss: 0.00001622
Iteration 124/1000 | Loss: 0.00001622
Iteration 125/1000 | Loss: 0.00001621
Iteration 126/1000 | Loss: 0.00001621
Iteration 127/1000 | Loss: 0.00001621
Iteration 128/1000 | Loss: 0.00001621
Iteration 129/1000 | Loss: 0.00001620
Iteration 130/1000 | Loss: 0.00001620
Iteration 131/1000 | Loss: 0.00001620
Iteration 132/1000 | Loss: 0.00001620
Iteration 133/1000 | Loss: 0.00001620
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001619
Iteration 137/1000 | Loss: 0.00001619
Iteration 138/1000 | Loss: 0.00001619
Iteration 139/1000 | Loss: 0.00001619
Iteration 140/1000 | Loss: 0.00001619
Iteration 141/1000 | Loss: 0.00001619
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001618
Iteration 144/1000 | Loss: 0.00001618
Iteration 145/1000 | Loss: 0.00001618
Iteration 146/1000 | Loss: 0.00001618
Iteration 147/1000 | Loss: 0.00001618
Iteration 148/1000 | Loss: 0.00001617
Iteration 149/1000 | Loss: 0.00001617
Iteration 150/1000 | Loss: 0.00001617
Iteration 151/1000 | Loss: 0.00001617
Iteration 152/1000 | Loss: 0.00001617
Iteration 153/1000 | Loss: 0.00001617
Iteration 154/1000 | Loss: 0.00001617
Iteration 155/1000 | Loss: 0.00001617
Iteration 156/1000 | Loss: 0.00001617
Iteration 157/1000 | Loss: 0.00001617
Iteration 158/1000 | Loss: 0.00001617
Iteration 159/1000 | Loss: 0.00001617
Iteration 160/1000 | Loss: 0.00001616
Iteration 161/1000 | Loss: 0.00001616
Iteration 162/1000 | Loss: 0.00001616
Iteration 163/1000 | Loss: 0.00001616
Iteration 164/1000 | Loss: 0.00001616
Iteration 165/1000 | Loss: 0.00001616
Iteration 166/1000 | Loss: 0.00001616
Iteration 167/1000 | Loss: 0.00001616
Iteration 168/1000 | Loss: 0.00001616
Iteration 169/1000 | Loss: 0.00001616
Iteration 170/1000 | Loss: 0.00001616
Iteration 171/1000 | Loss: 0.00001616
Iteration 172/1000 | Loss: 0.00001616
Iteration 173/1000 | Loss: 0.00001616
Iteration 174/1000 | Loss: 0.00001616
Iteration 175/1000 | Loss: 0.00001616
Iteration 176/1000 | Loss: 0.00001616
Iteration 177/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.616488953004591e-05, 1.616488953004591e-05, 1.616488953004591e-05, 1.616488953004591e-05, 1.616488953004591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.616488953004591e-05

Optimization complete. Final v2v error: 3.3873558044433594 mm

Highest mean error: 4.123078346252441 mm for frame 71

Lowest mean error: 2.9724066257476807 mm for frame 102

Saving results

Total time: 38.95112752914429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794352
Iteration 2/25 | Loss: 0.00166247
Iteration 3/25 | Loss: 0.00152518
Iteration 4/25 | Loss: 0.00151407
Iteration 5/25 | Loss: 0.00151086
Iteration 6/25 | Loss: 0.00151070
Iteration 7/25 | Loss: 0.00151070
Iteration 8/25 | Loss: 0.00151070
Iteration 9/25 | Loss: 0.00151070
Iteration 10/25 | Loss: 0.00151070
Iteration 11/25 | Loss: 0.00151070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015107024228200316, 0.0015107024228200316, 0.0015107024228200316, 0.0015107024228200316, 0.0015107024228200316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015107024228200316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.03293324
Iteration 2/25 | Loss: 0.00140023
Iteration 3/25 | Loss: 0.00140023
Iteration 4/25 | Loss: 0.00140023
Iteration 5/25 | Loss: 0.00140023
Iteration 6/25 | Loss: 0.00140023
Iteration 7/25 | Loss: 0.00140023
Iteration 8/25 | Loss: 0.00140023
Iteration 9/25 | Loss: 0.00140023
Iteration 10/25 | Loss: 0.00140023
Iteration 11/25 | Loss: 0.00140023
Iteration 12/25 | Loss: 0.00140023
Iteration 13/25 | Loss: 0.00140023
Iteration 14/25 | Loss: 0.00140023
Iteration 15/25 | Loss: 0.00140023
Iteration 16/25 | Loss: 0.00140023
Iteration 17/25 | Loss: 0.00140023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014002290554344654, 0.0014002290554344654, 0.0014002290554344654, 0.0014002290554344654, 0.0014002290554344654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014002290554344654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140023
Iteration 2/1000 | Loss: 0.00005885
Iteration 3/1000 | Loss: 0.00003679
Iteration 4/1000 | Loss: 0.00003132
Iteration 5/1000 | Loss: 0.00002965
Iteration 6/1000 | Loss: 0.00002832
Iteration 7/1000 | Loss: 0.00002737
Iteration 8/1000 | Loss: 0.00002681
Iteration 9/1000 | Loss: 0.00002653
Iteration 10/1000 | Loss: 0.00002631
Iteration 11/1000 | Loss: 0.00002624
Iteration 12/1000 | Loss: 0.00002615
Iteration 13/1000 | Loss: 0.00002610
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002609
Iteration 16/1000 | Loss: 0.00002608
Iteration 17/1000 | Loss: 0.00002604
Iteration 18/1000 | Loss: 0.00002596
Iteration 19/1000 | Loss: 0.00002596
Iteration 20/1000 | Loss: 0.00002591
Iteration 21/1000 | Loss: 0.00002583
Iteration 22/1000 | Loss: 0.00002583
Iteration 23/1000 | Loss: 0.00002583
Iteration 24/1000 | Loss: 0.00002582
Iteration 25/1000 | Loss: 0.00002582
Iteration 26/1000 | Loss: 0.00002581
Iteration 27/1000 | Loss: 0.00002580
Iteration 28/1000 | Loss: 0.00002580
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00002580
Iteration 31/1000 | Loss: 0.00002580
Iteration 32/1000 | Loss: 0.00002580
Iteration 33/1000 | Loss: 0.00002580
Iteration 34/1000 | Loss: 0.00002580
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002579
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00002578
Iteration 39/1000 | Loss: 0.00002578
Iteration 40/1000 | Loss: 0.00002577
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002577
Iteration 43/1000 | Loss: 0.00002577
Iteration 44/1000 | Loss: 0.00002577
Iteration 45/1000 | Loss: 0.00002577
Iteration 46/1000 | Loss: 0.00002577
Iteration 47/1000 | Loss: 0.00002577
Iteration 48/1000 | Loss: 0.00002577
Iteration 49/1000 | Loss: 0.00002577
Iteration 50/1000 | Loss: 0.00002577
Iteration 51/1000 | Loss: 0.00002577
Iteration 52/1000 | Loss: 0.00002576
Iteration 53/1000 | Loss: 0.00002576
Iteration 54/1000 | Loss: 0.00002576
Iteration 55/1000 | Loss: 0.00002575
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002575
Iteration 58/1000 | Loss: 0.00002575
Iteration 59/1000 | Loss: 0.00002575
Iteration 60/1000 | Loss: 0.00002575
Iteration 61/1000 | Loss: 0.00002575
Iteration 62/1000 | Loss: 0.00002575
Iteration 63/1000 | Loss: 0.00002575
Iteration 64/1000 | Loss: 0.00002575
Iteration 65/1000 | Loss: 0.00002575
Iteration 66/1000 | Loss: 0.00002575
Iteration 67/1000 | Loss: 0.00002575
Iteration 68/1000 | Loss: 0.00002575
Iteration 69/1000 | Loss: 0.00002575
Iteration 70/1000 | Loss: 0.00002574
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00002574
Iteration 73/1000 | Loss: 0.00002574
Iteration 74/1000 | Loss: 0.00002574
Iteration 75/1000 | Loss: 0.00002574
Iteration 76/1000 | Loss: 0.00002574
Iteration 77/1000 | Loss: 0.00002574
Iteration 78/1000 | Loss: 0.00002574
Iteration 79/1000 | Loss: 0.00002574
Iteration 80/1000 | Loss: 0.00002574
Iteration 81/1000 | Loss: 0.00002574
Iteration 82/1000 | Loss: 0.00002573
Iteration 83/1000 | Loss: 0.00002573
Iteration 84/1000 | Loss: 0.00002573
Iteration 85/1000 | Loss: 0.00002573
Iteration 86/1000 | Loss: 0.00002573
Iteration 87/1000 | Loss: 0.00002573
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002573
Iteration 91/1000 | Loss: 0.00002573
Iteration 92/1000 | Loss: 0.00002573
Iteration 93/1000 | Loss: 0.00002573
Iteration 94/1000 | Loss: 0.00002573
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002573
Iteration 97/1000 | Loss: 0.00002573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.572656376287341e-05, 2.572656376287341e-05, 2.572656376287341e-05, 2.572656376287341e-05, 2.572656376287341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.572656376287341e-05

Optimization complete. Final v2v error: 4.26898193359375 mm

Highest mean error: 4.789706230163574 mm for frame 114

Lowest mean error: 3.888293504714966 mm for frame 175

Saving results

Total time: 31.8454487323761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824484
Iteration 2/25 | Loss: 0.00193841
Iteration 3/25 | Loss: 0.00166354
Iteration 4/25 | Loss: 0.00165080
Iteration 5/25 | Loss: 0.00164895
Iteration 6/25 | Loss: 0.00164895
Iteration 7/25 | Loss: 0.00164895
Iteration 8/25 | Loss: 0.00164895
Iteration 9/25 | Loss: 0.00164895
Iteration 10/25 | Loss: 0.00164895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0016489468980580568, 0.0016489468980580568, 0.0016489468980580568, 0.0016489468980580568, 0.0016489468980580568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016489468980580568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.38361880
Iteration 2/25 | Loss: 0.00127289
Iteration 3/25 | Loss: 0.00127288
Iteration 4/25 | Loss: 0.00127288
Iteration 5/25 | Loss: 0.00127288
Iteration 6/25 | Loss: 0.00127288
Iteration 7/25 | Loss: 0.00127288
Iteration 8/25 | Loss: 0.00127288
Iteration 9/25 | Loss: 0.00127288
Iteration 10/25 | Loss: 0.00127288
Iteration 11/25 | Loss: 0.00127288
Iteration 12/25 | Loss: 0.00127288
Iteration 13/25 | Loss: 0.00127288
Iteration 14/25 | Loss: 0.00127288
Iteration 15/25 | Loss: 0.00127288
Iteration 16/25 | Loss: 0.00127288
Iteration 17/25 | Loss: 0.00127288
Iteration 18/25 | Loss: 0.00127288
Iteration 19/25 | Loss: 0.00127288
Iteration 20/25 | Loss: 0.00127288
Iteration 21/25 | Loss: 0.00127288
Iteration 22/25 | Loss: 0.00127288
Iteration 23/25 | Loss: 0.00127288
Iteration 24/25 | Loss: 0.00127288
Iteration 25/25 | Loss: 0.00127288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127288
Iteration 2/1000 | Loss: 0.00010689
Iteration 3/1000 | Loss: 0.00007332
Iteration 4/1000 | Loss: 0.00005658
Iteration 5/1000 | Loss: 0.00005253
Iteration 6/1000 | Loss: 0.00005031
Iteration 7/1000 | Loss: 0.00004912
Iteration 8/1000 | Loss: 0.00004742
Iteration 9/1000 | Loss: 0.00004601
Iteration 10/1000 | Loss: 0.00004511
Iteration 11/1000 | Loss: 0.00004444
Iteration 12/1000 | Loss: 0.00004379
Iteration 13/1000 | Loss: 0.00004325
Iteration 14/1000 | Loss: 0.00004267
Iteration 15/1000 | Loss: 0.00004235
Iteration 16/1000 | Loss: 0.00004208
Iteration 17/1000 | Loss: 0.00004190
Iteration 18/1000 | Loss: 0.00004176
Iteration 19/1000 | Loss: 0.00004172
Iteration 20/1000 | Loss: 0.00004163
Iteration 21/1000 | Loss: 0.00004159
Iteration 22/1000 | Loss: 0.00004158
Iteration 23/1000 | Loss: 0.00004156
Iteration 24/1000 | Loss: 0.00004155
Iteration 25/1000 | Loss: 0.00004155
Iteration 26/1000 | Loss: 0.00004154
Iteration 27/1000 | Loss: 0.00004153
Iteration 28/1000 | Loss: 0.00004150
Iteration 29/1000 | Loss: 0.00004149
Iteration 30/1000 | Loss: 0.00004149
Iteration 31/1000 | Loss: 0.00004149
Iteration 32/1000 | Loss: 0.00004147
Iteration 33/1000 | Loss: 0.00004146
Iteration 34/1000 | Loss: 0.00004146
Iteration 35/1000 | Loss: 0.00004145
Iteration 36/1000 | Loss: 0.00004144
Iteration 37/1000 | Loss: 0.00004144
Iteration 38/1000 | Loss: 0.00004143
Iteration 39/1000 | Loss: 0.00004142
Iteration 40/1000 | Loss: 0.00004141
Iteration 41/1000 | Loss: 0.00004128
Iteration 42/1000 | Loss: 0.00004120
Iteration 43/1000 | Loss: 0.00004120
Iteration 44/1000 | Loss: 0.00004120
Iteration 45/1000 | Loss: 0.00004120
Iteration 46/1000 | Loss: 0.00004120
Iteration 47/1000 | Loss: 0.00004120
Iteration 48/1000 | Loss: 0.00004119
Iteration 49/1000 | Loss: 0.00004119
Iteration 50/1000 | Loss: 0.00004119
Iteration 51/1000 | Loss: 0.00004119
Iteration 52/1000 | Loss: 0.00004119
Iteration 53/1000 | Loss: 0.00004118
Iteration 54/1000 | Loss: 0.00004117
Iteration 55/1000 | Loss: 0.00004117
Iteration 56/1000 | Loss: 0.00004117
Iteration 57/1000 | Loss: 0.00004117
Iteration 58/1000 | Loss: 0.00004117
Iteration 59/1000 | Loss: 0.00004117
Iteration 60/1000 | Loss: 0.00004117
Iteration 61/1000 | Loss: 0.00004116
Iteration 62/1000 | Loss: 0.00004116
Iteration 63/1000 | Loss: 0.00004116
Iteration 64/1000 | Loss: 0.00004116
Iteration 65/1000 | Loss: 0.00004116
Iteration 66/1000 | Loss: 0.00004116
Iteration 67/1000 | Loss: 0.00004116
Iteration 68/1000 | Loss: 0.00004116
Iteration 69/1000 | Loss: 0.00004116
Iteration 70/1000 | Loss: 0.00004116
Iteration 71/1000 | Loss: 0.00004115
Iteration 72/1000 | Loss: 0.00004115
Iteration 73/1000 | Loss: 0.00004114
Iteration 74/1000 | Loss: 0.00004114
Iteration 75/1000 | Loss: 0.00004113
Iteration 76/1000 | Loss: 0.00004113
Iteration 77/1000 | Loss: 0.00004113
Iteration 78/1000 | Loss: 0.00004113
Iteration 79/1000 | Loss: 0.00004113
Iteration 80/1000 | Loss: 0.00004113
Iteration 81/1000 | Loss: 0.00004112
Iteration 82/1000 | Loss: 0.00004112
Iteration 83/1000 | Loss: 0.00004111
Iteration 84/1000 | Loss: 0.00004111
Iteration 85/1000 | Loss: 0.00004111
Iteration 86/1000 | Loss: 0.00004111
Iteration 87/1000 | Loss: 0.00004111
Iteration 88/1000 | Loss: 0.00004111
Iteration 89/1000 | Loss: 0.00004111
Iteration 90/1000 | Loss: 0.00004111
Iteration 91/1000 | Loss: 0.00004110
Iteration 92/1000 | Loss: 0.00004110
Iteration 93/1000 | Loss: 0.00004110
Iteration 94/1000 | Loss: 0.00004110
Iteration 95/1000 | Loss: 0.00004110
Iteration 96/1000 | Loss: 0.00004110
Iteration 97/1000 | Loss: 0.00004110
Iteration 98/1000 | Loss: 0.00004110
Iteration 99/1000 | Loss: 0.00004110
Iteration 100/1000 | Loss: 0.00004109
Iteration 101/1000 | Loss: 0.00004108
Iteration 102/1000 | Loss: 0.00004108
Iteration 103/1000 | Loss: 0.00004108
Iteration 104/1000 | Loss: 0.00004108
Iteration 105/1000 | Loss: 0.00004108
Iteration 106/1000 | Loss: 0.00004108
Iteration 107/1000 | Loss: 0.00004107
Iteration 108/1000 | Loss: 0.00004107
Iteration 109/1000 | Loss: 0.00004107
Iteration 110/1000 | Loss: 0.00004107
Iteration 111/1000 | Loss: 0.00004107
Iteration 112/1000 | Loss: 0.00004107
Iteration 113/1000 | Loss: 0.00004107
Iteration 114/1000 | Loss: 0.00004107
Iteration 115/1000 | Loss: 0.00004107
Iteration 116/1000 | Loss: 0.00004107
Iteration 117/1000 | Loss: 0.00004107
Iteration 118/1000 | Loss: 0.00004106
Iteration 119/1000 | Loss: 0.00004106
Iteration 120/1000 | Loss: 0.00004106
Iteration 121/1000 | Loss: 0.00004106
Iteration 122/1000 | Loss: 0.00004106
Iteration 123/1000 | Loss: 0.00004106
Iteration 124/1000 | Loss: 0.00004106
Iteration 125/1000 | Loss: 0.00004106
Iteration 126/1000 | Loss: 0.00004105
Iteration 127/1000 | Loss: 0.00004105
Iteration 128/1000 | Loss: 0.00004105
Iteration 129/1000 | Loss: 0.00004104
Iteration 130/1000 | Loss: 0.00004104
Iteration 131/1000 | Loss: 0.00004104
Iteration 132/1000 | Loss: 0.00004104
Iteration 133/1000 | Loss: 0.00004104
Iteration 134/1000 | Loss: 0.00004104
Iteration 135/1000 | Loss: 0.00004104
Iteration 136/1000 | Loss: 0.00004103
Iteration 137/1000 | Loss: 0.00004103
Iteration 138/1000 | Loss: 0.00004103
Iteration 139/1000 | Loss: 0.00004103
Iteration 140/1000 | Loss: 0.00004103
Iteration 141/1000 | Loss: 0.00004102
Iteration 142/1000 | Loss: 0.00004102
Iteration 143/1000 | Loss: 0.00004102
Iteration 144/1000 | Loss: 0.00004102
Iteration 145/1000 | Loss: 0.00004102
Iteration 146/1000 | Loss: 0.00004102
Iteration 147/1000 | Loss: 0.00004102
Iteration 148/1000 | Loss: 0.00004102
Iteration 149/1000 | Loss: 0.00004102
Iteration 150/1000 | Loss: 0.00004102
Iteration 151/1000 | Loss: 0.00004101
Iteration 152/1000 | Loss: 0.00004101
Iteration 153/1000 | Loss: 0.00004101
Iteration 154/1000 | Loss: 0.00004101
Iteration 155/1000 | Loss: 0.00004101
Iteration 156/1000 | Loss: 0.00004101
Iteration 157/1000 | Loss: 0.00004101
Iteration 158/1000 | Loss: 0.00004101
Iteration 159/1000 | Loss: 0.00004101
Iteration 160/1000 | Loss: 0.00004101
Iteration 161/1000 | Loss: 0.00004101
Iteration 162/1000 | Loss: 0.00004100
Iteration 163/1000 | Loss: 0.00004100
Iteration 164/1000 | Loss: 0.00004100
Iteration 165/1000 | Loss: 0.00004100
Iteration 166/1000 | Loss: 0.00004100
Iteration 167/1000 | Loss: 0.00004100
Iteration 168/1000 | Loss: 0.00004100
Iteration 169/1000 | Loss: 0.00004100
Iteration 170/1000 | Loss: 0.00004100
Iteration 171/1000 | Loss: 0.00004100
Iteration 172/1000 | Loss: 0.00004100
Iteration 173/1000 | Loss: 0.00004100
Iteration 174/1000 | Loss: 0.00004100
Iteration 175/1000 | Loss: 0.00004099
Iteration 176/1000 | Loss: 0.00004099
Iteration 177/1000 | Loss: 0.00004099
Iteration 178/1000 | Loss: 0.00004099
Iteration 179/1000 | Loss: 0.00004098
Iteration 180/1000 | Loss: 0.00004098
Iteration 181/1000 | Loss: 0.00004097
Iteration 182/1000 | Loss: 0.00004097
Iteration 183/1000 | Loss: 0.00004096
Iteration 184/1000 | Loss: 0.00004096
Iteration 185/1000 | Loss: 0.00004096
Iteration 186/1000 | Loss: 0.00004096
Iteration 187/1000 | Loss: 0.00004095
Iteration 188/1000 | Loss: 0.00004095
Iteration 189/1000 | Loss: 0.00004095
Iteration 190/1000 | Loss: 0.00004095
Iteration 191/1000 | Loss: 0.00004094
Iteration 192/1000 | Loss: 0.00004094
Iteration 193/1000 | Loss: 0.00004094
Iteration 194/1000 | Loss: 0.00004094
Iteration 195/1000 | Loss: 0.00004094
Iteration 196/1000 | Loss: 0.00004094
Iteration 197/1000 | Loss: 0.00004094
Iteration 198/1000 | Loss: 0.00004094
Iteration 199/1000 | Loss: 0.00004093
Iteration 200/1000 | Loss: 0.00004093
Iteration 201/1000 | Loss: 0.00004093
Iteration 202/1000 | Loss: 0.00004093
Iteration 203/1000 | Loss: 0.00004093
Iteration 204/1000 | Loss: 0.00004093
Iteration 205/1000 | Loss: 0.00004093
Iteration 206/1000 | Loss: 0.00004093
Iteration 207/1000 | Loss: 0.00004093
Iteration 208/1000 | Loss: 0.00004093
Iteration 209/1000 | Loss: 0.00004093
Iteration 210/1000 | Loss: 0.00004093
Iteration 211/1000 | Loss: 0.00004093
Iteration 212/1000 | Loss: 0.00004093
Iteration 213/1000 | Loss: 0.00004092
Iteration 214/1000 | Loss: 0.00004092
Iteration 215/1000 | Loss: 0.00004092
Iteration 216/1000 | Loss: 0.00004092
Iteration 217/1000 | Loss: 0.00004092
Iteration 218/1000 | Loss: 0.00004092
Iteration 219/1000 | Loss: 0.00004092
Iteration 220/1000 | Loss: 0.00004092
Iteration 221/1000 | Loss: 0.00004092
Iteration 222/1000 | Loss: 0.00004092
Iteration 223/1000 | Loss: 0.00004092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [4.092120434506796e-05, 4.092120434506796e-05, 4.092120434506796e-05, 4.092120434506796e-05, 4.092120434506796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.092120434506796e-05

Optimization complete. Final v2v error: 5.213202953338623 mm

Highest mean error: 5.365584373474121 mm for frame 93

Lowest mean error: 5.102207660675049 mm for frame 74

Saving results

Total time: 50.78512358665466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402032
Iteration 2/25 | Loss: 0.00136518
Iteration 3/25 | Loss: 0.00129314
Iteration 4/25 | Loss: 0.00128736
Iteration 5/25 | Loss: 0.00128569
Iteration 6/25 | Loss: 0.00128569
Iteration 7/25 | Loss: 0.00128569
Iteration 8/25 | Loss: 0.00128569
Iteration 9/25 | Loss: 0.00128569
Iteration 10/25 | Loss: 0.00128569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012856858083978295, 0.0012856858083978295, 0.0012856858083978295, 0.0012856858083978295, 0.0012856858083978295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012856858083978295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64259517
Iteration 2/25 | Loss: 0.00072454
Iteration 3/25 | Loss: 0.00072454
Iteration 4/25 | Loss: 0.00072454
Iteration 5/25 | Loss: 0.00072454
Iteration 6/25 | Loss: 0.00072454
Iteration 7/25 | Loss: 0.00072454
Iteration 8/25 | Loss: 0.00072454
Iteration 9/25 | Loss: 0.00072454
Iteration 10/25 | Loss: 0.00072454
Iteration 11/25 | Loss: 0.00072454
Iteration 12/25 | Loss: 0.00072454
Iteration 13/25 | Loss: 0.00072454
Iteration 14/25 | Loss: 0.00072454
Iteration 15/25 | Loss: 0.00072454
Iteration 16/25 | Loss: 0.00072454
Iteration 17/25 | Loss: 0.00072454
Iteration 18/25 | Loss: 0.00072454
Iteration 19/25 | Loss: 0.00072454
Iteration 20/25 | Loss: 0.00072454
Iteration 21/25 | Loss: 0.00072454
Iteration 22/25 | Loss: 0.00072454
Iteration 23/25 | Loss: 0.00072454
Iteration 24/25 | Loss: 0.00072454
Iteration 25/25 | Loss: 0.00072454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072454
Iteration 2/1000 | Loss: 0.00003454
Iteration 3/1000 | Loss: 0.00001982
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001369
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001320
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001319
Iteration 30/1000 | Loss: 0.00001318
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001316
Iteration 38/1000 | Loss: 0.00001316
Iteration 39/1000 | Loss: 0.00001315
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001309
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001303
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001303
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.3025991393078584e-05, 1.3025991393078584e-05, 1.3025991393078584e-05, 1.3025991393078584e-05, 1.3025991393078584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3025991393078584e-05

Optimization complete. Final v2v error: 3.0786705017089844 mm

Highest mean error: 3.394717216491699 mm for frame 235

Lowest mean error: 2.811614513397217 mm for frame 259

Saving results

Total time: 38.90489101409912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034118
Iteration 2/25 | Loss: 0.00184327
Iteration 3/25 | Loss: 0.00161248
Iteration 4/25 | Loss: 0.00133419
Iteration 5/25 | Loss: 0.00132408
Iteration 6/25 | Loss: 0.00129503
Iteration 7/25 | Loss: 0.00129454
Iteration 8/25 | Loss: 0.00129088
Iteration 9/25 | Loss: 0.00128879
Iteration 10/25 | Loss: 0.00128776
Iteration 11/25 | Loss: 0.00128735
Iteration 12/25 | Loss: 0.00128720
Iteration 13/25 | Loss: 0.00128711
Iteration 14/25 | Loss: 0.00128711
Iteration 15/25 | Loss: 0.00128710
Iteration 16/25 | Loss: 0.00128710
Iteration 17/25 | Loss: 0.00128710
Iteration 18/25 | Loss: 0.00128710
Iteration 19/25 | Loss: 0.00128710
Iteration 20/25 | Loss: 0.00128710
Iteration 21/25 | Loss: 0.00128710
Iteration 22/25 | Loss: 0.00128710
Iteration 23/25 | Loss: 0.00128710
Iteration 24/25 | Loss: 0.00128710
Iteration 25/25 | Loss: 0.00128710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.61092281
Iteration 2/25 | Loss: 0.00115102
Iteration 3/25 | Loss: 0.00092373
Iteration 4/25 | Loss: 0.00092372
Iteration 5/25 | Loss: 0.00092372
Iteration 6/25 | Loss: 0.00092372
Iteration 7/25 | Loss: 0.00092372
Iteration 8/25 | Loss: 0.00092372
Iteration 9/25 | Loss: 0.00092372
Iteration 10/25 | Loss: 0.00092372
Iteration 11/25 | Loss: 0.00092372
Iteration 12/25 | Loss: 0.00092372
Iteration 13/25 | Loss: 0.00092372
Iteration 14/25 | Loss: 0.00092372
Iteration 15/25 | Loss: 0.00092372
Iteration 16/25 | Loss: 0.00092372
Iteration 17/25 | Loss: 0.00092372
Iteration 18/25 | Loss: 0.00092372
Iteration 19/25 | Loss: 0.00092372
Iteration 20/25 | Loss: 0.00092372
Iteration 21/25 | Loss: 0.00092372
Iteration 22/25 | Loss: 0.00092372
Iteration 23/25 | Loss: 0.00092372
Iteration 24/25 | Loss: 0.00092372
Iteration 25/25 | Loss: 0.00092372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092372
Iteration 2/1000 | Loss: 0.00026298
Iteration 3/1000 | Loss: 0.00011333
Iteration 4/1000 | Loss: 0.00001923
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00011898
Iteration 7/1000 | Loss: 0.00071477
Iteration 8/1000 | Loss: 0.00006731
Iteration 9/1000 | Loss: 0.00002602
Iteration 10/1000 | Loss: 0.00003116
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001588
Iteration 15/1000 | Loss: 0.00006182
Iteration 16/1000 | Loss: 0.00010034
Iteration 17/1000 | Loss: 0.00009751
Iteration 18/1000 | Loss: 0.00002104
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00004104
Iteration 21/1000 | Loss: 0.00013490
Iteration 22/1000 | Loss: 0.00026716
Iteration 23/1000 | Loss: 0.00013706
Iteration 24/1000 | Loss: 0.00003360
Iteration 25/1000 | Loss: 0.00002446
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001513
Iteration 29/1000 | Loss: 0.00004364
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00008790
Iteration 32/1000 | Loss: 0.00002373
Iteration 33/1000 | Loss: 0.00001501
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001496
Iteration 37/1000 | Loss: 0.00002648
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00003012
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001482
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001482
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001481
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00006441
Iteration 83/1000 | Loss: 0.00001781
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00003884
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001471
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001466
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001464
Iteration 146/1000 | Loss: 0.00001464
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001463
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00006684
Iteration 159/1000 | Loss: 0.00001482
Iteration 160/1000 | Loss: 0.00001476
Iteration 161/1000 | Loss: 0.00001468
Iteration 162/1000 | Loss: 0.00001468
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001459
Iteration 176/1000 | Loss: 0.00001459
Iteration 177/1000 | Loss: 0.00001459
Iteration 178/1000 | Loss: 0.00001459
Iteration 179/1000 | Loss: 0.00001459
Iteration 180/1000 | Loss: 0.00001459
Iteration 181/1000 | Loss: 0.00001459
Iteration 182/1000 | Loss: 0.00001459
Iteration 183/1000 | Loss: 0.00001459
Iteration 184/1000 | Loss: 0.00001459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4593349078495521e-05, 1.4593349078495521e-05, 1.4593349078495521e-05, 1.4593349078495521e-05, 1.4593349078495521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4593349078495521e-05

Optimization complete. Final v2v error: 3.262071132659912 mm

Highest mean error: 3.6677517890930176 mm for frame 9

Lowest mean error: 2.9447925090789795 mm for frame 67

Saving results

Total time: 91.84549736976624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915861
Iteration 2/25 | Loss: 0.00212824
Iteration 3/25 | Loss: 0.00164802
Iteration 4/25 | Loss: 0.00158508
Iteration 5/25 | Loss: 0.00160725
Iteration 6/25 | Loss: 0.00158338
Iteration 7/25 | Loss: 0.00155051
Iteration 8/25 | Loss: 0.00152469
Iteration 9/25 | Loss: 0.00151490
Iteration 10/25 | Loss: 0.00151028
Iteration 11/25 | Loss: 0.00150837
Iteration 12/25 | Loss: 0.00150774
Iteration 13/25 | Loss: 0.00151811
Iteration 14/25 | Loss: 0.00151633
Iteration 15/25 | Loss: 0.00151468
Iteration 16/25 | Loss: 0.00151243
Iteration 17/25 | Loss: 0.00150914
Iteration 18/25 | Loss: 0.00151390
Iteration 19/25 | Loss: 0.00151071
Iteration 20/25 | Loss: 0.00151274
Iteration 21/25 | Loss: 0.00151298
Iteration 22/25 | Loss: 0.00151320
Iteration 23/25 | Loss: 0.00151012
Iteration 24/25 | Loss: 0.00151043
Iteration 25/25 | Loss: 0.00151245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37742376
Iteration 2/25 | Loss: 0.00290928
Iteration 3/25 | Loss: 0.00290928
Iteration 4/25 | Loss: 0.00290928
Iteration 5/25 | Loss: 0.00290928
Iteration 6/25 | Loss: 0.00290928
Iteration 7/25 | Loss: 0.00290928
Iteration 8/25 | Loss: 0.00290928
Iteration 9/25 | Loss: 0.00290928
Iteration 10/25 | Loss: 0.00290928
Iteration 11/25 | Loss: 0.00290928
Iteration 12/25 | Loss: 0.00290928
Iteration 13/25 | Loss: 0.00290927
Iteration 14/25 | Loss: 0.00290927
Iteration 15/25 | Loss: 0.00290927
Iteration 16/25 | Loss: 0.00290927
Iteration 17/25 | Loss: 0.00290927
Iteration 18/25 | Loss: 0.00290927
Iteration 19/25 | Loss: 0.00290927
Iteration 20/25 | Loss: 0.00290927
Iteration 21/25 | Loss: 0.00290927
Iteration 22/25 | Loss: 0.00290927
Iteration 23/25 | Loss: 0.00290927
Iteration 24/25 | Loss: 0.00290927
Iteration 25/25 | Loss: 0.00290927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290927
Iteration 2/1000 | Loss: 0.00033511
Iteration 3/1000 | Loss: 0.00126827
Iteration 4/1000 | Loss: 0.00324599
Iteration 5/1000 | Loss: 0.00073554
Iteration 6/1000 | Loss: 0.00044898
Iteration 7/1000 | Loss: 0.00028275
Iteration 8/1000 | Loss: 0.00175603
Iteration 9/1000 | Loss: 0.00038117
Iteration 10/1000 | Loss: 0.00088324
Iteration 11/1000 | Loss: 0.00069499
Iteration 12/1000 | Loss: 0.00034006
Iteration 13/1000 | Loss: 0.00060862
Iteration 14/1000 | Loss: 0.00010872
Iteration 15/1000 | Loss: 0.00062030
Iteration 16/1000 | Loss: 0.00010893
Iteration 17/1000 | Loss: 0.00235594
Iteration 18/1000 | Loss: 0.00149523
Iteration 19/1000 | Loss: 0.00007870
Iteration 20/1000 | Loss: 0.00005901
Iteration 21/1000 | Loss: 0.00004920
Iteration 22/1000 | Loss: 0.00044146
Iteration 23/1000 | Loss: 0.00004849
Iteration 24/1000 | Loss: 0.00004007
Iteration 25/1000 | Loss: 0.00003759
Iteration 26/1000 | Loss: 0.00090997
Iteration 27/1000 | Loss: 0.00004380
Iteration 28/1000 | Loss: 0.00003387
Iteration 29/1000 | Loss: 0.00003109
Iteration 30/1000 | Loss: 0.00002948
Iteration 31/1000 | Loss: 0.00002861
Iteration 32/1000 | Loss: 0.00089927
Iteration 33/1000 | Loss: 0.00005802
Iteration 34/1000 | Loss: 0.00003895
Iteration 35/1000 | Loss: 0.00002851
Iteration 36/1000 | Loss: 0.00002532
Iteration 37/1000 | Loss: 0.00002429
Iteration 38/1000 | Loss: 0.00002326
Iteration 39/1000 | Loss: 0.00002269
Iteration 40/1000 | Loss: 0.00003493
Iteration 41/1000 | Loss: 0.00003221
Iteration 42/1000 | Loss: 0.00003441
Iteration 43/1000 | Loss: 0.00003168
Iteration 44/1000 | Loss: 0.00003475
Iteration 45/1000 | Loss: 0.00003490
Iteration 46/1000 | Loss: 0.00003467
Iteration 47/1000 | Loss: 0.00003277
Iteration 48/1000 | Loss: 0.00003456
Iteration 49/1000 | Loss: 0.00003261
Iteration 50/1000 | Loss: 0.00003457
Iteration 51/1000 | Loss: 0.00002331
Iteration 52/1000 | Loss: 0.00002226
Iteration 53/1000 | Loss: 0.00002187
Iteration 54/1000 | Loss: 0.00003199
Iteration 55/1000 | Loss: 0.00016459
Iteration 56/1000 | Loss: 0.00003487
Iteration 57/1000 | Loss: 0.00008029
Iteration 58/1000 | Loss: 0.00006106
Iteration 59/1000 | Loss: 0.00004303
Iteration 60/1000 | Loss: 0.00003172
Iteration 61/1000 | Loss: 0.00006557
Iteration 62/1000 | Loss: 0.00002322
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002207
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00003486
Iteration 67/1000 | Loss: 0.00002383
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00003108
Iteration 70/1000 | Loss: 0.00002205
Iteration 71/1000 | Loss: 0.00003407
Iteration 72/1000 | Loss: 0.00003097
Iteration 73/1000 | Loss: 0.00003883
Iteration 74/1000 | Loss: 0.00003502
Iteration 75/1000 | Loss: 0.00010514
Iteration 76/1000 | Loss: 0.00002510
Iteration 77/1000 | Loss: 0.00002269
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002152
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002142
Iteration 82/1000 | Loss: 0.00002137
Iteration 83/1000 | Loss: 0.00002137
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002135
Iteration 86/1000 | Loss: 0.00002134
Iteration 87/1000 | Loss: 0.00002133
Iteration 88/1000 | Loss: 0.00002132
Iteration 89/1000 | Loss: 0.00002131
Iteration 90/1000 | Loss: 0.00002127
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002110
Iteration 93/1000 | Loss: 0.00002094
Iteration 94/1000 | Loss: 0.00002075
Iteration 95/1000 | Loss: 0.00003420
Iteration 96/1000 | Loss: 0.00002562
Iteration 97/1000 | Loss: 0.00002339
Iteration 98/1000 | Loss: 0.00002200
Iteration 99/1000 | Loss: 0.00002162
Iteration 100/1000 | Loss: 0.00002132
Iteration 101/1000 | Loss: 0.00002079
Iteration 102/1000 | Loss: 0.00002981
Iteration 103/1000 | Loss: 0.00002713
Iteration 104/1000 | Loss: 0.00003029
Iteration 105/1000 | Loss: 0.00002398
Iteration 106/1000 | Loss: 0.00002238
Iteration 107/1000 | Loss: 0.00002174
Iteration 108/1000 | Loss: 0.00002168
Iteration 109/1000 | Loss: 0.00002165
Iteration 110/1000 | Loss: 0.00002151
Iteration 111/1000 | Loss: 0.00002150
Iteration 112/1000 | Loss: 0.00002149
Iteration 113/1000 | Loss: 0.00002147
Iteration 114/1000 | Loss: 0.00002142
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002134
Iteration 117/1000 | Loss: 0.00002133
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002130
Iteration 120/1000 | Loss: 0.00002128
Iteration 121/1000 | Loss: 0.00002127
Iteration 122/1000 | Loss: 0.00002125
Iteration 123/1000 | Loss: 0.00002125
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002121
Iteration 128/1000 | Loss: 0.00002120
Iteration 129/1000 | Loss: 0.00002119
Iteration 130/1000 | Loss: 0.00002119
Iteration 131/1000 | Loss: 0.00002118
Iteration 132/1000 | Loss: 0.00002118
Iteration 133/1000 | Loss: 0.00002117
Iteration 134/1000 | Loss: 0.00002117
Iteration 135/1000 | Loss: 0.00002116
Iteration 136/1000 | Loss: 0.00002116
Iteration 137/1000 | Loss: 0.00002114
Iteration 138/1000 | Loss: 0.00002114
Iteration 139/1000 | Loss: 0.00002114
Iteration 140/1000 | Loss: 0.00002114
Iteration 141/1000 | Loss: 0.00002114
Iteration 142/1000 | Loss: 0.00002114
Iteration 143/1000 | Loss: 0.00002114
Iteration 144/1000 | Loss: 0.00002114
Iteration 145/1000 | Loss: 0.00002114
Iteration 146/1000 | Loss: 0.00002113
Iteration 147/1000 | Loss: 0.00002113
Iteration 148/1000 | Loss: 0.00002113
Iteration 149/1000 | Loss: 0.00002112
Iteration 150/1000 | Loss: 0.00002112
Iteration 151/1000 | Loss: 0.00002111
Iteration 152/1000 | Loss: 0.00002111
Iteration 153/1000 | Loss: 0.00002110
Iteration 154/1000 | Loss: 0.00002110
Iteration 155/1000 | Loss: 0.00002110
Iteration 156/1000 | Loss: 0.00002109
Iteration 157/1000 | Loss: 0.00002109
Iteration 158/1000 | Loss: 0.00002108
Iteration 159/1000 | Loss: 0.00002108
Iteration 160/1000 | Loss: 0.00002107
Iteration 161/1000 | Loss: 0.00002107
Iteration 162/1000 | Loss: 0.00002106
Iteration 163/1000 | Loss: 0.00002106
Iteration 164/1000 | Loss: 0.00002106
Iteration 165/1000 | Loss: 0.00002105
Iteration 166/1000 | Loss: 0.00002105
Iteration 167/1000 | Loss: 0.00002104
Iteration 168/1000 | Loss: 0.00002104
Iteration 169/1000 | Loss: 0.00002104
Iteration 170/1000 | Loss: 0.00002103
Iteration 171/1000 | Loss: 0.00002103
Iteration 172/1000 | Loss: 0.00002103
Iteration 173/1000 | Loss: 0.00002102
Iteration 174/1000 | Loss: 0.00002102
Iteration 175/1000 | Loss: 0.00002101
Iteration 176/1000 | Loss: 0.00002101
Iteration 177/1000 | Loss: 0.00002100
Iteration 178/1000 | Loss: 0.00002096
Iteration 179/1000 | Loss: 0.00002095
Iteration 180/1000 | Loss: 0.00002093
Iteration 181/1000 | Loss: 0.00002090
Iteration 182/1000 | Loss: 0.00002090
Iteration 183/1000 | Loss: 0.00002088
Iteration 184/1000 | Loss: 0.00002086
Iteration 185/1000 | Loss: 0.00002085
Iteration 186/1000 | Loss: 0.00002085
Iteration 187/1000 | Loss: 0.00002085
Iteration 188/1000 | Loss: 0.00002084
Iteration 189/1000 | Loss: 0.00002083
Iteration 190/1000 | Loss: 0.00002083
Iteration 191/1000 | Loss: 0.00002082
Iteration 192/1000 | Loss: 0.00002082
Iteration 193/1000 | Loss: 0.00002081
Iteration 194/1000 | Loss: 0.00002081
Iteration 195/1000 | Loss: 0.00002080
Iteration 196/1000 | Loss: 0.00002078
Iteration 197/1000 | Loss: 0.00003211
Iteration 198/1000 | Loss: 0.00002964
Iteration 199/1000 | Loss: 0.00002136
Iteration 200/1000 | Loss: 0.00003109
Iteration 201/1000 | Loss: 0.00003144
Iteration 202/1000 | Loss: 0.00003024
Iteration 203/1000 | Loss: 0.00002549
Iteration 204/1000 | Loss: 0.00003088
Iteration 205/1000 | Loss: 0.00003006
Iteration 206/1000 | Loss: 0.00003124
Iteration 207/1000 | Loss: 0.00002990
Iteration 208/1000 | Loss: 0.00003193
Iteration 209/1000 | Loss: 0.00002602
Iteration 210/1000 | Loss: 0.00003138
Iteration 211/1000 | Loss: 0.00002091
Iteration 212/1000 | Loss: 0.00002926
Iteration 213/1000 | Loss: 0.00003260
Iteration 214/1000 | Loss: 0.00002918
Iteration 215/1000 | Loss: 0.00003124
Iteration 216/1000 | Loss: 0.00002221
Iteration 217/1000 | Loss: 0.00002123
Iteration 218/1000 | Loss: 0.00002062
Iteration 219/1000 | Loss: 0.00002062
Iteration 220/1000 | Loss: 0.00002062
Iteration 221/1000 | Loss: 0.00002062
Iteration 222/1000 | Loss: 0.00002062
Iteration 223/1000 | Loss: 0.00002062
Iteration 224/1000 | Loss: 0.00002062
Iteration 225/1000 | Loss: 0.00002062
Iteration 226/1000 | Loss: 0.00002062
Iteration 227/1000 | Loss: 0.00002062
Iteration 228/1000 | Loss: 0.00002062
Iteration 229/1000 | Loss: 0.00002062
Iteration 230/1000 | Loss: 0.00002062
Iteration 231/1000 | Loss: 0.00002061
Iteration 232/1000 | Loss: 0.00002061
Iteration 233/1000 | Loss: 0.00002061
Iteration 234/1000 | Loss: 0.00002061
Iteration 235/1000 | Loss: 0.00002061
Iteration 236/1000 | Loss: 0.00002061
Iteration 237/1000 | Loss: 0.00002061
Iteration 238/1000 | Loss: 0.00002061
Iteration 239/1000 | Loss: 0.00002061
Iteration 240/1000 | Loss: 0.00002061
Iteration 241/1000 | Loss: 0.00002061
Iteration 242/1000 | Loss: 0.00002061
Iteration 243/1000 | Loss: 0.00002061
Iteration 244/1000 | Loss: 0.00002060
Iteration 245/1000 | Loss: 0.00002060
Iteration 246/1000 | Loss: 0.00002060
Iteration 247/1000 | Loss: 0.00002059
Iteration 248/1000 | Loss: 0.00002059
Iteration 249/1000 | Loss: 0.00002055
Iteration 250/1000 | Loss: 0.00002055
Iteration 251/1000 | Loss: 0.00002055
Iteration 252/1000 | Loss: 0.00002054
Iteration 253/1000 | Loss: 0.00002054
Iteration 254/1000 | Loss: 0.00002053
Iteration 255/1000 | Loss: 0.00002053
Iteration 256/1000 | Loss: 0.00002052
Iteration 257/1000 | Loss: 0.00002052
Iteration 258/1000 | Loss: 0.00002052
Iteration 259/1000 | Loss: 0.00002052
Iteration 260/1000 | Loss: 0.00002052
Iteration 261/1000 | Loss: 0.00002052
Iteration 262/1000 | Loss: 0.00002051
Iteration 263/1000 | Loss: 0.00002051
Iteration 264/1000 | Loss: 0.00002051
Iteration 265/1000 | Loss: 0.00002051
Iteration 266/1000 | Loss: 0.00002051
Iteration 267/1000 | Loss: 0.00002051
Iteration 268/1000 | Loss: 0.00002050
Iteration 269/1000 | Loss: 0.00002050
Iteration 270/1000 | Loss: 0.00002050
Iteration 271/1000 | Loss: 0.00002050
Iteration 272/1000 | Loss: 0.00002050
Iteration 273/1000 | Loss: 0.00002050
Iteration 274/1000 | Loss: 0.00002050
Iteration 275/1000 | Loss: 0.00002050
Iteration 276/1000 | Loss: 0.00002050
Iteration 277/1000 | Loss: 0.00002050
Iteration 278/1000 | Loss: 0.00002049
Iteration 279/1000 | Loss: 0.00002049
Iteration 280/1000 | Loss: 0.00002049
Iteration 281/1000 | Loss: 0.00002049
Iteration 282/1000 | Loss: 0.00002049
Iteration 283/1000 | Loss: 0.00002049
Iteration 284/1000 | Loss: 0.00002049
Iteration 285/1000 | Loss: 0.00002048
Iteration 286/1000 | Loss: 0.00002048
Iteration 287/1000 | Loss: 0.00002048
Iteration 288/1000 | Loss: 0.00002048
Iteration 289/1000 | Loss: 0.00002048
Iteration 290/1000 | Loss: 0.00002047
Iteration 291/1000 | Loss: 0.00002047
Iteration 292/1000 | Loss: 0.00002047
Iteration 293/1000 | Loss: 0.00002046
Iteration 294/1000 | Loss: 0.00002046
Iteration 295/1000 | Loss: 0.00002045
Iteration 296/1000 | Loss: 0.00002045
Iteration 297/1000 | Loss: 0.00002044
Iteration 298/1000 | Loss: 0.00002040
Iteration 299/1000 | Loss: 0.00002039
Iteration 300/1000 | Loss: 0.00002039
Iteration 301/1000 | Loss: 0.00002039
Iteration 302/1000 | Loss: 0.00002038
Iteration 303/1000 | Loss: 0.00002038
Iteration 304/1000 | Loss: 0.00002037
Iteration 305/1000 | Loss: 0.00002036
Iteration 306/1000 | Loss: 0.00002035
Iteration 307/1000 | Loss: 0.00002034
Iteration 308/1000 | Loss: 0.00002034
Iteration 309/1000 | Loss: 0.00002033
Iteration 310/1000 | Loss: 0.00002032
Iteration 311/1000 | Loss: 0.00002030
Iteration 312/1000 | Loss: 0.00002027
Iteration 313/1000 | Loss: 0.00002026
Iteration 314/1000 | Loss: 0.00002022
Iteration 315/1000 | Loss: 0.00002022
Iteration 316/1000 | Loss: 0.00002021
Iteration 317/1000 | Loss: 0.00002020
Iteration 318/1000 | Loss: 0.00002020
Iteration 319/1000 | Loss: 0.00002020
Iteration 320/1000 | Loss: 0.00002019
Iteration 321/1000 | Loss: 0.00002019
Iteration 322/1000 | Loss: 0.00002019
Iteration 323/1000 | Loss: 0.00002018
Iteration 324/1000 | Loss: 0.00002018
Iteration 325/1000 | Loss: 0.00002018
Iteration 326/1000 | Loss: 0.00002018
Iteration 327/1000 | Loss: 0.00002018
Iteration 328/1000 | Loss: 0.00002018
Iteration 329/1000 | Loss: 0.00002018
Iteration 330/1000 | Loss: 0.00002018
Iteration 331/1000 | Loss: 0.00002017
Iteration 332/1000 | Loss: 0.00002017
Iteration 333/1000 | Loss: 0.00002017
Iteration 334/1000 | Loss: 0.00002017
Iteration 335/1000 | Loss: 0.00003168
Iteration 336/1000 | Loss: 0.00002071
Iteration 337/1000 | Loss: 0.00002055
Iteration 338/1000 | Loss: 0.00002052
Iteration 339/1000 | Loss: 0.00002031
Iteration 340/1000 | Loss: 0.00001998
Iteration 341/1000 | Loss: 0.00001979
Iteration 342/1000 | Loss: 0.00001972
Iteration 343/1000 | Loss: 0.00001971
Iteration 344/1000 | Loss: 0.00001969
Iteration 345/1000 | Loss: 0.00001968
Iteration 346/1000 | Loss: 0.00001967
Iteration 347/1000 | Loss: 0.00001967
Iteration 348/1000 | Loss: 0.00001966
Iteration 349/1000 | Loss: 0.00001966
Iteration 350/1000 | Loss: 0.00001965
Iteration 351/1000 | Loss: 0.00001964
Iteration 352/1000 | Loss: 0.00001964
Iteration 353/1000 | Loss: 0.00001964
Iteration 354/1000 | Loss: 0.00001963
Iteration 355/1000 | Loss: 0.00001963
Iteration 356/1000 | Loss: 0.00001962
Iteration 357/1000 | Loss: 0.00001962
Iteration 358/1000 | Loss: 0.00001962
Iteration 359/1000 | Loss: 0.00001961
Iteration 360/1000 | Loss: 0.00001961
Iteration 361/1000 | Loss: 0.00001961
Iteration 362/1000 | Loss: 0.00001961
Iteration 363/1000 | Loss: 0.00001961
Iteration 364/1000 | Loss: 0.00001960
Iteration 365/1000 | Loss: 0.00001960
Iteration 366/1000 | Loss: 0.00001960
Iteration 367/1000 | Loss: 0.00001960
Iteration 368/1000 | Loss: 0.00001959
Iteration 369/1000 | Loss: 0.00001958
Iteration 370/1000 | Loss: 0.00001958
Iteration 371/1000 | Loss: 0.00001958
Iteration 372/1000 | Loss: 0.00001957
Iteration 373/1000 | Loss: 0.00001957
Iteration 374/1000 | Loss: 0.00001957
Iteration 375/1000 | Loss: 0.00001956
Iteration 376/1000 | Loss: 0.00001956
Iteration 377/1000 | Loss: 0.00001956
Iteration 378/1000 | Loss: 0.00001956
Iteration 379/1000 | Loss: 0.00001956
Iteration 380/1000 | Loss: 0.00001955
Iteration 381/1000 | Loss: 0.00001955
Iteration 382/1000 | Loss: 0.00001955
Iteration 383/1000 | Loss: 0.00001954
Iteration 384/1000 | Loss: 0.00001954
Iteration 385/1000 | Loss: 0.00001954
Iteration 386/1000 | Loss: 0.00001953
Iteration 387/1000 | Loss: 0.00001953
Iteration 388/1000 | Loss: 0.00001953
Iteration 389/1000 | Loss: 0.00001952
Iteration 390/1000 | Loss: 0.00001946
Iteration 391/1000 | Loss: 0.00001945
Iteration 392/1000 | Loss: 0.00001939
Iteration 393/1000 | Loss: 0.00001934
Iteration 394/1000 | Loss: 0.00001934
Iteration 395/1000 | Loss: 0.00001934
Iteration 396/1000 | Loss: 0.00001933
Iteration 397/1000 | Loss: 0.00001933
Iteration 398/1000 | Loss: 0.00001932
Iteration 399/1000 | Loss: 0.00001932
Iteration 400/1000 | Loss: 0.00017214
Iteration 401/1000 | Loss: 0.00014100
Iteration 402/1000 | Loss: 0.00001975
Iteration 403/1000 | Loss: 0.00001937
Iteration 404/1000 | Loss: 0.00001933
Iteration 405/1000 | Loss: 0.00017168
Iteration 406/1000 | Loss: 0.00014755
Iteration 407/1000 | Loss: 0.00016779
Iteration 408/1000 | Loss: 0.00002539
Iteration 409/1000 | Loss: 0.00002237
Iteration 410/1000 | Loss: 0.00002051
Iteration 411/1000 | Loss: 0.00001964
Iteration 412/1000 | Loss: 0.00001892
Iteration 413/1000 | Loss: 0.00001834
Iteration 414/1000 | Loss: 0.00001804
Iteration 415/1000 | Loss: 0.00001787
Iteration 416/1000 | Loss: 0.00001785
Iteration 417/1000 | Loss: 0.00001784
Iteration 418/1000 | Loss: 0.00001780
Iteration 419/1000 | Loss: 0.00001776
Iteration 420/1000 | Loss: 0.00001768
Iteration 421/1000 | Loss: 0.00001767
Iteration 422/1000 | Loss: 0.00001766
Iteration 423/1000 | Loss: 0.00001765
Iteration 424/1000 | Loss: 0.00001764
Iteration 425/1000 | Loss: 0.00001761
Iteration 426/1000 | Loss: 0.00001760
Iteration 427/1000 | Loss: 0.00001759
Iteration 428/1000 | Loss: 0.00001756
Iteration 429/1000 | Loss: 0.00001756
Iteration 430/1000 | Loss: 0.00001754
Iteration 431/1000 | Loss: 0.00001754
Iteration 432/1000 | Loss: 0.00001754
Iteration 433/1000 | Loss: 0.00001754
Iteration 434/1000 | Loss: 0.00001753
Iteration 435/1000 | Loss: 0.00001753
Iteration 436/1000 | Loss: 0.00001753
Iteration 437/1000 | Loss: 0.00001751
Iteration 438/1000 | Loss: 0.00001748
Iteration 439/1000 | Loss: 0.00001748
Iteration 440/1000 | Loss: 0.00001747
Iteration 441/1000 | Loss: 0.00001747
Iteration 442/1000 | Loss: 0.00001746
Iteration 443/1000 | Loss: 0.00001745
Iteration 444/1000 | Loss: 0.00001745
Iteration 445/1000 | Loss: 0.00001745
Iteration 446/1000 | Loss: 0.00001745
Iteration 447/1000 | Loss: 0.00001745
Iteration 448/1000 | Loss: 0.00001745
Iteration 449/1000 | Loss: 0.00001744
Iteration 450/1000 | Loss: 0.00001744
Iteration 451/1000 | Loss: 0.00001744
Iteration 452/1000 | Loss: 0.00001743
Iteration 453/1000 | Loss: 0.00001743
Iteration 454/1000 | Loss: 0.00001741
Iteration 455/1000 | Loss: 0.00001740
Iteration 456/1000 | Loss: 0.00001740
Iteration 457/1000 | Loss: 0.00001739
Iteration 458/1000 | Loss: 0.00001739
Iteration 459/1000 | Loss: 0.00001739
Iteration 460/1000 | Loss: 0.00001738
Iteration 461/1000 | Loss: 0.00001738
Iteration 462/1000 | Loss: 0.00001738
Iteration 463/1000 | Loss: 0.00001737
Iteration 464/1000 | Loss: 0.00001737
Iteration 465/1000 | Loss: 0.00001737
Iteration 466/1000 | Loss: 0.00001736
Iteration 467/1000 | Loss: 0.00001736
Iteration 468/1000 | Loss: 0.00001736
Iteration 469/1000 | Loss: 0.00001736
Iteration 470/1000 | Loss: 0.00001736
Iteration 471/1000 | Loss: 0.00001736
Iteration 472/1000 | Loss: 0.00001735
Iteration 473/1000 | Loss: 0.00001735
Iteration 474/1000 | Loss: 0.00001735
Iteration 475/1000 | Loss: 0.00001735
Iteration 476/1000 | Loss: 0.00001734
Iteration 477/1000 | Loss: 0.00001734
Iteration 478/1000 | Loss: 0.00001734
Iteration 479/1000 | Loss: 0.00001734
Iteration 480/1000 | Loss: 0.00001734
Iteration 481/1000 | Loss: 0.00001734
Iteration 482/1000 | Loss: 0.00001734
Iteration 483/1000 | Loss: 0.00001734
Iteration 484/1000 | Loss: 0.00001734
Iteration 485/1000 | Loss: 0.00001734
Iteration 486/1000 | Loss: 0.00001734
Iteration 487/1000 | Loss: 0.00001733
Iteration 488/1000 | Loss: 0.00001733
Iteration 489/1000 | Loss: 0.00001733
Iteration 490/1000 | Loss: 0.00001733
Iteration 491/1000 | Loss: 0.00001733
Iteration 492/1000 | Loss: 0.00001733
Iteration 493/1000 | Loss: 0.00001733
Iteration 494/1000 | Loss: 0.00001733
Iteration 495/1000 | Loss: 0.00001733
Iteration 496/1000 | Loss: 0.00001733
Iteration 497/1000 | Loss: 0.00001733
Iteration 498/1000 | Loss: 0.00001733
Iteration 499/1000 | Loss: 0.00001733
Iteration 500/1000 | Loss: 0.00001733
Iteration 501/1000 | Loss: 0.00001733
Iteration 502/1000 | Loss: 0.00001733
Iteration 503/1000 | Loss: 0.00001733
Iteration 504/1000 | Loss: 0.00001733
Iteration 505/1000 | Loss: 0.00001733
Iteration 506/1000 | Loss: 0.00001733
Iteration 507/1000 | Loss: 0.00001733
Iteration 508/1000 | Loss: 0.00001733
Iteration 509/1000 | Loss: 0.00001733
Iteration 510/1000 | Loss: 0.00001733
Iteration 511/1000 | Loss: 0.00001733
Iteration 512/1000 | Loss: 0.00001733
Iteration 513/1000 | Loss: 0.00001733
Iteration 514/1000 | Loss: 0.00001733
Iteration 515/1000 | Loss: 0.00001733
Iteration 516/1000 | Loss: 0.00001733
Iteration 517/1000 | Loss: 0.00001733
Iteration 518/1000 | Loss: 0.00001733
Iteration 519/1000 | Loss: 0.00001733
Iteration 520/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 520. Stopping optimization.
Last 5 losses: [1.7333779396722093e-05, 1.7333779396722093e-05, 1.7333779396722093e-05, 1.7333779396722093e-05, 1.7333779396722093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7333779396722093e-05

Optimization complete. Final v2v error: 3.4485394954681396 mm

Highest mean error: 5.151272773742676 mm for frame 63

Lowest mean error: 2.855576515197754 mm for frame 106

Saving results

Total time: 280.1266248226166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506576
Iteration 2/25 | Loss: 0.00165893
Iteration 3/25 | Loss: 0.00140937
Iteration 4/25 | Loss: 0.00138795
Iteration 5/25 | Loss: 0.00138019
Iteration 6/25 | Loss: 0.00137877
Iteration 7/25 | Loss: 0.00137877
Iteration 8/25 | Loss: 0.00137877
Iteration 9/25 | Loss: 0.00137877
Iteration 10/25 | Loss: 0.00137877
Iteration 11/25 | Loss: 0.00137877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013787748757749796, 0.0013787748757749796, 0.0013787748757749796, 0.0013787748757749796, 0.0013787748757749796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013787748757749796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64893174
Iteration 2/25 | Loss: 0.00101924
Iteration 3/25 | Loss: 0.00101924
Iteration 4/25 | Loss: 0.00101924
Iteration 5/25 | Loss: 0.00101923
Iteration 6/25 | Loss: 0.00101923
Iteration 7/25 | Loss: 0.00101923
Iteration 8/25 | Loss: 0.00101923
Iteration 9/25 | Loss: 0.00101923
Iteration 10/25 | Loss: 0.00101923
Iteration 11/25 | Loss: 0.00101923
Iteration 12/25 | Loss: 0.00101923
Iteration 13/25 | Loss: 0.00101923
Iteration 14/25 | Loss: 0.00101923
Iteration 15/25 | Loss: 0.00101923
Iteration 16/25 | Loss: 0.00101923
Iteration 17/25 | Loss: 0.00101923
Iteration 18/25 | Loss: 0.00101923
Iteration 19/25 | Loss: 0.00101923
Iteration 20/25 | Loss: 0.00101923
Iteration 21/25 | Loss: 0.00101923
Iteration 22/25 | Loss: 0.00101923
Iteration 23/25 | Loss: 0.00101923
Iteration 24/25 | Loss: 0.00101923
Iteration 25/25 | Loss: 0.00101923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010192322079092264, 0.0010192322079092264, 0.0010192322079092264, 0.0010192322079092264, 0.0010192322079092264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010192322079092264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101923
Iteration 2/1000 | Loss: 0.00004512
Iteration 3/1000 | Loss: 0.00003316
Iteration 4/1000 | Loss: 0.00002967
Iteration 5/1000 | Loss: 0.00002800
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002571
Iteration 8/1000 | Loss: 0.00002526
Iteration 9/1000 | Loss: 0.00002473
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002380
Iteration 12/1000 | Loss: 0.00002341
Iteration 13/1000 | Loss: 0.00002315
Iteration 14/1000 | Loss: 0.00002294
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00002236
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002210
Iteration 20/1000 | Loss: 0.00002207
Iteration 21/1000 | Loss: 0.00002204
Iteration 22/1000 | Loss: 0.00002197
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002190
Iteration 26/1000 | Loss: 0.00002189
Iteration 27/1000 | Loss: 0.00002189
Iteration 28/1000 | Loss: 0.00002189
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002188
Iteration 31/1000 | Loss: 0.00002188
Iteration 32/1000 | Loss: 0.00002188
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002185
Iteration 37/1000 | Loss: 0.00002185
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002185
Iteration 41/1000 | Loss: 0.00002185
Iteration 42/1000 | Loss: 0.00002185
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002184
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002178
Iteration 55/1000 | Loss: 0.00002177
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002175
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00002174
Iteration 61/1000 | Loss: 0.00002174
Iteration 62/1000 | Loss: 0.00002174
Iteration 63/1000 | Loss: 0.00002174
Iteration 64/1000 | Loss: 0.00002174
Iteration 65/1000 | Loss: 0.00002174
Iteration 66/1000 | Loss: 0.00002174
Iteration 67/1000 | Loss: 0.00002174
Iteration 68/1000 | Loss: 0.00002174
Iteration 69/1000 | Loss: 0.00002174
Iteration 70/1000 | Loss: 0.00002174
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002172
Iteration 73/1000 | Loss: 0.00002172
Iteration 74/1000 | Loss: 0.00002172
Iteration 75/1000 | Loss: 0.00002172
Iteration 76/1000 | Loss: 0.00002172
Iteration 77/1000 | Loss: 0.00002172
Iteration 78/1000 | Loss: 0.00002172
Iteration 79/1000 | Loss: 0.00002172
Iteration 80/1000 | Loss: 0.00002172
Iteration 81/1000 | Loss: 0.00002172
Iteration 82/1000 | Loss: 0.00002172
Iteration 83/1000 | Loss: 0.00002171
Iteration 84/1000 | Loss: 0.00002171
Iteration 85/1000 | Loss: 0.00002171
Iteration 86/1000 | Loss: 0.00002171
Iteration 87/1000 | Loss: 0.00002170
Iteration 88/1000 | Loss: 0.00002170
Iteration 89/1000 | Loss: 0.00002170
Iteration 90/1000 | Loss: 0.00002170
Iteration 91/1000 | Loss: 0.00002169
Iteration 92/1000 | Loss: 0.00002169
Iteration 93/1000 | Loss: 0.00002169
Iteration 94/1000 | Loss: 0.00002169
Iteration 95/1000 | Loss: 0.00002169
Iteration 96/1000 | Loss: 0.00002169
Iteration 97/1000 | Loss: 0.00002169
Iteration 98/1000 | Loss: 0.00002168
Iteration 99/1000 | Loss: 0.00002168
Iteration 100/1000 | Loss: 0.00002168
Iteration 101/1000 | Loss: 0.00002167
Iteration 102/1000 | Loss: 0.00002167
Iteration 103/1000 | Loss: 0.00002167
Iteration 104/1000 | Loss: 0.00002167
Iteration 105/1000 | Loss: 0.00002166
Iteration 106/1000 | Loss: 0.00002166
Iteration 107/1000 | Loss: 0.00002166
Iteration 108/1000 | Loss: 0.00002166
Iteration 109/1000 | Loss: 0.00002166
Iteration 110/1000 | Loss: 0.00002166
Iteration 111/1000 | Loss: 0.00002166
Iteration 112/1000 | Loss: 0.00002165
Iteration 113/1000 | Loss: 0.00002165
Iteration 114/1000 | Loss: 0.00002165
Iteration 115/1000 | Loss: 0.00002165
Iteration 116/1000 | Loss: 0.00002165
Iteration 117/1000 | Loss: 0.00002165
Iteration 118/1000 | Loss: 0.00002165
Iteration 119/1000 | Loss: 0.00002165
Iteration 120/1000 | Loss: 0.00002164
Iteration 121/1000 | Loss: 0.00002164
Iteration 122/1000 | Loss: 0.00002164
Iteration 123/1000 | Loss: 0.00002164
Iteration 124/1000 | Loss: 0.00002164
Iteration 125/1000 | Loss: 0.00002163
Iteration 126/1000 | Loss: 0.00002162
Iteration 127/1000 | Loss: 0.00002162
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002162
Iteration 132/1000 | Loss: 0.00002161
Iteration 133/1000 | Loss: 0.00002161
Iteration 134/1000 | Loss: 0.00002161
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002161
Iteration 137/1000 | Loss: 0.00002161
Iteration 138/1000 | Loss: 0.00002160
Iteration 139/1000 | Loss: 0.00002160
Iteration 140/1000 | Loss: 0.00002160
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002159
Iteration 144/1000 | Loss: 0.00002159
Iteration 145/1000 | Loss: 0.00002159
Iteration 146/1000 | Loss: 0.00002158
Iteration 147/1000 | Loss: 0.00002158
Iteration 148/1000 | Loss: 0.00002158
Iteration 149/1000 | Loss: 0.00002158
Iteration 150/1000 | Loss: 0.00002157
Iteration 151/1000 | Loss: 0.00002156
Iteration 152/1000 | Loss: 0.00002156
Iteration 153/1000 | Loss: 0.00002156
Iteration 154/1000 | Loss: 0.00002156
Iteration 155/1000 | Loss: 0.00002156
Iteration 156/1000 | Loss: 0.00002156
Iteration 157/1000 | Loss: 0.00002155
Iteration 158/1000 | Loss: 0.00002155
Iteration 159/1000 | Loss: 0.00002155
Iteration 160/1000 | Loss: 0.00002155
Iteration 161/1000 | Loss: 0.00002155
Iteration 162/1000 | Loss: 0.00002155
Iteration 163/1000 | Loss: 0.00002154
Iteration 164/1000 | Loss: 0.00002154
Iteration 165/1000 | Loss: 0.00002154
Iteration 166/1000 | Loss: 0.00002154
Iteration 167/1000 | Loss: 0.00002154
Iteration 168/1000 | Loss: 0.00002154
Iteration 169/1000 | Loss: 0.00002154
Iteration 170/1000 | Loss: 0.00002154
Iteration 171/1000 | Loss: 0.00002154
Iteration 172/1000 | Loss: 0.00002154
Iteration 173/1000 | Loss: 0.00002154
Iteration 174/1000 | Loss: 0.00002154
Iteration 175/1000 | Loss: 0.00002154
Iteration 176/1000 | Loss: 0.00002153
Iteration 177/1000 | Loss: 0.00002153
Iteration 178/1000 | Loss: 0.00002153
Iteration 179/1000 | Loss: 0.00002153
Iteration 180/1000 | Loss: 0.00002153
Iteration 181/1000 | Loss: 0.00002153
Iteration 182/1000 | Loss: 0.00002153
Iteration 183/1000 | Loss: 0.00002153
Iteration 184/1000 | Loss: 0.00002153
Iteration 185/1000 | Loss: 0.00002153
Iteration 186/1000 | Loss: 0.00002152
Iteration 187/1000 | Loss: 0.00002152
Iteration 188/1000 | Loss: 0.00002152
Iteration 189/1000 | Loss: 0.00002152
Iteration 190/1000 | Loss: 0.00002152
Iteration 191/1000 | Loss: 0.00002152
Iteration 192/1000 | Loss: 0.00002152
Iteration 193/1000 | Loss: 0.00002152
Iteration 194/1000 | Loss: 0.00002152
Iteration 195/1000 | Loss: 0.00002151
Iteration 196/1000 | Loss: 0.00002151
Iteration 197/1000 | Loss: 0.00002151
Iteration 198/1000 | Loss: 0.00002151
Iteration 199/1000 | Loss: 0.00002151
Iteration 200/1000 | Loss: 0.00002151
Iteration 201/1000 | Loss: 0.00002151
Iteration 202/1000 | Loss: 0.00002151
Iteration 203/1000 | Loss: 0.00002151
Iteration 204/1000 | Loss: 0.00002150
Iteration 205/1000 | Loss: 0.00002150
Iteration 206/1000 | Loss: 0.00002150
Iteration 207/1000 | Loss: 0.00002150
Iteration 208/1000 | Loss: 0.00002150
Iteration 209/1000 | Loss: 0.00002150
Iteration 210/1000 | Loss: 0.00002150
Iteration 211/1000 | Loss: 0.00002150
Iteration 212/1000 | Loss: 0.00002150
Iteration 213/1000 | Loss: 0.00002150
Iteration 214/1000 | Loss: 0.00002150
Iteration 215/1000 | Loss: 0.00002150
Iteration 216/1000 | Loss: 0.00002150
Iteration 217/1000 | Loss: 0.00002149
Iteration 218/1000 | Loss: 0.00002149
Iteration 219/1000 | Loss: 0.00002149
Iteration 220/1000 | Loss: 0.00002149
Iteration 221/1000 | Loss: 0.00002149
Iteration 222/1000 | Loss: 0.00002148
Iteration 223/1000 | Loss: 0.00002148
Iteration 224/1000 | Loss: 0.00002148
Iteration 225/1000 | Loss: 0.00002148
Iteration 226/1000 | Loss: 0.00002148
Iteration 227/1000 | Loss: 0.00002148
Iteration 228/1000 | Loss: 0.00002148
Iteration 229/1000 | Loss: 0.00002148
Iteration 230/1000 | Loss: 0.00002148
Iteration 231/1000 | Loss: 0.00002148
Iteration 232/1000 | Loss: 0.00002148
Iteration 233/1000 | Loss: 0.00002148
Iteration 234/1000 | Loss: 0.00002148
Iteration 235/1000 | Loss: 0.00002148
Iteration 236/1000 | Loss: 0.00002148
Iteration 237/1000 | Loss: 0.00002148
Iteration 238/1000 | Loss: 0.00002147
Iteration 239/1000 | Loss: 0.00002147
Iteration 240/1000 | Loss: 0.00002147
Iteration 241/1000 | Loss: 0.00002147
Iteration 242/1000 | Loss: 0.00002147
Iteration 243/1000 | Loss: 0.00002147
Iteration 244/1000 | Loss: 0.00002147
Iteration 245/1000 | Loss: 0.00002147
Iteration 246/1000 | Loss: 0.00002147
Iteration 247/1000 | Loss: 0.00002147
Iteration 248/1000 | Loss: 0.00002147
Iteration 249/1000 | Loss: 0.00002147
Iteration 250/1000 | Loss: 0.00002147
Iteration 251/1000 | Loss: 0.00002147
Iteration 252/1000 | Loss: 0.00002147
Iteration 253/1000 | Loss: 0.00002147
Iteration 254/1000 | Loss: 0.00002147
Iteration 255/1000 | Loss: 0.00002147
Iteration 256/1000 | Loss: 0.00002147
Iteration 257/1000 | Loss: 0.00002147
Iteration 258/1000 | Loss: 0.00002147
Iteration 259/1000 | Loss: 0.00002147
Iteration 260/1000 | Loss: 0.00002147
Iteration 261/1000 | Loss: 0.00002147
Iteration 262/1000 | Loss: 0.00002147
Iteration 263/1000 | Loss: 0.00002147
Iteration 264/1000 | Loss: 0.00002147
Iteration 265/1000 | Loss: 0.00002147
Iteration 266/1000 | Loss: 0.00002147
Iteration 267/1000 | Loss: 0.00002147
Iteration 268/1000 | Loss: 0.00002147
Iteration 269/1000 | Loss: 0.00002147
Iteration 270/1000 | Loss: 0.00002147
Iteration 271/1000 | Loss: 0.00002147
Iteration 272/1000 | Loss: 0.00002147
Iteration 273/1000 | Loss: 0.00002147
Iteration 274/1000 | Loss: 0.00002147
Iteration 275/1000 | Loss: 0.00002147
Iteration 276/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [2.1465290046762675e-05, 2.1465290046762675e-05, 2.1465290046762675e-05, 2.1465290046762675e-05, 2.1465290046762675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1465290046762675e-05

Optimization complete. Final v2v error: 3.8743903636932373 mm

Highest mean error: 4.2841877937316895 mm for frame 241

Lowest mean error: 3.7304627895355225 mm for frame 146

Saving results

Total time: 62.79999136924744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426764
Iteration 2/25 | Loss: 0.00133709
Iteration 3/25 | Loss: 0.00128674
Iteration 4/25 | Loss: 0.00127973
Iteration 5/25 | Loss: 0.00127770
Iteration 6/25 | Loss: 0.00127770
Iteration 7/25 | Loss: 0.00127770
Iteration 8/25 | Loss: 0.00127770
Iteration 9/25 | Loss: 0.00127770
Iteration 10/25 | Loss: 0.00127770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001277695526368916, 0.001277695526368916, 0.001277695526368916, 0.001277695526368916, 0.001277695526368916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277695526368916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41719317
Iteration 2/25 | Loss: 0.00080440
Iteration 3/25 | Loss: 0.00080440
Iteration 4/25 | Loss: 0.00080440
Iteration 5/25 | Loss: 0.00080440
Iteration 6/25 | Loss: 0.00080440
Iteration 7/25 | Loss: 0.00080440
Iteration 8/25 | Loss: 0.00080440
Iteration 9/25 | Loss: 0.00080440
Iteration 10/25 | Loss: 0.00080440
Iteration 11/25 | Loss: 0.00080440
Iteration 12/25 | Loss: 0.00080440
Iteration 13/25 | Loss: 0.00080440
Iteration 14/25 | Loss: 0.00080440
Iteration 15/25 | Loss: 0.00080440
Iteration 16/25 | Loss: 0.00080440
Iteration 17/25 | Loss: 0.00080440
Iteration 18/25 | Loss: 0.00080440
Iteration 19/25 | Loss: 0.00080440
Iteration 20/25 | Loss: 0.00080440
Iteration 21/25 | Loss: 0.00080440
Iteration 22/25 | Loss: 0.00080440
Iteration 23/25 | Loss: 0.00080440
Iteration 24/25 | Loss: 0.00080440
Iteration 25/25 | Loss: 0.00080440

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080440
Iteration 2/1000 | Loss: 0.00002514
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001864
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001691
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001622
Iteration 9/1000 | Loss: 0.00001603
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001564
Iteration 13/1000 | Loss: 0.00001547
Iteration 14/1000 | Loss: 0.00001537
Iteration 15/1000 | Loss: 0.00001531
Iteration 16/1000 | Loss: 0.00001531
Iteration 17/1000 | Loss: 0.00001530
Iteration 18/1000 | Loss: 0.00001530
Iteration 19/1000 | Loss: 0.00001530
Iteration 20/1000 | Loss: 0.00001528
Iteration 21/1000 | Loss: 0.00001523
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001513
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001506
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001503
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001500
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001499
Iteration 54/1000 | Loss: 0.00001499
Iteration 55/1000 | Loss: 0.00001499
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001495
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001481
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001481
Iteration 99/1000 | Loss: 0.00001481
Iteration 100/1000 | Loss: 0.00001481
Iteration 101/1000 | Loss: 0.00001481
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001480
Iteration 109/1000 | Loss: 0.00001480
Iteration 110/1000 | Loss: 0.00001480
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001480
Iteration 114/1000 | Loss: 0.00001480
Iteration 115/1000 | Loss: 0.00001480
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001480
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001480
Iteration 124/1000 | Loss: 0.00001479
Iteration 125/1000 | Loss: 0.00001479
Iteration 126/1000 | Loss: 0.00001479
Iteration 127/1000 | Loss: 0.00001479
Iteration 128/1000 | Loss: 0.00001479
Iteration 129/1000 | Loss: 0.00001479
Iteration 130/1000 | Loss: 0.00001479
Iteration 131/1000 | Loss: 0.00001479
Iteration 132/1000 | Loss: 0.00001479
Iteration 133/1000 | Loss: 0.00001479
Iteration 134/1000 | Loss: 0.00001479
Iteration 135/1000 | Loss: 0.00001479
Iteration 136/1000 | Loss: 0.00001479
Iteration 137/1000 | Loss: 0.00001479
Iteration 138/1000 | Loss: 0.00001479
Iteration 139/1000 | Loss: 0.00001479
Iteration 140/1000 | Loss: 0.00001479
Iteration 141/1000 | Loss: 0.00001479
Iteration 142/1000 | Loss: 0.00001479
Iteration 143/1000 | Loss: 0.00001479
Iteration 144/1000 | Loss: 0.00001478
Iteration 145/1000 | Loss: 0.00001478
Iteration 146/1000 | Loss: 0.00001478
Iteration 147/1000 | Loss: 0.00001478
Iteration 148/1000 | Loss: 0.00001478
Iteration 149/1000 | Loss: 0.00001478
Iteration 150/1000 | Loss: 0.00001478
Iteration 151/1000 | Loss: 0.00001478
Iteration 152/1000 | Loss: 0.00001478
Iteration 153/1000 | Loss: 0.00001478
Iteration 154/1000 | Loss: 0.00001478
Iteration 155/1000 | Loss: 0.00001478
Iteration 156/1000 | Loss: 0.00001478
Iteration 157/1000 | Loss: 0.00001478
Iteration 158/1000 | Loss: 0.00001478
Iteration 159/1000 | Loss: 0.00001478
Iteration 160/1000 | Loss: 0.00001478
Iteration 161/1000 | Loss: 0.00001477
Iteration 162/1000 | Loss: 0.00001477
Iteration 163/1000 | Loss: 0.00001477
Iteration 164/1000 | Loss: 0.00001477
Iteration 165/1000 | Loss: 0.00001477
Iteration 166/1000 | Loss: 0.00001477
Iteration 167/1000 | Loss: 0.00001477
Iteration 168/1000 | Loss: 0.00001477
Iteration 169/1000 | Loss: 0.00001477
Iteration 170/1000 | Loss: 0.00001477
Iteration 171/1000 | Loss: 0.00001477
Iteration 172/1000 | Loss: 0.00001477
Iteration 173/1000 | Loss: 0.00001477
Iteration 174/1000 | Loss: 0.00001477
Iteration 175/1000 | Loss: 0.00001477
Iteration 176/1000 | Loss: 0.00001477
Iteration 177/1000 | Loss: 0.00001476
Iteration 178/1000 | Loss: 0.00001476
Iteration 179/1000 | Loss: 0.00001476
Iteration 180/1000 | Loss: 0.00001476
Iteration 181/1000 | Loss: 0.00001476
Iteration 182/1000 | Loss: 0.00001476
Iteration 183/1000 | Loss: 0.00001476
Iteration 184/1000 | Loss: 0.00001476
Iteration 185/1000 | Loss: 0.00001476
Iteration 186/1000 | Loss: 0.00001476
Iteration 187/1000 | Loss: 0.00001476
Iteration 188/1000 | Loss: 0.00001476
Iteration 189/1000 | Loss: 0.00001476
Iteration 190/1000 | Loss: 0.00001476
Iteration 191/1000 | Loss: 0.00001475
Iteration 192/1000 | Loss: 0.00001475
Iteration 193/1000 | Loss: 0.00001475
Iteration 194/1000 | Loss: 0.00001475
Iteration 195/1000 | Loss: 0.00001475
Iteration 196/1000 | Loss: 0.00001475
Iteration 197/1000 | Loss: 0.00001475
Iteration 198/1000 | Loss: 0.00001475
Iteration 199/1000 | Loss: 0.00001475
Iteration 200/1000 | Loss: 0.00001475
Iteration 201/1000 | Loss: 0.00001475
Iteration 202/1000 | Loss: 0.00001475
Iteration 203/1000 | Loss: 0.00001475
Iteration 204/1000 | Loss: 0.00001475
Iteration 205/1000 | Loss: 0.00001475
Iteration 206/1000 | Loss: 0.00001475
Iteration 207/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.4750134141650051e-05, 1.4750134141650051e-05, 1.4750134141650051e-05, 1.4750134141650051e-05, 1.4750134141650051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4750134141650051e-05

Optimization complete. Final v2v error: 3.294811248779297 mm

Highest mean error: 3.4604899883270264 mm for frame 5

Lowest mean error: 3.1184098720550537 mm for frame 62

Saving results

Total time: 39.99206042289734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006087
Iteration 2/25 | Loss: 0.01006087
Iteration 3/25 | Loss: 0.00259033
Iteration 4/25 | Loss: 0.00224092
Iteration 5/25 | Loss: 0.00216740
Iteration 6/25 | Loss: 0.00212627
Iteration 7/25 | Loss: 0.00209305
Iteration 8/25 | Loss: 0.00207088
Iteration 9/25 | Loss: 0.00204002
Iteration 10/25 | Loss: 0.00202003
Iteration 11/25 | Loss: 0.00201339
Iteration 12/25 | Loss: 0.00201140
Iteration 13/25 | Loss: 0.00201063
Iteration 14/25 | Loss: 0.00201047
Iteration 15/25 | Loss: 0.00201047
Iteration 16/25 | Loss: 0.00201047
Iteration 17/25 | Loss: 0.00201047
Iteration 18/25 | Loss: 0.00201047
Iteration 19/25 | Loss: 0.00201047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020104662980884314, 0.0020104662980884314, 0.0020104662980884314, 0.0020104662980884314, 0.0020104662980884314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020104662980884314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27880716
Iteration 2/25 | Loss: 0.01009314
Iteration 3/25 | Loss: 0.01009314
Iteration 4/25 | Loss: 0.01009314
Iteration 5/25 | Loss: 0.01009314
Iteration 6/25 | Loss: 0.01009314
Iteration 7/25 | Loss: 0.01009314
Iteration 8/25 | Loss: 0.01009314
Iteration 9/25 | Loss: 0.01009314
Iteration 10/25 | Loss: 0.01009314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.010093135759234428, 0.010093135759234428, 0.010093135759234428, 0.010093135759234428, 0.010093135759234428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.010093135759234428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01009314
Iteration 2/1000 | Loss: 0.00123284
Iteration 3/1000 | Loss: 0.00084334
Iteration 4/1000 | Loss: 0.00064850
Iteration 5/1000 | Loss: 0.00057678
Iteration 6/1000 | Loss: 0.00052810
Iteration 7/1000 | Loss: 0.00047857
Iteration 8/1000 | Loss: 0.00044731
Iteration 9/1000 | Loss: 0.00043355
Iteration 10/1000 | Loss: 0.00041807
Iteration 11/1000 | Loss: 0.00041037
Iteration 12/1000 | Loss: 0.00040369
Iteration 13/1000 | Loss: 0.00039963
Iteration 14/1000 | Loss: 0.00039671
Iteration 15/1000 | Loss: 0.00039445
Iteration 16/1000 | Loss: 0.00039270
Iteration 17/1000 | Loss: 0.00039136
Iteration 18/1000 | Loss: 0.00039006
Iteration 19/1000 | Loss: 0.00038892
Iteration 20/1000 | Loss: 0.00038793
Iteration 21/1000 | Loss: 0.00038737
Iteration 22/1000 | Loss: 0.00038699
Iteration 23/1000 | Loss: 0.00038659
Iteration 24/1000 | Loss: 0.00038623
Iteration 25/1000 | Loss: 0.00038601
Iteration 26/1000 | Loss: 0.00038573
Iteration 27/1000 | Loss: 0.00038542
Iteration 28/1000 | Loss: 0.00038510
Iteration 29/1000 | Loss: 0.00038481
Iteration 30/1000 | Loss: 0.00038434
Iteration 31/1000 | Loss: 0.00038301
Iteration 32/1000 | Loss: 0.00037883
Iteration 33/1000 | Loss: 0.00036883
Iteration 34/1000 | Loss: 0.00079909
Iteration 35/1000 | Loss: 0.00036119
Iteration 36/1000 | Loss: 0.00035164
Iteration 37/1000 | Loss: 0.00034542
Iteration 38/1000 | Loss: 0.00033694
Iteration 39/1000 | Loss: 0.00032977
Iteration 40/1000 | Loss: 0.00032438
Iteration 41/1000 | Loss: 0.00031682
Iteration 42/1000 | Loss: 0.00031131
Iteration 43/1000 | Loss: 0.00030747
Iteration 44/1000 | Loss: 0.00030457
Iteration 45/1000 | Loss: 0.00030160
Iteration 46/1000 | Loss: 0.00029954
Iteration 47/1000 | Loss: 0.00029818
Iteration 48/1000 | Loss: 0.00029628
Iteration 49/1000 | Loss: 0.00029504
Iteration 50/1000 | Loss: 0.00029373
Iteration 51/1000 | Loss: 0.00029265
Iteration 52/1000 | Loss: 0.00029167
Iteration 53/1000 | Loss: 0.00029104
Iteration 54/1000 | Loss: 0.00029050
Iteration 55/1000 | Loss: 0.00029004
Iteration 56/1000 | Loss: 0.00028964
Iteration 57/1000 | Loss: 0.00028930
Iteration 58/1000 | Loss: 0.00028894
Iteration 59/1000 | Loss: 0.00028866
Iteration 60/1000 | Loss: 0.00028825
Iteration 61/1000 | Loss: 0.00028782
Iteration 62/1000 | Loss: 0.00028723
Iteration 63/1000 | Loss: 0.00028647
Iteration 64/1000 | Loss: 0.00028579
Iteration 65/1000 | Loss: 0.00028501
Iteration 66/1000 | Loss: 0.00028429
Iteration 67/1000 | Loss: 0.00028322
Iteration 68/1000 | Loss: 0.00028181
Iteration 69/1000 | Loss: 0.00027932
Iteration 70/1000 | Loss: 0.00183699
Iteration 71/1000 | Loss: 0.00066737
Iteration 72/1000 | Loss: 0.00027853
Iteration 73/1000 | Loss: 0.01072915
Iteration 74/1000 | Loss: 0.00214261
Iteration 75/1000 | Loss: 0.00029577
Iteration 76/1000 | Loss: 0.00028445
Iteration 77/1000 | Loss: 0.00028095
Iteration 78/1000 | Loss: 0.00288660
Iteration 79/1000 | Loss: 0.01888770
Iteration 80/1000 | Loss: 0.00913687
Iteration 81/1000 | Loss: 0.00054922
Iteration 82/1000 | Loss: 0.00531764
Iteration 83/1000 | Loss: 0.00149512
Iteration 84/1000 | Loss: 0.00248916
Iteration 85/1000 | Loss: 0.00072148
Iteration 86/1000 | Loss: 0.00745033
Iteration 87/1000 | Loss: 0.00131076
Iteration 88/1000 | Loss: 0.01041646
Iteration 89/1000 | Loss: 0.00208657
Iteration 90/1000 | Loss: 0.00796246
Iteration 91/1000 | Loss: 0.00172997
Iteration 92/1000 | Loss: 0.00206224
Iteration 93/1000 | Loss: 0.00082635
Iteration 94/1000 | Loss: 0.01325442
Iteration 95/1000 | Loss: 0.00278130
Iteration 96/1000 | Loss: 0.00043527
Iteration 97/1000 | Loss: 0.00767579
Iteration 98/1000 | Loss: 0.00115056
Iteration 99/1000 | Loss: 0.00091735
Iteration 100/1000 | Loss: 0.00222362
Iteration 101/1000 | Loss: 0.00079861
Iteration 102/1000 | Loss: 0.00663653
Iteration 103/1000 | Loss: 0.00117992
Iteration 104/1000 | Loss: 0.00069404
Iteration 105/1000 | Loss: 0.00556152
Iteration 106/1000 | Loss: 0.00178134
Iteration 107/1000 | Loss: 0.00034592
Iteration 108/1000 | Loss: 0.00608239
Iteration 109/1000 | Loss: 0.00031196
Iteration 110/1000 | Loss: 0.00016802
Iteration 111/1000 | Loss: 0.00011836
Iteration 112/1000 | Loss: 0.00009910
Iteration 113/1000 | Loss: 0.00008147
Iteration 114/1000 | Loss: 0.00007039
Iteration 115/1000 | Loss: 0.00005889
Iteration 116/1000 | Loss: 0.00005027
Iteration 117/1000 | Loss: 0.00004453
Iteration 118/1000 | Loss: 0.00003932
Iteration 119/1000 | Loss: 0.00003431
Iteration 120/1000 | Loss: 0.00003052
Iteration 121/1000 | Loss: 0.00002683
Iteration 122/1000 | Loss: 0.00002395
Iteration 123/1000 | Loss: 0.00002232
Iteration 124/1000 | Loss: 0.00002114
Iteration 125/1000 | Loss: 0.00002017
Iteration 126/1000 | Loss: 0.00001949
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001825
Iteration 130/1000 | Loss: 0.00001787
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001752
Iteration 133/1000 | Loss: 0.00001733
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001714
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001700
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001691
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001678
Iteration 145/1000 | Loss: 0.00001677
Iteration 146/1000 | Loss: 0.00001671
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001666
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001664
Iteration 153/1000 | Loss: 0.00001663
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001656
Iteration 158/1000 | Loss: 0.00001652
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001647
Iteration 161/1000 | Loss: 0.00001647
Iteration 162/1000 | Loss: 0.00001647
Iteration 163/1000 | Loss: 0.00001647
Iteration 164/1000 | Loss: 0.00001647
Iteration 165/1000 | Loss: 0.00001646
Iteration 166/1000 | Loss: 0.00001646
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001645
Iteration 169/1000 | Loss: 0.00001644
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001643
Iteration 172/1000 | Loss: 0.00001643
Iteration 173/1000 | Loss: 0.00001643
Iteration 174/1000 | Loss: 0.00001643
Iteration 175/1000 | Loss: 0.00001643
Iteration 176/1000 | Loss: 0.00001643
Iteration 177/1000 | Loss: 0.00001643
Iteration 178/1000 | Loss: 0.00001643
Iteration 179/1000 | Loss: 0.00001643
Iteration 180/1000 | Loss: 0.00001643
Iteration 181/1000 | Loss: 0.00001642
Iteration 182/1000 | Loss: 0.00001642
Iteration 183/1000 | Loss: 0.00001641
Iteration 184/1000 | Loss: 0.00001640
Iteration 185/1000 | Loss: 0.00001640
Iteration 186/1000 | Loss: 0.00001640
Iteration 187/1000 | Loss: 0.00001640
Iteration 188/1000 | Loss: 0.00001640
Iteration 189/1000 | Loss: 0.00001639
Iteration 190/1000 | Loss: 0.00001639
Iteration 191/1000 | Loss: 0.00001639
Iteration 192/1000 | Loss: 0.00001639
Iteration 193/1000 | Loss: 0.00001639
Iteration 194/1000 | Loss: 0.00001639
Iteration 195/1000 | Loss: 0.00001638
Iteration 196/1000 | Loss: 0.00001637
Iteration 197/1000 | Loss: 0.00001636
Iteration 198/1000 | Loss: 0.00001636
Iteration 199/1000 | Loss: 0.00001636
Iteration 200/1000 | Loss: 0.00001636
Iteration 201/1000 | Loss: 0.00001636
Iteration 202/1000 | Loss: 0.00001636
Iteration 203/1000 | Loss: 0.00001636
Iteration 204/1000 | Loss: 0.00001636
Iteration 205/1000 | Loss: 0.00001635
Iteration 206/1000 | Loss: 0.00001635
Iteration 207/1000 | Loss: 0.00001635
Iteration 208/1000 | Loss: 0.00001635
Iteration 209/1000 | Loss: 0.00001635
Iteration 210/1000 | Loss: 0.00001635
Iteration 211/1000 | Loss: 0.00001635
Iteration 212/1000 | Loss: 0.00001634
Iteration 213/1000 | Loss: 0.00001634
Iteration 214/1000 | Loss: 0.00001634
Iteration 215/1000 | Loss: 0.00001634
Iteration 216/1000 | Loss: 0.00001634
Iteration 217/1000 | Loss: 0.00001634
Iteration 218/1000 | Loss: 0.00001634
Iteration 219/1000 | Loss: 0.00001634
Iteration 220/1000 | Loss: 0.00001634
Iteration 221/1000 | Loss: 0.00001634
Iteration 222/1000 | Loss: 0.00001634
Iteration 223/1000 | Loss: 0.00001634
Iteration 224/1000 | Loss: 0.00001634
Iteration 225/1000 | Loss: 0.00001634
Iteration 226/1000 | Loss: 0.00001634
Iteration 227/1000 | Loss: 0.00001634
Iteration 228/1000 | Loss: 0.00001634
Iteration 229/1000 | Loss: 0.00001634
Iteration 230/1000 | Loss: 0.00001634
Iteration 231/1000 | Loss: 0.00001634
Iteration 232/1000 | Loss: 0.00001634
Iteration 233/1000 | Loss: 0.00001634
Iteration 234/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.634367072256282e-05, 1.634367072256282e-05, 1.634367072256282e-05, 1.634367072256282e-05, 1.634367072256282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.634367072256282e-05

Optimization complete. Final v2v error: 3.3933496475219727 mm

Highest mean error: 3.4919605255126953 mm for frame 38

Lowest mean error: 3.3103106021881104 mm for frame 239

Saving results

Total time: 251.64390397071838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842446
Iteration 2/25 | Loss: 0.00139016
Iteration 3/25 | Loss: 0.00131373
Iteration 4/25 | Loss: 0.00130142
Iteration 5/25 | Loss: 0.00129824
Iteration 6/25 | Loss: 0.00129821
Iteration 7/25 | Loss: 0.00129821
Iteration 8/25 | Loss: 0.00129821
Iteration 9/25 | Loss: 0.00129821
Iteration 10/25 | Loss: 0.00129821
Iteration 11/25 | Loss: 0.00129821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012982113985344768, 0.0012982113985344768, 0.0012982113985344768, 0.0012982113985344768, 0.0012982113985344768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012982113985344768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38166082
Iteration 2/25 | Loss: 0.00097431
Iteration 3/25 | Loss: 0.00097430
Iteration 4/25 | Loss: 0.00097430
Iteration 5/25 | Loss: 0.00097430
Iteration 6/25 | Loss: 0.00097430
Iteration 7/25 | Loss: 0.00097430
Iteration 8/25 | Loss: 0.00097430
Iteration 9/25 | Loss: 0.00097430
Iteration 10/25 | Loss: 0.00097430
Iteration 11/25 | Loss: 0.00097430
Iteration 12/25 | Loss: 0.00097430
Iteration 13/25 | Loss: 0.00097430
Iteration 14/25 | Loss: 0.00097430
Iteration 15/25 | Loss: 0.00097430
Iteration 16/25 | Loss: 0.00097430
Iteration 17/25 | Loss: 0.00097430
Iteration 18/25 | Loss: 0.00097430
Iteration 19/25 | Loss: 0.00097430
Iteration 20/25 | Loss: 0.00097430
Iteration 21/25 | Loss: 0.00097430
Iteration 22/25 | Loss: 0.00097430
Iteration 23/25 | Loss: 0.00097430
Iteration 24/25 | Loss: 0.00097430
Iteration 25/25 | Loss: 0.00097430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097430
Iteration 2/1000 | Loss: 0.00002680
Iteration 3/1000 | Loss: 0.00001772
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001544
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001447
Iteration 9/1000 | Loss: 0.00001424
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001373
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001349
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001340
Iteration 33/1000 | Loss: 0.00001339
Iteration 34/1000 | Loss: 0.00001339
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001336
Iteration 40/1000 | Loss: 0.00001335
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001329
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001322
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001320
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001320
Iteration 82/1000 | Loss: 0.00001320
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001319
Iteration 85/1000 | Loss: 0.00001319
Iteration 86/1000 | Loss: 0.00001319
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001318
Iteration 90/1000 | Loss: 0.00001318
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001318
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001317
Iteration 96/1000 | Loss: 0.00001317
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001315
Iteration 103/1000 | Loss: 0.00001315
Iteration 104/1000 | Loss: 0.00001315
Iteration 105/1000 | Loss: 0.00001315
Iteration 106/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.3145947377779521e-05, 1.3145947377779521e-05, 1.3145947377779521e-05, 1.3145947377779521e-05, 1.3145947377779521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3145947377779521e-05

Optimization complete. Final v2v error: 3.093309164047241 mm

Highest mean error: 3.4697225093841553 mm for frame 192

Lowest mean error: 2.7634315490722656 mm for frame 111

Saving results

Total time: 34.800225496292114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816543
Iteration 2/25 | Loss: 0.00156515
Iteration 3/25 | Loss: 0.00146586
Iteration 4/25 | Loss: 0.00145265
Iteration 5/25 | Loss: 0.00144893
Iteration 6/25 | Loss: 0.00144855
Iteration 7/25 | Loss: 0.00144855
Iteration 8/25 | Loss: 0.00144855
Iteration 9/25 | Loss: 0.00144855
Iteration 10/25 | Loss: 0.00144855
Iteration 11/25 | Loss: 0.00144855
Iteration 12/25 | Loss: 0.00144855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001448552357032895, 0.001448552357032895, 0.001448552357032895, 0.001448552357032895, 0.001448552357032895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001448552357032895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15147364
Iteration 2/25 | Loss: 0.00099351
Iteration 3/25 | Loss: 0.00099348
Iteration 4/25 | Loss: 0.00099348
Iteration 5/25 | Loss: 0.00099347
Iteration 6/25 | Loss: 0.00099347
Iteration 7/25 | Loss: 0.00099347
Iteration 8/25 | Loss: 0.00099347
Iteration 9/25 | Loss: 0.00099347
Iteration 10/25 | Loss: 0.00099347
Iteration 11/25 | Loss: 0.00099347
Iteration 12/25 | Loss: 0.00099347
Iteration 13/25 | Loss: 0.00099347
Iteration 14/25 | Loss: 0.00099347
Iteration 15/25 | Loss: 0.00099347
Iteration 16/25 | Loss: 0.00099347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009934729896485806, 0.0009934729896485806, 0.0009934729896485806, 0.0009934729896485806, 0.0009934729896485806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009934729896485806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099347
Iteration 2/1000 | Loss: 0.00006335
Iteration 3/1000 | Loss: 0.00004142
Iteration 4/1000 | Loss: 0.00003178
Iteration 5/1000 | Loss: 0.00002936
Iteration 6/1000 | Loss: 0.00002803
Iteration 7/1000 | Loss: 0.00002724
Iteration 8/1000 | Loss: 0.00002655
Iteration 9/1000 | Loss: 0.00002609
Iteration 10/1000 | Loss: 0.00002584
Iteration 11/1000 | Loss: 0.00002565
Iteration 12/1000 | Loss: 0.00002543
Iteration 13/1000 | Loss: 0.00002526
Iteration 14/1000 | Loss: 0.00002516
Iteration 15/1000 | Loss: 0.00002505
Iteration 16/1000 | Loss: 0.00002502
Iteration 17/1000 | Loss: 0.00002498
Iteration 18/1000 | Loss: 0.00002496
Iteration 19/1000 | Loss: 0.00002495
Iteration 20/1000 | Loss: 0.00002495
Iteration 21/1000 | Loss: 0.00002495
Iteration 22/1000 | Loss: 0.00002495
Iteration 23/1000 | Loss: 0.00002495
Iteration 24/1000 | Loss: 0.00002495
Iteration 25/1000 | Loss: 0.00002495
Iteration 26/1000 | Loss: 0.00002494
Iteration 27/1000 | Loss: 0.00002494
Iteration 28/1000 | Loss: 0.00002494
Iteration 29/1000 | Loss: 0.00002494
Iteration 30/1000 | Loss: 0.00002494
Iteration 31/1000 | Loss: 0.00002494
Iteration 32/1000 | Loss: 0.00002493
Iteration 33/1000 | Loss: 0.00002493
Iteration 34/1000 | Loss: 0.00002493
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002492
Iteration 37/1000 | Loss: 0.00002492
Iteration 38/1000 | Loss: 0.00002492
Iteration 39/1000 | Loss: 0.00002492
Iteration 40/1000 | Loss: 0.00002492
Iteration 41/1000 | Loss: 0.00002491
Iteration 42/1000 | Loss: 0.00002491
Iteration 43/1000 | Loss: 0.00002491
Iteration 44/1000 | Loss: 0.00002491
Iteration 45/1000 | Loss: 0.00002490
Iteration 46/1000 | Loss: 0.00002490
Iteration 47/1000 | Loss: 0.00002490
Iteration 48/1000 | Loss: 0.00002489
Iteration 49/1000 | Loss: 0.00002489
Iteration 50/1000 | Loss: 0.00002489
Iteration 51/1000 | Loss: 0.00002489
Iteration 52/1000 | Loss: 0.00002489
Iteration 53/1000 | Loss: 0.00002489
Iteration 54/1000 | Loss: 0.00002489
Iteration 55/1000 | Loss: 0.00002489
Iteration 56/1000 | Loss: 0.00002489
Iteration 57/1000 | Loss: 0.00002489
Iteration 58/1000 | Loss: 0.00002489
Iteration 59/1000 | Loss: 0.00002488
Iteration 60/1000 | Loss: 0.00002488
Iteration 61/1000 | Loss: 0.00002488
Iteration 62/1000 | Loss: 0.00002488
Iteration 63/1000 | Loss: 0.00002487
Iteration 64/1000 | Loss: 0.00002487
Iteration 65/1000 | Loss: 0.00002487
Iteration 66/1000 | Loss: 0.00002487
Iteration 67/1000 | Loss: 0.00002486
Iteration 68/1000 | Loss: 0.00002486
Iteration 69/1000 | Loss: 0.00002486
Iteration 70/1000 | Loss: 0.00002485
Iteration 71/1000 | Loss: 0.00002485
Iteration 72/1000 | Loss: 0.00002485
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002483
Iteration 77/1000 | Loss: 0.00002483
Iteration 78/1000 | Loss: 0.00002482
Iteration 79/1000 | Loss: 0.00002482
Iteration 80/1000 | Loss: 0.00002482
Iteration 81/1000 | Loss: 0.00002481
Iteration 82/1000 | Loss: 0.00002481
Iteration 83/1000 | Loss: 0.00002481
Iteration 84/1000 | Loss: 0.00002480
Iteration 85/1000 | Loss: 0.00002480
Iteration 86/1000 | Loss: 0.00002480
Iteration 87/1000 | Loss: 0.00002480
Iteration 88/1000 | Loss: 0.00002480
Iteration 89/1000 | Loss: 0.00002480
Iteration 90/1000 | Loss: 0.00002480
Iteration 91/1000 | Loss: 0.00002480
Iteration 92/1000 | Loss: 0.00002480
Iteration 93/1000 | Loss: 0.00002480
Iteration 94/1000 | Loss: 0.00002479
Iteration 95/1000 | Loss: 0.00002479
Iteration 96/1000 | Loss: 0.00002479
Iteration 97/1000 | Loss: 0.00002479
Iteration 98/1000 | Loss: 0.00002478
Iteration 99/1000 | Loss: 0.00002478
Iteration 100/1000 | Loss: 0.00002477
Iteration 101/1000 | Loss: 0.00002477
Iteration 102/1000 | Loss: 0.00002477
Iteration 103/1000 | Loss: 0.00002477
Iteration 104/1000 | Loss: 0.00002477
Iteration 105/1000 | Loss: 0.00002476
Iteration 106/1000 | Loss: 0.00002476
Iteration 107/1000 | Loss: 0.00002476
Iteration 108/1000 | Loss: 0.00002475
Iteration 109/1000 | Loss: 0.00002475
Iteration 110/1000 | Loss: 0.00002475
Iteration 111/1000 | Loss: 0.00002475
Iteration 112/1000 | Loss: 0.00002474
Iteration 113/1000 | Loss: 0.00002474
Iteration 114/1000 | Loss: 0.00002474
Iteration 115/1000 | Loss: 0.00002474
Iteration 116/1000 | Loss: 0.00002474
Iteration 117/1000 | Loss: 0.00002473
Iteration 118/1000 | Loss: 0.00002473
Iteration 119/1000 | Loss: 0.00002473
Iteration 120/1000 | Loss: 0.00002473
Iteration 121/1000 | Loss: 0.00002473
Iteration 122/1000 | Loss: 0.00002473
Iteration 123/1000 | Loss: 0.00002473
Iteration 124/1000 | Loss: 0.00002473
Iteration 125/1000 | Loss: 0.00002473
Iteration 126/1000 | Loss: 0.00002473
Iteration 127/1000 | Loss: 0.00002473
Iteration 128/1000 | Loss: 0.00002472
Iteration 129/1000 | Loss: 0.00002472
Iteration 130/1000 | Loss: 0.00002472
Iteration 131/1000 | Loss: 0.00002472
Iteration 132/1000 | Loss: 0.00002472
Iteration 133/1000 | Loss: 0.00002472
Iteration 134/1000 | Loss: 0.00002472
Iteration 135/1000 | Loss: 0.00002471
Iteration 136/1000 | Loss: 0.00002471
Iteration 137/1000 | Loss: 0.00002471
Iteration 138/1000 | Loss: 0.00002471
Iteration 139/1000 | Loss: 0.00002471
Iteration 140/1000 | Loss: 0.00002471
Iteration 141/1000 | Loss: 0.00002471
Iteration 142/1000 | Loss: 0.00002471
Iteration 143/1000 | Loss: 0.00002471
Iteration 144/1000 | Loss: 0.00002471
Iteration 145/1000 | Loss: 0.00002471
Iteration 146/1000 | Loss: 0.00002471
Iteration 147/1000 | Loss: 0.00002471
Iteration 148/1000 | Loss: 0.00002471
Iteration 149/1000 | Loss: 0.00002471
Iteration 150/1000 | Loss: 0.00002471
Iteration 151/1000 | Loss: 0.00002471
Iteration 152/1000 | Loss: 0.00002471
Iteration 153/1000 | Loss: 0.00002471
Iteration 154/1000 | Loss: 0.00002471
Iteration 155/1000 | Loss: 0.00002471
Iteration 156/1000 | Loss: 0.00002471
Iteration 157/1000 | Loss: 0.00002471
Iteration 158/1000 | Loss: 0.00002471
Iteration 159/1000 | Loss: 0.00002471
Iteration 160/1000 | Loss: 0.00002471
Iteration 161/1000 | Loss: 0.00002471
Iteration 162/1000 | Loss: 0.00002471
Iteration 163/1000 | Loss: 0.00002471
Iteration 164/1000 | Loss: 0.00002471
Iteration 165/1000 | Loss: 0.00002471
Iteration 166/1000 | Loss: 0.00002471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.470607250870671e-05, 2.470607250870671e-05, 2.470607250870671e-05, 2.470607250870671e-05, 2.470607250870671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.470607250870671e-05

Optimization complete. Final v2v error: 4.003103733062744 mm

Highest mean error: 4.751350402832031 mm for frame 60

Lowest mean error: 3.503175973892212 mm for frame 25

Saving results

Total time: 37.74192476272583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006617
Iteration 2/25 | Loss: 0.01006617
Iteration 3/25 | Loss: 0.00306226
Iteration 4/25 | Loss: 0.00216291
Iteration 5/25 | Loss: 0.00198928
Iteration 6/25 | Loss: 0.00190900
Iteration 7/25 | Loss: 0.00183974
Iteration 8/25 | Loss: 0.00178081
Iteration 9/25 | Loss: 0.00175509
Iteration 10/25 | Loss: 0.00174442
Iteration 11/25 | Loss: 0.00173055
Iteration 12/25 | Loss: 0.00169117
Iteration 13/25 | Loss: 0.00166839
Iteration 14/25 | Loss: 0.00165437
Iteration 15/25 | Loss: 0.00163995
Iteration 16/25 | Loss: 0.00162296
Iteration 17/25 | Loss: 0.00162365
Iteration 18/25 | Loss: 0.00161570
Iteration 19/25 | Loss: 0.00161026
Iteration 20/25 | Loss: 0.00160728
Iteration 21/25 | Loss: 0.00160635
Iteration 22/25 | Loss: 0.00160588
Iteration 23/25 | Loss: 0.00160550
Iteration 24/25 | Loss: 0.00160474
Iteration 25/25 | Loss: 0.00160646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35810769
Iteration 2/25 | Loss: 0.00496870
Iteration 3/25 | Loss: 0.00328064
Iteration 4/25 | Loss: 0.00328064
Iteration 5/25 | Loss: 0.00328064
Iteration 6/25 | Loss: 0.00328064
Iteration 7/25 | Loss: 0.00328064
Iteration 8/25 | Loss: 0.00328064
Iteration 9/25 | Loss: 0.00328064
Iteration 10/25 | Loss: 0.00328064
Iteration 11/25 | Loss: 0.00328064
Iteration 12/25 | Loss: 0.00328064
Iteration 13/25 | Loss: 0.00328064
Iteration 14/25 | Loss: 0.00328064
Iteration 15/25 | Loss: 0.00328064
Iteration 16/25 | Loss: 0.00328064
Iteration 17/25 | Loss: 0.00328064
Iteration 18/25 | Loss: 0.00328064
Iteration 19/25 | Loss: 0.00328064
Iteration 20/25 | Loss: 0.00328064
Iteration 21/25 | Loss: 0.00328064
Iteration 22/25 | Loss: 0.00328064
Iteration 23/25 | Loss: 0.00328064
Iteration 24/25 | Loss: 0.00328064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0032806384842842817, 0.0032806384842842817, 0.0032806384842842817, 0.0032806384842842817, 0.0032806384842842817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032806384842842817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00328064
Iteration 2/1000 | Loss: 0.00098567
Iteration 3/1000 | Loss: 0.00098778
Iteration 4/1000 | Loss: 0.00082539
Iteration 5/1000 | Loss: 0.00094616
Iteration 6/1000 | Loss: 0.00030596
Iteration 7/1000 | Loss: 0.00109129
Iteration 8/1000 | Loss: 0.00150226
Iteration 9/1000 | Loss: 0.00068514
Iteration 10/1000 | Loss: 0.00279288
Iteration 11/1000 | Loss: 0.00131965
Iteration 12/1000 | Loss: 0.00042435
Iteration 13/1000 | Loss: 0.00072645
Iteration 14/1000 | Loss: 0.00078470
Iteration 15/1000 | Loss: 0.00138439
Iteration 16/1000 | Loss: 0.00096657
Iteration 17/1000 | Loss: 0.00053311
Iteration 18/1000 | Loss: 0.00065866
Iteration 19/1000 | Loss: 0.00219677
Iteration 20/1000 | Loss: 0.00031477
Iteration 21/1000 | Loss: 0.00075926
Iteration 22/1000 | Loss: 0.00028305
Iteration 23/1000 | Loss: 0.00067502
Iteration 24/1000 | Loss: 0.00154631
Iteration 25/1000 | Loss: 0.00182879
Iteration 26/1000 | Loss: 0.00136164
Iteration 27/1000 | Loss: 0.00124635
Iteration 28/1000 | Loss: 0.00252690
Iteration 29/1000 | Loss: 0.00209979
Iteration 30/1000 | Loss: 0.00138187
Iteration 31/1000 | Loss: 0.00147993
Iteration 32/1000 | Loss: 0.00175327
Iteration 33/1000 | Loss: 0.00090182
Iteration 34/1000 | Loss: 0.00138097
Iteration 35/1000 | Loss: 0.00137480
Iteration 36/1000 | Loss: 0.00047880
Iteration 37/1000 | Loss: 0.00142062
Iteration 38/1000 | Loss: 0.00035505
Iteration 39/1000 | Loss: 0.00061511
Iteration 40/1000 | Loss: 0.00086758
Iteration 41/1000 | Loss: 0.00080201
Iteration 42/1000 | Loss: 0.00043879
Iteration 43/1000 | Loss: 0.00044466
Iteration 44/1000 | Loss: 0.00072324
Iteration 45/1000 | Loss: 0.00065282
Iteration 46/1000 | Loss: 0.00155893
Iteration 47/1000 | Loss: 0.00054663
Iteration 48/1000 | Loss: 0.00027966
Iteration 49/1000 | Loss: 0.00027438
Iteration 50/1000 | Loss: 0.00062073
Iteration 51/1000 | Loss: 0.00045173
Iteration 52/1000 | Loss: 0.00143970
Iteration 53/1000 | Loss: 0.00123017
Iteration 54/1000 | Loss: 0.00230397
Iteration 55/1000 | Loss: 0.00151556
Iteration 56/1000 | Loss: 0.00231225
Iteration 57/1000 | Loss: 0.00125455
Iteration 58/1000 | Loss: 0.00052100
Iteration 59/1000 | Loss: 0.00030071
Iteration 60/1000 | Loss: 0.00068895
Iteration 61/1000 | Loss: 0.00084322
Iteration 62/1000 | Loss: 0.00058561
Iteration 63/1000 | Loss: 0.00042351
Iteration 64/1000 | Loss: 0.00081746
Iteration 65/1000 | Loss: 0.00016017
Iteration 66/1000 | Loss: 0.00020487
Iteration 67/1000 | Loss: 0.00063378
Iteration 68/1000 | Loss: 0.00325985
Iteration 69/1000 | Loss: 0.00055930
Iteration 70/1000 | Loss: 0.00025568
Iteration 71/1000 | Loss: 0.00035637
Iteration 72/1000 | Loss: 0.00041714
Iteration 73/1000 | Loss: 0.00109998
Iteration 74/1000 | Loss: 0.00066300
Iteration 75/1000 | Loss: 0.00072169
Iteration 76/1000 | Loss: 0.00006931
Iteration 77/1000 | Loss: 0.00006468
Iteration 78/1000 | Loss: 0.00100944
Iteration 79/1000 | Loss: 0.00135564
Iteration 80/1000 | Loss: 0.00033617
Iteration 81/1000 | Loss: 0.00049096
Iteration 82/1000 | Loss: 0.00039416
Iteration 83/1000 | Loss: 0.00021492
Iteration 84/1000 | Loss: 0.00006760
Iteration 85/1000 | Loss: 0.00005298
Iteration 86/1000 | Loss: 0.00023696
Iteration 87/1000 | Loss: 0.00004801
Iteration 88/1000 | Loss: 0.00046658
Iteration 89/1000 | Loss: 0.00026483
Iteration 90/1000 | Loss: 0.00012043
Iteration 91/1000 | Loss: 0.00032349
Iteration 92/1000 | Loss: 0.00145515
Iteration 93/1000 | Loss: 0.00007397
Iteration 94/1000 | Loss: 0.00031852
Iteration 95/1000 | Loss: 0.00009814
Iteration 96/1000 | Loss: 0.00009370
Iteration 97/1000 | Loss: 0.00004429
Iteration 98/1000 | Loss: 0.00004312
Iteration 99/1000 | Loss: 0.00028530
Iteration 100/1000 | Loss: 0.00009761
Iteration 101/1000 | Loss: 0.00008703
Iteration 102/1000 | Loss: 0.00005653
Iteration 103/1000 | Loss: 0.00014398
Iteration 104/1000 | Loss: 0.00004208
Iteration 105/1000 | Loss: 0.00006341
Iteration 106/1000 | Loss: 0.00003988
Iteration 107/1000 | Loss: 0.00007134
Iteration 108/1000 | Loss: 0.00005161
Iteration 109/1000 | Loss: 0.00025151
Iteration 110/1000 | Loss: 0.00013837
Iteration 111/1000 | Loss: 0.00004212
Iteration 112/1000 | Loss: 0.00003953
Iteration 113/1000 | Loss: 0.00003839
Iteration 114/1000 | Loss: 0.00016000
Iteration 115/1000 | Loss: 0.00003837
Iteration 116/1000 | Loss: 0.00003797
Iteration 117/1000 | Loss: 0.00015339
Iteration 118/1000 | Loss: 0.00082838
Iteration 119/1000 | Loss: 0.00138166
Iteration 120/1000 | Loss: 0.00024513
Iteration 121/1000 | Loss: 0.00004223
Iteration 122/1000 | Loss: 0.00003763
Iteration 123/1000 | Loss: 0.00003565
Iteration 124/1000 | Loss: 0.00003450
Iteration 125/1000 | Loss: 0.00015087
Iteration 126/1000 | Loss: 0.00034800
Iteration 127/1000 | Loss: 0.00008533
Iteration 128/1000 | Loss: 0.00005139
Iteration 129/1000 | Loss: 0.00012332
Iteration 130/1000 | Loss: 0.00003978
Iteration 131/1000 | Loss: 0.00005994
Iteration 132/1000 | Loss: 0.00003283
Iteration 133/1000 | Loss: 0.00003216
Iteration 134/1000 | Loss: 0.00003174
Iteration 135/1000 | Loss: 0.00003134
Iteration 136/1000 | Loss: 0.00003090
Iteration 137/1000 | Loss: 0.00016746
Iteration 138/1000 | Loss: 0.00021969
Iteration 139/1000 | Loss: 0.00003180
Iteration 140/1000 | Loss: 0.00003060
Iteration 141/1000 | Loss: 0.00016016
Iteration 142/1000 | Loss: 0.00003061
Iteration 143/1000 | Loss: 0.00003019
Iteration 144/1000 | Loss: 0.00002999
Iteration 145/1000 | Loss: 0.00002977
Iteration 146/1000 | Loss: 0.00002969
Iteration 147/1000 | Loss: 0.00002968
Iteration 148/1000 | Loss: 0.00002968
Iteration 149/1000 | Loss: 0.00002957
Iteration 150/1000 | Loss: 0.00002942
Iteration 151/1000 | Loss: 0.00002925
Iteration 152/1000 | Loss: 0.00002924
Iteration 153/1000 | Loss: 0.00002923
Iteration 154/1000 | Loss: 0.00002922
Iteration 155/1000 | Loss: 0.00002921
Iteration 156/1000 | Loss: 0.00002919
Iteration 157/1000 | Loss: 0.00002919
Iteration 158/1000 | Loss: 0.00002917
Iteration 159/1000 | Loss: 0.00002914
Iteration 160/1000 | Loss: 0.00002907
Iteration 161/1000 | Loss: 0.00016183
Iteration 162/1000 | Loss: 0.00002915
Iteration 163/1000 | Loss: 0.00002886
Iteration 164/1000 | Loss: 0.00002885
Iteration 165/1000 | Loss: 0.00002884
Iteration 166/1000 | Loss: 0.00002883
Iteration 167/1000 | Loss: 0.00002883
Iteration 168/1000 | Loss: 0.00002882
Iteration 169/1000 | Loss: 0.00002881
Iteration 170/1000 | Loss: 0.00002881
Iteration 171/1000 | Loss: 0.00002880
Iteration 172/1000 | Loss: 0.00002879
Iteration 173/1000 | Loss: 0.00002879
Iteration 174/1000 | Loss: 0.00002864
Iteration 175/1000 | Loss: 0.00002864
Iteration 176/1000 | Loss: 0.00002859
Iteration 177/1000 | Loss: 0.00002846
Iteration 178/1000 | Loss: 0.00002830
Iteration 179/1000 | Loss: 0.00024633
Iteration 180/1000 | Loss: 0.00002863
Iteration 181/1000 | Loss: 0.00002802
Iteration 182/1000 | Loss: 0.00002785
Iteration 183/1000 | Loss: 0.00002785
Iteration 184/1000 | Loss: 0.00002784
Iteration 185/1000 | Loss: 0.00002773
Iteration 186/1000 | Loss: 0.00016040
Iteration 187/1000 | Loss: 0.00070038
Iteration 188/1000 | Loss: 0.00034653
Iteration 189/1000 | Loss: 0.00017653
Iteration 190/1000 | Loss: 0.00017480
Iteration 191/1000 | Loss: 0.00066972
Iteration 192/1000 | Loss: 0.00007974
Iteration 193/1000 | Loss: 0.00003470
Iteration 194/1000 | Loss: 0.00003148
Iteration 195/1000 | Loss: 0.00032488
Iteration 196/1000 | Loss: 0.00056696
Iteration 197/1000 | Loss: 0.00033843
Iteration 198/1000 | Loss: 0.00004236
Iteration 199/1000 | Loss: 0.00002839
Iteration 200/1000 | Loss: 0.00003774
Iteration 201/1000 | Loss: 0.00017665
Iteration 202/1000 | Loss: 0.00027227
Iteration 203/1000 | Loss: 0.00008248
Iteration 204/1000 | Loss: 0.00003567
Iteration 205/1000 | Loss: 0.00048251
Iteration 206/1000 | Loss: 0.00061238
Iteration 207/1000 | Loss: 0.00030237
Iteration 208/1000 | Loss: 0.00020287
Iteration 209/1000 | Loss: 0.00039554
Iteration 210/1000 | Loss: 0.00040326
Iteration 211/1000 | Loss: 0.00040589
Iteration 212/1000 | Loss: 0.00016345
Iteration 213/1000 | Loss: 0.00013245
Iteration 214/1000 | Loss: 0.00003603
Iteration 215/1000 | Loss: 0.00016553
Iteration 216/1000 | Loss: 0.00021196
Iteration 217/1000 | Loss: 0.00045834
Iteration 218/1000 | Loss: 0.00017830
Iteration 219/1000 | Loss: 0.00004710
Iteration 220/1000 | Loss: 0.00006004
Iteration 221/1000 | Loss: 0.00002962
Iteration 222/1000 | Loss: 0.00019345
Iteration 223/1000 | Loss: 0.00003101
Iteration 224/1000 | Loss: 0.00002722
Iteration 225/1000 | Loss: 0.00002506
Iteration 226/1000 | Loss: 0.00015482
Iteration 227/1000 | Loss: 0.00017752
Iteration 228/1000 | Loss: 0.00002920
Iteration 229/1000 | Loss: 0.00002273
Iteration 230/1000 | Loss: 0.00014113
Iteration 231/1000 | Loss: 0.00002837
Iteration 232/1000 | Loss: 0.00002476
Iteration 233/1000 | Loss: 0.00002322
Iteration 234/1000 | Loss: 0.00002192
Iteration 235/1000 | Loss: 0.00002147
Iteration 236/1000 | Loss: 0.00002099
Iteration 237/1000 | Loss: 0.00002040
Iteration 238/1000 | Loss: 0.00002014
Iteration 239/1000 | Loss: 0.00002010
Iteration 240/1000 | Loss: 0.00002007
Iteration 241/1000 | Loss: 0.00001997
Iteration 242/1000 | Loss: 0.00001989
Iteration 243/1000 | Loss: 0.00002931
Iteration 244/1000 | Loss: 0.00002235
Iteration 245/1000 | Loss: 0.00002060
Iteration 246/1000 | Loss: 0.00002002
Iteration 247/1000 | Loss: 0.00001975
Iteration 248/1000 | Loss: 0.00001961
Iteration 249/1000 | Loss: 0.00001960
Iteration 250/1000 | Loss: 0.00001960
Iteration 251/1000 | Loss: 0.00001960
Iteration 252/1000 | Loss: 0.00001955
Iteration 253/1000 | Loss: 0.00001954
Iteration 254/1000 | Loss: 0.00001954
Iteration 255/1000 | Loss: 0.00001953
Iteration 256/1000 | Loss: 0.00001953
Iteration 257/1000 | Loss: 0.00001953
Iteration 258/1000 | Loss: 0.00001953
Iteration 259/1000 | Loss: 0.00001953
Iteration 260/1000 | Loss: 0.00001953
Iteration 261/1000 | Loss: 0.00001953
Iteration 262/1000 | Loss: 0.00001951
Iteration 263/1000 | Loss: 0.00001951
Iteration 264/1000 | Loss: 0.00001951
Iteration 265/1000 | Loss: 0.00001951
Iteration 266/1000 | Loss: 0.00001950
Iteration 267/1000 | Loss: 0.00001950
Iteration 268/1000 | Loss: 0.00001950
Iteration 269/1000 | Loss: 0.00001950
Iteration 270/1000 | Loss: 0.00001950
Iteration 271/1000 | Loss: 0.00001950
Iteration 272/1000 | Loss: 0.00001950
Iteration 273/1000 | Loss: 0.00001950
Iteration 274/1000 | Loss: 0.00001950
Iteration 275/1000 | Loss: 0.00001950
Iteration 276/1000 | Loss: 0.00001950
Iteration 277/1000 | Loss: 0.00001950
Iteration 278/1000 | Loss: 0.00001950
Iteration 279/1000 | Loss: 0.00001950
Iteration 280/1000 | Loss: 0.00001950
Iteration 281/1000 | Loss: 0.00001950
Iteration 282/1000 | Loss: 0.00001950
Iteration 283/1000 | Loss: 0.00001950
Iteration 284/1000 | Loss: 0.00001950
Iteration 285/1000 | Loss: 0.00001950
Iteration 286/1000 | Loss: 0.00001950
Iteration 287/1000 | Loss: 0.00001950
Iteration 288/1000 | Loss: 0.00001950
Iteration 289/1000 | Loss: 0.00001950
Iteration 290/1000 | Loss: 0.00001950
Iteration 291/1000 | Loss: 0.00001950
Iteration 292/1000 | Loss: 0.00001950
Iteration 293/1000 | Loss: 0.00001950
Iteration 294/1000 | Loss: 0.00001950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.9504968804540113e-05, 1.9504968804540113e-05, 1.9504968804540113e-05, 1.9504968804540113e-05, 1.9504968804540113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9504968804540113e-05

Optimization complete. Final v2v error: 3.5567357540130615 mm

Highest mean error: 12.695989608764648 mm for frame 119

Lowest mean error: 3.183580160140991 mm for frame 209

Saving results

Total time: 405.9052560329437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953870
Iteration 2/25 | Loss: 0.00179797
Iteration 3/25 | Loss: 0.00151095
Iteration 4/25 | Loss: 0.00148706
Iteration 5/25 | Loss: 0.00147905
Iteration 6/25 | Loss: 0.00147845
Iteration 7/25 | Loss: 0.00147845
Iteration 8/25 | Loss: 0.00147845
Iteration 9/25 | Loss: 0.00147810
Iteration 10/25 | Loss: 0.00147810
Iteration 11/25 | Loss: 0.00147810
Iteration 12/25 | Loss: 0.00147810
Iteration 13/25 | Loss: 0.00147810
Iteration 14/25 | Loss: 0.00147810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014781035715714097, 0.0014781035715714097, 0.0014781035715714097, 0.0014781035715714097, 0.0014781035715714097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014781035715714097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84687388
Iteration 2/25 | Loss: 0.00106101
Iteration 3/25 | Loss: 0.00106100
Iteration 4/25 | Loss: 0.00106100
Iteration 5/25 | Loss: 0.00106100
Iteration 6/25 | Loss: 0.00106100
Iteration 7/25 | Loss: 0.00106100
Iteration 8/25 | Loss: 0.00106100
Iteration 9/25 | Loss: 0.00106100
Iteration 10/25 | Loss: 0.00106100
Iteration 11/25 | Loss: 0.00106100
Iteration 12/25 | Loss: 0.00106100
Iteration 13/25 | Loss: 0.00106100
Iteration 14/25 | Loss: 0.00106100
Iteration 15/25 | Loss: 0.00106100
Iteration 16/25 | Loss: 0.00106100
Iteration 17/25 | Loss: 0.00106100
Iteration 18/25 | Loss: 0.00106100
Iteration 19/25 | Loss: 0.00106100
Iteration 20/25 | Loss: 0.00106100
Iteration 21/25 | Loss: 0.00106100
Iteration 22/25 | Loss: 0.00106100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010609989985823631, 0.0010609989985823631, 0.0010609989985823631, 0.0010609989985823631, 0.0010609989985823631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010609989985823631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106100
Iteration 2/1000 | Loss: 0.00006694
Iteration 3/1000 | Loss: 0.00005093
Iteration 4/1000 | Loss: 0.00004646
Iteration 5/1000 | Loss: 0.00004417
Iteration 6/1000 | Loss: 0.00004284
Iteration 7/1000 | Loss: 0.00004163
Iteration 8/1000 | Loss: 0.00004101
Iteration 9/1000 | Loss: 0.00004039
Iteration 10/1000 | Loss: 0.00003985
Iteration 11/1000 | Loss: 0.00003935
Iteration 12/1000 | Loss: 0.00003887
Iteration 13/1000 | Loss: 0.00003846
Iteration 14/1000 | Loss: 0.00003813
Iteration 15/1000 | Loss: 0.00003777
Iteration 16/1000 | Loss: 0.00003740
Iteration 17/1000 | Loss: 0.00003712
Iteration 18/1000 | Loss: 0.00003691
Iteration 19/1000 | Loss: 0.00003679
Iteration 20/1000 | Loss: 0.00003662
Iteration 21/1000 | Loss: 0.00003662
Iteration 22/1000 | Loss: 0.00003655
Iteration 23/1000 | Loss: 0.00003647
Iteration 24/1000 | Loss: 0.00003644
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003642
Iteration 27/1000 | Loss: 0.00003642
Iteration 28/1000 | Loss: 0.00003641
Iteration 29/1000 | Loss: 0.00003641
Iteration 30/1000 | Loss: 0.00003635
Iteration 31/1000 | Loss: 0.00003635
Iteration 32/1000 | Loss: 0.00003634
Iteration 33/1000 | Loss: 0.00003634
Iteration 34/1000 | Loss: 0.00003633
Iteration 35/1000 | Loss: 0.00003633
Iteration 36/1000 | Loss: 0.00003633
Iteration 37/1000 | Loss: 0.00003631
Iteration 38/1000 | Loss: 0.00003631
Iteration 39/1000 | Loss: 0.00003631
Iteration 40/1000 | Loss: 0.00003631
Iteration 41/1000 | Loss: 0.00003631
Iteration 42/1000 | Loss: 0.00003631
Iteration 43/1000 | Loss: 0.00003631
Iteration 44/1000 | Loss: 0.00003631
Iteration 45/1000 | Loss: 0.00003631
Iteration 46/1000 | Loss: 0.00003631
Iteration 47/1000 | Loss: 0.00003630
Iteration 48/1000 | Loss: 0.00003630
Iteration 49/1000 | Loss: 0.00003630
Iteration 50/1000 | Loss: 0.00003630
Iteration 51/1000 | Loss: 0.00003630
Iteration 52/1000 | Loss: 0.00003629
Iteration 53/1000 | Loss: 0.00003629
Iteration 54/1000 | Loss: 0.00003628
Iteration 55/1000 | Loss: 0.00003628
Iteration 56/1000 | Loss: 0.00003627
Iteration 57/1000 | Loss: 0.00003627
Iteration 58/1000 | Loss: 0.00003627
Iteration 59/1000 | Loss: 0.00003626
Iteration 60/1000 | Loss: 0.00003626
Iteration 61/1000 | Loss: 0.00003626
Iteration 62/1000 | Loss: 0.00003626
Iteration 63/1000 | Loss: 0.00003626
Iteration 64/1000 | Loss: 0.00003625
Iteration 65/1000 | Loss: 0.00003625
Iteration 66/1000 | Loss: 0.00003624
Iteration 67/1000 | Loss: 0.00003624
Iteration 68/1000 | Loss: 0.00003624
Iteration 69/1000 | Loss: 0.00003624
Iteration 70/1000 | Loss: 0.00003624
Iteration 71/1000 | Loss: 0.00003623
Iteration 72/1000 | Loss: 0.00003623
Iteration 73/1000 | Loss: 0.00003623
Iteration 74/1000 | Loss: 0.00003623
Iteration 75/1000 | Loss: 0.00003623
Iteration 76/1000 | Loss: 0.00003623
Iteration 77/1000 | Loss: 0.00003623
Iteration 78/1000 | Loss: 0.00003623
Iteration 79/1000 | Loss: 0.00003623
Iteration 80/1000 | Loss: 0.00003621
Iteration 81/1000 | Loss: 0.00003621
Iteration 82/1000 | Loss: 0.00003621
Iteration 83/1000 | Loss: 0.00003620
Iteration 84/1000 | Loss: 0.00003620
Iteration 85/1000 | Loss: 0.00003620
Iteration 86/1000 | Loss: 0.00003619
Iteration 87/1000 | Loss: 0.00003619
Iteration 88/1000 | Loss: 0.00003619
Iteration 89/1000 | Loss: 0.00003619
Iteration 90/1000 | Loss: 0.00003619
Iteration 91/1000 | Loss: 0.00003618
Iteration 92/1000 | Loss: 0.00003618
Iteration 93/1000 | Loss: 0.00003618
Iteration 94/1000 | Loss: 0.00003618
Iteration 95/1000 | Loss: 0.00003618
Iteration 96/1000 | Loss: 0.00003618
Iteration 97/1000 | Loss: 0.00003618
Iteration 98/1000 | Loss: 0.00003618
Iteration 99/1000 | Loss: 0.00003618
Iteration 100/1000 | Loss: 0.00003618
Iteration 101/1000 | Loss: 0.00003618
Iteration 102/1000 | Loss: 0.00003617
Iteration 103/1000 | Loss: 0.00003617
Iteration 104/1000 | Loss: 0.00003617
Iteration 105/1000 | Loss: 0.00003617
Iteration 106/1000 | Loss: 0.00003617
Iteration 107/1000 | Loss: 0.00003616
Iteration 108/1000 | Loss: 0.00003616
Iteration 109/1000 | Loss: 0.00003616
Iteration 110/1000 | Loss: 0.00003616
Iteration 111/1000 | Loss: 0.00003616
Iteration 112/1000 | Loss: 0.00003615
Iteration 113/1000 | Loss: 0.00003615
Iteration 114/1000 | Loss: 0.00003615
Iteration 115/1000 | Loss: 0.00003615
Iteration 116/1000 | Loss: 0.00003615
Iteration 117/1000 | Loss: 0.00003615
Iteration 118/1000 | Loss: 0.00003615
Iteration 119/1000 | Loss: 0.00003615
Iteration 120/1000 | Loss: 0.00003615
Iteration 121/1000 | Loss: 0.00003615
Iteration 122/1000 | Loss: 0.00003615
Iteration 123/1000 | Loss: 0.00003615
Iteration 124/1000 | Loss: 0.00003615
Iteration 125/1000 | Loss: 0.00003615
Iteration 126/1000 | Loss: 0.00003615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [3.614727029344067e-05, 3.614727029344067e-05, 3.614727029344067e-05, 3.614727029344067e-05, 3.614727029344067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.614727029344067e-05

Optimization complete. Final v2v error: 5.061610698699951 mm

Highest mean error: 5.557627201080322 mm for frame 96

Lowest mean error: 4.19726037979126 mm for frame 48

Saving results

Total time: 53.81762647628784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953425
Iteration 2/25 | Loss: 0.00425452
Iteration 3/25 | Loss: 0.00298549
Iteration 4/25 | Loss: 0.00252614
Iteration 5/25 | Loss: 0.00247218
Iteration 6/25 | Loss: 0.00237646
Iteration 7/25 | Loss: 0.00231393
Iteration 8/25 | Loss: 0.00216208
Iteration 9/25 | Loss: 0.00210231
Iteration 10/25 | Loss: 0.00202135
Iteration 11/25 | Loss: 0.00195089
Iteration 12/25 | Loss: 0.00190324
Iteration 13/25 | Loss: 0.00190256
Iteration 14/25 | Loss: 0.00185741
Iteration 15/25 | Loss: 0.00183809
Iteration 16/25 | Loss: 0.00182168
Iteration 17/25 | Loss: 0.00181383
Iteration 18/25 | Loss: 0.00181909
Iteration 19/25 | Loss: 0.00180615
Iteration 20/25 | Loss: 0.00178678
Iteration 21/25 | Loss: 0.00177052
Iteration 22/25 | Loss: 0.00178243
Iteration 23/25 | Loss: 0.00177940
Iteration 24/25 | Loss: 0.00176249
Iteration 25/25 | Loss: 0.00175542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47714710
Iteration 2/25 | Loss: 0.00806271
Iteration 3/25 | Loss: 0.00546146
Iteration 4/25 | Loss: 0.00546146
Iteration 5/25 | Loss: 0.00546146
Iteration 6/25 | Loss: 0.00546146
Iteration 7/25 | Loss: 0.00546146
Iteration 8/25 | Loss: 0.00546146
Iteration 9/25 | Loss: 0.00546145
Iteration 10/25 | Loss: 0.00546145
Iteration 11/25 | Loss: 0.00546145
Iteration 12/25 | Loss: 0.00546145
Iteration 13/25 | Loss: 0.00546145
Iteration 14/25 | Loss: 0.00546145
Iteration 15/25 | Loss: 0.00546145
Iteration 16/25 | Loss: 0.00546145
Iteration 17/25 | Loss: 0.00546145
Iteration 18/25 | Loss: 0.00546145
Iteration 19/25 | Loss: 0.00546145
Iteration 20/25 | Loss: 0.00546145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.005461453925818205, 0.005461453925818205, 0.005461453925818205, 0.005461453925818205, 0.005461453925818205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005461453925818205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00546145
Iteration 2/1000 | Loss: 0.00380518
Iteration 3/1000 | Loss: 0.00566074
Iteration 4/1000 | Loss: 0.00134490
Iteration 5/1000 | Loss: 0.00110224
Iteration 6/1000 | Loss: 0.00124190
Iteration 7/1000 | Loss: 0.00052844
Iteration 8/1000 | Loss: 0.00123928
Iteration 9/1000 | Loss: 0.00298589
Iteration 10/1000 | Loss: 0.00135914
Iteration 11/1000 | Loss: 0.00289633
Iteration 12/1000 | Loss: 0.00499906
Iteration 13/1000 | Loss: 0.00087639
Iteration 14/1000 | Loss: 0.00076910
Iteration 15/1000 | Loss: 0.00151198
Iteration 16/1000 | Loss: 0.00055537
Iteration 17/1000 | Loss: 0.00062942
Iteration 18/1000 | Loss: 0.00051367
Iteration 19/1000 | Loss: 0.00111698
Iteration 20/1000 | Loss: 0.00017532
Iteration 21/1000 | Loss: 0.00026623
Iteration 22/1000 | Loss: 0.00084036
Iteration 23/1000 | Loss: 0.00208756
Iteration 24/1000 | Loss: 0.00032074
Iteration 25/1000 | Loss: 0.00034562
Iteration 26/1000 | Loss: 0.00088564
Iteration 27/1000 | Loss: 0.00047750
Iteration 28/1000 | Loss: 0.00013058
Iteration 29/1000 | Loss: 0.00020955
Iteration 30/1000 | Loss: 0.00053325
Iteration 31/1000 | Loss: 0.00102577
Iteration 32/1000 | Loss: 0.00033075
Iteration 33/1000 | Loss: 0.00045452
Iteration 34/1000 | Loss: 0.00023280
Iteration 35/1000 | Loss: 0.00025659
Iteration 36/1000 | Loss: 0.00011258
Iteration 37/1000 | Loss: 0.00018530
Iteration 38/1000 | Loss: 0.00022164
Iteration 39/1000 | Loss: 0.00016850
Iteration 40/1000 | Loss: 0.00023222
Iteration 41/1000 | Loss: 0.00104840
Iteration 42/1000 | Loss: 0.00014947
Iteration 43/1000 | Loss: 0.00021946
Iteration 44/1000 | Loss: 0.00013443
Iteration 45/1000 | Loss: 0.00009648
Iteration 46/1000 | Loss: 0.00019174
Iteration 47/1000 | Loss: 0.00012241
Iteration 48/1000 | Loss: 0.00025552
Iteration 49/1000 | Loss: 0.00451303
Iteration 50/1000 | Loss: 0.00104045
Iteration 51/1000 | Loss: 0.00102402
Iteration 52/1000 | Loss: 0.00135228
Iteration 53/1000 | Loss: 0.00042494
Iteration 54/1000 | Loss: 0.00075451
Iteration 55/1000 | Loss: 0.00117851
Iteration 56/1000 | Loss: 0.00014079
Iteration 57/1000 | Loss: 0.00030274
Iteration 58/1000 | Loss: 0.00033979
Iteration 59/1000 | Loss: 0.00009961
Iteration 60/1000 | Loss: 0.00026029
Iteration 61/1000 | Loss: 0.00035896
Iteration 62/1000 | Loss: 0.00070117
Iteration 63/1000 | Loss: 0.00060129
Iteration 64/1000 | Loss: 0.00026458
Iteration 65/1000 | Loss: 0.00036607
Iteration 66/1000 | Loss: 0.00034296
Iteration 67/1000 | Loss: 0.00010225
Iteration 68/1000 | Loss: 0.00028001
Iteration 69/1000 | Loss: 0.00020288
Iteration 70/1000 | Loss: 0.00008298
Iteration 71/1000 | Loss: 0.00016251
Iteration 72/1000 | Loss: 0.00028027
Iteration 73/1000 | Loss: 0.00043151
Iteration 74/1000 | Loss: 0.00009103
Iteration 75/1000 | Loss: 0.00011653
Iteration 76/1000 | Loss: 0.00010436
Iteration 77/1000 | Loss: 0.00037237
Iteration 78/1000 | Loss: 0.00018591
Iteration 79/1000 | Loss: 0.00009634
Iteration 80/1000 | Loss: 0.00021338
Iteration 81/1000 | Loss: 0.00097843
Iteration 82/1000 | Loss: 0.00037693
Iteration 83/1000 | Loss: 0.00037472
Iteration 84/1000 | Loss: 0.00010143
Iteration 85/1000 | Loss: 0.00013399
Iteration 86/1000 | Loss: 0.00008122
Iteration 87/1000 | Loss: 0.00017224
Iteration 88/1000 | Loss: 0.00016409
Iteration 89/1000 | Loss: 0.00008415
Iteration 90/1000 | Loss: 0.00007351
Iteration 91/1000 | Loss: 0.00008407
Iteration 92/1000 | Loss: 0.00028289
Iteration 93/1000 | Loss: 0.00007570
Iteration 94/1000 | Loss: 0.00007644
Iteration 95/1000 | Loss: 0.00028146
Iteration 96/1000 | Loss: 0.00147018
Iteration 97/1000 | Loss: 0.00023203
Iteration 98/1000 | Loss: 0.00012527
Iteration 99/1000 | Loss: 0.00007821
Iteration 100/1000 | Loss: 0.00006964
Iteration 101/1000 | Loss: 0.00007976
Iteration 102/1000 | Loss: 0.00007597
Iteration 103/1000 | Loss: 0.00007655
Iteration 104/1000 | Loss: 0.00007257
Iteration 105/1000 | Loss: 0.00007162
Iteration 106/1000 | Loss: 0.00006905
Iteration 107/1000 | Loss: 0.00006618
Iteration 108/1000 | Loss: 0.00006857
Iteration 109/1000 | Loss: 0.00006699
Iteration 110/1000 | Loss: 0.00023264
Iteration 111/1000 | Loss: 0.00007329
Iteration 112/1000 | Loss: 0.00030342
Iteration 113/1000 | Loss: 0.00006593
Iteration 114/1000 | Loss: 0.00006633
Iteration 115/1000 | Loss: 0.00007246
Iteration 116/1000 | Loss: 0.00016074
Iteration 117/1000 | Loss: 0.00035275
Iteration 118/1000 | Loss: 0.00027711
Iteration 119/1000 | Loss: 0.00019435
Iteration 120/1000 | Loss: 0.00019723
Iteration 121/1000 | Loss: 0.00018022
Iteration 122/1000 | Loss: 0.00006158
Iteration 123/1000 | Loss: 0.00006306
Iteration 124/1000 | Loss: 0.00007857
Iteration 125/1000 | Loss: 0.00005975
Iteration 126/1000 | Loss: 0.00006114
Iteration 127/1000 | Loss: 0.00007330
Iteration 128/1000 | Loss: 0.00023672
Iteration 129/1000 | Loss: 0.00040381
Iteration 130/1000 | Loss: 0.00009749
Iteration 131/1000 | Loss: 0.00008864
Iteration 132/1000 | Loss: 0.00007743
Iteration 133/1000 | Loss: 0.00027574
Iteration 134/1000 | Loss: 0.00027530
Iteration 135/1000 | Loss: 0.00022099
Iteration 136/1000 | Loss: 0.00021168
Iteration 137/1000 | Loss: 0.00021510
Iteration 138/1000 | Loss: 0.00031261
Iteration 139/1000 | Loss: 0.00007496
Iteration 140/1000 | Loss: 0.00006169
Iteration 141/1000 | Loss: 0.00005969
Iteration 142/1000 | Loss: 0.00010859
Iteration 143/1000 | Loss: 0.00031544
Iteration 144/1000 | Loss: 0.00006857
Iteration 145/1000 | Loss: 0.00005722
Iteration 146/1000 | Loss: 0.00015430
Iteration 147/1000 | Loss: 0.00005756
Iteration 148/1000 | Loss: 0.00006343
Iteration 149/1000 | Loss: 0.00014675
Iteration 150/1000 | Loss: 0.00005725
Iteration 151/1000 | Loss: 0.00005670
Iteration 152/1000 | Loss: 0.00005629
Iteration 153/1000 | Loss: 0.00005595
Iteration 154/1000 | Loss: 0.00005571
Iteration 155/1000 | Loss: 0.00005571
Iteration 156/1000 | Loss: 0.00005558
Iteration 157/1000 | Loss: 0.00005539
Iteration 158/1000 | Loss: 0.00005523
Iteration 159/1000 | Loss: 0.00005523
Iteration 160/1000 | Loss: 0.00005522
Iteration 161/1000 | Loss: 0.00005518
Iteration 162/1000 | Loss: 0.00005515
Iteration 163/1000 | Loss: 0.00005513
Iteration 164/1000 | Loss: 0.00005507
Iteration 165/1000 | Loss: 0.00005504
Iteration 166/1000 | Loss: 0.00022037
Iteration 167/1000 | Loss: 0.00109756
Iteration 168/1000 | Loss: 0.00035370
Iteration 169/1000 | Loss: 0.00109750
Iteration 170/1000 | Loss: 0.00252332
Iteration 171/1000 | Loss: 0.00034690
Iteration 172/1000 | Loss: 0.00131036
Iteration 173/1000 | Loss: 0.00042898
Iteration 174/1000 | Loss: 0.00026600
Iteration 175/1000 | Loss: 0.00043964
Iteration 176/1000 | Loss: 0.00067520
Iteration 177/1000 | Loss: 0.00023705
Iteration 178/1000 | Loss: 0.00040272
Iteration 179/1000 | Loss: 0.00026178
Iteration 180/1000 | Loss: 0.00024342
Iteration 181/1000 | Loss: 0.00026981
Iteration 182/1000 | Loss: 0.00011444
Iteration 183/1000 | Loss: 0.00014307
Iteration 184/1000 | Loss: 0.00032091
Iteration 185/1000 | Loss: 0.00013946
Iteration 186/1000 | Loss: 0.00006936
Iteration 187/1000 | Loss: 0.00005559
Iteration 188/1000 | Loss: 0.00006549
Iteration 189/1000 | Loss: 0.00019175
Iteration 190/1000 | Loss: 0.00013723
Iteration 191/1000 | Loss: 0.00012667
Iteration 192/1000 | Loss: 0.00012746
Iteration 193/1000 | Loss: 0.00009358
Iteration 194/1000 | Loss: 0.00020773
Iteration 195/1000 | Loss: 0.00018551
Iteration 196/1000 | Loss: 0.00008365
Iteration 197/1000 | Loss: 0.00012258
Iteration 198/1000 | Loss: 0.00008052
Iteration 199/1000 | Loss: 0.00011826
Iteration 200/1000 | Loss: 0.00004548
Iteration 201/1000 | Loss: 0.00055330
Iteration 202/1000 | Loss: 0.00141885
Iteration 203/1000 | Loss: 0.00043404
Iteration 204/1000 | Loss: 0.00005317
Iteration 205/1000 | Loss: 0.00004796
Iteration 206/1000 | Loss: 0.00017118
Iteration 207/1000 | Loss: 0.00006866
Iteration 208/1000 | Loss: 0.00004573
Iteration 209/1000 | Loss: 0.00004489
Iteration 210/1000 | Loss: 0.00004327
Iteration 211/1000 | Loss: 0.00013608
Iteration 212/1000 | Loss: 0.00004227
Iteration 213/1000 | Loss: 0.00013271
Iteration 214/1000 | Loss: 0.00004184
Iteration 215/1000 | Loss: 0.00004157
Iteration 216/1000 | Loss: 0.00004126
Iteration 217/1000 | Loss: 0.00004109
Iteration 218/1000 | Loss: 0.00019805
Iteration 219/1000 | Loss: 0.00026213
Iteration 220/1000 | Loss: 0.00016735
Iteration 221/1000 | Loss: 0.00004632
Iteration 222/1000 | Loss: 0.00004411
Iteration 223/1000 | Loss: 0.00021426
Iteration 224/1000 | Loss: 0.00004298
Iteration 225/1000 | Loss: 0.00004192
Iteration 226/1000 | Loss: 0.00014696
Iteration 227/1000 | Loss: 0.00004166
Iteration 228/1000 | Loss: 0.00004119
Iteration 229/1000 | Loss: 0.00004094
Iteration 230/1000 | Loss: 0.00018096
Iteration 231/1000 | Loss: 0.00005521
Iteration 232/1000 | Loss: 0.00004246
Iteration 233/1000 | Loss: 0.00004087
Iteration 234/1000 | Loss: 0.00004084
Iteration 235/1000 | Loss: 0.00004069
Iteration 236/1000 | Loss: 0.00004064
Iteration 237/1000 | Loss: 0.00004058
Iteration 238/1000 | Loss: 0.00004049
Iteration 239/1000 | Loss: 0.00004045
Iteration 240/1000 | Loss: 0.00004045
Iteration 241/1000 | Loss: 0.00004023
Iteration 242/1000 | Loss: 0.00004019
Iteration 243/1000 | Loss: 0.00004497
Iteration 244/1000 | Loss: 0.00004036
Iteration 245/1000 | Loss: 0.00003993
Iteration 246/1000 | Loss: 0.00003975
Iteration 247/1000 | Loss: 0.00003971
Iteration 248/1000 | Loss: 0.00003953
Iteration 249/1000 | Loss: 0.00003948
Iteration 250/1000 | Loss: 0.00003941
Iteration 251/1000 | Loss: 0.00003933
Iteration 252/1000 | Loss: 0.00003930
Iteration 253/1000 | Loss: 0.00003930
Iteration 254/1000 | Loss: 0.00003929
Iteration 255/1000 | Loss: 0.00003925
Iteration 256/1000 | Loss: 0.00003917
Iteration 257/1000 | Loss: 0.00003913
Iteration 258/1000 | Loss: 0.00003910
Iteration 259/1000 | Loss: 0.00003910
Iteration 260/1000 | Loss: 0.00003906
Iteration 261/1000 | Loss: 0.00003902
Iteration 262/1000 | Loss: 0.00003899
Iteration 263/1000 | Loss: 0.00003895
Iteration 264/1000 | Loss: 0.00003891
Iteration 265/1000 | Loss: 0.00003888
Iteration 266/1000 | Loss: 0.00003887
Iteration 267/1000 | Loss: 0.00003887
Iteration 268/1000 | Loss: 0.00003887
Iteration 269/1000 | Loss: 0.00003887
Iteration 270/1000 | Loss: 0.00003887
Iteration 271/1000 | Loss: 0.00003887
Iteration 272/1000 | Loss: 0.00003887
Iteration 273/1000 | Loss: 0.00003886
Iteration 274/1000 | Loss: 0.00003886
Iteration 275/1000 | Loss: 0.00003885
Iteration 276/1000 | Loss: 0.00003885
Iteration 277/1000 | Loss: 0.00003885
Iteration 278/1000 | Loss: 0.00003885
Iteration 279/1000 | Loss: 0.00003885
Iteration 280/1000 | Loss: 0.00003885
Iteration 281/1000 | Loss: 0.00003885
Iteration 282/1000 | Loss: 0.00003884
Iteration 283/1000 | Loss: 0.00013115
Iteration 284/1000 | Loss: 0.00003889
Iteration 285/1000 | Loss: 0.00003873
Iteration 286/1000 | Loss: 0.00003870
Iteration 287/1000 | Loss: 0.00003869
Iteration 288/1000 | Loss: 0.00003865
Iteration 289/1000 | Loss: 0.00003864
Iteration 290/1000 | Loss: 0.00003864
Iteration 291/1000 | Loss: 0.00003864
Iteration 292/1000 | Loss: 0.00003864
Iteration 293/1000 | Loss: 0.00003864
Iteration 294/1000 | Loss: 0.00003864
Iteration 295/1000 | Loss: 0.00003864
Iteration 296/1000 | Loss: 0.00003864
Iteration 297/1000 | Loss: 0.00003863
Iteration 298/1000 | Loss: 0.00003859
Iteration 299/1000 | Loss: 0.00003859
Iteration 300/1000 | Loss: 0.00003859
Iteration 301/1000 | Loss: 0.00003847
Iteration 302/1000 | Loss: 0.00003844
Iteration 303/1000 | Loss: 0.00003844
Iteration 304/1000 | Loss: 0.00003844
Iteration 305/1000 | Loss: 0.00003844
Iteration 306/1000 | Loss: 0.00003828
Iteration 307/1000 | Loss: 0.00003825
Iteration 308/1000 | Loss: 0.00003818
Iteration 309/1000 | Loss: 0.00020885
Iteration 310/1000 | Loss: 0.00007391
Iteration 311/1000 | Loss: 0.00011827
Iteration 312/1000 | Loss: 0.00003923
Iteration 313/1000 | Loss: 0.00003850
Iteration 314/1000 | Loss: 0.00003839
Iteration 315/1000 | Loss: 0.00003818
Iteration 316/1000 | Loss: 0.00021961
Iteration 317/1000 | Loss: 0.00009739
Iteration 318/1000 | Loss: 0.00003878
Iteration 319/1000 | Loss: 0.00003827
Iteration 320/1000 | Loss: 0.00021540
Iteration 321/1000 | Loss: 0.00019410
Iteration 322/1000 | Loss: 0.00003822
Iteration 323/1000 | Loss: 0.00005295
Iteration 324/1000 | Loss: 0.00013340
Iteration 325/1000 | Loss: 0.00006567
Iteration 326/1000 | Loss: 0.00003625
Iteration 327/1000 | Loss: 0.00003584
Iteration 328/1000 | Loss: 0.00003552
Iteration 329/1000 | Loss: 0.00020687
Iteration 330/1000 | Loss: 0.00021055
Iteration 331/1000 | Loss: 0.00003640
Iteration 332/1000 | Loss: 0.00003538
Iteration 333/1000 | Loss: 0.00025258
Iteration 334/1000 | Loss: 0.00012679
Iteration 335/1000 | Loss: 0.00003833
Iteration 336/1000 | Loss: 0.00003717
Iteration 337/1000 | Loss: 0.00009784
Iteration 338/1000 | Loss: 0.00003674
Iteration 339/1000 | Loss: 0.00003638
Iteration 340/1000 | Loss: 0.00003617
Iteration 341/1000 | Loss: 0.00011250
Iteration 342/1000 | Loss: 0.00003685
Iteration 343/1000 | Loss: 0.00003601
Iteration 344/1000 | Loss: 0.00003589
Iteration 345/1000 | Loss: 0.00003587
Iteration 346/1000 | Loss: 0.00003586
Iteration 347/1000 | Loss: 0.00003585
Iteration 348/1000 | Loss: 0.00003585
Iteration 349/1000 | Loss: 0.00003585
Iteration 350/1000 | Loss: 0.00003585
Iteration 351/1000 | Loss: 0.00003585
Iteration 352/1000 | Loss: 0.00003584
Iteration 353/1000 | Loss: 0.00003584
Iteration 354/1000 | Loss: 0.00003584
Iteration 355/1000 | Loss: 0.00003584
Iteration 356/1000 | Loss: 0.00003583
Iteration 357/1000 | Loss: 0.00003583
Iteration 358/1000 | Loss: 0.00003582
Iteration 359/1000 | Loss: 0.00003580
Iteration 360/1000 | Loss: 0.00003578
Iteration 361/1000 | Loss: 0.00003578
Iteration 362/1000 | Loss: 0.00003577
Iteration 363/1000 | Loss: 0.00003577
Iteration 364/1000 | Loss: 0.00010179
Iteration 365/1000 | Loss: 0.00004440
Iteration 366/1000 | Loss: 0.00014771
Iteration 367/1000 | Loss: 0.00037932
Iteration 368/1000 | Loss: 0.00003820
Iteration 369/1000 | Loss: 0.00003616
Iteration 370/1000 | Loss: 0.00004430
Iteration 371/1000 | Loss: 0.00003579
Iteration 372/1000 | Loss: 0.00003577
Iteration 373/1000 | Loss: 0.00003569
Iteration 374/1000 | Loss: 0.00003569
Iteration 375/1000 | Loss: 0.00003564
Iteration 376/1000 | Loss: 0.00003564
Iteration 377/1000 | Loss: 0.00003564
Iteration 378/1000 | Loss: 0.00003564
Iteration 379/1000 | Loss: 0.00003564
Iteration 380/1000 | Loss: 0.00003563
Iteration 381/1000 | Loss: 0.00003563
Iteration 382/1000 | Loss: 0.00003563
Iteration 383/1000 | Loss: 0.00003563
Iteration 384/1000 | Loss: 0.00003563
Iteration 385/1000 | Loss: 0.00003563
Iteration 386/1000 | Loss: 0.00003563
Iteration 387/1000 | Loss: 0.00003563
Iteration 388/1000 | Loss: 0.00003562
Iteration 389/1000 | Loss: 0.00003562
Iteration 390/1000 | Loss: 0.00003562
Iteration 391/1000 | Loss: 0.00003562
Iteration 392/1000 | Loss: 0.00003562
Iteration 393/1000 | Loss: 0.00003562
Iteration 394/1000 | Loss: 0.00003562
Iteration 395/1000 | Loss: 0.00003562
Iteration 396/1000 | Loss: 0.00003562
Iteration 397/1000 | Loss: 0.00003562
Iteration 398/1000 | Loss: 0.00003562
Iteration 399/1000 | Loss: 0.00003562
Iteration 400/1000 | Loss: 0.00003562
Iteration 401/1000 | Loss: 0.00003562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 401. Stopping optimization.
Last 5 losses: [3.561735138646327e-05, 3.561735138646327e-05, 3.561735138646327e-05, 3.561735138646327e-05, 3.561735138646327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.561735138646327e-05

Optimization complete. Final v2v error: 4.053078651428223 mm

Highest mean error: 12.11168384552002 mm for frame 109

Lowest mean error: 3.3367881774902344 mm for frame 171

Saving results

Total time: 516.5063745975494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430700
Iteration 2/25 | Loss: 0.00152850
Iteration 3/25 | Loss: 0.00138194
Iteration 4/25 | Loss: 0.00136351
Iteration 5/25 | Loss: 0.00135843
Iteration 6/25 | Loss: 0.00135707
Iteration 7/25 | Loss: 0.00135690
Iteration 8/25 | Loss: 0.00135690
Iteration 9/25 | Loss: 0.00135690
Iteration 10/25 | Loss: 0.00135690
Iteration 11/25 | Loss: 0.00135690
Iteration 12/25 | Loss: 0.00135690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013569029979407787, 0.0013569029979407787, 0.0013569029979407787, 0.0013569029979407787, 0.0013569029979407787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013569029979407787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91856670
Iteration 2/25 | Loss: 0.00089063
Iteration 3/25 | Loss: 0.00089061
Iteration 4/25 | Loss: 0.00089061
Iteration 5/25 | Loss: 0.00089061
Iteration 6/25 | Loss: 0.00089061
Iteration 7/25 | Loss: 0.00089061
Iteration 8/25 | Loss: 0.00089061
Iteration 9/25 | Loss: 0.00089061
Iteration 10/25 | Loss: 0.00089061
Iteration 11/25 | Loss: 0.00089061
Iteration 12/25 | Loss: 0.00089061
Iteration 13/25 | Loss: 0.00089061
Iteration 14/25 | Loss: 0.00089061
Iteration 15/25 | Loss: 0.00089061
Iteration 16/25 | Loss: 0.00089061
Iteration 17/25 | Loss: 0.00089061
Iteration 18/25 | Loss: 0.00089061
Iteration 19/25 | Loss: 0.00089061
Iteration 20/25 | Loss: 0.00089061
Iteration 21/25 | Loss: 0.00089061
Iteration 22/25 | Loss: 0.00089061
Iteration 23/25 | Loss: 0.00089061
Iteration 24/25 | Loss: 0.00089061
Iteration 25/25 | Loss: 0.00089061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089061
Iteration 2/1000 | Loss: 0.00004428
Iteration 3/1000 | Loss: 0.00002963
Iteration 4/1000 | Loss: 0.00002723
Iteration 5/1000 | Loss: 0.00002619
Iteration 6/1000 | Loss: 0.00002537
Iteration 7/1000 | Loss: 0.00002470
Iteration 8/1000 | Loss: 0.00002411
Iteration 9/1000 | Loss: 0.00002371
Iteration 10/1000 | Loss: 0.00002335
Iteration 11/1000 | Loss: 0.00002308
Iteration 12/1000 | Loss: 0.00002288
Iteration 13/1000 | Loss: 0.00002274
Iteration 14/1000 | Loss: 0.00002272
Iteration 15/1000 | Loss: 0.00002269
Iteration 16/1000 | Loss: 0.00002268
Iteration 17/1000 | Loss: 0.00002267
Iteration 18/1000 | Loss: 0.00002259
Iteration 19/1000 | Loss: 0.00002254
Iteration 20/1000 | Loss: 0.00002250
Iteration 21/1000 | Loss: 0.00002249
Iteration 22/1000 | Loss: 0.00002249
Iteration 23/1000 | Loss: 0.00002247
Iteration 24/1000 | Loss: 0.00002247
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002246
Iteration 27/1000 | Loss: 0.00002246
Iteration 28/1000 | Loss: 0.00002245
Iteration 29/1000 | Loss: 0.00002245
Iteration 30/1000 | Loss: 0.00002245
Iteration 31/1000 | Loss: 0.00002245
Iteration 32/1000 | Loss: 0.00002244
Iteration 33/1000 | Loss: 0.00002244
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002244
Iteration 36/1000 | Loss: 0.00002244
Iteration 37/1000 | Loss: 0.00002244
Iteration 38/1000 | Loss: 0.00002243
Iteration 39/1000 | Loss: 0.00002243
Iteration 40/1000 | Loss: 0.00002243
Iteration 41/1000 | Loss: 0.00002242
Iteration 42/1000 | Loss: 0.00002242
Iteration 43/1000 | Loss: 0.00002242
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002241
Iteration 46/1000 | Loss: 0.00002241
Iteration 47/1000 | Loss: 0.00002240
Iteration 48/1000 | Loss: 0.00002240
Iteration 49/1000 | Loss: 0.00002239
Iteration 50/1000 | Loss: 0.00002239
Iteration 51/1000 | Loss: 0.00002239
Iteration 52/1000 | Loss: 0.00002239
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002238
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00002237
Iteration 57/1000 | Loss: 0.00002237
Iteration 58/1000 | Loss: 0.00002237
Iteration 59/1000 | Loss: 0.00002237
Iteration 60/1000 | Loss: 0.00002236
Iteration 61/1000 | Loss: 0.00002236
Iteration 62/1000 | Loss: 0.00002235
Iteration 63/1000 | Loss: 0.00002235
Iteration 64/1000 | Loss: 0.00002235
Iteration 65/1000 | Loss: 0.00002235
Iteration 66/1000 | Loss: 0.00002235
Iteration 67/1000 | Loss: 0.00002235
Iteration 68/1000 | Loss: 0.00002235
Iteration 69/1000 | Loss: 0.00002235
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00002234
Iteration 73/1000 | Loss: 0.00002234
Iteration 74/1000 | Loss: 0.00002234
Iteration 75/1000 | Loss: 0.00002234
Iteration 76/1000 | Loss: 0.00002234
Iteration 77/1000 | Loss: 0.00002234
Iteration 78/1000 | Loss: 0.00002234
Iteration 79/1000 | Loss: 0.00002234
Iteration 80/1000 | Loss: 0.00002234
Iteration 81/1000 | Loss: 0.00002233
Iteration 82/1000 | Loss: 0.00002233
Iteration 83/1000 | Loss: 0.00002233
Iteration 84/1000 | Loss: 0.00002233
Iteration 85/1000 | Loss: 0.00002233
Iteration 86/1000 | Loss: 0.00002233
Iteration 87/1000 | Loss: 0.00002233
Iteration 88/1000 | Loss: 0.00002233
Iteration 89/1000 | Loss: 0.00002233
Iteration 90/1000 | Loss: 0.00002233
Iteration 91/1000 | Loss: 0.00002233
Iteration 92/1000 | Loss: 0.00002233
Iteration 93/1000 | Loss: 0.00002232
Iteration 94/1000 | Loss: 0.00002232
Iteration 95/1000 | Loss: 0.00002232
Iteration 96/1000 | Loss: 0.00002232
Iteration 97/1000 | Loss: 0.00002232
Iteration 98/1000 | Loss: 0.00002231
Iteration 99/1000 | Loss: 0.00002231
Iteration 100/1000 | Loss: 0.00002231
Iteration 101/1000 | Loss: 0.00002231
Iteration 102/1000 | Loss: 0.00002231
Iteration 103/1000 | Loss: 0.00002231
Iteration 104/1000 | Loss: 0.00002231
Iteration 105/1000 | Loss: 0.00002230
Iteration 106/1000 | Loss: 0.00002230
Iteration 107/1000 | Loss: 0.00002230
Iteration 108/1000 | Loss: 0.00002230
Iteration 109/1000 | Loss: 0.00002230
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002230
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002229
Iteration 119/1000 | Loss: 0.00002229
Iteration 120/1000 | Loss: 0.00002229
Iteration 121/1000 | Loss: 0.00002229
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002228
Iteration 125/1000 | Loss: 0.00002228
Iteration 126/1000 | Loss: 0.00002228
Iteration 127/1000 | Loss: 0.00002228
Iteration 128/1000 | Loss: 0.00002228
Iteration 129/1000 | Loss: 0.00002227
Iteration 130/1000 | Loss: 0.00002227
Iteration 131/1000 | Loss: 0.00002227
Iteration 132/1000 | Loss: 0.00002227
Iteration 133/1000 | Loss: 0.00002227
Iteration 134/1000 | Loss: 0.00002227
Iteration 135/1000 | Loss: 0.00002227
Iteration 136/1000 | Loss: 0.00002227
Iteration 137/1000 | Loss: 0.00002227
Iteration 138/1000 | Loss: 0.00002227
Iteration 139/1000 | Loss: 0.00002227
Iteration 140/1000 | Loss: 0.00002227
Iteration 141/1000 | Loss: 0.00002227
Iteration 142/1000 | Loss: 0.00002227
Iteration 143/1000 | Loss: 0.00002227
Iteration 144/1000 | Loss: 0.00002227
Iteration 145/1000 | Loss: 0.00002227
Iteration 146/1000 | Loss: 0.00002226
Iteration 147/1000 | Loss: 0.00002226
Iteration 148/1000 | Loss: 0.00002226
Iteration 149/1000 | Loss: 0.00002226
Iteration 150/1000 | Loss: 0.00002226
Iteration 151/1000 | Loss: 0.00002226
Iteration 152/1000 | Loss: 0.00002226
Iteration 153/1000 | Loss: 0.00002226
Iteration 154/1000 | Loss: 0.00002226
Iteration 155/1000 | Loss: 0.00002226
Iteration 156/1000 | Loss: 0.00002226
Iteration 157/1000 | Loss: 0.00002226
Iteration 158/1000 | Loss: 0.00002226
Iteration 159/1000 | Loss: 0.00002226
Iteration 160/1000 | Loss: 0.00002226
Iteration 161/1000 | Loss: 0.00002226
Iteration 162/1000 | Loss: 0.00002226
Iteration 163/1000 | Loss: 0.00002226
Iteration 164/1000 | Loss: 0.00002226
Iteration 165/1000 | Loss: 0.00002226
Iteration 166/1000 | Loss: 0.00002226
Iteration 167/1000 | Loss: 0.00002226
Iteration 168/1000 | Loss: 0.00002226
Iteration 169/1000 | Loss: 0.00002226
Iteration 170/1000 | Loss: 0.00002226
Iteration 171/1000 | Loss: 0.00002226
Iteration 172/1000 | Loss: 0.00002226
Iteration 173/1000 | Loss: 0.00002226
Iteration 174/1000 | Loss: 0.00002226
Iteration 175/1000 | Loss: 0.00002226
Iteration 176/1000 | Loss: 0.00002226
Iteration 177/1000 | Loss: 0.00002226
Iteration 178/1000 | Loss: 0.00002226
Iteration 179/1000 | Loss: 0.00002226
Iteration 180/1000 | Loss: 0.00002226
Iteration 181/1000 | Loss: 0.00002226
Iteration 182/1000 | Loss: 0.00002226
Iteration 183/1000 | Loss: 0.00002226
Iteration 184/1000 | Loss: 0.00002226
Iteration 185/1000 | Loss: 0.00002226
Iteration 186/1000 | Loss: 0.00002226
Iteration 187/1000 | Loss: 0.00002226
Iteration 188/1000 | Loss: 0.00002226
Iteration 189/1000 | Loss: 0.00002226
Iteration 190/1000 | Loss: 0.00002226
Iteration 191/1000 | Loss: 0.00002226
Iteration 192/1000 | Loss: 0.00002226
Iteration 193/1000 | Loss: 0.00002226
Iteration 194/1000 | Loss: 0.00002226
Iteration 195/1000 | Loss: 0.00002226
Iteration 196/1000 | Loss: 0.00002226
Iteration 197/1000 | Loss: 0.00002226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.2256519514485262e-05, 2.2256519514485262e-05, 2.2256519514485262e-05, 2.2256519514485262e-05, 2.2256519514485262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2256519514485262e-05

Optimization complete. Final v2v error: 3.9836559295654297 mm

Highest mean error: 4.663481712341309 mm for frame 49

Lowest mean error: 3.5572245121002197 mm for frame 2

Saving results

Total time: 40.005202770233154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879458
Iteration 2/25 | Loss: 0.00214683
Iteration 3/25 | Loss: 0.00174389
Iteration 4/25 | Loss: 0.00156319
Iteration 5/25 | Loss: 0.00153028
Iteration 6/25 | Loss: 0.00148766
Iteration 7/25 | Loss: 0.00148416
Iteration 8/25 | Loss: 0.00141777
Iteration 9/25 | Loss: 0.00140505
Iteration 10/25 | Loss: 0.00140337
Iteration 11/25 | Loss: 0.00139075
Iteration 12/25 | Loss: 0.00138906
Iteration 13/25 | Loss: 0.00138321
Iteration 14/25 | Loss: 0.00137791
Iteration 15/25 | Loss: 0.00138254
Iteration 16/25 | Loss: 0.00137849
Iteration 17/25 | Loss: 0.00137433
Iteration 18/25 | Loss: 0.00137751
Iteration 19/25 | Loss: 0.00138218
Iteration 20/25 | Loss: 0.00138695
Iteration 21/25 | Loss: 0.00138885
Iteration 22/25 | Loss: 0.00137811
Iteration 23/25 | Loss: 0.00136840
Iteration 24/25 | Loss: 0.00136533
Iteration 25/25 | Loss: 0.00136582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52654755
Iteration 2/25 | Loss: 0.00086738
Iteration 3/25 | Loss: 0.00086736
Iteration 4/25 | Loss: 0.00086736
Iteration 5/25 | Loss: 0.00086736
Iteration 6/25 | Loss: 0.00086736
Iteration 7/25 | Loss: 0.00086736
Iteration 8/25 | Loss: 0.00086736
Iteration 9/25 | Loss: 0.00086736
Iteration 10/25 | Loss: 0.00086736
Iteration 11/25 | Loss: 0.00086736
Iteration 12/25 | Loss: 0.00086736
Iteration 13/25 | Loss: 0.00086736
Iteration 14/25 | Loss: 0.00086736
Iteration 15/25 | Loss: 0.00086736
Iteration 16/25 | Loss: 0.00086736
Iteration 17/25 | Loss: 0.00086736
Iteration 18/25 | Loss: 0.00086736
Iteration 19/25 | Loss: 0.00086736
Iteration 20/25 | Loss: 0.00086736
Iteration 21/25 | Loss: 0.00086736
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008673592819832265, 0.0008673592819832265, 0.0008673592819832265, 0.0008673592819832265, 0.0008673592819832265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008673592819832265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086736
Iteration 2/1000 | Loss: 0.00006453
Iteration 3/1000 | Loss: 0.00004272
Iteration 4/1000 | Loss: 0.00003560
Iteration 5/1000 | Loss: 0.00003375
Iteration 6/1000 | Loss: 0.00003175
Iteration 7/1000 | Loss: 0.00003054
Iteration 8/1000 | Loss: 0.00002959
Iteration 9/1000 | Loss: 0.00002895
Iteration 10/1000 | Loss: 0.00025904
Iteration 11/1000 | Loss: 0.00021476
Iteration 12/1000 | Loss: 0.00003300
Iteration 13/1000 | Loss: 0.00002894
Iteration 14/1000 | Loss: 0.00002599
Iteration 15/1000 | Loss: 0.00002362
Iteration 16/1000 | Loss: 0.00002232
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002122
Iteration 19/1000 | Loss: 0.00002094
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002062
Iteration 22/1000 | Loss: 0.00002057
Iteration 23/1000 | Loss: 0.00002034
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002020
Iteration 26/1000 | Loss: 0.00002017
Iteration 27/1000 | Loss: 0.00002012
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002009
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002008
Iteration 33/1000 | Loss: 0.00002007
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00002000
Iteration 40/1000 | Loss: 0.00001999
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001998
Iteration 45/1000 | Loss: 0.00001998
Iteration 46/1000 | Loss: 0.00001997
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001996
Iteration 50/1000 | Loss: 0.00001996
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001992
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001991
Iteration 74/1000 | Loss: 0.00001991
Iteration 75/1000 | Loss: 0.00001991
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001990
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001990
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001989
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001987
Iteration 99/1000 | Loss: 0.00001987
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001987
Iteration 102/1000 | Loss: 0.00001987
Iteration 103/1000 | Loss: 0.00001987
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001987
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001987
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001986
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001986
Iteration 119/1000 | Loss: 0.00001986
Iteration 120/1000 | Loss: 0.00001986
Iteration 121/1000 | Loss: 0.00001986
Iteration 122/1000 | Loss: 0.00001986
Iteration 123/1000 | Loss: 0.00001986
Iteration 124/1000 | Loss: 0.00001986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.986342431337107e-05, 1.986342431337107e-05, 1.986342431337107e-05, 1.986342431337107e-05, 1.986342431337107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.986342431337107e-05

Optimization complete. Final v2v error: 3.7365691661834717 mm

Highest mean error: 4.425992012023926 mm for frame 94

Lowest mean error: 3.128166913986206 mm for frame 141

Saving results

Total time: 85.33107399940491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985612
Iteration 2/25 | Loss: 0.00177564
Iteration 3/25 | Loss: 0.00155561
Iteration 4/25 | Loss: 0.00143253
Iteration 5/25 | Loss: 0.00140169
Iteration 6/25 | Loss: 0.00136582
Iteration 7/25 | Loss: 0.00135883
Iteration 8/25 | Loss: 0.00135091
Iteration 9/25 | Loss: 0.00134996
Iteration 10/25 | Loss: 0.00134335
Iteration 11/25 | Loss: 0.00134053
Iteration 12/25 | Loss: 0.00133953
Iteration 13/25 | Loss: 0.00133932
Iteration 14/25 | Loss: 0.00133924
Iteration 15/25 | Loss: 0.00133916
Iteration 16/25 | Loss: 0.00133913
Iteration 17/25 | Loss: 0.00133912
Iteration 18/25 | Loss: 0.00133912
Iteration 19/25 | Loss: 0.00133912
Iteration 20/25 | Loss: 0.00133912
Iteration 21/25 | Loss: 0.00133912
Iteration 22/25 | Loss: 0.00133912
Iteration 23/25 | Loss: 0.00133912
Iteration 24/25 | Loss: 0.00133911
Iteration 25/25 | Loss: 0.00133911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18689632
Iteration 2/25 | Loss: 0.00109828
Iteration 3/25 | Loss: 0.00109828
Iteration 4/25 | Loss: 0.00109828
Iteration 5/25 | Loss: 0.00109828
Iteration 6/25 | Loss: 0.00109828
Iteration 7/25 | Loss: 0.00109828
Iteration 8/25 | Loss: 0.00109828
Iteration 9/25 | Loss: 0.00109828
Iteration 10/25 | Loss: 0.00109828
Iteration 11/25 | Loss: 0.00109827
Iteration 12/25 | Loss: 0.00109827
Iteration 13/25 | Loss: 0.00109827
Iteration 14/25 | Loss: 0.00109827
Iteration 15/25 | Loss: 0.00109827
Iteration 16/25 | Loss: 0.00109827
Iteration 17/25 | Loss: 0.00109827
Iteration 18/25 | Loss: 0.00109827
Iteration 19/25 | Loss: 0.00109827
Iteration 20/25 | Loss: 0.00109827
Iteration 21/25 | Loss: 0.00109827
Iteration 22/25 | Loss: 0.00109827
Iteration 23/25 | Loss: 0.00109827
Iteration 24/25 | Loss: 0.00109827
Iteration 25/25 | Loss: 0.00109827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109827
Iteration 2/1000 | Loss: 0.00005302
Iteration 3/1000 | Loss: 0.00003867
Iteration 4/1000 | Loss: 0.00003314
Iteration 5/1000 | Loss: 0.00021075
Iteration 6/1000 | Loss: 0.00005039
Iteration 7/1000 | Loss: 0.00002843
Iteration 8/1000 | Loss: 0.00012691
Iteration 9/1000 | Loss: 0.00002807
Iteration 10/1000 | Loss: 0.00002661
Iteration 11/1000 | Loss: 0.00002550
Iteration 12/1000 | Loss: 0.00002465
Iteration 13/1000 | Loss: 0.00072425
Iteration 14/1000 | Loss: 0.00022650
Iteration 15/1000 | Loss: 0.00025267
Iteration 16/1000 | Loss: 0.00016370
Iteration 17/1000 | Loss: 0.00002651
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002114
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001828
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001787
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001773
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001759
Iteration 51/1000 | Loss: 0.00001759
Iteration 52/1000 | Loss: 0.00001758
Iteration 53/1000 | Loss: 0.00001758
Iteration 54/1000 | Loss: 0.00001758
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001755
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001753
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001750
Iteration 96/1000 | Loss: 0.00001750
Iteration 97/1000 | Loss: 0.00001750
Iteration 98/1000 | Loss: 0.00001750
Iteration 99/1000 | Loss: 0.00001750
Iteration 100/1000 | Loss: 0.00001750
Iteration 101/1000 | Loss: 0.00001750
Iteration 102/1000 | Loss: 0.00001750
Iteration 103/1000 | Loss: 0.00001750
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001749
Iteration 106/1000 | Loss: 0.00001749
Iteration 107/1000 | Loss: 0.00001749
Iteration 108/1000 | Loss: 0.00001749
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.749077273416333e-05, 1.749077273416333e-05, 1.749077273416333e-05, 1.749077273416333e-05, 1.749077273416333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.749077273416333e-05

Optimization complete. Final v2v error: 3.547537088394165 mm

Highest mean error: 4.801883697509766 mm for frame 151

Lowest mean error: 3.038870096206665 mm for frame 81

Saving results

Total time: 77.91450357437134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439413
Iteration 2/25 | Loss: 0.00189887
Iteration 3/25 | Loss: 0.00161243
Iteration 4/25 | Loss: 0.00154921
Iteration 5/25 | Loss: 0.00153296
Iteration 6/25 | Loss: 0.00153965
Iteration 7/25 | Loss: 0.00153923
Iteration 8/25 | Loss: 0.00153635
Iteration 9/25 | Loss: 0.00154493
Iteration 10/25 | Loss: 0.00161978
Iteration 11/25 | Loss: 0.00158984
Iteration 12/25 | Loss: 0.00150684
Iteration 13/25 | Loss: 0.00147638
Iteration 14/25 | Loss: 0.00147061
Iteration 15/25 | Loss: 0.00146957
Iteration 16/25 | Loss: 0.00147252
Iteration 17/25 | Loss: 0.00147230
Iteration 18/25 | Loss: 0.00147013
Iteration 19/25 | Loss: 0.00146889
Iteration 20/25 | Loss: 0.00146845
Iteration 21/25 | Loss: 0.00146839
Iteration 22/25 | Loss: 0.00146839
Iteration 23/25 | Loss: 0.00146839
Iteration 24/25 | Loss: 0.00146839
Iteration 25/25 | Loss: 0.00146839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37466979
Iteration 2/25 | Loss: 0.00100073
Iteration 3/25 | Loss: 0.00100073
Iteration 4/25 | Loss: 0.00100073
Iteration 5/25 | Loss: 0.00100073
Iteration 6/25 | Loss: 0.00100073
Iteration 7/25 | Loss: 0.00100073
Iteration 8/25 | Loss: 0.00100073
Iteration 9/25 | Loss: 0.00100073
Iteration 10/25 | Loss: 0.00100073
Iteration 11/25 | Loss: 0.00100073
Iteration 12/25 | Loss: 0.00100073
Iteration 13/25 | Loss: 0.00100073
Iteration 14/25 | Loss: 0.00100073
Iteration 15/25 | Loss: 0.00100073
Iteration 16/25 | Loss: 0.00100073
Iteration 17/25 | Loss: 0.00100073
Iteration 18/25 | Loss: 0.00100073
Iteration 19/25 | Loss: 0.00100073
Iteration 20/25 | Loss: 0.00100073
Iteration 21/25 | Loss: 0.00100073
Iteration 22/25 | Loss: 0.00100073
Iteration 23/25 | Loss: 0.00100073
Iteration 24/25 | Loss: 0.00100073
Iteration 25/25 | Loss: 0.00100073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100073
Iteration 2/1000 | Loss: 0.00510194
Iteration 3/1000 | Loss: 0.00489587
Iteration 4/1000 | Loss: 0.00199696
Iteration 5/1000 | Loss: 0.00233045
Iteration 6/1000 | Loss: 0.00170341
Iteration 7/1000 | Loss: 0.00182456
Iteration 8/1000 | Loss: 0.00250201
Iteration 9/1000 | Loss: 0.00169254
Iteration 10/1000 | Loss: 0.00058946
Iteration 11/1000 | Loss: 0.00014384
Iteration 12/1000 | Loss: 0.00009705
Iteration 13/1000 | Loss: 0.00023653
Iteration 14/1000 | Loss: 0.00064198
Iteration 15/1000 | Loss: 0.00032233
Iteration 16/1000 | Loss: 0.00122124
Iteration 17/1000 | Loss: 0.00089380
Iteration 18/1000 | Loss: 0.00057555
Iteration 19/1000 | Loss: 0.00036394
Iteration 20/1000 | Loss: 0.00029588
Iteration 21/1000 | Loss: 0.00009203
Iteration 22/1000 | Loss: 0.00008561
Iteration 23/1000 | Loss: 0.00008066
Iteration 24/1000 | Loss: 0.00081790
Iteration 25/1000 | Loss: 0.00205016
Iteration 26/1000 | Loss: 0.00162247
Iteration 27/1000 | Loss: 0.00064207
Iteration 28/1000 | Loss: 0.00041492
Iteration 29/1000 | Loss: 0.00015562
Iteration 30/1000 | Loss: 0.00010485
Iteration 31/1000 | Loss: 0.00007504
Iteration 32/1000 | Loss: 0.00007075
Iteration 33/1000 | Loss: 0.00006831
Iteration 34/1000 | Loss: 0.00027599
Iteration 35/1000 | Loss: 0.00013463
Iteration 36/1000 | Loss: 0.00022304
Iteration 37/1000 | Loss: 0.00028655
Iteration 38/1000 | Loss: 0.00058919
Iteration 39/1000 | Loss: 0.00030830
Iteration 40/1000 | Loss: 0.00024482
Iteration 41/1000 | Loss: 0.00014741
Iteration 42/1000 | Loss: 0.00006535
Iteration 43/1000 | Loss: 0.00024244
Iteration 44/1000 | Loss: 0.00013959
Iteration 45/1000 | Loss: 0.00020151
Iteration 46/1000 | Loss: 0.00012918
Iteration 47/1000 | Loss: 0.00048316
Iteration 48/1000 | Loss: 0.00026445
Iteration 49/1000 | Loss: 0.00047355
Iteration 50/1000 | Loss: 0.00024440
Iteration 51/1000 | Loss: 0.00046997
Iteration 52/1000 | Loss: 0.00020827
Iteration 53/1000 | Loss: 0.00018764
Iteration 54/1000 | Loss: 0.00024801
Iteration 55/1000 | Loss: 0.00010927
Iteration 56/1000 | Loss: 0.00017466
Iteration 57/1000 | Loss: 0.00006756
Iteration 58/1000 | Loss: 0.00006452
Iteration 59/1000 | Loss: 0.00006319
Iteration 60/1000 | Loss: 0.00006665
Iteration 61/1000 | Loss: 0.00014841
Iteration 62/1000 | Loss: 0.00006439
Iteration 63/1000 | Loss: 0.00006231
Iteration 64/1000 | Loss: 0.00006124
Iteration 65/1000 | Loss: 0.00006083
Iteration 66/1000 | Loss: 0.00006035
Iteration 67/1000 | Loss: 0.00005999
Iteration 68/1000 | Loss: 0.00005926
Iteration 69/1000 | Loss: 0.00005844
Iteration 70/1000 | Loss: 0.00005789
Iteration 71/1000 | Loss: 0.00005761
Iteration 72/1000 | Loss: 0.00005744
Iteration 73/1000 | Loss: 0.00005723
Iteration 74/1000 | Loss: 0.00005718
Iteration 75/1000 | Loss: 0.00005714
Iteration 76/1000 | Loss: 0.00005713
Iteration 77/1000 | Loss: 0.00005709
Iteration 78/1000 | Loss: 0.00005694
Iteration 79/1000 | Loss: 0.00005691
Iteration 80/1000 | Loss: 0.00005690
Iteration 81/1000 | Loss: 0.00005690
Iteration 82/1000 | Loss: 0.00005690
Iteration 83/1000 | Loss: 0.00005690
Iteration 84/1000 | Loss: 0.00005690
Iteration 85/1000 | Loss: 0.00005690
Iteration 86/1000 | Loss: 0.00005690
Iteration 87/1000 | Loss: 0.00005689
Iteration 88/1000 | Loss: 0.00005689
Iteration 89/1000 | Loss: 0.00005688
Iteration 90/1000 | Loss: 0.00005688
Iteration 91/1000 | Loss: 0.00005688
Iteration 92/1000 | Loss: 0.00005687
Iteration 93/1000 | Loss: 0.00005687
Iteration 94/1000 | Loss: 0.00005687
Iteration 95/1000 | Loss: 0.00005687
Iteration 96/1000 | Loss: 0.00005686
Iteration 97/1000 | Loss: 0.00005686
Iteration 98/1000 | Loss: 0.00005686
Iteration 99/1000 | Loss: 0.00005685
Iteration 100/1000 | Loss: 0.00005685
Iteration 101/1000 | Loss: 0.00005685
Iteration 102/1000 | Loss: 0.00005685
Iteration 103/1000 | Loss: 0.00005685
Iteration 104/1000 | Loss: 0.00005685
Iteration 105/1000 | Loss: 0.00005684
Iteration 106/1000 | Loss: 0.00005684
Iteration 107/1000 | Loss: 0.00005684
Iteration 108/1000 | Loss: 0.00005684
Iteration 109/1000 | Loss: 0.00005684
Iteration 110/1000 | Loss: 0.00005684
Iteration 111/1000 | Loss: 0.00005684
Iteration 112/1000 | Loss: 0.00005683
Iteration 113/1000 | Loss: 0.00005683
Iteration 114/1000 | Loss: 0.00005683
Iteration 115/1000 | Loss: 0.00005683
Iteration 116/1000 | Loss: 0.00005682
Iteration 117/1000 | Loss: 0.00005682
Iteration 118/1000 | Loss: 0.00005682
Iteration 119/1000 | Loss: 0.00005682
Iteration 120/1000 | Loss: 0.00005681
Iteration 121/1000 | Loss: 0.00005681
Iteration 122/1000 | Loss: 0.00005680
Iteration 123/1000 | Loss: 0.00005680
Iteration 124/1000 | Loss: 0.00005680
Iteration 125/1000 | Loss: 0.00005679
Iteration 126/1000 | Loss: 0.00005677
Iteration 127/1000 | Loss: 0.00005677
Iteration 128/1000 | Loss: 0.00005677
Iteration 129/1000 | Loss: 0.00005676
Iteration 130/1000 | Loss: 0.00005676
Iteration 131/1000 | Loss: 0.00005676
Iteration 132/1000 | Loss: 0.00005675
Iteration 133/1000 | Loss: 0.00005675
Iteration 134/1000 | Loss: 0.00005675
Iteration 135/1000 | Loss: 0.00005674
Iteration 136/1000 | Loss: 0.00005674
Iteration 137/1000 | Loss: 0.00005674
Iteration 138/1000 | Loss: 0.00005667
Iteration 139/1000 | Loss: 0.00005667
Iteration 140/1000 | Loss: 0.00005667
Iteration 141/1000 | Loss: 0.00005665
Iteration 142/1000 | Loss: 0.00005665
Iteration 143/1000 | Loss: 0.00005665
Iteration 144/1000 | Loss: 0.00005665
Iteration 145/1000 | Loss: 0.00005665
Iteration 146/1000 | Loss: 0.00005665
Iteration 147/1000 | Loss: 0.00005665
Iteration 148/1000 | Loss: 0.00005665
Iteration 149/1000 | Loss: 0.00005665
Iteration 150/1000 | Loss: 0.00005665
Iteration 151/1000 | Loss: 0.00005665
Iteration 152/1000 | Loss: 0.00005665
Iteration 153/1000 | Loss: 0.00005664
Iteration 154/1000 | Loss: 0.00005664
Iteration 155/1000 | Loss: 0.00005664
Iteration 156/1000 | Loss: 0.00005663
Iteration 157/1000 | Loss: 0.00005663
Iteration 158/1000 | Loss: 0.00005663
Iteration 159/1000 | Loss: 0.00005663
Iteration 160/1000 | Loss: 0.00005663
Iteration 161/1000 | Loss: 0.00005663
Iteration 162/1000 | Loss: 0.00005662
Iteration 163/1000 | Loss: 0.00005662
Iteration 164/1000 | Loss: 0.00005662
Iteration 165/1000 | Loss: 0.00005662
Iteration 166/1000 | Loss: 0.00005662
Iteration 167/1000 | Loss: 0.00005662
Iteration 168/1000 | Loss: 0.00005662
Iteration 169/1000 | Loss: 0.00005662
Iteration 170/1000 | Loss: 0.00005662
Iteration 171/1000 | Loss: 0.00005662
Iteration 172/1000 | Loss: 0.00005662
Iteration 173/1000 | Loss: 0.00005661
Iteration 174/1000 | Loss: 0.00005661
Iteration 175/1000 | Loss: 0.00005661
Iteration 176/1000 | Loss: 0.00005661
Iteration 177/1000 | Loss: 0.00005661
Iteration 178/1000 | Loss: 0.00005661
Iteration 179/1000 | Loss: 0.00005661
Iteration 180/1000 | Loss: 0.00005661
Iteration 181/1000 | Loss: 0.00005661
Iteration 182/1000 | Loss: 0.00005661
Iteration 183/1000 | Loss: 0.00005661
Iteration 184/1000 | Loss: 0.00005661
Iteration 185/1000 | Loss: 0.00005661
Iteration 186/1000 | Loss: 0.00005661
Iteration 187/1000 | Loss: 0.00005661
Iteration 188/1000 | Loss: 0.00005661
Iteration 189/1000 | Loss: 0.00005661
Iteration 190/1000 | Loss: 0.00005661
Iteration 191/1000 | Loss: 0.00005661
Iteration 192/1000 | Loss: 0.00005661
Iteration 193/1000 | Loss: 0.00005661
Iteration 194/1000 | Loss: 0.00005661
Iteration 195/1000 | Loss: 0.00005661
Iteration 196/1000 | Loss: 0.00005661
Iteration 197/1000 | Loss: 0.00005661
Iteration 198/1000 | Loss: 0.00005661
Iteration 199/1000 | Loss: 0.00005661
Iteration 200/1000 | Loss: 0.00005661
Iteration 201/1000 | Loss: 0.00005661
Iteration 202/1000 | Loss: 0.00005661
Iteration 203/1000 | Loss: 0.00005661
Iteration 204/1000 | Loss: 0.00005661
Iteration 205/1000 | Loss: 0.00005661
Iteration 206/1000 | Loss: 0.00005661
Iteration 207/1000 | Loss: 0.00005661
Iteration 208/1000 | Loss: 0.00005661
Iteration 209/1000 | Loss: 0.00005661
Iteration 210/1000 | Loss: 0.00005661
Iteration 211/1000 | Loss: 0.00005661
Iteration 212/1000 | Loss: 0.00005661
Iteration 213/1000 | Loss: 0.00005661
Iteration 214/1000 | Loss: 0.00005661
Iteration 215/1000 | Loss: 0.00005661
Iteration 216/1000 | Loss: 0.00005661
Iteration 217/1000 | Loss: 0.00005661
Iteration 218/1000 | Loss: 0.00005661
Iteration 219/1000 | Loss: 0.00005661
Iteration 220/1000 | Loss: 0.00005661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [5.661048635374755e-05, 5.661048635374755e-05, 5.661048635374755e-05, 5.661048635374755e-05, 5.661048635374755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.661048635374755e-05

Optimization complete. Final v2v error: 5.458093166351318 mm

Highest mean error: 10.680685043334961 mm for frame 233

Lowest mean error: 4.5561604499816895 mm for frame 216

Saving results

Total time: 171.69144940376282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966636
Iteration 2/25 | Loss: 0.00286695
Iteration 3/25 | Loss: 0.00220322
Iteration 4/25 | Loss: 0.00198141
Iteration 5/25 | Loss: 0.00201368
Iteration 6/25 | Loss: 0.00203345
Iteration 7/25 | Loss: 0.00190973
Iteration 8/25 | Loss: 0.00184700
Iteration 9/25 | Loss: 0.00176907
Iteration 10/25 | Loss: 0.00175437
Iteration 11/25 | Loss: 0.00175797
Iteration 12/25 | Loss: 0.00173660
Iteration 13/25 | Loss: 0.00170114
Iteration 14/25 | Loss: 0.00168610
Iteration 15/25 | Loss: 0.00166561
Iteration 16/25 | Loss: 0.00166412
Iteration 17/25 | Loss: 0.00166727
Iteration 18/25 | Loss: 0.00163304
Iteration 19/25 | Loss: 0.00161919
Iteration 20/25 | Loss: 0.00161379
Iteration 21/25 | Loss: 0.00161493
Iteration 22/25 | Loss: 0.00160507
Iteration 23/25 | Loss: 0.00160892
Iteration 24/25 | Loss: 0.00160021
Iteration 25/25 | Loss: 0.00159570

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40652335
Iteration 2/25 | Loss: 0.00499486
Iteration 3/25 | Loss: 0.00296066
Iteration 4/25 | Loss: 0.00296064
Iteration 5/25 | Loss: 0.00296064
Iteration 6/25 | Loss: 0.00296064
Iteration 7/25 | Loss: 0.00296064
Iteration 8/25 | Loss: 0.00296064
Iteration 9/25 | Loss: 0.00296064
Iteration 10/25 | Loss: 0.00296064
Iteration 11/25 | Loss: 0.00296064
Iteration 12/25 | Loss: 0.00296064
Iteration 13/25 | Loss: 0.00296064
Iteration 14/25 | Loss: 0.00296064
Iteration 15/25 | Loss: 0.00296064
Iteration 16/25 | Loss: 0.00296064
Iteration 17/25 | Loss: 0.00296064
Iteration 18/25 | Loss: 0.00296064
Iteration 19/25 | Loss: 0.00296064
Iteration 20/25 | Loss: 0.00296064
Iteration 21/25 | Loss: 0.00296064
Iteration 22/25 | Loss: 0.00296064
Iteration 23/25 | Loss: 0.00296064
Iteration 24/25 | Loss: 0.00296064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002960639074444771, 0.002960639074444771, 0.002960639074444771, 0.002960639074444771, 0.002960639074444771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002960639074444771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296064
Iteration 2/1000 | Loss: 0.00408719
Iteration 3/1000 | Loss: 0.00946150
Iteration 4/1000 | Loss: 0.00054120
Iteration 5/1000 | Loss: 0.00116113
Iteration 6/1000 | Loss: 0.00202230
Iteration 7/1000 | Loss: 0.00069178
Iteration 8/1000 | Loss: 0.00088178
Iteration 9/1000 | Loss: 0.00063365
Iteration 10/1000 | Loss: 0.00062492
Iteration 11/1000 | Loss: 0.00129965
Iteration 12/1000 | Loss: 0.00080990
Iteration 13/1000 | Loss: 0.00097317
Iteration 14/1000 | Loss: 0.00071218
Iteration 15/1000 | Loss: 0.00038470
Iteration 16/1000 | Loss: 0.00060727
Iteration 17/1000 | Loss: 0.00030479
Iteration 18/1000 | Loss: 0.00022548
Iteration 19/1000 | Loss: 0.00089966
Iteration 20/1000 | Loss: 0.00059819
Iteration 21/1000 | Loss: 0.00082108
Iteration 22/1000 | Loss: 0.00070352
Iteration 23/1000 | Loss: 0.00041983
Iteration 24/1000 | Loss: 0.00026874
Iteration 25/1000 | Loss: 0.00025777
Iteration 26/1000 | Loss: 0.00020262
Iteration 27/1000 | Loss: 0.00022721
Iteration 28/1000 | Loss: 0.00089725
Iteration 29/1000 | Loss: 0.00220913
Iteration 30/1000 | Loss: 0.00321451
Iteration 31/1000 | Loss: 0.00135738
Iteration 32/1000 | Loss: 0.00063340
Iteration 33/1000 | Loss: 0.00033563
Iteration 34/1000 | Loss: 0.00051541
Iteration 35/1000 | Loss: 0.00074921
Iteration 36/1000 | Loss: 0.00050774
Iteration 37/1000 | Loss: 0.00030189
Iteration 38/1000 | Loss: 0.00024656
Iteration 39/1000 | Loss: 0.00063030
Iteration 40/1000 | Loss: 0.00055191
Iteration 41/1000 | Loss: 0.00020746
Iteration 42/1000 | Loss: 0.00023783
Iteration 43/1000 | Loss: 0.00017262
Iteration 44/1000 | Loss: 0.00017566
Iteration 45/1000 | Loss: 0.00035623
Iteration 46/1000 | Loss: 0.00299150
Iteration 47/1000 | Loss: 0.00028614
Iteration 48/1000 | Loss: 0.00055190
Iteration 49/1000 | Loss: 0.00027266
Iteration 50/1000 | Loss: 0.00030554
Iteration 51/1000 | Loss: 0.00033377
Iteration 52/1000 | Loss: 0.00026994
Iteration 53/1000 | Loss: 0.00017792
Iteration 54/1000 | Loss: 0.00017233
Iteration 55/1000 | Loss: 0.00021296
Iteration 56/1000 | Loss: 0.00015772
Iteration 57/1000 | Loss: 0.00019272
Iteration 58/1000 | Loss: 0.00016602
Iteration 59/1000 | Loss: 0.00018085
Iteration 60/1000 | Loss: 0.00017302
Iteration 61/1000 | Loss: 0.00103579
Iteration 62/1000 | Loss: 0.00142945
Iteration 63/1000 | Loss: 0.00047862
Iteration 64/1000 | Loss: 0.00067784
Iteration 65/1000 | Loss: 0.00023822
Iteration 66/1000 | Loss: 0.00023347
Iteration 67/1000 | Loss: 0.00021879
Iteration 68/1000 | Loss: 0.00182666
Iteration 69/1000 | Loss: 0.00049399
Iteration 70/1000 | Loss: 0.00063177
Iteration 71/1000 | Loss: 0.00032323
Iteration 72/1000 | Loss: 0.00035141
Iteration 73/1000 | Loss: 0.00017491
Iteration 74/1000 | Loss: 0.00072232
Iteration 75/1000 | Loss: 0.00288640
Iteration 76/1000 | Loss: 0.00146688
Iteration 77/1000 | Loss: 0.00093633
Iteration 78/1000 | Loss: 0.00062597
Iteration 79/1000 | Loss: 0.00075275
Iteration 80/1000 | Loss: 0.00022154
Iteration 81/1000 | Loss: 0.00017125
Iteration 82/1000 | Loss: 0.00047851
Iteration 83/1000 | Loss: 0.00042282
Iteration 84/1000 | Loss: 0.00093130
Iteration 85/1000 | Loss: 0.00034488
Iteration 86/1000 | Loss: 0.00033872
Iteration 87/1000 | Loss: 0.00033704
Iteration 88/1000 | Loss: 0.00191572
Iteration 89/1000 | Loss: 0.00042758
Iteration 90/1000 | Loss: 0.00027453
Iteration 91/1000 | Loss: 0.00011855
Iteration 92/1000 | Loss: 0.00058115
Iteration 93/1000 | Loss: 0.00018677
Iteration 94/1000 | Loss: 0.00079055
Iteration 95/1000 | Loss: 0.00064819
Iteration 96/1000 | Loss: 0.00021779
Iteration 97/1000 | Loss: 0.00008254
Iteration 98/1000 | Loss: 0.00016511
Iteration 99/1000 | Loss: 0.00103107
Iteration 100/1000 | Loss: 0.00048600
Iteration 101/1000 | Loss: 0.00058748
Iteration 102/1000 | Loss: 0.00020785
Iteration 103/1000 | Loss: 0.00021196
Iteration 104/1000 | Loss: 0.00017624
Iteration 105/1000 | Loss: 0.00011708
Iteration 106/1000 | Loss: 0.00010486
Iteration 107/1000 | Loss: 0.00016041
Iteration 108/1000 | Loss: 0.00014023
Iteration 109/1000 | Loss: 0.00033459
Iteration 110/1000 | Loss: 0.00231786
Iteration 111/1000 | Loss: 0.00036316
Iteration 112/1000 | Loss: 0.00200775
Iteration 113/1000 | Loss: 0.00096677
Iteration 114/1000 | Loss: 0.00141692
Iteration 115/1000 | Loss: 0.00192449
Iteration 116/1000 | Loss: 0.00061065
Iteration 117/1000 | Loss: 0.00052024
Iteration 118/1000 | Loss: 0.00032313
Iteration 119/1000 | Loss: 0.00040480
Iteration 120/1000 | Loss: 0.00083571
Iteration 121/1000 | Loss: 0.00046943
Iteration 122/1000 | Loss: 0.00007570
Iteration 123/1000 | Loss: 0.00042077
Iteration 124/1000 | Loss: 0.00005505
Iteration 125/1000 | Loss: 0.00008465
Iteration 126/1000 | Loss: 0.00008377
Iteration 127/1000 | Loss: 0.00021414
Iteration 128/1000 | Loss: 0.00066632
Iteration 129/1000 | Loss: 0.00007692
Iteration 130/1000 | Loss: 0.00005083
Iteration 131/1000 | Loss: 0.00007941
Iteration 132/1000 | Loss: 0.00025380
Iteration 133/1000 | Loss: 0.00092000
Iteration 134/1000 | Loss: 0.00073864
Iteration 135/1000 | Loss: 0.00078159
Iteration 136/1000 | Loss: 0.00005833
Iteration 137/1000 | Loss: 0.00009897
Iteration 138/1000 | Loss: 0.00084048
Iteration 139/1000 | Loss: 0.00007129
Iteration 140/1000 | Loss: 0.00009872
Iteration 141/1000 | Loss: 0.00005269
Iteration 142/1000 | Loss: 0.00026925
Iteration 143/1000 | Loss: 0.00004257
Iteration 144/1000 | Loss: 0.00003402
Iteration 145/1000 | Loss: 0.00010326
Iteration 146/1000 | Loss: 0.00002911
Iteration 147/1000 | Loss: 0.00026521
Iteration 148/1000 | Loss: 0.00052679
Iteration 149/1000 | Loss: 0.00063695
Iteration 150/1000 | Loss: 0.00010340
Iteration 151/1000 | Loss: 0.00012078
Iteration 152/1000 | Loss: 0.00003111
Iteration 153/1000 | Loss: 0.00005117
Iteration 154/1000 | Loss: 0.00002500
Iteration 155/1000 | Loss: 0.00009002
Iteration 156/1000 | Loss: 0.00046473
Iteration 157/1000 | Loss: 0.00005699
Iteration 158/1000 | Loss: 0.00003963
Iteration 159/1000 | Loss: 0.00004302
Iteration 160/1000 | Loss: 0.00003567
Iteration 161/1000 | Loss: 0.00019575
Iteration 162/1000 | Loss: 0.00027895
Iteration 163/1000 | Loss: 0.00009631
Iteration 164/1000 | Loss: 0.00019406
Iteration 165/1000 | Loss: 0.00003872
Iteration 166/1000 | Loss: 0.00003762
Iteration 167/1000 | Loss: 0.00001977
Iteration 168/1000 | Loss: 0.00005343
Iteration 169/1000 | Loss: 0.00023357
Iteration 170/1000 | Loss: 0.00006093
Iteration 171/1000 | Loss: 0.00002187
Iteration 172/1000 | Loss: 0.00016092
Iteration 173/1000 | Loss: 0.00001677
Iteration 174/1000 | Loss: 0.00001645
Iteration 175/1000 | Loss: 0.00001617
Iteration 176/1000 | Loss: 0.00001588
Iteration 177/1000 | Loss: 0.00001585
Iteration 178/1000 | Loss: 0.00001574
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001572
Iteration 181/1000 | Loss: 0.00001569
Iteration 182/1000 | Loss: 0.00001568
Iteration 183/1000 | Loss: 0.00001565
Iteration 184/1000 | Loss: 0.00006385
Iteration 185/1000 | Loss: 0.00001558
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00005131
Iteration 189/1000 | Loss: 0.00003490
Iteration 190/1000 | Loss: 0.00005820
Iteration 191/1000 | Loss: 0.00001535
Iteration 192/1000 | Loss: 0.00001533
Iteration 193/1000 | Loss: 0.00001533
Iteration 194/1000 | Loss: 0.00001533
Iteration 195/1000 | Loss: 0.00001533
Iteration 196/1000 | Loss: 0.00001533
Iteration 197/1000 | Loss: 0.00001533
Iteration 198/1000 | Loss: 0.00001533
Iteration 199/1000 | Loss: 0.00001533
Iteration 200/1000 | Loss: 0.00001533
Iteration 201/1000 | Loss: 0.00001533
Iteration 202/1000 | Loss: 0.00001533
Iteration 203/1000 | Loss: 0.00001533
Iteration 204/1000 | Loss: 0.00001532
Iteration 205/1000 | Loss: 0.00001532
Iteration 206/1000 | Loss: 0.00001531
Iteration 207/1000 | Loss: 0.00001530
Iteration 208/1000 | Loss: 0.00001529
Iteration 209/1000 | Loss: 0.00001529
Iteration 210/1000 | Loss: 0.00001528
Iteration 211/1000 | Loss: 0.00001528
Iteration 212/1000 | Loss: 0.00001528
Iteration 213/1000 | Loss: 0.00001528
Iteration 214/1000 | Loss: 0.00001528
Iteration 215/1000 | Loss: 0.00001528
Iteration 216/1000 | Loss: 0.00001527
Iteration 217/1000 | Loss: 0.00004182
Iteration 218/1000 | Loss: 0.00002980
Iteration 219/1000 | Loss: 0.00001525
Iteration 220/1000 | Loss: 0.00001525
Iteration 221/1000 | Loss: 0.00001525
Iteration 222/1000 | Loss: 0.00001525
Iteration 223/1000 | Loss: 0.00001525
Iteration 224/1000 | Loss: 0.00001525
Iteration 225/1000 | Loss: 0.00001525
Iteration 226/1000 | Loss: 0.00001525
Iteration 227/1000 | Loss: 0.00001524
Iteration 228/1000 | Loss: 0.00001524
Iteration 229/1000 | Loss: 0.00001524
Iteration 230/1000 | Loss: 0.00001524
Iteration 231/1000 | Loss: 0.00001524
Iteration 232/1000 | Loss: 0.00001524
Iteration 233/1000 | Loss: 0.00001524
Iteration 234/1000 | Loss: 0.00001524
Iteration 235/1000 | Loss: 0.00001524
Iteration 236/1000 | Loss: 0.00001524
Iteration 237/1000 | Loss: 0.00001524
Iteration 238/1000 | Loss: 0.00001523
Iteration 239/1000 | Loss: 0.00001523
Iteration 240/1000 | Loss: 0.00001523
Iteration 241/1000 | Loss: 0.00001523
Iteration 242/1000 | Loss: 0.00001523
Iteration 243/1000 | Loss: 0.00001523
Iteration 244/1000 | Loss: 0.00003723
Iteration 245/1000 | Loss: 0.00064761
Iteration 246/1000 | Loss: 0.00022329
Iteration 247/1000 | Loss: 0.00002062
Iteration 248/1000 | Loss: 0.00012366
Iteration 249/1000 | Loss: 0.00002250
Iteration 250/1000 | Loss: 0.00009623
Iteration 251/1000 | Loss: 0.00037170
Iteration 252/1000 | Loss: 0.00075034
Iteration 253/1000 | Loss: 0.00002201
Iteration 254/1000 | Loss: 0.00001562
Iteration 255/1000 | Loss: 0.00013975
Iteration 256/1000 | Loss: 0.00006598
Iteration 257/1000 | Loss: 0.00006032
Iteration 258/1000 | Loss: 0.00022830
Iteration 259/1000 | Loss: 0.00016517
Iteration 260/1000 | Loss: 0.00007814
Iteration 261/1000 | Loss: 0.00010788
Iteration 262/1000 | Loss: 0.00012168
Iteration 263/1000 | Loss: 0.00005556
Iteration 264/1000 | Loss: 0.00006182
Iteration 265/1000 | Loss: 0.00002235
Iteration 266/1000 | Loss: 0.00002821
Iteration 267/1000 | Loss: 0.00002172
Iteration 268/1000 | Loss: 0.00001917
Iteration 269/1000 | Loss: 0.00001526
Iteration 270/1000 | Loss: 0.00001524
Iteration 271/1000 | Loss: 0.00001524
Iteration 272/1000 | Loss: 0.00001523
Iteration 273/1000 | Loss: 0.00001520
Iteration 274/1000 | Loss: 0.00001520
Iteration 275/1000 | Loss: 0.00001519
Iteration 276/1000 | Loss: 0.00001519
Iteration 277/1000 | Loss: 0.00001519
Iteration 278/1000 | Loss: 0.00001518
Iteration 279/1000 | Loss: 0.00001518
Iteration 280/1000 | Loss: 0.00001517
Iteration 281/1000 | Loss: 0.00001517
Iteration 282/1000 | Loss: 0.00001517
Iteration 283/1000 | Loss: 0.00001516
Iteration 284/1000 | Loss: 0.00001516
Iteration 285/1000 | Loss: 0.00001515
Iteration 286/1000 | Loss: 0.00001515
Iteration 287/1000 | Loss: 0.00001515
Iteration 288/1000 | Loss: 0.00001514
Iteration 289/1000 | Loss: 0.00001514
Iteration 290/1000 | Loss: 0.00001513
Iteration 291/1000 | Loss: 0.00001513
Iteration 292/1000 | Loss: 0.00001513
Iteration 293/1000 | Loss: 0.00001513
Iteration 294/1000 | Loss: 0.00001512
Iteration 295/1000 | Loss: 0.00001512
Iteration 296/1000 | Loss: 0.00001512
Iteration 297/1000 | Loss: 0.00001512
Iteration 298/1000 | Loss: 0.00001512
Iteration 299/1000 | Loss: 0.00001511
Iteration 300/1000 | Loss: 0.00001511
Iteration 301/1000 | Loss: 0.00001511
Iteration 302/1000 | Loss: 0.00001511
Iteration 303/1000 | Loss: 0.00001511
Iteration 304/1000 | Loss: 0.00001511
Iteration 305/1000 | Loss: 0.00001511
Iteration 306/1000 | Loss: 0.00001511
Iteration 307/1000 | Loss: 0.00001511
Iteration 308/1000 | Loss: 0.00001511
Iteration 309/1000 | Loss: 0.00001511
Iteration 310/1000 | Loss: 0.00001511
Iteration 311/1000 | Loss: 0.00001511
Iteration 312/1000 | Loss: 0.00001511
Iteration 313/1000 | Loss: 0.00001510
Iteration 314/1000 | Loss: 0.00001510
Iteration 315/1000 | Loss: 0.00001510
Iteration 316/1000 | Loss: 0.00001510
Iteration 317/1000 | Loss: 0.00001510
Iteration 318/1000 | Loss: 0.00001510
Iteration 319/1000 | Loss: 0.00001510
Iteration 320/1000 | Loss: 0.00001510
Iteration 321/1000 | Loss: 0.00001510
Iteration 322/1000 | Loss: 0.00001510
Iteration 323/1000 | Loss: 0.00001510
Iteration 324/1000 | Loss: 0.00001510
Iteration 325/1000 | Loss: 0.00001510
Iteration 326/1000 | Loss: 0.00001510
Iteration 327/1000 | Loss: 0.00001510
Iteration 328/1000 | Loss: 0.00001510
Iteration 329/1000 | Loss: 0.00001509
Iteration 330/1000 | Loss: 0.00001509
Iteration 331/1000 | Loss: 0.00001509
Iteration 332/1000 | Loss: 0.00001509
Iteration 333/1000 | Loss: 0.00001509
Iteration 334/1000 | Loss: 0.00001509
Iteration 335/1000 | Loss: 0.00001509
Iteration 336/1000 | Loss: 0.00001509
Iteration 337/1000 | Loss: 0.00001509
Iteration 338/1000 | Loss: 0.00001509
Iteration 339/1000 | Loss: 0.00001509
Iteration 340/1000 | Loss: 0.00001509
Iteration 341/1000 | Loss: 0.00001509
Iteration 342/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [1.5091822206159122e-05, 1.5091822206159122e-05, 1.5091822206159122e-05, 1.5091822206159122e-05, 1.5091822206159122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5091822206159122e-05

Optimization complete. Final v2v error: 3.301511526107788 mm

Highest mean error: 3.905759811401367 mm for frame 0

Lowest mean error: 3.0093061923980713 mm for frame 65

Saving results

Total time: 396.0899386405945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145961
Iteration 2/25 | Loss: 0.00448739
Iteration 3/25 | Loss: 0.00309197
Iteration 4/25 | Loss: 0.00339698
Iteration 5/25 | Loss: 0.00268155
Iteration 6/25 | Loss: 0.00252111
Iteration 7/25 | Loss: 0.00239688
Iteration 8/25 | Loss: 0.00228199
Iteration 9/25 | Loss: 0.00222893
Iteration 10/25 | Loss: 0.00221179
Iteration 11/25 | Loss: 0.00219395
Iteration 12/25 | Loss: 0.00218814
Iteration 13/25 | Loss: 0.00216941
Iteration 14/25 | Loss: 0.00215147
Iteration 15/25 | Loss: 0.00214021
Iteration 16/25 | Loss: 0.00213305
Iteration 17/25 | Loss: 0.00213580
Iteration 18/25 | Loss: 0.00213400
Iteration 19/25 | Loss: 0.00213076
Iteration 20/25 | Loss: 0.00213022
Iteration 21/25 | Loss: 0.00213014
Iteration 22/25 | Loss: 0.00213014
Iteration 23/25 | Loss: 0.00213014
Iteration 24/25 | Loss: 0.00213014
Iteration 25/25 | Loss: 0.00213014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51802295
Iteration 2/25 | Loss: 0.02825610
Iteration 3/25 | Loss: 0.02611206
Iteration 4/25 | Loss: 0.01173040
Iteration 5/25 | Loss: 0.01111681
Iteration 6/25 | Loss: 0.01124308
Iteration 7/25 | Loss: 0.00678138
Iteration 8/25 | Loss: 0.00678134
Iteration 9/25 | Loss: 0.00678134
Iteration 10/25 | Loss: 0.00678134
Iteration 11/25 | Loss: 0.00678134
Iteration 12/25 | Loss: 0.00678134
Iteration 13/25 | Loss: 0.00678134
Iteration 14/25 | Loss: 0.00678134
Iteration 15/25 | Loss: 0.00678134
Iteration 16/25 | Loss: 0.00678134
Iteration 17/25 | Loss: 0.00678134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00678133824840188, 0.00678133824840188, 0.00678133824840188, 0.00678133824840188, 0.00678133824840188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00678133824840188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00678134
Iteration 2/1000 | Loss: 0.00072629
Iteration 3/1000 | Loss: 0.00077771
Iteration 4/1000 | Loss: 0.00042660
Iteration 5/1000 | Loss: 0.00044251
Iteration 6/1000 | Loss: 0.00035794
Iteration 7/1000 | Loss: 0.00032345
Iteration 8/1000 | Loss: 0.00143069
Iteration 9/1000 | Loss: 0.01051587
Iteration 10/1000 | Loss: 0.01835992
Iteration 11/1000 | Loss: 0.00869286
Iteration 12/1000 | Loss: 0.00099593
Iteration 13/1000 | Loss: 0.00048638
Iteration 14/1000 | Loss: 0.00029758
Iteration 15/1000 | Loss: 0.00022117
Iteration 16/1000 | Loss: 0.00065258
Iteration 17/1000 | Loss: 0.00472802
Iteration 18/1000 | Loss: 0.00062729
Iteration 19/1000 | Loss: 0.00021274
Iteration 20/1000 | Loss: 0.00012278
Iteration 21/1000 | Loss: 0.00381499
Iteration 22/1000 | Loss: 0.00047842
Iteration 23/1000 | Loss: 0.00024536
Iteration 24/1000 | Loss: 0.00009634
Iteration 25/1000 | Loss: 0.00375252
Iteration 26/1000 | Loss: 0.00139045
Iteration 27/1000 | Loss: 0.00037271
Iteration 28/1000 | Loss: 0.00326730
Iteration 29/1000 | Loss: 0.00078277
Iteration 30/1000 | Loss: 0.00117675
Iteration 31/1000 | Loss: 0.00047375
Iteration 32/1000 | Loss: 0.00019970
Iteration 33/1000 | Loss: 0.00007990
Iteration 34/1000 | Loss: 0.00061592
Iteration 35/1000 | Loss: 0.00059121
Iteration 36/1000 | Loss: 0.00114066
Iteration 37/1000 | Loss: 0.00008026
Iteration 38/1000 | Loss: 0.00007097
Iteration 39/1000 | Loss: 0.00073509
Iteration 40/1000 | Loss: 0.00006557
Iteration 41/1000 | Loss: 0.00006276
Iteration 42/1000 | Loss: 0.00006079
Iteration 43/1000 | Loss: 0.00005906
Iteration 44/1000 | Loss: 0.00005803
Iteration 45/1000 | Loss: 0.00005724
Iteration 46/1000 | Loss: 0.00005645
Iteration 47/1000 | Loss: 0.00005592
Iteration 48/1000 | Loss: 0.00005558
Iteration 49/1000 | Loss: 0.00005526
Iteration 50/1000 | Loss: 0.00005502
Iteration 51/1000 | Loss: 0.00005479
Iteration 52/1000 | Loss: 0.00005472
Iteration 53/1000 | Loss: 0.00005457
Iteration 54/1000 | Loss: 0.00005455
Iteration 55/1000 | Loss: 0.00005452
Iteration 56/1000 | Loss: 0.00005436
Iteration 57/1000 | Loss: 0.00005426
Iteration 58/1000 | Loss: 0.00005422
Iteration 59/1000 | Loss: 0.00005410
Iteration 60/1000 | Loss: 0.00005410
Iteration 61/1000 | Loss: 0.00005401
Iteration 62/1000 | Loss: 0.00005398
Iteration 63/1000 | Loss: 0.00005394
Iteration 64/1000 | Loss: 0.00005392
Iteration 65/1000 | Loss: 0.00005390
Iteration 66/1000 | Loss: 0.00005389
Iteration 67/1000 | Loss: 0.00005389
Iteration 68/1000 | Loss: 0.00005386
Iteration 69/1000 | Loss: 0.00005383
Iteration 70/1000 | Loss: 0.00005382
Iteration 71/1000 | Loss: 0.00005378
Iteration 72/1000 | Loss: 0.00005375
Iteration 73/1000 | Loss: 0.00005375
Iteration 74/1000 | Loss: 0.00005371
Iteration 75/1000 | Loss: 0.00005371
Iteration 76/1000 | Loss: 0.00005370
Iteration 77/1000 | Loss: 0.00005370
Iteration 78/1000 | Loss: 0.00005369
Iteration 79/1000 | Loss: 0.00005368
Iteration 80/1000 | Loss: 0.00005368
Iteration 81/1000 | Loss: 0.00005367
Iteration 82/1000 | Loss: 0.00005366
Iteration 83/1000 | Loss: 0.00005366
Iteration 84/1000 | Loss: 0.00005365
Iteration 85/1000 | Loss: 0.00005364
Iteration 86/1000 | Loss: 0.00005363
Iteration 87/1000 | Loss: 0.00005363
Iteration 88/1000 | Loss: 0.00005363
Iteration 89/1000 | Loss: 0.00005363
Iteration 90/1000 | Loss: 0.00005363
Iteration 91/1000 | Loss: 0.00005362
Iteration 92/1000 | Loss: 0.00005362
Iteration 93/1000 | Loss: 0.00005362
Iteration 94/1000 | Loss: 0.00005362
Iteration 95/1000 | Loss: 0.00005362
Iteration 96/1000 | Loss: 0.00005362
Iteration 97/1000 | Loss: 0.00005362
Iteration 98/1000 | Loss: 0.00005362
Iteration 99/1000 | Loss: 0.00005362
Iteration 100/1000 | Loss: 0.00005362
Iteration 101/1000 | Loss: 0.00005362
Iteration 102/1000 | Loss: 0.00005362
Iteration 103/1000 | Loss: 0.00005362
Iteration 104/1000 | Loss: 0.00005362
Iteration 105/1000 | Loss: 0.00005362
Iteration 106/1000 | Loss: 0.00005362
Iteration 107/1000 | Loss: 0.00005362
Iteration 108/1000 | Loss: 0.00005362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [5.362094088923186e-05, 5.362094088923186e-05, 5.362094088923186e-05, 5.362094088923186e-05, 5.362094088923186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.362094088923186e-05

Optimization complete. Final v2v error: 5.594278812408447 mm

Highest mean error: 11.823875427246094 mm for frame 239

Lowest mean error: 4.027211666107178 mm for frame 3

Saving results

Total time: 148.343031167984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831283
Iteration 2/25 | Loss: 0.00136671
Iteration 3/25 | Loss: 0.00129510
Iteration 4/25 | Loss: 0.00128875
Iteration 5/25 | Loss: 0.00128815
Iteration 6/25 | Loss: 0.00128815
Iteration 7/25 | Loss: 0.00128815
Iteration 8/25 | Loss: 0.00128815
Iteration 9/25 | Loss: 0.00128815
Iteration 10/25 | Loss: 0.00128815
Iteration 11/25 | Loss: 0.00128815
Iteration 12/25 | Loss: 0.00128815
Iteration 13/25 | Loss: 0.00128815
Iteration 14/25 | Loss: 0.00128815
Iteration 15/25 | Loss: 0.00128815
Iteration 16/25 | Loss: 0.00128815
Iteration 17/25 | Loss: 0.00128815
Iteration 18/25 | Loss: 0.00128815
Iteration 19/25 | Loss: 0.00128815
Iteration 20/25 | Loss: 0.00128815
Iteration 21/25 | Loss: 0.00128815
Iteration 22/25 | Loss: 0.00128815
Iteration 23/25 | Loss: 0.00128815
Iteration 24/25 | Loss: 0.00128815
Iteration 25/25 | Loss: 0.00128815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.83435726
Iteration 2/25 | Loss: 0.00089677
Iteration 3/25 | Loss: 0.00089677
Iteration 4/25 | Loss: 0.00089677
Iteration 5/25 | Loss: 0.00089677
Iteration 6/25 | Loss: 0.00089677
Iteration 7/25 | Loss: 0.00089677
Iteration 8/25 | Loss: 0.00089677
Iteration 9/25 | Loss: 0.00089677
Iteration 10/25 | Loss: 0.00089677
Iteration 11/25 | Loss: 0.00089677
Iteration 12/25 | Loss: 0.00089677
Iteration 13/25 | Loss: 0.00089677
Iteration 14/25 | Loss: 0.00089677
Iteration 15/25 | Loss: 0.00089677
Iteration 16/25 | Loss: 0.00089677
Iteration 17/25 | Loss: 0.00089677
Iteration 18/25 | Loss: 0.00089677
Iteration 19/25 | Loss: 0.00089677
Iteration 20/25 | Loss: 0.00089677
Iteration 21/25 | Loss: 0.00089677
Iteration 22/25 | Loss: 0.00089677
Iteration 23/25 | Loss: 0.00089677
Iteration 24/25 | Loss: 0.00089677
Iteration 25/25 | Loss: 0.00089677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089677
Iteration 2/1000 | Loss: 0.00002203
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001220
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001216
Iteration 29/1000 | Loss: 0.00001215
Iteration 30/1000 | Loss: 0.00001214
Iteration 31/1000 | Loss: 0.00001213
Iteration 32/1000 | Loss: 0.00001213
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001209
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001187
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001178
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1783761692640837e-05, 1.1783761692640837e-05, 1.1783761692640837e-05, 1.1783761692640837e-05, 1.1783761692640837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1783761692640837e-05

Optimization complete. Final v2v error: 2.9443509578704834 mm

Highest mean error: 3.176081895828247 mm for frame 238

Lowest mean error: 2.7445855140686035 mm for frame 135

Saving results

Total time: 41.33324837684631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440046
Iteration 2/25 | Loss: 0.00144856
Iteration 3/25 | Loss: 0.00134150
Iteration 4/25 | Loss: 0.00133083
Iteration 5/25 | Loss: 0.00132835
Iteration 6/25 | Loss: 0.00132835
Iteration 7/25 | Loss: 0.00132835
Iteration 8/25 | Loss: 0.00132835
Iteration 9/25 | Loss: 0.00132835
Iteration 10/25 | Loss: 0.00132835
Iteration 11/25 | Loss: 0.00132835
Iteration 12/25 | Loss: 0.00132835
Iteration 13/25 | Loss: 0.00132835
Iteration 14/25 | Loss: 0.00132835
Iteration 15/25 | Loss: 0.00132835
Iteration 16/25 | Loss: 0.00132835
Iteration 17/25 | Loss: 0.00132835
Iteration 18/25 | Loss: 0.00132835
Iteration 19/25 | Loss: 0.00132835
Iteration 20/25 | Loss: 0.00132835
Iteration 21/25 | Loss: 0.00132835
Iteration 22/25 | Loss: 0.00132835
Iteration 23/25 | Loss: 0.00132835
Iteration 24/25 | Loss: 0.00132835
Iteration 25/25 | Loss: 0.00132835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38398778
Iteration 2/25 | Loss: 0.00099664
Iteration 3/25 | Loss: 0.00099664
Iteration 4/25 | Loss: 0.00099664
Iteration 5/25 | Loss: 0.00099663
Iteration 6/25 | Loss: 0.00099663
Iteration 7/25 | Loss: 0.00099663
Iteration 8/25 | Loss: 0.00099663
Iteration 9/25 | Loss: 0.00099663
Iteration 10/25 | Loss: 0.00099663
Iteration 11/25 | Loss: 0.00099663
Iteration 12/25 | Loss: 0.00099663
Iteration 13/25 | Loss: 0.00099663
Iteration 14/25 | Loss: 0.00099663
Iteration 15/25 | Loss: 0.00099663
Iteration 16/25 | Loss: 0.00099663
Iteration 17/25 | Loss: 0.00099663
Iteration 18/25 | Loss: 0.00099663
Iteration 19/25 | Loss: 0.00099663
Iteration 20/25 | Loss: 0.00099663
Iteration 21/25 | Loss: 0.00099663
Iteration 22/25 | Loss: 0.00099663
Iteration 23/25 | Loss: 0.00099663
Iteration 24/25 | Loss: 0.00099663
Iteration 25/25 | Loss: 0.00099663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099663
Iteration 2/1000 | Loss: 0.00003583
Iteration 3/1000 | Loss: 0.00002228
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001799
Iteration 6/1000 | Loss: 0.00001713
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001598
Iteration 12/1000 | Loss: 0.00001595
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001579
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001569
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001566
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001564
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001560
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001557
Iteration 27/1000 | Loss: 0.00001556
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001534
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001531
Iteration 46/1000 | Loss: 0.00001531
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001516
Iteration 75/1000 | Loss: 0.00001516
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001515
Iteration 83/1000 | Loss: 0.00001515
Iteration 84/1000 | Loss: 0.00001515
Iteration 85/1000 | Loss: 0.00001515
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001514
Iteration 88/1000 | Loss: 0.00001514
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001513
Iteration 94/1000 | Loss: 0.00001513
Iteration 95/1000 | Loss: 0.00001513
Iteration 96/1000 | Loss: 0.00001513
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001511
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001506
Iteration 126/1000 | Loss: 0.00001506
Iteration 127/1000 | Loss: 0.00001506
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001504
Iteration 137/1000 | Loss: 0.00001504
Iteration 138/1000 | Loss: 0.00001504
Iteration 139/1000 | Loss: 0.00001504
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001501
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.5013619304227177e-05, 1.5013619304227177e-05, 1.5013619304227177e-05, 1.5013619304227177e-05, 1.5013619304227177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5013619304227177e-05

Optimization complete. Final v2v error: 3.2201595306396484 mm

Highest mean error: 3.499070882797241 mm for frame 7

Lowest mean error: 3.013906955718994 mm for frame 180

Saving results

Total time: 46.561450719833374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984168
Iteration 2/25 | Loss: 0.00984168
Iteration 3/25 | Loss: 0.00281186
Iteration 4/25 | Loss: 0.00191026
Iteration 5/25 | Loss: 0.00176841
Iteration 6/25 | Loss: 0.00181560
Iteration 7/25 | Loss: 0.00188665
Iteration 8/25 | Loss: 0.00175911
Iteration 9/25 | Loss: 0.00164900
Iteration 10/25 | Loss: 0.00159107
Iteration 11/25 | Loss: 0.00153807
Iteration 12/25 | Loss: 0.00151480
Iteration 13/25 | Loss: 0.00151649
Iteration 14/25 | Loss: 0.00151944
Iteration 15/25 | Loss: 0.00150446
Iteration 16/25 | Loss: 0.00150357
Iteration 17/25 | Loss: 0.00150303
Iteration 18/25 | Loss: 0.00150717
Iteration 19/25 | Loss: 0.00150157
Iteration 20/25 | Loss: 0.00149696
Iteration 21/25 | Loss: 0.00149784
Iteration 22/25 | Loss: 0.00149836
Iteration 23/25 | Loss: 0.00149784
Iteration 24/25 | Loss: 0.00149481
Iteration 25/25 | Loss: 0.00150079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41286790
Iteration 2/25 | Loss: 0.00295798
Iteration 3/25 | Loss: 0.00265099
Iteration 4/25 | Loss: 0.00265099
Iteration 5/25 | Loss: 0.00265099
Iteration 6/25 | Loss: 0.00265099
Iteration 7/25 | Loss: 0.00265099
Iteration 8/25 | Loss: 0.00265099
Iteration 9/25 | Loss: 0.00265099
Iteration 10/25 | Loss: 0.00265099
Iteration 11/25 | Loss: 0.00265099
Iteration 12/25 | Loss: 0.00265099
Iteration 13/25 | Loss: 0.00265099
Iteration 14/25 | Loss: 0.00265099
Iteration 15/25 | Loss: 0.00265099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026509868912398815, 0.0026509868912398815, 0.0026509868912398815, 0.0026509868912398815, 0.0026509868912398815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026509868912398815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265099
Iteration 2/1000 | Loss: 0.00156155
Iteration 3/1000 | Loss: 0.00312209
Iteration 4/1000 | Loss: 0.00251926
Iteration 5/1000 | Loss: 0.00062356
Iteration 6/1000 | Loss: 0.00086145
Iteration 7/1000 | Loss: 0.00060690
Iteration 8/1000 | Loss: 0.00081816
Iteration 9/1000 | Loss: 0.00134731
Iteration 10/1000 | Loss: 0.00075299
Iteration 11/1000 | Loss: 0.00110937
Iteration 12/1000 | Loss: 0.00041597
Iteration 13/1000 | Loss: 0.00111999
Iteration 14/1000 | Loss: 0.00084355
Iteration 15/1000 | Loss: 0.00060715
Iteration 16/1000 | Loss: 0.00088789
Iteration 17/1000 | Loss: 0.00076647
Iteration 18/1000 | Loss: 0.00025179
Iteration 19/1000 | Loss: 0.00042496
Iteration 20/1000 | Loss: 0.00035736
Iteration 21/1000 | Loss: 0.00034270
Iteration 22/1000 | Loss: 0.00022251
Iteration 23/1000 | Loss: 0.00021833
Iteration 24/1000 | Loss: 0.00088026
Iteration 25/1000 | Loss: 0.00138153
Iteration 26/1000 | Loss: 0.00079342
Iteration 27/1000 | Loss: 0.00026208
Iteration 28/1000 | Loss: 0.00103164
Iteration 29/1000 | Loss: 0.00043287
Iteration 30/1000 | Loss: 0.00018133
Iteration 31/1000 | Loss: 0.00026695
Iteration 32/1000 | Loss: 0.00024652
Iteration 33/1000 | Loss: 0.00017406
Iteration 34/1000 | Loss: 0.00016023
Iteration 35/1000 | Loss: 0.00018755
Iteration 36/1000 | Loss: 0.00077669
Iteration 37/1000 | Loss: 0.00035626
Iteration 38/1000 | Loss: 0.00046433
Iteration 39/1000 | Loss: 0.00031792
Iteration 40/1000 | Loss: 0.00026521
Iteration 41/1000 | Loss: 0.00039778
Iteration 42/1000 | Loss: 0.00027016
Iteration 43/1000 | Loss: 0.00038651
Iteration 44/1000 | Loss: 0.00020605
Iteration 45/1000 | Loss: 0.00013611
Iteration 46/1000 | Loss: 0.00021062
Iteration 47/1000 | Loss: 0.00041486
Iteration 48/1000 | Loss: 0.00026884
Iteration 49/1000 | Loss: 0.00022954
Iteration 50/1000 | Loss: 0.00021980
Iteration 51/1000 | Loss: 0.00022403
Iteration 52/1000 | Loss: 0.00024299
Iteration 53/1000 | Loss: 0.00015985
Iteration 54/1000 | Loss: 0.00016097
Iteration 55/1000 | Loss: 0.00015179
Iteration 56/1000 | Loss: 0.00017848
Iteration 57/1000 | Loss: 0.00016294
Iteration 58/1000 | Loss: 0.00029023
Iteration 59/1000 | Loss: 0.00023824
Iteration 60/1000 | Loss: 0.00027838
Iteration 61/1000 | Loss: 0.00024145
Iteration 62/1000 | Loss: 0.00017348
Iteration 63/1000 | Loss: 0.00016189
Iteration 64/1000 | Loss: 0.00014243
Iteration 65/1000 | Loss: 0.00014966
Iteration 66/1000 | Loss: 0.00015318
Iteration 67/1000 | Loss: 0.00017506
Iteration 68/1000 | Loss: 0.00011854
Iteration 69/1000 | Loss: 0.00015357
Iteration 70/1000 | Loss: 0.00014896
Iteration 71/1000 | Loss: 0.00035430
Iteration 72/1000 | Loss: 0.00013393
Iteration 73/1000 | Loss: 0.00024868
Iteration 74/1000 | Loss: 0.00020527
Iteration 75/1000 | Loss: 0.00030493
Iteration 76/1000 | Loss: 0.00024696
Iteration 77/1000 | Loss: 0.00017858
Iteration 78/1000 | Loss: 0.00012489
Iteration 79/1000 | Loss: 0.00011368
Iteration 80/1000 | Loss: 0.00012043
Iteration 81/1000 | Loss: 0.00042626
Iteration 82/1000 | Loss: 0.00045934
Iteration 83/1000 | Loss: 0.00029544
Iteration 84/1000 | Loss: 0.00018707
Iteration 85/1000 | Loss: 0.00016008
Iteration 86/1000 | Loss: 0.00018367
Iteration 87/1000 | Loss: 0.00016018
Iteration 88/1000 | Loss: 0.00016152
Iteration 89/1000 | Loss: 0.00016073
Iteration 90/1000 | Loss: 0.00014545
Iteration 91/1000 | Loss: 0.00014695
Iteration 92/1000 | Loss: 0.00017020
Iteration 93/1000 | Loss: 0.00017076
Iteration 94/1000 | Loss: 0.00016240
Iteration 95/1000 | Loss: 0.00089890
Iteration 96/1000 | Loss: 0.00160639
Iteration 97/1000 | Loss: 0.00179063
Iteration 98/1000 | Loss: 0.00046123
Iteration 99/1000 | Loss: 0.00035227
Iteration 100/1000 | Loss: 0.00021852
Iteration 101/1000 | Loss: 0.00015724
Iteration 102/1000 | Loss: 0.00025593
Iteration 103/1000 | Loss: 0.00033020
Iteration 104/1000 | Loss: 0.00018185
Iteration 105/1000 | Loss: 0.00028565
Iteration 106/1000 | Loss: 0.00051985
Iteration 107/1000 | Loss: 0.00032840
Iteration 108/1000 | Loss: 0.00048518
Iteration 109/1000 | Loss: 0.00029290
Iteration 110/1000 | Loss: 0.00018496
Iteration 111/1000 | Loss: 0.00015773
Iteration 112/1000 | Loss: 0.00015555
Iteration 113/1000 | Loss: 0.00051468
Iteration 114/1000 | Loss: 0.00054468
Iteration 115/1000 | Loss: 0.00025570
Iteration 116/1000 | Loss: 0.00009328
Iteration 117/1000 | Loss: 0.00007214
Iteration 118/1000 | Loss: 0.00072966
Iteration 119/1000 | Loss: 0.00046691
Iteration 120/1000 | Loss: 0.00063434
Iteration 121/1000 | Loss: 0.00009018
Iteration 122/1000 | Loss: 0.00005454
Iteration 123/1000 | Loss: 0.00025301
Iteration 124/1000 | Loss: 0.00033095
Iteration 125/1000 | Loss: 0.00057105
Iteration 126/1000 | Loss: 0.00105021
Iteration 127/1000 | Loss: 0.00028525
Iteration 128/1000 | Loss: 0.00005918
Iteration 129/1000 | Loss: 0.00009459
Iteration 130/1000 | Loss: 0.00056308
Iteration 131/1000 | Loss: 0.00009116
Iteration 132/1000 | Loss: 0.00050104
Iteration 133/1000 | Loss: 0.00047582
Iteration 134/1000 | Loss: 0.00015069
Iteration 135/1000 | Loss: 0.00026268
Iteration 136/1000 | Loss: 0.00019751
Iteration 137/1000 | Loss: 0.00004466
Iteration 138/1000 | Loss: 0.00004713
Iteration 139/1000 | Loss: 0.00003730
Iteration 140/1000 | Loss: 0.00003518
Iteration 141/1000 | Loss: 0.00003285
Iteration 142/1000 | Loss: 0.00003114
Iteration 143/1000 | Loss: 0.00002979
Iteration 144/1000 | Loss: 0.00071399
Iteration 145/1000 | Loss: 0.00089196
Iteration 146/1000 | Loss: 0.00078907
Iteration 147/1000 | Loss: 0.00047025
Iteration 148/1000 | Loss: 0.00037292
Iteration 149/1000 | Loss: 0.00025325
Iteration 150/1000 | Loss: 0.00014365
Iteration 151/1000 | Loss: 0.00018532
Iteration 152/1000 | Loss: 0.00003249
Iteration 153/1000 | Loss: 0.00023621
Iteration 154/1000 | Loss: 0.00041486
Iteration 155/1000 | Loss: 0.00004165
Iteration 156/1000 | Loss: 0.00003163
Iteration 157/1000 | Loss: 0.00002291
Iteration 158/1000 | Loss: 0.00002040
Iteration 159/1000 | Loss: 0.00003058
Iteration 160/1000 | Loss: 0.00005345
Iteration 161/1000 | Loss: 0.00001885
Iteration 162/1000 | Loss: 0.00001791
Iteration 163/1000 | Loss: 0.00001729
Iteration 164/1000 | Loss: 0.00001694
Iteration 165/1000 | Loss: 0.00001658
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001583
Iteration 168/1000 | Loss: 0.00001561
Iteration 169/1000 | Loss: 0.00001542
Iteration 170/1000 | Loss: 0.00001532
Iteration 171/1000 | Loss: 0.00001524
Iteration 172/1000 | Loss: 0.00001521
Iteration 173/1000 | Loss: 0.00001518
Iteration 174/1000 | Loss: 0.00001518
Iteration 175/1000 | Loss: 0.00001518
Iteration 176/1000 | Loss: 0.00001515
Iteration 177/1000 | Loss: 0.00001512
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001512
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001510
Iteration 182/1000 | Loss: 0.00001510
Iteration 183/1000 | Loss: 0.00001509
Iteration 184/1000 | Loss: 0.00001508
Iteration 185/1000 | Loss: 0.00001507
Iteration 186/1000 | Loss: 0.00001506
Iteration 187/1000 | Loss: 0.00001504
Iteration 188/1000 | Loss: 0.00001499
Iteration 189/1000 | Loss: 0.00001496
Iteration 190/1000 | Loss: 0.00001495
Iteration 191/1000 | Loss: 0.00001495
Iteration 192/1000 | Loss: 0.00001494
Iteration 193/1000 | Loss: 0.00001493
Iteration 194/1000 | Loss: 0.00001493
Iteration 195/1000 | Loss: 0.00001492
Iteration 196/1000 | Loss: 0.00001491
Iteration 197/1000 | Loss: 0.00001491
Iteration 198/1000 | Loss: 0.00001490
Iteration 199/1000 | Loss: 0.00001488
Iteration 200/1000 | Loss: 0.00001488
Iteration 201/1000 | Loss: 0.00001488
Iteration 202/1000 | Loss: 0.00001488
Iteration 203/1000 | Loss: 0.00001488
Iteration 204/1000 | Loss: 0.00001487
Iteration 205/1000 | Loss: 0.00001487
Iteration 206/1000 | Loss: 0.00001485
Iteration 207/1000 | Loss: 0.00001485
Iteration 208/1000 | Loss: 0.00001484
Iteration 209/1000 | Loss: 0.00001484
Iteration 210/1000 | Loss: 0.00001484
Iteration 211/1000 | Loss: 0.00001483
Iteration 212/1000 | Loss: 0.00001483
Iteration 213/1000 | Loss: 0.00001483
Iteration 214/1000 | Loss: 0.00001483
Iteration 215/1000 | Loss: 0.00001483
Iteration 216/1000 | Loss: 0.00001483
Iteration 217/1000 | Loss: 0.00001483
Iteration 218/1000 | Loss: 0.00001483
Iteration 219/1000 | Loss: 0.00001482
Iteration 220/1000 | Loss: 0.00001482
Iteration 221/1000 | Loss: 0.00001482
Iteration 222/1000 | Loss: 0.00001482
Iteration 223/1000 | Loss: 0.00001482
Iteration 224/1000 | Loss: 0.00001482
Iteration 225/1000 | Loss: 0.00001482
Iteration 226/1000 | Loss: 0.00001482
Iteration 227/1000 | Loss: 0.00001481
Iteration 228/1000 | Loss: 0.00001481
Iteration 229/1000 | Loss: 0.00001481
Iteration 230/1000 | Loss: 0.00001481
Iteration 231/1000 | Loss: 0.00001481
Iteration 232/1000 | Loss: 0.00001481
Iteration 233/1000 | Loss: 0.00001481
Iteration 234/1000 | Loss: 0.00001481
Iteration 235/1000 | Loss: 0.00001481
Iteration 236/1000 | Loss: 0.00001481
Iteration 237/1000 | Loss: 0.00001481
Iteration 238/1000 | Loss: 0.00001481
Iteration 239/1000 | Loss: 0.00001481
Iteration 240/1000 | Loss: 0.00001481
Iteration 241/1000 | Loss: 0.00001480
Iteration 242/1000 | Loss: 0.00001480
Iteration 243/1000 | Loss: 0.00001480
Iteration 244/1000 | Loss: 0.00001480
Iteration 245/1000 | Loss: 0.00001480
Iteration 246/1000 | Loss: 0.00001480
Iteration 247/1000 | Loss: 0.00001479
Iteration 248/1000 | Loss: 0.00001479
Iteration 249/1000 | Loss: 0.00001479
Iteration 250/1000 | Loss: 0.00001479
Iteration 251/1000 | Loss: 0.00001479
Iteration 252/1000 | Loss: 0.00001479
Iteration 253/1000 | Loss: 0.00001478
Iteration 254/1000 | Loss: 0.00001478
Iteration 255/1000 | Loss: 0.00001478
Iteration 256/1000 | Loss: 0.00001478
Iteration 257/1000 | Loss: 0.00001478
Iteration 258/1000 | Loss: 0.00001478
Iteration 259/1000 | Loss: 0.00001478
Iteration 260/1000 | Loss: 0.00001477
Iteration 261/1000 | Loss: 0.00001477
Iteration 262/1000 | Loss: 0.00001477
Iteration 263/1000 | Loss: 0.00001477
Iteration 264/1000 | Loss: 0.00001477
Iteration 265/1000 | Loss: 0.00001477
Iteration 266/1000 | Loss: 0.00001477
Iteration 267/1000 | Loss: 0.00001477
Iteration 268/1000 | Loss: 0.00001477
Iteration 269/1000 | Loss: 0.00001477
Iteration 270/1000 | Loss: 0.00001476
Iteration 271/1000 | Loss: 0.00001476
Iteration 272/1000 | Loss: 0.00001476
Iteration 273/1000 | Loss: 0.00001476
Iteration 274/1000 | Loss: 0.00001476
Iteration 275/1000 | Loss: 0.00001476
Iteration 276/1000 | Loss: 0.00001476
Iteration 277/1000 | Loss: 0.00001476
Iteration 278/1000 | Loss: 0.00001476
Iteration 279/1000 | Loss: 0.00001476
Iteration 280/1000 | Loss: 0.00001476
Iteration 281/1000 | Loss: 0.00001476
Iteration 282/1000 | Loss: 0.00001476
Iteration 283/1000 | Loss: 0.00001476
Iteration 284/1000 | Loss: 0.00001475
Iteration 285/1000 | Loss: 0.00001475
Iteration 286/1000 | Loss: 0.00001475
Iteration 287/1000 | Loss: 0.00001475
Iteration 288/1000 | Loss: 0.00001475
Iteration 289/1000 | Loss: 0.00001475
Iteration 290/1000 | Loss: 0.00001475
Iteration 291/1000 | Loss: 0.00001475
Iteration 292/1000 | Loss: 0.00001475
Iteration 293/1000 | Loss: 0.00001475
Iteration 294/1000 | Loss: 0.00001474
Iteration 295/1000 | Loss: 0.00001474
Iteration 296/1000 | Loss: 0.00001474
Iteration 297/1000 | Loss: 0.00001474
Iteration 298/1000 | Loss: 0.00001474
Iteration 299/1000 | Loss: 0.00001474
Iteration 300/1000 | Loss: 0.00001473
Iteration 301/1000 | Loss: 0.00001473
Iteration 302/1000 | Loss: 0.00001473
Iteration 303/1000 | Loss: 0.00001473
Iteration 304/1000 | Loss: 0.00001473
Iteration 305/1000 | Loss: 0.00001473
Iteration 306/1000 | Loss: 0.00001473
Iteration 307/1000 | Loss: 0.00001473
Iteration 308/1000 | Loss: 0.00001473
Iteration 309/1000 | Loss: 0.00001473
Iteration 310/1000 | Loss: 0.00001473
Iteration 311/1000 | Loss: 0.00001473
Iteration 312/1000 | Loss: 0.00001473
Iteration 313/1000 | Loss: 0.00001473
Iteration 314/1000 | Loss: 0.00001473
Iteration 315/1000 | Loss: 0.00001473
Iteration 316/1000 | Loss: 0.00001473
Iteration 317/1000 | Loss: 0.00001473
Iteration 318/1000 | Loss: 0.00001473
Iteration 319/1000 | Loss: 0.00001473
Iteration 320/1000 | Loss: 0.00001473
Iteration 321/1000 | Loss: 0.00001473
Iteration 322/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [1.4731730516359676e-05, 1.4731730516359676e-05, 1.4731730516359676e-05, 1.4731730516359676e-05, 1.4731730516359676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4731730516359676e-05

Optimization complete. Final v2v error: 3.240846633911133 mm

Highest mean error: 4.5674285888671875 mm for frame 47

Lowest mean error: 2.779383897781372 mm for frame 68

Saving results

Total time: 334.3592338562012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002332
Iteration 2/25 | Loss: 0.00330196
Iteration 3/25 | Loss: 0.00188450
Iteration 4/25 | Loss: 0.00171079
Iteration 5/25 | Loss: 0.00162705
Iteration 6/25 | Loss: 0.00150724
Iteration 7/25 | Loss: 0.00159463
Iteration 8/25 | Loss: 0.00148580
Iteration 9/25 | Loss: 0.00135530
Iteration 10/25 | Loss: 0.00134161
Iteration 11/25 | Loss: 0.00134029
Iteration 12/25 | Loss: 0.00133000
Iteration 13/25 | Loss: 0.00132866
Iteration 14/25 | Loss: 0.00131398
Iteration 15/25 | Loss: 0.00131379
Iteration 16/25 | Loss: 0.00133888
Iteration 17/25 | Loss: 0.00131240
Iteration 18/25 | Loss: 0.00131188
Iteration 19/25 | Loss: 0.00130645
Iteration 20/25 | Loss: 0.00130610
Iteration 21/25 | Loss: 0.00130610
Iteration 22/25 | Loss: 0.00130610
Iteration 23/25 | Loss: 0.00130610
Iteration 24/25 | Loss: 0.00130610
Iteration 25/25 | Loss: 0.00130610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013061037752777338, 0.0013061037752777338, 0.0013061037752777338, 0.0013061037752777338, 0.0013061037752777338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013061037752777338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38458419
Iteration 2/25 | Loss: 0.00083713
Iteration 3/25 | Loss: 0.00077908
Iteration 4/25 | Loss: 0.00077908
Iteration 5/25 | Loss: 0.00077908
Iteration 6/25 | Loss: 0.00077908
Iteration 7/25 | Loss: 0.00077908
Iteration 8/25 | Loss: 0.00077908
Iteration 9/25 | Loss: 0.00077908
Iteration 10/25 | Loss: 0.00077908
Iteration 11/25 | Loss: 0.00077908
Iteration 12/25 | Loss: 0.00077908
Iteration 13/25 | Loss: 0.00077908
Iteration 14/25 | Loss: 0.00077908
Iteration 15/25 | Loss: 0.00077908
Iteration 16/25 | Loss: 0.00077908
Iteration 17/25 | Loss: 0.00077908
Iteration 18/25 | Loss: 0.00077908
Iteration 19/25 | Loss: 0.00077908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007790784002281725, 0.0007790784002281725, 0.0007790784002281725, 0.0007790784002281725, 0.0007790784002281725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007790784002281725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077908
Iteration 2/1000 | Loss: 0.00007782
Iteration 3/1000 | Loss: 0.00003961
Iteration 4/1000 | Loss: 0.00007652
Iteration 5/1000 | Loss: 0.00002689
Iteration 6/1000 | Loss: 0.00011210
Iteration 7/1000 | Loss: 0.00040044
Iteration 8/1000 | Loss: 0.00020754
Iteration 9/1000 | Loss: 0.00026263
Iteration 10/1000 | Loss: 0.00004353
Iteration 11/1000 | Loss: 0.00005467
Iteration 12/1000 | Loss: 0.00002473
Iteration 13/1000 | Loss: 0.00003018
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002653
Iteration 16/1000 | Loss: 0.00002120
Iteration 17/1000 | Loss: 0.00002385
Iteration 18/1000 | Loss: 0.00013197
Iteration 19/1000 | Loss: 0.00039762
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00003744
Iteration 23/1000 | Loss: 0.00002637
Iteration 24/1000 | Loss: 0.00002226
Iteration 25/1000 | Loss: 0.00002692
Iteration 26/1000 | Loss: 0.00002085
Iteration 27/1000 | Loss: 0.00002637
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00003916
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00002226
Iteration 36/1000 | Loss: 0.00005155
Iteration 37/1000 | Loss: 0.00001830
Iteration 38/1000 | Loss: 0.00001994
Iteration 39/1000 | Loss: 0.00002049
Iteration 40/1000 | Loss: 0.00003098
Iteration 41/1000 | Loss: 0.00001816
Iteration 42/1000 | Loss: 0.00002626
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001769
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001769
Iteration 53/1000 | Loss: 0.00001769
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001768
Iteration 57/1000 | Loss: 0.00001769
Iteration 58/1000 | Loss: 0.00001768
Iteration 59/1000 | Loss: 0.00001768
Iteration 60/1000 | Loss: 0.00001768
Iteration 61/1000 | Loss: 0.00001768
Iteration 62/1000 | Loss: 0.00001768
Iteration 63/1000 | Loss: 0.00001768
Iteration 64/1000 | Loss: 0.00001768
Iteration 65/1000 | Loss: 0.00001768
Iteration 66/1000 | Loss: 0.00001768
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001768
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001768
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00002256
Iteration 82/1000 | Loss: 0.00001844
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001788
Iteration 85/1000 | Loss: 0.00002677
Iteration 86/1000 | Loss: 0.00001949
Iteration 87/1000 | Loss: 0.00001764
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001764
Iteration 93/1000 | Loss: 0.00001762
Iteration 94/1000 | Loss: 0.00001761
Iteration 95/1000 | Loss: 0.00001761
Iteration 96/1000 | Loss: 0.00001761
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001777
Iteration 99/1000 | Loss: 0.00001776
Iteration 100/1000 | Loss: 0.00001776
Iteration 101/1000 | Loss: 0.00001776
Iteration 102/1000 | Loss: 0.00001778
Iteration 103/1000 | Loss: 0.00001778
Iteration 104/1000 | Loss: 0.00001778
Iteration 105/1000 | Loss: 0.00001778
Iteration 106/1000 | Loss: 0.00001778
Iteration 107/1000 | Loss: 0.00001777
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00002279
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001773
Iteration 114/1000 | Loss: 0.00001763
Iteration 115/1000 | Loss: 0.00001954
Iteration 116/1000 | Loss: 0.00002163
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001758
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001755
Iteration 124/1000 | Loss: 0.00001755
Iteration 125/1000 | Loss: 0.00001755
Iteration 126/1000 | Loss: 0.00001754
Iteration 127/1000 | Loss: 0.00001754
Iteration 128/1000 | Loss: 0.00001754
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001754
Iteration 131/1000 | Loss: 0.00001754
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.754381628416013e-05, 1.754381628416013e-05, 1.754381628416013e-05, 1.754381628416013e-05, 1.754381628416013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.754381628416013e-05

Optimization complete. Final v2v error: 3.554807186126709 mm

Highest mean error: 4.1012067794799805 mm for frame 202

Lowest mean error: 3.325627326965332 mm for frame 60

Saving results

Total time: 125.15726900100708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792571
Iteration 2/25 | Loss: 0.00163553
Iteration 3/25 | Loss: 0.00146559
Iteration 4/25 | Loss: 0.00143967
Iteration 5/25 | Loss: 0.00143083
Iteration 6/25 | Loss: 0.00142679
Iteration 7/25 | Loss: 0.00142529
Iteration 8/25 | Loss: 0.00142500
Iteration 9/25 | Loss: 0.00142493
Iteration 10/25 | Loss: 0.00142493
Iteration 11/25 | Loss: 0.00142492
Iteration 12/25 | Loss: 0.00142492
Iteration 13/25 | Loss: 0.00142492
Iteration 14/25 | Loss: 0.00142492
Iteration 15/25 | Loss: 0.00142491
Iteration 16/25 | Loss: 0.00142491
Iteration 17/25 | Loss: 0.00142491
Iteration 18/25 | Loss: 0.00142491
Iteration 19/25 | Loss: 0.00142491
Iteration 20/25 | Loss: 0.00142491
Iteration 21/25 | Loss: 0.00142491
Iteration 22/25 | Loss: 0.00142491
Iteration 23/25 | Loss: 0.00142491
Iteration 24/25 | Loss: 0.00142491
Iteration 25/25 | Loss: 0.00142490

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50480604
Iteration 2/25 | Loss: 0.00118975
Iteration 3/25 | Loss: 0.00118975
Iteration 4/25 | Loss: 0.00118975
Iteration 5/25 | Loss: 0.00118975
Iteration 6/25 | Loss: 0.00118975
Iteration 7/25 | Loss: 0.00118975
Iteration 8/25 | Loss: 0.00118975
Iteration 9/25 | Loss: 0.00118975
Iteration 10/25 | Loss: 0.00118975
Iteration 11/25 | Loss: 0.00118975
Iteration 12/25 | Loss: 0.00118975
Iteration 13/25 | Loss: 0.00118975
Iteration 14/25 | Loss: 0.00118975
Iteration 15/25 | Loss: 0.00118975
Iteration 16/25 | Loss: 0.00118975
Iteration 17/25 | Loss: 0.00118975
Iteration 18/25 | Loss: 0.00118975
Iteration 19/25 | Loss: 0.00118975
Iteration 20/25 | Loss: 0.00118975
Iteration 21/25 | Loss: 0.00118975
Iteration 22/25 | Loss: 0.00118975
Iteration 23/25 | Loss: 0.00118975
Iteration 24/25 | Loss: 0.00118975
Iteration 25/25 | Loss: 0.00118975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118975
Iteration 2/1000 | Loss: 0.00007730
Iteration 3/1000 | Loss: 0.00003964
Iteration 4/1000 | Loss: 0.00003139
Iteration 5/1000 | Loss: 0.00002912
Iteration 6/1000 | Loss: 0.00002751
Iteration 7/1000 | Loss: 0.00002669
Iteration 8/1000 | Loss: 0.00002580
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002490
Iteration 11/1000 | Loss: 0.00002449
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00002411
Iteration 14/1000 | Loss: 0.00002407
Iteration 15/1000 | Loss: 0.00002401
Iteration 16/1000 | Loss: 0.00002387
Iteration 17/1000 | Loss: 0.00002382
Iteration 18/1000 | Loss: 0.00002378
Iteration 19/1000 | Loss: 0.00002377
Iteration 20/1000 | Loss: 0.00002376
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002375
Iteration 23/1000 | Loss: 0.00002374
Iteration 24/1000 | Loss: 0.00002374
Iteration 25/1000 | Loss: 0.00002373
Iteration 26/1000 | Loss: 0.00002373
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002371
Iteration 30/1000 | Loss: 0.00002371
Iteration 31/1000 | Loss: 0.00002371
Iteration 32/1000 | Loss: 0.00002371
Iteration 33/1000 | Loss: 0.00002371
Iteration 34/1000 | Loss: 0.00002371
Iteration 35/1000 | Loss: 0.00002371
Iteration 36/1000 | Loss: 0.00002371
Iteration 37/1000 | Loss: 0.00002370
Iteration 38/1000 | Loss: 0.00002370
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002368
Iteration 41/1000 | Loss: 0.00002368
Iteration 42/1000 | Loss: 0.00002367
Iteration 43/1000 | Loss: 0.00002367
Iteration 44/1000 | Loss: 0.00002367
Iteration 45/1000 | Loss: 0.00002366
Iteration 46/1000 | Loss: 0.00002366
Iteration 47/1000 | Loss: 0.00002366
Iteration 48/1000 | Loss: 0.00002365
Iteration 49/1000 | Loss: 0.00002365
Iteration 50/1000 | Loss: 0.00002364
Iteration 51/1000 | Loss: 0.00002364
Iteration 52/1000 | Loss: 0.00002364
Iteration 53/1000 | Loss: 0.00002364
Iteration 54/1000 | Loss: 0.00002364
Iteration 55/1000 | Loss: 0.00002364
Iteration 56/1000 | Loss: 0.00002364
Iteration 57/1000 | Loss: 0.00002364
Iteration 58/1000 | Loss: 0.00002364
Iteration 59/1000 | Loss: 0.00002363
Iteration 60/1000 | Loss: 0.00002363
Iteration 61/1000 | Loss: 0.00002363
Iteration 62/1000 | Loss: 0.00002363
Iteration 63/1000 | Loss: 0.00002363
Iteration 64/1000 | Loss: 0.00002362
Iteration 65/1000 | Loss: 0.00002362
Iteration 66/1000 | Loss: 0.00002362
Iteration 67/1000 | Loss: 0.00002362
Iteration 68/1000 | Loss: 0.00002362
Iteration 69/1000 | Loss: 0.00002361
Iteration 70/1000 | Loss: 0.00002361
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002360
Iteration 73/1000 | Loss: 0.00002360
Iteration 74/1000 | Loss: 0.00002360
Iteration 75/1000 | Loss: 0.00002360
Iteration 76/1000 | Loss: 0.00002359
Iteration 77/1000 | Loss: 0.00002359
Iteration 78/1000 | Loss: 0.00002359
Iteration 79/1000 | Loss: 0.00002359
Iteration 80/1000 | Loss: 0.00002359
Iteration 81/1000 | Loss: 0.00002359
Iteration 82/1000 | Loss: 0.00002359
Iteration 83/1000 | Loss: 0.00002359
Iteration 84/1000 | Loss: 0.00002359
Iteration 85/1000 | Loss: 0.00002358
Iteration 86/1000 | Loss: 0.00002358
Iteration 87/1000 | Loss: 0.00002358
Iteration 88/1000 | Loss: 0.00002358
Iteration 89/1000 | Loss: 0.00002357
Iteration 90/1000 | Loss: 0.00002357
Iteration 91/1000 | Loss: 0.00002357
Iteration 92/1000 | Loss: 0.00002357
Iteration 93/1000 | Loss: 0.00002357
Iteration 94/1000 | Loss: 0.00002357
Iteration 95/1000 | Loss: 0.00002357
Iteration 96/1000 | Loss: 0.00002357
Iteration 97/1000 | Loss: 0.00002357
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002356
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002356
Iteration 108/1000 | Loss: 0.00002356
Iteration 109/1000 | Loss: 0.00002356
Iteration 110/1000 | Loss: 0.00002356
Iteration 111/1000 | Loss: 0.00002355
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002355
Iteration 114/1000 | Loss: 0.00002355
Iteration 115/1000 | Loss: 0.00002355
Iteration 116/1000 | Loss: 0.00002355
Iteration 117/1000 | Loss: 0.00002355
Iteration 118/1000 | Loss: 0.00002355
Iteration 119/1000 | Loss: 0.00002355
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002355
Iteration 122/1000 | Loss: 0.00002355
Iteration 123/1000 | Loss: 0.00002355
Iteration 124/1000 | Loss: 0.00002355
Iteration 125/1000 | Loss: 0.00002354
Iteration 126/1000 | Loss: 0.00002354
Iteration 127/1000 | Loss: 0.00002354
Iteration 128/1000 | Loss: 0.00002354
Iteration 129/1000 | Loss: 0.00002354
Iteration 130/1000 | Loss: 0.00002354
Iteration 131/1000 | Loss: 0.00002354
Iteration 132/1000 | Loss: 0.00002353
Iteration 133/1000 | Loss: 0.00002353
Iteration 134/1000 | Loss: 0.00002353
Iteration 135/1000 | Loss: 0.00002353
Iteration 136/1000 | Loss: 0.00002353
Iteration 137/1000 | Loss: 0.00002353
Iteration 138/1000 | Loss: 0.00002353
Iteration 139/1000 | Loss: 0.00002352
Iteration 140/1000 | Loss: 0.00002352
Iteration 141/1000 | Loss: 0.00002352
Iteration 142/1000 | Loss: 0.00002352
Iteration 143/1000 | Loss: 0.00002351
Iteration 144/1000 | Loss: 0.00002351
Iteration 145/1000 | Loss: 0.00002351
Iteration 146/1000 | Loss: 0.00002351
Iteration 147/1000 | Loss: 0.00002351
Iteration 148/1000 | Loss: 0.00002351
Iteration 149/1000 | Loss: 0.00002351
Iteration 150/1000 | Loss: 0.00002351
Iteration 151/1000 | Loss: 0.00002351
Iteration 152/1000 | Loss: 0.00002351
Iteration 153/1000 | Loss: 0.00002351
Iteration 154/1000 | Loss: 0.00002351
Iteration 155/1000 | Loss: 0.00002351
Iteration 156/1000 | Loss: 0.00002351
Iteration 157/1000 | Loss: 0.00002350
Iteration 158/1000 | Loss: 0.00002350
Iteration 159/1000 | Loss: 0.00002350
Iteration 160/1000 | Loss: 0.00002350
Iteration 161/1000 | Loss: 0.00002350
Iteration 162/1000 | Loss: 0.00002350
Iteration 163/1000 | Loss: 0.00002350
Iteration 164/1000 | Loss: 0.00002350
Iteration 165/1000 | Loss: 0.00002350
Iteration 166/1000 | Loss: 0.00002350
Iteration 167/1000 | Loss: 0.00002350
Iteration 168/1000 | Loss: 0.00002350
Iteration 169/1000 | Loss: 0.00002349
Iteration 170/1000 | Loss: 0.00002349
Iteration 171/1000 | Loss: 0.00002349
Iteration 172/1000 | Loss: 0.00002349
Iteration 173/1000 | Loss: 0.00002349
Iteration 174/1000 | Loss: 0.00002349
Iteration 175/1000 | Loss: 0.00002349
Iteration 176/1000 | Loss: 0.00002349
Iteration 177/1000 | Loss: 0.00002349
Iteration 178/1000 | Loss: 0.00002349
Iteration 179/1000 | Loss: 0.00002349
Iteration 180/1000 | Loss: 0.00002349
Iteration 181/1000 | Loss: 0.00002349
Iteration 182/1000 | Loss: 0.00002349
Iteration 183/1000 | Loss: 0.00002349
Iteration 184/1000 | Loss: 0.00002349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.3491498723160475e-05, 2.3491498723160475e-05, 2.3491498723160475e-05, 2.3491498723160475e-05, 2.3491498723160475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3491498723160475e-05

Optimization complete. Final v2v error: 4.092584133148193 mm

Highest mean error: 4.305500507354736 mm for frame 215

Lowest mean error: 3.836122512817383 mm for frame 116

Saving results

Total time: 53.33086609840393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040785
Iteration 2/25 | Loss: 0.00500818
Iteration 3/25 | Loss: 0.00365986
Iteration 4/25 | Loss: 0.00285395
Iteration 5/25 | Loss: 0.00246821
Iteration 6/25 | Loss: 0.00235844
Iteration 7/25 | Loss: 0.00241686
Iteration 8/25 | Loss: 0.00220908
Iteration 9/25 | Loss: 0.00212416
Iteration 10/25 | Loss: 0.00196478
Iteration 11/25 | Loss: 0.00184849
Iteration 12/25 | Loss: 0.00180563
Iteration 13/25 | Loss: 0.00175141
Iteration 14/25 | Loss: 0.00171477
Iteration 15/25 | Loss: 0.00170020
Iteration 16/25 | Loss: 0.00169733
Iteration 17/25 | Loss: 0.00168538
Iteration 18/25 | Loss: 0.00168303
Iteration 19/25 | Loss: 0.00168209
Iteration 20/25 | Loss: 0.00168873
Iteration 21/25 | Loss: 0.00168152
Iteration 22/25 | Loss: 0.00167824
Iteration 23/25 | Loss: 0.00167761
Iteration 24/25 | Loss: 0.00167629
Iteration 25/25 | Loss: 0.00167639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68465060
Iteration 2/25 | Loss: 0.00203665
Iteration 3/25 | Loss: 0.00203665
Iteration 4/25 | Loss: 0.00203664
Iteration 5/25 | Loss: 0.00203664
Iteration 6/25 | Loss: 0.00203664
Iteration 7/25 | Loss: 0.00203664
Iteration 8/25 | Loss: 0.00203664
Iteration 9/25 | Loss: 0.00203664
Iteration 10/25 | Loss: 0.00203664
Iteration 11/25 | Loss: 0.00203664
Iteration 12/25 | Loss: 0.00203664
Iteration 13/25 | Loss: 0.00203664
Iteration 14/25 | Loss: 0.00203664
Iteration 15/25 | Loss: 0.00203664
Iteration 16/25 | Loss: 0.00203664
Iteration 17/25 | Loss: 0.00203664
Iteration 18/25 | Loss: 0.00203664
Iteration 19/25 | Loss: 0.00203664
Iteration 20/25 | Loss: 0.00203664
Iteration 21/25 | Loss: 0.00203664
Iteration 22/25 | Loss: 0.00203664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0020366415847092867, 0.0020366415847092867, 0.0020366415847092867, 0.0020366415847092867, 0.0020366415847092867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020366415847092867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203664
Iteration 2/1000 | Loss: 0.00197054
Iteration 3/1000 | Loss: 0.00127328
Iteration 4/1000 | Loss: 0.00105277
Iteration 5/1000 | Loss: 0.00117198
Iteration 6/1000 | Loss: 0.00047584
Iteration 7/1000 | Loss: 0.00034402
Iteration 8/1000 | Loss: 0.00040324
Iteration 9/1000 | Loss: 0.00039900
Iteration 10/1000 | Loss: 0.00024978
Iteration 11/1000 | Loss: 0.00035763
Iteration 12/1000 | Loss: 0.00021829
Iteration 13/1000 | Loss: 0.00024212
Iteration 14/1000 | Loss: 0.00027653
Iteration 15/1000 | Loss: 0.00020776
Iteration 16/1000 | Loss: 0.00038218
Iteration 17/1000 | Loss: 0.00013518
Iteration 18/1000 | Loss: 0.00016415
Iteration 19/1000 | Loss: 0.00043727
Iteration 20/1000 | Loss: 0.00073034
Iteration 21/1000 | Loss: 0.00013087
Iteration 22/1000 | Loss: 0.00042510
Iteration 23/1000 | Loss: 0.00008388
Iteration 24/1000 | Loss: 0.00011724
Iteration 25/1000 | Loss: 0.00019207
Iteration 26/1000 | Loss: 0.00010367
Iteration 27/1000 | Loss: 0.00012697
Iteration 28/1000 | Loss: 0.00021677
Iteration 29/1000 | Loss: 0.00012826
Iteration 30/1000 | Loss: 0.00006936
Iteration 31/1000 | Loss: 0.00005750
Iteration 32/1000 | Loss: 0.00020938
Iteration 33/1000 | Loss: 0.00056121
Iteration 34/1000 | Loss: 0.00021732
Iteration 35/1000 | Loss: 0.00005884
Iteration 36/1000 | Loss: 0.00005018
Iteration 37/1000 | Loss: 0.00004737
Iteration 38/1000 | Loss: 0.00004553
Iteration 39/1000 | Loss: 0.00004218
Iteration 40/1000 | Loss: 0.00004118
Iteration 41/1000 | Loss: 0.00010471
Iteration 42/1000 | Loss: 0.00013258
Iteration 43/1000 | Loss: 0.00006512
Iteration 44/1000 | Loss: 0.00004629
Iteration 45/1000 | Loss: 0.00004158
Iteration 46/1000 | Loss: 0.00003940
Iteration 47/1000 | Loss: 0.00003923
Iteration 48/1000 | Loss: 0.00003742
Iteration 49/1000 | Loss: 0.00012650
Iteration 50/1000 | Loss: 0.00013743
Iteration 51/1000 | Loss: 0.00004472
Iteration 52/1000 | Loss: 0.00027678
Iteration 53/1000 | Loss: 0.00016135
Iteration 54/1000 | Loss: 0.00005309
Iteration 55/1000 | Loss: 0.00004239
Iteration 56/1000 | Loss: 0.00003604
Iteration 57/1000 | Loss: 0.00003383
Iteration 58/1000 | Loss: 0.00013429
Iteration 59/1000 | Loss: 0.00003698
Iteration 60/1000 | Loss: 0.00003398
Iteration 61/1000 | Loss: 0.00003169
Iteration 62/1000 | Loss: 0.00003081
Iteration 63/1000 | Loss: 0.00002990
Iteration 64/1000 | Loss: 0.00003031
Iteration 65/1000 | Loss: 0.00002979
Iteration 66/1000 | Loss: 0.00002998
Iteration 67/1000 | Loss: 0.00002947
Iteration 68/1000 | Loss: 0.00002946
Iteration 69/1000 | Loss: 0.00002878
Iteration 70/1000 | Loss: 0.00002610
Iteration 71/1000 | Loss: 0.00002748
Iteration 72/1000 | Loss: 0.00002820
Iteration 73/1000 | Loss: 0.00002875
Iteration 74/1000 | Loss: 0.00002797
Iteration 75/1000 | Loss: 0.00002784
Iteration 76/1000 | Loss: 0.00002925
Iteration 77/1000 | Loss: 0.00002717
Iteration 78/1000 | Loss: 0.00002699
Iteration 79/1000 | Loss: 0.00002828
Iteration 80/1000 | Loss: 0.00002813
Iteration 81/1000 | Loss: 0.00002863
Iteration 82/1000 | Loss: 0.00002810
Iteration 83/1000 | Loss: 0.00003250
Iteration 84/1000 | Loss: 0.00002931
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002947
Iteration 87/1000 | Loss: 0.00002793
Iteration 88/1000 | Loss: 0.00002659
Iteration 89/1000 | Loss: 0.00002759
Iteration 90/1000 | Loss: 0.00002872
Iteration 91/1000 | Loss: 0.00002890
Iteration 92/1000 | Loss: 0.00002878
Iteration 93/1000 | Loss: 0.00002882
Iteration 94/1000 | Loss: 0.00002811
Iteration 95/1000 | Loss: 0.00002824
Iteration 96/1000 | Loss: 0.00002824
Iteration 97/1000 | Loss: 0.00003154
Iteration 98/1000 | Loss: 0.00002776
Iteration 99/1000 | Loss: 0.00002656
Iteration 100/1000 | Loss: 0.00002608
Iteration 101/1000 | Loss: 0.00002544
Iteration 102/1000 | Loss: 0.00002516
Iteration 103/1000 | Loss: 0.00002509
Iteration 104/1000 | Loss: 0.00002491
Iteration 105/1000 | Loss: 0.00002469
Iteration 106/1000 | Loss: 0.00002463
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002461
Iteration 109/1000 | Loss: 0.00002461
Iteration 110/1000 | Loss: 0.00002460
Iteration 111/1000 | Loss: 0.00002460
Iteration 112/1000 | Loss: 0.00002460
Iteration 113/1000 | Loss: 0.00002460
Iteration 114/1000 | Loss: 0.00002460
Iteration 115/1000 | Loss: 0.00002460
Iteration 116/1000 | Loss: 0.00002460
Iteration 117/1000 | Loss: 0.00002458
Iteration 118/1000 | Loss: 0.00002456
Iteration 119/1000 | Loss: 0.00002456
Iteration 120/1000 | Loss: 0.00002455
Iteration 121/1000 | Loss: 0.00002454
Iteration 122/1000 | Loss: 0.00002454
Iteration 123/1000 | Loss: 0.00002453
Iteration 124/1000 | Loss: 0.00002453
Iteration 125/1000 | Loss: 0.00002453
Iteration 126/1000 | Loss: 0.00002453
Iteration 127/1000 | Loss: 0.00002453
Iteration 128/1000 | Loss: 0.00002453
Iteration 129/1000 | Loss: 0.00002453
Iteration 130/1000 | Loss: 0.00002453
Iteration 131/1000 | Loss: 0.00002452
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002452
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002452
Iteration 140/1000 | Loss: 0.00002452
Iteration 141/1000 | Loss: 0.00002452
Iteration 142/1000 | Loss: 0.00002452
Iteration 143/1000 | Loss: 0.00002452
Iteration 144/1000 | Loss: 0.00002452
Iteration 145/1000 | Loss: 0.00002452
Iteration 146/1000 | Loss: 0.00002452
Iteration 147/1000 | Loss: 0.00002452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.4523007596144453e-05, 2.4523007596144453e-05, 2.4523007596144453e-05, 2.4523007596144453e-05, 2.4523007596144453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4523007596144453e-05

Optimization complete. Final v2v error: 4.069370746612549 mm

Highest mean error: 5.057249546051025 mm for frame 16

Lowest mean error: 3.824139356613159 mm for frame 86

Saving results

Total time: 220.40945076942444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477867
Iteration 2/25 | Loss: 0.00147118
Iteration 3/25 | Loss: 0.00135734
Iteration 4/25 | Loss: 0.00134130
Iteration 5/25 | Loss: 0.00133627
Iteration 6/25 | Loss: 0.00133530
Iteration 7/25 | Loss: 0.00133530
Iteration 8/25 | Loss: 0.00133530
Iteration 9/25 | Loss: 0.00133530
Iteration 10/25 | Loss: 0.00133530
Iteration 11/25 | Loss: 0.00133530
Iteration 12/25 | Loss: 0.00133530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013353029498830438, 0.0013353029498830438, 0.0013353029498830438, 0.0013353029498830438, 0.0013353029498830438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013353029498830438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26808786
Iteration 2/25 | Loss: 0.00101357
Iteration 3/25 | Loss: 0.00101356
Iteration 4/25 | Loss: 0.00101356
Iteration 5/25 | Loss: 0.00101356
Iteration 6/25 | Loss: 0.00101356
Iteration 7/25 | Loss: 0.00101356
Iteration 8/25 | Loss: 0.00101356
Iteration 9/25 | Loss: 0.00101356
Iteration 10/25 | Loss: 0.00101356
Iteration 11/25 | Loss: 0.00101356
Iteration 12/25 | Loss: 0.00101356
Iteration 13/25 | Loss: 0.00101356
Iteration 14/25 | Loss: 0.00101356
Iteration 15/25 | Loss: 0.00101356
Iteration 16/25 | Loss: 0.00101356
Iteration 17/25 | Loss: 0.00101356
Iteration 18/25 | Loss: 0.00101356
Iteration 19/25 | Loss: 0.00101356
Iteration 20/25 | Loss: 0.00101356
Iteration 21/25 | Loss: 0.00101356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001013559172861278, 0.001013559172861278, 0.001013559172861278, 0.001013559172861278, 0.001013559172861278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001013559172861278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101356
Iteration 2/1000 | Loss: 0.00004202
Iteration 3/1000 | Loss: 0.00003013
Iteration 4/1000 | Loss: 0.00002671
Iteration 5/1000 | Loss: 0.00002514
Iteration 6/1000 | Loss: 0.00002400
Iteration 7/1000 | Loss: 0.00002319
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002178
Iteration 11/1000 | Loss: 0.00002146
Iteration 12/1000 | Loss: 0.00002125
Iteration 13/1000 | Loss: 0.00002106
Iteration 14/1000 | Loss: 0.00002104
Iteration 15/1000 | Loss: 0.00002098
Iteration 16/1000 | Loss: 0.00002093
Iteration 17/1000 | Loss: 0.00002092
Iteration 18/1000 | Loss: 0.00002092
Iteration 19/1000 | Loss: 0.00002091
Iteration 20/1000 | Loss: 0.00002091
Iteration 21/1000 | Loss: 0.00002090
Iteration 22/1000 | Loss: 0.00002088
Iteration 23/1000 | Loss: 0.00002087
Iteration 24/1000 | Loss: 0.00002084
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002083
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002080
Iteration 29/1000 | Loss: 0.00002079
Iteration 30/1000 | Loss: 0.00002072
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002068
Iteration 37/1000 | Loss: 0.00002067
Iteration 38/1000 | Loss: 0.00002067
Iteration 39/1000 | Loss: 0.00002067
Iteration 40/1000 | Loss: 0.00002066
Iteration 41/1000 | Loss: 0.00002066
Iteration 42/1000 | Loss: 0.00002065
Iteration 43/1000 | Loss: 0.00002065
Iteration 44/1000 | Loss: 0.00002064
Iteration 45/1000 | Loss: 0.00002064
Iteration 46/1000 | Loss: 0.00002064
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002063
Iteration 49/1000 | Loss: 0.00002062
Iteration 50/1000 | Loss: 0.00002062
Iteration 51/1000 | Loss: 0.00002062
Iteration 52/1000 | Loss: 0.00002061
Iteration 53/1000 | Loss: 0.00002061
Iteration 54/1000 | Loss: 0.00002060
Iteration 55/1000 | Loss: 0.00002060
Iteration 56/1000 | Loss: 0.00002060
Iteration 57/1000 | Loss: 0.00002059
Iteration 58/1000 | Loss: 0.00002059
Iteration 59/1000 | Loss: 0.00002059
Iteration 60/1000 | Loss: 0.00002059
Iteration 61/1000 | Loss: 0.00002059
Iteration 62/1000 | Loss: 0.00002059
Iteration 63/1000 | Loss: 0.00002058
Iteration 64/1000 | Loss: 0.00002058
Iteration 65/1000 | Loss: 0.00002058
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002056
Iteration 69/1000 | Loss: 0.00002056
Iteration 70/1000 | Loss: 0.00002056
Iteration 71/1000 | Loss: 0.00002055
Iteration 72/1000 | Loss: 0.00002055
Iteration 73/1000 | Loss: 0.00002055
Iteration 74/1000 | Loss: 0.00002055
Iteration 75/1000 | Loss: 0.00002055
Iteration 76/1000 | Loss: 0.00002054
Iteration 77/1000 | Loss: 0.00002054
Iteration 78/1000 | Loss: 0.00002054
Iteration 79/1000 | Loss: 0.00002053
Iteration 80/1000 | Loss: 0.00002053
Iteration 81/1000 | Loss: 0.00002053
Iteration 82/1000 | Loss: 0.00002052
Iteration 83/1000 | Loss: 0.00002052
Iteration 84/1000 | Loss: 0.00002052
Iteration 85/1000 | Loss: 0.00002051
Iteration 86/1000 | Loss: 0.00002051
Iteration 87/1000 | Loss: 0.00002051
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002049
Iteration 91/1000 | Loss: 0.00002049
Iteration 92/1000 | Loss: 0.00002049
Iteration 93/1000 | Loss: 0.00002049
Iteration 94/1000 | Loss: 0.00002049
Iteration 95/1000 | Loss: 0.00002049
Iteration 96/1000 | Loss: 0.00002048
Iteration 97/1000 | Loss: 0.00002048
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002047
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002047
Iteration 106/1000 | Loss: 0.00002047
Iteration 107/1000 | Loss: 0.00002047
Iteration 108/1000 | Loss: 0.00002047
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002045
Iteration 114/1000 | Loss: 0.00002045
Iteration 115/1000 | Loss: 0.00002045
Iteration 116/1000 | Loss: 0.00002045
Iteration 117/1000 | Loss: 0.00002045
Iteration 118/1000 | Loss: 0.00002045
Iteration 119/1000 | Loss: 0.00002044
Iteration 120/1000 | Loss: 0.00002044
Iteration 121/1000 | Loss: 0.00002044
Iteration 122/1000 | Loss: 0.00002044
Iteration 123/1000 | Loss: 0.00002044
Iteration 124/1000 | Loss: 0.00002044
Iteration 125/1000 | Loss: 0.00002044
Iteration 126/1000 | Loss: 0.00002044
Iteration 127/1000 | Loss: 0.00002044
Iteration 128/1000 | Loss: 0.00002044
Iteration 129/1000 | Loss: 0.00002044
Iteration 130/1000 | Loss: 0.00002044
Iteration 131/1000 | Loss: 0.00002044
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.04362440854311e-05, 2.04362440854311e-05, 2.04362440854311e-05, 2.04362440854311e-05, 2.04362440854311e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.04362440854311e-05

Optimization complete. Final v2v error: 3.7705302238464355 mm

Highest mean error: 5.113348484039307 mm for frame 66

Lowest mean error: 3.0968499183654785 mm for frame 78

Saving results

Total time: 44.93982148170471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795313
Iteration 2/25 | Loss: 0.00142199
Iteration 3/25 | Loss: 0.00130402
Iteration 4/25 | Loss: 0.00128895
Iteration 5/25 | Loss: 0.00128473
Iteration 6/25 | Loss: 0.00128387
Iteration 7/25 | Loss: 0.00128387
Iteration 8/25 | Loss: 0.00128387
Iteration 9/25 | Loss: 0.00128387
Iteration 10/25 | Loss: 0.00128387
Iteration 11/25 | Loss: 0.00128387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012838703114539385, 0.0012838703114539385, 0.0012838703114539385, 0.0012838703114539385, 0.0012838703114539385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012838703114539385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37026191
Iteration 2/25 | Loss: 0.00070551
Iteration 3/25 | Loss: 0.00070549
Iteration 4/25 | Loss: 0.00070549
Iteration 5/25 | Loss: 0.00070549
Iteration 6/25 | Loss: 0.00070549
Iteration 7/25 | Loss: 0.00070549
Iteration 8/25 | Loss: 0.00070549
Iteration 9/25 | Loss: 0.00070549
Iteration 10/25 | Loss: 0.00070549
Iteration 11/25 | Loss: 0.00070549
Iteration 12/25 | Loss: 0.00070549
Iteration 13/25 | Loss: 0.00070549
Iteration 14/25 | Loss: 0.00070549
Iteration 15/25 | Loss: 0.00070549
Iteration 16/25 | Loss: 0.00070549
Iteration 17/25 | Loss: 0.00070549
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007054884335957468, 0.0007054884335957468, 0.0007054884335957468, 0.0007054884335957468, 0.0007054884335957468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007054884335957468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070549
Iteration 2/1000 | Loss: 0.00004564
Iteration 3/1000 | Loss: 0.00003258
Iteration 4/1000 | Loss: 0.00002792
Iteration 5/1000 | Loss: 0.00002675
Iteration 6/1000 | Loss: 0.00002572
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00002430
Iteration 9/1000 | Loss: 0.00002377
Iteration 10/1000 | Loss: 0.00002347
Iteration 11/1000 | Loss: 0.00002314
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002278
Iteration 14/1000 | Loss: 0.00002269
Iteration 15/1000 | Loss: 0.00002257
Iteration 16/1000 | Loss: 0.00002256
Iteration 17/1000 | Loss: 0.00002255
Iteration 18/1000 | Loss: 0.00002240
Iteration 19/1000 | Loss: 0.00002235
Iteration 20/1000 | Loss: 0.00002232
Iteration 21/1000 | Loss: 0.00002231
Iteration 22/1000 | Loss: 0.00002226
Iteration 23/1000 | Loss: 0.00002226
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00002223
Iteration 26/1000 | Loss: 0.00002222
Iteration 27/1000 | Loss: 0.00002222
Iteration 28/1000 | Loss: 0.00002222
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002221
Iteration 31/1000 | Loss: 0.00002220
Iteration 32/1000 | Loss: 0.00002220
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002219
Iteration 35/1000 | Loss: 0.00002219
Iteration 36/1000 | Loss: 0.00002219
Iteration 37/1000 | Loss: 0.00002219
Iteration 38/1000 | Loss: 0.00002219
Iteration 39/1000 | Loss: 0.00002218
Iteration 40/1000 | Loss: 0.00002218
Iteration 41/1000 | Loss: 0.00002218
Iteration 42/1000 | Loss: 0.00002217
Iteration 43/1000 | Loss: 0.00002217
Iteration 44/1000 | Loss: 0.00002217
Iteration 45/1000 | Loss: 0.00002217
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002216
Iteration 48/1000 | Loss: 0.00002216
Iteration 49/1000 | Loss: 0.00002216
Iteration 50/1000 | Loss: 0.00002215
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002214
Iteration 55/1000 | Loss: 0.00002214
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002213
Iteration 59/1000 | Loss: 0.00002212
Iteration 60/1000 | Loss: 0.00002212
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002210
Iteration 66/1000 | Loss: 0.00002210
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002209
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002208
Iteration 73/1000 | Loss: 0.00002208
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002206
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002205
Iteration 81/1000 | Loss: 0.00002205
Iteration 82/1000 | Loss: 0.00002205
Iteration 83/1000 | Loss: 0.00002205
Iteration 84/1000 | Loss: 0.00002205
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002204
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002203
Iteration 91/1000 | Loss: 0.00002203
Iteration 92/1000 | Loss: 0.00002203
Iteration 93/1000 | Loss: 0.00002203
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002203
Iteration 96/1000 | Loss: 0.00002203
Iteration 97/1000 | Loss: 0.00002203
Iteration 98/1000 | Loss: 0.00002203
Iteration 99/1000 | Loss: 0.00002203
Iteration 100/1000 | Loss: 0.00002203
Iteration 101/1000 | Loss: 0.00002203
Iteration 102/1000 | Loss: 0.00002202
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002202
Iteration 108/1000 | Loss: 0.00002201
Iteration 109/1000 | Loss: 0.00002201
Iteration 110/1000 | Loss: 0.00002201
Iteration 111/1000 | Loss: 0.00002201
Iteration 112/1000 | Loss: 0.00002201
Iteration 113/1000 | Loss: 0.00002201
Iteration 114/1000 | Loss: 0.00002201
Iteration 115/1000 | Loss: 0.00002201
Iteration 116/1000 | Loss: 0.00002201
Iteration 117/1000 | Loss: 0.00002200
Iteration 118/1000 | Loss: 0.00002200
Iteration 119/1000 | Loss: 0.00002200
Iteration 120/1000 | Loss: 0.00002200
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002200
Iteration 124/1000 | Loss: 0.00002200
Iteration 125/1000 | Loss: 0.00002200
Iteration 126/1000 | Loss: 0.00002200
Iteration 127/1000 | Loss: 0.00002199
Iteration 128/1000 | Loss: 0.00002199
Iteration 129/1000 | Loss: 0.00002199
Iteration 130/1000 | Loss: 0.00002199
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002199
Iteration 133/1000 | Loss: 0.00002199
Iteration 134/1000 | Loss: 0.00002198
Iteration 135/1000 | Loss: 0.00002198
Iteration 136/1000 | Loss: 0.00002198
Iteration 137/1000 | Loss: 0.00002198
Iteration 138/1000 | Loss: 0.00002198
Iteration 139/1000 | Loss: 0.00002198
Iteration 140/1000 | Loss: 0.00002198
Iteration 141/1000 | Loss: 0.00002197
Iteration 142/1000 | Loss: 0.00002197
Iteration 143/1000 | Loss: 0.00002197
Iteration 144/1000 | Loss: 0.00002197
Iteration 145/1000 | Loss: 0.00002197
Iteration 146/1000 | Loss: 0.00002196
Iteration 147/1000 | Loss: 0.00002196
Iteration 148/1000 | Loss: 0.00002196
Iteration 149/1000 | Loss: 0.00002196
Iteration 150/1000 | Loss: 0.00002196
Iteration 151/1000 | Loss: 0.00002196
Iteration 152/1000 | Loss: 0.00002196
Iteration 153/1000 | Loss: 0.00002196
Iteration 154/1000 | Loss: 0.00002195
Iteration 155/1000 | Loss: 0.00002195
Iteration 156/1000 | Loss: 0.00002195
Iteration 157/1000 | Loss: 0.00002195
Iteration 158/1000 | Loss: 0.00002195
Iteration 159/1000 | Loss: 0.00002195
Iteration 160/1000 | Loss: 0.00002195
Iteration 161/1000 | Loss: 0.00002195
Iteration 162/1000 | Loss: 0.00002195
Iteration 163/1000 | Loss: 0.00002194
Iteration 164/1000 | Loss: 0.00002194
Iteration 165/1000 | Loss: 0.00002194
Iteration 166/1000 | Loss: 0.00002194
Iteration 167/1000 | Loss: 0.00002193
Iteration 168/1000 | Loss: 0.00002193
Iteration 169/1000 | Loss: 0.00002193
Iteration 170/1000 | Loss: 0.00002193
Iteration 171/1000 | Loss: 0.00002193
Iteration 172/1000 | Loss: 0.00002193
Iteration 173/1000 | Loss: 0.00002193
Iteration 174/1000 | Loss: 0.00002193
Iteration 175/1000 | Loss: 0.00002192
Iteration 176/1000 | Loss: 0.00002192
Iteration 177/1000 | Loss: 0.00002192
Iteration 178/1000 | Loss: 0.00002192
Iteration 179/1000 | Loss: 0.00002192
Iteration 180/1000 | Loss: 0.00002192
Iteration 181/1000 | Loss: 0.00002192
Iteration 182/1000 | Loss: 0.00002192
Iteration 183/1000 | Loss: 0.00002192
Iteration 184/1000 | Loss: 0.00002192
Iteration 185/1000 | Loss: 0.00002192
Iteration 186/1000 | Loss: 0.00002192
Iteration 187/1000 | Loss: 0.00002192
Iteration 188/1000 | Loss: 0.00002192
Iteration 189/1000 | Loss: 0.00002192
Iteration 190/1000 | Loss: 0.00002192
Iteration 191/1000 | Loss: 0.00002192
Iteration 192/1000 | Loss: 0.00002192
Iteration 193/1000 | Loss: 0.00002192
Iteration 194/1000 | Loss: 0.00002191
Iteration 195/1000 | Loss: 0.00002191
Iteration 196/1000 | Loss: 0.00002191
Iteration 197/1000 | Loss: 0.00002191
Iteration 198/1000 | Loss: 0.00002191
Iteration 199/1000 | Loss: 0.00002191
Iteration 200/1000 | Loss: 0.00002191
Iteration 201/1000 | Loss: 0.00002191
Iteration 202/1000 | Loss: 0.00002191
Iteration 203/1000 | Loss: 0.00002191
Iteration 204/1000 | Loss: 0.00002191
Iteration 205/1000 | Loss: 0.00002191
Iteration 206/1000 | Loss: 0.00002191
Iteration 207/1000 | Loss: 0.00002191
Iteration 208/1000 | Loss: 0.00002191
Iteration 209/1000 | Loss: 0.00002191
Iteration 210/1000 | Loss: 0.00002191
Iteration 211/1000 | Loss: 0.00002191
Iteration 212/1000 | Loss: 0.00002191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.190891609643586e-05, 2.190891609643586e-05, 2.190891609643586e-05, 2.190891609643586e-05, 2.190891609643586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.190891609643586e-05

Optimization complete. Final v2v error: 3.995008707046509 mm

Highest mean error: 4.334887504577637 mm for frame 23

Lowest mean error: 3.714526653289795 mm for frame 138

Saving results

Total time: 44.94524908065796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979717
Iteration 2/25 | Loss: 0.00253246
Iteration 3/25 | Loss: 0.00192866
Iteration 4/25 | Loss: 0.00179282
Iteration 5/25 | Loss: 0.00188307
Iteration 6/25 | Loss: 0.00169017
Iteration 7/25 | Loss: 0.00151028
Iteration 8/25 | Loss: 0.00146736
Iteration 9/25 | Loss: 0.00143497
Iteration 10/25 | Loss: 0.00142051
Iteration 11/25 | Loss: 0.00141359
Iteration 12/25 | Loss: 0.00140178
Iteration 13/25 | Loss: 0.00139621
Iteration 14/25 | Loss: 0.00139691
Iteration 15/25 | Loss: 0.00139390
Iteration 16/25 | Loss: 0.00139251
Iteration 17/25 | Loss: 0.00139208
Iteration 18/25 | Loss: 0.00139206
Iteration 19/25 | Loss: 0.00139188
Iteration 20/25 | Loss: 0.00139211
Iteration 21/25 | Loss: 0.00139174
Iteration 22/25 | Loss: 0.00139202
Iteration 23/25 | Loss: 0.00139694
Iteration 24/25 | Loss: 0.00139139
Iteration 25/25 | Loss: 0.00138757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37559712
Iteration 2/25 | Loss: 0.00079089
Iteration 3/25 | Loss: 0.00079089
Iteration 4/25 | Loss: 0.00079089
Iteration 5/25 | Loss: 0.00079089
Iteration 6/25 | Loss: 0.00079089
Iteration 7/25 | Loss: 0.00079089
Iteration 8/25 | Loss: 0.00079089
Iteration 9/25 | Loss: 0.00079089
Iteration 10/25 | Loss: 0.00079089
Iteration 11/25 | Loss: 0.00079089
Iteration 12/25 | Loss: 0.00079089
Iteration 13/25 | Loss: 0.00079089
Iteration 14/25 | Loss: 0.00079089
Iteration 15/25 | Loss: 0.00079089
Iteration 16/25 | Loss: 0.00079089
Iteration 17/25 | Loss: 0.00079089
Iteration 18/25 | Loss: 0.00079089
Iteration 19/25 | Loss: 0.00079089
Iteration 20/25 | Loss: 0.00079089
Iteration 21/25 | Loss: 0.00079089
Iteration 22/25 | Loss: 0.00079089
Iteration 23/25 | Loss: 0.00079089
Iteration 24/25 | Loss: 0.00079089
Iteration 25/25 | Loss: 0.00079089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079089
Iteration 2/1000 | Loss: 0.00012792
Iteration 3/1000 | Loss: 0.00003593
Iteration 4/1000 | Loss: 0.00003118
Iteration 5/1000 | Loss: 0.00002848
Iteration 6/1000 | Loss: 0.00002724
Iteration 7/1000 | Loss: 0.00002645
Iteration 8/1000 | Loss: 0.00002591
Iteration 9/1000 | Loss: 0.00002557
Iteration 10/1000 | Loss: 0.00002522
Iteration 11/1000 | Loss: 0.00002504
Iteration 12/1000 | Loss: 0.00002486
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002479
Iteration 15/1000 | Loss: 0.00002475
Iteration 16/1000 | Loss: 0.00002474
Iteration 17/1000 | Loss: 0.00002470
Iteration 18/1000 | Loss: 0.00002470
Iteration 19/1000 | Loss: 0.00002469
Iteration 20/1000 | Loss: 0.00002469
Iteration 21/1000 | Loss: 0.00002466
Iteration 22/1000 | Loss: 0.00002465
Iteration 23/1000 | Loss: 0.00002465
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002464
Iteration 26/1000 | Loss: 0.00002462
Iteration 27/1000 | Loss: 0.00002462
Iteration 28/1000 | Loss: 0.00002461
Iteration 29/1000 | Loss: 0.00002460
Iteration 30/1000 | Loss: 0.00002460
Iteration 31/1000 | Loss: 0.00002459
Iteration 32/1000 | Loss: 0.00002459
Iteration 33/1000 | Loss: 0.00002458
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002456
Iteration 36/1000 | Loss: 0.00002456
Iteration 37/1000 | Loss: 0.00002455
Iteration 38/1000 | Loss: 0.00002455
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002452
Iteration 41/1000 | Loss: 0.00002452
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002452
Iteration 44/1000 | Loss: 0.00002452
Iteration 45/1000 | Loss: 0.00002452
Iteration 46/1000 | Loss: 0.00002452
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002451
Iteration 51/1000 | Loss: 0.00002451
Iteration 52/1000 | Loss: 0.00002451
Iteration 53/1000 | Loss: 0.00002451
Iteration 54/1000 | Loss: 0.00002451
Iteration 55/1000 | Loss: 0.00002451
Iteration 56/1000 | Loss: 0.00002451
Iteration 57/1000 | Loss: 0.00002450
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002449
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002449
Iteration 62/1000 | Loss: 0.00002448
Iteration 63/1000 | Loss: 0.00002448
Iteration 64/1000 | Loss: 0.00002448
Iteration 65/1000 | Loss: 0.00002447
Iteration 66/1000 | Loss: 0.00002447
Iteration 67/1000 | Loss: 0.00002447
Iteration 68/1000 | Loss: 0.00002447
Iteration 69/1000 | Loss: 0.00002446
Iteration 70/1000 | Loss: 0.00002446
Iteration 71/1000 | Loss: 0.00002446
Iteration 72/1000 | Loss: 0.00002446
Iteration 73/1000 | Loss: 0.00002446
Iteration 74/1000 | Loss: 0.00002446
Iteration 75/1000 | Loss: 0.00002446
Iteration 76/1000 | Loss: 0.00002446
Iteration 77/1000 | Loss: 0.00002445
Iteration 78/1000 | Loss: 0.00002445
Iteration 79/1000 | Loss: 0.00002445
Iteration 80/1000 | Loss: 0.00002445
Iteration 81/1000 | Loss: 0.00002444
Iteration 82/1000 | Loss: 0.00002444
Iteration 83/1000 | Loss: 0.00002444
Iteration 84/1000 | Loss: 0.00002444
Iteration 85/1000 | Loss: 0.00002444
Iteration 86/1000 | Loss: 0.00002444
Iteration 87/1000 | Loss: 0.00002443
Iteration 88/1000 | Loss: 0.00002443
Iteration 89/1000 | Loss: 0.00002443
Iteration 90/1000 | Loss: 0.00002443
Iteration 91/1000 | Loss: 0.00002443
Iteration 92/1000 | Loss: 0.00002442
Iteration 93/1000 | Loss: 0.00002442
Iteration 94/1000 | Loss: 0.00002442
Iteration 95/1000 | Loss: 0.00002442
Iteration 96/1000 | Loss: 0.00002442
Iteration 97/1000 | Loss: 0.00002442
Iteration 98/1000 | Loss: 0.00002442
Iteration 99/1000 | Loss: 0.00002441
Iteration 100/1000 | Loss: 0.00002441
Iteration 101/1000 | Loss: 0.00002441
Iteration 102/1000 | Loss: 0.00002441
Iteration 103/1000 | Loss: 0.00002441
Iteration 104/1000 | Loss: 0.00002441
Iteration 105/1000 | Loss: 0.00002441
Iteration 106/1000 | Loss: 0.00002441
Iteration 107/1000 | Loss: 0.00002441
Iteration 108/1000 | Loss: 0.00002441
Iteration 109/1000 | Loss: 0.00002441
Iteration 110/1000 | Loss: 0.00002440
Iteration 111/1000 | Loss: 0.00002440
Iteration 112/1000 | Loss: 0.00002440
Iteration 113/1000 | Loss: 0.00002440
Iteration 114/1000 | Loss: 0.00002440
Iteration 115/1000 | Loss: 0.00002440
Iteration 116/1000 | Loss: 0.00002440
Iteration 117/1000 | Loss: 0.00002440
Iteration 118/1000 | Loss: 0.00002440
Iteration 119/1000 | Loss: 0.00002439
Iteration 120/1000 | Loss: 0.00002439
Iteration 121/1000 | Loss: 0.00002439
Iteration 122/1000 | Loss: 0.00002439
Iteration 123/1000 | Loss: 0.00002439
Iteration 124/1000 | Loss: 0.00002439
Iteration 125/1000 | Loss: 0.00002439
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002439
Iteration 128/1000 | Loss: 0.00002439
Iteration 129/1000 | Loss: 0.00002439
Iteration 130/1000 | Loss: 0.00002439
Iteration 131/1000 | Loss: 0.00002439
Iteration 132/1000 | Loss: 0.00002439
Iteration 133/1000 | Loss: 0.00002439
Iteration 134/1000 | Loss: 0.00002439
Iteration 135/1000 | Loss: 0.00002439
Iteration 136/1000 | Loss: 0.00002439
Iteration 137/1000 | Loss: 0.00002439
Iteration 138/1000 | Loss: 0.00002439
Iteration 139/1000 | Loss: 0.00002439
Iteration 140/1000 | Loss: 0.00002439
Iteration 141/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.4390292310272343e-05, 2.4390292310272343e-05, 2.4390292310272343e-05, 2.4390292310272343e-05, 2.4390292310272343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4390292310272343e-05

Optimization complete. Final v2v error: 4.175461769104004 mm

Highest mean error: 4.82515287399292 mm for frame 0

Lowest mean error: 3.9705605506896973 mm for frame 70

Saving results

Total time: 82.51621580123901
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418143
Iteration 2/25 | Loss: 0.00141152
Iteration 3/25 | Loss: 0.00133307
Iteration 4/25 | Loss: 0.00132142
Iteration 5/25 | Loss: 0.00131814
Iteration 6/25 | Loss: 0.00131814
Iteration 7/25 | Loss: 0.00131814
Iteration 8/25 | Loss: 0.00131814
Iteration 9/25 | Loss: 0.00131814
Iteration 10/25 | Loss: 0.00131814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013181382091715932, 0.0013181382091715932, 0.0013181382091715932, 0.0013181382091715932, 0.0013181382091715932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013181382091715932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.50580025
Iteration 2/25 | Loss: 0.00094101
Iteration 3/25 | Loss: 0.00094098
Iteration 4/25 | Loss: 0.00094098
Iteration 5/25 | Loss: 0.00094098
Iteration 6/25 | Loss: 0.00094098
Iteration 7/25 | Loss: 0.00094098
Iteration 8/25 | Loss: 0.00094097
Iteration 9/25 | Loss: 0.00094097
Iteration 10/25 | Loss: 0.00094097
Iteration 11/25 | Loss: 0.00094097
Iteration 12/25 | Loss: 0.00094097
Iteration 13/25 | Loss: 0.00094097
Iteration 14/25 | Loss: 0.00094097
Iteration 15/25 | Loss: 0.00094097
Iteration 16/25 | Loss: 0.00094097
Iteration 17/25 | Loss: 0.00094097
Iteration 18/25 | Loss: 0.00094097
Iteration 19/25 | Loss: 0.00094097
Iteration 20/25 | Loss: 0.00094097
Iteration 21/25 | Loss: 0.00094097
Iteration 22/25 | Loss: 0.00094097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009409738704562187, 0.0009409738704562187, 0.0009409738704562187, 0.0009409738704562187, 0.0009409738704562187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009409738704562187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094097
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002386
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00002190
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002046
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001990
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001953
Iteration 13/1000 | Loss: 0.00001941
Iteration 14/1000 | Loss: 0.00001939
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001930
Iteration 18/1000 | Loss: 0.00001930
Iteration 19/1000 | Loss: 0.00001929
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001926
Iteration 24/1000 | Loss: 0.00001926
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001925
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001921
Iteration 33/1000 | Loss: 0.00001917
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001915
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001910
Iteration 48/1000 | Loss: 0.00001910
Iteration 49/1000 | Loss: 0.00001910
Iteration 50/1000 | Loss: 0.00001910
Iteration 51/1000 | Loss: 0.00001910
Iteration 52/1000 | Loss: 0.00001910
Iteration 53/1000 | Loss: 0.00001910
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00001910
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001910
Iteration 58/1000 | Loss: 0.00001910
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001909
Iteration 61/1000 | Loss: 0.00001909
Iteration 62/1000 | Loss: 0.00001909
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001907
Iteration 70/1000 | Loss: 0.00001907
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001906
Iteration 77/1000 | Loss: 0.00001906
Iteration 78/1000 | Loss: 0.00001906
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001905
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001904
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001903
Iteration 92/1000 | Loss: 0.00001903
Iteration 93/1000 | Loss: 0.00001903
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001902
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001902
Iteration 99/1000 | Loss: 0.00001902
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001902
Iteration 102/1000 | Loss: 0.00001902
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001902
Iteration 105/1000 | Loss: 0.00001902
Iteration 106/1000 | Loss: 0.00001902
Iteration 107/1000 | Loss: 0.00001902
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Iteration 112/1000 | Loss: 0.00001902
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001902
Iteration 115/1000 | Loss: 0.00001902
Iteration 116/1000 | Loss: 0.00001902
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001902
Iteration 121/1000 | Loss: 0.00001902
Iteration 122/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.901952964544762e-05, 1.901952964544762e-05, 1.901952964544762e-05, 1.901952964544762e-05, 1.901952964544762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.901952964544762e-05

Optimization complete. Final v2v error: 3.741619348526001 mm

Highest mean error: 3.9163079261779785 mm for frame 32

Lowest mean error: 3.58034086227417 mm for frame 49

Saving results

Total time: 35.03637766838074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017103
Iteration 2/25 | Loss: 0.01017103
Iteration 3/25 | Loss: 0.00259403
Iteration 4/25 | Loss: 0.00195007
Iteration 5/25 | Loss: 0.00166109
Iteration 6/25 | Loss: 0.00159067
Iteration 7/25 | Loss: 0.00158986
Iteration 8/25 | Loss: 0.00156277
Iteration 9/25 | Loss: 0.00150540
Iteration 10/25 | Loss: 0.00146059
Iteration 11/25 | Loss: 0.00146474
Iteration 12/25 | Loss: 0.00146016
Iteration 13/25 | Loss: 0.00146715
Iteration 14/25 | Loss: 0.00145195
Iteration 15/25 | Loss: 0.00144039
Iteration 16/25 | Loss: 0.00143362
Iteration 17/25 | Loss: 0.00143463
Iteration 18/25 | Loss: 0.00142887
Iteration 19/25 | Loss: 0.00142447
Iteration 20/25 | Loss: 0.00142042
Iteration 21/25 | Loss: 0.00141924
Iteration 22/25 | Loss: 0.00141866
Iteration 23/25 | Loss: 0.00141855
Iteration 24/25 | Loss: 0.00141850
Iteration 25/25 | Loss: 0.00141850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33605552
Iteration 2/25 | Loss: 0.00291101
Iteration 3/25 | Loss: 0.00191844
Iteration 4/25 | Loss: 0.00191840
Iteration 5/25 | Loss: 0.00191840
Iteration 6/25 | Loss: 0.00191840
Iteration 7/25 | Loss: 0.00191839
Iteration 8/25 | Loss: 0.00191839
Iteration 9/25 | Loss: 0.00191839
Iteration 10/25 | Loss: 0.00191839
Iteration 11/25 | Loss: 0.00191839
Iteration 12/25 | Loss: 0.00191839
Iteration 13/25 | Loss: 0.00191839
Iteration 14/25 | Loss: 0.00191839
Iteration 15/25 | Loss: 0.00191839
Iteration 16/25 | Loss: 0.00191839
Iteration 17/25 | Loss: 0.00191839
Iteration 18/25 | Loss: 0.00191839
Iteration 19/25 | Loss: 0.00191839
Iteration 20/25 | Loss: 0.00191839
Iteration 21/25 | Loss: 0.00191839
Iteration 22/25 | Loss: 0.00191839
Iteration 23/25 | Loss: 0.00191839
Iteration 24/25 | Loss: 0.00191839
Iteration 25/25 | Loss: 0.00191839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191839
Iteration 2/1000 | Loss: 0.00025539
Iteration 3/1000 | Loss: 0.00057650
Iteration 4/1000 | Loss: 0.00057424
Iteration 5/1000 | Loss: 0.00348021
Iteration 6/1000 | Loss: 0.00094557
Iteration 7/1000 | Loss: 0.00255406
Iteration 8/1000 | Loss: 0.00012225
Iteration 9/1000 | Loss: 0.00010151
Iteration 10/1000 | Loss: 0.00008867
Iteration 11/1000 | Loss: 0.00008075
Iteration 12/1000 | Loss: 0.00030772
Iteration 13/1000 | Loss: 0.00042025
Iteration 14/1000 | Loss: 0.00027299
Iteration 15/1000 | Loss: 0.00030988
Iteration 16/1000 | Loss: 0.00340483
Iteration 17/1000 | Loss: 0.00501725
Iteration 18/1000 | Loss: 0.00155630
Iteration 19/1000 | Loss: 0.00071728
Iteration 20/1000 | Loss: 0.00027933
Iteration 21/1000 | Loss: 0.00059335
Iteration 22/1000 | Loss: 0.00009134
Iteration 23/1000 | Loss: 0.00186345
Iteration 24/1000 | Loss: 0.00016423
Iteration 25/1000 | Loss: 0.00047583
Iteration 26/1000 | Loss: 0.00012100
Iteration 27/1000 | Loss: 0.00006927
Iteration 28/1000 | Loss: 0.00062213
Iteration 29/1000 | Loss: 0.00023688
Iteration 30/1000 | Loss: 0.00042574
Iteration 31/1000 | Loss: 0.00053163
Iteration 32/1000 | Loss: 0.00017059
Iteration 33/1000 | Loss: 0.00044204
Iteration 34/1000 | Loss: 0.00008863
Iteration 35/1000 | Loss: 0.00006478
Iteration 36/1000 | Loss: 0.00033257
Iteration 37/1000 | Loss: 0.00028840
Iteration 38/1000 | Loss: 0.00008798
Iteration 39/1000 | Loss: 0.00035360
Iteration 40/1000 | Loss: 0.00031609
Iteration 41/1000 | Loss: 0.00040760
Iteration 42/1000 | Loss: 0.00038539
Iteration 43/1000 | Loss: 0.00030750
Iteration 44/1000 | Loss: 0.00006331
Iteration 45/1000 | Loss: 0.00016126
Iteration 46/1000 | Loss: 0.00028074
Iteration 47/1000 | Loss: 0.00029179
Iteration 48/1000 | Loss: 0.00043111
Iteration 49/1000 | Loss: 0.00014537
Iteration 50/1000 | Loss: 0.00041874
Iteration 51/1000 | Loss: 0.00074665
Iteration 52/1000 | Loss: 0.00366816
Iteration 53/1000 | Loss: 0.00140022
Iteration 54/1000 | Loss: 0.00119505
Iteration 55/1000 | Loss: 0.00105490
Iteration 56/1000 | Loss: 0.00158356
Iteration 57/1000 | Loss: 0.00092260
Iteration 58/1000 | Loss: 0.00203095
Iteration 59/1000 | Loss: 0.00121314
Iteration 60/1000 | Loss: 0.00037369
Iteration 61/1000 | Loss: 0.00044159
Iteration 62/1000 | Loss: 0.00094049
Iteration 63/1000 | Loss: 0.00056828
Iteration 64/1000 | Loss: 0.00008718
Iteration 65/1000 | Loss: 0.00049513
Iteration 66/1000 | Loss: 0.00005893
Iteration 67/1000 | Loss: 0.00005465
Iteration 68/1000 | Loss: 0.00005116
Iteration 69/1000 | Loss: 0.00004904
Iteration 70/1000 | Loss: 0.00004756
Iteration 71/1000 | Loss: 0.00004660
Iteration 72/1000 | Loss: 0.00004549
Iteration 73/1000 | Loss: 0.00006470
Iteration 74/1000 | Loss: 0.00005034
Iteration 75/1000 | Loss: 0.00089629
Iteration 76/1000 | Loss: 0.00334664
Iteration 77/1000 | Loss: 0.00031305
Iteration 78/1000 | Loss: 0.00005732
Iteration 79/1000 | Loss: 0.00004682
Iteration 80/1000 | Loss: 0.00004514
Iteration 81/1000 | Loss: 0.00003701
Iteration 82/1000 | Loss: 0.00003115
Iteration 83/1000 | Loss: 0.00004968
Iteration 84/1000 | Loss: 0.00002618
Iteration 85/1000 | Loss: 0.00002455
Iteration 86/1000 | Loss: 0.00002379
Iteration 87/1000 | Loss: 0.00002299
Iteration 88/1000 | Loss: 0.00002252
Iteration 89/1000 | Loss: 0.00004638
Iteration 90/1000 | Loss: 0.00002725
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002815
Iteration 93/1000 | Loss: 0.00002815
Iteration 94/1000 | Loss: 0.00002814
Iteration 95/1000 | Loss: 0.00002813
Iteration 96/1000 | Loss: 0.00017911
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002131
Iteration 100/1000 | Loss: 0.00002121
Iteration 101/1000 | Loss: 0.00002120
Iteration 102/1000 | Loss: 0.00003642
Iteration 103/1000 | Loss: 0.00002210
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002114
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002114
Iteration 109/1000 | Loss: 0.00002113
Iteration 110/1000 | Loss: 0.00002113
Iteration 111/1000 | Loss: 0.00002113
Iteration 112/1000 | Loss: 0.00002113
Iteration 113/1000 | Loss: 0.00002109
Iteration 114/1000 | Loss: 0.00002109
Iteration 115/1000 | Loss: 0.00002109
Iteration 116/1000 | Loss: 0.00002108
Iteration 117/1000 | Loss: 0.00002108
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002106
Iteration 123/1000 | Loss: 0.00002106
Iteration 124/1000 | Loss: 0.00002106
Iteration 125/1000 | Loss: 0.00002106
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002106
Iteration 128/1000 | Loss: 0.00002106
Iteration 129/1000 | Loss: 0.00002106
Iteration 130/1000 | Loss: 0.00002106
Iteration 131/1000 | Loss: 0.00002106
Iteration 132/1000 | Loss: 0.00002106
Iteration 133/1000 | Loss: 0.00002106
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00002105
Iteration 136/1000 | Loss: 0.00002105
Iteration 137/1000 | Loss: 0.00002105
Iteration 138/1000 | Loss: 0.00002105
Iteration 139/1000 | Loss: 0.00002105
Iteration 140/1000 | Loss: 0.00002105
Iteration 141/1000 | Loss: 0.00002105
Iteration 142/1000 | Loss: 0.00002104
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Iteration 148/1000 | Loss: 0.00002102
Iteration 149/1000 | Loss: 0.00002102
Iteration 150/1000 | Loss: 0.00002102
Iteration 151/1000 | Loss: 0.00002102
Iteration 152/1000 | Loss: 0.00002102
Iteration 153/1000 | Loss: 0.00002102
Iteration 154/1000 | Loss: 0.00002102
Iteration 155/1000 | Loss: 0.00002102
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002101
Iteration 158/1000 | Loss: 0.00002101
Iteration 159/1000 | Loss: 0.00002101
Iteration 160/1000 | Loss: 0.00002101
Iteration 161/1000 | Loss: 0.00002101
Iteration 162/1000 | Loss: 0.00002100
Iteration 163/1000 | Loss: 0.00002100
Iteration 164/1000 | Loss: 0.00002100
Iteration 165/1000 | Loss: 0.00002100
Iteration 166/1000 | Loss: 0.00002100
Iteration 167/1000 | Loss: 0.00002100
Iteration 168/1000 | Loss: 0.00002100
Iteration 169/1000 | Loss: 0.00002100
Iteration 170/1000 | Loss: 0.00002100
Iteration 171/1000 | Loss: 0.00002100
Iteration 172/1000 | Loss: 0.00002100
Iteration 173/1000 | Loss: 0.00002099
Iteration 174/1000 | Loss: 0.00002099
Iteration 175/1000 | Loss: 0.00002099
Iteration 176/1000 | Loss: 0.00002099
Iteration 177/1000 | Loss: 0.00002099
Iteration 178/1000 | Loss: 0.00002098
Iteration 179/1000 | Loss: 0.00002097
Iteration 180/1000 | Loss: 0.00002097
Iteration 181/1000 | Loss: 0.00002097
Iteration 182/1000 | Loss: 0.00002097
Iteration 183/1000 | Loss: 0.00002096
Iteration 184/1000 | Loss: 0.00002096
Iteration 185/1000 | Loss: 0.00002096
Iteration 186/1000 | Loss: 0.00002096
Iteration 187/1000 | Loss: 0.00002095
Iteration 188/1000 | Loss: 0.00002095
Iteration 189/1000 | Loss: 0.00002095
Iteration 190/1000 | Loss: 0.00002095
Iteration 191/1000 | Loss: 0.00002095
Iteration 192/1000 | Loss: 0.00002095
Iteration 193/1000 | Loss: 0.00002095
Iteration 194/1000 | Loss: 0.00002095
Iteration 195/1000 | Loss: 0.00002095
Iteration 196/1000 | Loss: 0.00002094
Iteration 197/1000 | Loss: 0.00002094
Iteration 198/1000 | Loss: 0.00002094
Iteration 199/1000 | Loss: 0.00002094
Iteration 200/1000 | Loss: 0.00002094
Iteration 201/1000 | Loss: 0.00002094
Iteration 202/1000 | Loss: 0.00002094
Iteration 203/1000 | Loss: 0.00002094
Iteration 204/1000 | Loss: 0.00002094
Iteration 205/1000 | Loss: 0.00002094
Iteration 206/1000 | Loss: 0.00002094
Iteration 207/1000 | Loss: 0.00002094
Iteration 208/1000 | Loss: 0.00002094
Iteration 209/1000 | Loss: 0.00002094
Iteration 210/1000 | Loss: 0.00002094
Iteration 211/1000 | Loss: 0.00002094
Iteration 212/1000 | Loss: 0.00002094
Iteration 213/1000 | Loss: 0.00002094
Iteration 214/1000 | Loss: 0.00002094
Iteration 215/1000 | Loss: 0.00002094
Iteration 216/1000 | Loss: 0.00002094
Iteration 217/1000 | Loss: 0.00002094
Iteration 218/1000 | Loss: 0.00002094
Iteration 219/1000 | Loss: 0.00002094
Iteration 220/1000 | Loss: 0.00002094
Iteration 221/1000 | Loss: 0.00002094
Iteration 222/1000 | Loss: 0.00002094
Iteration 223/1000 | Loss: 0.00002094
Iteration 224/1000 | Loss: 0.00002094
Iteration 225/1000 | Loss: 0.00002094
Iteration 226/1000 | Loss: 0.00002094
Iteration 227/1000 | Loss: 0.00002094
Iteration 228/1000 | Loss: 0.00002094
Iteration 229/1000 | Loss: 0.00002094
Iteration 230/1000 | Loss: 0.00002094
Iteration 231/1000 | Loss: 0.00002094
Iteration 232/1000 | Loss: 0.00002094
Iteration 233/1000 | Loss: 0.00002094
Iteration 234/1000 | Loss: 0.00002094
Iteration 235/1000 | Loss: 0.00002094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.094022238452453e-05, 2.094022238452453e-05, 2.094022238452453e-05, 2.094022238452453e-05, 2.094022238452453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.094022238452453e-05

Optimization complete. Final v2v error: 3.8726086616516113 mm

Highest mean error: 5.156686305999756 mm for frame 222

Lowest mean error: 3.43331241607666 mm for frame 239

Saving results

Total time: 214.31537222862244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449329
Iteration 2/25 | Loss: 0.00135924
Iteration 3/25 | Loss: 0.00130659
Iteration 4/25 | Loss: 0.00129448
Iteration 5/25 | Loss: 0.00129058
Iteration 6/25 | Loss: 0.00129031
Iteration 7/25 | Loss: 0.00129031
Iteration 8/25 | Loss: 0.00129031
Iteration 9/25 | Loss: 0.00129031
Iteration 10/25 | Loss: 0.00129031
Iteration 11/25 | Loss: 0.00129031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012903099413961172, 0.0012903099413961172, 0.0012903099413961172, 0.0012903099413961172, 0.0012903099413961172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012903099413961172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40732539
Iteration 2/25 | Loss: 0.00087191
Iteration 3/25 | Loss: 0.00087191
Iteration 4/25 | Loss: 0.00087191
Iteration 5/25 | Loss: 0.00087191
Iteration 6/25 | Loss: 0.00087191
Iteration 7/25 | Loss: 0.00087191
Iteration 8/25 | Loss: 0.00087191
Iteration 9/25 | Loss: 0.00087191
Iteration 10/25 | Loss: 0.00087191
Iteration 11/25 | Loss: 0.00087191
Iteration 12/25 | Loss: 0.00087191
Iteration 13/25 | Loss: 0.00087191
Iteration 14/25 | Loss: 0.00087191
Iteration 15/25 | Loss: 0.00087191
Iteration 16/25 | Loss: 0.00087191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000871908909175545, 0.000871908909175545, 0.000871908909175545, 0.000871908909175545, 0.000871908909175545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000871908909175545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087191
Iteration 2/1000 | Loss: 0.00003023
Iteration 3/1000 | Loss: 0.00002216
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001957
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001841
Iteration 8/1000 | Loss: 0.00001808
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001729
Iteration 11/1000 | Loss: 0.00001718
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001675
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001672
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001668
Iteration 26/1000 | Loss: 0.00001667
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001649
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001642
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001632
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001621
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001620
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001618
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001617
Iteration 138/1000 | Loss: 0.00001617
Iteration 139/1000 | Loss: 0.00001617
Iteration 140/1000 | Loss: 0.00001617
Iteration 141/1000 | Loss: 0.00001617
Iteration 142/1000 | Loss: 0.00001617
Iteration 143/1000 | Loss: 0.00001617
Iteration 144/1000 | Loss: 0.00001617
Iteration 145/1000 | Loss: 0.00001617
Iteration 146/1000 | Loss: 0.00001617
Iteration 147/1000 | Loss: 0.00001617
Iteration 148/1000 | Loss: 0.00001616
Iteration 149/1000 | Loss: 0.00001616
Iteration 150/1000 | Loss: 0.00001616
Iteration 151/1000 | Loss: 0.00001616
Iteration 152/1000 | Loss: 0.00001616
Iteration 153/1000 | Loss: 0.00001616
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001616
Iteration 159/1000 | Loss: 0.00001616
Iteration 160/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6157653590198606e-05, 1.6157653590198606e-05, 1.6157653590198606e-05, 1.6157653590198606e-05, 1.6157653590198606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6157653590198606e-05

Optimization complete. Final v2v error: 3.4137744903564453 mm

Highest mean error: 3.557079553604126 mm for frame 139

Lowest mean error: 3.2794604301452637 mm for frame 18

Saving results

Total time: 40.474366426467896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725150
Iteration 2/25 | Loss: 0.00148056
Iteration 3/25 | Loss: 0.00142741
Iteration 4/25 | Loss: 0.00142170
Iteration 5/25 | Loss: 0.00141968
Iteration 6/25 | Loss: 0.00141953
Iteration 7/25 | Loss: 0.00141953
Iteration 8/25 | Loss: 0.00141953
Iteration 9/25 | Loss: 0.00141953
Iteration 10/25 | Loss: 0.00141953
Iteration 11/25 | Loss: 0.00141953
Iteration 12/25 | Loss: 0.00141953
Iteration 13/25 | Loss: 0.00141953
Iteration 14/25 | Loss: 0.00141953
Iteration 15/25 | Loss: 0.00141953
Iteration 16/25 | Loss: 0.00141953
Iteration 17/25 | Loss: 0.00141953
Iteration 18/25 | Loss: 0.00141953
Iteration 19/25 | Loss: 0.00141953
Iteration 20/25 | Loss: 0.00141953
Iteration 21/25 | Loss: 0.00141953
Iteration 22/25 | Loss: 0.00141953
Iteration 23/25 | Loss: 0.00141953
Iteration 24/25 | Loss: 0.00141953
Iteration 25/25 | Loss: 0.00141953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21213102
Iteration 2/25 | Loss: 0.00100159
Iteration 3/25 | Loss: 0.00100157
Iteration 4/25 | Loss: 0.00100157
Iteration 5/25 | Loss: 0.00100157
Iteration 6/25 | Loss: 0.00100157
Iteration 7/25 | Loss: 0.00100157
Iteration 8/25 | Loss: 0.00100157
Iteration 9/25 | Loss: 0.00100157
Iteration 10/25 | Loss: 0.00100157
Iteration 11/25 | Loss: 0.00100157
Iteration 12/25 | Loss: 0.00100157
Iteration 13/25 | Loss: 0.00100157
Iteration 14/25 | Loss: 0.00100157
Iteration 15/25 | Loss: 0.00100157
Iteration 16/25 | Loss: 0.00100157
Iteration 17/25 | Loss: 0.00100157
Iteration 18/25 | Loss: 0.00100157
Iteration 19/25 | Loss: 0.00100157
Iteration 20/25 | Loss: 0.00100157
Iteration 21/25 | Loss: 0.00100157
Iteration 22/25 | Loss: 0.00100157
Iteration 23/25 | Loss: 0.00100157
Iteration 24/25 | Loss: 0.00100157
Iteration 25/25 | Loss: 0.00100157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100157
Iteration 2/1000 | Loss: 0.00004017
Iteration 3/1000 | Loss: 0.00002824
Iteration 4/1000 | Loss: 0.00002567
Iteration 5/1000 | Loss: 0.00002472
Iteration 6/1000 | Loss: 0.00002428
Iteration 7/1000 | Loss: 0.00002379
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002305
Iteration 10/1000 | Loss: 0.00002277
Iteration 11/1000 | Loss: 0.00002247
Iteration 12/1000 | Loss: 0.00002218
Iteration 13/1000 | Loss: 0.00002199
Iteration 14/1000 | Loss: 0.00002175
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002152
Iteration 18/1000 | Loss: 0.00002151
Iteration 19/1000 | Loss: 0.00002147
Iteration 20/1000 | Loss: 0.00002144
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002135
Iteration 26/1000 | Loss: 0.00002135
Iteration 27/1000 | Loss: 0.00002134
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002132
Iteration 30/1000 | Loss: 0.00002132
Iteration 31/1000 | Loss: 0.00002125
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00002124
Iteration 34/1000 | Loss: 0.00002124
Iteration 35/1000 | Loss: 0.00002124
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002123
Iteration 38/1000 | Loss: 0.00002123
Iteration 39/1000 | Loss: 0.00002121
Iteration 40/1000 | Loss: 0.00002121
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002120
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00002118
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002116
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002114
Iteration 68/1000 | Loss: 0.00002114
Iteration 69/1000 | Loss: 0.00002114
Iteration 70/1000 | Loss: 0.00002114
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002113
Iteration 75/1000 | Loss: 0.00002113
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002112
Iteration 79/1000 | Loss: 0.00002112
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002111
Iteration 83/1000 | Loss: 0.00002111
Iteration 84/1000 | Loss: 0.00002111
Iteration 85/1000 | Loss: 0.00002111
Iteration 86/1000 | Loss: 0.00002110
Iteration 87/1000 | Loss: 0.00002110
Iteration 88/1000 | Loss: 0.00002110
Iteration 89/1000 | Loss: 0.00002110
Iteration 90/1000 | Loss: 0.00002110
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002110
Iteration 93/1000 | Loss: 0.00002110
Iteration 94/1000 | Loss: 0.00002110
Iteration 95/1000 | Loss: 0.00002110
Iteration 96/1000 | Loss: 0.00002110
Iteration 97/1000 | Loss: 0.00002110
Iteration 98/1000 | Loss: 0.00002110
Iteration 99/1000 | Loss: 0.00002110
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.1096044292789884e-05, 2.1096044292789884e-05, 2.1096044292789884e-05, 2.1096044292789884e-05, 2.1096044292789884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1096044292789884e-05

Optimization complete. Final v2v error: 3.805321216583252 mm

Highest mean error: 3.939528465270996 mm for frame 10

Lowest mean error: 3.5535643100738525 mm for frame 132

Saving results

Total time: 37.42695474624634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507743
Iteration 2/25 | Loss: 0.00139267
Iteration 3/25 | Loss: 0.00132698
Iteration 4/25 | Loss: 0.00131421
Iteration 5/25 | Loss: 0.00131146
Iteration 6/25 | Loss: 0.00131066
Iteration 7/25 | Loss: 0.00131066
Iteration 8/25 | Loss: 0.00131066
Iteration 9/25 | Loss: 0.00131066
Iteration 10/25 | Loss: 0.00131066
Iteration 11/25 | Loss: 0.00131066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013106630649417639, 0.0013106630649417639, 0.0013106630649417639, 0.0013106630649417639, 0.0013106630649417639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013106630649417639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.73703527
Iteration 2/25 | Loss: 0.00090169
Iteration 3/25 | Loss: 0.00090169
Iteration 4/25 | Loss: 0.00090169
Iteration 5/25 | Loss: 0.00090169
Iteration 6/25 | Loss: 0.00090169
Iteration 7/25 | Loss: 0.00090169
Iteration 8/25 | Loss: 0.00090169
Iteration 9/25 | Loss: 0.00090169
Iteration 10/25 | Loss: 0.00090169
Iteration 11/25 | Loss: 0.00090169
Iteration 12/25 | Loss: 0.00090169
Iteration 13/25 | Loss: 0.00090169
Iteration 14/25 | Loss: 0.00090169
Iteration 15/25 | Loss: 0.00090169
Iteration 16/25 | Loss: 0.00090169
Iteration 17/25 | Loss: 0.00090169
Iteration 18/25 | Loss: 0.00090169
Iteration 19/25 | Loss: 0.00090169
Iteration 20/25 | Loss: 0.00090169
Iteration 21/25 | Loss: 0.00090169
Iteration 22/25 | Loss: 0.00090169
Iteration 23/25 | Loss: 0.00090169
Iteration 24/25 | Loss: 0.00090169
Iteration 25/25 | Loss: 0.00090169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090169
Iteration 2/1000 | Loss: 0.00004150
Iteration 3/1000 | Loss: 0.00003121
Iteration 4/1000 | Loss: 0.00002683
Iteration 5/1000 | Loss: 0.00002565
Iteration 6/1000 | Loss: 0.00002459
Iteration 7/1000 | Loss: 0.00002401
Iteration 8/1000 | Loss: 0.00002333
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002219
Iteration 12/1000 | Loss: 0.00002194
Iteration 13/1000 | Loss: 0.00002180
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002155
Iteration 16/1000 | Loss: 0.00002154
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002144
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002140
Iteration 22/1000 | Loss: 0.00002134
Iteration 23/1000 | Loss: 0.00002131
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002127
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002126
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002123
Iteration 32/1000 | Loss: 0.00002120
Iteration 33/1000 | Loss: 0.00002120
Iteration 34/1000 | Loss: 0.00002120
Iteration 35/1000 | Loss: 0.00002120
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002119
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002119
Iteration 45/1000 | Loss: 0.00002118
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002116
Iteration 48/1000 | Loss: 0.00002116
Iteration 49/1000 | Loss: 0.00002116
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002113
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00002113
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002112
Iteration 62/1000 | Loss: 0.00002112
Iteration 63/1000 | Loss: 0.00002112
Iteration 64/1000 | Loss: 0.00002112
Iteration 65/1000 | Loss: 0.00002112
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002111
Iteration 70/1000 | Loss: 0.00002111
Iteration 71/1000 | Loss: 0.00002111
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002111
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002110
Iteration 76/1000 | Loss: 0.00002110
Iteration 77/1000 | Loss: 0.00002109
Iteration 78/1000 | Loss: 0.00002109
Iteration 79/1000 | Loss: 0.00002109
Iteration 80/1000 | Loss: 0.00002109
Iteration 81/1000 | Loss: 0.00002109
Iteration 82/1000 | Loss: 0.00002109
Iteration 83/1000 | Loss: 0.00002108
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002107
Iteration 86/1000 | Loss: 0.00002107
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002106
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002105
Iteration 91/1000 | Loss: 0.00002105
Iteration 92/1000 | Loss: 0.00002105
Iteration 93/1000 | Loss: 0.00002105
Iteration 94/1000 | Loss: 0.00002105
Iteration 95/1000 | Loss: 0.00002105
Iteration 96/1000 | Loss: 0.00002105
Iteration 97/1000 | Loss: 0.00002104
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002104
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002104
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002104
Iteration 107/1000 | Loss: 0.00002104
Iteration 108/1000 | Loss: 0.00002104
Iteration 109/1000 | Loss: 0.00002104
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002103
Iteration 112/1000 | Loss: 0.00002103
Iteration 113/1000 | Loss: 0.00002103
Iteration 114/1000 | Loss: 0.00002102
Iteration 115/1000 | Loss: 0.00002102
Iteration 116/1000 | Loss: 0.00002102
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002100
Iteration 128/1000 | Loss: 0.00002099
Iteration 129/1000 | Loss: 0.00002099
Iteration 130/1000 | Loss: 0.00002099
Iteration 131/1000 | Loss: 0.00002099
Iteration 132/1000 | Loss: 0.00002099
Iteration 133/1000 | Loss: 0.00002099
Iteration 134/1000 | Loss: 0.00002099
Iteration 135/1000 | Loss: 0.00002099
Iteration 136/1000 | Loss: 0.00002099
Iteration 137/1000 | Loss: 0.00002098
Iteration 138/1000 | Loss: 0.00002098
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002098
Iteration 141/1000 | Loss: 0.00002098
Iteration 142/1000 | Loss: 0.00002098
Iteration 143/1000 | Loss: 0.00002097
Iteration 144/1000 | Loss: 0.00002097
Iteration 145/1000 | Loss: 0.00002097
Iteration 146/1000 | Loss: 0.00002097
Iteration 147/1000 | Loss: 0.00002097
Iteration 148/1000 | Loss: 0.00002097
Iteration 149/1000 | Loss: 0.00002097
Iteration 150/1000 | Loss: 0.00002097
Iteration 151/1000 | Loss: 0.00002097
Iteration 152/1000 | Loss: 0.00002097
Iteration 153/1000 | Loss: 0.00002097
Iteration 154/1000 | Loss: 0.00002097
Iteration 155/1000 | Loss: 0.00002097
Iteration 156/1000 | Loss: 0.00002096
Iteration 157/1000 | Loss: 0.00002096
Iteration 158/1000 | Loss: 0.00002096
Iteration 159/1000 | Loss: 0.00002096
Iteration 160/1000 | Loss: 0.00002096
Iteration 161/1000 | Loss: 0.00002096
Iteration 162/1000 | Loss: 0.00002095
Iteration 163/1000 | Loss: 0.00002095
Iteration 164/1000 | Loss: 0.00002095
Iteration 165/1000 | Loss: 0.00002095
Iteration 166/1000 | Loss: 0.00002095
Iteration 167/1000 | Loss: 0.00002095
Iteration 168/1000 | Loss: 0.00002095
Iteration 169/1000 | Loss: 0.00002095
Iteration 170/1000 | Loss: 0.00002095
Iteration 171/1000 | Loss: 0.00002095
Iteration 172/1000 | Loss: 0.00002095
Iteration 173/1000 | Loss: 0.00002095
Iteration 174/1000 | Loss: 0.00002095
Iteration 175/1000 | Loss: 0.00002095
Iteration 176/1000 | Loss: 0.00002095
Iteration 177/1000 | Loss: 0.00002095
Iteration 178/1000 | Loss: 0.00002095
Iteration 179/1000 | Loss: 0.00002095
Iteration 180/1000 | Loss: 0.00002095
Iteration 181/1000 | Loss: 0.00002095
Iteration 182/1000 | Loss: 0.00002095
Iteration 183/1000 | Loss: 0.00002095
Iteration 184/1000 | Loss: 0.00002095
Iteration 185/1000 | Loss: 0.00002095
Iteration 186/1000 | Loss: 0.00002095
Iteration 187/1000 | Loss: 0.00002095
Iteration 188/1000 | Loss: 0.00002095
Iteration 189/1000 | Loss: 0.00002095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.0951401893398724e-05, 2.0951401893398724e-05, 2.0951401893398724e-05, 2.0951401893398724e-05, 2.0951401893398724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0951401893398724e-05

Optimization complete. Final v2v error: 3.872405529022217 mm

Highest mean error: 4.224942684173584 mm for frame 51

Lowest mean error: 3.4021832942962646 mm for frame 31

Saving results

Total time: 41.88516449928284
