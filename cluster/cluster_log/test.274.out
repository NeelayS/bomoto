Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=274, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15344-15399
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837483
Iteration 2/25 | Loss: 0.00094189
Iteration 3/25 | Loss: 0.00076386
Iteration 4/25 | Loss: 0.00073623
Iteration 5/25 | Loss: 0.00072791
Iteration 6/25 | Loss: 0.00072592
Iteration 7/25 | Loss: 0.00072542
Iteration 8/25 | Loss: 0.00072542
Iteration 9/25 | Loss: 0.00072542
Iteration 10/25 | Loss: 0.00072542
Iteration 11/25 | Loss: 0.00072542
Iteration 12/25 | Loss: 0.00072542
Iteration 13/25 | Loss: 0.00072542
Iteration 14/25 | Loss: 0.00072542
Iteration 15/25 | Loss: 0.00072542
Iteration 16/25 | Loss: 0.00072542
Iteration 17/25 | Loss: 0.00072542
Iteration 18/25 | Loss: 0.00072542
Iteration 19/25 | Loss: 0.00072542
Iteration 20/25 | Loss: 0.00072542
Iteration 21/25 | Loss: 0.00072542
Iteration 22/25 | Loss: 0.00072542
Iteration 23/25 | Loss: 0.00072542
Iteration 24/25 | Loss: 0.00072542
Iteration 25/25 | Loss: 0.00072542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.51630235
Iteration 2/25 | Loss: 0.00045293
Iteration 3/25 | Loss: 0.00045293
Iteration 4/25 | Loss: 0.00045292
Iteration 5/25 | Loss: 0.00045292
Iteration 6/25 | Loss: 0.00045292
Iteration 7/25 | Loss: 0.00045292
Iteration 8/25 | Loss: 0.00045292
Iteration 9/25 | Loss: 0.00045292
Iteration 10/25 | Loss: 0.00045292
Iteration 11/25 | Loss: 0.00045292
Iteration 12/25 | Loss: 0.00045292
Iteration 13/25 | Loss: 0.00045292
Iteration 14/25 | Loss: 0.00045292
Iteration 15/25 | Loss: 0.00045292
Iteration 16/25 | Loss: 0.00045292
Iteration 17/25 | Loss: 0.00045292
Iteration 18/25 | Loss: 0.00045292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00045292178401723504, 0.00045292178401723504, 0.00045292178401723504, 0.00045292178401723504, 0.00045292178401723504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045292178401723504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045292
Iteration 2/1000 | Loss: 0.00002209
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001355
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001267
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001236
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001194
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001177
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001154
Iteration 124/1000 | Loss: 0.00001154
Iteration 125/1000 | Loss: 0.00001154
Iteration 126/1000 | Loss: 0.00001154
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001153
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001152
Iteration 150/1000 | Loss: 0.00001152
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001152
Iteration 154/1000 | Loss: 0.00001152
Iteration 155/1000 | Loss: 0.00001152
Iteration 156/1000 | Loss: 0.00001152
Iteration 157/1000 | Loss: 0.00001152
Iteration 158/1000 | Loss: 0.00001152
Iteration 159/1000 | Loss: 0.00001152
Iteration 160/1000 | Loss: 0.00001152
Iteration 161/1000 | Loss: 0.00001152
Iteration 162/1000 | Loss: 0.00001152
Iteration 163/1000 | Loss: 0.00001152
Iteration 164/1000 | Loss: 0.00001151
Iteration 165/1000 | Loss: 0.00001151
Iteration 166/1000 | Loss: 0.00001151
Iteration 167/1000 | Loss: 0.00001151
Iteration 168/1000 | Loss: 0.00001151
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001151
Iteration 171/1000 | Loss: 0.00001151
Iteration 172/1000 | Loss: 0.00001151
Iteration 173/1000 | Loss: 0.00001151
Iteration 174/1000 | Loss: 0.00001151
Iteration 175/1000 | Loss: 0.00001151
Iteration 176/1000 | Loss: 0.00001151
Iteration 177/1000 | Loss: 0.00001151
Iteration 178/1000 | Loss: 0.00001151
Iteration 179/1000 | Loss: 0.00001151
Iteration 180/1000 | Loss: 0.00001151
Iteration 181/1000 | Loss: 0.00001151
Iteration 182/1000 | Loss: 0.00001151
Iteration 183/1000 | Loss: 0.00001150
Iteration 184/1000 | Loss: 0.00001150
Iteration 185/1000 | Loss: 0.00001150
Iteration 186/1000 | Loss: 0.00001150
Iteration 187/1000 | Loss: 0.00001150
Iteration 188/1000 | Loss: 0.00001150
Iteration 189/1000 | Loss: 0.00001150
Iteration 190/1000 | Loss: 0.00001150
Iteration 191/1000 | Loss: 0.00001150
Iteration 192/1000 | Loss: 0.00001150
Iteration 193/1000 | Loss: 0.00001150
Iteration 194/1000 | Loss: 0.00001150
Iteration 195/1000 | Loss: 0.00001150
Iteration 196/1000 | Loss: 0.00001150
Iteration 197/1000 | Loss: 0.00001150
Iteration 198/1000 | Loss: 0.00001150
Iteration 199/1000 | Loss: 0.00001150
Iteration 200/1000 | Loss: 0.00001150
Iteration 201/1000 | Loss: 0.00001149
Iteration 202/1000 | Loss: 0.00001149
Iteration 203/1000 | Loss: 0.00001149
Iteration 204/1000 | Loss: 0.00001149
Iteration 205/1000 | Loss: 0.00001149
Iteration 206/1000 | Loss: 0.00001149
Iteration 207/1000 | Loss: 0.00001149
Iteration 208/1000 | Loss: 0.00001149
Iteration 209/1000 | Loss: 0.00001149
Iteration 210/1000 | Loss: 0.00001149
Iteration 211/1000 | Loss: 0.00001149
Iteration 212/1000 | Loss: 0.00001149
Iteration 213/1000 | Loss: 0.00001149
Iteration 214/1000 | Loss: 0.00001149
Iteration 215/1000 | Loss: 0.00001148
Iteration 216/1000 | Loss: 0.00001148
Iteration 217/1000 | Loss: 0.00001148
Iteration 218/1000 | Loss: 0.00001148
Iteration 219/1000 | Loss: 0.00001148
Iteration 220/1000 | Loss: 0.00001148
Iteration 221/1000 | Loss: 0.00001148
Iteration 222/1000 | Loss: 0.00001148
Iteration 223/1000 | Loss: 0.00001148
Iteration 224/1000 | Loss: 0.00001148
Iteration 225/1000 | Loss: 0.00001148
Iteration 226/1000 | Loss: 0.00001148
Iteration 227/1000 | Loss: 0.00001148
Iteration 228/1000 | Loss: 0.00001148
Iteration 229/1000 | Loss: 0.00001148
Iteration 230/1000 | Loss: 0.00001148
Iteration 231/1000 | Loss: 0.00001148
Iteration 232/1000 | Loss: 0.00001148
Iteration 233/1000 | Loss: 0.00001148
Iteration 234/1000 | Loss: 0.00001148
Iteration 235/1000 | Loss: 0.00001148
Iteration 236/1000 | Loss: 0.00001148
Iteration 237/1000 | Loss: 0.00001148
Iteration 238/1000 | Loss: 0.00001148
Iteration 239/1000 | Loss: 0.00001148
Iteration 240/1000 | Loss: 0.00001148
Iteration 241/1000 | Loss: 0.00001148
Iteration 242/1000 | Loss: 0.00001148
Iteration 243/1000 | Loss: 0.00001148
Iteration 244/1000 | Loss: 0.00001148
Iteration 245/1000 | Loss: 0.00001148
Iteration 246/1000 | Loss: 0.00001148
Iteration 247/1000 | Loss: 0.00001148
Iteration 248/1000 | Loss: 0.00001148
Iteration 249/1000 | Loss: 0.00001148
Iteration 250/1000 | Loss: 0.00001148
Iteration 251/1000 | Loss: 0.00001148
Iteration 252/1000 | Loss: 0.00001148
Iteration 253/1000 | Loss: 0.00001148
Iteration 254/1000 | Loss: 0.00001148
Iteration 255/1000 | Loss: 0.00001148
Iteration 256/1000 | Loss: 0.00001148
Iteration 257/1000 | Loss: 0.00001148
Iteration 258/1000 | Loss: 0.00001148
Iteration 259/1000 | Loss: 0.00001148
Iteration 260/1000 | Loss: 0.00001148
Iteration 261/1000 | Loss: 0.00001148
Iteration 262/1000 | Loss: 0.00001148
Iteration 263/1000 | Loss: 0.00001148
Iteration 264/1000 | Loss: 0.00001148
Iteration 265/1000 | Loss: 0.00001148
Iteration 266/1000 | Loss: 0.00001148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.1476357940409798e-05, 1.1476357940409798e-05, 1.1476357940409798e-05, 1.1476357940409798e-05, 1.1476357940409798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1476357940409798e-05

Optimization complete. Final v2v error: 2.8558621406555176 mm

Highest mean error: 3.8385157585144043 mm for frame 85

Lowest mean error: 2.509570598602295 mm for frame 114

Saving results

Total time: 48.58246946334839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962125
Iteration 2/25 | Loss: 0.00139502
Iteration 3/25 | Loss: 0.00102841
Iteration 4/25 | Loss: 0.00088974
Iteration 5/25 | Loss: 0.00085580
Iteration 6/25 | Loss: 0.00083377
Iteration 7/25 | Loss: 0.00083177
Iteration 8/25 | Loss: 0.00083407
Iteration 9/25 | Loss: 0.00082478
Iteration 10/25 | Loss: 0.00081567
Iteration 11/25 | Loss: 0.00080800
Iteration 12/25 | Loss: 0.00080510
Iteration 13/25 | Loss: 0.00080012
Iteration 14/25 | Loss: 0.00080357
Iteration 15/25 | Loss: 0.00080030
Iteration 16/25 | Loss: 0.00079727
Iteration 17/25 | Loss: 0.00080002
Iteration 18/25 | Loss: 0.00079641
Iteration 19/25 | Loss: 0.00079662
Iteration 20/25 | Loss: 0.00079589
Iteration 21/25 | Loss: 0.00079578
Iteration 22/25 | Loss: 0.00079836
Iteration 23/25 | Loss: 0.00079574
Iteration 24/25 | Loss: 0.00079573
Iteration 25/25 | Loss: 0.00079573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60514188
Iteration 2/25 | Loss: 0.00058872
Iteration 3/25 | Loss: 0.00050439
Iteration 4/25 | Loss: 0.00050439
Iteration 5/25 | Loss: 0.00050439
Iteration 6/25 | Loss: 0.00050439
Iteration 7/25 | Loss: 0.00050439
Iteration 8/25 | Loss: 0.00050439
Iteration 9/25 | Loss: 0.00050439
Iteration 10/25 | Loss: 0.00050439
Iteration 11/25 | Loss: 0.00050439
Iteration 12/25 | Loss: 0.00050439
Iteration 13/25 | Loss: 0.00050439
Iteration 14/25 | Loss: 0.00050439
Iteration 15/25 | Loss: 0.00050439
Iteration 16/25 | Loss: 0.00050439
Iteration 17/25 | Loss: 0.00050439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000504387600813061, 0.000504387600813061, 0.000504387600813061, 0.000504387600813061, 0.000504387600813061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000504387600813061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050439
Iteration 2/1000 | Loss: 0.00004046
Iteration 3/1000 | Loss: 0.00002762
Iteration 4/1000 | Loss: 0.00002541
Iteration 5/1000 | Loss: 0.00002401
Iteration 6/1000 | Loss: 0.00002288
Iteration 7/1000 | Loss: 0.00002229
Iteration 8/1000 | Loss: 0.00002175
Iteration 9/1000 | Loss: 0.00052405
Iteration 10/1000 | Loss: 0.00002330
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002039
Iteration 13/1000 | Loss: 0.00001985
Iteration 14/1000 | Loss: 0.00001943
Iteration 15/1000 | Loss: 0.00001922
Iteration 16/1000 | Loss: 0.00001917
Iteration 17/1000 | Loss: 0.00001916
Iteration 18/1000 | Loss: 0.00001914
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001907
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001893
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001881
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001874
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001871
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001871
Iteration 48/1000 | Loss: 0.00001871
Iteration 49/1000 | Loss: 0.00001871
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001868
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001867
Iteration 57/1000 | Loss: 0.00001867
Iteration 58/1000 | Loss: 0.00001867
Iteration 59/1000 | Loss: 0.00001867
Iteration 60/1000 | Loss: 0.00001866
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001866
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001865
Iteration 65/1000 | Loss: 0.00001865
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001864
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001859
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001857
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001857
Iteration 121/1000 | Loss: 0.00001857
Iteration 122/1000 | Loss: 0.00001856
Iteration 123/1000 | Loss: 0.00001856
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001856
Iteration 131/1000 | Loss: 0.00001855
Iteration 132/1000 | Loss: 0.00001855
Iteration 133/1000 | Loss: 0.00001855
Iteration 134/1000 | Loss: 0.00001855
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001854
Iteration 143/1000 | Loss: 0.00001854
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001853
Iteration 148/1000 | Loss: 0.00001853
Iteration 149/1000 | Loss: 0.00001853
Iteration 150/1000 | Loss: 0.00001853
Iteration 151/1000 | Loss: 0.00001853
Iteration 152/1000 | Loss: 0.00001853
Iteration 153/1000 | Loss: 0.00001853
Iteration 154/1000 | Loss: 0.00001853
Iteration 155/1000 | Loss: 0.00001853
Iteration 156/1000 | Loss: 0.00001853
Iteration 157/1000 | Loss: 0.00001853
Iteration 158/1000 | Loss: 0.00001853
Iteration 159/1000 | Loss: 0.00001853
Iteration 160/1000 | Loss: 0.00001852
Iteration 161/1000 | Loss: 0.00001852
Iteration 162/1000 | Loss: 0.00001852
Iteration 163/1000 | Loss: 0.00001852
Iteration 164/1000 | Loss: 0.00001852
Iteration 165/1000 | Loss: 0.00001852
Iteration 166/1000 | Loss: 0.00001852
Iteration 167/1000 | Loss: 0.00001852
Iteration 168/1000 | Loss: 0.00001852
Iteration 169/1000 | Loss: 0.00001852
Iteration 170/1000 | Loss: 0.00001852
Iteration 171/1000 | Loss: 0.00001852
Iteration 172/1000 | Loss: 0.00001852
Iteration 173/1000 | Loss: 0.00001852
Iteration 174/1000 | Loss: 0.00001852
Iteration 175/1000 | Loss: 0.00001852
Iteration 176/1000 | Loss: 0.00001852
Iteration 177/1000 | Loss: 0.00001852
Iteration 178/1000 | Loss: 0.00001852
Iteration 179/1000 | Loss: 0.00001852
Iteration 180/1000 | Loss: 0.00001852
Iteration 181/1000 | Loss: 0.00001852
Iteration 182/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.8518057913752273e-05, 1.8518057913752273e-05, 1.8518057913752273e-05, 1.8518057913752273e-05, 1.8518057913752273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8518057913752273e-05

Optimization complete. Final v2v error: 3.6364991664886475 mm

Highest mean error: 4.696370601654053 mm for frame 39

Lowest mean error: 3.055321455001831 mm for frame 200

Saving results

Total time: 95.98367500305176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121368
Iteration 2/25 | Loss: 0.00222461
Iteration 3/25 | Loss: 0.00127926
Iteration 4/25 | Loss: 0.00109207
Iteration 5/25 | Loss: 0.00103162
Iteration 6/25 | Loss: 0.00100271
Iteration 7/25 | Loss: 0.00099140
Iteration 8/25 | Loss: 0.00098445
Iteration 9/25 | Loss: 0.00098017
Iteration 10/25 | Loss: 0.00097586
Iteration 11/25 | Loss: 0.00097202
Iteration 12/25 | Loss: 0.00097084
Iteration 13/25 | Loss: 0.00097042
Iteration 14/25 | Loss: 0.00097035
Iteration 15/25 | Loss: 0.00097035
Iteration 16/25 | Loss: 0.00097035
Iteration 17/25 | Loss: 0.00097034
Iteration 18/25 | Loss: 0.00097034
Iteration 19/25 | Loss: 0.00097034
Iteration 20/25 | Loss: 0.00097034
Iteration 21/25 | Loss: 0.00097034
Iteration 22/25 | Loss: 0.00097034
Iteration 23/25 | Loss: 0.00097034
Iteration 24/25 | Loss: 0.00097034
Iteration 25/25 | Loss: 0.00097034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68710136
Iteration 2/25 | Loss: 0.00090952
Iteration 3/25 | Loss: 0.00090939
Iteration 4/25 | Loss: 0.00090939
Iteration 5/25 | Loss: 0.00090939
Iteration 6/25 | Loss: 0.00090938
Iteration 7/25 | Loss: 0.00090938
Iteration 8/25 | Loss: 0.00090938
Iteration 9/25 | Loss: 0.00090938
Iteration 10/25 | Loss: 0.00090938
Iteration 11/25 | Loss: 0.00090938
Iteration 12/25 | Loss: 0.00090938
Iteration 13/25 | Loss: 0.00090938
Iteration 14/25 | Loss: 0.00090938
Iteration 15/25 | Loss: 0.00090938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009093835251405835, 0.0009093835251405835, 0.0009093835251405835, 0.0009093835251405835, 0.0009093835251405835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009093835251405835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090938
Iteration 2/1000 | Loss: 0.00010838
Iteration 3/1000 | Loss: 0.00005808
Iteration 4/1000 | Loss: 0.00007314
Iteration 5/1000 | Loss: 0.00004558
Iteration 6/1000 | Loss: 0.00004906
Iteration 7/1000 | Loss: 0.00004179
Iteration 8/1000 | Loss: 0.00007532
Iteration 9/1000 | Loss: 0.00004482
Iteration 10/1000 | Loss: 0.00006696
Iteration 11/1000 | Loss: 0.00004322
Iteration 12/1000 | Loss: 0.00004042
Iteration 13/1000 | Loss: 0.00005368
Iteration 14/1000 | Loss: 0.00005072
Iteration 15/1000 | Loss: 0.00005215
Iteration 16/1000 | Loss: 0.00004898
Iteration 17/1000 | Loss: 0.00007687
Iteration 18/1000 | Loss: 0.00006391
Iteration 19/1000 | Loss: 0.00008318
Iteration 20/1000 | Loss: 0.00007592
Iteration 21/1000 | Loss: 0.00003837
Iteration 22/1000 | Loss: 0.00008354
Iteration 23/1000 | Loss: 0.00007782
Iteration 24/1000 | Loss: 0.00008574
Iteration 25/1000 | Loss: 0.00007855
Iteration 26/1000 | Loss: 0.00007773
Iteration 27/1000 | Loss: 0.00008023
Iteration 28/1000 | Loss: 0.00005165
Iteration 29/1000 | Loss: 0.00008111
Iteration 30/1000 | Loss: 0.00007692
Iteration 31/1000 | Loss: 0.00005621
Iteration 32/1000 | Loss: 0.00005998
Iteration 33/1000 | Loss: 0.00007479
Iteration 34/1000 | Loss: 0.00006560
Iteration 35/1000 | Loss: 0.00007223
Iteration 36/1000 | Loss: 0.00005582
Iteration 37/1000 | Loss: 0.00005082
Iteration 38/1000 | Loss: 0.00005371
Iteration 39/1000 | Loss: 0.00004771
Iteration 40/1000 | Loss: 0.00007650
Iteration 41/1000 | Loss: 0.00006994
Iteration 42/1000 | Loss: 0.00018699
Iteration 43/1000 | Loss: 0.00007408
Iteration 44/1000 | Loss: 0.00007914
Iteration 45/1000 | Loss: 0.00007942
Iteration 46/1000 | Loss: 0.00008917
Iteration 47/1000 | Loss: 0.00008496
Iteration 48/1000 | Loss: 0.00011642
Iteration 49/1000 | Loss: 0.00008998
Iteration 50/1000 | Loss: 0.00006954
Iteration 51/1000 | Loss: 0.00007118
Iteration 52/1000 | Loss: 0.00007771
Iteration 53/1000 | Loss: 0.00005131
Iteration 54/1000 | Loss: 0.00006448
Iteration 55/1000 | Loss: 0.00006580
Iteration 56/1000 | Loss: 0.00006474
Iteration 57/1000 | Loss: 0.00005762
Iteration 58/1000 | Loss: 0.00004315
Iteration 59/1000 | Loss: 0.00006820
Iteration 60/1000 | Loss: 0.00018261
Iteration 61/1000 | Loss: 0.00009081
Iteration 62/1000 | Loss: 0.00007112
Iteration 63/1000 | Loss: 0.00015590
Iteration 64/1000 | Loss: 0.00007583
Iteration 65/1000 | Loss: 0.00009709
Iteration 66/1000 | Loss: 0.00008178
Iteration 67/1000 | Loss: 0.00008976
Iteration 68/1000 | Loss: 0.00005032
Iteration 69/1000 | Loss: 0.00006047
Iteration 70/1000 | Loss: 0.00018008
Iteration 71/1000 | Loss: 0.00010861
Iteration 72/1000 | Loss: 0.00006980
Iteration 73/1000 | Loss: 0.00007820
Iteration 74/1000 | Loss: 0.00006234
Iteration 75/1000 | Loss: 0.00006523
Iteration 76/1000 | Loss: 0.00004825
Iteration 77/1000 | Loss: 0.00006190
Iteration 78/1000 | Loss: 0.00006731
Iteration 79/1000 | Loss: 0.00005875
Iteration 80/1000 | Loss: 0.00007789
Iteration 81/1000 | Loss: 0.00007113
Iteration 82/1000 | Loss: 0.00005339
Iteration 83/1000 | Loss: 0.00005948
Iteration 84/1000 | Loss: 0.00005161
Iteration 85/1000 | Loss: 0.00012498
Iteration 86/1000 | Loss: 0.00010142
Iteration 87/1000 | Loss: 0.00007818
Iteration 88/1000 | Loss: 0.00007234
Iteration 89/1000 | Loss: 0.00008488
Iteration 90/1000 | Loss: 0.00023131
Iteration 91/1000 | Loss: 0.00015106
Iteration 92/1000 | Loss: 0.00018774
Iteration 93/1000 | Loss: 0.00014011
Iteration 94/1000 | Loss: 0.00013470
Iteration 95/1000 | Loss: 0.00011696
Iteration 96/1000 | Loss: 0.00008670
Iteration 97/1000 | Loss: 0.00007079
Iteration 98/1000 | Loss: 0.00006558
Iteration 99/1000 | Loss: 0.00005141
Iteration 100/1000 | Loss: 0.00007848
Iteration 101/1000 | Loss: 0.00006554
Iteration 102/1000 | Loss: 0.00009000
Iteration 103/1000 | Loss: 0.00008888
Iteration 104/1000 | Loss: 0.00007491
Iteration 105/1000 | Loss: 0.00007200
Iteration 106/1000 | Loss: 0.00007837
Iteration 107/1000 | Loss: 0.00010287
Iteration 108/1000 | Loss: 0.00012539
Iteration 109/1000 | Loss: 0.00011127
Iteration 110/1000 | Loss: 0.00004644
Iteration 111/1000 | Loss: 0.00004253
Iteration 112/1000 | Loss: 0.00008897
Iteration 113/1000 | Loss: 0.00011141
Iteration 114/1000 | Loss: 0.00005933
Iteration 115/1000 | Loss: 0.00004680
Iteration 116/1000 | Loss: 0.00006282
Iteration 117/1000 | Loss: 0.00004829
Iteration 118/1000 | Loss: 0.00004407
Iteration 119/1000 | Loss: 0.00004244
Iteration 120/1000 | Loss: 0.00003963
Iteration 121/1000 | Loss: 0.00003875
Iteration 122/1000 | Loss: 0.00003774
Iteration 123/1000 | Loss: 0.00003715
Iteration 124/1000 | Loss: 0.00004969
Iteration 125/1000 | Loss: 0.00004850
Iteration 126/1000 | Loss: 0.00004987
Iteration 127/1000 | Loss: 0.00004690
Iteration 128/1000 | Loss: 0.00004527
Iteration 129/1000 | Loss: 0.00005964
Iteration 130/1000 | Loss: 0.00004095
Iteration 131/1000 | Loss: 0.00003815
Iteration 132/1000 | Loss: 0.00005263
Iteration 133/1000 | Loss: 0.00003547
Iteration 134/1000 | Loss: 0.00003473
Iteration 135/1000 | Loss: 0.00003814
Iteration 136/1000 | Loss: 0.00004794
Iteration 137/1000 | Loss: 0.00004039
Iteration 138/1000 | Loss: 0.00005118
Iteration 139/1000 | Loss: 0.00003836
Iteration 140/1000 | Loss: 0.00005159
Iteration 141/1000 | Loss: 0.00005344
Iteration 142/1000 | Loss: 0.00003686
Iteration 143/1000 | Loss: 0.00004921
Iteration 144/1000 | Loss: 0.00005140
Iteration 145/1000 | Loss: 0.00003708
Iteration 146/1000 | Loss: 0.00004763
Iteration 147/1000 | Loss: 0.00004574
Iteration 148/1000 | Loss: 0.00003686
Iteration 149/1000 | Loss: 0.00003487
Iteration 150/1000 | Loss: 0.00004602
Iteration 151/1000 | Loss: 0.00004833
Iteration 152/1000 | Loss: 0.00004277
Iteration 153/1000 | Loss: 0.00003599
Iteration 154/1000 | Loss: 0.00004555
Iteration 155/1000 | Loss: 0.00005174
Iteration 156/1000 | Loss: 0.00003955
Iteration 157/1000 | Loss: 0.00003681
Iteration 158/1000 | Loss: 0.00004884
Iteration 159/1000 | Loss: 0.00004946
Iteration 160/1000 | Loss: 0.00003705
Iteration 161/1000 | Loss: 0.00003602
Iteration 162/1000 | Loss: 0.00003492
Iteration 163/1000 | Loss: 0.00003417
Iteration 164/1000 | Loss: 0.00003392
Iteration 165/1000 | Loss: 0.00003372
Iteration 166/1000 | Loss: 0.00003354
Iteration 167/1000 | Loss: 0.00003353
Iteration 168/1000 | Loss: 0.00003352
Iteration 169/1000 | Loss: 0.00003350
Iteration 170/1000 | Loss: 0.00003350
Iteration 171/1000 | Loss: 0.00003347
Iteration 172/1000 | Loss: 0.00003343
Iteration 173/1000 | Loss: 0.00003339
Iteration 174/1000 | Loss: 0.00003339
Iteration 175/1000 | Loss: 0.00003339
Iteration 176/1000 | Loss: 0.00003338
Iteration 177/1000 | Loss: 0.00003338
Iteration 178/1000 | Loss: 0.00003335
Iteration 179/1000 | Loss: 0.00003335
Iteration 180/1000 | Loss: 0.00003331
Iteration 181/1000 | Loss: 0.00003331
Iteration 182/1000 | Loss: 0.00003330
Iteration 183/1000 | Loss: 0.00003327
Iteration 184/1000 | Loss: 0.00003327
Iteration 185/1000 | Loss: 0.00003327
Iteration 186/1000 | Loss: 0.00003327
Iteration 187/1000 | Loss: 0.00003327
Iteration 188/1000 | Loss: 0.00003327
Iteration 189/1000 | Loss: 0.00003327
Iteration 190/1000 | Loss: 0.00003327
Iteration 191/1000 | Loss: 0.00003327
Iteration 192/1000 | Loss: 0.00003326
Iteration 193/1000 | Loss: 0.00003326
Iteration 194/1000 | Loss: 0.00003324
Iteration 195/1000 | Loss: 0.00003324
Iteration 196/1000 | Loss: 0.00003324
Iteration 197/1000 | Loss: 0.00003324
Iteration 198/1000 | Loss: 0.00003324
Iteration 199/1000 | Loss: 0.00003323
Iteration 200/1000 | Loss: 0.00003323
Iteration 201/1000 | Loss: 0.00003323
Iteration 202/1000 | Loss: 0.00003323
Iteration 203/1000 | Loss: 0.00003322
Iteration 204/1000 | Loss: 0.00003321
Iteration 205/1000 | Loss: 0.00003321
Iteration 206/1000 | Loss: 0.00003320
Iteration 207/1000 | Loss: 0.00003320
Iteration 208/1000 | Loss: 0.00003319
Iteration 209/1000 | Loss: 0.00003319
Iteration 210/1000 | Loss: 0.00003319
Iteration 211/1000 | Loss: 0.00003318
Iteration 212/1000 | Loss: 0.00003318
Iteration 213/1000 | Loss: 0.00003318
Iteration 214/1000 | Loss: 0.00003317
Iteration 215/1000 | Loss: 0.00003317
Iteration 216/1000 | Loss: 0.00003317
Iteration 217/1000 | Loss: 0.00003317
Iteration 218/1000 | Loss: 0.00003316
Iteration 219/1000 | Loss: 0.00003316
Iteration 220/1000 | Loss: 0.00003315
Iteration 221/1000 | Loss: 0.00003315
Iteration 222/1000 | Loss: 0.00003315
Iteration 223/1000 | Loss: 0.00003308
Iteration 224/1000 | Loss: 0.00003308
Iteration 225/1000 | Loss: 0.00003308
Iteration 226/1000 | Loss: 0.00003308
Iteration 227/1000 | Loss: 0.00003308
Iteration 228/1000 | Loss: 0.00003308
Iteration 229/1000 | Loss: 0.00003307
Iteration 230/1000 | Loss: 0.00003307
Iteration 231/1000 | Loss: 0.00003305
Iteration 232/1000 | Loss: 0.00003305
Iteration 233/1000 | Loss: 0.00003305
Iteration 234/1000 | Loss: 0.00003305
Iteration 235/1000 | Loss: 0.00003305
Iteration 236/1000 | Loss: 0.00003305
Iteration 237/1000 | Loss: 0.00003304
Iteration 238/1000 | Loss: 0.00003304
Iteration 239/1000 | Loss: 0.00003301
Iteration 240/1000 | Loss: 0.00003299
Iteration 241/1000 | Loss: 0.00003298
Iteration 242/1000 | Loss: 0.00003298
Iteration 243/1000 | Loss: 0.00003296
Iteration 244/1000 | Loss: 0.00003295
Iteration 245/1000 | Loss: 0.00003294
Iteration 246/1000 | Loss: 0.00003294
Iteration 247/1000 | Loss: 0.00003294
Iteration 248/1000 | Loss: 0.00003294
Iteration 249/1000 | Loss: 0.00003294
Iteration 250/1000 | Loss: 0.00003294
Iteration 251/1000 | Loss: 0.00003294
Iteration 252/1000 | Loss: 0.00003293
Iteration 253/1000 | Loss: 0.00003292
Iteration 254/1000 | Loss: 0.00003292
Iteration 255/1000 | Loss: 0.00003292
Iteration 256/1000 | Loss: 0.00003291
Iteration 257/1000 | Loss: 0.00003290
Iteration 258/1000 | Loss: 0.00003290
Iteration 259/1000 | Loss: 0.00003290
Iteration 260/1000 | Loss: 0.00003290
Iteration 261/1000 | Loss: 0.00003290
Iteration 262/1000 | Loss: 0.00003289
Iteration 263/1000 | Loss: 0.00003289
Iteration 264/1000 | Loss: 0.00003288
Iteration 265/1000 | Loss: 0.00003288
Iteration 266/1000 | Loss: 0.00003288
Iteration 267/1000 | Loss: 0.00003288
Iteration 268/1000 | Loss: 0.00003288
Iteration 269/1000 | Loss: 0.00003287
Iteration 270/1000 | Loss: 0.00003287
Iteration 271/1000 | Loss: 0.00003287
Iteration 272/1000 | Loss: 0.00003287
Iteration 273/1000 | Loss: 0.00003287
Iteration 274/1000 | Loss: 0.00003287
Iteration 275/1000 | Loss: 0.00003287
Iteration 276/1000 | Loss: 0.00003287
Iteration 277/1000 | Loss: 0.00003287
Iteration 278/1000 | Loss: 0.00003287
Iteration 279/1000 | Loss: 0.00003287
Iteration 280/1000 | Loss: 0.00003286
Iteration 281/1000 | Loss: 0.00003286
Iteration 282/1000 | Loss: 0.00003286
Iteration 283/1000 | Loss: 0.00003286
Iteration 284/1000 | Loss: 0.00003286
Iteration 285/1000 | Loss: 0.00003286
Iteration 286/1000 | Loss: 0.00003285
Iteration 287/1000 | Loss: 0.00003285
Iteration 288/1000 | Loss: 0.00003285
Iteration 289/1000 | Loss: 0.00003285
Iteration 290/1000 | Loss: 0.00003284
Iteration 291/1000 | Loss: 0.00003284
Iteration 292/1000 | Loss: 0.00003284
Iteration 293/1000 | Loss: 0.00003283
Iteration 294/1000 | Loss: 0.00003283
Iteration 295/1000 | Loss: 0.00003283
Iteration 296/1000 | Loss: 0.00003283
Iteration 297/1000 | Loss: 0.00003282
Iteration 298/1000 | Loss: 0.00003282
Iteration 299/1000 | Loss: 0.00003282
Iteration 300/1000 | Loss: 0.00003282
Iteration 301/1000 | Loss: 0.00003282
Iteration 302/1000 | Loss: 0.00003281
Iteration 303/1000 | Loss: 0.00003281
Iteration 304/1000 | Loss: 0.00003281
Iteration 305/1000 | Loss: 0.00003281
Iteration 306/1000 | Loss: 0.00003281
Iteration 307/1000 | Loss: 0.00003281
Iteration 308/1000 | Loss: 0.00003281
Iteration 309/1000 | Loss: 0.00003281
Iteration 310/1000 | Loss: 0.00003280
Iteration 311/1000 | Loss: 0.00003280
Iteration 312/1000 | Loss: 0.00003280
Iteration 313/1000 | Loss: 0.00003280
Iteration 314/1000 | Loss: 0.00003280
Iteration 315/1000 | Loss: 0.00003280
Iteration 316/1000 | Loss: 0.00003280
Iteration 317/1000 | Loss: 0.00003280
Iteration 318/1000 | Loss: 0.00003280
Iteration 319/1000 | Loss: 0.00003279
Iteration 320/1000 | Loss: 0.00003279
Iteration 321/1000 | Loss: 0.00003279
Iteration 322/1000 | Loss: 0.00003279
Iteration 323/1000 | Loss: 0.00003279
Iteration 324/1000 | Loss: 0.00003279
Iteration 325/1000 | Loss: 0.00003279
Iteration 326/1000 | Loss: 0.00003279
Iteration 327/1000 | Loss: 0.00003279
Iteration 328/1000 | Loss: 0.00003279
Iteration 329/1000 | Loss: 0.00003279
Iteration 330/1000 | Loss: 0.00003279
Iteration 331/1000 | Loss: 0.00003279
Iteration 332/1000 | Loss: 0.00003279
Iteration 333/1000 | Loss: 0.00003278
Iteration 334/1000 | Loss: 0.00003277
Iteration 335/1000 | Loss: 0.00003277
Iteration 336/1000 | Loss: 0.00003277
Iteration 337/1000 | Loss: 0.00003277
Iteration 338/1000 | Loss: 0.00003277
Iteration 339/1000 | Loss: 0.00003277
Iteration 340/1000 | Loss: 0.00003277
Iteration 341/1000 | Loss: 0.00003277
Iteration 342/1000 | Loss: 0.00003277
Iteration 343/1000 | Loss: 0.00003276
Iteration 344/1000 | Loss: 0.00003276
Iteration 345/1000 | Loss: 0.00003276
Iteration 346/1000 | Loss: 0.00003276
Iteration 347/1000 | Loss: 0.00003276
Iteration 348/1000 | Loss: 0.00003276
Iteration 349/1000 | Loss: 0.00003276
Iteration 350/1000 | Loss: 0.00003275
Iteration 351/1000 | Loss: 0.00003275
Iteration 352/1000 | Loss: 0.00003275
Iteration 353/1000 | Loss: 0.00003275
Iteration 354/1000 | Loss: 0.00003275
Iteration 355/1000 | Loss: 0.00003275
Iteration 356/1000 | Loss: 0.00003275
Iteration 357/1000 | Loss: 0.00003275
Iteration 358/1000 | Loss: 0.00003275
Iteration 359/1000 | Loss: 0.00003274
Iteration 360/1000 | Loss: 0.00003274
Iteration 361/1000 | Loss: 0.00003274
Iteration 362/1000 | Loss: 0.00003274
Iteration 363/1000 | Loss: 0.00003273
Iteration 364/1000 | Loss: 0.00003273
Iteration 365/1000 | Loss: 0.00003273
Iteration 366/1000 | Loss: 0.00003273
Iteration 367/1000 | Loss: 0.00003273
Iteration 368/1000 | Loss: 0.00003273
Iteration 369/1000 | Loss: 0.00003273
Iteration 370/1000 | Loss: 0.00003273
Iteration 371/1000 | Loss: 0.00003273
Iteration 372/1000 | Loss: 0.00003273
Iteration 373/1000 | Loss: 0.00003273
Iteration 374/1000 | Loss: 0.00003272
Iteration 375/1000 | Loss: 0.00003272
Iteration 376/1000 | Loss: 0.00003272
Iteration 377/1000 | Loss: 0.00003272
Iteration 378/1000 | Loss: 0.00003272
Iteration 379/1000 | Loss: 0.00003272
Iteration 380/1000 | Loss: 0.00003272
Iteration 381/1000 | Loss: 0.00003272
Iteration 382/1000 | Loss: 0.00003272
Iteration 383/1000 | Loss: 0.00003272
Iteration 384/1000 | Loss: 0.00003272
Iteration 385/1000 | Loss: 0.00003271
Iteration 386/1000 | Loss: 0.00003271
Iteration 387/1000 | Loss: 0.00003271
Iteration 388/1000 | Loss: 0.00003271
Iteration 389/1000 | Loss: 0.00003271
Iteration 390/1000 | Loss: 0.00003271
Iteration 391/1000 | Loss: 0.00003271
Iteration 392/1000 | Loss: 0.00003271
Iteration 393/1000 | Loss: 0.00003271
Iteration 394/1000 | Loss: 0.00003271
Iteration 395/1000 | Loss: 0.00003271
Iteration 396/1000 | Loss: 0.00003270
Iteration 397/1000 | Loss: 0.00003270
Iteration 398/1000 | Loss: 0.00003270
Iteration 399/1000 | Loss: 0.00003270
Iteration 400/1000 | Loss: 0.00003270
Iteration 401/1000 | Loss: 0.00003270
Iteration 402/1000 | Loss: 0.00003269
Iteration 403/1000 | Loss: 0.00003269
Iteration 404/1000 | Loss: 0.00003269
Iteration 405/1000 | Loss: 0.00003269
Iteration 406/1000 | Loss: 0.00003269
Iteration 407/1000 | Loss: 0.00003269
Iteration 408/1000 | Loss: 0.00003269
Iteration 409/1000 | Loss: 0.00003269
Iteration 410/1000 | Loss: 0.00003268
Iteration 411/1000 | Loss: 0.00003268
Iteration 412/1000 | Loss: 0.00003268
Iteration 413/1000 | Loss: 0.00003268
Iteration 414/1000 | Loss: 0.00003268
Iteration 415/1000 | Loss: 0.00003268
Iteration 416/1000 | Loss: 0.00003268
Iteration 417/1000 | Loss: 0.00003268
Iteration 418/1000 | Loss: 0.00003268
Iteration 419/1000 | Loss: 0.00003268
Iteration 420/1000 | Loss: 0.00003268
Iteration 421/1000 | Loss: 0.00003268
Iteration 422/1000 | Loss: 0.00003268
Iteration 423/1000 | Loss: 0.00003268
Iteration 424/1000 | Loss: 0.00003268
Iteration 425/1000 | Loss: 0.00003267
Iteration 426/1000 | Loss: 0.00003267
Iteration 427/1000 | Loss: 0.00003267
Iteration 428/1000 | Loss: 0.00003267
Iteration 429/1000 | Loss: 0.00003267
Iteration 430/1000 | Loss: 0.00003267
Iteration 431/1000 | Loss: 0.00003267
Iteration 432/1000 | Loss: 0.00003267
Iteration 433/1000 | Loss: 0.00003267
Iteration 434/1000 | Loss: 0.00003267
Iteration 435/1000 | Loss: 0.00003267
Iteration 436/1000 | Loss: 0.00003267
Iteration 437/1000 | Loss: 0.00003267
Iteration 438/1000 | Loss: 0.00003266
Iteration 439/1000 | Loss: 0.00003266
Iteration 440/1000 | Loss: 0.00003266
Iteration 441/1000 | Loss: 0.00003266
Iteration 442/1000 | Loss: 0.00003266
Iteration 443/1000 | Loss: 0.00003266
Iteration 444/1000 | Loss: 0.00003266
Iteration 445/1000 | Loss: 0.00003266
Iteration 446/1000 | Loss: 0.00003266
Iteration 447/1000 | Loss: 0.00003266
Iteration 448/1000 | Loss: 0.00003266
Iteration 449/1000 | Loss: 0.00003266
Iteration 450/1000 | Loss: 0.00003265
Iteration 451/1000 | Loss: 0.00003265
Iteration 452/1000 | Loss: 0.00003265
Iteration 453/1000 | Loss: 0.00003265
Iteration 454/1000 | Loss: 0.00003265
Iteration 455/1000 | Loss: 0.00003265
Iteration 456/1000 | Loss: 0.00003265
Iteration 457/1000 | Loss: 0.00003265
Iteration 458/1000 | Loss: 0.00003265
Iteration 459/1000 | Loss: 0.00003265
Iteration 460/1000 | Loss: 0.00003265
Iteration 461/1000 | Loss: 0.00003265
Iteration 462/1000 | Loss: 0.00003265
Iteration 463/1000 | Loss: 0.00003265
Iteration 464/1000 | Loss: 0.00003265
Iteration 465/1000 | Loss: 0.00003265
Iteration 466/1000 | Loss: 0.00003265
Iteration 467/1000 | Loss: 0.00003265
Iteration 468/1000 | Loss: 0.00003265
Iteration 469/1000 | Loss: 0.00003265
Iteration 470/1000 | Loss: 0.00003265
Iteration 471/1000 | Loss: 0.00003265
Iteration 472/1000 | Loss: 0.00003265
Iteration 473/1000 | Loss: 0.00003265
Iteration 474/1000 | Loss: 0.00003265
Iteration 475/1000 | Loss: 0.00003265
Iteration 476/1000 | Loss: 0.00003265
Iteration 477/1000 | Loss: 0.00003265
Iteration 478/1000 | Loss: 0.00003265
Iteration 479/1000 | Loss: 0.00003265
Iteration 480/1000 | Loss: 0.00003265
Iteration 481/1000 | Loss: 0.00003265
Iteration 482/1000 | Loss: 0.00003265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 482. Stopping optimization.
Last 5 losses: [3.265226769144647e-05, 3.265226769144647e-05, 3.265226769144647e-05, 3.265226769144647e-05, 3.265226769144647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.265226769144647e-05

Optimization complete. Final v2v error: 4.5469584465026855 mm

Highest mean error: 6.411419868469238 mm for frame 206

Lowest mean error: 3.256296157836914 mm for frame 20

Saving results

Total time: 334.11569356918335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797696
Iteration 2/25 | Loss: 0.00131487
Iteration 3/25 | Loss: 0.00101194
Iteration 4/25 | Loss: 0.00094325
Iteration 5/25 | Loss: 0.00092437
Iteration 6/25 | Loss: 0.00093778
Iteration 7/25 | Loss: 0.00095392
Iteration 8/25 | Loss: 0.00089977
Iteration 9/25 | Loss: 0.00087389
Iteration 10/25 | Loss: 0.00086860
Iteration 11/25 | Loss: 0.00086766
Iteration 12/25 | Loss: 0.00086749
Iteration 13/25 | Loss: 0.00086746
Iteration 14/25 | Loss: 0.00086746
Iteration 15/25 | Loss: 0.00086746
Iteration 16/25 | Loss: 0.00086746
Iteration 17/25 | Loss: 0.00086746
Iteration 18/25 | Loss: 0.00086746
Iteration 19/25 | Loss: 0.00086746
Iteration 20/25 | Loss: 0.00086746
Iteration 21/25 | Loss: 0.00086746
Iteration 22/25 | Loss: 0.00086746
Iteration 23/25 | Loss: 0.00086746
Iteration 24/25 | Loss: 0.00086746
Iteration 25/25 | Loss: 0.00086745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44773197
Iteration 2/25 | Loss: 0.00046796
Iteration 3/25 | Loss: 0.00046792
Iteration 4/25 | Loss: 0.00046792
Iteration 5/25 | Loss: 0.00046792
Iteration 6/25 | Loss: 0.00046792
Iteration 7/25 | Loss: 0.00046792
Iteration 8/25 | Loss: 0.00046792
Iteration 9/25 | Loss: 0.00046792
Iteration 10/25 | Loss: 0.00046792
Iteration 11/25 | Loss: 0.00046792
Iteration 12/25 | Loss: 0.00046792
Iteration 13/25 | Loss: 0.00046792
Iteration 14/25 | Loss: 0.00046792
Iteration 15/25 | Loss: 0.00046792
Iteration 16/25 | Loss: 0.00046792
Iteration 17/25 | Loss: 0.00046792
Iteration 18/25 | Loss: 0.00046792
Iteration 19/25 | Loss: 0.00046792
Iteration 20/25 | Loss: 0.00046792
Iteration 21/25 | Loss: 0.00046792
Iteration 22/25 | Loss: 0.00046792
Iteration 23/25 | Loss: 0.00046792
Iteration 24/25 | Loss: 0.00046792
Iteration 25/25 | Loss: 0.00046792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046792
Iteration 2/1000 | Loss: 0.00004766
Iteration 3/1000 | Loss: 0.00003730
Iteration 4/1000 | Loss: 0.00003319
Iteration 5/1000 | Loss: 0.00003133
Iteration 6/1000 | Loss: 0.00003010
Iteration 7/1000 | Loss: 0.00002915
Iteration 8/1000 | Loss: 0.00002842
Iteration 9/1000 | Loss: 0.00002806
Iteration 10/1000 | Loss: 0.00002780
Iteration 11/1000 | Loss: 0.00002750
Iteration 12/1000 | Loss: 0.00002732
Iteration 13/1000 | Loss: 0.00002729
Iteration 14/1000 | Loss: 0.00002721
Iteration 15/1000 | Loss: 0.00002716
Iteration 16/1000 | Loss: 0.00002705
Iteration 17/1000 | Loss: 0.00002702
Iteration 18/1000 | Loss: 0.00002700
Iteration 19/1000 | Loss: 0.00002700
Iteration 20/1000 | Loss: 0.00002699
Iteration 21/1000 | Loss: 0.00002699
Iteration 22/1000 | Loss: 0.00002698
Iteration 23/1000 | Loss: 0.00002698
Iteration 24/1000 | Loss: 0.00002697
Iteration 25/1000 | Loss: 0.00002696
Iteration 26/1000 | Loss: 0.00002696
Iteration 27/1000 | Loss: 0.00002696
Iteration 28/1000 | Loss: 0.00002695
Iteration 29/1000 | Loss: 0.00002694
Iteration 30/1000 | Loss: 0.00002693
Iteration 31/1000 | Loss: 0.00002693
Iteration 32/1000 | Loss: 0.00002692
Iteration 33/1000 | Loss: 0.00002692
Iteration 34/1000 | Loss: 0.00002691
Iteration 35/1000 | Loss: 0.00002691
Iteration 36/1000 | Loss: 0.00002690
Iteration 37/1000 | Loss: 0.00002690
Iteration 38/1000 | Loss: 0.00002690
Iteration 39/1000 | Loss: 0.00002690
Iteration 40/1000 | Loss: 0.00002690
Iteration 41/1000 | Loss: 0.00002690
Iteration 42/1000 | Loss: 0.00002690
Iteration 43/1000 | Loss: 0.00002690
Iteration 44/1000 | Loss: 0.00002690
Iteration 45/1000 | Loss: 0.00002690
Iteration 46/1000 | Loss: 0.00002689
Iteration 47/1000 | Loss: 0.00002689
Iteration 48/1000 | Loss: 0.00002689
Iteration 49/1000 | Loss: 0.00002689
Iteration 50/1000 | Loss: 0.00002688
Iteration 51/1000 | Loss: 0.00002688
Iteration 52/1000 | Loss: 0.00002688
Iteration 53/1000 | Loss: 0.00002687
Iteration 54/1000 | Loss: 0.00002687
Iteration 55/1000 | Loss: 0.00002687
Iteration 56/1000 | Loss: 0.00002687
Iteration 57/1000 | Loss: 0.00002687
Iteration 58/1000 | Loss: 0.00002687
Iteration 59/1000 | Loss: 0.00002687
Iteration 60/1000 | Loss: 0.00002687
Iteration 61/1000 | Loss: 0.00002687
Iteration 62/1000 | Loss: 0.00002687
Iteration 63/1000 | Loss: 0.00002686
Iteration 64/1000 | Loss: 0.00002686
Iteration 65/1000 | Loss: 0.00002686
Iteration 66/1000 | Loss: 0.00002686
Iteration 67/1000 | Loss: 0.00002686
Iteration 68/1000 | Loss: 0.00002686
Iteration 69/1000 | Loss: 0.00002686
Iteration 70/1000 | Loss: 0.00002686
Iteration 71/1000 | Loss: 0.00002686
Iteration 72/1000 | Loss: 0.00002685
Iteration 73/1000 | Loss: 0.00002685
Iteration 74/1000 | Loss: 0.00002685
Iteration 75/1000 | Loss: 0.00002685
Iteration 76/1000 | Loss: 0.00002684
Iteration 77/1000 | Loss: 0.00002684
Iteration 78/1000 | Loss: 0.00002684
Iteration 79/1000 | Loss: 0.00002684
Iteration 80/1000 | Loss: 0.00002683
Iteration 81/1000 | Loss: 0.00002683
Iteration 82/1000 | Loss: 0.00002683
Iteration 83/1000 | Loss: 0.00002683
Iteration 84/1000 | Loss: 0.00002683
Iteration 85/1000 | Loss: 0.00002683
Iteration 86/1000 | Loss: 0.00002683
Iteration 87/1000 | Loss: 0.00002683
Iteration 88/1000 | Loss: 0.00002683
Iteration 89/1000 | Loss: 0.00002683
Iteration 90/1000 | Loss: 0.00002683
Iteration 91/1000 | Loss: 0.00002682
Iteration 92/1000 | Loss: 0.00002682
Iteration 93/1000 | Loss: 0.00002682
Iteration 94/1000 | Loss: 0.00002682
Iteration 95/1000 | Loss: 0.00002681
Iteration 96/1000 | Loss: 0.00002681
Iteration 97/1000 | Loss: 0.00002681
Iteration 98/1000 | Loss: 0.00002680
Iteration 99/1000 | Loss: 0.00002680
Iteration 100/1000 | Loss: 0.00002680
Iteration 101/1000 | Loss: 0.00002680
Iteration 102/1000 | Loss: 0.00002679
Iteration 103/1000 | Loss: 0.00002679
Iteration 104/1000 | Loss: 0.00002679
Iteration 105/1000 | Loss: 0.00002679
Iteration 106/1000 | Loss: 0.00002678
Iteration 107/1000 | Loss: 0.00002678
Iteration 108/1000 | Loss: 0.00002678
Iteration 109/1000 | Loss: 0.00002678
Iteration 110/1000 | Loss: 0.00002678
Iteration 111/1000 | Loss: 0.00002678
Iteration 112/1000 | Loss: 0.00002678
Iteration 113/1000 | Loss: 0.00002678
Iteration 114/1000 | Loss: 0.00002677
Iteration 115/1000 | Loss: 0.00002677
Iteration 116/1000 | Loss: 0.00002677
Iteration 117/1000 | Loss: 0.00002677
Iteration 118/1000 | Loss: 0.00002677
Iteration 119/1000 | Loss: 0.00002677
Iteration 120/1000 | Loss: 0.00002677
Iteration 121/1000 | Loss: 0.00002677
Iteration 122/1000 | Loss: 0.00002677
Iteration 123/1000 | Loss: 0.00002676
Iteration 124/1000 | Loss: 0.00002676
Iteration 125/1000 | Loss: 0.00002676
Iteration 126/1000 | Loss: 0.00002676
Iteration 127/1000 | Loss: 0.00002676
Iteration 128/1000 | Loss: 0.00002676
Iteration 129/1000 | Loss: 0.00002676
Iteration 130/1000 | Loss: 0.00002676
Iteration 131/1000 | Loss: 0.00002676
Iteration 132/1000 | Loss: 0.00002675
Iteration 133/1000 | Loss: 0.00002675
Iteration 134/1000 | Loss: 0.00002675
Iteration 135/1000 | Loss: 0.00002675
Iteration 136/1000 | Loss: 0.00002675
Iteration 137/1000 | Loss: 0.00002675
Iteration 138/1000 | Loss: 0.00002675
Iteration 139/1000 | Loss: 0.00002675
Iteration 140/1000 | Loss: 0.00002675
Iteration 141/1000 | Loss: 0.00002674
Iteration 142/1000 | Loss: 0.00002674
Iteration 143/1000 | Loss: 0.00002674
Iteration 144/1000 | Loss: 0.00002674
Iteration 145/1000 | Loss: 0.00002674
Iteration 146/1000 | Loss: 0.00002674
Iteration 147/1000 | Loss: 0.00002674
Iteration 148/1000 | Loss: 0.00002674
Iteration 149/1000 | Loss: 0.00002673
Iteration 150/1000 | Loss: 0.00002673
Iteration 151/1000 | Loss: 0.00002673
Iteration 152/1000 | Loss: 0.00002673
Iteration 153/1000 | Loss: 0.00002673
Iteration 154/1000 | Loss: 0.00002673
Iteration 155/1000 | Loss: 0.00002673
Iteration 156/1000 | Loss: 0.00002673
Iteration 157/1000 | Loss: 0.00002673
Iteration 158/1000 | Loss: 0.00002673
Iteration 159/1000 | Loss: 0.00002673
Iteration 160/1000 | Loss: 0.00002673
Iteration 161/1000 | Loss: 0.00002672
Iteration 162/1000 | Loss: 0.00002672
Iteration 163/1000 | Loss: 0.00002672
Iteration 164/1000 | Loss: 0.00002672
Iteration 165/1000 | Loss: 0.00002672
Iteration 166/1000 | Loss: 0.00002672
Iteration 167/1000 | Loss: 0.00002672
Iteration 168/1000 | Loss: 0.00002672
Iteration 169/1000 | Loss: 0.00002672
Iteration 170/1000 | Loss: 0.00002672
Iteration 171/1000 | Loss: 0.00002672
Iteration 172/1000 | Loss: 0.00002672
Iteration 173/1000 | Loss: 0.00002672
Iteration 174/1000 | Loss: 0.00002672
Iteration 175/1000 | Loss: 0.00002671
Iteration 176/1000 | Loss: 0.00002671
Iteration 177/1000 | Loss: 0.00002671
Iteration 178/1000 | Loss: 0.00002671
Iteration 179/1000 | Loss: 0.00002671
Iteration 180/1000 | Loss: 0.00002671
Iteration 181/1000 | Loss: 0.00002671
Iteration 182/1000 | Loss: 0.00002671
Iteration 183/1000 | Loss: 0.00002671
Iteration 184/1000 | Loss: 0.00002671
Iteration 185/1000 | Loss: 0.00002670
Iteration 186/1000 | Loss: 0.00002670
Iteration 187/1000 | Loss: 0.00002670
Iteration 188/1000 | Loss: 0.00002670
Iteration 189/1000 | Loss: 0.00002670
Iteration 190/1000 | Loss: 0.00002670
Iteration 191/1000 | Loss: 0.00002670
Iteration 192/1000 | Loss: 0.00002670
Iteration 193/1000 | Loss: 0.00002670
Iteration 194/1000 | Loss: 0.00002670
Iteration 195/1000 | Loss: 0.00002670
Iteration 196/1000 | Loss: 0.00002670
Iteration 197/1000 | Loss: 0.00002670
Iteration 198/1000 | Loss: 0.00002670
Iteration 199/1000 | Loss: 0.00002670
Iteration 200/1000 | Loss: 0.00002670
Iteration 201/1000 | Loss: 0.00002670
Iteration 202/1000 | Loss: 0.00002670
Iteration 203/1000 | Loss: 0.00002670
Iteration 204/1000 | Loss: 0.00002670
Iteration 205/1000 | Loss: 0.00002669
Iteration 206/1000 | Loss: 0.00002669
Iteration 207/1000 | Loss: 0.00002669
Iteration 208/1000 | Loss: 0.00002669
Iteration 209/1000 | Loss: 0.00002669
Iteration 210/1000 | Loss: 0.00002669
Iteration 211/1000 | Loss: 0.00002669
Iteration 212/1000 | Loss: 0.00002669
Iteration 213/1000 | Loss: 0.00002669
Iteration 214/1000 | Loss: 0.00002669
Iteration 215/1000 | Loss: 0.00002669
Iteration 216/1000 | Loss: 0.00002669
Iteration 217/1000 | Loss: 0.00002669
Iteration 218/1000 | Loss: 0.00002669
Iteration 219/1000 | Loss: 0.00002669
Iteration 220/1000 | Loss: 0.00002669
Iteration 221/1000 | Loss: 0.00002669
Iteration 222/1000 | Loss: 0.00002668
Iteration 223/1000 | Loss: 0.00002668
Iteration 224/1000 | Loss: 0.00002668
Iteration 225/1000 | Loss: 0.00002668
Iteration 226/1000 | Loss: 0.00002668
Iteration 227/1000 | Loss: 0.00002668
Iteration 228/1000 | Loss: 0.00002668
Iteration 229/1000 | Loss: 0.00002668
Iteration 230/1000 | Loss: 0.00002668
Iteration 231/1000 | Loss: 0.00002668
Iteration 232/1000 | Loss: 0.00002668
Iteration 233/1000 | Loss: 0.00002668
Iteration 234/1000 | Loss: 0.00002667
Iteration 235/1000 | Loss: 0.00002667
Iteration 236/1000 | Loss: 0.00002667
Iteration 237/1000 | Loss: 0.00002667
Iteration 238/1000 | Loss: 0.00002667
Iteration 239/1000 | Loss: 0.00002667
Iteration 240/1000 | Loss: 0.00002667
Iteration 241/1000 | Loss: 0.00002667
Iteration 242/1000 | Loss: 0.00002667
Iteration 243/1000 | Loss: 0.00002667
Iteration 244/1000 | Loss: 0.00002667
Iteration 245/1000 | Loss: 0.00002667
Iteration 246/1000 | Loss: 0.00002667
Iteration 247/1000 | Loss: 0.00002667
Iteration 248/1000 | Loss: 0.00002667
Iteration 249/1000 | Loss: 0.00002667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.667248190846294e-05, 2.667248190846294e-05, 2.667248190846294e-05, 2.667248190846294e-05, 2.667248190846294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.667248190846294e-05

Optimization complete. Final v2v error: 4.210660457611084 mm

Highest mean error: 4.935474872589111 mm for frame 126

Lowest mean error: 3.6754684448242188 mm for frame 12

Saving results

Total time: 55.25940537452698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013747
Iteration 2/25 | Loss: 0.00157524
Iteration 3/25 | Loss: 0.00093911
Iteration 4/25 | Loss: 0.00085677
Iteration 5/25 | Loss: 0.00084787
Iteration 6/25 | Loss: 0.00084691
Iteration 7/25 | Loss: 0.00084691
Iteration 8/25 | Loss: 0.00084691
Iteration 9/25 | Loss: 0.00084691
Iteration 10/25 | Loss: 0.00084691
Iteration 11/25 | Loss: 0.00084691
Iteration 12/25 | Loss: 0.00084691
Iteration 13/25 | Loss: 0.00084691
Iteration 14/25 | Loss: 0.00084691
Iteration 15/25 | Loss: 0.00084691
Iteration 16/25 | Loss: 0.00084691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008469059830531478, 0.0008469059830531478, 0.0008469059830531478, 0.0008469059830531478, 0.0008469059830531478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008469059830531478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49209952
Iteration 2/25 | Loss: 0.00044163
Iteration 3/25 | Loss: 0.00044163
Iteration 4/25 | Loss: 0.00044163
Iteration 5/25 | Loss: 0.00044163
Iteration 6/25 | Loss: 0.00044163
Iteration 7/25 | Loss: 0.00044163
Iteration 8/25 | Loss: 0.00044163
Iteration 9/25 | Loss: 0.00044163
Iteration 10/25 | Loss: 0.00044163
Iteration 11/25 | Loss: 0.00044163
Iteration 12/25 | Loss: 0.00044163
Iteration 13/25 | Loss: 0.00044163
Iteration 14/25 | Loss: 0.00044163
Iteration 15/25 | Loss: 0.00044163
Iteration 16/25 | Loss: 0.00044163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00044162513222545385, 0.00044162513222545385, 0.00044162513222545385, 0.00044162513222545385, 0.00044162513222545385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044162513222545385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044163
Iteration 2/1000 | Loss: 0.00003067
Iteration 3/1000 | Loss: 0.00002397
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002034
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00002000
Iteration 11/1000 | Loss: 0.00001984
Iteration 12/1000 | Loss: 0.00001979
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001976
Iteration 15/1000 | Loss: 0.00001976
Iteration 16/1000 | Loss: 0.00001976
Iteration 17/1000 | Loss: 0.00001975
Iteration 18/1000 | Loss: 0.00001975
Iteration 19/1000 | Loss: 0.00001974
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001968
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001968
Iteration 24/1000 | Loss: 0.00001968
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001966
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001965
Iteration 31/1000 | Loss: 0.00001965
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001965
Iteration 34/1000 | Loss: 0.00001964
Iteration 35/1000 | Loss: 0.00001964
Iteration 36/1000 | Loss: 0.00001964
Iteration 37/1000 | Loss: 0.00001964
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001964
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001963
Iteration 49/1000 | Loss: 0.00001963
Iteration 50/1000 | Loss: 0.00001963
Iteration 51/1000 | Loss: 0.00001963
Iteration 52/1000 | Loss: 0.00001963
Iteration 53/1000 | Loss: 0.00001962
Iteration 54/1000 | Loss: 0.00001962
Iteration 55/1000 | Loss: 0.00001962
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001961
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001959
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001956
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001956
Iteration 101/1000 | Loss: 0.00001956
Iteration 102/1000 | Loss: 0.00001956
Iteration 103/1000 | Loss: 0.00001956
Iteration 104/1000 | Loss: 0.00001956
Iteration 105/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.9559809516067617e-05, 1.9559809516067617e-05, 1.9559809516067617e-05, 1.9559809516067617e-05, 1.9559809516067617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9559809516067617e-05

Optimization complete. Final v2v error: 3.633565664291382 mm

Highest mean error: 3.6674914360046387 mm for frame 170

Lowest mean error: 3.509857177734375 mm for frame 1

Saving results

Total time: 31.68797254562378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447104
Iteration 2/25 | Loss: 0.00084681
Iteration 3/25 | Loss: 0.00074700
Iteration 4/25 | Loss: 0.00072329
Iteration 5/25 | Loss: 0.00071650
Iteration 6/25 | Loss: 0.00071476
Iteration 7/25 | Loss: 0.00071428
Iteration 8/25 | Loss: 0.00071428
Iteration 9/25 | Loss: 0.00071428
Iteration 10/25 | Loss: 0.00071428
Iteration 11/25 | Loss: 0.00071428
Iteration 12/25 | Loss: 0.00071428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007142812246456742, 0.0007142812246456742, 0.0007142812246456742, 0.0007142812246456742, 0.0007142812246456742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007142812246456742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63504326
Iteration 2/25 | Loss: 0.00043548
Iteration 3/25 | Loss: 0.00043548
Iteration 4/25 | Loss: 0.00043548
Iteration 5/25 | Loss: 0.00043548
Iteration 6/25 | Loss: 0.00043547
Iteration 7/25 | Loss: 0.00043547
Iteration 8/25 | Loss: 0.00043547
Iteration 9/25 | Loss: 0.00043547
Iteration 10/25 | Loss: 0.00043547
Iteration 11/25 | Loss: 0.00043547
Iteration 12/25 | Loss: 0.00043547
Iteration 13/25 | Loss: 0.00043547
Iteration 14/25 | Loss: 0.00043547
Iteration 15/25 | Loss: 0.00043547
Iteration 16/25 | Loss: 0.00043547
Iteration 17/25 | Loss: 0.00043547
Iteration 18/25 | Loss: 0.00043547
Iteration 19/25 | Loss: 0.00043547
Iteration 20/25 | Loss: 0.00043547
Iteration 21/25 | Loss: 0.00043547
Iteration 22/25 | Loss: 0.00043547
Iteration 23/25 | Loss: 0.00043547
Iteration 24/25 | Loss: 0.00043547
Iteration 25/25 | Loss: 0.00043547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043547
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001517
Iteration 6/1000 | Loss: 0.00001439
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001364
Iteration 11/1000 | Loss: 0.00001360
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001332
Iteration 16/1000 | Loss: 0.00001331
Iteration 17/1000 | Loss: 0.00001331
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001330
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001303
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001299
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001290
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001288
Iteration 80/1000 | Loss: 0.00001288
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001287
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001282
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001282
Iteration 116/1000 | Loss: 0.00001282
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001282
Iteration 120/1000 | Loss: 0.00001282
Iteration 121/1000 | Loss: 0.00001282
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001281
Iteration 124/1000 | Loss: 0.00001281
Iteration 125/1000 | Loss: 0.00001281
Iteration 126/1000 | Loss: 0.00001281
Iteration 127/1000 | Loss: 0.00001281
Iteration 128/1000 | Loss: 0.00001281
Iteration 129/1000 | Loss: 0.00001281
Iteration 130/1000 | Loss: 0.00001281
Iteration 131/1000 | Loss: 0.00001281
Iteration 132/1000 | Loss: 0.00001281
Iteration 133/1000 | Loss: 0.00001281
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001281
Iteration 136/1000 | Loss: 0.00001280
Iteration 137/1000 | Loss: 0.00001280
Iteration 138/1000 | Loss: 0.00001280
Iteration 139/1000 | Loss: 0.00001280
Iteration 140/1000 | Loss: 0.00001280
Iteration 141/1000 | Loss: 0.00001280
Iteration 142/1000 | Loss: 0.00001280
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001280
Iteration 149/1000 | Loss: 0.00001280
Iteration 150/1000 | Loss: 0.00001279
Iteration 151/1000 | Loss: 0.00001279
Iteration 152/1000 | Loss: 0.00001279
Iteration 153/1000 | Loss: 0.00001279
Iteration 154/1000 | Loss: 0.00001279
Iteration 155/1000 | Loss: 0.00001279
Iteration 156/1000 | Loss: 0.00001279
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001278
Iteration 163/1000 | Loss: 0.00001278
Iteration 164/1000 | Loss: 0.00001278
Iteration 165/1000 | Loss: 0.00001278
Iteration 166/1000 | Loss: 0.00001278
Iteration 167/1000 | Loss: 0.00001278
Iteration 168/1000 | Loss: 0.00001278
Iteration 169/1000 | Loss: 0.00001278
Iteration 170/1000 | Loss: 0.00001278
Iteration 171/1000 | Loss: 0.00001278
Iteration 172/1000 | Loss: 0.00001278
Iteration 173/1000 | Loss: 0.00001278
Iteration 174/1000 | Loss: 0.00001277
Iteration 175/1000 | Loss: 0.00001277
Iteration 176/1000 | Loss: 0.00001277
Iteration 177/1000 | Loss: 0.00001277
Iteration 178/1000 | Loss: 0.00001277
Iteration 179/1000 | Loss: 0.00001277
Iteration 180/1000 | Loss: 0.00001277
Iteration 181/1000 | Loss: 0.00001277
Iteration 182/1000 | Loss: 0.00001277
Iteration 183/1000 | Loss: 0.00001277
Iteration 184/1000 | Loss: 0.00001277
Iteration 185/1000 | Loss: 0.00001277
Iteration 186/1000 | Loss: 0.00001277
Iteration 187/1000 | Loss: 0.00001277
Iteration 188/1000 | Loss: 0.00001277
Iteration 189/1000 | Loss: 0.00001277
Iteration 190/1000 | Loss: 0.00001277
Iteration 191/1000 | Loss: 0.00001277
Iteration 192/1000 | Loss: 0.00001277
Iteration 193/1000 | Loss: 0.00001277
Iteration 194/1000 | Loss: 0.00001277
Iteration 195/1000 | Loss: 0.00001277
Iteration 196/1000 | Loss: 0.00001277
Iteration 197/1000 | Loss: 0.00001277
Iteration 198/1000 | Loss: 0.00001277
Iteration 199/1000 | Loss: 0.00001277
Iteration 200/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.2766601685143542e-05, 1.2766601685143542e-05, 1.2766601685143542e-05, 1.2766601685143542e-05, 1.2766601685143542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2766601685143542e-05

Optimization complete. Final v2v error: 3.058215379714966 mm

Highest mean error: 3.2414824962615967 mm for frame 30

Lowest mean error: 2.8135550022125244 mm for frame 15

Saving results

Total time: 55.925113677978516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690738
Iteration 2/25 | Loss: 0.00155701
Iteration 3/25 | Loss: 0.00130435
Iteration 4/25 | Loss: 0.00117847
Iteration 5/25 | Loss: 0.00098238
Iteration 6/25 | Loss: 0.00091467
Iteration 7/25 | Loss: 0.00085157
Iteration 8/25 | Loss: 0.00083968
Iteration 9/25 | Loss: 0.00083551
Iteration 10/25 | Loss: 0.00083020
Iteration 11/25 | Loss: 0.00084599
Iteration 12/25 | Loss: 0.00082225
Iteration 13/25 | Loss: 0.00082053
Iteration 14/25 | Loss: 0.00082026
Iteration 15/25 | Loss: 0.00082025
Iteration 16/25 | Loss: 0.00082025
Iteration 17/25 | Loss: 0.00082025
Iteration 18/25 | Loss: 0.00082025
Iteration 19/25 | Loss: 0.00082025
Iteration 20/25 | Loss: 0.00082025
Iteration 21/25 | Loss: 0.00082025
Iteration 22/25 | Loss: 0.00082025
Iteration 23/25 | Loss: 0.00082025
Iteration 24/25 | Loss: 0.00082025
Iteration 25/25 | Loss: 0.00082024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29474878
Iteration 2/25 | Loss: 0.00055605
Iteration 3/25 | Loss: 0.00055605
Iteration 4/25 | Loss: 0.00055605
Iteration 5/25 | Loss: 0.00055605
Iteration 6/25 | Loss: 0.00055605
Iteration 7/25 | Loss: 0.00055605
Iteration 8/25 | Loss: 0.00055120
Iteration 9/25 | Loss: 0.00055120
Iteration 10/25 | Loss: 0.00055120
Iteration 11/25 | Loss: 0.00055120
Iteration 12/25 | Loss: 0.00055120
Iteration 13/25 | Loss: 0.00055120
Iteration 14/25 | Loss: 0.00055120
Iteration 15/25 | Loss: 0.00055119
Iteration 16/25 | Loss: 0.00055119
Iteration 17/25 | Loss: 0.00055119
Iteration 18/25 | Loss: 0.00055119
Iteration 19/25 | Loss: 0.00055119
Iteration 20/25 | Loss: 0.00055119
Iteration 21/25 | Loss: 0.00055119
Iteration 22/25 | Loss: 0.00055119
Iteration 23/25 | Loss: 0.00055119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005511943018063903, 0.0005511943018063903, 0.0005511943018063903, 0.0005511943018063903, 0.0005511943018063903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005511943018063903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055119
Iteration 2/1000 | Loss: 0.00004695
Iteration 3/1000 | Loss: 0.00002929
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002250
Iteration 7/1000 | Loss: 0.00002158
Iteration 8/1000 | Loss: 0.00002103
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002020
Iteration 11/1000 | Loss: 0.00001996
Iteration 12/1000 | Loss: 0.00001976
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001932
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001925
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001924
Iteration 20/1000 | Loss: 0.00001923
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001923
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001922
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001921
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001921
Iteration 33/1000 | Loss: 0.00001921
Iteration 34/1000 | Loss: 0.00001920
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001920
Iteration 37/1000 | Loss: 0.00001920
Iteration 38/1000 | Loss: 0.00001920
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001919
Iteration 44/1000 | Loss: 0.00001919
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00001916
Iteration 49/1000 | Loss: 0.00001916
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001913
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001910
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001909
Iteration 73/1000 | Loss: 0.00001909
Iteration 74/1000 | Loss: 0.00001909
Iteration 75/1000 | Loss: 0.00001909
Iteration 76/1000 | Loss: 0.00001909
Iteration 77/1000 | Loss: 0.00001909
Iteration 78/1000 | Loss: 0.00001909
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001906
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001905
Iteration 90/1000 | Loss: 0.00001905
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001902
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001902
Iteration 102/1000 | Loss: 0.00001901
Iteration 103/1000 | Loss: 0.00001901
Iteration 104/1000 | Loss: 0.00001901
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001898
Iteration 111/1000 | Loss: 0.00001898
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001898
Iteration 114/1000 | Loss: 0.00001898
Iteration 115/1000 | Loss: 0.00001898
Iteration 116/1000 | Loss: 0.00001898
Iteration 117/1000 | Loss: 0.00001898
Iteration 118/1000 | Loss: 0.00001898
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001897
Iteration 121/1000 | Loss: 0.00001897
Iteration 122/1000 | Loss: 0.00001897
Iteration 123/1000 | Loss: 0.00001897
Iteration 124/1000 | Loss: 0.00001896
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00001896
Iteration 127/1000 | Loss: 0.00001896
Iteration 128/1000 | Loss: 0.00001896
Iteration 129/1000 | Loss: 0.00001896
Iteration 130/1000 | Loss: 0.00001896
Iteration 131/1000 | Loss: 0.00001895
Iteration 132/1000 | Loss: 0.00001895
Iteration 133/1000 | Loss: 0.00001895
Iteration 134/1000 | Loss: 0.00001894
Iteration 135/1000 | Loss: 0.00001894
Iteration 136/1000 | Loss: 0.00001894
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001891
Iteration 149/1000 | Loss: 0.00001891
Iteration 150/1000 | Loss: 0.00001891
Iteration 151/1000 | Loss: 0.00001891
Iteration 152/1000 | Loss: 0.00001891
Iteration 153/1000 | Loss: 0.00001891
Iteration 154/1000 | Loss: 0.00001891
Iteration 155/1000 | Loss: 0.00001891
Iteration 156/1000 | Loss: 0.00001891
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001890
Iteration 160/1000 | Loss: 0.00001890
Iteration 161/1000 | Loss: 0.00001890
Iteration 162/1000 | Loss: 0.00001890
Iteration 163/1000 | Loss: 0.00001889
Iteration 164/1000 | Loss: 0.00001889
Iteration 165/1000 | Loss: 0.00001889
Iteration 166/1000 | Loss: 0.00001889
Iteration 167/1000 | Loss: 0.00001889
Iteration 168/1000 | Loss: 0.00001889
Iteration 169/1000 | Loss: 0.00001889
Iteration 170/1000 | Loss: 0.00001889
Iteration 171/1000 | Loss: 0.00001889
Iteration 172/1000 | Loss: 0.00001888
Iteration 173/1000 | Loss: 0.00001888
Iteration 174/1000 | Loss: 0.00001888
Iteration 175/1000 | Loss: 0.00001888
Iteration 176/1000 | Loss: 0.00001888
Iteration 177/1000 | Loss: 0.00001888
Iteration 178/1000 | Loss: 0.00001888
Iteration 179/1000 | Loss: 0.00001887
Iteration 180/1000 | Loss: 0.00001887
Iteration 181/1000 | Loss: 0.00001887
Iteration 182/1000 | Loss: 0.00001887
Iteration 183/1000 | Loss: 0.00001887
Iteration 184/1000 | Loss: 0.00001887
Iteration 185/1000 | Loss: 0.00001886
Iteration 186/1000 | Loss: 0.00001886
Iteration 187/1000 | Loss: 0.00001886
Iteration 188/1000 | Loss: 0.00001886
Iteration 189/1000 | Loss: 0.00001886
Iteration 190/1000 | Loss: 0.00001886
Iteration 191/1000 | Loss: 0.00001886
Iteration 192/1000 | Loss: 0.00001886
Iteration 193/1000 | Loss: 0.00001885
Iteration 194/1000 | Loss: 0.00001885
Iteration 195/1000 | Loss: 0.00001885
Iteration 196/1000 | Loss: 0.00001885
Iteration 197/1000 | Loss: 0.00001885
Iteration 198/1000 | Loss: 0.00001885
Iteration 199/1000 | Loss: 0.00001885
Iteration 200/1000 | Loss: 0.00001885
Iteration 201/1000 | Loss: 0.00001885
Iteration 202/1000 | Loss: 0.00001885
Iteration 203/1000 | Loss: 0.00001885
Iteration 204/1000 | Loss: 0.00001885
Iteration 205/1000 | Loss: 0.00001885
Iteration 206/1000 | Loss: 0.00001885
Iteration 207/1000 | Loss: 0.00001885
Iteration 208/1000 | Loss: 0.00001885
Iteration 209/1000 | Loss: 0.00001885
Iteration 210/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.8845386875909753e-05, 1.8845386875909753e-05, 1.8845386875909753e-05, 1.8845386875909753e-05, 1.8845386875909753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8845386875909753e-05

Optimization complete. Final v2v error: 3.5112195014953613 mm

Highest mean error: 5.719799518585205 mm for frame 12

Lowest mean error: 2.597435235977173 mm for frame 178

Saving results

Total time: 90.84628939628601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062309
Iteration 2/25 | Loss: 0.00270792
Iteration 3/25 | Loss: 0.00164233
Iteration 4/25 | Loss: 0.00127397
Iteration 5/25 | Loss: 0.00115972
Iteration 6/25 | Loss: 0.00116807
Iteration 7/25 | Loss: 0.00109735
Iteration 8/25 | Loss: 0.00106383
Iteration 9/25 | Loss: 0.00099704
Iteration 10/25 | Loss: 0.00098787
Iteration 11/25 | Loss: 0.00095870
Iteration 12/25 | Loss: 0.00094432
Iteration 13/25 | Loss: 0.00093618
Iteration 14/25 | Loss: 0.00092722
Iteration 15/25 | Loss: 0.00092766
Iteration 16/25 | Loss: 0.00092831
Iteration 17/25 | Loss: 0.00092314
Iteration 18/25 | Loss: 0.00092625
Iteration 19/25 | Loss: 0.00091909
Iteration 20/25 | Loss: 0.00091959
Iteration 21/25 | Loss: 0.00091953
Iteration 22/25 | Loss: 0.00092017
Iteration 23/25 | Loss: 0.00092503
Iteration 24/25 | Loss: 0.00092545
Iteration 25/25 | Loss: 0.00091770

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53204143
Iteration 2/25 | Loss: 0.00191890
Iteration 3/25 | Loss: 0.00158474
Iteration 4/25 | Loss: 0.00158474
Iteration 5/25 | Loss: 0.00158474
Iteration 6/25 | Loss: 0.00158474
Iteration 7/25 | Loss: 0.00158474
Iteration 8/25 | Loss: 0.00158474
Iteration 9/25 | Loss: 0.00158474
Iteration 10/25 | Loss: 0.00158474
Iteration 11/25 | Loss: 0.00158474
Iteration 12/25 | Loss: 0.00158474
Iteration 13/25 | Loss: 0.00158474
Iteration 14/25 | Loss: 0.00158474
Iteration 15/25 | Loss: 0.00158474
Iteration 16/25 | Loss: 0.00158474
Iteration 17/25 | Loss: 0.00158474
Iteration 18/25 | Loss: 0.00158474
Iteration 19/25 | Loss: 0.00158474
Iteration 20/25 | Loss: 0.00158474
Iteration 21/25 | Loss: 0.00158474
Iteration 22/25 | Loss: 0.00158474
Iteration 23/25 | Loss: 0.00158474
Iteration 24/25 | Loss: 0.00158474
Iteration 25/25 | Loss: 0.00158474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158474
Iteration 2/1000 | Loss: 0.00070930
Iteration 3/1000 | Loss: 0.00012569
Iteration 4/1000 | Loss: 0.00107385
Iteration 5/1000 | Loss: 0.00188247
Iteration 6/1000 | Loss: 0.00135669
Iteration 7/1000 | Loss: 0.00031633
Iteration 8/1000 | Loss: 0.00158060
Iteration 9/1000 | Loss: 0.00075100
Iteration 10/1000 | Loss: 0.00010978
Iteration 11/1000 | Loss: 0.00123115
Iteration 12/1000 | Loss: 0.00036500
Iteration 13/1000 | Loss: 0.00208642
Iteration 14/1000 | Loss: 0.00071826
Iteration 15/1000 | Loss: 0.00132450
Iteration 16/1000 | Loss: 0.00026983
Iteration 17/1000 | Loss: 0.00094433
Iteration 18/1000 | Loss: 0.00068803
Iteration 19/1000 | Loss: 0.00045298
Iteration 20/1000 | Loss: 0.00020020
Iteration 21/1000 | Loss: 0.00111432
Iteration 22/1000 | Loss: 0.00027948
Iteration 23/1000 | Loss: 0.00199226
Iteration 24/1000 | Loss: 0.00124788
Iteration 25/1000 | Loss: 0.00011718
Iteration 26/1000 | Loss: 0.00013387
Iteration 27/1000 | Loss: 0.00030027
Iteration 28/1000 | Loss: 0.00012296
Iteration 29/1000 | Loss: 0.00023489
Iteration 30/1000 | Loss: 0.00022869
Iteration 31/1000 | Loss: 0.00009052
Iteration 32/1000 | Loss: 0.00007499
Iteration 33/1000 | Loss: 0.00006733
Iteration 34/1000 | Loss: 0.00034517
Iteration 35/1000 | Loss: 0.00503055
Iteration 36/1000 | Loss: 0.00269866
Iteration 37/1000 | Loss: 0.00186985
Iteration 38/1000 | Loss: 0.00274700
Iteration 39/1000 | Loss: 0.00258237
Iteration 40/1000 | Loss: 0.00106971
Iteration 41/1000 | Loss: 0.00101969
Iteration 42/1000 | Loss: 0.00091678
Iteration 43/1000 | Loss: 0.00151332
Iteration 44/1000 | Loss: 0.00005640
Iteration 45/1000 | Loss: 0.00004376
Iteration 46/1000 | Loss: 0.00028892
Iteration 47/1000 | Loss: 0.00076048
Iteration 48/1000 | Loss: 0.00030079
Iteration 49/1000 | Loss: 0.00029535
Iteration 50/1000 | Loss: 0.00004143
Iteration 51/1000 | Loss: 0.00002574
Iteration 52/1000 | Loss: 0.00026458
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00025168
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00020364
Iteration 58/1000 | Loss: 0.00006977
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00015287
Iteration 61/1000 | Loss: 0.00016430
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00021890
Iteration 64/1000 | Loss: 0.00015045
Iteration 65/1000 | Loss: 0.00011489
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001372
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00016841
Iteration 70/1000 | Loss: 0.00001882
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00019115
Iteration 77/1000 | Loss: 0.00008371
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001287
Iteration 86/1000 | Loss: 0.00015639
Iteration 87/1000 | Loss: 0.00010603
Iteration 88/1000 | Loss: 0.00009260
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001289
Iteration 93/1000 | Loss: 0.00001288
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001276
Iteration 113/1000 | Loss: 0.00001276
Iteration 114/1000 | Loss: 0.00001275
Iteration 115/1000 | Loss: 0.00001275
Iteration 116/1000 | Loss: 0.00001275
Iteration 117/1000 | Loss: 0.00001275
Iteration 118/1000 | Loss: 0.00001275
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001274
Iteration 125/1000 | Loss: 0.00001274
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001274
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001273
Iteration 141/1000 | Loss: 0.00001273
Iteration 142/1000 | Loss: 0.00001273
Iteration 143/1000 | Loss: 0.00001273
Iteration 144/1000 | Loss: 0.00001273
Iteration 145/1000 | Loss: 0.00001273
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.2732444702123757e-05, 1.2732444702123757e-05, 1.2732444702123757e-05, 1.2732444702123757e-05, 1.2732444702123757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2732444702123757e-05

Optimization complete. Final v2v error: 2.986628770828247 mm

Highest mean error: 3.851005792617798 mm for frame 62

Lowest mean error: 2.5302975177764893 mm for frame 111

Saving results

Total time: 202.6108329296112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854277
Iteration 2/25 | Loss: 0.00141802
Iteration 3/25 | Loss: 0.00093037
Iteration 4/25 | Loss: 0.00083386
Iteration 5/25 | Loss: 0.00081446
Iteration 6/25 | Loss: 0.00080370
Iteration 7/25 | Loss: 0.00080165
Iteration 8/25 | Loss: 0.00079437
Iteration 9/25 | Loss: 0.00079361
Iteration 10/25 | Loss: 0.00079341
Iteration 11/25 | Loss: 0.00079311
Iteration 12/25 | Loss: 0.00079157
Iteration 13/25 | Loss: 0.00078944
Iteration 14/25 | Loss: 0.00078950
Iteration 15/25 | Loss: 0.00078811
Iteration 16/25 | Loss: 0.00078752
Iteration 17/25 | Loss: 0.00078727
Iteration 18/25 | Loss: 0.00078720
Iteration 19/25 | Loss: 0.00078719
Iteration 20/25 | Loss: 0.00078719
Iteration 21/25 | Loss: 0.00078719
Iteration 22/25 | Loss: 0.00078719
Iteration 23/25 | Loss: 0.00078717
Iteration 24/25 | Loss: 0.00078716
Iteration 25/25 | Loss: 0.00078716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97686625
Iteration 2/25 | Loss: 0.00046708
Iteration 3/25 | Loss: 0.00046708
Iteration 4/25 | Loss: 0.00046708
Iteration 5/25 | Loss: 0.00046708
Iteration 6/25 | Loss: 0.00046708
Iteration 7/25 | Loss: 0.00046708
Iteration 8/25 | Loss: 0.00046708
Iteration 9/25 | Loss: 0.00046708
Iteration 10/25 | Loss: 0.00046708
Iteration 11/25 | Loss: 0.00046708
Iteration 12/25 | Loss: 0.00046708
Iteration 13/25 | Loss: 0.00046708
Iteration 14/25 | Loss: 0.00046708
Iteration 15/25 | Loss: 0.00046707
Iteration 16/25 | Loss: 0.00046707
Iteration 17/25 | Loss: 0.00046707
Iteration 18/25 | Loss: 0.00046707
Iteration 19/25 | Loss: 0.00046707
Iteration 20/25 | Loss: 0.00046707
Iteration 21/25 | Loss: 0.00046707
Iteration 22/25 | Loss: 0.00046707
Iteration 23/25 | Loss: 0.00046707
Iteration 24/25 | Loss: 0.00046707
Iteration 25/25 | Loss: 0.00046707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046707
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001871
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001585
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001554
Iteration 16/1000 | Loss: 0.00001553
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001550
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001549
Iteration 21/1000 | Loss: 0.00001548
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001542
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001541
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001539
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001533
Iteration 49/1000 | Loss: 0.00001533
Iteration 50/1000 | Loss: 0.00001533
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001522
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001522
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001522
Iteration 111/1000 | Loss: 0.00001522
Iteration 112/1000 | Loss: 0.00001522
Iteration 113/1000 | Loss: 0.00001522
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001522
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001522
Iteration 125/1000 | Loss: 0.00001522
Iteration 126/1000 | Loss: 0.00001522
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001522
Iteration 129/1000 | Loss: 0.00001522
Iteration 130/1000 | Loss: 0.00001522
Iteration 131/1000 | Loss: 0.00001522
Iteration 132/1000 | Loss: 0.00001522
Iteration 133/1000 | Loss: 0.00001522
Iteration 134/1000 | Loss: 0.00001522
Iteration 135/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.5222362890199292e-05, 1.5222362890199292e-05, 1.5222362890199292e-05, 1.5222362890199292e-05, 1.5222362890199292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5222362890199292e-05

Optimization complete. Final v2v error: 3.3222365379333496 mm

Highest mean error: 3.61862850189209 mm for frame 4

Lowest mean error: 3.0273895263671875 mm for frame 234

Saving results

Total time: 61.99537658691406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060759
Iteration 2/25 | Loss: 0.01060759
Iteration 3/25 | Loss: 0.01060758
Iteration 4/25 | Loss: 0.00290097
Iteration 5/25 | Loss: 0.00179242
Iteration 6/25 | Loss: 0.00140050
Iteration 7/25 | Loss: 0.00135023
Iteration 8/25 | Loss: 0.00116105
Iteration 9/25 | Loss: 0.00114168
Iteration 10/25 | Loss: 0.00109859
Iteration 11/25 | Loss: 0.00103574
Iteration 12/25 | Loss: 0.00097663
Iteration 13/25 | Loss: 0.00096095
Iteration 14/25 | Loss: 0.00095376
Iteration 15/25 | Loss: 0.00094645
Iteration 16/25 | Loss: 0.00094057
Iteration 17/25 | Loss: 0.00094483
Iteration 18/25 | Loss: 0.00092506
Iteration 19/25 | Loss: 0.00091001
Iteration 20/25 | Loss: 0.00090004
Iteration 21/25 | Loss: 0.00089469
Iteration 22/25 | Loss: 0.00087656
Iteration 23/25 | Loss: 0.00086574
Iteration 24/25 | Loss: 0.00086051
Iteration 25/25 | Loss: 0.00085733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51937830
Iteration 2/25 | Loss: 0.00220236
Iteration 3/25 | Loss: 0.00149968
Iteration 4/25 | Loss: 0.00149968
Iteration 5/25 | Loss: 0.00149968
Iteration 6/25 | Loss: 0.00149967
Iteration 7/25 | Loss: 0.00149967
Iteration 8/25 | Loss: 0.00149967
Iteration 9/25 | Loss: 0.00149967
Iteration 10/25 | Loss: 0.00149967
Iteration 11/25 | Loss: 0.00149967
Iteration 12/25 | Loss: 0.00149967
Iteration 13/25 | Loss: 0.00149967
Iteration 14/25 | Loss: 0.00149967
Iteration 15/25 | Loss: 0.00149967
Iteration 16/25 | Loss: 0.00149967
Iteration 17/25 | Loss: 0.00149967
Iteration 18/25 | Loss: 0.00149967
Iteration 19/25 | Loss: 0.00149967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014996725367382169, 0.0014996725367382169, 0.0014996725367382169, 0.0014996725367382169, 0.0014996725367382169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014996725367382169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149967
Iteration 2/1000 | Loss: 0.00094503
Iteration 3/1000 | Loss: 0.00035782
Iteration 4/1000 | Loss: 0.00019100
Iteration 5/1000 | Loss: 0.00010083
Iteration 6/1000 | Loss: 0.00013941
Iteration 7/1000 | Loss: 0.00008175
Iteration 8/1000 | Loss: 0.00023530
Iteration 9/1000 | Loss: 0.00016666
Iteration 10/1000 | Loss: 0.00031901
Iteration 11/1000 | Loss: 0.00011640
Iteration 12/1000 | Loss: 0.00020089
Iteration 13/1000 | Loss: 0.00011555
Iteration 14/1000 | Loss: 0.00008868
Iteration 15/1000 | Loss: 0.00006800
Iteration 16/1000 | Loss: 0.00011589
Iteration 17/1000 | Loss: 0.00051262
Iteration 18/1000 | Loss: 0.00039869
Iteration 19/1000 | Loss: 0.00009141
Iteration 20/1000 | Loss: 0.00007274
Iteration 21/1000 | Loss: 0.00006617
Iteration 22/1000 | Loss: 0.00011136
Iteration 23/1000 | Loss: 0.00017765
Iteration 24/1000 | Loss: 0.00010546
Iteration 25/1000 | Loss: 0.00005432
Iteration 26/1000 | Loss: 0.00005296
Iteration 27/1000 | Loss: 0.00005183
Iteration 28/1000 | Loss: 0.00005106
Iteration 29/1000 | Loss: 0.00013027
Iteration 30/1000 | Loss: 0.00005193
Iteration 31/1000 | Loss: 0.00004982
Iteration 32/1000 | Loss: 0.00004935
Iteration 33/1000 | Loss: 0.00004890
Iteration 34/1000 | Loss: 0.00027673
Iteration 35/1000 | Loss: 0.00018630
Iteration 36/1000 | Loss: 0.00028088
Iteration 37/1000 | Loss: 0.00020294
Iteration 38/1000 | Loss: 0.00021810
Iteration 39/1000 | Loss: 0.00031517
Iteration 40/1000 | Loss: 0.00025857
Iteration 41/1000 | Loss: 0.00013986
Iteration 42/1000 | Loss: 0.00005744
Iteration 43/1000 | Loss: 0.00005172
Iteration 44/1000 | Loss: 0.00027824
Iteration 45/1000 | Loss: 0.00016375
Iteration 46/1000 | Loss: 0.00017597
Iteration 47/1000 | Loss: 0.00015062
Iteration 48/1000 | Loss: 0.00023661
Iteration 49/1000 | Loss: 0.00012815
Iteration 50/1000 | Loss: 0.00022426
Iteration 51/1000 | Loss: 0.00011934
Iteration 52/1000 | Loss: 0.00008292
Iteration 53/1000 | Loss: 0.00005229
Iteration 54/1000 | Loss: 0.00004990
Iteration 55/1000 | Loss: 0.00004881
Iteration 56/1000 | Loss: 0.00029097
Iteration 57/1000 | Loss: 0.00016127
Iteration 58/1000 | Loss: 0.00027526
Iteration 59/1000 | Loss: 0.00016570
Iteration 60/1000 | Loss: 0.00015164
Iteration 61/1000 | Loss: 0.00026348
Iteration 62/1000 | Loss: 0.00039962
Iteration 63/1000 | Loss: 0.00016166
Iteration 64/1000 | Loss: 0.00005705
Iteration 65/1000 | Loss: 0.00004809
Iteration 66/1000 | Loss: 0.00004732
Iteration 67/1000 | Loss: 0.00037884
Iteration 68/1000 | Loss: 0.00006256
Iteration 69/1000 | Loss: 0.00008625
Iteration 70/1000 | Loss: 0.00019553
Iteration 71/1000 | Loss: 0.00017000
Iteration 72/1000 | Loss: 0.00005468
Iteration 73/1000 | Loss: 0.00004944
Iteration 74/1000 | Loss: 0.00004642
Iteration 75/1000 | Loss: 0.00013450
Iteration 76/1000 | Loss: 0.00004685
Iteration 77/1000 | Loss: 0.00004249
Iteration 78/1000 | Loss: 0.00004128
Iteration 79/1000 | Loss: 0.00004071
Iteration 80/1000 | Loss: 0.00004015
Iteration 81/1000 | Loss: 0.00037837
Iteration 82/1000 | Loss: 0.00021647
Iteration 83/1000 | Loss: 0.00005454
Iteration 84/1000 | Loss: 0.00030546
Iteration 85/1000 | Loss: 0.00006075
Iteration 86/1000 | Loss: 0.00005301
Iteration 87/1000 | Loss: 0.00004229
Iteration 88/1000 | Loss: 0.00003973
Iteration 89/1000 | Loss: 0.00003904
Iteration 90/1000 | Loss: 0.00026419
Iteration 91/1000 | Loss: 0.00008042
Iteration 92/1000 | Loss: 0.00003980
Iteration 93/1000 | Loss: 0.00025836
Iteration 94/1000 | Loss: 0.00006840
Iteration 95/1000 | Loss: 0.00003972
Iteration 96/1000 | Loss: 0.00026143
Iteration 97/1000 | Loss: 0.00006535
Iteration 98/1000 | Loss: 0.00003949
Iteration 99/1000 | Loss: 0.00003853
Iteration 100/1000 | Loss: 0.00026220
Iteration 101/1000 | Loss: 0.00020374
Iteration 102/1000 | Loss: 0.00031753
Iteration 103/1000 | Loss: 0.00010016
Iteration 104/1000 | Loss: 0.00019708
Iteration 105/1000 | Loss: 0.00074724
Iteration 106/1000 | Loss: 0.00030235
Iteration 107/1000 | Loss: 0.00006978
Iteration 108/1000 | Loss: 0.00005222
Iteration 109/1000 | Loss: 0.00004437
Iteration 110/1000 | Loss: 0.00003915
Iteration 111/1000 | Loss: 0.00003580
Iteration 112/1000 | Loss: 0.00004807
Iteration 113/1000 | Loss: 0.00003306
Iteration 114/1000 | Loss: 0.00003121
Iteration 115/1000 | Loss: 0.00003046
Iteration 116/1000 | Loss: 0.00012084
Iteration 117/1000 | Loss: 0.00002993
Iteration 118/1000 | Loss: 0.00002947
Iteration 119/1000 | Loss: 0.00002913
Iteration 120/1000 | Loss: 0.00011934
Iteration 121/1000 | Loss: 0.00003574
Iteration 122/1000 | Loss: 0.00006274
Iteration 123/1000 | Loss: 0.00002875
Iteration 124/1000 | Loss: 0.00002873
Iteration 125/1000 | Loss: 0.00002872
Iteration 126/1000 | Loss: 0.00002869
Iteration 127/1000 | Loss: 0.00002869
Iteration 128/1000 | Loss: 0.00002869
Iteration 129/1000 | Loss: 0.00002869
Iteration 130/1000 | Loss: 0.00002868
Iteration 131/1000 | Loss: 0.00002865
Iteration 132/1000 | Loss: 0.00002864
Iteration 133/1000 | Loss: 0.00002863
Iteration 134/1000 | Loss: 0.00002863
Iteration 135/1000 | Loss: 0.00002862
Iteration 136/1000 | Loss: 0.00002862
Iteration 137/1000 | Loss: 0.00002861
Iteration 138/1000 | Loss: 0.00002860
Iteration 139/1000 | Loss: 0.00002860
Iteration 140/1000 | Loss: 0.00002859
Iteration 141/1000 | Loss: 0.00002859
Iteration 142/1000 | Loss: 0.00002859
Iteration 143/1000 | Loss: 0.00002858
Iteration 144/1000 | Loss: 0.00002858
Iteration 145/1000 | Loss: 0.00002857
Iteration 146/1000 | Loss: 0.00002857
Iteration 147/1000 | Loss: 0.00002857
Iteration 148/1000 | Loss: 0.00002857
Iteration 149/1000 | Loss: 0.00002857
Iteration 150/1000 | Loss: 0.00002857
Iteration 151/1000 | Loss: 0.00002857
Iteration 152/1000 | Loss: 0.00002857
Iteration 153/1000 | Loss: 0.00002857
Iteration 154/1000 | Loss: 0.00002857
Iteration 155/1000 | Loss: 0.00002857
Iteration 156/1000 | Loss: 0.00002857
Iteration 157/1000 | Loss: 0.00002857
Iteration 158/1000 | Loss: 0.00002857
Iteration 159/1000 | Loss: 0.00002857
Iteration 160/1000 | Loss: 0.00002856
Iteration 161/1000 | Loss: 0.00002856
Iteration 162/1000 | Loss: 0.00002856
Iteration 163/1000 | Loss: 0.00002856
Iteration 164/1000 | Loss: 0.00002855
Iteration 165/1000 | Loss: 0.00002855
Iteration 166/1000 | Loss: 0.00002855
Iteration 167/1000 | Loss: 0.00002854
Iteration 168/1000 | Loss: 0.00002854
Iteration 169/1000 | Loss: 0.00002854
Iteration 170/1000 | Loss: 0.00002854
Iteration 171/1000 | Loss: 0.00002854
Iteration 172/1000 | Loss: 0.00002854
Iteration 173/1000 | Loss: 0.00002854
Iteration 174/1000 | Loss: 0.00002853
Iteration 175/1000 | Loss: 0.00002853
Iteration 176/1000 | Loss: 0.00002853
Iteration 177/1000 | Loss: 0.00002852
Iteration 178/1000 | Loss: 0.00002852
Iteration 179/1000 | Loss: 0.00002852
Iteration 180/1000 | Loss: 0.00002852
Iteration 181/1000 | Loss: 0.00002852
Iteration 182/1000 | Loss: 0.00002852
Iteration 183/1000 | Loss: 0.00002852
Iteration 184/1000 | Loss: 0.00002852
Iteration 185/1000 | Loss: 0.00002851
Iteration 186/1000 | Loss: 0.00002851
Iteration 187/1000 | Loss: 0.00002851
Iteration 188/1000 | Loss: 0.00002851
Iteration 189/1000 | Loss: 0.00002851
Iteration 190/1000 | Loss: 0.00002851
Iteration 191/1000 | Loss: 0.00002851
Iteration 192/1000 | Loss: 0.00002851
Iteration 193/1000 | Loss: 0.00002851
Iteration 194/1000 | Loss: 0.00002851
Iteration 195/1000 | Loss: 0.00002851
Iteration 196/1000 | Loss: 0.00002851
Iteration 197/1000 | Loss: 0.00002851
Iteration 198/1000 | Loss: 0.00002851
Iteration 199/1000 | Loss: 0.00002851
Iteration 200/1000 | Loss: 0.00002851
Iteration 201/1000 | Loss: 0.00002851
Iteration 202/1000 | Loss: 0.00002851
Iteration 203/1000 | Loss: 0.00002851
Iteration 204/1000 | Loss: 0.00002851
Iteration 205/1000 | Loss: 0.00002850
Iteration 206/1000 | Loss: 0.00002850
Iteration 207/1000 | Loss: 0.00002850
Iteration 208/1000 | Loss: 0.00002850
Iteration 209/1000 | Loss: 0.00002850
Iteration 210/1000 | Loss: 0.00002850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.8504673537099734e-05, 2.8504673537099734e-05, 2.8504673537099734e-05, 2.8504673537099734e-05, 2.8504673537099734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8504673537099734e-05

Optimization complete. Final v2v error: 3.3577730655670166 mm

Highest mean error: 11.501489639282227 mm for frame 140

Lowest mean error: 2.804530382156372 mm for frame 187

Saving results

Total time: 266.02604627609253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004378
Iteration 2/25 | Loss: 0.00281882
Iteration 3/25 | Loss: 0.00169894
Iteration 4/25 | Loss: 0.00137980
Iteration 5/25 | Loss: 0.00129950
Iteration 6/25 | Loss: 0.00141663
Iteration 7/25 | Loss: 0.00106589
Iteration 8/25 | Loss: 0.00098663
Iteration 9/25 | Loss: 0.00091885
Iteration 10/25 | Loss: 0.00087998
Iteration 11/25 | Loss: 0.00085988
Iteration 12/25 | Loss: 0.00087012
Iteration 13/25 | Loss: 0.00084656
Iteration 14/25 | Loss: 0.00083925
Iteration 15/25 | Loss: 0.00084188
Iteration 16/25 | Loss: 0.00087368
Iteration 17/25 | Loss: 0.00083501
Iteration 18/25 | Loss: 0.00082657
Iteration 19/25 | Loss: 0.00082512
Iteration 20/25 | Loss: 0.00082484
Iteration 21/25 | Loss: 0.00082477
Iteration 22/25 | Loss: 0.00082468
Iteration 23/25 | Loss: 0.00082451
Iteration 24/25 | Loss: 0.00082420
Iteration 25/25 | Loss: 0.00082384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53676879
Iteration 2/25 | Loss: 0.00166334
Iteration 3/25 | Loss: 0.00068649
Iteration 4/25 | Loss: 0.00068648
Iteration 5/25 | Loss: 0.00068648
Iteration 6/25 | Loss: 0.00068648
Iteration 7/25 | Loss: 0.00068648
Iteration 8/25 | Loss: 0.00068648
Iteration 9/25 | Loss: 0.00068648
Iteration 10/25 | Loss: 0.00068648
Iteration 11/25 | Loss: 0.00068648
Iteration 12/25 | Loss: 0.00068648
Iteration 13/25 | Loss: 0.00068648
Iteration 14/25 | Loss: 0.00068648
Iteration 15/25 | Loss: 0.00068648
Iteration 16/25 | Loss: 0.00068648
Iteration 17/25 | Loss: 0.00068648
Iteration 18/25 | Loss: 0.00068648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006864823517389596, 0.0006864823517389596, 0.0006864823517389596, 0.0006864823517389596, 0.0006864823517389596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006864823517389596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068648
Iteration 2/1000 | Loss: 0.00020590
Iteration 3/1000 | Loss: 0.00038490
Iteration 4/1000 | Loss: 0.00058381
Iteration 5/1000 | Loss: 0.00010599
Iteration 6/1000 | Loss: 0.00007933
Iteration 7/1000 | Loss: 0.00002485
Iteration 8/1000 | Loss: 0.00006745
Iteration 9/1000 | Loss: 0.00013189
Iteration 10/1000 | Loss: 0.00002068
Iteration 11/1000 | Loss: 0.00001996
Iteration 12/1000 | Loss: 0.00039209
Iteration 13/1000 | Loss: 0.00007157
Iteration 14/1000 | Loss: 0.00013473
Iteration 15/1000 | Loss: 0.00006573
Iteration 16/1000 | Loss: 0.00003514
Iteration 17/1000 | Loss: 0.00007382
Iteration 18/1000 | Loss: 0.00002124
Iteration 19/1000 | Loss: 0.00002648
Iteration 20/1000 | Loss: 0.00002041
Iteration 21/1000 | Loss: 0.00010778
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00001995
Iteration 24/1000 | Loss: 0.00001990
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001970
Iteration 27/1000 | Loss: 0.00001955
Iteration 28/1000 | Loss: 0.00010466
Iteration 29/1000 | Loss: 0.00005034
Iteration 30/1000 | Loss: 0.00002131
Iteration 31/1000 | Loss: 0.00007583
Iteration 32/1000 | Loss: 0.00002305
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001943
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001940
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001939
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001935
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001934
Iteration 57/1000 | Loss: 0.00001934
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001933
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001928
Iteration 71/1000 | Loss: 0.00001928
Iteration 72/1000 | Loss: 0.00001928
Iteration 73/1000 | Loss: 0.00001927
Iteration 74/1000 | Loss: 0.00001927
Iteration 75/1000 | Loss: 0.00001927
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001925
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001925
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001925
Iteration 90/1000 | Loss: 0.00001925
Iteration 91/1000 | Loss: 0.00001924
Iteration 92/1000 | Loss: 0.00001924
Iteration 93/1000 | Loss: 0.00001924
Iteration 94/1000 | Loss: 0.00001924
Iteration 95/1000 | Loss: 0.00001924
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001924
Iteration 98/1000 | Loss: 0.00001924
Iteration 99/1000 | Loss: 0.00001924
Iteration 100/1000 | Loss: 0.00001924
Iteration 101/1000 | Loss: 0.00001924
Iteration 102/1000 | Loss: 0.00001924
Iteration 103/1000 | Loss: 0.00001924
Iteration 104/1000 | Loss: 0.00001924
Iteration 105/1000 | Loss: 0.00001924
Iteration 106/1000 | Loss: 0.00001923
Iteration 107/1000 | Loss: 0.00001923
Iteration 108/1000 | Loss: 0.00001923
Iteration 109/1000 | Loss: 0.00001923
Iteration 110/1000 | Loss: 0.00001923
Iteration 111/1000 | Loss: 0.00001923
Iteration 112/1000 | Loss: 0.00001923
Iteration 113/1000 | Loss: 0.00001923
Iteration 114/1000 | Loss: 0.00001923
Iteration 115/1000 | Loss: 0.00001923
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001923
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001922
Iteration 126/1000 | Loss: 0.00001922
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001922
Iteration 131/1000 | Loss: 0.00001922
Iteration 132/1000 | Loss: 0.00001922
Iteration 133/1000 | Loss: 0.00001922
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001921
Iteration 136/1000 | Loss: 0.00001921
Iteration 137/1000 | Loss: 0.00001921
Iteration 138/1000 | Loss: 0.00001921
Iteration 139/1000 | Loss: 0.00001921
Iteration 140/1000 | Loss: 0.00001921
Iteration 141/1000 | Loss: 0.00001921
Iteration 142/1000 | Loss: 0.00001921
Iteration 143/1000 | Loss: 0.00001921
Iteration 144/1000 | Loss: 0.00001921
Iteration 145/1000 | Loss: 0.00001921
Iteration 146/1000 | Loss: 0.00001921
Iteration 147/1000 | Loss: 0.00001921
Iteration 148/1000 | Loss: 0.00001921
Iteration 149/1000 | Loss: 0.00001921
Iteration 150/1000 | Loss: 0.00001921
Iteration 151/1000 | Loss: 0.00001921
Iteration 152/1000 | Loss: 0.00001921
Iteration 153/1000 | Loss: 0.00001921
Iteration 154/1000 | Loss: 0.00001921
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001921
Iteration 159/1000 | Loss: 0.00001921
Iteration 160/1000 | Loss: 0.00001921
Iteration 161/1000 | Loss: 0.00001921
Iteration 162/1000 | Loss: 0.00001921
Iteration 163/1000 | Loss: 0.00001921
Iteration 164/1000 | Loss: 0.00001921
Iteration 165/1000 | Loss: 0.00001921
Iteration 166/1000 | Loss: 0.00001921
Iteration 167/1000 | Loss: 0.00001921
Iteration 168/1000 | Loss: 0.00001921
Iteration 169/1000 | Loss: 0.00001921
Iteration 170/1000 | Loss: 0.00001921
Iteration 171/1000 | Loss: 0.00001921
Iteration 172/1000 | Loss: 0.00001921
Iteration 173/1000 | Loss: 0.00001921
Iteration 174/1000 | Loss: 0.00001921
Iteration 175/1000 | Loss: 0.00001921
Iteration 176/1000 | Loss: 0.00001921
Iteration 177/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.9206421711714938e-05, 1.9206421711714938e-05, 1.9206421711714938e-05, 1.9206421711714938e-05, 1.9206421711714938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9206421711714938e-05

Optimization complete. Final v2v error: 3.4156277179718018 mm

Highest mean error: 9.61770248413086 mm for frame 100

Lowest mean error: 2.9233314990997314 mm for frame 171

Saving results

Total time: 165.6338722705841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385108
Iteration 2/25 | Loss: 0.00095225
Iteration 3/25 | Loss: 0.00078735
Iteration 4/25 | Loss: 0.00076326
Iteration 5/25 | Loss: 0.00075775
Iteration 6/25 | Loss: 0.00075600
Iteration 7/25 | Loss: 0.00075553
Iteration 8/25 | Loss: 0.00075553
Iteration 9/25 | Loss: 0.00075553
Iteration 10/25 | Loss: 0.00075553
Iteration 11/25 | Loss: 0.00075553
Iteration 12/25 | Loss: 0.00075553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007555284537374973, 0.0007555284537374973, 0.0007555284537374973, 0.0007555284537374973, 0.0007555284537374973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007555284537374973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51094556
Iteration 2/25 | Loss: 0.00053024
Iteration 3/25 | Loss: 0.00053024
Iteration 4/25 | Loss: 0.00053024
Iteration 5/25 | Loss: 0.00053024
Iteration 6/25 | Loss: 0.00053024
Iteration 7/25 | Loss: 0.00053024
Iteration 8/25 | Loss: 0.00053024
Iteration 9/25 | Loss: 0.00053024
Iteration 10/25 | Loss: 0.00053024
Iteration 11/25 | Loss: 0.00053024
Iteration 12/25 | Loss: 0.00053024
Iteration 13/25 | Loss: 0.00053024
Iteration 14/25 | Loss: 0.00053024
Iteration 15/25 | Loss: 0.00053024
Iteration 16/25 | Loss: 0.00053024
Iteration 17/25 | Loss: 0.00053024
Iteration 18/25 | Loss: 0.00053024
Iteration 19/25 | Loss: 0.00053024
Iteration 20/25 | Loss: 0.00053024
Iteration 21/25 | Loss: 0.00053024
Iteration 22/25 | Loss: 0.00053024
Iteration 23/25 | Loss: 0.00053024
Iteration 24/25 | Loss: 0.00053024
Iteration 25/25 | Loss: 0.00053024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053024
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001645
Iteration 4/1000 | Loss: 0.00001492
Iteration 5/1000 | Loss: 0.00001384
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00001272
Iteration 30/1000 | Loss: 0.00001272
Iteration 31/1000 | Loss: 0.00001271
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001269
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001269
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001252
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001252
Iteration 112/1000 | Loss: 0.00001252
Iteration 113/1000 | Loss: 0.00001252
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001251
Iteration 131/1000 | Loss: 0.00001251
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001250
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001250
Iteration 152/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.2499930562626105e-05, 1.2499930562626105e-05, 1.2499930562626105e-05, 1.2499930562626105e-05, 1.2499930562626105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2499930562626105e-05

Optimization complete. Final v2v error: 2.909761428833008 mm

Highest mean error: 3.4704976081848145 mm for frame 73

Lowest mean error: 2.4594383239746094 mm for frame 53

Saving results

Total time: 34.51871609687805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807423
Iteration 2/25 | Loss: 0.00141117
Iteration 3/25 | Loss: 0.00093028
Iteration 4/25 | Loss: 0.00082499
Iteration 5/25 | Loss: 0.00080004
Iteration 6/25 | Loss: 0.00078860
Iteration 7/25 | Loss: 0.00079799
Iteration 8/25 | Loss: 0.00078613
Iteration 9/25 | Loss: 0.00078942
Iteration 10/25 | Loss: 0.00077884
Iteration 11/25 | Loss: 0.00077793
Iteration 12/25 | Loss: 0.00077729
Iteration 13/25 | Loss: 0.00077648
Iteration 14/25 | Loss: 0.00077575
Iteration 15/25 | Loss: 0.00077533
Iteration 16/25 | Loss: 0.00077521
Iteration 17/25 | Loss: 0.00077517
Iteration 18/25 | Loss: 0.00077517
Iteration 19/25 | Loss: 0.00077517
Iteration 20/25 | Loss: 0.00077517
Iteration 21/25 | Loss: 0.00077517
Iteration 22/25 | Loss: 0.00077517
Iteration 23/25 | Loss: 0.00077517
Iteration 24/25 | Loss: 0.00077517
Iteration 25/25 | Loss: 0.00077516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.27183223
Iteration 2/25 | Loss: 0.00054803
Iteration 3/25 | Loss: 0.00054803
Iteration 4/25 | Loss: 0.00054803
Iteration 5/25 | Loss: 0.00054803
Iteration 6/25 | Loss: 0.00054803
Iteration 7/25 | Loss: 0.00054803
Iteration 8/25 | Loss: 0.00054803
Iteration 9/25 | Loss: 0.00054803
Iteration 10/25 | Loss: 0.00054803
Iteration 11/25 | Loss: 0.00054803
Iteration 12/25 | Loss: 0.00054803
Iteration 13/25 | Loss: 0.00054803
Iteration 14/25 | Loss: 0.00054803
Iteration 15/25 | Loss: 0.00054803
Iteration 16/25 | Loss: 0.00054803
Iteration 17/25 | Loss: 0.00054803
Iteration 18/25 | Loss: 0.00054803
Iteration 19/25 | Loss: 0.00054803
Iteration 20/25 | Loss: 0.00054803
Iteration 21/25 | Loss: 0.00054803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005480274558067322, 0.0005480274558067322, 0.0005480274558067322, 0.0005480274558067322, 0.0005480274558067322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005480274558067322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054803
Iteration 2/1000 | Loss: 0.00002277
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00011521
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001513
Iteration 8/1000 | Loss: 0.00007670
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001455
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001437
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001428
Iteration 18/1000 | Loss: 0.00001427
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001416
Iteration 33/1000 | Loss: 0.00001416
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001416
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001413
Iteration 46/1000 | Loss: 0.00001413
Iteration 47/1000 | Loss: 0.00001413
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001412
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001411
Iteration 55/1000 | Loss: 0.00001411
Iteration 56/1000 | Loss: 0.00001411
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001410
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001402
Iteration 99/1000 | Loss: 0.00001402
Iteration 100/1000 | Loss: 0.00001402
Iteration 101/1000 | Loss: 0.00001402
Iteration 102/1000 | Loss: 0.00001402
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001402
Iteration 107/1000 | Loss: 0.00001402
Iteration 108/1000 | Loss: 0.00001402
Iteration 109/1000 | Loss: 0.00001402
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00007195
Iteration 123/1000 | Loss: 0.00001408
Iteration 124/1000 | Loss: 0.00001398
Iteration 125/1000 | Loss: 0.00001398
Iteration 126/1000 | Loss: 0.00001398
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001398
Iteration 129/1000 | Loss: 0.00001398
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001397
Iteration 134/1000 | Loss: 0.00001397
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001396
Iteration 143/1000 | Loss: 0.00001396
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001393
Iteration 154/1000 | Loss: 0.00001393
Iteration 155/1000 | Loss: 0.00001393
Iteration 156/1000 | Loss: 0.00001393
Iteration 157/1000 | Loss: 0.00001392
Iteration 158/1000 | Loss: 0.00001392
Iteration 159/1000 | Loss: 0.00001392
Iteration 160/1000 | Loss: 0.00001392
Iteration 161/1000 | Loss: 0.00001392
Iteration 162/1000 | Loss: 0.00001392
Iteration 163/1000 | Loss: 0.00001392
Iteration 164/1000 | Loss: 0.00001392
Iteration 165/1000 | Loss: 0.00001392
Iteration 166/1000 | Loss: 0.00001392
Iteration 167/1000 | Loss: 0.00001392
Iteration 168/1000 | Loss: 0.00001392
Iteration 169/1000 | Loss: 0.00001392
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001392
Iteration 173/1000 | Loss: 0.00001391
Iteration 174/1000 | Loss: 0.00001391
Iteration 175/1000 | Loss: 0.00001391
Iteration 176/1000 | Loss: 0.00001391
Iteration 177/1000 | Loss: 0.00001391
Iteration 178/1000 | Loss: 0.00001391
Iteration 179/1000 | Loss: 0.00001391
Iteration 180/1000 | Loss: 0.00001391
Iteration 181/1000 | Loss: 0.00001391
Iteration 182/1000 | Loss: 0.00001391
Iteration 183/1000 | Loss: 0.00001391
Iteration 184/1000 | Loss: 0.00001391
Iteration 185/1000 | Loss: 0.00001391
Iteration 186/1000 | Loss: 0.00001391
Iteration 187/1000 | Loss: 0.00001391
Iteration 188/1000 | Loss: 0.00001391
Iteration 189/1000 | Loss: 0.00001391
Iteration 190/1000 | Loss: 0.00001391
Iteration 191/1000 | Loss: 0.00001391
Iteration 192/1000 | Loss: 0.00001391
Iteration 193/1000 | Loss: 0.00001391
Iteration 194/1000 | Loss: 0.00001391
Iteration 195/1000 | Loss: 0.00001391
Iteration 196/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.3909741028328426e-05, 1.3909741028328426e-05, 1.3909741028328426e-05, 1.3909741028328426e-05, 1.3909741028328426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3909741028328426e-05

Optimization complete. Final v2v error: 3.144767999649048 mm

Highest mean error: 3.4962027072906494 mm for frame 10

Lowest mean error: 2.7767536640167236 mm for frame 75

Saving results

Total time: 129.26230931282043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041884
Iteration 2/25 | Loss: 0.00411193
Iteration 3/25 | Loss: 0.00218387
Iteration 4/25 | Loss: 0.00171736
Iteration 5/25 | Loss: 0.00144202
Iteration 6/25 | Loss: 0.00132902
Iteration 7/25 | Loss: 0.00120189
Iteration 8/25 | Loss: 0.00113005
Iteration 9/25 | Loss: 0.00106173
Iteration 10/25 | Loss: 0.00101860
Iteration 11/25 | Loss: 0.00099008
Iteration 12/25 | Loss: 0.00097538
Iteration 13/25 | Loss: 0.00096881
Iteration 14/25 | Loss: 0.00095518
Iteration 15/25 | Loss: 0.00094360
Iteration 16/25 | Loss: 0.00094134
Iteration 17/25 | Loss: 0.00094320
Iteration 18/25 | Loss: 0.00094486
Iteration 19/25 | Loss: 0.00093938
Iteration 20/25 | Loss: 0.00092951
Iteration 21/25 | Loss: 0.00092282
Iteration 22/25 | Loss: 0.00091855
Iteration 23/25 | Loss: 0.00091574
Iteration 24/25 | Loss: 0.00091211
Iteration 25/25 | Loss: 0.00091141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49612927
Iteration 2/25 | Loss: 0.00205455
Iteration 3/25 | Loss: 0.00149302
Iteration 4/25 | Loss: 0.00149302
Iteration 5/25 | Loss: 0.00149302
Iteration 6/25 | Loss: 0.00149302
Iteration 7/25 | Loss: 0.00149302
Iteration 8/25 | Loss: 0.00149302
Iteration 9/25 | Loss: 0.00149302
Iteration 10/25 | Loss: 0.00149302
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014930159086361527, 0.0014930159086361527, 0.0014930159086361527, 0.0014930159086361527, 0.0014930159086361527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014930159086361527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149302
Iteration 2/1000 | Loss: 0.00069737
Iteration 3/1000 | Loss: 0.00116479
Iteration 4/1000 | Loss: 0.00019009
Iteration 5/1000 | Loss: 0.00016370
Iteration 6/1000 | Loss: 0.00050961
Iteration 7/1000 | Loss: 0.00116518
Iteration 8/1000 | Loss: 0.00019069
Iteration 9/1000 | Loss: 0.00017788
Iteration 10/1000 | Loss: 0.00015078
Iteration 11/1000 | Loss: 0.00011928
Iteration 12/1000 | Loss: 0.00017470
Iteration 13/1000 | Loss: 0.00018229
Iteration 14/1000 | Loss: 0.00021663
Iteration 15/1000 | Loss: 0.00031786
Iteration 16/1000 | Loss: 0.00691932
Iteration 17/1000 | Loss: 0.00602692
Iteration 18/1000 | Loss: 0.00026063
Iteration 19/1000 | Loss: 0.00013038
Iteration 20/1000 | Loss: 0.00015808
Iteration 21/1000 | Loss: 0.00008159
Iteration 22/1000 | Loss: 0.00009551
Iteration 23/1000 | Loss: 0.00010216
Iteration 24/1000 | Loss: 0.00004310
Iteration 25/1000 | Loss: 0.00006183
Iteration 26/1000 | Loss: 0.00003601
Iteration 27/1000 | Loss: 0.00005484
Iteration 28/1000 | Loss: 0.00004298
Iteration 29/1000 | Loss: 0.00002708
Iteration 30/1000 | Loss: 0.00018622
Iteration 31/1000 | Loss: 0.00004422
Iteration 32/1000 | Loss: 0.00001844
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00003185
Iteration 35/1000 | Loss: 0.00002482
Iteration 36/1000 | Loss: 0.00002766
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00002740
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00003218
Iteration 42/1000 | Loss: 0.00003504
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00003076
Iteration 46/1000 | Loss: 0.00025171
Iteration 47/1000 | Loss: 0.00002269
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001736
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001347
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001346
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001345
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001345
Iteration 95/1000 | Loss: 0.00001345
Iteration 96/1000 | Loss: 0.00001345
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00002774
Iteration 107/1000 | Loss: 0.00001500
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001338
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001338
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001338
Iteration 127/1000 | Loss: 0.00001338
Iteration 128/1000 | Loss: 0.00001338
Iteration 129/1000 | Loss: 0.00001338
Iteration 130/1000 | Loss: 0.00001338
Iteration 131/1000 | Loss: 0.00001338
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00001337
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001337
Iteration 143/1000 | Loss: 0.00001337
Iteration 144/1000 | Loss: 0.00001337
Iteration 145/1000 | Loss: 0.00001337
Iteration 146/1000 | Loss: 0.00001337
Iteration 147/1000 | Loss: 0.00001337
Iteration 148/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3371203749557026e-05, 1.3371203749557026e-05, 1.3371203749557026e-05, 1.3371203749557026e-05, 1.3371203749557026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3371203749557026e-05

Optimization complete. Final v2v error: 3.079470634460449 mm

Highest mean error: 4.691252708435059 mm for frame 8

Lowest mean error: 2.7775580883026123 mm for frame 198

Saving results

Total time: 171.51751923561096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880419
Iteration 2/25 | Loss: 0.00142830
Iteration 3/25 | Loss: 0.00098190
Iteration 4/25 | Loss: 0.00087106
Iteration 5/25 | Loss: 0.00084562
Iteration 6/25 | Loss: 0.00081733
Iteration 7/25 | Loss: 0.00081248
Iteration 8/25 | Loss: 0.00078493
Iteration 9/25 | Loss: 0.00077135
Iteration 10/25 | Loss: 0.00075769
Iteration 11/25 | Loss: 0.00075140
Iteration 12/25 | Loss: 0.00074871
Iteration 13/25 | Loss: 0.00074851
Iteration 14/25 | Loss: 0.00074429
Iteration 15/25 | Loss: 0.00074646
Iteration 16/25 | Loss: 0.00074441
Iteration 17/25 | Loss: 0.00074503
Iteration 18/25 | Loss: 0.00074320
Iteration 19/25 | Loss: 0.00074320
Iteration 20/25 | Loss: 0.00074320
Iteration 21/25 | Loss: 0.00074320
Iteration 22/25 | Loss: 0.00074320
Iteration 23/25 | Loss: 0.00074320
Iteration 24/25 | Loss: 0.00074320
Iteration 25/25 | Loss: 0.00074320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.02123356
Iteration 2/25 | Loss: 0.00054867
Iteration 3/25 | Loss: 0.00053825
Iteration 4/25 | Loss: 0.00053825
Iteration 5/25 | Loss: 0.00053825
Iteration 6/25 | Loss: 0.00053825
Iteration 7/25 | Loss: 0.00053825
Iteration 8/25 | Loss: 0.00053825
Iteration 9/25 | Loss: 0.00053825
Iteration 10/25 | Loss: 0.00053825
Iteration 11/25 | Loss: 0.00053825
Iteration 12/25 | Loss: 0.00053825
Iteration 13/25 | Loss: 0.00053825
Iteration 14/25 | Loss: 0.00053825
Iteration 15/25 | Loss: 0.00053825
Iteration 16/25 | Loss: 0.00053825
Iteration 17/25 | Loss: 0.00053825
Iteration 18/25 | Loss: 0.00053825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000538249674718827, 0.000538249674718827, 0.000538249674718827, 0.000538249674718827, 0.000538249674718827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000538249674718827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053825
Iteration 2/1000 | Loss: 0.00002762
Iteration 3/1000 | Loss: 0.00007227
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00010092
Iteration 6/1000 | Loss: 0.00009775
Iteration 7/1000 | Loss: 0.00003961
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00007725
Iteration 12/1000 | Loss: 0.00004755
Iteration 13/1000 | Loss: 0.00003912
Iteration 14/1000 | Loss: 0.00012429
Iteration 15/1000 | Loss: 0.00057967
Iteration 16/1000 | Loss: 0.00003136
Iteration 17/1000 | Loss: 0.00005374
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00003231
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00003597
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001253
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00001251
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001249
Iteration 38/1000 | Loss: 0.00001249
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00005420
Iteration 64/1000 | Loss: 0.00005420
Iteration 65/1000 | Loss: 0.00003922
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00006022
Iteration 69/1000 | Loss: 0.00016792
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00007492
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001227
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001226
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00003398
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001844
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001291
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001223
Iteration 111/1000 | Loss: 0.00001223
Iteration 112/1000 | Loss: 0.00001223
Iteration 113/1000 | Loss: 0.00001222
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001217
Iteration 117/1000 | Loss: 0.00001217
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001213
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00001212
Iteration 146/1000 | Loss: 0.00001212
Iteration 147/1000 | Loss: 0.00001212
Iteration 148/1000 | Loss: 0.00001212
Iteration 149/1000 | Loss: 0.00001212
Iteration 150/1000 | Loss: 0.00001212
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001211
Iteration 155/1000 | Loss: 0.00001211
Iteration 156/1000 | Loss: 0.00001211
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001211
Iteration 159/1000 | Loss: 0.00001211
Iteration 160/1000 | Loss: 0.00001211
Iteration 161/1000 | Loss: 0.00001211
Iteration 162/1000 | Loss: 0.00001211
Iteration 163/1000 | Loss: 0.00001211
Iteration 164/1000 | Loss: 0.00001211
Iteration 165/1000 | Loss: 0.00001211
Iteration 166/1000 | Loss: 0.00001211
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001210
Iteration 169/1000 | Loss: 0.00001210
Iteration 170/1000 | Loss: 0.00001210
Iteration 171/1000 | Loss: 0.00001210
Iteration 172/1000 | Loss: 0.00001210
Iteration 173/1000 | Loss: 0.00001210
Iteration 174/1000 | Loss: 0.00001210
Iteration 175/1000 | Loss: 0.00001210
Iteration 176/1000 | Loss: 0.00001210
Iteration 177/1000 | Loss: 0.00001210
Iteration 178/1000 | Loss: 0.00001210
Iteration 179/1000 | Loss: 0.00001209
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001209
Iteration 184/1000 | Loss: 0.00001209
Iteration 185/1000 | Loss: 0.00001209
Iteration 186/1000 | Loss: 0.00001209
Iteration 187/1000 | Loss: 0.00001208
Iteration 188/1000 | Loss: 0.00001208
Iteration 189/1000 | Loss: 0.00001208
Iteration 190/1000 | Loss: 0.00001208
Iteration 191/1000 | Loss: 0.00001208
Iteration 192/1000 | Loss: 0.00001208
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001207
Iteration 195/1000 | Loss: 0.00001207
Iteration 196/1000 | Loss: 0.00001206
Iteration 197/1000 | Loss: 0.00001206
Iteration 198/1000 | Loss: 0.00001206
Iteration 199/1000 | Loss: 0.00001206
Iteration 200/1000 | Loss: 0.00001206
Iteration 201/1000 | Loss: 0.00001206
Iteration 202/1000 | Loss: 0.00001206
Iteration 203/1000 | Loss: 0.00001206
Iteration 204/1000 | Loss: 0.00001206
Iteration 205/1000 | Loss: 0.00001205
Iteration 206/1000 | Loss: 0.00001205
Iteration 207/1000 | Loss: 0.00001205
Iteration 208/1000 | Loss: 0.00001205
Iteration 209/1000 | Loss: 0.00001205
Iteration 210/1000 | Loss: 0.00001205
Iteration 211/1000 | Loss: 0.00001205
Iteration 212/1000 | Loss: 0.00001205
Iteration 213/1000 | Loss: 0.00001205
Iteration 214/1000 | Loss: 0.00001205
Iteration 215/1000 | Loss: 0.00001205
Iteration 216/1000 | Loss: 0.00001205
Iteration 217/1000 | Loss: 0.00001205
Iteration 218/1000 | Loss: 0.00001205
Iteration 219/1000 | Loss: 0.00001205
Iteration 220/1000 | Loss: 0.00001205
Iteration 221/1000 | Loss: 0.00001205
Iteration 222/1000 | Loss: 0.00001205
Iteration 223/1000 | Loss: 0.00001205
Iteration 224/1000 | Loss: 0.00001205
Iteration 225/1000 | Loss: 0.00001205
Iteration 226/1000 | Loss: 0.00001205
Iteration 227/1000 | Loss: 0.00001205
Iteration 228/1000 | Loss: 0.00001205
Iteration 229/1000 | Loss: 0.00001205
Iteration 230/1000 | Loss: 0.00001205
Iteration 231/1000 | Loss: 0.00001205
Iteration 232/1000 | Loss: 0.00001205
Iteration 233/1000 | Loss: 0.00001205
Iteration 234/1000 | Loss: 0.00001205
Iteration 235/1000 | Loss: 0.00001205
Iteration 236/1000 | Loss: 0.00001205
Iteration 237/1000 | Loss: 0.00001205
Iteration 238/1000 | Loss: 0.00001205
Iteration 239/1000 | Loss: 0.00001205
Iteration 240/1000 | Loss: 0.00001205
Iteration 241/1000 | Loss: 0.00001205
Iteration 242/1000 | Loss: 0.00001205
Iteration 243/1000 | Loss: 0.00001205
Iteration 244/1000 | Loss: 0.00001205
Iteration 245/1000 | Loss: 0.00001205
Iteration 246/1000 | Loss: 0.00001205
Iteration 247/1000 | Loss: 0.00001205
Iteration 248/1000 | Loss: 0.00001205
Iteration 249/1000 | Loss: 0.00001205
Iteration 250/1000 | Loss: 0.00001205
Iteration 251/1000 | Loss: 0.00001205
Iteration 252/1000 | Loss: 0.00001205
Iteration 253/1000 | Loss: 0.00001205
Iteration 254/1000 | Loss: 0.00001205
Iteration 255/1000 | Loss: 0.00001205
Iteration 256/1000 | Loss: 0.00001205
Iteration 257/1000 | Loss: 0.00001205
Iteration 258/1000 | Loss: 0.00001205
Iteration 259/1000 | Loss: 0.00001205
Iteration 260/1000 | Loss: 0.00001205
Iteration 261/1000 | Loss: 0.00001205
Iteration 262/1000 | Loss: 0.00001205
Iteration 263/1000 | Loss: 0.00001205
Iteration 264/1000 | Loss: 0.00001205
Iteration 265/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.2045409675920382e-05, 1.2045409675920382e-05, 1.2045409675920382e-05, 1.2045409675920382e-05, 1.2045409675920382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2045409675920382e-05

Optimization complete. Final v2v error: 2.9475514888763428 mm

Highest mean error: 3.6183204650878906 mm for frame 189

Lowest mean error: 2.5454583168029785 mm for frame 25

Saving results

Total time: 130.88671350479126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660866
Iteration 2/25 | Loss: 0.00143937
Iteration 3/25 | Loss: 0.00100233
Iteration 4/25 | Loss: 0.00095403
Iteration 5/25 | Loss: 0.00094533
Iteration 6/25 | Loss: 0.00094411
Iteration 7/25 | Loss: 0.00094393
Iteration 8/25 | Loss: 0.00094393
Iteration 9/25 | Loss: 0.00094393
Iteration 10/25 | Loss: 0.00094393
Iteration 11/25 | Loss: 0.00094393
Iteration 12/25 | Loss: 0.00094393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009439336718060076, 0.0009439336718060076, 0.0009439336718060076, 0.0009439336718060076, 0.0009439336718060076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009439336718060076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58570457
Iteration 2/25 | Loss: 0.00063907
Iteration 3/25 | Loss: 0.00063904
Iteration 4/25 | Loss: 0.00063904
Iteration 5/25 | Loss: 0.00063904
Iteration 6/25 | Loss: 0.00063904
Iteration 7/25 | Loss: 0.00063904
Iteration 8/25 | Loss: 0.00063904
Iteration 9/25 | Loss: 0.00063904
Iteration 10/25 | Loss: 0.00063904
Iteration 11/25 | Loss: 0.00063904
Iteration 12/25 | Loss: 0.00063904
Iteration 13/25 | Loss: 0.00063904
Iteration 14/25 | Loss: 0.00063904
Iteration 15/25 | Loss: 0.00063904
Iteration 16/25 | Loss: 0.00063904
Iteration 17/25 | Loss: 0.00063904
Iteration 18/25 | Loss: 0.00063904
Iteration 19/25 | Loss: 0.00063904
Iteration 20/25 | Loss: 0.00063904
Iteration 21/25 | Loss: 0.00063904
Iteration 22/25 | Loss: 0.00063904
Iteration 23/25 | Loss: 0.00063904
Iteration 24/25 | Loss: 0.00063904
Iteration 25/25 | Loss: 0.00063904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063904
Iteration 2/1000 | Loss: 0.00004062
Iteration 3/1000 | Loss: 0.00002946
Iteration 4/1000 | Loss: 0.00002616
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002431
Iteration 7/1000 | Loss: 0.00002377
Iteration 8/1000 | Loss: 0.00002350
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002307
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002285
Iteration 13/1000 | Loss: 0.00002279
Iteration 14/1000 | Loss: 0.00002278
Iteration 15/1000 | Loss: 0.00002277
Iteration 16/1000 | Loss: 0.00002276
Iteration 17/1000 | Loss: 0.00002276
Iteration 18/1000 | Loss: 0.00002276
Iteration 19/1000 | Loss: 0.00002275
Iteration 20/1000 | Loss: 0.00002275
Iteration 21/1000 | Loss: 0.00002275
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002275
Iteration 24/1000 | Loss: 0.00002275
Iteration 25/1000 | Loss: 0.00002274
Iteration 26/1000 | Loss: 0.00002274
Iteration 27/1000 | Loss: 0.00002274
Iteration 28/1000 | Loss: 0.00002273
Iteration 29/1000 | Loss: 0.00002273
Iteration 30/1000 | Loss: 0.00002272
Iteration 31/1000 | Loss: 0.00002272
Iteration 32/1000 | Loss: 0.00002271
Iteration 33/1000 | Loss: 0.00002271
Iteration 34/1000 | Loss: 0.00002271
Iteration 35/1000 | Loss: 0.00002271
Iteration 36/1000 | Loss: 0.00002271
Iteration 37/1000 | Loss: 0.00002271
Iteration 38/1000 | Loss: 0.00002271
Iteration 39/1000 | Loss: 0.00002270
Iteration 40/1000 | Loss: 0.00002270
Iteration 41/1000 | Loss: 0.00002270
Iteration 42/1000 | Loss: 0.00002270
Iteration 43/1000 | Loss: 0.00002269
Iteration 44/1000 | Loss: 0.00002269
Iteration 45/1000 | Loss: 0.00002268
Iteration 46/1000 | Loss: 0.00002268
Iteration 47/1000 | Loss: 0.00002268
Iteration 48/1000 | Loss: 0.00002268
Iteration 49/1000 | Loss: 0.00002268
Iteration 50/1000 | Loss: 0.00002267
Iteration 51/1000 | Loss: 0.00002267
Iteration 52/1000 | Loss: 0.00002267
Iteration 53/1000 | Loss: 0.00002267
Iteration 54/1000 | Loss: 0.00002267
Iteration 55/1000 | Loss: 0.00002267
Iteration 56/1000 | Loss: 0.00002266
Iteration 57/1000 | Loss: 0.00002266
Iteration 58/1000 | Loss: 0.00002266
Iteration 59/1000 | Loss: 0.00002266
Iteration 60/1000 | Loss: 0.00002266
Iteration 61/1000 | Loss: 0.00002266
Iteration 62/1000 | Loss: 0.00002266
Iteration 63/1000 | Loss: 0.00002266
Iteration 64/1000 | Loss: 0.00002265
Iteration 65/1000 | Loss: 0.00002265
Iteration 66/1000 | Loss: 0.00002265
Iteration 67/1000 | Loss: 0.00002265
Iteration 68/1000 | Loss: 0.00002265
Iteration 69/1000 | Loss: 0.00002265
Iteration 70/1000 | Loss: 0.00002265
Iteration 71/1000 | Loss: 0.00002265
Iteration 72/1000 | Loss: 0.00002265
Iteration 73/1000 | Loss: 0.00002265
Iteration 74/1000 | Loss: 0.00002265
Iteration 75/1000 | Loss: 0.00002265
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002264
Iteration 78/1000 | Loss: 0.00002264
Iteration 79/1000 | Loss: 0.00002264
Iteration 80/1000 | Loss: 0.00002264
Iteration 81/1000 | Loss: 0.00002264
Iteration 82/1000 | Loss: 0.00002264
Iteration 83/1000 | Loss: 0.00002264
Iteration 84/1000 | Loss: 0.00002264
Iteration 85/1000 | Loss: 0.00002264
Iteration 86/1000 | Loss: 0.00002264
Iteration 87/1000 | Loss: 0.00002264
Iteration 88/1000 | Loss: 0.00002264
Iteration 89/1000 | Loss: 0.00002264
Iteration 90/1000 | Loss: 0.00002264
Iteration 91/1000 | Loss: 0.00002264
Iteration 92/1000 | Loss: 0.00002264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.2640664610662498e-05, 2.2640664610662498e-05, 2.2640664610662498e-05, 2.2640664610662498e-05, 2.2640664610662498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2640664610662498e-05

Optimization complete. Final v2v error: 3.7897708415985107 mm

Highest mean error: 4.165258407592773 mm for frame 85

Lowest mean error: 3.5397655963897705 mm for frame 121

Saving results

Total time: 34.38426327705383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066046
Iteration 2/25 | Loss: 0.00206977
Iteration 3/25 | Loss: 0.00115945
Iteration 4/25 | Loss: 0.00108691
Iteration 5/25 | Loss: 0.00107131
Iteration 6/25 | Loss: 0.00106437
Iteration 7/25 | Loss: 0.00106337
Iteration 8/25 | Loss: 0.00106321
Iteration 9/25 | Loss: 0.00106321
Iteration 10/25 | Loss: 0.00106321
Iteration 11/25 | Loss: 0.00106321
Iteration 12/25 | Loss: 0.00106321
Iteration 13/25 | Loss: 0.00106321
Iteration 14/25 | Loss: 0.00106321
Iteration 15/25 | Loss: 0.00106321
Iteration 16/25 | Loss: 0.00106321
Iteration 17/25 | Loss: 0.00106321
Iteration 18/25 | Loss: 0.00106321
Iteration 19/25 | Loss: 0.00106321
Iteration 20/25 | Loss: 0.00106321
Iteration 21/25 | Loss: 0.00106321
Iteration 22/25 | Loss: 0.00106321
Iteration 23/25 | Loss: 0.00106321
Iteration 24/25 | Loss: 0.00106321
Iteration 25/25 | Loss: 0.00106321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68427140
Iteration 2/25 | Loss: 0.00050544
Iteration 3/25 | Loss: 0.00050541
Iteration 4/25 | Loss: 0.00050541
Iteration 5/25 | Loss: 0.00050541
Iteration 6/25 | Loss: 0.00050541
Iteration 7/25 | Loss: 0.00050541
Iteration 8/25 | Loss: 0.00050541
Iteration 9/25 | Loss: 0.00050541
Iteration 10/25 | Loss: 0.00050541
Iteration 11/25 | Loss: 0.00050541
Iteration 12/25 | Loss: 0.00050541
Iteration 13/25 | Loss: 0.00050541
Iteration 14/25 | Loss: 0.00050541
Iteration 15/25 | Loss: 0.00050541
Iteration 16/25 | Loss: 0.00050541
Iteration 17/25 | Loss: 0.00050541
Iteration 18/25 | Loss: 0.00050541
Iteration 19/25 | Loss: 0.00050541
Iteration 20/25 | Loss: 0.00050541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005054088542237878, 0.0005054088542237878, 0.0005054088542237878, 0.0005054088542237878, 0.0005054088542237878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005054088542237878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050541
Iteration 2/1000 | Loss: 0.00009701
Iteration 3/1000 | Loss: 0.00006558
Iteration 4/1000 | Loss: 0.00005710
Iteration 5/1000 | Loss: 0.00005524
Iteration 6/1000 | Loss: 0.00005397
Iteration 7/1000 | Loss: 0.00005333
Iteration 8/1000 | Loss: 0.00005273
Iteration 9/1000 | Loss: 0.00005214
Iteration 10/1000 | Loss: 0.00005179
Iteration 11/1000 | Loss: 0.00005154
Iteration 12/1000 | Loss: 0.00005127
Iteration 13/1000 | Loss: 0.00005090
Iteration 14/1000 | Loss: 0.00005060
Iteration 15/1000 | Loss: 0.00005037
Iteration 16/1000 | Loss: 0.00005020
Iteration 17/1000 | Loss: 0.00005020
Iteration 18/1000 | Loss: 0.00005008
Iteration 19/1000 | Loss: 0.00004994
Iteration 20/1000 | Loss: 0.00004982
Iteration 21/1000 | Loss: 0.00004981
Iteration 22/1000 | Loss: 0.00004974
Iteration 23/1000 | Loss: 0.00004971
Iteration 24/1000 | Loss: 0.00004965
Iteration 25/1000 | Loss: 0.00004964
Iteration 26/1000 | Loss: 0.00004964
Iteration 27/1000 | Loss: 0.00004963
Iteration 28/1000 | Loss: 0.00004963
Iteration 29/1000 | Loss: 0.00004963
Iteration 30/1000 | Loss: 0.00004962
Iteration 31/1000 | Loss: 0.00004962
Iteration 32/1000 | Loss: 0.00004962
Iteration 33/1000 | Loss: 0.00004961
Iteration 34/1000 | Loss: 0.00004961
Iteration 35/1000 | Loss: 0.00004961
Iteration 36/1000 | Loss: 0.00004960
Iteration 37/1000 | Loss: 0.00004960
Iteration 38/1000 | Loss: 0.00004960
Iteration 39/1000 | Loss: 0.00004960
Iteration 40/1000 | Loss: 0.00004960
Iteration 41/1000 | Loss: 0.00004960
Iteration 42/1000 | Loss: 0.00004960
Iteration 43/1000 | Loss: 0.00004960
Iteration 44/1000 | Loss: 0.00004960
Iteration 45/1000 | Loss: 0.00004960
Iteration 46/1000 | Loss: 0.00004959
Iteration 47/1000 | Loss: 0.00004959
Iteration 48/1000 | Loss: 0.00004958
Iteration 49/1000 | Loss: 0.00004958
Iteration 50/1000 | Loss: 0.00004958
Iteration 51/1000 | Loss: 0.00004958
Iteration 52/1000 | Loss: 0.00004957
Iteration 53/1000 | Loss: 0.00004957
Iteration 54/1000 | Loss: 0.00004957
Iteration 55/1000 | Loss: 0.00004957
Iteration 56/1000 | Loss: 0.00004957
Iteration 57/1000 | Loss: 0.00004957
Iteration 58/1000 | Loss: 0.00004957
Iteration 59/1000 | Loss: 0.00004957
Iteration 60/1000 | Loss: 0.00004956
Iteration 61/1000 | Loss: 0.00004956
Iteration 62/1000 | Loss: 0.00004956
Iteration 63/1000 | Loss: 0.00004956
Iteration 64/1000 | Loss: 0.00004956
Iteration 65/1000 | Loss: 0.00004956
Iteration 66/1000 | Loss: 0.00004955
Iteration 67/1000 | Loss: 0.00004955
Iteration 68/1000 | Loss: 0.00004955
Iteration 69/1000 | Loss: 0.00004955
Iteration 70/1000 | Loss: 0.00004955
Iteration 71/1000 | Loss: 0.00004954
Iteration 72/1000 | Loss: 0.00004954
Iteration 73/1000 | Loss: 0.00004954
Iteration 74/1000 | Loss: 0.00004954
Iteration 75/1000 | Loss: 0.00004954
Iteration 76/1000 | Loss: 0.00004954
Iteration 77/1000 | Loss: 0.00004954
Iteration 78/1000 | Loss: 0.00004954
Iteration 79/1000 | Loss: 0.00004954
Iteration 80/1000 | Loss: 0.00004954
Iteration 81/1000 | Loss: 0.00004954
Iteration 82/1000 | Loss: 0.00004954
Iteration 83/1000 | Loss: 0.00004953
Iteration 84/1000 | Loss: 0.00004953
Iteration 85/1000 | Loss: 0.00004953
Iteration 86/1000 | Loss: 0.00004953
Iteration 87/1000 | Loss: 0.00004953
Iteration 88/1000 | Loss: 0.00004953
Iteration 89/1000 | Loss: 0.00004953
Iteration 90/1000 | Loss: 0.00004953
Iteration 91/1000 | Loss: 0.00004953
Iteration 92/1000 | Loss: 0.00004953
Iteration 93/1000 | Loss: 0.00004953
Iteration 94/1000 | Loss: 0.00004953
Iteration 95/1000 | Loss: 0.00004953
Iteration 96/1000 | Loss: 0.00004953
Iteration 97/1000 | Loss: 0.00004953
Iteration 98/1000 | Loss: 0.00004953
Iteration 99/1000 | Loss: 0.00004953
Iteration 100/1000 | Loss: 0.00004953
Iteration 101/1000 | Loss: 0.00004953
Iteration 102/1000 | Loss: 0.00004953
Iteration 103/1000 | Loss: 0.00004953
Iteration 104/1000 | Loss: 0.00004953
Iteration 105/1000 | Loss: 0.00004953
Iteration 106/1000 | Loss: 0.00004953
Iteration 107/1000 | Loss: 0.00004953
Iteration 108/1000 | Loss: 0.00004953
Iteration 109/1000 | Loss: 0.00004953
Iteration 110/1000 | Loss: 0.00004953
Iteration 111/1000 | Loss: 0.00004953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [4.953246389050037e-05, 4.953246389050037e-05, 4.953246389050037e-05, 4.953246389050037e-05, 4.953246389050037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.953246389050037e-05

Optimization complete. Final v2v error: 4.926290988922119 mm

Highest mean error: 5.433636665344238 mm for frame 77

Lowest mean error: 3.3605940341949463 mm for frame 18

Saving results

Total time: 64.77750277519226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039443
Iteration 2/25 | Loss: 0.00326566
Iteration 3/25 | Loss: 0.00188668
Iteration 4/25 | Loss: 0.00156091
Iteration 5/25 | Loss: 0.00144999
Iteration 6/25 | Loss: 0.00138966
Iteration 7/25 | Loss: 0.00138266
Iteration 8/25 | Loss: 0.00133185
Iteration 9/25 | Loss: 0.00128146
Iteration 10/25 | Loss: 0.00124073
Iteration 11/25 | Loss: 0.00120393
Iteration 12/25 | Loss: 0.00113834
Iteration 13/25 | Loss: 0.00111391
Iteration 14/25 | Loss: 0.00110552
Iteration 15/25 | Loss: 0.00109395
Iteration 16/25 | Loss: 0.00105838
Iteration 17/25 | Loss: 0.00104231
Iteration 18/25 | Loss: 0.00102832
Iteration 19/25 | Loss: 0.00102438
Iteration 20/25 | Loss: 0.00101649
Iteration 21/25 | Loss: 0.00100953
Iteration 22/25 | Loss: 0.00100316
Iteration 23/25 | Loss: 0.00100114
Iteration 24/25 | Loss: 0.00100628
Iteration 25/25 | Loss: 0.00100374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51247883
Iteration 2/25 | Loss: 0.00372485
Iteration 3/25 | Loss: 0.00308456
Iteration 4/25 | Loss: 0.00308456
Iteration 5/25 | Loss: 0.00308455
Iteration 6/25 | Loss: 0.00308455
Iteration 7/25 | Loss: 0.00308455
Iteration 8/25 | Loss: 0.00308455
Iteration 9/25 | Loss: 0.00308455
Iteration 10/25 | Loss: 0.00308455
Iteration 11/25 | Loss: 0.00308455
Iteration 12/25 | Loss: 0.00308455
Iteration 13/25 | Loss: 0.00308455
Iteration 14/25 | Loss: 0.00308455
Iteration 15/25 | Loss: 0.00308455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003084553172811866, 0.003084553172811866, 0.003084553172811866, 0.003084553172811866, 0.003084553172811866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003084553172811866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308455
Iteration 2/1000 | Loss: 0.00083327
Iteration 3/1000 | Loss: 0.00050351
Iteration 4/1000 | Loss: 0.00102188
Iteration 5/1000 | Loss: 0.00080815
Iteration 6/1000 | Loss: 0.00061133
Iteration 7/1000 | Loss: 0.00028539
Iteration 8/1000 | Loss: 0.00025879
Iteration 9/1000 | Loss: 0.00039520
Iteration 10/1000 | Loss: 0.00022744
Iteration 11/1000 | Loss: 0.00021942
Iteration 12/1000 | Loss: 0.00062211
Iteration 13/1000 | Loss: 0.00107597
Iteration 14/1000 | Loss: 0.00019767
Iteration 15/1000 | Loss: 0.00023162
Iteration 16/1000 | Loss: 0.00016943
Iteration 17/1000 | Loss: 0.00065981
Iteration 18/1000 | Loss: 0.00015879
Iteration 19/1000 | Loss: 0.00017351
Iteration 20/1000 | Loss: 0.00059262
Iteration 21/1000 | Loss: 0.00022635
Iteration 22/1000 | Loss: 0.00026793
Iteration 23/1000 | Loss: 0.00018162
Iteration 24/1000 | Loss: 0.00017272
Iteration 25/1000 | Loss: 0.00046259
Iteration 26/1000 | Loss: 0.00297052
Iteration 27/1000 | Loss: 0.00156602
Iteration 28/1000 | Loss: 0.00077186
Iteration 29/1000 | Loss: 0.00109655
Iteration 30/1000 | Loss: 0.00044036
Iteration 31/1000 | Loss: 0.00033637
Iteration 32/1000 | Loss: 0.00046905
Iteration 33/1000 | Loss: 0.00046640
Iteration 34/1000 | Loss: 0.00036606
Iteration 35/1000 | Loss: 0.00020964
Iteration 36/1000 | Loss: 0.00029650
Iteration 37/1000 | Loss: 0.00032447
Iteration 38/1000 | Loss: 0.00067476
Iteration 39/1000 | Loss: 0.00018144
Iteration 40/1000 | Loss: 0.00011496
Iteration 41/1000 | Loss: 0.00012094
Iteration 42/1000 | Loss: 0.00033205
Iteration 43/1000 | Loss: 0.00012361
Iteration 44/1000 | Loss: 0.00037930
Iteration 45/1000 | Loss: 0.00010135
Iteration 46/1000 | Loss: 0.00010842
Iteration 47/1000 | Loss: 0.00010697
Iteration 48/1000 | Loss: 0.00009353
Iteration 49/1000 | Loss: 0.00008879
Iteration 50/1000 | Loss: 0.00034201
Iteration 51/1000 | Loss: 0.00013261
Iteration 52/1000 | Loss: 0.00009140
Iteration 53/1000 | Loss: 0.00013960
Iteration 54/1000 | Loss: 0.00010258
Iteration 55/1000 | Loss: 0.00009858
Iteration 56/1000 | Loss: 0.00008998
Iteration 57/1000 | Loss: 0.00048661
Iteration 58/1000 | Loss: 0.00043244
Iteration 59/1000 | Loss: 0.00073754
Iteration 60/1000 | Loss: 0.00029300
Iteration 61/1000 | Loss: 0.00013646
Iteration 62/1000 | Loss: 0.00013462
Iteration 63/1000 | Loss: 0.00011510
Iteration 64/1000 | Loss: 0.00008758
Iteration 65/1000 | Loss: 0.00028082
Iteration 66/1000 | Loss: 0.00009129
Iteration 67/1000 | Loss: 0.00010049
Iteration 68/1000 | Loss: 0.00036795
Iteration 69/1000 | Loss: 0.00013720
Iteration 70/1000 | Loss: 0.00010964
Iteration 71/1000 | Loss: 0.00014076
Iteration 72/1000 | Loss: 0.00025113
Iteration 73/1000 | Loss: 0.00008731
Iteration 74/1000 | Loss: 0.00009745
Iteration 75/1000 | Loss: 0.00008440
Iteration 76/1000 | Loss: 0.00014227
Iteration 77/1000 | Loss: 0.00007747
Iteration 78/1000 | Loss: 0.00006926
Iteration 79/1000 | Loss: 0.00006719
Iteration 80/1000 | Loss: 0.00071201
Iteration 81/1000 | Loss: 0.00006810
Iteration 82/1000 | Loss: 0.00006371
Iteration 83/1000 | Loss: 0.00006340
Iteration 84/1000 | Loss: 0.00041300
Iteration 85/1000 | Loss: 0.00175255
Iteration 86/1000 | Loss: 0.00016698
Iteration 87/1000 | Loss: 0.00009370
Iteration 88/1000 | Loss: 0.00067400
Iteration 89/1000 | Loss: 0.00006408
Iteration 90/1000 | Loss: 0.00005717
Iteration 91/1000 | Loss: 0.00006212
Iteration 92/1000 | Loss: 0.00005161
Iteration 93/1000 | Loss: 0.00005474
Iteration 94/1000 | Loss: 0.00004893
Iteration 95/1000 | Loss: 0.00004855
Iteration 96/1000 | Loss: 0.00004748
Iteration 97/1000 | Loss: 0.00004763
Iteration 98/1000 | Loss: 0.00005201
Iteration 99/1000 | Loss: 0.00004604
Iteration 100/1000 | Loss: 0.00061344
Iteration 101/1000 | Loss: 0.00195589
Iteration 102/1000 | Loss: 0.00173239
Iteration 103/1000 | Loss: 0.00026601
Iteration 104/1000 | Loss: 0.00062555
Iteration 105/1000 | Loss: 0.00152667
Iteration 106/1000 | Loss: 0.00085955
Iteration 107/1000 | Loss: 0.00090296
Iteration 108/1000 | Loss: 0.00058424
Iteration 109/1000 | Loss: 0.00071493
Iteration 110/1000 | Loss: 0.00048553
Iteration 111/1000 | Loss: 0.00049315
Iteration 112/1000 | Loss: 0.00009685
Iteration 113/1000 | Loss: 0.00032890
Iteration 114/1000 | Loss: 0.00009055
Iteration 115/1000 | Loss: 0.00006464
Iteration 116/1000 | Loss: 0.00004981
Iteration 117/1000 | Loss: 0.00027717
Iteration 118/1000 | Loss: 0.00008611
Iteration 119/1000 | Loss: 0.00004301
Iteration 120/1000 | Loss: 0.00060003
Iteration 121/1000 | Loss: 0.00004297
Iteration 122/1000 | Loss: 0.00003930
Iteration 123/1000 | Loss: 0.00003749
Iteration 124/1000 | Loss: 0.00004268
Iteration 125/1000 | Loss: 0.00003610
Iteration 126/1000 | Loss: 0.00003505
Iteration 127/1000 | Loss: 0.00003472
Iteration 128/1000 | Loss: 0.00003765
Iteration 129/1000 | Loss: 0.00003437
Iteration 130/1000 | Loss: 0.00003453
Iteration 131/1000 | Loss: 0.00003417
Iteration 132/1000 | Loss: 0.00003403
Iteration 133/1000 | Loss: 0.00003396
Iteration 134/1000 | Loss: 0.00003549
Iteration 135/1000 | Loss: 0.00003807
Iteration 136/1000 | Loss: 0.00003388
Iteration 137/1000 | Loss: 0.00003388
Iteration 138/1000 | Loss: 0.00003388
Iteration 139/1000 | Loss: 0.00003388
Iteration 140/1000 | Loss: 0.00003388
Iteration 141/1000 | Loss: 0.00003386
Iteration 142/1000 | Loss: 0.00003383
Iteration 143/1000 | Loss: 0.00003382
Iteration 144/1000 | Loss: 0.00003381
Iteration 145/1000 | Loss: 0.00003379
Iteration 146/1000 | Loss: 0.00003377
Iteration 147/1000 | Loss: 0.00003376
Iteration 148/1000 | Loss: 0.00025140
Iteration 149/1000 | Loss: 0.00060250
Iteration 150/1000 | Loss: 0.00005250
Iteration 151/1000 | Loss: 0.00020890
Iteration 152/1000 | Loss: 0.00061950
Iteration 153/1000 | Loss: 0.00003524
Iteration 154/1000 | Loss: 0.00009619
Iteration 155/1000 | Loss: 0.00032555
Iteration 156/1000 | Loss: 0.00063638
Iteration 157/1000 | Loss: 0.00016536
Iteration 158/1000 | Loss: 0.00007456
Iteration 159/1000 | Loss: 0.00003235
Iteration 160/1000 | Loss: 0.00003119
Iteration 161/1000 | Loss: 0.00002982
Iteration 162/1000 | Loss: 0.00003173
Iteration 163/1000 | Loss: 0.00013712
Iteration 164/1000 | Loss: 0.00002987
Iteration 165/1000 | Loss: 0.00002826
Iteration 166/1000 | Loss: 0.00002807
Iteration 167/1000 | Loss: 0.00002805
Iteration 168/1000 | Loss: 0.00002782
Iteration 169/1000 | Loss: 0.00002782
Iteration 170/1000 | Loss: 0.00002776
Iteration 171/1000 | Loss: 0.00002757
Iteration 172/1000 | Loss: 0.00002742
Iteration 173/1000 | Loss: 0.00002738
Iteration 174/1000 | Loss: 0.00003157
Iteration 175/1000 | Loss: 0.00002733
Iteration 176/1000 | Loss: 0.00002808
Iteration 177/1000 | Loss: 0.00032003
Iteration 178/1000 | Loss: 0.00002987
Iteration 179/1000 | Loss: 0.00002793
Iteration 180/1000 | Loss: 0.00002783
Iteration 181/1000 | Loss: 0.00003537
Iteration 182/1000 | Loss: 0.00002576
Iteration 183/1000 | Loss: 0.00002567
Iteration 184/1000 | Loss: 0.00002538
Iteration 185/1000 | Loss: 0.00002534
Iteration 186/1000 | Loss: 0.00002808
Iteration 187/1000 | Loss: 0.00002588
Iteration 188/1000 | Loss: 0.00002588
Iteration 189/1000 | Loss: 0.00002522
Iteration 190/1000 | Loss: 0.00002522
Iteration 191/1000 | Loss: 0.00002522
Iteration 192/1000 | Loss: 0.00002521
Iteration 193/1000 | Loss: 0.00002521
Iteration 194/1000 | Loss: 0.00002521
Iteration 195/1000 | Loss: 0.00002521
Iteration 196/1000 | Loss: 0.00002521
Iteration 197/1000 | Loss: 0.00002521
Iteration 198/1000 | Loss: 0.00002521
Iteration 199/1000 | Loss: 0.00002521
Iteration 200/1000 | Loss: 0.00002518
Iteration 201/1000 | Loss: 0.00002517
Iteration 202/1000 | Loss: 0.00002516
Iteration 203/1000 | Loss: 0.00002516
Iteration 204/1000 | Loss: 0.00002515
Iteration 205/1000 | Loss: 0.00002514
Iteration 206/1000 | Loss: 0.00002514
Iteration 207/1000 | Loss: 0.00002512
Iteration 208/1000 | Loss: 0.00002511
Iteration 209/1000 | Loss: 0.00002508
Iteration 210/1000 | Loss: 0.00002508
Iteration 211/1000 | Loss: 0.00002523
Iteration 212/1000 | Loss: 0.00002506
Iteration 213/1000 | Loss: 0.00002506
Iteration 214/1000 | Loss: 0.00002506
Iteration 215/1000 | Loss: 0.00002506
Iteration 216/1000 | Loss: 0.00002505
Iteration 217/1000 | Loss: 0.00002504
Iteration 218/1000 | Loss: 0.00002504
Iteration 219/1000 | Loss: 0.00002504
Iteration 220/1000 | Loss: 0.00002504
Iteration 221/1000 | Loss: 0.00002504
Iteration 222/1000 | Loss: 0.00002504
Iteration 223/1000 | Loss: 0.00002504
Iteration 224/1000 | Loss: 0.00002504
Iteration 225/1000 | Loss: 0.00002504
Iteration 226/1000 | Loss: 0.00002504
Iteration 227/1000 | Loss: 0.00002504
Iteration 228/1000 | Loss: 0.00002504
Iteration 229/1000 | Loss: 0.00002503
Iteration 230/1000 | Loss: 0.00002502
Iteration 231/1000 | Loss: 0.00002502
Iteration 232/1000 | Loss: 0.00002502
Iteration 233/1000 | Loss: 0.00002501
Iteration 234/1000 | Loss: 0.00002501
Iteration 235/1000 | Loss: 0.00002501
Iteration 236/1000 | Loss: 0.00002501
Iteration 237/1000 | Loss: 0.00002501
Iteration 238/1000 | Loss: 0.00002500
Iteration 239/1000 | Loss: 0.00002500
Iteration 240/1000 | Loss: 0.00002500
Iteration 241/1000 | Loss: 0.00002500
Iteration 242/1000 | Loss: 0.00002499
Iteration 243/1000 | Loss: 0.00002499
Iteration 244/1000 | Loss: 0.00002498
Iteration 245/1000 | Loss: 0.00002498
Iteration 246/1000 | Loss: 0.00002498
Iteration 247/1000 | Loss: 0.00002498
Iteration 248/1000 | Loss: 0.00002498
Iteration 249/1000 | Loss: 0.00002498
Iteration 250/1000 | Loss: 0.00002498
Iteration 251/1000 | Loss: 0.00002497
Iteration 252/1000 | Loss: 0.00002497
Iteration 253/1000 | Loss: 0.00002497
Iteration 254/1000 | Loss: 0.00002497
Iteration 255/1000 | Loss: 0.00002497
Iteration 256/1000 | Loss: 0.00002497
Iteration 257/1000 | Loss: 0.00002497
Iteration 258/1000 | Loss: 0.00002497
Iteration 259/1000 | Loss: 0.00002497
Iteration 260/1000 | Loss: 0.00002496
Iteration 261/1000 | Loss: 0.00002496
Iteration 262/1000 | Loss: 0.00002496
Iteration 263/1000 | Loss: 0.00002495
Iteration 264/1000 | Loss: 0.00002495
Iteration 265/1000 | Loss: 0.00002494
Iteration 266/1000 | Loss: 0.00002494
Iteration 267/1000 | Loss: 0.00002493
Iteration 268/1000 | Loss: 0.00002493
Iteration 269/1000 | Loss: 0.00002493
Iteration 270/1000 | Loss: 0.00002493
Iteration 271/1000 | Loss: 0.00002493
Iteration 272/1000 | Loss: 0.00002493
Iteration 273/1000 | Loss: 0.00002493
Iteration 274/1000 | Loss: 0.00002493
Iteration 275/1000 | Loss: 0.00002493
Iteration 276/1000 | Loss: 0.00002493
Iteration 277/1000 | Loss: 0.00002492
Iteration 278/1000 | Loss: 0.00002492
Iteration 279/1000 | Loss: 0.00002492
Iteration 280/1000 | Loss: 0.00002492
Iteration 281/1000 | Loss: 0.00002492
Iteration 282/1000 | Loss: 0.00002492
Iteration 283/1000 | Loss: 0.00002492
Iteration 284/1000 | Loss: 0.00002492
Iteration 285/1000 | Loss: 0.00002492
Iteration 286/1000 | Loss: 0.00002492
Iteration 287/1000 | Loss: 0.00002491
Iteration 288/1000 | Loss: 0.00002491
Iteration 289/1000 | Loss: 0.00002491
Iteration 290/1000 | Loss: 0.00002490
Iteration 291/1000 | Loss: 0.00002490
Iteration 292/1000 | Loss: 0.00002490
Iteration 293/1000 | Loss: 0.00002490
Iteration 294/1000 | Loss: 0.00002490
Iteration 295/1000 | Loss: 0.00002490
Iteration 296/1000 | Loss: 0.00002490
Iteration 297/1000 | Loss: 0.00002489
Iteration 298/1000 | Loss: 0.00002489
Iteration 299/1000 | Loss: 0.00002489
Iteration 300/1000 | Loss: 0.00002489
Iteration 301/1000 | Loss: 0.00002489
Iteration 302/1000 | Loss: 0.00002489
Iteration 303/1000 | Loss: 0.00002489
Iteration 304/1000 | Loss: 0.00002489
Iteration 305/1000 | Loss: 0.00002489
Iteration 306/1000 | Loss: 0.00002489
Iteration 307/1000 | Loss: 0.00002489
Iteration 308/1000 | Loss: 0.00002489
Iteration 309/1000 | Loss: 0.00002489
Iteration 310/1000 | Loss: 0.00002489
Iteration 311/1000 | Loss: 0.00002489
Iteration 312/1000 | Loss: 0.00002489
Iteration 313/1000 | Loss: 0.00002489
Iteration 314/1000 | Loss: 0.00002489
Iteration 315/1000 | Loss: 0.00002489
Iteration 316/1000 | Loss: 0.00002488
Iteration 317/1000 | Loss: 0.00002488
Iteration 318/1000 | Loss: 0.00002488
Iteration 319/1000 | Loss: 0.00002488
Iteration 320/1000 | Loss: 0.00002488
Iteration 321/1000 | Loss: 0.00002488
Iteration 322/1000 | Loss: 0.00002488
Iteration 323/1000 | Loss: 0.00002488
Iteration 324/1000 | Loss: 0.00002488
Iteration 325/1000 | Loss: 0.00002488
Iteration 326/1000 | Loss: 0.00002488
Iteration 327/1000 | Loss: 0.00002488
Iteration 328/1000 | Loss: 0.00002488
Iteration 329/1000 | Loss: 0.00002488
Iteration 330/1000 | Loss: 0.00002488
Iteration 331/1000 | Loss: 0.00002488
Iteration 332/1000 | Loss: 0.00002488
Iteration 333/1000 | Loss: 0.00002488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [2.4879576812963933e-05, 2.4879576812963933e-05, 2.4879576812963933e-05, 2.4879576812963933e-05, 2.4879576812963933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4879576812963933e-05

Optimization complete. Final v2v error: 3.184947967529297 mm

Highest mean error: 11.357455253601074 mm for frame 165

Lowest mean error: 2.544687509536743 mm for frame 114

Saving results

Total time: 350.22390151023865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385915
Iteration 2/25 | Loss: 0.00082745
Iteration 3/25 | Loss: 0.00073992
Iteration 4/25 | Loss: 0.00072235
Iteration 5/25 | Loss: 0.00071615
Iteration 6/25 | Loss: 0.00071452
Iteration 7/25 | Loss: 0.00071434
Iteration 8/25 | Loss: 0.00071414
Iteration 9/25 | Loss: 0.00071414
Iteration 10/25 | Loss: 0.00071414
Iteration 11/25 | Loss: 0.00071414
Iteration 12/25 | Loss: 0.00071414
Iteration 13/25 | Loss: 0.00071414
Iteration 14/25 | Loss: 0.00071414
Iteration 15/25 | Loss: 0.00071414
Iteration 16/25 | Loss: 0.00071414
Iteration 17/25 | Loss: 0.00071414
Iteration 18/25 | Loss: 0.00071414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007141366368159652, 0.0007141366368159652, 0.0007141366368159652, 0.0007141366368159652, 0.0007141366368159652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007141366368159652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40984154
Iteration 2/25 | Loss: 0.00045754
Iteration 3/25 | Loss: 0.00045754
Iteration 4/25 | Loss: 0.00045754
Iteration 5/25 | Loss: 0.00045754
Iteration 6/25 | Loss: 0.00045754
Iteration 7/25 | Loss: 0.00045754
Iteration 8/25 | Loss: 0.00045754
Iteration 9/25 | Loss: 0.00045754
Iteration 10/25 | Loss: 0.00045754
Iteration 11/25 | Loss: 0.00045754
Iteration 12/25 | Loss: 0.00045754
Iteration 13/25 | Loss: 0.00045754
Iteration 14/25 | Loss: 0.00045754
Iteration 15/25 | Loss: 0.00045754
Iteration 16/25 | Loss: 0.00045754
Iteration 17/25 | Loss: 0.00045754
Iteration 18/25 | Loss: 0.00045754
Iteration 19/25 | Loss: 0.00045754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004575352359097451, 0.0004575352359097451, 0.0004575352359097451, 0.0004575352359097451, 0.0004575352359097451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004575352359097451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045754
Iteration 2/1000 | Loss: 0.00002193
Iteration 3/1000 | Loss: 0.00001432
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001271
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001234
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001215
Iteration 10/1000 | Loss: 0.00001212
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001208
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001179
Iteration 17/1000 | Loss: 0.00001177
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001171
Iteration 22/1000 | Loss: 0.00001170
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001153
Iteration 33/1000 | Loss: 0.00001153
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001153
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001150
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001149
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001148
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001144
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001133
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001127
Iteration 80/1000 | Loss: 0.00001127
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001125
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001124
Iteration 141/1000 | Loss: 0.00001124
Iteration 142/1000 | Loss: 0.00001124
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001124
Iteration 145/1000 | Loss: 0.00001124
Iteration 146/1000 | Loss: 0.00001123
Iteration 147/1000 | Loss: 0.00001123
Iteration 148/1000 | Loss: 0.00001123
Iteration 149/1000 | Loss: 0.00001123
Iteration 150/1000 | Loss: 0.00001123
Iteration 151/1000 | Loss: 0.00001123
Iteration 152/1000 | Loss: 0.00001123
Iteration 153/1000 | Loss: 0.00001123
Iteration 154/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.1232521501369774e-05, 1.1232521501369774e-05, 1.1232521501369774e-05, 1.1232521501369774e-05, 1.1232521501369774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1232521501369774e-05

Optimization complete. Final v2v error: 2.8889999389648438 mm

Highest mean error: 3.03572416305542 mm for frame 135

Lowest mean error: 2.815913438796997 mm for frame 65

Saving results

Total time: 64.76220989227295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919837
Iteration 2/25 | Loss: 0.00104019
Iteration 3/25 | Loss: 0.00086119
Iteration 4/25 | Loss: 0.00082014
Iteration 5/25 | Loss: 0.00080270
Iteration 6/25 | Loss: 0.00079915
Iteration 7/25 | Loss: 0.00079810
Iteration 8/25 | Loss: 0.00079805
Iteration 9/25 | Loss: 0.00079801
Iteration 10/25 | Loss: 0.00079801
Iteration 11/25 | Loss: 0.00079801
Iteration 12/25 | Loss: 0.00079801
Iteration 13/25 | Loss: 0.00079801
Iteration 14/25 | Loss: 0.00079801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007980145164765418, 0.0007980145164765418, 0.0007980145164765418, 0.0007980145164765418, 0.0007980145164765418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007980145164765418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49578071
Iteration 2/25 | Loss: 0.00046144
Iteration 3/25 | Loss: 0.00046143
Iteration 4/25 | Loss: 0.00046143
Iteration 5/25 | Loss: 0.00046143
Iteration 6/25 | Loss: 0.00046143
Iteration 7/25 | Loss: 0.00046143
Iteration 8/25 | Loss: 0.00046143
Iteration 9/25 | Loss: 0.00046143
Iteration 10/25 | Loss: 0.00046143
Iteration 11/25 | Loss: 0.00046143
Iteration 12/25 | Loss: 0.00046143
Iteration 13/25 | Loss: 0.00046143
Iteration 14/25 | Loss: 0.00046143
Iteration 15/25 | Loss: 0.00046143
Iteration 16/25 | Loss: 0.00046143
Iteration 17/25 | Loss: 0.00046143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000461427349364385, 0.000461427349364385, 0.000461427349364385, 0.000461427349364385, 0.000461427349364385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000461427349364385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046143
Iteration 2/1000 | Loss: 0.00005153
Iteration 3/1000 | Loss: 0.00003287
Iteration 4/1000 | Loss: 0.00002901
Iteration 5/1000 | Loss: 0.00002757
Iteration 6/1000 | Loss: 0.00002649
Iteration 7/1000 | Loss: 0.00002565
Iteration 8/1000 | Loss: 0.00002493
Iteration 9/1000 | Loss: 0.00002438
Iteration 10/1000 | Loss: 0.00002410
Iteration 11/1000 | Loss: 0.00002388
Iteration 12/1000 | Loss: 0.00002383
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002376
Iteration 15/1000 | Loss: 0.00002365
Iteration 16/1000 | Loss: 0.00002358
Iteration 17/1000 | Loss: 0.00002358
Iteration 18/1000 | Loss: 0.00002357
Iteration 19/1000 | Loss: 0.00002357
Iteration 20/1000 | Loss: 0.00002355
Iteration 21/1000 | Loss: 0.00002354
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002351
Iteration 24/1000 | Loss: 0.00002350
Iteration 25/1000 | Loss: 0.00002349
Iteration 26/1000 | Loss: 0.00002348
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002341
Iteration 29/1000 | Loss: 0.00002341
Iteration 30/1000 | Loss: 0.00002339
Iteration 31/1000 | Loss: 0.00002339
Iteration 32/1000 | Loss: 0.00002338
Iteration 33/1000 | Loss: 0.00002338
Iteration 34/1000 | Loss: 0.00002337
Iteration 35/1000 | Loss: 0.00002337
Iteration 36/1000 | Loss: 0.00002336
Iteration 37/1000 | Loss: 0.00002336
Iteration 38/1000 | Loss: 0.00002333
Iteration 39/1000 | Loss: 0.00002333
Iteration 40/1000 | Loss: 0.00002333
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002332
Iteration 43/1000 | Loss: 0.00002332
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002330
Iteration 48/1000 | Loss: 0.00002330
Iteration 49/1000 | Loss: 0.00002330
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002329
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00002329
Iteration 56/1000 | Loss: 0.00002329
Iteration 57/1000 | Loss: 0.00002329
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002328
Iteration 60/1000 | Loss: 0.00002328
Iteration 61/1000 | Loss: 0.00002327
Iteration 62/1000 | Loss: 0.00002327
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002326
Iteration 65/1000 | Loss: 0.00002326
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002326
Iteration 68/1000 | Loss: 0.00002325
Iteration 69/1000 | Loss: 0.00002325
Iteration 70/1000 | Loss: 0.00002324
Iteration 71/1000 | Loss: 0.00002323
Iteration 72/1000 | Loss: 0.00002323
Iteration 73/1000 | Loss: 0.00002322
Iteration 74/1000 | Loss: 0.00002322
Iteration 75/1000 | Loss: 0.00002322
Iteration 76/1000 | Loss: 0.00002321
Iteration 77/1000 | Loss: 0.00002321
Iteration 78/1000 | Loss: 0.00002321
Iteration 79/1000 | Loss: 0.00002320
Iteration 80/1000 | Loss: 0.00002319
Iteration 81/1000 | Loss: 0.00002319
Iteration 82/1000 | Loss: 0.00002318
Iteration 83/1000 | Loss: 0.00002317
Iteration 84/1000 | Loss: 0.00002317
Iteration 85/1000 | Loss: 0.00002317
Iteration 86/1000 | Loss: 0.00002316
Iteration 87/1000 | Loss: 0.00002316
Iteration 88/1000 | Loss: 0.00002316
Iteration 89/1000 | Loss: 0.00002316
Iteration 90/1000 | Loss: 0.00002316
Iteration 91/1000 | Loss: 0.00002315
Iteration 92/1000 | Loss: 0.00002315
Iteration 93/1000 | Loss: 0.00002315
Iteration 94/1000 | Loss: 0.00002315
Iteration 95/1000 | Loss: 0.00002315
Iteration 96/1000 | Loss: 0.00002315
Iteration 97/1000 | Loss: 0.00002315
Iteration 98/1000 | Loss: 0.00002314
Iteration 99/1000 | Loss: 0.00002314
Iteration 100/1000 | Loss: 0.00002314
Iteration 101/1000 | Loss: 0.00002313
Iteration 102/1000 | Loss: 0.00002313
Iteration 103/1000 | Loss: 0.00002313
Iteration 104/1000 | Loss: 0.00002313
Iteration 105/1000 | Loss: 0.00002313
Iteration 106/1000 | Loss: 0.00002313
Iteration 107/1000 | Loss: 0.00002313
Iteration 108/1000 | Loss: 0.00002313
Iteration 109/1000 | Loss: 0.00002313
Iteration 110/1000 | Loss: 0.00002313
Iteration 111/1000 | Loss: 0.00002313
Iteration 112/1000 | Loss: 0.00002312
Iteration 113/1000 | Loss: 0.00002312
Iteration 114/1000 | Loss: 0.00002312
Iteration 115/1000 | Loss: 0.00002312
Iteration 116/1000 | Loss: 0.00002311
Iteration 117/1000 | Loss: 0.00002311
Iteration 118/1000 | Loss: 0.00002311
Iteration 119/1000 | Loss: 0.00002311
Iteration 120/1000 | Loss: 0.00002311
Iteration 121/1000 | Loss: 0.00002311
Iteration 122/1000 | Loss: 0.00002311
Iteration 123/1000 | Loss: 0.00002311
Iteration 124/1000 | Loss: 0.00002310
Iteration 125/1000 | Loss: 0.00002310
Iteration 126/1000 | Loss: 0.00002310
Iteration 127/1000 | Loss: 0.00002310
Iteration 128/1000 | Loss: 0.00002310
Iteration 129/1000 | Loss: 0.00002310
Iteration 130/1000 | Loss: 0.00002310
Iteration 131/1000 | Loss: 0.00002310
Iteration 132/1000 | Loss: 0.00002310
Iteration 133/1000 | Loss: 0.00002310
Iteration 134/1000 | Loss: 0.00002309
Iteration 135/1000 | Loss: 0.00002309
Iteration 136/1000 | Loss: 0.00002309
Iteration 137/1000 | Loss: 0.00002309
Iteration 138/1000 | Loss: 0.00002309
Iteration 139/1000 | Loss: 0.00002309
Iteration 140/1000 | Loss: 0.00002308
Iteration 141/1000 | Loss: 0.00002308
Iteration 142/1000 | Loss: 0.00002308
Iteration 143/1000 | Loss: 0.00002308
Iteration 144/1000 | Loss: 0.00002308
Iteration 145/1000 | Loss: 0.00002308
Iteration 146/1000 | Loss: 0.00002308
Iteration 147/1000 | Loss: 0.00002308
Iteration 148/1000 | Loss: 0.00002308
Iteration 149/1000 | Loss: 0.00002308
Iteration 150/1000 | Loss: 0.00002308
Iteration 151/1000 | Loss: 0.00002308
Iteration 152/1000 | Loss: 0.00002308
Iteration 153/1000 | Loss: 0.00002308
Iteration 154/1000 | Loss: 0.00002307
Iteration 155/1000 | Loss: 0.00002307
Iteration 156/1000 | Loss: 0.00002307
Iteration 157/1000 | Loss: 0.00002307
Iteration 158/1000 | Loss: 0.00002307
Iteration 159/1000 | Loss: 0.00002306
Iteration 160/1000 | Loss: 0.00002306
Iteration 161/1000 | Loss: 0.00002306
Iteration 162/1000 | Loss: 0.00002306
Iteration 163/1000 | Loss: 0.00002306
Iteration 164/1000 | Loss: 0.00002306
Iteration 165/1000 | Loss: 0.00002306
Iteration 166/1000 | Loss: 0.00002306
Iteration 167/1000 | Loss: 0.00002306
Iteration 168/1000 | Loss: 0.00002306
Iteration 169/1000 | Loss: 0.00002306
Iteration 170/1000 | Loss: 0.00002306
Iteration 171/1000 | Loss: 0.00002306
Iteration 172/1000 | Loss: 0.00002305
Iteration 173/1000 | Loss: 0.00002305
Iteration 174/1000 | Loss: 0.00002305
Iteration 175/1000 | Loss: 0.00002305
Iteration 176/1000 | Loss: 0.00002304
Iteration 177/1000 | Loss: 0.00002304
Iteration 178/1000 | Loss: 0.00002304
Iteration 179/1000 | Loss: 0.00002304
Iteration 180/1000 | Loss: 0.00002304
Iteration 181/1000 | Loss: 0.00002304
Iteration 182/1000 | Loss: 0.00002304
Iteration 183/1000 | Loss: 0.00002304
Iteration 184/1000 | Loss: 0.00002304
Iteration 185/1000 | Loss: 0.00002304
Iteration 186/1000 | Loss: 0.00002304
Iteration 187/1000 | Loss: 0.00002304
Iteration 188/1000 | Loss: 0.00002304
Iteration 189/1000 | Loss: 0.00002304
Iteration 190/1000 | Loss: 0.00002304
Iteration 191/1000 | Loss: 0.00002303
Iteration 192/1000 | Loss: 0.00002303
Iteration 193/1000 | Loss: 0.00002303
Iteration 194/1000 | Loss: 0.00002303
Iteration 195/1000 | Loss: 0.00002303
Iteration 196/1000 | Loss: 0.00002303
Iteration 197/1000 | Loss: 0.00002303
Iteration 198/1000 | Loss: 0.00002303
Iteration 199/1000 | Loss: 0.00002303
Iteration 200/1000 | Loss: 0.00002303
Iteration 201/1000 | Loss: 0.00002303
Iteration 202/1000 | Loss: 0.00002303
Iteration 203/1000 | Loss: 0.00002303
Iteration 204/1000 | Loss: 0.00002302
Iteration 205/1000 | Loss: 0.00002302
Iteration 206/1000 | Loss: 0.00002302
Iteration 207/1000 | Loss: 0.00002302
Iteration 208/1000 | Loss: 0.00002302
Iteration 209/1000 | Loss: 0.00002302
Iteration 210/1000 | Loss: 0.00002302
Iteration 211/1000 | Loss: 0.00002302
Iteration 212/1000 | Loss: 0.00002302
Iteration 213/1000 | Loss: 0.00002302
Iteration 214/1000 | Loss: 0.00002302
Iteration 215/1000 | Loss: 0.00002302
Iteration 216/1000 | Loss: 0.00002302
Iteration 217/1000 | Loss: 0.00002302
Iteration 218/1000 | Loss: 0.00002302
Iteration 219/1000 | Loss: 0.00002302
Iteration 220/1000 | Loss: 0.00002301
Iteration 221/1000 | Loss: 0.00002301
Iteration 222/1000 | Loss: 0.00002301
Iteration 223/1000 | Loss: 0.00002301
Iteration 224/1000 | Loss: 0.00002301
Iteration 225/1000 | Loss: 0.00002301
Iteration 226/1000 | Loss: 0.00002301
Iteration 227/1000 | Loss: 0.00002301
Iteration 228/1000 | Loss: 0.00002301
Iteration 229/1000 | Loss: 0.00002301
Iteration 230/1000 | Loss: 0.00002301
Iteration 231/1000 | Loss: 0.00002301
Iteration 232/1000 | Loss: 0.00002301
Iteration 233/1000 | Loss: 0.00002301
Iteration 234/1000 | Loss: 0.00002300
Iteration 235/1000 | Loss: 0.00002300
Iteration 236/1000 | Loss: 0.00002300
Iteration 237/1000 | Loss: 0.00002300
Iteration 238/1000 | Loss: 0.00002300
Iteration 239/1000 | Loss: 0.00002300
Iteration 240/1000 | Loss: 0.00002300
Iteration 241/1000 | Loss: 0.00002300
Iteration 242/1000 | Loss: 0.00002300
Iteration 243/1000 | Loss: 0.00002300
Iteration 244/1000 | Loss: 0.00002300
Iteration 245/1000 | Loss: 0.00002300
Iteration 246/1000 | Loss: 0.00002300
Iteration 247/1000 | Loss: 0.00002300
Iteration 248/1000 | Loss: 0.00002300
Iteration 249/1000 | Loss: 0.00002300
Iteration 250/1000 | Loss: 0.00002299
Iteration 251/1000 | Loss: 0.00002299
Iteration 252/1000 | Loss: 0.00002299
Iteration 253/1000 | Loss: 0.00002299
Iteration 254/1000 | Loss: 0.00002299
Iteration 255/1000 | Loss: 0.00002299
Iteration 256/1000 | Loss: 0.00002299
Iteration 257/1000 | Loss: 0.00002299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [2.2994658138486557e-05, 2.2994658138486557e-05, 2.2994658138486557e-05, 2.2994658138486557e-05, 2.2994658138486557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2994658138486557e-05

Optimization complete. Final v2v error: 4.025992393493652 mm

Highest mean error: 5.727395534515381 mm for frame 68

Lowest mean error: 3.5959129333496094 mm for frame 93

Saving results

Total time: 95.57207894325256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443686
Iteration 2/25 | Loss: 0.00090049
Iteration 3/25 | Loss: 0.00076683
Iteration 4/25 | Loss: 0.00074375
Iteration 5/25 | Loss: 0.00073470
Iteration 6/25 | Loss: 0.00073189
Iteration 7/25 | Loss: 0.00073117
Iteration 8/25 | Loss: 0.00073117
Iteration 9/25 | Loss: 0.00073117
Iteration 10/25 | Loss: 0.00073117
Iteration 11/25 | Loss: 0.00073117
Iteration 12/25 | Loss: 0.00073117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007311729714274406, 0.0007311729714274406, 0.0007311729714274406, 0.0007311729714274406, 0.0007311729714274406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007311729714274406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56923115
Iteration 2/25 | Loss: 0.00049951
Iteration 3/25 | Loss: 0.00049950
Iteration 4/25 | Loss: 0.00049950
Iteration 5/25 | Loss: 0.00049950
Iteration 6/25 | Loss: 0.00049950
Iteration 7/25 | Loss: 0.00049950
Iteration 8/25 | Loss: 0.00049950
Iteration 9/25 | Loss: 0.00049950
Iteration 10/25 | Loss: 0.00049949
Iteration 11/25 | Loss: 0.00049949
Iteration 12/25 | Loss: 0.00049949
Iteration 13/25 | Loss: 0.00049949
Iteration 14/25 | Loss: 0.00049949
Iteration 15/25 | Loss: 0.00049949
Iteration 16/25 | Loss: 0.00049949
Iteration 17/25 | Loss: 0.00049949
Iteration 18/25 | Loss: 0.00049949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004994946648366749, 0.0004994946648366749, 0.0004994946648366749, 0.0004994946648366749, 0.0004994946648366749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004994946648366749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049949
Iteration 2/1000 | Loss: 0.00002876
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001394
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001324
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001295
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001292
Iteration 17/1000 | Loss: 0.00001289
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001287
Iteration 21/1000 | Loss: 0.00001286
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001283
Iteration 26/1000 | Loss: 0.00001282
Iteration 27/1000 | Loss: 0.00001282
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001281
Iteration 31/1000 | Loss: 0.00001280
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001272
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001272
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001269
Iteration 59/1000 | Loss: 0.00001269
Iteration 60/1000 | Loss: 0.00001269
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001268
Iteration 63/1000 | Loss: 0.00001268
Iteration 64/1000 | Loss: 0.00001268
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001267
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001267
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001266
Iteration 75/1000 | Loss: 0.00001266
Iteration 76/1000 | Loss: 0.00001266
Iteration 77/1000 | Loss: 0.00001266
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001265
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001261
Iteration 143/1000 | Loss: 0.00001260
Iteration 144/1000 | Loss: 0.00001260
Iteration 145/1000 | Loss: 0.00001260
Iteration 146/1000 | Loss: 0.00001260
Iteration 147/1000 | Loss: 0.00001260
Iteration 148/1000 | Loss: 0.00001260
Iteration 149/1000 | Loss: 0.00001260
Iteration 150/1000 | Loss: 0.00001260
Iteration 151/1000 | Loss: 0.00001260
Iteration 152/1000 | Loss: 0.00001260
Iteration 153/1000 | Loss: 0.00001259
Iteration 154/1000 | Loss: 0.00001259
Iteration 155/1000 | Loss: 0.00001259
Iteration 156/1000 | Loss: 0.00001259
Iteration 157/1000 | Loss: 0.00001259
Iteration 158/1000 | Loss: 0.00001258
Iteration 159/1000 | Loss: 0.00001258
Iteration 160/1000 | Loss: 0.00001258
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001257
Iteration 164/1000 | Loss: 0.00001257
Iteration 165/1000 | Loss: 0.00001257
Iteration 166/1000 | Loss: 0.00001257
Iteration 167/1000 | Loss: 0.00001256
Iteration 168/1000 | Loss: 0.00001256
Iteration 169/1000 | Loss: 0.00001256
Iteration 170/1000 | Loss: 0.00001256
Iteration 171/1000 | Loss: 0.00001256
Iteration 172/1000 | Loss: 0.00001256
Iteration 173/1000 | Loss: 0.00001256
Iteration 174/1000 | Loss: 0.00001255
Iteration 175/1000 | Loss: 0.00001255
Iteration 176/1000 | Loss: 0.00001255
Iteration 177/1000 | Loss: 0.00001255
Iteration 178/1000 | Loss: 0.00001255
Iteration 179/1000 | Loss: 0.00001255
Iteration 180/1000 | Loss: 0.00001255
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001254
Iteration 184/1000 | Loss: 0.00001254
Iteration 185/1000 | Loss: 0.00001254
Iteration 186/1000 | Loss: 0.00001254
Iteration 187/1000 | Loss: 0.00001254
Iteration 188/1000 | Loss: 0.00001254
Iteration 189/1000 | Loss: 0.00001254
Iteration 190/1000 | Loss: 0.00001254
Iteration 191/1000 | Loss: 0.00001254
Iteration 192/1000 | Loss: 0.00001254
Iteration 193/1000 | Loss: 0.00001253
Iteration 194/1000 | Loss: 0.00001253
Iteration 195/1000 | Loss: 0.00001253
Iteration 196/1000 | Loss: 0.00001253
Iteration 197/1000 | Loss: 0.00001253
Iteration 198/1000 | Loss: 0.00001253
Iteration 199/1000 | Loss: 0.00001253
Iteration 200/1000 | Loss: 0.00001252
Iteration 201/1000 | Loss: 0.00001252
Iteration 202/1000 | Loss: 0.00001252
Iteration 203/1000 | Loss: 0.00001252
Iteration 204/1000 | Loss: 0.00001252
Iteration 205/1000 | Loss: 0.00001252
Iteration 206/1000 | Loss: 0.00001252
Iteration 207/1000 | Loss: 0.00001252
Iteration 208/1000 | Loss: 0.00001252
Iteration 209/1000 | Loss: 0.00001252
Iteration 210/1000 | Loss: 0.00001252
Iteration 211/1000 | Loss: 0.00001252
Iteration 212/1000 | Loss: 0.00001252
Iteration 213/1000 | Loss: 0.00001252
Iteration 214/1000 | Loss: 0.00001252
Iteration 215/1000 | Loss: 0.00001252
Iteration 216/1000 | Loss: 0.00001252
Iteration 217/1000 | Loss: 0.00001252
Iteration 218/1000 | Loss: 0.00001252
Iteration 219/1000 | Loss: 0.00001252
Iteration 220/1000 | Loss: 0.00001252
Iteration 221/1000 | Loss: 0.00001252
Iteration 222/1000 | Loss: 0.00001252
Iteration 223/1000 | Loss: 0.00001252
Iteration 224/1000 | Loss: 0.00001252
Iteration 225/1000 | Loss: 0.00001252
Iteration 226/1000 | Loss: 0.00001252
Iteration 227/1000 | Loss: 0.00001252
Iteration 228/1000 | Loss: 0.00001252
Iteration 229/1000 | Loss: 0.00001252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.2518670700956136e-05, 1.2518670700956136e-05, 1.2518670700956136e-05, 1.2518670700956136e-05, 1.2518670700956136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2518670700956136e-05

Optimization complete. Final v2v error: 3.0150749683380127 mm

Highest mean error: 3.3179619312286377 mm for frame 131

Lowest mean error: 2.76051664352417 mm for frame 205

Saving results

Total time: 40.89349126815796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01168940
Iteration 2/25 | Loss: 0.01168940
Iteration 3/25 | Loss: 0.01168940
Iteration 4/25 | Loss: 0.01168940
Iteration 5/25 | Loss: 0.01168939
Iteration 6/25 | Loss: 0.01168939
Iteration 7/25 | Loss: 0.01168939
Iteration 8/25 | Loss: 0.01168939
Iteration 9/25 | Loss: 0.01168939
Iteration 10/25 | Loss: 0.01168939
Iteration 11/25 | Loss: 0.01168939
Iteration 12/25 | Loss: 0.01168939
Iteration 13/25 | Loss: 0.01168939
Iteration 14/25 | Loss: 0.01168939
Iteration 15/25 | Loss: 0.01168939
Iteration 16/25 | Loss: 0.01168939
Iteration 17/25 | Loss: 0.01168939
Iteration 18/25 | Loss: 0.01168939
Iteration 19/25 | Loss: 0.01168939
Iteration 20/25 | Loss: 0.01168938
Iteration 21/25 | Loss: 0.01168938
Iteration 22/25 | Loss: 0.01168938
Iteration 23/25 | Loss: 0.01168938
Iteration 24/25 | Loss: 0.01168938
Iteration 25/25 | Loss: 0.01168938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.71779823
Iteration 2/25 | Loss: 0.17882995
Iteration 3/25 | Loss: 0.17625710
Iteration 4/25 | Loss: 0.17609501
Iteration 5/25 | Loss: 0.17609501
Iteration 6/25 | Loss: 0.17609501
Iteration 7/25 | Loss: 0.17609501
Iteration 8/25 | Loss: 0.17609499
Iteration 9/25 | Loss: 0.17609499
Iteration 10/25 | Loss: 0.17609499
Iteration 11/25 | Loss: 0.17609499
Iteration 12/25 | Loss: 0.17609499
Iteration 13/25 | Loss: 0.17609499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17609499394893646, 0.17609499394893646, 0.17609499394893646, 0.17609499394893646, 0.17609499394893646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17609499394893646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17609499
Iteration 2/1000 | Loss: 0.00290669
Iteration 3/1000 | Loss: 0.00144431
Iteration 4/1000 | Loss: 0.00035099
Iteration 5/1000 | Loss: 0.00012766
Iteration 6/1000 | Loss: 0.00007563
Iteration 7/1000 | Loss: 0.00005519
Iteration 8/1000 | Loss: 0.00006422
Iteration 9/1000 | Loss: 0.00004403
Iteration 10/1000 | Loss: 0.00009682
Iteration 11/1000 | Loss: 0.00003412
Iteration 12/1000 | Loss: 0.00003697
Iteration 13/1000 | Loss: 0.00003213
Iteration 14/1000 | Loss: 0.00004826
Iteration 15/1000 | Loss: 0.00006058
Iteration 16/1000 | Loss: 0.00002617
Iteration 17/1000 | Loss: 0.00004203
Iteration 18/1000 | Loss: 0.00002413
Iteration 19/1000 | Loss: 0.00003315
Iteration 20/1000 | Loss: 0.00003240
Iteration 21/1000 | Loss: 0.00018923
Iteration 22/1000 | Loss: 0.00003928
Iteration 23/1000 | Loss: 0.00002737
Iteration 24/1000 | Loss: 0.00004360
Iteration 25/1000 | Loss: 0.00002025
Iteration 26/1000 | Loss: 0.00002063
Iteration 27/1000 | Loss: 0.00004160
Iteration 28/1000 | Loss: 0.00002038
Iteration 29/1000 | Loss: 0.00002878
Iteration 30/1000 | Loss: 0.00004193
Iteration 31/1000 | Loss: 0.00002453
Iteration 32/1000 | Loss: 0.00002228
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00003299
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001780
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001780
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001780
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.7796044630813412e-05, 1.7796044630813412e-05, 1.7796044630813412e-05, 1.7796044630813412e-05, 1.7796044630813412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7796044630813412e-05

Optimization complete. Final v2v error: 3.59220027923584 mm

Highest mean error: 3.926949977874756 mm for frame 38

Lowest mean error: 3.323049545288086 mm for frame 2

Saving results

Total time: 64.01164412498474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097977
Iteration 2/25 | Loss: 0.00155302
Iteration 3/25 | Loss: 0.00111007
Iteration 4/25 | Loss: 0.00100916
Iteration 5/25 | Loss: 0.00097471
Iteration 6/25 | Loss: 0.00095985
Iteration 7/25 | Loss: 0.00093941
Iteration 8/25 | Loss: 0.00092752
Iteration 9/25 | Loss: 0.00092170
Iteration 10/25 | Loss: 0.00091764
Iteration 11/25 | Loss: 0.00091588
Iteration 12/25 | Loss: 0.00091528
Iteration 13/25 | Loss: 0.00091507
Iteration 14/25 | Loss: 0.00091504
Iteration 15/25 | Loss: 0.00091504
Iteration 16/25 | Loss: 0.00091504
Iteration 17/25 | Loss: 0.00091504
Iteration 18/25 | Loss: 0.00091504
Iteration 19/25 | Loss: 0.00091504
Iteration 20/25 | Loss: 0.00091504
Iteration 21/25 | Loss: 0.00091504
Iteration 22/25 | Loss: 0.00091504
Iteration 23/25 | Loss: 0.00091503
Iteration 24/25 | Loss: 0.00091503
Iteration 25/25 | Loss: 0.00091503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.20466614
Iteration 2/25 | Loss: 0.00069118
Iteration 3/25 | Loss: 0.00069094
Iteration 4/25 | Loss: 0.00069094
Iteration 5/25 | Loss: 0.00069094
Iteration 6/25 | Loss: 0.00069094
Iteration 7/25 | Loss: 0.00069094
Iteration 8/25 | Loss: 0.00069094
Iteration 9/25 | Loss: 0.00069094
Iteration 10/25 | Loss: 0.00069094
Iteration 11/25 | Loss: 0.00069094
Iteration 12/25 | Loss: 0.00069094
Iteration 13/25 | Loss: 0.00069094
Iteration 14/25 | Loss: 0.00069094
Iteration 15/25 | Loss: 0.00069094
Iteration 16/25 | Loss: 0.00069094
Iteration 17/25 | Loss: 0.00069094
Iteration 18/25 | Loss: 0.00069094
Iteration 19/25 | Loss: 0.00069094
Iteration 20/25 | Loss: 0.00069094
Iteration 21/25 | Loss: 0.00069094
Iteration 22/25 | Loss: 0.00069094
Iteration 23/25 | Loss: 0.00069094
Iteration 24/25 | Loss: 0.00069094
Iteration 25/25 | Loss: 0.00069094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069094
Iteration 2/1000 | Loss: 0.00008692
Iteration 3/1000 | Loss: 0.00006072
Iteration 4/1000 | Loss: 0.00005059
Iteration 5/1000 | Loss: 0.00004704
Iteration 6/1000 | Loss: 0.00004497
Iteration 7/1000 | Loss: 0.00004336
Iteration 8/1000 | Loss: 0.00004235
Iteration 9/1000 | Loss: 0.00004161
Iteration 10/1000 | Loss: 0.00004096
Iteration 11/1000 | Loss: 0.00004057
Iteration 12/1000 | Loss: 0.00004024
Iteration 13/1000 | Loss: 0.00003998
Iteration 14/1000 | Loss: 0.00003974
Iteration 15/1000 | Loss: 0.00003952
Iteration 16/1000 | Loss: 0.00003934
Iteration 17/1000 | Loss: 0.00003920
Iteration 18/1000 | Loss: 0.00003909
Iteration 19/1000 | Loss: 0.00003904
Iteration 20/1000 | Loss: 0.00003902
Iteration 21/1000 | Loss: 0.00003898
Iteration 22/1000 | Loss: 0.00003897
Iteration 23/1000 | Loss: 0.00003897
Iteration 24/1000 | Loss: 0.00003896
Iteration 25/1000 | Loss: 0.00003896
Iteration 26/1000 | Loss: 0.00003895
Iteration 27/1000 | Loss: 0.00003895
Iteration 28/1000 | Loss: 0.00003895
Iteration 29/1000 | Loss: 0.00003894
Iteration 30/1000 | Loss: 0.00003893
Iteration 31/1000 | Loss: 0.00003893
Iteration 32/1000 | Loss: 0.00003891
Iteration 33/1000 | Loss: 0.00003891
Iteration 34/1000 | Loss: 0.00003888
Iteration 35/1000 | Loss: 0.00010916
Iteration 36/1000 | Loss: 0.00028857
Iteration 37/1000 | Loss: 0.00003884
Iteration 38/1000 | Loss: 0.00003881
Iteration 39/1000 | Loss: 0.00003881
Iteration 40/1000 | Loss: 0.00003881
Iteration 41/1000 | Loss: 0.00003880
Iteration 42/1000 | Loss: 0.00003880
Iteration 43/1000 | Loss: 0.00003880
Iteration 44/1000 | Loss: 0.00003880
Iteration 45/1000 | Loss: 0.00003880
Iteration 46/1000 | Loss: 0.00003880
Iteration 47/1000 | Loss: 0.00003880
Iteration 48/1000 | Loss: 0.00003880
Iteration 49/1000 | Loss: 0.00003880
Iteration 50/1000 | Loss: 0.00003880
Iteration 51/1000 | Loss: 0.00003880
Iteration 52/1000 | Loss: 0.00003880
Iteration 53/1000 | Loss: 0.00003879
Iteration 54/1000 | Loss: 0.00003879
Iteration 55/1000 | Loss: 0.00010727
Iteration 56/1000 | Loss: 0.00004743
Iteration 57/1000 | Loss: 0.00004273
Iteration 58/1000 | Loss: 0.00003897
Iteration 59/1000 | Loss: 0.00003873
Iteration 60/1000 | Loss: 0.00003873
Iteration 61/1000 | Loss: 0.00003872
Iteration 62/1000 | Loss: 0.00003872
Iteration 63/1000 | Loss: 0.00003872
Iteration 64/1000 | Loss: 0.00003872
Iteration 65/1000 | Loss: 0.00003872
Iteration 66/1000 | Loss: 0.00003872
Iteration 67/1000 | Loss: 0.00003872
Iteration 68/1000 | Loss: 0.00003872
Iteration 69/1000 | Loss: 0.00003872
Iteration 70/1000 | Loss: 0.00003871
Iteration 71/1000 | Loss: 0.00003871
Iteration 72/1000 | Loss: 0.00003871
Iteration 73/1000 | Loss: 0.00003871
Iteration 74/1000 | Loss: 0.00003871
Iteration 75/1000 | Loss: 0.00003871
Iteration 76/1000 | Loss: 0.00003871
Iteration 77/1000 | Loss: 0.00003871
Iteration 78/1000 | Loss: 0.00003871
Iteration 79/1000 | Loss: 0.00003870
Iteration 80/1000 | Loss: 0.00003869
Iteration 81/1000 | Loss: 0.00003869
Iteration 82/1000 | Loss: 0.00003868
Iteration 83/1000 | Loss: 0.00003868
Iteration 84/1000 | Loss: 0.00003868
Iteration 85/1000 | Loss: 0.00003868
Iteration 86/1000 | Loss: 0.00003867
Iteration 87/1000 | Loss: 0.00003867
Iteration 88/1000 | Loss: 0.00003867
Iteration 89/1000 | Loss: 0.00003867
Iteration 90/1000 | Loss: 0.00003867
Iteration 91/1000 | Loss: 0.00003866
Iteration 92/1000 | Loss: 0.00003866
Iteration 93/1000 | Loss: 0.00003866
Iteration 94/1000 | Loss: 0.00003866
Iteration 95/1000 | Loss: 0.00003866
Iteration 96/1000 | Loss: 0.00003866
Iteration 97/1000 | Loss: 0.00003865
Iteration 98/1000 | Loss: 0.00003865
Iteration 99/1000 | Loss: 0.00003864
Iteration 100/1000 | Loss: 0.00003862
Iteration 101/1000 | Loss: 0.00003862
Iteration 102/1000 | Loss: 0.00003862
Iteration 103/1000 | Loss: 0.00003862
Iteration 104/1000 | Loss: 0.00003862
Iteration 105/1000 | Loss: 0.00003862
Iteration 106/1000 | Loss: 0.00003862
Iteration 107/1000 | Loss: 0.00003862
Iteration 108/1000 | Loss: 0.00003862
Iteration 109/1000 | Loss: 0.00003861
Iteration 110/1000 | Loss: 0.00003861
Iteration 111/1000 | Loss: 0.00003860
Iteration 112/1000 | Loss: 0.00003859
Iteration 113/1000 | Loss: 0.00003859
Iteration 114/1000 | Loss: 0.00003859
Iteration 115/1000 | Loss: 0.00003858
Iteration 116/1000 | Loss: 0.00003858
Iteration 117/1000 | Loss: 0.00003858
Iteration 118/1000 | Loss: 0.00003857
Iteration 119/1000 | Loss: 0.00003857
Iteration 120/1000 | Loss: 0.00003857
Iteration 121/1000 | Loss: 0.00003857
Iteration 122/1000 | Loss: 0.00003856
Iteration 123/1000 | Loss: 0.00003856
Iteration 124/1000 | Loss: 0.00003856
Iteration 125/1000 | Loss: 0.00003856
Iteration 126/1000 | Loss: 0.00003855
Iteration 127/1000 | Loss: 0.00003855
Iteration 128/1000 | Loss: 0.00003855
Iteration 129/1000 | Loss: 0.00003855
Iteration 130/1000 | Loss: 0.00003854
Iteration 131/1000 | Loss: 0.00003854
Iteration 132/1000 | Loss: 0.00003854
Iteration 133/1000 | Loss: 0.00003853
Iteration 134/1000 | Loss: 0.00003853
Iteration 135/1000 | Loss: 0.00003853
Iteration 136/1000 | Loss: 0.00003853
Iteration 137/1000 | Loss: 0.00003853
Iteration 138/1000 | Loss: 0.00003853
Iteration 139/1000 | Loss: 0.00003852
Iteration 140/1000 | Loss: 0.00003852
Iteration 141/1000 | Loss: 0.00003852
Iteration 142/1000 | Loss: 0.00003852
Iteration 143/1000 | Loss: 0.00003852
Iteration 144/1000 | Loss: 0.00003852
Iteration 145/1000 | Loss: 0.00003852
Iteration 146/1000 | Loss: 0.00003851
Iteration 147/1000 | Loss: 0.00003851
Iteration 148/1000 | Loss: 0.00003851
Iteration 149/1000 | Loss: 0.00003851
Iteration 150/1000 | Loss: 0.00003851
Iteration 151/1000 | Loss: 0.00003851
Iteration 152/1000 | Loss: 0.00003850
Iteration 153/1000 | Loss: 0.00003850
Iteration 154/1000 | Loss: 0.00003850
Iteration 155/1000 | Loss: 0.00003850
Iteration 156/1000 | Loss: 0.00003850
Iteration 157/1000 | Loss: 0.00003850
Iteration 158/1000 | Loss: 0.00003850
Iteration 159/1000 | Loss: 0.00003850
Iteration 160/1000 | Loss: 0.00003849
Iteration 161/1000 | Loss: 0.00003849
Iteration 162/1000 | Loss: 0.00003849
Iteration 163/1000 | Loss: 0.00003849
Iteration 164/1000 | Loss: 0.00003849
Iteration 165/1000 | Loss: 0.00003848
Iteration 166/1000 | Loss: 0.00003848
Iteration 167/1000 | Loss: 0.00003848
Iteration 168/1000 | Loss: 0.00003848
Iteration 169/1000 | Loss: 0.00003848
Iteration 170/1000 | Loss: 0.00003848
Iteration 171/1000 | Loss: 0.00003847
Iteration 172/1000 | Loss: 0.00003847
Iteration 173/1000 | Loss: 0.00003847
Iteration 174/1000 | Loss: 0.00003847
Iteration 175/1000 | Loss: 0.00003847
Iteration 176/1000 | Loss: 0.00003847
Iteration 177/1000 | Loss: 0.00003847
Iteration 178/1000 | Loss: 0.00003847
Iteration 179/1000 | Loss: 0.00003847
Iteration 180/1000 | Loss: 0.00003847
Iteration 181/1000 | Loss: 0.00003847
Iteration 182/1000 | Loss: 0.00003847
Iteration 183/1000 | Loss: 0.00003847
Iteration 184/1000 | Loss: 0.00003846
Iteration 185/1000 | Loss: 0.00003846
Iteration 186/1000 | Loss: 0.00003846
Iteration 187/1000 | Loss: 0.00003846
Iteration 188/1000 | Loss: 0.00003846
Iteration 189/1000 | Loss: 0.00003846
Iteration 190/1000 | Loss: 0.00003846
Iteration 191/1000 | Loss: 0.00003846
Iteration 192/1000 | Loss: 0.00003846
Iteration 193/1000 | Loss: 0.00003846
Iteration 194/1000 | Loss: 0.00003846
Iteration 195/1000 | Loss: 0.00003846
Iteration 196/1000 | Loss: 0.00003846
Iteration 197/1000 | Loss: 0.00003845
Iteration 198/1000 | Loss: 0.00003845
Iteration 199/1000 | Loss: 0.00003845
Iteration 200/1000 | Loss: 0.00003845
Iteration 201/1000 | Loss: 0.00003845
Iteration 202/1000 | Loss: 0.00003845
Iteration 203/1000 | Loss: 0.00003845
Iteration 204/1000 | Loss: 0.00003845
Iteration 205/1000 | Loss: 0.00003845
Iteration 206/1000 | Loss: 0.00003845
Iteration 207/1000 | Loss: 0.00003845
Iteration 208/1000 | Loss: 0.00003845
Iteration 209/1000 | Loss: 0.00003845
Iteration 210/1000 | Loss: 0.00003845
Iteration 211/1000 | Loss: 0.00003845
Iteration 212/1000 | Loss: 0.00003845
Iteration 213/1000 | Loss: 0.00003845
Iteration 214/1000 | Loss: 0.00003844
Iteration 215/1000 | Loss: 0.00003844
Iteration 216/1000 | Loss: 0.00003844
Iteration 217/1000 | Loss: 0.00003844
Iteration 218/1000 | Loss: 0.00003843
Iteration 219/1000 | Loss: 0.00003843
Iteration 220/1000 | Loss: 0.00003843
Iteration 221/1000 | Loss: 0.00003843
Iteration 222/1000 | Loss: 0.00003842
Iteration 223/1000 | Loss: 0.00003842
Iteration 224/1000 | Loss: 0.00003842
Iteration 225/1000 | Loss: 0.00003842
Iteration 226/1000 | Loss: 0.00003842
Iteration 227/1000 | Loss: 0.00003842
Iteration 228/1000 | Loss: 0.00003841
Iteration 229/1000 | Loss: 0.00003841
Iteration 230/1000 | Loss: 0.00003841
Iteration 231/1000 | Loss: 0.00003841
Iteration 232/1000 | Loss: 0.00003841
Iteration 233/1000 | Loss: 0.00003841
Iteration 234/1000 | Loss: 0.00003841
Iteration 235/1000 | Loss: 0.00003841
Iteration 236/1000 | Loss: 0.00003840
Iteration 237/1000 | Loss: 0.00003840
Iteration 238/1000 | Loss: 0.00003840
Iteration 239/1000 | Loss: 0.00003840
Iteration 240/1000 | Loss: 0.00003840
Iteration 241/1000 | Loss: 0.00003840
Iteration 242/1000 | Loss: 0.00003840
Iteration 243/1000 | Loss: 0.00003840
Iteration 244/1000 | Loss: 0.00003840
Iteration 245/1000 | Loss: 0.00003840
Iteration 246/1000 | Loss: 0.00003840
Iteration 247/1000 | Loss: 0.00003840
Iteration 248/1000 | Loss: 0.00003840
Iteration 249/1000 | Loss: 0.00003840
Iteration 250/1000 | Loss: 0.00003840
Iteration 251/1000 | Loss: 0.00003840
Iteration 252/1000 | Loss: 0.00003840
Iteration 253/1000 | Loss: 0.00003840
Iteration 254/1000 | Loss: 0.00003840
Iteration 255/1000 | Loss: 0.00003840
Iteration 256/1000 | Loss: 0.00003840
Iteration 257/1000 | Loss: 0.00003840
Iteration 258/1000 | Loss: 0.00003840
Iteration 259/1000 | Loss: 0.00003840
Iteration 260/1000 | Loss: 0.00003840
Iteration 261/1000 | Loss: 0.00003840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [3.8400237826863304e-05, 3.8400237826863304e-05, 3.8400237826863304e-05, 3.8400237826863304e-05, 3.8400237826863304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8400237826863304e-05

Optimization complete. Final v2v error: 4.9397406578063965 mm

Highest mean error: 7.886464595794678 mm for frame 99

Lowest mean error: 3.5580966472625732 mm for frame 140

Saving results

Total time: 74.50749206542969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794259
Iteration 2/25 | Loss: 0.00136144
Iteration 3/25 | Loss: 0.00105744
Iteration 4/25 | Loss: 0.00093230
Iteration 5/25 | Loss: 0.00088842
Iteration 6/25 | Loss: 0.00088267
Iteration 7/25 | Loss: 0.00087648
Iteration 8/25 | Loss: 0.00087844
Iteration 9/25 | Loss: 0.00087710
Iteration 10/25 | Loss: 0.00087631
Iteration 11/25 | Loss: 0.00087701
Iteration 12/25 | Loss: 0.00087695
Iteration 13/25 | Loss: 0.00087760
Iteration 14/25 | Loss: 0.00087393
Iteration 15/25 | Loss: 0.00087157
Iteration 16/25 | Loss: 0.00087124
Iteration 17/25 | Loss: 0.00087120
Iteration 18/25 | Loss: 0.00087120
Iteration 19/25 | Loss: 0.00087120
Iteration 20/25 | Loss: 0.00087119
Iteration 21/25 | Loss: 0.00087119
Iteration 22/25 | Loss: 0.00087119
Iteration 23/25 | Loss: 0.00087119
Iteration 24/25 | Loss: 0.00087119
Iteration 25/25 | Loss: 0.00087119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22238231
Iteration 2/25 | Loss: 0.00057383
Iteration 3/25 | Loss: 0.00057383
Iteration 4/25 | Loss: 0.00057383
Iteration 5/25 | Loss: 0.00057383
Iteration 6/25 | Loss: 0.00057383
Iteration 7/25 | Loss: 0.00057383
Iteration 8/25 | Loss: 0.00057383
Iteration 9/25 | Loss: 0.00057383
Iteration 10/25 | Loss: 0.00057383
Iteration 11/25 | Loss: 0.00057382
Iteration 12/25 | Loss: 0.00057382
Iteration 13/25 | Loss: 0.00057382
Iteration 14/25 | Loss: 0.00057382
Iteration 15/25 | Loss: 0.00057382
Iteration 16/25 | Loss: 0.00057382
Iteration 17/25 | Loss: 0.00057382
Iteration 18/25 | Loss: 0.00057382
Iteration 19/25 | Loss: 0.00057382
Iteration 20/25 | Loss: 0.00057382
Iteration 21/25 | Loss: 0.00057382
Iteration 22/25 | Loss: 0.00057382
Iteration 23/25 | Loss: 0.00057382
Iteration 24/25 | Loss: 0.00057382
Iteration 25/25 | Loss: 0.00057382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057382
Iteration 2/1000 | Loss: 0.00004002
Iteration 3/1000 | Loss: 0.00004724
Iteration 4/1000 | Loss: 0.00002964
Iteration 5/1000 | Loss: 0.00002854
Iteration 6/1000 | Loss: 0.00002780
Iteration 7/1000 | Loss: 0.00002728
Iteration 8/1000 | Loss: 0.00002685
Iteration 9/1000 | Loss: 0.00002649
Iteration 10/1000 | Loss: 0.00002649
Iteration 11/1000 | Loss: 0.00002634
Iteration 12/1000 | Loss: 0.00002615
Iteration 13/1000 | Loss: 0.00002595
Iteration 14/1000 | Loss: 0.00002580
Iteration 15/1000 | Loss: 0.00002575
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002565
Iteration 18/1000 | Loss: 0.00002563
Iteration 19/1000 | Loss: 0.00002557
Iteration 20/1000 | Loss: 0.00002557
Iteration 21/1000 | Loss: 0.00002556
Iteration 22/1000 | Loss: 0.00002555
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002552
Iteration 25/1000 | Loss: 0.00002552
Iteration 26/1000 | Loss: 0.00002552
Iteration 27/1000 | Loss: 0.00002552
Iteration 28/1000 | Loss: 0.00002552
Iteration 29/1000 | Loss: 0.00002552
Iteration 30/1000 | Loss: 0.00002551
Iteration 31/1000 | Loss: 0.00002551
Iteration 32/1000 | Loss: 0.00002551
Iteration 33/1000 | Loss: 0.00002551
Iteration 34/1000 | Loss: 0.00002551
Iteration 35/1000 | Loss: 0.00002551
Iteration 36/1000 | Loss: 0.00002551
Iteration 37/1000 | Loss: 0.00002551
Iteration 38/1000 | Loss: 0.00002551
Iteration 39/1000 | Loss: 0.00002551
Iteration 40/1000 | Loss: 0.00002551
Iteration 41/1000 | Loss: 0.00002551
Iteration 42/1000 | Loss: 0.00002550
Iteration 43/1000 | Loss: 0.00002550
Iteration 44/1000 | Loss: 0.00002550
Iteration 45/1000 | Loss: 0.00002550
Iteration 46/1000 | Loss: 0.00002550
Iteration 47/1000 | Loss: 0.00002550
Iteration 48/1000 | Loss: 0.00002549
Iteration 49/1000 | Loss: 0.00002549
Iteration 50/1000 | Loss: 0.00002549
Iteration 51/1000 | Loss: 0.00002548
Iteration 52/1000 | Loss: 0.00002548
Iteration 53/1000 | Loss: 0.00002548
Iteration 54/1000 | Loss: 0.00002548
Iteration 55/1000 | Loss: 0.00002548
Iteration 56/1000 | Loss: 0.00002548
Iteration 57/1000 | Loss: 0.00002548
Iteration 58/1000 | Loss: 0.00002547
Iteration 59/1000 | Loss: 0.00002547
Iteration 60/1000 | Loss: 0.00002547
Iteration 61/1000 | Loss: 0.00002547
Iteration 62/1000 | Loss: 0.00002547
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00002547
Iteration 65/1000 | Loss: 0.00002547
Iteration 66/1000 | Loss: 0.00002547
Iteration 67/1000 | Loss: 0.00002547
Iteration 68/1000 | Loss: 0.00002547
Iteration 69/1000 | Loss: 0.00002547
Iteration 70/1000 | Loss: 0.00002547
Iteration 71/1000 | Loss: 0.00002547
Iteration 72/1000 | Loss: 0.00002546
Iteration 73/1000 | Loss: 0.00002546
Iteration 74/1000 | Loss: 0.00002546
Iteration 75/1000 | Loss: 0.00002546
Iteration 76/1000 | Loss: 0.00002546
Iteration 77/1000 | Loss: 0.00002546
Iteration 78/1000 | Loss: 0.00002546
Iteration 79/1000 | Loss: 0.00002545
Iteration 80/1000 | Loss: 0.00002545
Iteration 81/1000 | Loss: 0.00002545
Iteration 82/1000 | Loss: 0.00002545
Iteration 83/1000 | Loss: 0.00002545
Iteration 84/1000 | Loss: 0.00002545
Iteration 85/1000 | Loss: 0.00002545
Iteration 86/1000 | Loss: 0.00002545
Iteration 87/1000 | Loss: 0.00002545
Iteration 88/1000 | Loss: 0.00002545
Iteration 89/1000 | Loss: 0.00002545
Iteration 90/1000 | Loss: 0.00002544
Iteration 91/1000 | Loss: 0.00002544
Iteration 92/1000 | Loss: 0.00002544
Iteration 93/1000 | Loss: 0.00002544
Iteration 94/1000 | Loss: 0.00002544
Iteration 95/1000 | Loss: 0.00002544
Iteration 96/1000 | Loss: 0.00002544
Iteration 97/1000 | Loss: 0.00002543
Iteration 98/1000 | Loss: 0.00002543
Iteration 99/1000 | Loss: 0.00002543
Iteration 100/1000 | Loss: 0.00002543
Iteration 101/1000 | Loss: 0.00002543
Iteration 102/1000 | Loss: 0.00002543
Iteration 103/1000 | Loss: 0.00002543
Iteration 104/1000 | Loss: 0.00002543
Iteration 105/1000 | Loss: 0.00002543
Iteration 106/1000 | Loss: 0.00002543
Iteration 107/1000 | Loss: 0.00002543
Iteration 108/1000 | Loss: 0.00002543
Iteration 109/1000 | Loss: 0.00002543
Iteration 110/1000 | Loss: 0.00002543
Iteration 111/1000 | Loss: 0.00002543
Iteration 112/1000 | Loss: 0.00002543
Iteration 113/1000 | Loss: 0.00002542
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002542
Iteration 116/1000 | Loss: 0.00002542
Iteration 117/1000 | Loss: 0.00002542
Iteration 118/1000 | Loss: 0.00002541
Iteration 119/1000 | Loss: 0.00002541
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002541
Iteration 123/1000 | Loss: 0.00002541
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002539
Iteration 131/1000 | Loss: 0.00002539
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002539
Iteration 134/1000 | Loss: 0.00002539
Iteration 135/1000 | Loss: 0.00002539
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002539
Iteration 138/1000 | Loss: 0.00002539
Iteration 139/1000 | Loss: 0.00002539
Iteration 140/1000 | Loss: 0.00002539
Iteration 141/1000 | Loss: 0.00002539
Iteration 142/1000 | Loss: 0.00002539
Iteration 143/1000 | Loss: 0.00002539
Iteration 144/1000 | Loss: 0.00002539
Iteration 145/1000 | Loss: 0.00002539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.538640001148451e-05, 2.538640001148451e-05, 2.538640001148451e-05, 2.538640001148451e-05, 2.538640001148451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.538640001148451e-05

Optimization complete. Final v2v error: 4.242445945739746 mm

Highest mean error: 4.9275078773498535 mm for frame 118

Lowest mean error: 3.3039896488189697 mm for frame 193

Saving results

Total time: 64.3578429222107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062398
Iteration 2/25 | Loss: 0.00287221
Iteration 3/25 | Loss: 0.00235662
Iteration 4/25 | Loss: 0.00213125
Iteration 5/25 | Loss: 0.00194945
Iteration 6/25 | Loss: 0.00178244
Iteration 7/25 | Loss: 0.00169598
Iteration 8/25 | Loss: 0.00164877
Iteration 9/25 | Loss: 0.00158272
Iteration 10/25 | Loss: 0.00153076
Iteration 11/25 | Loss: 0.00144751
Iteration 12/25 | Loss: 0.00141102
Iteration 13/25 | Loss: 0.00136835
Iteration 14/25 | Loss: 0.00135517
Iteration 15/25 | Loss: 0.00135681
Iteration 16/25 | Loss: 0.00130839
Iteration 17/25 | Loss: 0.00127428
Iteration 18/25 | Loss: 0.00125980
Iteration 19/25 | Loss: 0.00124643
Iteration 20/25 | Loss: 0.00123850
Iteration 21/25 | Loss: 0.00121642
Iteration 22/25 | Loss: 0.00120915
Iteration 23/25 | Loss: 0.00121030
Iteration 24/25 | Loss: 0.00120348
Iteration 25/25 | Loss: 0.00119737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48748803
Iteration 2/25 | Loss: 0.00410140
Iteration 3/25 | Loss: 0.00410137
Iteration 4/25 | Loss: 0.00410136
Iteration 5/25 | Loss: 0.00410136
Iteration 6/25 | Loss: 0.00410136
Iteration 7/25 | Loss: 0.00410136
Iteration 8/25 | Loss: 0.00410136
Iteration 9/25 | Loss: 0.00410136
Iteration 10/25 | Loss: 0.00410136
Iteration 11/25 | Loss: 0.00410136
Iteration 12/25 | Loss: 0.00410136
Iteration 13/25 | Loss: 0.00410136
Iteration 14/25 | Loss: 0.00410136
Iteration 15/25 | Loss: 0.00410136
Iteration 16/25 | Loss: 0.00410136
Iteration 17/25 | Loss: 0.00410136
Iteration 18/25 | Loss: 0.00410136
Iteration 19/25 | Loss: 0.00410136
Iteration 20/25 | Loss: 0.00410136
Iteration 21/25 | Loss: 0.00410136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004101361148059368, 0.004101361148059368, 0.004101361148059368, 0.004101361148059368, 0.004101361148059368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004101361148059368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00410136
Iteration 2/1000 | Loss: 0.01056352
Iteration 3/1000 | Loss: 0.00102487
Iteration 4/1000 | Loss: 0.00052200
Iteration 5/1000 | Loss: 0.00037176
Iteration 6/1000 | Loss: 0.00025320
Iteration 7/1000 | Loss: 0.00017833
Iteration 8/1000 | Loss: 0.00014544
Iteration 9/1000 | Loss: 0.00012806
Iteration 10/1000 | Loss: 0.00011707
Iteration 11/1000 | Loss: 0.00048070
Iteration 12/1000 | Loss: 0.00021944
Iteration 13/1000 | Loss: 0.00044184
Iteration 14/1000 | Loss: 0.00021795
Iteration 15/1000 | Loss: 0.00010093
Iteration 16/1000 | Loss: 0.00047958
Iteration 17/1000 | Loss: 0.00011961
Iteration 18/1000 | Loss: 0.00011497
Iteration 19/1000 | Loss: 0.00009683
Iteration 20/1000 | Loss: 0.00009097
Iteration 21/1000 | Loss: 0.00008552
Iteration 22/1000 | Loss: 0.00008166
Iteration 23/1000 | Loss: 0.00007804
Iteration 24/1000 | Loss: 0.00007475
Iteration 25/1000 | Loss: 0.00007279
Iteration 26/1000 | Loss: 0.00047506
Iteration 27/1000 | Loss: 0.00008869
Iteration 28/1000 | Loss: 0.00007503
Iteration 29/1000 | Loss: 0.00007245
Iteration 30/1000 | Loss: 0.00086331
Iteration 31/1000 | Loss: 0.00691795
Iteration 32/1000 | Loss: 0.00056789
Iteration 33/1000 | Loss: 0.00020631
Iteration 34/1000 | Loss: 0.00014073
Iteration 35/1000 | Loss: 0.00010737
Iteration 36/1000 | Loss: 0.00007788
Iteration 37/1000 | Loss: 0.00021942
Iteration 38/1000 | Loss: 0.00006442
Iteration 39/1000 | Loss: 0.00004530
Iteration 40/1000 | Loss: 0.00003796
Iteration 41/1000 | Loss: 0.00003462
Iteration 42/1000 | Loss: 0.00003149
Iteration 43/1000 | Loss: 0.00002823
Iteration 44/1000 | Loss: 0.00002639
Iteration 45/1000 | Loss: 0.00023075
Iteration 46/1000 | Loss: 0.00003235
Iteration 47/1000 | Loss: 0.00002463
Iteration 48/1000 | Loss: 0.00002200
Iteration 49/1000 | Loss: 0.00002025
Iteration 50/1000 | Loss: 0.00001914
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001647
Iteration 58/1000 | Loss: 0.00002385
Iteration 59/1000 | Loss: 0.00001671
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001587
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001559
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001528
Iteration 93/1000 | Loss: 0.00001528
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001528
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001526
Iteration 106/1000 | Loss: 0.00001526
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001526
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001525
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00001525
Iteration 138/1000 | Loss: 0.00001525
Iteration 139/1000 | Loss: 0.00001525
Iteration 140/1000 | Loss: 0.00001525
Iteration 141/1000 | Loss: 0.00001525
Iteration 142/1000 | Loss: 0.00001525
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001525
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5252207958837971e-05, 1.5252207958837971e-05, 1.5252207958837971e-05, 1.5252207958837971e-05, 1.5252207958837971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5252207958837971e-05

Optimization complete. Final v2v error: 3.2494516372680664 mm

Highest mean error: 3.907407283782959 mm for frame 104

Lowest mean error: 3.0941948890686035 mm for frame 50

Saving results

Total time: 141.23232984542847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840819
Iteration 2/25 | Loss: 0.00150143
Iteration 3/25 | Loss: 0.00095660
Iteration 4/25 | Loss: 0.00083818
Iteration 5/25 | Loss: 0.00082997
Iteration 6/25 | Loss: 0.00082991
Iteration 7/25 | Loss: 0.00082991
Iteration 8/25 | Loss: 0.00082991
Iteration 9/25 | Loss: 0.00082991
Iteration 10/25 | Loss: 0.00082991
Iteration 11/25 | Loss: 0.00082991
Iteration 12/25 | Loss: 0.00082991
Iteration 13/25 | Loss: 0.00082991
Iteration 14/25 | Loss: 0.00082991
Iteration 15/25 | Loss: 0.00082991
Iteration 16/25 | Loss: 0.00082991
Iteration 17/25 | Loss: 0.00082991
Iteration 18/25 | Loss: 0.00082991
Iteration 19/25 | Loss: 0.00082991
Iteration 20/25 | Loss: 0.00082991
Iteration 21/25 | Loss: 0.00082991
Iteration 22/25 | Loss: 0.00082991
Iteration 23/25 | Loss: 0.00082991
Iteration 24/25 | Loss: 0.00082991
Iteration 25/25 | Loss: 0.00082991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49498641
Iteration 2/25 | Loss: 0.00043375
Iteration 3/25 | Loss: 0.00043375
Iteration 4/25 | Loss: 0.00043375
Iteration 5/25 | Loss: 0.00043375
Iteration 6/25 | Loss: 0.00043375
Iteration 7/25 | Loss: 0.00043375
Iteration 8/25 | Loss: 0.00043375
Iteration 9/25 | Loss: 0.00043375
Iteration 10/25 | Loss: 0.00043375
Iteration 11/25 | Loss: 0.00043375
Iteration 12/25 | Loss: 0.00043375
Iteration 13/25 | Loss: 0.00043375
Iteration 14/25 | Loss: 0.00043375
Iteration 15/25 | Loss: 0.00043375
Iteration 16/25 | Loss: 0.00043375
Iteration 17/25 | Loss: 0.00043375
Iteration 18/25 | Loss: 0.00043375
Iteration 19/25 | Loss: 0.00043375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004337491118349135, 0.0004337491118349135, 0.0004337491118349135, 0.0004337491118349135, 0.0004337491118349135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004337491118349135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043375
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002297
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002018
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001972
Iteration 10/1000 | Loss: 0.00001951
Iteration 11/1000 | Loss: 0.00001943
Iteration 12/1000 | Loss: 0.00001938
Iteration 13/1000 | Loss: 0.00001931
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001930
Iteration 16/1000 | Loss: 0.00001929
Iteration 17/1000 | Loss: 0.00001927
Iteration 18/1000 | Loss: 0.00001927
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001926
Iteration 21/1000 | Loss: 0.00001925
Iteration 22/1000 | Loss: 0.00001925
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001922
Iteration 25/1000 | Loss: 0.00001921
Iteration 26/1000 | Loss: 0.00001920
Iteration 27/1000 | Loss: 0.00001920
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001918
Iteration 31/1000 | Loss: 0.00001918
Iteration 32/1000 | Loss: 0.00001918
Iteration 33/1000 | Loss: 0.00001918
Iteration 34/1000 | Loss: 0.00001917
Iteration 35/1000 | Loss: 0.00001917
Iteration 36/1000 | Loss: 0.00001917
Iteration 37/1000 | Loss: 0.00001917
Iteration 38/1000 | Loss: 0.00001917
Iteration 39/1000 | Loss: 0.00001917
Iteration 40/1000 | Loss: 0.00001916
Iteration 41/1000 | Loss: 0.00001916
Iteration 42/1000 | Loss: 0.00001916
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001913
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001912
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001911
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001907
Iteration 70/1000 | Loss: 0.00001907
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001906
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001904
Iteration 89/1000 | Loss: 0.00001904
Iteration 90/1000 | Loss: 0.00001904
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001903
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001902
Iteration 107/1000 | Loss: 0.00001902
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Iteration 112/1000 | Loss: 0.00001902
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001902
Iteration 115/1000 | Loss: 0.00001902
Iteration 116/1000 | Loss: 0.00001902
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001901
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001901
Iteration 123/1000 | Loss: 0.00001901
Iteration 124/1000 | Loss: 0.00001901
Iteration 125/1000 | Loss: 0.00001901
Iteration 126/1000 | Loss: 0.00001901
Iteration 127/1000 | Loss: 0.00001901
Iteration 128/1000 | Loss: 0.00001901
Iteration 129/1000 | Loss: 0.00001901
Iteration 130/1000 | Loss: 0.00001901
Iteration 131/1000 | Loss: 0.00001901
Iteration 132/1000 | Loss: 0.00001901
Iteration 133/1000 | Loss: 0.00001901
Iteration 134/1000 | Loss: 0.00001901
Iteration 135/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.9010987671208568e-05, 1.9010987671208568e-05, 1.9010987671208568e-05, 1.9010987671208568e-05, 1.9010987671208568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9010987671208568e-05

Optimization complete. Final v2v error: 3.582077980041504 mm

Highest mean error: 3.6429057121276855 mm for frame 153

Lowest mean error: 3.2988553047180176 mm for frame 2

Saving results

Total time: 64.44930291175842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514000
Iteration 2/25 | Loss: 0.00096521
Iteration 3/25 | Loss: 0.00082163
Iteration 4/25 | Loss: 0.00079536
Iteration 5/25 | Loss: 0.00078976
Iteration 6/25 | Loss: 0.00078918
Iteration 7/25 | Loss: 0.00078918
Iteration 8/25 | Loss: 0.00078918
Iteration 9/25 | Loss: 0.00078918
Iteration 10/25 | Loss: 0.00078918
Iteration 11/25 | Loss: 0.00078918
Iteration 12/25 | Loss: 0.00078918
Iteration 13/25 | Loss: 0.00078918
Iteration 14/25 | Loss: 0.00078918
Iteration 15/25 | Loss: 0.00078918
Iteration 16/25 | Loss: 0.00078918
Iteration 17/25 | Loss: 0.00078918
Iteration 18/25 | Loss: 0.00078918
Iteration 19/25 | Loss: 0.00078918
Iteration 20/25 | Loss: 0.00078918
Iteration 21/25 | Loss: 0.00078918
Iteration 22/25 | Loss: 0.00078918
Iteration 23/25 | Loss: 0.00078918
Iteration 24/25 | Loss: 0.00078918
Iteration 25/25 | Loss: 0.00078918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50471354
Iteration 2/25 | Loss: 0.00042314
Iteration 3/25 | Loss: 0.00042310
Iteration 4/25 | Loss: 0.00042310
Iteration 5/25 | Loss: 0.00042310
Iteration 6/25 | Loss: 0.00042310
Iteration 7/25 | Loss: 0.00042310
Iteration 8/25 | Loss: 0.00042310
Iteration 9/25 | Loss: 0.00042310
Iteration 10/25 | Loss: 0.00042310
Iteration 11/25 | Loss: 0.00042310
Iteration 12/25 | Loss: 0.00042310
Iteration 13/25 | Loss: 0.00042310
Iteration 14/25 | Loss: 0.00042310
Iteration 15/25 | Loss: 0.00042310
Iteration 16/25 | Loss: 0.00042310
Iteration 17/25 | Loss: 0.00042310
Iteration 18/25 | Loss: 0.00042310
Iteration 19/25 | Loss: 0.00042310
Iteration 20/25 | Loss: 0.00042310
Iteration 21/25 | Loss: 0.00042310
Iteration 22/25 | Loss: 0.00042310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00042310135904699564, 0.00042310135904699564, 0.00042310135904699564, 0.00042310135904699564, 0.00042310135904699564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042310135904699564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042310
Iteration 2/1000 | Loss: 0.00003497
Iteration 3/1000 | Loss: 0.00002350
Iteration 4/1000 | Loss: 0.00002143
Iteration 5/1000 | Loss: 0.00002003
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001867
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001752
Iteration 17/1000 | Loss: 0.00001746
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001745
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00001744
Iteration 23/1000 | Loss: 0.00001743
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001740
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001738
Iteration 37/1000 | Loss: 0.00001738
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001737
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001735
Iteration 47/1000 | Loss: 0.00001735
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001734
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001734
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.7343141735182144e-05, 1.7343141735182144e-05, 1.7343141735182144e-05, 1.7343141735182144e-05, 1.7343141735182144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7343141735182144e-05

Optimization complete. Final v2v error: 3.4694478511810303 mm

Highest mean error: 4.405123233795166 mm for frame 157

Lowest mean error: 2.93019962310791 mm for frame 8

Saving results

Total time: 34.149646043777466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840236
Iteration 2/25 | Loss: 0.00092399
Iteration 3/25 | Loss: 0.00081024
Iteration 4/25 | Loss: 0.00078531
Iteration 5/25 | Loss: 0.00078096
Iteration 6/25 | Loss: 0.00077990
Iteration 7/25 | Loss: 0.00077990
Iteration 8/25 | Loss: 0.00077990
Iteration 9/25 | Loss: 0.00077990
Iteration 10/25 | Loss: 0.00077990
Iteration 11/25 | Loss: 0.00077990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007799008744768798, 0.0007799008744768798, 0.0007799008744768798, 0.0007799008744768798, 0.0007799008744768798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007799008744768798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.94435453
Iteration 2/25 | Loss: 0.00047782
Iteration 3/25 | Loss: 0.00047781
Iteration 4/25 | Loss: 0.00047781
Iteration 5/25 | Loss: 0.00047781
Iteration 6/25 | Loss: 0.00047781
Iteration 7/25 | Loss: 0.00047781
Iteration 8/25 | Loss: 0.00047781
Iteration 9/25 | Loss: 0.00047781
Iteration 10/25 | Loss: 0.00047781
Iteration 11/25 | Loss: 0.00047781
Iteration 12/25 | Loss: 0.00047781
Iteration 13/25 | Loss: 0.00047781
Iteration 14/25 | Loss: 0.00047781
Iteration 15/25 | Loss: 0.00047781
Iteration 16/25 | Loss: 0.00047781
Iteration 17/25 | Loss: 0.00047781
Iteration 18/25 | Loss: 0.00047781
Iteration 19/25 | Loss: 0.00047781
Iteration 20/25 | Loss: 0.00047781
Iteration 21/25 | Loss: 0.00047781
Iteration 22/25 | Loss: 0.00047781
Iteration 23/25 | Loss: 0.00047781
Iteration 24/25 | Loss: 0.00047781
Iteration 25/25 | Loss: 0.00047781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047781
Iteration 2/1000 | Loss: 0.00002980
Iteration 3/1000 | Loss: 0.00002480
Iteration 4/1000 | Loss: 0.00002361
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00002174
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002034
Iteration 10/1000 | Loss: 0.00002015
Iteration 11/1000 | Loss: 0.00002005
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001992
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001989
Iteration 19/1000 | Loss: 0.00001985
Iteration 20/1000 | Loss: 0.00001985
Iteration 21/1000 | Loss: 0.00001985
Iteration 22/1000 | Loss: 0.00001984
Iteration 23/1000 | Loss: 0.00001984
Iteration 24/1000 | Loss: 0.00001981
Iteration 25/1000 | Loss: 0.00001981
Iteration 26/1000 | Loss: 0.00001981
Iteration 27/1000 | Loss: 0.00001979
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001979
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001976
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001975
Iteration 39/1000 | Loss: 0.00001975
Iteration 40/1000 | Loss: 0.00001974
Iteration 41/1000 | Loss: 0.00001974
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001973
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001972
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001972
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001970
Iteration 58/1000 | Loss: 0.00001970
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001967
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001966
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001963
Iteration 74/1000 | Loss: 0.00001963
Iteration 75/1000 | Loss: 0.00001963
Iteration 76/1000 | Loss: 0.00001963
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001959
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001958
Iteration 132/1000 | Loss: 0.00001958
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001958
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001958
Iteration 137/1000 | Loss: 0.00001958
Iteration 138/1000 | Loss: 0.00001958
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.9582550521590747e-05, 1.9582550521590747e-05, 1.9582550521590747e-05, 1.9582550521590747e-05, 1.9582550521590747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9582550521590747e-05

Optimization complete. Final v2v error: 3.7015609741210938 mm

Highest mean error: 4.081325054168701 mm for frame 160

Lowest mean error: 3.4340906143188477 mm for frame 26

Saving results

Total time: 54.96575427055359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909752
Iteration 2/25 | Loss: 0.00207367
Iteration 3/25 | Loss: 0.00107800
Iteration 4/25 | Loss: 0.00102432
Iteration 5/25 | Loss: 0.00100736
Iteration 6/25 | Loss: 0.00100301
Iteration 7/25 | Loss: 0.00100251
Iteration 8/25 | Loss: 0.00100251
Iteration 9/25 | Loss: 0.00100251
Iteration 10/25 | Loss: 0.00100251
Iteration 11/25 | Loss: 0.00100251
Iteration 12/25 | Loss: 0.00100251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010025057708844543, 0.0010025057708844543, 0.0010025057708844543, 0.0010025057708844543, 0.0010025057708844543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010025057708844543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92994946
Iteration 2/25 | Loss: 0.00028002
Iteration 3/25 | Loss: 0.00028002
Iteration 4/25 | Loss: 0.00028002
Iteration 5/25 | Loss: 0.00028002
Iteration 6/25 | Loss: 0.00028002
Iteration 7/25 | Loss: 0.00028002
Iteration 8/25 | Loss: 0.00028002
Iteration 9/25 | Loss: 0.00028002
Iteration 10/25 | Loss: 0.00028002
Iteration 11/25 | Loss: 0.00028002
Iteration 12/25 | Loss: 0.00028002
Iteration 13/25 | Loss: 0.00028002
Iteration 14/25 | Loss: 0.00028002
Iteration 15/25 | Loss: 0.00028002
Iteration 16/25 | Loss: 0.00028002
Iteration 17/25 | Loss: 0.00028002
Iteration 18/25 | Loss: 0.00028002
Iteration 19/25 | Loss: 0.00028002
Iteration 20/25 | Loss: 0.00028002
Iteration 21/25 | Loss: 0.00028002
Iteration 22/25 | Loss: 0.00028002
Iteration 23/25 | Loss: 0.00028002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00028001790633425117, 0.00028001790633425117, 0.00028001790633425117, 0.00028001790633425117, 0.00028001790633425117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028001790633425117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028002
Iteration 2/1000 | Loss: 0.00006371
Iteration 3/1000 | Loss: 0.00004635
Iteration 4/1000 | Loss: 0.00004116
Iteration 5/1000 | Loss: 0.00003908
Iteration 6/1000 | Loss: 0.00003789
Iteration 7/1000 | Loss: 0.00003688
Iteration 8/1000 | Loss: 0.00003580
Iteration 9/1000 | Loss: 0.00003524
Iteration 10/1000 | Loss: 0.00003467
Iteration 11/1000 | Loss: 0.00003429
Iteration 12/1000 | Loss: 0.00003396
Iteration 13/1000 | Loss: 0.00003367
Iteration 14/1000 | Loss: 0.00003344
Iteration 15/1000 | Loss: 0.00003328
Iteration 16/1000 | Loss: 0.00003314
Iteration 17/1000 | Loss: 0.00003294
Iteration 18/1000 | Loss: 0.00003279
Iteration 19/1000 | Loss: 0.00003264
Iteration 20/1000 | Loss: 0.00003256
Iteration 21/1000 | Loss: 0.00003245
Iteration 22/1000 | Loss: 0.00003233
Iteration 23/1000 | Loss: 0.00003226
Iteration 24/1000 | Loss: 0.00003216
Iteration 25/1000 | Loss: 0.00003216
Iteration 26/1000 | Loss: 0.00003216
Iteration 27/1000 | Loss: 0.00003215
Iteration 28/1000 | Loss: 0.00003215
Iteration 29/1000 | Loss: 0.00003214
Iteration 30/1000 | Loss: 0.00003214
Iteration 31/1000 | Loss: 0.00003214
Iteration 32/1000 | Loss: 0.00003214
Iteration 33/1000 | Loss: 0.00003213
Iteration 34/1000 | Loss: 0.00003213
Iteration 35/1000 | Loss: 0.00003213
Iteration 36/1000 | Loss: 0.00003213
Iteration 37/1000 | Loss: 0.00003213
Iteration 38/1000 | Loss: 0.00003213
Iteration 39/1000 | Loss: 0.00003213
Iteration 40/1000 | Loss: 0.00003213
Iteration 41/1000 | Loss: 0.00003212
Iteration 42/1000 | Loss: 0.00003212
Iteration 43/1000 | Loss: 0.00003212
Iteration 44/1000 | Loss: 0.00003212
Iteration 45/1000 | Loss: 0.00003211
Iteration 46/1000 | Loss: 0.00003211
Iteration 47/1000 | Loss: 0.00003210
Iteration 48/1000 | Loss: 0.00003210
Iteration 49/1000 | Loss: 0.00003210
Iteration 50/1000 | Loss: 0.00003210
Iteration 51/1000 | Loss: 0.00003210
Iteration 52/1000 | Loss: 0.00003209
Iteration 53/1000 | Loss: 0.00003209
Iteration 54/1000 | Loss: 0.00003209
Iteration 55/1000 | Loss: 0.00003209
Iteration 56/1000 | Loss: 0.00003209
Iteration 57/1000 | Loss: 0.00003208
Iteration 58/1000 | Loss: 0.00003208
Iteration 59/1000 | Loss: 0.00003207
Iteration 60/1000 | Loss: 0.00003207
Iteration 61/1000 | Loss: 0.00003207
Iteration 62/1000 | Loss: 0.00003207
Iteration 63/1000 | Loss: 0.00003206
Iteration 64/1000 | Loss: 0.00003206
Iteration 65/1000 | Loss: 0.00003206
Iteration 66/1000 | Loss: 0.00003206
Iteration 67/1000 | Loss: 0.00003206
Iteration 68/1000 | Loss: 0.00003206
Iteration 69/1000 | Loss: 0.00003206
Iteration 70/1000 | Loss: 0.00003206
Iteration 71/1000 | Loss: 0.00003206
Iteration 72/1000 | Loss: 0.00003206
Iteration 73/1000 | Loss: 0.00003206
Iteration 74/1000 | Loss: 0.00003206
Iteration 75/1000 | Loss: 0.00003206
Iteration 76/1000 | Loss: 0.00003205
Iteration 77/1000 | Loss: 0.00003205
Iteration 78/1000 | Loss: 0.00003205
Iteration 79/1000 | Loss: 0.00003205
Iteration 80/1000 | Loss: 0.00003205
Iteration 81/1000 | Loss: 0.00003205
Iteration 82/1000 | Loss: 0.00003204
Iteration 83/1000 | Loss: 0.00003204
Iteration 84/1000 | Loss: 0.00003204
Iteration 85/1000 | Loss: 0.00003204
Iteration 86/1000 | Loss: 0.00003204
Iteration 87/1000 | Loss: 0.00003204
Iteration 88/1000 | Loss: 0.00003204
Iteration 89/1000 | Loss: 0.00003204
Iteration 90/1000 | Loss: 0.00003204
Iteration 91/1000 | Loss: 0.00003204
Iteration 92/1000 | Loss: 0.00003204
Iteration 93/1000 | Loss: 0.00003204
Iteration 94/1000 | Loss: 0.00003204
Iteration 95/1000 | Loss: 0.00003203
Iteration 96/1000 | Loss: 0.00003203
Iteration 97/1000 | Loss: 0.00003202
Iteration 98/1000 | Loss: 0.00003202
Iteration 99/1000 | Loss: 0.00003202
Iteration 100/1000 | Loss: 0.00003201
Iteration 101/1000 | Loss: 0.00003201
Iteration 102/1000 | Loss: 0.00003201
Iteration 103/1000 | Loss: 0.00003201
Iteration 104/1000 | Loss: 0.00003201
Iteration 105/1000 | Loss: 0.00003201
Iteration 106/1000 | Loss: 0.00003200
Iteration 107/1000 | Loss: 0.00003200
Iteration 108/1000 | Loss: 0.00003200
Iteration 109/1000 | Loss: 0.00003200
Iteration 110/1000 | Loss: 0.00003200
Iteration 111/1000 | Loss: 0.00003200
Iteration 112/1000 | Loss: 0.00003199
Iteration 113/1000 | Loss: 0.00003199
Iteration 114/1000 | Loss: 0.00003198
Iteration 115/1000 | Loss: 0.00003198
Iteration 116/1000 | Loss: 0.00003198
Iteration 117/1000 | Loss: 0.00003198
Iteration 118/1000 | Loss: 0.00003198
Iteration 119/1000 | Loss: 0.00003198
Iteration 120/1000 | Loss: 0.00003198
Iteration 121/1000 | Loss: 0.00003198
Iteration 122/1000 | Loss: 0.00003198
Iteration 123/1000 | Loss: 0.00003197
Iteration 124/1000 | Loss: 0.00003197
Iteration 125/1000 | Loss: 0.00003197
Iteration 126/1000 | Loss: 0.00003197
Iteration 127/1000 | Loss: 0.00003197
Iteration 128/1000 | Loss: 0.00003196
Iteration 129/1000 | Loss: 0.00003196
Iteration 130/1000 | Loss: 0.00003196
Iteration 131/1000 | Loss: 0.00003196
Iteration 132/1000 | Loss: 0.00003196
Iteration 133/1000 | Loss: 0.00003196
Iteration 134/1000 | Loss: 0.00003196
Iteration 135/1000 | Loss: 0.00003196
Iteration 136/1000 | Loss: 0.00003196
Iteration 137/1000 | Loss: 0.00003195
Iteration 138/1000 | Loss: 0.00003195
Iteration 139/1000 | Loss: 0.00003194
Iteration 140/1000 | Loss: 0.00003194
Iteration 141/1000 | Loss: 0.00003194
Iteration 142/1000 | Loss: 0.00003194
Iteration 143/1000 | Loss: 0.00003194
Iteration 144/1000 | Loss: 0.00003193
Iteration 145/1000 | Loss: 0.00003193
Iteration 146/1000 | Loss: 0.00003193
Iteration 147/1000 | Loss: 0.00003193
Iteration 148/1000 | Loss: 0.00003193
Iteration 149/1000 | Loss: 0.00003193
Iteration 150/1000 | Loss: 0.00003192
Iteration 151/1000 | Loss: 0.00003192
Iteration 152/1000 | Loss: 0.00003192
Iteration 153/1000 | Loss: 0.00003192
Iteration 154/1000 | Loss: 0.00003192
Iteration 155/1000 | Loss: 0.00003192
Iteration 156/1000 | Loss: 0.00003192
Iteration 157/1000 | Loss: 0.00003191
Iteration 158/1000 | Loss: 0.00003191
Iteration 159/1000 | Loss: 0.00003191
Iteration 160/1000 | Loss: 0.00003191
Iteration 161/1000 | Loss: 0.00003191
Iteration 162/1000 | Loss: 0.00003191
Iteration 163/1000 | Loss: 0.00003191
Iteration 164/1000 | Loss: 0.00003191
Iteration 165/1000 | Loss: 0.00003191
Iteration 166/1000 | Loss: 0.00003190
Iteration 167/1000 | Loss: 0.00003190
Iteration 168/1000 | Loss: 0.00003190
Iteration 169/1000 | Loss: 0.00003190
Iteration 170/1000 | Loss: 0.00003190
Iteration 171/1000 | Loss: 0.00003190
Iteration 172/1000 | Loss: 0.00003190
Iteration 173/1000 | Loss: 0.00003190
Iteration 174/1000 | Loss: 0.00003189
Iteration 175/1000 | Loss: 0.00003189
Iteration 176/1000 | Loss: 0.00003189
Iteration 177/1000 | Loss: 0.00003189
Iteration 178/1000 | Loss: 0.00003189
Iteration 179/1000 | Loss: 0.00003189
Iteration 180/1000 | Loss: 0.00003189
Iteration 181/1000 | Loss: 0.00003189
Iteration 182/1000 | Loss: 0.00003189
Iteration 183/1000 | Loss: 0.00003189
Iteration 184/1000 | Loss: 0.00003189
Iteration 185/1000 | Loss: 0.00003188
Iteration 186/1000 | Loss: 0.00003188
Iteration 187/1000 | Loss: 0.00003188
Iteration 188/1000 | Loss: 0.00003188
Iteration 189/1000 | Loss: 0.00003188
Iteration 190/1000 | Loss: 0.00003187
Iteration 191/1000 | Loss: 0.00003187
Iteration 192/1000 | Loss: 0.00003187
Iteration 193/1000 | Loss: 0.00003187
Iteration 194/1000 | Loss: 0.00003187
Iteration 195/1000 | Loss: 0.00003187
Iteration 196/1000 | Loss: 0.00003187
Iteration 197/1000 | Loss: 0.00003187
Iteration 198/1000 | Loss: 0.00003187
Iteration 199/1000 | Loss: 0.00003187
Iteration 200/1000 | Loss: 0.00003187
Iteration 201/1000 | Loss: 0.00003187
Iteration 202/1000 | Loss: 0.00003186
Iteration 203/1000 | Loss: 0.00003186
Iteration 204/1000 | Loss: 0.00003186
Iteration 205/1000 | Loss: 0.00003186
Iteration 206/1000 | Loss: 0.00003186
Iteration 207/1000 | Loss: 0.00003185
Iteration 208/1000 | Loss: 0.00003185
Iteration 209/1000 | Loss: 0.00003185
Iteration 210/1000 | Loss: 0.00003185
Iteration 211/1000 | Loss: 0.00003185
Iteration 212/1000 | Loss: 0.00003185
Iteration 213/1000 | Loss: 0.00003184
Iteration 214/1000 | Loss: 0.00003184
Iteration 215/1000 | Loss: 0.00003184
Iteration 216/1000 | Loss: 0.00003184
Iteration 217/1000 | Loss: 0.00003184
Iteration 218/1000 | Loss: 0.00003184
Iteration 219/1000 | Loss: 0.00003184
Iteration 220/1000 | Loss: 0.00003184
Iteration 221/1000 | Loss: 0.00003183
Iteration 222/1000 | Loss: 0.00003183
Iteration 223/1000 | Loss: 0.00003183
Iteration 224/1000 | Loss: 0.00003183
Iteration 225/1000 | Loss: 0.00003183
Iteration 226/1000 | Loss: 0.00003183
Iteration 227/1000 | Loss: 0.00003183
Iteration 228/1000 | Loss: 0.00003183
Iteration 229/1000 | Loss: 0.00003183
Iteration 230/1000 | Loss: 0.00003183
Iteration 231/1000 | Loss: 0.00003183
Iteration 232/1000 | Loss: 0.00003183
Iteration 233/1000 | Loss: 0.00003182
Iteration 234/1000 | Loss: 0.00003182
Iteration 235/1000 | Loss: 0.00003182
Iteration 236/1000 | Loss: 0.00003182
Iteration 237/1000 | Loss: 0.00003182
Iteration 238/1000 | Loss: 0.00003182
Iteration 239/1000 | Loss: 0.00003182
Iteration 240/1000 | Loss: 0.00003182
Iteration 241/1000 | Loss: 0.00003182
Iteration 242/1000 | Loss: 0.00003182
Iteration 243/1000 | Loss: 0.00003182
Iteration 244/1000 | Loss: 0.00003182
Iteration 245/1000 | Loss: 0.00003182
Iteration 246/1000 | Loss: 0.00003182
Iteration 247/1000 | Loss: 0.00003182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [3.181661304552108e-05, 3.181661304552108e-05, 3.181661304552108e-05, 3.181661304552108e-05, 3.181661304552108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.181661304552108e-05

Optimization complete. Final v2v error: 4.741207122802734 mm

Highest mean error: 5.4161858558654785 mm for frame 41

Lowest mean error: 4.1337890625 mm for frame 237

Saving results

Total time: 115.45212054252625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430233
Iteration 2/25 | Loss: 0.00097553
Iteration 3/25 | Loss: 0.00085327
Iteration 4/25 | Loss: 0.00082555
Iteration 5/25 | Loss: 0.00081735
Iteration 6/25 | Loss: 0.00081581
Iteration 7/25 | Loss: 0.00081535
Iteration 8/25 | Loss: 0.00081535
Iteration 9/25 | Loss: 0.00081535
Iteration 10/25 | Loss: 0.00081535
Iteration 11/25 | Loss: 0.00081535
Iteration 12/25 | Loss: 0.00081535
Iteration 13/25 | Loss: 0.00081535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008153541712090373, 0.0008153541712090373, 0.0008153541712090373, 0.0008153541712090373, 0.0008153541712090373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008153541712090373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03848529
Iteration 2/25 | Loss: 0.00049533
Iteration 3/25 | Loss: 0.00049532
Iteration 4/25 | Loss: 0.00049532
Iteration 5/25 | Loss: 0.00049532
Iteration 6/25 | Loss: 0.00049532
Iteration 7/25 | Loss: 0.00049532
Iteration 8/25 | Loss: 0.00049532
Iteration 9/25 | Loss: 0.00049532
Iteration 10/25 | Loss: 0.00049532
Iteration 11/25 | Loss: 0.00049532
Iteration 12/25 | Loss: 0.00049532
Iteration 13/25 | Loss: 0.00049532
Iteration 14/25 | Loss: 0.00049532
Iteration 15/25 | Loss: 0.00049532
Iteration 16/25 | Loss: 0.00049532
Iteration 17/25 | Loss: 0.00049532
Iteration 18/25 | Loss: 0.00049532
Iteration 19/25 | Loss: 0.00049532
Iteration 20/25 | Loss: 0.00049532
Iteration 21/25 | Loss: 0.00049532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004953185562044382, 0.0004953185562044382, 0.0004953185562044382, 0.0004953185562044382, 0.0004953185562044382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004953185562044382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049532
Iteration 2/1000 | Loss: 0.00005236
Iteration 3/1000 | Loss: 0.00003381
Iteration 4/1000 | Loss: 0.00003137
Iteration 5/1000 | Loss: 0.00003027
Iteration 6/1000 | Loss: 0.00002881
Iteration 7/1000 | Loss: 0.00002798
Iteration 8/1000 | Loss: 0.00002736
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00002669
Iteration 11/1000 | Loss: 0.00002653
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002626
Iteration 14/1000 | Loss: 0.00002625
Iteration 15/1000 | Loss: 0.00002624
Iteration 16/1000 | Loss: 0.00002617
Iteration 17/1000 | Loss: 0.00002614
Iteration 18/1000 | Loss: 0.00002613
Iteration 19/1000 | Loss: 0.00002613
Iteration 20/1000 | Loss: 0.00002612
Iteration 21/1000 | Loss: 0.00002612
Iteration 22/1000 | Loss: 0.00002611
Iteration 23/1000 | Loss: 0.00002611
Iteration 24/1000 | Loss: 0.00002610
Iteration 25/1000 | Loss: 0.00002608
Iteration 26/1000 | Loss: 0.00002607
Iteration 27/1000 | Loss: 0.00002600
Iteration 28/1000 | Loss: 0.00002595
Iteration 29/1000 | Loss: 0.00002592
Iteration 30/1000 | Loss: 0.00002591
Iteration 31/1000 | Loss: 0.00002591
Iteration 32/1000 | Loss: 0.00002591
Iteration 33/1000 | Loss: 0.00002591
Iteration 34/1000 | Loss: 0.00002591
Iteration 35/1000 | Loss: 0.00002591
Iteration 36/1000 | Loss: 0.00002591
Iteration 37/1000 | Loss: 0.00002591
Iteration 38/1000 | Loss: 0.00002591
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00002591
Iteration 41/1000 | Loss: 0.00002590
Iteration 42/1000 | Loss: 0.00002590
Iteration 43/1000 | Loss: 0.00002590
Iteration 44/1000 | Loss: 0.00002590
Iteration 45/1000 | Loss: 0.00002590
Iteration 46/1000 | Loss: 0.00002590
Iteration 47/1000 | Loss: 0.00002590
Iteration 48/1000 | Loss: 0.00002589
Iteration 49/1000 | Loss: 0.00002589
Iteration 50/1000 | Loss: 0.00002589
Iteration 51/1000 | Loss: 0.00002588
Iteration 52/1000 | Loss: 0.00002588
Iteration 53/1000 | Loss: 0.00002588
Iteration 54/1000 | Loss: 0.00002587
Iteration 55/1000 | Loss: 0.00002587
Iteration 56/1000 | Loss: 0.00002587
Iteration 57/1000 | Loss: 0.00002586
Iteration 58/1000 | Loss: 0.00002586
Iteration 59/1000 | Loss: 0.00002586
Iteration 60/1000 | Loss: 0.00002586
Iteration 61/1000 | Loss: 0.00002585
Iteration 62/1000 | Loss: 0.00002585
Iteration 63/1000 | Loss: 0.00002585
Iteration 64/1000 | Loss: 0.00002585
Iteration 65/1000 | Loss: 0.00002585
Iteration 66/1000 | Loss: 0.00002585
Iteration 67/1000 | Loss: 0.00002585
Iteration 68/1000 | Loss: 0.00002584
Iteration 69/1000 | Loss: 0.00002584
Iteration 70/1000 | Loss: 0.00002584
Iteration 71/1000 | Loss: 0.00002584
Iteration 72/1000 | Loss: 0.00002584
Iteration 73/1000 | Loss: 0.00002584
Iteration 74/1000 | Loss: 0.00002584
Iteration 75/1000 | Loss: 0.00002584
Iteration 76/1000 | Loss: 0.00002583
Iteration 77/1000 | Loss: 0.00002583
Iteration 78/1000 | Loss: 0.00002583
Iteration 79/1000 | Loss: 0.00002583
Iteration 80/1000 | Loss: 0.00002583
Iteration 81/1000 | Loss: 0.00002583
Iteration 82/1000 | Loss: 0.00002583
Iteration 83/1000 | Loss: 0.00002583
Iteration 84/1000 | Loss: 0.00002583
Iteration 85/1000 | Loss: 0.00002582
Iteration 86/1000 | Loss: 0.00002582
Iteration 87/1000 | Loss: 0.00002582
Iteration 88/1000 | Loss: 0.00002582
Iteration 89/1000 | Loss: 0.00002582
Iteration 90/1000 | Loss: 0.00002582
Iteration 91/1000 | Loss: 0.00002582
Iteration 92/1000 | Loss: 0.00002582
Iteration 93/1000 | Loss: 0.00002581
Iteration 94/1000 | Loss: 0.00002581
Iteration 95/1000 | Loss: 0.00002581
Iteration 96/1000 | Loss: 0.00002580
Iteration 97/1000 | Loss: 0.00002580
Iteration 98/1000 | Loss: 0.00002580
Iteration 99/1000 | Loss: 0.00002580
Iteration 100/1000 | Loss: 0.00002580
Iteration 101/1000 | Loss: 0.00002580
Iteration 102/1000 | Loss: 0.00002580
Iteration 103/1000 | Loss: 0.00002580
Iteration 104/1000 | Loss: 0.00002580
Iteration 105/1000 | Loss: 0.00002579
Iteration 106/1000 | Loss: 0.00002579
Iteration 107/1000 | Loss: 0.00002579
Iteration 108/1000 | Loss: 0.00002579
Iteration 109/1000 | Loss: 0.00002579
Iteration 110/1000 | Loss: 0.00002579
Iteration 111/1000 | Loss: 0.00002579
Iteration 112/1000 | Loss: 0.00002579
Iteration 113/1000 | Loss: 0.00002579
Iteration 114/1000 | Loss: 0.00002579
Iteration 115/1000 | Loss: 0.00002579
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002579
Iteration 118/1000 | Loss: 0.00002579
Iteration 119/1000 | Loss: 0.00002579
Iteration 120/1000 | Loss: 0.00002579
Iteration 121/1000 | Loss: 0.00002579
Iteration 122/1000 | Loss: 0.00002579
Iteration 123/1000 | Loss: 0.00002579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.5792644009925425e-05, 2.5792644009925425e-05, 2.5792644009925425e-05, 2.5792644009925425e-05, 2.5792644009925425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5792644009925425e-05

Optimization complete. Final v2v error: 4.232946872711182 mm

Highest mean error: 4.576279640197754 mm for frame 99

Lowest mean error: 3.69708251953125 mm for frame 40

Saving results

Total time: 37.8082377910614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060882
Iteration 2/25 | Loss: 0.01060882
Iteration 3/25 | Loss: 0.00157971
Iteration 4/25 | Loss: 0.00094558
Iteration 5/25 | Loss: 0.00086874
Iteration 6/25 | Loss: 0.00081664
Iteration 7/25 | Loss: 0.00080689
Iteration 8/25 | Loss: 0.00080374
Iteration 9/25 | Loss: 0.00078722
Iteration 10/25 | Loss: 0.00077684
Iteration 11/25 | Loss: 0.00076915
Iteration 12/25 | Loss: 0.00076570
Iteration 13/25 | Loss: 0.00075968
Iteration 14/25 | Loss: 0.00075981
Iteration 15/25 | Loss: 0.00076031
Iteration 16/25 | Loss: 0.00075966
Iteration 17/25 | Loss: 0.00075754
Iteration 18/25 | Loss: 0.00075553
Iteration 19/25 | Loss: 0.00075411
Iteration 20/25 | Loss: 0.00075367
Iteration 21/25 | Loss: 0.00075350
Iteration 22/25 | Loss: 0.00075350
Iteration 23/25 | Loss: 0.00075349
Iteration 24/25 | Loss: 0.00075349
Iteration 25/25 | Loss: 0.00075349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52278340
Iteration 2/25 | Loss: 0.00054235
Iteration 3/25 | Loss: 0.00054235
Iteration 4/25 | Loss: 0.00054235
Iteration 5/25 | Loss: 0.00054235
Iteration 6/25 | Loss: 0.00054235
Iteration 7/25 | Loss: 0.00054235
Iteration 8/25 | Loss: 0.00054235
Iteration 9/25 | Loss: 0.00054235
Iteration 10/25 | Loss: 0.00054235
Iteration 11/25 | Loss: 0.00054235
Iteration 12/25 | Loss: 0.00054235
Iteration 13/25 | Loss: 0.00054235
Iteration 14/25 | Loss: 0.00054235
Iteration 15/25 | Loss: 0.00054235
Iteration 16/25 | Loss: 0.00054235
Iteration 17/25 | Loss: 0.00054235
Iteration 18/25 | Loss: 0.00054235
Iteration 19/25 | Loss: 0.00054235
Iteration 20/25 | Loss: 0.00054235
Iteration 21/25 | Loss: 0.00054235
Iteration 22/25 | Loss: 0.00054235
Iteration 23/25 | Loss: 0.00054235
Iteration 24/25 | Loss: 0.00054235
Iteration 25/25 | Loss: 0.00054235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054235
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001587
Iteration 5/1000 | Loss: 0.00001529
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00004814
Iteration 8/1000 | Loss: 0.00029933
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001432
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001414
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00003794
Iteration 18/1000 | Loss: 0.00001540
Iteration 19/1000 | Loss: 0.00007249
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001405
Iteration 22/1000 | Loss: 0.00001403
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001402
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001396
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001393
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001392
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001383
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001378
Iteration 71/1000 | Loss: 0.00001378
Iteration 72/1000 | Loss: 0.00001378
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001377
Iteration 77/1000 | Loss: 0.00001377
Iteration 78/1000 | Loss: 0.00001377
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001376
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00001376
Iteration 94/1000 | Loss: 0.00001376
Iteration 95/1000 | Loss: 0.00001376
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001375
Iteration 98/1000 | Loss: 0.00001375
Iteration 99/1000 | Loss: 0.00001375
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001374
Iteration 104/1000 | Loss: 0.00001374
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001372
Iteration 117/1000 | Loss: 0.00001372
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001368
Iteration 134/1000 | Loss: 0.00001368
Iteration 135/1000 | Loss: 0.00001368
Iteration 136/1000 | Loss: 0.00001368
Iteration 137/1000 | Loss: 0.00001368
Iteration 138/1000 | Loss: 0.00001368
Iteration 139/1000 | Loss: 0.00001368
Iteration 140/1000 | Loss: 0.00001368
Iteration 141/1000 | Loss: 0.00001368
Iteration 142/1000 | Loss: 0.00001368
Iteration 143/1000 | Loss: 0.00001368
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001367
Iteration 146/1000 | Loss: 0.00001367
Iteration 147/1000 | Loss: 0.00001367
Iteration 148/1000 | Loss: 0.00001367
Iteration 149/1000 | Loss: 0.00001367
Iteration 150/1000 | Loss: 0.00001367
Iteration 151/1000 | Loss: 0.00001367
Iteration 152/1000 | Loss: 0.00001367
Iteration 153/1000 | Loss: 0.00001367
Iteration 154/1000 | Loss: 0.00001367
Iteration 155/1000 | Loss: 0.00001367
Iteration 156/1000 | Loss: 0.00001366
Iteration 157/1000 | Loss: 0.00001366
Iteration 158/1000 | Loss: 0.00001366
Iteration 159/1000 | Loss: 0.00001366
Iteration 160/1000 | Loss: 0.00001366
Iteration 161/1000 | Loss: 0.00001366
Iteration 162/1000 | Loss: 0.00001366
Iteration 163/1000 | Loss: 0.00001366
Iteration 164/1000 | Loss: 0.00001366
Iteration 165/1000 | Loss: 0.00001365
Iteration 166/1000 | Loss: 0.00001365
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001365
Iteration 170/1000 | Loss: 0.00001365
Iteration 171/1000 | Loss: 0.00001365
Iteration 172/1000 | Loss: 0.00001365
Iteration 173/1000 | Loss: 0.00001365
Iteration 174/1000 | Loss: 0.00001365
Iteration 175/1000 | Loss: 0.00001365
Iteration 176/1000 | Loss: 0.00001364
Iteration 177/1000 | Loss: 0.00001364
Iteration 178/1000 | Loss: 0.00001364
Iteration 179/1000 | Loss: 0.00001364
Iteration 180/1000 | Loss: 0.00001364
Iteration 181/1000 | Loss: 0.00001364
Iteration 182/1000 | Loss: 0.00001364
Iteration 183/1000 | Loss: 0.00001364
Iteration 184/1000 | Loss: 0.00001364
Iteration 185/1000 | Loss: 0.00001364
Iteration 186/1000 | Loss: 0.00001364
Iteration 187/1000 | Loss: 0.00001364
Iteration 188/1000 | Loss: 0.00001364
Iteration 189/1000 | Loss: 0.00001364
Iteration 190/1000 | Loss: 0.00001364
Iteration 191/1000 | Loss: 0.00001364
Iteration 192/1000 | Loss: 0.00001363
Iteration 193/1000 | Loss: 0.00001363
Iteration 194/1000 | Loss: 0.00001363
Iteration 195/1000 | Loss: 0.00001363
Iteration 196/1000 | Loss: 0.00001363
Iteration 197/1000 | Loss: 0.00001363
Iteration 198/1000 | Loss: 0.00001363
Iteration 199/1000 | Loss: 0.00001363
Iteration 200/1000 | Loss: 0.00001363
Iteration 201/1000 | Loss: 0.00001363
Iteration 202/1000 | Loss: 0.00001363
Iteration 203/1000 | Loss: 0.00001363
Iteration 204/1000 | Loss: 0.00001363
Iteration 205/1000 | Loss: 0.00001363
Iteration 206/1000 | Loss: 0.00001363
Iteration 207/1000 | Loss: 0.00001363
Iteration 208/1000 | Loss: 0.00001363
Iteration 209/1000 | Loss: 0.00001363
Iteration 210/1000 | Loss: 0.00001362
Iteration 211/1000 | Loss: 0.00001362
Iteration 212/1000 | Loss: 0.00001362
Iteration 213/1000 | Loss: 0.00001362
Iteration 214/1000 | Loss: 0.00001362
Iteration 215/1000 | Loss: 0.00001362
Iteration 216/1000 | Loss: 0.00001362
Iteration 217/1000 | Loss: 0.00001362
Iteration 218/1000 | Loss: 0.00001362
Iteration 219/1000 | Loss: 0.00001362
Iteration 220/1000 | Loss: 0.00001362
Iteration 221/1000 | Loss: 0.00001362
Iteration 222/1000 | Loss: 0.00001362
Iteration 223/1000 | Loss: 0.00001362
Iteration 224/1000 | Loss: 0.00001362
Iteration 225/1000 | Loss: 0.00001362
Iteration 226/1000 | Loss: 0.00001362
Iteration 227/1000 | Loss: 0.00001362
Iteration 228/1000 | Loss: 0.00001362
Iteration 229/1000 | Loss: 0.00001362
Iteration 230/1000 | Loss: 0.00001362
Iteration 231/1000 | Loss: 0.00001362
Iteration 232/1000 | Loss: 0.00001362
Iteration 233/1000 | Loss: 0.00001362
Iteration 234/1000 | Loss: 0.00001362
Iteration 235/1000 | Loss: 0.00001362
Iteration 236/1000 | Loss: 0.00001362
Iteration 237/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.361717477266211e-05, 1.361717477266211e-05, 1.361717477266211e-05, 1.361717477266211e-05, 1.361717477266211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.361717477266211e-05

Optimization complete. Final v2v error: 3.110438823699951 mm

Highest mean error: 4.083132266998291 mm for frame 1

Lowest mean error: 2.8840677738189697 mm for frame 235

Saving results

Total time: 86.98713517189026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396034
Iteration 2/25 | Loss: 0.00084126
Iteration 3/25 | Loss: 0.00074188
Iteration 4/25 | Loss: 0.00072336
Iteration 5/25 | Loss: 0.00072030
Iteration 6/25 | Loss: 0.00071950
Iteration 7/25 | Loss: 0.00071950
Iteration 8/25 | Loss: 0.00071950
Iteration 9/25 | Loss: 0.00071950
Iteration 10/25 | Loss: 0.00071950
Iteration 11/25 | Loss: 0.00071950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007194991922006011, 0.0007194991922006011, 0.0007194991922006011, 0.0007194991922006011, 0.0007194991922006011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007194991922006011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89378035
Iteration 2/25 | Loss: 0.00044303
Iteration 3/25 | Loss: 0.00044303
Iteration 4/25 | Loss: 0.00044303
Iteration 5/25 | Loss: 0.00044303
Iteration 6/25 | Loss: 0.00044303
Iteration 7/25 | Loss: 0.00044303
Iteration 8/25 | Loss: 0.00044303
Iteration 9/25 | Loss: 0.00044303
Iteration 10/25 | Loss: 0.00044303
Iteration 11/25 | Loss: 0.00044303
Iteration 12/25 | Loss: 0.00044303
Iteration 13/25 | Loss: 0.00044303
Iteration 14/25 | Loss: 0.00044303
Iteration 15/25 | Loss: 0.00044303
Iteration 16/25 | Loss: 0.00044303
Iteration 17/25 | Loss: 0.00044303
Iteration 18/25 | Loss: 0.00044303
Iteration 19/25 | Loss: 0.00044303
Iteration 20/25 | Loss: 0.00044303
Iteration 21/25 | Loss: 0.00044303
Iteration 22/25 | Loss: 0.00044303
Iteration 23/25 | Loss: 0.00044303
Iteration 24/25 | Loss: 0.00044303
Iteration 25/25 | Loss: 0.00044303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044303
Iteration 2/1000 | Loss: 0.00002422
Iteration 3/1000 | Loss: 0.00001889
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001685
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001557
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001535
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001528
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001520
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00001519
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001516
Iteration 24/1000 | Loss: 0.00001516
Iteration 25/1000 | Loss: 0.00001514
Iteration 26/1000 | Loss: 0.00001514
Iteration 27/1000 | Loss: 0.00001513
Iteration 28/1000 | Loss: 0.00001513
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001512
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001511
Iteration 34/1000 | Loss: 0.00001511
Iteration 35/1000 | Loss: 0.00001510
Iteration 36/1000 | Loss: 0.00001510
Iteration 37/1000 | Loss: 0.00001509
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001509
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001508
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001506
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001505
Iteration 49/1000 | Loss: 0.00001504
Iteration 50/1000 | Loss: 0.00001503
Iteration 51/1000 | Loss: 0.00001503
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001499
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001491
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001490
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001490
Iteration 72/1000 | Loss: 0.00001490
Iteration 73/1000 | Loss: 0.00001490
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.4898511835781392e-05, 1.4898511835781392e-05, 1.4898511835781392e-05, 1.4898511835781392e-05, 1.4898511835781392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4898511835781392e-05

Optimization complete. Final v2v error: 3.2915916442871094 mm

Highest mean error: 3.4757533073425293 mm for frame 147

Lowest mean error: 3.084343910217285 mm for frame 115

Saving results

Total time: 34.42036581039429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009598
Iteration 2/25 | Loss: 0.00496425
Iteration 3/25 | Loss: 0.00289345
Iteration 4/25 | Loss: 0.00256169
Iteration 5/25 | Loss: 0.00216201
Iteration 6/25 | Loss: 0.00205747
Iteration 7/25 | Loss: 0.00186099
Iteration 8/25 | Loss: 0.00163105
Iteration 9/25 | Loss: 0.00157919
Iteration 10/25 | Loss: 0.00165603
Iteration 11/25 | Loss: 0.00153517
Iteration 12/25 | Loss: 0.00149057
Iteration 13/25 | Loss: 0.00149478
Iteration 14/25 | Loss: 0.00152122
Iteration 15/25 | Loss: 0.00140430
Iteration 16/25 | Loss: 0.00127557
Iteration 17/25 | Loss: 0.00126143
Iteration 18/25 | Loss: 0.00123753
Iteration 19/25 | Loss: 0.00123761
Iteration 20/25 | Loss: 0.00125958
Iteration 21/25 | Loss: 0.00122550
Iteration 22/25 | Loss: 0.00121769
Iteration 23/25 | Loss: 0.00122273
Iteration 24/25 | Loss: 0.00121390
Iteration 25/25 | Loss: 0.00121377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49125540
Iteration 2/25 | Loss: 0.00602705
Iteration 3/25 | Loss: 0.00239908
Iteration 4/25 | Loss: 0.00239908
Iteration 5/25 | Loss: 0.00239907
Iteration 6/25 | Loss: 0.00239907
Iteration 7/25 | Loss: 0.00239907
Iteration 8/25 | Loss: 0.00239907
Iteration 9/25 | Loss: 0.00239907
Iteration 10/25 | Loss: 0.00239907
Iteration 11/25 | Loss: 0.00239907
Iteration 12/25 | Loss: 0.00239907
Iteration 13/25 | Loss: 0.00239907
Iteration 14/25 | Loss: 0.00239907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002399071818217635, 0.002399071818217635, 0.002399071818217635, 0.002399071818217635, 0.002399071818217635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002399071818217635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239907
Iteration 2/1000 | Loss: 0.00650900
Iteration 3/1000 | Loss: 0.00035184
Iteration 4/1000 | Loss: 0.00044195
Iteration 5/1000 | Loss: 0.00087189
Iteration 6/1000 | Loss: 0.00046302
Iteration 7/1000 | Loss: 0.00023336
Iteration 8/1000 | Loss: 0.00066323
Iteration 9/1000 | Loss: 0.00054108
Iteration 10/1000 | Loss: 0.00157703
Iteration 11/1000 | Loss: 0.01005941
Iteration 12/1000 | Loss: 0.00396756
Iteration 13/1000 | Loss: 0.00061069
Iteration 14/1000 | Loss: 0.00121453
Iteration 15/1000 | Loss: 0.00038023
Iteration 16/1000 | Loss: 0.00025216
Iteration 17/1000 | Loss: 0.00040863
Iteration 18/1000 | Loss: 0.00095638
Iteration 19/1000 | Loss: 0.00011122
Iteration 20/1000 | Loss: 0.00004954
Iteration 21/1000 | Loss: 0.00009549
Iteration 22/1000 | Loss: 0.00014010
Iteration 23/1000 | Loss: 0.00027980
Iteration 24/1000 | Loss: 0.00002725
Iteration 25/1000 | Loss: 0.00009876
Iteration 26/1000 | Loss: 0.00002089
Iteration 27/1000 | Loss: 0.00045515
Iteration 28/1000 | Loss: 0.00003099
Iteration 29/1000 | Loss: 0.00009686
Iteration 30/1000 | Loss: 0.00004913
Iteration 31/1000 | Loss: 0.00009527
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00015539
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00005163
Iteration 36/1000 | Loss: 0.00016505
Iteration 37/1000 | Loss: 0.00022223
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00005021
Iteration 40/1000 | Loss: 0.00005022
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00005454
Iteration 43/1000 | Loss: 0.00001178
Iteration 44/1000 | Loss: 0.00018367
Iteration 45/1000 | Loss: 0.00001975
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001150
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00010598
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001127
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001126
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001125
Iteration 74/1000 | Loss: 0.00001125
Iteration 75/1000 | Loss: 0.00001125
Iteration 76/1000 | Loss: 0.00001125
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001119
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001118
Iteration 104/1000 | Loss: 0.00001118
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001117
Iteration 124/1000 | Loss: 0.00001117
Iteration 125/1000 | Loss: 0.00001117
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001116
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001116
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001115
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001115
Iteration 144/1000 | Loss: 0.00001115
Iteration 145/1000 | Loss: 0.00001115
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001115
Iteration 148/1000 | Loss: 0.00001115
Iteration 149/1000 | Loss: 0.00001115
Iteration 150/1000 | Loss: 0.00001115
Iteration 151/1000 | Loss: 0.00001115
Iteration 152/1000 | Loss: 0.00001115
Iteration 153/1000 | Loss: 0.00001115
Iteration 154/1000 | Loss: 0.00001115
Iteration 155/1000 | Loss: 0.00001115
Iteration 156/1000 | Loss: 0.00001115
Iteration 157/1000 | Loss: 0.00001115
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001115
Iteration 163/1000 | Loss: 0.00001115
Iteration 164/1000 | Loss: 0.00001115
Iteration 165/1000 | Loss: 0.00001115
Iteration 166/1000 | Loss: 0.00001115
Iteration 167/1000 | Loss: 0.00001115
Iteration 168/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.1148358680657111e-05, 1.1148358680657111e-05, 1.1148358680657111e-05, 1.1148358680657111e-05, 1.1148358680657111e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1148358680657111e-05

Optimization complete. Final v2v error: 2.831089496612549 mm

Highest mean error: 3.0097568035125732 mm for frame 92

Lowest mean error: 2.6924588680267334 mm for frame 115

Saving results

Total time: 136.09591937065125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951977
Iteration 2/25 | Loss: 0.00249306
Iteration 3/25 | Loss: 0.00173771
Iteration 4/25 | Loss: 0.00142937
Iteration 5/25 | Loss: 0.00136340
Iteration 6/25 | Loss: 0.00123518
Iteration 7/25 | Loss: 0.00118943
Iteration 8/25 | Loss: 0.00117255
Iteration 9/25 | Loss: 0.00113115
Iteration 10/25 | Loss: 0.00110565
Iteration 11/25 | Loss: 0.00110005
Iteration 12/25 | Loss: 0.00107254
Iteration 13/25 | Loss: 0.00103835
Iteration 14/25 | Loss: 0.00101448
Iteration 15/25 | Loss: 0.00100382
Iteration 16/25 | Loss: 0.00096787
Iteration 17/25 | Loss: 0.00095430
Iteration 18/25 | Loss: 0.00095345
Iteration 19/25 | Loss: 0.00094641
Iteration 20/25 | Loss: 0.00093641
Iteration 21/25 | Loss: 0.00093969
Iteration 22/25 | Loss: 0.00093055
Iteration 23/25 | Loss: 0.00093373
Iteration 24/25 | Loss: 0.00093002
Iteration 25/25 | Loss: 0.00092472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51800287
Iteration 2/25 | Loss: 0.00104559
Iteration 3/25 | Loss: 0.00104559
Iteration 4/25 | Loss: 0.00104559
Iteration 5/25 | Loss: 0.00104559
Iteration 6/25 | Loss: 0.00104559
Iteration 7/25 | Loss: 0.00104559
Iteration 8/25 | Loss: 0.00104559
Iteration 9/25 | Loss: 0.00104559
Iteration 10/25 | Loss: 0.00104559
Iteration 11/25 | Loss: 0.00104559
Iteration 12/25 | Loss: 0.00104559
Iteration 13/25 | Loss: 0.00104559
Iteration 14/25 | Loss: 0.00104559
Iteration 15/25 | Loss: 0.00104559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010455885203555226, 0.0010455885203555226, 0.0010455885203555226, 0.0010455885203555226, 0.0010455885203555226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010455885203555226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104559
Iteration 2/1000 | Loss: 0.00008969
Iteration 3/1000 | Loss: 0.00065870
Iteration 4/1000 | Loss: 0.00042732
Iteration 5/1000 | Loss: 0.00039085
Iteration 6/1000 | Loss: 0.00007973
Iteration 7/1000 | Loss: 0.00007006
Iteration 8/1000 | Loss: 0.00006576
Iteration 9/1000 | Loss: 0.00006396
Iteration 10/1000 | Loss: 0.00112047
Iteration 11/1000 | Loss: 0.00113586
Iteration 12/1000 | Loss: 0.00025688
Iteration 13/1000 | Loss: 0.00005954
Iteration 14/1000 | Loss: 0.00018990
Iteration 15/1000 | Loss: 0.00005923
Iteration 16/1000 | Loss: 0.00005354
Iteration 17/1000 | Loss: 0.00005073
Iteration 18/1000 | Loss: 0.00004921
Iteration 19/1000 | Loss: 0.00039085
Iteration 20/1000 | Loss: 0.00083968
Iteration 21/1000 | Loss: 0.00006759
Iteration 22/1000 | Loss: 0.00005367
Iteration 23/1000 | Loss: 0.00004866
Iteration 24/1000 | Loss: 0.00004728
Iteration 25/1000 | Loss: 0.00030772
Iteration 26/1000 | Loss: 0.00008652
Iteration 27/1000 | Loss: 0.00014595
Iteration 28/1000 | Loss: 0.00004708
Iteration 29/1000 | Loss: 0.00004638
Iteration 30/1000 | Loss: 0.00004596
Iteration 31/1000 | Loss: 0.00004559
Iteration 32/1000 | Loss: 0.00004534
Iteration 33/1000 | Loss: 0.00004515
Iteration 34/1000 | Loss: 0.00004506
Iteration 35/1000 | Loss: 0.00004498
Iteration 36/1000 | Loss: 0.00028485
Iteration 37/1000 | Loss: 0.00004499
Iteration 38/1000 | Loss: 0.00004466
Iteration 39/1000 | Loss: 0.00004462
Iteration 40/1000 | Loss: 0.00004462
Iteration 41/1000 | Loss: 0.00004461
Iteration 42/1000 | Loss: 0.00004461
Iteration 43/1000 | Loss: 0.00004461
Iteration 44/1000 | Loss: 0.00004460
Iteration 45/1000 | Loss: 0.00004460
Iteration 46/1000 | Loss: 0.00004458
Iteration 47/1000 | Loss: 0.00004458
Iteration 48/1000 | Loss: 0.00004458
Iteration 49/1000 | Loss: 0.00004458
Iteration 50/1000 | Loss: 0.00004458
Iteration 51/1000 | Loss: 0.00004458
Iteration 52/1000 | Loss: 0.00004458
Iteration 53/1000 | Loss: 0.00004458
Iteration 54/1000 | Loss: 0.00004458
Iteration 55/1000 | Loss: 0.00004458
Iteration 56/1000 | Loss: 0.00004457
Iteration 57/1000 | Loss: 0.00004457
Iteration 58/1000 | Loss: 0.00004457
Iteration 59/1000 | Loss: 0.00004457
Iteration 60/1000 | Loss: 0.00004457
Iteration 61/1000 | Loss: 0.00004457
Iteration 62/1000 | Loss: 0.00004456
Iteration 63/1000 | Loss: 0.00004456
Iteration 64/1000 | Loss: 0.00004456
Iteration 65/1000 | Loss: 0.00004454
Iteration 66/1000 | Loss: 0.00004454
Iteration 67/1000 | Loss: 0.00004453
Iteration 68/1000 | Loss: 0.00004453
Iteration 69/1000 | Loss: 0.00004453
Iteration 70/1000 | Loss: 0.00004452
Iteration 71/1000 | Loss: 0.00004452
Iteration 72/1000 | Loss: 0.00004452
Iteration 73/1000 | Loss: 0.00004452
Iteration 74/1000 | Loss: 0.00004452
Iteration 75/1000 | Loss: 0.00004451
Iteration 76/1000 | Loss: 0.00004451
Iteration 77/1000 | Loss: 0.00004451
Iteration 78/1000 | Loss: 0.00004451
Iteration 79/1000 | Loss: 0.00004451
Iteration 80/1000 | Loss: 0.00004451
Iteration 81/1000 | Loss: 0.00004450
Iteration 82/1000 | Loss: 0.00004449
Iteration 83/1000 | Loss: 0.00004447
Iteration 84/1000 | Loss: 0.00004446
Iteration 85/1000 | Loss: 0.00004445
Iteration 86/1000 | Loss: 0.00004445
Iteration 87/1000 | Loss: 0.00004445
Iteration 88/1000 | Loss: 0.00004444
Iteration 89/1000 | Loss: 0.00004444
Iteration 90/1000 | Loss: 0.00004444
Iteration 91/1000 | Loss: 0.00004443
Iteration 92/1000 | Loss: 0.00004442
Iteration 93/1000 | Loss: 0.00004442
Iteration 94/1000 | Loss: 0.00004440
Iteration 95/1000 | Loss: 0.00004439
Iteration 96/1000 | Loss: 0.00004439
Iteration 97/1000 | Loss: 0.00004439
Iteration 98/1000 | Loss: 0.00004439
Iteration 99/1000 | Loss: 0.00004439
Iteration 100/1000 | Loss: 0.00004439
Iteration 101/1000 | Loss: 0.00004439
Iteration 102/1000 | Loss: 0.00004439
Iteration 103/1000 | Loss: 0.00004439
Iteration 104/1000 | Loss: 0.00004439
Iteration 105/1000 | Loss: 0.00004439
Iteration 106/1000 | Loss: 0.00004439
Iteration 107/1000 | Loss: 0.00004439
Iteration 108/1000 | Loss: 0.00004439
Iteration 109/1000 | Loss: 0.00004439
Iteration 110/1000 | Loss: 0.00004439
Iteration 111/1000 | Loss: 0.00004439
Iteration 112/1000 | Loss: 0.00004439
Iteration 113/1000 | Loss: 0.00004439
Iteration 114/1000 | Loss: 0.00004439
Iteration 115/1000 | Loss: 0.00004438
Iteration 116/1000 | Loss: 0.00004438
Iteration 117/1000 | Loss: 0.00004438
Iteration 118/1000 | Loss: 0.00004438
Iteration 119/1000 | Loss: 0.00004438
Iteration 120/1000 | Loss: 0.00004437
Iteration 121/1000 | Loss: 0.00004437
Iteration 122/1000 | Loss: 0.00004437
Iteration 123/1000 | Loss: 0.00004437
Iteration 124/1000 | Loss: 0.00004437
Iteration 125/1000 | Loss: 0.00004437
Iteration 126/1000 | Loss: 0.00004437
Iteration 127/1000 | Loss: 0.00004436
Iteration 128/1000 | Loss: 0.00004436
Iteration 129/1000 | Loss: 0.00004436
Iteration 130/1000 | Loss: 0.00004436
Iteration 131/1000 | Loss: 0.00004436
Iteration 132/1000 | Loss: 0.00004436
Iteration 133/1000 | Loss: 0.00004436
Iteration 134/1000 | Loss: 0.00004435
Iteration 135/1000 | Loss: 0.00004435
Iteration 136/1000 | Loss: 0.00004435
Iteration 137/1000 | Loss: 0.00004435
Iteration 138/1000 | Loss: 0.00004435
Iteration 139/1000 | Loss: 0.00004435
Iteration 140/1000 | Loss: 0.00004435
Iteration 141/1000 | Loss: 0.00004435
Iteration 142/1000 | Loss: 0.00004435
Iteration 143/1000 | Loss: 0.00004435
Iteration 144/1000 | Loss: 0.00004435
Iteration 145/1000 | Loss: 0.00004435
Iteration 146/1000 | Loss: 0.00004434
Iteration 147/1000 | Loss: 0.00004434
Iteration 148/1000 | Loss: 0.00004434
Iteration 149/1000 | Loss: 0.00004434
Iteration 150/1000 | Loss: 0.00004434
Iteration 151/1000 | Loss: 0.00004434
Iteration 152/1000 | Loss: 0.00004434
Iteration 153/1000 | Loss: 0.00004434
Iteration 154/1000 | Loss: 0.00004434
Iteration 155/1000 | Loss: 0.00004433
Iteration 156/1000 | Loss: 0.00004433
Iteration 157/1000 | Loss: 0.00004433
Iteration 158/1000 | Loss: 0.00004433
Iteration 159/1000 | Loss: 0.00004433
Iteration 160/1000 | Loss: 0.00004432
Iteration 161/1000 | Loss: 0.00004432
Iteration 162/1000 | Loss: 0.00004432
Iteration 163/1000 | Loss: 0.00004432
Iteration 164/1000 | Loss: 0.00004432
Iteration 165/1000 | Loss: 0.00004431
Iteration 166/1000 | Loss: 0.00004431
Iteration 167/1000 | Loss: 0.00004431
Iteration 168/1000 | Loss: 0.00004431
Iteration 169/1000 | Loss: 0.00004431
Iteration 170/1000 | Loss: 0.00004431
Iteration 171/1000 | Loss: 0.00004431
Iteration 172/1000 | Loss: 0.00004431
Iteration 173/1000 | Loss: 0.00004430
Iteration 174/1000 | Loss: 0.00004430
Iteration 175/1000 | Loss: 0.00004430
Iteration 176/1000 | Loss: 0.00004430
Iteration 177/1000 | Loss: 0.00004430
Iteration 178/1000 | Loss: 0.00004430
Iteration 179/1000 | Loss: 0.00004430
Iteration 180/1000 | Loss: 0.00004430
Iteration 181/1000 | Loss: 0.00004430
Iteration 182/1000 | Loss: 0.00004430
Iteration 183/1000 | Loss: 0.00004430
Iteration 184/1000 | Loss: 0.00004430
Iteration 185/1000 | Loss: 0.00004430
Iteration 186/1000 | Loss: 0.00004430
Iteration 187/1000 | Loss: 0.00004430
Iteration 188/1000 | Loss: 0.00004430
Iteration 189/1000 | Loss: 0.00004430
Iteration 190/1000 | Loss: 0.00004430
Iteration 191/1000 | Loss: 0.00004430
Iteration 192/1000 | Loss: 0.00004430
Iteration 193/1000 | Loss: 0.00004430
Iteration 194/1000 | Loss: 0.00004430
Iteration 195/1000 | Loss: 0.00004430
Iteration 196/1000 | Loss: 0.00004430
Iteration 197/1000 | Loss: 0.00004430
Iteration 198/1000 | Loss: 0.00004430
Iteration 199/1000 | Loss: 0.00004430
Iteration 200/1000 | Loss: 0.00004430
Iteration 201/1000 | Loss: 0.00004430
Iteration 202/1000 | Loss: 0.00004430
Iteration 203/1000 | Loss: 0.00004430
Iteration 204/1000 | Loss: 0.00004430
Iteration 205/1000 | Loss: 0.00004430
Iteration 206/1000 | Loss: 0.00004430
Iteration 207/1000 | Loss: 0.00004430
Iteration 208/1000 | Loss: 0.00004430
Iteration 209/1000 | Loss: 0.00004430
Iteration 210/1000 | Loss: 0.00004430
Iteration 211/1000 | Loss: 0.00004430
Iteration 212/1000 | Loss: 0.00004430
Iteration 213/1000 | Loss: 0.00004430
Iteration 214/1000 | Loss: 0.00004430
Iteration 215/1000 | Loss: 0.00004430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [4.430118860909715e-05, 4.430118860909715e-05, 4.430118860909715e-05, 4.430118860909715e-05, 4.430118860909715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.430118860909715e-05

Optimization complete. Final v2v error: 4.208621978759766 mm

Highest mean error: 11.73034381866455 mm for frame 19

Lowest mean error: 3.143599271774292 mm for frame 120

Saving results

Total time: 130.75225472450256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477386
Iteration 2/25 | Loss: 0.00091723
Iteration 3/25 | Loss: 0.00078388
Iteration 4/25 | Loss: 0.00076023
Iteration 5/25 | Loss: 0.00075349
Iteration 6/25 | Loss: 0.00075213
Iteration 7/25 | Loss: 0.00075202
Iteration 8/25 | Loss: 0.00075202
Iteration 9/25 | Loss: 0.00075202
Iteration 10/25 | Loss: 0.00075202
Iteration 11/25 | Loss: 0.00075202
Iteration 12/25 | Loss: 0.00075202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007520205690525472, 0.0007520205690525472, 0.0007520205690525472, 0.0007520205690525472, 0.0007520205690525472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007520205690525472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14818716
Iteration 2/25 | Loss: 0.00044542
Iteration 3/25 | Loss: 0.00044541
Iteration 4/25 | Loss: 0.00044541
Iteration 5/25 | Loss: 0.00044541
Iteration 6/25 | Loss: 0.00044541
Iteration 7/25 | Loss: 0.00044541
Iteration 8/25 | Loss: 0.00044541
Iteration 9/25 | Loss: 0.00044541
Iteration 10/25 | Loss: 0.00044541
Iteration 11/25 | Loss: 0.00044541
Iteration 12/25 | Loss: 0.00044541
Iteration 13/25 | Loss: 0.00044541
Iteration 14/25 | Loss: 0.00044541
Iteration 15/25 | Loss: 0.00044541
Iteration 16/25 | Loss: 0.00044541
Iteration 17/25 | Loss: 0.00044541
Iteration 18/25 | Loss: 0.00044541
Iteration 19/25 | Loss: 0.00044541
Iteration 20/25 | Loss: 0.00044541
Iteration 21/25 | Loss: 0.00044541
Iteration 22/25 | Loss: 0.00044541
Iteration 23/25 | Loss: 0.00044541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004454113368410617, 0.0004454113368410617, 0.0004454113368410617, 0.0004454113368410617, 0.0004454113368410617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004454113368410617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044541
Iteration 2/1000 | Loss: 0.00002179
Iteration 3/1000 | Loss: 0.00001725
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001447
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001401
Iteration 15/1000 | Loss: 0.00001397
Iteration 16/1000 | Loss: 0.00001397
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001392
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001387
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001378
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001371
Iteration 46/1000 | Loss: 0.00001371
Iteration 47/1000 | Loss: 0.00001371
Iteration 48/1000 | Loss: 0.00001371
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001370
Iteration 52/1000 | Loss: 0.00001370
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001368
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001367
Iteration 61/1000 | Loss: 0.00001367
Iteration 62/1000 | Loss: 0.00001367
Iteration 63/1000 | Loss: 0.00001367
Iteration 64/1000 | Loss: 0.00001367
Iteration 65/1000 | Loss: 0.00001366
Iteration 66/1000 | Loss: 0.00001366
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001366
Iteration 70/1000 | Loss: 0.00001366
Iteration 71/1000 | Loss: 0.00001365
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001365
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.3653006135427859e-05, 1.3653006135427859e-05, 1.3653006135427859e-05, 1.3653006135427859e-05, 1.3653006135427859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3653006135427859e-05

Optimization complete. Final v2v error: 3.152902841567993 mm

Highest mean error: 3.4837582111358643 mm for frame 212

Lowest mean error: 2.980232000350952 mm for frame 59

Saving results

Total time: 35.23689341545105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068420
Iteration 2/25 | Loss: 0.00168255
Iteration 3/25 | Loss: 0.00113693
Iteration 4/25 | Loss: 0.00113697
Iteration 5/25 | Loss: 0.00121007
Iteration 6/25 | Loss: 0.00106828
Iteration 7/25 | Loss: 0.00099183
Iteration 8/25 | Loss: 0.00097916
Iteration 9/25 | Loss: 0.00089697
Iteration 10/25 | Loss: 0.00086525
Iteration 11/25 | Loss: 0.00086462
Iteration 12/25 | Loss: 0.00087422
Iteration 13/25 | Loss: 0.00086048
Iteration 14/25 | Loss: 0.00089733
Iteration 15/25 | Loss: 0.00086208
Iteration 16/25 | Loss: 0.00085078
Iteration 17/25 | Loss: 0.00084519
Iteration 18/25 | Loss: 0.00088407
Iteration 19/25 | Loss: 0.00089974
Iteration 20/25 | Loss: 0.00082779
Iteration 21/25 | Loss: 0.00084047
Iteration 22/25 | Loss: 0.00081796
Iteration 23/25 | Loss: 0.00084345
Iteration 24/25 | Loss: 0.00083628
Iteration 25/25 | Loss: 0.00079933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65413857
Iteration 2/25 | Loss: 0.00480439
Iteration 3/25 | Loss: 0.00207803
Iteration 4/25 | Loss: 0.00207792
Iteration 5/25 | Loss: 0.00207792
Iteration 6/25 | Loss: 0.00207792
Iteration 7/25 | Loss: 0.00207792
Iteration 8/25 | Loss: 0.00207792
Iteration 9/25 | Loss: 0.00207792
Iteration 10/25 | Loss: 0.00207792
Iteration 11/25 | Loss: 0.00207792
Iteration 12/25 | Loss: 0.00207792
Iteration 13/25 | Loss: 0.00207792
Iteration 14/25 | Loss: 0.00207792
Iteration 15/25 | Loss: 0.00207792
Iteration 16/25 | Loss: 0.00207792
Iteration 17/25 | Loss: 0.00207792
Iteration 18/25 | Loss: 0.00207792
Iteration 19/25 | Loss: 0.00207792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020779180340468884, 0.0020779180340468884, 0.0020779180340468884, 0.0020779180340468884, 0.0020779180340468884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020779180340468884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207792
Iteration 2/1000 | Loss: 0.00011661
Iteration 3/1000 | Loss: 0.00015227
Iteration 4/1000 | Loss: 0.00015754
Iteration 5/1000 | Loss: 0.00020384
Iteration 6/1000 | Loss: 0.00469984
Iteration 7/1000 | Loss: 0.00251945
Iteration 8/1000 | Loss: 0.00490541
Iteration 9/1000 | Loss: 0.00524339
Iteration 10/1000 | Loss: 0.00348539
Iteration 11/1000 | Loss: 0.00351499
Iteration 12/1000 | Loss: 0.00382757
Iteration 13/1000 | Loss: 0.00327767
Iteration 14/1000 | Loss: 0.00265390
Iteration 15/1000 | Loss: 0.00299985
Iteration 16/1000 | Loss: 0.00298445
Iteration 17/1000 | Loss: 0.00112297
Iteration 18/1000 | Loss: 0.00547101
Iteration 19/1000 | Loss: 0.00259964
Iteration 20/1000 | Loss: 0.00293082
Iteration 21/1000 | Loss: 0.00354646
Iteration 22/1000 | Loss: 0.00154010
Iteration 23/1000 | Loss: 0.00247840
Iteration 24/1000 | Loss: 0.00231468
Iteration 25/1000 | Loss: 0.00293275
Iteration 26/1000 | Loss: 0.00292735
Iteration 27/1000 | Loss: 0.00226082
Iteration 28/1000 | Loss: 0.00287261
Iteration 29/1000 | Loss: 0.00428355
Iteration 30/1000 | Loss: 0.00130154
Iteration 31/1000 | Loss: 0.00532211
Iteration 32/1000 | Loss: 0.00101215
Iteration 33/1000 | Loss: 0.00178086
Iteration 34/1000 | Loss: 0.00130581
Iteration 35/1000 | Loss: 0.00020040
Iteration 36/1000 | Loss: 0.00123331
Iteration 37/1000 | Loss: 0.00006840
Iteration 38/1000 | Loss: 0.00112976
Iteration 39/1000 | Loss: 0.00059450
Iteration 40/1000 | Loss: 0.00104394
Iteration 41/1000 | Loss: 0.00006220
Iteration 42/1000 | Loss: 0.00004572
Iteration 43/1000 | Loss: 0.00003874
Iteration 44/1000 | Loss: 0.00003617
Iteration 45/1000 | Loss: 0.00224164
Iteration 46/1000 | Loss: 0.00106525
Iteration 47/1000 | Loss: 0.00186031
Iteration 48/1000 | Loss: 0.00187110
Iteration 49/1000 | Loss: 0.00205077
Iteration 50/1000 | Loss: 0.00146306
Iteration 51/1000 | Loss: 0.00178524
Iteration 52/1000 | Loss: 0.00122570
Iteration 53/1000 | Loss: 0.00057745
Iteration 54/1000 | Loss: 0.00183576
Iteration 55/1000 | Loss: 0.00095618
Iteration 56/1000 | Loss: 0.00076389
Iteration 57/1000 | Loss: 0.00034387
Iteration 58/1000 | Loss: 0.00093108
Iteration 59/1000 | Loss: 0.00022867
Iteration 60/1000 | Loss: 0.00007177
Iteration 61/1000 | Loss: 0.00053494
Iteration 62/1000 | Loss: 0.00064746
Iteration 63/1000 | Loss: 0.00059122
Iteration 64/1000 | Loss: 0.00015181
Iteration 65/1000 | Loss: 0.00003833
Iteration 66/1000 | Loss: 0.00003260
Iteration 67/1000 | Loss: 0.00002779
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00141445
Iteration 71/1000 | Loss: 0.00005589
Iteration 72/1000 | Loss: 0.00002544
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001351
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001213
Iteration 100/1000 | Loss: 0.00001213
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001209
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001202
Iteration 141/1000 | Loss: 0.00001202
Iteration 142/1000 | Loss: 0.00001202
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001202
Iteration 147/1000 | Loss: 0.00001202
Iteration 148/1000 | Loss: 0.00001202
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001201
Iteration 152/1000 | Loss: 0.00001201
Iteration 153/1000 | Loss: 0.00001201
Iteration 154/1000 | Loss: 0.00001201
Iteration 155/1000 | Loss: 0.00001201
Iteration 156/1000 | Loss: 0.00001201
Iteration 157/1000 | Loss: 0.00001201
Iteration 158/1000 | Loss: 0.00001201
Iteration 159/1000 | Loss: 0.00001201
Iteration 160/1000 | Loss: 0.00001201
Iteration 161/1000 | Loss: 0.00001201
Iteration 162/1000 | Loss: 0.00001201
Iteration 163/1000 | Loss: 0.00001201
Iteration 164/1000 | Loss: 0.00001201
Iteration 165/1000 | Loss: 0.00001201
Iteration 166/1000 | Loss: 0.00001201
Iteration 167/1000 | Loss: 0.00001200
Iteration 168/1000 | Loss: 0.00001200
Iteration 169/1000 | Loss: 0.00001200
Iteration 170/1000 | Loss: 0.00001200
Iteration 171/1000 | Loss: 0.00001200
Iteration 172/1000 | Loss: 0.00001200
Iteration 173/1000 | Loss: 0.00001200
Iteration 174/1000 | Loss: 0.00001200
Iteration 175/1000 | Loss: 0.00001200
Iteration 176/1000 | Loss: 0.00001200
Iteration 177/1000 | Loss: 0.00001200
Iteration 178/1000 | Loss: 0.00001200
Iteration 179/1000 | Loss: 0.00001200
Iteration 180/1000 | Loss: 0.00001200
Iteration 181/1000 | Loss: 0.00001199
Iteration 182/1000 | Loss: 0.00001199
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001199
Iteration 185/1000 | Loss: 0.00001199
Iteration 186/1000 | Loss: 0.00001199
Iteration 187/1000 | Loss: 0.00001199
Iteration 188/1000 | Loss: 0.00001199
Iteration 189/1000 | Loss: 0.00001199
Iteration 190/1000 | Loss: 0.00001199
Iteration 191/1000 | Loss: 0.00001199
Iteration 192/1000 | Loss: 0.00001199
Iteration 193/1000 | Loss: 0.00001199
Iteration 194/1000 | Loss: 0.00001199
Iteration 195/1000 | Loss: 0.00001199
Iteration 196/1000 | Loss: 0.00001199
Iteration 197/1000 | Loss: 0.00001199
Iteration 198/1000 | Loss: 0.00001199
Iteration 199/1000 | Loss: 0.00001199
Iteration 200/1000 | Loss: 0.00001199
Iteration 201/1000 | Loss: 0.00001199
Iteration 202/1000 | Loss: 0.00001199
Iteration 203/1000 | Loss: 0.00001199
Iteration 204/1000 | Loss: 0.00001199
Iteration 205/1000 | Loss: 0.00001199
Iteration 206/1000 | Loss: 0.00001199
Iteration 207/1000 | Loss: 0.00001199
Iteration 208/1000 | Loss: 0.00001199
Iteration 209/1000 | Loss: 0.00001199
Iteration 210/1000 | Loss: 0.00001199
Iteration 211/1000 | Loss: 0.00001199
Iteration 212/1000 | Loss: 0.00001199
Iteration 213/1000 | Loss: 0.00001199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.1989207450824324e-05, 1.1989207450824324e-05, 1.1989207450824324e-05, 1.1989207450824324e-05, 1.1989207450824324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1989207450824324e-05

Optimization complete. Final v2v error: 2.9383418560028076 mm

Highest mean error: 3.4608676433563232 mm for frame 60

Lowest mean error: 2.734001398086548 mm for frame 1

Saving results

Total time: 172.17873978614807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099177
Iteration 2/25 | Loss: 0.01099177
Iteration 3/25 | Loss: 0.01099177
Iteration 4/25 | Loss: 0.01099177
Iteration 5/25 | Loss: 0.01099177
Iteration 6/25 | Loss: 0.01099177
Iteration 7/25 | Loss: 0.01099176
Iteration 8/25 | Loss: 0.01099176
Iteration 9/25 | Loss: 0.01099176
Iteration 10/25 | Loss: 0.01099176
Iteration 11/25 | Loss: 0.01099176
Iteration 12/25 | Loss: 0.01099176
Iteration 13/25 | Loss: 0.01099176
Iteration 14/25 | Loss: 0.01099176
Iteration 15/25 | Loss: 0.01099176
Iteration 16/25 | Loss: 0.01099176
Iteration 17/25 | Loss: 0.01099176
Iteration 18/25 | Loss: 0.01099175
Iteration 19/25 | Loss: 0.01099175
Iteration 20/25 | Loss: 0.01099175
Iteration 21/25 | Loss: 0.01099175
Iteration 22/25 | Loss: 0.01099175
Iteration 23/25 | Loss: 0.01099175
Iteration 24/25 | Loss: 0.01099175
Iteration 25/25 | Loss: 0.01099175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73456299
Iteration 2/25 | Loss: 0.06099708
Iteration 3/25 | Loss: 0.06097974
Iteration 4/25 | Loss: 0.06097973
Iteration 5/25 | Loss: 0.06097973
Iteration 6/25 | Loss: 0.06097973
Iteration 7/25 | Loss: 0.06097973
Iteration 8/25 | Loss: 0.06097973
Iteration 9/25 | Loss: 0.06097973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.060979731380939484, 0.060979731380939484, 0.060979731380939484, 0.060979731380939484, 0.060979731380939484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.060979731380939484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06097973
Iteration 2/1000 | Loss: 0.00881582
Iteration 3/1000 | Loss: 0.00093166
Iteration 4/1000 | Loss: 0.00054025
Iteration 5/1000 | Loss: 0.00077618
Iteration 6/1000 | Loss: 0.00038317
Iteration 7/1000 | Loss: 0.00016010
Iteration 8/1000 | Loss: 0.00050530
Iteration 9/1000 | Loss: 0.00006739
Iteration 10/1000 | Loss: 0.00074398
Iteration 11/1000 | Loss: 0.00038713
Iteration 12/1000 | Loss: 0.00081603
Iteration 13/1000 | Loss: 0.00173085
Iteration 14/1000 | Loss: 0.00005818
Iteration 15/1000 | Loss: 0.00091107
Iteration 16/1000 | Loss: 0.00027090
Iteration 17/1000 | Loss: 0.00063099
Iteration 18/1000 | Loss: 0.00080441
Iteration 19/1000 | Loss: 0.00010595
Iteration 20/1000 | Loss: 0.00029338
Iteration 21/1000 | Loss: 0.00004727
Iteration 22/1000 | Loss: 0.00004450
Iteration 23/1000 | Loss: 0.00069752
Iteration 24/1000 | Loss: 0.00126486
Iteration 25/1000 | Loss: 0.00092976
Iteration 26/1000 | Loss: 0.00007383
Iteration 27/1000 | Loss: 0.00005050
Iteration 28/1000 | Loss: 0.00038063
Iteration 29/1000 | Loss: 0.00037601
Iteration 30/1000 | Loss: 0.00004193
Iteration 31/1000 | Loss: 0.00003771
Iteration 32/1000 | Loss: 0.00016106
Iteration 33/1000 | Loss: 0.00031194
Iteration 34/1000 | Loss: 0.00056225
Iteration 35/1000 | Loss: 0.00017314
Iteration 36/1000 | Loss: 0.00004101
Iteration 37/1000 | Loss: 0.00003519
Iteration 38/1000 | Loss: 0.00034964
Iteration 39/1000 | Loss: 0.00018017
Iteration 40/1000 | Loss: 0.00015269
Iteration 41/1000 | Loss: 0.00003199
Iteration 42/1000 | Loss: 0.00011590
Iteration 43/1000 | Loss: 0.00017174
Iteration 44/1000 | Loss: 0.00003069
Iteration 45/1000 | Loss: 0.00039531
Iteration 46/1000 | Loss: 0.00081435
Iteration 47/1000 | Loss: 0.00005254
Iteration 48/1000 | Loss: 0.00002948
Iteration 49/1000 | Loss: 0.00002791
Iteration 50/1000 | Loss: 0.00002672
Iteration 51/1000 | Loss: 0.00002599
Iteration 52/1000 | Loss: 0.00002530
Iteration 53/1000 | Loss: 0.00002469
Iteration 54/1000 | Loss: 0.00002416
Iteration 55/1000 | Loss: 0.00002368
Iteration 56/1000 | Loss: 0.00019693
Iteration 57/1000 | Loss: 0.00002382
Iteration 58/1000 | Loss: 0.00002327
Iteration 59/1000 | Loss: 0.00002344
Iteration 60/1000 | Loss: 0.00002317
Iteration 61/1000 | Loss: 0.00002302
Iteration 62/1000 | Loss: 0.00002286
Iteration 63/1000 | Loss: 0.00002282
Iteration 64/1000 | Loss: 0.00002281
Iteration 65/1000 | Loss: 0.00002281
Iteration 66/1000 | Loss: 0.00004061
Iteration 67/1000 | Loss: 0.00093502
Iteration 68/1000 | Loss: 0.00021452
Iteration 69/1000 | Loss: 0.00013092
Iteration 70/1000 | Loss: 0.00038259
Iteration 71/1000 | Loss: 0.00010253
Iteration 72/1000 | Loss: 0.00007709
Iteration 73/1000 | Loss: 0.00029884
Iteration 74/1000 | Loss: 0.00010910
Iteration 75/1000 | Loss: 0.00006706
Iteration 76/1000 | Loss: 0.00005290
Iteration 77/1000 | Loss: 0.00003786
Iteration 78/1000 | Loss: 0.00015423
Iteration 79/1000 | Loss: 0.00006565
Iteration 80/1000 | Loss: 0.00003764
Iteration 81/1000 | Loss: 0.00004125
Iteration 82/1000 | Loss: 0.00003825
Iteration 83/1000 | Loss: 0.00005131
Iteration 84/1000 | Loss: 0.00003059
Iteration 85/1000 | Loss: 0.00026482
Iteration 86/1000 | Loss: 0.00002694
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002349
Iteration 89/1000 | Loss: 0.00002325
Iteration 90/1000 | Loss: 0.00002318
Iteration 91/1000 | Loss: 0.00002313
Iteration 92/1000 | Loss: 0.00002299
Iteration 93/1000 | Loss: 0.00002298
Iteration 94/1000 | Loss: 0.00002286
Iteration 95/1000 | Loss: 0.00002303
Iteration 96/1000 | Loss: 0.00002301
Iteration 97/1000 | Loss: 0.00002295
Iteration 98/1000 | Loss: 0.00002295
Iteration 99/1000 | Loss: 0.00002294
Iteration 100/1000 | Loss: 0.00002294
Iteration 101/1000 | Loss: 0.00002294
Iteration 102/1000 | Loss: 0.00002293
Iteration 103/1000 | Loss: 0.00002293
Iteration 104/1000 | Loss: 0.00002292
Iteration 105/1000 | Loss: 0.00002292
Iteration 106/1000 | Loss: 0.00002291
Iteration 107/1000 | Loss: 0.00002285
Iteration 108/1000 | Loss: 0.00002278
Iteration 109/1000 | Loss: 0.00002278
Iteration 110/1000 | Loss: 0.00002277
Iteration 111/1000 | Loss: 0.00002292
Iteration 112/1000 | Loss: 0.00002289
Iteration 113/1000 | Loss: 0.00002272
Iteration 114/1000 | Loss: 0.00002272
Iteration 115/1000 | Loss: 0.00002269
Iteration 116/1000 | Loss: 0.00002267
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002267
Iteration 120/1000 | Loss: 0.00002266
Iteration 121/1000 | Loss: 0.00002266
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002266
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002264
Iteration 129/1000 | Loss: 0.00002264
Iteration 130/1000 | Loss: 0.00002264
Iteration 131/1000 | Loss: 0.00002263
Iteration 132/1000 | Loss: 0.00002263
Iteration 133/1000 | Loss: 0.00002263
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002262
Iteration 136/1000 | Loss: 0.00002261
Iteration 137/1000 | Loss: 0.00002260
Iteration 138/1000 | Loss: 0.00002259
Iteration 139/1000 | Loss: 0.00002258
Iteration 140/1000 | Loss: 0.00002257
Iteration 141/1000 | Loss: 0.00002257
Iteration 142/1000 | Loss: 0.00002256
Iteration 143/1000 | Loss: 0.00002254
Iteration 144/1000 | Loss: 0.00002253
Iteration 145/1000 | Loss: 0.00002252
Iteration 146/1000 | Loss: 0.00002252
Iteration 147/1000 | Loss: 0.00002250
Iteration 148/1000 | Loss: 0.00002249
Iteration 149/1000 | Loss: 0.00002249
Iteration 150/1000 | Loss: 0.00002248
Iteration 151/1000 | Loss: 0.00002248
Iteration 152/1000 | Loss: 0.00002247
Iteration 153/1000 | Loss: 0.00002250
Iteration 154/1000 | Loss: 0.00002250
Iteration 155/1000 | Loss: 0.00002250
Iteration 156/1000 | Loss: 0.00002246
Iteration 157/1000 | Loss: 0.00002246
Iteration 158/1000 | Loss: 0.00002245
Iteration 159/1000 | Loss: 0.00002245
Iteration 160/1000 | Loss: 0.00002244
Iteration 161/1000 | Loss: 0.00002244
Iteration 162/1000 | Loss: 0.00002244
Iteration 163/1000 | Loss: 0.00002243
Iteration 164/1000 | Loss: 0.00002243
Iteration 165/1000 | Loss: 0.00002243
Iteration 166/1000 | Loss: 0.00002242
Iteration 167/1000 | Loss: 0.00002241
Iteration 168/1000 | Loss: 0.00002241
Iteration 169/1000 | Loss: 0.00002240
Iteration 170/1000 | Loss: 0.00002240
Iteration 171/1000 | Loss: 0.00002240
Iteration 172/1000 | Loss: 0.00002240
Iteration 173/1000 | Loss: 0.00002240
Iteration 174/1000 | Loss: 0.00002240
Iteration 175/1000 | Loss: 0.00002239
Iteration 176/1000 | Loss: 0.00002239
Iteration 177/1000 | Loss: 0.00002239
Iteration 178/1000 | Loss: 0.00002239
Iteration 179/1000 | Loss: 0.00002239
Iteration 180/1000 | Loss: 0.00002238
Iteration 181/1000 | Loss: 0.00002238
Iteration 182/1000 | Loss: 0.00002238
Iteration 183/1000 | Loss: 0.00002238
Iteration 184/1000 | Loss: 0.00002237
Iteration 185/1000 | Loss: 0.00002237
Iteration 186/1000 | Loss: 0.00002237
Iteration 187/1000 | Loss: 0.00002237
Iteration 188/1000 | Loss: 0.00002237
Iteration 189/1000 | Loss: 0.00002237
Iteration 190/1000 | Loss: 0.00002237
Iteration 191/1000 | Loss: 0.00002237
Iteration 192/1000 | Loss: 0.00002236
Iteration 193/1000 | Loss: 0.00002236
Iteration 194/1000 | Loss: 0.00002236
Iteration 195/1000 | Loss: 0.00002236
Iteration 196/1000 | Loss: 0.00002236
Iteration 197/1000 | Loss: 0.00002236
Iteration 198/1000 | Loss: 0.00002236
Iteration 199/1000 | Loss: 0.00002236
Iteration 200/1000 | Loss: 0.00002236
Iteration 201/1000 | Loss: 0.00002236
Iteration 202/1000 | Loss: 0.00002236
Iteration 203/1000 | Loss: 0.00002235
Iteration 204/1000 | Loss: 0.00002235
Iteration 205/1000 | Loss: 0.00002235
Iteration 206/1000 | Loss: 0.00002235
Iteration 207/1000 | Loss: 0.00002235
Iteration 208/1000 | Loss: 0.00002235
Iteration 209/1000 | Loss: 0.00002235
Iteration 210/1000 | Loss: 0.00002235
Iteration 211/1000 | Loss: 0.00002234
Iteration 212/1000 | Loss: 0.00002234
Iteration 213/1000 | Loss: 0.00002234
Iteration 214/1000 | Loss: 0.00002234
Iteration 215/1000 | Loss: 0.00002234
Iteration 216/1000 | Loss: 0.00002233
Iteration 217/1000 | Loss: 0.00002233
Iteration 218/1000 | Loss: 0.00002233
Iteration 219/1000 | Loss: 0.00002233
Iteration 220/1000 | Loss: 0.00002233
Iteration 221/1000 | Loss: 0.00002233
Iteration 222/1000 | Loss: 0.00002233
Iteration 223/1000 | Loss: 0.00002233
Iteration 224/1000 | Loss: 0.00002233
Iteration 225/1000 | Loss: 0.00002233
Iteration 226/1000 | Loss: 0.00002232
Iteration 227/1000 | Loss: 0.00002232
Iteration 228/1000 | Loss: 0.00002232
Iteration 229/1000 | Loss: 0.00002232
Iteration 230/1000 | Loss: 0.00002232
Iteration 231/1000 | Loss: 0.00002232
Iteration 232/1000 | Loss: 0.00002231
Iteration 233/1000 | Loss: 0.00002231
Iteration 234/1000 | Loss: 0.00002231
Iteration 235/1000 | Loss: 0.00002231
Iteration 236/1000 | Loss: 0.00002231
Iteration 237/1000 | Loss: 0.00002231
Iteration 238/1000 | Loss: 0.00002231
Iteration 239/1000 | Loss: 0.00002231
Iteration 240/1000 | Loss: 0.00002231
Iteration 241/1000 | Loss: 0.00002231
Iteration 242/1000 | Loss: 0.00002231
Iteration 243/1000 | Loss: 0.00002231
Iteration 244/1000 | Loss: 0.00002231
Iteration 245/1000 | Loss: 0.00002231
Iteration 246/1000 | Loss: 0.00002231
Iteration 247/1000 | Loss: 0.00002231
Iteration 248/1000 | Loss: 0.00002231
Iteration 249/1000 | Loss: 0.00002231
Iteration 250/1000 | Loss: 0.00002231
Iteration 251/1000 | Loss: 0.00002231
Iteration 252/1000 | Loss: 0.00002231
Iteration 253/1000 | Loss: 0.00002231
Iteration 254/1000 | Loss: 0.00002231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.2306769096758217e-05, 2.2306769096758217e-05, 2.2306769096758217e-05, 2.2306769096758217e-05, 2.2306769096758217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2306769096758217e-05

Optimization complete. Final v2v error: 3.886361598968506 mm

Highest mean error: 9.942618370056152 mm for frame 16

Lowest mean error: 3.31949520111084 mm for frame 77

Saving results

Total time: 166.44233870506287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863646
Iteration 2/25 | Loss: 0.00087158
Iteration 3/25 | Loss: 0.00073281
Iteration 4/25 | Loss: 0.00070981
Iteration 5/25 | Loss: 0.00070315
Iteration 6/25 | Loss: 0.00070151
Iteration 7/25 | Loss: 0.00070146
Iteration 8/25 | Loss: 0.00070146
Iteration 9/25 | Loss: 0.00070146
Iteration 10/25 | Loss: 0.00070146
Iteration 11/25 | Loss: 0.00070146
Iteration 12/25 | Loss: 0.00070146
Iteration 13/25 | Loss: 0.00070146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007014611619524658, 0.0007014611619524658, 0.0007014611619524658, 0.0007014611619524658, 0.0007014611619524658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007014611619524658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.11734247
Iteration 2/25 | Loss: 0.00040245
Iteration 3/25 | Loss: 0.00040245
Iteration 4/25 | Loss: 0.00040245
Iteration 5/25 | Loss: 0.00040245
Iteration 6/25 | Loss: 0.00040245
Iteration 7/25 | Loss: 0.00040245
Iteration 8/25 | Loss: 0.00040245
Iteration 9/25 | Loss: 0.00040245
Iteration 10/25 | Loss: 0.00040245
Iteration 11/25 | Loss: 0.00040245
Iteration 12/25 | Loss: 0.00040245
Iteration 13/25 | Loss: 0.00040245
Iteration 14/25 | Loss: 0.00040245
Iteration 15/25 | Loss: 0.00040245
Iteration 16/25 | Loss: 0.00040245
Iteration 17/25 | Loss: 0.00040245
Iteration 18/25 | Loss: 0.00040245
Iteration 19/25 | Loss: 0.00040245
Iteration 20/25 | Loss: 0.00040245
Iteration 21/25 | Loss: 0.00040245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004024450608994812, 0.0004024450608994812, 0.0004024450608994812, 0.0004024450608994812, 0.0004024450608994812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004024450608994812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040245
Iteration 2/1000 | Loss: 0.00001696
Iteration 3/1000 | Loss: 0.00001344
Iteration 4/1000 | Loss: 0.00001245
Iteration 5/1000 | Loss: 0.00001148
Iteration 6/1000 | Loss: 0.00001115
Iteration 7/1000 | Loss: 0.00001093
Iteration 8/1000 | Loss: 0.00001079
Iteration 9/1000 | Loss: 0.00001068
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001048
Iteration 13/1000 | Loss: 0.00001044
Iteration 14/1000 | Loss: 0.00001043
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001040
Iteration 19/1000 | Loss: 0.00001040
Iteration 20/1000 | Loss: 0.00001039
Iteration 21/1000 | Loss: 0.00001039
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001037
Iteration 24/1000 | Loss: 0.00001037
Iteration 25/1000 | Loss: 0.00001036
Iteration 26/1000 | Loss: 0.00001035
Iteration 27/1000 | Loss: 0.00001034
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001033
Iteration 32/1000 | Loss: 0.00001032
Iteration 33/1000 | Loss: 0.00001032
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001031
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001030
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001029
Iteration 42/1000 | Loss: 0.00001029
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001028
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001026
Iteration 49/1000 | Loss: 0.00001026
Iteration 50/1000 | Loss: 0.00001026
Iteration 51/1000 | Loss: 0.00001026
Iteration 52/1000 | Loss: 0.00001026
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001024
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001023
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001022
Iteration 64/1000 | Loss: 0.00001022
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001022
Iteration 67/1000 | Loss: 0.00001022
Iteration 68/1000 | Loss: 0.00001021
Iteration 69/1000 | Loss: 0.00001021
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001020
Iteration 72/1000 | Loss: 0.00001020
Iteration 73/1000 | Loss: 0.00001019
Iteration 74/1000 | Loss: 0.00001019
Iteration 75/1000 | Loss: 0.00001019
Iteration 76/1000 | Loss: 0.00001018
Iteration 77/1000 | Loss: 0.00001018
Iteration 78/1000 | Loss: 0.00001018
Iteration 79/1000 | Loss: 0.00001018
Iteration 80/1000 | Loss: 0.00001018
Iteration 81/1000 | Loss: 0.00001017
Iteration 82/1000 | Loss: 0.00001017
Iteration 83/1000 | Loss: 0.00001017
Iteration 84/1000 | Loss: 0.00001016
Iteration 85/1000 | Loss: 0.00001016
Iteration 86/1000 | Loss: 0.00001016
Iteration 87/1000 | Loss: 0.00001016
Iteration 88/1000 | Loss: 0.00001016
Iteration 89/1000 | Loss: 0.00001016
Iteration 90/1000 | Loss: 0.00001015
Iteration 91/1000 | Loss: 0.00001015
Iteration 92/1000 | Loss: 0.00001015
Iteration 93/1000 | Loss: 0.00001015
Iteration 94/1000 | Loss: 0.00001015
Iteration 95/1000 | Loss: 0.00001015
Iteration 96/1000 | Loss: 0.00001015
Iteration 97/1000 | Loss: 0.00001014
Iteration 98/1000 | Loss: 0.00001014
Iteration 99/1000 | Loss: 0.00001014
Iteration 100/1000 | Loss: 0.00001013
Iteration 101/1000 | Loss: 0.00001013
Iteration 102/1000 | Loss: 0.00001013
Iteration 103/1000 | Loss: 0.00001012
Iteration 104/1000 | Loss: 0.00001012
Iteration 105/1000 | Loss: 0.00001012
Iteration 106/1000 | Loss: 0.00001011
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001011
Iteration 109/1000 | Loss: 0.00001010
Iteration 110/1000 | Loss: 0.00001010
Iteration 111/1000 | Loss: 0.00001010
Iteration 112/1000 | Loss: 0.00001010
Iteration 113/1000 | Loss: 0.00001009
Iteration 114/1000 | Loss: 0.00001009
Iteration 115/1000 | Loss: 0.00001009
Iteration 116/1000 | Loss: 0.00001009
Iteration 117/1000 | Loss: 0.00001009
Iteration 118/1000 | Loss: 0.00001009
Iteration 119/1000 | Loss: 0.00001009
Iteration 120/1000 | Loss: 0.00001009
Iteration 121/1000 | Loss: 0.00001009
Iteration 122/1000 | Loss: 0.00001009
Iteration 123/1000 | Loss: 0.00001009
Iteration 124/1000 | Loss: 0.00001009
Iteration 125/1000 | Loss: 0.00001009
Iteration 126/1000 | Loss: 0.00001009
Iteration 127/1000 | Loss: 0.00001009
Iteration 128/1000 | Loss: 0.00001009
Iteration 129/1000 | Loss: 0.00001009
Iteration 130/1000 | Loss: 0.00001009
Iteration 131/1000 | Loss: 0.00001009
Iteration 132/1000 | Loss: 0.00001009
Iteration 133/1000 | Loss: 0.00001009
Iteration 134/1000 | Loss: 0.00001009
Iteration 135/1000 | Loss: 0.00001009
Iteration 136/1000 | Loss: 0.00001009
Iteration 137/1000 | Loss: 0.00001009
Iteration 138/1000 | Loss: 0.00001009
Iteration 139/1000 | Loss: 0.00001009
Iteration 140/1000 | Loss: 0.00001009
Iteration 141/1000 | Loss: 0.00001009
Iteration 142/1000 | Loss: 0.00001009
Iteration 143/1000 | Loss: 0.00001009
Iteration 144/1000 | Loss: 0.00001009
Iteration 145/1000 | Loss: 0.00001009
Iteration 146/1000 | Loss: 0.00001009
Iteration 147/1000 | Loss: 0.00001009
Iteration 148/1000 | Loss: 0.00001009
Iteration 149/1000 | Loss: 0.00001009
Iteration 150/1000 | Loss: 0.00001009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0092206139233895e-05, 1.0092206139233895e-05, 1.0092206139233895e-05, 1.0092206139233895e-05, 1.0092206139233895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0092206139233895e-05

Optimization complete. Final v2v error: 2.711411237716675 mm

Highest mean error: 2.903008460998535 mm for frame 235

Lowest mean error: 2.596271276473999 mm for frame 171

Saving results

Total time: 58.82166862487793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090463
Iteration 2/25 | Loss: 0.00198261
Iteration 3/25 | Loss: 0.00114103
Iteration 4/25 | Loss: 0.00093002
Iteration 5/25 | Loss: 0.00087301
Iteration 6/25 | Loss: 0.00082426
Iteration 7/25 | Loss: 0.00077373
Iteration 8/25 | Loss: 0.00076173
Iteration 9/25 | Loss: 0.00075750
Iteration 10/25 | Loss: 0.00075710
Iteration 11/25 | Loss: 0.00075708
Iteration 12/25 | Loss: 0.00075708
Iteration 13/25 | Loss: 0.00075708
Iteration 14/25 | Loss: 0.00075708
Iteration 15/25 | Loss: 0.00075708
Iteration 16/25 | Loss: 0.00075708
Iteration 17/25 | Loss: 0.00075708
Iteration 18/25 | Loss: 0.00075708
Iteration 19/25 | Loss: 0.00075708
Iteration 20/25 | Loss: 0.00075708
Iteration 21/25 | Loss: 0.00075708
Iteration 22/25 | Loss: 0.00075708
Iteration 23/25 | Loss: 0.00075708
Iteration 24/25 | Loss: 0.00075708
Iteration 25/25 | Loss: 0.00075708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45297921
Iteration 2/25 | Loss: 0.00043627
Iteration 3/25 | Loss: 0.00043627
Iteration 4/25 | Loss: 0.00043627
Iteration 5/25 | Loss: 0.00043627
Iteration 6/25 | Loss: 0.00043627
Iteration 7/25 | Loss: 0.00043627
Iteration 8/25 | Loss: 0.00043627
Iteration 9/25 | Loss: 0.00043627
Iteration 10/25 | Loss: 0.00043627
Iteration 11/25 | Loss: 0.00043627
Iteration 12/25 | Loss: 0.00043627
Iteration 13/25 | Loss: 0.00043627
Iteration 14/25 | Loss: 0.00043627
Iteration 15/25 | Loss: 0.00043627
Iteration 16/25 | Loss: 0.00043627
Iteration 17/25 | Loss: 0.00043627
Iteration 18/25 | Loss: 0.00043627
Iteration 19/25 | Loss: 0.00043627
Iteration 20/25 | Loss: 0.00043627
Iteration 21/25 | Loss: 0.00043627
Iteration 22/25 | Loss: 0.00043627
Iteration 23/25 | Loss: 0.00043627
Iteration 24/25 | Loss: 0.00043627
Iteration 25/25 | Loss: 0.00043627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043627
Iteration 2/1000 | Loss: 0.00002817
Iteration 3/1000 | Loss: 0.00002202
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001939
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001777
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001756
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001715
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001712
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001697
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001691
Iteration 33/1000 | Loss: 0.00001691
Iteration 34/1000 | Loss: 0.00001691
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001691
Iteration 37/1000 | Loss: 0.00001691
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001690
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001690
Iteration 43/1000 | Loss: 0.00001690
Iteration 44/1000 | Loss: 0.00001689
Iteration 45/1000 | Loss: 0.00001689
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001688
Iteration 49/1000 | Loss: 0.00001687
Iteration 50/1000 | Loss: 0.00001687
Iteration 51/1000 | Loss: 0.00001687
Iteration 52/1000 | Loss: 0.00001687
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001687
Iteration 60/1000 | Loss: 0.00001687
Iteration 61/1000 | Loss: 0.00001687
Iteration 62/1000 | Loss: 0.00001686
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001685
Iteration 82/1000 | Loss: 0.00001685
Iteration 83/1000 | Loss: 0.00001685
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.6846053767949343e-05, 1.6846053767949343e-05, 1.6846053767949343e-05, 1.6846053767949343e-05, 1.6846053767949343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6846053767949343e-05

Optimization complete. Final v2v error: 3.456346273422241 mm

Highest mean error: 3.5691797733306885 mm for frame 53

Lowest mean error: 3.3315117359161377 mm for frame 86

Saving results

Total time: 95.3194363117218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967783
Iteration 2/25 | Loss: 0.00117483
Iteration 3/25 | Loss: 0.00097213
Iteration 4/25 | Loss: 0.00091691
Iteration 5/25 | Loss: 0.00089955
Iteration 6/25 | Loss: 0.00089656
Iteration 7/25 | Loss: 0.00089500
Iteration 8/25 | Loss: 0.00089494
Iteration 9/25 | Loss: 0.00089494
Iteration 10/25 | Loss: 0.00089494
Iteration 11/25 | Loss: 0.00089494
Iteration 12/25 | Loss: 0.00089494
Iteration 13/25 | Loss: 0.00089494
Iteration 14/25 | Loss: 0.00089494
Iteration 15/25 | Loss: 0.00089494
Iteration 16/25 | Loss: 0.00089494
Iteration 17/25 | Loss: 0.00089494
Iteration 18/25 | Loss: 0.00089494
Iteration 19/25 | Loss: 0.00089494
Iteration 20/25 | Loss: 0.00089494
Iteration 21/25 | Loss: 0.00089494
Iteration 22/25 | Loss: 0.00089494
Iteration 23/25 | Loss: 0.00089494
Iteration 24/25 | Loss: 0.00089494
Iteration 25/25 | Loss: 0.00089494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.17773429
Iteration 2/25 | Loss: 0.00047448
Iteration 3/25 | Loss: 0.00047448
Iteration 4/25 | Loss: 0.00047448
Iteration 5/25 | Loss: 0.00047448
Iteration 6/25 | Loss: 0.00047448
Iteration 7/25 | Loss: 0.00047448
Iteration 8/25 | Loss: 0.00047448
Iteration 9/25 | Loss: 0.00047448
Iteration 10/25 | Loss: 0.00047448
Iteration 11/25 | Loss: 0.00047448
Iteration 12/25 | Loss: 0.00047448
Iteration 13/25 | Loss: 0.00047448
Iteration 14/25 | Loss: 0.00047448
Iteration 15/25 | Loss: 0.00047448
Iteration 16/25 | Loss: 0.00047448
Iteration 17/25 | Loss: 0.00047448
Iteration 18/25 | Loss: 0.00047448
Iteration 19/25 | Loss: 0.00047448
Iteration 20/25 | Loss: 0.00047448
Iteration 21/25 | Loss: 0.00047448
Iteration 22/25 | Loss: 0.00047448
Iteration 23/25 | Loss: 0.00047448
Iteration 24/25 | Loss: 0.00047448
Iteration 25/25 | Loss: 0.00047448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047448
Iteration 2/1000 | Loss: 0.00006463
Iteration 3/1000 | Loss: 0.00004851
Iteration 4/1000 | Loss: 0.00004066
Iteration 5/1000 | Loss: 0.00003889
Iteration 6/1000 | Loss: 0.00003737
Iteration 7/1000 | Loss: 0.00003652
Iteration 8/1000 | Loss: 0.00003580
Iteration 9/1000 | Loss: 0.00003524
Iteration 10/1000 | Loss: 0.00003485
Iteration 11/1000 | Loss: 0.00003458
Iteration 12/1000 | Loss: 0.00003429
Iteration 13/1000 | Loss: 0.00003405
Iteration 14/1000 | Loss: 0.00003389
Iteration 15/1000 | Loss: 0.00003382
Iteration 16/1000 | Loss: 0.00003381
Iteration 17/1000 | Loss: 0.00003380
Iteration 18/1000 | Loss: 0.00003363
Iteration 19/1000 | Loss: 0.00003362
Iteration 20/1000 | Loss: 0.00003361
Iteration 21/1000 | Loss: 0.00003355
Iteration 22/1000 | Loss: 0.00003355
Iteration 23/1000 | Loss: 0.00003354
Iteration 24/1000 | Loss: 0.00003354
Iteration 25/1000 | Loss: 0.00003353
Iteration 26/1000 | Loss: 0.00003353
Iteration 27/1000 | Loss: 0.00003352
Iteration 28/1000 | Loss: 0.00003352
Iteration 29/1000 | Loss: 0.00003352
Iteration 30/1000 | Loss: 0.00003352
Iteration 31/1000 | Loss: 0.00003352
Iteration 32/1000 | Loss: 0.00003351
Iteration 33/1000 | Loss: 0.00003351
Iteration 34/1000 | Loss: 0.00003351
Iteration 35/1000 | Loss: 0.00003350
Iteration 36/1000 | Loss: 0.00003350
Iteration 37/1000 | Loss: 0.00003349
Iteration 38/1000 | Loss: 0.00003349
Iteration 39/1000 | Loss: 0.00003349
Iteration 40/1000 | Loss: 0.00003348
Iteration 41/1000 | Loss: 0.00003345
Iteration 42/1000 | Loss: 0.00003345
Iteration 43/1000 | Loss: 0.00003345
Iteration 44/1000 | Loss: 0.00003345
Iteration 45/1000 | Loss: 0.00003345
Iteration 46/1000 | Loss: 0.00003345
Iteration 47/1000 | Loss: 0.00003345
Iteration 48/1000 | Loss: 0.00003344
Iteration 49/1000 | Loss: 0.00003344
Iteration 50/1000 | Loss: 0.00003344
Iteration 51/1000 | Loss: 0.00003344
Iteration 52/1000 | Loss: 0.00003344
Iteration 53/1000 | Loss: 0.00003343
Iteration 54/1000 | Loss: 0.00003342
Iteration 55/1000 | Loss: 0.00003341
Iteration 56/1000 | Loss: 0.00003341
Iteration 57/1000 | Loss: 0.00003340
Iteration 58/1000 | Loss: 0.00003340
Iteration 59/1000 | Loss: 0.00003340
Iteration 60/1000 | Loss: 0.00003340
Iteration 61/1000 | Loss: 0.00003340
Iteration 62/1000 | Loss: 0.00003340
Iteration 63/1000 | Loss: 0.00003340
Iteration 64/1000 | Loss: 0.00003340
Iteration 65/1000 | Loss: 0.00003340
Iteration 66/1000 | Loss: 0.00003340
Iteration 67/1000 | Loss: 0.00003340
Iteration 68/1000 | Loss: 0.00003340
Iteration 69/1000 | Loss: 0.00003339
Iteration 70/1000 | Loss: 0.00003339
Iteration 71/1000 | Loss: 0.00003339
Iteration 72/1000 | Loss: 0.00003338
Iteration 73/1000 | Loss: 0.00003338
Iteration 74/1000 | Loss: 0.00003338
Iteration 75/1000 | Loss: 0.00003337
Iteration 76/1000 | Loss: 0.00003337
Iteration 77/1000 | Loss: 0.00003337
Iteration 78/1000 | Loss: 0.00003336
Iteration 79/1000 | Loss: 0.00003336
Iteration 80/1000 | Loss: 0.00003336
Iteration 81/1000 | Loss: 0.00003336
Iteration 82/1000 | Loss: 0.00003336
Iteration 83/1000 | Loss: 0.00003336
Iteration 84/1000 | Loss: 0.00003335
Iteration 85/1000 | Loss: 0.00003335
Iteration 86/1000 | Loss: 0.00003335
Iteration 87/1000 | Loss: 0.00003335
Iteration 88/1000 | Loss: 0.00003335
Iteration 89/1000 | Loss: 0.00003335
Iteration 90/1000 | Loss: 0.00003335
Iteration 91/1000 | Loss: 0.00003335
Iteration 92/1000 | Loss: 0.00003335
Iteration 93/1000 | Loss: 0.00003335
Iteration 94/1000 | Loss: 0.00003335
Iteration 95/1000 | Loss: 0.00003335
Iteration 96/1000 | Loss: 0.00003335
Iteration 97/1000 | Loss: 0.00003335
Iteration 98/1000 | Loss: 0.00003335
Iteration 99/1000 | Loss: 0.00003335
Iteration 100/1000 | Loss: 0.00003335
Iteration 101/1000 | Loss: 0.00003335
Iteration 102/1000 | Loss: 0.00003335
Iteration 103/1000 | Loss: 0.00003335
Iteration 104/1000 | Loss: 0.00003335
Iteration 105/1000 | Loss: 0.00003335
Iteration 106/1000 | Loss: 0.00003335
Iteration 107/1000 | Loss: 0.00003335
Iteration 108/1000 | Loss: 0.00003335
Iteration 109/1000 | Loss: 0.00003335
Iteration 110/1000 | Loss: 0.00003335
Iteration 111/1000 | Loss: 0.00003335
Iteration 112/1000 | Loss: 0.00003335
Iteration 113/1000 | Loss: 0.00003335
Iteration 114/1000 | Loss: 0.00003335
Iteration 115/1000 | Loss: 0.00003335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.335440487717278e-05, 3.335440487717278e-05, 3.335440487717278e-05, 3.335440487717278e-05, 3.335440487717278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.335440487717278e-05

Optimization complete. Final v2v error: 4.83572244644165 mm

Highest mean error: 5.055484771728516 mm for frame 40

Lowest mean error: 4.3931732177734375 mm for frame 11

Saving results

Total time: 38.97745704650879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804562
Iteration 2/25 | Loss: 0.00144304
Iteration 3/25 | Loss: 0.00106899
Iteration 4/25 | Loss: 0.00097139
Iteration 5/25 | Loss: 0.00095712
Iteration 6/25 | Loss: 0.00095079
Iteration 7/25 | Loss: 0.00094286
Iteration 8/25 | Loss: 0.00093120
Iteration 9/25 | Loss: 0.00093931
Iteration 10/25 | Loss: 0.00093515
Iteration 11/25 | Loss: 0.00092694
Iteration 12/25 | Loss: 0.00093488
Iteration 13/25 | Loss: 0.00093120
Iteration 14/25 | Loss: 0.00092847
Iteration 15/25 | Loss: 0.00093043
Iteration 16/25 | Loss: 0.00094767
Iteration 17/25 | Loss: 0.00093789
Iteration 18/25 | Loss: 0.00093697
Iteration 19/25 | Loss: 0.00092722
Iteration 20/25 | Loss: 0.00092204
Iteration 21/25 | Loss: 0.00092320
Iteration 22/25 | Loss: 0.00091668
Iteration 23/25 | Loss: 0.00093206
Iteration 24/25 | Loss: 0.00091793
Iteration 25/25 | Loss: 0.00092563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.96974134
Iteration 2/25 | Loss: 0.00196154
Iteration 3/25 | Loss: 0.00196132
Iteration 4/25 | Loss: 0.00196132
Iteration 5/25 | Loss: 0.00196132
Iteration 6/25 | Loss: 0.00196132
Iteration 7/25 | Loss: 0.00196132
Iteration 8/25 | Loss: 0.00196132
Iteration 9/25 | Loss: 0.00196132
Iteration 10/25 | Loss: 0.00196132
Iteration 11/25 | Loss: 0.00196132
Iteration 12/25 | Loss: 0.00196132
Iteration 13/25 | Loss: 0.00196132
Iteration 14/25 | Loss: 0.00196132
Iteration 15/25 | Loss: 0.00196132
Iteration 16/25 | Loss: 0.00196132
Iteration 17/25 | Loss: 0.00196132
Iteration 18/25 | Loss: 0.00196132
Iteration 19/25 | Loss: 0.00196132
Iteration 20/25 | Loss: 0.00196132
Iteration 21/25 | Loss: 0.00196132
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019613164477050304, 0.0019613164477050304, 0.0019613164477050304, 0.0019613164477050304, 0.0019613164477050304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019613164477050304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196132
Iteration 2/1000 | Loss: 0.00133797
Iteration 3/1000 | Loss: 0.01336905
Iteration 4/1000 | Loss: 0.01727482
Iteration 5/1000 | Loss: 0.00619380
Iteration 6/1000 | Loss: 0.01286558
Iteration 7/1000 | Loss: 0.00573522
Iteration 8/1000 | Loss: 0.00926847
Iteration 9/1000 | Loss: 0.00153717
Iteration 10/1000 | Loss: 0.00222296
Iteration 11/1000 | Loss: 0.00143256
Iteration 12/1000 | Loss: 0.00109423
Iteration 13/1000 | Loss: 0.00107178
Iteration 14/1000 | Loss: 0.00157014
Iteration 15/1000 | Loss: 0.00406282
Iteration 16/1000 | Loss: 0.00237217
Iteration 17/1000 | Loss: 0.00091083
Iteration 18/1000 | Loss: 0.00532309
Iteration 19/1000 | Loss: 0.00018049
Iteration 20/1000 | Loss: 0.00075984
Iteration 21/1000 | Loss: 0.00130704
Iteration 22/1000 | Loss: 0.00630999
Iteration 23/1000 | Loss: 0.00051735
Iteration 24/1000 | Loss: 0.00086455
Iteration 25/1000 | Loss: 0.00102708
Iteration 26/1000 | Loss: 0.00287348
Iteration 27/1000 | Loss: 0.00095774
Iteration 28/1000 | Loss: 0.00222854
Iteration 29/1000 | Loss: 0.00684376
Iteration 30/1000 | Loss: 0.00162076
Iteration 31/1000 | Loss: 0.00119155
Iteration 32/1000 | Loss: 0.00224980
Iteration 33/1000 | Loss: 0.00102007
Iteration 34/1000 | Loss: 0.00136557
Iteration 35/1000 | Loss: 0.00275525
Iteration 36/1000 | Loss: 0.00282444
Iteration 37/1000 | Loss: 0.00117577
Iteration 38/1000 | Loss: 0.00234480
Iteration 39/1000 | Loss: 0.00063884
Iteration 40/1000 | Loss: 0.00077852
Iteration 41/1000 | Loss: 0.00136743
Iteration 42/1000 | Loss: 0.00049829
Iteration 43/1000 | Loss: 0.00055442
Iteration 44/1000 | Loss: 0.00073236
Iteration 45/1000 | Loss: 0.00091906
Iteration 46/1000 | Loss: 0.00647441
Iteration 47/1000 | Loss: 0.00034953
Iteration 48/1000 | Loss: 0.00129136
Iteration 49/1000 | Loss: 0.00107758
Iteration 50/1000 | Loss: 0.00122537
Iteration 51/1000 | Loss: 0.00102896
Iteration 52/1000 | Loss: 0.00075803
Iteration 53/1000 | Loss: 0.00075135
Iteration 54/1000 | Loss: 0.00052727
Iteration 55/1000 | Loss: 0.00065966
Iteration 56/1000 | Loss: 0.00069532
Iteration 57/1000 | Loss: 0.00073604
Iteration 58/1000 | Loss: 0.00121599
Iteration 59/1000 | Loss: 0.00581789
Iteration 60/1000 | Loss: 0.00248391
Iteration 61/1000 | Loss: 0.00050900
Iteration 62/1000 | Loss: 0.00119857
Iteration 63/1000 | Loss: 0.00134523
Iteration 64/1000 | Loss: 0.00031489
Iteration 65/1000 | Loss: 0.00086810
Iteration 66/1000 | Loss: 0.00059599
Iteration 67/1000 | Loss: 0.00078844
Iteration 68/1000 | Loss: 0.00038608
Iteration 69/1000 | Loss: 0.00020615
Iteration 70/1000 | Loss: 0.00047308
Iteration 71/1000 | Loss: 0.00049070
Iteration 72/1000 | Loss: 0.00067914
Iteration 73/1000 | Loss: 0.00074078
Iteration 74/1000 | Loss: 0.00067154
Iteration 75/1000 | Loss: 0.00482234
Iteration 76/1000 | Loss: 0.00130143
Iteration 77/1000 | Loss: 0.00093847
Iteration 78/1000 | Loss: 0.00101796
Iteration 79/1000 | Loss: 0.00107273
Iteration 80/1000 | Loss: 0.00095446
Iteration 81/1000 | Loss: 0.00081357
Iteration 82/1000 | Loss: 0.00115736
Iteration 83/1000 | Loss: 0.00055792
Iteration 84/1000 | Loss: 0.00065111
Iteration 85/1000 | Loss: 0.00103738
Iteration 86/1000 | Loss: 0.00089486
Iteration 87/1000 | Loss: 0.00047674
Iteration 88/1000 | Loss: 0.00028140
Iteration 89/1000 | Loss: 0.00066231
Iteration 90/1000 | Loss: 0.00071424
Iteration 91/1000 | Loss: 0.00073793
Iteration 92/1000 | Loss: 0.00075563
Iteration 93/1000 | Loss: 0.00069259
Iteration 94/1000 | Loss: 0.00065578
Iteration 95/1000 | Loss: 0.00080524
Iteration 96/1000 | Loss: 0.00157723
Iteration 97/1000 | Loss: 0.00091973
Iteration 98/1000 | Loss: 0.00108650
Iteration 99/1000 | Loss: 0.00013537
Iteration 100/1000 | Loss: 0.00022788
Iteration 101/1000 | Loss: 0.00022387
Iteration 102/1000 | Loss: 0.00027230
Iteration 103/1000 | Loss: 0.00023745
Iteration 104/1000 | Loss: 0.00046807
Iteration 105/1000 | Loss: 0.00032598
Iteration 106/1000 | Loss: 0.00013836
Iteration 107/1000 | Loss: 0.00016622
Iteration 108/1000 | Loss: 0.00013762
Iteration 109/1000 | Loss: 0.00017937
Iteration 110/1000 | Loss: 0.00006284
Iteration 111/1000 | Loss: 0.00008254
Iteration 112/1000 | Loss: 0.00011074
Iteration 113/1000 | Loss: 0.00003909
Iteration 114/1000 | Loss: 0.00008081
Iteration 115/1000 | Loss: 0.00015963
Iteration 116/1000 | Loss: 0.00007726
Iteration 117/1000 | Loss: 0.00023530
Iteration 118/1000 | Loss: 0.00011677
Iteration 119/1000 | Loss: 0.00014923
Iteration 120/1000 | Loss: 0.00011020
Iteration 121/1000 | Loss: 0.00013219
Iteration 122/1000 | Loss: 0.00009348
Iteration 123/1000 | Loss: 0.00012764
Iteration 124/1000 | Loss: 0.00009734
Iteration 125/1000 | Loss: 0.00011790
Iteration 126/1000 | Loss: 0.00010682
Iteration 127/1000 | Loss: 0.00009230
Iteration 128/1000 | Loss: 0.00012600
Iteration 129/1000 | Loss: 0.00021056
Iteration 130/1000 | Loss: 0.00032679
Iteration 131/1000 | Loss: 0.00015905
Iteration 132/1000 | Loss: 0.00026526
Iteration 133/1000 | Loss: 0.00019553
Iteration 134/1000 | Loss: 0.00033427
Iteration 135/1000 | Loss: 0.00024606
Iteration 136/1000 | Loss: 0.00028958
Iteration 137/1000 | Loss: 0.00048188
Iteration 138/1000 | Loss: 0.00082514
Iteration 139/1000 | Loss: 0.00095742
Iteration 140/1000 | Loss: 0.00017146
Iteration 141/1000 | Loss: 0.00024235
Iteration 142/1000 | Loss: 0.00041905
Iteration 143/1000 | Loss: 0.00027606
Iteration 144/1000 | Loss: 0.00025093
Iteration 145/1000 | Loss: 0.00022977
Iteration 146/1000 | Loss: 0.00027052
Iteration 147/1000 | Loss: 0.00026985
Iteration 148/1000 | Loss: 0.00028214
Iteration 149/1000 | Loss: 0.00031480
Iteration 150/1000 | Loss: 0.00032392
Iteration 151/1000 | Loss: 0.00022215
Iteration 152/1000 | Loss: 0.00020570
Iteration 153/1000 | Loss: 0.00027223
Iteration 154/1000 | Loss: 0.00029120
Iteration 155/1000 | Loss: 0.00022049
Iteration 156/1000 | Loss: 0.00022449
Iteration 157/1000 | Loss: 0.00039259
Iteration 158/1000 | Loss: 0.00005236
Iteration 159/1000 | Loss: 0.00004204
Iteration 160/1000 | Loss: 0.00005120
Iteration 161/1000 | Loss: 0.00003453
Iteration 162/1000 | Loss: 0.00005156
Iteration 163/1000 | Loss: 0.00004608
Iteration 164/1000 | Loss: 0.00005054
Iteration 165/1000 | Loss: 0.00005591
Iteration 166/1000 | Loss: 0.00005157
Iteration 167/1000 | Loss: 0.00004161
Iteration 168/1000 | Loss: 0.00003274
Iteration 169/1000 | Loss: 0.00005078
Iteration 170/1000 | Loss: 0.00005687
Iteration 171/1000 | Loss: 0.00004918
Iteration 172/1000 | Loss: 0.00005596
Iteration 173/1000 | Loss: 0.00005556
Iteration 174/1000 | Loss: 0.00004810
Iteration 175/1000 | Loss: 0.00004616
Iteration 176/1000 | Loss: 0.00007166
Iteration 177/1000 | Loss: 0.00004759
Iteration 178/1000 | Loss: 0.00005055
Iteration 179/1000 | Loss: 0.00005502
Iteration 180/1000 | Loss: 0.00005021
Iteration 181/1000 | Loss: 0.00005964
Iteration 182/1000 | Loss: 0.00004934
Iteration 183/1000 | Loss: 0.00003184
Iteration 184/1000 | Loss: 0.00007373
Iteration 185/1000 | Loss: 0.00003610
Iteration 186/1000 | Loss: 0.00003221
Iteration 187/1000 | Loss: 0.00002987
Iteration 188/1000 | Loss: 0.00002885
Iteration 189/1000 | Loss: 0.00002772
Iteration 190/1000 | Loss: 0.00002652
Iteration 191/1000 | Loss: 0.00003751
Iteration 192/1000 | Loss: 0.00003001
Iteration 193/1000 | Loss: 0.00002887
Iteration 194/1000 | Loss: 0.00002833
Iteration 195/1000 | Loss: 0.00002771
Iteration 196/1000 | Loss: 0.00002704
Iteration 197/1000 | Loss: 0.00002643
Iteration 198/1000 | Loss: 0.00002637
Iteration 199/1000 | Loss: 0.00002633
Iteration 200/1000 | Loss: 0.00002625
Iteration 201/1000 | Loss: 0.00002625
Iteration 202/1000 | Loss: 0.00002624
Iteration 203/1000 | Loss: 0.00002623
Iteration 204/1000 | Loss: 0.00002622
Iteration 205/1000 | Loss: 0.00002617
Iteration 206/1000 | Loss: 0.00002613
Iteration 207/1000 | Loss: 0.00002611
Iteration 208/1000 | Loss: 0.00002609
Iteration 209/1000 | Loss: 0.00002598
Iteration 210/1000 | Loss: 0.00002597
Iteration 211/1000 | Loss: 0.00002597
Iteration 212/1000 | Loss: 0.00002593
Iteration 213/1000 | Loss: 0.00002591
Iteration 214/1000 | Loss: 0.00003902
Iteration 215/1000 | Loss: 0.00003560
Iteration 216/1000 | Loss: 0.00002940
Iteration 217/1000 | Loss: 0.00002622
Iteration 218/1000 | Loss: 0.00002519
Iteration 219/1000 | Loss: 0.00002469
Iteration 220/1000 | Loss: 0.00002433
Iteration 221/1000 | Loss: 0.00002410
Iteration 222/1000 | Loss: 0.00002398
Iteration 223/1000 | Loss: 0.00002398
Iteration 224/1000 | Loss: 0.00002397
Iteration 225/1000 | Loss: 0.00002396
Iteration 226/1000 | Loss: 0.00002395
Iteration 227/1000 | Loss: 0.00002395
Iteration 228/1000 | Loss: 0.00002395
Iteration 229/1000 | Loss: 0.00002394
Iteration 230/1000 | Loss: 0.00002394
Iteration 231/1000 | Loss: 0.00002393
Iteration 232/1000 | Loss: 0.00002393
Iteration 233/1000 | Loss: 0.00002393
Iteration 234/1000 | Loss: 0.00002393
Iteration 235/1000 | Loss: 0.00002393
Iteration 236/1000 | Loss: 0.00002393
Iteration 237/1000 | Loss: 0.00002393
Iteration 238/1000 | Loss: 0.00002392
Iteration 239/1000 | Loss: 0.00002392
Iteration 240/1000 | Loss: 0.00002392
Iteration 241/1000 | Loss: 0.00002392
Iteration 242/1000 | Loss: 0.00002392
Iteration 243/1000 | Loss: 0.00002392
Iteration 244/1000 | Loss: 0.00002392
Iteration 245/1000 | Loss: 0.00002392
Iteration 246/1000 | Loss: 0.00002392
Iteration 247/1000 | Loss: 0.00002392
Iteration 248/1000 | Loss: 0.00002391
Iteration 249/1000 | Loss: 0.00002391
Iteration 250/1000 | Loss: 0.00002391
Iteration 251/1000 | Loss: 0.00002391
Iteration 252/1000 | Loss: 0.00002391
Iteration 253/1000 | Loss: 0.00002391
Iteration 254/1000 | Loss: 0.00002391
Iteration 255/1000 | Loss: 0.00002391
Iteration 256/1000 | Loss: 0.00002391
Iteration 257/1000 | Loss: 0.00002391
Iteration 258/1000 | Loss: 0.00002391
Iteration 259/1000 | Loss: 0.00002390
Iteration 260/1000 | Loss: 0.00002390
Iteration 261/1000 | Loss: 0.00002390
Iteration 262/1000 | Loss: 0.00002390
Iteration 263/1000 | Loss: 0.00002390
Iteration 264/1000 | Loss: 0.00002389
Iteration 265/1000 | Loss: 0.00002389
Iteration 266/1000 | Loss: 0.00002389
Iteration 267/1000 | Loss: 0.00002389
Iteration 268/1000 | Loss: 0.00002389
Iteration 269/1000 | Loss: 0.00002389
Iteration 270/1000 | Loss: 0.00002389
Iteration 271/1000 | Loss: 0.00002389
Iteration 272/1000 | Loss: 0.00002389
Iteration 273/1000 | Loss: 0.00002389
Iteration 274/1000 | Loss: 0.00002389
Iteration 275/1000 | Loss: 0.00002389
Iteration 276/1000 | Loss: 0.00002389
Iteration 277/1000 | Loss: 0.00002389
Iteration 278/1000 | Loss: 0.00002389
Iteration 279/1000 | Loss: 0.00002389
Iteration 280/1000 | Loss: 0.00002389
Iteration 281/1000 | Loss: 0.00002388
Iteration 282/1000 | Loss: 0.00002388
Iteration 283/1000 | Loss: 0.00002388
Iteration 284/1000 | Loss: 0.00002388
Iteration 285/1000 | Loss: 0.00002388
Iteration 286/1000 | Loss: 0.00002388
Iteration 287/1000 | Loss: 0.00002388
Iteration 288/1000 | Loss: 0.00002388
Iteration 289/1000 | Loss: 0.00002388
Iteration 290/1000 | Loss: 0.00002388
Iteration 291/1000 | Loss: 0.00002388
Iteration 292/1000 | Loss: 0.00002388
Iteration 293/1000 | Loss: 0.00002388
Iteration 294/1000 | Loss: 0.00002388
Iteration 295/1000 | Loss: 0.00002388
Iteration 296/1000 | Loss: 0.00002388
Iteration 297/1000 | Loss: 0.00002388
Iteration 298/1000 | Loss: 0.00002388
Iteration 299/1000 | Loss: 0.00002388
Iteration 300/1000 | Loss: 0.00002388
Iteration 301/1000 | Loss: 0.00002388
Iteration 302/1000 | Loss: 0.00002388
Iteration 303/1000 | Loss: 0.00002388
Iteration 304/1000 | Loss: 0.00002388
Iteration 305/1000 | Loss: 0.00002388
Iteration 306/1000 | Loss: 0.00002388
Iteration 307/1000 | Loss: 0.00002388
Iteration 308/1000 | Loss: 0.00002388
Iteration 309/1000 | Loss: 0.00002388
Iteration 310/1000 | Loss: 0.00002388
Iteration 311/1000 | Loss: 0.00002388
Iteration 312/1000 | Loss: 0.00002388
Iteration 313/1000 | Loss: 0.00002388
Iteration 314/1000 | Loss: 0.00002388
Iteration 315/1000 | Loss: 0.00002388
Iteration 316/1000 | Loss: 0.00002388
Iteration 317/1000 | Loss: 0.00002388
Iteration 318/1000 | Loss: 0.00002388
Iteration 319/1000 | Loss: 0.00002388
Iteration 320/1000 | Loss: 0.00002388
Iteration 321/1000 | Loss: 0.00002388
Iteration 322/1000 | Loss: 0.00002388
Iteration 323/1000 | Loss: 0.00002388
Iteration 324/1000 | Loss: 0.00002388
Iteration 325/1000 | Loss: 0.00002388
Iteration 326/1000 | Loss: 0.00002388
Iteration 327/1000 | Loss: 0.00002388
Iteration 328/1000 | Loss: 0.00002388
Iteration 329/1000 | Loss: 0.00002388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [2.3879154468886554e-05, 2.3879154468886554e-05, 2.3879154468886554e-05, 2.3879154468886554e-05, 2.3879154468886554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3879154468886554e-05

Optimization complete. Final v2v error: 3.9008493423461914 mm

Highest mean error: 6.414363861083984 mm for frame 34

Lowest mean error: 2.8053975105285645 mm for frame 72

Saving results

Total time: 358.1224250793457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824990
Iteration 2/25 | Loss: 0.00132126
Iteration 3/25 | Loss: 0.00085056
Iteration 4/25 | Loss: 0.00079566
Iteration 5/25 | Loss: 0.00078315
Iteration 6/25 | Loss: 0.00077902
Iteration 7/25 | Loss: 0.00077785
Iteration 8/25 | Loss: 0.00077785
Iteration 9/25 | Loss: 0.00077785
Iteration 10/25 | Loss: 0.00077785
Iteration 11/25 | Loss: 0.00077785
Iteration 12/25 | Loss: 0.00077785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007778519066050649, 0.0007778519066050649, 0.0007778519066050649, 0.0007778519066050649, 0.0007778519066050649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007778519066050649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53390741
Iteration 2/25 | Loss: 0.00045057
Iteration 3/25 | Loss: 0.00045057
Iteration 4/25 | Loss: 0.00045057
Iteration 5/25 | Loss: 0.00045057
Iteration 6/25 | Loss: 0.00045057
Iteration 7/25 | Loss: 0.00045057
Iteration 8/25 | Loss: 0.00045057
Iteration 9/25 | Loss: 0.00045057
Iteration 10/25 | Loss: 0.00045057
Iteration 11/25 | Loss: 0.00045057
Iteration 12/25 | Loss: 0.00045057
Iteration 13/25 | Loss: 0.00045057
Iteration 14/25 | Loss: 0.00045057
Iteration 15/25 | Loss: 0.00045057
Iteration 16/25 | Loss: 0.00045057
Iteration 17/25 | Loss: 0.00045057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004505691467784345, 0.0004505691467784345, 0.0004505691467784345, 0.0004505691467784345, 0.0004505691467784345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004505691467784345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045057
Iteration 2/1000 | Loss: 0.00003257
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002342
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00002023
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001931
Iteration 10/1000 | Loss: 0.00001904
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001890
Iteration 14/1000 | Loss: 0.00001889
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001876
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001872
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00001861
Iteration 92/1000 | Loss: 0.00001861
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001860
Iteration 100/1000 | Loss: 0.00001860
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001859
Iteration 107/1000 | Loss: 0.00001859
Iteration 108/1000 | Loss: 0.00001859
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001858
Iteration 112/1000 | Loss: 0.00001858
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001858
Iteration 116/1000 | Loss: 0.00001858
Iteration 117/1000 | Loss: 0.00001858
Iteration 118/1000 | Loss: 0.00001858
Iteration 119/1000 | Loss: 0.00001858
Iteration 120/1000 | Loss: 0.00001858
Iteration 121/1000 | Loss: 0.00001858
Iteration 122/1000 | Loss: 0.00001858
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001857
Iteration 125/1000 | Loss: 0.00001857
Iteration 126/1000 | Loss: 0.00001857
Iteration 127/1000 | Loss: 0.00001857
Iteration 128/1000 | Loss: 0.00001857
Iteration 129/1000 | Loss: 0.00001857
Iteration 130/1000 | Loss: 0.00001857
Iteration 131/1000 | Loss: 0.00001857
Iteration 132/1000 | Loss: 0.00001857
Iteration 133/1000 | Loss: 0.00001857
Iteration 134/1000 | Loss: 0.00001857
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001857
Iteration 137/1000 | Loss: 0.00001857
Iteration 138/1000 | Loss: 0.00001857
Iteration 139/1000 | Loss: 0.00001857
Iteration 140/1000 | Loss: 0.00001857
Iteration 141/1000 | Loss: 0.00001857
Iteration 142/1000 | Loss: 0.00001857
Iteration 143/1000 | Loss: 0.00001857
Iteration 144/1000 | Loss: 0.00001857
Iteration 145/1000 | Loss: 0.00001857
Iteration 146/1000 | Loss: 0.00001857
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.8565833670436405e-05, 1.8565833670436405e-05, 1.8565833670436405e-05, 1.8565833670436405e-05, 1.8565833670436405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8565833670436405e-05

Optimization complete. Final v2v error: 3.5156009197235107 mm

Highest mean error: 4.004437446594238 mm for frame 57

Lowest mean error: 2.9628939628601074 mm for frame 0

Saving results

Total time: 41.33901596069336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852688
Iteration 2/25 | Loss: 0.00094297
Iteration 3/25 | Loss: 0.00076143
Iteration 4/25 | Loss: 0.00073582
Iteration 5/25 | Loss: 0.00072926
Iteration 6/25 | Loss: 0.00072703
Iteration 7/25 | Loss: 0.00072666
Iteration 8/25 | Loss: 0.00072666
Iteration 9/25 | Loss: 0.00072666
Iteration 10/25 | Loss: 0.00072666
Iteration 11/25 | Loss: 0.00072666
Iteration 12/25 | Loss: 0.00072666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007266647880896926, 0.0007266647880896926, 0.0007266647880896926, 0.0007266647880896926, 0.0007266647880896926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007266647880896926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49699461
Iteration 2/25 | Loss: 0.00044601
Iteration 3/25 | Loss: 0.00044599
Iteration 4/25 | Loss: 0.00044599
Iteration 5/25 | Loss: 0.00044599
Iteration 6/25 | Loss: 0.00044598
Iteration 7/25 | Loss: 0.00044598
Iteration 8/25 | Loss: 0.00044598
Iteration 9/25 | Loss: 0.00044598
Iteration 10/25 | Loss: 0.00044598
Iteration 11/25 | Loss: 0.00044598
Iteration 12/25 | Loss: 0.00044598
Iteration 13/25 | Loss: 0.00044598
Iteration 14/25 | Loss: 0.00044598
Iteration 15/25 | Loss: 0.00044598
Iteration 16/25 | Loss: 0.00044598
Iteration 17/25 | Loss: 0.00044598
Iteration 18/25 | Loss: 0.00044598
Iteration 19/25 | Loss: 0.00044598
Iteration 20/25 | Loss: 0.00044598
Iteration 21/25 | Loss: 0.00044598
Iteration 22/25 | Loss: 0.00044598
Iteration 23/25 | Loss: 0.00044598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004459834599401802, 0.0004459834599401802, 0.0004459834599401802, 0.0004459834599401802, 0.0004459834599401802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004459834599401802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044598
Iteration 2/1000 | Loss: 0.00002089
Iteration 3/1000 | Loss: 0.00001572
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001194
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.1815662219305523e-05, 1.1815662219305523e-05, 1.1815662219305523e-05, 1.1815662219305523e-05, 1.1815662219305523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1815662219305523e-05

Optimization complete. Final v2v error: 2.913959264755249 mm

Highest mean error: 3.080817937850952 mm for frame 124

Lowest mean error: 2.81972336769104 mm for frame 39

Saving results

Total time: 32.512009143829346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857481
Iteration 2/25 | Loss: 0.00163296
Iteration 3/25 | Loss: 0.00106940
Iteration 4/25 | Loss: 0.00094530
Iteration 5/25 | Loss: 0.00094172
Iteration 6/25 | Loss: 0.00094023
Iteration 7/25 | Loss: 0.00092745
Iteration 8/25 | Loss: 0.00090121
Iteration 9/25 | Loss: 0.00088736
Iteration 10/25 | Loss: 0.00087343
Iteration 11/25 | Loss: 0.00085910
Iteration 12/25 | Loss: 0.00085832
Iteration 13/25 | Loss: 0.00086239
Iteration 14/25 | Loss: 0.00085090
Iteration 15/25 | Loss: 0.00084410
Iteration 16/25 | Loss: 0.00084643
Iteration 17/25 | Loss: 0.00084036
Iteration 18/25 | Loss: 0.00083930
Iteration 19/25 | Loss: 0.00083899
Iteration 20/25 | Loss: 0.00084033
Iteration 21/25 | Loss: 0.00083878
Iteration 22/25 | Loss: 0.00083873
Iteration 23/25 | Loss: 0.00083873
Iteration 24/25 | Loss: 0.00083873
Iteration 25/25 | Loss: 0.00083873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80225348
Iteration 2/25 | Loss: 0.00068319
Iteration 3/25 | Loss: 0.00063185
Iteration 4/25 | Loss: 0.00061550
Iteration 5/25 | Loss: 0.00061550
Iteration 6/25 | Loss: 0.00061550
Iteration 7/25 | Loss: 0.00061550
Iteration 8/25 | Loss: 0.00061550
Iteration 9/25 | Loss: 0.00061550
Iteration 10/25 | Loss: 0.00061550
Iteration 11/25 | Loss: 0.00061550
Iteration 12/25 | Loss: 0.00061550
Iteration 13/25 | Loss: 0.00061550
Iteration 14/25 | Loss: 0.00061550
Iteration 15/25 | Loss: 0.00061550
Iteration 16/25 | Loss: 0.00061550
Iteration 17/25 | Loss: 0.00061550
Iteration 18/25 | Loss: 0.00061550
Iteration 19/25 | Loss: 0.00061550
Iteration 20/25 | Loss: 0.00061550
Iteration 21/25 | Loss: 0.00061550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006154963630251586, 0.0006154963630251586, 0.0006154963630251586, 0.0006154963630251586, 0.0006154963630251586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006154963630251586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061550
Iteration 2/1000 | Loss: 0.00015036
Iteration 3/1000 | Loss: 0.00003933
Iteration 4/1000 | Loss: 0.00003184
Iteration 5/1000 | Loss: 0.00002804
Iteration 6/1000 | Loss: 0.00015117
Iteration 7/1000 | Loss: 0.00011590
Iteration 8/1000 | Loss: 0.00004498
Iteration 9/1000 | Loss: 0.00013568
Iteration 10/1000 | Loss: 0.00002932
Iteration 11/1000 | Loss: 0.00002435
Iteration 12/1000 | Loss: 0.00012066
Iteration 13/1000 | Loss: 0.00018849
Iteration 14/1000 | Loss: 0.00002309
Iteration 15/1000 | Loss: 0.00006513
Iteration 16/1000 | Loss: 0.00002069
Iteration 17/1000 | Loss: 0.00001992
Iteration 18/1000 | Loss: 0.00006362
Iteration 19/1000 | Loss: 0.00008771
Iteration 20/1000 | Loss: 0.00005750
Iteration 21/1000 | Loss: 0.00002377
Iteration 22/1000 | Loss: 0.00001904
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001973
Iteration 25/1000 | Loss: 0.00005491
Iteration 26/1000 | Loss: 0.00001877
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001860
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001837
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001827
Iteration 37/1000 | Loss: 0.00001826
Iteration 38/1000 | Loss: 0.00001825
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001824
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001822
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001821
Iteration 47/1000 | Loss: 0.00001821
Iteration 48/1000 | Loss: 0.00001819
Iteration 49/1000 | Loss: 0.00001819
Iteration 50/1000 | Loss: 0.00001819
Iteration 51/1000 | Loss: 0.00001818
Iteration 52/1000 | Loss: 0.00001818
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001817
Iteration 55/1000 | Loss: 0.00001817
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001816
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00005422
Iteration 65/1000 | Loss: 0.00001821
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001813
Iteration 69/1000 | Loss: 0.00001813
Iteration 70/1000 | Loss: 0.00001813
Iteration 71/1000 | Loss: 0.00001813
Iteration 72/1000 | Loss: 0.00001813
Iteration 73/1000 | Loss: 0.00001813
Iteration 74/1000 | Loss: 0.00001812
Iteration 75/1000 | Loss: 0.00001812
Iteration 76/1000 | Loss: 0.00001812
Iteration 77/1000 | Loss: 0.00001812
Iteration 78/1000 | Loss: 0.00001812
Iteration 79/1000 | Loss: 0.00001812
Iteration 80/1000 | Loss: 0.00001812
Iteration 81/1000 | Loss: 0.00001811
Iteration 82/1000 | Loss: 0.00001811
Iteration 83/1000 | Loss: 0.00001811
Iteration 84/1000 | Loss: 0.00001811
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00001811
Iteration 88/1000 | Loss: 0.00001811
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001811
Iteration 91/1000 | Loss: 0.00001811
Iteration 92/1000 | Loss: 0.00001811
Iteration 93/1000 | Loss: 0.00001811
Iteration 94/1000 | Loss: 0.00001811
Iteration 95/1000 | Loss: 0.00001811
Iteration 96/1000 | Loss: 0.00001811
Iteration 97/1000 | Loss: 0.00001811
Iteration 98/1000 | Loss: 0.00001811
Iteration 99/1000 | Loss: 0.00001811
Iteration 100/1000 | Loss: 0.00001811
Iteration 101/1000 | Loss: 0.00001810
Iteration 102/1000 | Loss: 0.00001810
Iteration 103/1000 | Loss: 0.00001810
Iteration 104/1000 | Loss: 0.00001810
Iteration 105/1000 | Loss: 0.00001810
Iteration 106/1000 | Loss: 0.00001810
Iteration 107/1000 | Loss: 0.00001810
Iteration 108/1000 | Loss: 0.00001810
Iteration 109/1000 | Loss: 0.00001810
Iteration 110/1000 | Loss: 0.00001810
Iteration 111/1000 | Loss: 0.00001810
Iteration 112/1000 | Loss: 0.00001810
Iteration 113/1000 | Loss: 0.00001810
Iteration 114/1000 | Loss: 0.00001810
Iteration 115/1000 | Loss: 0.00001810
Iteration 116/1000 | Loss: 0.00001810
Iteration 117/1000 | Loss: 0.00001810
Iteration 118/1000 | Loss: 0.00001810
Iteration 119/1000 | Loss: 0.00001810
Iteration 120/1000 | Loss: 0.00001810
Iteration 121/1000 | Loss: 0.00001810
Iteration 122/1000 | Loss: 0.00001810
Iteration 123/1000 | Loss: 0.00001810
Iteration 124/1000 | Loss: 0.00001810
Iteration 125/1000 | Loss: 0.00001810
Iteration 126/1000 | Loss: 0.00001810
Iteration 127/1000 | Loss: 0.00001810
Iteration 128/1000 | Loss: 0.00001810
Iteration 129/1000 | Loss: 0.00001810
Iteration 130/1000 | Loss: 0.00001810
Iteration 131/1000 | Loss: 0.00001810
Iteration 132/1000 | Loss: 0.00001810
Iteration 133/1000 | Loss: 0.00001810
Iteration 134/1000 | Loss: 0.00001810
Iteration 135/1000 | Loss: 0.00001810
Iteration 136/1000 | Loss: 0.00001810
Iteration 137/1000 | Loss: 0.00001810
Iteration 138/1000 | Loss: 0.00001810
Iteration 139/1000 | Loss: 0.00001810
Iteration 140/1000 | Loss: 0.00001810
Iteration 141/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.809865716495551e-05, 1.809865716495551e-05, 1.809865716495551e-05, 1.809865716495551e-05, 1.809865716495551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.809865716495551e-05

Optimization complete. Final v2v error: 3.6129953861236572 mm

Highest mean error: 4.312773704528809 mm for frame 189

Lowest mean error: 3.2082679271698 mm for frame 0

Saving results

Total time: 105.20795941352844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688374
Iteration 2/25 | Loss: 0.00096707
Iteration 3/25 | Loss: 0.00081378
Iteration 4/25 | Loss: 0.00077873
Iteration 5/25 | Loss: 0.00076399
Iteration 6/25 | Loss: 0.00076133
Iteration 7/25 | Loss: 0.00076060
Iteration 8/25 | Loss: 0.00076060
Iteration 9/25 | Loss: 0.00076060
Iteration 10/25 | Loss: 0.00076060
Iteration 11/25 | Loss: 0.00076060
Iteration 12/25 | Loss: 0.00076060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007605991559103131, 0.0007605991559103131, 0.0007605991559103131, 0.0007605991559103131, 0.0007605991559103131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007605991559103131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07100725
Iteration 2/25 | Loss: 0.00047738
Iteration 3/25 | Loss: 0.00047738
Iteration 4/25 | Loss: 0.00047737
Iteration 5/25 | Loss: 0.00047737
Iteration 6/25 | Loss: 0.00047737
Iteration 7/25 | Loss: 0.00047737
Iteration 8/25 | Loss: 0.00047737
Iteration 9/25 | Loss: 0.00047737
Iteration 10/25 | Loss: 0.00047737
Iteration 11/25 | Loss: 0.00047737
Iteration 12/25 | Loss: 0.00047737
Iteration 13/25 | Loss: 0.00047737
Iteration 14/25 | Loss: 0.00047737
Iteration 15/25 | Loss: 0.00047737
Iteration 16/25 | Loss: 0.00047737
Iteration 17/25 | Loss: 0.00047737
Iteration 18/25 | Loss: 0.00047737
Iteration 19/25 | Loss: 0.00047737
Iteration 20/25 | Loss: 0.00047737
Iteration 21/25 | Loss: 0.00047737
Iteration 22/25 | Loss: 0.00047737
Iteration 23/25 | Loss: 0.00047737
Iteration 24/25 | Loss: 0.00047737
Iteration 25/25 | Loss: 0.00047737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047737
Iteration 2/1000 | Loss: 0.00002909
Iteration 3/1000 | Loss: 0.00001896
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001533
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001486
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001478
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001477
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001476
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001472
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001471
Iteration 43/1000 | Loss: 0.00001471
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001467
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001463
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001457
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001456
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001455
Iteration 92/1000 | Loss: 0.00001455
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001451
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001448
Iteration 111/1000 | Loss: 0.00001448
Iteration 112/1000 | Loss: 0.00001448
Iteration 113/1000 | Loss: 0.00001448
Iteration 114/1000 | Loss: 0.00001448
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001447
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001446
Iteration 127/1000 | Loss: 0.00001446
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001446
Iteration 130/1000 | Loss: 0.00001446
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001445
Iteration 138/1000 | Loss: 0.00001445
Iteration 139/1000 | Loss: 0.00001445
Iteration 140/1000 | Loss: 0.00001445
Iteration 141/1000 | Loss: 0.00001445
Iteration 142/1000 | Loss: 0.00001445
Iteration 143/1000 | Loss: 0.00001445
Iteration 144/1000 | Loss: 0.00001445
Iteration 145/1000 | Loss: 0.00001445
Iteration 146/1000 | Loss: 0.00001445
Iteration 147/1000 | Loss: 0.00001445
Iteration 148/1000 | Loss: 0.00001445
Iteration 149/1000 | Loss: 0.00001445
Iteration 150/1000 | Loss: 0.00001444
Iteration 151/1000 | Loss: 0.00001444
Iteration 152/1000 | Loss: 0.00001444
Iteration 153/1000 | Loss: 0.00001444
Iteration 154/1000 | Loss: 0.00001444
Iteration 155/1000 | Loss: 0.00001444
Iteration 156/1000 | Loss: 0.00001444
Iteration 157/1000 | Loss: 0.00001444
Iteration 158/1000 | Loss: 0.00001444
Iteration 159/1000 | Loss: 0.00001444
Iteration 160/1000 | Loss: 0.00001444
Iteration 161/1000 | Loss: 0.00001444
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001444
Iteration 164/1000 | Loss: 0.00001444
Iteration 165/1000 | Loss: 0.00001443
Iteration 166/1000 | Loss: 0.00001443
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001443
Iteration 169/1000 | Loss: 0.00001443
Iteration 170/1000 | Loss: 0.00001443
Iteration 171/1000 | Loss: 0.00001443
Iteration 172/1000 | Loss: 0.00001443
Iteration 173/1000 | Loss: 0.00001443
Iteration 174/1000 | Loss: 0.00001443
Iteration 175/1000 | Loss: 0.00001443
Iteration 176/1000 | Loss: 0.00001443
Iteration 177/1000 | Loss: 0.00001443
Iteration 178/1000 | Loss: 0.00001443
Iteration 179/1000 | Loss: 0.00001443
Iteration 180/1000 | Loss: 0.00001442
Iteration 181/1000 | Loss: 0.00001442
Iteration 182/1000 | Loss: 0.00001442
Iteration 183/1000 | Loss: 0.00001442
Iteration 184/1000 | Loss: 0.00001442
Iteration 185/1000 | Loss: 0.00001442
Iteration 186/1000 | Loss: 0.00001442
Iteration 187/1000 | Loss: 0.00001442
Iteration 188/1000 | Loss: 0.00001442
Iteration 189/1000 | Loss: 0.00001442
Iteration 190/1000 | Loss: 0.00001442
Iteration 191/1000 | Loss: 0.00001442
Iteration 192/1000 | Loss: 0.00001442
Iteration 193/1000 | Loss: 0.00001442
Iteration 194/1000 | Loss: 0.00001442
Iteration 195/1000 | Loss: 0.00001442
Iteration 196/1000 | Loss: 0.00001442
Iteration 197/1000 | Loss: 0.00001442
Iteration 198/1000 | Loss: 0.00001442
Iteration 199/1000 | Loss: 0.00001442
Iteration 200/1000 | Loss: 0.00001442
Iteration 201/1000 | Loss: 0.00001441
Iteration 202/1000 | Loss: 0.00001441
Iteration 203/1000 | Loss: 0.00001441
Iteration 204/1000 | Loss: 0.00001441
Iteration 205/1000 | Loss: 0.00001441
Iteration 206/1000 | Loss: 0.00001441
Iteration 207/1000 | Loss: 0.00001441
Iteration 208/1000 | Loss: 0.00001441
Iteration 209/1000 | Loss: 0.00001441
Iteration 210/1000 | Loss: 0.00001441
Iteration 211/1000 | Loss: 0.00001441
Iteration 212/1000 | Loss: 0.00001441
Iteration 213/1000 | Loss: 0.00001441
Iteration 214/1000 | Loss: 0.00001441
Iteration 215/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.441284257452935e-05, 1.441284257452935e-05, 1.441284257452935e-05, 1.441284257452935e-05, 1.441284257452935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.441284257452935e-05

Optimization complete. Final v2v error: 3.2257330417633057 mm

Highest mean error: 3.4521026611328125 mm for frame 45

Lowest mean error: 3.009701728820801 mm for frame 22

Saving results

Total time: 38.779136657714844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838338
Iteration 2/25 | Loss: 0.00129768
Iteration 3/25 | Loss: 0.00087438
Iteration 4/25 | Loss: 0.00083015
Iteration 5/25 | Loss: 0.00082370
Iteration 6/25 | Loss: 0.00082175
Iteration 7/25 | Loss: 0.00082162
Iteration 8/25 | Loss: 0.00082162
Iteration 9/25 | Loss: 0.00082162
Iteration 10/25 | Loss: 0.00082162
Iteration 11/25 | Loss: 0.00082162
Iteration 12/25 | Loss: 0.00082162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000821622961666435, 0.000821622961666435, 0.000821622961666435, 0.000821622961666435, 0.000821622961666435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000821622961666435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35269964
Iteration 2/25 | Loss: 0.00053970
Iteration 3/25 | Loss: 0.00053966
Iteration 4/25 | Loss: 0.00053966
Iteration 5/25 | Loss: 0.00053966
Iteration 6/25 | Loss: 0.00053966
Iteration 7/25 | Loss: 0.00053966
Iteration 8/25 | Loss: 0.00053966
Iteration 9/25 | Loss: 0.00053966
Iteration 10/25 | Loss: 0.00053966
Iteration 11/25 | Loss: 0.00053966
Iteration 12/25 | Loss: 0.00053966
Iteration 13/25 | Loss: 0.00053966
Iteration 14/25 | Loss: 0.00053966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005396608612500131, 0.0005396608612500131, 0.0005396608612500131, 0.0005396608612500131, 0.0005396608612500131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005396608612500131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053966
Iteration 2/1000 | Loss: 0.00002614
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001578
Iteration 11/1000 | Loss: 0.00001574
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001569
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001564
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001550
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001549
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001547
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001544
Iteration 73/1000 | Loss: 0.00001544
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001541
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001540
Iteration 96/1000 | Loss: 0.00001540
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001539
Iteration 106/1000 | Loss: 0.00001539
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.53943728946615e-05, 1.53943728946615e-05, 1.53943728946615e-05, 1.53943728946615e-05, 1.53943728946615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53943728946615e-05

Optimization complete. Final v2v error: 3.3306827545166016 mm

Highest mean error: 3.5455057621002197 mm for frame 158

Lowest mean error: 3.1003971099853516 mm for frame 71

Saving results

Total time: 61.06194233894348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737722
Iteration 2/25 | Loss: 0.00103730
Iteration 3/25 | Loss: 0.00085622
Iteration 4/25 | Loss: 0.00083393
Iteration 5/25 | Loss: 0.00082611
Iteration 6/25 | Loss: 0.00082369
Iteration 7/25 | Loss: 0.00082339
Iteration 8/25 | Loss: 0.00082339
Iteration 9/25 | Loss: 0.00082339
Iteration 10/25 | Loss: 0.00082339
Iteration 11/25 | Loss: 0.00082339
Iteration 12/25 | Loss: 0.00082339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008233935805037618, 0.0008233935805037618, 0.0008233935805037618, 0.0008233935805037618, 0.0008233935805037618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008233935805037618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50522625
Iteration 2/25 | Loss: 0.00053634
Iteration 3/25 | Loss: 0.00053634
Iteration 4/25 | Loss: 0.00053634
Iteration 5/25 | Loss: 0.00053634
Iteration 6/25 | Loss: 0.00053633
Iteration 7/25 | Loss: 0.00053633
Iteration 8/25 | Loss: 0.00053633
Iteration 9/25 | Loss: 0.00053633
Iteration 10/25 | Loss: 0.00053633
Iteration 11/25 | Loss: 0.00053633
Iteration 12/25 | Loss: 0.00053633
Iteration 13/25 | Loss: 0.00053633
Iteration 14/25 | Loss: 0.00053633
Iteration 15/25 | Loss: 0.00053633
Iteration 16/25 | Loss: 0.00053633
Iteration 17/25 | Loss: 0.00053633
Iteration 18/25 | Loss: 0.00053633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005363340023905039, 0.0005363340023905039, 0.0005363340023905039, 0.0005363340023905039, 0.0005363340023905039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005363340023905039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053633
Iteration 2/1000 | Loss: 0.00003589
Iteration 3/1000 | Loss: 0.00002688
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002265
Iteration 6/1000 | Loss: 0.00002160
Iteration 7/1000 | Loss: 0.00002109
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002031
Iteration 10/1000 | Loss: 0.00002003
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001962
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001953
Iteration 17/1000 | Loss: 0.00001953
Iteration 18/1000 | Loss: 0.00001952
Iteration 19/1000 | Loss: 0.00001952
Iteration 20/1000 | Loss: 0.00001950
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001950
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001948
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001946
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001945
Iteration 34/1000 | Loss: 0.00001944
Iteration 35/1000 | Loss: 0.00001944
Iteration 36/1000 | Loss: 0.00001944
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001943
Iteration 39/1000 | Loss: 0.00001943
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001943
Iteration 42/1000 | Loss: 0.00001943
Iteration 43/1000 | Loss: 0.00001942
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00001942
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001941
Iteration 49/1000 | Loss: 0.00001941
Iteration 50/1000 | Loss: 0.00001941
Iteration 51/1000 | Loss: 0.00001941
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001940
Iteration 54/1000 | Loss: 0.00001940
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001939
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001938
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001935
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001934
Iteration 97/1000 | Loss: 0.00001934
Iteration 98/1000 | Loss: 0.00001934
Iteration 99/1000 | Loss: 0.00001934
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001933
Iteration 102/1000 | Loss: 0.00001933
Iteration 103/1000 | Loss: 0.00001933
Iteration 104/1000 | Loss: 0.00001933
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001932
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001932
Iteration 118/1000 | Loss: 0.00001932
Iteration 119/1000 | Loss: 0.00001932
Iteration 120/1000 | Loss: 0.00001932
Iteration 121/1000 | Loss: 0.00001932
Iteration 122/1000 | Loss: 0.00001932
Iteration 123/1000 | Loss: 0.00001932
Iteration 124/1000 | Loss: 0.00001932
Iteration 125/1000 | Loss: 0.00001932
Iteration 126/1000 | Loss: 0.00001932
Iteration 127/1000 | Loss: 0.00001932
Iteration 128/1000 | Loss: 0.00001932
Iteration 129/1000 | Loss: 0.00001932
Iteration 130/1000 | Loss: 0.00001932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.9318775230203755e-05, 1.9318775230203755e-05, 1.9318775230203755e-05, 1.9318775230203755e-05, 1.9318775230203755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9318775230203755e-05

Optimization complete. Final v2v error: 3.6961796283721924 mm

Highest mean error: 5.351301193237305 mm for frame 239

Lowest mean error: 3.1200079917907715 mm for frame 18

Saving results

Total time: 45.54821228981018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862001
Iteration 2/25 | Loss: 0.00103697
Iteration 3/25 | Loss: 0.00084788
Iteration 4/25 | Loss: 0.00081115
Iteration 5/25 | Loss: 0.00080045
Iteration 6/25 | Loss: 0.00079677
Iteration 7/25 | Loss: 0.00079543
Iteration 8/25 | Loss: 0.00079471
Iteration 9/25 | Loss: 0.00079708
Iteration 10/25 | Loss: 0.00079997
Iteration 11/25 | Loss: 0.00079816
Iteration 12/25 | Loss: 0.00079340
Iteration 13/25 | Loss: 0.00079457
Iteration 14/25 | Loss: 0.00079270
Iteration 15/25 | Loss: 0.00079300
Iteration 16/25 | Loss: 0.00079234
Iteration 17/25 | Loss: 0.00079316
Iteration 18/25 | Loss: 0.00079311
Iteration 19/25 | Loss: 0.00079389
Iteration 20/25 | Loss: 0.00079306
Iteration 21/25 | Loss: 0.00079238
Iteration 22/25 | Loss: 0.00079283
Iteration 23/25 | Loss: 0.00079296
Iteration 24/25 | Loss: 0.00079338
Iteration 25/25 | Loss: 0.00079331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50520182
Iteration 2/25 | Loss: 0.00121877
Iteration 3/25 | Loss: 0.00121876
Iteration 4/25 | Loss: 0.00121876
Iteration 5/25 | Loss: 0.00121876
Iteration 6/25 | Loss: 0.00121876
Iteration 7/25 | Loss: 0.00121876
Iteration 8/25 | Loss: 0.00121876
Iteration 9/25 | Loss: 0.00121876
Iteration 10/25 | Loss: 0.00121876
Iteration 11/25 | Loss: 0.00121876
Iteration 12/25 | Loss: 0.00121876
Iteration 13/25 | Loss: 0.00121876
Iteration 14/25 | Loss: 0.00121876
Iteration 15/25 | Loss: 0.00121876
Iteration 16/25 | Loss: 0.00121876
Iteration 17/25 | Loss: 0.00121876
Iteration 18/25 | Loss: 0.00121876
Iteration 19/25 | Loss: 0.00121876
Iteration 20/25 | Loss: 0.00121876
Iteration 21/25 | Loss: 0.00121876
Iteration 22/25 | Loss: 0.00121876
Iteration 23/25 | Loss: 0.00121876
Iteration 24/25 | Loss: 0.00121876
Iteration 25/25 | Loss: 0.00121876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121876
Iteration 2/1000 | Loss: 0.00010730
Iteration 3/1000 | Loss: 0.00007599
Iteration 4/1000 | Loss: 0.00006549
Iteration 5/1000 | Loss: 0.00006011
Iteration 6/1000 | Loss: 0.00005707
Iteration 7/1000 | Loss: 0.00005532
Iteration 8/1000 | Loss: 0.00005339
Iteration 9/1000 | Loss: 0.00005191
Iteration 10/1000 | Loss: 0.00005082
Iteration 11/1000 | Loss: 0.00005018
Iteration 12/1000 | Loss: 0.00004968
Iteration 13/1000 | Loss: 0.00004911
Iteration 14/1000 | Loss: 0.00004847
Iteration 15/1000 | Loss: 0.00004810
Iteration 16/1000 | Loss: 0.00004775
Iteration 17/1000 | Loss: 0.00004742
Iteration 18/1000 | Loss: 0.00004706
Iteration 19/1000 | Loss: 0.00004674
Iteration 20/1000 | Loss: 0.00004640
Iteration 21/1000 | Loss: 0.00004630
Iteration 22/1000 | Loss: 0.00103235
Iteration 23/1000 | Loss: 0.00121761
Iteration 24/1000 | Loss: 0.00118406
Iteration 25/1000 | Loss: 0.00042236
Iteration 26/1000 | Loss: 0.00008103
Iteration 27/1000 | Loss: 0.00006938
Iteration 28/1000 | Loss: 0.00005382
Iteration 29/1000 | Loss: 0.00004933
Iteration 30/1000 | Loss: 0.00004728
Iteration 31/1000 | Loss: 0.00004561
Iteration 32/1000 | Loss: 0.00060642
Iteration 33/1000 | Loss: 0.00014941
Iteration 34/1000 | Loss: 0.00041193
Iteration 35/1000 | Loss: 0.00007025
Iteration 36/1000 | Loss: 0.00005815
Iteration 37/1000 | Loss: 0.00005238
Iteration 38/1000 | Loss: 0.00061184
Iteration 39/1000 | Loss: 0.00104046
Iteration 40/1000 | Loss: 0.00014173
Iteration 41/1000 | Loss: 0.00006757
Iteration 42/1000 | Loss: 0.00005754
Iteration 43/1000 | Loss: 0.00005176
Iteration 44/1000 | Loss: 0.00004888
Iteration 45/1000 | Loss: 0.00004587
Iteration 46/1000 | Loss: 0.00004389
Iteration 47/1000 | Loss: 0.00004296
Iteration 48/1000 | Loss: 0.00059559
Iteration 49/1000 | Loss: 0.00006073
Iteration 50/1000 | Loss: 0.00004686
Iteration 51/1000 | Loss: 0.00004503
Iteration 52/1000 | Loss: 0.00004323
Iteration 53/1000 | Loss: 0.00048435
Iteration 54/1000 | Loss: 0.00024501
Iteration 55/1000 | Loss: 0.00004183
Iteration 56/1000 | Loss: 0.00004090
Iteration 57/1000 | Loss: 0.00004048
Iteration 58/1000 | Loss: 0.00004016
Iteration 59/1000 | Loss: 0.00003996
Iteration 60/1000 | Loss: 0.00048754
Iteration 61/1000 | Loss: 0.00017013
Iteration 62/1000 | Loss: 0.00004391
Iteration 63/1000 | Loss: 0.00004074
Iteration 64/1000 | Loss: 0.00003996
Iteration 65/1000 | Loss: 0.00003975
Iteration 66/1000 | Loss: 0.00003967
Iteration 67/1000 | Loss: 0.00003963
Iteration 68/1000 | Loss: 0.00037502
Iteration 69/1000 | Loss: 0.00011954
Iteration 70/1000 | Loss: 0.00005107
Iteration 71/1000 | Loss: 0.00004600
Iteration 72/1000 | Loss: 0.00008592
Iteration 73/1000 | Loss: 0.00004240
Iteration 74/1000 | Loss: 0.00004089
Iteration 75/1000 | Loss: 0.00003980
Iteration 76/1000 | Loss: 0.00016712
Iteration 77/1000 | Loss: 0.00032709
Iteration 78/1000 | Loss: 0.00057781
Iteration 79/1000 | Loss: 0.00010549
Iteration 80/1000 | Loss: 0.00004908
Iteration 81/1000 | Loss: 0.00005034
Iteration 82/1000 | Loss: 0.00004323
Iteration 83/1000 | Loss: 0.00028700
Iteration 84/1000 | Loss: 0.00021159
Iteration 85/1000 | Loss: 0.00018502
Iteration 86/1000 | Loss: 0.00007763
Iteration 87/1000 | Loss: 0.00004831
Iteration 88/1000 | Loss: 0.00004142
Iteration 89/1000 | Loss: 0.00003970
Iteration 90/1000 | Loss: 0.00003916
Iteration 91/1000 | Loss: 0.00003869
Iteration 92/1000 | Loss: 0.00003846
Iteration 93/1000 | Loss: 0.00003833
Iteration 94/1000 | Loss: 0.00003830
Iteration 95/1000 | Loss: 0.00003829
Iteration 96/1000 | Loss: 0.00003829
Iteration 97/1000 | Loss: 0.00003825
Iteration 98/1000 | Loss: 0.00003816
Iteration 99/1000 | Loss: 0.00004270
Iteration 100/1000 | Loss: 0.00004101
Iteration 101/1000 | Loss: 0.00003931
Iteration 102/1000 | Loss: 0.00003830
Iteration 103/1000 | Loss: 0.00003799
Iteration 104/1000 | Loss: 0.00003794
Iteration 105/1000 | Loss: 0.00003784
Iteration 106/1000 | Loss: 0.00003774
Iteration 107/1000 | Loss: 0.00003774
Iteration 108/1000 | Loss: 0.00003773
Iteration 109/1000 | Loss: 0.00003772
Iteration 110/1000 | Loss: 0.00003767
Iteration 111/1000 | Loss: 0.00003763
Iteration 112/1000 | Loss: 0.00003762
Iteration 113/1000 | Loss: 0.00003761
Iteration 114/1000 | Loss: 0.00003760
Iteration 115/1000 | Loss: 0.00003759
Iteration 116/1000 | Loss: 0.00003758
Iteration 117/1000 | Loss: 0.00003757
Iteration 118/1000 | Loss: 0.00003755
Iteration 119/1000 | Loss: 0.00003748
Iteration 120/1000 | Loss: 0.00003746
Iteration 121/1000 | Loss: 0.00003745
Iteration 122/1000 | Loss: 0.00003732
Iteration 123/1000 | Loss: 0.00003731
Iteration 124/1000 | Loss: 0.00003729
Iteration 125/1000 | Loss: 0.00003728
Iteration 126/1000 | Loss: 0.00003728
Iteration 127/1000 | Loss: 0.00003728
Iteration 128/1000 | Loss: 0.00003728
Iteration 129/1000 | Loss: 0.00003727
Iteration 130/1000 | Loss: 0.00003727
Iteration 131/1000 | Loss: 0.00003727
Iteration 132/1000 | Loss: 0.00003725
Iteration 133/1000 | Loss: 0.00003723
Iteration 134/1000 | Loss: 0.00003722
Iteration 135/1000 | Loss: 0.00003722
Iteration 136/1000 | Loss: 0.00003722
Iteration 137/1000 | Loss: 0.00003722
Iteration 138/1000 | Loss: 0.00003722
Iteration 139/1000 | Loss: 0.00003722
Iteration 140/1000 | Loss: 0.00003722
Iteration 141/1000 | Loss: 0.00003722
Iteration 142/1000 | Loss: 0.00003722
Iteration 143/1000 | Loss: 0.00003722
Iteration 144/1000 | Loss: 0.00003722
Iteration 145/1000 | Loss: 0.00003721
Iteration 146/1000 | Loss: 0.00003721
Iteration 147/1000 | Loss: 0.00003721
Iteration 148/1000 | Loss: 0.00003720
Iteration 149/1000 | Loss: 0.00003720
Iteration 150/1000 | Loss: 0.00003720
Iteration 151/1000 | Loss: 0.00003719
Iteration 152/1000 | Loss: 0.00003719
Iteration 153/1000 | Loss: 0.00003719
Iteration 154/1000 | Loss: 0.00003719
Iteration 155/1000 | Loss: 0.00003718
Iteration 156/1000 | Loss: 0.00003718
Iteration 157/1000 | Loss: 0.00003718
Iteration 158/1000 | Loss: 0.00003718
Iteration 159/1000 | Loss: 0.00003718
Iteration 160/1000 | Loss: 0.00003717
Iteration 161/1000 | Loss: 0.00003717
Iteration 162/1000 | Loss: 0.00003717
Iteration 163/1000 | Loss: 0.00003717
Iteration 164/1000 | Loss: 0.00003717
Iteration 165/1000 | Loss: 0.00003717
Iteration 166/1000 | Loss: 0.00003717
Iteration 167/1000 | Loss: 0.00003717
Iteration 168/1000 | Loss: 0.00003717
Iteration 169/1000 | Loss: 0.00003717
Iteration 170/1000 | Loss: 0.00003717
Iteration 171/1000 | Loss: 0.00003717
Iteration 172/1000 | Loss: 0.00003716
Iteration 173/1000 | Loss: 0.00003716
Iteration 174/1000 | Loss: 0.00003716
Iteration 175/1000 | Loss: 0.00003716
Iteration 176/1000 | Loss: 0.00003716
Iteration 177/1000 | Loss: 0.00003715
Iteration 178/1000 | Loss: 0.00003715
Iteration 179/1000 | Loss: 0.00003715
Iteration 180/1000 | Loss: 0.00003714
Iteration 181/1000 | Loss: 0.00003714
Iteration 182/1000 | Loss: 0.00003714
Iteration 183/1000 | Loss: 0.00003714
Iteration 184/1000 | Loss: 0.00003714
Iteration 185/1000 | Loss: 0.00003713
Iteration 186/1000 | Loss: 0.00003713
Iteration 187/1000 | Loss: 0.00003713
Iteration 188/1000 | Loss: 0.00003713
Iteration 189/1000 | Loss: 0.00003712
Iteration 190/1000 | Loss: 0.00003712
Iteration 191/1000 | Loss: 0.00003712
Iteration 192/1000 | Loss: 0.00003712
Iteration 193/1000 | Loss: 0.00003712
Iteration 194/1000 | Loss: 0.00003712
Iteration 195/1000 | Loss: 0.00003712
Iteration 196/1000 | Loss: 0.00003711
Iteration 197/1000 | Loss: 0.00003711
Iteration 198/1000 | Loss: 0.00003711
Iteration 199/1000 | Loss: 0.00003711
Iteration 200/1000 | Loss: 0.00003711
Iteration 201/1000 | Loss: 0.00003711
Iteration 202/1000 | Loss: 0.00003711
Iteration 203/1000 | Loss: 0.00003711
Iteration 204/1000 | Loss: 0.00003711
Iteration 205/1000 | Loss: 0.00003711
Iteration 206/1000 | Loss: 0.00003710
Iteration 207/1000 | Loss: 0.00003710
Iteration 208/1000 | Loss: 0.00003710
Iteration 209/1000 | Loss: 0.00003710
Iteration 210/1000 | Loss: 0.00003710
Iteration 211/1000 | Loss: 0.00003710
Iteration 212/1000 | Loss: 0.00003710
Iteration 213/1000 | Loss: 0.00003709
Iteration 214/1000 | Loss: 0.00003709
Iteration 215/1000 | Loss: 0.00003709
Iteration 216/1000 | Loss: 0.00003709
Iteration 217/1000 | Loss: 0.00003709
Iteration 218/1000 | Loss: 0.00003709
Iteration 219/1000 | Loss: 0.00003708
Iteration 220/1000 | Loss: 0.00003708
Iteration 221/1000 | Loss: 0.00003708
Iteration 222/1000 | Loss: 0.00003708
Iteration 223/1000 | Loss: 0.00003708
Iteration 224/1000 | Loss: 0.00003708
Iteration 225/1000 | Loss: 0.00003708
Iteration 226/1000 | Loss: 0.00003708
Iteration 227/1000 | Loss: 0.00003708
Iteration 228/1000 | Loss: 0.00003707
Iteration 229/1000 | Loss: 0.00003707
Iteration 230/1000 | Loss: 0.00003707
Iteration 231/1000 | Loss: 0.00003707
Iteration 232/1000 | Loss: 0.00003707
Iteration 233/1000 | Loss: 0.00003707
Iteration 234/1000 | Loss: 0.00003707
Iteration 235/1000 | Loss: 0.00003707
Iteration 236/1000 | Loss: 0.00003707
Iteration 237/1000 | Loss: 0.00003707
Iteration 238/1000 | Loss: 0.00003707
Iteration 239/1000 | Loss: 0.00003707
Iteration 240/1000 | Loss: 0.00003707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [3.706512143253349e-05, 3.706512143253349e-05, 3.706512143253349e-05, 3.706512143253349e-05, 3.706512143253349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.706512143253349e-05

Optimization complete. Final v2v error: 3.1183836460113525 mm

Highest mean error: 11.067481994628906 mm for frame 125

Lowest mean error: 2.4228153228759766 mm for frame 81

Saving results

Total time: 230.82145619392395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953890
Iteration 2/25 | Loss: 0.00241954
Iteration 3/25 | Loss: 0.00500093
Iteration 4/25 | Loss: 0.00168879
Iteration 5/25 | Loss: 0.00131695
Iteration 6/25 | Loss: 0.00101412
Iteration 7/25 | Loss: 0.00095346
Iteration 8/25 | Loss: 0.00092043
Iteration 9/25 | Loss: 0.00089573
Iteration 10/25 | Loss: 0.00086156
Iteration 11/25 | Loss: 0.00084751
Iteration 12/25 | Loss: 0.00084091
Iteration 13/25 | Loss: 0.00084100
Iteration 14/25 | Loss: 0.00083689
Iteration 15/25 | Loss: 0.00085726
Iteration 16/25 | Loss: 0.00085807
Iteration 17/25 | Loss: 0.00085451
Iteration 18/25 | Loss: 0.00087667
Iteration 19/25 | Loss: 0.00087040
Iteration 20/25 | Loss: 0.00085738
Iteration 21/25 | Loss: 0.00085275
Iteration 22/25 | Loss: 0.00083661
Iteration 23/25 | Loss: 0.00083249
Iteration 24/25 | Loss: 0.00083105
Iteration 25/25 | Loss: 0.00087720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59855556
Iteration 2/25 | Loss: 0.00458660
Iteration 3/25 | Loss: 0.00458657
Iteration 4/25 | Loss: 0.00458657
Iteration 5/25 | Loss: 0.00458657
Iteration 6/25 | Loss: 0.00458657
Iteration 7/25 | Loss: 0.00458657
Iteration 8/25 | Loss: 0.00458657
Iteration 9/25 | Loss: 0.00458657
Iteration 10/25 | Loss: 0.00458657
Iteration 11/25 | Loss: 0.00458657
Iteration 12/25 | Loss: 0.00458657
Iteration 13/25 | Loss: 0.00458657
Iteration 14/25 | Loss: 0.00458657
Iteration 15/25 | Loss: 0.00458657
Iteration 16/25 | Loss: 0.00458657
Iteration 17/25 | Loss: 0.00458657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004586570896208286, 0.004586570896208286, 0.004586570896208286, 0.004586570896208286, 0.004586570896208286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004586570896208286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00458657
Iteration 2/1000 | Loss: 0.00405834
Iteration 3/1000 | Loss: 0.00780049
Iteration 4/1000 | Loss: 0.00581915
Iteration 5/1000 | Loss: 0.00472681
Iteration 6/1000 | Loss: 0.00092735
Iteration 7/1000 | Loss: 0.00062639
Iteration 8/1000 | Loss: 0.00069438
Iteration 9/1000 | Loss: 0.00082906
Iteration 10/1000 | Loss: 0.00094430
Iteration 11/1000 | Loss: 0.00071651
Iteration 12/1000 | Loss: 0.00080272
Iteration 13/1000 | Loss: 0.00007630
Iteration 14/1000 | Loss: 0.00008398
Iteration 15/1000 | Loss: 0.00155745
Iteration 16/1000 | Loss: 0.00006570
Iteration 17/1000 | Loss: 0.00004739
Iteration 18/1000 | Loss: 0.00015541
Iteration 19/1000 | Loss: 0.00004375
Iteration 20/1000 | Loss: 0.00038130
Iteration 21/1000 | Loss: 0.00025829
Iteration 22/1000 | Loss: 0.00054468
Iteration 23/1000 | Loss: 0.00031564
Iteration 24/1000 | Loss: 0.00004347
Iteration 25/1000 | Loss: 0.00003594
Iteration 26/1000 | Loss: 0.00003547
Iteration 27/1000 | Loss: 0.00029937
Iteration 28/1000 | Loss: 0.00031981
Iteration 29/1000 | Loss: 0.00008458
Iteration 30/1000 | Loss: 0.00003871
Iteration 31/1000 | Loss: 0.00016257
Iteration 32/1000 | Loss: 0.00036621
Iteration 33/1000 | Loss: 0.00026051
Iteration 34/1000 | Loss: 0.00003877
Iteration 35/1000 | Loss: 0.00003012
Iteration 36/1000 | Loss: 0.00013885
Iteration 37/1000 | Loss: 0.00002662
Iteration 38/1000 | Loss: 0.00013064
Iteration 39/1000 | Loss: 0.00010662
Iteration 40/1000 | Loss: 0.00022865
Iteration 41/1000 | Loss: 0.00003751
Iteration 42/1000 | Loss: 0.00015191
Iteration 43/1000 | Loss: 0.00010729
Iteration 44/1000 | Loss: 0.00002944
Iteration 45/1000 | Loss: 0.00002594
Iteration 46/1000 | Loss: 0.00005623
Iteration 47/1000 | Loss: 0.00002389
Iteration 48/1000 | Loss: 0.00002267
Iteration 49/1000 | Loss: 0.00002215
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002145
Iteration 52/1000 | Loss: 0.00015465
Iteration 53/1000 | Loss: 0.00017363
Iteration 54/1000 | Loss: 0.00007146
Iteration 55/1000 | Loss: 0.00012118
Iteration 56/1000 | Loss: 0.00015281
Iteration 57/1000 | Loss: 0.00016451
Iteration 58/1000 | Loss: 0.00002838
Iteration 59/1000 | Loss: 0.00002375
Iteration 60/1000 | Loss: 0.00014522
Iteration 61/1000 | Loss: 0.00016643
Iteration 62/1000 | Loss: 0.00013381
Iteration 63/1000 | Loss: 0.00002272
Iteration 64/1000 | Loss: 0.00020291
Iteration 65/1000 | Loss: 0.00003019
Iteration 66/1000 | Loss: 0.00023407
Iteration 67/1000 | Loss: 0.00003480
Iteration 68/1000 | Loss: 0.00014340
Iteration 69/1000 | Loss: 0.00002230
Iteration 70/1000 | Loss: 0.00002118
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00001997
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001942
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001935
Iteration 82/1000 | Loss: 0.00001935
Iteration 83/1000 | Loss: 0.00001935
Iteration 84/1000 | Loss: 0.00001935
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001934
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001933
Iteration 96/1000 | Loss: 0.00001933
Iteration 97/1000 | Loss: 0.00001933
Iteration 98/1000 | Loss: 0.00001932
Iteration 99/1000 | Loss: 0.00001932
Iteration 100/1000 | Loss: 0.00001932
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001931
Iteration 104/1000 | Loss: 0.00001931
Iteration 105/1000 | Loss: 0.00001931
Iteration 106/1000 | Loss: 0.00001931
Iteration 107/1000 | Loss: 0.00001930
Iteration 108/1000 | Loss: 0.00001930
Iteration 109/1000 | Loss: 0.00001930
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001930
Iteration 112/1000 | Loss: 0.00001929
Iteration 113/1000 | Loss: 0.00001929
Iteration 114/1000 | Loss: 0.00001929
Iteration 115/1000 | Loss: 0.00001929
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001924
Iteration 122/1000 | Loss: 0.00001924
Iteration 123/1000 | Loss: 0.00001924
Iteration 124/1000 | Loss: 0.00001924
Iteration 125/1000 | Loss: 0.00001924
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Iteration 137/1000 | Loss: 0.00001922
Iteration 138/1000 | Loss: 0.00001921
Iteration 139/1000 | Loss: 0.00001921
Iteration 140/1000 | Loss: 0.00001921
Iteration 141/1000 | Loss: 0.00001920
Iteration 142/1000 | Loss: 0.00001920
Iteration 143/1000 | Loss: 0.00001920
Iteration 144/1000 | Loss: 0.00001920
Iteration 145/1000 | Loss: 0.00001920
Iteration 146/1000 | Loss: 0.00001920
Iteration 147/1000 | Loss: 0.00001919
Iteration 148/1000 | Loss: 0.00001919
Iteration 149/1000 | Loss: 0.00001919
Iteration 150/1000 | Loss: 0.00001918
Iteration 151/1000 | Loss: 0.00001918
Iteration 152/1000 | Loss: 0.00001918
Iteration 153/1000 | Loss: 0.00001918
Iteration 154/1000 | Loss: 0.00001918
Iteration 155/1000 | Loss: 0.00001918
Iteration 156/1000 | Loss: 0.00001918
Iteration 157/1000 | Loss: 0.00001918
Iteration 158/1000 | Loss: 0.00001918
Iteration 159/1000 | Loss: 0.00001918
Iteration 160/1000 | Loss: 0.00001918
Iteration 161/1000 | Loss: 0.00001918
Iteration 162/1000 | Loss: 0.00001917
Iteration 163/1000 | Loss: 0.00001917
Iteration 164/1000 | Loss: 0.00001917
Iteration 165/1000 | Loss: 0.00001917
Iteration 166/1000 | Loss: 0.00001917
Iteration 167/1000 | Loss: 0.00001917
Iteration 168/1000 | Loss: 0.00001917
Iteration 169/1000 | Loss: 0.00001917
Iteration 170/1000 | Loss: 0.00001917
Iteration 171/1000 | Loss: 0.00001916
Iteration 172/1000 | Loss: 0.00001916
Iteration 173/1000 | Loss: 0.00001916
Iteration 174/1000 | Loss: 0.00001915
Iteration 175/1000 | Loss: 0.00001915
Iteration 176/1000 | Loss: 0.00001915
Iteration 177/1000 | Loss: 0.00001915
Iteration 178/1000 | Loss: 0.00001915
Iteration 179/1000 | Loss: 0.00001915
Iteration 180/1000 | Loss: 0.00001915
Iteration 181/1000 | Loss: 0.00001915
Iteration 182/1000 | Loss: 0.00001914
Iteration 183/1000 | Loss: 0.00001914
Iteration 184/1000 | Loss: 0.00001914
Iteration 185/1000 | Loss: 0.00001914
Iteration 186/1000 | Loss: 0.00001914
Iteration 187/1000 | Loss: 0.00001914
Iteration 188/1000 | Loss: 0.00001914
Iteration 189/1000 | Loss: 0.00001914
Iteration 190/1000 | Loss: 0.00001914
Iteration 191/1000 | Loss: 0.00001914
Iteration 192/1000 | Loss: 0.00001914
Iteration 193/1000 | Loss: 0.00001914
Iteration 194/1000 | Loss: 0.00001913
Iteration 195/1000 | Loss: 0.00001913
Iteration 196/1000 | Loss: 0.00001913
Iteration 197/1000 | Loss: 0.00001913
Iteration 198/1000 | Loss: 0.00001912
Iteration 199/1000 | Loss: 0.00001912
Iteration 200/1000 | Loss: 0.00001912
Iteration 201/1000 | Loss: 0.00001912
Iteration 202/1000 | Loss: 0.00001912
Iteration 203/1000 | Loss: 0.00001912
Iteration 204/1000 | Loss: 0.00001912
Iteration 205/1000 | Loss: 0.00001911
Iteration 206/1000 | Loss: 0.00001911
Iteration 207/1000 | Loss: 0.00001911
Iteration 208/1000 | Loss: 0.00001911
Iteration 209/1000 | Loss: 0.00001911
Iteration 210/1000 | Loss: 0.00001911
Iteration 211/1000 | Loss: 0.00001911
Iteration 212/1000 | Loss: 0.00001911
Iteration 213/1000 | Loss: 0.00001911
Iteration 214/1000 | Loss: 0.00001911
Iteration 215/1000 | Loss: 0.00001910
Iteration 216/1000 | Loss: 0.00001910
Iteration 217/1000 | Loss: 0.00001910
Iteration 218/1000 | Loss: 0.00001910
Iteration 219/1000 | Loss: 0.00001909
Iteration 220/1000 | Loss: 0.00001909
Iteration 221/1000 | Loss: 0.00001909
Iteration 222/1000 | Loss: 0.00001909
Iteration 223/1000 | Loss: 0.00001909
Iteration 224/1000 | Loss: 0.00001909
Iteration 225/1000 | Loss: 0.00001909
Iteration 226/1000 | Loss: 0.00001909
Iteration 227/1000 | Loss: 0.00001909
Iteration 228/1000 | Loss: 0.00001909
Iteration 229/1000 | Loss: 0.00001909
Iteration 230/1000 | Loss: 0.00001909
Iteration 231/1000 | Loss: 0.00001909
Iteration 232/1000 | Loss: 0.00001908
Iteration 233/1000 | Loss: 0.00001908
Iteration 234/1000 | Loss: 0.00001908
Iteration 235/1000 | Loss: 0.00001908
Iteration 236/1000 | Loss: 0.00001908
Iteration 237/1000 | Loss: 0.00001908
Iteration 238/1000 | Loss: 0.00001908
Iteration 239/1000 | Loss: 0.00001908
Iteration 240/1000 | Loss: 0.00001908
Iteration 241/1000 | Loss: 0.00001908
Iteration 242/1000 | Loss: 0.00001908
Iteration 243/1000 | Loss: 0.00001908
Iteration 244/1000 | Loss: 0.00001908
Iteration 245/1000 | Loss: 0.00001908
Iteration 246/1000 | Loss: 0.00001908
Iteration 247/1000 | Loss: 0.00001908
Iteration 248/1000 | Loss: 0.00001907
Iteration 249/1000 | Loss: 0.00001907
Iteration 250/1000 | Loss: 0.00001907
Iteration 251/1000 | Loss: 0.00001907
Iteration 252/1000 | Loss: 0.00001907
Iteration 253/1000 | Loss: 0.00001907
Iteration 254/1000 | Loss: 0.00001907
Iteration 255/1000 | Loss: 0.00001907
Iteration 256/1000 | Loss: 0.00001907
Iteration 257/1000 | Loss: 0.00001907
Iteration 258/1000 | Loss: 0.00001907
Iteration 259/1000 | Loss: 0.00001907
Iteration 260/1000 | Loss: 0.00001907
Iteration 261/1000 | Loss: 0.00001907
Iteration 262/1000 | Loss: 0.00001907
Iteration 263/1000 | Loss: 0.00001907
Iteration 264/1000 | Loss: 0.00001907
Iteration 265/1000 | Loss: 0.00001907
Iteration 266/1000 | Loss: 0.00001907
Iteration 267/1000 | Loss: 0.00001907
Iteration 268/1000 | Loss: 0.00001907
Iteration 269/1000 | Loss: 0.00001907
Iteration 270/1000 | Loss: 0.00001907
Iteration 271/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.9071107089985162e-05, 1.9071107089985162e-05, 1.9071107089985162e-05, 1.9071107089985162e-05, 1.9071107089985162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9071107089985162e-05

Optimization complete. Final v2v error: 3.4141430854797363 mm

Highest mean error: 5.752281665802002 mm for frame 179

Lowest mean error: 2.819525718688965 mm for frame 111

Saving results

Total time: 214.2301323413849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428403
Iteration 2/25 | Loss: 0.00095799
Iteration 3/25 | Loss: 0.00077597
Iteration 4/25 | Loss: 0.00075120
Iteration 5/25 | Loss: 0.00074532
Iteration 6/25 | Loss: 0.00074427
Iteration 7/25 | Loss: 0.00074390
Iteration 8/25 | Loss: 0.00074386
Iteration 9/25 | Loss: 0.00074386
Iteration 10/25 | Loss: 0.00074386
Iteration 11/25 | Loss: 0.00074386
Iteration 12/25 | Loss: 0.00074386
Iteration 13/25 | Loss: 0.00074386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007438602042384446, 0.0007438602042384446, 0.0007438602042384446, 0.0007438602042384446, 0.0007438602042384446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007438602042384446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64035988
Iteration 2/25 | Loss: 0.00048539
Iteration 3/25 | Loss: 0.00048538
Iteration 4/25 | Loss: 0.00048538
Iteration 5/25 | Loss: 0.00048538
Iteration 6/25 | Loss: 0.00048538
Iteration 7/25 | Loss: 0.00048538
Iteration 8/25 | Loss: 0.00048538
Iteration 9/25 | Loss: 0.00048538
Iteration 10/25 | Loss: 0.00048538
Iteration 11/25 | Loss: 0.00048538
Iteration 12/25 | Loss: 0.00048538
Iteration 13/25 | Loss: 0.00048538
Iteration 14/25 | Loss: 0.00048538
Iteration 15/25 | Loss: 0.00048538
Iteration 16/25 | Loss: 0.00048538
Iteration 17/25 | Loss: 0.00048538
Iteration 18/25 | Loss: 0.00048538
Iteration 19/25 | Loss: 0.00048538
Iteration 20/25 | Loss: 0.00048538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004853750579059124, 0.0004853750579059124, 0.0004853750579059124, 0.0004853750579059124, 0.0004853750579059124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004853750579059124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048538
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001446
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001420
Iteration 15/1000 | Loss: 0.00001418
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001414
Iteration 18/1000 | Loss: 0.00001414
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001409
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001404
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001402
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001400
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001394
Iteration 57/1000 | Loss: 0.00001394
Iteration 58/1000 | Loss: 0.00001394
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001392
Iteration 63/1000 | Loss: 0.00001392
Iteration 64/1000 | Loss: 0.00001391
Iteration 65/1000 | Loss: 0.00001391
Iteration 66/1000 | Loss: 0.00001391
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001390
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001390
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001389
Iteration 74/1000 | Loss: 0.00001389
Iteration 75/1000 | Loss: 0.00001388
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001383
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001378
Iteration 118/1000 | Loss: 0.00001378
Iteration 119/1000 | Loss: 0.00001378
Iteration 120/1000 | Loss: 0.00001378
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001378
Iteration 123/1000 | Loss: 0.00001378
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001378
Iteration 130/1000 | Loss: 0.00001378
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001377
Iteration 140/1000 | Loss: 0.00001377
Iteration 141/1000 | Loss: 0.00001377
Iteration 142/1000 | Loss: 0.00001376
Iteration 143/1000 | Loss: 0.00001376
Iteration 144/1000 | Loss: 0.00001376
Iteration 145/1000 | Loss: 0.00001376
Iteration 146/1000 | Loss: 0.00001376
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001376
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001376
Iteration 153/1000 | Loss: 0.00001376
Iteration 154/1000 | Loss: 0.00001376
Iteration 155/1000 | Loss: 0.00001376
Iteration 156/1000 | Loss: 0.00001376
Iteration 157/1000 | Loss: 0.00001376
Iteration 158/1000 | Loss: 0.00001376
Iteration 159/1000 | Loss: 0.00001376
Iteration 160/1000 | Loss: 0.00001376
Iteration 161/1000 | Loss: 0.00001376
Iteration 162/1000 | Loss: 0.00001376
Iteration 163/1000 | Loss: 0.00001376
Iteration 164/1000 | Loss: 0.00001376
Iteration 165/1000 | Loss: 0.00001376
Iteration 166/1000 | Loss: 0.00001376
Iteration 167/1000 | Loss: 0.00001376
Iteration 168/1000 | Loss: 0.00001376
Iteration 169/1000 | Loss: 0.00001376
Iteration 170/1000 | Loss: 0.00001376
Iteration 171/1000 | Loss: 0.00001376
Iteration 172/1000 | Loss: 0.00001376
Iteration 173/1000 | Loss: 0.00001376
Iteration 174/1000 | Loss: 0.00001376
Iteration 175/1000 | Loss: 0.00001376
Iteration 176/1000 | Loss: 0.00001376
Iteration 177/1000 | Loss: 0.00001376
Iteration 178/1000 | Loss: 0.00001376
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001376
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001376
Iteration 183/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.3759292414761148e-05, 1.3759292414761148e-05, 1.3759292414761148e-05, 1.3759292414761148e-05, 1.3759292414761148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3759292414761148e-05

Optimization complete. Final v2v error: 3.183833122253418 mm

Highest mean error: 3.8036069869995117 mm for frame 70

Lowest mean error: 2.9337878227233887 mm for frame 40

Saving results

Total time: 50.59234619140625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797729
Iteration 2/25 | Loss: 0.00116013
Iteration 3/25 | Loss: 0.00087712
Iteration 4/25 | Loss: 0.00082752
Iteration 5/25 | Loss: 0.00081222
Iteration 6/25 | Loss: 0.00080671
Iteration 7/25 | Loss: 0.00080508
Iteration 8/25 | Loss: 0.00080493
Iteration 9/25 | Loss: 0.00080493
Iteration 10/25 | Loss: 0.00080493
Iteration 11/25 | Loss: 0.00080493
Iteration 12/25 | Loss: 0.00080493
Iteration 13/25 | Loss: 0.00080493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008049252792261541, 0.0008049252792261541, 0.0008049252792261541, 0.0008049252792261541, 0.0008049252792261541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008049252792261541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45308256
Iteration 2/25 | Loss: 0.00052439
Iteration 3/25 | Loss: 0.00052439
Iteration 4/25 | Loss: 0.00052439
Iteration 5/25 | Loss: 0.00052439
Iteration 6/25 | Loss: 0.00052438
Iteration 7/25 | Loss: 0.00052438
Iteration 8/25 | Loss: 0.00052438
Iteration 9/25 | Loss: 0.00052438
Iteration 10/25 | Loss: 0.00052438
Iteration 11/25 | Loss: 0.00052438
Iteration 12/25 | Loss: 0.00052438
Iteration 13/25 | Loss: 0.00052438
Iteration 14/25 | Loss: 0.00052438
Iteration 15/25 | Loss: 0.00052438
Iteration 16/25 | Loss: 0.00052438
Iteration 17/25 | Loss: 0.00052438
Iteration 18/25 | Loss: 0.00052438
Iteration 19/25 | Loss: 0.00052438
Iteration 20/25 | Loss: 0.00052438
Iteration 21/25 | Loss: 0.00052438
Iteration 22/25 | Loss: 0.00052438
Iteration 23/25 | Loss: 0.00052438
Iteration 24/25 | Loss: 0.00052438
Iteration 25/25 | Loss: 0.00052438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052438
Iteration 2/1000 | Loss: 0.00003419
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002113
Iteration 6/1000 | Loss: 0.00002021
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001828
Iteration 17/1000 | Loss: 0.00001827
Iteration 18/1000 | Loss: 0.00001826
Iteration 19/1000 | Loss: 0.00001826
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001824
Iteration 23/1000 | Loss: 0.00001824
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001823
Iteration 30/1000 | Loss: 0.00001823
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001817
Iteration 44/1000 | Loss: 0.00001817
Iteration 45/1000 | Loss: 0.00001817
Iteration 46/1000 | Loss: 0.00001817
Iteration 47/1000 | Loss: 0.00001817
Iteration 48/1000 | Loss: 0.00001816
Iteration 49/1000 | Loss: 0.00001816
Iteration 50/1000 | Loss: 0.00001816
Iteration 51/1000 | Loss: 0.00001816
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00001816
Iteration 54/1000 | Loss: 0.00001815
Iteration 55/1000 | Loss: 0.00001815
Iteration 56/1000 | Loss: 0.00001815
Iteration 57/1000 | Loss: 0.00001814
Iteration 58/1000 | Loss: 0.00001814
Iteration 59/1000 | Loss: 0.00001814
Iteration 60/1000 | Loss: 0.00001814
Iteration 61/1000 | Loss: 0.00001813
Iteration 62/1000 | Loss: 0.00001813
Iteration 63/1000 | Loss: 0.00001813
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001810
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001809
Iteration 79/1000 | Loss: 0.00001809
Iteration 80/1000 | Loss: 0.00001809
Iteration 81/1000 | Loss: 0.00001809
Iteration 82/1000 | Loss: 0.00001809
Iteration 83/1000 | Loss: 0.00001809
Iteration 84/1000 | Loss: 0.00001809
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001807
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001806
Iteration 101/1000 | Loss: 0.00001806
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001805
Iteration 106/1000 | Loss: 0.00001805
Iteration 107/1000 | Loss: 0.00001805
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001805
Iteration 117/1000 | Loss: 0.00001805
Iteration 118/1000 | Loss: 0.00001805
Iteration 119/1000 | Loss: 0.00001804
Iteration 120/1000 | Loss: 0.00001804
Iteration 121/1000 | Loss: 0.00001804
Iteration 122/1000 | Loss: 0.00001804
Iteration 123/1000 | Loss: 0.00001804
Iteration 124/1000 | Loss: 0.00001804
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001804
Iteration 127/1000 | Loss: 0.00001804
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.804404382710345e-05, 1.804404382710345e-05, 1.804404382710345e-05, 1.804404382710345e-05, 1.804404382710345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.804404382710345e-05

Optimization complete. Final v2v error: 3.492469549179077 mm

Highest mean error: 4.069631576538086 mm for frame 187

Lowest mean error: 2.9561386108398438 mm for frame 211

Saving results

Total time: 41.67595958709717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883989
Iteration 2/25 | Loss: 0.00119536
Iteration 3/25 | Loss: 0.00094753
Iteration 4/25 | Loss: 0.00091637
Iteration 5/25 | Loss: 0.00091092
Iteration 6/25 | Loss: 0.00091006
Iteration 7/25 | Loss: 0.00091006
Iteration 8/25 | Loss: 0.00091006
Iteration 9/25 | Loss: 0.00091006
Iteration 10/25 | Loss: 0.00091006
Iteration 11/25 | Loss: 0.00091006
Iteration 12/25 | Loss: 0.00091006
Iteration 13/25 | Loss: 0.00091006
Iteration 14/25 | Loss: 0.00091006
Iteration 15/25 | Loss: 0.00091006
Iteration 16/25 | Loss: 0.00091006
Iteration 17/25 | Loss: 0.00091006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009100629249587655, 0.0009100629249587655, 0.0009100629249587655, 0.0009100629249587655, 0.0009100629249587655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009100629249587655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49170971
Iteration 2/25 | Loss: 0.00064148
Iteration 3/25 | Loss: 0.00064148
Iteration 4/25 | Loss: 0.00064148
Iteration 5/25 | Loss: 0.00064147
Iteration 6/25 | Loss: 0.00064147
Iteration 7/25 | Loss: 0.00064147
Iteration 8/25 | Loss: 0.00064147
Iteration 9/25 | Loss: 0.00064147
Iteration 10/25 | Loss: 0.00064147
Iteration 11/25 | Loss: 0.00064147
Iteration 12/25 | Loss: 0.00064147
Iteration 13/25 | Loss: 0.00064147
Iteration 14/25 | Loss: 0.00064147
Iteration 15/25 | Loss: 0.00064147
Iteration 16/25 | Loss: 0.00064147
Iteration 17/25 | Loss: 0.00064147
Iteration 18/25 | Loss: 0.00064147
Iteration 19/25 | Loss: 0.00064147
Iteration 20/25 | Loss: 0.00064147
Iteration 21/25 | Loss: 0.00064147
Iteration 22/25 | Loss: 0.00064147
Iteration 23/25 | Loss: 0.00064147
Iteration 24/25 | Loss: 0.00064147
Iteration 25/25 | Loss: 0.00064147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064147
Iteration 2/1000 | Loss: 0.00005572
Iteration 3/1000 | Loss: 0.00004531
Iteration 4/1000 | Loss: 0.00004100
Iteration 5/1000 | Loss: 0.00003860
Iteration 6/1000 | Loss: 0.00003728
Iteration 7/1000 | Loss: 0.00003614
Iteration 8/1000 | Loss: 0.00003544
Iteration 9/1000 | Loss: 0.00003506
Iteration 10/1000 | Loss: 0.00003479
Iteration 11/1000 | Loss: 0.00003460
Iteration 12/1000 | Loss: 0.00003448
Iteration 13/1000 | Loss: 0.00003440
Iteration 14/1000 | Loss: 0.00003433
Iteration 15/1000 | Loss: 0.00003432
Iteration 16/1000 | Loss: 0.00003431
Iteration 17/1000 | Loss: 0.00003431
Iteration 18/1000 | Loss: 0.00003427
Iteration 19/1000 | Loss: 0.00003423
Iteration 20/1000 | Loss: 0.00003419
Iteration 21/1000 | Loss: 0.00003412
Iteration 22/1000 | Loss: 0.00003412
Iteration 23/1000 | Loss: 0.00003409
Iteration 24/1000 | Loss: 0.00003408
Iteration 25/1000 | Loss: 0.00003408
Iteration 26/1000 | Loss: 0.00003407
Iteration 27/1000 | Loss: 0.00003406
Iteration 28/1000 | Loss: 0.00003406
Iteration 29/1000 | Loss: 0.00003405
Iteration 30/1000 | Loss: 0.00003405
Iteration 31/1000 | Loss: 0.00003404
Iteration 32/1000 | Loss: 0.00003404
Iteration 33/1000 | Loss: 0.00003403
Iteration 34/1000 | Loss: 0.00003403
Iteration 35/1000 | Loss: 0.00003403
Iteration 36/1000 | Loss: 0.00003403
Iteration 37/1000 | Loss: 0.00003402
Iteration 38/1000 | Loss: 0.00003402
Iteration 39/1000 | Loss: 0.00003402
Iteration 40/1000 | Loss: 0.00003402
Iteration 41/1000 | Loss: 0.00003402
Iteration 42/1000 | Loss: 0.00003401
Iteration 43/1000 | Loss: 0.00003401
Iteration 44/1000 | Loss: 0.00003400
Iteration 45/1000 | Loss: 0.00003400
Iteration 46/1000 | Loss: 0.00003400
Iteration 47/1000 | Loss: 0.00003399
Iteration 48/1000 | Loss: 0.00003399
Iteration 49/1000 | Loss: 0.00003399
Iteration 50/1000 | Loss: 0.00003399
Iteration 51/1000 | Loss: 0.00003398
Iteration 52/1000 | Loss: 0.00003398
Iteration 53/1000 | Loss: 0.00003398
Iteration 54/1000 | Loss: 0.00003398
Iteration 55/1000 | Loss: 0.00003398
Iteration 56/1000 | Loss: 0.00003397
Iteration 57/1000 | Loss: 0.00003396
Iteration 58/1000 | Loss: 0.00003396
Iteration 59/1000 | Loss: 0.00003396
Iteration 60/1000 | Loss: 0.00003396
Iteration 61/1000 | Loss: 0.00003396
Iteration 62/1000 | Loss: 0.00003396
Iteration 63/1000 | Loss: 0.00003396
Iteration 64/1000 | Loss: 0.00003395
Iteration 65/1000 | Loss: 0.00003395
Iteration 66/1000 | Loss: 0.00003395
Iteration 67/1000 | Loss: 0.00003394
Iteration 68/1000 | Loss: 0.00003394
Iteration 69/1000 | Loss: 0.00003394
Iteration 70/1000 | Loss: 0.00003394
Iteration 71/1000 | Loss: 0.00003394
Iteration 72/1000 | Loss: 0.00003394
Iteration 73/1000 | Loss: 0.00003394
Iteration 74/1000 | Loss: 0.00003394
Iteration 75/1000 | Loss: 0.00003394
Iteration 76/1000 | Loss: 0.00003394
Iteration 77/1000 | Loss: 0.00003394
Iteration 78/1000 | Loss: 0.00003394
Iteration 79/1000 | Loss: 0.00003394
Iteration 80/1000 | Loss: 0.00003394
Iteration 81/1000 | Loss: 0.00003393
Iteration 82/1000 | Loss: 0.00003393
Iteration 83/1000 | Loss: 0.00003393
Iteration 84/1000 | Loss: 0.00003393
Iteration 85/1000 | Loss: 0.00003393
Iteration 86/1000 | Loss: 0.00003393
Iteration 87/1000 | Loss: 0.00003392
Iteration 88/1000 | Loss: 0.00003392
Iteration 89/1000 | Loss: 0.00003392
Iteration 90/1000 | Loss: 0.00003392
Iteration 91/1000 | Loss: 0.00003391
Iteration 92/1000 | Loss: 0.00003391
Iteration 93/1000 | Loss: 0.00003391
Iteration 94/1000 | Loss: 0.00003391
Iteration 95/1000 | Loss: 0.00003391
Iteration 96/1000 | Loss: 0.00003391
Iteration 97/1000 | Loss: 0.00003390
Iteration 98/1000 | Loss: 0.00003390
Iteration 99/1000 | Loss: 0.00003390
Iteration 100/1000 | Loss: 0.00003390
Iteration 101/1000 | Loss: 0.00003390
Iteration 102/1000 | Loss: 0.00003390
Iteration 103/1000 | Loss: 0.00003390
Iteration 104/1000 | Loss: 0.00003390
Iteration 105/1000 | Loss: 0.00003390
Iteration 106/1000 | Loss: 0.00003389
Iteration 107/1000 | Loss: 0.00003389
Iteration 108/1000 | Loss: 0.00003389
Iteration 109/1000 | Loss: 0.00003389
Iteration 110/1000 | Loss: 0.00003389
Iteration 111/1000 | Loss: 0.00003389
Iteration 112/1000 | Loss: 0.00003389
Iteration 113/1000 | Loss: 0.00003389
Iteration 114/1000 | Loss: 0.00003389
Iteration 115/1000 | Loss: 0.00003388
Iteration 116/1000 | Loss: 0.00003388
Iteration 117/1000 | Loss: 0.00003388
Iteration 118/1000 | Loss: 0.00003388
Iteration 119/1000 | Loss: 0.00003388
Iteration 120/1000 | Loss: 0.00003388
Iteration 121/1000 | Loss: 0.00003387
Iteration 122/1000 | Loss: 0.00003387
Iteration 123/1000 | Loss: 0.00003387
Iteration 124/1000 | Loss: 0.00003387
Iteration 125/1000 | Loss: 0.00003387
Iteration 126/1000 | Loss: 0.00003387
Iteration 127/1000 | Loss: 0.00003387
Iteration 128/1000 | Loss: 0.00003387
Iteration 129/1000 | Loss: 0.00003387
Iteration 130/1000 | Loss: 0.00003387
Iteration 131/1000 | Loss: 0.00003387
Iteration 132/1000 | Loss: 0.00003386
Iteration 133/1000 | Loss: 0.00003386
Iteration 134/1000 | Loss: 0.00003386
Iteration 135/1000 | Loss: 0.00003386
Iteration 136/1000 | Loss: 0.00003386
Iteration 137/1000 | Loss: 0.00003386
Iteration 138/1000 | Loss: 0.00003386
Iteration 139/1000 | Loss: 0.00003386
Iteration 140/1000 | Loss: 0.00003386
Iteration 141/1000 | Loss: 0.00003386
Iteration 142/1000 | Loss: 0.00003386
Iteration 143/1000 | Loss: 0.00003386
Iteration 144/1000 | Loss: 0.00003386
Iteration 145/1000 | Loss: 0.00003386
Iteration 146/1000 | Loss: 0.00003386
Iteration 147/1000 | Loss: 0.00003386
Iteration 148/1000 | Loss: 0.00003386
Iteration 149/1000 | Loss: 0.00003386
Iteration 150/1000 | Loss: 0.00003386
Iteration 151/1000 | Loss: 0.00003386
Iteration 152/1000 | Loss: 0.00003386
Iteration 153/1000 | Loss: 0.00003386
Iteration 154/1000 | Loss: 0.00003386
Iteration 155/1000 | Loss: 0.00003386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.385835952940397e-05, 3.385835952940397e-05, 3.385835952940397e-05, 3.385835952940397e-05, 3.385835952940397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.385835952940397e-05

Optimization complete. Final v2v error: 4.85644006729126 mm

Highest mean error: 5.344336032867432 mm for frame 22

Lowest mean error: 4.3890461921691895 mm for frame 83

Saving results

Total time: 39.272531509399414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604704
Iteration 2/25 | Loss: 0.00104045
Iteration 3/25 | Loss: 0.00079410
Iteration 4/25 | Loss: 0.00076284
Iteration 5/25 | Loss: 0.00075619
Iteration 6/25 | Loss: 0.00075420
Iteration 7/25 | Loss: 0.00075417
Iteration 8/25 | Loss: 0.00075417
Iteration 9/25 | Loss: 0.00075417
Iteration 10/25 | Loss: 0.00075417
Iteration 11/25 | Loss: 0.00075417
Iteration 12/25 | Loss: 0.00075417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007541741942986846, 0.0007541741942986846, 0.0007541741942986846, 0.0007541741942986846, 0.0007541741942986846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007541741942986846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69492769
Iteration 2/25 | Loss: 0.00046205
Iteration 3/25 | Loss: 0.00046205
Iteration 4/25 | Loss: 0.00046205
Iteration 5/25 | Loss: 0.00046205
Iteration 6/25 | Loss: 0.00046205
Iteration 7/25 | Loss: 0.00046205
Iteration 8/25 | Loss: 0.00046205
Iteration 9/25 | Loss: 0.00046205
Iteration 10/25 | Loss: 0.00046205
Iteration 11/25 | Loss: 0.00046205
Iteration 12/25 | Loss: 0.00046205
Iteration 13/25 | Loss: 0.00046205
Iteration 14/25 | Loss: 0.00046205
Iteration 15/25 | Loss: 0.00046205
Iteration 16/25 | Loss: 0.00046205
Iteration 17/25 | Loss: 0.00046205
Iteration 18/25 | Loss: 0.00046205
Iteration 19/25 | Loss: 0.00046205
Iteration 20/25 | Loss: 0.00046205
Iteration 21/25 | Loss: 0.00046205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00046204691170714796, 0.00046204691170714796, 0.00046204691170714796, 0.00046204691170714796, 0.00046204691170714796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046204691170714796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046205
Iteration 2/1000 | Loss: 0.00002024
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001480
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001406
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001356
Iteration 55/1000 | Loss: 0.00001356
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001354
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001354
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001353
Iteration 68/1000 | Loss: 0.00001353
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001352
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001340
Iteration 115/1000 | Loss: 0.00001340
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001339
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001339
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001339
Iteration 142/1000 | Loss: 0.00001339
Iteration 143/1000 | Loss: 0.00001339
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.339138725597877e-05, 1.339138725597877e-05, 1.339138725597877e-05, 1.339138725597877e-05, 1.339138725597877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.339138725597877e-05

Optimization complete. Final v2v error: 3.129197835922241 mm

Highest mean error: 3.3080852031707764 mm for frame 81

Lowest mean error: 2.9716031551361084 mm for frame 124

Saving results

Total time: 52.78345847129822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844968
Iteration 2/25 | Loss: 0.00088944
Iteration 3/25 | Loss: 0.00074012
Iteration 4/25 | Loss: 0.00071984
Iteration 5/25 | Loss: 0.00071470
Iteration 6/25 | Loss: 0.00071314
Iteration 7/25 | Loss: 0.00071291
Iteration 8/25 | Loss: 0.00071291
Iteration 9/25 | Loss: 0.00071291
Iteration 10/25 | Loss: 0.00071291
Iteration 11/25 | Loss: 0.00071291
Iteration 12/25 | Loss: 0.00071291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007129117730073631, 0.0007129117730073631, 0.0007129117730073631, 0.0007129117730073631, 0.0007129117730073631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007129117730073631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50004864
Iteration 2/25 | Loss: 0.00043116
Iteration 3/25 | Loss: 0.00043116
Iteration 4/25 | Loss: 0.00043116
Iteration 5/25 | Loss: 0.00043116
Iteration 6/25 | Loss: 0.00043116
Iteration 7/25 | Loss: 0.00043116
Iteration 8/25 | Loss: 0.00043116
Iteration 9/25 | Loss: 0.00043116
Iteration 10/25 | Loss: 0.00043116
Iteration 11/25 | Loss: 0.00043116
Iteration 12/25 | Loss: 0.00043116
Iteration 13/25 | Loss: 0.00043116
Iteration 14/25 | Loss: 0.00043116
Iteration 15/25 | Loss: 0.00043116
Iteration 16/25 | Loss: 0.00043116
Iteration 17/25 | Loss: 0.00043116
Iteration 18/25 | Loss: 0.00043116
Iteration 19/25 | Loss: 0.00043116
Iteration 20/25 | Loss: 0.00043116
Iteration 21/25 | Loss: 0.00043116
Iteration 22/25 | Loss: 0.00043116
Iteration 23/25 | Loss: 0.00043116
Iteration 24/25 | Loss: 0.00043116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00043115828884765506, 0.00043115828884765506, 0.00043115828884765506, 0.00043115828884765506, 0.00043115828884765506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043115828884765506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043116
Iteration 2/1000 | Loss: 0.00002157
Iteration 3/1000 | Loss: 0.00001409
Iteration 4/1000 | Loss: 0.00001282
Iteration 5/1000 | Loss: 0.00001194
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001126
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001105
Iteration 11/1000 | Loss: 0.00001105
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001085
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001076
Iteration 16/1000 | Loss: 0.00001076
Iteration 17/1000 | Loss: 0.00001075
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001074
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001073
Iteration 27/1000 | Loss: 0.00001073
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001072
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001070
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001064
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001062
Iteration 44/1000 | Loss: 0.00001062
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001061
Iteration 47/1000 | Loss: 0.00001061
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001061
Iteration 53/1000 | Loss: 0.00001061
Iteration 54/1000 | Loss: 0.00001061
Iteration 55/1000 | Loss: 0.00001060
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001058
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001057
Iteration 67/1000 | Loss: 0.00001057
Iteration 68/1000 | Loss: 0.00001057
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001054
Iteration 79/1000 | Loss: 0.00001054
Iteration 80/1000 | Loss: 0.00001054
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001050
Iteration 83/1000 | Loss: 0.00001050
Iteration 84/1000 | Loss: 0.00001050
Iteration 85/1000 | Loss: 0.00001050
Iteration 86/1000 | Loss: 0.00001050
Iteration 87/1000 | Loss: 0.00001050
Iteration 88/1000 | Loss: 0.00001050
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001046
Iteration 95/1000 | Loss: 0.00001046
Iteration 96/1000 | Loss: 0.00001046
Iteration 97/1000 | Loss: 0.00001045
Iteration 98/1000 | Loss: 0.00001045
Iteration 99/1000 | Loss: 0.00001045
Iteration 100/1000 | Loss: 0.00001045
Iteration 101/1000 | Loss: 0.00001045
Iteration 102/1000 | Loss: 0.00001045
Iteration 103/1000 | Loss: 0.00001045
Iteration 104/1000 | Loss: 0.00001045
Iteration 105/1000 | Loss: 0.00001044
Iteration 106/1000 | Loss: 0.00001043
Iteration 107/1000 | Loss: 0.00001043
Iteration 108/1000 | Loss: 0.00001043
Iteration 109/1000 | Loss: 0.00001042
Iteration 110/1000 | Loss: 0.00001042
Iteration 111/1000 | Loss: 0.00001042
Iteration 112/1000 | Loss: 0.00001042
Iteration 113/1000 | Loss: 0.00001041
Iteration 114/1000 | Loss: 0.00001041
Iteration 115/1000 | Loss: 0.00001041
Iteration 116/1000 | Loss: 0.00001041
Iteration 117/1000 | Loss: 0.00001041
Iteration 118/1000 | Loss: 0.00001041
Iteration 119/1000 | Loss: 0.00001041
Iteration 120/1000 | Loss: 0.00001041
Iteration 121/1000 | Loss: 0.00001041
Iteration 122/1000 | Loss: 0.00001041
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001041
Iteration 127/1000 | Loss: 0.00001041
Iteration 128/1000 | Loss: 0.00001041
Iteration 129/1000 | Loss: 0.00001041
Iteration 130/1000 | Loss: 0.00001041
Iteration 131/1000 | Loss: 0.00001041
Iteration 132/1000 | Loss: 0.00001041
Iteration 133/1000 | Loss: 0.00001041
Iteration 134/1000 | Loss: 0.00001041
Iteration 135/1000 | Loss: 0.00001041
Iteration 136/1000 | Loss: 0.00001041
Iteration 137/1000 | Loss: 0.00001041
Iteration 138/1000 | Loss: 0.00001041
Iteration 139/1000 | Loss: 0.00001041
Iteration 140/1000 | Loss: 0.00001041
Iteration 141/1000 | Loss: 0.00001041
Iteration 142/1000 | Loss: 0.00001041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.0407884474261664e-05, 1.0407884474261664e-05, 1.0407884474261664e-05, 1.0407884474261664e-05, 1.0407884474261664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0407884474261664e-05

Optimization complete. Final v2v error: 2.715364694595337 mm

Highest mean error: 2.9552383422851562 mm for frame 105

Lowest mean error: 2.53234601020813 mm for frame 201

Saving results

Total time: 59.30914807319641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517013
Iteration 2/25 | Loss: 0.00109702
Iteration 3/25 | Loss: 0.00084919
Iteration 4/25 | Loss: 0.00080441
Iteration 5/25 | Loss: 0.00079553
Iteration 6/25 | Loss: 0.00079504
Iteration 7/25 | Loss: 0.00079504
Iteration 8/25 | Loss: 0.00079504
Iteration 9/25 | Loss: 0.00079504
Iteration 10/25 | Loss: 0.00079504
Iteration 11/25 | Loss: 0.00079504
Iteration 12/25 | Loss: 0.00079504
Iteration 13/25 | Loss: 0.00079504
Iteration 14/25 | Loss: 0.00079504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007950433064252138, 0.0007950433064252138, 0.0007950433064252138, 0.0007950433064252138, 0.0007950433064252138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007950433064252138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79719985
Iteration 2/25 | Loss: 0.00034370
Iteration 3/25 | Loss: 0.00034370
Iteration 4/25 | Loss: 0.00034370
Iteration 5/25 | Loss: 0.00034370
Iteration 6/25 | Loss: 0.00034370
Iteration 7/25 | Loss: 0.00034370
Iteration 8/25 | Loss: 0.00034370
Iteration 9/25 | Loss: 0.00034370
Iteration 10/25 | Loss: 0.00034370
Iteration 11/25 | Loss: 0.00034370
Iteration 12/25 | Loss: 0.00034370
Iteration 13/25 | Loss: 0.00034370
Iteration 14/25 | Loss: 0.00034370
Iteration 15/25 | Loss: 0.00034370
Iteration 16/25 | Loss: 0.00034370
Iteration 17/25 | Loss: 0.00034370
Iteration 18/25 | Loss: 0.00034370
Iteration 19/25 | Loss: 0.00034370
Iteration 20/25 | Loss: 0.00034370
Iteration 21/25 | Loss: 0.00034370
Iteration 22/25 | Loss: 0.00034370
Iteration 23/25 | Loss: 0.00034370
Iteration 24/25 | Loss: 0.00034370
Iteration 25/25 | Loss: 0.00034370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034370
Iteration 2/1000 | Loss: 0.00004306
Iteration 3/1000 | Loss: 0.00002979
Iteration 4/1000 | Loss: 0.00002803
Iteration 5/1000 | Loss: 0.00002674
Iteration 6/1000 | Loss: 0.00002603
Iteration 7/1000 | Loss: 0.00002539
Iteration 8/1000 | Loss: 0.00002479
Iteration 9/1000 | Loss: 0.00002439
Iteration 10/1000 | Loss: 0.00002411
Iteration 11/1000 | Loss: 0.00002384
Iteration 12/1000 | Loss: 0.00002366
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00002348
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002331
Iteration 17/1000 | Loss: 0.00002331
Iteration 18/1000 | Loss: 0.00002329
Iteration 19/1000 | Loss: 0.00002329
Iteration 20/1000 | Loss: 0.00002328
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002327
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002314
Iteration 25/1000 | Loss: 0.00002313
Iteration 26/1000 | Loss: 0.00002313
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002311
Iteration 29/1000 | Loss: 0.00002311
Iteration 30/1000 | Loss: 0.00002311
Iteration 31/1000 | Loss: 0.00002311
Iteration 32/1000 | Loss: 0.00002310
Iteration 33/1000 | Loss: 0.00002310
Iteration 34/1000 | Loss: 0.00002310
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002310
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002308
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002307
Iteration 41/1000 | Loss: 0.00002307
Iteration 42/1000 | Loss: 0.00002307
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00002307
Iteration 45/1000 | Loss: 0.00002307
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002305
Iteration 48/1000 | Loss: 0.00002305
Iteration 49/1000 | Loss: 0.00002305
Iteration 50/1000 | Loss: 0.00002305
Iteration 51/1000 | Loss: 0.00002304
Iteration 52/1000 | Loss: 0.00002304
Iteration 53/1000 | Loss: 0.00002303
Iteration 54/1000 | Loss: 0.00002302
Iteration 55/1000 | Loss: 0.00002299
Iteration 56/1000 | Loss: 0.00002299
Iteration 57/1000 | Loss: 0.00002299
Iteration 58/1000 | Loss: 0.00002299
Iteration 59/1000 | Loss: 0.00002299
Iteration 60/1000 | Loss: 0.00002299
Iteration 61/1000 | Loss: 0.00002299
Iteration 62/1000 | Loss: 0.00002299
Iteration 63/1000 | Loss: 0.00002299
Iteration 64/1000 | Loss: 0.00002298
Iteration 65/1000 | Loss: 0.00002298
Iteration 66/1000 | Loss: 0.00002297
Iteration 67/1000 | Loss: 0.00002296
Iteration 68/1000 | Loss: 0.00002296
Iteration 69/1000 | Loss: 0.00002296
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002295
Iteration 72/1000 | Loss: 0.00002295
Iteration 73/1000 | Loss: 0.00002294
Iteration 74/1000 | Loss: 0.00002294
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002293
Iteration 77/1000 | Loss: 0.00002293
Iteration 78/1000 | Loss: 0.00002293
Iteration 79/1000 | Loss: 0.00002293
Iteration 80/1000 | Loss: 0.00002293
Iteration 81/1000 | Loss: 0.00002293
Iteration 82/1000 | Loss: 0.00002292
Iteration 83/1000 | Loss: 0.00002292
Iteration 84/1000 | Loss: 0.00002292
Iteration 85/1000 | Loss: 0.00002292
Iteration 86/1000 | Loss: 0.00002292
Iteration 87/1000 | Loss: 0.00002292
Iteration 88/1000 | Loss: 0.00002292
Iteration 89/1000 | Loss: 0.00002292
Iteration 90/1000 | Loss: 0.00002292
Iteration 91/1000 | Loss: 0.00002291
Iteration 92/1000 | Loss: 0.00002291
Iteration 93/1000 | Loss: 0.00002291
Iteration 94/1000 | Loss: 0.00002291
Iteration 95/1000 | Loss: 0.00002291
Iteration 96/1000 | Loss: 0.00002290
Iteration 97/1000 | Loss: 0.00002290
Iteration 98/1000 | Loss: 0.00002290
Iteration 99/1000 | Loss: 0.00002290
Iteration 100/1000 | Loss: 0.00002289
Iteration 101/1000 | Loss: 0.00002289
Iteration 102/1000 | Loss: 0.00002289
Iteration 103/1000 | Loss: 0.00002288
Iteration 104/1000 | Loss: 0.00002288
Iteration 105/1000 | Loss: 0.00002288
Iteration 106/1000 | Loss: 0.00002288
Iteration 107/1000 | Loss: 0.00002288
Iteration 108/1000 | Loss: 0.00002288
Iteration 109/1000 | Loss: 0.00002288
Iteration 110/1000 | Loss: 0.00002288
Iteration 111/1000 | Loss: 0.00002287
Iteration 112/1000 | Loss: 0.00002287
Iteration 113/1000 | Loss: 0.00002287
Iteration 114/1000 | Loss: 0.00002287
Iteration 115/1000 | Loss: 0.00002287
Iteration 116/1000 | Loss: 0.00002287
Iteration 117/1000 | Loss: 0.00002287
Iteration 118/1000 | Loss: 0.00002287
Iteration 119/1000 | Loss: 0.00002287
Iteration 120/1000 | Loss: 0.00002287
Iteration 121/1000 | Loss: 0.00002286
Iteration 122/1000 | Loss: 0.00002286
Iteration 123/1000 | Loss: 0.00002286
Iteration 124/1000 | Loss: 0.00002286
Iteration 125/1000 | Loss: 0.00002286
Iteration 126/1000 | Loss: 0.00002285
Iteration 127/1000 | Loss: 0.00002285
Iteration 128/1000 | Loss: 0.00002285
Iteration 129/1000 | Loss: 0.00002285
Iteration 130/1000 | Loss: 0.00002285
Iteration 131/1000 | Loss: 0.00002285
Iteration 132/1000 | Loss: 0.00002285
Iteration 133/1000 | Loss: 0.00002285
Iteration 134/1000 | Loss: 0.00002285
Iteration 135/1000 | Loss: 0.00002285
Iteration 136/1000 | Loss: 0.00002284
Iteration 137/1000 | Loss: 0.00002284
Iteration 138/1000 | Loss: 0.00002284
Iteration 139/1000 | Loss: 0.00002284
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002284
Iteration 142/1000 | Loss: 0.00002284
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002283
Iteration 145/1000 | Loss: 0.00002283
Iteration 146/1000 | Loss: 0.00002283
Iteration 147/1000 | Loss: 0.00002282
Iteration 148/1000 | Loss: 0.00002282
Iteration 149/1000 | Loss: 0.00002282
Iteration 150/1000 | Loss: 0.00002281
Iteration 151/1000 | Loss: 0.00002281
Iteration 152/1000 | Loss: 0.00002281
Iteration 153/1000 | Loss: 0.00002281
Iteration 154/1000 | Loss: 0.00002281
Iteration 155/1000 | Loss: 0.00002281
Iteration 156/1000 | Loss: 0.00002281
Iteration 157/1000 | Loss: 0.00002280
Iteration 158/1000 | Loss: 0.00002280
Iteration 159/1000 | Loss: 0.00002280
Iteration 160/1000 | Loss: 0.00002280
Iteration 161/1000 | Loss: 0.00002280
Iteration 162/1000 | Loss: 0.00002280
Iteration 163/1000 | Loss: 0.00002280
Iteration 164/1000 | Loss: 0.00002279
Iteration 165/1000 | Loss: 0.00002279
Iteration 166/1000 | Loss: 0.00002279
Iteration 167/1000 | Loss: 0.00002279
Iteration 168/1000 | Loss: 0.00002279
Iteration 169/1000 | Loss: 0.00002279
Iteration 170/1000 | Loss: 0.00002279
Iteration 171/1000 | Loss: 0.00002279
Iteration 172/1000 | Loss: 0.00002279
Iteration 173/1000 | Loss: 0.00002279
Iteration 174/1000 | Loss: 0.00002279
Iteration 175/1000 | Loss: 0.00002279
Iteration 176/1000 | Loss: 0.00002279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.2789741706219502e-05, 2.2789741706219502e-05, 2.2789741706219502e-05, 2.2789741706219502e-05, 2.2789741706219502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2789741706219502e-05

Optimization complete. Final v2v error: 4.050173282623291 mm

Highest mean error: 4.399257183074951 mm for frame 11

Lowest mean error: 3.8324832916259766 mm for frame 50

Saving results

Total time: 50.35343551635742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00612347
Iteration 2/25 | Loss: 0.00106275
Iteration 3/25 | Loss: 0.00091992
Iteration 4/25 | Loss: 0.00088699
Iteration 5/25 | Loss: 0.00088084
Iteration 6/25 | Loss: 0.00087927
Iteration 7/25 | Loss: 0.00087875
Iteration 8/25 | Loss: 0.00087875
Iteration 9/25 | Loss: 0.00087875
Iteration 10/25 | Loss: 0.00087875
Iteration 11/25 | Loss: 0.00087875
Iteration 12/25 | Loss: 0.00087875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008787450497038662, 0.0008787450497038662, 0.0008787450497038662, 0.0008787450497038662, 0.0008787450497038662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008787450497038662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76433104
Iteration 2/25 | Loss: 0.00080991
Iteration 3/25 | Loss: 0.00080988
Iteration 4/25 | Loss: 0.00080988
Iteration 5/25 | Loss: 0.00080988
Iteration 6/25 | Loss: 0.00080988
Iteration 7/25 | Loss: 0.00080988
Iteration 8/25 | Loss: 0.00080988
Iteration 9/25 | Loss: 0.00080988
Iteration 10/25 | Loss: 0.00080988
Iteration 11/25 | Loss: 0.00080988
Iteration 12/25 | Loss: 0.00080988
Iteration 13/25 | Loss: 0.00080988
Iteration 14/25 | Loss: 0.00080988
Iteration 15/25 | Loss: 0.00080988
Iteration 16/25 | Loss: 0.00080988
Iteration 17/25 | Loss: 0.00080988
Iteration 18/25 | Loss: 0.00080988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008098793332464993, 0.0008098793332464993, 0.0008098793332464993, 0.0008098793332464993, 0.0008098793332464993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008098793332464993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080988
Iteration 2/1000 | Loss: 0.00005260
Iteration 3/1000 | Loss: 0.00002769
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002072
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001784
Iteration 8/1000 | Loss: 0.00001734
Iteration 9/1000 | Loss: 0.00001708
Iteration 10/1000 | Loss: 0.00001699
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001670
Iteration 18/1000 | Loss: 0.00001670
Iteration 19/1000 | Loss: 0.00001669
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001668
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001666
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001666
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001651
Iteration 36/1000 | Loss: 0.00001650
Iteration 37/1000 | Loss: 0.00001649
Iteration 38/1000 | Loss: 0.00001649
Iteration 39/1000 | Loss: 0.00001649
Iteration 40/1000 | Loss: 0.00001648
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001647
Iteration 43/1000 | Loss: 0.00001646
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001645
Iteration 47/1000 | Loss: 0.00001645
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001645
Iteration 50/1000 | Loss: 0.00001645
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001644
Iteration 53/1000 | Loss: 0.00001644
Iteration 54/1000 | Loss: 0.00001644
Iteration 55/1000 | Loss: 0.00001644
Iteration 56/1000 | Loss: 0.00001644
Iteration 57/1000 | Loss: 0.00001644
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001643
Iteration 65/1000 | Loss: 0.00001643
Iteration 66/1000 | Loss: 0.00001643
Iteration 67/1000 | Loss: 0.00001643
Iteration 68/1000 | Loss: 0.00001643
Iteration 69/1000 | Loss: 0.00001643
Iteration 70/1000 | Loss: 0.00001643
Iteration 71/1000 | Loss: 0.00001643
Iteration 72/1000 | Loss: 0.00001643
Iteration 73/1000 | Loss: 0.00001643
Iteration 74/1000 | Loss: 0.00001642
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001642
Iteration 78/1000 | Loss: 0.00001642
Iteration 79/1000 | Loss: 0.00001642
Iteration 80/1000 | Loss: 0.00001642
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001642
Iteration 84/1000 | Loss: 0.00001642
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.642317874939181e-05, 1.642317874939181e-05, 1.642317874939181e-05, 1.642317874939181e-05, 1.642317874939181e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.642317874939181e-05

Optimization complete. Final v2v error: 3.3372957706451416 mm

Highest mean error: 4.655179500579834 mm for frame 0

Lowest mean error: 3.179253339767456 mm for frame 29

Saving results

Total time: 33.24996757507324
