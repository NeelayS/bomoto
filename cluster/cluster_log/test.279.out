Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=279, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15624-15679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012326
Iteration 2/25 | Loss: 0.00275359
Iteration 3/25 | Loss: 0.00161352
Iteration 4/25 | Loss: 0.00134009
Iteration 5/25 | Loss: 0.00117627
Iteration 6/25 | Loss: 0.00105468
Iteration 7/25 | Loss: 0.00095661
Iteration 8/25 | Loss: 0.00087756
Iteration 9/25 | Loss: 0.00084101
Iteration 10/25 | Loss: 0.00081911
Iteration 11/25 | Loss: 0.00081279
Iteration 12/25 | Loss: 0.00080361
Iteration 13/25 | Loss: 0.00079014
Iteration 14/25 | Loss: 0.00079878
Iteration 15/25 | Loss: 0.00077455
Iteration 16/25 | Loss: 0.00075483
Iteration 17/25 | Loss: 0.00073969
Iteration 18/25 | Loss: 0.00073437
Iteration 19/25 | Loss: 0.00073322
Iteration 20/25 | Loss: 0.00073287
Iteration 21/25 | Loss: 0.00073255
Iteration 22/25 | Loss: 0.00073223
Iteration 23/25 | Loss: 0.00073551
Iteration 24/25 | Loss: 0.00073414
Iteration 25/25 | Loss: 0.00073133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45907199
Iteration 2/25 | Loss: 0.00030874
Iteration 3/25 | Loss: 0.00030874
Iteration 4/25 | Loss: 0.00030874
Iteration 5/25 | Loss: 0.00030873
Iteration 6/25 | Loss: 0.00030873
Iteration 7/25 | Loss: 0.00030873
Iteration 8/25 | Loss: 0.00030873
Iteration 9/25 | Loss: 0.00030873
Iteration 10/25 | Loss: 0.00030873
Iteration 11/25 | Loss: 0.00030873
Iteration 12/25 | Loss: 0.00030873
Iteration 13/25 | Loss: 0.00030873
Iteration 14/25 | Loss: 0.00030873
Iteration 15/25 | Loss: 0.00030873
Iteration 16/25 | Loss: 0.00030873
Iteration 17/25 | Loss: 0.00030873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00030873293871991336, 0.00030873293871991336, 0.00030873293871991336, 0.00030873293871991336, 0.00030873293871991336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030873293871991336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030873
Iteration 2/1000 | Loss: 0.00004605
Iteration 3/1000 | Loss: 0.00005523
Iteration 4/1000 | Loss: 0.00003556
Iteration 5/1000 | Loss: 0.00003201
Iteration 6/1000 | Loss: 0.00007524
Iteration 7/1000 | Loss: 0.00008496
Iteration 8/1000 | Loss: 0.00009352
Iteration 9/1000 | Loss: 0.00002899
Iteration 10/1000 | Loss: 0.00007737
Iteration 11/1000 | Loss: 0.00002874
Iteration 12/1000 | Loss: 0.00011922
Iteration 13/1000 | Loss: 0.00002715
Iteration 14/1000 | Loss: 0.00009710
Iteration 15/1000 | Loss: 0.00005146
Iteration 16/1000 | Loss: 0.00002639
Iteration 17/1000 | Loss: 0.00004544
Iteration 18/1000 | Loss: 0.00006648
Iteration 19/1000 | Loss: 0.00002833
Iteration 20/1000 | Loss: 0.00002611
Iteration 21/1000 | Loss: 0.00002598
Iteration 22/1000 | Loss: 0.00002595
Iteration 23/1000 | Loss: 0.00002591
Iteration 24/1000 | Loss: 0.00002590
Iteration 25/1000 | Loss: 0.00004753
Iteration 26/1000 | Loss: 0.00002590
Iteration 27/1000 | Loss: 0.00002580
Iteration 28/1000 | Loss: 0.00002580
Iteration 29/1000 | Loss: 0.00002579
Iteration 30/1000 | Loss: 0.00002579
Iteration 31/1000 | Loss: 0.00002579
Iteration 32/1000 | Loss: 0.00002579
Iteration 33/1000 | Loss: 0.00002578
Iteration 34/1000 | Loss: 0.00002578
Iteration 35/1000 | Loss: 0.00002578
Iteration 36/1000 | Loss: 0.00002578
Iteration 37/1000 | Loss: 0.00002577
Iteration 38/1000 | Loss: 0.00002577
Iteration 39/1000 | Loss: 0.00002577
Iteration 40/1000 | Loss: 0.00002577
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002577
Iteration 43/1000 | Loss: 0.00002576
Iteration 44/1000 | Loss: 0.00002576
Iteration 45/1000 | Loss: 0.00002575
Iteration 46/1000 | Loss: 0.00002575
Iteration 47/1000 | Loss: 0.00002574
Iteration 48/1000 | Loss: 0.00002574
Iteration 49/1000 | Loss: 0.00002574
Iteration 50/1000 | Loss: 0.00002574
Iteration 51/1000 | Loss: 0.00002573
Iteration 52/1000 | Loss: 0.00002573
Iteration 53/1000 | Loss: 0.00002573
Iteration 54/1000 | Loss: 0.00002572
Iteration 55/1000 | Loss: 0.00002572
Iteration 56/1000 | Loss: 0.00002572
Iteration 57/1000 | Loss: 0.00002571
Iteration 58/1000 | Loss: 0.00002571
Iteration 59/1000 | Loss: 0.00002571
Iteration 60/1000 | Loss: 0.00002571
Iteration 61/1000 | Loss: 0.00002571
Iteration 62/1000 | Loss: 0.00002571
Iteration 63/1000 | Loss: 0.00002571
Iteration 64/1000 | Loss: 0.00002571
Iteration 65/1000 | Loss: 0.00002571
Iteration 66/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.571004915807862e-05, 2.571004915807862e-05, 2.571004915807862e-05, 2.571004915807862e-05, 2.571004915807862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.571004915807862e-05

Optimization complete. Final v2v error: 4.171038627624512 mm

Highest mean error: 4.940677642822266 mm for frame 168

Lowest mean error: 3.681502342224121 mm for frame 55

Saving results

Total time: 92.09056282043457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445428
Iteration 2/25 | Loss: 0.00112752
Iteration 3/25 | Loss: 0.00080946
Iteration 4/25 | Loss: 0.00075062
Iteration 5/25 | Loss: 0.00073879
Iteration 6/25 | Loss: 0.00073623
Iteration 7/25 | Loss: 0.00073527
Iteration 8/25 | Loss: 0.00073515
Iteration 9/25 | Loss: 0.00073515
Iteration 10/25 | Loss: 0.00073515
Iteration 11/25 | Loss: 0.00073515
Iteration 12/25 | Loss: 0.00073515
Iteration 13/25 | Loss: 0.00073515
Iteration 14/25 | Loss: 0.00073515
Iteration 15/25 | Loss: 0.00073515
Iteration 16/25 | Loss: 0.00073515
Iteration 17/25 | Loss: 0.00073515
Iteration 18/25 | Loss: 0.00073515
Iteration 19/25 | Loss: 0.00073515
Iteration 20/25 | Loss: 0.00073515
Iteration 21/25 | Loss: 0.00073515
Iteration 22/25 | Loss: 0.00073515
Iteration 23/25 | Loss: 0.00073515
Iteration 24/25 | Loss: 0.00073515
Iteration 25/25 | Loss: 0.00073515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20788276
Iteration 2/25 | Loss: 0.00025271
Iteration 3/25 | Loss: 0.00025269
Iteration 4/25 | Loss: 0.00025269
Iteration 5/25 | Loss: 0.00025269
Iteration 6/25 | Loss: 0.00025269
Iteration 7/25 | Loss: 0.00025269
Iteration 8/25 | Loss: 0.00025269
Iteration 9/25 | Loss: 0.00025269
Iteration 10/25 | Loss: 0.00025268
Iteration 11/25 | Loss: 0.00025268
Iteration 12/25 | Loss: 0.00025268
Iteration 13/25 | Loss: 0.00025268
Iteration 14/25 | Loss: 0.00025268
Iteration 15/25 | Loss: 0.00025268
Iteration 16/25 | Loss: 0.00025268
Iteration 17/25 | Loss: 0.00025268
Iteration 18/25 | Loss: 0.00025268
Iteration 19/25 | Loss: 0.00025268
Iteration 20/25 | Loss: 0.00025268
Iteration 21/25 | Loss: 0.00025268
Iteration 22/25 | Loss: 0.00025268
Iteration 23/25 | Loss: 0.00025268
Iteration 24/25 | Loss: 0.00025268
Iteration 25/25 | Loss: 0.00025268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025268
Iteration 2/1000 | Loss: 0.00006129
Iteration 3/1000 | Loss: 0.00004372
Iteration 4/1000 | Loss: 0.00003793
Iteration 5/1000 | Loss: 0.00003574
Iteration 6/1000 | Loss: 0.00003446
Iteration 7/1000 | Loss: 0.00003366
Iteration 8/1000 | Loss: 0.00003304
Iteration 9/1000 | Loss: 0.00003243
Iteration 10/1000 | Loss: 0.00003201
Iteration 11/1000 | Loss: 0.00003167
Iteration 12/1000 | Loss: 0.00003144
Iteration 13/1000 | Loss: 0.00003123
Iteration 14/1000 | Loss: 0.00003111
Iteration 15/1000 | Loss: 0.00003093
Iteration 16/1000 | Loss: 0.00003086
Iteration 17/1000 | Loss: 0.00003078
Iteration 18/1000 | Loss: 0.00003078
Iteration 19/1000 | Loss: 0.00003075
Iteration 20/1000 | Loss: 0.00003074
Iteration 21/1000 | Loss: 0.00003073
Iteration 22/1000 | Loss: 0.00003073
Iteration 23/1000 | Loss: 0.00003073
Iteration 24/1000 | Loss: 0.00003072
Iteration 25/1000 | Loss: 0.00003071
Iteration 26/1000 | Loss: 0.00003071
Iteration 27/1000 | Loss: 0.00003071
Iteration 28/1000 | Loss: 0.00003068
Iteration 29/1000 | Loss: 0.00003065
Iteration 30/1000 | Loss: 0.00003065
Iteration 31/1000 | Loss: 0.00003065
Iteration 32/1000 | Loss: 0.00003064
Iteration 33/1000 | Loss: 0.00003064
Iteration 34/1000 | Loss: 0.00003063
Iteration 35/1000 | Loss: 0.00003063
Iteration 36/1000 | Loss: 0.00003063
Iteration 37/1000 | Loss: 0.00003063
Iteration 38/1000 | Loss: 0.00003062
Iteration 39/1000 | Loss: 0.00003062
Iteration 40/1000 | Loss: 0.00003061
Iteration 41/1000 | Loss: 0.00003060
Iteration 42/1000 | Loss: 0.00003060
Iteration 43/1000 | Loss: 0.00003059
Iteration 44/1000 | Loss: 0.00003059
Iteration 45/1000 | Loss: 0.00003059
Iteration 46/1000 | Loss: 0.00003058
Iteration 47/1000 | Loss: 0.00003058
Iteration 48/1000 | Loss: 0.00003057
Iteration 49/1000 | Loss: 0.00003057
Iteration 50/1000 | Loss: 0.00003055
Iteration 51/1000 | Loss: 0.00003053
Iteration 52/1000 | Loss: 0.00003052
Iteration 53/1000 | Loss: 0.00003052
Iteration 54/1000 | Loss: 0.00003052
Iteration 55/1000 | Loss: 0.00003051
Iteration 56/1000 | Loss: 0.00003051
Iteration 57/1000 | Loss: 0.00003051
Iteration 58/1000 | Loss: 0.00003049
Iteration 59/1000 | Loss: 0.00003049
Iteration 60/1000 | Loss: 0.00003047
Iteration 61/1000 | Loss: 0.00003047
Iteration 62/1000 | Loss: 0.00003047
Iteration 63/1000 | Loss: 0.00003046
Iteration 64/1000 | Loss: 0.00003046
Iteration 65/1000 | Loss: 0.00003046
Iteration 66/1000 | Loss: 0.00003046
Iteration 67/1000 | Loss: 0.00003046
Iteration 68/1000 | Loss: 0.00003045
Iteration 69/1000 | Loss: 0.00003045
Iteration 70/1000 | Loss: 0.00003045
Iteration 71/1000 | Loss: 0.00003044
Iteration 72/1000 | Loss: 0.00003044
Iteration 73/1000 | Loss: 0.00003044
Iteration 74/1000 | Loss: 0.00003044
Iteration 75/1000 | Loss: 0.00003043
Iteration 76/1000 | Loss: 0.00003043
Iteration 77/1000 | Loss: 0.00003043
Iteration 78/1000 | Loss: 0.00003042
Iteration 79/1000 | Loss: 0.00003042
Iteration 80/1000 | Loss: 0.00003042
Iteration 81/1000 | Loss: 0.00003042
Iteration 82/1000 | Loss: 0.00003042
Iteration 83/1000 | Loss: 0.00003042
Iteration 84/1000 | Loss: 0.00003042
Iteration 85/1000 | Loss: 0.00003042
Iteration 86/1000 | Loss: 0.00003042
Iteration 87/1000 | Loss: 0.00003041
Iteration 88/1000 | Loss: 0.00003041
Iteration 89/1000 | Loss: 0.00003041
Iteration 90/1000 | Loss: 0.00003041
Iteration 91/1000 | Loss: 0.00003041
Iteration 92/1000 | Loss: 0.00003041
Iteration 93/1000 | Loss: 0.00003040
Iteration 94/1000 | Loss: 0.00003040
Iteration 95/1000 | Loss: 0.00003040
Iteration 96/1000 | Loss: 0.00003039
Iteration 97/1000 | Loss: 0.00003039
Iteration 98/1000 | Loss: 0.00003039
Iteration 99/1000 | Loss: 0.00003038
Iteration 100/1000 | Loss: 0.00003038
Iteration 101/1000 | Loss: 0.00003038
Iteration 102/1000 | Loss: 0.00003038
Iteration 103/1000 | Loss: 0.00003037
Iteration 104/1000 | Loss: 0.00003037
Iteration 105/1000 | Loss: 0.00003037
Iteration 106/1000 | Loss: 0.00003036
Iteration 107/1000 | Loss: 0.00003036
Iteration 108/1000 | Loss: 0.00003036
Iteration 109/1000 | Loss: 0.00003035
Iteration 110/1000 | Loss: 0.00003035
Iteration 111/1000 | Loss: 0.00003035
Iteration 112/1000 | Loss: 0.00003034
Iteration 113/1000 | Loss: 0.00003034
Iteration 114/1000 | Loss: 0.00003034
Iteration 115/1000 | Loss: 0.00003034
Iteration 116/1000 | Loss: 0.00003034
Iteration 117/1000 | Loss: 0.00003034
Iteration 118/1000 | Loss: 0.00003034
Iteration 119/1000 | Loss: 0.00003033
Iteration 120/1000 | Loss: 0.00003033
Iteration 121/1000 | Loss: 0.00003033
Iteration 122/1000 | Loss: 0.00003033
Iteration 123/1000 | Loss: 0.00003033
Iteration 124/1000 | Loss: 0.00003033
Iteration 125/1000 | Loss: 0.00003033
Iteration 126/1000 | Loss: 0.00003033
Iteration 127/1000 | Loss: 0.00003033
Iteration 128/1000 | Loss: 0.00003033
Iteration 129/1000 | Loss: 0.00003033
Iteration 130/1000 | Loss: 0.00003032
Iteration 131/1000 | Loss: 0.00003032
Iteration 132/1000 | Loss: 0.00003032
Iteration 133/1000 | Loss: 0.00003032
Iteration 134/1000 | Loss: 0.00003032
Iteration 135/1000 | Loss: 0.00003032
Iteration 136/1000 | Loss: 0.00003032
Iteration 137/1000 | Loss: 0.00003032
Iteration 138/1000 | Loss: 0.00003032
Iteration 139/1000 | Loss: 0.00003032
Iteration 140/1000 | Loss: 0.00003032
Iteration 141/1000 | Loss: 0.00003032
Iteration 142/1000 | Loss: 0.00003032
Iteration 143/1000 | Loss: 0.00003032
Iteration 144/1000 | Loss: 0.00003032
Iteration 145/1000 | Loss: 0.00003032
Iteration 146/1000 | Loss: 0.00003032
Iteration 147/1000 | Loss: 0.00003032
Iteration 148/1000 | Loss: 0.00003032
Iteration 149/1000 | Loss: 0.00003032
Iteration 150/1000 | Loss: 0.00003032
Iteration 151/1000 | Loss: 0.00003032
Iteration 152/1000 | Loss: 0.00003032
Iteration 153/1000 | Loss: 0.00003032
Iteration 154/1000 | Loss: 0.00003032
Iteration 155/1000 | Loss: 0.00003032
Iteration 156/1000 | Loss: 0.00003032
Iteration 157/1000 | Loss: 0.00003032
Iteration 158/1000 | Loss: 0.00003032
Iteration 159/1000 | Loss: 0.00003032
Iteration 160/1000 | Loss: 0.00003032
Iteration 161/1000 | Loss: 0.00003032
Iteration 162/1000 | Loss: 0.00003032
Iteration 163/1000 | Loss: 0.00003032
Iteration 164/1000 | Loss: 0.00003032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [3.0322535167215392e-05, 3.0322535167215392e-05, 3.0322535167215392e-05, 3.0322535167215392e-05, 3.0322535167215392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0322535167215392e-05

Optimization complete. Final v2v error: 4.468716621398926 mm

Highest mean error: 6.0927205085754395 mm for frame 82

Lowest mean error: 3.424560308456421 mm for frame 49

Saving results

Total time: 46.01553273200989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924554
Iteration 2/25 | Loss: 0.00152037
Iteration 3/25 | Loss: 0.00097371
Iteration 4/25 | Loss: 0.00084933
Iteration 5/25 | Loss: 0.00079004
Iteration 6/25 | Loss: 0.00078450
Iteration 7/25 | Loss: 0.00082302
Iteration 8/25 | Loss: 0.00074262
Iteration 9/25 | Loss: 0.00071496
Iteration 10/25 | Loss: 0.00068307
Iteration 11/25 | Loss: 0.00067590
Iteration 12/25 | Loss: 0.00066758
Iteration 13/25 | Loss: 0.00066417
Iteration 14/25 | Loss: 0.00066389
Iteration 15/25 | Loss: 0.00066041
Iteration 16/25 | Loss: 0.00066027
Iteration 17/25 | Loss: 0.00066019
Iteration 18/25 | Loss: 0.00066017
Iteration 19/25 | Loss: 0.00066017
Iteration 20/25 | Loss: 0.00066017
Iteration 21/25 | Loss: 0.00066017
Iteration 22/25 | Loss: 0.00066017
Iteration 23/25 | Loss: 0.00066017
Iteration 24/25 | Loss: 0.00066017
Iteration 25/25 | Loss: 0.00066017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49007249
Iteration 2/25 | Loss: 0.00038820
Iteration 3/25 | Loss: 0.00034821
Iteration 4/25 | Loss: 0.00034821
Iteration 5/25 | Loss: 0.00034820
Iteration 6/25 | Loss: 0.00034820
Iteration 7/25 | Loss: 0.00034820
Iteration 8/25 | Loss: 0.00034820
Iteration 9/25 | Loss: 0.00034820
Iteration 10/25 | Loss: 0.00034820
Iteration 11/25 | Loss: 0.00034820
Iteration 12/25 | Loss: 0.00034820
Iteration 13/25 | Loss: 0.00034820
Iteration 14/25 | Loss: 0.00034820
Iteration 15/25 | Loss: 0.00034820
Iteration 16/25 | Loss: 0.00034820
Iteration 17/25 | Loss: 0.00034820
Iteration 18/25 | Loss: 0.00034820
Iteration 19/25 | Loss: 0.00034820
Iteration 20/25 | Loss: 0.00034820
Iteration 21/25 | Loss: 0.00034820
Iteration 22/25 | Loss: 0.00034820
Iteration 23/25 | Loss: 0.00034820
Iteration 24/25 | Loss: 0.00034820
Iteration 25/25 | Loss: 0.00034820

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034820
Iteration 2/1000 | Loss: 0.00006652
Iteration 3/1000 | Loss: 0.00007937
Iteration 4/1000 | Loss: 0.00008310
Iteration 5/1000 | Loss: 0.00003414
Iteration 6/1000 | Loss: 0.00002691
Iteration 7/1000 | Loss: 0.00002603
Iteration 8/1000 | Loss: 0.00005793
Iteration 9/1000 | Loss: 0.00002493
Iteration 10/1000 | Loss: 0.00120067
Iteration 11/1000 | Loss: 0.00079805
Iteration 12/1000 | Loss: 0.00147789
Iteration 13/1000 | Loss: 0.00056856
Iteration 14/1000 | Loss: 0.00004783
Iteration 15/1000 | Loss: 0.00002435
Iteration 16/1000 | Loss: 0.00008615
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00005404
Iteration 19/1000 | Loss: 0.00002992
Iteration 20/1000 | Loss: 0.00002007
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00010731
Iteration 23/1000 | Loss: 0.00002080
Iteration 24/1000 | Loss: 0.00001858
Iteration 25/1000 | Loss: 0.00002470
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00003876
Iteration 29/1000 | Loss: 0.00002403
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00003924
Iteration 32/1000 | Loss: 0.00001777
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001765
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001762
Iteration 43/1000 | Loss: 0.00001762
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001759
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001756
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001755
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001755
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001754
Iteration 77/1000 | Loss: 0.00001754
Iteration 78/1000 | Loss: 0.00001754
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001753
Iteration 82/1000 | Loss: 0.00001753
Iteration 83/1000 | Loss: 0.00001753
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00001746
Iteration 152/1000 | Loss: 0.00001746
Iteration 153/1000 | Loss: 0.00001746
Iteration 154/1000 | Loss: 0.00001746
Iteration 155/1000 | Loss: 0.00001746
Iteration 156/1000 | Loss: 0.00001746
Iteration 157/1000 | Loss: 0.00001746
Iteration 158/1000 | Loss: 0.00001746
Iteration 159/1000 | Loss: 0.00001746
Iteration 160/1000 | Loss: 0.00001746
Iteration 161/1000 | Loss: 0.00001746
Iteration 162/1000 | Loss: 0.00001746
Iteration 163/1000 | Loss: 0.00001746
Iteration 164/1000 | Loss: 0.00001746
Iteration 165/1000 | Loss: 0.00001746
Iteration 166/1000 | Loss: 0.00001746
Iteration 167/1000 | Loss: 0.00001746
Iteration 168/1000 | Loss: 0.00001746
Iteration 169/1000 | Loss: 0.00001746
Iteration 170/1000 | Loss: 0.00001746
Iteration 171/1000 | Loss: 0.00001746
Iteration 172/1000 | Loss: 0.00001746
Iteration 173/1000 | Loss: 0.00001746
Iteration 174/1000 | Loss: 0.00001746
Iteration 175/1000 | Loss: 0.00001746
Iteration 176/1000 | Loss: 0.00001746
Iteration 177/1000 | Loss: 0.00001746
Iteration 178/1000 | Loss: 0.00001746
Iteration 179/1000 | Loss: 0.00001746
Iteration 180/1000 | Loss: 0.00001746
Iteration 181/1000 | Loss: 0.00001746
Iteration 182/1000 | Loss: 0.00001746
Iteration 183/1000 | Loss: 0.00001746
Iteration 184/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.7458982256357558e-05, 1.7458982256357558e-05, 1.7458982256357558e-05, 1.7458982256357558e-05, 1.7458982256357558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7458982256357558e-05

Optimization complete. Final v2v error: 3.5630199909210205 mm

Highest mean error: 4.404555320739746 mm for frame 74

Lowest mean error: 3.1824228763580322 mm for frame 105

Saving results

Total time: 82.92485761642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391818
Iteration 2/25 | Loss: 0.00076067
Iteration 3/25 | Loss: 0.00059966
Iteration 4/25 | Loss: 0.00058493
Iteration 5/25 | Loss: 0.00058090
Iteration 6/25 | Loss: 0.00057967
Iteration 7/25 | Loss: 0.00057953
Iteration 8/25 | Loss: 0.00057953
Iteration 9/25 | Loss: 0.00057953
Iteration 10/25 | Loss: 0.00057953
Iteration 11/25 | Loss: 0.00057953
Iteration 12/25 | Loss: 0.00057953
Iteration 13/25 | Loss: 0.00057953
Iteration 14/25 | Loss: 0.00057953
Iteration 15/25 | Loss: 0.00057953
Iteration 16/25 | Loss: 0.00057953
Iteration 17/25 | Loss: 0.00057953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005795286851935089, 0.0005795286851935089, 0.0005795286851935089, 0.0005795286851935089, 0.0005795286851935089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005795286851935089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46138442
Iteration 2/25 | Loss: 0.00024125
Iteration 3/25 | Loss: 0.00024124
Iteration 4/25 | Loss: 0.00024124
Iteration 5/25 | Loss: 0.00024124
Iteration 6/25 | Loss: 0.00024124
Iteration 7/25 | Loss: 0.00024124
Iteration 8/25 | Loss: 0.00024124
Iteration 9/25 | Loss: 0.00024124
Iteration 10/25 | Loss: 0.00024124
Iteration 11/25 | Loss: 0.00024124
Iteration 12/25 | Loss: 0.00024124
Iteration 13/25 | Loss: 0.00024124
Iteration 14/25 | Loss: 0.00024124
Iteration 15/25 | Loss: 0.00024124
Iteration 16/25 | Loss: 0.00024124
Iteration 17/25 | Loss: 0.00024124
Iteration 18/25 | Loss: 0.00024124
Iteration 19/25 | Loss: 0.00024124
Iteration 20/25 | Loss: 0.00024124
Iteration 21/25 | Loss: 0.00024124
Iteration 22/25 | Loss: 0.00024124
Iteration 23/25 | Loss: 0.00024124
Iteration 24/25 | Loss: 0.00024124
Iteration 25/25 | Loss: 0.00024124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024124
Iteration 2/1000 | Loss: 0.00002250
Iteration 3/1000 | Loss: 0.00001486
Iteration 4/1000 | Loss: 0.00001153
Iteration 5/1000 | Loss: 0.00001082
Iteration 6/1000 | Loss: 0.00001041
Iteration 7/1000 | Loss: 0.00001016
Iteration 8/1000 | Loss: 0.00000998
Iteration 9/1000 | Loss: 0.00000996
Iteration 10/1000 | Loss: 0.00000992
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000991
Iteration 13/1000 | Loss: 0.00000988
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000987
Iteration 16/1000 | Loss: 0.00000987
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000986
Iteration 19/1000 | Loss: 0.00000986
Iteration 20/1000 | Loss: 0.00000985
Iteration 21/1000 | Loss: 0.00000985
Iteration 22/1000 | Loss: 0.00000984
Iteration 23/1000 | Loss: 0.00000984
Iteration 24/1000 | Loss: 0.00000984
Iteration 25/1000 | Loss: 0.00000984
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000983
Iteration 28/1000 | Loss: 0.00000983
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000983
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000982
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000980
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000978
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000972
Iteration 55/1000 | Loss: 0.00000971
Iteration 56/1000 | Loss: 0.00000971
Iteration 57/1000 | Loss: 0.00000971
Iteration 58/1000 | Loss: 0.00000971
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000970
Iteration 61/1000 | Loss: 0.00000970
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000970
Iteration 66/1000 | Loss: 0.00000970
Iteration 67/1000 | Loss: 0.00000970
Iteration 68/1000 | Loss: 0.00000970
Iteration 69/1000 | Loss: 0.00000970
Iteration 70/1000 | Loss: 0.00000970
Iteration 71/1000 | Loss: 0.00000970
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000970
Iteration 75/1000 | Loss: 0.00000970
Iteration 76/1000 | Loss: 0.00000970
Iteration 77/1000 | Loss: 0.00000970
Iteration 78/1000 | Loss: 0.00000970
Iteration 79/1000 | Loss: 0.00000970
Iteration 80/1000 | Loss: 0.00000970
Iteration 81/1000 | Loss: 0.00000970
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000970
Iteration 85/1000 | Loss: 0.00000970
Iteration 86/1000 | Loss: 0.00000970
Iteration 87/1000 | Loss: 0.00000970
Iteration 88/1000 | Loss: 0.00000970
Iteration 89/1000 | Loss: 0.00000970
Iteration 90/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [9.697845598566346e-06, 9.697845598566346e-06, 9.697845598566346e-06, 9.697845598566346e-06, 9.697845598566346e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.697845598566346e-06

Optimization complete. Final v2v error: 2.6273419857025146 mm

Highest mean error: 3.4189085960388184 mm for frame 61

Lowest mean error: 2.4610087871551514 mm for frame 84

Saving results

Total time: 26.751375436782837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500182
Iteration 2/25 | Loss: 0.00092881
Iteration 3/25 | Loss: 0.00073776
Iteration 4/25 | Loss: 0.00069269
Iteration 5/25 | Loss: 0.00068715
Iteration 6/25 | Loss: 0.00068645
Iteration 7/25 | Loss: 0.00068634
Iteration 8/25 | Loss: 0.00068634
Iteration 9/25 | Loss: 0.00068634
Iteration 10/25 | Loss: 0.00068634
Iteration 11/25 | Loss: 0.00068634
Iteration 12/25 | Loss: 0.00068634
Iteration 13/25 | Loss: 0.00068634
Iteration 14/25 | Loss: 0.00068634
Iteration 15/25 | Loss: 0.00068634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006863392773084342, 0.0006863392773084342, 0.0006863392773084342, 0.0006863392773084342, 0.0006863392773084342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006863392773084342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06023610
Iteration 2/25 | Loss: 0.00028728
Iteration 3/25 | Loss: 0.00028722
Iteration 4/25 | Loss: 0.00028722
Iteration 5/25 | Loss: 0.00028722
Iteration 6/25 | Loss: 0.00028722
Iteration 7/25 | Loss: 0.00028722
Iteration 8/25 | Loss: 0.00028722
Iteration 9/25 | Loss: 0.00028722
Iteration 10/25 | Loss: 0.00028722
Iteration 11/25 | Loss: 0.00028722
Iteration 12/25 | Loss: 0.00028722
Iteration 13/25 | Loss: 0.00028722
Iteration 14/25 | Loss: 0.00028722
Iteration 15/25 | Loss: 0.00028722
Iteration 16/25 | Loss: 0.00028722
Iteration 17/25 | Loss: 0.00028722
Iteration 18/25 | Loss: 0.00028722
Iteration 19/25 | Loss: 0.00028722
Iteration 20/25 | Loss: 0.00028722
Iteration 21/25 | Loss: 0.00028722
Iteration 22/25 | Loss: 0.00028722
Iteration 23/25 | Loss: 0.00028722
Iteration 24/25 | Loss: 0.00028722
Iteration 25/25 | Loss: 0.00028722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028722
Iteration 2/1000 | Loss: 0.00004554
Iteration 3/1000 | Loss: 0.00002940
Iteration 4/1000 | Loss: 0.00002592
Iteration 5/1000 | Loss: 0.00002429
Iteration 6/1000 | Loss: 0.00002355
Iteration 7/1000 | Loss: 0.00002265
Iteration 8/1000 | Loss: 0.00002228
Iteration 9/1000 | Loss: 0.00002193
Iteration 10/1000 | Loss: 0.00002168
Iteration 11/1000 | Loss: 0.00002132
Iteration 12/1000 | Loss: 0.00002112
Iteration 13/1000 | Loss: 0.00002111
Iteration 14/1000 | Loss: 0.00002101
Iteration 15/1000 | Loss: 0.00002101
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002093
Iteration 18/1000 | Loss: 0.00002093
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002092
Iteration 21/1000 | Loss: 0.00002092
Iteration 22/1000 | Loss: 0.00002091
Iteration 23/1000 | Loss: 0.00002090
Iteration 24/1000 | Loss: 0.00002089
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002079
Iteration 37/1000 | Loss: 0.00002078
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00002076
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002076
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002076
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002073
Iteration 67/1000 | Loss: 0.00002073
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002068
Iteration 96/1000 | Loss: 0.00002068
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002067
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002060
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002059
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002059
Iteration 123/1000 | Loss: 0.00002059
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002058
Iteration 126/1000 | Loss: 0.00002058
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Iteration 138/1000 | Loss: 0.00002057
Iteration 139/1000 | Loss: 0.00002057
Iteration 140/1000 | Loss: 0.00002057
Iteration 141/1000 | Loss: 0.00002057
Iteration 142/1000 | Loss: 0.00002057
Iteration 143/1000 | Loss: 0.00002057
Iteration 144/1000 | Loss: 0.00002057
Iteration 145/1000 | Loss: 0.00002057
Iteration 146/1000 | Loss: 0.00002057
Iteration 147/1000 | Loss: 0.00002057
Iteration 148/1000 | Loss: 0.00002057
Iteration 149/1000 | Loss: 0.00002057
Iteration 150/1000 | Loss: 0.00002057
Iteration 151/1000 | Loss: 0.00002057
Iteration 152/1000 | Loss: 0.00002057
Iteration 153/1000 | Loss: 0.00002057
Iteration 154/1000 | Loss: 0.00002057
Iteration 155/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0568793843267485e-05, 2.0568793843267485e-05, 2.0568793843267485e-05, 2.0568793843267485e-05, 2.0568793843267485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0568793843267485e-05

Optimization complete. Final v2v error: 3.7835657596588135 mm

Highest mean error: 3.8116636276245117 mm for frame 46

Lowest mean error: 3.7392444610595703 mm for frame 8

Saving results

Total time: 37.96606397628784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804432
Iteration 2/25 | Loss: 0.00108875
Iteration 3/25 | Loss: 0.00081065
Iteration 4/25 | Loss: 0.00074116
Iteration 5/25 | Loss: 0.00072689
Iteration 6/25 | Loss: 0.00072244
Iteration 7/25 | Loss: 0.00072105
Iteration 8/25 | Loss: 0.00072100
Iteration 9/25 | Loss: 0.00072100
Iteration 10/25 | Loss: 0.00072100
Iteration 11/25 | Loss: 0.00072100
Iteration 12/25 | Loss: 0.00072100
Iteration 13/25 | Loss: 0.00072100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007209975156001747, 0.0007209975156001747, 0.0007209975156001747, 0.0007209975156001747, 0.0007209975156001747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007209975156001747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65127230
Iteration 2/25 | Loss: 0.00046012
Iteration 3/25 | Loss: 0.00046012
Iteration 4/25 | Loss: 0.00046012
Iteration 5/25 | Loss: 0.00046012
Iteration 6/25 | Loss: 0.00046012
Iteration 7/25 | Loss: 0.00046011
Iteration 8/25 | Loss: 0.00046011
Iteration 9/25 | Loss: 0.00046011
Iteration 10/25 | Loss: 0.00046011
Iteration 11/25 | Loss: 0.00046011
Iteration 12/25 | Loss: 0.00046011
Iteration 13/25 | Loss: 0.00046011
Iteration 14/25 | Loss: 0.00046011
Iteration 15/25 | Loss: 0.00046011
Iteration 16/25 | Loss: 0.00046011
Iteration 17/25 | Loss: 0.00046011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004601141845341772, 0.0004601141845341772, 0.0004601141845341772, 0.0004601141845341772, 0.0004601141845341772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004601141845341772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046011
Iteration 2/1000 | Loss: 0.00005368
Iteration 3/1000 | Loss: 0.00003438
Iteration 4/1000 | Loss: 0.00002746
Iteration 5/1000 | Loss: 0.00002538
Iteration 6/1000 | Loss: 0.00002366
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002224
Iteration 9/1000 | Loss: 0.00002169
Iteration 10/1000 | Loss: 0.00002134
Iteration 11/1000 | Loss: 0.00002106
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002067
Iteration 14/1000 | Loss: 0.00002065
Iteration 15/1000 | Loss: 0.00002059
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002051
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002049
Iteration 20/1000 | Loss: 0.00002041
Iteration 21/1000 | Loss: 0.00002037
Iteration 22/1000 | Loss: 0.00002037
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002033
Iteration 25/1000 | Loss: 0.00002031
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002030
Iteration 28/1000 | Loss: 0.00002030
Iteration 29/1000 | Loss: 0.00002029
Iteration 30/1000 | Loss: 0.00002028
Iteration 31/1000 | Loss: 0.00002028
Iteration 32/1000 | Loss: 0.00002027
Iteration 33/1000 | Loss: 0.00002027
Iteration 34/1000 | Loss: 0.00002027
Iteration 35/1000 | Loss: 0.00002026
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002025
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00002025
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002024
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002020
Iteration 54/1000 | Loss: 0.00002020
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002019
Iteration 57/1000 | Loss: 0.00002019
Iteration 58/1000 | Loss: 0.00002019
Iteration 59/1000 | Loss: 0.00002019
Iteration 60/1000 | Loss: 0.00002019
Iteration 61/1000 | Loss: 0.00002019
Iteration 62/1000 | Loss: 0.00002018
Iteration 63/1000 | Loss: 0.00002018
Iteration 64/1000 | Loss: 0.00002018
Iteration 65/1000 | Loss: 0.00002018
Iteration 66/1000 | Loss: 0.00002017
Iteration 67/1000 | Loss: 0.00002017
Iteration 68/1000 | Loss: 0.00002017
Iteration 69/1000 | Loss: 0.00002017
Iteration 70/1000 | Loss: 0.00002017
Iteration 71/1000 | Loss: 0.00002017
Iteration 72/1000 | Loss: 0.00002017
Iteration 73/1000 | Loss: 0.00002017
Iteration 74/1000 | Loss: 0.00002017
Iteration 75/1000 | Loss: 0.00002017
Iteration 76/1000 | Loss: 0.00002016
Iteration 77/1000 | Loss: 0.00002016
Iteration 78/1000 | Loss: 0.00002016
Iteration 79/1000 | Loss: 0.00002016
Iteration 80/1000 | Loss: 0.00002016
Iteration 81/1000 | Loss: 0.00002016
Iteration 82/1000 | Loss: 0.00002016
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00002015
Iteration 86/1000 | Loss: 0.00002015
Iteration 87/1000 | Loss: 0.00002015
Iteration 88/1000 | Loss: 0.00002015
Iteration 89/1000 | Loss: 0.00002015
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002015
Iteration 96/1000 | Loss: 0.00002015
Iteration 97/1000 | Loss: 0.00002015
Iteration 98/1000 | Loss: 0.00002014
Iteration 99/1000 | Loss: 0.00002014
Iteration 100/1000 | Loss: 0.00002014
Iteration 101/1000 | Loss: 0.00002014
Iteration 102/1000 | Loss: 0.00002014
Iteration 103/1000 | Loss: 0.00002013
Iteration 104/1000 | Loss: 0.00002013
Iteration 105/1000 | Loss: 0.00002013
Iteration 106/1000 | Loss: 0.00002013
Iteration 107/1000 | Loss: 0.00002013
Iteration 108/1000 | Loss: 0.00002013
Iteration 109/1000 | Loss: 0.00002013
Iteration 110/1000 | Loss: 0.00002013
Iteration 111/1000 | Loss: 0.00002012
Iteration 112/1000 | Loss: 0.00002012
Iteration 113/1000 | Loss: 0.00002012
Iteration 114/1000 | Loss: 0.00002012
Iteration 115/1000 | Loss: 0.00002012
Iteration 116/1000 | Loss: 0.00002012
Iteration 117/1000 | Loss: 0.00002012
Iteration 118/1000 | Loss: 0.00002012
Iteration 119/1000 | Loss: 0.00002012
Iteration 120/1000 | Loss: 0.00002011
Iteration 121/1000 | Loss: 0.00002011
Iteration 122/1000 | Loss: 0.00002011
Iteration 123/1000 | Loss: 0.00002011
Iteration 124/1000 | Loss: 0.00002011
Iteration 125/1000 | Loss: 0.00002011
Iteration 126/1000 | Loss: 0.00002011
Iteration 127/1000 | Loss: 0.00002011
Iteration 128/1000 | Loss: 0.00002011
Iteration 129/1000 | Loss: 0.00002011
Iteration 130/1000 | Loss: 0.00002011
Iteration 131/1000 | Loss: 0.00002011
Iteration 132/1000 | Loss: 0.00002011
Iteration 133/1000 | Loss: 0.00002011
Iteration 134/1000 | Loss: 0.00002011
Iteration 135/1000 | Loss: 0.00002011
Iteration 136/1000 | Loss: 0.00002011
Iteration 137/1000 | Loss: 0.00002011
Iteration 138/1000 | Loss: 0.00002010
Iteration 139/1000 | Loss: 0.00002010
Iteration 140/1000 | Loss: 0.00002010
Iteration 141/1000 | Loss: 0.00002010
Iteration 142/1000 | Loss: 0.00002010
Iteration 143/1000 | Loss: 0.00002010
Iteration 144/1000 | Loss: 0.00002010
Iteration 145/1000 | Loss: 0.00002010
Iteration 146/1000 | Loss: 0.00002009
Iteration 147/1000 | Loss: 0.00002009
Iteration 148/1000 | Loss: 0.00002009
Iteration 149/1000 | Loss: 0.00002009
Iteration 150/1000 | Loss: 0.00002009
Iteration 151/1000 | Loss: 0.00002009
Iteration 152/1000 | Loss: 0.00002009
Iteration 153/1000 | Loss: 0.00002009
Iteration 154/1000 | Loss: 0.00002009
Iteration 155/1000 | Loss: 0.00002009
Iteration 156/1000 | Loss: 0.00002009
Iteration 157/1000 | Loss: 0.00002009
Iteration 158/1000 | Loss: 0.00002009
Iteration 159/1000 | Loss: 0.00002008
Iteration 160/1000 | Loss: 0.00002008
Iteration 161/1000 | Loss: 0.00002008
Iteration 162/1000 | Loss: 0.00002008
Iteration 163/1000 | Loss: 0.00002008
Iteration 164/1000 | Loss: 0.00002008
Iteration 165/1000 | Loss: 0.00002008
Iteration 166/1000 | Loss: 0.00002008
Iteration 167/1000 | Loss: 0.00002008
Iteration 168/1000 | Loss: 0.00002008
Iteration 169/1000 | Loss: 0.00002008
Iteration 170/1000 | Loss: 0.00002008
Iteration 171/1000 | Loss: 0.00002008
Iteration 172/1000 | Loss: 0.00002008
Iteration 173/1000 | Loss: 0.00002008
Iteration 174/1000 | Loss: 0.00002008
Iteration 175/1000 | Loss: 0.00002008
Iteration 176/1000 | Loss: 0.00002008
Iteration 177/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.0077755834790878e-05, 2.0077755834790878e-05, 2.0077755834790878e-05, 2.0077755834790878e-05, 2.0077755834790878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0077755834790878e-05

Optimization complete. Final v2v error: 3.754333019256592 mm

Highest mean error: 5.367987632751465 mm for frame 176

Lowest mean error: 2.8141582012176514 mm for frame 130

Saving results

Total time: 48.17476987838745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995660
Iteration 2/25 | Loss: 0.00995659
Iteration 3/25 | Loss: 0.00995659
Iteration 4/25 | Loss: 0.00995659
Iteration 5/25 | Loss: 0.00995659
Iteration 6/25 | Loss: 0.00995658
Iteration 7/25 | Loss: 0.00995658
Iteration 8/25 | Loss: 0.00995658
Iteration 9/25 | Loss: 0.00995657
Iteration 10/25 | Loss: 0.00995657
Iteration 11/25 | Loss: 0.00995657
Iteration 12/25 | Loss: 0.00995657
Iteration 13/25 | Loss: 0.00995657
Iteration 14/25 | Loss: 0.00995657
Iteration 15/25 | Loss: 0.00995657
Iteration 16/25 | Loss: 0.00995656
Iteration 17/25 | Loss: 0.00995656
Iteration 18/25 | Loss: 0.00995656
Iteration 19/25 | Loss: 0.00995656
Iteration 20/25 | Loss: 0.00995656
Iteration 21/25 | Loss: 0.00995656
Iteration 22/25 | Loss: 0.00995655
Iteration 23/25 | Loss: 0.00995655
Iteration 24/25 | Loss: 0.00995655
Iteration 25/25 | Loss: 0.00995655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74591148
Iteration 2/25 | Loss: 0.18954068
Iteration 3/25 | Loss: 0.18953805
Iteration 4/25 | Loss: 0.18953794
Iteration 5/25 | Loss: 0.18953794
Iteration 6/25 | Loss: 0.18953794
Iteration 7/25 | Loss: 0.18953794
Iteration 8/25 | Loss: 0.18953791
Iteration 9/25 | Loss: 0.18953791
Iteration 10/25 | Loss: 0.18953791
Iteration 11/25 | Loss: 0.18953791
Iteration 12/25 | Loss: 0.18953791
Iteration 13/25 | Loss: 0.18953791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.189537912607193, 0.189537912607193, 0.189537912607193, 0.189537912607193, 0.189537912607193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.189537912607193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18953791
Iteration 2/1000 | Loss: 0.00394075
Iteration 3/1000 | Loss: 0.00112039
Iteration 4/1000 | Loss: 0.00048967
Iteration 5/1000 | Loss: 0.00024162
Iteration 6/1000 | Loss: 0.00013782
Iteration 7/1000 | Loss: 0.00008618
Iteration 8/1000 | Loss: 0.00006605
Iteration 9/1000 | Loss: 0.00004927
Iteration 10/1000 | Loss: 0.00004271
Iteration 11/1000 | Loss: 0.00003702
Iteration 12/1000 | Loss: 0.00003337
Iteration 13/1000 | Loss: 0.00003069
Iteration 14/1000 | Loss: 0.00002896
Iteration 15/1000 | Loss: 0.00002580
Iteration 16/1000 | Loss: 0.00002338
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002078
Iteration 19/1000 | Loss: 0.00001998
Iteration 20/1000 | Loss: 0.00001936
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001833
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001762
Iteration 29/1000 | Loss: 0.00001761
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001748
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001748
Iteration 54/1000 | Loss: 0.00001748
Iteration 55/1000 | Loss: 0.00001748
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001743
Iteration 60/1000 | Loss: 0.00001743
Iteration 61/1000 | Loss: 0.00001742
Iteration 62/1000 | Loss: 0.00001742
Iteration 63/1000 | Loss: 0.00001742
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001741
Iteration 70/1000 | Loss: 0.00001740
Iteration 71/1000 | Loss: 0.00001740
Iteration 72/1000 | Loss: 0.00001740
Iteration 73/1000 | Loss: 0.00001740
Iteration 74/1000 | Loss: 0.00001740
Iteration 75/1000 | Loss: 0.00001740
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00001740
Iteration 78/1000 | Loss: 0.00001740
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.7400083379470743e-05, 1.7400083379470743e-05, 1.7400083379470743e-05, 1.7400083379470743e-05, 1.7400083379470743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7400083379470743e-05

Optimization complete. Final v2v error: 3.5924670696258545 mm

Highest mean error: 4.265537261962891 mm for frame 141

Lowest mean error: 3.3406624794006348 mm for frame 3

Saving results

Total time: 53.28518629074097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908367
Iteration 2/25 | Loss: 0.00226492
Iteration 3/25 | Loss: 0.00145571
Iteration 4/25 | Loss: 0.00118203
Iteration 5/25 | Loss: 0.00110960
Iteration 6/25 | Loss: 0.00123937
Iteration 7/25 | Loss: 0.00102566
Iteration 8/25 | Loss: 0.00093253
Iteration 9/25 | Loss: 0.00090788
Iteration 10/25 | Loss: 0.00090815
Iteration 11/25 | Loss: 0.00089731
Iteration 12/25 | Loss: 0.00089189
Iteration 13/25 | Loss: 0.00088606
Iteration 14/25 | Loss: 0.00086478
Iteration 15/25 | Loss: 0.00085049
Iteration 16/25 | Loss: 0.00084445
Iteration 17/25 | Loss: 0.00084790
Iteration 18/25 | Loss: 0.00085191
Iteration 19/25 | Loss: 0.00084345
Iteration 20/25 | Loss: 0.00084239
Iteration 21/25 | Loss: 0.00083624
Iteration 22/25 | Loss: 0.00083277
Iteration 23/25 | Loss: 0.00082989
Iteration 24/25 | Loss: 0.00082631
Iteration 25/25 | Loss: 0.00082973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60489774
Iteration 2/25 | Loss: 0.00069613
Iteration 3/25 | Loss: 0.00061497
Iteration 4/25 | Loss: 0.00061497
Iteration 5/25 | Loss: 0.00061497
Iteration 6/25 | Loss: 0.00061497
Iteration 7/25 | Loss: 0.00061496
Iteration 8/25 | Loss: 0.00061496
Iteration 9/25 | Loss: 0.00061496
Iteration 10/25 | Loss: 0.00061496
Iteration 11/25 | Loss: 0.00061496
Iteration 12/25 | Loss: 0.00061496
Iteration 13/25 | Loss: 0.00061496
Iteration 14/25 | Loss: 0.00061496
Iteration 15/25 | Loss: 0.00061496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006149638211354613, 0.0006149638211354613, 0.0006149638211354613, 0.0006149638211354613, 0.0006149638211354613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006149638211354613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061496
Iteration 2/1000 | Loss: 0.00032824
Iteration 3/1000 | Loss: 0.00013471
Iteration 4/1000 | Loss: 0.00004750
Iteration 5/1000 | Loss: 0.00004239
Iteration 6/1000 | Loss: 0.00038151
Iteration 7/1000 | Loss: 0.00005635
Iteration 8/1000 | Loss: 0.00004456
Iteration 9/1000 | Loss: 0.00004112
Iteration 10/1000 | Loss: 0.00003827
Iteration 11/1000 | Loss: 0.00008281
Iteration 12/1000 | Loss: 0.00003574
Iteration 13/1000 | Loss: 0.00003463
Iteration 14/1000 | Loss: 0.00003355
Iteration 15/1000 | Loss: 0.00021189
Iteration 16/1000 | Loss: 0.00018093
Iteration 17/1000 | Loss: 0.00003353
Iteration 18/1000 | Loss: 0.00003170
Iteration 19/1000 | Loss: 0.00003085
Iteration 20/1000 | Loss: 0.00016901
Iteration 21/1000 | Loss: 0.00073688
Iteration 22/1000 | Loss: 0.00017457
Iteration 23/1000 | Loss: 0.00026383
Iteration 24/1000 | Loss: 0.00018203
Iteration 25/1000 | Loss: 0.00017109
Iteration 26/1000 | Loss: 0.00014580
Iteration 27/1000 | Loss: 0.00002941
Iteration 28/1000 | Loss: 0.00002747
Iteration 29/1000 | Loss: 0.00002549
Iteration 30/1000 | Loss: 0.00002410
Iteration 31/1000 | Loss: 0.00002315
Iteration 32/1000 | Loss: 0.00002274
Iteration 33/1000 | Loss: 0.00002248
Iteration 34/1000 | Loss: 0.00002246
Iteration 35/1000 | Loss: 0.00002238
Iteration 36/1000 | Loss: 0.00002231
Iteration 37/1000 | Loss: 0.00002225
Iteration 38/1000 | Loss: 0.00002223
Iteration 39/1000 | Loss: 0.00002223
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002214
Iteration 42/1000 | Loss: 0.00002210
Iteration 43/1000 | Loss: 0.00002210
Iteration 44/1000 | Loss: 0.00002209
Iteration 45/1000 | Loss: 0.00002208
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002200
Iteration 48/1000 | Loss: 0.00002199
Iteration 49/1000 | Loss: 0.00002198
Iteration 50/1000 | Loss: 0.00002198
Iteration 51/1000 | Loss: 0.00002198
Iteration 52/1000 | Loss: 0.00002198
Iteration 53/1000 | Loss: 0.00002198
Iteration 54/1000 | Loss: 0.00002198
Iteration 55/1000 | Loss: 0.00002198
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002198
Iteration 58/1000 | Loss: 0.00002197
Iteration 59/1000 | Loss: 0.00002197
Iteration 60/1000 | Loss: 0.00002197
Iteration 61/1000 | Loss: 0.00002197
Iteration 62/1000 | Loss: 0.00002196
Iteration 63/1000 | Loss: 0.00002196
Iteration 64/1000 | Loss: 0.00002195
Iteration 65/1000 | Loss: 0.00002195
Iteration 66/1000 | Loss: 0.00002195
Iteration 67/1000 | Loss: 0.00013278
Iteration 68/1000 | Loss: 0.00024489
Iteration 69/1000 | Loss: 0.00031314
Iteration 70/1000 | Loss: 0.00002219
Iteration 71/1000 | Loss: 0.00002196
Iteration 72/1000 | Loss: 0.00002195
Iteration 73/1000 | Loss: 0.00002192
Iteration 74/1000 | Loss: 0.00002192
Iteration 75/1000 | Loss: 0.00002191
Iteration 76/1000 | Loss: 0.00002190
Iteration 77/1000 | Loss: 0.00002190
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002189
Iteration 80/1000 | Loss: 0.00002189
Iteration 81/1000 | Loss: 0.00002188
Iteration 82/1000 | Loss: 0.00002188
Iteration 83/1000 | Loss: 0.00002187
Iteration 84/1000 | Loss: 0.00002187
Iteration 85/1000 | Loss: 0.00002187
Iteration 86/1000 | Loss: 0.00002187
Iteration 87/1000 | Loss: 0.00002187
Iteration 88/1000 | Loss: 0.00002187
Iteration 89/1000 | Loss: 0.00002187
Iteration 90/1000 | Loss: 0.00002186
Iteration 91/1000 | Loss: 0.00002185
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002184
Iteration 96/1000 | Loss: 0.00002184
Iteration 97/1000 | Loss: 0.00002184
Iteration 98/1000 | Loss: 0.00002183
Iteration 99/1000 | Loss: 0.00002183
Iteration 100/1000 | Loss: 0.00002182
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002181
Iteration 107/1000 | Loss: 0.00002181
Iteration 108/1000 | Loss: 0.00002181
Iteration 109/1000 | Loss: 0.00002181
Iteration 110/1000 | Loss: 0.00002181
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002181
Iteration 113/1000 | Loss: 0.00002181
Iteration 114/1000 | Loss: 0.00002181
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002180
Iteration 119/1000 | Loss: 0.00002180
Iteration 120/1000 | Loss: 0.00002180
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Iteration 124/1000 | Loss: 0.00002179
Iteration 125/1000 | Loss: 0.00002179
Iteration 126/1000 | Loss: 0.00002179
Iteration 127/1000 | Loss: 0.00002179
Iteration 128/1000 | Loss: 0.00002179
Iteration 129/1000 | Loss: 0.00002179
Iteration 130/1000 | Loss: 0.00002179
Iteration 131/1000 | Loss: 0.00002179
Iteration 132/1000 | Loss: 0.00002179
Iteration 133/1000 | Loss: 0.00002179
Iteration 134/1000 | Loss: 0.00002179
Iteration 135/1000 | Loss: 0.00002179
Iteration 136/1000 | Loss: 0.00002179
Iteration 137/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.179086732212454e-05, 2.179086732212454e-05, 2.179086732212454e-05, 2.179086732212454e-05, 2.179086732212454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.179086732212454e-05

Optimization complete. Final v2v error: 3.874818801879883 mm

Highest mean error: 6.559638977050781 mm for frame 35

Lowest mean error: 3.265299081802368 mm for frame 59

Saving results

Total time: 125.32468700408936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794388
Iteration 2/25 | Loss: 0.00235122
Iteration 3/25 | Loss: 0.00128120
Iteration 4/25 | Loss: 0.00104144
Iteration 5/25 | Loss: 0.00097828
Iteration 6/25 | Loss: 0.00092419
Iteration 7/25 | Loss: 0.00087905
Iteration 8/25 | Loss: 0.00080978
Iteration 9/25 | Loss: 0.00082456
Iteration 10/25 | Loss: 0.00082299
Iteration 11/25 | Loss: 0.00078227
Iteration 12/25 | Loss: 0.00075080
Iteration 13/25 | Loss: 0.00074081
Iteration 14/25 | Loss: 0.00074644
Iteration 15/25 | Loss: 0.00073952
Iteration 16/25 | Loss: 0.00073656
Iteration 17/25 | Loss: 0.00073360
Iteration 18/25 | Loss: 0.00073330
Iteration 19/25 | Loss: 0.00072978
Iteration 20/25 | Loss: 0.00072310
Iteration 21/25 | Loss: 0.00072427
Iteration 22/25 | Loss: 0.00072182
Iteration 23/25 | Loss: 0.00072206
Iteration 24/25 | Loss: 0.00072500
Iteration 25/25 | Loss: 0.00072143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.20879555
Iteration 2/25 | Loss: 0.00061878
Iteration 3/25 | Loss: 0.00061878
Iteration 4/25 | Loss: 0.00061878
Iteration 5/25 | Loss: 0.00061878
Iteration 6/25 | Loss: 0.00061878
Iteration 7/25 | Loss: 0.00061878
Iteration 8/25 | Loss: 0.00061878
Iteration 9/25 | Loss: 0.00061878
Iteration 10/25 | Loss: 0.00061878
Iteration 11/25 | Loss: 0.00061878
Iteration 12/25 | Loss: 0.00061878
Iteration 13/25 | Loss: 0.00061878
Iteration 14/25 | Loss: 0.00061878
Iteration 15/25 | Loss: 0.00061878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006187751423567533, 0.0006187751423567533, 0.0006187751423567533, 0.0006187751423567533, 0.0006187751423567533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006187751423567533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061878
Iteration 2/1000 | Loss: 0.00027322
Iteration 3/1000 | Loss: 0.00039005
Iteration 4/1000 | Loss: 0.00035077
Iteration 5/1000 | Loss: 0.00026522
Iteration 6/1000 | Loss: 0.00024757
Iteration 7/1000 | Loss: 0.00026176
Iteration 8/1000 | Loss: 0.00006750
Iteration 9/1000 | Loss: 0.00033339
Iteration 10/1000 | Loss: 0.00101110
Iteration 11/1000 | Loss: 0.00022752
Iteration 12/1000 | Loss: 0.00023373
Iteration 13/1000 | Loss: 0.00007865
Iteration 14/1000 | Loss: 0.00004271
Iteration 15/1000 | Loss: 0.00003226
Iteration 16/1000 | Loss: 0.00002757
Iteration 17/1000 | Loss: 0.00002464
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001752
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001700
Iteration 32/1000 | Loss: 0.00001699
Iteration 33/1000 | Loss: 0.00001699
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001688
Iteration 36/1000 | Loss: 0.00001688
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001678
Iteration 67/1000 | Loss: 0.00001678
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001678
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001678
Iteration 72/1000 | Loss: 0.00001678
Iteration 73/1000 | Loss: 0.00001678
Iteration 74/1000 | Loss: 0.00001678
Iteration 75/1000 | Loss: 0.00001678
Iteration 76/1000 | Loss: 0.00001678
Iteration 77/1000 | Loss: 0.00001678
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001677
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001676
Iteration 91/1000 | Loss: 0.00001676
Iteration 92/1000 | Loss: 0.00001676
Iteration 93/1000 | Loss: 0.00001675
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001675
Iteration 96/1000 | Loss: 0.00001675
Iteration 97/1000 | Loss: 0.00001675
Iteration 98/1000 | Loss: 0.00001675
Iteration 99/1000 | Loss: 0.00001675
Iteration 100/1000 | Loss: 0.00001675
Iteration 101/1000 | Loss: 0.00001674
Iteration 102/1000 | Loss: 0.00001674
Iteration 103/1000 | Loss: 0.00001674
Iteration 104/1000 | Loss: 0.00001674
Iteration 105/1000 | Loss: 0.00001674
Iteration 106/1000 | Loss: 0.00001674
Iteration 107/1000 | Loss: 0.00001674
Iteration 108/1000 | Loss: 0.00001674
Iteration 109/1000 | Loss: 0.00001674
Iteration 110/1000 | Loss: 0.00001674
Iteration 111/1000 | Loss: 0.00001674
Iteration 112/1000 | Loss: 0.00001674
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001674
Iteration 116/1000 | Loss: 0.00001674
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001673
Iteration 124/1000 | Loss: 0.00001673
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001672
Iteration 127/1000 | Loss: 0.00001672
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001671
Iteration 132/1000 | Loss: 0.00001671
Iteration 133/1000 | Loss: 0.00001671
Iteration 134/1000 | Loss: 0.00001671
Iteration 135/1000 | Loss: 0.00001671
Iteration 136/1000 | Loss: 0.00001671
Iteration 137/1000 | Loss: 0.00001671
Iteration 138/1000 | Loss: 0.00001671
Iteration 139/1000 | Loss: 0.00001671
Iteration 140/1000 | Loss: 0.00001671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.6714553566998802e-05, 1.6714553566998802e-05, 1.6714553566998802e-05, 1.6714553566998802e-05, 1.6714553566998802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6714553566998802e-05

Optimization complete. Final v2v error: 3.4222254753112793 mm

Highest mean error: 4.048516273498535 mm for frame 7

Lowest mean error: 2.9760162830352783 mm for frame 41

Saving results

Total time: 91.65759611129761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795199
Iteration 2/25 | Loss: 0.00140179
Iteration 3/25 | Loss: 0.00110183
Iteration 4/25 | Loss: 0.00104320
Iteration 5/25 | Loss: 0.00103315
Iteration 6/25 | Loss: 0.00103034
Iteration 7/25 | Loss: 0.00102998
Iteration 8/25 | Loss: 0.00102998
Iteration 9/25 | Loss: 0.00102998
Iteration 10/25 | Loss: 0.00102998
Iteration 11/25 | Loss: 0.00102998
Iteration 12/25 | Loss: 0.00102998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010299811838194728, 0.0010299811838194728, 0.0010299811838194728, 0.0010299811838194728, 0.0010299811838194728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010299811838194728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91485608
Iteration 2/25 | Loss: 0.00048482
Iteration 3/25 | Loss: 0.00048475
Iteration 4/25 | Loss: 0.00048475
Iteration 5/25 | Loss: 0.00048475
Iteration 6/25 | Loss: 0.00048475
Iteration 7/25 | Loss: 0.00048475
Iteration 8/25 | Loss: 0.00048475
Iteration 9/25 | Loss: 0.00048475
Iteration 10/25 | Loss: 0.00048475
Iteration 11/25 | Loss: 0.00048475
Iteration 12/25 | Loss: 0.00048474
Iteration 13/25 | Loss: 0.00048474
Iteration 14/25 | Loss: 0.00048474
Iteration 15/25 | Loss: 0.00048474
Iteration 16/25 | Loss: 0.00048474
Iteration 17/25 | Loss: 0.00048474
Iteration 18/25 | Loss: 0.00048475
Iteration 19/25 | Loss: 0.00048475
Iteration 20/25 | Loss: 0.00048474
Iteration 21/25 | Loss: 0.00048475
Iteration 22/25 | Loss: 0.00048475
Iteration 23/25 | Loss: 0.00048475
Iteration 24/25 | Loss: 0.00048475
Iteration 25/25 | Loss: 0.00048475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048475
Iteration 2/1000 | Loss: 0.00009869
Iteration 3/1000 | Loss: 0.00007618
Iteration 4/1000 | Loss: 0.00006728
Iteration 5/1000 | Loss: 0.00006390
Iteration 6/1000 | Loss: 0.00006122
Iteration 7/1000 | Loss: 0.00005957
Iteration 8/1000 | Loss: 0.00005866
Iteration 9/1000 | Loss: 0.00005792
Iteration 10/1000 | Loss: 0.00005743
Iteration 11/1000 | Loss: 0.00005702
Iteration 12/1000 | Loss: 0.00005674
Iteration 13/1000 | Loss: 0.00005656
Iteration 14/1000 | Loss: 0.00005626
Iteration 15/1000 | Loss: 0.00005621
Iteration 16/1000 | Loss: 0.00005604
Iteration 17/1000 | Loss: 0.00005602
Iteration 18/1000 | Loss: 0.00005593
Iteration 19/1000 | Loss: 0.00005587
Iteration 20/1000 | Loss: 0.00005573
Iteration 21/1000 | Loss: 0.00005553
Iteration 22/1000 | Loss: 0.00005542
Iteration 23/1000 | Loss: 0.00005527
Iteration 24/1000 | Loss: 0.00005517
Iteration 25/1000 | Loss: 0.00005509
Iteration 26/1000 | Loss: 0.00005496
Iteration 27/1000 | Loss: 0.00005490
Iteration 28/1000 | Loss: 0.00005489
Iteration 29/1000 | Loss: 0.00005485
Iteration 30/1000 | Loss: 0.00005485
Iteration 31/1000 | Loss: 0.00005485
Iteration 32/1000 | Loss: 0.00005485
Iteration 33/1000 | Loss: 0.00005485
Iteration 34/1000 | Loss: 0.00005485
Iteration 35/1000 | Loss: 0.00005484
Iteration 36/1000 | Loss: 0.00005484
Iteration 37/1000 | Loss: 0.00005482
Iteration 38/1000 | Loss: 0.00005482
Iteration 39/1000 | Loss: 0.00005482
Iteration 40/1000 | Loss: 0.00005481
Iteration 41/1000 | Loss: 0.00005481
Iteration 42/1000 | Loss: 0.00005481
Iteration 43/1000 | Loss: 0.00005481
Iteration 44/1000 | Loss: 0.00005481
Iteration 45/1000 | Loss: 0.00005481
Iteration 46/1000 | Loss: 0.00005481
Iteration 47/1000 | Loss: 0.00005481
Iteration 48/1000 | Loss: 0.00005480
Iteration 49/1000 | Loss: 0.00005480
Iteration 50/1000 | Loss: 0.00005480
Iteration 51/1000 | Loss: 0.00005480
Iteration 52/1000 | Loss: 0.00005479
Iteration 53/1000 | Loss: 0.00005479
Iteration 54/1000 | Loss: 0.00005479
Iteration 55/1000 | Loss: 0.00005479
Iteration 56/1000 | Loss: 0.00005479
Iteration 57/1000 | Loss: 0.00005479
Iteration 58/1000 | Loss: 0.00005479
Iteration 59/1000 | Loss: 0.00005479
Iteration 60/1000 | Loss: 0.00005479
Iteration 61/1000 | Loss: 0.00005479
Iteration 62/1000 | Loss: 0.00005479
Iteration 63/1000 | Loss: 0.00005479
Iteration 64/1000 | Loss: 0.00005479
Iteration 65/1000 | Loss: 0.00005479
Iteration 66/1000 | Loss: 0.00005479
Iteration 67/1000 | Loss: 0.00005479
Iteration 68/1000 | Loss: 0.00005479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [5.478589082485996e-05, 5.478589082485996e-05, 5.478589082485996e-05, 5.478589082485996e-05, 5.478589082485996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.478589082485996e-05

Optimization complete. Final v2v error: 5.8427348136901855 mm

Highest mean error: 8.000946044921875 mm for frame 120

Lowest mean error: 4.729257583618164 mm for frame 156

Saving results

Total time: 51.74354386329651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430822
Iteration 2/25 | Loss: 0.00076815
Iteration 3/25 | Loss: 0.00066582
Iteration 4/25 | Loss: 0.00064383
Iteration 5/25 | Loss: 0.00063541
Iteration 6/25 | Loss: 0.00063396
Iteration 7/25 | Loss: 0.00063373
Iteration 8/25 | Loss: 0.00063373
Iteration 9/25 | Loss: 0.00063373
Iteration 10/25 | Loss: 0.00063373
Iteration 11/25 | Loss: 0.00063373
Iteration 12/25 | Loss: 0.00063373
Iteration 13/25 | Loss: 0.00063373
Iteration 14/25 | Loss: 0.00063373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006337274680845439, 0.0006337274680845439, 0.0006337274680845439, 0.0006337274680845439, 0.0006337274680845439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006337274680845439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46595061
Iteration 2/25 | Loss: 0.00028001
Iteration 3/25 | Loss: 0.00028001
Iteration 4/25 | Loss: 0.00028001
Iteration 5/25 | Loss: 0.00028001
Iteration 6/25 | Loss: 0.00028001
Iteration 7/25 | Loss: 0.00028001
Iteration 8/25 | Loss: 0.00028001
Iteration 9/25 | Loss: 0.00028001
Iteration 10/25 | Loss: 0.00028001
Iteration 11/25 | Loss: 0.00028001
Iteration 12/25 | Loss: 0.00028001
Iteration 13/25 | Loss: 0.00028001
Iteration 14/25 | Loss: 0.00028001
Iteration 15/25 | Loss: 0.00028001
Iteration 16/25 | Loss: 0.00028001
Iteration 17/25 | Loss: 0.00028001
Iteration 18/25 | Loss: 0.00028001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00028000661404803395, 0.00028000661404803395, 0.00028000661404803395, 0.00028000661404803395, 0.00028000661404803395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028000661404803395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028001
Iteration 2/1000 | Loss: 0.00002354
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001613
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001513
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001470
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001450
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001434
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001430
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001427
Iteration 42/1000 | Loss: 0.00001427
Iteration 43/1000 | Loss: 0.00001427
Iteration 44/1000 | Loss: 0.00001427
Iteration 45/1000 | Loss: 0.00001427
Iteration 46/1000 | Loss: 0.00001427
Iteration 47/1000 | Loss: 0.00001427
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001427
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001427
Iteration 69/1000 | Loss: 0.00001427
Iteration 70/1000 | Loss: 0.00001427
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001427
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001427
Iteration 99/1000 | Loss: 0.00001427
Iteration 100/1000 | Loss: 0.00001427
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.4265427125792485e-05, 1.4265427125792485e-05, 1.4265427125792485e-05, 1.4265427125792485e-05, 1.4265427125792485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4265427125792485e-05

Optimization complete. Final v2v error: 3.237409830093384 mm

Highest mean error: 3.3495562076568604 mm for frame 97

Lowest mean error: 3.0945255756378174 mm for frame 191

Saving results

Total time: 28.410851001739502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823549
Iteration 2/25 | Loss: 0.00127115
Iteration 3/25 | Loss: 0.00086793
Iteration 4/25 | Loss: 0.00079375
Iteration 5/25 | Loss: 0.00077073
Iteration 6/25 | Loss: 0.00076731
Iteration 7/25 | Loss: 0.00076566
Iteration 8/25 | Loss: 0.00075601
Iteration 9/25 | Loss: 0.00073777
Iteration 10/25 | Loss: 0.00072749
Iteration 11/25 | Loss: 0.00072431
Iteration 12/25 | Loss: 0.00073142
Iteration 13/25 | Loss: 0.00072645
Iteration 14/25 | Loss: 0.00072021
Iteration 15/25 | Loss: 0.00071892
Iteration 16/25 | Loss: 0.00071847
Iteration 17/25 | Loss: 0.00071839
Iteration 18/25 | Loss: 0.00071835
Iteration 19/25 | Loss: 0.00071835
Iteration 20/25 | Loss: 0.00071834
Iteration 21/25 | Loss: 0.00071834
Iteration 22/25 | Loss: 0.00071834
Iteration 23/25 | Loss: 0.00071834
Iteration 24/25 | Loss: 0.00071834
Iteration 25/25 | Loss: 0.00071834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58417988
Iteration 2/25 | Loss: 0.00044792
Iteration 3/25 | Loss: 0.00044792
Iteration 4/25 | Loss: 0.00044792
Iteration 5/25 | Loss: 0.00044792
Iteration 6/25 | Loss: 0.00044792
Iteration 7/25 | Loss: 0.00044792
Iteration 8/25 | Loss: 0.00044792
Iteration 9/25 | Loss: 0.00044792
Iteration 10/25 | Loss: 0.00044792
Iteration 11/25 | Loss: 0.00044792
Iteration 12/25 | Loss: 0.00044792
Iteration 13/25 | Loss: 0.00044792
Iteration 14/25 | Loss: 0.00044792
Iteration 15/25 | Loss: 0.00044792
Iteration 16/25 | Loss: 0.00044792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00044792090193368495, 0.00044792090193368495, 0.00044792090193368495, 0.00044792090193368495, 0.00044792090193368495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044792090193368495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044792
Iteration 2/1000 | Loss: 0.00003493
Iteration 3/1000 | Loss: 0.00002696
Iteration 4/1000 | Loss: 0.00002506
Iteration 5/1000 | Loss: 0.00002383
Iteration 6/1000 | Loss: 0.00002321
Iteration 7/1000 | Loss: 0.00004066
Iteration 8/1000 | Loss: 0.00004069
Iteration 9/1000 | Loss: 0.00002220
Iteration 10/1000 | Loss: 0.00002199
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00003572
Iteration 13/1000 | Loss: 0.00016203
Iteration 14/1000 | Loss: 0.00006943
Iteration 15/1000 | Loss: 0.00002413
Iteration 16/1000 | Loss: 0.00004030
Iteration 17/1000 | Loss: 0.00002366
Iteration 18/1000 | Loss: 0.00002300
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001846
Iteration 22/1000 | Loss: 0.00001819
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00004229
Iteration 26/1000 | Loss: 0.00002028
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00004177
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001762
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001762
Iteration 38/1000 | Loss: 0.00001761
Iteration 39/1000 | Loss: 0.00001761
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00002744
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00002016
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.7578553524799645e-05, 1.7578553524799645e-05, 1.7578553524799645e-05, 1.7578553524799645e-05, 1.7578553524799645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7578553524799645e-05

Optimization complete. Final v2v error: 3.585622787475586 mm

Highest mean error: 4.301339149475098 mm for frame 104

Lowest mean error: 3.136461019515991 mm for frame 93

Saving results

Total time: 89.44861578941345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842533
Iteration 2/25 | Loss: 0.00098891
Iteration 3/25 | Loss: 0.00076980
Iteration 4/25 | Loss: 0.00072580
Iteration 5/25 | Loss: 0.00071097
Iteration 6/25 | Loss: 0.00070877
Iteration 7/25 | Loss: 0.00070847
Iteration 8/25 | Loss: 0.00070847
Iteration 9/25 | Loss: 0.00070847
Iteration 10/25 | Loss: 0.00070847
Iteration 11/25 | Loss: 0.00070847
Iteration 12/25 | Loss: 0.00070847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007084700628183782, 0.0007084700628183782, 0.0007084700628183782, 0.0007084700628183782, 0.0007084700628183782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007084700628183782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33234334
Iteration 2/25 | Loss: 0.00030567
Iteration 3/25 | Loss: 0.00030563
Iteration 4/25 | Loss: 0.00030563
Iteration 5/25 | Loss: 0.00030563
Iteration 6/25 | Loss: 0.00030563
Iteration 7/25 | Loss: 0.00030563
Iteration 8/25 | Loss: 0.00030563
Iteration 9/25 | Loss: 0.00030563
Iteration 10/25 | Loss: 0.00030563
Iteration 11/25 | Loss: 0.00030563
Iteration 12/25 | Loss: 0.00030563
Iteration 13/25 | Loss: 0.00030563
Iteration 14/25 | Loss: 0.00030563
Iteration 15/25 | Loss: 0.00030563
Iteration 16/25 | Loss: 0.00030563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003056312270928174, 0.0003056312270928174, 0.0003056312270928174, 0.0003056312270928174, 0.0003056312270928174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003056312270928174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030563
Iteration 2/1000 | Loss: 0.00003863
Iteration 3/1000 | Loss: 0.00002883
Iteration 4/1000 | Loss: 0.00002591
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002380
Iteration 7/1000 | Loss: 0.00002311
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002228
Iteration 10/1000 | Loss: 0.00002206
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002192
Iteration 13/1000 | Loss: 0.00002188
Iteration 14/1000 | Loss: 0.00002186
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002162
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002158
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002157
Iteration 25/1000 | Loss: 0.00002157
Iteration 26/1000 | Loss: 0.00002155
Iteration 27/1000 | Loss: 0.00002155
Iteration 28/1000 | Loss: 0.00002154
Iteration 29/1000 | Loss: 0.00002154
Iteration 30/1000 | Loss: 0.00002153
Iteration 31/1000 | Loss: 0.00002153
Iteration 32/1000 | Loss: 0.00002153
Iteration 33/1000 | Loss: 0.00002152
Iteration 34/1000 | Loss: 0.00002151
Iteration 35/1000 | Loss: 0.00002151
Iteration 36/1000 | Loss: 0.00002150
Iteration 37/1000 | Loss: 0.00002145
Iteration 38/1000 | Loss: 0.00002145
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002145
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002142
Iteration 43/1000 | Loss: 0.00002142
Iteration 44/1000 | Loss: 0.00002141
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002141
Iteration 47/1000 | Loss: 0.00002141
Iteration 48/1000 | Loss: 0.00002140
Iteration 49/1000 | Loss: 0.00002140
Iteration 50/1000 | Loss: 0.00002140
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002139
Iteration 53/1000 | Loss: 0.00002139
Iteration 54/1000 | Loss: 0.00002139
Iteration 55/1000 | Loss: 0.00002139
Iteration 56/1000 | Loss: 0.00002139
Iteration 57/1000 | Loss: 0.00002139
Iteration 58/1000 | Loss: 0.00002139
Iteration 59/1000 | Loss: 0.00002139
Iteration 60/1000 | Loss: 0.00002139
Iteration 61/1000 | Loss: 0.00002139
Iteration 62/1000 | Loss: 0.00002138
Iteration 63/1000 | Loss: 0.00002138
Iteration 64/1000 | Loss: 0.00002138
Iteration 65/1000 | Loss: 0.00002138
Iteration 66/1000 | Loss: 0.00002138
Iteration 67/1000 | Loss: 0.00002138
Iteration 68/1000 | Loss: 0.00002138
Iteration 69/1000 | Loss: 0.00002138
Iteration 70/1000 | Loss: 0.00002138
Iteration 71/1000 | Loss: 0.00002138
Iteration 72/1000 | Loss: 0.00002138
Iteration 73/1000 | Loss: 0.00002138
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002138
Iteration 76/1000 | Loss: 0.00002138
Iteration 77/1000 | Loss: 0.00002138
Iteration 78/1000 | Loss: 0.00002138
Iteration 79/1000 | Loss: 0.00002138
Iteration 80/1000 | Loss: 0.00002138
Iteration 81/1000 | Loss: 0.00002138
Iteration 82/1000 | Loss: 0.00002138
Iteration 83/1000 | Loss: 0.00002138
Iteration 84/1000 | Loss: 0.00002138
Iteration 85/1000 | Loss: 0.00002138
Iteration 86/1000 | Loss: 0.00002138
Iteration 87/1000 | Loss: 0.00002138
Iteration 88/1000 | Loss: 0.00002138
Iteration 89/1000 | Loss: 0.00002138
Iteration 90/1000 | Loss: 0.00002138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.1381390979513526e-05, 2.1381390979513526e-05, 2.1381390979513526e-05, 2.1381390979513526e-05, 2.1381390979513526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1381390979513526e-05

Optimization complete. Final v2v error: 3.9098029136657715 mm

Highest mean error: 4.682822227478027 mm for frame 62

Lowest mean error: 3.432171106338501 mm for frame 239

Saving results

Total time: 37.994770526885986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044262
Iteration 2/25 | Loss: 0.00198715
Iteration 3/25 | Loss: 0.00165307
Iteration 4/25 | Loss: 0.00164207
Iteration 5/25 | Loss: 0.00118302
Iteration 6/25 | Loss: 0.00132509
Iteration 7/25 | Loss: 0.00104028
Iteration 8/25 | Loss: 0.00081926
Iteration 9/25 | Loss: 0.00077189
Iteration 10/25 | Loss: 0.00076313
Iteration 11/25 | Loss: 0.00076102
Iteration 12/25 | Loss: 0.00076087
Iteration 13/25 | Loss: 0.00076087
Iteration 14/25 | Loss: 0.00076087
Iteration 15/25 | Loss: 0.00076087
Iteration 16/25 | Loss: 0.00076087
Iteration 17/25 | Loss: 0.00076087
Iteration 18/25 | Loss: 0.00076087
Iteration 19/25 | Loss: 0.00076087
Iteration 20/25 | Loss: 0.00076087
Iteration 21/25 | Loss: 0.00076087
Iteration 22/25 | Loss: 0.00076087
Iteration 23/25 | Loss: 0.00076087
Iteration 24/25 | Loss: 0.00076087
Iteration 25/25 | Loss: 0.00076087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007608715677633882, 0.0007608715677633882, 0.0007608715677633882, 0.0007608715677633882, 0.0007608715677633882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007608715677633882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45535982
Iteration 2/25 | Loss: 0.00027289
Iteration 3/25 | Loss: 0.00027288
Iteration 4/25 | Loss: 0.00027288
Iteration 5/25 | Loss: 0.00027288
Iteration 6/25 | Loss: 0.00027288
Iteration 7/25 | Loss: 0.00027288
Iteration 8/25 | Loss: 0.00027288
Iteration 9/25 | Loss: 0.00027288
Iteration 10/25 | Loss: 0.00027288
Iteration 11/25 | Loss: 0.00027288
Iteration 12/25 | Loss: 0.00027288
Iteration 13/25 | Loss: 0.00027288
Iteration 14/25 | Loss: 0.00027288
Iteration 15/25 | Loss: 0.00027288
Iteration 16/25 | Loss: 0.00027288
Iteration 17/25 | Loss: 0.00027288
Iteration 18/25 | Loss: 0.00027288
Iteration 19/25 | Loss: 0.00027288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00027288225828669965, 0.00027288225828669965, 0.00027288225828669965, 0.00027288225828669965, 0.00027288225828669965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027288225828669965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027288
Iteration 2/1000 | Loss: 0.00003139
Iteration 3/1000 | Loss: 0.00002344
Iteration 4/1000 | Loss: 0.00002181
Iteration 5/1000 | Loss: 0.00002083
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001975
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001965
Iteration 12/1000 | Loss: 0.00001964
Iteration 13/1000 | Loss: 0.00001963
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001946
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001946
Iteration 26/1000 | Loss: 0.00001944
Iteration 27/1000 | Loss: 0.00001944
Iteration 28/1000 | Loss: 0.00001944
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001944
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001942
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001940
Iteration 44/1000 | Loss: 0.00001940
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001939
Iteration 48/1000 | Loss: 0.00001939
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001935
Iteration 64/1000 | Loss: 0.00001935
Iteration 65/1000 | Loss: 0.00001935
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001934
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001934
Iteration 83/1000 | Loss: 0.00001934
Iteration 84/1000 | Loss: 0.00001934
Iteration 85/1000 | Loss: 0.00001934
Iteration 86/1000 | Loss: 0.00001934
Iteration 87/1000 | Loss: 0.00001934
Iteration 88/1000 | Loss: 0.00001934
Iteration 89/1000 | Loss: 0.00001934
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.934401734615676e-05, 1.934401734615676e-05, 1.934401734615676e-05, 1.934401734615676e-05, 1.934401734615676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.934401734615676e-05

Optimization complete. Final v2v error: 3.712242603302002 mm

Highest mean error: 3.8249144554138184 mm for frame 212

Lowest mean error: 3.5431671142578125 mm for frame 237

Saving results

Total time: 42.897544860839844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481958
Iteration 2/25 | Loss: 0.00075990
Iteration 3/25 | Loss: 0.00062896
Iteration 4/25 | Loss: 0.00060685
Iteration 5/25 | Loss: 0.00059990
Iteration 6/25 | Loss: 0.00059832
Iteration 7/25 | Loss: 0.00059788
Iteration 8/25 | Loss: 0.00059788
Iteration 9/25 | Loss: 0.00059788
Iteration 10/25 | Loss: 0.00059788
Iteration 11/25 | Loss: 0.00059788
Iteration 12/25 | Loss: 0.00059788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005978760891593993, 0.0005978760891593993, 0.0005978760891593993, 0.0005978760891593993, 0.0005978760891593993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005978760891593993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.68405247
Iteration 2/25 | Loss: 0.00026685
Iteration 3/25 | Loss: 0.00026684
Iteration 4/25 | Loss: 0.00026684
Iteration 5/25 | Loss: 0.00026684
Iteration 6/25 | Loss: 0.00026684
Iteration 7/25 | Loss: 0.00026684
Iteration 8/25 | Loss: 0.00026684
Iteration 9/25 | Loss: 0.00026684
Iteration 10/25 | Loss: 0.00026684
Iteration 11/25 | Loss: 0.00026684
Iteration 12/25 | Loss: 0.00026684
Iteration 13/25 | Loss: 0.00026684
Iteration 14/25 | Loss: 0.00026684
Iteration 15/25 | Loss: 0.00026684
Iteration 16/25 | Loss: 0.00026684
Iteration 17/25 | Loss: 0.00026684
Iteration 18/25 | Loss: 0.00026684
Iteration 19/25 | Loss: 0.00026684
Iteration 20/25 | Loss: 0.00026684
Iteration 21/25 | Loss: 0.00026684
Iteration 22/25 | Loss: 0.00026684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002668390516191721, 0.0002668390516191721, 0.0002668390516191721, 0.0002668390516191721, 0.0002668390516191721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002668390516191721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026684
Iteration 2/1000 | Loss: 0.00002375
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001461
Iteration 6/1000 | Loss: 0.00001419
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001366
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001350
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001346
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001322
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001310
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001307
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001306
Iteration 50/1000 | Loss: 0.00001306
Iteration 51/1000 | Loss: 0.00001306
Iteration 52/1000 | Loss: 0.00001306
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001290
Iteration 130/1000 | Loss: 0.00001290
Iteration 131/1000 | Loss: 0.00001290
Iteration 132/1000 | Loss: 0.00001290
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001289
Iteration 151/1000 | Loss: 0.00001289
Iteration 152/1000 | Loss: 0.00001289
Iteration 153/1000 | Loss: 0.00001289
Iteration 154/1000 | Loss: 0.00001289
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001288
Iteration 166/1000 | Loss: 0.00001288
Iteration 167/1000 | Loss: 0.00001288
Iteration 168/1000 | Loss: 0.00001288
Iteration 169/1000 | Loss: 0.00001288
Iteration 170/1000 | Loss: 0.00001288
Iteration 171/1000 | Loss: 0.00001288
Iteration 172/1000 | Loss: 0.00001288
Iteration 173/1000 | Loss: 0.00001288
Iteration 174/1000 | Loss: 0.00001288
Iteration 175/1000 | Loss: 0.00001288
Iteration 176/1000 | Loss: 0.00001287
Iteration 177/1000 | Loss: 0.00001287
Iteration 178/1000 | Loss: 0.00001287
Iteration 179/1000 | Loss: 0.00001287
Iteration 180/1000 | Loss: 0.00001287
Iteration 181/1000 | Loss: 0.00001287
Iteration 182/1000 | Loss: 0.00001287
Iteration 183/1000 | Loss: 0.00001287
Iteration 184/1000 | Loss: 0.00001287
Iteration 185/1000 | Loss: 0.00001287
Iteration 186/1000 | Loss: 0.00001287
Iteration 187/1000 | Loss: 0.00001287
Iteration 188/1000 | Loss: 0.00001287
Iteration 189/1000 | Loss: 0.00001286
Iteration 190/1000 | Loss: 0.00001286
Iteration 191/1000 | Loss: 0.00001286
Iteration 192/1000 | Loss: 0.00001286
Iteration 193/1000 | Loss: 0.00001286
Iteration 194/1000 | Loss: 0.00001286
Iteration 195/1000 | Loss: 0.00001286
Iteration 196/1000 | Loss: 0.00001286
Iteration 197/1000 | Loss: 0.00001286
Iteration 198/1000 | Loss: 0.00001286
Iteration 199/1000 | Loss: 0.00001286
Iteration 200/1000 | Loss: 0.00001286
Iteration 201/1000 | Loss: 0.00001286
Iteration 202/1000 | Loss: 0.00001286
Iteration 203/1000 | Loss: 0.00001286
Iteration 204/1000 | Loss: 0.00001286
Iteration 205/1000 | Loss: 0.00001286
Iteration 206/1000 | Loss: 0.00001285
Iteration 207/1000 | Loss: 0.00001285
Iteration 208/1000 | Loss: 0.00001285
Iteration 209/1000 | Loss: 0.00001285
Iteration 210/1000 | Loss: 0.00001285
Iteration 211/1000 | Loss: 0.00001285
Iteration 212/1000 | Loss: 0.00001285
Iteration 213/1000 | Loss: 0.00001285
Iteration 214/1000 | Loss: 0.00001285
Iteration 215/1000 | Loss: 0.00001285
Iteration 216/1000 | Loss: 0.00001285
Iteration 217/1000 | Loss: 0.00001285
Iteration 218/1000 | Loss: 0.00001285
Iteration 219/1000 | Loss: 0.00001285
Iteration 220/1000 | Loss: 0.00001285
Iteration 221/1000 | Loss: 0.00001285
Iteration 222/1000 | Loss: 0.00001285
Iteration 223/1000 | Loss: 0.00001285
Iteration 224/1000 | Loss: 0.00001285
Iteration 225/1000 | Loss: 0.00001285
Iteration 226/1000 | Loss: 0.00001285
Iteration 227/1000 | Loss: 0.00001285
Iteration 228/1000 | Loss: 0.00001285
Iteration 229/1000 | Loss: 0.00001285
Iteration 230/1000 | Loss: 0.00001285
Iteration 231/1000 | Loss: 0.00001285
Iteration 232/1000 | Loss: 0.00001285
Iteration 233/1000 | Loss: 0.00001285
Iteration 234/1000 | Loss: 0.00001285
Iteration 235/1000 | Loss: 0.00001285
Iteration 236/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.2851128303736914e-05, 1.2851128303736914e-05, 1.2851128303736914e-05, 1.2851128303736914e-05, 1.2851128303736914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2851128303736914e-05

Optimization complete. Final v2v error: 3.0704410076141357 mm

Highest mean error: 3.489732503890991 mm for frame 62

Lowest mean error: 2.8520257472991943 mm for frame 132

Saving results

Total time: 38.81219816207886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851287
Iteration 2/25 | Loss: 0.00109634
Iteration 3/25 | Loss: 0.00075624
Iteration 4/25 | Loss: 0.00071169
Iteration 5/25 | Loss: 0.00070526
Iteration 6/25 | Loss: 0.00070434
Iteration 7/25 | Loss: 0.00070433
Iteration 8/25 | Loss: 0.00070433
Iteration 9/25 | Loss: 0.00070433
Iteration 10/25 | Loss: 0.00070433
Iteration 11/25 | Loss: 0.00070433
Iteration 12/25 | Loss: 0.00070433
Iteration 13/25 | Loss: 0.00070433
Iteration 14/25 | Loss: 0.00070433
Iteration 15/25 | Loss: 0.00070433
Iteration 16/25 | Loss: 0.00070433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007043307414278388, 0.0007043307414278388, 0.0007043307414278388, 0.0007043307414278388, 0.0007043307414278388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007043307414278388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04008663
Iteration 2/25 | Loss: 0.00026272
Iteration 3/25 | Loss: 0.00026271
Iteration 4/25 | Loss: 0.00026271
Iteration 5/25 | Loss: 0.00026271
Iteration 6/25 | Loss: 0.00026271
Iteration 7/25 | Loss: 0.00026271
Iteration 8/25 | Loss: 0.00026270
Iteration 9/25 | Loss: 0.00026270
Iteration 10/25 | Loss: 0.00026270
Iteration 11/25 | Loss: 0.00026270
Iteration 12/25 | Loss: 0.00026270
Iteration 13/25 | Loss: 0.00026270
Iteration 14/25 | Loss: 0.00026270
Iteration 15/25 | Loss: 0.00026270
Iteration 16/25 | Loss: 0.00026270
Iteration 17/25 | Loss: 0.00026270
Iteration 18/25 | Loss: 0.00026270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00026270412490703166, 0.00026270412490703166, 0.00026270412490703166, 0.00026270412490703166, 0.00026270412490703166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026270412490703166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026270
Iteration 2/1000 | Loss: 0.00002969
Iteration 3/1000 | Loss: 0.00002509
Iteration 4/1000 | Loss: 0.00002392
Iteration 5/1000 | Loss: 0.00002277
Iteration 6/1000 | Loss: 0.00002202
Iteration 7/1000 | Loss: 0.00002157
Iteration 8/1000 | Loss: 0.00002132
Iteration 9/1000 | Loss: 0.00002122
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002107
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002103
Iteration 15/1000 | Loss: 0.00002103
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002103
Iteration 18/1000 | Loss: 0.00002103
Iteration 19/1000 | Loss: 0.00002098
Iteration 20/1000 | Loss: 0.00002096
Iteration 21/1000 | Loss: 0.00002095
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002095
Iteration 25/1000 | Loss: 0.00002095
Iteration 26/1000 | Loss: 0.00002095
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002094
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002094
Iteration 32/1000 | Loss: 0.00002094
Iteration 33/1000 | Loss: 0.00002093
Iteration 34/1000 | Loss: 0.00002093
Iteration 35/1000 | Loss: 0.00002092
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002091
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002090
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002089
Iteration 45/1000 | Loss: 0.00002089
Iteration 46/1000 | Loss: 0.00002089
Iteration 47/1000 | Loss: 0.00002089
Iteration 48/1000 | Loss: 0.00002089
Iteration 49/1000 | Loss: 0.00002089
Iteration 50/1000 | Loss: 0.00002089
Iteration 51/1000 | Loss: 0.00002088
Iteration 52/1000 | Loss: 0.00002088
Iteration 53/1000 | Loss: 0.00002088
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002088
Iteration 56/1000 | Loss: 0.00002087
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002087
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002084
Iteration 87/1000 | Loss: 0.00002084
Iteration 88/1000 | Loss: 0.00002084
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002084
Iteration 94/1000 | Loss: 0.00002084
Iteration 95/1000 | Loss: 0.00002084
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Iteration 98/1000 | Loss: 0.00002084
Iteration 99/1000 | Loss: 0.00002084
Iteration 100/1000 | Loss: 0.00002084
Iteration 101/1000 | Loss: 0.00002084
Iteration 102/1000 | Loss: 0.00002084
Iteration 103/1000 | Loss: 0.00002084
Iteration 104/1000 | Loss: 0.00002084
Iteration 105/1000 | Loss: 0.00002084
Iteration 106/1000 | Loss: 0.00002084
Iteration 107/1000 | Loss: 0.00002084
Iteration 108/1000 | Loss: 0.00002084
Iteration 109/1000 | Loss: 0.00002084
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002084
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002084
Iteration 117/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.084005427605007e-05, 2.084005427605007e-05, 2.084005427605007e-05, 2.084005427605007e-05, 2.084005427605007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.084005427605007e-05

Optimization complete. Final v2v error: 3.9007787704467773 mm

Highest mean error: 4.088022232055664 mm for frame 108

Lowest mean error: 3.6111645698547363 mm for frame 44

Saving results

Total time: 29.943569898605347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116251
Iteration 2/25 | Loss: 0.00417865
Iteration 3/25 | Loss: 0.00179636
Iteration 4/25 | Loss: 0.00150075
Iteration 5/25 | Loss: 0.00148408
Iteration 6/25 | Loss: 0.00188769
Iteration 7/25 | Loss: 0.00173315
Iteration 8/25 | Loss: 0.00142816
Iteration 9/25 | Loss: 0.00124920
Iteration 10/25 | Loss: 0.00115595
Iteration 11/25 | Loss: 0.00107652
Iteration 12/25 | Loss: 0.00101806
Iteration 13/25 | Loss: 0.00096791
Iteration 14/25 | Loss: 0.00092078
Iteration 15/25 | Loss: 0.00090306
Iteration 16/25 | Loss: 0.00090533
Iteration 17/25 | Loss: 0.00087960
Iteration 18/25 | Loss: 0.00084572
Iteration 19/25 | Loss: 0.00082192
Iteration 20/25 | Loss: 0.00082231
Iteration 21/25 | Loss: 0.00082185
Iteration 22/25 | Loss: 0.00080660
Iteration 23/25 | Loss: 0.00079065
Iteration 24/25 | Loss: 0.00078304
Iteration 25/25 | Loss: 0.00077673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06058621
Iteration 2/25 | Loss: 0.00214243
Iteration 3/25 | Loss: 0.00123669
Iteration 4/25 | Loss: 0.00123669
Iteration 5/25 | Loss: 0.00123669
Iteration 6/25 | Loss: 0.00123669
Iteration 7/25 | Loss: 0.00123669
Iteration 8/25 | Loss: 0.00123669
Iteration 9/25 | Loss: 0.00123669
Iteration 10/25 | Loss: 0.00123669
Iteration 11/25 | Loss: 0.00123669
Iteration 12/25 | Loss: 0.00123669
Iteration 13/25 | Loss: 0.00123669
Iteration 14/25 | Loss: 0.00123669
Iteration 15/25 | Loss: 0.00123669
Iteration 16/25 | Loss: 0.00123669
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012366864830255508, 0.0012366864830255508, 0.0012366864830255508, 0.0012366864830255508, 0.0012366864830255508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012366864830255508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123669
Iteration 2/1000 | Loss: 0.00110348
Iteration 3/1000 | Loss: 0.00044144
Iteration 4/1000 | Loss: 0.00027053
Iteration 5/1000 | Loss: 0.00041257
Iteration 6/1000 | Loss: 0.00078256
Iteration 7/1000 | Loss: 0.00072463
Iteration 8/1000 | Loss: 0.00192830
Iteration 9/1000 | Loss: 0.00146847
Iteration 10/1000 | Loss: 0.00178716
Iteration 11/1000 | Loss: 0.00209030
Iteration 12/1000 | Loss: 0.00132692
Iteration 13/1000 | Loss: 0.00248824
Iteration 14/1000 | Loss: 0.00019981
Iteration 15/1000 | Loss: 0.00016763
Iteration 16/1000 | Loss: 0.00096933
Iteration 17/1000 | Loss: 0.00010193
Iteration 18/1000 | Loss: 0.00011912
Iteration 19/1000 | Loss: 0.00014109
Iteration 20/1000 | Loss: 0.00011338
Iteration 21/1000 | Loss: 0.00011862
Iteration 22/1000 | Loss: 0.00013360
Iteration 23/1000 | Loss: 0.00011040
Iteration 24/1000 | Loss: 0.00010245
Iteration 25/1000 | Loss: 0.00011105
Iteration 26/1000 | Loss: 0.00014062
Iteration 27/1000 | Loss: 0.00013546
Iteration 28/1000 | Loss: 0.00016290
Iteration 29/1000 | Loss: 0.00021789
Iteration 30/1000 | Loss: 0.00021393
Iteration 31/1000 | Loss: 0.00025634
Iteration 32/1000 | Loss: 0.00026325
Iteration 33/1000 | Loss: 0.00010236
Iteration 34/1000 | Loss: 0.00051536
Iteration 35/1000 | Loss: 0.00007577
Iteration 36/1000 | Loss: 0.00005193
Iteration 37/1000 | Loss: 0.00023541
Iteration 38/1000 | Loss: 0.00009396
Iteration 39/1000 | Loss: 0.00007895
Iteration 40/1000 | Loss: 0.00006194
Iteration 41/1000 | Loss: 0.00023462
Iteration 42/1000 | Loss: 0.00049392
Iteration 43/1000 | Loss: 0.00009070
Iteration 44/1000 | Loss: 0.00008487
Iteration 45/1000 | Loss: 0.00035054
Iteration 46/1000 | Loss: 0.00034010
Iteration 47/1000 | Loss: 0.00036400
Iteration 48/1000 | Loss: 0.00037312
Iteration 49/1000 | Loss: 0.00028207
Iteration 50/1000 | Loss: 0.00013532
Iteration 51/1000 | Loss: 0.00026827
Iteration 52/1000 | Loss: 0.00006867
Iteration 53/1000 | Loss: 0.00007852
Iteration 54/1000 | Loss: 0.00032191
Iteration 55/1000 | Loss: 0.00030573
Iteration 56/1000 | Loss: 0.00030860
Iteration 57/1000 | Loss: 0.00042467
Iteration 58/1000 | Loss: 0.00022608
Iteration 59/1000 | Loss: 0.00006605
Iteration 60/1000 | Loss: 0.00024500
Iteration 61/1000 | Loss: 0.00025826
Iteration 62/1000 | Loss: 0.00018231
Iteration 63/1000 | Loss: 0.00007499
Iteration 64/1000 | Loss: 0.00006883
Iteration 65/1000 | Loss: 0.00010138
Iteration 66/1000 | Loss: 0.00008427
Iteration 67/1000 | Loss: 0.00008425
Iteration 68/1000 | Loss: 0.00008283
Iteration 69/1000 | Loss: 0.00024685
Iteration 70/1000 | Loss: 0.00008736
Iteration 71/1000 | Loss: 0.00009138
Iteration 72/1000 | Loss: 0.00022345
Iteration 73/1000 | Loss: 0.00005888
Iteration 74/1000 | Loss: 0.00023121
Iteration 75/1000 | Loss: 0.00040871
Iteration 76/1000 | Loss: 0.00038129
Iteration 77/1000 | Loss: 0.00011953
Iteration 78/1000 | Loss: 0.00009188
Iteration 79/1000 | Loss: 0.00008915
Iteration 80/1000 | Loss: 0.00010147
Iteration 81/1000 | Loss: 0.00008758
Iteration 82/1000 | Loss: 0.00017168
Iteration 83/1000 | Loss: 0.00008780
Iteration 84/1000 | Loss: 0.00009634
Iteration 85/1000 | Loss: 0.00009357
Iteration 86/1000 | Loss: 0.00008527
Iteration 87/1000 | Loss: 0.00009192
Iteration 88/1000 | Loss: 0.00004081
Iteration 89/1000 | Loss: 0.00006789
Iteration 90/1000 | Loss: 0.00005586
Iteration 91/1000 | Loss: 0.00004269
Iteration 92/1000 | Loss: 0.00004779
Iteration 93/1000 | Loss: 0.00006005
Iteration 94/1000 | Loss: 0.00006150
Iteration 95/1000 | Loss: 0.00007888
Iteration 96/1000 | Loss: 0.00007110
Iteration 97/1000 | Loss: 0.00006729
Iteration 98/1000 | Loss: 0.00006419
Iteration 99/1000 | Loss: 0.00006718
Iteration 100/1000 | Loss: 0.00006507
Iteration 101/1000 | Loss: 0.00006726
Iteration 102/1000 | Loss: 0.00006204
Iteration 103/1000 | Loss: 0.00008781
Iteration 104/1000 | Loss: 0.00007111
Iteration 105/1000 | Loss: 0.00005500
Iteration 106/1000 | Loss: 0.00006356
Iteration 107/1000 | Loss: 0.00005580
Iteration 108/1000 | Loss: 0.00005370
Iteration 109/1000 | Loss: 0.00005020
Iteration 110/1000 | Loss: 0.00003715
Iteration 111/1000 | Loss: 0.00020272
Iteration 112/1000 | Loss: 0.00010032
Iteration 113/1000 | Loss: 0.00007677
Iteration 114/1000 | Loss: 0.00019449
Iteration 115/1000 | Loss: 0.00020031
Iteration 116/1000 | Loss: 0.00017518
Iteration 117/1000 | Loss: 0.00023134
Iteration 118/1000 | Loss: 0.00007211
Iteration 119/1000 | Loss: 0.00005285
Iteration 120/1000 | Loss: 0.00005362
Iteration 121/1000 | Loss: 0.00006741
Iteration 122/1000 | Loss: 0.00005911
Iteration 123/1000 | Loss: 0.00006419
Iteration 124/1000 | Loss: 0.00004501
Iteration 125/1000 | Loss: 0.00005021
Iteration 126/1000 | Loss: 0.00006691
Iteration 127/1000 | Loss: 0.00006906
Iteration 128/1000 | Loss: 0.00005281
Iteration 129/1000 | Loss: 0.00005297
Iteration 130/1000 | Loss: 0.00005060
Iteration 131/1000 | Loss: 0.00005398
Iteration 132/1000 | Loss: 0.00020326
Iteration 133/1000 | Loss: 0.00008026
Iteration 134/1000 | Loss: 0.00005521
Iteration 135/1000 | Loss: 0.00004195
Iteration 136/1000 | Loss: 0.00006222
Iteration 137/1000 | Loss: 0.00004920
Iteration 138/1000 | Loss: 0.00003975
Iteration 139/1000 | Loss: 0.00007012
Iteration 140/1000 | Loss: 0.00006119
Iteration 141/1000 | Loss: 0.00005534
Iteration 142/1000 | Loss: 0.00025443
Iteration 143/1000 | Loss: 0.00007656
Iteration 144/1000 | Loss: 0.00004719
Iteration 145/1000 | Loss: 0.00005292
Iteration 146/1000 | Loss: 0.00018624
Iteration 147/1000 | Loss: 0.00003844
Iteration 148/1000 | Loss: 0.00006099
Iteration 149/1000 | Loss: 0.00005038
Iteration 150/1000 | Loss: 0.00006058
Iteration 151/1000 | Loss: 0.00025325
Iteration 152/1000 | Loss: 0.00006146
Iteration 153/1000 | Loss: 0.00006032
Iteration 154/1000 | Loss: 0.00005799
Iteration 155/1000 | Loss: 0.00006185
Iteration 156/1000 | Loss: 0.00006885
Iteration 157/1000 | Loss: 0.00007209
Iteration 158/1000 | Loss: 0.00007554
Iteration 159/1000 | Loss: 0.00006999
Iteration 160/1000 | Loss: 0.00005657
Iteration 161/1000 | Loss: 0.00004991
Iteration 162/1000 | Loss: 0.00004643
Iteration 163/1000 | Loss: 0.00003700
Iteration 164/1000 | Loss: 0.00004151
Iteration 165/1000 | Loss: 0.00006881
Iteration 166/1000 | Loss: 0.00005306
Iteration 167/1000 | Loss: 0.00005465
Iteration 168/1000 | Loss: 0.00004686
Iteration 169/1000 | Loss: 0.00004297
Iteration 170/1000 | Loss: 0.00005286
Iteration 171/1000 | Loss: 0.00005741
Iteration 172/1000 | Loss: 0.00005334
Iteration 173/1000 | Loss: 0.00005850
Iteration 174/1000 | Loss: 0.00003449
Iteration 175/1000 | Loss: 0.00004678
Iteration 176/1000 | Loss: 0.00004698
Iteration 177/1000 | Loss: 0.00004085
Iteration 178/1000 | Loss: 0.00004280
Iteration 179/1000 | Loss: 0.00004075
Iteration 180/1000 | Loss: 0.00003947
Iteration 181/1000 | Loss: 0.00004005
Iteration 182/1000 | Loss: 0.00003897
Iteration 183/1000 | Loss: 0.00004254
Iteration 184/1000 | Loss: 0.00003815
Iteration 185/1000 | Loss: 0.00002622
Iteration 186/1000 | Loss: 0.00002705
Iteration 187/1000 | Loss: 0.00003124
Iteration 188/1000 | Loss: 0.00002556
Iteration 189/1000 | Loss: 0.00003340
Iteration 190/1000 | Loss: 0.00003973
Iteration 191/1000 | Loss: 0.00002514
Iteration 192/1000 | Loss: 0.00003755
Iteration 193/1000 | Loss: 0.00004046
Iteration 194/1000 | Loss: 0.00004224
Iteration 195/1000 | Loss: 0.00003894
Iteration 196/1000 | Loss: 0.00003716
Iteration 197/1000 | Loss: 0.00004406
Iteration 198/1000 | Loss: 0.00003568
Iteration 199/1000 | Loss: 0.00004017
Iteration 200/1000 | Loss: 0.00004800
Iteration 201/1000 | Loss: 0.00004067
Iteration 202/1000 | Loss: 0.00004937
Iteration 203/1000 | Loss: 0.00003458
Iteration 204/1000 | Loss: 0.00003491
Iteration 205/1000 | Loss: 0.00004160
Iteration 206/1000 | Loss: 0.00003571
Iteration 207/1000 | Loss: 0.00004214
Iteration 208/1000 | Loss: 0.00003203
Iteration 209/1000 | Loss: 0.00002671
Iteration 210/1000 | Loss: 0.00003185
Iteration 211/1000 | Loss: 0.00003880
Iteration 212/1000 | Loss: 0.00003835
Iteration 213/1000 | Loss: 0.00005090
Iteration 214/1000 | Loss: 0.00004068
Iteration 215/1000 | Loss: 0.00003839
Iteration 216/1000 | Loss: 0.00003695
Iteration 217/1000 | Loss: 0.00003721
Iteration 218/1000 | Loss: 0.00002735
Iteration 219/1000 | Loss: 0.00003775
Iteration 220/1000 | Loss: 0.00004484
Iteration 221/1000 | Loss: 0.00005652
Iteration 222/1000 | Loss: 0.00002664
Iteration 223/1000 | Loss: 0.00002248
Iteration 224/1000 | Loss: 0.00002021
Iteration 225/1000 | Loss: 0.00001952
Iteration 226/1000 | Loss: 0.00001940
Iteration 227/1000 | Loss: 0.00001912
Iteration 228/1000 | Loss: 0.00001876
Iteration 229/1000 | Loss: 0.00001854
Iteration 230/1000 | Loss: 0.00001837
Iteration 231/1000 | Loss: 0.00001813
Iteration 232/1000 | Loss: 0.00001792
Iteration 233/1000 | Loss: 0.00019094
Iteration 234/1000 | Loss: 0.00002444
Iteration 235/1000 | Loss: 0.00002225
Iteration 236/1000 | Loss: 0.00002028
Iteration 237/1000 | Loss: 0.00001940
Iteration 238/1000 | Loss: 0.00001900
Iteration 239/1000 | Loss: 0.00018280
Iteration 240/1000 | Loss: 0.00002526
Iteration 241/1000 | Loss: 0.00002288
Iteration 242/1000 | Loss: 0.00002094
Iteration 243/1000 | Loss: 0.00001990
Iteration 244/1000 | Loss: 0.00001941
Iteration 245/1000 | Loss: 0.00001895
Iteration 246/1000 | Loss: 0.00001855
Iteration 247/1000 | Loss: 0.00001853
Iteration 248/1000 | Loss: 0.00017973
Iteration 249/1000 | Loss: 0.00002399
Iteration 250/1000 | Loss: 0.00002219
Iteration 251/1000 | Loss: 0.00002042
Iteration 252/1000 | Loss: 0.00016569
Iteration 253/1000 | Loss: 0.00002662
Iteration 254/1000 | Loss: 0.00002331
Iteration 255/1000 | Loss: 0.00002134
Iteration 256/1000 | Loss: 0.00002070
Iteration 257/1000 | Loss: 0.00002015
Iteration 258/1000 | Loss: 0.00001960
Iteration 259/1000 | Loss: 0.00001916
Iteration 260/1000 | Loss: 0.00001890
Iteration 261/1000 | Loss: 0.00001869
Iteration 262/1000 | Loss: 0.00001846
Iteration 263/1000 | Loss: 0.00001842
Iteration 264/1000 | Loss: 0.00001842
Iteration 265/1000 | Loss: 0.00001841
Iteration 266/1000 | Loss: 0.00001839
Iteration 267/1000 | Loss: 0.00001837
Iteration 268/1000 | Loss: 0.00001836
Iteration 269/1000 | Loss: 0.00001836
Iteration 270/1000 | Loss: 0.00001831
Iteration 271/1000 | Loss: 0.00001817
Iteration 272/1000 | Loss: 0.00001805
Iteration 273/1000 | Loss: 0.00001804
Iteration 274/1000 | Loss: 0.00001788
Iteration 275/1000 | Loss: 0.00018249
Iteration 276/1000 | Loss: 0.00002358
Iteration 277/1000 | Loss: 0.00002181
Iteration 278/1000 | Loss: 0.00001994
Iteration 279/1000 | Loss: 0.00001923
Iteration 280/1000 | Loss: 0.00017349
Iteration 281/1000 | Loss: 0.00002532
Iteration 282/1000 | Loss: 0.00002249
Iteration 283/1000 | Loss: 0.00002077
Iteration 284/1000 | Loss: 0.00002004
Iteration 285/1000 | Loss: 0.00001952
Iteration 286/1000 | Loss: 0.00001902
Iteration 287/1000 | Loss: 0.00015311
Iteration 288/1000 | Loss: 0.00002506
Iteration 289/1000 | Loss: 0.00002301
Iteration 290/1000 | Loss: 0.00002072
Iteration 291/1000 | Loss: 0.00001984
Iteration 292/1000 | Loss: 0.00001931
Iteration 293/1000 | Loss: 0.00017698
Iteration 294/1000 | Loss: 0.00002546
Iteration 295/1000 | Loss: 0.00002249
Iteration 296/1000 | Loss: 0.00002071
Iteration 297/1000 | Loss: 0.00001985
Iteration 298/1000 | Loss: 0.00001934
Iteration 299/1000 | Loss: 0.00001897
Iteration 300/1000 | Loss: 0.00001857
Iteration 301/1000 | Loss: 0.00001833
Iteration 302/1000 | Loss: 0.00001831
Iteration 303/1000 | Loss: 0.00001820
Iteration 304/1000 | Loss: 0.00001818
Iteration 305/1000 | Loss: 0.00001808
Iteration 306/1000 | Loss: 0.00001808
Iteration 307/1000 | Loss: 0.00001808
Iteration 308/1000 | Loss: 0.00001808
Iteration 309/1000 | Loss: 0.00001808
Iteration 310/1000 | Loss: 0.00001808
Iteration 311/1000 | Loss: 0.00001808
Iteration 312/1000 | Loss: 0.00001808
Iteration 313/1000 | Loss: 0.00001808
Iteration 314/1000 | Loss: 0.00001807
Iteration 315/1000 | Loss: 0.00001807
Iteration 316/1000 | Loss: 0.00001806
Iteration 317/1000 | Loss: 0.00001804
Iteration 318/1000 | Loss: 0.00001804
Iteration 319/1000 | Loss: 0.00001803
Iteration 320/1000 | Loss: 0.00019143
Iteration 321/1000 | Loss: 0.00016327
Iteration 322/1000 | Loss: 0.00020989
Iteration 323/1000 | Loss: 0.00012622
Iteration 324/1000 | Loss: 0.00018161
Iteration 325/1000 | Loss: 0.00011792
Iteration 326/1000 | Loss: 0.00003246
Iteration 327/1000 | Loss: 0.00002607
Iteration 328/1000 | Loss: 0.00002305
Iteration 329/1000 | Loss: 0.00002212
Iteration 330/1000 | Loss: 0.00002160
Iteration 331/1000 | Loss: 0.00002122
Iteration 332/1000 | Loss: 0.00002089
Iteration 333/1000 | Loss: 0.00002059
Iteration 334/1000 | Loss: 0.00002037
Iteration 335/1000 | Loss: 0.00002010
Iteration 336/1000 | Loss: 0.00001979
Iteration 337/1000 | Loss: 0.00001956
Iteration 338/1000 | Loss: 0.00001920
Iteration 339/1000 | Loss: 0.00014984
Iteration 340/1000 | Loss: 0.00012222
Iteration 341/1000 | Loss: 0.00003478
Iteration 342/1000 | Loss: 0.00002740
Iteration 343/1000 | Loss: 0.00002283
Iteration 344/1000 | Loss: 0.00002129
Iteration 345/1000 | Loss: 0.00001968
Iteration 346/1000 | Loss: 0.00001850
Iteration 347/1000 | Loss: 0.00001786
Iteration 348/1000 | Loss: 0.00001740
Iteration 349/1000 | Loss: 0.00001717
Iteration 350/1000 | Loss: 0.00001717
Iteration 351/1000 | Loss: 0.00001710
Iteration 352/1000 | Loss: 0.00001693
Iteration 353/1000 | Loss: 0.00001675
Iteration 354/1000 | Loss: 0.00001669
Iteration 355/1000 | Loss: 0.00001669
Iteration 356/1000 | Loss: 0.00001669
Iteration 357/1000 | Loss: 0.00001669
Iteration 358/1000 | Loss: 0.00001668
Iteration 359/1000 | Loss: 0.00001668
Iteration 360/1000 | Loss: 0.00001666
Iteration 361/1000 | Loss: 0.00001666
Iteration 362/1000 | Loss: 0.00001666
Iteration 363/1000 | Loss: 0.00001666
Iteration 364/1000 | Loss: 0.00001666
Iteration 365/1000 | Loss: 0.00001666
Iteration 366/1000 | Loss: 0.00001665
Iteration 367/1000 | Loss: 0.00001660
Iteration 368/1000 | Loss: 0.00001656
Iteration 369/1000 | Loss: 0.00001653
Iteration 370/1000 | Loss: 0.00001652
Iteration 371/1000 | Loss: 0.00001651
Iteration 372/1000 | Loss: 0.00001651
Iteration 373/1000 | Loss: 0.00019234
Iteration 374/1000 | Loss: 0.00002316
Iteration 375/1000 | Loss: 0.00002013
Iteration 376/1000 | Loss: 0.00001871
Iteration 377/1000 | Loss: 0.00001823
Iteration 378/1000 | Loss: 0.00001788
Iteration 379/1000 | Loss: 0.00001760
Iteration 380/1000 | Loss: 0.00001733
Iteration 381/1000 | Loss: 0.00001706
Iteration 382/1000 | Loss: 0.00001686
Iteration 383/1000 | Loss: 0.00001686
Iteration 384/1000 | Loss: 0.00001685
Iteration 385/1000 | Loss: 0.00001683
Iteration 386/1000 | Loss: 0.00001679
Iteration 387/1000 | Loss: 0.00001676
Iteration 388/1000 | Loss: 0.00001675
Iteration 389/1000 | Loss: 0.00001674
Iteration 390/1000 | Loss: 0.00001670
Iteration 391/1000 | Loss: 0.00001670
Iteration 392/1000 | Loss: 0.00001670
Iteration 393/1000 | Loss: 0.00001670
Iteration 394/1000 | Loss: 0.00001670
Iteration 395/1000 | Loss: 0.00001670
Iteration 396/1000 | Loss: 0.00001669
Iteration 397/1000 | Loss: 0.00001669
Iteration 398/1000 | Loss: 0.00001669
Iteration 399/1000 | Loss: 0.00001668
Iteration 400/1000 | Loss: 0.00001668
Iteration 401/1000 | Loss: 0.00001668
Iteration 402/1000 | Loss: 0.00001668
Iteration 403/1000 | Loss: 0.00001668
Iteration 404/1000 | Loss: 0.00001667
Iteration 405/1000 | Loss: 0.00001667
Iteration 406/1000 | Loss: 0.00001667
Iteration 407/1000 | Loss: 0.00001667
Iteration 408/1000 | Loss: 0.00001667
Iteration 409/1000 | Loss: 0.00001666
Iteration 410/1000 | Loss: 0.00001666
Iteration 411/1000 | Loss: 0.00001665
Iteration 412/1000 | Loss: 0.00001664
Iteration 413/1000 | Loss: 0.00001664
Iteration 414/1000 | Loss: 0.00001664
Iteration 415/1000 | Loss: 0.00001663
Iteration 416/1000 | Loss: 0.00001663
Iteration 417/1000 | Loss: 0.00001663
Iteration 418/1000 | Loss: 0.00001663
Iteration 419/1000 | Loss: 0.00001663
Iteration 420/1000 | Loss: 0.00001663
Iteration 421/1000 | Loss: 0.00001663
Iteration 422/1000 | Loss: 0.00001663
Iteration 423/1000 | Loss: 0.00001663
Iteration 424/1000 | Loss: 0.00001663
Iteration 425/1000 | Loss: 0.00001663
Iteration 426/1000 | Loss: 0.00001662
Iteration 427/1000 | Loss: 0.00001662
Iteration 428/1000 | Loss: 0.00001661
Iteration 429/1000 | Loss: 0.00001660
Iteration 430/1000 | Loss: 0.00001659
Iteration 431/1000 | Loss: 0.00001655
Iteration 432/1000 | Loss: 0.00001653
Iteration 433/1000 | Loss: 0.00001653
Iteration 434/1000 | Loss: 0.00001653
Iteration 435/1000 | Loss: 0.00001653
Iteration 436/1000 | Loss: 0.00001653
Iteration 437/1000 | Loss: 0.00001653
Iteration 438/1000 | Loss: 0.00001652
Iteration 439/1000 | Loss: 0.00001652
Iteration 440/1000 | Loss: 0.00001652
Iteration 441/1000 | Loss: 0.00001652
Iteration 442/1000 | Loss: 0.00001652
Iteration 443/1000 | Loss: 0.00001651
Iteration 444/1000 | Loss: 0.00001651
Iteration 445/1000 | Loss: 0.00001651
Iteration 446/1000 | Loss: 0.00001650
Iteration 447/1000 | Loss: 0.00001649
Iteration 448/1000 | Loss: 0.00001649
Iteration 449/1000 | Loss: 0.00001648
Iteration 450/1000 | Loss: 0.00001648
Iteration 451/1000 | Loss: 0.00001647
Iteration 452/1000 | Loss: 0.00001647
Iteration 453/1000 | Loss: 0.00001647
Iteration 454/1000 | Loss: 0.00018669
Iteration 455/1000 | Loss: 0.00002390
Iteration 456/1000 | Loss: 0.00002128
Iteration 457/1000 | Loss: 0.00001929
Iteration 458/1000 | Loss: 0.00001827
Iteration 459/1000 | Loss: 0.00001782
Iteration 460/1000 | Loss: 0.00001746
Iteration 461/1000 | Loss: 0.00001716
Iteration 462/1000 | Loss: 0.00001696
Iteration 463/1000 | Loss: 0.00001687
Iteration 464/1000 | Loss: 0.00001686
Iteration 465/1000 | Loss: 0.00001684
Iteration 466/1000 | Loss: 0.00001683
Iteration 467/1000 | Loss: 0.00001682
Iteration 468/1000 | Loss: 0.00001682
Iteration 469/1000 | Loss: 0.00001681
Iteration 470/1000 | Loss: 0.00001681
Iteration 471/1000 | Loss: 0.00001680
Iteration 472/1000 | Loss: 0.00001679
Iteration 473/1000 | Loss: 0.00001678
Iteration 474/1000 | Loss: 0.00001677
Iteration 475/1000 | Loss: 0.00001677
Iteration 476/1000 | Loss: 0.00001676
Iteration 477/1000 | Loss: 0.00001676
Iteration 478/1000 | Loss: 0.00001676
Iteration 479/1000 | Loss: 0.00001675
Iteration 480/1000 | Loss: 0.00001675
Iteration 481/1000 | Loss: 0.00001675
Iteration 482/1000 | Loss: 0.00001674
Iteration 483/1000 | Loss: 0.00001674
Iteration 484/1000 | Loss: 0.00001674
Iteration 485/1000 | Loss: 0.00001674
Iteration 486/1000 | Loss: 0.00001673
Iteration 487/1000 | Loss: 0.00001673
Iteration 488/1000 | Loss: 0.00001673
Iteration 489/1000 | Loss: 0.00001673
Iteration 490/1000 | Loss: 0.00001672
Iteration 491/1000 | Loss: 0.00001672
Iteration 492/1000 | Loss: 0.00001672
Iteration 493/1000 | Loss: 0.00001671
Iteration 494/1000 | Loss: 0.00001671
Iteration 495/1000 | Loss: 0.00001671
Iteration 496/1000 | Loss: 0.00001670
Iteration 497/1000 | Loss: 0.00001670
Iteration 498/1000 | Loss: 0.00001670
Iteration 499/1000 | Loss: 0.00001670
Iteration 500/1000 | Loss: 0.00001670
Iteration 501/1000 | Loss: 0.00001670
Iteration 502/1000 | Loss: 0.00001670
Iteration 503/1000 | Loss: 0.00001670
Iteration 504/1000 | Loss: 0.00001670
Iteration 505/1000 | Loss: 0.00001670
Iteration 506/1000 | Loss: 0.00001669
Iteration 507/1000 | Loss: 0.00001669
Iteration 508/1000 | Loss: 0.00001669
Iteration 509/1000 | Loss: 0.00001669
Iteration 510/1000 | Loss: 0.00001669
Iteration 511/1000 | Loss: 0.00001669
Iteration 512/1000 | Loss: 0.00001669
Iteration 513/1000 | Loss: 0.00001669
Iteration 514/1000 | Loss: 0.00001669
Iteration 515/1000 | Loss: 0.00001669
Iteration 516/1000 | Loss: 0.00001669
Iteration 517/1000 | Loss: 0.00001669
Iteration 518/1000 | Loss: 0.00001669
Iteration 519/1000 | Loss: 0.00001668
Iteration 520/1000 | Loss: 0.00001668
Iteration 521/1000 | Loss: 0.00001668
Iteration 522/1000 | Loss: 0.00001668
Iteration 523/1000 | Loss: 0.00001668
Iteration 524/1000 | Loss: 0.00001668
Iteration 525/1000 | Loss: 0.00001668
Iteration 526/1000 | Loss: 0.00001668
Iteration 527/1000 | Loss: 0.00001668
Iteration 528/1000 | Loss: 0.00001668
Iteration 529/1000 | Loss: 0.00001668
Iteration 530/1000 | Loss: 0.00001668
Iteration 531/1000 | Loss: 0.00001668
Iteration 532/1000 | Loss: 0.00001668
Iteration 533/1000 | Loss: 0.00001668
Iteration 534/1000 | Loss: 0.00001668
Iteration 535/1000 | Loss: 0.00001668
Iteration 536/1000 | Loss: 0.00001667
Iteration 537/1000 | Loss: 0.00001667
Iteration 538/1000 | Loss: 0.00001667
Iteration 539/1000 | Loss: 0.00001667
Iteration 540/1000 | Loss: 0.00001667
Iteration 541/1000 | Loss: 0.00001667
Iteration 542/1000 | Loss: 0.00001667
Iteration 543/1000 | Loss: 0.00001667
Iteration 544/1000 | Loss: 0.00001667
Iteration 545/1000 | Loss: 0.00001667
Iteration 546/1000 | Loss: 0.00001667
Iteration 547/1000 | Loss: 0.00001667
Iteration 548/1000 | Loss: 0.00001666
Iteration 549/1000 | Loss: 0.00001666
Iteration 550/1000 | Loss: 0.00001666
Iteration 551/1000 | Loss: 0.00001666
Iteration 552/1000 | Loss: 0.00001666
Iteration 553/1000 | Loss: 0.00001666
Iteration 554/1000 | Loss: 0.00001666
Iteration 555/1000 | Loss: 0.00001666
Iteration 556/1000 | Loss: 0.00001666
Iteration 557/1000 | Loss: 0.00001666
Iteration 558/1000 | Loss: 0.00001665
Iteration 559/1000 | Loss: 0.00001665
Iteration 560/1000 | Loss: 0.00001665
Iteration 561/1000 | Loss: 0.00001665
Iteration 562/1000 | Loss: 0.00001665
Iteration 563/1000 | Loss: 0.00001665
Iteration 564/1000 | Loss: 0.00001665
Iteration 565/1000 | Loss: 0.00001665
Iteration 566/1000 | Loss: 0.00001664
Iteration 567/1000 | Loss: 0.00001664
Iteration 568/1000 | Loss: 0.00001664
Iteration 569/1000 | Loss: 0.00001664
Iteration 570/1000 | Loss: 0.00001664
Iteration 571/1000 | Loss: 0.00001664
Iteration 572/1000 | Loss: 0.00001664
Iteration 573/1000 | Loss: 0.00001664
Iteration 574/1000 | Loss: 0.00001664
Iteration 575/1000 | Loss: 0.00001664
Iteration 576/1000 | Loss: 0.00001664
Iteration 577/1000 | Loss: 0.00001663
Iteration 578/1000 | Loss: 0.00001663
Iteration 579/1000 | Loss: 0.00001663
Iteration 580/1000 | Loss: 0.00001663
Iteration 581/1000 | Loss: 0.00001663
Iteration 582/1000 | Loss: 0.00001663
Iteration 583/1000 | Loss: 0.00001663
Iteration 584/1000 | Loss: 0.00001663
Iteration 585/1000 | Loss: 0.00001663
Iteration 586/1000 | Loss: 0.00001663
Iteration 587/1000 | Loss: 0.00001663
Iteration 588/1000 | Loss: 0.00001663
Iteration 589/1000 | Loss: 0.00001663
Iteration 590/1000 | Loss: 0.00001663
Iteration 591/1000 | Loss: 0.00001663
Iteration 592/1000 | Loss: 0.00001663
Iteration 593/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 593. Stopping optimization.
Last 5 losses: [1.663059083512053e-05, 1.663059083512053e-05, 1.663059083512053e-05, 1.663059083512053e-05, 1.663059083512053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.663059083512053e-05

Optimization complete. Final v2v error: 3.3696210384368896 mm

Highest mean error: 6.800396919250488 mm for frame 52

Lowest mean error: 3.1447861194610596 mm for frame 147

Saving results

Total time: 556.086678981781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099646
Iteration 2/25 | Loss: 0.01099646
Iteration 3/25 | Loss: 0.01099645
Iteration 4/25 | Loss: 0.01099645
Iteration 5/25 | Loss: 0.01099645
Iteration 6/25 | Loss: 0.00583155
Iteration 7/25 | Loss: 0.00359649
Iteration 8/25 | Loss: 0.00327626
Iteration 9/25 | Loss: 0.00232602
Iteration 10/25 | Loss: 0.00205579
Iteration 11/25 | Loss: 0.00195622
Iteration 12/25 | Loss: 0.00187329
Iteration 13/25 | Loss: 0.00181477
Iteration 14/25 | Loss: 0.00181907
Iteration 15/25 | Loss: 0.00172856
Iteration 16/25 | Loss: 0.00164619
Iteration 17/25 | Loss: 0.00155408
Iteration 18/25 | Loss: 0.00150543
Iteration 19/25 | Loss: 0.00140446
Iteration 20/25 | Loss: 0.00129360
Iteration 21/25 | Loss: 0.00119160
Iteration 22/25 | Loss: 0.00114549
Iteration 23/25 | Loss: 0.00113699
Iteration 24/25 | Loss: 0.00112047
Iteration 25/25 | Loss: 0.00110342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71152818
Iteration 2/25 | Loss: 0.00446355
Iteration 3/25 | Loss: 0.00294038
Iteration 4/25 | Loss: 0.00294038
Iteration 5/25 | Loss: 0.00294037
Iteration 6/25 | Loss: 0.00294037
Iteration 7/25 | Loss: 0.00294037
Iteration 8/25 | Loss: 0.00294037
Iteration 9/25 | Loss: 0.00294037
Iteration 10/25 | Loss: 0.00294037
Iteration 11/25 | Loss: 0.00294037
Iteration 12/25 | Loss: 0.00294037
Iteration 13/25 | Loss: 0.00294037
Iteration 14/25 | Loss: 0.00294037
Iteration 15/25 | Loss: 0.00294037
Iteration 16/25 | Loss: 0.00294037
Iteration 17/25 | Loss: 0.00294037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0029403732623904943, 0.0029403732623904943, 0.0029403732623904943, 0.0029403732623904943, 0.0029403732623904943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029403732623904943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294037
Iteration 2/1000 | Loss: 0.00302800
Iteration 3/1000 | Loss: 0.00261684
Iteration 4/1000 | Loss: 0.00172263
Iteration 5/1000 | Loss: 0.00211840
Iteration 6/1000 | Loss: 0.00291455
Iteration 7/1000 | Loss: 0.00155761
Iteration 8/1000 | Loss: 0.00210590
Iteration 9/1000 | Loss: 0.00217767
Iteration 10/1000 | Loss: 0.00155201
Iteration 11/1000 | Loss: 0.00354595
Iteration 12/1000 | Loss: 0.00118249
Iteration 13/1000 | Loss: 0.00118522
Iteration 14/1000 | Loss: 0.00260782
Iteration 15/1000 | Loss: 0.00213082
Iteration 16/1000 | Loss: 0.00209938
Iteration 17/1000 | Loss: 0.00241677
Iteration 18/1000 | Loss: 0.00178812
Iteration 19/1000 | Loss: 0.00173364
Iteration 20/1000 | Loss: 0.00062785
Iteration 21/1000 | Loss: 0.00061629
Iteration 22/1000 | Loss: 0.00068954
Iteration 23/1000 | Loss: 0.00018430
Iteration 24/1000 | Loss: 0.00059864
Iteration 25/1000 | Loss: 0.00010879
Iteration 26/1000 | Loss: 0.00022133
Iteration 27/1000 | Loss: 0.00021563
Iteration 28/1000 | Loss: 0.00020653
Iteration 29/1000 | Loss: 0.00039967
Iteration 30/1000 | Loss: 0.00039315
Iteration 31/1000 | Loss: 0.00011221
Iteration 32/1000 | Loss: 0.00008112
Iteration 33/1000 | Loss: 0.00007566
Iteration 34/1000 | Loss: 0.00005487
Iteration 35/1000 | Loss: 0.00019290
Iteration 36/1000 | Loss: 0.00006254
Iteration 37/1000 | Loss: 0.00005619
Iteration 38/1000 | Loss: 0.00005423
Iteration 39/1000 | Loss: 0.00004922
Iteration 40/1000 | Loss: 0.00006319
Iteration 41/1000 | Loss: 0.00008069
Iteration 42/1000 | Loss: 0.00047884
Iteration 43/1000 | Loss: 0.00020672
Iteration 44/1000 | Loss: 0.00004900
Iteration 45/1000 | Loss: 0.00007562
Iteration 46/1000 | Loss: 0.00005518
Iteration 47/1000 | Loss: 0.00006335
Iteration 48/1000 | Loss: 0.00031311
Iteration 49/1000 | Loss: 0.00024893
Iteration 50/1000 | Loss: 0.00021093
Iteration 51/1000 | Loss: 0.00041811
Iteration 52/1000 | Loss: 0.00050884
Iteration 53/1000 | Loss: 0.00017951
Iteration 54/1000 | Loss: 0.00045246
Iteration 55/1000 | Loss: 0.00046729
Iteration 56/1000 | Loss: 0.00034759
Iteration 57/1000 | Loss: 0.00021378
Iteration 58/1000 | Loss: 0.00024668
Iteration 59/1000 | Loss: 0.00107245
Iteration 60/1000 | Loss: 0.00022708
Iteration 61/1000 | Loss: 0.00006067
Iteration 62/1000 | Loss: 0.00006311
Iteration 63/1000 | Loss: 0.00005495
Iteration 64/1000 | Loss: 0.00004150
Iteration 65/1000 | Loss: 0.00018983
Iteration 66/1000 | Loss: 0.00013052
Iteration 67/1000 | Loss: 0.00044975
Iteration 68/1000 | Loss: 0.00011925
Iteration 69/1000 | Loss: 0.00018085
Iteration 70/1000 | Loss: 0.00004554
Iteration 71/1000 | Loss: 0.00004235
Iteration 72/1000 | Loss: 0.00008554
Iteration 73/1000 | Loss: 0.00009011
Iteration 74/1000 | Loss: 0.00008228
Iteration 75/1000 | Loss: 0.00005965
Iteration 76/1000 | Loss: 0.00016352
Iteration 77/1000 | Loss: 0.00006098
Iteration 78/1000 | Loss: 0.00010905
Iteration 79/1000 | Loss: 0.00011672
Iteration 80/1000 | Loss: 0.00014750
Iteration 81/1000 | Loss: 0.00008696
Iteration 82/1000 | Loss: 0.00012986
Iteration 83/1000 | Loss: 0.00007650
Iteration 84/1000 | Loss: 0.00005017
Iteration 85/1000 | Loss: 0.00003791
Iteration 86/1000 | Loss: 0.00003572
Iteration 87/1000 | Loss: 0.00004046
Iteration 88/1000 | Loss: 0.00005889
Iteration 89/1000 | Loss: 0.00004705
Iteration 90/1000 | Loss: 0.00004720
Iteration 91/1000 | Loss: 0.00005255
Iteration 92/1000 | Loss: 0.00004669
Iteration 93/1000 | Loss: 0.00004553
Iteration 94/1000 | Loss: 0.00004702
Iteration 95/1000 | Loss: 0.00004385
Iteration 96/1000 | Loss: 0.00005042
Iteration 97/1000 | Loss: 0.00004334
Iteration 98/1000 | Loss: 0.00004540
Iteration 99/1000 | Loss: 0.00004220
Iteration 100/1000 | Loss: 0.00004530
Iteration 101/1000 | Loss: 0.00004107
Iteration 102/1000 | Loss: 0.00024800
Iteration 103/1000 | Loss: 0.00010648
Iteration 104/1000 | Loss: 0.00004464
Iteration 105/1000 | Loss: 0.00026595
Iteration 106/1000 | Loss: 0.00010161
Iteration 107/1000 | Loss: 0.00004715
Iteration 108/1000 | Loss: 0.00023590
Iteration 109/1000 | Loss: 0.00011888
Iteration 110/1000 | Loss: 0.00004589
Iteration 111/1000 | Loss: 0.00027223
Iteration 112/1000 | Loss: 0.00010268
Iteration 113/1000 | Loss: 0.00004808
Iteration 114/1000 | Loss: 0.00023292
Iteration 115/1000 | Loss: 0.00012976
Iteration 116/1000 | Loss: 0.00019816
Iteration 117/1000 | Loss: 0.00011836
Iteration 118/1000 | Loss: 0.00004296
Iteration 119/1000 | Loss: 0.00005334
Iteration 120/1000 | Loss: 0.00021147
Iteration 121/1000 | Loss: 0.00009516
Iteration 122/1000 | Loss: 0.00004776
Iteration 123/1000 | Loss: 0.00005074
Iteration 124/1000 | Loss: 0.00022277
Iteration 125/1000 | Loss: 0.00004637
Iteration 126/1000 | Loss: 0.00003771
Iteration 127/1000 | Loss: 0.00003459
Iteration 128/1000 | Loss: 0.00003327
Iteration 129/1000 | Loss: 0.00003235
Iteration 130/1000 | Loss: 0.00003200
Iteration 131/1000 | Loss: 0.00003136
Iteration 132/1000 | Loss: 0.00003748
Iteration 133/1000 | Loss: 0.00003065
Iteration 134/1000 | Loss: 0.00002987
Iteration 135/1000 | Loss: 0.00002933
Iteration 136/1000 | Loss: 0.00002901
Iteration 137/1000 | Loss: 0.00002875
Iteration 138/1000 | Loss: 0.00002849
Iteration 139/1000 | Loss: 0.00002843
Iteration 140/1000 | Loss: 0.00002834
Iteration 141/1000 | Loss: 0.00002834
Iteration 142/1000 | Loss: 0.00002834
Iteration 143/1000 | Loss: 0.00002833
Iteration 144/1000 | Loss: 0.00002833
Iteration 145/1000 | Loss: 0.00002833
Iteration 146/1000 | Loss: 0.00002829
Iteration 147/1000 | Loss: 0.00002829
Iteration 148/1000 | Loss: 0.00002826
Iteration 149/1000 | Loss: 0.00002819
Iteration 150/1000 | Loss: 0.00002819
Iteration 151/1000 | Loss: 0.00002818
Iteration 152/1000 | Loss: 0.00002818
Iteration 153/1000 | Loss: 0.00002817
Iteration 154/1000 | Loss: 0.00002817
Iteration 155/1000 | Loss: 0.00002817
Iteration 156/1000 | Loss: 0.00002816
Iteration 157/1000 | Loss: 0.00002815
Iteration 158/1000 | Loss: 0.00002815
Iteration 159/1000 | Loss: 0.00002814
Iteration 160/1000 | Loss: 0.00002814
Iteration 161/1000 | Loss: 0.00002814
Iteration 162/1000 | Loss: 0.00002814
Iteration 163/1000 | Loss: 0.00002813
Iteration 164/1000 | Loss: 0.00002813
Iteration 165/1000 | Loss: 0.00002812
Iteration 166/1000 | Loss: 0.00002812
Iteration 167/1000 | Loss: 0.00002811
Iteration 168/1000 | Loss: 0.00002811
Iteration 169/1000 | Loss: 0.00002811
Iteration 170/1000 | Loss: 0.00002811
Iteration 171/1000 | Loss: 0.00002811
Iteration 172/1000 | Loss: 0.00002810
Iteration 173/1000 | Loss: 0.00002810
Iteration 174/1000 | Loss: 0.00002809
Iteration 175/1000 | Loss: 0.00002808
Iteration 176/1000 | Loss: 0.00002808
Iteration 177/1000 | Loss: 0.00002808
Iteration 178/1000 | Loss: 0.00002808
Iteration 179/1000 | Loss: 0.00002808
Iteration 180/1000 | Loss: 0.00002808
Iteration 181/1000 | Loss: 0.00002808
Iteration 182/1000 | Loss: 0.00002808
Iteration 183/1000 | Loss: 0.00002808
Iteration 184/1000 | Loss: 0.00002807
Iteration 185/1000 | Loss: 0.00002807
Iteration 186/1000 | Loss: 0.00002807
Iteration 187/1000 | Loss: 0.00002807
Iteration 188/1000 | Loss: 0.00002807
Iteration 189/1000 | Loss: 0.00002807
Iteration 190/1000 | Loss: 0.00002807
Iteration 191/1000 | Loss: 0.00002807
Iteration 192/1000 | Loss: 0.00002807
Iteration 193/1000 | Loss: 0.00002807
Iteration 194/1000 | Loss: 0.00002807
Iteration 195/1000 | Loss: 0.00002807
Iteration 196/1000 | Loss: 0.00002807
Iteration 197/1000 | Loss: 0.00002807
Iteration 198/1000 | Loss: 0.00002807
Iteration 199/1000 | Loss: 0.00002807
Iteration 200/1000 | Loss: 0.00002807
Iteration 201/1000 | Loss: 0.00002807
Iteration 202/1000 | Loss: 0.00002807
Iteration 203/1000 | Loss: 0.00002807
Iteration 204/1000 | Loss: 0.00002807
Iteration 205/1000 | Loss: 0.00002807
Iteration 206/1000 | Loss: 0.00002807
Iteration 207/1000 | Loss: 0.00002807
Iteration 208/1000 | Loss: 0.00002807
Iteration 209/1000 | Loss: 0.00002807
Iteration 210/1000 | Loss: 0.00002807
Iteration 211/1000 | Loss: 0.00002807
Iteration 212/1000 | Loss: 0.00002807
Iteration 213/1000 | Loss: 0.00002807
Iteration 214/1000 | Loss: 0.00002807
Iteration 215/1000 | Loss: 0.00002807
Iteration 216/1000 | Loss: 0.00002807
Iteration 217/1000 | Loss: 0.00002807
Iteration 218/1000 | Loss: 0.00002807
Iteration 219/1000 | Loss: 0.00002807
Iteration 220/1000 | Loss: 0.00002807
Iteration 221/1000 | Loss: 0.00002807
Iteration 222/1000 | Loss: 0.00002807
Iteration 223/1000 | Loss: 0.00002807
Iteration 224/1000 | Loss: 0.00002807
Iteration 225/1000 | Loss: 0.00002807
Iteration 226/1000 | Loss: 0.00002807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.8068092433386482e-05, 2.8068092433386482e-05, 2.8068092433386482e-05, 2.8068092433386482e-05, 2.8068092433386482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8068092433386482e-05

Optimization complete. Final v2v error: 3.88657546043396 mm

Highest mean error: 11.048162460327148 mm for frame 93

Lowest mean error: 3.5119128227233887 mm for frame 61

Saving results

Total time: 276.1019539833069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006541
Iteration 2/25 | Loss: 0.00267883
Iteration 3/25 | Loss: 0.00217957
Iteration 4/25 | Loss: 0.00183874
Iteration 5/25 | Loss: 0.00169934
Iteration 6/25 | Loss: 0.00183381
Iteration 7/25 | Loss: 0.00159476
Iteration 8/25 | Loss: 0.00154975
Iteration 9/25 | Loss: 0.00155008
Iteration 10/25 | Loss: 0.00154881
Iteration 11/25 | Loss: 0.00152829
Iteration 12/25 | Loss: 0.00152729
Iteration 13/25 | Loss: 0.00152642
Iteration 14/25 | Loss: 0.00152611
Iteration 15/25 | Loss: 0.00152592
Iteration 16/25 | Loss: 0.00152584
Iteration 17/25 | Loss: 0.00152584
Iteration 18/25 | Loss: 0.00152584
Iteration 19/25 | Loss: 0.00152584
Iteration 20/25 | Loss: 0.00152584
Iteration 21/25 | Loss: 0.00152584
Iteration 22/25 | Loss: 0.00152584
Iteration 23/25 | Loss: 0.00152584
Iteration 24/25 | Loss: 0.00152584
Iteration 25/25 | Loss: 0.00152584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41044927
Iteration 2/25 | Loss: 0.00763607
Iteration 3/25 | Loss: 0.00539145
Iteration 4/25 | Loss: 0.00535933
Iteration 5/25 | Loss: 0.00535933
Iteration 6/25 | Loss: 0.00535933
Iteration 7/25 | Loss: 0.00535932
Iteration 8/25 | Loss: 0.00535932
Iteration 9/25 | Loss: 0.00535932
Iteration 10/25 | Loss: 0.00535932
Iteration 11/25 | Loss: 0.00535932
Iteration 12/25 | Loss: 0.00535932
Iteration 13/25 | Loss: 0.00535932
Iteration 14/25 | Loss: 0.00535932
Iteration 15/25 | Loss: 0.00535932
Iteration 16/25 | Loss: 0.00535932
Iteration 17/25 | Loss: 0.00535932
Iteration 18/25 | Loss: 0.00535932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005359322763979435, 0.005359322763979435, 0.005359322763979435, 0.005359322763979435, 0.005359322763979435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005359322763979435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00535932
Iteration 2/1000 | Loss: 0.00650987
Iteration 3/1000 | Loss: 0.00126291
Iteration 4/1000 | Loss: 0.00208801
Iteration 5/1000 | Loss: 0.00842178
Iteration 6/1000 | Loss: 0.00164420
Iteration 7/1000 | Loss: 0.00112927
Iteration 8/1000 | Loss: 0.00050601
Iteration 9/1000 | Loss: 0.00041831
Iteration 10/1000 | Loss: 0.00038713
Iteration 11/1000 | Loss: 0.02305271
Iteration 12/1000 | Loss: 0.01363849
Iteration 13/1000 | Loss: 0.00173637
Iteration 14/1000 | Loss: 0.00463967
Iteration 15/1000 | Loss: 0.00370302
Iteration 16/1000 | Loss: 0.00239499
Iteration 17/1000 | Loss: 0.00140028
Iteration 18/1000 | Loss: 0.00092732
Iteration 19/1000 | Loss: 0.00691569
Iteration 20/1000 | Loss: 0.00188569
Iteration 21/1000 | Loss: 0.00218544
Iteration 22/1000 | Loss: 0.00056272
Iteration 23/1000 | Loss: 0.00063540
Iteration 24/1000 | Loss: 0.00033958
Iteration 25/1000 | Loss: 0.00278093
Iteration 26/1000 | Loss: 0.00008601
Iteration 27/1000 | Loss: 0.00033158
Iteration 28/1000 | Loss: 0.00321111
Iteration 29/1000 | Loss: 0.00033134
Iteration 30/1000 | Loss: 0.00018538
Iteration 31/1000 | Loss: 0.00046583
Iteration 32/1000 | Loss: 0.00008206
Iteration 33/1000 | Loss: 0.00035787
Iteration 34/1000 | Loss: 0.00017316
Iteration 35/1000 | Loss: 0.00012755
Iteration 36/1000 | Loss: 0.00016701
Iteration 37/1000 | Loss: 0.00082171
Iteration 38/1000 | Loss: 0.00003661
Iteration 39/1000 | Loss: 0.00026198
Iteration 40/1000 | Loss: 0.00024336
Iteration 41/1000 | Loss: 0.00171571
Iteration 42/1000 | Loss: 0.00035180
Iteration 43/1000 | Loss: 0.00018928
Iteration 44/1000 | Loss: 0.00014622
Iteration 45/1000 | Loss: 0.00014074
Iteration 46/1000 | Loss: 0.00095409
Iteration 47/1000 | Loss: 0.00013680
Iteration 48/1000 | Loss: 0.00027024
Iteration 49/1000 | Loss: 0.00015402
Iteration 50/1000 | Loss: 0.00017428
Iteration 51/1000 | Loss: 0.00002519
Iteration 52/1000 | Loss: 0.00003779
Iteration 53/1000 | Loss: 0.00006687
Iteration 54/1000 | Loss: 0.00002030
Iteration 55/1000 | Loss: 0.00013004
Iteration 56/1000 | Loss: 0.00002357
Iteration 57/1000 | Loss: 0.00011244
Iteration 58/1000 | Loss: 0.00011243
Iteration 59/1000 | Loss: 0.00074275
Iteration 60/1000 | Loss: 0.00008185
Iteration 61/1000 | Loss: 0.00019529
Iteration 62/1000 | Loss: 0.00001986
Iteration 63/1000 | Loss: 0.00001914
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001883
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00013899
Iteration 69/1000 | Loss: 0.00001879
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001859
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001854
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001853
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00001852
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001848
Iteration 111/1000 | Loss: 0.00001848
Iteration 112/1000 | Loss: 0.00001848
Iteration 113/1000 | Loss: 0.00001848
Iteration 114/1000 | Loss: 0.00001848
Iteration 115/1000 | Loss: 0.00001848
Iteration 116/1000 | Loss: 0.00001848
Iteration 117/1000 | Loss: 0.00001848
Iteration 118/1000 | Loss: 0.00001848
Iteration 119/1000 | Loss: 0.00001847
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001847
Iteration 122/1000 | Loss: 0.00001847
Iteration 123/1000 | Loss: 0.00001847
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001847
Iteration 126/1000 | Loss: 0.00001847
Iteration 127/1000 | Loss: 0.00001847
Iteration 128/1000 | Loss: 0.00001847
Iteration 129/1000 | Loss: 0.00001847
Iteration 130/1000 | Loss: 0.00001847
Iteration 131/1000 | Loss: 0.00001847
Iteration 132/1000 | Loss: 0.00001847
Iteration 133/1000 | Loss: 0.00001847
Iteration 134/1000 | Loss: 0.00001847
Iteration 135/1000 | Loss: 0.00001847
Iteration 136/1000 | Loss: 0.00001846
Iteration 137/1000 | Loss: 0.00001846
Iteration 138/1000 | Loss: 0.00001846
Iteration 139/1000 | Loss: 0.00001846
Iteration 140/1000 | Loss: 0.00001846
Iteration 141/1000 | Loss: 0.00001846
Iteration 142/1000 | Loss: 0.00001846
Iteration 143/1000 | Loss: 0.00001845
Iteration 144/1000 | Loss: 0.00001845
Iteration 145/1000 | Loss: 0.00001845
Iteration 146/1000 | Loss: 0.00001845
Iteration 147/1000 | Loss: 0.00001845
Iteration 148/1000 | Loss: 0.00001845
Iteration 149/1000 | Loss: 0.00001845
Iteration 150/1000 | Loss: 0.00001845
Iteration 151/1000 | Loss: 0.00001845
Iteration 152/1000 | Loss: 0.00001845
Iteration 153/1000 | Loss: 0.00001845
Iteration 154/1000 | Loss: 0.00001845
Iteration 155/1000 | Loss: 0.00001845
Iteration 156/1000 | Loss: 0.00001845
Iteration 157/1000 | Loss: 0.00001845
Iteration 158/1000 | Loss: 0.00001845
Iteration 159/1000 | Loss: 0.00001845
Iteration 160/1000 | Loss: 0.00001845
Iteration 161/1000 | Loss: 0.00001845
Iteration 162/1000 | Loss: 0.00001845
Iteration 163/1000 | Loss: 0.00001845
Iteration 164/1000 | Loss: 0.00001845
Iteration 165/1000 | Loss: 0.00001845
Iteration 166/1000 | Loss: 0.00001845
Iteration 167/1000 | Loss: 0.00001845
Iteration 168/1000 | Loss: 0.00001845
Iteration 169/1000 | Loss: 0.00001845
Iteration 170/1000 | Loss: 0.00001845
Iteration 171/1000 | Loss: 0.00001845
Iteration 172/1000 | Loss: 0.00001845
Iteration 173/1000 | Loss: 0.00001845
Iteration 174/1000 | Loss: 0.00001845
Iteration 175/1000 | Loss: 0.00001845
Iteration 176/1000 | Loss: 0.00001845
Iteration 177/1000 | Loss: 0.00001845
Iteration 178/1000 | Loss: 0.00001845
Iteration 179/1000 | Loss: 0.00001845
Iteration 180/1000 | Loss: 0.00001845
Iteration 181/1000 | Loss: 0.00001845
Iteration 182/1000 | Loss: 0.00001845
Iteration 183/1000 | Loss: 0.00001845
Iteration 184/1000 | Loss: 0.00001845
Iteration 185/1000 | Loss: 0.00001845
Iteration 186/1000 | Loss: 0.00001845
Iteration 187/1000 | Loss: 0.00001845
Iteration 188/1000 | Loss: 0.00001845
Iteration 189/1000 | Loss: 0.00001845
Iteration 190/1000 | Loss: 0.00001845
Iteration 191/1000 | Loss: 0.00001845
Iteration 192/1000 | Loss: 0.00001845
Iteration 193/1000 | Loss: 0.00001845
Iteration 194/1000 | Loss: 0.00001845
Iteration 195/1000 | Loss: 0.00001845
Iteration 196/1000 | Loss: 0.00001845
Iteration 197/1000 | Loss: 0.00001845
Iteration 198/1000 | Loss: 0.00001845
Iteration 199/1000 | Loss: 0.00001845
Iteration 200/1000 | Loss: 0.00001845
Iteration 201/1000 | Loss: 0.00001845
Iteration 202/1000 | Loss: 0.00001845
Iteration 203/1000 | Loss: 0.00001845
Iteration 204/1000 | Loss: 0.00001845
Iteration 205/1000 | Loss: 0.00001845
Iteration 206/1000 | Loss: 0.00001845
Iteration 207/1000 | Loss: 0.00001845
Iteration 208/1000 | Loss: 0.00001845
Iteration 209/1000 | Loss: 0.00001845
Iteration 210/1000 | Loss: 0.00001845
Iteration 211/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.8447004549670964e-05, 1.8447004549670964e-05, 1.8447004549670964e-05, 1.8447004549670964e-05, 1.8447004549670964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8447004549670964e-05

Optimization complete. Final v2v error: 3.6574854850769043 mm

Highest mean error: 4.018265724182129 mm for frame 135

Lowest mean error: 3.4786577224731445 mm for frame 105

Saving results

Total time: 134.67548084259033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884389
Iteration 2/25 | Loss: 0.00119467
Iteration 3/25 | Loss: 0.00082661
Iteration 4/25 | Loss: 0.00074593
Iteration 5/25 | Loss: 0.00073581
Iteration 6/25 | Loss: 0.00073493
Iteration 7/25 | Loss: 0.00073493
Iteration 8/25 | Loss: 0.00073493
Iteration 9/25 | Loss: 0.00073493
Iteration 10/25 | Loss: 0.00073493
Iteration 11/25 | Loss: 0.00073493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007349268416874111, 0.0007349268416874111, 0.0007349268416874111, 0.0007349268416874111, 0.0007349268416874111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007349268416874111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99189359
Iteration 2/25 | Loss: 0.00024371
Iteration 3/25 | Loss: 0.00024370
Iteration 4/25 | Loss: 0.00024370
Iteration 5/25 | Loss: 0.00024370
Iteration 6/25 | Loss: 0.00024370
Iteration 7/25 | Loss: 0.00024369
Iteration 8/25 | Loss: 0.00024369
Iteration 9/25 | Loss: 0.00024369
Iteration 10/25 | Loss: 0.00024369
Iteration 11/25 | Loss: 0.00024369
Iteration 12/25 | Loss: 0.00024369
Iteration 13/25 | Loss: 0.00024369
Iteration 14/25 | Loss: 0.00024369
Iteration 15/25 | Loss: 0.00024369
Iteration 16/25 | Loss: 0.00024369
Iteration 17/25 | Loss: 0.00024369
Iteration 18/25 | Loss: 0.00024369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002436941140331328, 0.0002436941140331328, 0.0002436941140331328, 0.0002436941140331328, 0.0002436941140331328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002436941140331328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024369
Iteration 2/1000 | Loss: 0.00003584
Iteration 3/1000 | Loss: 0.00002946
Iteration 4/1000 | Loss: 0.00002759
Iteration 5/1000 | Loss: 0.00002629
Iteration 6/1000 | Loss: 0.00002555
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002406
Iteration 10/1000 | Loss: 0.00002388
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002380
Iteration 13/1000 | Loss: 0.00002379
Iteration 14/1000 | Loss: 0.00002379
Iteration 15/1000 | Loss: 0.00002379
Iteration 16/1000 | Loss: 0.00002379
Iteration 17/1000 | Loss: 0.00002379
Iteration 18/1000 | Loss: 0.00002373
Iteration 19/1000 | Loss: 0.00002371
Iteration 20/1000 | Loss: 0.00002370
Iteration 21/1000 | Loss: 0.00002370
Iteration 22/1000 | Loss: 0.00002370
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00002370
Iteration 25/1000 | Loss: 0.00002370
Iteration 26/1000 | Loss: 0.00002369
Iteration 27/1000 | Loss: 0.00002369
Iteration 28/1000 | Loss: 0.00002369
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002367
Iteration 32/1000 | Loss: 0.00002367
Iteration 33/1000 | Loss: 0.00002365
Iteration 34/1000 | Loss: 0.00002365
Iteration 35/1000 | Loss: 0.00002365
Iteration 36/1000 | Loss: 0.00002364
Iteration 37/1000 | Loss: 0.00002364
Iteration 38/1000 | Loss: 0.00002364
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002363
Iteration 41/1000 | Loss: 0.00002363
Iteration 42/1000 | Loss: 0.00002363
Iteration 43/1000 | Loss: 0.00002363
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002363
Iteration 46/1000 | Loss: 0.00002362
Iteration 47/1000 | Loss: 0.00002362
Iteration 48/1000 | Loss: 0.00002362
Iteration 49/1000 | Loss: 0.00002362
Iteration 50/1000 | Loss: 0.00002362
Iteration 51/1000 | Loss: 0.00002362
Iteration 52/1000 | Loss: 0.00002362
Iteration 53/1000 | Loss: 0.00002362
Iteration 54/1000 | Loss: 0.00002362
Iteration 55/1000 | Loss: 0.00002362
Iteration 56/1000 | Loss: 0.00002361
Iteration 57/1000 | Loss: 0.00002361
Iteration 58/1000 | Loss: 0.00002361
Iteration 59/1000 | Loss: 0.00002361
Iteration 60/1000 | Loss: 0.00002361
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002361
Iteration 64/1000 | Loss: 0.00002361
Iteration 65/1000 | Loss: 0.00002361
Iteration 66/1000 | Loss: 0.00002361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.3613303710590117e-05, 2.3613303710590117e-05, 2.3613303710590117e-05, 2.3613303710590117e-05, 2.3613303710590117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3613303710590117e-05

Optimization complete. Final v2v error: 4.111547946929932 mm

Highest mean error: 4.315544605255127 mm for frame 153

Lowest mean error: 3.937034845352173 mm for frame 6

Saving results

Total time: 32.83933877944946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861983
Iteration 2/25 | Loss: 0.00076199
Iteration 3/25 | Loss: 0.00065055
Iteration 4/25 | Loss: 0.00062473
Iteration 5/25 | Loss: 0.00061767
Iteration 6/25 | Loss: 0.00061588
Iteration 7/25 | Loss: 0.00061544
Iteration 8/25 | Loss: 0.00061544
Iteration 9/25 | Loss: 0.00061544
Iteration 10/25 | Loss: 0.00061544
Iteration 11/25 | Loss: 0.00061544
Iteration 12/25 | Loss: 0.00061544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006154444999992847, 0.0006154444999992847, 0.0006154444999992847, 0.0006154444999992847, 0.0006154444999992847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006154444999992847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.54858971
Iteration 2/25 | Loss: 0.00028312
Iteration 3/25 | Loss: 0.00028311
Iteration 4/25 | Loss: 0.00028311
Iteration 5/25 | Loss: 0.00028311
Iteration 6/25 | Loss: 0.00028311
Iteration 7/25 | Loss: 0.00028311
Iteration 8/25 | Loss: 0.00028311
Iteration 9/25 | Loss: 0.00028311
Iteration 10/25 | Loss: 0.00028311
Iteration 11/25 | Loss: 0.00028311
Iteration 12/25 | Loss: 0.00028311
Iteration 13/25 | Loss: 0.00028311
Iteration 14/25 | Loss: 0.00028311
Iteration 15/25 | Loss: 0.00028311
Iteration 16/25 | Loss: 0.00028311
Iteration 17/25 | Loss: 0.00028311
Iteration 18/25 | Loss: 0.00028311
Iteration 19/25 | Loss: 0.00028311
Iteration 20/25 | Loss: 0.00028311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002831095189321786, 0.0002831095189321786, 0.0002831095189321786, 0.0002831095189321786, 0.0002831095189321786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002831095189321786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028311
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001634
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00001578
Iteration 10/1000 | Loss: 0.00001578
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001560
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001553
Iteration 20/1000 | Loss: 0.00001553
Iteration 21/1000 | Loss: 0.00001552
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001542
Iteration 29/1000 | Loss: 0.00001542
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001539
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001533
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001525
Iteration 82/1000 | Loss: 0.00001525
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001518
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001514
Iteration 132/1000 | Loss: 0.00001514
Iteration 133/1000 | Loss: 0.00001514
Iteration 134/1000 | Loss: 0.00001514
Iteration 135/1000 | Loss: 0.00001514
Iteration 136/1000 | Loss: 0.00001513
Iteration 137/1000 | Loss: 0.00001513
Iteration 138/1000 | Loss: 0.00001513
Iteration 139/1000 | Loss: 0.00001513
Iteration 140/1000 | Loss: 0.00001513
Iteration 141/1000 | Loss: 0.00001513
Iteration 142/1000 | Loss: 0.00001513
Iteration 143/1000 | Loss: 0.00001513
Iteration 144/1000 | Loss: 0.00001513
Iteration 145/1000 | Loss: 0.00001513
Iteration 146/1000 | Loss: 0.00001512
Iteration 147/1000 | Loss: 0.00001512
Iteration 148/1000 | Loss: 0.00001512
Iteration 149/1000 | Loss: 0.00001512
Iteration 150/1000 | Loss: 0.00001512
Iteration 151/1000 | Loss: 0.00001512
Iteration 152/1000 | Loss: 0.00001512
Iteration 153/1000 | Loss: 0.00001512
Iteration 154/1000 | Loss: 0.00001511
Iteration 155/1000 | Loss: 0.00001511
Iteration 156/1000 | Loss: 0.00001511
Iteration 157/1000 | Loss: 0.00001511
Iteration 158/1000 | Loss: 0.00001511
Iteration 159/1000 | Loss: 0.00001511
Iteration 160/1000 | Loss: 0.00001511
Iteration 161/1000 | Loss: 0.00001511
Iteration 162/1000 | Loss: 0.00001511
Iteration 163/1000 | Loss: 0.00001511
Iteration 164/1000 | Loss: 0.00001511
Iteration 165/1000 | Loss: 0.00001511
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001511
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001510
Iteration 171/1000 | Loss: 0.00001510
Iteration 172/1000 | Loss: 0.00001510
Iteration 173/1000 | Loss: 0.00001510
Iteration 174/1000 | Loss: 0.00001510
Iteration 175/1000 | Loss: 0.00001510
Iteration 176/1000 | Loss: 0.00001510
Iteration 177/1000 | Loss: 0.00001510
Iteration 178/1000 | Loss: 0.00001510
Iteration 179/1000 | Loss: 0.00001510
Iteration 180/1000 | Loss: 0.00001510
Iteration 181/1000 | Loss: 0.00001509
Iteration 182/1000 | Loss: 0.00001509
Iteration 183/1000 | Loss: 0.00001509
Iteration 184/1000 | Loss: 0.00001509
Iteration 185/1000 | Loss: 0.00001509
Iteration 186/1000 | Loss: 0.00001509
Iteration 187/1000 | Loss: 0.00001509
Iteration 188/1000 | Loss: 0.00001509
Iteration 189/1000 | Loss: 0.00001509
Iteration 190/1000 | Loss: 0.00001509
Iteration 191/1000 | Loss: 0.00001508
Iteration 192/1000 | Loss: 0.00001508
Iteration 193/1000 | Loss: 0.00001508
Iteration 194/1000 | Loss: 0.00001508
Iteration 195/1000 | Loss: 0.00001508
Iteration 196/1000 | Loss: 0.00001508
Iteration 197/1000 | Loss: 0.00001508
Iteration 198/1000 | Loss: 0.00001508
Iteration 199/1000 | Loss: 0.00001508
Iteration 200/1000 | Loss: 0.00001508
Iteration 201/1000 | Loss: 0.00001508
Iteration 202/1000 | Loss: 0.00001507
Iteration 203/1000 | Loss: 0.00001507
Iteration 204/1000 | Loss: 0.00001507
Iteration 205/1000 | Loss: 0.00001507
Iteration 206/1000 | Loss: 0.00001507
Iteration 207/1000 | Loss: 0.00001507
Iteration 208/1000 | Loss: 0.00001507
Iteration 209/1000 | Loss: 0.00001507
Iteration 210/1000 | Loss: 0.00001507
Iteration 211/1000 | Loss: 0.00001507
Iteration 212/1000 | Loss: 0.00001506
Iteration 213/1000 | Loss: 0.00001506
Iteration 214/1000 | Loss: 0.00001506
Iteration 215/1000 | Loss: 0.00001506
Iteration 216/1000 | Loss: 0.00001506
Iteration 217/1000 | Loss: 0.00001506
Iteration 218/1000 | Loss: 0.00001506
Iteration 219/1000 | Loss: 0.00001506
Iteration 220/1000 | Loss: 0.00001506
Iteration 221/1000 | Loss: 0.00001505
Iteration 222/1000 | Loss: 0.00001505
Iteration 223/1000 | Loss: 0.00001505
Iteration 224/1000 | Loss: 0.00001505
Iteration 225/1000 | Loss: 0.00001505
Iteration 226/1000 | Loss: 0.00001505
Iteration 227/1000 | Loss: 0.00001505
Iteration 228/1000 | Loss: 0.00001505
Iteration 229/1000 | Loss: 0.00001505
Iteration 230/1000 | Loss: 0.00001505
Iteration 231/1000 | Loss: 0.00001505
Iteration 232/1000 | Loss: 0.00001505
Iteration 233/1000 | Loss: 0.00001505
Iteration 234/1000 | Loss: 0.00001505
Iteration 235/1000 | Loss: 0.00001505
Iteration 236/1000 | Loss: 0.00001505
Iteration 237/1000 | Loss: 0.00001505
Iteration 238/1000 | Loss: 0.00001505
Iteration 239/1000 | Loss: 0.00001505
Iteration 240/1000 | Loss: 0.00001505
Iteration 241/1000 | Loss: 0.00001505
Iteration 242/1000 | Loss: 0.00001505
Iteration 243/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.5050250112835784e-05, 1.5050250112835784e-05, 1.5050250112835784e-05, 1.5050250112835784e-05, 1.5050250112835784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5050250112835784e-05

Optimization complete. Final v2v error: 3.3072776794433594 mm

Highest mean error: 3.846175193786621 mm for frame 99

Lowest mean error: 2.9430716037750244 mm for frame 45

Saving results

Total time: 40.79452109336853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465394
Iteration 2/25 | Loss: 0.00102776
Iteration 3/25 | Loss: 0.00075411
Iteration 4/25 | Loss: 0.00068416
Iteration 5/25 | Loss: 0.00066804
Iteration 6/25 | Loss: 0.00066539
Iteration 7/25 | Loss: 0.00066489
Iteration 8/25 | Loss: 0.00066489
Iteration 9/25 | Loss: 0.00066489
Iteration 10/25 | Loss: 0.00066489
Iteration 11/25 | Loss: 0.00066489
Iteration 12/25 | Loss: 0.00066489
Iteration 13/25 | Loss: 0.00066489
Iteration 14/25 | Loss: 0.00066489
Iteration 15/25 | Loss: 0.00066489
Iteration 16/25 | Loss: 0.00066489
Iteration 17/25 | Loss: 0.00066489
Iteration 18/25 | Loss: 0.00066489
Iteration 19/25 | Loss: 0.00066489
Iteration 20/25 | Loss: 0.00066489
Iteration 21/25 | Loss: 0.00066489
Iteration 22/25 | Loss: 0.00066489
Iteration 23/25 | Loss: 0.00066489
Iteration 24/25 | Loss: 0.00066489
Iteration 25/25 | Loss: 0.00066489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006648934213444591, 0.0006648934213444591, 0.0006648934213444591, 0.0006648934213444591, 0.0006648934213444591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006648934213444591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39567065
Iteration 2/25 | Loss: 0.00031611
Iteration 3/25 | Loss: 0.00031607
Iteration 4/25 | Loss: 0.00031607
Iteration 5/25 | Loss: 0.00031607
Iteration 6/25 | Loss: 0.00031607
Iteration 7/25 | Loss: 0.00031607
Iteration 8/25 | Loss: 0.00031607
Iteration 9/25 | Loss: 0.00031607
Iteration 10/25 | Loss: 0.00031607
Iteration 11/25 | Loss: 0.00031607
Iteration 12/25 | Loss: 0.00031607
Iteration 13/25 | Loss: 0.00031607
Iteration 14/25 | Loss: 0.00031607
Iteration 15/25 | Loss: 0.00031607
Iteration 16/25 | Loss: 0.00031607
Iteration 17/25 | Loss: 0.00031607
Iteration 18/25 | Loss: 0.00031607
Iteration 19/25 | Loss: 0.00031607
Iteration 20/25 | Loss: 0.00031607
Iteration 21/25 | Loss: 0.00031607
Iteration 22/25 | Loss: 0.00031607
Iteration 23/25 | Loss: 0.00031607
Iteration 24/25 | Loss: 0.00031607
Iteration 25/25 | Loss: 0.00031607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031607
Iteration 2/1000 | Loss: 0.00003387
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001956
Iteration 6/1000 | Loss: 0.00001870
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001786
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001737
Iteration 11/1000 | Loss: 0.00001716
Iteration 12/1000 | Loss: 0.00001715
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00001710
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001686
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001684
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001681
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001678
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001677
Iteration 49/1000 | Loss: 0.00001677
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001676
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001673
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001672
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001668
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001666
Iteration 101/1000 | Loss: 0.00001666
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001666
Iteration 104/1000 | Loss: 0.00001666
Iteration 105/1000 | Loss: 0.00001666
Iteration 106/1000 | Loss: 0.00001666
Iteration 107/1000 | Loss: 0.00001666
Iteration 108/1000 | Loss: 0.00001665
Iteration 109/1000 | Loss: 0.00001665
Iteration 110/1000 | Loss: 0.00001665
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001664
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001662
Iteration 121/1000 | Loss: 0.00001662
Iteration 122/1000 | Loss: 0.00001662
Iteration 123/1000 | Loss: 0.00001662
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001661
Iteration 126/1000 | Loss: 0.00001661
Iteration 127/1000 | Loss: 0.00001661
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001659
Iteration 133/1000 | Loss: 0.00001659
Iteration 134/1000 | Loss: 0.00001659
Iteration 135/1000 | Loss: 0.00001659
Iteration 136/1000 | Loss: 0.00001659
Iteration 137/1000 | Loss: 0.00001659
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.658867950027343e-05, 1.658867950027343e-05, 1.658867950027343e-05, 1.658867950027343e-05, 1.658867950027343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.658867950027343e-05

Optimization complete. Final v2v error: 3.42028546333313 mm

Highest mean error: 4.442773342132568 mm for frame 56

Lowest mean error: 2.867884397506714 mm for frame 102

Saving results

Total time: 44.781893730163574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366238
Iteration 2/25 | Loss: 0.00094226
Iteration 3/25 | Loss: 0.00071548
Iteration 4/25 | Loss: 0.00066881
Iteration 5/25 | Loss: 0.00065335
Iteration 6/25 | Loss: 0.00064993
Iteration 7/25 | Loss: 0.00064923
Iteration 8/25 | Loss: 0.00064923
Iteration 9/25 | Loss: 0.00064923
Iteration 10/25 | Loss: 0.00064923
Iteration 11/25 | Loss: 0.00064923
Iteration 12/25 | Loss: 0.00064923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006492254906333983, 0.0006492254906333983, 0.0006492254906333983, 0.0006492254906333983, 0.0006492254906333983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006492254906333983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44927931
Iteration 2/25 | Loss: 0.00026472
Iteration 3/25 | Loss: 0.00026472
Iteration 4/25 | Loss: 0.00026472
Iteration 5/25 | Loss: 0.00026472
Iteration 6/25 | Loss: 0.00026472
Iteration 7/25 | Loss: 0.00026472
Iteration 8/25 | Loss: 0.00026472
Iteration 9/25 | Loss: 0.00026472
Iteration 10/25 | Loss: 0.00026472
Iteration 11/25 | Loss: 0.00026472
Iteration 12/25 | Loss: 0.00026472
Iteration 13/25 | Loss: 0.00026471
Iteration 14/25 | Loss: 0.00026471
Iteration 15/25 | Loss: 0.00026471
Iteration 16/25 | Loss: 0.00026471
Iteration 17/25 | Loss: 0.00026471
Iteration 18/25 | Loss: 0.00026471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002647149667609483, 0.0002647149667609483, 0.0002647149667609483, 0.0002647149667609483, 0.0002647149667609483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002647149667609483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026471
Iteration 2/1000 | Loss: 0.00002868
Iteration 3/1000 | Loss: 0.00002260
Iteration 4/1000 | Loss: 0.00002075
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001813
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001779
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001727
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001724
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001722
Iteration 31/1000 | Loss: 0.00001722
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001722
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001704
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001703
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001701
Iteration 135/1000 | Loss: 0.00001701
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001700
Iteration 159/1000 | Loss: 0.00001700
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001699
Iteration 166/1000 | Loss: 0.00001699
Iteration 167/1000 | Loss: 0.00001699
Iteration 168/1000 | Loss: 0.00001699
Iteration 169/1000 | Loss: 0.00001699
Iteration 170/1000 | Loss: 0.00001699
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001699
Iteration 176/1000 | Loss: 0.00001699
Iteration 177/1000 | Loss: 0.00001699
Iteration 178/1000 | Loss: 0.00001699
Iteration 179/1000 | Loss: 0.00001699
Iteration 180/1000 | Loss: 0.00001699
Iteration 181/1000 | Loss: 0.00001699
Iteration 182/1000 | Loss: 0.00001699
Iteration 183/1000 | Loss: 0.00001699
Iteration 184/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.6986910850391723e-05, 1.6986910850391723e-05, 1.6986910850391723e-05, 1.6986910850391723e-05, 1.6986910850391723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6986910850391723e-05

Optimization complete. Final v2v error: 3.3934948444366455 mm

Highest mean error: 3.643602132797241 mm for frame 175

Lowest mean error: 3.189279079437256 mm for frame 146

Saving results

Total time: 42.1491425037384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411928
Iteration 2/25 | Loss: 0.00079637
Iteration 3/25 | Loss: 0.00068221
Iteration 4/25 | Loss: 0.00065481
Iteration 5/25 | Loss: 0.00064774
Iteration 6/25 | Loss: 0.00064648
Iteration 7/25 | Loss: 0.00064621
Iteration 8/25 | Loss: 0.00064621
Iteration 9/25 | Loss: 0.00064621
Iteration 10/25 | Loss: 0.00064621
Iteration 11/25 | Loss: 0.00064621
Iteration 12/25 | Loss: 0.00064621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006462058518081903, 0.0006462058518081903, 0.0006462058518081903, 0.0006462058518081903, 0.0006462058518081903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006462058518081903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89613461
Iteration 2/25 | Loss: 0.00027470
Iteration 3/25 | Loss: 0.00027470
Iteration 4/25 | Loss: 0.00027470
Iteration 5/25 | Loss: 0.00027470
Iteration 6/25 | Loss: 0.00027470
Iteration 7/25 | Loss: 0.00027470
Iteration 8/25 | Loss: 0.00027470
Iteration 9/25 | Loss: 0.00027470
Iteration 10/25 | Loss: 0.00027470
Iteration 11/25 | Loss: 0.00027470
Iteration 12/25 | Loss: 0.00027470
Iteration 13/25 | Loss: 0.00027470
Iteration 14/25 | Loss: 0.00027470
Iteration 15/25 | Loss: 0.00027470
Iteration 16/25 | Loss: 0.00027470
Iteration 17/25 | Loss: 0.00027470
Iteration 18/25 | Loss: 0.00027470
Iteration 19/25 | Loss: 0.00027470
Iteration 20/25 | Loss: 0.00027470
Iteration 21/25 | Loss: 0.00027470
Iteration 22/25 | Loss: 0.00027470
Iteration 23/25 | Loss: 0.00027470
Iteration 24/25 | Loss: 0.00027470
Iteration 25/25 | Loss: 0.00027470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027470
Iteration 2/1000 | Loss: 0.00003668
Iteration 3/1000 | Loss: 0.00002378
Iteration 4/1000 | Loss: 0.00002100
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001761
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001761
Iteration 15/1000 | Loss: 0.00001760
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001758
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001748
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001743
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001742
Iteration 47/1000 | Loss: 0.00001742
Iteration 48/1000 | Loss: 0.00001742
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001741
Iteration 51/1000 | Loss: 0.00001741
Iteration 52/1000 | Loss: 0.00001741
Iteration 53/1000 | Loss: 0.00001741
Iteration 54/1000 | Loss: 0.00001740
Iteration 55/1000 | Loss: 0.00001740
Iteration 56/1000 | Loss: 0.00001740
Iteration 57/1000 | Loss: 0.00001740
Iteration 58/1000 | Loss: 0.00001740
Iteration 59/1000 | Loss: 0.00001740
Iteration 60/1000 | Loss: 0.00001740
Iteration 61/1000 | Loss: 0.00001740
Iteration 62/1000 | Loss: 0.00001740
Iteration 63/1000 | Loss: 0.00001740
Iteration 64/1000 | Loss: 0.00001739
Iteration 65/1000 | Loss: 0.00001739
Iteration 66/1000 | Loss: 0.00001739
Iteration 67/1000 | Loss: 0.00001738
Iteration 68/1000 | Loss: 0.00001738
Iteration 69/1000 | Loss: 0.00001737
Iteration 70/1000 | Loss: 0.00001737
Iteration 71/1000 | Loss: 0.00001737
Iteration 72/1000 | Loss: 0.00001737
Iteration 73/1000 | Loss: 0.00001737
Iteration 74/1000 | Loss: 0.00001737
Iteration 75/1000 | Loss: 0.00001737
Iteration 76/1000 | Loss: 0.00001736
Iteration 77/1000 | Loss: 0.00001736
Iteration 78/1000 | Loss: 0.00001736
Iteration 79/1000 | Loss: 0.00001736
Iteration 80/1000 | Loss: 0.00001735
Iteration 81/1000 | Loss: 0.00001735
Iteration 82/1000 | Loss: 0.00001735
Iteration 83/1000 | Loss: 0.00001735
Iteration 84/1000 | Loss: 0.00001735
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001735
Iteration 88/1000 | Loss: 0.00001735
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001732
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001731
Iteration 105/1000 | Loss: 0.00001731
Iteration 106/1000 | Loss: 0.00001731
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001730
Iteration 109/1000 | Loss: 0.00001730
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001730
Iteration 113/1000 | Loss: 0.00001730
Iteration 114/1000 | Loss: 0.00001730
Iteration 115/1000 | Loss: 0.00001730
Iteration 116/1000 | Loss: 0.00001730
Iteration 117/1000 | Loss: 0.00001730
Iteration 118/1000 | Loss: 0.00001729
Iteration 119/1000 | Loss: 0.00001729
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001727
Iteration 126/1000 | Loss: 0.00001727
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001725
Iteration 130/1000 | Loss: 0.00001725
Iteration 131/1000 | Loss: 0.00001725
Iteration 132/1000 | Loss: 0.00001725
Iteration 133/1000 | Loss: 0.00001725
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001725
Iteration 136/1000 | Loss: 0.00001725
Iteration 137/1000 | Loss: 0.00001724
Iteration 138/1000 | Loss: 0.00001724
Iteration 139/1000 | Loss: 0.00001724
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001723
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.7223046597791836e-05, 1.7223046597791836e-05, 1.7223046597791836e-05, 1.7223046597791836e-05, 1.7223046597791836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7223046597791836e-05

Optimization complete. Final v2v error: 3.494614362716675 mm

Highest mean error: 4.055756568908691 mm for frame 106

Lowest mean error: 3.305387020111084 mm for frame 130

Saving results

Total time: 35.4410924911499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844025
Iteration 2/25 | Loss: 0.00081844
Iteration 3/25 | Loss: 0.00063638
Iteration 4/25 | Loss: 0.00060923
Iteration 5/25 | Loss: 0.00060199
Iteration 6/25 | Loss: 0.00059934
Iteration 7/25 | Loss: 0.00059855
Iteration 8/25 | Loss: 0.00059849
Iteration 9/25 | Loss: 0.00059849
Iteration 10/25 | Loss: 0.00059849
Iteration 11/25 | Loss: 0.00059849
Iteration 12/25 | Loss: 0.00059849
Iteration 13/25 | Loss: 0.00059849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005984861636534333, 0.0005984861636534333, 0.0005984861636534333, 0.0005984861636534333, 0.0005984861636534333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005984861636534333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45369518
Iteration 2/25 | Loss: 0.00026946
Iteration 3/25 | Loss: 0.00026946
Iteration 4/25 | Loss: 0.00026946
Iteration 5/25 | Loss: 0.00026946
Iteration 6/25 | Loss: 0.00026946
Iteration 7/25 | Loss: 0.00026945
Iteration 8/25 | Loss: 0.00026945
Iteration 9/25 | Loss: 0.00026945
Iteration 10/25 | Loss: 0.00026945
Iteration 11/25 | Loss: 0.00026945
Iteration 12/25 | Loss: 0.00026945
Iteration 13/25 | Loss: 0.00026945
Iteration 14/25 | Loss: 0.00026945
Iteration 15/25 | Loss: 0.00026945
Iteration 16/25 | Loss: 0.00026945
Iteration 17/25 | Loss: 0.00026945
Iteration 18/25 | Loss: 0.00026945
Iteration 19/25 | Loss: 0.00026945
Iteration 20/25 | Loss: 0.00026945
Iteration 21/25 | Loss: 0.00026945
Iteration 22/25 | Loss: 0.00026945
Iteration 23/25 | Loss: 0.00026945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002694541763048619, 0.0002694541763048619, 0.0002694541763048619, 0.0002694541763048619, 0.0002694541763048619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002694541763048619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026945
Iteration 2/1000 | Loss: 0.00002179
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001195
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001155
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001131
Iteration 20/1000 | Loss: 0.00001130
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001127
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001124
Iteration 37/1000 | Loss: 0.00001123
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001122
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001121
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001117
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001115
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001113
Iteration 64/1000 | Loss: 0.00001113
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001112
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001111
Iteration 79/1000 | Loss: 0.00001111
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001108
Iteration 117/1000 | Loss: 0.00001108
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001106
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001106
Iteration 140/1000 | Loss: 0.00001106
Iteration 141/1000 | Loss: 0.00001106
Iteration 142/1000 | Loss: 0.00001106
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001105
Iteration 148/1000 | Loss: 0.00001105
Iteration 149/1000 | Loss: 0.00001105
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001104
Iteration 162/1000 | Loss: 0.00001104
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001104
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001104
Iteration 168/1000 | Loss: 0.00001104
Iteration 169/1000 | Loss: 0.00001104
Iteration 170/1000 | Loss: 0.00001104
Iteration 171/1000 | Loss: 0.00001104
Iteration 172/1000 | Loss: 0.00001104
Iteration 173/1000 | Loss: 0.00001104
Iteration 174/1000 | Loss: 0.00001104
Iteration 175/1000 | Loss: 0.00001104
Iteration 176/1000 | Loss: 0.00001103
Iteration 177/1000 | Loss: 0.00001103
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001103
Iteration 181/1000 | Loss: 0.00001103
Iteration 182/1000 | Loss: 0.00001103
Iteration 183/1000 | Loss: 0.00001103
Iteration 184/1000 | Loss: 0.00001103
Iteration 185/1000 | Loss: 0.00001103
Iteration 186/1000 | Loss: 0.00001103
Iteration 187/1000 | Loss: 0.00001103
Iteration 188/1000 | Loss: 0.00001103
Iteration 189/1000 | Loss: 0.00001103
Iteration 190/1000 | Loss: 0.00001103
Iteration 191/1000 | Loss: 0.00001103
Iteration 192/1000 | Loss: 0.00001103
Iteration 193/1000 | Loss: 0.00001103
Iteration 194/1000 | Loss: 0.00001103
Iteration 195/1000 | Loss: 0.00001103
Iteration 196/1000 | Loss: 0.00001103
Iteration 197/1000 | Loss: 0.00001103
Iteration 198/1000 | Loss: 0.00001103
Iteration 199/1000 | Loss: 0.00001103
Iteration 200/1000 | Loss: 0.00001103
Iteration 201/1000 | Loss: 0.00001103
Iteration 202/1000 | Loss: 0.00001103
Iteration 203/1000 | Loss: 0.00001103
Iteration 204/1000 | Loss: 0.00001103
Iteration 205/1000 | Loss: 0.00001103
Iteration 206/1000 | Loss: 0.00001103
Iteration 207/1000 | Loss: 0.00001103
Iteration 208/1000 | Loss: 0.00001103
Iteration 209/1000 | Loss: 0.00001103
Iteration 210/1000 | Loss: 0.00001103
Iteration 211/1000 | Loss: 0.00001103
Iteration 212/1000 | Loss: 0.00001103
Iteration 213/1000 | Loss: 0.00001103
Iteration 214/1000 | Loss: 0.00001103
Iteration 215/1000 | Loss: 0.00001103
Iteration 216/1000 | Loss: 0.00001103
Iteration 217/1000 | Loss: 0.00001103
Iteration 218/1000 | Loss: 0.00001103
Iteration 219/1000 | Loss: 0.00001103
Iteration 220/1000 | Loss: 0.00001103
Iteration 221/1000 | Loss: 0.00001103
Iteration 222/1000 | Loss: 0.00001103
Iteration 223/1000 | Loss: 0.00001103
Iteration 224/1000 | Loss: 0.00001103
Iteration 225/1000 | Loss: 0.00001103
Iteration 226/1000 | Loss: 0.00001103
Iteration 227/1000 | Loss: 0.00001103
Iteration 228/1000 | Loss: 0.00001103
Iteration 229/1000 | Loss: 0.00001103
Iteration 230/1000 | Loss: 0.00001103
Iteration 231/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.1025756066374015e-05, 1.1025756066374015e-05, 1.1025756066374015e-05, 1.1025756066374015e-05, 1.1025756066374015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1025756066374015e-05

Optimization complete. Final v2v error: 2.7588186264038086 mm

Highest mean error: 3.6759490966796875 mm for frame 58

Lowest mean error: 2.4238698482513428 mm for frame 131

Saving results

Total time: 40.642765283584595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536798
Iteration 2/25 | Loss: 0.00077557
Iteration 3/25 | Loss: 0.00064410
Iteration 4/25 | Loss: 0.00062003
Iteration 5/25 | Loss: 0.00061228
Iteration 6/25 | Loss: 0.00061060
Iteration 7/25 | Loss: 0.00061019
Iteration 8/25 | Loss: 0.00061019
Iteration 9/25 | Loss: 0.00061019
Iteration 10/25 | Loss: 0.00061019
Iteration 11/25 | Loss: 0.00061019
Iteration 12/25 | Loss: 0.00061019
Iteration 13/25 | Loss: 0.00061019
Iteration 14/25 | Loss: 0.00061019
Iteration 15/25 | Loss: 0.00061019
Iteration 16/25 | Loss: 0.00061019
Iteration 17/25 | Loss: 0.00061019
Iteration 18/25 | Loss: 0.00061019
Iteration 19/25 | Loss: 0.00061019
Iteration 20/25 | Loss: 0.00061019
Iteration 21/25 | Loss: 0.00061019
Iteration 22/25 | Loss: 0.00061019
Iteration 23/25 | Loss: 0.00061019
Iteration 24/25 | Loss: 0.00061019
Iteration 25/25 | Loss: 0.00061019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006101934122852981, 0.0006101934122852981, 0.0006101934122852981, 0.0006101934122852981, 0.0006101934122852981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006101934122852981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.79072952
Iteration 2/25 | Loss: 0.00025833
Iteration 3/25 | Loss: 0.00025832
Iteration 4/25 | Loss: 0.00025832
Iteration 5/25 | Loss: 0.00025831
Iteration 6/25 | Loss: 0.00025831
Iteration 7/25 | Loss: 0.00025831
Iteration 8/25 | Loss: 0.00025831
Iteration 9/25 | Loss: 0.00025831
Iteration 10/25 | Loss: 0.00025831
Iteration 11/25 | Loss: 0.00025831
Iteration 12/25 | Loss: 0.00025831
Iteration 13/25 | Loss: 0.00025831
Iteration 14/25 | Loss: 0.00025831
Iteration 15/25 | Loss: 0.00025831
Iteration 16/25 | Loss: 0.00025831
Iteration 17/25 | Loss: 0.00025831
Iteration 18/25 | Loss: 0.00025831
Iteration 19/25 | Loss: 0.00025831
Iteration 20/25 | Loss: 0.00025831
Iteration 21/25 | Loss: 0.00025831
Iteration 22/25 | Loss: 0.00025831
Iteration 23/25 | Loss: 0.00025831
Iteration 24/25 | Loss: 0.00025831
Iteration 25/25 | Loss: 0.00025831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025831
Iteration 2/1000 | Loss: 0.00002710
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001622
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001482
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001480
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001464
Iteration 19/1000 | Loss: 0.00001463
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001457
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001453
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001449
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001448
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Iteration 75/1000 | Loss: 0.00001448
Iteration 76/1000 | Loss: 0.00001448
Iteration 77/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.4481746802630369e-05, 1.4481746802630369e-05, 1.4481746802630369e-05, 1.4481746802630369e-05, 1.4481746802630369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4481746802630369e-05

Optimization complete. Final v2v error: 3.2322630882263184 mm

Highest mean error: 3.8463709354400635 mm for frame 70

Lowest mean error: 2.9620678424835205 mm for frame 31

Saving results

Total time: 30.240509748458862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840354
Iteration 2/25 | Loss: 0.00107521
Iteration 3/25 | Loss: 0.00078419
Iteration 4/25 | Loss: 0.00072464
Iteration 5/25 | Loss: 0.00070504
Iteration 6/25 | Loss: 0.00070280
Iteration 7/25 | Loss: 0.00070212
Iteration 8/25 | Loss: 0.00070212
Iteration 9/25 | Loss: 0.00070212
Iteration 10/25 | Loss: 0.00070212
Iteration 11/25 | Loss: 0.00070212
Iteration 12/25 | Loss: 0.00070212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007021230994723737, 0.0007021230994723737, 0.0007021230994723737, 0.0007021230994723737, 0.0007021230994723737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007021230994723737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41794813
Iteration 2/25 | Loss: 0.00034927
Iteration 3/25 | Loss: 0.00034927
Iteration 4/25 | Loss: 0.00034927
Iteration 5/25 | Loss: 0.00034927
Iteration 6/25 | Loss: 0.00034927
Iteration 7/25 | Loss: 0.00034927
Iteration 8/25 | Loss: 0.00034927
Iteration 9/25 | Loss: 0.00034927
Iteration 10/25 | Loss: 0.00034927
Iteration 11/25 | Loss: 0.00034927
Iteration 12/25 | Loss: 0.00034927
Iteration 13/25 | Loss: 0.00034927
Iteration 14/25 | Loss: 0.00034927
Iteration 15/25 | Loss: 0.00034927
Iteration 16/25 | Loss: 0.00034927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00034926689113490283, 0.00034926689113490283, 0.00034926689113490283, 0.00034926689113490283, 0.00034926689113490283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034926689113490283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034927
Iteration 2/1000 | Loss: 0.00003962
Iteration 3/1000 | Loss: 0.00002893
Iteration 4/1000 | Loss: 0.00002559
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00002310
Iteration 7/1000 | Loss: 0.00002242
Iteration 8/1000 | Loss: 0.00002192
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002120
Iteration 11/1000 | Loss: 0.00002099
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002082
Iteration 14/1000 | Loss: 0.00002068
Iteration 15/1000 | Loss: 0.00002059
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002046
Iteration 18/1000 | Loss: 0.00002044
Iteration 19/1000 | Loss: 0.00002044
Iteration 20/1000 | Loss: 0.00002044
Iteration 21/1000 | Loss: 0.00002043
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00002042
Iteration 24/1000 | Loss: 0.00002042
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002042
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002041
Iteration 34/1000 | Loss: 0.00002041
Iteration 35/1000 | Loss: 0.00002040
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002040
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002039
Iteration 40/1000 | Loss: 0.00002039
Iteration 41/1000 | Loss: 0.00002039
Iteration 42/1000 | Loss: 0.00002039
Iteration 43/1000 | Loss: 0.00002039
Iteration 44/1000 | Loss: 0.00002039
Iteration 45/1000 | Loss: 0.00002039
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002038
Iteration 50/1000 | Loss: 0.00002038
Iteration 51/1000 | Loss: 0.00002038
Iteration 52/1000 | Loss: 0.00002038
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002038
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002038
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002037
Iteration 63/1000 | Loss: 0.00002037
Iteration 64/1000 | Loss: 0.00002037
Iteration 65/1000 | Loss: 0.00002037
Iteration 66/1000 | Loss: 0.00002037
Iteration 67/1000 | Loss: 0.00002037
Iteration 68/1000 | Loss: 0.00002037
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002036
Iteration 71/1000 | Loss: 0.00002036
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002036
Iteration 80/1000 | Loss: 0.00002036
Iteration 81/1000 | Loss: 0.00002036
Iteration 82/1000 | Loss: 0.00002036
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002035
Iteration 90/1000 | Loss: 0.00002035
Iteration 91/1000 | Loss: 0.00002035
Iteration 92/1000 | Loss: 0.00002035
Iteration 93/1000 | Loss: 0.00002035
Iteration 94/1000 | Loss: 0.00002034
Iteration 95/1000 | Loss: 0.00002034
Iteration 96/1000 | Loss: 0.00002034
Iteration 97/1000 | Loss: 0.00002034
Iteration 98/1000 | Loss: 0.00002034
Iteration 99/1000 | Loss: 0.00002034
Iteration 100/1000 | Loss: 0.00002034
Iteration 101/1000 | Loss: 0.00002034
Iteration 102/1000 | Loss: 0.00002034
Iteration 103/1000 | Loss: 0.00002034
Iteration 104/1000 | Loss: 0.00002034
Iteration 105/1000 | Loss: 0.00002034
Iteration 106/1000 | Loss: 0.00002034
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002033
Iteration 114/1000 | Loss: 0.00002033
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002033
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002032
Iteration 123/1000 | Loss: 0.00002032
Iteration 124/1000 | Loss: 0.00002032
Iteration 125/1000 | Loss: 0.00002032
Iteration 126/1000 | Loss: 0.00002032
Iteration 127/1000 | Loss: 0.00002032
Iteration 128/1000 | Loss: 0.00002032
Iteration 129/1000 | Loss: 0.00002032
Iteration 130/1000 | Loss: 0.00002032
Iteration 131/1000 | Loss: 0.00002032
Iteration 132/1000 | Loss: 0.00002032
Iteration 133/1000 | Loss: 0.00002032
Iteration 134/1000 | Loss: 0.00002032
Iteration 135/1000 | Loss: 0.00002032
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00002031
Iteration 140/1000 | Loss: 0.00002031
Iteration 141/1000 | Loss: 0.00002031
Iteration 142/1000 | Loss: 0.00002031
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002031
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.03147683350835e-05, 2.03147683350835e-05, 2.03147683350835e-05, 2.03147683350835e-05, 2.03147683350835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.03147683350835e-05

Optimization complete. Final v2v error: 3.795314073562622 mm

Highest mean error: 5.007646083831787 mm for frame 151

Lowest mean error: 3.2685470581054688 mm for frame 34

Saving results

Total time: 46.653218507766724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822446
Iteration 2/25 | Loss: 0.00079499
Iteration 3/25 | Loss: 0.00062760
Iteration 4/25 | Loss: 0.00060186
Iteration 5/25 | Loss: 0.00059562
Iteration 6/25 | Loss: 0.00059411
Iteration 7/25 | Loss: 0.00059393
Iteration 8/25 | Loss: 0.00059393
Iteration 9/25 | Loss: 0.00059393
Iteration 10/25 | Loss: 0.00059393
Iteration 11/25 | Loss: 0.00059393
Iteration 12/25 | Loss: 0.00059393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005939251277595758, 0.0005939251277595758, 0.0005939251277595758, 0.0005939251277595758, 0.0005939251277595758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005939251277595758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46106613
Iteration 2/25 | Loss: 0.00027747
Iteration 3/25 | Loss: 0.00027747
Iteration 4/25 | Loss: 0.00027747
Iteration 5/25 | Loss: 0.00027747
Iteration 6/25 | Loss: 0.00027747
Iteration 7/25 | Loss: 0.00027747
Iteration 8/25 | Loss: 0.00027747
Iteration 9/25 | Loss: 0.00027747
Iteration 10/25 | Loss: 0.00027747
Iteration 11/25 | Loss: 0.00027747
Iteration 12/25 | Loss: 0.00027747
Iteration 13/25 | Loss: 0.00027747
Iteration 14/25 | Loss: 0.00027747
Iteration 15/25 | Loss: 0.00027747
Iteration 16/25 | Loss: 0.00027747
Iteration 17/25 | Loss: 0.00027747
Iteration 18/25 | Loss: 0.00027747
Iteration 19/25 | Loss: 0.00027747
Iteration 20/25 | Loss: 0.00027747
Iteration 21/25 | Loss: 0.00027747
Iteration 22/25 | Loss: 0.00027747
Iteration 23/25 | Loss: 0.00027747
Iteration 24/25 | Loss: 0.00027747
Iteration 25/25 | Loss: 0.00027747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027747
Iteration 2/1000 | Loss: 0.00001905
Iteration 3/1000 | Loss: 0.00001275
Iteration 4/1000 | Loss: 0.00001189
Iteration 5/1000 | Loss: 0.00001127
Iteration 6/1000 | Loss: 0.00001099
Iteration 7/1000 | Loss: 0.00001078
Iteration 8/1000 | Loss: 0.00001071
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001062
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001058
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001057
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001053
Iteration 22/1000 | Loss: 0.00001053
Iteration 23/1000 | Loss: 0.00001051
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001049
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001046
Iteration 37/1000 | Loss: 0.00001045
Iteration 38/1000 | Loss: 0.00001045
Iteration 39/1000 | Loss: 0.00001045
Iteration 40/1000 | Loss: 0.00001045
Iteration 41/1000 | Loss: 0.00001044
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001043
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001042
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001040
Iteration 50/1000 | Loss: 0.00001040
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001040
Iteration 53/1000 | Loss: 0.00001040
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001038
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001037
Iteration 59/1000 | Loss: 0.00001037
Iteration 60/1000 | Loss: 0.00001036
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001035
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001032
Iteration 69/1000 | Loss: 0.00001032
Iteration 70/1000 | Loss: 0.00001032
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001030
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001030
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001029
Iteration 90/1000 | Loss: 0.00001029
Iteration 91/1000 | Loss: 0.00001029
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001029
Iteration 99/1000 | Loss: 0.00001029
Iteration 100/1000 | Loss: 0.00001029
Iteration 101/1000 | Loss: 0.00001028
Iteration 102/1000 | Loss: 0.00001028
Iteration 103/1000 | Loss: 0.00001028
Iteration 104/1000 | Loss: 0.00001028
Iteration 105/1000 | Loss: 0.00001028
Iteration 106/1000 | Loss: 0.00001028
Iteration 107/1000 | Loss: 0.00001028
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001026
Iteration 117/1000 | Loss: 0.00001026
Iteration 118/1000 | Loss: 0.00001026
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001025
Iteration 126/1000 | Loss: 0.00001025
Iteration 127/1000 | Loss: 0.00001025
Iteration 128/1000 | Loss: 0.00001025
Iteration 129/1000 | Loss: 0.00001025
Iteration 130/1000 | Loss: 0.00001025
Iteration 131/1000 | Loss: 0.00001024
Iteration 132/1000 | Loss: 0.00001024
Iteration 133/1000 | Loss: 0.00001024
Iteration 134/1000 | Loss: 0.00001024
Iteration 135/1000 | Loss: 0.00001024
Iteration 136/1000 | Loss: 0.00001024
Iteration 137/1000 | Loss: 0.00001024
Iteration 138/1000 | Loss: 0.00001024
Iteration 139/1000 | Loss: 0.00001024
Iteration 140/1000 | Loss: 0.00001024
Iteration 141/1000 | Loss: 0.00001024
Iteration 142/1000 | Loss: 0.00001023
Iteration 143/1000 | Loss: 0.00001023
Iteration 144/1000 | Loss: 0.00001023
Iteration 145/1000 | Loss: 0.00001023
Iteration 146/1000 | Loss: 0.00001022
Iteration 147/1000 | Loss: 0.00001022
Iteration 148/1000 | Loss: 0.00001022
Iteration 149/1000 | Loss: 0.00001022
Iteration 150/1000 | Loss: 0.00001022
Iteration 151/1000 | Loss: 0.00001022
Iteration 152/1000 | Loss: 0.00001022
Iteration 153/1000 | Loss: 0.00001020
Iteration 154/1000 | Loss: 0.00001020
Iteration 155/1000 | Loss: 0.00001020
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001018
Iteration 168/1000 | Loss: 0.00001018
Iteration 169/1000 | Loss: 0.00001018
Iteration 170/1000 | Loss: 0.00001018
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001018
Iteration 174/1000 | Loss: 0.00001017
Iteration 175/1000 | Loss: 0.00001017
Iteration 176/1000 | Loss: 0.00001017
Iteration 177/1000 | Loss: 0.00001017
Iteration 178/1000 | Loss: 0.00001017
Iteration 179/1000 | Loss: 0.00001017
Iteration 180/1000 | Loss: 0.00001017
Iteration 181/1000 | Loss: 0.00001017
Iteration 182/1000 | Loss: 0.00001017
Iteration 183/1000 | Loss: 0.00001017
Iteration 184/1000 | Loss: 0.00001017
Iteration 185/1000 | Loss: 0.00001017
Iteration 186/1000 | Loss: 0.00001017
Iteration 187/1000 | Loss: 0.00001017
Iteration 188/1000 | Loss: 0.00001017
Iteration 189/1000 | Loss: 0.00001017
Iteration 190/1000 | Loss: 0.00001017
Iteration 191/1000 | Loss: 0.00001017
Iteration 192/1000 | Loss: 0.00001016
Iteration 193/1000 | Loss: 0.00001016
Iteration 194/1000 | Loss: 0.00001016
Iteration 195/1000 | Loss: 0.00001016
Iteration 196/1000 | Loss: 0.00001016
Iteration 197/1000 | Loss: 0.00001016
Iteration 198/1000 | Loss: 0.00001016
Iteration 199/1000 | Loss: 0.00001016
Iteration 200/1000 | Loss: 0.00001016
Iteration 201/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.0163354090764187e-05, 1.0163354090764187e-05, 1.0163354090764187e-05, 1.0163354090764187e-05, 1.0163354090764187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0163354090764187e-05

Optimization complete. Final v2v error: 2.681478977203369 mm

Highest mean error: 2.843975305557251 mm for frame 84

Lowest mean error: 2.545633316040039 mm for frame 147

Saving results

Total time: 35.583852767944336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821186
Iteration 2/25 | Loss: 0.00106121
Iteration 3/25 | Loss: 0.00080568
Iteration 4/25 | Loss: 0.00073287
Iteration 5/25 | Loss: 0.00071260
Iteration 6/25 | Loss: 0.00070972
Iteration 7/25 | Loss: 0.00070844
Iteration 8/25 | Loss: 0.00070802
Iteration 9/25 | Loss: 0.00070802
Iteration 10/25 | Loss: 0.00070802
Iteration 11/25 | Loss: 0.00070802
Iteration 12/25 | Loss: 0.00070802
Iteration 13/25 | Loss: 0.00070802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007080209325067699, 0.0007080209325067699, 0.0007080209325067699, 0.0007080209325067699, 0.0007080209325067699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007080209325067699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43375099
Iteration 2/25 | Loss: 0.00034056
Iteration 3/25 | Loss: 0.00034056
Iteration 4/25 | Loss: 0.00034056
Iteration 5/25 | Loss: 0.00034056
Iteration 6/25 | Loss: 0.00034056
Iteration 7/25 | Loss: 0.00034055
Iteration 8/25 | Loss: 0.00034055
Iteration 9/25 | Loss: 0.00034055
Iteration 10/25 | Loss: 0.00034055
Iteration 11/25 | Loss: 0.00034055
Iteration 12/25 | Loss: 0.00034055
Iteration 13/25 | Loss: 0.00034055
Iteration 14/25 | Loss: 0.00034055
Iteration 15/25 | Loss: 0.00034055
Iteration 16/25 | Loss: 0.00034055
Iteration 17/25 | Loss: 0.00034055
Iteration 18/25 | Loss: 0.00034055
Iteration 19/25 | Loss: 0.00034055
Iteration 20/25 | Loss: 0.00034055
Iteration 21/25 | Loss: 0.00034055
Iteration 22/25 | Loss: 0.00034055
Iteration 23/25 | Loss: 0.00034055
Iteration 24/25 | Loss: 0.00034055
Iteration 25/25 | Loss: 0.00034055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034055
Iteration 2/1000 | Loss: 0.00004976
Iteration 3/1000 | Loss: 0.00003603
Iteration 4/1000 | Loss: 0.00002870
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00002525
Iteration 7/1000 | Loss: 0.00002428
Iteration 8/1000 | Loss: 0.00002361
Iteration 9/1000 | Loss: 0.00002308
Iteration 10/1000 | Loss: 0.00002282
Iteration 11/1000 | Loss: 0.00002259
Iteration 12/1000 | Loss: 0.00002243
Iteration 13/1000 | Loss: 0.00002226
Iteration 14/1000 | Loss: 0.00002219
Iteration 15/1000 | Loss: 0.00002212
Iteration 16/1000 | Loss: 0.00002211
Iteration 17/1000 | Loss: 0.00002210
Iteration 18/1000 | Loss: 0.00002207
Iteration 19/1000 | Loss: 0.00002207
Iteration 20/1000 | Loss: 0.00002206
Iteration 21/1000 | Loss: 0.00002204
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002201
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002198
Iteration 27/1000 | Loss: 0.00002198
Iteration 28/1000 | Loss: 0.00002197
Iteration 29/1000 | Loss: 0.00002193
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00002190
Iteration 36/1000 | Loss: 0.00002190
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002190
Iteration 40/1000 | Loss: 0.00002190
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00002189
Iteration 43/1000 | Loss: 0.00002188
Iteration 44/1000 | Loss: 0.00002188
Iteration 45/1000 | Loss: 0.00002187
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002184
Iteration 55/1000 | Loss: 0.00002184
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002184
Iteration 58/1000 | Loss: 0.00002184
Iteration 59/1000 | Loss: 0.00002183
Iteration 60/1000 | Loss: 0.00002183
Iteration 61/1000 | Loss: 0.00002183
Iteration 62/1000 | Loss: 0.00002183
Iteration 63/1000 | Loss: 0.00002183
Iteration 64/1000 | Loss: 0.00002183
Iteration 65/1000 | Loss: 0.00002183
Iteration 66/1000 | Loss: 0.00002183
Iteration 67/1000 | Loss: 0.00002183
Iteration 68/1000 | Loss: 0.00002182
Iteration 69/1000 | Loss: 0.00002182
Iteration 70/1000 | Loss: 0.00002182
Iteration 71/1000 | Loss: 0.00002182
Iteration 72/1000 | Loss: 0.00002182
Iteration 73/1000 | Loss: 0.00002182
Iteration 74/1000 | Loss: 0.00002182
Iteration 75/1000 | Loss: 0.00002182
Iteration 76/1000 | Loss: 0.00002181
Iteration 77/1000 | Loss: 0.00002181
Iteration 78/1000 | Loss: 0.00002181
Iteration 79/1000 | Loss: 0.00002181
Iteration 80/1000 | Loss: 0.00002181
Iteration 81/1000 | Loss: 0.00002181
Iteration 82/1000 | Loss: 0.00002181
Iteration 83/1000 | Loss: 0.00002180
Iteration 84/1000 | Loss: 0.00002180
Iteration 85/1000 | Loss: 0.00002180
Iteration 86/1000 | Loss: 0.00002180
Iteration 87/1000 | Loss: 0.00002179
Iteration 88/1000 | Loss: 0.00002179
Iteration 89/1000 | Loss: 0.00002179
Iteration 90/1000 | Loss: 0.00002179
Iteration 91/1000 | Loss: 0.00002179
Iteration 92/1000 | Loss: 0.00002179
Iteration 93/1000 | Loss: 0.00002179
Iteration 94/1000 | Loss: 0.00002179
Iteration 95/1000 | Loss: 0.00002179
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002177
Iteration 101/1000 | Loss: 0.00002177
Iteration 102/1000 | Loss: 0.00002177
Iteration 103/1000 | Loss: 0.00002177
Iteration 104/1000 | Loss: 0.00002177
Iteration 105/1000 | Loss: 0.00002176
Iteration 106/1000 | Loss: 0.00002176
Iteration 107/1000 | Loss: 0.00002176
Iteration 108/1000 | Loss: 0.00002176
Iteration 109/1000 | Loss: 0.00002176
Iteration 110/1000 | Loss: 0.00002176
Iteration 111/1000 | Loss: 0.00002176
Iteration 112/1000 | Loss: 0.00002176
Iteration 113/1000 | Loss: 0.00002176
Iteration 114/1000 | Loss: 0.00002176
Iteration 115/1000 | Loss: 0.00002175
Iteration 116/1000 | Loss: 0.00002175
Iteration 117/1000 | Loss: 0.00002175
Iteration 118/1000 | Loss: 0.00002175
Iteration 119/1000 | Loss: 0.00002175
Iteration 120/1000 | Loss: 0.00002175
Iteration 121/1000 | Loss: 0.00002175
Iteration 122/1000 | Loss: 0.00002175
Iteration 123/1000 | Loss: 0.00002175
Iteration 124/1000 | Loss: 0.00002175
Iteration 125/1000 | Loss: 0.00002175
Iteration 126/1000 | Loss: 0.00002175
Iteration 127/1000 | Loss: 0.00002175
Iteration 128/1000 | Loss: 0.00002175
Iteration 129/1000 | Loss: 0.00002175
Iteration 130/1000 | Loss: 0.00002174
Iteration 131/1000 | Loss: 0.00002174
Iteration 132/1000 | Loss: 0.00002174
Iteration 133/1000 | Loss: 0.00002174
Iteration 134/1000 | Loss: 0.00002174
Iteration 135/1000 | Loss: 0.00002174
Iteration 136/1000 | Loss: 0.00002174
Iteration 137/1000 | Loss: 0.00002174
Iteration 138/1000 | Loss: 0.00002174
Iteration 139/1000 | Loss: 0.00002173
Iteration 140/1000 | Loss: 0.00002173
Iteration 141/1000 | Loss: 0.00002173
Iteration 142/1000 | Loss: 0.00002173
Iteration 143/1000 | Loss: 0.00002173
Iteration 144/1000 | Loss: 0.00002173
Iteration 145/1000 | Loss: 0.00002173
Iteration 146/1000 | Loss: 0.00002173
Iteration 147/1000 | Loss: 0.00002173
Iteration 148/1000 | Loss: 0.00002173
Iteration 149/1000 | Loss: 0.00002173
Iteration 150/1000 | Loss: 0.00002173
Iteration 151/1000 | Loss: 0.00002173
Iteration 152/1000 | Loss: 0.00002173
Iteration 153/1000 | Loss: 0.00002173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.172791755583603e-05, 2.172791755583603e-05, 2.172791755583603e-05, 2.172791755583603e-05, 2.172791755583603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.172791755583603e-05

Optimization complete. Final v2v error: 3.9177708625793457 mm

Highest mean error: 4.464234352111816 mm for frame 58

Lowest mean error: 3.2734668254852295 mm for frame 132

Saving results

Total time: 41.77562212944031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861941
Iteration 2/25 | Loss: 0.00083551
Iteration 3/25 | Loss: 0.00068059
Iteration 4/25 | Loss: 0.00064682
Iteration 5/25 | Loss: 0.00063946
Iteration 6/25 | Loss: 0.00063840
Iteration 7/25 | Loss: 0.00063827
Iteration 8/25 | Loss: 0.00063827
Iteration 9/25 | Loss: 0.00063827
Iteration 10/25 | Loss: 0.00063827
Iteration 11/25 | Loss: 0.00063827
Iteration 12/25 | Loss: 0.00063827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000638273311778903, 0.000638273311778903, 0.000638273311778903, 0.000638273311778903, 0.000638273311778903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000638273311778903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40190697
Iteration 2/25 | Loss: 0.00022588
Iteration 3/25 | Loss: 0.00022580
Iteration 4/25 | Loss: 0.00022580
Iteration 5/25 | Loss: 0.00022580
Iteration 6/25 | Loss: 0.00022580
Iteration 7/25 | Loss: 0.00022580
Iteration 8/25 | Loss: 0.00022580
Iteration 9/25 | Loss: 0.00022580
Iteration 10/25 | Loss: 0.00022580
Iteration 11/25 | Loss: 0.00022580
Iteration 12/25 | Loss: 0.00022580
Iteration 13/25 | Loss: 0.00022580
Iteration 14/25 | Loss: 0.00022580
Iteration 15/25 | Loss: 0.00022580
Iteration 16/25 | Loss: 0.00022580
Iteration 17/25 | Loss: 0.00022580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002258008753415197, 0.0002258008753415197, 0.0002258008753415197, 0.0002258008753415197, 0.0002258008753415197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002258008753415197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022580
Iteration 2/1000 | Loss: 0.00003184
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00001748
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001437
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001367
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001298
Iteration 38/1000 | Loss: 0.00001298
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001297
Iteration 41/1000 | Loss: 0.00001297
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001294
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001286
Iteration 70/1000 | Loss: 0.00001286
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001282
Iteration 108/1000 | Loss: 0.00001282
Iteration 109/1000 | Loss: 0.00001282
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001282
Iteration 112/1000 | Loss: 0.00001282
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001282
Iteration 116/1000 | Loss: 0.00001282
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001282
Iteration 120/1000 | Loss: 0.00001282
Iteration 121/1000 | Loss: 0.00001282
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001282
Iteration 124/1000 | Loss: 0.00001282
Iteration 125/1000 | Loss: 0.00001282
Iteration 126/1000 | Loss: 0.00001282
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001282
Iteration 137/1000 | Loss: 0.00001282
Iteration 138/1000 | Loss: 0.00001282
Iteration 139/1000 | Loss: 0.00001282
Iteration 140/1000 | Loss: 0.00001282
Iteration 141/1000 | Loss: 0.00001282
Iteration 142/1000 | Loss: 0.00001282
Iteration 143/1000 | Loss: 0.00001282
Iteration 144/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2818343748222105e-05, 1.2818343748222105e-05, 1.2818343748222105e-05, 1.2818343748222105e-05, 1.2818343748222105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2818343748222105e-05

Optimization complete. Final v2v error: 3.1190667152404785 mm

Highest mean error: 3.4952685832977295 mm for frame 1

Lowest mean error: 2.936119794845581 mm for frame 101

Saving results

Total time: 34.63779544830322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137848
Iteration 2/25 | Loss: 0.00175153
Iteration 3/25 | Loss: 0.00101677
Iteration 4/25 | Loss: 0.00092156
Iteration 5/25 | Loss: 0.00088708
Iteration 6/25 | Loss: 0.00089085
Iteration 7/25 | Loss: 0.00089764
Iteration 8/25 | Loss: 0.00088840
Iteration 9/25 | Loss: 0.00088873
Iteration 10/25 | Loss: 0.00088620
Iteration 11/25 | Loss: 0.00087265
Iteration 12/25 | Loss: 0.00086796
Iteration 13/25 | Loss: 0.00086608
Iteration 14/25 | Loss: 0.00086457
Iteration 15/25 | Loss: 0.00086542
Iteration 16/25 | Loss: 0.00086918
Iteration 17/25 | Loss: 0.00086829
Iteration 18/25 | Loss: 0.00085744
Iteration 19/25 | Loss: 0.00086495
Iteration 20/25 | Loss: 0.00086211
Iteration 21/25 | Loss: 0.00086147
Iteration 22/25 | Loss: 0.00085627
Iteration 23/25 | Loss: 0.00085544
Iteration 24/25 | Loss: 0.00085313
Iteration 25/25 | Loss: 0.00085993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05269063
Iteration 2/25 | Loss: 0.00093567
Iteration 3/25 | Loss: 0.00093566
Iteration 4/25 | Loss: 0.00093566
Iteration 5/25 | Loss: 0.00093566
Iteration 6/25 | Loss: 0.00093566
Iteration 7/25 | Loss: 0.00093566
Iteration 8/25 | Loss: 0.00093566
Iteration 9/25 | Loss: 0.00093566
Iteration 10/25 | Loss: 0.00093566
Iteration 11/25 | Loss: 0.00093566
Iteration 12/25 | Loss: 0.00093566
Iteration 13/25 | Loss: 0.00093566
Iteration 14/25 | Loss: 0.00093566
Iteration 15/25 | Loss: 0.00093566
Iteration 16/25 | Loss: 0.00093566
Iteration 17/25 | Loss: 0.00093566
Iteration 18/25 | Loss: 0.00093566
Iteration 19/25 | Loss: 0.00093566
Iteration 20/25 | Loss: 0.00093566
Iteration 21/25 | Loss: 0.00093566
Iteration 22/25 | Loss: 0.00093566
Iteration 23/25 | Loss: 0.00093566
Iteration 24/25 | Loss: 0.00093566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009356559603475034, 0.0009356559603475034, 0.0009356559603475034, 0.0009356559603475034, 0.0009356559603475034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009356559603475034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093566
Iteration 2/1000 | Loss: 0.00080829
Iteration 3/1000 | Loss: 0.00085746
Iteration 4/1000 | Loss: 0.00100056
Iteration 5/1000 | Loss: 0.00108925
Iteration 6/1000 | Loss: 0.00117081
Iteration 7/1000 | Loss: 0.00124189
Iteration 8/1000 | Loss: 0.00119797
Iteration 9/1000 | Loss: 0.00120908
Iteration 10/1000 | Loss: 0.00122139
Iteration 11/1000 | Loss: 0.00089705
Iteration 12/1000 | Loss: 0.00056046
Iteration 13/1000 | Loss: 0.00075504
Iteration 14/1000 | Loss: 0.00099330
Iteration 15/1000 | Loss: 0.00100547
Iteration 16/1000 | Loss: 0.00071936
Iteration 17/1000 | Loss: 0.00063911
Iteration 18/1000 | Loss: 0.00069792
Iteration 19/1000 | Loss: 0.00046774
Iteration 20/1000 | Loss: 0.00058834
Iteration 21/1000 | Loss: 0.00059776
Iteration 22/1000 | Loss: 0.00052158
Iteration 23/1000 | Loss: 0.00045255
Iteration 24/1000 | Loss: 0.00046473
Iteration 25/1000 | Loss: 0.00045308
Iteration 26/1000 | Loss: 0.00056977
Iteration 27/1000 | Loss: 0.00054153
Iteration 28/1000 | Loss: 0.00044156
Iteration 29/1000 | Loss: 0.00058194
Iteration 30/1000 | Loss: 0.00108192
Iteration 31/1000 | Loss: 0.00046108
Iteration 32/1000 | Loss: 0.00035677
Iteration 33/1000 | Loss: 0.00019891
Iteration 34/1000 | Loss: 0.00023189
Iteration 35/1000 | Loss: 0.00031000
Iteration 36/1000 | Loss: 0.00037371
Iteration 37/1000 | Loss: 0.00039593
Iteration 38/1000 | Loss: 0.00028460
Iteration 39/1000 | Loss: 0.00023996
Iteration 40/1000 | Loss: 0.00021527
Iteration 41/1000 | Loss: 0.00020580
Iteration 42/1000 | Loss: 0.00015777
Iteration 43/1000 | Loss: 0.00089753
Iteration 44/1000 | Loss: 0.00058351
Iteration 45/1000 | Loss: 0.00075954
Iteration 46/1000 | Loss: 0.00087752
Iteration 47/1000 | Loss: 0.00072827
Iteration 48/1000 | Loss: 0.00056230
Iteration 49/1000 | Loss: 0.00041475
Iteration 50/1000 | Loss: 0.00041928
Iteration 51/1000 | Loss: 0.00051002
Iteration 52/1000 | Loss: 0.00053222
Iteration 53/1000 | Loss: 0.00050136
Iteration 54/1000 | Loss: 0.00056592
Iteration 55/1000 | Loss: 0.00141666
Iteration 56/1000 | Loss: 0.00094467
Iteration 57/1000 | Loss: 0.00055303
Iteration 58/1000 | Loss: 0.00039358
Iteration 59/1000 | Loss: 0.00039857
Iteration 60/1000 | Loss: 0.00034161
Iteration 61/1000 | Loss: 0.00024467
Iteration 62/1000 | Loss: 0.00033988
Iteration 63/1000 | Loss: 0.00031036
Iteration 64/1000 | Loss: 0.00036349
Iteration 65/1000 | Loss: 0.00030780
Iteration 66/1000 | Loss: 0.00038611
Iteration 67/1000 | Loss: 0.00037008
Iteration 68/1000 | Loss: 0.00042793
Iteration 69/1000 | Loss: 0.00034228
Iteration 70/1000 | Loss: 0.00012582
Iteration 71/1000 | Loss: 0.00038703
Iteration 72/1000 | Loss: 0.00014967
Iteration 73/1000 | Loss: 0.00020403
Iteration 74/1000 | Loss: 0.00029758
Iteration 75/1000 | Loss: 0.00022985
Iteration 76/1000 | Loss: 0.00046138
Iteration 77/1000 | Loss: 0.00036603
Iteration 78/1000 | Loss: 0.00038270
Iteration 79/1000 | Loss: 0.00040411
Iteration 80/1000 | Loss: 0.00024562
Iteration 81/1000 | Loss: 0.00032791
Iteration 82/1000 | Loss: 0.00034746
Iteration 83/1000 | Loss: 0.00045416
Iteration 84/1000 | Loss: 0.00046763
Iteration 85/1000 | Loss: 0.00044910
Iteration 86/1000 | Loss: 0.00041843
Iteration 87/1000 | Loss: 0.00034685
Iteration 88/1000 | Loss: 0.00035802
Iteration 89/1000 | Loss: 0.00022582
Iteration 90/1000 | Loss: 0.00031777
Iteration 91/1000 | Loss: 0.00035456
Iteration 92/1000 | Loss: 0.00045528
Iteration 93/1000 | Loss: 0.00051741
Iteration 94/1000 | Loss: 0.00064590
Iteration 95/1000 | Loss: 0.00044573
Iteration 96/1000 | Loss: 0.00119112
Iteration 97/1000 | Loss: 0.00051888
Iteration 98/1000 | Loss: 0.00041145
Iteration 99/1000 | Loss: 0.00032117
Iteration 100/1000 | Loss: 0.00030445
Iteration 101/1000 | Loss: 0.00047052
Iteration 102/1000 | Loss: 0.00046459
Iteration 103/1000 | Loss: 0.00058288
Iteration 104/1000 | Loss: 0.00054764
Iteration 105/1000 | Loss: 0.00038740
Iteration 106/1000 | Loss: 0.00037904
Iteration 107/1000 | Loss: 0.00032480
Iteration 108/1000 | Loss: 0.00036172
Iteration 109/1000 | Loss: 0.00044795
Iteration 110/1000 | Loss: 0.00036057
Iteration 111/1000 | Loss: 0.00027738
Iteration 112/1000 | Loss: 0.00038092
Iteration 113/1000 | Loss: 0.00035472
Iteration 114/1000 | Loss: 0.00034021
Iteration 115/1000 | Loss: 0.00028092
Iteration 116/1000 | Loss: 0.00054705
Iteration 117/1000 | Loss: 0.00069479
Iteration 118/1000 | Loss: 0.00047697
Iteration 119/1000 | Loss: 0.00066176
Iteration 120/1000 | Loss: 0.00053071
Iteration 121/1000 | Loss: 0.00067466
Iteration 122/1000 | Loss: 0.00026245
Iteration 123/1000 | Loss: 0.00036196
Iteration 124/1000 | Loss: 0.00041492
Iteration 125/1000 | Loss: 0.00005468
Iteration 126/1000 | Loss: 0.00009880
Iteration 127/1000 | Loss: 0.00016783
Iteration 128/1000 | Loss: 0.00005190
Iteration 129/1000 | Loss: 0.00004635
Iteration 130/1000 | Loss: 0.00005862
Iteration 131/1000 | Loss: 0.00006328
Iteration 132/1000 | Loss: 0.00016243
Iteration 133/1000 | Loss: 0.00012567
Iteration 134/1000 | Loss: 0.00017571
Iteration 135/1000 | Loss: 0.00031316
Iteration 136/1000 | Loss: 0.00016530
Iteration 137/1000 | Loss: 0.00018770
Iteration 138/1000 | Loss: 0.00039174
Iteration 139/1000 | Loss: 0.00021119
Iteration 140/1000 | Loss: 0.00014057
Iteration 141/1000 | Loss: 0.00017477
Iteration 142/1000 | Loss: 0.00011528
Iteration 143/1000 | Loss: 0.00013453
Iteration 144/1000 | Loss: 0.00012237
Iteration 145/1000 | Loss: 0.00023832
Iteration 146/1000 | Loss: 0.00014610
Iteration 147/1000 | Loss: 0.00004418
Iteration 148/1000 | Loss: 0.00004630
Iteration 149/1000 | Loss: 0.00022980
Iteration 150/1000 | Loss: 0.00019062
Iteration 151/1000 | Loss: 0.00015184
Iteration 152/1000 | Loss: 0.00016310
Iteration 153/1000 | Loss: 0.00015351
Iteration 154/1000 | Loss: 0.00015158
Iteration 155/1000 | Loss: 0.00016871
Iteration 156/1000 | Loss: 0.00018674
Iteration 157/1000 | Loss: 0.00022029
Iteration 158/1000 | Loss: 0.00022699
Iteration 159/1000 | Loss: 0.00005041
Iteration 160/1000 | Loss: 0.00004969
Iteration 161/1000 | Loss: 0.00023712
Iteration 162/1000 | Loss: 0.00005642
Iteration 163/1000 | Loss: 0.00025492
Iteration 164/1000 | Loss: 0.00004261
Iteration 165/1000 | Loss: 0.00004210
Iteration 166/1000 | Loss: 0.00003787
Iteration 167/1000 | Loss: 0.00004135
Iteration 168/1000 | Loss: 0.00004654
Iteration 169/1000 | Loss: 0.00004925
Iteration 170/1000 | Loss: 0.00004260
Iteration 171/1000 | Loss: 0.00004385
Iteration 172/1000 | Loss: 0.00004225
Iteration 173/1000 | Loss: 0.00004845
Iteration 174/1000 | Loss: 0.00003632
Iteration 175/1000 | Loss: 0.00004290
Iteration 176/1000 | Loss: 0.00004218
Iteration 177/1000 | Loss: 0.00006269
Iteration 178/1000 | Loss: 0.00004518
Iteration 179/1000 | Loss: 0.00004365
Iteration 180/1000 | Loss: 0.00004407
Iteration 181/1000 | Loss: 0.00004495
Iteration 182/1000 | Loss: 0.00003162
Iteration 183/1000 | Loss: 0.00002961
Iteration 184/1000 | Loss: 0.00002870
Iteration 185/1000 | Loss: 0.00002817
Iteration 186/1000 | Loss: 0.00002775
Iteration 187/1000 | Loss: 0.00002733
Iteration 188/1000 | Loss: 0.00002708
Iteration 189/1000 | Loss: 0.00002686
Iteration 190/1000 | Loss: 0.00002668
Iteration 191/1000 | Loss: 0.00002649
Iteration 192/1000 | Loss: 0.00002635
Iteration 193/1000 | Loss: 0.00002635
Iteration 194/1000 | Loss: 0.00002635
Iteration 195/1000 | Loss: 0.00002635
Iteration 196/1000 | Loss: 0.00002634
Iteration 197/1000 | Loss: 0.00002633
Iteration 198/1000 | Loss: 0.00002633
Iteration 199/1000 | Loss: 0.00002631
Iteration 200/1000 | Loss: 0.00002631
Iteration 201/1000 | Loss: 0.00002631
Iteration 202/1000 | Loss: 0.00002629
Iteration 203/1000 | Loss: 0.00002626
Iteration 204/1000 | Loss: 0.00002625
Iteration 205/1000 | Loss: 0.00002625
Iteration 206/1000 | Loss: 0.00002625
Iteration 207/1000 | Loss: 0.00002625
Iteration 208/1000 | Loss: 0.00002624
Iteration 209/1000 | Loss: 0.00002624
Iteration 210/1000 | Loss: 0.00002624
Iteration 211/1000 | Loss: 0.00002624
Iteration 212/1000 | Loss: 0.00002624
Iteration 213/1000 | Loss: 0.00002624
Iteration 214/1000 | Loss: 0.00002624
Iteration 215/1000 | Loss: 0.00002624
Iteration 216/1000 | Loss: 0.00002623
Iteration 217/1000 | Loss: 0.00002623
Iteration 218/1000 | Loss: 0.00002623
Iteration 219/1000 | Loss: 0.00002623
Iteration 220/1000 | Loss: 0.00002623
Iteration 221/1000 | Loss: 0.00002623
Iteration 222/1000 | Loss: 0.00002623
Iteration 223/1000 | Loss: 0.00002622
Iteration 224/1000 | Loss: 0.00002622
Iteration 225/1000 | Loss: 0.00002622
Iteration 226/1000 | Loss: 0.00002621
Iteration 227/1000 | Loss: 0.00002621
Iteration 228/1000 | Loss: 0.00002621
Iteration 229/1000 | Loss: 0.00002620
Iteration 230/1000 | Loss: 0.00002620
Iteration 231/1000 | Loss: 0.00002620
Iteration 232/1000 | Loss: 0.00002620
Iteration 233/1000 | Loss: 0.00002619
Iteration 234/1000 | Loss: 0.00002619
Iteration 235/1000 | Loss: 0.00002619
Iteration 236/1000 | Loss: 0.00002618
Iteration 237/1000 | Loss: 0.00002617
Iteration 238/1000 | Loss: 0.00002617
Iteration 239/1000 | Loss: 0.00002616
Iteration 240/1000 | Loss: 0.00002616
Iteration 241/1000 | Loss: 0.00002616
Iteration 242/1000 | Loss: 0.00002616
Iteration 243/1000 | Loss: 0.00002615
Iteration 244/1000 | Loss: 0.00002615
Iteration 245/1000 | Loss: 0.00002615
Iteration 246/1000 | Loss: 0.00002614
Iteration 247/1000 | Loss: 0.00002614
Iteration 248/1000 | Loss: 0.00002614
Iteration 249/1000 | Loss: 0.00002614
Iteration 250/1000 | Loss: 0.00002613
Iteration 251/1000 | Loss: 0.00002613
Iteration 252/1000 | Loss: 0.00002612
Iteration 253/1000 | Loss: 0.00002612
Iteration 254/1000 | Loss: 0.00002612
Iteration 255/1000 | Loss: 0.00002612
Iteration 256/1000 | Loss: 0.00002612
Iteration 257/1000 | Loss: 0.00002611
Iteration 258/1000 | Loss: 0.00002611
Iteration 259/1000 | Loss: 0.00002611
Iteration 260/1000 | Loss: 0.00002611
Iteration 261/1000 | Loss: 0.00002611
Iteration 262/1000 | Loss: 0.00002611
Iteration 263/1000 | Loss: 0.00002611
Iteration 264/1000 | Loss: 0.00002611
Iteration 265/1000 | Loss: 0.00002610
Iteration 266/1000 | Loss: 0.00002610
Iteration 267/1000 | Loss: 0.00002610
Iteration 268/1000 | Loss: 0.00002610
Iteration 269/1000 | Loss: 0.00002610
Iteration 270/1000 | Loss: 0.00002610
Iteration 271/1000 | Loss: 0.00002609
Iteration 272/1000 | Loss: 0.00002609
Iteration 273/1000 | Loss: 0.00002608
Iteration 274/1000 | Loss: 0.00002608
Iteration 275/1000 | Loss: 0.00002608
Iteration 276/1000 | Loss: 0.00002608
Iteration 277/1000 | Loss: 0.00002608
Iteration 278/1000 | Loss: 0.00002608
Iteration 279/1000 | Loss: 0.00002608
Iteration 280/1000 | Loss: 0.00002607
Iteration 281/1000 | Loss: 0.00002607
Iteration 282/1000 | Loss: 0.00002607
Iteration 283/1000 | Loss: 0.00002607
Iteration 284/1000 | Loss: 0.00002607
Iteration 285/1000 | Loss: 0.00002607
Iteration 286/1000 | Loss: 0.00002607
Iteration 287/1000 | Loss: 0.00002607
Iteration 288/1000 | Loss: 0.00002606
Iteration 289/1000 | Loss: 0.00002606
Iteration 290/1000 | Loss: 0.00002606
Iteration 291/1000 | Loss: 0.00002606
Iteration 292/1000 | Loss: 0.00002605
Iteration 293/1000 | Loss: 0.00002605
Iteration 294/1000 | Loss: 0.00002605
Iteration 295/1000 | Loss: 0.00002605
Iteration 296/1000 | Loss: 0.00002605
Iteration 297/1000 | Loss: 0.00002605
Iteration 298/1000 | Loss: 0.00002605
Iteration 299/1000 | Loss: 0.00002605
Iteration 300/1000 | Loss: 0.00002605
Iteration 301/1000 | Loss: 0.00002605
Iteration 302/1000 | Loss: 0.00002605
Iteration 303/1000 | Loss: 0.00002605
Iteration 304/1000 | Loss: 0.00002605
Iteration 305/1000 | Loss: 0.00002605
Iteration 306/1000 | Loss: 0.00002605
Iteration 307/1000 | Loss: 0.00002605
Iteration 308/1000 | Loss: 0.00002604
Iteration 309/1000 | Loss: 0.00002604
Iteration 310/1000 | Loss: 0.00002604
Iteration 311/1000 | Loss: 0.00002604
Iteration 312/1000 | Loss: 0.00002604
Iteration 313/1000 | Loss: 0.00002604
Iteration 314/1000 | Loss: 0.00002604
Iteration 315/1000 | Loss: 0.00002604
Iteration 316/1000 | Loss: 0.00002604
Iteration 317/1000 | Loss: 0.00002604
Iteration 318/1000 | Loss: 0.00002604
Iteration 319/1000 | Loss: 0.00002604
Iteration 320/1000 | Loss: 0.00002604
Iteration 321/1000 | Loss: 0.00002604
Iteration 322/1000 | Loss: 0.00002604
Iteration 323/1000 | Loss: 0.00002604
Iteration 324/1000 | Loss: 0.00002604
Iteration 325/1000 | Loss: 0.00002604
Iteration 326/1000 | Loss: 0.00002603
Iteration 327/1000 | Loss: 0.00002603
Iteration 328/1000 | Loss: 0.00002603
Iteration 329/1000 | Loss: 0.00002603
Iteration 330/1000 | Loss: 0.00002603
Iteration 331/1000 | Loss: 0.00002603
Iteration 332/1000 | Loss: 0.00002603
Iteration 333/1000 | Loss: 0.00002603
Iteration 334/1000 | Loss: 0.00002603
Iteration 335/1000 | Loss: 0.00002603
Iteration 336/1000 | Loss: 0.00002603
Iteration 337/1000 | Loss: 0.00002603
Iteration 338/1000 | Loss: 0.00002603
Iteration 339/1000 | Loss: 0.00002602
Iteration 340/1000 | Loss: 0.00002602
Iteration 341/1000 | Loss: 0.00002602
Iteration 342/1000 | Loss: 0.00002602
Iteration 343/1000 | Loss: 0.00002602
Iteration 344/1000 | Loss: 0.00002602
Iteration 345/1000 | Loss: 0.00002602
Iteration 346/1000 | Loss: 0.00002602
Iteration 347/1000 | Loss: 0.00002602
Iteration 348/1000 | Loss: 0.00002602
Iteration 349/1000 | Loss: 0.00002602
Iteration 350/1000 | Loss: 0.00002601
Iteration 351/1000 | Loss: 0.00002601
Iteration 352/1000 | Loss: 0.00002601
Iteration 353/1000 | Loss: 0.00002601
Iteration 354/1000 | Loss: 0.00002601
Iteration 355/1000 | Loss: 0.00002601
Iteration 356/1000 | Loss: 0.00002601
Iteration 357/1000 | Loss: 0.00002601
Iteration 358/1000 | Loss: 0.00002601
Iteration 359/1000 | Loss: 0.00002601
Iteration 360/1000 | Loss: 0.00002601
Iteration 361/1000 | Loss: 0.00002601
Iteration 362/1000 | Loss: 0.00002601
Iteration 363/1000 | Loss: 0.00002601
Iteration 364/1000 | Loss: 0.00002601
Iteration 365/1000 | Loss: 0.00002601
Iteration 366/1000 | Loss: 0.00002601
Iteration 367/1000 | Loss: 0.00002601
Iteration 368/1000 | Loss: 0.00002601
Iteration 369/1000 | Loss: 0.00002601
Iteration 370/1000 | Loss: 0.00002601
Iteration 371/1000 | Loss: 0.00002601
Iteration 372/1000 | Loss: 0.00002601
Iteration 373/1000 | Loss: 0.00002601
Iteration 374/1000 | Loss: 0.00002601
Iteration 375/1000 | Loss: 0.00002601
Iteration 376/1000 | Loss: 0.00002601
Iteration 377/1000 | Loss: 0.00002601
Iteration 378/1000 | Loss: 0.00002601
Iteration 379/1000 | Loss: 0.00002601
Iteration 380/1000 | Loss: 0.00002601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 380. Stopping optimization.
Last 5 losses: [2.601090272946749e-05, 2.601090272946749e-05, 2.601090272946749e-05, 2.601090272946749e-05, 2.601090272946749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.601090272946749e-05

Optimization complete. Final v2v error: 4.1591010093688965 mm

Highest mean error: 4.90045166015625 mm for frame 148

Lowest mean error: 3.727391242980957 mm for frame 129

Saving results

Total time: 347.70175981521606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042961
Iteration 2/25 | Loss: 0.00483242
Iteration 3/25 | Loss: 0.00354594
Iteration 4/25 | Loss: 0.00216996
Iteration 5/25 | Loss: 0.00182147
Iteration 6/25 | Loss: 0.00165284
Iteration 7/25 | Loss: 0.00160025
Iteration 8/25 | Loss: 0.00147494
Iteration 9/25 | Loss: 0.00138423
Iteration 10/25 | Loss: 0.00134885
Iteration 11/25 | Loss: 0.00130318
Iteration 12/25 | Loss: 0.00127653
Iteration 13/25 | Loss: 0.00117171
Iteration 14/25 | Loss: 0.00110903
Iteration 15/25 | Loss: 0.00108360
Iteration 16/25 | Loss: 0.00108687
Iteration 17/25 | Loss: 0.00107852
Iteration 18/25 | Loss: 0.00106750
Iteration 19/25 | Loss: 0.00106932
Iteration 20/25 | Loss: 0.00106387
Iteration 21/25 | Loss: 0.00105890
Iteration 22/25 | Loss: 0.00105740
Iteration 23/25 | Loss: 0.00105681
Iteration 24/25 | Loss: 0.00105654
Iteration 25/25 | Loss: 0.00105640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43492985
Iteration 2/25 | Loss: 0.00248037
Iteration 3/25 | Loss: 0.00248037
Iteration 4/25 | Loss: 0.00248037
Iteration 5/25 | Loss: 0.00248037
Iteration 6/25 | Loss: 0.00248037
Iteration 7/25 | Loss: 0.00248037
Iteration 8/25 | Loss: 0.00248037
Iteration 9/25 | Loss: 0.00248037
Iteration 10/25 | Loss: 0.00248037
Iteration 11/25 | Loss: 0.00248037
Iteration 12/25 | Loss: 0.00248037
Iteration 13/25 | Loss: 0.00248037
Iteration 14/25 | Loss: 0.00248037
Iteration 15/25 | Loss: 0.00248037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0024803655687719584, 0.0024803655687719584, 0.0024803655687719584, 0.0024803655687719584, 0.0024803655687719584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024803655687719584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248037
Iteration 2/1000 | Loss: 0.00054013
Iteration 3/1000 | Loss: 0.00035883
Iteration 4/1000 | Loss: 0.00031401
Iteration 5/1000 | Loss: 0.00028162
Iteration 6/1000 | Loss: 0.00025854
Iteration 7/1000 | Loss: 0.00024103
Iteration 8/1000 | Loss: 0.00231474
Iteration 9/1000 | Loss: 0.04042473
Iteration 10/1000 | Loss: 0.00073737
Iteration 11/1000 | Loss: 0.00026782
Iteration 12/1000 | Loss: 0.00017609
Iteration 13/1000 | Loss: 0.00055187
Iteration 14/1000 | Loss: 0.00009723
Iteration 15/1000 | Loss: 0.00007242
Iteration 16/1000 | Loss: 0.00005265
Iteration 17/1000 | Loss: 0.00005200
Iteration 18/1000 | Loss: 0.00004662
Iteration 19/1000 | Loss: 0.00002728
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001285
Iteration 43/1000 | Loss: 0.00001285
Iteration 44/1000 | Loss: 0.00001284
Iteration 45/1000 | Loss: 0.00001284
Iteration 46/1000 | Loss: 0.00001281
Iteration 47/1000 | Loss: 0.00001281
Iteration 48/1000 | Loss: 0.00001281
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001280
Iteration 52/1000 | Loss: 0.00001280
Iteration 53/1000 | Loss: 0.00001280
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001278
Iteration 59/1000 | Loss: 0.00001278
Iteration 60/1000 | Loss: 0.00001278
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001277
Iteration 64/1000 | Loss: 0.00001277
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001276
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001276
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001276
Iteration 73/1000 | Loss: 0.00001276
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001275
Iteration 77/1000 | Loss: 0.00001275
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001275
Iteration 83/1000 | Loss: 0.00001275
Iteration 84/1000 | Loss: 0.00001275
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001273
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001271
Iteration 131/1000 | Loss: 0.00001271
Iteration 132/1000 | Loss: 0.00001271
Iteration 133/1000 | Loss: 0.00001271
Iteration 134/1000 | Loss: 0.00001271
Iteration 135/1000 | Loss: 0.00001271
Iteration 136/1000 | Loss: 0.00001271
Iteration 137/1000 | Loss: 0.00001271
Iteration 138/1000 | Loss: 0.00001271
Iteration 139/1000 | Loss: 0.00001271
Iteration 140/1000 | Loss: 0.00001271
Iteration 141/1000 | Loss: 0.00001271
Iteration 142/1000 | Loss: 0.00001271
Iteration 143/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.2714725926343817e-05, 1.2714725926343817e-05, 1.2714725926343817e-05, 1.2714725926343817e-05, 1.2714725926343817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2714725926343817e-05

Optimization complete. Final v2v error: 3.023090362548828 mm

Highest mean error: 3.1943199634552 mm for frame 115

Lowest mean error: 2.925037145614624 mm for frame 57

Saving results

Total time: 90.85529208183289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930167
Iteration 2/25 | Loss: 0.00131237
Iteration 3/25 | Loss: 0.00090090
Iteration 4/25 | Loss: 0.00084182
Iteration 5/25 | Loss: 0.00083190
Iteration 6/25 | Loss: 0.00081010
Iteration 7/25 | Loss: 0.00080479
Iteration 8/25 | Loss: 0.00080505
Iteration 9/25 | Loss: 0.00080106
Iteration 10/25 | Loss: 0.00079835
Iteration 11/25 | Loss: 0.00079915
Iteration 12/25 | Loss: 0.00079673
Iteration 13/25 | Loss: 0.00079871
Iteration 14/25 | Loss: 0.00079363
Iteration 15/25 | Loss: 0.00079299
Iteration 16/25 | Loss: 0.00079422
Iteration 17/25 | Loss: 0.00079012
Iteration 18/25 | Loss: 0.00079088
Iteration 19/25 | Loss: 0.00079050
Iteration 20/25 | Loss: 0.00079041
Iteration 21/25 | Loss: 0.00079098
Iteration 22/25 | Loss: 0.00079010
Iteration 23/25 | Loss: 0.00079034
Iteration 24/25 | Loss: 0.00079059
Iteration 25/25 | Loss: 0.00079056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44973207
Iteration 2/25 | Loss: 0.00038692
Iteration 3/25 | Loss: 0.00038692
Iteration 4/25 | Loss: 0.00038692
Iteration 5/25 | Loss: 0.00038692
Iteration 6/25 | Loss: 0.00038692
Iteration 7/25 | Loss: 0.00038692
Iteration 8/25 | Loss: 0.00038692
Iteration 9/25 | Loss: 0.00038691
Iteration 10/25 | Loss: 0.00038691
Iteration 11/25 | Loss: 0.00038691
Iteration 12/25 | Loss: 0.00038691
Iteration 13/25 | Loss: 0.00038691
Iteration 14/25 | Loss: 0.00038691
Iteration 15/25 | Loss: 0.00038691
Iteration 16/25 | Loss: 0.00038691
Iteration 17/25 | Loss: 0.00038691
Iteration 18/25 | Loss: 0.00038691
Iteration 19/25 | Loss: 0.00038691
Iteration 20/25 | Loss: 0.00038691
Iteration 21/25 | Loss: 0.00038691
Iteration 22/25 | Loss: 0.00038691
Iteration 23/25 | Loss: 0.00038691
Iteration 24/25 | Loss: 0.00038691
Iteration 25/25 | Loss: 0.00038691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038691
Iteration 2/1000 | Loss: 0.00015602
Iteration 3/1000 | Loss: 0.00005638
Iteration 4/1000 | Loss: 0.00005782
Iteration 5/1000 | Loss: 0.00004484
Iteration 6/1000 | Loss: 0.00004966
Iteration 7/1000 | Loss: 0.00003958
Iteration 8/1000 | Loss: 0.00039448
Iteration 9/1000 | Loss: 0.00005801
Iteration 10/1000 | Loss: 0.00004968
Iteration 11/1000 | Loss: 0.00003526
Iteration 12/1000 | Loss: 0.00003745
Iteration 13/1000 | Loss: 0.00003039
Iteration 14/1000 | Loss: 0.00004351
Iteration 15/1000 | Loss: 0.00003915
Iteration 16/1000 | Loss: 0.00003581
Iteration 17/1000 | Loss: 0.00003423
Iteration 18/1000 | Loss: 0.00004208
Iteration 19/1000 | Loss: 0.00004125
Iteration 20/1000 | Loss: 0.00004096
Iteration 21/1000 | Loss: 0.00004167
Iteration 22/1000 | Loss: 0.00003977
Iteration 23/1000 | Loss: 0.00004185
Iteration 24/1000 | Loss: 0.00004323
Iteration 25/1000 | Loss: 0.00004172
Iteration 26/1000 | Loss: 0.00004153
Iteration 27/1000 | Loss: 0.00004731
Iteration 28/1000 | Loss: 0.00004096
Iteration 29/1000 | Loss: 0.00003756
Iteration 30/1000 | Loss: 0.00004443
Iteration 31/1000 | Loss: 0.00004195
Iteration 32/1000 | Loss: 0.00004422
Iteration 33/1000 | Loss: 0.00003727
Iteration 34/1000 | Loss: 0.00004409
Iteration 35/1000 | Loss: 0.00003434
Iteration 36/1000 | Loss: 0.00003746
Iteration 37/1000 | Loss: 0.00003753
Iteration 38/1000 | Loss: 0.00004313
Iteration 39/1000 | Loss: 0.00004346
Iteration 40/1000 | Loss: 0.00004151
Iteration 41/1000 | Loss: 0.00003075
Iteration 42/1000 | Loss: 0.00003466
Iteration 43/1000 | Loss: 0.00003724
Iteration 44/1000 | Loss: 0.00004267
Iteration 45/1000 | Loss: 0.00004136
Iteration 46/1000 | Loss: 0.00004257
Iteration 47/1000 | Loss: 0.00002895
Iteration 48/1000 | Loss: 0.00003202
Iteration 49/1000 | Loss: 0.00004143
Iteration 50/1000 | Loss: 0.00004085
Iteration 51/1000 | Loss: 0.00004146
Iteration 52/1000 | Loss: 0.00004462
Iteration 53/1000 | Loss: 0.00004123
Iteration 54/1000 | Loss: 0.00004071
Iteration 55/1000 | Loss: 0.00004164
Iteration 56/1000 | Loss: 0.00004193
Iteration 57/1000 | Loss: 0.00002983
Iteration 58/1000 | Loss: 0.00004019
Iteration 59/1000 | Loss: 0.00002732
Iteration 60/1000 | Loss: 0.00003926
Iteration 61/1000 | Loss: 0.00004649
Iteration 62/1000 | Loss: 0.00004354
Iteration 63/1000 | Loss: 0.00004079
Iteration 64/1000 | Loss: 0.00002736
Iteration 65/1000 | Loss: 0.00002804
Iteration 66/1000 | Loss: 0.00003847
Iteration 67/1000 | Loss: 0.00004683
Iteration 68/1000 | Loss: 0.00003497
Iteration 69/1000 | Loss: 0.00004258
Iteration 70/1000 | Loss: 0.00003814
Iteration 71/1000 | Loss: 0.00004942
Iteration 72/1000 | Loss: 0.00004215
Iteration 73/1000 | Loss: 0.00004388
Iteration 74/1000 | Loss: 0.00004127
Iteration 75/1000 | Loss: 0.00003087
Iteration 76/1000 | Loss: 0.00002927
Iteration 77/1000 | Loss: 0.00004469
Iteration 78/1000 | Loss: 0.00003672
Iteration 79/1000 | Loss: 0.00003562
Iteration 80/1000 | Loss: 0.00003511
Iteration 81/1000 | Loss: 0.00003868
Iteration 82/1000 | Loss: 0.00003583
Iteration 83/1000 | Loss: 0.00003906
Iteration 84/1000 | Loss: 0.00003480
Iteration 85/1000 | Loss: 0.00003867
Iteration 86/1000 | Loss: 0.00003518
Iteration 87/1000 | Loss: 0.00004945
Iteration 88/1000 | Loss: 0.00005045
Iteration 89/1000 | Loss: 0.00004393
Iteration 90/1000 | Loss: 0.00003905
Iteration 91/1000 | Loss: 0.00003445
Iteration 92/1000 | Loss: 0.00003834
Iteration 93/1000 | Loss: 0.00003540
Iteration 94/1000 | Loss: 0.00003856
Iteration 95/1000 | Loss: 0.00003533
Iteration 96/1000 | Loss: 0.00003885
Iteration 97/1000 | Loss: 0.00003613
Iteration 98/1000 | Loss: 0.00004275
Iteration 99/1000 | Loss: 0.00003787
Iteration 100/1000 | Loss: 0.00003967
Iteration 101/1000 | Loss: 0.00003986
Iteration 102/1000 | Loss: 0.00004312
Iteration 103/1000 | Loss: 0.00003873
Iteration 104/1000 | Loss: 0.00004341
Iteration 105/1000 | Loss: 0.00004908
Iteration 106/1000 | Loss: 0.00003040
Iteration 107/1000 | Loss: 0.00002824
Iteration 108/1000 | Loss: 0.00002765
Iteration 109/1000 | Loss: 0.00002657
Iteration 110/1000 | Loss: 0.00002589
Iteration 111/1000 | Loss: 0.00002581
Iteration 112/1000 | Loss: 0.00002581
Iteration 113/1000 | Loss: 0.00002580
Iteration 114/1000 | Loss: 0.00002580
Iteration 115/1000 | Loss: 0.00002579
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002578
Iteration 118/1000 | Loss: 0.00002577
Iteration 119/1000 | Loss: 0.00002576
Iteration 120/1000 | Loss: 0.00002574
Iteration 121/1000 | Loss: 0.00002573
Iteration 122/1000 | Loss: 0.00002570
Iteration 123/1000 | Loss: 0.00002569
Iteration 124/1000 | Loss: 0.00002566
Iteration 125/1000 | Loss: 0.00002565
Iteration 126/1000 | Loss: 0.00002563
Iteration 127/1000 | Loss: 0.00002563
Iteration 128/1000 | Loss: 0.00002562
Iteration 129/1000 | Loss: 0.00002562
Iteration 130/1000 | Loss: 0.00002561
Iteration 131/1000 | Loss: 0.00002556
Iteration 132/1000 | Loss: 0.00002554
Iteration 133/1000 | Loss: 0.00002552
Iteration 134/1000 | Loss: 0.00002552
Iteration 135/1000 | Loss: 0.00002552
Iteration 136/1000 | Loss: 0.00002552
Iteration 137/1000 | Loss: 0.00002552
Iteration 138/1000 | Loss: 0.00002552
Iteration 139/1000 | Loss: 0.00002551
Iteration 140/1000 | Loss: 0.00002551
Iteration 141/1000 | Loss: 0.00002551
Iteration 142/1000 | Loss: 0.00002551
Iteration 143/1000 | Loss: 0.00002551
Iteration 144/1000 | Loss: 0.00002551
Iteration 145/1000 | Loss: 0.00002551
Iteration 146/1000 | Loss: 0.00002551
Iteration 147/1000 | Loss: 0.00002550
Iteration 148/1000 | Loss: 0.00002549
Iteration 149/1000 | Loss: 0.00002549
Iteration 150/1000 | Loss: 0.00002549
Iteration 151/1000 | Loss: 0.00002549
Iteration 152/1000 | Loss: 0.00002548
Iteration 153/1000 | Loss: 0.00002547
Iteration 154/1000 | Loss: 0.00002547
Iteration 155/1000 | Loss: 0.00002547
Iteration 156/1000 | Loss: 0.00002546
Iteration 157/1000 | Loss: 0.00002546
Iteration 158/1000 | Loss: 0.00002546
Iteration 159/1000 | Loss: 0.00002546
Iteration 160/1000 | Loss: 0.00002546
Iteration 161/1000 | Loss: 0.00002546
Iteration 162/1000 | Loss: 0.00002546
Iteration 163/1000 | Loss: 0.00002546
Iteration 164/1000 | Loss: 0.00002545
Iteration 165/1000 | Loss: 0.00002545
Iteration 166/1000 | Loss: 0.00002545
Iteration 167/1000 | Loss: 0.00002545
Iteration 168/1000 | Loss: 0.00002545
Iteration 169/1000 | Loss: 0.00002545
Iteration 170/1000 | Loss: 0.00002544
Iteration 171/1000 | Loss: 0.00002544
Iteration 172/1000 | Loss: 0.00002543
Iteration 173/1000 | Loss: 0.00002542
Iteration 174/1000 | Loss: 0.00002542
Iteration 175/1000 | Loss: 0.00002541
Iteration 176/1000 | Loss: 0.00002541
Iteration 177/1000 | Loss: 0.00002540
Iteration 178/1000 | Loss: 0.00002540
Iteration 179/1000 | Loss: 0.00002540
Iteration 180/1000 | Loss: 0.00002540
Iteration 181/1000 | Loss: 0.00002539
Iteration 182/1000 | Loss: 0.00002539
Iteration 183/1000 | Loss: 0.00002539
Iteration 184/1000 | Loss: 0.00002539
Iteration 185/1000 | Loss: 0.00002539
Iteration 186/1000 | Loss: 0.00002539
Iteration 187/1000 | Loss: 0.00002539
Iteration 188/1000 | Loss: 0.00002538
Iteration 189/1000 | Loss: 0.00002538
Iteration 190/1000 | Loss: 0.00002538
Iteration 191/1000 | Loss: 0.00002538
Iteration 192/1000 | Loss: 0.00002538
Iteration 193/1000 | Loss: 0.00002537
Iteration 194/1000 | Loss: 0.00002537
Iteration 195/1000 | Loss: 0.00002537
Iteration 196/1000 | Loss: 0.00002537
Iteration 197/1000 | Loss: 0.00002537
Iteration 198/1000 | Loss: 0.00002537
Iteration 199/1000 | Loss: 0.00002536
Iteration 200/1000 | Loss: 0.00002536
Iteration 201/1000 | Loss: 0.00002536
Iteration 202/1000 | Loss: 0.00002535
Iteration 203/1000 | Loss: 0.00002535
Iteration 204/1000 | Loss: 0.00002535
Iteration 205/1000 | Loss: 0.00002535
Iteration 206/1000 | Loss: 0.00002535
Iteration 207/1000 | Loss: 0.00002534
Iteration 208/1000 | Loss: 0.00002534
Iteration 209/1000 | Loss: 0.00002534
Iteration 210/1000 | Loss: 0.00002533
Iteration 211/1000 | Loss: 0.00002533
Iteration 212/1000 | Loss: 0.00002533
Iteration 213/1000 | Loss: 0.00002533
Iteration 214/1000 | Loss: 0.00002532
Iteration 215/1000 | Loss: 0.00002532
Iteration 216/1000 | Loss: 0.00002532
Iteration 217/1000 | Loss: 0.00002532
Iteration 218/1000 | Loss: 0.00002532
Iteration 219/1000 | Loss: 0.00002532
Iteration 220/1000 | Loss: 0.00002532
Iteration 221/1000 | Loss: 0.00002532
Iteration 222/1000 | Loss: 0.00002532
Iteration 223/1000 | Loss: 0.00002532
Iteration 224/1000 | Loss: 0.00002531
Iteration 225/1000 | Loss: 0.00002531
Iteration 226/1000 | Loss: 0.00002531
Iteration 227/1000 | Loss: 0.00002531
Iteration 228/1000 | Loss: 0.00002531
Iteration 229/1000 | Loss: 0.00002531
Iteration 230/1000 | Loss: 0.00002531
Iteration 231/1000 | Loss: 0.00002531
Iteration 232/1000 | Loss: 0.00002531
Iteration 233/1000 | Loss: 0.00002531
Iteration 234/1000 | Loss: 0.00002531
Iteration 235/1000 | Loss: 0.00002531
Iteration 236/1000 | Loss: 0.00002531
Iteration 237/1000 | Loss: 0.00002531
Iteration 238/1000 | Loss: 0.00002531
Iteration 239/1000 | Loss: 0.00002531
Iteration 240/1000 | Loss: 0.00002531
Iteration 241/1000 | Loss: 0.00002531
Iteration 242/1000 | Loss: 0.00002531
Iteration 243/1000 | Loss: 0.00002531
Iteration 244/1000 | Loss: 0.00002531
Iteration 245/1000 | Loss: 0.00002531
Iteration 246/1000 | Loss: 0.00002531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.5308416297775693e-05, 2.5308416297775693e-05, 2.5308416297775693e-05, 2.5308416297775693e-05, 2.5308416297775693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5308416297775693e-05

Optimization complete. Final v2v error: 4.172181129455566 mm

Highest mean error: 5.418973922729492 mm for frame 222

Lowest mean error: 3.741556406021118 mm for frame 200

Saving results

Total time: 239.44961762428284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842868
Iteration 2/25 | Loss: 0.00102741
Iteration 3/25 | Loss: 0.00087443
Iteration 4/25 | Loss: 0.00081033
Iteration 5/25 | Loss: 0.00078778
Iteration 6/25 | Loss: 0.00078279
Iteration 7/25 | Loss: 0.00078018
Iteration 8/25 | Loss: 0.00077987
Iteration 9/25 | Loss: 0.00077987
Iteration 10/25 | Loss: 0.00077987
Iteration 11/25 | Loss: 0.00077987
Iteration 12/25 | Loss: 0.00077984
Iteration 13/25 | Loss: 0.00077984
Iteration 14/25 | Loss: 0.00077984
Iteration 15/25 | Loss: 0.00077984
Iteration 16/25 | Loss: 0.00077984
Iteration 17/25 | Loss: 0.00077984
Iteration 18/25 | Loss: 0.00077984
Iteration 19/25 | Loss: 0.00077984
Iteration 20/25 | Loss: 0.00077984
Iteration 21/25 | Loss: 0.00077984
Iteration 22/25 | Loss: 0.00077984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007798359729349613, 0.0007798359729349613, 0.0007798359729349613, 0.0007798359729349613, 0.0007798359729349613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007798359729349613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55632353
Iteration 2/25 | Loss: 0.00037130
Iteration 3/25 | Loss: 0.00037129
Iteration 4/25 | Loss: 0.00037129
Iteration 5/25 | Loss: 0.00037129
Iteration 6/25 | Loss: 0.00037129
Iteration 7/25 | Loss: 0.00037129
Iteration 8/25 | Loss: 0.00037129
Iteration 9/25 | Loss: 0.00037129
Iteration 10/25 | Loss: 0.00037129
Iteration 11/25 | Loss: 0.00037129
Iteration 12/25 | Loss: 0.00037129
Iteration 13/25 | Loss: 0.00037129
Iteration 14/25 | Loss: 0.00037129
Iteration 15/25 | Loss: 0.00037129
Iteration 16/25 | Loss: 0.00037129
Iteration 17/25 | Loss: 0.00037129
Iteration 18/25 | Loss: 0.00037129
Iteration 19/25 | Loss: 0.00037129
Iteration 20/25 | Loss: 0.00037129
Iteration 21/25 | Loss: 0.00037129
Iteration 22/25 | Loss: 0.00037129
Iteration 23/25 | Loss: 0.00037129
Iteration 24/25 | Loss: 0.00037129
Iteration 25/25 | Loss: 0.00037129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037129
Iteration 2/1000 | Loss: 0.00006234
Iteration 3/1000 | Loss: 0.00004326
Iteration 4/1000 | Loss: 0.00003670
Iteration 5/1000 | Loss: 0.00003461
Iteration 6/1000 | Loss: 0.00003364
Iteration 7/1000 | Loss: 0.00003297
Iteration 8/1000 | Loss: 0.00003233
Iteration 9/1000 | Loss: 0.00003183
Iteration 10/1000 | Loss: 0.00003145
Iteration 11/1000 | Loss: 0.00003118
Iteration 12/1000 | Loss: 0.00003101
Iteration 13/1000 | Loss: 0.00003100
Iteration 14/1000 | Loss: 0.00003090
Iteration 15/1000 | Loss: 0.00003085
Iteration 16/1000 | Loss: 0.00003084
Iteration 17/1000 | Loss: 0.00003084
Iteration 18/1000 | Loss: 0.00003084
Iteration 19/1000 | Loss: 0.00003084
Iteration 20/1000 | Loss: 0.00003082
Iteration 21/1000 | Loss: 0.00003081
Iteration 22/1000 | Loss: 0.00003079
Iteration 23/1000 | Loss: 0.00003078
Iteration 24/1000 | Loss: 0.00003077
Iteration 25/1000 | Loss: 0.00003077
Iteration 26/1000 | Loss: 0.00003075
Iteration 27/1000 | Loss: 0.00003074
Iteration 28/1000 | Loss: 0.00003074
Iteration 29/1000 | Loss: 0.00003074
Iteration 30/1000 | Loss: 0.00003073
Iteration 31/1000 | Loss: 0.00003070
Iteration 32/1000 | Loss: 0.00003070
Iteration 33/1000 | Loss: 0.00003069
Iteration 34/1000 | Loss: 0.00003069
Iteration 35/1000 | Loss: 0.00003069
Iteration 36/1000 | Loss: 0.00003068
Iteration 37/1000 | Loss: 0.00003067
Iteration 38/1000 | Loss: 0.00003066
Iteration 39/1000 | Loss: 0.00003066
Iteration 40/1000 | Loss: 0.00003066
Iteration 41/1000 | Loss: 0.00003066
Iteration 42/1000 | Loss: 0.00003066
Iteration 43/1000 | Loss: 0.00003065
Iteration 44/1000 | Loss: 0.00003065
Iteration 45/1000 | Loss: 0.00003065
Iteration 46/1000 | Loss: 0.00003064
Iteration 47/1000 | Loss: 0.00003064
Iteration 48/1000 | Loss: 0.00003064
Iteration 49/1000 | Loss: 0.00003064
Iteration 50/1000 | Loss: 0.00003063
Iteration 51/1000 | Loss: 0.00003063
Iteration 52/1000 | Loss: 0.00003063
Iteration 53/1000 | Loss: 0.00003063
Iteration 54/1000 | Loss: 0.00003063
Iteration 55/1000 | Loss: 0.00003063
Iteration 56/1000 | Loss: 0.00003063
Iteration 57/1000 | Loss: 0.00003063
Iteration 58/1000 | Loss: 0.00003063
Iteration 59/1000 | Loss: 0.00003063
Iteration 60/1000 | Loss: 0.00003063
Iteration 61/1000 | Loss: 0.00003063
Iteration 62/1000 | Loss: 0.00003062
Iteration 63/1000 | Loss: 0.00003062
Iteration 64/1000 | Loss: 0.00003062
Iteration 65/1000 | Loss: 0.00003062
Iteration 66/1000 | Loss: 0.00003062
Iteration 67/1000 | Loss: 0.00003062
Iteration 68/1000 | Loss: 0.00003062
Iteration 69/1000 | Loss: 0.00003062
Iteration 70/1000 | Loss: 0.00003062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [3.062385439989157e-05, 3.062385439989157e-05, 3.062385439989157e-05, 3.062385439989157e-05, 3.062385439989157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.062385439989157e-05

Optimization complete. Final v2v error: 4.536118030548096 mm

Highest mean error: 5.143341064453125 mm for frame 77

Lowest mean error: 3.7994213104248047 mm for frame 0

Saving results

Total time: 34.635587215423584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839853
Iteration 2/25 | Loss: 0.00130194
Iteration 3/25 | Loss: 0.00088468
Iteration 4/25 | Loss: 0.00083021
Iteration 5/25 | Loss: 0.00081593
Iteration 6/25 | Loss: 0.00081280
Iteration 7/25 | Loss: 0.00081197
Iteration 8/25 | Loss: 0.00081197
Iteration 9/25 | Loss: 0.00081197
Iteration 10/25 | Loss: 0.00081197
Iteration 11/25 | Loss: 0.00081197
Iteration 12/25 | Loss: 0.00081197
Iteration 13/25 | Loss: 0.00081197
Iteration 14/25 | Loss: 0.00081197
Iteration 15/25 | Loss: 0.00081197
Iteration 16/25 | Loss: 0.00081197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000811971549410373, 0.000811971549410373, 0.000811971549410373, 0.000811971549410373, 0.000811971549410373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000811971549410373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05498385
Iteration 2/25 | Loss: 0.00034180
Iteration 3/25 | Loss: 0.00034175
Iteration 4/25 | Loss: 0.00034175
Iteration 5/25 | Loss: 0.00034174
Iteration 6/25 | Loss: 0.00034174
Iteration 7/25 | Loss: 0.00034174
Iteration 8/25 | Loss: 0.00034174
Iteration 9/25 | Loss: 0.00034174
Iteration 10/25 | Loss: 0.00034174
Iteration 11/25 | Loss: 0.00034174
Iteration 12/25 | Loss: 0.00034174
Iteration 13/25 | Loss: 0.00034174
Iteration 14/25 | Loss: 0.00034174
Iteration 15/25 | Loss: 0.00034174
Iteration 16/25 | Loss: 0.00034174
Iteration 17/25 | Loss: 0.00034174
Iteration 18/25 | Loss: 0.00034174
Iteration 19/25 | Loss: 0.00034174
Iteration 20/25 | Loss: 0.00034174
Iteration 21/25 | Loss: 0.00034174
Iteration 22/25 | Loss: 0.00034174
Iteration 23/25 | Loss: 0.00034174
Iteration 24/25 | Loss: 0.00034174
Iteration 25/25 | Loss: 0.00034174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034174
Iteration 2/1000 | Loss: 0.00006690
Iteration 3/1000 | Loss: 0.00004964
Iteration 4/1000 | Loss: 0.00004166
Iteration 5/1000 | Loss: 0.00003922
Iteration 6/1000 | Loss: 0.00003775
Iteration 7/1000 | Loss: 0.00003693
Iteration 8/1000 | Loss: 0.00003595
Iteration 9/1000 | Loss: 0.00003528
Iteration 10/1000 | Loss: 0.00003487
Iteration 11/1000 | Loss: 0.00003454
Iteration 12/1000 | Loss: 0.00003426
Iteration 13/1000 | Loss: 0.00003406
Iteration 14/1000 | Loss: 0.00003389
Iteration 15/1000 | Loss: 0.00003368
Iteration 16/1000 | Loss: 0.00003364
Iteration 17/1000 | Loss: 0.00003353
Iteration 18/1000 | Loss: 0.00003343
Iteration 19/1000 | Loss: 0.00003342
Iteration 20/1000 | Loss: 0.00003338
Iteration 21/1000 | Loss: 0.00003336
Iteration 22/1000 | Loss: 0.00003335
Iteration 23/1000 | Loss: 0.00003335
Iteration 24/1000 | Loss: 0.00003332
Iteration 25/1000 | Loss: 0.00003332
Iteration 26/1000 | Loss: 0.00003329
Iteration 27/1000 | Loss: 0.00003329
Iteration 28/1000 | Loss: 0.00003328
Iteration 29/1000 | Loss: 0.00003328
Iteration 30/1000 | Loss: 0.00003327
Iteration 31/1000 | Loss: 0.00003327
Iteration 32/1000 | Loss: 0.00003325
Iteration 33/1000 | Loss: 0.00003325
Iteration 34/1000 | Loss: 0.00003324
Iteration 35/1000 | Loss: 0.00003324
Iteration 36/1000 | Loss: 0.00003324
Iteration 37/1000 | Loss: 0.00003323
Iteration 38/1000 | Loss: 0.00003323
Iteration 39/1000 | Loss: 0.00003323
Iteration 40/1000 | Loss: 0.00003322
Iteration 41/1000 | Loss: 0.00003322
Iteration 42/1000 | Loss: 0.00003321
Iteration 43/1000 | Loss: 0.00003321
Iteration 44/1000 | Loss: 0.00003321
Iteration 45/1000 | Loss: 0.00003320
Iteration 46/1000 | Loss: 0.00003320
Iteration 47/1000 | Loss: 0.00003320
Iteration 48/1000 | Loss: 0.00003319
Iteration 49/1000 | Loss: 0.00003319
Iteration 50/1000 | Loss: 0.00003319
Iteration 51/1000 | Loss: 0.00003318
Iteration 52/1000 | Loss: 0.00003318
Iteration 53/1000 | Loss: 0.00003318
Iteration 54/1000 | Loss: 0.00003317
Iteration 55/1000 | Loss: 0.00003317
Iteration 56/1000 | Loss: 0.00003314
Iteration 57/1000 | Loss: 0.00003314
Iteration 58/1000 | Loss: 0.00003313
Iteration 59/1000 | Loss: 0.00003313
Iteration 60/1000 | Loss: 0.00003313
Iteration 61/1000 | Loss: 0.00003312
Iteration 62/1000 | Loss: 0.00003312
Iteration 63/1000 | Loss: 0.00003312
Iteration 64/1000 | Loss: 0.00003311
Iteration 65/1000 | Loss: 0.00003311
Iteration 66/1000 | Loss: 0.00003310
Iteration 67/1000 | Loss: 0.00003310
Iteration 68/1000 | Loss: 0.00003310
Iteration 69/1000 | Loss: 0.00003309
Iteration 70/1000 | Loss: 0.00003309
Iteration 71/1000 | Loss: 0.00003309
Iteration 72/1000 | Loss: 0.00003309
Iteration 73/1000 | Loss: 0.00003309
Iteration 74/1000 | Loss: 0.00003309
Iteration 75/1000 | Loss: 0.00003309
Iteration 76/1000 | Loss: 0.00003309
Iteration 77/1000 | Loss: 0.00003308
Iteration 78/1000 | Loss: 0.00003308
Iteration 79/1000 | Loss: 0.00003308
Iteration 80/1000 | Loss: 0.00003308
Iteration 81/1000 | Loss: 0.00003307
Iteration 82/1000 | Loss: 0.00003307
Iteration 83/1000 | Loss: 0.00003307
Iteration 84/1000 | Loss: 0.00003307
Iteration 85/1000 | Loss: 0.00003306
Iteration 86/1000 | Loss: 0.00003306
Iteration 87/1000 | Loss: 0.00003306
Iteration 88/1000 | Loss: 0.00003306
Iteration 89/1000 | Loss: 0.00003306
Iteration 90/1000 | Loss: 0.00003306
Iteration 91/1000 | Loss: 0.00003306
Iteration 92/1000 | Loss: 0.00003306
Iteration 93/1000 | Loss: 0.00003306
Iteration 94/1000 | Loss: 0.00003306
Iteration 95/1000 | Loss: 0.00003305
Iteration 96/1000 | Loss: 0.00003305
Iteration 97/1000 | Loss: 0.00003305
Iteration 98/1000 | Loss: 0.00003305
Iteration 99/1000 | Loss: 0.00003305
Iteration 100/1000 | Loss: 0.00003304
Iteration 101/1000 | Loss: 0.00003304
Iteration 102/1000 | Loss: 0.00003304
Iteration 103/1000 | Loss: 0.00003304
Iteration 104/1000 | Loss: 0.00003304
Iteration 105/1000 | Loss: 0.00003304
Iteration 106/1000 | Loss: 0.00003304
Iteration 107/1000 | Loss: 0.00003303
Iteration 108/1000 | Loss: 0.00003303
Iteration 109/1000 | Loss: 0.00003303
Iteration 110/1000 | Loss: 0.00003302
Iteration 111/1000 | Loss: 0.00003302
Iteration 112/1000 | Loss: 0.00003302
Iteration 113/1000 | Loss: 0.00003302
Iteration 114/1000 | Loss: 0.00003301
Iteration 115/1000 | Loss: 0.00003301
Iteration 116/1000 | Loss: 0.00003301
Iteration 117/1000 | Loss: 0.00003301
Iteration 118/1000 | Loss: 0.00003300
Iteration 119/1000 | Loss: 0.00003299
Iteration 120/1000 | Loss: 0.00003299
Iteration 121/1000 | Loss: 0.00003299
Iteration 122/1000 | Loss: 0.00003299
Iteration 123/1000 | Loss: 0.00003299
Iteration 124/1000 | Loss: 0.00003299
Iteration 125/1000 | Loss: 0.00003299
Iteration 126/1000 | Loss: 0.00003299
Iteration 127/1000 | Loss: 0.00003298
Iteration 128/1000 | Loss: 0.00003298
Iteration 129/1000 | Loss: 0.00003298
Iteration 130/1000 | Loss: 0.00003298
Iteration 131/1000 | Loss: 0.00003298
Iteration 132/1000 | Loss: 0.00003298
Iteration 133/1000 | Loss: 0.00003298
Iteration 134/1000 | Loss: 0.00003297
Iteration 135/1000 | Loss: 0.00003297
Iteration 136/1000 | Loss: 0.00003297
Iteration 137/1000 | Loss: 0.00003297
Iteration 138/1000 | Loss: 0.00003297
Iteration 139/1000 | Loss: 0.00003296
Iteration 140/1000 | Loss: 0.00003296
Iteration 141/1000 | Loss: 0.00003296
Iteration 142/1000 | Loss: 0.00003296
Iteration 143/1000 | Loss: 0.00003295
Iteration 144/1000 | Loss: 0.00003295
Iteration 145/1000 | Loss: 0.00003295
Iteration 146/1000 | Loss: 0.00003295
Iteration 147/1000 | Loss: 0.00003295
Iteration 148/1000 | Loss: 0.00003295
Iteration 149/1000 | Loss: 0.00003295
Iteration 150/1000 | Loss: 0.00003295
Iteration 151/1000 | Loss: 0.00003295
Iteration 152/1000 | Loss: 0.00003295
Iteration 153/1000 | Loss: 0.00003295
Iteration 154/1000 | Loss: 0.00003295
Iteration 155/1000 | Loss: 0.00003295
Iteration 156/1000 | Loss: 0.00003295
Iteration 157/1000 | Loss: 0.00003295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.294625275884755e-05, 3.294625275884755e-05, 3.294625275884755e-05, 3.294625275884755e-05, 3.294625275884755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.294625275884755e-05

Optimization complete. Final v2v error: 4.570946216583252 mm

Highest mean error: 6.142256259918213 mm for frame 154

Lowest mean error: 3.757045269012451 mm for frame 110

Saving results

Total time: 52.68790078163147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810086
Iteration 2/25 | Loss: 0.00146200
Iteration 3/25 | Loss: 0.00093544
Iteration 4/25 | Loss: 0.00084688
Iteration 5/25 | Loss: 0.00082531
Iteration 6/25 | Loss: 0.00081375
Iteration 7/25 | Loss: 0.00081200
Iteration 8/25 | Loss: 0.00081232
Iteration 9/25 | Loss: 0.00080866
Iteration 10/25 | Loss: 0.00080752
Iteration 11/25 | Loss: 0.00079306
Iteration 12/25 | Loss: 0.00078537
Iteration 13/25 | Loss: 0.00078280
Iteration 14/25 | Loss: 0.00077739
Iteration 15/25 | Loss: 0.00077307
Iteration 16/25 | Loss: 0.00076545
Iteration 17/25 | Loss: 0.00076297
Iteration 18/25 | Loss: 0.00076058
Iteration 19/25 | Loss: 0.00076158
Iteration 20/25 | Loss: 0.00076458
Iteration 21/25 | Loss: 0.00075932
Iteration 22/25 | Loss: 0.00075540
Iteration 23/25 | Loss: 0.00075429
Iteration 24/25 | Loss: 0.00075426
Iteration 25/25 | Loss: 0.00075356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39544868
Iteration 2/25 | Loss: 0.00038154
Iteration 3/25 | Loss: 0.00038153
Iteration 4/25 | Loss: 0.00038153
Iteration 5/25 | Loss: 0.00038153
Iteration 6/25 | Loss: 0.00038153
Iteration 7/25 | Loss: 0.00038153
Iteration 8/25 | Loss: 0.00038153
Iteration 9/25 | Loss: 0.00038153
Iteration 10/25 | Loss: 0.00038153
Iteration 11/25 | Loss: 0.00038153
Iteration 12/25 | Loss: 0.00038153
Iteration 13/25 | Loss: 0.00038153
Iteration 14/25 | Loss: 0.00038153
Iteration 15/25 | Loss: 0.00038153
Iteration 16/25 | Loss: 0.00038153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00038153017521835864, 0.00038153017521835864, 0.00038153017521835864, 0.00038153017521835864, 0.00038153017521835864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038153017521835864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038153
Iteration 2/1000 | Loss: 0.00005775
Iteration 3/1000 | Loss: 0.00004679
Iteration 4/1000 | Loss: 0.00004553
Iteration 5/1000 | Loss: 0.00003271
Iteration 6/1000 | Loss: 0.00003952
Iteration 7/1000 | Loss: 0.00003103
Iteration 8/1000 | Loss: 0.00003371
Iteration 9/1000 | Loss: 0.00004420
Iteration 10/1000 | Loss: 0.00090583
Iteration 11/1000 | Loss: 0.00004270
Iteration 12/1000 | Loss: 0.00003115
Iteration 13/1000 | Loss: 0.00002801
Iteration 14/1000 | Loss: 0.00002620
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00002431
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002324
Iteration 19/1000 | Loss: 0.00002264
Iteration 20/1000 | Loss: 0.00002219
Iteration 21/1000 | Loss: 0.00002193
Iteration 22/1000 | Loss: 0.00002176
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002157
Iteration 25/1000 | Loss: 0.00002151
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002145
Iteration 28/1000 | Loss: 0.00002140
Iteration 29/1000 | Loss: 0.00002140
Iteration 30/1000 | Loss: 0.00002136
Iteration 31/1000 | Loss: 0.00002133
Iteration 32/1000 | Loss: 0.00002133
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002131
Iteration 35/1000 | Loss: 0.00002131
Iteration 36/1000 | Loss: 0.00002131
Iteration 37/1000 | Loss: 0.00002131
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002130
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002129
Iteration 44/1000 | Loss: 0.00002129
Iteration 45/1000 | Loss: 0.00002129
Iteration 46/1000 | Loss: 0.00002129
Iteration 47/1000 | Loss: 0.00002129
Iteration 48/1000 | Loss: 0.00002128
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002128
Iteration 53/1000 | Loss: 0.00002128
Iteration 54/1000 | Loss: 0.00002128
Iteration 55/1000 | Loss: 0.00002128
Iteration 56/1000 | Loss: 0.00002127
Iteration 57/1000 | Loss: 0.00002127
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002126
Iteration 60/1000 | Loss: 0.00002126
Iteration 61/1000 | Loss: 0.00002126
Iteration 62/1000 | Loss: 0.00002126
Iteration 63/1000 | Loss: 0.00002126
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002125
Iteration 67/1000 | Loss: 0.00002125
Iteration 68/1000 | Loss: 0.00002125
Iteration 69/1000 | Loss: 0.00002125
Iteration 70/1000 | Loss: 0.00002125
Iteration 71/1000 | Loss: 0.00002125
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00002124
Iteration 76/1000 | Loss: 0.00002124
Iteration 77/1000 | Loss: 0.00002124
Iteration 78/1000 | Loss: 0.00002124
Iteration 79/1000 | Loss: 0.00002124
Iteration 80/1000 | Loss: 0.00002124
Iteration 81/1000 | Loss: 0.00002123
Iteration 82/1000 | Loss: 0.00002123
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002123
Iteration 85/1000 | Loss: 0.00002123
Iteration 86/1000 | Loss: 0.00002123
Iteration 87/1000 | Loss: 0.00002123
Iteration 88/1000 | Loss: 0.00002123
Iteration 89/1000 | Loss: 0.00002122
Iteration 90/1000 | Loss: 0.00002122
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002121
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002118
Iteration 100/1000 | Loss: 0.00002118
Iteration 101/1000 | Loss: 0.00002118
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002117
Iteration 104/1000 | Loss: 0.00002117
Iteration 105/1000 | Loss: 0.00002117
Iteration 106/1000 | Loss: 0.00002116
Iteration 107/1000 | Loss: 0.00002116
Iteration 108/1000 | Loss: 0.00002115
Iteration 109/1000 | Loss: 0.00002115
Iteration 110/1000 | Loss: 0.00002114
Iteration 111/1000 | Loss: 0.00002113
Iteration 112/1000 | Loss: 0.00002113
Iteration 113/1000 | Loss: 0.00002113
Iteration 114/1000 | Loss: 0.00002113
Iteration 115/1000 | Loss: 0.00002113
Iteration 116/1000 | Loss: 0.00002112
Iteration 117/1000 | Loss: 0.00002112
Iteration 118/1000 | Loss: 0.00002111
Iteration 119/1000 | Loss: 0.00002111
Iteration 120/1000 | Loss: 0.00002111
Iteration 121/1000 | Loss: 0.00002110
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002109
Iteration 129/1000 | Loss: 0.00002109
Iteration 130/1000 | Loss: 0.00002109
Iteration 131/1000 | Loss: 0.00002109
Iteration 132/1000 | Loss: 0.00002109
Iteration 133/1000 | Loss: 0.00002109
Iteration 134/1000 | Loss: 0.00002109
Iteration 135/1000 | Loss: 0.00002109
Iteration 136/1000 | Loss: 0.00002109
Iteration 137/1000 | Loss: 0.00002108
Iteration 138/1000 | Loss: 0.00002108
Iteration 139/1000 | Loss: 0.00002108
Iteration 140/1000 | Loss: 0.00002108
Iteration 141/1000 | Loss: 0.00002108
Iteration 142/1000 | Loss: 0.00002108
Iteration 143/1000 | Loss: 0.00002108
Iteration 144/1000 | Loss: 0.00002108
Iteration 145/1000 | Loss: 0.00002108
Iteration 146/1000 | Loss: 0.00002108
Iteration 147/1000 | Loss: 0.00002108
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002108
Iteration 150/1000 | Loss: 0.00002108
Iteration 151/1000 | Loss: 0.00002108
Iteration 152/1000 | Loss: 0.00002108
Iteration 153/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.108016451529693e-05, 2.108016451529693e-05, 2.108016451529693e-05, 2.108016451529693e-05, 2.108016451529693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.108016451529693e-05

Optimization complete. Final v2v error: 3.8206005096435547 mm

Highest mean error: 4.398351669311523 mm for frame 61

Lowest mean error: 3.1827352046966553 mm for frame 209

Saving results

Total time: 101.73579692840576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362004
Iteration 2/25 | Loss: 0.00072995
Iteration 3/25 | Loss: 0.00060182
Iteration 4/25 | Loss: 0.00058554
Iteration 5/25 | Loss: 0.00058063
Iteration 6/25 | Loss: 0.00057930
Iteration 7/25 | Loss: 0.00057900
Iteration 8/25 | Loss: 0.00057900
Iteration 9/25 | Loss: 0.00057900
Iteration 10/25 | Loss: 0.00057900
Iteration 11/25 | Loss: 0.00057900
Iteration 12/25 | Loss: 0.00057900
Iteration 13/25 | Loss: 0.00057900
Iteration 14/25 | Loss: 0.00057900
Iteration 15/25 | Loss: 0.00057900
Iteration 16/25 | Loss: 0.00057900
Iteration 17/25 | Loss: 0.00057900
Iteration 18/25 | Loss: 0.00057900
Iteration 19/25 | Loss: 0.00057900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005790017312392592, 0.0005790017312392592, 0.0005790017312392592, 0.0005790017312392592, 0.0005790017312392592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005790017312392592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70496786
Iteration 2/25 | Loss: 0.00029129
Iteration 3/25 | Loss: 0.00029129
Iteration 4/25 | Loss: 0.00029129
Iteration 5/25 | Loss: 0.00029129
Iteration 6/25 | Loss: 0.00029129
Iteration 7/25 | Loss: 0.00029129
Iteration 8/25 | Loss: 0.00029129
Iteration 9/25 | Loss: 0.00029129
Iteration 10/25 | Loss: 0.00029129
Iteration 11/25 | Loss: 0.00029129
Iteration 12/25 | Loss: 0.00029129
Iteration 13/25 | Loss: 0.00029129
Iteration 14/25 | Loss: 0.00029129
Iteration 15/25 | Loss: 0.00029129
Iteration 16/25 | Loss: 0.00029129
Iteration 17/25 | Loss: 0.00029129
Iteration 18/25 | Loss: 0.00029129
Iteration 19/25 | Loss: 0.00029129
Iteration 20/25 | Loss: 0.00029129
Iteration 21/25 | Loss: 0.00029129
Iteration 22/25 | Loss: 0.00029129
Iteration 23/25 | Loss: 0.00029129
Iteration 24/25 | Loss: 0.00029129
Iteration 25/25 | Loss: 0.00029129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029129
Iteration 2/1000 | Loss: 0.00002068
Iteration 3/1000 | Loss: 0.00001156
Iteration 4/1000 | Loss: 0.00001071
Iteration 5/1000 | Loss: 0.00001017
Iteration 6/1000 | Loss: 0.00000984
Iteration 7/1000 | Loss: 0.00000974
Iteration 8/1000 | Loss: 0.00000959
Iteration 9/1000 | Loss: 0.00000954
Iteration 10/1000 | Loss: 0.00000953
Iteration 11/1000 | Loss: 0.00000952
Iteration 12/1000 | Loss: 0.00000952
Iteration 13/1000 | Loss: 0.00000951
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000949
Iteration 16/1000 | Loss: 0.00000945
Iteration 17/1000 | Loss: 0.00000945
Iteration 18/1000 | Loss: 0.00000941
Iteration 19/1000 | Loss: 0.00000941
Iteration 20/1000 | Loss: 0.00000940
Iteration 21/1000 | Loss: 0.00000940
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000939
Iteration 24/1000 | Loss: 0.00000935
Iteration 25/1000 | Loss: 0.00000933
Iteration 26/1000 | Loss: 0.00000932
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000931
Iteration 29/1000 | Loss: 0.00000930
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000927
Iteration 34/1000 | Loss: 0.00000927
Iteration 35/1000 | Loss: 0.00000927
Iteration 36/1000 | Loss: 0.00000927
Iteration 37/1000 | Loss: 0.00000926
Iteration 38/1000 | Loss: 0.00000925
Iteration 39/1000 | Loss: 0.00000925
Iteration 40/1000 | Loss: 0.00000925
Iteration 41/1000 | Loss: 0.00000925
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000924
Iteration 44/1000 | Loss: 0.00000923
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000922
Iteration 47/1000 | Loss: 0.00000921
Iteration 48/1000 | Loss: 0.00000921
Iteration 49/1000 | Loss: 0.00000921
Iteration 50/1000 | Loss: 0.00000921
Iteration 51/1000 | Loss: 0.00000920
Iteration 52/1000 | Loss: 0.00000920
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000919
Iteration 55/1000 | Loss: 0.00000919
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000917
Iteration 58/1000 | Loss: 0.00000917
Iteration 59/1000 | Loss: 0.00000917
Iteration 60/1000 | Loss: 0.00000917
Iteration 61/1000 | Loss: 0.00000916
Iteration 62/1000 | Loss: 0.00000916
Iteration 63/1000 | Loss: 0.00000916
Iteration 64/1000 | Loss: 0.00000916
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000915
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000912
Iteration 73/1000 | Loss: 0.00000912
Iteration 74/1000 | Loss: 0.00000911
Iteration 75/1000 | Loss: 0.00000911
Iteration 76/1000 | Loss: 0.00000910
Iteration 77/1000 | Loss: 0.00000910
Iteration 78/1000 | Loss: 0.00000910
Iteration 79/1000 | Loss: 0.00000910
Iteration 80/1000 | Loss: 0.00000909
Iteration 81/1000 | Loss: 0.00000909
Iteration 82/1000 | Loss: 0.00000909
Iteration 83/1000 | Loss: 0.00000909
Iteration 84/1000 | Loss: 0.00000909
Iteration 85/1000 | Loss: 0.00000908
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000908
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000908
Iteration 93/1000 | Loss: 0.00000907
Iteration 94/1000 | Loss: 0.00000907
Iteration 95/1000 | Loss: 0.00000907
Iteration 96/1000 | Loss: 0.00000907
Iteration 97/1000 | Loss: 0.00000907
Iteration 98/1000 | Loss: 0.00000906
Iteration 99/1000 | Loss: 0.00000906
Iteration 100/1000 | Loss: 0.00000906
Iteration 101/1000 | Loss: 0.00000906
Iteration 102/1000 | Loss: 0.00000906
Iteration 103/1000 | Loss: 0.00000906
Iteration 104/1000 | Loss: 0.00000906
Iteration 105/1000 | Loss: 0.00000905
Iteration 106/1000 | Loss: 0.00000905
Iteration 107/1000 | Loss: 0.00000905
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000905
Iteration 110/1000 | Loss: 0.00000905
Iteration 111/1000 | Loss: 0.00000905
Iteration 112/1000 | Loss: 0.00000905
Iteration 113/1000 | Loss: 0.00000905
Iteration 114/1000 | Loss: 0.00000905
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000905
Iteration 117/1000 | Loss: 0.00000905
Iteration 118/1000 | Loss: 0.00000905
Iteration 119/1000 | Loss: 0.00000905
Iteration 120/1000 | Loss: 0.00000905
Iteration 121/1000 | Loss: 0.00000905
Iteration 122/1000 | Loss: 0.00000905
Iteration 123/1000 | Loss: 0.00000905
Iteration 124/1000 | Loss: 0.00000905
Iteration 125/1000 | Loss: 0.00000905
Iteration 126/1000 | Loss: 0.00000905
Iteration 127/1000 | Loss: 0.00000905
Iteration 128/1000 | Loss: 0.00000905
Iteration 129/1000 | Loss: 0.00000905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [9.049998880072962e-06, 9.049998880072962e-06, 9.049998880072962e-06, 9.049998880072962e-06, 9.049998880072962e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.049998880072962e-06

Optimization complete. Final v2v error: 2.5633630752563477 mm

Highest mean error: 3.097590446472168 mm for frame 77

Lowest mean error: 2.4730241298675537 mm for frame 109

Saving results

Total time: 31.540598154067993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515238
Iteration 2/25 | Loss: 0.00092301
Iteration 3/25 | Loss: 0.00070598
Iteration 4/25 | Loss: 0.00066386
Iteration 5/25 | Loss: 0.00065021
Iteration 6/25 | Loss: 0.00064671
Iteration 7/25 | Loss: 0.00064573
Iteration 8/25 | Loss: 0.00064549
Iteration 9/25 | Loss: 0.00064549
Iteration 10/25 | Loss: 0.00064549
Iteration 11/25 | Loss: 0.00064549
Iteration 12/25 | Loss: 0.00064549
Iteration 13/25 | Loss: 0.00064549
Iteration 14/25 | Loss: 0.00064549
Iteration 15/25 | Loss: 0.00064549
Iteration 16/25 | Loss: 0.00064549
Iteration 17/25 | Loss: 0.00064549
Iteration 18/25 | Loss: 0.00064549
Iteration 19/25 | Loss: 0.00064549
Iteration 20/25 | Loss: 0.00064549
Iteration 21/25 | Loss: 0.00064549
Iteration 22/25 | Loss: 0.00064549
Iteration 23/25 | Loss: 0.00064549
Iteration 24/25 | Loss: 0.00064549
Iteration 25/25 | Loss: 0.00064549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.50318813
Iteration 2/25 | Loss: 0.00041952
Iteration 3/25 | Loss: 0.00041952
Iteration 4/25 | Loss: 0.00041952
Iteration 5/25 | Loss: 0.00041952
Iteration 6/25 | Loss: 0.00041952
Iteration 7/25 | Loss: 0.00041952
Iteration 8/25 | Loss: 0.00041952
Iteration 9/25 | Loss: 0.00041952
Iteration 10/25 | Loss: 0.00041952
Iteration 11/25 | Loss: 0.00041952
Iteration 12/25 | Loss: 0.00041952
Iteration 13/25 | Loss: 0.00041952
Iteration 14/25 | Loss: 0.00041952
Iteration 15/25 | Loss: 0.00041952
Iteration 16/25 | Loss: 0.00041952
Iteration 17/25 | Loss: 0.00041952
Iteration 18/25 | Loss: 0.00041952
Iteration 19/25 | Loss: 0.00041952
Iteration 20/25 | Loss: 0.00041952
Iteration 21/25 | Loss: 0.00041952
Iteration 22/25 | Loss: 0.00041952
Iteration 23/25 | Loss: 0.00041952
Iteration 24/25 | Loss: 0.00041952
Iteration 25/25 | Loss: 0.00041952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041952
Iteration 2/1000 | Loss: 0.00003643
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00001574
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001343
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001330
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001325
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001323
Iteration 30/1000 | Loss: 0.00001322
Iteration 31/1000 | Loss: 0.00001322
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001312
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001311
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001306
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001304
Iteration 118/1000 | Loss: 0.00001304
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001302
Iteration 134/1000 | Loss: 0.00001302
Iteration 135/1000 | Loss: 0.00001302
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001301
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001301
Iteration 144/1000 | Loss: 0.00001301
Iteration 145/1000 | Loss: 0.00001301
Iteration 146/1000 | Loss: 0.00001301
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001301
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001299
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001298
Iteration 176/1000 | Loss: 0.00001298
Iteration 177/1000 | Loss: 0.00001298
Iteration 178/1000 | Loss: 0.00001298
Iteration 179/1000 | Loss: 0.00001298
Iteration 180/1000 | Loss: 0.00001298
Iteration 181/1000 | Loss: 0.00001298
Iteration 182/1000 | Loss: 0.00001298
Iteration 183/1000 | Loss: 0.00001298
Iteration 184/1000 | Loss: 0.00001298
Iteration 185/1000 | Loss: 0.00001297
Iteration 186/1000 | Loss: 0.00001297
Iteration 187/1000 | Loss: 0.00001297
Iteration 188/1000 | Loss: 0.00001297
Iteration 189/1000 | Loss: 0.00001297
Iteration 190/1000 | Loss: 0.00001297
Iteration 191/1000 | Loss: 0.00001297
Iteration 192/1000 | Loss: 0.00001297
Iteration 193/1000 | Loss: 0.00001297
Iteration 194/1000 | Loss: 0.00001297
Iteration 195/1000 | Loss: 0.00001297
Iteration 196/1000 | Loss: 0.00001297
Iteration 197/1000 | Loss: 0.00001297
Iteration 198/1000 | Loss: 0.00001297
Iteration 199/1000 | Loss: 0.00001297
Iteration 200/1000 | Loss: 0.00001297
Iteration 201/1000 | Loss: 0.00001297
Iteration 202/1000 | Loss: 0.00001297
Iteration 203/1000 | Loss: 0.00001297
Iteration 204/1000 | Loss: 0.00001297
Iteration 205/1000 | Loss: 0.00001297
Iteration 206/1000 | Loss: 0.00001296
Iteration 207/1000 | Loss: 0.00001296
Iteration 208/1000 | Loss: 0.00001296
Iteration 209/1000 | Loss: 0.00001296
Iteration 210/1000 | Loss: 0.00001296
Iteration 211/1000 | Loss: 0.00001296
Iteration 212/1000 | Loss: 0.00001296
Iteration 213/1000 | Loss: 0.00001296
Iteration 214/1000 | Loss: 0.00001296
Iteration 215/1000 | Loss: 0.00001296
Iteration 216/1000 | Loss: 0.00001296
Iteration 217/1000 | Loss: 0.00001296
Iteration 218/1000 | Loss: 0.00001296
Iteration 219/1000 | Loss: 0.00001296
Iteration 220/1000 | Loss: 0.00001296
Iteration 221/1000 | Loss: 0.00001296
Iteration 222/1000 | Loss: 0.00001296
Iteration 223/1000 | Loss: 0.00001296
Iteration 224/1000 | Loss: 0.00001296
Iteration 225/1000 | Loss: 0.00001296
Iteration 226/1000 | Loss: 0.00001296
Iteration 227/1000 | Loss: 0.00001296
Iteration 228/1000 | Loss: 0.00001296
Iteration 229/1000 | Loss: 0.00001296
Iteration 230/1000 | Loss: 0.00001296
Iteration 231/1000 | Loss: 0.00001296
Iteration 232/1000 | Loss: 0.00001296
Iteration 233/1000 | Loss: 0.00001296
Iteration 234/1000 | Loss: 0.00001296
Iteration 235/1000 | Loss: 0.00001296
Iteration 236/1000 | Loss: 0.00001296
Iteration 237/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.2956834325450473e-05, 1.2956834325450473e-05, 1.2956834325450473e-05, 1.2956834325450473e-05, 1.2956834325450473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2956834325450473e-05

Optimization complete. Final v2v error: 3.063849687576294 mm

Highest mean error: 3.412059783935547 mm for frame 124

Lowest mean error: 2.7094595432281494 mm for frame 92

Saving results

Total time: 42.837624073028564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478979
Iteration 2/25 | Loss: 0.00110876
Iteration 3/25 | Loss: 0.00070976
Iteration 4/25 | Loss: 0.00064174
Iteration 5/25 | Loss: 0.00062514
Iteration 6/25 | Loss: 0.00062145
Iteration 7/25 | Loss: 0.00061980
Iteration 8/25 | Loss: 0.00061953
Iteration 9/25 | Loss: 0.00061953
Iteration 10/25 | Loss: 0.00061953
Iteration 11/25 | Loss: 0.00061953
Iteration 12/25 | Loss: 0.00061953
Iteration 13/25 | Loss: 0.00061953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006195274763740599, 0.0006195274763740599, 0.0006195274763740599, 0.0006195274763740599, 0.0006195274763740599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006195274763740599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47326338
Iteration 2/25 | Loss: 0.00027305
Iteration 3/25 | Loss: 0.00027304
Iteration 4/25 | Loss: 0.00027304
Iteration 5/25 | Loss: 0.00027304
Iteration 6/25 | Loss: 0.00027304
Iteration 7/25 | Loss: 0.00027304
Iteration 8/25 | Loss: 0.00027304
Iteration 9/25 | Loss: 0.00027304
Iteration 10/25 | Loss: 0.00027304
Iteration 11/25 | Loss: 0.00027304
Iteration 12/25 | Loss: 0.00027304
Iteration 13/25 | Loss: 0.00027304
Iteration 14/25 | Loss: 0.00027304
Iteration 15/25 | Loss: 0.00027304
Iteration 16/25 | Loss: 0.00027304
Iteration 17/25 | Loss: 0.00027304
Iteration 18/25 | Loss: 0.00027304
Iteration 19/25 | Loss: 0.00027304
Iteration 20/25 | Loss: 0.00027304
Iteration 21/25 | Loss: 0.00027304
Iteration 22/25 | Loss: 0.00027304
Iteration 23/25 | Loss: 0.00027304
Iteration 24/25 | Loss: 0.00027304
Iteration 25/25 | Loss: 0.00027304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027304
Iteration 2/1000 | Loss: 0.00002442
Iteration 3/1000 | Loss: 0.00001679
Iteration 4/1000 | Loss: 0.00001559
Iteration 5/1000 | Loss: 0.00001505
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001384
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001350
Iteration 23/1000 | Loss: 0.00001348
Iteration 24/1000 | Loss: 0.00001343
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001341
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001339
Iteration 33/1000 | Loss: 0.00001338
Iteration 34/1000 | Loss: 0.00001338
Iteration 35/1000 | Loss: 0.00001338
Iteration 36/1000 | Loss: 0.00001337
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001330
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001329
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001329
Iteration 58/1000 | Loss: 0.00001329
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001326
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001321
Iteration 121/1000 | Loss: 0.00001321
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.3207610209065024e-05, 1.3207610209065024e-05, 1.3207610209065024e-05, 1.3207610209065024e-05, 1.3207610209065024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3207610209065024e-05

Optimization complete. Final v2v error: 2.880094289779663 mm

Highest mean error: 4.4784255027771 mm for frame 82

Lowest mean error: 2.3954050540924072 mm for frame 153

Saving results

Total time: 36.789339780807495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052633
Iteration 2/25 | Loss: 0.01052633
Iteration 3/25 | Loss: 0.00284929
Iteration 4/25 | Loss: 0.00163748
Iteration 5/25 | Loss: 0.00134109
Iteration 6/25 | Loss: 0.00122702
Iteration 7/25 | Loss: 0.00110972
Iteration 8/25 | Loss: 0.00101623
Iteration 9/25 | Loss: 0.00095149
Iteration 10/25 | Loss: 0.00093822
Iteration 11/25 | Loss: 0.00092278
Iteration 12/25 | Loss: 0.00092008
Iteration 13/25 | Loss: 0.00093639
Iteration 14/25 | Loss: 0.00092301
Iteration 15/25 | Loss: 0.00091696
Iteration 16/25 | Loss: 0.00091478
Iteration 17/25 | Loss: 0.00090464
Iteration 18/25 | Loss: 0.00089425
Iteration 19/25 | Loss: 0.00089100
Iteration 20/25 | Loss: 0.00088777
Iteration 21/25 | Loss: 0.00088637
Iteration 22/25 | Loss: 0.00088839
Iteration 23/25 | Loss: 0.00088523
Iteration 24/25 | Loss: 0.00088428
Iteration 25/25 | Loss: 0.00088399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42842579
Iteration 2/25 | Loss: 0.00161602
Iteration 3/25 | Loss: 0.00126536
Iteration 4/25 | Loss: 0.00126536
Iteration 5/25 | Loss: 0.00126536
Iteration 6/25 | Loss: 0.00126536
Iteration 7/25 | Loss: 0.00126536
Iteration 8/25 | Loss: 0.00126536
Iteration 9/25 | Loss: 0.00126536
Iteration 10/25 | Loss: 0.00126535
Iteration 11/25 | Loss: 0.00126535
Iteration 12/25 | Loss: 0.00126535
Iteration 13/25 | Loss: 0.00126535
Iteration 14/25 | Loss: 0.00126535
Iteration 15/25 | Loss: 0.00126535
Iteration 16/25 | Loss: 0.00126535
Iteration 17/25 | Loss: 0.00126535
Iteration 18/25 | Loss: 0.00126535
Iteration 19/25 | Loss: 0.00126535
Iteration 20/25 | Loss: 0.00126535
Iteration 21/25 | Loss: 0.00126535
Iteration 22/25 | Loss: 0.00126535
Iteration 23/25 | Loss: 0.00126535
Iteration 24/25 | Loss: 0.00126535
Iteration 25/25 | Loss: 0.00126535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126535
Iteration 2/1000 | Loss: 0.00219876
Iteration 3/1000 | Loss: 0.00041078
Iteration 4/1000 | Loss: 0.00021144
Iteration 5/1000 | Loss: 0.00101773
Iteration 6/1000 | Loss: 0.00028247
Iteration 7/1000 | Loss: 0.00056066
Iteration 8/1000 | Loss: 0.00039934
Iteration 9/1000 | Loss: 0.00057295
Iteration 10/1000 | Loss: 0.00082691
Iteration 11/1000 | Loss: 0.00032773
Iteration 12/1000 | Loss: 0.00011689
Iteration 13/1000 | Loss: 0.00042269
Iteration 14/1000 | Loss: 0.00022139
Iteration 15/1000 | Loss: 0.00014200
Iteration 16/1000 | Loss: 0.00049109
Iteration 17/1000 | Loss: 0.00050025
Iteration 18/1000 | Loss: 0.00174939
Iteration 19/1000 | Loss: 0.00076659
Iteration 20/1000 | Loss: 0.00053854
Iteration 21/1000 | Loss: 0.00012857
Iteration 22/1000 | Loss: 0.00009207
Iteration 23/1000 | Loss: 0.00026528
Iteration 24/1000 | Loss: 0.00053112
Iteration 25/1000 | Loss: 0.00045715
Iteration 26/1000 | Loss: 0.00013871
Iteration 27/1000 | Loss: 0.00007157
Iteration 28/1000 | Loss: 0.00004941
Iteration 29/1000 | Loss: 0.00004040
Iteration 30/1000 | Loss: 0.00024101
Iteration 31/1000 | Loss: 0.00003982
Iteration 32/1000 | Loss: 0.00002982
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002488
Iteration 35/1000 | Loss: 0.00002371
Iteration 36/1000 | Loss: 0.00002250
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002144
Iteration 39/1000 | Loss: 0.00002104
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002059
Iteration 42/1000 | Loss: 0.00002034
Iteration 43/1000 | Loss: 0.00002034
Iteration 44/1000 | Loss: 0.00002024
Iteration 45/1000 | Loss: 0.00031527
Iteration 46/1000 | Loss: 0.00018247
Iteration 47/1000 | Loss: 0.00002657
Iteration 48/1000 | Loss: 0.00032302
Iteration 49/1000 | Loss: 0.00002631
Iteration 50/1000 | Loss: 0.00002111
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001883
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001788
Iteration 56/1000 | Loss: 0.00001787
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001763
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001755
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001752
Iteration 90/1000 | Loss: 0.00001752
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001751
Iteration 97/1000 | Loss: 0.00001751
Iteration 98/1000 | Loss: 0.00001751
Iteration 99/1000 | Loss: 0.00001751
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001748
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00001748
Iteration 136/1000 | Loss: 0.00001748
Iteration 137/1000 | Loss: 0.00001748
Iteration 138/1000 | Loss: 0.00001748
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001747
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00001747
Iteration 146/1000 | Loss: 0.00001747
Iteration 147/1000 | Loss: 0.00001747
Iteration 148/1000 | Loss: 0.00001747
Iteration 149/1000 | Loss: 0.00001747
Iteration 150/1000 | Loss: 0.00001747
Iteration 151/1000 | Loss: 0.00001747
Iteration 152/1000 | Loss: 0.00001747
Iteration 153/1000 | Loss: 0.00001747
Iteration 154/1000 | Loss: 0.00001746
Iteration 155/1000 | Loss: 0.00001746
Iteration 156/1000 | Loss: 0.00001746
Iteration 157/1000 | Loss: 0.00001746
Iteration 158/1000 | Loss: 0.00001746
Iteration 159/1000 | Loss: 0.00001746
Iteration 160/1000 | Loss: 0.00001746
Iteration 161/1000 | Loss: 0.00001746
Iteration 162/1000 | Loss: 0.00001746
Iteration 163/1000 | Loss: 0.00001746
Iteration 164/1000 | Loss: 0.00001746
Iteration 165/1000 | Loss: 0.00001746
Iteration 166/1000 | Loss: 0.00001746
Iteration 167/1000 | Loss: 0.00001746
Iteration 168/1000 | Loss: 0.00001746
Iteration 169/1000 | Loss: 0.00001746
Iteration 170/1000 | Loss: 0.00001746
Iteration 171/1000 | Loss: 0.00001746
Iteration 172/1000 | Loss: 0.00001746
Iteration 173/1000 | Loss: 0.00001746
Iteration 174/1000 | Loss: 0.00001746
Iteration 175/1000 | Loss: 0.00001745
Iteration 176/1000 | Loss: 0.00001745
Iteration 177/1000 | Loss: 0.00001745
Iteration 178/1000 | Loss: 0.00001745
Iteration 179/1000 | Loss: 0.00001745
Iteration 180/1000 | Loss: 0.00001745
Iteration 181/1000 | Loss: 0.00001745
Iteration 182/1000 | Loss: 0.00001745
Iteration 183/1000 | Loss: 0.00001745
Iteration 184/1000 | Loss: 0.00001745
Iteration 185/1000 | Loss: 0.00001745
Iteration 186/1000 | Loss: 0.00001745
Iteration 187/1000 | Loss: 0.00001745
Iteration 188/1000 | Loss: 0.00001745
Iteration 189/1000 | Loss: 0.00001745
Iteration 190/1000 | Loss: 0.00001744
Iteration 191/1000 | Loss: 0.00001744
Iteration 192/1000 | Loss: 0.00001744
Iteration 193/1000 | Loss: 0.00001744
Iteration 194/1000 | Loss: 0.00001744
Iteration 195/1000 | Loss: 0.00001744
Iteration 196/1000 | Loss: 0.00001744
Iteration 197/1000 | Loss: 0.00001744
Iteration 198/1000 | Loss: 0.00001744
Iteration 199/1000 | Loss: 0.00001744
Iteration 200/1000 | Loss: 0.00001744
Iteration 201/1000 | Loss: 0.00001744
Iteration 202/1000 | Loss: 0.00001744
Iteration 203/1000 | Loss: 0.00001743
Iteration 204/1000 | Loss: 0.00001743
Iteration 205/1000 | Loss: 0.00001743
Iteration 206/1000 | Loss: 0.00001743
Iteration 207/1000 | Loss: 0.00001743
Iteration 208/1000 | Loss: 0.00001743
Iteration 209/1000 | Loss: 0.00001743
Iteration 210/1000 | Loss: 0.00001743
Iteration 211/1000 | Loss: 0.00001743
Iteration 212/1000 | Loss: 0.00001743
Iteration 213/1000 | Loss: 0.00001743
Iteration 214/1000 | Loss: 0.00001743
Iteration 215/1000 | Loss: 0.00001743
Iteration 216/1000 | Loss: 0.00001743
Iteration 217/1000 | Loss: 0.00001743
Iteration 218/1000 | Loss: 0.00001743
Iteration 219/1000 | Loss: 0.00001743
Iteration 220/1000 | Loss: 0.00001743
Iteration 221/1000 | Loss: 0.00001743
Iteration 222/1000 | Loss: 0.00001743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.7428674254915677e-05, 1.7428674254915677e-05, 1.7428674254915677e-05, 1.7428674254915677e-05, 1.7428674254915677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7428674254915677e-05

Optimization complete. Final v2v error: 3.5629050731658936 mm

Highest mean error: 4.348762512207031 mm for frame 46

Lowest mean error: 3.227426052093506 mm for frame 233

Saving results

Total time: 148.91606283187866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860959
Iteration 2/25 | Loss: 0.00169697
Iteration 3/25 | Loss: 0.00095314
Iteration 4/25 | Loss: 0.00081115
Iteration 5/25 | Loss: 0.00079318
Iteration 6/25 | Loss: 0.00079163
Iteration 7/25 | Loss: 0.00078908
Iteration 8/25 | Loss: 0.00078828
Iteration 9/25 | Loss: 0.00078797
Iteration 10/25 | Loss: 0.00078782
Iteration 11/25 | Loss: 0.00078777
Iteration 12/25 | Loss: 0.00078777
Iteration 13/25 | Loss: 0.00078777
Iteration 14/25 | Loss: 0.00078777
Iteration 15/25 | Loss: 0.00078777
Iteration 16/25 | Loss: 0.00078777
Iteration 17/25 | Loss: 0.00078777
Iteration 18/25 | Loss: 0.00078777
Iteration 19/25 | Loss: 0.00078777
Iteration 20/25 | Loss: 0.00078777
Iteration 21/25 | Loss: 0.00078777
Iteration 22/25 | Loss: 0.00078776
Iteration 23/25 | Loss: 0.00078776
Iteration 24/25 | Loss: 0.00078776
Iteration 25/25 | Loss: 0.00078776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29209948
Iteration 2/25 | Loss: 0.00036944
Iteration 3/25 | Loss: 0.00036944
Iteration 4/25 | Loss: 0.00036944
Iteration 5/25 | Loss: 0.00036944
Iteration 6/25 | Loss: 0.00036944
Iteration 7/25 | Loss: 0.00036944
Iteration 8/25 | Loss: 0.00036944
Iteration 9/25 | Loss: 0.00036944
Iteration 10/25 | Loss: 0.00036944
Iteration 11/25 | Loss: 0.00036944
Iteration 12/25 | Loss: 0.00036944
Iteration 13/25 | Loss: 0.00036944
Iteration 14/25 | Loss: 0.00036944
Iteration 15/25 | Loss: 0.00036944
Iteration 16/25 | Loss: 0.00036944
Iteration 17/25 | Loss: 0.00036944
Iteration 18/25 | Loss: 0.00036944
Iteration 19/25 | Loss: 0.00036944
Iteration 20/25 | Loss: 0.00036944
Iteration 21/25 | Loss: 0.00036944
Iteration 22/25 | Loss: 0.00036944
Iteration 23/25 | Loss: 0.00036944
Iteration 24/25 | Loss: 0.00036944
Iteration 25/25 | Loss: 0.00036944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036944
Iteration 2/1000 | Loss: 0.00003370
Iteration 3/1000 | Loss: 0.00002524
Iteration 4/1000 | Loss: 0.00002288
Iteration 5/1000 | Loss: 0.00002188
Iteration 6/1000 | Loss: 0.00002125
Iteration 7/1000 | Loss: 0.00002101
Iteration 8/1000 | Loss: 0.00002093
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002053
Iteration 13/1000 | Loss: 0.00002046
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002042
Iteration 16/1000 | Loss: 0.00002042
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002035
Iteration 21/1000 | Loss: 0.00002034
Iteration 22/1000 | Loss: 0.00002034
Iteration 23/1000 | Loss: 0.00002024
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002024
Iteration 28/1000 | Loss: 0.00002024
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002024
Iteration 32/1000 | Loss: 0.00002024
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002023
Iteration 35/1000 | Loss: 0.00002023
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002021
Iteration 40/1000 | Loss: 0.00002020
Iteration 41/1000 | Loss: 0.00002020
Iteration 42/1000 | Loss: 0.00002020
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002019
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00002017
Iteration 47/1000 | Loss: 0.00002016
Iteration 48/1000 | Loss: 0.00002016
Iteration 49/1000 | Loss: 0.00002016
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00002015
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00002014
Iteration 54/1000 | Loss: 0.00002014
Iteration 55/1000 | Loss: 0.00002013
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002013
Iteration 58/1000 | Loss: 0.00002013
Iteration 59/1000 | Loss: 0.00002013
Iteration 60/1000 | Loss: 0.00002012
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002012
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002012
Iteration 67/1000 | Loss: 0.00002012
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002012
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002011
Iteration 74/1000 | Loss: 0.00002011
Iteration 75/1000 | Loss: 0.00002011
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00002011
Iteration 78/1000 | Loss: 0.00002011
Iteration 79/1000 | Loss: 0.00002011
Iteration 80/1000 | Loss: 0.00002011
Iteration 81/1000 | Loss: 0.00002011
Iteration 82/1000 | Loss: 0.00002010
Iteration 83/1000 | Loss: 0.00002010
Iteration 84/1000 | Loss: 0.00002010
Iteration 85/1000 | Loss: 0.00002010
Iteration 86/1000 | Loss: 0.00002009
Iteration 87/1000 | Loss: 0.00002009
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00002009
Iteration 90/1000 | Loss: 0.00002009
Iteration 91/1000 | Loss: 0.00002009
Iteration 92/1000 | Loss: 0.00002009
Iteration 93/1000 | Loss: 0.00002009
Iteration 94/1000 | Loss: 0.00002009
Iteration 95/1000 | Loss: 0.00002009
Iteration 96/1000 | Loss: 0.00002009
Iteration 97/1000 | Loss: 0.00002009
Iteration 98/1000 | Loss: 0.00002009
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002009
Iteration 106/1000 | Loss: 0.00002009
Iteration 107/1000 | Loss: 0.00002009
Iteration 108/1000 | Loss: 0.00002009
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002009
Iteration 112/1000 | Loss: 0.00002009
Iteration 113/1000 | Loss: 0.00002009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.0088182282052003e-05, 2.0088182282052003e-05, 2.0088182282052003e-05, 2.0088182282052003e-05, 2.0088182282052003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0088182282052003e-05

Optimization complete. Final v2v error: 3.7013778686523438 mm

Highest mean error: 3.85062837600708 mm for frame 146

Lowest mean error: 3.5734035968780518 mm for frame 74

Saving results

Total time: 45.27593755722046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060776
Iteration 2/25 | Loss: 0.00210362
Iteration 3/25 | Loss: 0.00119120
Iteration 4/25 | Loss: 0.00096606
Iteration 5/25 | Loss: 0.00085434
Iteration 6/25 | Loss: 0.00082666
Iteration 7/25 | Loss: 0.00081322
Iteration 8/25 | Loss: 0.00080552
Iteration 9/25 | Loss: 0.00080346
Iteration 10/25 | Loss: 0.00080624
Iteration 11/25 | Loss: 0.00080525
Iteration 12/25 | Loss: 0.00080000
Iteration 13/25 | Loss: 0.00079687
Iteration 14/25 | Loss: 0.00079571
Iteration 15/25 | Loss: 0.00079199
Iteration 16/25 | Loss: 0.00079674
Iteration 17/25 | Loss: 0.00079305
Iteration 18/25 | Loss: 0.00078974
Iteration 19/25 | Loss: 0.00078236
Iteration 20/25 | Loss: 0.00078513
Iteration 21/25 | Loss: 0.00078158
Iteration 22/25 | Loss: 0.00077499
Iteration 23/25 | Loss: 0.00077265
Iteration 24/25 | Loss: 0.00077569
Iteration 25/25 | Loss: 0.00077222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52898300
Iteration 2/25 | Loss: 0.00391217
Iteration 3/25 | Loss: 0.00284358
Iteration 4/25 | Loss: 0.00146826
Iteration 5/25 | Loss: 0.00146826
Iteration 6/25 | Loss: 0.00146826
Iteration 7/25 | Loss: 0.00146826
Iteration 8/25 | Loss: 0.00146826
Iteration 9/25 | Loss: 0.00146826
Iteration 10/25 | Loss: 0.00146826
Iteration 11/25 | Loss: 0.00146826
Iteration 12/25 | Loss: 0.00146826
Iteration 13/25 | Loss: 0.00146826
Iteration 14/25 | Loss: 0.00146826
Iteration 15/25 | Loss: 0.00146826
Iteration 16/25 | Loss: 0.00146826
Iteration 17/25 | Loss: 0.00146826
Iteration 18/25 | Loss: 0.00146826
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001468257512897253, 0.001468257512897253, 0.001468257512897253, 0.001468257512897253, 0.001468257512897253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001468257512897253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146826
Iteration 2/1000 | Loss: 0.00053784
Iteration 3/1000 | Loss: 0.00057035
Iteration 4/1000 | Loss: 0.00050789
Iteration 5/1000 | Loss: 0.00304960
Iteration 6/1000 | Loss: 0.00419781
Iteration 7/1000 | Loss: 0.00319577
Iteration 8/1000 | Loss: 0.00429590
Iteration 9/1000 | Loss: 0.00294242
Iteration 10/1000 | Loss: 0.00342708
Iteration 11/1000 | Loss: 0.00216959
Iteration 12/1000 | Loss: 0.00202071
Iteration 13/1000 | Loss: 0.00721749
Iteration 14/1000 | Loss: 0.00430946
Iteration 15/1000 | Loss: 0.00323396
Iteration 16/1000 | Loss: 0.00243980
Iteration 17/1000 | Loss: 0.00212216
Iteration 18/1000 | Loss: 0.00215404
Iteration 19/1000 | Loss: 0.00139701
Iteration 20/1000 | Loss: 0.00150208
Iteration 21/1000 | Loss: 0.00158379
Iteration 22/1000 | Loss: 0.00232375
Iteration 23/1000 | Loss: 0.00168016
Iteration 24/1000 | Loss: 0.00156245
Iteration 25/1000 | Loss: 0.00203091
Iteration 26/1000 | Loss: 0.00145761
Iteration 27/1000 | Loss: 0.00160792
Iteration 28/1000 | Loss: 0.00136034
Iteration 29/1000 | Loss: 0.00162993
Iteration 30/1000 | Loss: 0.00129989
Iteration 31/1000 | Loss: 0.00190286
Iteration 32/1000 | Loss: 0.00118984
Iteration 33/1000 | Loss: 0.00128498
Iteration 34/1000 | Loss: 0.00135729
Iteration 35/1000 | Loss: 0.00160684
Iteration 36/1000 | Loss: 0.00154820
Iteration 37/1000 | Loss: 0.00163809
Iteration 38/1000 | Loss: 0.00149297
Iteration 39/1000 | Loss: 0.00140154
Iteration 40/1000 | Loss: 0.00231016
Iteration 41/1000 | Loss: 0.00211282
Iteration 42/1000 | Loss: 0.00323041
Iteration 43/1000 | Loss: 0.00192288
Iteration 44/1000 | Loss: 0.00082111
Iteration 45/1000 | Loss: 0.00082120
Iteration 46/1000 | Loss: 0.00107630
Iteration 47/1000 | Loss: 0.00091156
Iteration 48/1000 | Loss: 0.00147652
Iteration 49/1000 | Loss: 0.00127787
Iteration 50/1000 | Loss: 0.00092228
Iteration 51/1000 | Loss: 0.00125592
Iteration 52/1000 | Loss: 0.00203241
Iteration 53/1000 | Loss: 0.00076831
Iteration 54/1000 | Loss: 0.00079541
Iteration 55/1000 | Loss: 0.00043059
Iteration 56/1000 | Loss: 0.00097527
Iteration 57/1000 | Loss: 0.00102832
Iteration 58/1000 | Loss: 0.00123139
Iteration 59/1000 | Loss: 0.00073576
Iteration 60/1000 | Loss: 0.00110828
Iteration 61/1000 | Loss: 0.00196127
Iteration 62/1000 | Loss: 0.00129260
Iteration 63/1000 | Loss: 0.00218213
Iteration 64/1000 | Loss: 0.00120972
Iteration 65/1000 | Loss: 0.00090459
Iteration 66/1000 | Loss: 0.00138341
Iteration 67/1000 | Loss: 0.00135801
Iteration 68/1000 | Loss: 0.00082166
Iteration 69/1000 | Loss: 0.00068713
Iteration 70/1000 | Loss: 0.00093866
Iteration 71/1000 | Loss: 0.00063342
Iteration 72/1000 | Loss: 0.00068738
Iteration 73/1000 | Loss: 0.00065448
Iteration 74/1000 | Loss: 0.00082248
Iteration 75/1000 | Loss: 0.00096819
Iteration 76/1000 | Loss: 0.00034914
Iteration 77/1000 | Loss: 0.00040365
Iteration 78/1000 | Loss: 0.00063970
Iteration 79/1000 | Loss: 0.00092698
Iteration 80/1000 | Loss: 0.00102687
Iteration 81/1000 | Loss: 0.00043253
Iteration 82/1000 | Loss: 0.00056441
Iteration 83/1000 | Loss: 0.00110903
Iteration 84/1000 | Loss: 0.00098146
Iteration 85/1000 | Loss: 0.00089904
Iteration 86/1000 | Loss: 0.00070613
Iteration 87/1000 | Loss: 0.00033374
Iteration 88/1000 | Loss: 0.00058443
Iteration 89/1000 | Loss: 0.00028193
Iteration 90/1000 | Loss: 0.00036999
Iteration 91/1000 | Loss: 0.00027416
Iteration 92/1000 | Loss: 0.00069571
Iteration 93/1000 | Loss: 0.00051811
Iteration 94/1000 | Loss: 0.00047513
Iteration 95/1000 | Loss: 0.00044444
Iteration 96/1000 | Loss: 0.00045544
Iteration 97/1000 | Loss: 0.00019733
Iteration 98/1000 | Loss: 0.00066903
Iteration 99/1000 | Loss: 0.00052963
Iteration 100/1000 | Loss: 0.00040917
Iteration 101/1000 | Loss: 0.00055666
Iteration 102/1000 | Loss: 0.00042050
Iteration 103/1000 | Loss: 0.00046606
Iteration 104/1000 | Loss: 0.00026419
Iteration 105/1000 | Loss: 0.00082536
Iteration 106/1000 | Loss: 0.00047322
Iteration 107/1000 | Loss: 0.00032101
Iteration 108/1000 | Loss: 0.00012497
Iteration 109/1000 | Loss: 0.00010517
Iteration 110/1000 | Loss: 0.00052740
Iteration 111/1000 | Loss: 0.00070112
Iteration 112/1000 | Loss: 0.00080754
Iteration 113/1000 | Loss: 0.00074957
Iteration 114/1000 | Loss: 0.00063894
Iteration 115/1000 | Loss: 0.00071571
Iteration 116/1000 | Loss: 0.00079196
Iteration 117/1000 | Loss: 0.00105903
Iteration 118/1000 | Loss: 0.00021985
Iteration 119/1000 | Loss: 0.00060423
Iteration 120/1000 | Loss: 0.00093604
Iteration 121/1000 | Loss: 0.00036656
Iteration 122/1000 | Loss: 0.00067459
Iteration 123/1000 | Loss: 0.00087578
Iteration 124/1000 | Loss: 0.00108085
Iteration 125/1000 | Loss: 0.00098229
Iteration 126/1000 | Loss: 0.00110373
Iteration 127/1000 | Loss: 0.00126543
Iteration 128/1000 | Loss: 0.00142613
Iteration 129/1000 | Loss: 0.00080480
Iteration 130/1000 | Loss: 0.00129088
Iteration 131/1000 | Loss: 0.00093220
Iteration 132/1000 | Loss: 0.00163033
Iteration 133/1000 | Loss: 0.00077957
Iteration 134/1000 | Loss: 0.00044841
Iteration 135/1000 | Loss: 0.00055395
Iteration 136/1000 | Loss: 0.00081552
Iteration 137/1000 | Loss: 0.00087599
Iteration 138/1000 | Loss: 0.00053316
Iteration 139/1000 | Loss: 0.00082594
Iteration 140/1000 | Loss: 0.00056633
Iteration 141/1000 | Loss: 0.00017986
Iteration 142/1000 | Loss: 0.00027023
Iteration 143/1000 | Loss: 0.00059181
Iteration 144/1000 | Loss: 0.00038530
Iteration 145/1000 | Loss: 0.00045368
Iteration 146/1000 | Loss: 0.00045555
Iteration 147/1000 | Loss: 0.00063449
Iteration 148/1000 | Loss: 0.00061322
Iteration 149/1000 | Loss: 0.00091796
Iteration 150/1000 | Loss: 0.00067526
Iteration 151/1000 | Loss: 0.00096963
Iteration 152/1000 | Loss: 0.00068050
Iteration 153/1000 | Loss: 0.00104222
Iteration 154/1000 | Loss: 0.00066173
Iteration 155/1000 | Loss: 0.00210957
Iteration 156/1000 | Loss: 0.00046436
Iteration 157/1000 | Loss: 0.00027026
Iteration 158/1000 | Loss: 0.00080644
Iteration 159/1000 | Loss: 0.00026234
Iteration 160/1000 | Loss: 0.00017327
Iteration 161/1000 | Loss: 0.00039951
Iteration 162/1000 | Loss: 0.00044070
Iteration 163/1000 | Loss: 0.00042905
Iteration 164/1000 | Loss: 0.00046044
Iteration 165/1000 | Loss: 0.00061798
Iteration 166/1000 | Loss: 0.00046231
Iteration 167/1000 | Loss: 0.00045860
Iteration 168/1000 | Loss: 0.00036659
Iteration 169/1000 | Loss: 0.00061668
Iteration 170/1000 | Loss: 0.00060108
Iteration 171/1000 | Loss: 0.00060225
Iteration 172/1000 | Loss: 0.00016015
Iteration 173/1000 | Loss: 0.00015829
Iteration 174/1000 | Loss: 0.00018988
Iteration 175/1000 | Loss: 0.00024002
Iteration 176/1000 | Loss: 0.00058882
Iteration 177/1000 | Loss: 0.00045292
Iteration 178/1000 | Loss: 0.00062521
Iteration 179/1000 | Loss: 0.00071634
Iteration 180/1000 | Loss: 0.00073683
Iteration 181/1000 | Loss: 0.00071934
Iteration 182/1000 | Loss: 0.00073391
Iteration 183/1000 | Loss: 0.00088876
Iteration 184/1000 | Loss: 0.00066176
Iteration 185/1000 | Loss: 0.00063168
Iteration 186/1000 | Loss: 0.00071789
Iteration 187/1000 | Loss: 0.00028352
Iteration 188/1000 | Loss: 0.00006508
Iteration 189/1000 | Loss: 0.00045496
Iteration 190/1000 | Loss: 0.00044474
Iteration 191/1000 | Loss: 0.00029809
Iteration 192/1000 | Loss: 0.00038451
Iteration 193/1000 | Loss: 0.00038160
Iteration 194/1000 | Loss: 0.00080867
Iteration 195/1000 | Loss: 0.00051057
Iteration 196/1000 | Loss: 0.00065594
Iteration 197/1000 | Loss: 0.00127784
Iteration 198/1000 | Loss: 0.00078531
Iteration 199/1000 | Loss: 0.00098274
Iteration 200/1000 | Loss: 0.00106365
Iteration 201/1000 | Loss: 0.00006264
Iteration 202/1000 | Loss: 0.00091311
Iteration 203/1000 | Loss: 0.00004559
Iteration 204/1000 | Loss: 0.00003802
Iteration 205/1000 | Loss: 0.00003350
Iteration 206/1000 | Loss: 0.00027614
Iteration 207/1000 | Loss: 0.00027027
Iteration 208/1000 | Loss: 0.00033013
Iteration 209/1000 | Loss: 0.00030002
Iteration 210/1000 | Loss: 0.00041684
Iteration 211/1000 | Loss: 0.00031431
Iteration 212/1000 | Loss: 0.00050909
Iteration 213/1000 | Loss: 0.00059201
Iteration 214/1000 | Loss: 0.00003311
Iteration 215/1000 | Loss: 0.00003011
Iteration 216/1000 | Loss: 0.00002925
Iteration 217/1000 | Loss: 0.00030193
Iteration 218/1000 | Loss: 0.00019152
Iteration 219/1000 | Loss: 0.00003091
Iteration 220/1000 | Loss: 0.00023163
Iteration 221/1000 | Loss: 0.00017393
Iteration 222/1000 | Loss: 0.00025107
Iteration 223/1000 | Loss: 0.00032174
Iteration 224/1000 | Loss: 0.00044042
Iteration 225/1000 | Loss: 0.00033872
Iteration 226/1000 | Loss: 0.00037890
Iteration 227/1000 | Loss: 0.00035979
Iteration 228/1000 | Loss: 0.00011472
Iteration 229/1000 | Loss: 0.00035505
Iteration 230/1000 | Loss: 0.00035844
Iteration 231/1000 | Loss: 0.00012140
Iteration 232/1000 | Loss: 0.00049795
Iteration 233/1000 | Loss: 0.00035309
Iteration 234/1000 | Loss: 0.00018162
Iteration 235/1000 | Loss: 0.00018069
Iteration 236/1000 | Loss: 0.00033587
Iteration 237/1000 | Loss: 0.00028346
Iteration 238/1000 | Loss: 0.00037021
Iteration 239/1000 | Loss: 0.00037225
Iteration 240/1000 | Loss: 0.00022606
Iteration 241/1000 | Loss: 0.00027545
Iteration 242/1000 | Loss: 0.00004214
Iteration 243/1000 | Loss: 0.00003635
Iteration 244/1000 | Loss: 0.00003400
Iteration 245/1000 | Loss: 0.00003265
Iteration 246/1000 | Loss: 0.00003100
Iteration 247/1000 | Loss: 0.00002955
Iteration 248/1000 | Loss: 0.00002809
Iteration 249/1000 | Loss: 0.00002697
Iteration 250/1000 | Loss: 0.00002621
Iteration 251/1000 | Loss: 0.00002582
Iteration 252/1000 | Loss: 0.00002543
Iteration 253/1000 | Loss: 0.00002515
Iteration 254/1000 | Loss: 0.00002491
Iteration 255/1000 | Loss: 0.00002472
Iteration 256/1000 | Loss: 0.00028983
Iteration 257/1000 | Loss: 0.00015057
Iteration 258/1000 | Loss: 0.00016324
Iteration 259/1000 | Loss: 0.00002684
Iteration 260/1000 | Loss: 0.00002419
Iteration 261/1000 | Loss: 0.00002312
Iteration 262/1000 | Loss: 0.00002277
Iteration 263/1000 | Loss: 0.00002250
Iteration 264/1000 | Loss: 0.00002248
Iteration 265/1000 | Loss: 0.00002227
Iteration 266/1000 | Loss: 0.00002222
Iteration 267/1000 | Loss: 0.00002221
Iteration 268/1000 | Loss: 0.00002216
Iteration 269/1000 | Loss: 0.00002216
Iteration 270/1000 | Loss: 0.00002214
Iteration 271/1000 | Loss: 0.00002214
Iteration 272/1000 | Loss: 0.00002214
Iteration 273/1000 | Loss: 0.00002213
Iteration 274/1000 | Loss: 0.00002213
Iteration 275/1000 | Loss: 0.00002213
Iteration 276/1000 | Loss: 0.00002212
Iteration 277/1000 | Loss: 0.00002212
Iteration 278/1000 | Loss: 0.00002211
Iteration 279/1000 | Loss: 0.00002211
Iteration 280/1000 | Loss: 0.00002211
Iteration 281/1000 | Loss: 0.00002204
Iteration 282/1000 | Loss: 0.00002204
Iteration 283/1000 | Loss: 0.00002204
Iteration 284/1000 | Loss: 0.00002204
Iteration 285/1000 | Loss: 0.00002204
Iteration 286/1000 | Loss: 0.00002204
Iteration 287/1000 | Loss: 0.00002204
Iteration 288/1000 | Loss: 0.00002204
Iteration 289/1000 | Loss: 0.00002204
Iteration 290/1000 | Loss: 0.00002204
Iteration 291/1000 | Loss: 0.00002204
Iteration 292/1000 | Loss: 0.00002204
Iteration 293/1000 | Loss: 0.00002203
Iteration 294/1000 | Loss: 0.00002203
Iteration 295/1000 | Loss: 0.00002203
Iteration 296/1000 | Loss: 0.00002203
Iteration 297/1000 | Loss: 0.00002202
Iteration 298/1000 | Loss: 0.00002202
Iteration 299/1000 | Loss: 0.00002202
Iteration 300/1000 | Loss: 0.00002201
Iteration 301/1000 | Loss: 0.00002201
Iteration 302/1000 | Loss: 0.00002201
Iteration 303/1000 | Loss: 0.00002201
Iteration 304/1000 | Loss: 0.00002201
Iteration 305/1000 | Loss: 0.00002200
Iteration 306/1000 | Loss: 0.00002200
Iteration 307/1000 | Loss: 0.00002200
Iteration 308/1000 | Loss: 0.00002200
Iteration 309/1000 | Loss: 0.00002200
Iteration 310/1000 | Loss: 0.00002200
Iteration 311/1000 | Loss: 0.00002200
Iteration 312/1000 | Loss: 0.00002199
Iteration 313/1000 | Loss: 0.00002199
Iteration 314/1000 | Loss: 0.00002199
Iteration 315/1000 | Loss: 0.00002199
Iteration 316/1000 | Loss: 0.00002199
Iteration 317/1000 | Loss: 0.00002199
Iteration 318/1000 | Loss: 0.00002199
Iteration 319/1000 | Loss: 0.00002199
Iteration 320/1000 | Loss: 0.00002199
Iteration 321/1000 | Loss: 0.00002199
Iteration 322/1000 | Loss: 0.00002199
Iteration 323/1000 | Loss: 0.00002199
Iteration 324/1000 | Loss: 0.00002199
Iteration 325/1000 | Loss: 0.00002199
Iteration 326/1000 | Loss: 0.00002199
Iteration 327/1000 | Loss: 0.00002199
Iteration 328/1000 | Loss: 0.00002199
Iteration 329/1000 | Loss: 0.00002199
Iteration 330/1000 | Loss: 0.00002199
Iteration 331/1000 | Loss: 0.00002199
Iteration 332/1000 | Loss: 0.00002199
Iteration 333/1000 | Loss: 0.00002199
Iteration 334/1000 | Loss: 0.00002199
Iteration 335/1000 | Loss: 0.00002199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [2.1985933926771395e-05, 2.1985933926771395e-05, 2.1985933926771395e-05, 2.1985933926771395e-05, 2.1985933926771395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1985933926771395e-05

Optimization complete. Final v2v error: 3.9058799743652344 mm

Highest mean error: 6.487213611602783 mm for frame 238

Lowest mean error: 3.4473578929901123 mm for frame 215

Saving results

Total time: 486.8137722015381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754648
Iteration 2/25 | Loss: 0.00145200
Iteration 3/25 | Loss: 0.00102408
Iteration 4/25 | Loss: 0.00087315
Iteration 5/25 | Loss: 0.00085071
Iteration 6/25 | Loss: 0.00082676
Iteration 7/25 | Loss: 0.00080694
Iteration 8/25 | Loss: 0.00076430
Iteration 9/25 | Loss: 0.00073879
Iteration 10/25 | Loss: 0.00073379
Iteration 11/25 | Loss: 0.00073711
Iteration 12/25 | Loss: 0.00072710
Iteration 13/25 | Loss: 0.00071939
Iteration 14/25 | Loss: 0.00072212
Iteration 15/25 | Loss: 0.00071832
Iteration 16/25 | Loss: 0.00071291
Iteration 17/25 | Loss: 0.00071073
Iteration 18/25 | Loss: 0.00070865
Iteration 19/25 | Loss: 0.00070780
Iteration 20/25 | Loss: 0.00070703
Iteration 21/25 | Loss: 0.00070689
Iteration 22/25 | Loss: 0.00070688
Iteration 23/25 | Loss: 0.00070681
Iteration 24/25 | Loss: 0.00070672
Iteration 25/25 | Loss: 0.00070664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19699669
Iteration 2/25 | Loss: 0.00051966
Iteration 3/25 | Loss: 0.00051961
Iteration 4/25 | Loss: 0.00051961
Iteration 5/25 | Loss: 0.00051961
Iteration 6/25 | Loss: 0.00051961
Iteration 7/25 | Loss: 0.00051961
Iteration 8/25 | Loss: 0.00051961
Iteration 9/25 | Loss: 0.00051961
Iteration 10/25 | Loss: 0.00051961
Iteration 11/25 | Loss: 0.00051961
Iteration 12/25 | Loss: 0.00051961
Iteration 13/25 | Loss: 0.00051961
Iteration 14/25 | Loss: 0.00051961
Iteration 15/25 | Loss: 0.00051961
Iteration 16/25 | Loss: 0.00051961
Iteration 17/25 | Loss: 0.00051961
Iteration 18/25 | Loss: 0.00051961
Iteration 19/25 | Loss: 0.00051961
Iteration 20/25 | Loss: 0.00051961
Iteration 21/25 | Loss: 0.00051961
Iteration 22/25 | Loss: 0.00051961
Iteration 23/25 | Loss: 0.00051961
Iteration 24/25 | Loss: 0.00051961
Iteration 25/25 | Loss: 0.00051961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005196077981963754, 0.0005196077981963754, 0.0005196077981963754, 0.0005196077981963754, 0.0005196077981963754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005196077981963754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051961
Iteration 2/1000 | Loss: 0.00013059
Iteration 3/1000 | Loss: 0.00007646
Iteration 4/1000 | Loss: 0.00005128
Iteration 5/1000 | Loss: 0.00003525
Iteration 6/1000 | Loss: 0.00011564
Iteration 7/1000 | Loss: 0.00008576
Iteration 8/1000 | Loss: 0.00002898
Iteration 9/1000 | Loss: 0.00002657
Iteration 10/1000 | Loss: 0.00002517
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002320
Iteration 13/1000 | Loss: 0.00002263
Iteration 14/1000 | Loss: 0.00022673
Iteration 15/1000 | Loss: 0.00019449
Iteration 16/1000 | Loss: 0.00004285
Iteration 17/1000 | Loss: 0.00002755
Iteration 18/1000 | Loss: 0.00002417
Iteration 19/1000 | Loss: 0.00002210
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001823
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001780
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001780
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001779
Iteration 48/1000 | Loss: 0.00001779
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001772
Iteration 75/1000 | Loss: 0.00001772
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001771
Iteration 79/1000 | Loss: 0.00001771
Iteration 80/1000 | Loss: 0.00001771
Iteration 81/1000 | Loss: 0.00001771
Iteration 82/1000 | Loss: 0.00001770
Iteration 83/1000 | Loss: 0.00001770
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001769
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001766
Iteration 96/1000 | Loss: 0.00001766
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001765
Iteration 99/1000 | Loss: 0.00001765
Iteration 100/1000 | Loss: 0.00001765
Iteration 101/1000 | Loss: 0.00001764
Iteration 102/1000 | Loss: 0.00001764
Iteration 103/1000 | Loss: 0.00001764
Iteration 104/1000 | Loss: 0.00001764
Iteration 105/1000 | Loss: 0.00001763
Iteration 106/1000 | Loss: 0.00001763
Iteration 107/1000 | Loss: 0.00001763
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001762
Iteration 112/1000 | Loss: 0.00001762
Iteration 113/1000 | Loss: 0.00001762
Iteration 114/1000 | Loss: 0.00001762
Iteration 115/1000 | Loss: 0.00001762
Iteration 116/1000 | Loss: 0.00001761
Iteration 117/1000 | Loss: 0.00001761
Iteration 118/1000 | Loss: 0.00001761
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001761
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001760
Iteration 123/1000 | Loss: 0.00001760
Iteration 124/1000 | Loss: 0.00001760
Iteration 125/1000 | Loss: 0.00001760
Iteration 126/1000 | Loss: 0.00001760
Iteration 127/1000 | Loss: 0.00001759
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001758
Iteration 132/1000 | Loss: 0.00001758
Iteration 133/1000 | Loss: 0.00001758
Iteration 134/1000 | Loss: 0.00001758
Iteration 135/1000 | Loss: 0.00001758
Iteration 136/1000 | Loss: 0.00001758
Iteration 137/1000 | Loss: 0.00001758
Iteration 138/1000 | Loss: 0.00001758
Iteration 139/1000 | Loss: 0.00001757
Iteration 140/1000 | Loss: 0.00001757
Iteration 141/1000 | Loss: 0.00001757
Iteration 142/1000 | Loss: 0.00001757
Iteration 143/1000 | Loss: 0.00001757
Iteration 144/1000 | Loss: 0.00001757
Iteration 145/1000 | Loss: 0.00001757
Iteration 146/1000 | Loss: 0.00001757
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001757
Iteration 149/1000 | Loss: 0.00001757
Iteration 150/1000 | Loss: 0.00001757
Iteration 151/1000 | Loss: 0.00001757
Iteration 152/1000 | Loss: 0.00001757
Iteration 153/1000 | Loss: 0.00001757
Iteration 154/1000 | Loss: 0.00001756
Iteration 155/1000 | Loss: 0.00001756
Iteration 156/1000 | Loss: 0.00001756
Iteration 157/1000 | Loss: 0.00001756
Iteration 158/1000 | Loss: 0.00001756
Iteration 159/1000 | Loss: 0.00001756
Iteration 160/1000 | Loss: 0.00001756
Iteration 161/1000 | Loss: 0.00001756
Iteration 162/1000 | Loss: 0.00001756
Iteration 163/1000 | Loss: 0.00001756
Iteration 164/1000 | Loss: 0.00001756
Iteration 165/1000 | Loss: 0.00001756
Iteration 166/1000 | Loss: 0.00001756
Iteration 167/1000 | Loss: 0.00001755
Iteration 168/1000 | Loss: 0.00001755
Iteration 169/1000 | Loss: 0.00001755
Iteration 170/1000 | Loss: 0.00001755
Iteration 171/1000 | Loss: 0.00001755
Iteration 172/1000 | Loss: 0.00001755
Iteration 173/1000 | Loss: 0.00001755
Iteration 174/1000 | Loss: 0.00001754
Iteration 175/1000 | Loss: 0.00001754
Iteration 176/1000 | Loss: 0.00001754
Iteration 177/1000 | Loss: 0.00001754
Iteration 178/1000 | Loss: 0.00001754
Iteration 179/1000 | Loss: 0.00001754
Iteration 180/1000 | Loss: 0.00001754
Iteration 181/1000 | Loss: 0.00001754
Iteration 182/1000 | Loss: 0.00001754
Iteration 183/1000 | Loss: 0.00001753
Iteration 184/1000 | Loss: 0.00001753
Iteration 185/1000 | Loss: 0.00001753
Iteration 186/1000 | Loss: 0.00001753
Iteration 187/1000 | Loss: 0.00001753
Iteration 188/1000 | Loss: 0.00001753
Iteration 189/1000 | Loss: 0.00001753
Iteration 190/1000 | Loss: 0.00001752
Iteration 191/1000 | Loss: 0.00001752
Iteration 192/1000 | Loss: 0.00001752
Iteration 193/1000 | Loss: 0.00001752
Iteration 194/1000 | Loss: 0.00001752
Iteration 195/1000 | Loss: 0.00001752
Iteration 196/1000 | Loss: 0.00001752
Iteration 197/1000 | Loss: 0.00001751
Iteration 198/1000 | Loss: 0.00001751
Iteration 199/1000 | Loss: 0.00001751
Iteration 200/1000 | Loss: 0.00001751
Iteration 201/1000 | Loss: 0.00001751
Iteration 202/1000 | Loss: 0.00001751
Iteration 203/1000 | Loss: 0.00001751
Iteration 204/1000 | Loss: 0.00001751
Iteration 205/1000 | Loss: 0.00001751
Iteration 206/1000 | Loss: 0.00001750
Iteration 207/1000 | Loss: 0.00001750
Iteration 208/1000 | Loss: 0.00001750
Iteration 209/1000 | Loss: 0.00001750
Iteration 210/1000 | Loss: 0.00001750
Iteration 211/1000 | Loss: 0.00001750
Iteration 212/1000 | Loss: 0.00001750
Iteration 213/1000 | Loss: 0.00001750
Iteration 214/1000 | Loss: 0.00001750
Iteration 215/1000 | Loss: 0.00001750
Iteration 216/1000 | Loss: 0.00001750
Iteration 217/1000 | Loss: 0.00001750
Iteration 218/1000 | Loss: 0.00001750
Iteration 219/1000 | Loss: 0.00001750
Iteration 220/1000 | Loss: 0.00001750
Iteration 221/1000 | Loss: 0.00001750
Iteration 222/1000 | Loss: 0.00001750
Iteration 223/1000 | Loss: 0.00001750
Iteration 224/1000 | Loss: 0.00001750
Iteration 225/1000 | Loss: 0.00001750
Iteration 226/1000 | Loss: 0.00001750
Iteration 227/1000 | Loss: 0.00001750
Iteration 228/1000 | Loss: 0.00001750
Iteration 229/1000 | Loss: 0.00001750
Iteration 230/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.7497228327556513e-05, 1.7497228327556513e-05, 1.7497228327556513e-05, 1.7497228327556513e-05, 1.7497228327556513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7497228327556513e-05

Optimization complete. Final v2v error: 3.4903507232666016 mm

Highest mean error: 5.2541327476501465 mm for frame 104

Lowest mean error: 2.6454901695251465 mm for frame 146

Saving results

Total time: 107.11801624298096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885149
Iteration 2/25 | Loss: 0.00166944
Iteration 3/25 | Loss: 0.00110850
Iteration 4/25 | Loss: 0.00096823
Iteration 5/25 | Loss: 0.00092852
Iteration 6/25 | Loss: 0.00091789
Iteration 7/25 | Loss: 0.00091875
Iteration 8/25 | Loss: 0.00091271
Iteration 9/25 | Loss: 0.00091106
Iteration 10/25 | Loss: 0.00090955
Iteration 11/25 | Loss: 0.00090803
Iteration 12/25 | Loss: 0.00090582
Iteration 13/25 | Loss: 0.00090558
Iteration 14/25 | Loss: 0.00090556
Iteration 15/25 | Loss: 0.00090556
Iteration 16/25 | Loss: 0.00090556
Iteration 17/25 | Loss: 0.00090556
Iteration 18/25 | Loss: 0.00090556
Iteration 19/25 | Loss: 0.00090556
Iteration 20/25 | Loss: 0.00090556
Iteration 21/25 | Loss: 0.00090556
Iteration 22/25 | Loss: 0.00090556
Iteration 23/25 | Loss: 0.00090556
Iteration 24/25 | Loss: 0.00090556
Iteration 25/25 | Loss: 0.00090556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.33601141
Iteration 2/25 | Loss: 0.00038878
Iteration 3/25 | Loss: 0.00038878
Iteration 4/25 | Loss: 0.00038878
Iteration 5/25 | Loss: 0.00038878
Iteration 6/25 | Loss: 0.00038878
Iteration 7/25 | Loss: 0.00038878
Iteration 8/25 | Loss: 0.00038878
Iteration 9/25 | Loss: 0.00038878
Iteration 10/25 | Loss: 0.00038878
Iteration 11/25 | Loss: 0.00038878
Iteration 12/25 | Loss: 0.00038878
Iteration 13/25 | Loss: 0.00038878
Iteration 14/25 | Loss: 0.00038878
Iteration 15/25 | Loss: 0.00038878
Iteration 16/25 | Loss: 0.00038878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003887756320182234, 0.0003887756320182234, 0.0003887756320182234, 0.0003887756320182234, 0.0003887756320182234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003887756320182234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038878
Iteration 2/1000 | Loss: 0.00022926
Iteration 3/1000 | Loss: 0.00005614
Iteration 4/1000 | Loss: 0.00019225
Iteration 5/1000 | Loss: 0.00006136
Iteration 6/1000 | Loss: 0.00004680
Iteration 7/1000 | Loss: 0.00004317
Iteration 8/1000 | Loss: 0.00004169
Iteration 9/1000 | Loss: 0.00016058
Iteration 10/1000 | Loss: 0.00008233
Iteration 11/1000 | Loss: 0.00032378
Iteration 12/1000 | Loss: 0.00008040
Iteration 13/1000 | Loss: 0.00006746
Iteration 14/1000 | Loss: 0.00003930
Iteration 15/1000 | Loss: 0.00018984
Iteration 16/1000 | Loss: 0.00004854
Iteration 17/1000 | Loss: 0.00004124
Iteration 18/1000 | Loss: 0.00003859
Iteration 19/1000 | Loss: 0.00003799
Iteration 20/1000 | Loss: 0.00003761
Iteration 21/1000 | Loss: 0.00037465
Iteration 22/1000 | Loss: 0.00025085
Iteration 23/1000 | Loss: 0.00004219
Iteration 24/1000 | Loss: 0.00003789
Iteration 25/1000 | Loss: 0.00003650
Iteration 26/1000 | Loss: 0.00003591
Iteration 27/1000 | Loss: 0.00003548
Iteration 28/1000 | Loss: 0.00003523
Iteration 29/1000 | Loss: 0.00003503
Iteration 30/1000 | Loss: 0.00003493
Iteration 31/1000 | Loss: 0.00003491
Iteration 32/1000 | Loss: 0.00003491
Iteration 33/1000 | Loss: 0.00003490
Iteration 34/1000 | Loss: 0.00003489
Iteration 35/1000 | Loss: 0.00003489
Iteration 36/1000 | Loss: 0.00003488
Iteration 37/1000 | Loss: 0.00003488
Iteration 38/1000 | Loss: 0.00003488
Iteration 39/1000 | Loss: 0.00003487
Iteration 40/1000 | Loss: 0.00003487
Iteration 41/1000 | Loss: 0.00003487
Iteration 42/1000 | Loss: 0.00003486
Iteration 43/1000 | Loss: 0.00003486
Iteration 44/1000 | Loss: 0.00003486
Iteration 45/1000 | Loss: 0.00003486
Iteration 46/1000 | Loss: 0.00003486
Iteration 47/1000 | Loss: 0.00003486
Iteration 48/1000 | Loss: 0.00003486
Iteration 49/1000 | Loss: 0.00003485
Iteration 50/1000 | Loss: 0.00003485
Iteration 51/1000 | Loss: 0.00003485
Iteration 52/1000 | Loss: 0.00003485
Iteration 53/1000 | Loss: 0.00003485
Iteration 54/1000 | Loss: 0.00003485
Iteration 55/1000 | Loss: 0.00003485
Iteration 56/1000 | Loss: 0.00003485
Iteration 57/1000 | Loss: 0.00003484
Iteration 58/1000 | Loss: 0.00003484
Iteration 59/1000 | Loss: 0.00003484
Iteration 60/1000 | Loss: 0.00003483
Iteration 61/1000 | Loss: 0.00003483
Iteration 62/1000 | Loss: 0.00003483
Iteration 63/1000 | Loss: 0.00003483
Iteration 64/1000 | Loss: 0.00003482
Iteration 65/1000 | Loss: 0.00003482
Iteration 66/1000 | Loss: 0.00003482
Iteration 67/1000 | Loss: 0.00003482
Iteration 68/1000 | Loss: 0.00003482
Iteration 69/1000 | Loss: 0.00003482
Iteration 70/1000 | Loss: 0.00003482
Iteration 71/1000 | Loss: 0.00003482
Iteration 72/1000 | Loss: 0.00003482
Iteration 73/1000 | Loss: 0.00003482
Iteration 74/1000 | Loss: 0.00003482
Iteration 75/1000 | Loss: 0.00003482
Iteration 76/1000 | Loss: 0.00003482
Iteration 77/1000 | Loss: 0.00003482
Iteration 78/1000 | Loss: 0.00003482
Iteration 79/1000 | Loss: 0.00003482
Iteration 80/1000 | Loss: 0.00003482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [3.481625026324764e-05, 3.481625026324764e-05, 3.481625026324764e-05, 3.481625026324764e-05, 3.481625026324764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.481625026324764e-05

Optimization complete. Final v2v error: 4.8329854011535645 mm

Highest mean error: 5.835667133331299 mm for frame 223

Lowest mean error: 4.265209197998047 mm for frame 21

Saving results

Total time: 78.00219631195068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853163
Iteration 2/25 | Loss: 0.00101695
Iteration 3/25 | Loss: 0.00078516
Iteration 4/25 | Loss: 0.00073085
Iteration 5/25 | Loss: 0.00070263
Iteration 6/25 | Loss: 0.00069545
Iteration 7/25 | Loss: 0.00069348
Iteration 8/25 | Loss: 0.00069248
Iteration 9/25 | Loss: 0.00069243
Iteration 10/25 | Loss: 0.00069243
Iteration 11/25 | Loss: 0.00069243
Iteration 12/25 | Loss: 0.00069243
Iteration 13/25 | Loss: 0.00069243
Iteration 14/25 | Loss: 0.00069243
Iteration 15/25 | Loss: 0.00069243
Iteration 16/25 | Loss: 0.00069243
Iteration 17/25 | Loss: 0.00069243
Iteration 18/25 | Loss: 0.00069243
Iteration 19/25 | Loss: 0.00069243
Iteration 20/25 | Loss: 0.00069243
Iteration 21/25 | Loss: 0.00069243
Iteration 22/25 | Loss: 0.00069243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006924282643012702, 0.0006924282643012702, 0.0006924282643012702, 0.0006924282643012702, 0.0006924282643012702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006924282643012702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61803997
Iteration 2/25 | Loss: 0.00043955
Iteration 3/25 | Loss: 0.00043954
Iteration 4/25 | Loss: 0.00043954
Iteration 5/25 | Loss: 0.00043954
Iteration 6/25 | Loss: 0.00043954
Iteration 7/25 | Loss: 0.00043954
Iteration 8/25 | Loss: 0.00043954
Iteration 9/25 | Loss: 0.00043954
Iteration 10/25 | Loss: 0.00043954
Iteration 11/25 | Loss: 0.00043954
Iteration 12/25 | Loss: 0.00043954
Iteration 13/25 | Loss: 0.00043954
Iteration 14/25 | Loss: 0.00043954
Iteration 15/25 | Loss: 0.00043954
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004395428695715964, 0.0004395428695715964, 0.0004395428695715964, 0.0004395428695715964, 0.0004395428695715964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004395428695715964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043954
Iteration 2/1000 | Loss: 0.00004641
Iteration 3/1000 | Loss: 0.00003729
Iteration 4/1000 | Loss: 0.00003098
Iteration 5/1000 | Loss: 0.00002915
Iteration 6/1000 | Loss: 0.00002795
Iteration 7/1000 | Loss: 0.00002716
Iteration 8/1000 | Loss: 0.00002642
Iteration 9/1000 | Loss: 0.00002568
Iteration 10/1000 | Loss: 0.00002533
Iteration 11/1000 | Loss: 0.00002505
Iteration 12/1000 | Loss: 0.00002483
Iteration 13/1000 | Loss: 0.00002465
Iteration 14/1000 | Loss: 0.00002451
Iteration 15/1000 | Loss: 0.00002448
Iteration 16/1000 | Loss: 0.00002443
Iteration 17/1000 | Loss: 0.00002442
Iteration 18/1000 | Loss: 0.00002442
Iteration 19/1000 | Loss: 0.00002440
Iteration 20/1000 | Loss: 0.00002439
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002437
Iteration 23/1000 | Loss: 0.00002437
Iteration 24/1000 | Loss: 0.00002437
Iteration 25/1000 | Loss: 0.00002436
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002435
Iteration 30/1000 | Loss: 0.00002434
Iteration 31/1000 | Loss: 0.00002434
Iteration 32/1000 | Loss: 0.00002434
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002433
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002432
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002431
Iteration 41/1000 | Loss: 0.00002431
Iteration 42/1000 | Loss: 0.00002431
Iteration 43/1000 | Loss: 0.00002430
Iteration 44/1000 | Loss: 0.00002430
Iteration 45/1000 | Loss: 0.00002430
Iteration 46/1000 | Loss: 0.00002429
Iteration 47/1000 | Loss: 0.00002429
Iteration 48/1000 | Loss: 0.00002429
Iteration 49/1000 | Loss: 0.00002428
Iteration 50/1000 | Loss: 0.00002428
Iteration 51/1000 | Loss: 0.00002428
Iteration 52/1000 | Loss: 0.00002428
Iteration 53/1000 | Loss: 0.00002426
Iteration 54/1000 | Loss: 0.00002426
Iteration 55/1000 | Loss: 0.00002426
Iteration 56/1000 | Loss: 0.00002425
Iteration 57/1000 | Loss: 0.00002425
Iteration 58/1000 | Loss: 0.00002425
Iteration 59/1000 | Loss: 0.00002424
Iteration 60/1000 | Loss: 0.00002424
Iteration 61/1000 | Loss: 0.00002424
Iteration 62/1000 | Loss: 0.00002424
Iteration 63/1000 | Loss: 0.00002424
Iteration 64/1000 | Loss: 0.00002424
Iteration 65/1000 | Loss: 0.00002423
Iteration 66/1000 | Loss: 0.00002423
Iteration 67/1000 | Loss: 0.00002423
Iteration 68/1000 | Loss: 0.00002423
Iteration 69/1000 | Loss: 0.00002423
Iteration 70/1000 | Loss: 0.00002423
Iteration 71/1000 | Loss: 0.00002422
Iteration 72/1000 | Loss: 0.00002422
Iteration 73/1000 | Loss: 0.00002421
Iteration 74/1000 | Loss: 0.00002421
Iteration 75/1000 | Loss: 0.00002421
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00002420
Iteration 78/1000 | Loss: 0.00002420
Iteration 79/1000 | Loss: 0.00002420
Iteration 80/1000 | Loss: 0.00002419
Iteration 81/1000 | Loss: 0.00002419
Iteration 82/1000 | Loss: 0.00002419
Iteration 83/1000 | Loss: 0.00002419
Iteration 84/1000 | Loss: 0.00002419
Iteration 85/1000 | Loss: 0.00002419
Iteration 86/1000 | Loss: 0.00002419
Iteration 87/1000 | Loss: 0.00002419
Iteration 88/1000 | Loss: 0.00002419
Iteration 89/1000 | Loss: 0.00002419
Iteration 90/1000 | Loss: 0.00002418
Iteration 91/1000 | Loss: 0.00002418
Iteration 92/1000 | Loss: 0.00002418
Iteration 93/1000 | Loss: 0.00002418
Iteration 94/1000 | Loss: 0.00002418
Iteration 95/1000 | Loss: 0.00002418
Iteration 96/1000 | Loss: 0.00002418
Iteration 97/1000 | Loss: 0.00002418
Iteration 98/1000 | Loss: 0.00002418
Iteration 99/1000 | Loss: 0.00002418
Iteration 100/1000 | Loss: 0.00002417
Iteration 101/1000 | Loss: 0.00002417
Iteration 102/1000 | Loss: 0.00002417
Iteration 103/1000 | Loss: 0.00002417
Iteration 104/1000 | Loss: 0.00002417
Iteration 105/1000 | Loss: 0.00002417
Iteration 106/1000 | Loss: 0.00002417
Iteration 107/1000 | Loss: 0.00002416
Iteration 108/1000 | Loss: 0.00002416
Iteration 109/1000 | Loss: 0.00002416
Iteration 110/1000 | Loss: 0.00002416
Iteration 111/1000 | Loss: 0.00002416
Iteration 112/1000 | Loss: 0.00002415
Iteration 113/1000 | Loss: 0.00002415
Iteration 114/1000 | Loss: 0.00002415
Iteration 115/1000 | Loss: 0.00002415
Iteration 116/1000 | Loss: 0.00002415
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002415
Iteration 119/1000 | Loss: 0.00002415
Iteration 120/1000 | Loss: 0.00002415
Iteration 121/1000 | Loss: 0.00002415
Iteration 122/1000 | Loss: 0.00002415
Iteration 123/1000 | Loss: 0.00002415
Iteration 124/1000 | Loss: 0.00002415
Iteration 125/1000 | Loss: 0.00002415
Iteration 126/1000 | Loss: 0.00002414
Iteration 127/1000 | Loss: 0.00002414
Iteration 128/1000 | Loss: 0.00002414
Iteration 129/1000 | Loss: 0.00002414
Iteration 130/1000 | Loss: 0.00002414
Iteration 131/1000 | Loss: 0.00002414
Iteration 132/1000 | Loss: 0.00002414
Iteration 133/1000 | Loss: 0.00002414
Iteration 134/1000 | Loss: 0.00002414
Iteration 135/1000 | Loss: 0.00002414
Iteration 136/1000 | Loss: 0.00002414
Iteration 137/1000 | Loss: 0.00002414
Iteration 138/1000 | Loss: 0.00002413
Iteration 139/1000 | Loss: 0.00002413
Iteration 140/1000 | Loss: 0.00002413
Iteration 141/1000 | Loss: 0.00002413
Iteration 142/1000 | Loss: 0.00002413
Iteration 143/1000 | Loss: 0.00002413
Iteration 144/1000 | Loss: 0.00002413
Iteration 145/1000 | Loss: 0.00002412
Iteration 146/1000 | Loss: 0.00002412
Iteration 147/1000 | Loss: 0.00002412
Iteration 148/1000 | Loss: 0.00002411
Iteration 149/1000 | Loss: 0.00002411
Iteration 150/1000 | Loss: 0.00002411
Iteration 151/1000 | Loss: 0.00002411
Iteration 152/1000 | Loss: 0.00002411
Iteration 153/1000 | Loss: 0.00002411
Iteration 154/1000 | Loss: 0.00002411
Iteration 155/1000 | Loss: 0.00002411
Iteration 156/1000 | Loss: 0.00002411
Iteration 157/1000 | Loss: 0.00002411
Iteration 158/1000 | Loss: 0.00002411
Iteration 159/1000 | Loss: 0.00002411
Iteration 160/1000 | Loss: 0.00002411
Iteration 161/1000 | Loss: 0.00002411
Iteration 162/1000 | Loss: 0.00002411
Iteration 163/1000 | Loss: 0.00002411
Iteration 164/1000 | Loss: 0.00002410
Iteration 165/1000 | Loss: 0.00002410
Iteration 166/1000 | Loss: 0.00002410
Iteration 167/1000 | Loss: 0.00002410
Iteration 168/1000 | Loss: 0.00002410
Iteration 169/1000 | Loss: 0.00002410
Iteration 170/1000 | Loss: 0.00002410
Iteration 171/1000 | Loss: 0.00002410
Iteration 172/1000 | Loss: 0.00002409
Iteration 173/1000 | Loss: 0.00002409
Iteration 174/1000 | Loss: 0.00002409
Iteration 175/1000 | Loss: 0.00002409
Iteration 176/1000 | Loss: 0.00002409
Iteration 177/1000 | Loss: 0.00002409
Iteration 178/1000 | Loss: 0.00002409
Iteration 179/1000 | Loss: 0.00002409
Iteration 180/1000 | Loss: 0.00002409
Iteration 181/1000 | Loss: 0.00002409
Iteration 182/1000 | Loss: 0.00002408
Iteration 183/1000 | Loss: 0.00002408
Iteration 184/1000 | Loss: 0.00002408
Iteration 185/1000 | Loss: 0.00002408
Iteration 186/1000 | Loss: 0.00002408
Iteration 187/1000 | Loss: 0.00002408
Iteration 188/1000 | Loss: 0.00002408
Iteration 189/1000 | Loss: 0.00002408
Iteration 190/1000 | Loss: 0.00002408
Iteration 191/1000 | Loss: 0.00002408
Iteration 192/1000 | Loss: 0.00002408
Iteration 193/1000 | Loss: 0.00002408
Iteration 194/1000 | Loss: 0.00002408
Iteration 195/1000 | Loss: 0.00002407
Iteration 196/1000 | Loss: 0.00002407
Iteration 197/1000 | Loss: 0.00002407
Iteration 198/1000 | Loss: 0.00002407
Iteration 199/1000 | Loss: 0.00002407
Iteration 200/1000 | Loss: 0.00002407
Iteration 201/1000 | Loss: 0.00002407
Iteration 202/1000 | Loss: 0.00002407
Iteration 203/1000 | Loss: 0.00002407
Iteration 204/1000 | Loss: 0.00002407
Iteration 205/1000 | Loss: 0.00002407
Iteration 206/1000 | Loss: 0.00002407
Iteration 207/1000 | Loss: 0.00002407
Iteration 208/1000 | Loss: 0.00002407
Iteration 209/1000 | Loss: 0.00002407
Iteration 210/1000 | Loss: 0.00002407
Iteration 211/1000 | Loss: 0.00002407
Iteration 212/1000 | Loss: 0.00002407
Iteration 213/1000 | Loss: 0.00002407
Iteration 214/1000 | Loss: 0.00002407
Iteration 215/1000 | Loss: 0.00002407
Iteration 216/1000 | Loss: 0.00002407
Iteration 217/1000 | Loss: 0.00002407
Iteration 218/1000 | Loss: 0.00002407
Iteration 219/1000 | Loss: 0.00002407
Iteration 220/1000 | Loss: 0.00002407
Iteration 221/1000 | Loss: 0.00002407
Iteration 222/1000 | Loss: 0.00002407
Iteration 223/1000 | Loss: 0.00002407
Iteration 224/1000 | Loss: 0.00002407
Iteration 225/1000 | Loss: 0.00002407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.4069397113635205e-05, 2.4069397113635205e-05, 2.4069397113635205e-05, 2.4069397113635205e-05, 2.4069397113635205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4069397113635205e-05

Optimization complete. Final v2v error: 4.027374744415283 mm

Highest mean error: 6.208078861236572 mm for frame 124

Lowest mean error: 2.884094715118408 mm for frame 1

Saving results

Total time: 47.29126715660095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855058
Iteration 2/25 | Loss: 0.00083681
Iteration 3/25 | Loss: 0.00064410
Iteration 4/25 | Loss: 0.00060993
Iteration 5/25 | Loss: 0.00059885
Iteration 6/25 | Loss: 0.00059651
Iteration 7/25 | Loss: 0.00059587
Iteration 8/25 | Loss: 0.00059585
Iteration 9/25 | Loss: 0.00059585
Iteration 10/25 | Loss: 0.00059585
Iteration 11/25 | Loss: 0.00059585
Iteration 12/25 | Loss: 0.00059585
Iteration 13/25 | Loss: 0.00059585
Iteration 14/25 | Loss: 0.00059585
Iteration 15/25 | Loss: 0.00059585
Iteration 16/25 | Loss: 0.00059585
Iteration 17/25 | Loss: 0.00059585
Iteration 18/25 | Loss: 0.00059585
Iteration 19/25 | Loss: 0.00059585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005958512192592025, 0.0005958512192592025, 0.0005958512192592025, 0.0005958512192592025, 0.0005958512192592025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005958512192592025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28586340
Iteration 2/25 | Loss: 0.00025893
Iteration 3/25 | Loss: 0.00025892
Iteration 4/25 | Loss: 0.00025892
Iteration 5/25 | Loss: 0.00025892
Iteration 6/25 | Loss: 0.00025892
Iteration 7/25 | Loss: 0.00025892
Iteration 8/25 | Loss: 0.00025892
Iteration 9/25 | Loss: 0.00025892
Iteration 10/25 | Loss: 0.00025892
Iteration 11/25 | Loss: 0.00025892
Iteration 12/25 | Loss: 0.00025892
Iteration 13/25 | Loss: 0.00025892
Iteration 14/25 | Loss: 0.00025892
Iteration 15/25 | Loss: 0.00025892
Iteration 16/25 | Loss: 0.00025892
Iteration 17/25 | Loss: 0.00025892
Iteration 18/25 | Loss: 0.00025892
Iteration 19/25 | Loss: 0.00025892
Iteration 20/25 | Loss: 0.00025892
Iteration 21/25 | Loss: 0.00025892
Iteration 22/25 | Loss: 0.00025892
Iteration 23/25 | Loss: 0.00025892
Iteration 24/25 | Loss: 0.00025892
Iteration 25/25 | Loss: 0.00025892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00025892150006257, 0.00025892150006257, 0.00025892150006257, 0.00025892150006257, 0.00025892150006257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025892150006257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025892
Iteration 2/1000 | Loss: 0.00002387
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001349
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001338
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001337
Iteration 28/1000 | Loss: 0.00001337
Iteration 29/1000 | Loss: 0.00001336
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001334
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001324
Iteration 77/1000 | Loss: 0.00001324
Iteration 78/1000 | Loss: 0.00001324
Iteration 79/1000 | Loss: 0.00001324
Iteration 80/1000 | Loss: 0.00001324
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001324
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001323
Iteration 88/1000 | Loss: 0.00001323
Iteration 89/1000 | Loss: 0.00001323
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001318
Iteration 124/1000 | Loss: 0.00001318
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001315
Iteration 134/1000 | Loss: 0.00001315
Iteration 135/1000 | Loss: 0.00001315
Iteration 136/1000 | Loss: 0.00001314
Iteration 137/1000 | Loss: 0.00001314
Iteration 138/1000 | Loss: 0.00001314
Iteration 139/1000 | Loss: 0.00001314
Iteration 140/1000 | Loss: 0.00001314
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001313
Iteration 145/1000 | Loss: 0.00001313
Iteration 146/1000 | Loss: 0.00001313
Iteration 147/1000 | Loss: 0.00001313
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001312
Iteration 153/1000 | Loss: 0.00001312
Iteration 154/1000 | Loss: 0.00001312
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001311
Iteration 160/1000 | Loss: 0.00001311
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001311
Iteration 171/1000 | Loss: 0.00001311
Iteration 172/1000 | Loss: 0.00001311
Iteration 173/1000 | Loss: 0.00001311
Iteration 174/1000 | Loss: 0.00001311
Iteration 175/1000 | Loss: 0.00001311
Iteration 176/1000 | Loss: 0.00001311
Iteration 177/1000 | Loss: 0.00001311
Iteration 178/1000 | Loss: 0.00001311
Iteration 179/1000 | Loss: 0.00001311
Iteration 180/1000 | Loss: 0.00001310
Iteration 181/1000 | Loss: 0.00001310
Iteration 182/1000 | Loss: 0.00001310
Iteration 183/1000 | Loss: 0.00001310
Iteration 184/1000 | Loss: 0.00001310
Iteration 185/1000 | Loss: 0.00001310
Iteration 186/1000 | Loss: 0.00001310
Iteration 187/1000 | Loss: 0.00001310
Iteration 188/1000 | Loss: 0.00001310
Iteration 189/1000 | Loss: 0.00001310
Iteration 190/1000 | Loss: 0.00001310
Iteration 191/1000 | Loss: 0.00001310
Iteration 192/1000 | Loss: 0.00001310
Iteration 193/1000 | Loss: 0.00001310
Iteration 194/1000 | Loss: 0.00001310
Iteration 195/1000 | Loss: 0.00001310
Iteration 196/1000 | Loss: 0.00001310
Iteration 197/1000 | Loss: 0.00001310
Iteration 198/1000 | Loss: 0.00001310
Iteration 199/1000 | Loss: 0.00001310
Iteration 200/1000 | Loss: 0.00001310
Iteration 201/1000 | Loss: 0.00001310
Iteration 202/1000 | Loss: 0.00001310
Iteration 203/1000 | Loss: 0.00001310
Iteration 204/1000 | Loss: 0.00001310
Iteration 205/1000 | Loss: 0.00001310
Iteration 206/1000 | Loss: 0.00001310
Iteration 207/1000 | Loss: 0.00001310
Iteration 208/1000 | Loss: 0.00001310
Iteration 209/1000 | Loss: 0.00001310
Iteration 210/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.3097554983687587e-05, 1.3097554983687587e-05, 1.3097554983687587e-05, 1.3097554983687587e-05, 1.3097554983687587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3097554983687587e-05

Optimization complete. Final v2v error: 3.052311658859253 mm

Highest mean error: 3.818005323410034 mm for frame 56

Lowest mean error: 2.7281064987182617 mm for frame 102

Saving results

Total time: 39.077250719070435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855537
Iteration 2/25 | Loss: 0.00123906
Iteration 3/25 | Loss: 0.00082972
Iteration 4/25 | Loss: 0.00079098
Iteration 5/25 | Loss: 0.00077614
Iteration 6/25 | Loss: 0.00077253
Iteration 7/25 | Loss: 0.00077196
Iteration 8/25 | Loss: 0.00077196
Iteration 9/25 | Loss: 0.00077196
Iteration 10/25 | Loss: 0.00077196
Iteration 11/25 | Loss: 0.00077196
Iteration 12/25 | Loss: 0.00077196
Iteration 13/25 | Loss: 0.00077196
Iteration 14/25 | Loss: 0.00077196
Iteration 15/25 | Loss: 0.00077196
Iteration 16/25 | Loss: 0.00077196
Iteration 17/25 | Loss: 0.00077196
Iteration 18/25 | Loss: 0.00077196
Iteration 19/25 | Loss: 0.00077196
Iteration 20/25 | Loss: 0.00077196
Iteration 21/25 | Loss: 0.00077196
Iteration 22/25 | Loss: 0.00077196
Iteration 23/25 | Loss: 0.00077196
Iteration 24/25 | Loss: 0.00077196
Iteration 25/25 | Loss: 0.00077196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14784968
Iteration 2/25 | Loss: 0.00021745
Iteration 3/25 | Loss: 0.00021742
Iteration 4/25 | Loss: 0.00021742
Iteration 5/25 | Loss: 0.00021742
Iteration 6/25 | Loss: 0.00021742
Iteration 7/25 | Loss: 0.00021742
Iteration 8/25 | Loss: 0.00021742
Iteration 9/25 | Loss: 0.00021742
Iteration 10/25 | Loss: 0.00021742
Iteration 11/25 | Loss: 0.00021742
Iteration 12/25 | Loss: 0.00021742
Iteration 13/25 | Loss: 0.00021742
Iteration 14/25 | Loss: 0.00021742
Iteration 15/25 | Loss: 0.00021742
Iteration 16/25 | Loss: 0.00021742
Iteration 17/25 | Loss: 0.00021742
Iteration 18/25 | Loss: 0.00021742
Iteration 19/25 | Loss: 0.00021742
Iteration 20/25 | Loss: 0.00021742
Iteration 21/25 | Loss: 0.00021742
Iteration 22/25 | Loss: 0.00021742
Iteration 23/25 | Loss: 0.00021742
Iteration 24/25 | Loss: 0.00021742
Iteration 25/25 | Loss: 0.00021742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021742
Iteration 2/1000 | Loss: 0.00004183
Iteration 3/1000 | Loss: 0.00003171
Iteration 4/1000 | Loss: 0.00002827
Iteration 5/1000 | Loss: 0.00002724
Iteration 6/1000 | Loss: 0.00002621
Iteration 7/1000 | Loss: 0.00002544
Iteration 8/1000 | Loss: 0.00002489
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00002426
Iteration 11/1000 | Loss: 0.00002415
Iteration 12/1000 | Loss: 0.00002397
Iteration 13/1000 | Loss: 0.00002393
Iteration 14/1000 | Loss: 0.00002393
Iteration 15/1000 | Loss: 0.00002384
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00002381
Iteration 18/1000 | Loss: 0.00002381
Iteration 19/1000 | Loss: 0.00002381
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002380
Iteration 22/1000 | Loss: 0.00002380
Iteration 23/1000 | Loss: 0.00002380
Iteration 24/1000 | Loss: 0.00002379
Iteration 25/1000 | Loss: 0.00002379
Iteration 26/1000 | Loss: 0.00002378
Iteration 27/1000 | Loss: 0.00002378
Iteration 28/1000 | Loss: 0.00002378
Iteration 29/1000 | Loss: 0.00002378
Iteration 30/1000 | Loss: 0.00002378
Iteration 31/1000 | Loss: 0.00002378
Iteration 32/1000 | Loss: 0.00002378
Iteration 33/1000 | Loss: 0.00002377
Iteration 34/1000 | Loss: 0.00002377
Iteration 35/1000 | Loss: 0.00002377
Iteration 36/1000 | Loss: 0.00002377
Iteration 37/1000 | Loss: 0.00002377
Iteration 38/1000 | Loss: 0.00002377
Iteration 39/1000 | Loss: 0.00002376
Iteration 40/1000 | Loss: 0.00002376
Iteration 41/1000 | Loss: 0.00002376
Iteration 42/1000 | Loss: 0.00002375
Iteration 43/1000 | Loss: 0.00002374
Iteration 44/1000 | Loss: 0.00002373
Iteration 45/1000 | Loss: 0.00002373
Iteration 46/1000 | Loss: 0.00002373
Iteration 47/1000 | Loss: 0.00002373
Iteration 48/1000 | Loss: 0.00002373
Iteration 49/1000 | Loss: 0.00002372
Iteration 50/1000 | Loss: 0.00002372
Iteration 51/1000 | Loss: 0.00002372
Iteration 52/1000 | Loss: 0.00002370
Iteration 53/1000 | Loss: 0.00002370
Iteration 54/1000 | Loss: 0.00002370
Iteration 55/1000 | Loss: 0.00002370
Iteration 56/1000 | Loss: 0.00002369
Iteration 57/1000 | Loss: 0.00002369
Iteration 58/1000 | Loss: 0.00002369
Iteration 59/1000 | Loss: 0.00002369
Iteration 60/1000 | Loss: 0.00002369
Iteration 61/1000 | Loss: 0.00002369
Iteration 62/1000 | Loss: 0.00002369
Iteration 63/1000 | Loss: 0.00002369
Iteration 64/1000 | Loss: 0.00002369
Iteration 65/1000 | Loss: 0.00002369
Iteration 66/1000 | Loss: 0.00002369
Iteration 67/1000 | Loss: 0.00002368
Iteration 68/1000 | Loss: 0.00002367
Iteration 69/1000 | Loss: 0.00002367
Iteration 70/1000 | Loss: 0.00002366
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002366
Iteration 73/1000 | Loss: 0.00002366
Iteration 74/1000 | Loss: 0.00002366
Iteration 75/1000 | Loss: 0.00002366
Iteration 76/1000 | Loss: 0.00002366
Iteration 77/1000 | Loss: 0.00002366
Iteration 78/1000 | Loss: 0.00002366
Iteration 79/1000 | Loss: 0.00002366
Iteration 80/1000 | Loss: 0.00002366
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002365
Iteration 83/1000 | Loss: 0.00002365
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00002365
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002365
Iteration 90/1000 | Loss: 0.00002364
Iteration 91/1000 | Loss: 0.00002364
Iteration 92/1000 | Loss: 0.00002364
Iteration 93/1000 | Loss: 0.00002363
Iteration 94/1000 | Loss: 0.00002363
Iteration 95/1000 | Loss: 0.00002363
Iteration 96/1000 | Loss: 0.00002363
Iteration 97/1000 | Loss: 0.00002363
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002363
Iteration 100/1000 | Loss: 0.00002362
Iteration 101/1000 | Loss: 0.00002362
Iteration 102/1000 | Loss: 0.00002362
Iteration 103/1000 | Loss: 0.00002362
Iteration 104/1000 | Loss: 0.00002361
Iteration 105/1000 | Loss: 0.00002361
Iteration 106/1000 | Loss: 0.00002361
Iteration 107/1000 | Loss: 0.00002360
Iteration 108/1000 | Loss: 0.00002360
Iteration 109/1000 | Loss: 0.00002360
Iteration 110/1000 | Loss: 0.00002360
Iteration 111/1000 | Loss: 0.00002359
Iteration 112/1000 | Loss: 0.00002359
Iteration 113/1000 | Loss: 0.00002359
Iteration 114/1000 | Loss: 0.00002359
Iteration 115/1000 | Loss: 0.00002359
Iteration 116/1000 | Loss: 0.00002359
Iteration 117/1000 | Loss: 0.00002359
Iteration 118/1000 | Loss: 0.00002359
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002357
Iteration 121/1000 | Loss: 0.00002357
Iteration 122/1000 | Loss: 0.00002357
Iteration 123/1000 | Loss: 0.00002357
Iteration 124/1000 | Loss: 0.00002356
Iteration 125/1000 | Loss: 0.00002356
Iteration 126/1000 | Loss: 0.00002356
Iteration 127/1000 | Loss: 0.00002356
Iteration 128/1000 | Loss: 0.00002356
Iteration 129/1000 | Loss: 0.00002356
Iteration 130/1000 | Loss: 0.00002356
Iteration 131/1000 | Loss: 0.00002355
Iteration 132/1000 | Loss: 0.00002355
Iteration 133/1000 | Loss: 0.00002355
Iteration 134/1000 | Loss: 0.00002354
Iteration 135/1000 | Loss: 0.00002354
Iteration 136/1000 | Loss: 0.00002354
Iteration 137/1000 | Loss: 0.00002353
Iteration 138/1000 | Loss: 0.00002353
Iteration 139/1000 | Loss: 0.00002353
Iteration 140/1000 | Loss: 0.00002353
Iteration 141/1000 | Loss: 0.00002352
Iteration 142/1000 | Loss: 0.00002352
Iteration 143/1000 | Loss: 0.00002352
Iteration 144/1000 | Loss: 0.00002352
Iteration 145/1000 | Loss: 0.00002352
Iteration 146/1000 | Loss: 0.00002352
Iteration 147/1000 | Loss: 0.00002352
Iteration 148/1000 | Loss: 0.00002352
Iteration 149/1000 | Loss: 0.00002352
Iteration 150/1000 | Loss: 0.00002352
Iteration 151/1000 | Loss: 0.00002351
Iteration 152/1000 | Loss: 0.00002351
Iteration 153/1000 | Loss: 0.00002351
Iteration 154/1000 | Loss: 0.00002351
Iteration 155/1000 | Loss: 0.00002351
Iteration 156/1000 | Loss: 0.00002351
Iteration 157/1000 | Loss: 0.00002350
Iteration 158/1000 | Loss: 0.00002350
Iteration 159/1000 | Loss: 0.00002350
Iteration 160/1000 | Loss: 0.00002350
Iteration 161/1000 | Loss: 0.00002350
Iteration 162/1000 | Loss: 0.00002350
Iteration 163/1000 | Loss: 0.00002350
Iteration 164/1000 | Loss: 0.00002350
Iteration 165/1000 | Loss: 0.00002350
Iteration 166/1000 | Loss: 0.00002350
Iteration 167/1000 | Loss: 0.00002350
Iteration 168/1000 | Loss: 0.00002350
Iteration 169/1000 | Loss: 0.00002350
Iteration 170/1000 | Loss: 0.00002350
Iteration 171/1000 | Loss: 0.00002349
Iteration 172/1000 | Loss: 0.00002349
Iteration 173/1000 | Loss: 0.00002349
Iteration 174/1000 | Loss: 0.00002349
Iteration 175/1000 | Loss: 0.00002349
Iteration 176/1000 | Loss: 0.00002349
Iteration 177/1000 | Loss: 0.00002349
Iteration 178/1000 | Loss: 0.00002349
Iteration 179/1000 | Loss: 0.00002349
Iteration 180/1000 | Loss: 0.00002349
Iteration 181/1000 | Loss: 0.00002349
Iteration 182/1000 | Loss: 0.00002349
Iteration 183/1000 | Loss: 0.00002349
Iteration 184/1000 | Loss: 0.00002349
Iteration 185/1000 | Loss: 0.00002349
Iteration 186/1000 | Loss: 0.00002348
Iteration 187/1000 | Loss: 0.00002348
Iteration 188/1000 | Loss: 0.00002348
Iteration 189/1000 | Loss: 0.00002348
Iteration 190/1000 | Loss: 0.00002348
Iteration 191/1000 | Loss: 0.00002348
Iteration 192/1000 | Loss: 0.00002348
Iteration 193/1000 | Loss: 0.00002348
Iteration 194/1000 | Loss: 0.00002348
Iteration 195/1000 | Loss: 0.00002348
Iteration 196/1000 | Loss: 0.00002348
Iteration 197/1000 | Loss: 0.00002348
Iteration 198/1000 | Loss: 0.00002348
Iteration 199/1000 | Loss: 0.00002348
Iteration 200/1000 | Loss: 0.00002347
Iteration 201/1000 | Loss: 0.00002347
Iteration 202/1000 | Loss: 0.00002347
Iteration 203/1000 | Loss: 0.00002347
Iteration 204/1000 | Loss: 0.00002347
Iteration 205/1000 | Loss: 0.00002347
Iteration 206/1000 | Loss: 0.00002347
Iteration 207/1000 | Loss: 0.00002347
Iteration 208/1000 | Loss: 0.00002347
Iteration 209/1000 | Loss: 0.00002347
Iteration 210/1000 | Loss: 0.00002347
Iteration 211/1000 | Loss: 0.00002347
Iteration 212/1000 | Loss: 0.00002347
Iteration 213/1000 | Loss: 0.00002347
Iteration 214/1000 | Loss: 0.00002347
Iteration 215/1000 | Loss: 0.00002347
Iteration 216/1000 | Loss: 0.00002347
Iteration 217/1000 | Loss: 0.00002347
Iteration 218/1000 | Loss: 0.00002347
Iteration 219/1000 | Loss: 0.00002347
Iteration 220/1000 | Loss: 0.00002347
Iteration 221/1000 | Loss: 0.00002347
Iteration 222/1000 | Loss: 0.00002347
Iteration 223/1000 | Loss: 0.00002347
Iteration 224/1000 | Loss: 0.00002347
Iteration 225/1000 | Loss: 0.00002347
Iteration 226/1000 | Loss: 0.00002347
Iteration 227/1000 | Loss: 0.00002347
Iteration 228/1000 | Loss: 0.00002347
Iteration 229/1000 | Loss: 0.00002347
Iteration 230/1000 | Loss: 0.00002347
Iteration 231/1000 | Loss: 0.00002347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.3467566279578023e-05, 2.3467566279578023e-05, 2.3467566279578023e-05, 2.3467566279578023e-05, 2.3467566279578023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3467566279578023e-05

Optimization complete. Final v2v error: 3.8894362449645996 mm

Highest mean error: 4.7896294593811035 mm for frame 82

Lowest mean error: 3.19771671295166 mm for frame 8

Saving results

Total time: 42.92359375953674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935028
Iteration 2/25 | Loss: 0.00118388
Iteration 3/25 | Loss: 0.00089766
Iteration 4/25 | Loss: 0.00083370
Iteration 5/25 | Loss: 0.00080174
Iteration 6/25 | Loss: 0.00077820
Iteration 7/25 | Loss: 0.00075004
Iteration 8/25 | Loss: 0.00073530
Iteration 9/25 | Loss: 0.00073197
Iteration 10/25 | Loss: 0.00072652
Iteration 11/25 | Loss: 0.00072605
Iteration 12/25 | Loss: 0.00074270
Iteration 13/25 | Loss: 0.00078798
Iteration 14/25 | Loss: 0.00077226
Iteration 15/25 | Loss: 0.00074715
Iteration 16/25 | Loss: 0.00072676
Iteration 17/25 | Loss: 0.00072981
Iteration 18/25 | Loss: 0.00073708
Iteration 19/25 | Loss: 0.00073654
Iteration 20/25 | Loss: 0.00071889
Iteration 21/25 | Loss: 0.00071063
Iteration 22/25 | Loss: 0.00070613
Iteration 23/25 | Loss: 0.00070170
Iteration 24/25 | Loss: 0.00070020
Iteration 25/25 | Loss: 0.00070024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48298740
Iteration 2/25 | Loss: 0.00045999
Iteration 3/25 | Loss: 0.00045998
Iteration 4/25 | Loss: 0.00045998
Iteration 5/25 | Loss: 0.00045998
Iteration 6/25 | Loss: 0.00045998
Iteration 7/25 | Loss: 0.00045998
Iteration 8/25 | Loss: 0.00045998
Iteration 9/25 | Loss: 0.00045998
Iteration 10/25 | Loss: 0.00045998
Iteration 11/25 | Loss: 0.00045998
Iteration 12/25 | Loss: 0.00045998
Iteration 13/25 | Loss: 0.00045998
Iteration 14/25 | Loss: 0.00045998
Iteration 15/25 | Loss: 0.00045998
Iteration 16/25 | Loss: 0.00045998
Iteration 17/25 | Loss: 0.00045998
Iteration 18/25 | Loss: 0.00045998
Iteration 19/25 | Loss: 0.00045998
Iteration 20/25 | Loss: 0.00045998
Iteration 21/25 | Loss: 0.00045998
Iteration 22/25 | Loss: 0.00045998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00045998094719834626, 0.00045998094719834626, 0.00045998094719834626, 0.00045998094719834626, 0.00045998094719834626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045998094719834626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045998
Iteration 2/1000 | Loss: 0.00005897
Iteration 3/1000 | Loss: 0.00077171
Iteration 4/1000 | Loss: 0.00083500
Iteration 5/1000 | Loss: 0.00010368
Iteration 6/1000 | Loss: 0.00031931
Iteration 7/1000 | Loss: 0.00023219
Iteration 8/1000 | Loss: 0.00007912
Iteration 9/1000 | Loss: 0.00020846
Iteration 10/1000 | Loss: 0.00036855
Iteration 11/1000 | Loss: 0.00020289
Iteration 12/1000 | Loss: 0.00017058
Iteration 13/1000 | Loss: 0.00017404
Iteration 14/1000 | Loss: 0.00037383
Iteration 15/1000 | Loss: 0.00037139
Iteration 16/1000 | Loss: 0.00030490
Iteration 17/1000 | Loss: 0.00020053
Iteration 18/1000 | Loss: 0.00022190
Iteration 19/1000 | Loss: 0.00019321
Iteration 20/1000 | Loss: 0.00005192
Iteration 21/1000 | Loss: 0.00006491
Iteration 22/1000 | Loss: 0.00004298
Iteration 23/1000 | Loss: 0.00005520
Iteration 24/1000 | Loss: 0.00013003
Iteration 25/1000 | Loss: 0.00029925
Iteration 26/1000 | Loss: 0.00018337
Iteration 27/1000 | Loss: 0.00043894
Iteration 28/1000 | Loss: 0.00032155
Iteration 29/1000 | Loss: 0.00045869
Iteration 30/1000 | Loss: 0.00016007
Iteration 31/1000 | Loss: 0.00022400
Iteration 32/1000 | Loss: 0.00019349
Iteration 33/1000 | Loss: 0.00011870
Iteration 34/1000 | Loss: 0.00027454
Iteration 35/1000 | Loss: 0.00038427
Iteration 36/1000 | Loss: 0.00031166
Iteration 37/1000 | Loss: 0.00020640
Iteration 38/1000 | Loss: 0.00034736
Iteration 39/1000 | Loss: 0.00016244
Iteration 40/1000 | Loss: 0.00018173
Iteration 41/1000 | Loss: 0.00017250
Iteration 42/1000 | Loss: 0.00022052
Iteration 43/1000 | Loss: 0.00021602
Iteration 44/1000 | Loss: 0.00015329
Iteration 45/1000 | Loss: 0.00015511
Iteration 46/1000 | Loss: 0.00022464
Iteration 47/1000 | Loss: 0.00013836
Iteration 48/1000 | Loss: 0.00028808
Iteration 49/1000 | Loss: 0.00015852
Iteration 50/1000 | Loss: 0.00011707
Iteration 51/1000 | Loss: 0.00013326
Iteration 52/1000 | Loss: 0.00008538
Iteration 53/1000 | Loss: 0.00010046
Iteration 54/1000 | Loss: 0.00009649
Iteration 55/1000 | Loss: 0.00009080
Iteration 56/1000 | Loss: 0.00014630
Iteration 57/1000 | Loss: 0.00026025
Iteration 58/1000 | Loss: 0.00004471
Iteration 59/1000 | Loss: 0.00004878
Iteration 60/1000 | Loss: 0.00004030
Iteration 61/1000 | Loss: 0.00005264
Iteration 62/1000 | Loss: 0.00024072
Iteration 63/1000 | Loss: 0.00004370
Iteration 64/1000 | Loss: 0.00023578
Iteration 65/1000 | Loss: 0.00017658
Iteration 66/1000 | Loss: 0.00062775
Iteration 67/1000 | Loss: 0.00038563
Iteration 68/1000 | Loss: 0.00036307
Iteration 69/1000 | Loss: 0.00029434
Iteration 70/1000 | Loss: 0.00020888
Iteration 71/1000 | Loss: 0.00009026
Iteration 72/1000 | Loss: 0.00003780
Iteration 73/1000 | Loss: 0.00022489
Iteration 74/1000 | Loss: 0.00024075
Iteration 75/1000 | Loss: 0.00004924
Iteration 76/1000 | Loss: 0.00023669
Iteration 77/1000 | Loss: 0.00044353
Iteration 78/1000 | Loss: 0.00090912
Iteration 79/1000 | Loss: 0.00030917
Iteration 80/1000 | Loss: 0.00021966
Iteration 81/1000 | Loss: 0.00014905
Iteration 82/1000 | Loss: 0.00016958
Iteration 83/1000 | Loss: 0.00013345
Iteration 84/1000 | Loss: 0.00012546
Iteration 85/1000 | Loss: 0.00016170
Iteration 86/1000 | Loss: 0.00005931
Iteration 87/1000 | Loss: 0.00023305
Iteration 88/1000 | Loss: 0.00032356
Iteration 89/1000 | Loss: 0.00066099
Iteration 90/1000 | Loss: 0.00013436
Iteration 91/1000 | Loss: 0.00018642
Iteration 92/1000 | Loss: 0.00016240
Iteration 93/1000 | Loss: 0.00009012
Iteration 94/1000 | Loss: 0.00034815
Iteration 95/1000 | Loss: 0.00022475
Iteration 96/1000 | Loss: 0.00030723
Iteration 97/1000 | Loss: 0.00011225
Iteration 98/1000 | Loss: 0.00009695
Iteration 99/1000 | Loss: 0.00017718
Iteration 100/1000 | Loss: 0.00012057
Iteration 101/1000 | Loss: 0.00012851
Iteration 102/1000 | Loss: 0.00026619
Iteration 103/1000 | Loss: 0.00011210
Iteration 104/1000 | Loss: 0.00008815
Iteration 105/1000 | Loss: 0.00007817
Iteration 106/1000 | Loss: 0.00007187
Iteration 107/1000 | Loss: 0.00006427
Iteration 108/1000 | Loss: 0.00006536
Iteration 109/1000 | Loss: 0.00004514
Iteration 110/1000 | Loss: 0.00013233
Iteration 111/1000 | Loss: 0.00015509
Iteration 112/1000 | Loss: 0.00013587
Iteration 113/1000 | Loss: 0.00015317
Iteration 114/1000 | Loss: 0.00005536
Iteration 115/1000 | Loss: 0.00006318
Iteration 116/1000 | Loss: 0.00004005
Iteration 117/1000 | Loss: 0.00003954
Iteration 118/1000 | Loss: 0.00003802
Iteration 119/1000 | Loss: 0.00003889
Iteration 120/1000 | Loss: 0.00029900
Iteration 121/1000 | Loss: 0.00032440
Iteration 122/1000 | Loss: 0.00022520
Iteration 123/1000 | Loss: 0.00034501
Iteration 124/1000 | Loss: 0.00019684
Iteration 125/1000 | Loss: 0.00018839
Iteration 126/1000 | Loss: 0.00027151
Iteration 127/1000 | Loss: 0.00065496
Iteration 128/1000 | Loss: 0.00030336
Iteration 129/1000 | Loss: 0.00074942
Iteration 130/1000 | Loss: 0.00050012
Iteration 131/1000 | Loss: 0.00052534
Iteration 132/1000 | Loss: 0.00066180
Iteration 133/1000 | Loss: 0.00054053
Iteration 134/1000 | Loss: 0.00043064
Iteration 135/1000 | Loss: 0.00053067
Iteration 136/1000 | Loss: 0.00066274
Iteration 137/1000 | Loss: 0.00033469
Iteration 138/1000 | Loss: 0.00034627
Iteration 139/1000 | Loss: 0.00009756
Iteration 140/1000 | Loss: 0.00008386
Iteration 141/1000 | Loss: 0.00010841
Iteration 142/1000 | Loss: 0.00013440
Iteration 143/1000 | Loss: 0.00015133
Iteration 144/1000 | Loss: 0.00006416
Iteration 145/1000 | Loss: 0.00010766
Iteration 146/1000 | Loss: 0.00034187
Iteration 147/1000 | Loss: 0.00011517
Iteration 148/1000 | Loss: 0.00005022
Iteration 149/1000 | Loss: 0.00004476
Iteration 150/1000 | Loss: 0.00004750
Iteration 151/1000 | Loss: 0.00004406
Iteration 152/1000 | Loss: 0.00004460
Iteration 153/1000 | Loss: 0.00004384
Iteration 154/1000 | Loss: 0.00004242
Iteration 155/1000 | Loss: 0.00004371
Iteration 156/1000 | Loss: 0.00023951
Iteration 157/1000 | Loss: 0.00006054
Iteration 158/1000 | Loss: 0.00003316
Iteration 159/1000 | Loss: 0.00002740
Iteration 160/1000 | Loss: 0.00002474
Iteration 161/1000 | Loss: 0.00002305
Iteration 162/1000 | Loss: 0.00002219
Iteration 163/1000 | Loss: 0.00002147
Iteration 164/1000 | Loss: 0.00002076
Iteration 165/1000 | Loss: 0.00002035
Iteration 166/1000 | Loss: 0.00002010
Iteration 167/1000 | Loss: 0.00002000
Iteration 168/1000 | Loss: 0.00001980
Iteration 169/1000 | Loss: 0.00001976
Iteration 170/1000 | Loss: 0.00001959
Iteration 171/1000 | Loss: 0.00001957
Iteration 172/1000 | Loss: 0.00001954
Iteration 173/1000 | Loss: 0.00001953
Iteration 174/1000 | Loss: 0.00001939
Iteration 175/1000 | Loss: 0.00022807
Iteration 176/1000 | Loss: 0.00015561
Iteration 177/1000 | Loss: 0.00026060
Iteration 178/1000 | Loss: 0.00012783
Iteration 179/1000 | Loss: 0.00002496
Iteration 180/1000 | Loss: 0.00002304
Iteration 181/1000 | Loss: 0.00002133
Iteration 182/1000 | Loss: 0.00002042
Iteration 183/1000 | Loss: 0.00001966
Iteration 184/1000 | Loss: 0.00001916
Iteration 185/1000 | Loss: 0.00001896
Iteration 186/1000 | Loss: 0.00001888
Iteration 187/1000 | Loss: 0.00001882
Iteration 188/1000 | Loss: 0.00001879
Iteration 189/1000 | Loss: 0.00001878
Iteration 190/1000 | Loss: 0.00001878
Iteration 191/1000 | Loss: 0.00001877
Iteration 192/1000 | Loss: 0.00001877
Iteration 193/1000 | Loss: 0.00001876
Iteration 194/1000 | Loss: 0.00001876
Iteration 195/1000 | Loss: 0.00001876
Iteration 196/1000 | Loss: 0.00001875
Iteration 197/1000 | Loss: 0.00001875
Iteration 198/1000 | Loss: 0.00001874
Iteration 199/1000 | Loss: 0.00001874
Iteration 200/1000 | Loss: 0.00001873
Iteration 201/1000 | Loss: 0.00001873
Iteration 202/1000 | Loss: 0.00001873
Iteration 203/1000 | Loss: 0.00001872
Iteration 204/1000 | Loss: 0.00001872
Iteration 205/1000 | Loss: 0.00001872
Iteration 206/1000 | Loss: 0.00001872
Iteration 207/1000 | Loss: 0.00001872
Iteration 208/1000 | Loss: 0.00001872
Iteration 209/1000 | Loss: 0.00001872
Iteration 210/1000 | Loss: 0.00001872
Iteration 211/1000 | Loss: 0.00001872
Iteration 212/1000 | Loss: 0.00001872
Iteration 213/1000 | Loss: 0.00001872
Iteration 214/1000 | Loss: 0.00001872
Iteration 215/1000 | Loss: 0.00001872
Iteration 216/1000 | Loss: 0.00001872
Iteration 217/1000 | Loss: 0.00001872
Iteration 218/1000 | Loss: 0.00001872
Iteration 219/1000 | Loss: 0.00001871
Iteration 220/1000 | Loss: 0.00001869
Iteration 221/1000 | Loss: 0.00001869
Iteration 222/1000 | Loss: 0.00001869
Iteration 223/1000 | Loss: 0.00001869
Iteration 224/1000 | Loss: 0.00001869
Iteration 225/1000 | Loss: 0.00001869
Iteration 226/1000 | Loss: 0.00001869
Iteration 227/1000 | Loss: 0.00001869
Iteration 228/1000 | Loss: 0.00001869
Iteration 229/1000 | Loss: 0.00001869
Iteration 230/1000 | Loss: 0.00001869
Iteration 231/1000 | Loss: 0.00001868
Iteration 232/1000 | Loss: 0.00001868
Iteration 233/1000 | Loss: 0.00001867
Iteration 234/1000 | Loss: 0.00001867
Iteration 235/1000 | Loss: 0.00001866
Iteration 236/1000 | Loss: 0.00001866
Iteration 237/1000 | Loss: 0.00001865
Iteration 238/1000 | Loss: 0.00001865
Iteration 239/1000 | Loss: 0.00001865
Iteration 240/1000 | Loss: 0.00001864
Iteration 241/1000 | Loss: 0.00001864
Iteration 242/1000 | Loss: 0.00001864
Iteration 243/1000 | Loss: 0.00001863
Iteration 244/1000 | Loss: 0.00001863
Iteration 245/1000 | Loss: 0.00001863
Iteration 246/1000 | Loss: 0.00001863
Iteration 247/1000 | Loss: 0.00001863
Iteration 248/1000 | Loss: 0.00001862
Iteration 249/1000 | Loss: 0.00001862
Iteration 250/1000 | Loss: 0.00001861
Iteration 251/1000 | Loss: 0.00001861
Iteration 252/1000 | Loss: 0.00001860
Iteration 253/1000 | Loss: 0.00001859
Iteration 254/1000 | Loss: 0.00001857
Iteration 255/1000 | Loss: 0.00001856
Iteration 256/1000 | Loss: 0.00001856
Iteration 257/1000 | Loss: 0.00001856
Iteration 258/1000 | Loss: 0.00001856
Iteration 259/1000 | Loss: 0.00001856
Iteration 260/1000 | Loss: 0.00001856
Iteration 261/1000 | Loss: 0.00001856
Iteration 262/1000 | Loss: 0.00001856
Iteration 263/1000 | Loss: 0.00001855
Iteration 264/1000 | Loss: 0.00001855
Iteration 265/1000 | Loss: 0.00001854
Iteration 266/1000 | Loss: 0.00001854
Iteration 267/1000 | Loss: 0.00001854
Iteration 268/1000 | Loss: 0.00001854
Iteration 269/1000 | Loss: 0.00001854
Iteration 270/1000 | Loss: 0.00001854
Iteration 271/1000 | Loss: 0.00001854
Iteration 272/1000 | Loss: 0.00001854
Iteration 273/1000 | Loss: 0.00001854
Iteration 274/1000 | Loss: 0.00001854
Iteration 275/1000 | Loss: 0.00001854
Iteration 276/1000 | Loss: 0.00001854
Iteration 277/1000 | Loss: 0.00001854
Iteration 278/1000 | Loss: 0.00001854
Iteration 279/1000 | Loss: 0.00001853
Iteration 280/1000 | Loss: 0.00001853
Iteration 281/1000 | Loss: 0.00001853
Iteration 282/1000 | Loss: 0.00001853
Iteration 283/1000 | Loss: 0.00001853
Iteration 284/1000 | Loss: 0.00001853
Iteration 285/1000 | Loss: 0.00001853
Iteration 286/1000 | Loss: 0.00001853
Iteration 287/1000 | Loss: 0.00001853
Iteration 288/1000 | Loss: 0.00001853
Iteration 289/1000 | Loss: 0.00001852
Iteration 290/1000 | Loss: 0.00001852
Iteration 291/1000 | Loss: 0.00001852
Iteration 292/1000 | Loss: 0.00001852
Iteration 293/1000 | Loss: 0.00001852
Iteration 294/1000 | Loss: 0.00001852
Iteration 295/1000 | Loss: 0.00001852
Iteration 296/1000 | Loss: 0.00001852
Iteration 297/1000 | Loss: 0.00001851
Iteration 298/1000 | Loss: 0.00001851
Iteration 299/1000 | Loss: 0.00001851
Iteration 300/1000 | Loss: 0.00001851
Iteration 301/1000 | Loss: 0.00001851
Iteration 302/1000 | Loss: 0.00001851
Iteration 303/1000 | Loss: 0.00001850
Iteration 304/1000 | Loss: 0.00001850
Iteration 305/1000 | Loss: 0.00001850
Iteration 306/1000 | Loss: 0.00001850
Iteration 307/1000 | Loss: 0.00001850
Iteration 308/1000 | Loss: 0.00001850
Iteration 309/1000 | Loss: 0.00001850
Iteration 310/1000 | Loss: 0.00001850
Iteration 311/1000 | Loss: 0.00001850
Iteration 312/1000 | Loss: 0.00001850
Iteration 313/1000 | Loss: 0.00001849
Iteration 314/1000 | Loss: 0.00001849
Iteration 315/1000 | Loss: 0.00001849
Iteration 316/1000 | Loss: 0.00001849
Iteration 317/1000 | Loss: 0.00001849
Iteration 318/1000 | Loss: 0.00001849
Iteration 319/1000 | Loss: 0.00001849
Iteration 320/1000 | Loss: 0.00001849
Iteration 321/1000 | Loss: 0.00001849
Iteration 322/1000 | Loss: 0.00001849
Iteration 323/1000 | Loss: 0.00001849
Iteration 324/1000 | Loss: 0.00001849
Iteration 325/1000 | Loss: 0.00001849
Iteration 326/1000 | Loss: 0.00001849
Iteration 327/1000 | Loss: 0.00001849
Iteration 328/1000 | Loss: 0.00001849
Iteration 329/1000 | Loss: 0.00001849
Iteration 330/1000 | Loss: 0.00001849
Iteration 331/1000 | Loss: 0.00001849
Iteration 332/1000 | Loss: 0.00001849
Iteration 333/1000 | Loss: 0.00001849
Iteration 334/1000 | Loss: 0.00001849
Iteration 335/1000 | Loss: 0.00001848
Iteration 336/1000 | Loss: 0.00001848
Iteration 337/1000 | Loss: 0.00001848
Iteration 338/1000 | Loss: 0.00001848
Iteration 339/1000 | Loss: 0.00001848
Iteration 340/1000 | Loss: 0.00001848
Iteration 341/1000 | Loss: 0.00001848
Iteration 342/1000 | Loss: 0.00001848
Iteration 343/1000 | Loss: 0.00001848
Iteration 344/1000 | Loss: 0.00001848
Iteration 345/1000 | Loss: 0.00001848
Iteration 346/1000 | Loss: 0.00001848
Iteration 347/1000 | Loss: 0.00001848
Iteration 348/1000 | Loss: 0.00001848
Iteration 349/1000 | Loss: 0.00001848
Iteration 350/1000 | Loss: 0.00001848
Iteration 351/1000 | Loss: 0.00001848
Iteration 352/1000 | Loss: 0.00001848
Iteration 353/1000 | Loss: 0.00001847
Iteration 354/1000 | Loss: 0.00001847
Iteration 355/1000 | Loss: 0.00001847
Iteration 356/1000 | Loss: 0.00001847
Iteration 357/1000 | Loss: 0.00001847
Iteration 358/1000 | Loss: 0.00001847
Iteration 359/1000 | Loss: 0.00001847
Iteration 360/1000 | Loss: 0.00001847
Iteration 361/1000 | Loss: 0.00001847
Iteration 362/1000 | Loss: 0.00001847
Iteration 363/1000 | Loss: 0.00001847
Iteration 364/1000 | Loss: 0.00001847
Iteration 365/1000 | Loss: 0.00001847
Iteration 366/1000 | Loss: 0.00001847
Iteration 367/1000 | Loss: 0.00001847
Iteration 368/1000 | Loss: 0.00001847
Iteration 369/1000 | Loss: 0.00001847
Iteration 370/1000 | Loss: 0.00001847
Iteration 371/1000 | Loss: 0.00001846
Iteration 372/1000 | Loss: 0.00001846
Iteration 373/1000 | Loss: 0.00001846
Iteration 374/1000 | Loss: 0.00001846
Iteration 375/1000 | Loss: 0.00001846
Iteration 376/1000 | Loss: 0.00001846
Iteration 377/1000 | Loss: 0.00001846
Iteration 378/1000 | Loss: 0.00001846
Iteration 379/1000 | Loss: 0.00001846
Iteration 380/1000 | Loss: 0.00001846
Iteration 381/1000 | Loss: 0.00001846
Iteration 382/1000 | Loss: 0.00001846
Iteration 383/1000 | Loss: 0.00001846
Iteration 384/1000 | Loss: 0.00001846
Iteration 385/1000 | Loss: 0.00001846
Iteration 386/1000 | Loss: 0.00001846
Iteration 387/1000 | Loss: 0.00001846
Iteration 388/1000 | Loss: 0.00001845
Iteration 389/1000 | Loss: 0.00001845
Iteration 390/1000 | Loss: 0.00001845
Iteration 391/1000 | Loss: 0.00001845
Iteration 392/1000 | Loss: 0.00001845
Iteration 393/1000 | Loss: 0.00001845
Iteration 394/1000 | Loss: 0.00001845
Iteration 395/1000 | Loss: 0.00001845
Iteration 396/1000 | Loss: 0.00001845
Iteration 397/1000 | Loss: 0.00001845
Iteration 398/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [1.8452812582836486e-05, 1.8452812582836486e-05, 1.8452812582836486e-05, 1.8452812582836486e-05, 1.8452812582836486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8452812582836486e-05

Optimization complete. Final v2v error: 3.5380918979644775 mm

Highest mean error: 5.493165493011475 mm for frame 209

Lowest mean error: 3.0541911125183105 mm for frame 19

Saving results

Total time: 359.3577835559845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722420
Iteration 2/25 | Loss: 0.00091491
Iteration 3/25 | Loss: 0.00071363
Iteration 4/25 | Loss: 0.00066937
Iteration 5/25 | Loss: 0.00064774
Iteration 6/25 | Loss: 0.00064067
Iteration 7/25 | Loss: 0.00063927
Iteration 8/25 | Loss: 0.00063706
Iteration 9/25 | Loss: 0.00063653
Iteration 10/25 | Loss: 0.00063944
Iteration 11/25 | Loss: 0.00063989
Iteration 12/25 | Loss: 0.00063794
Iteration 13/25 | Loss: 0.00063698
Iteration 14/25 | Loss: 0.00063773
Iteration 15/25 | Loss: 0.00063532
Iteration 16/25 | Loss: 0.00063513
Iteration 17/25 | Loss: 0.00063512
Iteration 18/25 | Loss: 0.00063511
Iteration 19/25 | Loss: 0.00063511
Iteration 20/25 | Loss: 0.00063511
Iteration 21/25 | Loss: 0.00063511
Iteration 22/25 | Loss: 0.00063511
Iteration 23/25 | Loss: 0.00063511
Iteration 24/25 | Loss: 0.00063511
Iteration 25/25 | Loss: 0.00063511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65794468
Iteration 2/25 | Loss: 0.00026645
Iteration 3/25 | Loss: 0.00026645
Iteration 4/25 | Loss: 0.00026645
Iteration 5/25 | Loss: 0.00026645
Iteration 6/25 | Loss: 0.00026645
Iteration 7/25 | Loss: 0.00026645
Iteration 8/25 | Loss: 0.00026645
Iteration 9/25 | Loss: 0.00026645
Iteration 10/25 | Loss: 0.00026645
Iteration 11/25 | Loss: 0.00026645
Iteration 12/25 | Loss: 0.00026645
Iteration 13/25 | Loss: 0.00026645
Iteration 14/25 | Loss: 0.00026645
Iteration 15/25 | Loss: 0.00026645
Iteration 16/25 | Loss: 0.00026645
Iteration 17/25 | Loss: 0.00026645
Iteration 18/25 | Loss: 0.00026645
Iteration 19/25 | Loss: 0.00026645
Iteration 20/25 | Loss: 0.00026645
Iteration 21/25 | Loss: 0.00026645
Iteration 22/25 | Loss: 0.00026645
Iteration 23/25 | Loss: 0.00026645
Iteration 24/25 | Loss: 0.00026645
Iteration 25/25 | Loss: 0.00026645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026645
Iteration 2/1000 | Loss: 0.00002906
Iteration 3/1000 | Loss: 0.00002098
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00040173
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001764
Iteration 8/1000 | Loss: 0.00001685
Iteration 9/1000 | Loss: 0.00001632
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001594
Iteration 13/1000 | Loss: 0.00001587
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00001581
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001574
Iteration 21/1000 | Loss: 0.00001569
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001559
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001556
Iteration 33/1000 | Loss: 0.00001556
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001548
Iteration 54/1000 | Loss: 0.00001547
Iteration 55/1000 | Loss: 0.00001547
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001545
Iteration 61/1000 | Loss: 0.00001544
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001541
Iteration 80/1000 | Loss: 0.00001541
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001540
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001539
Iteration 91/1000 | Loss: 0.00001539
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001538
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001537
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001537
Iteration 105/1000 | Loss: 0.00001537
Iteration 106/1000 | Loss: 0.00001537
Iteration 107/1000 | Loss: 0.00001537
Iteration 108/1000 | Loss: 0.00001537
Iteration 109/1000 | Loss: 0.00001537
Iteration 110/1000 | Loss: 0.00001537
Iteration 111/1000 | Loss: 0.00001537
Iteration 112/1000 | Loss: 0.00001537
Iteration 113/1000 | Loss: 0.00001537
Iteration 114/1000 | Loss: 0.00001537
Iteration 115/1000 | Loss: 0.00001537
Iteration 116/1000 | Loss: 0.00001537
Iteration 117/1000 | Loss: 0.00001537
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001537
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001537
Iteration 122/1000 | Loss: 0.00001537
Iteration 123/1000 | Loss: 0.00001537
Iteration 124/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.5373276255559176e-05, 1.5373276255559176e-05, 1.5373276255559176e-05, 1.5373276255559176e-05, 1.5373276255559176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5373276255559176e-05

Optimization complete. Final v2v error: 3.3201026916503906 mm

Highest mean error: 4.067173004150391 mm for frame 140

Lowest mean error: 3.0995898246765137 mm for frame 238

Saving results

Total time: 61.59530735015869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070492
Iteration 2/25 | Loss: 0.00245956
Iteration 3/25 | Loss: 0.00152540
Iteration 4/25 | Loss: 0.00125716
Iteration 5/25 | Loss: 0.00119257
Iteration 6/25 | Loss: 0.00109340
Iteration 7/25 | Loss: 0.00097173
Iteration 8/25 | Loss: 0.00092782
Iteration 9/25 | Loss: 0.00083614
Iteration 10/25 | Loss: 0.00083873
Iteration 11/25 | Loss: 0.00082898
Iteration 12/25 | Loss: 0.00080212
Iteration 13/25 | Loss: 0.00077808
Iteration 14/25 | Loss: 0.00076953
Iteration 15/25 | Loss: 0.00075421
Iteration 16/25 | Loss: 0.00075326
Iteration 17/25 | Loss: 0.00075400
Iteration 18/25 | Loss: 0.00075316
Iteration 19/25 | Loss: 0.00075404
Iteration 20/25 | Loss: 0.00075280
Iteration 21/25 | Loss: 0.00075329
Iteration 22/25 | Loss: 0.00075254
Iteration 23/25 | Loss: 0.00075253
Iteration 24/25 | Loss: 0.00075434
Iteration 25/25 | Loss: 0.00074887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48862135
Iteration 2/25 | Loss: 0.00113281
Iteration 3/25 | Loss: 0.00082393
Iteration 4/25 | Loss: 0.00082393
Iteration 5/25 | Loss: 0.00082393
Iteration 6/25 | Loss: 0.00082393
Iteration 7/25 | Loss: 0.00082393
Iteration 8/25 | Loss: 0.00082393
Iteration 9/25 | Loss: 0.00082393
Iteration 10/25 | Loss: 0.00082393
Iteration 11/25 | Loss: 0.00082393
Iteration 12/25 | Loss: 0.00082393
Iteration 13/25 | Loss: 0.00082393
Iteration 14/25 | Loss: 0.00082393
Iteration 15/25 | Loss: 0.00082393
Iteration 16/25 | Loss: 0.00082393
Iteration 17/25 | Loss: 0.00082393
Iteration 18/25 | Loss: 0.00082393
Iteration 19/25 | Loss: 0.00082393
Iteration 20/25 | Loss: 0.00082393
Iteration 21/25 | Loss: 0.00082393
Iteration 22/25 | Loss: 0.00082393
Iteration 23/25 | Loss: 0.00082393
Iteration 24/25 | Loss: 0.00082393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008239296148531139, 0.0008239296148531139, 0.0008239296148531139, 0.0008239296148531139, 0.0008239296148531139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008239296148531139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082393
Iteration 2/1000 | Loss: 0.00041431
Iteration 3/1000 | Loss: 0.00010623
Iteration 4/1000 | Loss: 0.00008221
Iteration 5/1000 | Loss: 0.00096289
Iteration 6/1000 | Loss: 0.00186170
Iteration 7/1000 | Loss: 0.00324430
Iteration 8/1000 | Loss: 0.00173698
Iteration 9/1000 | Loss: 0.00260154
Iteration 10/1000 | Loss: 0.00078121
Iteration 11/1000 | Loss: 0.00036608
Iteration 12/1000 | Loss: 0.00008529
Iteration 13/1000 | Loss: 0.00197945
Iteration 14/1000 | Loss: 0.00077136
Iteration 15/1000 | Loss: 0.00293118
Iteration 16/1000 | Loss: 0.00214434
Iteration 17/1000 | Loss: 0.00145491
Iteration 18/1000 | Loss: 0.00102494
Iteration 19/1000 | Loss: 0.00060984
Iteration 20/1000 | Loss: 0.00036158
Iteration 21/1000 | Loss: 0.00008277
Iteration 22/1000 | Loss: 0.00038081
Iteration 23/1000 | Loss: 0.00025573
Iteration 24/1000 | Loss: 0.00010009
Iteration 25/1000 | Loss: 0.00033208
Iteration 26/1000 | Loss: 0.00041028
Iteration 27/1000 | Loss: 0.00006426
Iteration 28/1000 | Loss: 0.00006733
Iteration 29/1000 | Loss: 0.00023078
Iteration 30/1000 | Loss: 0.00039907
Iteration 31/1000 | Loss: 0.00125561
Iteration 32/1000 | Loss: 0.00069782
Iteration 33/1000 | Loss: 0.00081521
Iteration 34/1000 | Loss: 0.00006694
Iteration 35/1000 | Loss: 0.00123789
Iteration 36/1000 | Loss: 0.00065187
Iteration 37/1000 | Loss: 0.00102329
Iteration 38/1000 | Loss: 0.00008013
Iteration 39/1000 | Loss: 0.00005070
Iteration 40/1000 | Loss: 0.00006295
Iteration 41/1000 | Loss: 0.00121651
Iteration 42/1000 | Loss: 0.00067850
Iteration 43/1000 | Loss: 0.00073811
Iteration 44/1000 | Loss: 0.00005862
Iteration 45/1000 | Loss: 0.00005885
Iteration 46/1000 | Loss: 0.00003556
Iteration 47/1000 | Loss: 0.00005849
Iteration 48/1000 | Loss: 0.00004584
Iteration 49/1000 | Loss: 0.00004027
Iteration 50/1000 | Loss: 0.00005736
Iteration 51/1000 | Loss: 0.00035830
Iteration 52/1000 | Loss: 0.00025701
Iteration 53/1000 | Loss: 0.00010225
Iteration 54/1000 | Loss: 0.00055386
Iteration 55/1000 | Loss: 0.00027132
Iteration 56/1000 | Loss: 0.00019444
Iteration 57/1000 | Loss: 0.00006620
Iteration 58/1000 | Loss: 0.00015397
Iteration 59/1000 | Loss: 0.00034922
Iteration 60/1000 | Loss: 0.00006690
Iteration 61/1000 | Loss: 0.00022868
Iteration 62/1000 | Loss: 0.00046790
Iteration 63/1000 | Loss: 0.00013489
Iteration 64/1000 | Loss: 0.00005361
Iteration 65/1000 | Loss: 0.00005843
Iteration 66/1000 | Loss: 0.00006459
Iteration 67/1000 | Loss: 0.00006562
Iteration 68/1000 | Loss: 0.00093668
Iteration 69/1000 | Loss: 0.00038837
Iteration 70/1000 | Loss: 0.00046493
Iteration 71/1000 | Loss: 0.00007410
Iteration 72/1000 | Loss: 0.00012673
Iteration 73/1000 | Loss: 0.00010570
Iteration 74/1000 | Loss: 0.00006895
Iteration 75/1000 | Loss: 0.00004986
Iteration 76/1000 | Loss: 0.00009709
Iteration 77/1000 | Loss: 0.00007696
Iteration 78/1000 | Loss: 0.00008639
Iteration 79/1000 | Loss: 0.00008915
Iteration 80/1000 | Loss: 0.00005241
Iteration 81/1000 | Loss: 0.00005520
Iteration 82/1000 | Loss: 0.00006693
Iteration 83/1000 | Loss: 0.00003330
Iteration 84/1000 | Loss: 0.00002750
Iteration 85/1000 | Loss: 0.00004505
Iteration 86/1000 | Loss: 0.00003544
Iteration 87/1000 | Loss: 0.00003062
Iteration 88/1000 | Loss: 0.00002835
Iteration 89/1000 | Loss: 0.00005637
Iteration 90/1000 | Loss: 0.00003830
Iteration 91/1000 | Loss: 0.00005463
Iteration 92/1000 | Loss: 0.00003380
Iteration 93/1000 | Loss: 0.00005397
Iteration 94/1000 | Loss: 0.00005693
Iteration 95/1000 | Loss: 0.00005586
Iteration 96/1000 | Loss: 0.00005481
Iteration 97/1000 | Loss: 0.00005563
Iteration 98/1000 | Loss: 0.00005509
Iteration 99/1000 | Loss: 0.00006222
Iteration 100/1000 | Loss: 0.00006550
Iteration 101/1000 | Loss: 0.00005687
Iteration 102/1000 | Loss: 0.00001895
Iteration 103/1000 | Loss: 0.00001629
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001341
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001302
Iteration 134/1000 | Loss: 0.00001302
Iteration 135/1000 | Loss: 0.00001302
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001302
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001298
Iteration 151/1000 | Loss: 0.00001298
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001297
Iteration 155/1000 | Loss: 0.00001296
Iteration 156/1000 | Loss: 0.00001296
Iteration 157/1000 | Loss: 0.00001296
Iteration 158/1000 | Loss: 0.00001296
Iteration 159/1000 | Loss: 0.00001295
Iteration 160/1000 | Loss: 0.00001295
Iteration 161/1000 | Loss: 0.00001295
Iteration 162/1000 | Loss: 0.00001295
Iteration 163/1000 | Loss: 0.00001295
Iteration 164/1000 | Loss: 0.00001295
Iteration 165/1000 | Loss: 0.00001295
Iteration 166/1000 | Loss: 0.00001295
Iteration 167/1000 | Loss: 0.00001295
Iteration 168/1000 | Loss: 0.00001294
Iteration 169/1000 | Loss: 0.00001294
Iteration 170/1000 | Loss: 0.00001294
Iteration 171/1000 | Loss: 0.00001294
Iteration 172/1000 | Loss: 0.00001293
Iteration 173/1000 | Loss: 0.00001293
Iteration 174/1000 | Loss: 0.00001293
Iteration 175/1000 | Loss: 0.00001293
Iteration 176/1000 | Loss: 0.00001293
Iteration 177/1000 | Loss: 0.00001293
Iteration 178/1000 | Loss: 0.00001293
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001292
Iteration 181/1000 | Loss: 0.00001292
Iteration 182/1000 | Loss: 0.00001292
Iteration 183/1000 | Loss: 0.00001292
Iteration 184/1000 | Loss: 0.00001292
Iteration 185/1000 | Loss: 0.00001291
Iteration 186/1000 | Loss: 0.00001291
Iteration 187/1000 | Loss: 0.00001291
Iteration 188/1000 | Loss: 0.00001290
Iteration 189/1000 | Loss: 0.00001290
Iteration 190/1000 | Loss: 0.00001289
Iteration 191/1000 | Loss: 0.00001289
Iteration 192/1000 | Loss: 0.00001289
Iteration 193/1000 | Loss: 0.00001289
Iteration 194/1000 | Loss: 0.00001289
Iteration 195/1000 | Loss: 0.00001288
Iteration 196/1000 | Loss: 0.00001288
Iteration 197/1000 | Loss: 0.00001287
Iteration 198/1000 | Loss: 0.00001287
Iteration 199/1000 | Loss: 0.00001287
Iteration 200/1000 | Loss: 0.00001287
Iteration 201/1000 | Loss: 0.00001287
Iteration 202/1000 | Loss: 0.00001286
Iteration 203/1000 | Loss: 0.00001286
Iteration 204/1000 | Loss: 0.00001286
Iteration 205/1000 | Loss: 0.00001286
Iteration 206/1000 | Loss: 0.00001286
Iteration 207/1000 | Loss: 0.00001286
Iteration 208/1000 | Loss: 0.00001286
Iteration 209/1000 | Loss: 0.00001286
Iteration 210/1000 | Loss: 0.00001286
Iteration 211/1000 | Loss: 0.00001286
Iteration 212/1000 | Loss: 0.00001286
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001286
Iteration 219/1000 | Loss: 0.00001285
Iteration 220/1000 | Loss: 0.00001285
Iteration 221/1000 | Loss: 0.00001285
Iteration 222/1000 | Loss: 0.00001285
Iteration 223/1000 | Loss: 0.00001285
Iteration 224/1000 | Loss: 0.00001285
Iteration 225/1000 | Loss: 0.00001285
Iteration 226/1000 | Loss: 0.00001285
Iteration 227/1000 | Loss: 0.00001285
Iteration 228/1000 | Loss: 0.00001285
Iteration 229/1000 | Loss: 0.00001285
Iteration 230/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.285459893551888e-05, 1.285459893551888e-05, 1.285459893551888e-05, 1.285459893551888e-05, 1.285459893551888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.285459893551888e-05

Optimization complete. Final v2v error: 3.009911060333252 mm

Highest mean error: 4.348639011383057 mm for frame 55

Lowest mean error: 2.6134719848632812 mm for frame 113

Saving results

Total time: 202.65258049964905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881104
Iteration 2/25 | Loss: 0.00146317
Iteration 3/25 | Loss: 0.00099816
Iteration 4/25 | Loss: 0.00090779
Iteration 5/25 | Loss: 0.00088857
Iteration 6/25 | Loss: 0.00084460
Iteration 7/25 | Loss: 0.00083845
Iteration 8/25 | Loss: 0.00082627
Iteration 9/25 | Loss: 0.00082121
Iteration 10/25 | Loss: 0.00080847
Iteration 11/25 | Loss: 0.00080497
Iteration 12/25 | Loss: 0.00080203
Iteration 13/25 | Loss: 0.00079764
Iteration 14/25 | Loss: 0.00079129
Iteration 15/25 | Loss: 0.00078760
Iteration 16/25 | Loss: 0.00079233
Iteration 17/25 | Loss: 0.00079101
Iteration 18/25 | Loss: 0.00078961
Iteration 19/25 | Loss: 0.00079046
Iteration 20/25 | Loss: 0.00079096
Iteration 21/25 | Loss: 0.00079152
Iteration 22/25 | Loss: 0.00079336
Iteration 23/25 | Loss: 0.00079240
Iteration 24/25 | Loss: 0.00079171
Iteration 25/25 | Loss: 0.00078947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87588441
Iteration 2/25 | Loss: 0.00070079
Iteration 3/25 | Loss: 0.00070076
Iteration 4/25 | Loss: 0.00070076
Iteration 5/25 | Loss: 0.00070076
Iteration 6/25 | Loss: 0.00070076
Iteration 7/25 | Loss: 0.00070076
Iteration 8/25 | Loss: 0.00070076
Iteration 9/25 | Loss: 0.00070076
Iteration 10/25 | Loss: 0.00070076
Iteration 11/25 | Loss: 0.00070076
Iteration 12/25 | Loss: 0.00070076
Iteration 13/25 | Loss: 0.00070076
Iteration 14/25 | Loss: 0.00070076
Iteration 15/25 | Loss: 0.00070076
Iteration 16/25 | Loss: 0.00070076
Iteration 17/25 | Loss: 0.00070076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007007608073763549, 0.0007007608073763549, 0.0007007608073763549, 0.0007007608073763549, 0.0007007608073763549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007007608073763549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070076
Iteration 2/1000 | Loss: 0.00024753
Iteration 3/1000 | Loss: 0.00026825
Iteration 4/1000 | Loss: 0.00041616
Iteration 5/1000 | Loss: 0.00042574
Iteration 6/1000 | Loss: 0.00019899
Iteration 7/1000 | Loss: 0.00031402
Iteration 8/1000 | Loss: 0.00032762
Iteration 9/1000 | Loss: 0.00025287
Iteration 10/1000 | Loss: 0.00033529
Iteration 11/1000 | Loss: 0.00017256
Iteration 12/1000 | Loss: 0.00029935
Iteration 13/1000 | Loss: 0.00069792
Iteration 14/1000 | Loss: 0.00037415
Iteration 15/1000 | Loss: 0.00033909
Iteration 16/1000 | Loss: 0.00043008
Iteration 17/1000 | Loss: 0.00017685
Iteration 18/1000 | Loss: 0.00024825
Iteration 19/1000 | Loss: 0.00024045
Iteration 20/1000 | Loss: 0.00059448
Iteration 21/1000 | Loss: 0.00062868
Iteration 22/1000 | Loss: 0.00027270
Iteration 23/1000 | Loss: 0.00028308
Iteration 24/1000 | Loss: 0.00015645
Iteration 25/1000 | Loss: 0.00015108
Iteration 26/1000 | Loss: 0.00011355
Iteration 27/1000 | Loss: 0.00064694
Iteration 28/1000 | Loss: 0.00050940
Iteration 29/1000 | Loss: 0.00056491
Iteration 30/1000 | Loss: 0.00040848
Iteration 31/1000 | Loss: 0.00036117
Iteration 32/1000 | Loss: 0.00054156
Iteration 33/1000 | Loss: 0.00021759
Iteration 34/1000 | Loss: 0.00024680
Iteration 35/1000 | Loss: 0.00024236
Iteration 36/1000 | Loss: 0.00032646
Iteration 37/1000 | Loss: 0.00047647
Iteration 38/1000 | Loss: 0.00060276
Iteration 39/1000 | Loss: 0.00018766
Iteration 40/1000 | Loss: 0.00030102
Iteration 41/1000 | Loss: 0.00022682
Iteration 42/1000 | Loss: 0.00027036
Iteration 43/1000 | Loss: 0.00029409
Iteration 44/1000 | Loss: 0.00030391
Iteration 45/1000 | Loss: 0.00048615
Iteration 46/1000 | Loss: 0.00026707
Iteration 47/1000 | Loss: 0.00034610
Iteration 48/1000 | Loss: 0.00018233
Iteration 49/1000 | Loss: 0.00007988
Iteration 50/1000 | Loss: 0.00029405
Iteration 51/1000 | Loss: 0.00008659
Iteration 52/1000 | Loss: 0.00018419
Iteration 53/1000 | Loss: 0.00028462
Iteration 54/1000 | Loss: 0.00017789
Iteration 55/1000 | Loss: 0.00028057
Iteration 56/1000 | Loss: 0.00031131
Iteration 57/1000 | Loss: 0.00025657
Iteration 58/1000 | Loss: 0.00030151
Iteration 59/1000 | Loss: 0.00021444
Iteration 60/1000 | Loss: 0.00020269
Iteration 61/1000 | Loss: 0.00011527
Iteration 62/1000 | Loss: 0.00020763
Iteration 63/1000 | Loss: 0.00013288
Iteration 64/1000 | Loss: 0.00018731
Iteration 65/1000 | Loss: 0.00016854
Iteration 66/1000 | Loss: 0.00024493
Iteration 67/1000 | Loss: 0.00019404
Iteration 68/1000 | Loss: 0.00024414
Iteration 69/1000 | Loss: 0.00020692
Iteration 70/1000 | Loss: 0.00025662
Iteration 71/1000 | Loss: 0.00021299
Iteration 72/1000 | Loss: 0.00019427
Iteration 73/1000 | Loss: 0.00018148
Iteration 74/1000 | Loss: 0.00026099
Iteration 75/1000 | Loss: 0.00024954
Iteration 76/1000 | Loss: 0.00016590
Iteration 77/1000 | Loss: 0.00015645
Iteration 78/1000 | Loss: 0.00050771
Iteration 79/1000 | Loss: 0.00029696
Iteration 80/1000 | Loss: 0.00025438
Iteration 81/1000 | Loss: 0.00021962
Iteration 82/1000 | Loss: 0.00027207
Iteration 83/1000 | Loss: 0.00024972
Iteration 84/1000 | Loss: 0.00016196
Iteration 85/1000 | Loss: 0.00005304
Iteration 86/1000 | Loss: 0.00004312
Iteration 87/1000 | Loss: 0.00003857
Iteration 88/1000 | Loss: 0.00063587
Iteration 89/1000 | Loss: 0.00006159
Iteration 90/1000 | Loss: 0.00003819
Iteration 91/1000 | Loss: 0.00003343
Iteration 92/1000 | Loss: 0.00003159
Iteration 93/1000 | Loss: 0.00003041
Iteration 94/1000 | Loss: 0.00002963
Iteration 95/1000 | Loss: 0.00002918
Iteration 96/1000 | Loss: 0.00002882
Iteration 97/1000 | Loss: 0.00002851
Iteration 98/1000 | Loss: 0.00002830
Iteration 99/1000 | Loss: 0.00002809
Iteration 100/1000 | Loss: 0.00002790
Iteration 101/1000 | Loss: 0.00002776
Iteration 102/1000 | Loss: 0.00002776
Iteration 103/1000 | Loss: 0.00002776
Iteration 104/1000 | Loss: 0.00002776
Iteration 105/1000 | Loss: 0.00002776
Iteration 106/1000 | Loss: 0.00002775
Iteration 107/1000 | Loss: 0.00002775
Iteration 108/1000 | Loss: 0.00002771
Iteration 109/1000 | Loss: 0.00002769
Iteration 110/1000 | Loss: 0.00002768
Iteration 111/1000 | Loss: 0.00002767
Iteration 112/1000 | Loss: 0.00002762
Iteration 113/1000 | Loss: 0.00002757
Iteration 114/1000 | Loss: 0.00002747
Iteration 115/1000 | Loss: 0.00002747
Iteration 116/1000 | Loss: 0.00002747
Iteration 117/1000 | Loss: 0.00002747
Iteration 118/1000 | Loss: 0.00002747
Iteration 119/1000 | Loss: 0.00002747
Iteration 120/1000 | Loss: 0.00002747
Iteration 121/1000 | Loss: 0.00002747
Iteration 122/1000 | Loss: 0.00002747
Iteration 123/1000 | Loss: 0.00002747
Iteration 124/1000 | Loss: 0.00002746
Iteration 125/1000 | Loss: 0.00002746
Iteration 126/1000 | Loss: 0.00002746
Iteration 127/1000 | Loss: 0.00002745
Iteration 128/1000 | Loss: 0.00002745
Iteration 129/1000 | Loss: 0.00002744
Iteration 130/1000 | Loss: 0.00002744
Iteration 131/1000 | Loss: 0.00002744
Iteration 132/1000 | Loss: 0.00002744
Iteration 133/1000 | Loss: 0.00002743
Iteration 134/1000 | Loss: 0.00002743
Iteration 135/1000 | Loss: 0.00002743
Iteration 136/1000 | Loss: 0.00002743
Iteration 137/1000 | Loss: 0.00002743
Iteration 138/1000 | Loss: 0.00002743
Iteration 139/1000 | Loss: 0.00002743
Iteration 140/1000 | Loss: 0.00002743
Iteration 141/1000 | Loss: 0.00002742
Iteration 142/1000 | Loss: 0.00002742
Iteration 143/1000 | Loss: 0.00002742
Iteration 144/1000 | Loss: 0.00002742
Iteration 145/1000 | Loss: 0.00002742
Iteration 146/1000 | Loss: 0.00002741
Iteration 147/1000 | Loss: 0.00002741
Iteration 148/1000 | Loss: 0.00002741
Iteration 149/1000 | Loss: 0.00002740
Iteration 150/1000 | Loss: 0.00002740
Iteration 151/1000 | Loss: 0.00002740
Iteration 152/1000 | Loss: 0.00002739
Iteration 153/1000 | Loss: 0.00002739
Iteration 154/1000 | Loss: 0.00002739
Iteration 155/1000 | Loss: 0.00002738
Iteration 156/1000 | Loss: 0.00002738
Iteration 157/1000 | Loss: 0.00002738
Iteration 158/1000 | Loss: 0.00002737
Iteration 159/1000 | Loss: 0.00002736
Iteration 160/1000 | Loss: 0.00002736
Iteration 161/1000 | Loss: 0.00002736
Iteration 162/1000 | Loss: 0.00002735
Iteration 163/1000 | Loss: 0.00002734
Iteration 164/1000 | Loss: 0.00002734
Iteration 165/1000 | Loss: 0.00002734
Iteration 166/1000 | Loss: 0.00002734
Iteration 167/1000 | Loss: 0.00002734
Iteration 168/1000 | Loss: 0.00002734
Iteration 169/1000 | Loss: 0.00002734
Iteration 170/1000 | Loss: 0.00002734
Iteration 171/1000 | Loss: 0.00002734
Iteration 172/1000 | Loss: 0.00002734
Iteration 173/1000 | Loss: 0.00002734
Iteration 174/1000 | Loss: 0.00002734
Iteration 175/1000 | Loss: 0.00002734
Iteration 176/1000 | Loss: 0.00002734
Iteration 177/1000 | Loss: 0.00002734
Iteration 178/1000 | Loss: 0.00002734
Iteration 179/1000 | Loss: 0.00002734
Iteration 180/1000 | Loss: 0.00002733
Iteration 181/1000 | Loss: 0.00002733
Iteration 182/1000 | Loss: 0.00002733
Iteration 183/1000 | Loss: 0.00002732
Iteration 184/1000 | Loss: 0.00002732
Iteration 185/1000 | Loss: 0.00002732
Iteration 186/1000 | Loss: 0.00002732
Iteration 187/1000 | Loss: 0.00002732
Iteration 188/1000 | Loss: 0.00002732
Iteration 189/1000 | Loss: 0.00002732
Iteration 190/1000 | Loss: 0.00002732
Iteration 191/1000 | Loss: 0.00002732
Iteration 192/1000 | Loss: 0.00002732
Iteration 193/1000 | Loss: 0.00002732
Iteration 194/1000 | Loss: 0.00002732
Iteration 195/1000 | Loss: 0.00002732
Iteration 196/1000 | Loss: 0.00002731
Iteration 197/1000 | Loss: 0.00002731
Iteration 198/1000 | Loss: 0.00002731
Iteration 199/1000 | Loss: 0.00002731
Iteration 200/1000 | Loss: 0.00002731
Iteration 201/1000 | Loss: 0.00002731
Iteration 202/1000 | Loss: 0.00002731
Iteration 203/1000 | Loss: 0.00002731
Iteration 204/1000 | Loss: 0.00002730
Iteration 205/1000 | Loss: 0.00002730
Iteration 206/1000 | Loss: 0.00002730
Iteration 207/1000 | Loss: 0.00002730
Iteration 208/1000 | Loss: 0.00002730
Iteration 209/1000 | Loss: 0.00002729
Iteration 210/1000 | Loss: 0.00002729
Iteration 211/1000 | Loss: 0.00002729
Iteration 212/1000 | Loss: 0.00002729
Iteration 213/1000 | Loss: 0.00002729
Iteration 214/1000 | Loss: 0.00002729
Iteration 215/1000 | Loss: 0.00002729
Iteration 216/1000 | Loss: 0.00002729
Iteration 217/1000 | Loss: 0.00002729
Iteration 218/1000 | Loss: 0.00002729
Iteration 219/1000 | Loss: 0.00002729
Iteration 220/1000 | Loss: 0.00002729
Iteration 221/1000 | Loss: 0.00002728
Iteration 222/1000 | Loss: 0.00002728
Iteration 223/1000 | Loss: 0.00002728
Iteration 224/1000 | Loss: 0.00002728
Iteration 225/1000 | Loss: 0.00002728
Iteration 226/1000 | Loss: 0.00002728
Iteration 227/1000 | Loss: 0.00002728
Iteration 228/1000 | Loss: 0.00002728
Iteration 229/1000 | Loss: 0.00002728
Iteration 230/1000 | Loss: 0.00002728
Iteration 231/1000 | Loss: 0.00002728
Iteration 232/1000 | Loss: 0.00002728
Iteration 233/1000 | Loss: 0.00002728
Iteration 234/1000 | Loss: 0.00002728
Iteration 235/1000 | Loss: 0.00002728
Iteration 236/1000 | Loss: 0.00002728
Iteration 237/1000 | Loss: 0.00002728
Iteration 238/1000 | Loss: 0.00002728
Iteration 239/1000 | Loss: 0.00002728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.7281977963866666e-05, 2.7281977963866666e-05, 2.7281977963866666e-05, 2.7281977963866666e-05, 2.7281977963866666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7281977963866666e-05

Optimization complete. Final v2v error: 4.355105876922607 mm

Highest mean error: 5.974325180053711 mm for frame 142

Lowest mean error: 3.388969659805298 mm for frame 0

Saving results

Total time: 226.24514389038086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070618
Iteration 2/25 | Loss: 0.00265502
Iteration 3/25 | Loss: 0.00166005
Iteration 4/25 | Loss: 0.00148557
Iteration 5/25 | Loss: 0.00134838
Iteration 6/25 | Loss: 0.00117225
Iteration 7/25 | Loss: 0.00111382
Iteration 8/25 | Loss: 0.00106070
Iteration 9/25 | Loss: 0.00103141
Iteration 10/25 | Loss: 0.00102316
Iteration 11/25 | Loss: 0.00100521
Iteration 12/25 | Loss: 0.00098770
Iteration 13/25 | Loss: 0.00097268
Iteration 14/25 | Loss: 0.00097250
Iteration 15/25 | Loss: 0.00097460
Iteration 16/25 | Loss: 0.00096556
Iteration 17/25 | Loss: 0.00095438
Iteration 18/25 | Loss: 0.00094973
Iteration 19/25 | Loss: 0.00094724
Iteration 20/25 | Loss: 0.00095406
Iteration 21/25 | Loss: 0.00095206
Iteration 22/25 | Loss: 0.00094507
Iteration 23/25 | Loss: 0.00094325
Iteration 24/25 | Loss: 0.00094258
Iteration 25/25 | Loss: 0.00094199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89737582
Iteration 2/25 | Loss: 0.00237256
Iteration 3/25 | Loss: 0.00237256
Iteration 4/25 | Loss: 0.00237256
Iteration 5/25 | Loss: 0.00237256
Iteration 6/25 | Loss: 0.00237256
Iteration 7/25 | Loss: 0.00237256
Iteration 8/25 | Loss: 0.00237256
Iteration 9/25 | Loss: 0.00237256
Iteration 10/25 | Loss: 0.00237256
Iteration 11/25 | Loss: 0.00237256
Iteration 12/25 | Loss: 0.00237256
Iteration 13/25 | Loss: 0.00237256
Iteration 14/25 | Loss: 0.00237256
Iteration 15/25 | Loss: 0.00237256
Iteration 16/25 | Loss: 0.00237256
Iteration 17/25 | Loss: 0.00237256
Iteration 18/25 | Loss: 0.00237256
Iteration 19/25 | Loss: 0.00237256
Iteration 20/25 | Loss: 0.00237256
Iteration 21/25 | Loss: 0.00237256
Iteration 22/25 | Loss: 0.00237256
Iteration 23/25 | Loss: 0.00237256
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002372561488300562, 0.002372561488300562, 0.002372561488300562, 0.002372561488300562, 0.002372561488300562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002372561488300562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237256
Iteration 2/1000 | Loss: 0.00113148
Iteration 3/1000 | Loss: 0.00080412
Iteration 4/1000 | Loss: 0.00228564
Iteration 5/1000 | Loss: 0.00039496
Iteration 6/1000 | Loss: 0.00022419
Iteration 7/1000 | Loss: 0.00091285
Iteration 8/1000 | Loss: 0.00061259
Iteration 9/1000 | Loss: 0.00018675
Iteration 10/1000 | Loss: 0.00016042
Iteration 11/1000 | Loss: 0.00027971
Iteration 12/1000 | Loss: 0.00014591
Iteration 13/1000 | Loss: 0.00013464
Iteration 14/1000 | Loss: 0.00012880
Iteration 15/1000 | Loss: 0.00020779
Iteration 16/1000 | Loss: 0.00045346
Iteration 17/1000 | Loss: 0.00012955
Iteration 18/1000 | Loss: 0.00012288
Iteration 19/1000 | Loss: 0.00128676
Iteration 20/1000 | Loss: 0.00016331
Iteration 21/1000 | Loss: 0.00075429
Iteration 22/1000 | Loss: 0.00030555
Iteration 23/1000 | Loss: 0.00055750
Iteration 24/1000 | Loss: 0.00012693
Iteration 25/1000 | Loss: 0.00011770
Iteration 26/1000 | Loss: 0.00011158
Iteration 27/1000 | Loss: 0.00016721
Iteration 28/1000 | Loss: 0.00029173
Iteration 29/1000 | Loss: 0.00013400
Iteration 30/1000 | Loss: 0.00011519
Iteration 31/1000 | Loss: 0.00010602
Iteration 32/1000 | Loss: 0.00009737
Iteration 33/1000 | Loss: 0.00009336
Iteration 34/1000 | Loss: 0.00022700
Iteration 35/1000 | Loss: 0.00008978
Iteration 36/1000 | Loss: 0.00008852
Iteration 37/1000 | Loss: 0.00008763
Iteration 38/1000 | Loss: 0.00008673
Iteration 39/1000 | Loss: 0.00008599
Iteration 40/1000 | Loss: 0.00008535
Iteration 41/1000 | Loss: 0.00008488
Iteration 42/1000 | Loss: 0.00008442
Iteration 43/1000 | Loss: 0.00008407
Iteration 44/1000 | Loss: 0.00008377
Iteration 45/1000 | Loss: 0.00008342
Iteration 46/1000 | Loss: 0.00008323
Iteration 47/1000 | Loss: 0.00008305
Iteration 48/1000 | Loss: 0.00008303
Iteration 49/1000 | Loss: 0.00008289
Iteration 50/1000 | Loss: 0.00008287
Iteration 51/1000 | Loss: 0.00008282
Iteration 52/1000 | Loss: 0.00008277
Iteration 53/1000 | Loss: 0.00008273
Iteration 54/1000 | Loss: 0.00008267
Iteration 55/1000 | Loss: 0.00008264
Iteration 56/1000 | Loss: 0.00008264
Iteration 57/1000 | Loss: 0.00008263
Iteration 58/1000 | Loss: 0.00008263
Iteration 59/1000 | Loss: 0.00008263
Iteration 60/1000 | Loss: 0.00008263
Iteration 61/1000 | Loss: 0.00008263
Iteration 62/1000 | Loss: 0.00008263
Iteration 63/1000 | Loss: 0.00008263
Iteration 64/1000 | Loss: 0.00008263
Iteration 65/1000 | Loss: 0.00008263
Iteration 66/1000 | Loss: 0.00008263
Iteration 67/1000 | Loss: 0.00008262
Iteration 68/1000 | Loss: 0.00008262
Iteration 69/1000 | Loss: 0.00008262
Iteration 70/1000 | Loss: 0.00008261
Iteration 71/1000 | Loss: 0.00008261
Iteration 72/1000 | Loss: 0.00008261
Iteration 73/1000 | Loss: 0.00008261
Iteration 74/1000 | Loss: 0.00008260
Iteration 75/1000 | Loss: 0.00008260
Iteration 76/1000 | Loss: 0.00008260
Iteration 77/1000 | Loss: 0.00008259
Iteration 78/1000 | Loss: 0.00008259
Iteration 79/1000 | Loss: 0.00008259
Iteration 80/1000 | Loss: 0.00008258
Iteration 81/1000 | Loss: 0.00008258
Iteration 82/1000 | Loss: 0.00008258
Iteration 83/1000 | Loss: 0.00008257
Iteration 84/1000 | Loss: 0.00008257
Iteration 85/1000 | Loss: 0.00008257
Iteration 86/1000 | Loss: 0.00008257
Iteration 87/1000 | Loss: 0.00008256
Iteration 88/1000 | Loss: 0.00008256
Iteration 89/1000 | Loss: 0.00008256
Iteration 90/1000 | Loss: 0.00008256
Iteration 91/1000 | Loss: 0.00008255
Iteration 92/1000 | Loss: 0.00008255
Iteration 93/1000 | Loss: 0.00008255
Iteration 94/1000 | Loss: 0.00008255
Iteration 95/1000 | Loss: 0.00008255
Iteration 96/1000 | Loss: 0.00008255
Iteration 97/1000 | Loss: 0.00008255
Iteration 98/1000 | Loss: 0.00008255
Iteration 99/1000 | Loss: 0.00008255
Iteration 100/1000 | Loss: 0.00008254
Iteration 101/1000 | Loss: 0.00008254
Iteration 102/1000 | Loss: 0.00008254
Iteration 103/1000 | Loss: 0.00008254
Iteration 104/1000 | Loss: 0.00008254
Iteration 105/1000 | Loss: 0.00008253
Iteration 106/1000 | Loss: 0.00008253
Iteration 107/1000 | Loss: 0.00008253
Iteration 108/1000 | Loss: 0.00008253
Iteration 109/1000 | Loss: 0.00008253
Iteration 110/1000 | Loss: 0.00008253
Iteration 111/1000 | Loss: 0.00008252
Iteration 112/1000 | Loss: 0.00008252
Iteration 113/1000 | Loss: 0.00008252
Iteration 114/1000 | Loss: 0.00008252
Iteration 115/1000 | Loss: 0.00008252
Iteration 116/1000 | Loss: 0.00008252
Iteration 117/1000 | Loss: 0.00008252
Iteration 118/1000 | Loss: 0.00008252
Iteration 119/1000 | Loss: 0.00008252
Iteration 120/1000 | Loss: 0.00008251
Iteration 121/1000 | Loss: 0.00008251
Iteration 122/1000 | Loss: 0.00008251
Iteration 123/1000 | Loss: 0.00008251
Iteration 124/1000 | Loss: 0.00008251
Iteration 125/1000 | Loss: 0.00008251
Iteration 126/1000 | Loss: 0.00008250
Iteration 127/1000 | Loss: 0.00008250
Iteration 128/1000 | Loss: 0.00008250
Iteration 129/1000 | Loss: 0.00008250
Iteration 130/1000 | Loss: 0.00008250
Iteration 131/1000 | Loss: 0.00008249
Iteration 132/1000 | Loss: 0.00008249
Iteration 133/1000 | Loss: 0.00008249
Iteration 134/1000 | Loss: 0.00008248
Iteration 135/1000 | Loss: 0.00008248
Iteration 136/1000 | Loss: 0.00008248
Iteration 137/1000 | Loss: 0.00008248
Iteration 138/1000 | Loss: 0.00008247
Iteration 139/1000 | Loss: 0.00008247
Iteration 140/1000 | Loss: 0.00008247
Iteration 141/1000 | Loss: 0.00008247
Iteration 142/1000 | Loss: 0.00008247
Iteration 143/1000 | Loss: 0.00008247
Iteration 144/1000 | Loss: 0.00008246
Iteration 145/1000 | Loss: 0.00008246
Iteration 146/1000 | Loss: 0.00008246
Iteration 147/1000 | Loss: 0.00008246
Iteration 148/1000 | Loss: 0.00008246
Iteration 149/1000 | Loss: 0.00008246
Iteration 150/1000 | Loss: 0.00008245
Iteration 151/1000 | Loss: 0.00008245
Iteration 152/1000 | Loss: 0.00008245
Iteration 153/1000 | Loss: 0.00008245
Iteration 154/1000 | Loss: 0.00008244
Iteration 155/1000 | Loss: 0.00008244
Iteration 156/1000 | Loss: 0.00008244
Iteration 157/1000 | Loss: 0.00008244
Iteration 158/1000 | Loss: 0.00008244
Iteration 159/1000 | Loss: 0.00008244
Iteration 160/1000 | Loss: 0.00008243
Iteration 161/1000 | Loss: 0.00008243
Iteration 162/1000 | Loss: 0.00008243
Iteration 163/1000 | Loss: 0.00008243
Iteration 164/1000 | Loss: 0.00008243
Iteration 165/1000 | Loss: 0.00008243
Iteration 166/1000 | Loss: 0.00008242
Iteration 167/1000 | Loss: 0.00008242
Iteration 168/1000 | Loss: 0.00008242
Iteration 169/1000 | Loss: 0.00008241
Iteration 170/1000 | Loss: 0.00008241
Iteration 171/1000 | Loss: 0.00008241
Iteration 172/1000 | Loss: 0.00008241
Iteration 173/1000 | Loss: 0.00008241
Iteration 174/1000 | Loss: 0.00008241
Iteration 175/1000 | Loss: 0.00008241
Iteration 176/1000 | Loss: 0.00008241
Iteration 177/1000 | Loss: 0.00008241
Iteration 178/1000 | Loss: 0.00008241
Iteration 179/1000 | Loss: 0.00008241
Iteration 180/1000 | Loss: 0.00008240
Iteration 181/1000 | Loss: 0.00008240
Iteration 182/1000 | Loss: 0.00008240
Iteration 183/1000 | Loss: 0.00008240
Iteration 184/1000 | Loss: 0.00008240
Iteration 185/1000 | Loss: 0.00008240
Iteration 186/1000 | Loss: 0.00008240
Iteration 187/1000 | Loss: 0.00008240
Iteration 188/1000 | Loss: 0.00008240
Iteration 189/1000 | Loss: 0.00008240
Iteration 190/1000 | Loss: 0.00008240
Iteration 191/1000 | Loss: 0.00008240
Iteration 192/1000 | Loss: 0.00008240
Iteration 193/1000 | Loss: 0.00008240
Iteration 194/1000 | Loss: 0.00008240
Iteration 195/1000 | Loss: 0.00008240
Iteration 196/1000 | Loss: 0.00008240
Iteration 197/1000 | Loss: 0.00008239
Iteration 198/1000 | Loss: 0.00008239
Iteration 199/1000 | Loss: 0.00008239
Iteration 200/1000 | Loss: 0.00008239
Iteration 201/1000 | Loss: 0.00008239
Iteration 202/1000 | Loss: 0.00008239
Iteration 203/1000 | Loss: 0.00008239
Iteration 204/1000 | Loss: 0.00008239
Iteration 205/1000 | Loss: 0.00008239
Iteration 206/1000 | Loss: 0.00008239
Iteration 207/1000 | Loss: 0.00008239
Iteration 208/1000 | Loss: 0.00008239
Iteration 209/1000 | Loss: 0.00008238
Iteration 210/1000 | Loss: 0.00008238
Iteration 211/1000 | Loss: 0.00008238
Iteration 212/1000 | Loss: 0.00008238
Iteration 213/1000 | Loss: 0.00008238
Iteration 214/1000 | Loss: 0.00008238
Iteration 215/1000 | Loss: 0.00008238
Iteration 216/1000 | Loss: 0.00008238
Iteration 217/1000 | Loss: 0.00008238
Iteration 218/1000 | Loss: 0.00008238
Iteration 219/1000 | Loss: 0.00008238
Iteration 220/1000 | Loss: 0.00008238
Iteration 221/1000 | Loss: 0.00008238
Iteration 222/1000 | Loss: 0.00008238
Iteration 223/1000 | Loss: 0.00008238
Iteration 224/1000 | Loss: 0.00008238
Iteration 225/1000 | Loss: 0.00008238
Iteration 226/1000 | Loss: 0.00008238
Iteration 227/1000 | Loss: 0.00008237
Iteration 228/1000 | Loss: 0.00008237
Iteration 229/1000 | Loss: 0.00008237
Iteration 230/1000 | Loss: 0.00008237
Iteration 231/1000 | Loss: 0.00008237
Iteration 232/1000 | Loss: 0.00008237
Iteration 233/1000 | Loss: 0.00008237
Iteration 234/1000 | Loss: 0.00008237
Iteration 235/1000 | Loss: 0.00008237
Iteration 236/1000 | Loss: 0.00008237
Iteration 237/1000 | Loss: 0.00008237
Iteration 238/1000 | Loss: 0.00008237
Iteration 239/1000 | Loss: 0.00008237
Iteration 240/1000 | Loss: 0.00008237
Iteration 241/1000 | Loss: 0.00008237
Iteration 242/1000 | Loss: 0.00008237
Iteration 243/1000 | Loss: 0.00008237
Iteration 244/1000 | Loss: 0.00008237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [8.237051224568859e-05, 8.237051224568859e-05, 8.237051224568859e-05, 8.237051224568859e-05, 8.237051224568859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.237051224568859e-05

Optimization complete. Final v2v error: 4.945095539093018 mm

Highest mean error: 11.942920684814453 mm for frame 53

Lowest mean error: 3.243589401245117 mm for frame 5

Saving results

Total time: 124.24111199378967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428049
Iteration 2/25 | Loss: 0.00082651
Iteration 3/25 | Loss: 0.00071439
Iteration 4/25 | Loss: 0.00069222
Iteration 5/25 | Loss: 0.00068459
Iteration 6/25 | Loss: 0.00068242
Iteration 7/25 | Loss: 0.00068145
Iteration 8/25 | Loss: 0.00068139
Iteration 9/25 | Loss: 0.00068139
Iteration 10/25 | Loss: 0.00068139
Iteration 11/25 | Loss: 0.00068139
Iteration 12/25 | Loss: 0.00068139
Iteration 13/25 | Loss: 0.00068139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006813919171690941, 0.0006813919171690941, 0.0006813919171690941, 0.0006813919171690941, 0.0006813919171690941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006813919171690941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85571134
Iteration 2/25 | Loss: 0.00030951
Iteration 3/25 | Loss: 0.00030951
Iteration 4/25 | Loss: 0.00030951
Iteration 5/25 | Loss: 0.00030951
Iteration 6/25 | Loss: 0.00030951
Iteration 7/25 | Loss: 0.00030951
Iteration 8/25 | Loss: 0.00030951
Iteration 9/25 | Loss: 0.00030951
Iteration 10/25 | Loss: 0.00030951
Iteration 11/25 | Loss: 0.00030951
Iteration 12/25 | Loss: 0.00030951
Iteration 13/25 | Loss: 0.00030951
Iteration 14/25 | Loss: 0.00030951
Iteration 15/25 | Loss: 0.00030951
Iteration 16/25 | Loss: 0.00030951
Iteration 17/25 | Loss: 0.00030951
Iteration 18/25 | Loss: 0.00030951
Iteration 19/25 | Loss: 0.00030951
Iteration 20/25 | Loss: 0.00030951
Iteration 21/25 | Loss: 0.00030951
Iteration 22/25 | Loss: 0.00030951
Iteration 23/25 | Loss: 0.00030951
Iteration 24/25 | Loss: 0.00030951
Iteration 25/25 | Loss: 0.00030951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030951
Iteration 2/1000 | Loss: 0.00003150
Iteration 3/1000 | Loss: 0.00002211
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00001848
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001842
Iteration 15/1000 | Loss: 0.00001842
Iteration 16/1000 | Loss: 0.00001842
Iteration 17/1000 | Loss: 0.00001842
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001838
Iteration 20/1000 | Loss: 0.00001838
Iteration 21/1000 | Loss: 0.00001837
Iteration 22/1000 | Loss: 0.00001837
Iteration 23/1000 | Loss: 0.00001836
Iteration 24/1000 | Loss: 0.00001836
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001832
Iteration 30/1000 | Loss: 0.00001831
Iteration 31/1000 | Loss: 0.00001830
Iteration 32/1000 | Loss: 0.00001826
Iteration 33/1000 | Loss: 0.00001826
Iteration 34/1000 | Loss: 0.00001826
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001824
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001823
Iteration 40/1000 | Loss: 0.00001823
Iteration 41/1000 | Loss: 0.00001822
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001822
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001821
Iteration 47/1000 | Loss: 0.00001821
Iteration 48/1000 | Loss: 0.00001821
Iteration 49/1000 | Loss: 0.00001820
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001820
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001819
Iteration 55/1000 | Loss: 0.00001819
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001818
Iteration 58/1000 | Loss: 0.00001818
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001814
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001814
Iteration 63/1000 | Loss: 0.00001813
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001813
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001811
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001810
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001807
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001807
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001805
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001804
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001803
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001803
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001801
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001801
Iteration 107/1000 | Loss: 0.00001801
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001800
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001800
Iteration 112/1000 | Loss: 0.00001800
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001800
Iteration 115/1000 | Loss: 0.00001800
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001800
Iteration 118/1000 | Loss: 0.00001800
Iteration 119/1000 | Loss: 0.00001800
Iteration 120/1000 | Loss: 0.00001800
Iteration 121/1000 | Loss: 0.00001800
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.800286008801777e-05, 1.800286008801777e-05, 1.800286008801777e-05, 1.800286008801777e-05, 1.800286008801777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.800286008801777e-05

Optimization complete. Final v2v error: 3.5987820625305176 mm

Highest mean error: 3.89809513092041 mm for frame 158

Lowest mean error: 3.3883302211761475 mm for frame 191

Saving results

Total time: 40.160221099853516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895921
Iteration 2/25 | Loss: 0.00083930
Iteration 3/25 | Loss: 0.00069982
Iteration 4/25 | Loss: 0.00067040
Iteration 5/25 | Loss: 0.00065884
Iteration 6/25 | Loss: 0.00065722
Iteration 7/25 | Loss: 0.00065683
Iteration 8/25 | Loss: 0.00065683
Iteration 9/25 | Loss: 0.00065681
Iteration 10/25 | Loss: 0.00065681
Iteration 11/25 | Loss: 0.00065681
Iteration 12/25 | Loss: 0.00065681
Iteration 13/25 | Loss: 0.00065681
Iteration 14/25 | Loss: 0.00065681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006568080862052739, 0.0006568080862052739, 0.0006568080862052739, 0.0006568080862052739, 0.0006568080862052739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006568080862052739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.57807493
Iteration 2/25 | Loss: 0.00024806
Iteration 3/25 | Loss: 0.00024806
Iteration 4/25 | Loss: 0.00024806
Iteration 5/25 | Loss: 0.00024806
Iteration 6/25 | Loss: 0.00024806
Iteration 7/25 | Loss: 0.00024806
Iteration 8/25 | Loss: 0.00024806
Iteration 9/25 | Loss: 0.00024806
Iteration 10/25 | Loss: 0.00024806
Iteration 11/25 | Loss: 0.00024806
Iteration 12/25 | Loss: 0.00024806
Iteration 13/25 | Loss: 0.00024806
Iteration 14/25 | Loss: 0.00024806
Iteration 15/25 | Loss: 0.00024806
Iteration 16/25 | Loss: 0.00024806
Iteration 17/25 | Loss: 0.00024806
Iteration 18/25 | Loss: 0.00024806
Iteration 19/25 | Loss: 0.00024806
Iteration 20/25 | Loss: 0.00024806
Iteration 21/25 | Loss: 0.00024806
Iteration 22/25 | Loss: 0.00024806
Iteration 23/25 | Loss: 0.00024806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002480575640220195, 0.0002480575640220195, 0.0002480575640220195, 0.0002480575640220195, 0.0002480575640220195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002480575640220195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024806
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00002220
Iteration 4/1000 | Loss: 0.00002083
Iteration 5/1000 | Loss: 0.00001953
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001861
Iteration 8/1000 | Loss: 0.00001835
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001817
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001808
Iteration 14/1000 | Loss: 0.00001808
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001792
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001791
Iteration 34/1000 | Loss: 0.00001791
Iteration 35/1000 | Loss: 0.00001791
Iteration 36/1000 | Loss: 0.00001790
Iteration 37/1000 | Loss: 0.00001790
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.7831900549936108e-05, 1.7831900549936108e-05, 1.7831900549936108e-05, 1.7831900549936108e-05, 1.7831900549936108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7831900549936108e-05

Optimization complete. Final v2v error: 3.582899808883667 mm

Highest mean error: 4.063967704772949 mm for frame 180

Lowest mean error: 3.3133246898651123 mm for frame 11

Saving results

Total time: 29.68674325942993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021675
Iteration 2/25 | Loss: 0.00413913
Iteration 3/25 | Loss: 0.00307781
Iteration 4/25 | Loss: 0.00238225
Iteration 5/25 | Loss: 0.00220055
Iteration 6/25 | Loss: 0.00219087
Iteration 7/25 | Loss: 0.00197994
Iteration 8/25 | Loss: 0.00208010
Iteration 9/25 | Loss: 0.00190275
Iteration 10/25 | Loss: 0.00179732
Iteration 11/25 | Loss: 0.00169892
Iteration 12/25 | Loss: 0.00154688
Iteration 13/25 | Loss: 0.00144907
Iteration 14/25 | Loss: 0.00145284
Iteration 15/25 | Loss: 0.00144018
Iteration 16/25 | Loss: 0.00140269
Iteration 17/25 | Loss: 0.00136074
Iteration 18/25 | Loss: 0.00139062
Iteration 19/25 | Loss: 0.00135560
Iteration 20/25 | Loss: 0.00136794
Iteration 21/25 | Loss: 0.00136453
Iteration 22/25 | Loss: 0.00136118
Iteration 23/25 | Loss: 0.00132282
Iteration 24/25 | Loss: 0.00136617
Iteration 25/25 | Loss: 0.00132491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22776318
Iteration 2/25 | Loss: 0.03067833
Iteration 3/25 | Loss: 0.01934203
Iteration 4/25 | Loss: 0.00982380
Iteration 5/25 | Loss: 0.00847959
Iteration 6/25 | Loss: 0.00847959
Iteration 7/25 | Loss: 0.00847959
Iteration 8/25 | Loss: 0.00847959
Iteration 9/25 | Loss: 0.00847959
Iteration 10/25 | Loss: 0.00847959
Iteration 11/25 | Loss: 0.00847959
Iteration 12/25 | Loss: 0.00847959
Iteration 13/25 | Loss: 0.00847959
Iteration 14/25 | Loss: 0.00847959
Iteration 15/25 | Loss: 0.00847959
Iteration 16/25 | Loss: 0.00847959
Iteration 17/25 | Loss: 0.00847959
Iteration 18/25 | Loss: 0.00847959
Iteration 19/25 | Loss: 0.00847959
Iteration 20/25 | Loss: 0.00847959
Iteration 21/25 | Loss: 0.00847959
Iteration 22/25 | Loss: 0.00847959
Iteration 23/25 | Loss: 0.00847959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.008479587733745575, 0.008479587733745575, 0.008479587733745575, 0.008479587733745575, 0.008479587733745575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008479587733745575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00847959
Iteration 2/1000 | Loss: 0.00783863
Iteration 3/1000 | Loss: 0.00750789
Iteration 4/1000 | Loss: 0.01265098
Iteration 5/1000 | Loss: 0.00869702
Iteration 6/1000 | Loss: 0.01272977
Iteration 7/1000 | Loss: 0.01190798
Iteration 8/1000 | Loss: 0.01033805
Iteration 9/1000 | Loss: 0.01300961
Iteration 10/1000 | Loss: 0.01032673
Iteration 11/1000 | Loss: 0.01111926
Iteration 12/1000 | Loss: 0.01786162
Iteration 13/1000 | Loss: 0.01375971
Iteration 14/1000 | Loss: 0.01452364
Iteration 15/1000 | Loss: 0.01122153
Iteration 16/1000 | Loss: 0.00654232
Iteration 17/1000 | Loss: 0.00916842
Iteration 18/1000 | Loss: 0.01597800
Iteration 19/1000 | Loss: 0.01190102
Iteration 20/1000 | Loss: 0.00509337
Iteration 21/1000 | Loss: 0.01011893
Iteration 22/1000 | Loss: 0.00677851
Iteration 23/1000 | Loss: 0.01007354
Iteration 24/1000 | Loss: 0.00663796
Iteration 25/1000 | Loss: 0.01870487
Iteration 26/1000 | Loss: 0.01169966
Iteration 27/1000 | Loss: 0.00914455
Iteration 28/1000 | Loss: 0.00794973
Iteration 29/1000 | Loss: 0.00935258
Iteration 30/1000 | Loss: 0.00683745
Iteration 31/1000 | Loss: 0.01953190
Iteration 32/1000 | Loss: 0.00759094
Iteration 33/1000 | Loss: 0.01387797
Iteration 34/1000 | Loss: 0.00669382
Iteration 35/1000 | Loss: 0.00818445
Iteration 36/1000 | Loss: 0.00745372
Iteration 37/1000 | Loss: 0.00883823
Iteration 38/1000 | Loss: 0.00817851
Iteration 39/1000 | Loss: 0.00807405
Iteration 40/1000 | Loss: 0.00833514
Iteration 41/1000 | Loss: 0.00554424
Iteration 42/1000 | Loss: 0.00809655
Iteration 43/1000 | Loss: 0.00914072
Iteration 44/1000 | Loss: 0.00863921
Iteration 45/1000 | Loss: 0.00948703
Iteration 46/1000 | Loss: 0.00734821
Iteration 47/1000 | Loss: 0.01244332
Iteration 48/1000 | Loss: 0.01112506
Iteration 49/1000 | Loss: 0.00683728
Iteration 50/1000 | Loss: 0.01665170
Iteration 51/1000 | Loss: 0.00969288
Iteration 52/1000 | Loss: 0.00523857
Iteration 53/1000 | Loss: 0.00905527
Iteration 54/1000 | Loss: 0.01117571
Iteration 55/1000 | Loss: 0.01184967
Iteration 56/1000 | Loss: 0.01391780
Iteration 57/1000 | Loss: 0.01010553
Iteration 58/1000 | Loss: 0.01131775
Iteration 59/1000 | Loss: 0.01351186
Iteration 60/1000 | Loss: 0.00518580
Iteration 61/1000 | Loss: 0.01626924
Iteration 62/1000 | Loss: 0.00914364
Iteration 63/1000 | Loss: 0.00928615
Iteration 64/1000 | Loss: 0.00704849
Iteration 65/1000 | Loss: 0.00798201
Iteration 66/1000 | Loss: 0.00790475
Iteration 67/1000 | Loss: 0.00761336
Iteration 68/1000 | Loss: 0.01086498
Iteration 69/1000 | Loss: 0.00735868
Iteration 70/1000 | Loss: 0.00535487
Iteration 71/1000 | Loss: 0.00858455
Iteration 72/1000 | Loss: 0.00702696
Iteration 73/1000 | Loss: 0.00916856
Iteration 74/1000 | Loss: 0.00391498
Iteration 75/1000 | Loss: 0.00441006
Iteration 76/1000 | Loss: 0.00525668
Iteration 77/1000 | Loss: 0.00610027
Iteration 78/1000 | Loss: 0.00928512
Iteration 79/1000 | Loss: 0.00598715
Iteration 80/1000 | Loss: 0.00431378
Iteration 81/1000 | Loss: 0.00458904
Iteration 82/1000 | Loss: 0.01566271
Iteration 83/1000 | Loss: 0.01218378
Iteration 84/1000 | Loss: 0.00599376
Iteration 85/1000 | Loss: 0.01241876
Iteration 86/1000 | Loss: 0.00512853
Iteration 87/1000 | Loss: 0.00468546
Iteration 88/1000 | Loss: 0.01397908
Iteration 89/1000 | Loss: 0.00697040
Iteration 90/1000 | Loss: 0.01143030
Iteration 91/1000 | Loss: 0.00917436
Iteration 92/1000 | Loss: 0.00795073
Iteration 93/1000 | Loss: 0.00612667
Iteration 94/1000 | Loss: 0.00753647
Iteration 95/1000 | Loss: 0.00614722
Iteration 96/1000 | Loss: 0.00612494
Iteration 97/1000 | Loss: 0.00786711
Iteration 98/1000 | Loss: 0.00803141
Iteration 99/1000 | Loss: 0.00941997
Iteration 100/1000 | Loss: 0.00650174
Iteration 101/1000 | Loss: 0.00743380
Iteration 102/1000 | Loss: 0.00553205
Iteration 103/1000 | Loss: 0.00665512
Iteration 104/1000 | Loss: 0.00558613
Iteration 105/1000 | Loss: 0.00537253
Iteration 106/1000 | Loss: 0.00544563
Iteration 107/1000 | Loss: 0.01274122
Iteration 108/1000 | Loss: 0.00604742
Iteration 109/1000 | Loss: 0.00602858
Iteration 110/1000 | Loss: 0.00575276
Iteration 111/1000 | Loss: 0.00518831
Iteration 112/1000 | Loss: 0.00741449
Iteration 113/1000 | Loss: 0.00616513
Iteration 114/1000 | Loss: 0.00568903
Iteration 115/1000 | Loss: 0.00430111
Iteration 116/1000 | Loss: 0.00460392
Iteration 117/1000 | Loss: 0.00625037
Iteration 118/1000 | Loss: 0.00470136
Iteration 119/1000 | Loss: 0.00971039
Iteration 120/1000 | Loss: 0.00506733
Iteration 121/1000 | Loss: 0.00703144
Iteration 122/1000 | Loss: 0.00446115
Iteration 123/1000 | Loss: 0.00667865
Iteration 124/1000 | Loss: 0.00711313
Iteration 125/1000 | Loss: 0.01055804
Iteration 126/1000 | Loss: 0.00621630
Iteration 127/1000 | Loss: 0.00503949
Iteration 128/1000 | Loss: 0.00984544
Iteration 129/1000 | Loss: 0.00492655
Iteration 130/1000 | Loss: 0.00940437
Iteration 131/1000 | Loss: 0.00591580
Iteration 132/1000 | Loss: 0.00520216
Iteration 133/1000 | Loss: 0.00632982
Iteration 134/1000 | Loss: 0.00581066
Iteration 135/1000 | Loss: 0.00463637
Iteration 136/1000 | Loss: 0.00644942
Iteration 137/1000 | Loss: 0.00686965
Iteration 138/1000 | Loss: 0.00963350
Iteration 139/1000 | Loss: 0.00978442
Iteration 140/1000 | Loss: 0.00545022
Iteration 141/1000 | Loss: 0.00450897
Iteration 142/1000 | Loss: 0.00839173
Iteration 143/1000 | Loss: 0.00642663
Iteration 144/1000 | Loss: 0.00670716
Iteration 145/1000 | Loss: 0.00985736
Iteration 146/1000 | Loss: 0.00808593
Iteration 147/1000 | Loss: 0.00857574
Iteration 148/1000 | Loss: 0.00707457
Iteration 149/1000 | Loss: 0.01501323
Iteration 150/1000 | Loss: 0.00762324
Iteration 151/1000 | Loss: 0.00920356
Iteration 152/1000 | Loss: 0.00638794
Iteration 153/1000 | Loss: 0.00681330
Iteration 154/1000 | Loss: 0.01016912
Iteration 155/1000 | Loss: 0.00564238
Iteration 156/1000 | Loss: 0.00417781
Iteration 157/1000 | Loss: 0.00890689
Iteration 158/1000 | Loss: 0.00435978
Iteration 159/1000 | Loss: 0.00608271
Iteration 160/1000 | Loss: 0.00590190
Iteration 161/1000 | Loss: 0.00492240
Iteration 162/1000 | Loss: 0.00381612
Iteration 163/1000 | Loss: 0.00616196
Iteration 164/1000 | Loss: 0.00440272
Iteration 165/1000 | Loss: 0.00522083
Iteration 166/1000 | Loss: 0.00346592
Iteration 167/1000 | Loss: 0.00615004
Iteration 168/1000 | Loss: 0.00325259
Iteration 169/1000 | Loss: 0.00354506
Iteration 170/1000 | Loss: 0.00467557
Iteration 171/1000 | Loss: 0.00374410
Iteration 172/1000 | Loss: 0.00321416
Iteration 173/1000 | Loss: 0.00299791
Iteration 174/1000 | Loss: 0.00312164
Iteration 175/1000 | Loss: 0.00370420
Iteration 176/1000 | Loss: 0.00617815
Iteration 177/1000 | Loss: 0.00451461
Iteration 178/1000 | Loss: 0.00456461
Iteration 179/1000 | Loss: 0.00382970
Iteration 180/1000 | Loss: 0.00439340
Iteration 181/1000 | Loss: 0.00618164
Iteration 182/1000 | Loss: 0.00610754
Iteration 183/1000 | Loss: 0.00457337
Iteration 184/1000 | Loss: 0.00400178
Iteration 185/1000 | Loss: 0.00346777
Iteration 186/1000 | Loss: 0.00413212
Iteration 187/1000 | Loss: 0.00245368
Iteration 188/1000 | Loss: 0.00532169
Iteration 189/1000 | Loss: 0.00250561
Iteration 190/1000 | Loss: 0.00457516
Iteration 191/1000 | Loss: 0.00941982
Iteration 192/1000 | Loss: 0.00727576
Iteration 193/1000 | Loss: 0.00356577
Iteration 194/1000 | Loss: 0.00898761
Iteration 195/1000 | Loss: 0.00408310
Iteration 196/1000 | Loss: 0.00326078
Iteration 197/1000 | Loss: 0.00191093
Iteration 198/1000 | Loss: 0.00698262
Iteration 199/1000 | Loss: 0.00385767
Iteration 200/1000 | Loss: 0.00603309
Iteration 201/1000 | Loss: 0.00359374
Iteration 202/1000 | Loss: 0.00283502
Iteration 203/1000 | Loss: 0.00968265
Iteration 204/1000 | Loss: 0.00299812
Iteration 205/1000 | Loss: 0.00267536
Iteration 206/1000 | Loss: 0.00233383
Iteration 207/1000 | Loss: 0.00265941
Iteration 208/1000 | Loss: 0.00216597
Iteration 209/1000 | Loss: 0.00806146
Iteration 210/1000 | Loss: 0.00479334
Iteration 211/1000 | Loss: 0.00401292
Iteration 212/1000 | Loss: 0.00267832
Iteration 213/1000 | Loss: 0.00448726
Iteration 214/1000 | Loss: 0.00773523
Iteration 215/1000 | Loss: 0.00256513
Iteration 216/1000 | Loss: 0.00584153
Iteration 217/1000 | Loss: 0.00394473
Iteration 218/1000 | Loss: 0.00123399
Iteration 219/1000 | Loss: 0.00355573
Iteration 220/1000 | Loss: 0.00179421
Iteration 221/1000 | Loss: 0.00104121
Iteration 222/1000 | Loss: 0.00188234
Iteration 223/1000 | Loss: 0.00173534
Iteration 224/1000 | Loss: 0.00216541
Iteration 225/1000 | Loss: 0.00156914
Iteration 226/1000 | Loss: 0.00185664
Iteration 227/1000 | Loss: 0.00167613
Iteration 228/1000 | Loss: 0.00136816
Iteration 229/1000 | Loss: 0.00125902
Iteration 230/1000 | Loss: 0.00123155
Iteration 231/1000 | Loss: 0.00170781
Iteration 232/1000 | Loss: 0.00086537
Iteration 233/1000 | Loss: 0.00102818
Iteration 234/1000 | Loss: 0.00172807
Iteration 235/1000 | Loss: 0.00309916
Iteration 236/1000 | Loss: 0.00131559
Iteration 237/1000 | Loss: 0.00092797
Iteration 238/1000 | Loss: 0.00367293
Iteration 239/1000 | Loss: 0.00102105
Iteration 240/1000 | Loss: 0.00308997
Iteration 241/1000 | Loss: 0.00101858
Iteration 242/1000 | Loss: 0.00056319
Iteration 243/1000 | Loss: 0.00113119
Iteration 244/1000 | Loss: 0.00043872
Iteration 245/1000 | Loss: 0.00052166
Iteration 246/1000 | Loss: 0.00113260
Iteration 247/1000 | Loss: 0.00071739
Iteration 248/1000 | Loss: 0.00032794
Iteration 249/1000 | Loss: 0.00149377
Iteration 250/1000 | Loss: 0.00052928
Iteration 251/1000 | Loss: 0.00017365
Iteration 252/1000 | Loss: 0.00023457
Iteration 253/1000 | Loss: 0.00024129
Iteration 254/1000 | Loss: 0.00024471
Iteration 255/1000 | Loss: 0.00029686
Iteration 256/1000 | Loss: 0.00007841
Iteration 257/1000 | Loss: 0.00414538
Iteration 258/1000 | Loss: 0.00094982
Iteration 259/1000 | Loss: 0.00011826
Iteration 260/1000 | Loss: 0.00006872
Iteration 261/1000 | Loss: 0.00004879
Iteration 262/1000 | Loss: 0.00437630
Iteration 263/1000 | Loss: 0.00120444
Iteration 264/1000 | Loss: 0.00132269
Iteration 265/1000 | Loss: 0.00005201
Iteration 266/1000 | Loss: 0.00221393
Iteration 267/1000 | Loss: 0.00005303
Iteration 268/1000 | Loss: 0.00003686
Iteration 269/1000 | Loss: 0.00003249
Iteration 270/1000 | Loss: 0.00085957
Iteration 271/1000 | Loss: 0.00040103
Iteration 272/1000 | Loss: 0.00050866
Iteration 273/1000 | Loss: 0.00005119
Iteration 274/1000 | Loss: 0.00003367
Iteration 275/1000 | Loss: 0.00002860
Iteration 276/1000 | Loss: 0.00002654
Iteration 277/1000 | Loss: 0.00002514
Iteration 278/1000 | Loss: 0.00002397
Iteration 279/1000 | Loss: 0.00191932
Iteration 280/1000 | Loss: 0.00002427
Iteration 281/1000 | Loss: 0.00002196
Iteration 282/1000 | Loss: 0.00002134
Iteration 283/1000 | Loss: 0.00002088
Iteration 284/1000 | Loss: 0.00002058
Iteration 285/1000 | Loss: 0.00002024
Iteration 286/1000 | Loss: 0.00001995
Iteration 287/1000 | Loss: 0.00001964
Iteration 288/1000 | Loss: 0.00001952
Iteration 289/1000 | Loss: 0.00001943
Iteration 290/1000 | Loss: 0.00001940
Iteration 291/1000 | Loss: 0.00001932
Iteration 292/1000 | Loss: 0.00001931
Iteration 293/1000 | Loss: 0.00001930
Iteration 294/1000 | Loss: 0.00001930
Iteration 295/1000 | Loss: 0.00001928
Iteration 296/1000 | Loss: 0.00001927
Iteration 297/1000 | Loss: 0.00001927
Iteration 298/1000 | Loss: 0.00001926
Iteration 299/1000 | Loss: 0.00001925
Iteration 300/1000 | Loss: 0.00001924
Iteration 301/1000 | Loss: 0.00001923
Iteration 302/1000 | Loss: 0.00001920
Iteration 303/1000 | Loss: 0.00001920
Iteration 304/1000 | Loss: 0.00001919
Iteration 305/1000 | Loss: 0.00001918
Iteration 306/1000 | Loss: 0.00001918
Iteration 307/1000 | Loss: 0.00001917
Iteration 308/1000 | Loss: 0.00001917
Iteration 309/1000 | Loss: 0.00001912
Iteration 310/1000 | Loss: 0.00001910
Iteration 311/1000 | Loss: 0.00001910
Iteration 312/1000 | Loss: 0.00001910
Iteration 313/1000 | Loss: 0.00001910
Iteration 314/1000 | Loss: 0.00001910
Iteration 315/1000 | Loss: 0.00001909
Iteration 316/1000 | Loss: 0.00001909
Iteration 317/1000 | Loss: 0.00001907
Iteration 318/1000 | Loss: 0.00001907
Iteration 319/1000 | Loss: 0.00001906
Iteration 320/1000 | Loss: 0.00001906
Iteration 321/1000 | Loss: 0.00001906
Iteration 322/1000 | Loss: 0.00001906
Iteration 323/1000 | Loss: 0.00001906
Iteration 324/1000 | Loss: 0.00001906
Iteration 325/1000 | Loss: 0.00001906
Iteration 326/1000 | Loss: 0.00001906
Iteration 327/1000 | Loss: 0.00001906
Iteration 328/1000 | Loss: 0.00001905
Iteration 329/1000 | Loss: 0.00001905
Iteration 330/1000 | Loss: 0.00001904
Iteration 331/1000 | Loss: 0.00001903
Iteration 332/1000 | Loss: 0.00001903
Iteration 333/1000 | Loss: 0.00001902
Iteration 334/1000 | Loss: 0.00001902
Iteration 335/1000 | Loss: 0.00001902
Iteration 336/1000 | Loss: 0.00001902
Iteration 337/1000 | Loss: 0.00001901
Iteration 338/1000 | Loss: 0.00001901
Iteration 339/1000 | Loss: 0.00001901
Iteration 340/1000 | Loss: 0.00001900
Iteration 341/1000 | Loss: 0.00001900
Iteration 342/1000 | Loss: 0.00001900
Iteration 343/1000 | Loss: 0.00001900
Iteration 344/1000 | Loss: 0.00001899
Iteration 345/1000 | Loss: 0.00001899
Iteration 346/1000 | Loss: 0.00001899
Iteration 347/1000 | Loss: 0.00001898
Iteration 348/1000 | Loss: 0.00001898
Iteration 349/1000 | Loss: 0.00001897
Iteration 350/1000 | Loss: 0.00001897
Iteration 351/1000 | Loss: 0.00001897
Iteration 352/1000 | Loss: 0.00001896
Iteration 353/1000 | Loss: 0.00001896
Iteration 354/1000 | Loss: 0.00001896
Iteration 355/1000 | Loss: 0.00001895
Iteration 356/1000 | Loss: 0.00001895
Iteration 357/1000 | Loss: 0.00001895
Iteration 358/1000 | Loss: 0.00001894
Iteration 359/1000 | Loss: 0.00001894
Iteration 360/1000 | Loss: 0.00001894
Iteration 361/1000 | Loss: 0.00001894
Iteration 362/1000 | Loss: 0.00001894
Iteration 363/1000 | Loss: 0.00001893
Iteration 364/1000 | Loss: 0.00001893
Iteration 365/1000 | Loss: 0.00001892
Iteration 366/1000 | Loss: 0.00001892
Iteration 367/1000 | Loss: 0.00001892
Iteration 368/1000 | Loss: 0.00001891
Iteration 369/1000 | Loss: 0.00001891
Iteration 370/1000 | Loss: 0.00001891
Iteration 371/1000 | Loss: 0.00001891
Iteration 372/1000 | Loss: 0.00001890
Iteration 373/1000 | Loss: 0.00001890
Iteration 374/1000 | Loss: 0.00001890
Iteration 375/1000 | Loss: 0.00001890
Iteration 376/1000 | Loss: 0.00001889
Iteration 377/1000 | Loss: 0.00001889
Iteration 378/1000 | Loss: 0.00001889
Iteration 379/1000 | Loss: 0.00001889
Iteration 380/1000 | Loss: 0.00001889
Iteration 381/1000 | Loss: 0.00001889
Iteration 382/1000 | Loss: 0.00001888
Iteration 383/1000 | Loss: 0.00001888
Iteration 384/1000 | Loss: 0.00001888
Iteration 385/1000 | Loss: 0.00001888
Iteration 386/1000 | Loss: 0.00001888
Iteration 387/1000 | Loss: 0.00001887
Iteration 388/1000 | Loss: 0.00001887
Iteration 389/1000 | Loss: 0.00001887
Iteration 390/1000 | Loss: 0.00001887
Iteration 391/1000 | Loss: 0.00001886
Iteration 392/1000 | Loss: 0.00001886
Iteration 393/1000 | Loss: 0.00001886
Iteration 394/1000 | Loss: 0.00001886
Iteration 395/1000 | Loss: 0.00001885
Iteration 396/1000 | Loss: 0.00001885
Iteration 397/1000 | Loss: 0.00001885
Iteration 398/1000 | Loss: 0.00001884
Iteration 399/1000 | Loss: 0.00001884
Iteration 400/1000 | Loss: 0.00001884
Iteration 401/1000 | Loss: 0.00001884
Iteration 402/1000 | Loss: 0.00001884
Iteration 403/1000 | Loss: 0.00001884
Iteration 404/1000 | Loss: 0.00001884
Iteration 405/1000 | Loss: 0.00001883
Iteration 406/1000 | Loss: 0.00001883
Iteration 407/1000 | Loss: 0.00001883
Iteration 408/1000 | Loss: 0.00001883
Iteration 409/1000 | Loss: 0.00001883
Iteration 410/1000 | Loss: 0.00001883
Iteration 411/1000 | Loss: 0.00001883
Iteration 412/1000 | Loss: 0.00001883
Iteration 413/1000 | Loss: 0.00001883
Iteration 414/1000 | Loss: 0.00001883
Iteration 415/1000 | Loss: 0.00001883
Iteration 416/1000 | Loss: 0.00001882
Iteration 417/1000 | Loss: 0.00001882
Iteration 418/1000 | Loss: 0.00001882
Iteration 419/1000 | Loss: 0.00001882
Iteration 420/1000 | Loss: 0.00001882
Iteration 421/1000 | Loss: 0.00001882
Iteration 422/1000 | Loss: 0.00001882
Iteration 423/1000 | Loss: 0.00001882
Iteration 424/1000 | Loss: 0.00001882
Iteration 425/1000 | Loss: 0.00001882
Iteration 426/1000 | Loss: 0.00001882
Iteration 427/1000 | Loss: 0.00001882
Iteration 428/1000 | Loss: 0.00001882
Iteration 429/1000 | Loss: 0.00001881
Iteration 430/1000 | Loss: 0.00001881
Iteration 431/1000 | Loss: 0.00001881
Iteration 432/1000 | Loss: 0.00001881
Iteration 433/1000 | Loss: 0.00001881
Iteration 434/1000 | Loss: 0.00001881
Iteration 435/1000 | Loss: 0.00001881
Iteration 436/1000 | Loss: 0.00001881
Iteration 437/1000 | Loss: 0.00001881
Iteration 438/1000 | Loss: 0.00001881
Iteration 439/1000 | Loss: 0.00001881
Iteration 440/1000 | Loss: 0.00001881
Iteration 441/1000 | Loss: 0.00001881
Iteration 442/1000 | Loss: 0.00001881
Iteration 443/1000 | Loss: 0.00001881
Iteration 444/1000 | Loss: 0.00001881
Iteration 445/1000 | Loss: 0.00001881
Iteration 446/1000 | Loss: 0.00001880
Iteration 447/1000 | Loss: 0.00001880
Iteration 448/1000 | Loss: 0.00001880
Iteration 449/1000 | Loss: 0.00001880
Iteration 450/1000 | Loss: 0.00001880
Iteration 451/1000 | Loss: 0.00001880
Iteration 452/1000 | Loss: 0.00001880
Iteration 453/1000 | Loss: 0.00001880
Iteration 454/1000 | Loss: 0.00001880
Iteration 455/1000 | Loss: 0.00001880
Iteration 456/1000 | Loss: 0.00001880
Iteration 457/1000 | Loss: 0.00001880
Iteration 458/1000 | Loss: 0.00001880
Iteration 459/1000 | Loss: 0.00001880
Iteration 460/1000 | Loss: 0.00001880
Iteration 461/1000 | Loss: 0.00001880
Iteration 462/1000 | Loss: 0.00001880
Iteration 463/1000 | Loss: 0.00001880
Iteration 464/1000 | Loss: 0.00001880
Iteration 465/1000 | Loss: 0.00001880
Iteration 466/1000 | Loss: 0.00001880
Iteration 467/1000 | Loss: 0.00001880
Iteration 468/1000 | Loss: 0.00001880
Iteration 469/1000 | Loss: 0.00001880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 469. Stopping optimization.
Last 5 losses: [1.8802291378960945e-05, 1.8802291378960945e-05, 1.8802291378960945e-05, 1.8802291378960945e-05, 1.8802291378960945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8802291378960945e-05

Optimization complete. Final v2v error: 3.5882554054260254 mm

Highest mean error: 5.707197666168213 mm for frame 45

Lowest mean error: 2.6730868816375732 mm for frame 100

Saving results

Total time: 456.6323778629303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463659
Iteration 2/25 | Loss: 0.00102231
Iteration 3/25 | Loss: 0.00073606
Iteration 4/25 | Loss: 0.00068621
Iteration 5/25 | Loss: 0.00067755
Iteration 6/25 | Loss: 0.00067529
Iteration 7/25 | Loss: 0.00067527
Iteration 8/25 | Loss: 0.00067527
Iteration 9/25 | Loss: 0.00067527
Iteration 10/25 | Loss: 0.00067527
Iteration 11/25 | Loss: 0.00067527
Iteration 12/25 | Loss: 0.00067527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006752719637006521, 0.0006752719637006521, 0.0006752719637006521, 0.0006752719637006521, 0.0006752719637006521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006752719637006521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47619009
Iteration 2/25 | Loss: 0.00029795
Iteration 3/25 | Loss: 0.00029792
Iteration 4/25 | Loss: 0.00029792
Iteration 5/25 | Loss: 0.00029792
Iteration 6/25 | Loss: 0.00029792
Iteration 7/25 | Loss: 0.00029792
Iteration 8/25 | Loss: 0.00029792
Iteration 9/25 | Loss: 0.00029792
Iteration 10/25 | Loss: 0.00029792
Iteration 11/25 | Loss: 0.00029792
Iteration 12/25 | Loss: 0.00029792
Iteration 13/25 | Loss: 0.00029792
Iteration 14/25 | Loss: 0.00029792
Iteration 15/25 | Loss: 0.00029792
Iteration 16/25 | Loss: 0.00029792
Iteration 17/25 | Loss: 0.00029792
Iteration 18/25 | Loss: 0.00029792
Iteration 19/25 | Loss: 0.00029792
Iteration 20/25 | Loss: 0.00029792
Iteration 21/25 | Loss: 0.00029792
Iteration 22/25 | Loss: 0.00029792
Iteration 23/25 | Loss: 0.00029792
Iteration 24/25 | Loss: 0.00029792
Iteration 25/25 | Loss: 0.00029792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029792
Iteration 2/1000 | Loss: 0.00002666
Iteration 3/1000 | Loss: 0.00001998
Iteration 4/1000 | Loss: 0.00001848
Iteration 5/1000 | Loss: 0.00001749
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001573
Iteration 18/1000 | Loss: 0.00001572
Iteration 19/1000 | Loss: 0.00001571
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001569
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001564
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001557
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001551
Iteration 44/1000 | Loss: 0.00001551
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001547
Iteration 54/1000 | Loss: 0.00001547
Iteration 55/1000 | Loss: 0.00001547
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001545
Iteration 62/1000 | Loss: 0.00001545
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001544
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001544
Iteration 68/1000 | Loss: 0.00001544
Iteration 69/1000 | Loss: 0.00001544
Iteration 70/1000 | Loss: 0.00001544
Iteration 71/1000 | Loss: 0.00001543
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001543
Iteration 74/1000 | Loss: 0.00001543
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001541
Iteration 80/1000 | Loss: 0.00001541
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001540
Iteration 85/1000 | Loss: 0.00001540
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001538
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001538
Iteration 99/1000 | Loss: 0.00001538
Iteration 100/1000 | Loss: 0.00001538
Iteration 101/1000 | Loss: 0.00001538
Iteration 102/1000 | Loss: 0.00001538
Iteration 103/1000 | Loss: 0.00001538
Iteration 104/1000 | Loss: 0.00001538
Iteration 105/1000 | Loss: 0.00001538
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001538
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001538
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001538
Iteration 119/1000 | Loss: 0.00001538
Iteration 120/1000 | Loss: 0.00001538
Iteration 121/1000 | Loss: 0.00001538
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.5376408555312082e-05, 1.5376408555312082e-05, 1.5376408555312082e-05, 1.5376408555312082e-05, 1.5376408555312082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5376408555312082e-05

Optimization complete. Final v2v error: 3.2648239135742188 mm

Highest mean error: 3.5087180137634277 mm for frame 172

Lowest mean error: 2.93094801902771 mm for frame 158

Saving results

Total time: 39.20502805709839
