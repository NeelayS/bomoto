Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=130, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7280-7335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923673
Iteration 2/25 | Loss: 0.00152152
Iteration 3/25 | Loss: 0.00123487
Iteration 4/25 | Loss: 0.00119993
Iteration 5/25 | Loss: 0.00119667
Iteration 6/25 | Loss: 0.00119642
Iteration 7/25 | Loss: 0.00119642
Iteration 8/25 | Loss: 0.00119642
Iteration 9/25 | Loss: 0.00119642
Iteration 10/25 | Loss: 0.00119642
Iteration 11/25 | Loss: 0.00119642
Iteration 12/25 | Loss: 0.00119642
Iteration 13/25 | Loss: 0.00119642
Iteration 14/25 | Loss: 0.00119642
Iteration 15/25 | Loss: 0.00119642
Iteration 16/25 | Loss: 0.00119642
Iteration 17/25 | Loss: 0.00119642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011964197037741542, 0.0011964197037741542, 0.0011964197037741542, 0.0011964197037741542, 0.0011964197037741542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011964197037741542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81718332
Iteration 2/25 | Loss: 0.00080811
Iteration 3/25 | Loss: 0.00080810
Iteration 4/25 | Loss: 0.00080810
Iteration 5/25 | Loss: 0.00080810
Iteration 6/25 | Loss: 0.00080810
Iteration 7/25 | Loss: 0.00080810
Iteration 8/25 | Loss: 0.00080810
Iteration 9/25 | Loss: 0.00080810
Iteration 10/25 | Loss: 0.00080810
Iteration 11/25 | Loss: 0.00080810
Iteration 12/25 | Loss: 0.00080810
Iteration 13/25 | Loss: 0.00080810
Iteration 14/25 | Loss: 0.00080810
Iteration 15/25 | Loss: 0.00080810
Iteration 16/25 | Loss: 0.00080810
Iteration 17/25 | Loss: 0.00080810
Iteration 18/25 | Loss: 0.00080810
Iteration 19/25 | Loss: 0.00080810
Iteration 20/25 | Loss: 0.00080810
Iteration 21/25 | Loss: 0.00080810
Iteration 22/25 | Loss: 0.00080810
Iteration 23/25 | Loss: 0.00080810
Iteration 24/25 | Loss: 0.00080810
Iteration 25/25 | Loss: 0.00080810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080810
Iteration 2/1000 | Loss: 0.00004436
Iteration 3/1000 | Loss: 0.00003302
Iteration 4/1000 | Loss: 0.00002812
Iteration 5/1000 | Loss: 0.00002686
Iteration 6/1000 | Loss: 0.00002594
Iteration 7/1000 | Loss: 0.00002548
Iteration 8/1000 | Loss: 0.00002520
Iteration 9/1000 | Loss: 0.00002495
Iteration 10/1000 | Loss: 0.00002484
Iteration 11/1000 | Loss: 0.00002475
Iteration 12/1000 | Loss: 0.00002475
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002473
Iteration 15/1000 | Loss: 0.00002473
Iteration 16/1000 | Loss: 0.00002473
Iteration 17/1000 | Loss: 0.00002473
Iteration 18/1000 | Loss: 0.00002473
Iteration 19/1000 | Loss: 0.00002472
Iteration 20/1000 | Loss: 0.00002472
Iteration 21/1000 | Loss: 0.00002472
Iteration 22/1000 | Loss: 0.00002472
Iteration 23/1000 | Loss: 0.00002472
Iteration 24/1000 | Loss: 0.00002471
Iteration 25/1000 | Loss: 0.00002471
Iteration 26/1000 | Loss: 0.00002469
Iteration 27/1000 | Loss: 0.00002469
Iteration 28/1000 | Loss: 0.00002465
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002463
Iteration 31/1000 | Loss: 0.00002462
Iteration 32/1000 | Loss: 0.00002461
Iteration 33/1000 | Loss: 0.00002458
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002457
Iteration 36/1000 | Loss: 0.00002457
Iteration 37/1000 | Loss: 0.00002457
Iteration 38/1000 | Loss: 0.00002457
Iteration 39/1000 | Loss: 0.00002457
Iteration 40/1000 | Loss: 0.00002457
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002457
Iteration 44/1000 | Loss: 0.00002456
Iteration 45/1000 | Loss: 0.00002455
Iteration 46/1000 | Loss: 0.00002455
Iteration 47/1000 | Loss: 0.00002454
Iteration 48/1000 | Loss: 0.00002454
Iteration 49/1000 | Loss: 0.00002452
Iteration 50/1000 | Loss: 0.00002451
Iteration 51/1000 | Loss: 0.00002449
Iteration 52/1000 | Loss: 0.00002449
Iteration 53/1000 | Loss: 0.00002449
Iteration 54/1000 | Loss: 0.00002449
Iteration 55/1000 | Loss: 0.00002449
Iteration 56/1000 | Loss: 0.00002448
Iteration 57/1000 | Loss: 0.00002448
Iteration 58/1000 | Loss: 0.00002448
Iteration 59/1000 | Loss: 0.00002448
Iteration 60/1000 | Loss: 0.00002448
Iteration 61/1000 | Loss: 0.00002448
Iteration 62/1000 | Loss: 0.00002448
Iteration 63/1000 | Loss: 0.00002448
Iteration 64/1000 | Loss: 0.00002448
Iteration 65/1000 | Loss: 0.00002448
Iteration 66/1000 | Loss: 0.00002447
Iteration 67/1000 | Loss: 0.00002447
Iteration 68/1000 | Loss: 0.00002447
Iteration 69/1000 | Loss: 0.00002447
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002447
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002446
Iteration 78/1000 | Loss: 0.00002446
Iteration 79/1000 | Loss: 0.00002446
Iteration 80/1000 | Loss: 0.00002446
Iteration 81/1000 | Loss: 0.00002446
Iteration 82/1000 | Loss: 0.00002446
Iteration 83/1000 | Loss: 0.00002446
Iteration 84/1000 | Loss: 0.00002446
Iteration 85/1000 | Loss: 0.00002446
Iteration 86/1000 | Loss: 0.00002446
Iteration 87/1000 | Loss: 0.00002446
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002445
Iteration 90/1000 | Loss: 0.00002445
Iteration 91/1000 | Loss: 0.00002445
Iteration 92/1000 | Loss: 0.00002445
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002444
Iteration 98/1000 | Loss: 0.00002444
Iteration 99/1000 | Loss: 0.00002444
Iteration 100/1000 | Loss: 0.00002444
Iteration 101/1000 | Loss: 0.00002444
Iteration 102/1000 | Loss: 0.00002444
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002444
Iteration 106/1000 | Loss: 0.00002444
Iteration 107/1000 | Loss: 0.00002444
Iteration 108/1000 | Loss: 0.00002444
Iteration 109/1000 | Loss: 0.00002444
Iteration 110/1000 | Loss: 0.00002444
Iteration 111/1000 | Loss: 0.00002444
Iteration 112/1000 | Loss: 0.00002444
Iteration 113/1000 | Loss: 0.00002444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.44441343966173e-05, 2.44441343966173e-05, 2.44441343966173e-05, 2.44441343966173e-05, 2.44441343966173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.44441343966173e-05

Optimization complete. Final v2v error: 4.260013580322266 mm

Highest mean error: 4.53973388671875 mm for frame 33

Lowest mean error: 4.091277122497559 mm for frame 132

Saving results

Total time: 33.10503888130188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839642
Iteration 2/25 | Loss: 0.00153504
Iteration 3/25 | Loss: 0.00129227
Iteration 4/25 | Loss: 0.00123773
Iteration 5/25 | Loss: 0.00124423
Iteration 6/25 | Loss: 0.00125202
Iteration 7/25 | Loss: 0.00123930
Iteration 8/25 | Loss: 0.00122807
Iteration 9/25 | Loss: 0.00121248
Iteration 10/25 | Loss: 0.00120672
Iteration 11/25 | Loss: 0.00120174
Iteration 12/25 | Loss: 0.00119641
Iteration 13/25 | Loss: 0.00119668
Iteration 14/25 | Loss: 0.00119445
Iteration 15/25 | Loss: 0.00119200
Iteration 16/25 | Loss: 0.00119279
Iteration 17/25 | Loss: 0.00119261
Iteration 18/25 | Loss: 0.00119194
Iteration 19/25 | Loss: 0.00119206
Iteration 20/25 | Loss: 0.00119193
Iteration 21/25 | Loss: 0.00119298
Iteration 22/25 | Loss: 0.00119384
Iteration 23/25 | Loss: 0.00119395
Iteration 24/25 | Loss: 0.00119297
Iteration 25/25 | Loss: 0.00119192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25870001
Iteration 2/25 | Loss: 0.00182909
Iteration 3/25 | Loss: 0.00182908
Iteration 4/25 | Loss: 0.00182908
Iteration 5/25 | Loss: 0.00182908
Iteration 6/25 | Loss: 0.00182908
Iteration 7/25 | Loss: 0.00182908
Iteration 8/25 | Loss: 0.00182908
Iteration 9/25 | Loss: 0.00182908
Iteration 10/25 | Loss: 0.00182908
Iteration 11/25 | Loss: 0.00182908
Iteration 12/25 | Loss: 0.00182908
Iteration 13/25 | Loss: 0.00182908
Iteration 14/25 | Loss: 0.00182908
Iteration 15/25 | Loss: 0.00182908
Iteration 16/25 | Loss: 0.00182908
Iteration 17/25 | Loss: 0.00182908
Iteration 18/25 | Loss: 0.00182908
Iteration 19/25 | Loss: 0.00182908
Iteration 20/25 | Loss: 0.00182908
Iteration 21/25 | Loss: 0.00182908
Iteration 22/25 | Loss: 0.00182908
Iteration 23/25 | Loss: 0.00182908
Iteration 24/25 | Loss: 0.00182908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0018290752777829766, 0.0018290752777829766, 0.0018290752777829766, 0.0018290752777829766, 0.0018290752777829766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018290752777829766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182908
Iteration 2/1000 | Loss: 0.00006442
Iteration 3/1000 | Loss: 0.00006436
Iteration 4/1000 | Loss: 0.00003459
Iteration 5/1000 | Loss: 0.00006678
Iteration 6/1000 | Loss: 0.00005175
Iteration 7/1000 | Loss: 0.00006010
Iteration 8/1000 | Loss: 0.00005406
Iteration 9/1000 | Loss: 0.00005202
Iteration 10/1000 | Loss: 0.00005120
Iteration 11/1000 | Loss: 0.00005524
Iteration 12/1000 | Loss: 0.00005764
Iteration 13/1000 | Loss: 0.00006319
Iteration 14/1000 | Loss: 0.00005901
Iteration 15/1000 | Loss: 0.00005737
Iteration 16/1000 | Loss: 0.00006098
Iteration 17/1000 | Loss: 0.00006373
Iteration 18/1000 | Loss: 0.00006259
Iteration 19/1000 | Loss: 0.00006299
Iteration 20/1000 | Loss: 0.00005951
Iteration 21/1000 | Loss: 0.00006145
Iteration 22/1000 | Loss: 0.00005879
Iteration 23/1000 | Loss: 0.00005910
Iteration 24/1000 | Loss: 0.00007078
Iteration 25/1000 | Loss: 0.00005845
Iteration 26/1000 | Loss: 0.00005679
Iteration 27/1000 | Loss: 0.00006784
Iteration 28/1000 | Loss: 0.00007027
Iteration 29/1000 | Loss: 0.00007005
Iteration 30/1000 | Loss: 0.00006685
Iteration 31/1000 | Loss: 0.00008169
Iteration 32/1000 | Loss: 0.00006100
Iteration 33/1000 | Loss: 0.00007326
Iteration 34/1000 | Loss: 0.00006058
Iteration 35/1000 | Loss: 0.00005996
Iteration 36/1000 | Loss: 0.00006188
Iteration 37/1000 | Loss: 0.00006093
Iteration 38/1000 | Loss: 0.00005857
Iteration 39/1000 | Loss: 0.00005992
Iteration 40/1000 | Loss: 0.00007023
Iteration 41/1000 | Loss: 0.00006654
Iteration 42/1000 | Loss: 0.00006088
Iteration 43/1000 | Loss: 0.00005964
Iteration 44/1000 | Loss: 0.00006011
Iteration 45/1000 | Loss: 0.00006401
Iteration 46/1000 | Loss: 0.00005639
Iteration 47/1000 | Loss: 0.00006516
Iteration 48/1000 | Loss: 0.00005628
Iteration 49/1000 | Loss: 0.00006092
Iteration 50/1000 | Loss: 0.00006751
Iteration 51/1000 | Loss: 0.00006107
Iteration 52/1000 | Loss: 0.00006275
Iteration 53/1000 | Loss: 0.00006023
Iteration 54/1000 | Loss: 0.00007451
Iteration 55/1000 | Loss: 0.00015457
Iteration 56/1000 | Loss: 0.00014218
Iteration 57/1000 | Loss: 0.00007326
Iteration 58/1000 | Loss: 0.00003976
Iteration 59/1000 | Loss: 0.00007538
Iteration 60/1000 | Loss: 0.00006812
Iteration 61/1000 | Loss: 0.00007402
Iteration 62/1000 | Loss: 0.00006523
Iteration 63/1000 | Loss: 0.00005774
Iteration 64/1000 | Loss: 0.00005866
Iteration 65/1000 | Loss: 0.00006160
Iteration 66/1000 | Loss: 0.00005586
Iteration 67/1000 | Loss: 0.00006716
Iteration 68/1000 | Loss: 0.00006121
Iteration 69/1000 | Loss: 0.00004999
Iteration 70/1000 | Loss: 0.00042827
Iteration 71/1000 | Loss: 0.00019586
Iteration 72/1000 | Loss: 0.00011177
Iteration 73/1000 | Loss: 0.00011357
Iteration 74/1000 | Loss: 0.00003885
Iteration 75/1000 | Loss: 0.00003219
Iteration 76/1000 | Loss: 0.00011644
Iteration 77/1000 | Loss: 0.00003024
Iteration 78/1000 | Loss: 0.00007482
Iteration 79/1000 | Loss: 0.00002648
Iteration 80/1000 | Loss: 0.00002398
Iteration 81/1000 | Loss: 0.00002189
Iteration 82/1000 | Loss: 0.00002078
Iteration 83/1000 | Loss: 0.00002036
Iteration 84/1000 | Loss: 0.00001996
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001917
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001915
Iteration 90/1000 | Loss: 0.00001915
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001908
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001907
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001904
Iteration 112/1000 | Loss: 0.00001904
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001901
Iteration 115/1000 | Loss: 0.00001901
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001899
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001899
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001898
Iteration 126/1000 | Loss: 0.00001898
Iteration 127/1000 | Loss: 0.00001898
Iteration 128/1000 | Loss: 0.00001898
Iteration 129/1000 | Loss: 0.00001898
Iteration 130/1000 | Loss: 0.00001897
Iteration 131/1000 | Loss: 0.00001897
Iteration 132/1000 | Loss: 0.00001896
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001892
Iteration 136/1000 | Loss: 0.00001892
Iteration 137/1000 | Loss: 0.00001892
Iteration 138/1000 | Loss: 0.00001891
Iteration 139/1000 | Loss: 0.00001891
Iteration 140/1000 | Loss: 0.00001890
Iteration 141/1000 | Loss: 0.00001890
Iteration 142/1000 | Loss: 0.00001890
Iteration 143/1000 | Loss: 0.00001889
Iteration 144/1000 | Loss: 0.00001889
Iteration 145/1000 | Loss: 0.00001889
Iteration 146/1000 | Loss: 0.00001889
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001887
Iteration 153/1000 | Loss: 0.00001887
Iteration 154/1000 | Loss: 0.00001887
Iteration 155/1000 | Loss: 0.00001887
Iteration 156/1000 | Loss: 0.00001887
Iteration 157/1000 | Loss: 0.00001887
Iteration 158/1000 | Loss: 0.00001887
Iteration 159/1000 | Loss: 0.00001887
Iteration 160/1000 | Loss: 0.00001887
Iteration 161/1000 | Loss: 0.00001887
Iteration 162/1000 | Loss: 0.00001887
Iteration 163/1000 | Loss: 0.00001887
Iteration 164/1000 | Loss: 0.00001886
Iteration 165/1000 | Loss: 0.00001886
Iteration 166/1000 | Loss: 0.00001886
Iteration 167/1000 | Loss: 0.00001886
Iteration 168/1000 | Loss: 0.00001886
Iteration 169/1000 | Loss: 0.00001886
Iteration 170/1000 | Loss: 0.00001886
Iteration 171/1000 | Loss: 0.00001886
Iteration 172/1000 | Loss: 0.00001885
Iteration 173/1000 | Loss: 0.00001885
Iteration 174/1000 | Loss: 0.00001885
Iteration 175/1000 | Loss: 0.00001885
Iteration 176/1000 | Loss: 0.00001885
Iteration 177/1000 | Loss: 0.00001884
Iteration 178/1000 | Loss: 0.00001884
Iteration 179/1000 | Loss: 0.00001884
Iteration 180/1000 | Loss: 0.00001884
Iteration 181/1000 | Loss: 0.00001884
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001883
Iteration 185/1000 | Loss: 0.00001883
Iteration 186/1000 | Loss: 0.00001883
Iteration 187/1000 | Loss: 0.00001883
Iteration 188/1000 | Loss: 0.00001883
Iteration 189/1000 | Loss: 0.00001882
Iteration 190/1000 | Loss: 0.00001882
Iteration 191/1000 | Loss: 0.00001882
Iteration 192/1000 | Loss: 0.00001882
Iteration 193/1000 | Loss: 0.00001882
Iteration 194/1000 | Loss: 0.00001882
Iteration 195/1000 | Loss: 0.00001882
Iteration 196/1000 | Loss: 0.00001882
Iteration 197/1000 | Loss: 0.00001882
Iteration 198/1000 | Loss: 0.00001882
Iteration 199/1000 | Loss: 0.00001882
Iteration 200/1000 | Loss: 0.00001882
Iteration 201/1000 | Loss: 0.00001882
Iteration 202/1000 | Loss: 0.00001882
Iteration 203/1000 | Loss: 0.00001882
Iteration 204/1000 | Loss: 0.00001882
Iteration 205/1000 | Loss: 0.00001882
Iteration 206/1000 | Loss: 0.00001881
Iteration 207/1000 | Loss: 0.00001881
Iteration 208/1000 | Loss: 0.00001881
Iteration 209/1000 | Loss: 0.00001881
Iteration 210/1000 | Loss: 0.00001881
Iteration 211/1000 | Loss: 0.00001881
Iteration 212/1000 | Loss: 0.00001881
Iteration 213/1000 | Loss: 0.00001881
Iteration 214/1000 | Loss: 0.00001881
Iteration 215/1000 | Loss: 0.00001881
Iteration 216/1000 | Loss: 0.00001881
Iteration 217/1000 | Loss: 0.00001881
Iteration 218/1000 | Loss: 0.00001881
Iteration 219/1000 | Loss: 0.00001881
Iteration 220/1000 | Loss: 0.00001881
Iteration 221/1000 | Loss: 0.00001881
Iteration 222/1000 | Loss: 0.00001881
Iteration 223/1000 | Loss: 0.00001881
Iteration 224/1000 | Loss: 0.00001881
Iteration 225/1000 | Loss: 0.00001881
Iteration 226/1000 | Loss: 0.00001881
Iteration 227/1000 | Loss: 0.00001880
Iteration 228/1000 | Loss: 0.00001880
Iteration 229/1000 | Loss: 0.00001880
Iteration 230/1000 | Loss: 0.00001880
Iteration 231/1000 | Loss: 0.00001880
Iteration 232/1000 | Loss: 0.00001880
Iteration 233/1000 | Loss: 0.00001880
Iteration 234/1000 | Loss: 0.00001880
Iteration 235/1000 | Loss: 0.00001880
Iteration 236/1000 | Loss: 0.00001880
Iteration 237/1000 | Loss: 0.00001880
Iteration 238/1000 | Loss: 0.00001880
Iteration 239/1000 | Loss: 0.00001880
Iteration 240/1000 | Loss: 0.00001880
Iteration 241/1000 | Loss: 0.00001880
Iteration 242/1000 | Loss: 0.00001880
Iteration 243/1000 | Loss: 0.00001880
Iteration 244/1000 | Loss: 0.00001880
Iteration 245/1000 | Loss: 0.00001880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.8801774785970338e-05, 1.8801774785970338e-05, 1.8801774785970338e-05, 1.8801774785970338e-05, 1.8801774785970338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8801774785970338e-05

Optimization complete. Final v2v error: 3.508277654647827 mm

Highest mean error: 5.10819149017334 mm for frame 220

Lowest mean error: 2.8450493812561035 mm for frame 112

Saving results

Total time: 207.553893327713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499117
Iteration 2/25 | Loss: 0.00135822
Iteration 3/25 | Loss: 0.00121228
Iteration 4/25 | Loss: 0.00119169
Iteration 5/25 | Loss: 0.00118729
Iteration 6/25 | Loss: 0.00118610
Iteration 7/25 | Loss: 0.00118599
Iteration 8/25 | Loss: 0.00118599
Iteration 9/25 | Loss: 0.00118599
Iteration 10/25 | Loss: 0.00118599
Iteration 11/25 | Loss: 0.00118599
Iteration 12/25 | Loss: 0.00118599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011859927326440811, 0.0011859927326440811, 0.0011859927326440811, 0.0011859927326440811, 0.0011859927326440811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859927326440811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25493038
Iteration 2/25 | Loss: 0.00143895
Iteration 3/25 | Loss: 0.00143895
Iteration 4/25 | Loss: 0.00143895
Iteration 5/25 | Loss: 0.00143895
Iteration 6/25 | Loss: 0.00143895
Iteration 7/25 | Loss: 0.00143895
Iteration 8/25 | Loss: 0.00143895
Iteration 9/25 | Loss: 0.00143895
Iteration 10/25 | Loss: 0.00143895
Iteration 11/25 | Loss: 0.00143895
Iteration 12/25 | Loss: 0.00143895
Iteration 13/25 | Loss: 0.00143895
Iteration 14/25 | Loss: 0.00143895
Iteration 15/25 | Loss: 0.00143895
Iteration 16/25 | Loss: 0.00143895
Iteration 17/25 | Loss: 0.00143895
Iteration 18/25 | Loss: 0.00143895
Iteration 19/25 | Loss: 0.00143895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001438950072042644, 0.001438950072042644, 0.001438950072042644, 0.001438950072042644, 0.001438950072042644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001438950072042644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143895
Iteration 2/1000 | Loss: 0.00005115
Iteration 3/1000 | Loss: 0.00003648
Iteration 4/1000 | Loss: 0.00003233
Iteration 5/1000 | Loss: 0.00003049
Iteration 6/1000 | Loss: 0.00002965
Iteration 7/1000 | Loss: 0.00002890
Iteration 8/1000 | Loss: 0.00002831
Iteration 9/1000 | Loss: 0.00002792
Iteration 10/1000 | Loss: 0.00002761
Iteration 11/1000 | Loss: 0.00002748
Iteration 12/1000 | Loss: 0.00002748
Iteration 13/1000 | Loss: 0.00002747
Iteration 14/1000 | Loss: 0.00002732
Iteration 15/1000 | Loss: 0.00002718
Iteration 16/1000 | Loss: 0.00002716
Iteration 17/1000 | Loss: 0.00002705
Iteration 18/1000 | Loss: 0.00002704
Iteration 19/1000 | Loss: 0.00002704
Iteration 20/1000 | Loss: 0.00002701
Iteration 21/1000 | Loss: 0.00002700
Iteration 22/1000 | Loss: 0.00002699
Iteration 23/1000 | Loss: 0.00002699
Iteration 24/1000 | Loss: 0.00002699
Iteration 25/1000 | Loss: 0.00002698
Iteration 26/1000 | Loss: 0.00002698
Iteration 27/1000 | Loss: 0.00002697
Iteration 28/1000 | Loss: 0.00002697
Iteration 29/1000 | Loss: 0.00002696
Iteration 30/1000 | Loss: 0.00002696
Iteration 31/1000 | Loss: 0.00002696
Iteration 32/1000 | Loss: 0.00002696
Iteration 33/1000 | Loss: 0.00002695
Iteration 34/1000 | Loss: 0.00002695
Iteration 35/1000 | Loss: 0.00002695
Iteration 36/1000 | Loss: 0.00002694
Iteration 37/1000 | Loss: 0.00002694
Iteration 38/1000 | Loss: 0.00002693
Iteration 39/1000 | Loss: 0.00002693
Iteration 40/1000 | Loss: 0.00002692
Iteration 41/1000 | Loss: 0.00002692
Iteration 42/1000 | Loss: 0.00002692
Iteration 43/1000 | Loss: 0.00002691
Iteration 44/1000 | Loss: 0.00002691
Iteration 45/1000 | Loss: 0.00002691
Iteration 46/1000 | Loss: 0.00002690
Iteration 47/1000 | Loss: 0.00002690
Iteration 48/1000 | Loss: 0.00002690
Iteration 49/1000 | Loss: 0.00002690
Iteration 50/1000 | Loss: 0.00002689
Iteration 51/1000 | Loss: 0.00002689
Iteration 52/1000 | Loss: 0.00002689
Iteration 53/1000 | Loss: 0.00002689
Iteration 54/1000 | Loss: 0.00002689
Iteration 55/1000 | Loss: 0.00002689
Iteration 56/1000 | Loss: 0.00002689
Iteration 57/1000 | Loss: 0.00002689
Iteration 58/1000 | Loss: 0.00002688
Iteration 59/1000 | Loss: 0.00002688
Iteration 60/1000 | Loss: 0.00002688
Iteration 61/1000 | Loss: 0.00002688
Iteration 62/1000 | Loss: 0.00002688
Iteration 63/1000 | Loss: 0.00002688
Iteration 64/1000 | Loss: 0.00002688
Iteration 65/1000 | Loss: 0.00002688
Iteration 66/1000 | Loss: 0.00002687
Iteration 67/1000 | Loss: 0.00002687
Iteration 68/1000 | Loss: 0.00002687
Iteration 69/1000 | Loss: 0.00002687
Iteration 70/1000 | Loss: 0.00002686
Iteration 71/1000 | Loss: 0.00002686
Iteration 72/1000 | Loss: 0.00002685
Iteration 73/1000 | Loss: 0.00002685
Iteration 74/1000 | Loss: 0.00002685
Iteration 75/1000 | Loss: 0.00002684
Iteration 76/1000 | Loss: 0.00002684
Iteration 77/1000 | Loss: 0.00002683
Iteration 78/1000 | Loss: 0.00002683
Iteration 79/1000 | Loss: 0.00002683
Iteration 80/1000 | Loss: 0.00002683
Iteration 81/1000 | Loss: 0.00002683
Iteration 82/1000 | Loss: 0.00002683
Iteration 83/1000 | Loss: 0.00002682
Iteration 84/1000 | Loss: 0.00002682
Iteration 85/1000 | Loss: 0.00002682
Iteration 86/1000 | Loss: 0.00002682
Iteration 87/1000 | Loss: 0.00002682
Iteration 88/1000 | Loss: 0.00002681
Iteration 89/1000 | Loss: 0.00002681
Iteration 90/1000 | Loss: 0.00002681
Iteration 91/1000 | Loss: 0.00002680
Iteration 92/1000 | Loss: 0.00002679
Iteration 93/1000 | Loss: 0.00002679
Iteration 94/1000 | Loss: 0.00002679
Iteration 95/1000 | Loss: 0.00002679
Iteration 96/1000 | Loss: 0.00002678
Iteration 97/1000 | Loss: 0.00002678
Iteration 98/1000 | Loss: 0.00002678
Iteration 99/1000 | Loss: 0.00002676
Iteration 100/1000 | Loss: 0.00002676
Iteration 101/1000 | Loss: 0.00002676
Iteration 102/1000 | Loss: 0.00002676
Iteration 103/1000 | Loss: 0.00002676
Iteration 104/1000 | Loss: 0.00002676
Iteration 105/1000 | Loss: 0.00002676
Iteration 106/1000 | Loss: 0.00002676
Iteration 107/1000 | Loss: 0.00002675
Iteration 108/1000 | Loss: 0.00002675
Iteration 109/1000 | Loss: 0.00002675
Iteration 110/1000 | Loss: 0.00002674
Iteration 111/1000 | Loss: 0.00002674
Iteration 112/1000 | Loss: 0.00002674
Iteration 113/1000 | Loss: 0.00002674
Iteration 114/1000 | Loss: 0.00002674
Iteration 115/1000 | Loss: 0.00002674
Iteration 116/1000 | Loss: 0.00002673
Iteration 117/1000 | Loss: 0.00002673
Iteration 118/1000 | Loss: 0.00002673
Iteration 119/1000 | Loss: 0.00002672
Iteration 120/1000 | Loss: 0.00002672
Iteration 121/1000 | Loss: 0.00002672
Iteration 122/1000 | Loss: 0.00002671
Iteration 123/1000 | Loss: 0.00002671
Iteration 124/1000 | Loss: 0.00002671
Iteration 125/1000 | Loss: 0.00002671
Iteration 126/1000 | Loss: 0.00002671
Iteration 127/1000 | Loss: 0.00002671
Iteration 128/1000 | Loss: 0.00002670
Iteration 129/1000 | Loss: 0.00002670
Iteration 130/1000 | Loss: 0.00002670
Iteration 131/1000 | Loss: 0.00002670
Iteration 132/1000 | Loss: 0.00002669
Iteration 133/1000 | Loss: 0.00002669
Iteration 134/1000 | Loss: 0.00002669
Iteration 135/1000 | Loss: 0.00002669
Iteration 136/1000 | Loss: 0.00002669
Iteration 137/1000 | Loss: 0.00002669
Iteration 138/1000 | Loss: 0.00002669
Iteration 139/1000 | Loss: 0.00002669
Iteration 140/1000 | Loss: 0.00002668
Iteration 141/1000 | Loss: 0.00002668
Iteration 142/1000 | Loss: 0.00002668
Iteration 143/1000 | Loss: 0.00002668
Iteration 144/1000 | Loss: 0.00002668
Iteration 145/1000 | Loss: 0.00002668
Iteration 146/1000 | Loss: 0.00002668
Iteration 147/1000 | Loss: 0.00002668
Iteration 148/1000 | Loss: 0.00002668
Iteration 149/1000 | Loss: 0.00002667
Iteration 150/1000 | Loss: 0.00002667
Iteration 151/1000 | Loss: 0.00002667
Iteration 152/1000 | Loss: 0.00002667
Iteration 153/1000 | Loss: 0.00002667
Iteration 154/1000 | Loss: 0.00002667
Iteration 155/1000 | Loss: 0.00002666
Iteration 156/1000 | Loss: 0.00002666
Iteration 157/1000 | Loss: 0.00002666
Iteration 158/1000 | Loss: 0.00002666
Iteration 159/1000 | Loss: 0.00002666
Iteration 160/1000 | Loss: 0.00002666
Iteration 161/1000 | Loss: 0.00002666
Iteration 162/1000 | Loss: 0.00002665
Iteration 163/1000 | Loss: 0.00002665
Iteration 164/1000 | Loss: 0.00002665
Iteration 165/1000 | Loss: 0.00002665
Iteration 166/1000 | Loss: 0.00002665
Iteration 167/1000 | Loss: 0.00002665
Iteration 168/1000 | Loss: 0.00002665
Iteration 169/1000 | Loss: 0.00002665
Iteration 170/1000 | Loss: 0.00002664
Iteration 171/1000 | Loss: 0.00002664
Iteration 172/1000 | Loss: 0.00002664
Iteration 173/1000 | Loss: 0.00002664
Iteration 174/1000 | Loss: 0.00002664
Iteration 175/1000 | Loss: 0.00002664
Iteration 176/1000 | Loss: 0.00002664
Iteration 177/1000 | Loss: 0.00002664
Iteration 178/1000 | Loss: 0.00002664
Iteration 179/1000 | Loss: 0.00002663
Iteration 180/1000 | Loss: 0.00002663
Iteration 181/1000 | Loss: 0.00002663
Iteration 182/1000 | Loss: 0.00002663
Iteration 183/1000 | Loss: 0.00002663
Iteration 184/1000 | Loss: 0.00002663
Iteration 185/1000 | Loss: 0.00002663
Iteration 186/1000 | Loss: 0.00002663
Iteration 187/1000 | Loss: 0.00002662
Iteration 188/1000 | Loss: 0.00002662
Iteration 189/1000 | Loss: 0.00002662
Iteration 190/1000 | Loss: 0.00002662
Iteration 191/1000 | Loss: 0.00002662
Iteration 192/1000 | Loss: 0.00002662
Iteration 193/1000 | Loss: 0.00002662
Iteration 194/1000 | Loss: 0.00002662
Iteration 195/1000 | Loss: 0.00002662
Iteration 196/1000 | Loss: 0.00002662
Iteration 197/1000 | Loss: 0.00002661
Iteration 198/1000 | Loss: 0.00002661
Iteration 199/1000 | Loss: 0.00002661
Iteration 200/1000 | Loss: 0.00002661
Iteration 201/1000 | Loss: 0.00002661
Iteration 202/1000 | Loss: 0.00002661
Iteration 203/1000 | Loss: 0.00002661
Iteration 204/1000 | Loss: 0.00002661
Iteration 205/1000 | Loss: 0.00002661
Iteration 206/1000 | Loss: 0.00002661
Iteration 207/1000 | Loss: 0.00002661
Iteration 208/1000 | Loss: 0.00002661
Iteration 209/1000 | Loss: 0.00002661
Iteration 210/1000 | Loss: 0.00002661
Iteration 211/1000 | Loss: 0.00002661
Iteration 212/1000 | Loss: 0.00002661
Iteration 213/1000 | Loss: 0.00002661
Iteration 214/1000 | Loss: 0.00002661
Iteration 215/1000 | Loss: 0.00002661
Iteration 216/1000 | Loss: 0.00002661
Iteration 217/1000 | Loss: 0.00002661
Iteration 218/1000 | Loss: 0.00002661
Iteration 219/1000 | Loss: 0.00002661
Iteration 220/1000 | Loss: 0.00002661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.660784412000794e-05, 2.660784412000794e-05, 2.660784412000794e-05, 2.660784412000794e-05, 2.660784412000794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.660784412000794e-05

Optimization complete. Final v2v error: 4.2583184242248535 mm

Highest mean error: 6.037082195281982 mm for frame 38

Lowest mean error: 3.6460509300231934 mm for frame 9

Saving results

Total time: 45.02611589431763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405081
Iteration 2/25 | Loss: 0.00130584
Iteration 3/25 | Loss: 0.00122180
Iteration 4/25 | Loss: 0.00119961
Iteration 5/25 | Loss: 0.00119134
Iteration 6/25 | Loss: 0.00118927
Iteration 7/25 | Loss: 0.00118921
Iteration 8/25 | Loss: 0.00118921
Iteration 9/25 | Loss: 0.00118921
Iteration 10/25 | Loss: 0.00118921
Iteration 11/25 | Loss: 0.00118921
Iteration 12/25 | Loss: 0.00118921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011892146430909634, 0.0011892146430909634, 0.0011892146430909634, 0.0011892146430909634, 0.0011892146430909634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011892146430909634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21976614
Iteration 2/25 | Loss: 0.00253003
Iteration 3/25 | Loss: 0.00253002
Iteration 4/25 | Loss: 0.00253002
Iteration 5/25 | Loss: 0.00253002
Iteration 6/25 | Loss: 0.00253002
Iteration 7/25 | Loss: 0.00253002
Iteration 8/25 | Loss: 0.00253002
Iteration 9/25 | Loss: 0.00253002
Iteration 10/25 | Loss: 0.00253002
Iteration 11/25 | Loss: 0.00253002
Iteration 12/25 | Loss: 0.00253002
Iteration 13/25 | Loss: 0.00253002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0025300218258053064, 0.0025300218258053064, 0.0025300218258053064, 0.0025300218258053064, 0.0025300218258053064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025300218258053064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253002
Iteration 2/1000 | Loss: 0.00005608
Iteration 3/1000 | Loss: 0.00003648
Iteration 4/1000 | Loss: 0.00002738
Iteration 5/1000 | Loss: 0.00002454
Iteration 6/1000 | Loss: 0.00002357
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002242
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002193
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002175
Iteration 13/1000 | Loss: 0.00002174
Iteration 14/1000 | Loss: 0.00002174
Iteration 15/1000 | Loss: 0.00002173
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002168
Iteration 19/1000 | Loss: 0.00002168
Iteration 20/1000 | Loss: 0.00002167
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002166
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002161
Iteration 25/1000 | Loss: 0.00002160
Iteration 26/1000 | Loss: 0.00002160
Iteration 27/1000 | Loss: 0.00002159
Iteration 28/1000 | Loss: 0.00002159
Iteration 29/1000 | Loss: 0.00002159
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002158
Iteration 32/1000 | Loss: 0.00002158
Iteration 33/1000 | Loss: 0.00002158
Iteration 34/1000 | Loss: 0.00002157
Iteration 35/1000 | Loss: 0.00002157
Iteration 36/1000 | Loss: 0.00002156
Iteration 37/1000 | Loss: 0.00002156
Iteration 38/1000 | Loss: 0.00002156
Iteration 39/1000 | Loss: 0.00002156
Iteration 40/1000 | Loss: 0.00002156
Iteration 41/1000 | Loss: 0.00002156
Iteration 42/1000 | Loss: 0.00002156
Iteration 43/1000 | Loss: 0.00002156
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002155
Iteration 46/1000 | Loss: 0.00002154
Iteration 47/1000 | Loss: 0.00002154
Iteration 48/1000 | Loss: 0.00002153
Iteration 49/1000 | Loss: 0.00002153
Iteration 50/1000 | Loss: 0.00002153
Iteration 51/1000 | Loss: 0.00002152
Iteration 52/1000 | Loss: 0.00002152
Iteration 53/1000 | Loss: 0.00002152
Iteration 54/1000 | Loss: 0.00002152
Iteration 55/1000 | Loss: 0.00002152
Iteration 56/1000 | Loss: 0.00002151
Iteration 57/1000 | Loss: 0.00002151
Iteration 58/1000 | Loss: 0.00002151
Iteration 59/1000 | Loss: 0.00002151
Iteration 60/1000 | Loss: 0.00002150
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00002150
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002149
Iteration 65/1000 | Loss: 0.00002149
Iteration 66/1000 | Loss: 0.00002149
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002147
Iteration 71/1000 | Loss: 0.00002147
Iteration 72/1000 | Loss: 0.00002146
Iteration 73/1000 | Loss: 0.00002146
Iteration 74/1000 | Loss: 0.00002146
Iteration 75/1000 | Loss: 0.00002146
Iteration 76/1000 | Loss: 0.00002146
Iteration 77/1000 | Loss: 0.00002145
Iteration 78/1000 | Loss: 0.00002145
Iteration 79/1000 | Loss: 0.00002145
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002144
Iteration 82/1000 | Loss: 0.00002144
Iteration 83/1000 | Loss: 0.00002144
Iteration 84/1000 | Loss: 0.00002144
Iteration 85/1000 | Loss: 0.00002144
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002143
Iteration 90/1000 | Loss: 0.00002143
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00002143
Iteration 93/1000 | Loss: 0.00002143
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002142
Iteration 96/1000 | Loss: 0.00002142
Iteration 97/1000 | Loss: 0.00002142
Iteration 98/1000 | Loss: 0.00002142
Iteration 99/1000 | Loss: 0.00002142
Iteration 100/1000 | Loss: 0.00002142
Iteration 101/1000 | Loss: 0.00002142
Iteration 102/1000 | Loss: 0.00002142
Iteration 103/1000 | Loss: 0.00002142
Iteration 104/1000 | Loss: 0.00002142
Iteration 105/1000 | Loss: 0.00002141
Iteration 106/1000 | Loss: 0.00002141
Iteration 107/1000 | Loss: 0.00002141
Iteration 108/1000 | Loss: 0.00002141
Iteration 109/1000 | Loss: 0.00002141
Iteration 110/1000 | Loss: 0.00002141
Iteration 111/1000 | Loss: 0.00002141
Iteration 112/1000 | Loss: 0.00002141
Iteration 113/1000 | Loss: 0.00002141
Iteration 114/1000 | Loss: 0.00002141
Iteration 115/1000 | Loss: 0.00002140
Iteration 116/1000 | Loss: 0.00002140
Iteration 117/1000 | Loss: 0.00002140
Iteration 118/1000 | Loss: 0.00002140
Iteration 119/1000 | Loss: 0.00002140
Iteration 120/1000 | Loss: 0.00002140
Iteration 121/1000 | Loss: 0.00002140
Iteration 122/1000 | Loss: 0.00002140
Iteration 123/1000 | Loss: 0.00002140
Iteration 124/1000 | Loss: 0.00002140
Iteration 125/1000 | Loss: 0.00002140
Iteration 126/1000 | Loss: 0.00002140
Iteration 127/1000 | Loss: 0.00002140
Iteration 128/1000 | Loss: 0.00002140
Iteration 129/1000 | Loss: 0.00002140
Iteration 130/1000 | Loss: 0.00002140
Iteration 131/1000 | Loss: 0.00002140
Iteration 132/1000 | Loss: 0.00002140
Iteration 133/1000 | Loss: 0.00002140
Iteration 134/1000 | Loss: 0.00002139
Iteration 135/1000 | Loss: 0.00002139
Iteration 136/1000 | Loss: 0.00002139
Iteration 137/1000 | Loss: 0.00002139
Iteration 138/1000 | Loss: 0.00002139
Iteration 139/1000 | Loss: 0.00002139
Iteration 140/1000 | Loss: 0.00002139
Iteration 141/1000 | Loss: 0.00002139
Iteration 142/1000 | Loss: 0.00002139
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00002139
Iteration 146/1000 | Loss: 0.00002138
Iteration 147/1000 | Loss: 0.00002138
Iteration 148/1000 | Loss: 0.00002138
Iteration 149/1000 | Loss: 0.00002138
Iteration 150/1000 | Loss: 0.00002138
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00002138
Iteration 153/1000 | Loss: 0.00002138
Iteration 154/1000 | Loss: 0.00002138
Iteration 155/1000 | Loss: 0.00002138
Iteration 156/1000 | Loss: 0.00002138
Iteration 157/1000 | Loss: 0.00002138
Iteration 158/1000 | Loss: 0.00002138
Iteration 159/1000 | Loss: 0.00002137
Iteration 160/1000 | Loss: 0.00002137
Iteration 161/1000 | Loss: 0.00002137
Iteration 162/1000 | Loss: 0.00002137
Iteration 163/1000 | Loss: 0.00002137
Iteration 164/1000 | Loss: 0.00002137
Iteration 165/1000 | Loss: 0.00002137
Iteration 166/1000 | Loss: 0.00002137
Iteration 167/1000 | Loss: 0.00002137
Iteration 168/1000 | Loss: 0.00002137
Iteration 169/1000 | Loss: 0.00002137
Iteration 170/1000 | Loss: 0.00002137
Iteration 171/1000 | Loss: 0.00002137
Iteration 172/1000 | Loss: 0.00002137
Iteration 173/1000 | Loss: 0.00002137
Iteration 174/1000 | Loss: 0.00002137
Iteration 175/1000 | Loss: 0.00002137
Iteration 176/1000 | Loss: 0.00002137
Iteration 177/1000 | Loss: 0.00002137
Iteration 178/1000 | Loss: 0.00002136
Iteration 179/1000 | Loss: 0.00002136
Iteration 180/1000 | Loss: 0.00002136
Iteration 181/1000 | Loss: 0.00002136
Iteration 182/1000 | Loss: 0.00002136
Iteration 183/1000 | Loss: 0.00002136
Iteration 184/1000 | Loss: 0.00002136
Iteration 185/1000 | Loss: 0.00002136
Iteration 186/1000 | Loss: 0.00002136
Iteration 187/1000 | Loss: 0.00002136
Iteration 188/1000 | Loss: 0.00002136
Iteration 189/1000 | Loss: 0.00002136
Iteration 190/1000 | Loss: 0.00002136
Iteration 191/1000 | Loss: 0.00002136
Iteration 192/1000 | Loss: 0.00002135
Iteration 193/1000 | Loss: 0.00002135
Iteration 194/1000 | Loss: 0.00002135
Iteration 195/1000 | Loss: 0.00002135
Iteration 196/1000 | Loss: 0.00002135
Iteration 197/1000 | Loss: 0.00002135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.135364593414124e-05, 2.135364593414124e-05, 2.135364593414124e-05, 2.135364593414124e-05, 2.135364593414124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.135364593414124e-05

Optimization complete. Final v2v error: 3.6938226222991943 mm

Highest mean error: 4.344345569610596 mm for frame 65

Lowest mean error: 2.8267786502838135 mm for frame 232

Saving results

Total time: 43.64535999298096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101315
Iteration 2/25 | Loss: 0.01101315
Iteration 3/25 | Loss: 0.01101315
Iteration 4/25 | Loss: 0.01101315
Iteration 5/25 | Loss: 0.01101314
Iteration 6/25 | Loss: 0.01101314
Iteration 7/25 | Loss: 0.01101314
Iteration 8/25 | Loss: 0.01101314
Iteration 9/25 | Loss: 0.01101314
Iteration 10/25 | Loss: 0.01101314
Iteration 11/25 | Loss: 0.01101314
Iteration 12/25 | Loss: 0.01101314
Iteration 13/25 | Loss: 0.01101314
Iteration 14/25 | Loss: 0.01101314
Iteration 15/25 | Loss: 0.01101314
Iteration 16/25 | Loss: 0.01101314
Iteration 17/25 | Loss: 0.01101314
Iteration 18/25 | Loss: 0.01101314
Iteration 19/25 | Loss: 0.01101313
Iteration 20/25 | Loss: 0.01101313
Iteration 21/25 | Loss: 0.01101313
Iteration 22/25 | Loss: 0.01101313
Iteration 23/25 | Loss: 0.01101313
Iteration 24/25 | Loss: 0.01101313
Iteration 25/25 | Loss: 0.01101313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.43296146
Iteration 2/25 | Loss: 0.19647989
Iteration 3/25 | Loss: 0.19526090
Iteration 4/25 | Loss: 0.19474937
Iteration 5/25 | Loss: 0.19474928
Iteration 6/25 | Loss: 0.19474319
Iteration 7/25 | Loss: 0.19496265
Iteration 8/25 | Loss: 0.19472364
Iteration 9/25 | Loss: 0.19467941
Iteration 10/25 | Loss: 0.19467941
Iteration 11/25 | Loss: 0.19467936
Iteration 12/25 | Loss: 0.19467936
Iteration 13/25 | Loss: 0.19467936
Iteration 14/25 | Loss: 0.19467936
Iteration 15/25 | Loss: 0.19467935
Iteration 16/25 | Loss: 0.19467935
Iteration 17/25 | Loss: 0.19467935
Iteration 18/25 | Loss: 0.19467935
Iteration 19/25 | Loss: 0.19467935
Iteration 20/25 | Loss: 0.19467935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.1946793496608734, 0.1946793496608734, 0.1946793496608734, 0.1946793496608734, 0.1946793496608734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1946793496608734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.19467935
Iteration 2/1000 | Loss: 0.00410708
Iteration 3/1000 | Loss: 0.00307410
Iteration 4/1000 | Loss: 0.00092735
Iteration 5/1000 | Loss: 0.00057691
Iteration 6/1000 | Loss: 0.00042319
Iteration 7/1000 | Loss: 0.00016618
Iteration 8/1000 | Loss: 0.00019782
Iteration 9/1000 | Loss: 0.00038749
Iteration 10/1000 | Loss: 0.00017615
Iteration 11/1000 | Loss: 0.00012821
Iteration 12/1000 | Loss: 0.00010797
Iteration 13/1000 | Loss: 0.00008109
Iteration 14/1000 | Loss: 0.00005808
Iteration 15/1000 | Loss: 0.00005554
Iteration 16/1000 | Loss: 0.00004675
Iteration 17/1000 | Loss: 0.00004292
Iteration 18/1000 | Loss: 0.00004061
Iteration 19/1000 | Loss: 0.00004495
Iteration 20/1000 | Loss: 0.00005168
Iteration 21/1000 | Loss: 0.00005520
Iteration 22/1000 | Loss: 0.00003863
Iteration 23/1000 | Loss: 0.00004550
Iteration 24/1000 | Loss: 0.00004504
Iteration 25/1000 | Loss: 0.00009781
Iteration 26/1000 | Loss: 0.00003896
Iteration 27/1000 | Loss: 0.00003593
Iteration 28/1000 | Loss: 0.00003742
Iteration 29/1000 | Loss: 0.00003759
Iteration 30/1000 | Loss: 0.00003818
Iteration 31/1000 | Loss: 0.00003583
Iteration 32/1000 | Loss: 0.00003512
Iteration 33/1000 | Loss: 0.00003781
Iteration 34/1000 | Loss: 0.00003406
Iteration 35/1000 | Loss: 0.00003326
Iteration 36/1000 | Loss: 0.00003300
Iteration 37/1000 | Loss: 0.00003347
Iteration 38/1000 | Loss: 0.00003206
Iteration 39/1000 | Loss: 0.00003263
Iteration 40/1000 | Loss: 0.00003216
Iteration 41/1000 | Loss: 0.00003148
Iteration 42/1000 | Loss: 0.00003431
Iteration 43/1000 | Loss: 0.00003224
Iteration 44/1000 | Loss: 0.00003267
Iteration 45/1000 | Loss: 0.00003139
Iteration 46/1000 | Loss: 0.00003094
Iteration 47/1000 | Loss: 0.00003064
Iteration 48/1000 | Loss: 0.00003061
Iteration 49/1000 | Loss: 0.00003303
Iteration 50/1000 | Loss: 0.00003148
Iteration 51/1000 | Loss: 0.00003244
Iteration 52/1000 | Loss: 0.00003244
Iteration 53/1000 | Loss: 0.00003261
Iteration 54/1000 | Loss: 0.00003097
Iteration 55/1000 | Loss: 0.00003150
Iteration 56/1000 | Loss: 0.00003201
Iteration 57/1000 | Loss: 0.00003029
Iteration 58/1000 | Loss: 0.00003176
Iteration 59/1000 | Loss: 0.00003199
Iteration 60/1000 | Loss: 0.00003082
Iteration 61/1000 | Loss: 0.00003009
Iteration 62/1000 | Loss: 0.00002998
Iteration 63/1000 | Loss: 0.00002998
Iteration 64/1000 | Loss: 0.00002998
Iteration 65/1000 | Loss: 0.00002997
Iteration 66/1000 | Loss: 0.00002997
Iteration 67/1000 | Loss: 0.00002996
Iteration 68/1000 | Loss: 0.00002995
Iteration 69/1000 | Loss: 0.00002995
Iteration 70/1000 | Loss: 0.00002995
Iteration 71/1000 | Loss: 0.00002995
Iteration 72/1000 | Loss: 0.00002995
Iteration 73/1000 | Loss: 0.00002994
Iteration 74/1000 | Loss: 0.00002994
Iteration 75/1000 | Loss: 0.00002993
Iteration 76/1000 | Loss: 0.00002992
Iteration 77/1000 | Loss: 0.00002991
Iteration 78/1000 | Loss: 0.00002991
Iteration 79/1000 | Loss: 0.00002991
Iteration 80/1000 | Loss: 0.00002990
Iteration 81/1000 | Loss: 0.00002990
Iteration 82/1000 | Loss: 0.00002990
Iteration 83/1000 | Loss: 0.00002990
Iteration 84/1000 | Loss: 0.00002989
Iteration 85/1000 | Loss: 0.00002988
Iteration 86/1000 | Loss: 0.00002987
Iteration 87/1000 | Loss: 0.00002987
Iteration 88/1000 | Loss: 0.00002987
Iteration 89/1000 | Loss: 0.00002987
Iteration 90/1000 | Loss: 0.00002987
Iteration 91/1000 | Loss: 0.00002986
Iteration 92/1000 | Loss: 0.00002986
Iteration 93/1000 | Loss: 0.00002986
Iteration 94/1000 | Loss: 0.00002986
Iteration 95/1000 | Loss: 0.00002986
Iteration 96/1000 | Loss: 0.00002986
Iteration 97/1000 | Loss: 0.00002985
Iteration 98/1000 | Loss: 0.00002984
Iteration 99/1000 | Loss: 0.00002984
Iteration 100/1000 | Loss: 0.00002984
Iteration 101/1000 | Loss: 0.00003120
Iteration 102/1000 | Loss: 0.00003120
Iteration 103/1000 | Loss: 0.00003120
Iteration 104/1000 | Loss: 0.00003120
Iteration 105/1000 | Loss: 0.00003067
Iteration 106/1000 | Loss: 0.00002996
Iteration 107/1000 | Loss: 0.00002985
Iteration 108/1000 | Loss: 0.00002978
Iteration 109/1000 | Loss: 0.00002978
Iteration 110/1000 | Loss: 0.00002978
Iteration 111/1000 | Loss: 0.00002977
Iteration 112/1000 | Loss: 0.00002977
Iteration 113/1000 | Loss: 0.00002977
Iteration 114/1000 | Loss: 0.00002977
Iteration 115/1000 | Loss: 0.00002977
Iteration 116/1000 | Loss: 0.00002977
Iteration 117/1000 | Loss: 0.00002977
Iteration 118/1000 | Loss: 0.00002977
Iteration 119/1000 | Loss: 0.00002977
Iteration 120/1000 | Loss: 0.00002977
Iteration 121/1000 | Loss: 0.00002977
Iteration 122/1000 | Loss: 0.00002977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.9771925255772658e-05, 2.9771925255772658e-05, 2.9771925255772658e-05, 2.9771925255772658e-05, 2.9771925255772658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9771925255772658e-05

Optimization complete. Final v2v error: 3.9640913009643555 mm

Highest mean error: 19.842845916748047 mm for frame 25

Lowest mean error: 2.9179258346557617 mm for frame 238

Saving results

Total time: 120.34163403511047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433654
Iteration 2/25 | Loss: 0.00126259
Iteration 3/25 | Loss: 0.00117697
Iteration 4/25 | Loss: 0.00115641
Iteration 5/25 | Loss: 0.00115179
Iteration 6/25 | Loss: 0.00115097
Iteration 7/25 | Loss: 0.00115097
Iteration 8/25 | Loss: 0.00115097
Iteration 9/25 | Loss: 0.00115097
Iteration 10/25 | Loss: 0.00115097
Iteration 11/25 | Loss: 0.00115097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011509695323184133, 0.0011509695323184133, 0.0011509695323184133, 0.0011509695323184133, 0.0011509695323184133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011509695323184133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26808488
Iteration 2/25 | Loss: 0.00162854
Iteration 3/25 | Loss: 0.00162853
Iteration 4/25 | Loss: 0.00162853
Iteration 5/25 | Loss: 0.00162853
Iteration 6/25 | Loss: 0.00162853
Iteration 7/25 | Loss: 0.00162853
Iteration 8/25 | Loss: 0.00162853
Iteration 9/25 | Loss: 0.00162853
Iteration 10/25 | Loss: 0.00162853
Iteration 11/25 | Loss: 0.00162853
Iteration 12/25 | Loss: 0.00162853
Iteration 13/25 | Loss: 0.00162853
Iteration 14/25 | Loss: 0.00162853
Iteration 15/25 | Loss: 0.00162853
Iteration 16/25 | Loss: 0.00162853
Iteration 17/25 | Loss: 0.00162853
Iteration 18/25 | Loss: 0.00162853
Iteration 19/25 | Loss: 0.00162853
Iteration 20/25 | Loss: 0.00162853
Iteration 21/25 | Loss: 0.00162853
Iteration 22/25 | Loss: 0.00162853
Iteration 23/25 | Loss: 0.00162853
Iteration 24/25 | Loss: 0.00162853
Iteration 25/25 | Loss: 0.00162853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162853
Iteration 2/1000 | Loss: 0.00004202
Iteration 3/1000 | Loss: 0.00002554
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001771
Iteration 10/1000 | Loss: 0.00001760
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001758
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001728
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001721
Iteration 22/1000 | Loss: 0.00001721
Iteration 23/1000 | Loss: 0.00001720
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001715
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001715
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001711
Iteration 46/1000 | Loss: 0.00001711
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001711
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001707
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001706
Iteration 77/1000 | Loss: 0.00001706
Iteration 78/1000 | Loss: 0.00001706
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001703
Iteration 96/1000 | Loss: 0.00001703
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00001703
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001702
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001701
Iteration 113/1000 | Loss: 0.00001701
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001701
Iteration 119/1000 | Loss: 0.00001701
Iteration 120/1000 | Loss: 0.00001701
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001699
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001699
Iteration 142/1000 | Loss: 0.00001699
Iteration 143/1000 | Loss: 0.00001699
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001699
Iteration 147/1000 | Loss: 0.00001699
Iteration 148/1000 | Loss: 0.00001699
Iteration 149/1000 | Loss: 0.00001699
Iteration 150/1000 | Loss: 0.00001699
Iteration 151/1000 | Loss: 0.00001699
Iteration 152/1000 | Loss: 0.00001699
Iteration 153/1000 | Loss: 0.00001699
Iteration 154/1000 | Loss: 0.00001699
Iteration 155/1000 | Loss: 0.00001699
Iteration 156/1000 | Loss: 0.00001699
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001699
Iteration 164/1000 | Loss: 0.00001699
Iteration 165/1000 | Loss: 0.00001699
Iteration 166/1000 | Loss: 0.00001699
Iteration 167/1000 | Loss: 0.00001699
Iteration 168/1000 | Loss: 0.00001699
Iteration 169/1000 | Loss: 0.00001699
Iteration 170/1000 | Loss: 0.00001699
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001699
Iteration 176/1000 | Loss: 0.00001699
Iteration 177/1000 | Loss: 0.00001699
Iteration 178/1000 | Loss: 0.00001699
Iteration 179/1000 | Loss: 0.00001699
Iteration 180/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6986843547783792e-05, 1.6986843547783792e-05, 1.6986843547783792e-05, 1.6986843547783792e-05, 1.6986843547783792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6986843547783792e-05

Optimization complete. Final v2v error: 3.567192792892456 mm

Highest mean error: 3.8030800819396973 mm for frame 39

Lowest mean error: 3.157259941101074 mm for frame 0

Saving results

Total time: 35.93693828582764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412470
Iteration 2/25 | Loss: 0.00134870
Iteration 3/25 | Loss: 0.00117417
Iteration 4/25 | Loss: 0.00115483
Iteration 5/25 | Loss: 0.00115258
Iteration 6/25 | Loss: 0.00115206
Iteration 7/25 | Loss: 0.00115206
Iteration 8/25 | Loss: 0.00115206
Iteration 9/25 | Loss: 0.00115206
Iteration 10/25 | Loss: 0.00115206
Iteration 11/25 | Loss: 0.00115206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011520631378516555, 0.0011520631378516555, 0.0011520631378516555, 0.0011520631378516555, 0.0011520631378516555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011520631378516555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23039877
Iteration 2/25 | Loss: 0.00135744
Iteration 3/25 | Loss: 0.00135743
Iteration 4/25 | Loss: 0.00135743
Iteration 5/25 | Loss: 0.00135743
Iteration 6/25 | Loss: 0.00135743
Iteration 7/25 | Loss: 0.00135743
Iteration 8/25 | Loss: 0.00135743
Iteration 9/25 | Loss: 0.00135743
Iteration 10/25 | Loss: 0.00135743
Iteration 11/25 | Loss: 0.00135743
Iteration 12/25 | Loss: 0.00135743
Iteration 13/25 | Loss: 0.00135743
Iteration 14/25 | Loss: 0.00135743
Iteration 15/25 | Loss: 0.00135743
Iteration 16/25 | Loss: 0.00135743
Iteration 17/25 | Loss: 0.00135743
Iteration 18/25 | Loss: 0.00135743
Iteration 19/25 | Loss: 0.00135743
Iteration 20/25 | Loss: 0.00135743
Iteration 21/25 | Loss: 0.00135743
Iteration 22/25 | Loss: 0.00135743
Iteration 23/25 | Loss: 0.00135743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013574310578405857, 0.0013574310578405857, 0.0013574310578405857, 0.0013574310578405857, 0.0013574310578405857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013574310578405857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135743
Iteration 2/1000 | Loss: 0.00003098
Iteration 3/1000 | Loss: 0.00001998
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001375
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001288
Iteration 17/1000 | Loss: 0.00001288
Iteration 18/1000 | Loss: 0.00001287
Iteration 19/1000 | Loss: 0.00001287
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001277
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001273
Iteration 34/1000 | Loss: 0.00001273
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001269
Iteration 47/1000 | Loss: 0.00001269
Iteration 48/1000 | Loss: 0.00001269
Iteration 49/1000 | Loss: 0.00001269
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001267
Iteration 64/1000 | Loss: 0.00001267
Iteration 65/1000 | Loss: 0.00001267
Iteration 66/1000 | Loss: 0.00001267
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001266
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001264
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001255
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001254
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001253
Iteration 147/1000 | Loss: 0.00001253
Iteration 148/1000 | Loss: 0.00001253
Iteration 149/1000 | Loss: 0.00001253
Iteration 150/1000 | Loss: 0.00001253
Iteration 151/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.2534575944300741e-05, 1.2534575944300741e-05, 1.2534575944300741e-05, 1.2534575944300741e-05, 1.2534575944300741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2534575944300741e-05

Optimization complete. Final v2v error: 3.043980360031128 mm

Highest mean error: 3.431929588317871 mm for frame 124

Lowest mean error: 2.793506145477295 mm for frame 102

Saving results

Total time: 35.4474892616272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457449
Iteration 2/25 | Loss: 0.00131511
Iteration 3/25 | Loss: 0.00118715
Iteration 4/25 | Loss: 0.00117967
Iteration 5/25 | Loss: 0.00117797
Iteration 6/25 | Loss: 0.00117797
Iteration 7/25 | Loss: 0.00117797
Iteration 8/25 | Loss: 0.00117797
Iteration 9/25 | Loss: 0.00117797
Iteration 10/25 | Loss: 0.00117797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011779674096032977, 0.0011779674096032977, 0.0011779674096032977, 0.0011779674096032977, 0.0011779674096032977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011779674096032977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75608891
Iteration 2/25 | Loss: 0.00128233
Iteration 3/25 | Loss: 0.00128233
Iteration 4/25 | Loss: 0.00128232
Iteration 5/25 | Loss: 0.00128232
Iteration 6/25 | Loss: 0.00128232
Iteration 7/25 | Loss: 0.00128232
Iteration 8/25 | Loss: 0.00128232
Iteration 9/25 | Loss: 0.00128232
Iteration 10/25 | Loss: 0.00128232
Iteration 11/25 | Loss: 0.00128232
Iteration 12/25 | Loss: 0.00128232
Iteration 13/25 | Loss: 0.00128232
Iteration 14/25 | Loss: 0.00128232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012823233846575022, 0.0012823233846575022, 0.0012823233846575022, 0.0012823233846575022, 0.0012823233846575022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012823233846575022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128232
Iteration 2/1000 | Loss: 0.00006069
Iteration 3/1000 | Loss: 0.00003253
Iteration 4/1000 | Loss: 0.00002739
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002381
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002222
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002166
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002154
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002121
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002093
Iteration 18/1000 | Loss: 0.00002093
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002092
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00002091
Iteration 23/1000 | Loss: 0.00002089
Iteration 24/1000 | Loss: 0.00002073
Iteration 25/1000 | Loss: 0.00002072
Iteration 26/1000 | Loss: 0.00002072
Iteration 27/1000 | Loss: 0.00002069
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00002050
Iteration 30/1000 | Loss: 0.00002037
Iteration 31/1000 | Loss: 0.00002022
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002014
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00002004
Iteration 39/1000 | Loss: 0.00002004
Iteration 40/1000 | Loss: 0.00002003
Iteration 41/1000 | Loss: 0.00002003
Iteration 42/1000 | Loss: 0.00002003
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002002
Iteration 45/1000 | Loss: 0.00002002
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001999
Iteration 50/1000 | Loss: 0.00001999
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001999
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001998
Iteration 56/1000 | Loss: 0.00001998
Iteration 57/1000 | Loss: 0.00001997
Iteration 58/1000 | Loss: 0.00001997
Iteration 59/1000 | Loss: 0.00001995
Iteration 60/1000 | Loss: 0.00001995
Iteration 61/1000 | Loss: 0.00001995
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001993
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001993
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001992
Iteration 74/1000 | Loss: 0.00001992
Iteration 75/1000 | Loss: 0.00001992
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001990
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001990
Iteration 80/1000 | Loss: 0.00001990
Iteration 81/1000 | Loss: 0.00001990
Iteration 82/1000 | Loss: 0.00001990
Iteration 83/1000 | Loss: 0.00001990
Iteration 84/1000 | Loss: 0.00001990
Iteration 85/1000 | Loss: 0.00001989
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001989
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001988
Iteration 94/1000 | Loss: 0.00001988
Iteration 95/1000 | Loss: 0.00001988
Iteration 96/1000 | Loss: 0.00001988
Iteration 97/1000 | Loss: 0.00001988
Iteration 98/1000 | Loss: 0.00001988
Iteration 99/1000 | Loss: 0.00001988
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001987
Iteration 102/1000 | Loss: 0.00001987
Iteration 103/1000 | Loss: 0.00001987
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001987
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001986
Iteration 109/1000 | Loss: 0.00001985
Iteration 110/1000 | Loss: 0.00001985
Iteration 111/1000 | Loss: 0.00001985
Iteration 112/1000 | Loss: 0.00001984
Iteration 113/1000 | Loss: 0.00001984
Iteration 114/1000 | Loss: 0.00001984
Iteration 115/1000 | Loss: 0.00001984
Iteration 116/1000 | Loss: 0.00001984
Iteration 117/1000 | Loss: 0.00001983
Iteration 118/1000 | Loss: 0.00001983
Iteration 119/1000 | Loss: 0.00001983
Iteration 120/1000 | Loss: 0.00001983
Iteration 121/1000 | Loss: 0.00001983
Iteration 122/1000 | Loss: 0.00001983
Iteration 123/1000 | Loss: 0.00001983
Iteration 124/1000 | Loss: 0.00001983
Iteration 125/1000 | Loss: 0.00001983
Iteration 126/1000 | Loss: 0.00001983
Iteration 127/1000 | Loss: 0.00001983
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001982
Iteration 130/1000 | Loss: 0.00001982
Iteration 131/1000 | Loss: 0.00001982
Iteration 132/1000 | Loss: 0.00001982
Iteration 133/1000 | Loss: 0.00001982
Iteration 134/1000 | Loss: 0.00001982
Iteration 135/1000 | Loss: 0.00001982
Iteration 136/1000 | Loss: 0.00001982
Iteration 137/1000 | Loss: 0.00001982
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001981
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001980
Iteration 149/1000 | Loss: 0.00001980
Iteration 150/1000 | Loss: 0.00001980
Iteration 151/1000 | Loss: 0.00001980
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Iteration 155/1000 | Loss: 0.00001979
Iteration 156/1000 | Loss: 0.00001979
Iteration 157/1000 | Loss: 0.00001979
Iteration 158/1000 | Loss: 0.00001978
Iteration 159/1000 | Loss: 0.00001978
Iteration 160/1000 | Loss: 0.00001978
Iteration 161/1000 | Loss: 0.00001977
Iteration 162/1000 | Loss: 0.00001977
Iteration 163/1000 | Loss: 0.00001977
Iteration 164/1000 | Loss: 0.00001976
Iteration 165/1000 | Loss: 0.00001976
Iteration 166/1000 | Loss: 0.00001976
Iteration 167/1000 | Loss: 0.00001976
Iteration 168/1000 | Loss: 0.00001976
Iteration 169/1000 | Loss: 0.00001975
Iteration 170/1000 | Loss: 0.00001975
Iteration 171/1000 | Loss: 0.00001975
Iteration 172/1000 | Loss: 0.00001975
Iteration 173/1000 | Loss: 0.00001975
Iteration 174/1000 | Loss: 0.00001975
Iteration 175/1000 | Loss: 0.00001975
Iteration 176/1000 | Loss: 0.00001975
Iteration 177/1000 | Loss: 0.00001975
Iteration 178/1000 | Loss: 0.00001975
Iteration 179/1000 | Loss: 0.00001975
Iteration 180/1000 | Loss: 0.00001975
Iteration 181/1000 | Loss: 0.00001975
Iteration 182/1000 | Loss: 0.00001975
Iteration 183/1000 | Loss: 0.00001975
Iteration 184/1000 | Loss: 0.00001975
Iteration 185/1000 | Loss: 0.00001975
Iteration 186/1000 | Loss: 0.00001975
Iteration 187/1000 | Loss: 0.00001975
Iteration 188/1000 | Loss: 0.00001975
Iteration 189/1000 | Loss: 0.00001975
Iteration 190/1000 | Loss: 0.00001975
Iteration 191/1000 | Loss: 0.00001975
Iteration 192/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.9749917555600405e-05, 1.9749917555600405e-05, 1.9749917555600405e-05, 1.9749917555600405e-05, 1.9749917555600405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9749917555600405e-05

Optimization complete. Final v2v error: 3.693431854248047 mm

Highest mean error: 4.10705041885376 mm for frame 240

Lowest mean error: 3.446589469909668 mm for frame 185

Saving results

Total time: 59.703731536865234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043139
Iteration 2/25 | Loss: 0.00247858
Iteration 3/25 | Loss: 0.00138431
Iteration 4/25 | Loss: 0.00125858
Iteration 5/25 | Loss: 0.00124605
Iteration 6/25 | Loss: 0.00124294
Iteration 7/25 | Loss: 0.00124286
Iteration 8/25 | Loss: 0.00124286
Iteration 9/25 | Loss: 0.00124286
Iteration 10/25 | Loss: 0.00124286
Iteration 11/25 | Loss: 0.00124286
Iteration 12/25 | Loss: 0.00124286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012428577756509185, 0.0012428577756509185, 0.0012428577756509185, 0.0012428577756509185, 0.0012428577756509185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012428577756509185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84374690
Iteration 2/25 | Loss: 0.00105268
Iteration 3/25 | Loss: 0.00105268
Iteration 4/25 | Loss: 0.00105268
Iteration 5/25 | Loss: 0.00105268
Iteration 6/25 | Loss: 0.00105268
Iteration 7/25 | Loss: 0.00105268
Iteration 8/25 | Loss: 0.00105268
Iteration 9/25 | Loss: 0.00105268
Iteration 10/25 | Loss: 0.00105268
Iteration 11/25 | Loss: 0.00105268
Iteration 12/25 | Loss: 0.00105268
Iteration 13/25 | Loss: 0.00105268
Iteration 14/25 | Loss: 0.00105268
Iteration 15/25 | Loss: 0.00105268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010526806581765413, 0.0010526806581765413, 0.0010526806581765413, 0.0010526806581765413, 0.0010526806581765413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010526806581765413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105268
Iteration 2/1000 | Loss: 0.00005413
Iteration 3/1000 | Loss: 0.00002986
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002214
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00002020
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001943
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001936
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001934
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001928
Iteration 49/1000 | Loss: 0.00001928
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001927
Iteration 53/1000 | Loss: 0.00001927
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001926
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001926
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001926
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001925
Iteration 67/1000 | Loss: 0.00001925
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00001925
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001925
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001925
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001925
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001923
Iteration 82/1000 | Loss: 0.00001923
Iteration 83/1000 | Loss: 0.00001923
Iteration 84/1000 | Loss: 0.00001923
Iteration 85/1000 | Loss: 0.00001923
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001922
Iteration 92/1000 | Loss: 0.00001922
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001922
Iteration 95/1000 | Loss: 0.00001922
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001922
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001922
Iteration 107/1000 | Loss: 0.00001922
Iteration 108/1000 | Loss: 0.00001922
Iteration 109/1000 | Loss: 0.00001922
Iteration 110/1000 | Loss: 0.00001922
Iteration 111/1000 | Loss: 0.00001922
Iteration 112/1000 | Loss: 0.00001922
Iteration 113/1000 | Loss: 0.00001922
Iteration 114/1000 | Loss: 0.00001922
Iteration 115/1000 | Loss: 0.00001922
Iteration 116/1000 | Loss: 0.00001922
Iteration 117/1000 | Loss: 0.00001922
Iteration 118/1000 | Loss: 0.00001922
Iteration 119/1000 | Loss: 0.00001922
Iteration 120/1000 | Loss: 0.00001922
Iteration 121/1000 | Loss: 0.00001922
Iteration 122/1000 | Loss: 0.00001922
Iteration 123/1000 | Loss: 0.00001922
Iteration 124/1000 | Loss: 0.00001922
Iteration 125/1000 | Loss: 0.00001922
Iteration 126/1000 | Loss: 0.00001922
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.921719012898393e-05, 1.921719012898393e-05, 1.921719012898393e-05, 1.921719012898393e-05, 1.921719012898393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.921719012898393e-05

Optimization complete. Final v2v error: 3.769150733947754 mm

Highest mean error: 4.070392608642578 mm for frame 98

Lowest mean error: 3.5526304244995117 mm for frame 85

Saving results

Total time: 35.10904359817505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416578
Iteration 2/25 | Loss: 0.00126910
Iteration 3/25 | Loss: 0.00117780
Iteration 4/25 | Loss: 0.00115607
Iteration 5/25 | Loss: 0.00114710
Iteration 6/25 | Loss: 0.00114458
Iteration 7/25 | Loss: 0.00114432
Iteration 8/25 | Loss: 0.00114432
Iteration 9/25 | Loss: 0.00114432
Iteration 10/25 | Loss: 0.00114432
Iteration 11/25 | Loss: 0.00114432
Iteration 12/25 | Loss: 0.00114432
Iteration 13/25 | Loss: 0.00114432
Iteration 14/25 | Loss: 0.00114432
Iteration 15/25 | Loss: 0.00114432
Iteration 16/25 | Loss: 0.00114432
Iteration 17/25 | Loss: 0.00114432
Iteration 18/25 | Loss: 0.00114432
Iteration 19/25 | Loss: 0.00114432
Iteration 20/25 | Loss: 0.00114432
Iteration 21/25 | Loss: 0.00114432
Iteration 22/25 | Loss: 0.00114432
Iteration 23/25 | Loss: 0.00114432
Iteration 24/25 | Loss: 0.00114432
Iteration 25/25 | Loss: 0.00114432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31857932
Iteration 2/25 | Loss: 0.00167889
Iteration 3/25 | Loss: 0.00167889
Iteration 4/25 | Loss: 0.00167889
Iteration 5/25 | Loss: 0.00167889
Iteration 6/25 | Loss: 0.00167889
Iteration 7/25 | Loss: 0.00167888
Iteration 8/25 | Loss: 0.00167888
Iteration 9/25 | Loss: 0.00167888
Iteration 10/25 | Loss: 0.00167888
Iteration 11/25 | Loss: 0.00167888
Iteration 12/25 | Loss: 0.00167888
Iteration 13/25 | Loss: 0.00167888
Iteration 14/25 | Loss: 0.00167888
Iteration 15/25 | Loss: 0.00167888
Iteration 16/25 | Loss: 0.00167888
Iteration 17/25 | Loss: 0.00167888
Iteration 18/25 | Loss: 0.00167888
Iteration 19/25 | Loss: 0.00167888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016788843786343932, 0.0016788843786343932, 0.0016788843786343932, 0.0016788843786343932, 0.0016788843786343932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016788843786343932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167888
Iteration 2/1000 | Loss: 0.00003282
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001719
Iteration 11/1000 | Loss: 0.00001718
Iteration 12/1000 | Loss: 0.00001717
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001709
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001702
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001672
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001670
Iteration 30/1000 | Loss: 0.00001670
Iteration 31/1000 | Loss: 0.00001669
Iteration 32/1000 | Loss: 0.00001669
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00001669
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001667
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001665
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001665
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001664
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001663
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001662
Iteration 56/1000 | Loss: 0.00001662
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001661
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001660
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001657
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Iteration 103/1000 | Loss: 0.00001656
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001655
Iteration 106/1000 | Loss: 0.00001655
Iteration 107/1000 | Loss: 0.00001655
Iteration 108/1000 | Loss: 0.00001655
Iteration 109/1000 | Loss: 0.00001655
Iteration 110/1000 | Loss: 0.00001655
Iteration 111/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6551666703890078e-05, 1.6551666703890078e-05, 1.6551666703890078e-05, 1.6551666703890078e-05, 1.6551666703890078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6551666703890078e-05

Optimization complete. Final v2v error: 3.519287347793579 mm

Highest mean error: 3.9343619346618652 mm for frame 116

Lowest mean error: 3.117788553237915 mm for frame 165

Saving results

Total time: 35.22094964981079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875400
Iteration 2/25 | Loss: 0.00151541
Iteration 3/25 | Loss: 0.00123684
Iteration 4/25 | Loss: 0.00120764
Iteration 5/25 | Loss: 0.00120514
Iteration 6/25 | Loss: 0.00120499
Iteration 7/25 | Loss: 0.00120499
Iteration 8/25 | Loss: 0.00120499
Iteration 9/25 | Loss: 0.00120499
Iteration 10/25 | Loss: 0.00120499
Iteration 11/25 | Loss: 0.00120499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012049939250573516, 0.0012049939250573516, 0.0012049939250573516, 0.0012049939250573516, 0.0012049939250573516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012049939250573516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87261319
Iteration 2/25 | Loss: 0.00075873
Iteration 3/25 | Loss: 0.00075873
Iteration 4/25 | Loss: 0.00075873
Iteration 5/25 | Loss: 0.00075873
Iteration 6/25 | Loss: 0.00075873
Iteration 7/25 | Loss: 0.00075873
Iteration 8/25 | Loss: 0.00075873
Iteration 9/25 | Loss: 0.00075873
Iteration 10/25 | Loss: 0.00075873
Iteration 11/25 | Loss: 0.00075873
Iteration 12/25 | Loss: 0.00075873
Iteration 13/25 | Loss: 0.00075873
Iteration 14/25 | Loss: 0.00075873
Iteration 15/25 | Loss: 0.00075873
Iteration 16/25 | Loss: 0.00075873
Iteration 17/25 | Loss: 0.00075873
Iteration 18/25 | Loss: 0.00075873
Iteration 19/25 | Loss: 0.00075873
Iteration 20/25 | Loss: 0.00075873
Iteration 21/25 | Loss: 0.00075873
Iteration 22/25 | Loss: 0.00075873
Iteration 23/25 | Loss: 0.00075873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007587266736663878, 0.0007587266736663878, 0.0007587266736663878, 0.0007587266736663878, 0.0007587266736663878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007587266736663878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075873
Iteration 2/1000 | Loss: 0.00003878
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00002419
Iteration 5/1000 | Loss: 0.00002276
Iteration 6/1000 | Loss: 0.00002191
Iteration 7/1000 | Loss: 0.00002146
Iteration 8/1000 | Loss: 0.00002118
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002098
Iteration 11/1000 | Loss: 0.00002090
Iteration 12/1000 | Loss: 0.00002090
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002090
Iteration 15/1000 | Loss: 0.00002090
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00002089
Iteration 18/1000 | Loss: 0.00002089
Iteration 19/1000 | Loss: 0.00002089
Iteration 20/1000 | Loss: 0.00002089
Iteration 21/1000 | Loss: 0.00002088
Iteration 22/1000 | Loss: 0.00002088
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002088
Iteration 25/1000 | Loss: 0.00002087
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002086
Iteration 30/1000 | Loss: 0.00002086
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002086
Iteration 34/1000 | Loss: 0.00002086
Iteration 35/1000 | Loss: 0.00002086
Iteration 36/1000 | Loss: 0.00002086
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002086
Iteration 39/1000 | Loss: 0.00002086
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002071
Iteration 45/1000 | Loss: 0.00002070
Iteration 46/1000 | Loss: 0.00002070
Iteration 47/1000 | Loss: 0.00002069
Iteration 48/1000 | Loss: 0.00002069
Iteration 49/1000 | Loss: 0.00002068
Iteration 50/1000 | Loss: 0.00002067
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00002066
Iteration 53/1000 | Loss: 0.00002066
Iteration 54/1000 | Loss: 0.00002066
Iteration 55/1000 | Loss: 0.00002066
Iteration 56/1000 | Loss: 0.00002065
Iteration 57/1000 | Loss: 0.00002065
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002065
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002064
Iteration 62/1000 | Loss: 0.00002063
Iteration 63/1000 | Loss: 0.00002063
Iteration 64/1000 | Loss: 0.00002062
Iteration 65/1000 | Loss: 0.00002062
Iteration 66/1000 | Loss: 0.00002062
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002061
Iteration 69/1000 | Loss: 0.00002061
Iteration 70/1000 | Loss: 0.00002061
Iteration 71/1000 | Loss: 0.00002061
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002061
Iteration 74/1000 | Loss: 0.00002061
Iteration 75/1000 | Loss: 0.00002061
Iteration 76/1000 | Loss: 0.00002061
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002061
Iteration 81/1000 | Loss: 0.00002061
Iteration 82/1000 | Loss: 0.00002061
Iteration 83/1000 | Loss: 0.00002061
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002060
Iteration 88/1000 | Loss: 0.00002060
Iteration 89/1000 | Loss: 0.00002060
Iteration 90/1000 | Loss: 0.00002060
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002060
Iteration 104/1000 | Loss: 0.00002060
Iteration 105/1000 | Loss: 0.00002060
Iteration 106/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.0603560187737457e-05, 2.0603560187737457e-05, 2.0603560187737457e-05, 2.0603560187737457e-05, 2.0603560187737457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0603560187737457e-05

Optimization complete. Final v2v error: 3.8966903686523438 mm

Highest mean error: 4.3536577224731445 mm for frame 1

Lowest mean error: 3.711993455886841 mm for frame 156

Saving results

Total time: 30.092439889907837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078979
Iteration 2/25 | Loss: 0.00185358
Iteration 3/25 | Loss: 0.00150077
Iteration 4/25 | Loss: 0.00136634
Iteration 5/25 | Loss: 0.00134935
Iteration 6/25 | Loss: 0.00130248
Iteration 7/25 | Loss: 0.00130138
Iteration 8/25 | Loss: 0.00128697
Iteration 9/25 | Loss: 0.00126083
Iteration 10/25 | Loss: 0.00125754
Iteration 11/25 | Loss: 0.00127489
Iteration 12/25 | Loss: 0.00125571
Iteration 13/25 | Loss: 0.00124690
Iteration 14/25 | Loss: 0.00124600
Iteration 15/25 | Loss: 0.00124583
Iteration 16/25 | Loss: 0.00124582
Iteration 17/25 | Loss: 0.00124576
Iteration 18/25 | Loss: 0.00124576
Iteration 19/25 | Loss: 0.00124576
Iteration 20/25 | Loss: 0.00124576
Iteration 21/25 | Loss: 0.00124576
Iteration 22/25 | Loss: 0.00124576
Iteration 23/25 | Loss: 0.00124576
Iteration 24/25 | Loss: 0.00124576
Iteration 25/25 | Loss: 0.00124576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45807409
Iteration 2/25 | Loss: 0.00187248
Iteration 3/25 | Loss: 0.00187247
Iteration 4/25 | Loss: 0.00187247
Iteration 5/25 | Loss: 0.00187247
Iteration 6/25 | Loss: 0.00187247
Iteration 7/25 | Loss: 0.00187247
Iteration 8/25 | Loss: 0.00187247
Iteration 9/25 | Loss: 0.00187247
Iteration 10/25 | Loss: 0.00187247
Iteration 11/25 | Loss: 0.00187247
Iteration 12/25 | Loss: 0.00187247
Iteration 13/25 | Loss: 0.00187247
Iteration 14/25 | Loss: 0.00187247
Iteration 15/25 | Loss: 0.00187247
Iteration 16/25 | Loss: 0.00187247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018724707188084722, 0.0018724707188084722, 0.0018724707188084722, 0.0018724707188084722, 0.0018724707188084722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018724707188084722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187247
Iteration 2/1000 | Loss: 0.00231323
Iteration 3/1000 | Loss: 0.00161806
Iteration 4/1000 | Loss: 0.00224716
Iteration 5/1000 | Loss: 0.00359990
Iteration 6/1000 | Loss: 0.00006164
Iteration 7/1000 | Loss: 0.00004096
Iteration 8/1000 | Loss: 0.00002819
Iteration 9/1000 | Loss: 0.00002421
Iteration 10/1000 | Loss: 0.00002249
Iteration 11/1000 | Loss: 0.00002128
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00001961
Iteration 14/1000 | Loss: 0.00001917
Iteration 15/1000 | Loss: 0.00001883
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001858
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001830
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001826
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001823
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001818
Iteration 32/1000 | Loss: 0.00001818
Iteration 33/1000 | Loss: 0.00001817
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001815
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001814
Iteration 38/1000 | Loss: 0.00001814
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001811
Iteration 43/1000 | Loss: 0.00001811
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001803
Iteration 70/1000 | Loss: 0.00001803
Iteration 71/1000 | Loss: 0.00001803
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001802
Iteration 76/1000 | Loss: 0.00001801
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001800
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001799
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001798
Iteration 84/1000 | Loss: 0.00001798
Iteration 85/1000 | Loss: 0.00001798
Iteration 86/1000 | Loss: 0.00001798
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001797
Iteration 94/1000 | Loss: 0.00001797
Iteration 95/1000 | Loss: 0.00001797
Iteration 96/1000 | Loss: 0.00001797
Iteration 97/1000 | Loss: 0.00001797
Iteration 98/1000 | Loss: 0.00001797
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001795
Iteration 108/1000 | Loss: 0.00001795
Iteration 109/1000 | Loss: 0.00001795
Iteration 110/1000 | Loss: 0.00001795
Iteration 111/1000 | Loss: 0.00001795
Iteration 112/1000 | Loss: 0.00001795
Iteration 113/1000 | Loss: 0.00001795
Iteration 114/1000 | Loss: 0.00001795
Iteration 115/1000 | Loss: 0.00001795
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001794
Iteration 120/1000 | Loss: 0.00001794
Iteration 121/1000 | Loss: 0.00001794
Iteration 122/1000 | Loss: 0.00001794
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001793
Iteration 129/1000 | Loss: 0.00001792
Iteration 130/1000 | Loss: 0.00001792
Iteration 131/1000 | Loss: 0.00001792
Iteration 132/1000 | Loss: 0.00001792
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001792
Iteration 135/1000 | Loss: 0.00001792
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001791
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001790
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001790
Iteration 146/1000 | Loss: 0.00001789
Iteration 147/1000 | Loss: 0.00001789
Iteration 148/1000 | Loss: 0.00001789
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001788
Iteration 152/1000 | Loss: 0.00001788
Iteration 153/1000 | Loss: 0.00001787
Iteration 154/1000 | Loss: 0.00001787
Iteration 155/1000 | Loss: 0.00001787
Iteration 156/1000 | Loss: 0.00001787
Iteration 157/1000 | Loss: 0.00001786
Iteration 158/1000 | Loss: 0.00001786
Iteration 159/1000 | Loss: 0.00001786
Iteration 160/1000 | Loss: 0.00001786
Iteration 161/1000 | Loss: 0.00001786
Iteration 162/1000 | Loss: 0.00001786
Iteration 163/1000 | Loss: 0.00001786
Iteration 164/1000 | Loss: 0.00001786
Iteration 165/1000 | Loss: 0.00001786
Iteration 166/1000 | Loss: 0.00001786
Iteration 167/1000 | Loss: 0.00001786
Iteration 168/1000 | Loss: 0.00001785
Iteration 169/1000 | Loss: 0.00001785
Iteration 170/1000 | Loss: 0.00001785
Iteration 171/1000 | Loss: 0.00001785
Iteration 172/1000 | Loss: 0.00001785
Iteration 173/1000 | Loss: 0.00001785
Iteration 174/1000 | Loss: 0.00001785
Iteration 175/1000 | Loss: 0.00001785
Iteration 176/1000 | Loss: 0.00001785
Iteration 177/1000 | Loss: 0.00001785
Iteration 178/1000 | Loss: 0.00001785
Iteration 179/1000 | Loss: 0.00001785
Iteration 180/1000 | Loss: 0.00001785
Iteration 181/1000 | Loss: 0.00001785
Iteration 182/1000 | Loss: 0.00001784
Iteration 183/1000 | Loss: 0.00001784
Iteration 184/1000 | Loss: 0.00001784
Iteration 185/1000 | Loss: 0.00001784
Iteration 186/1000 | Loss: 0.00001784
Iteration 187/1000 | Loss: 0.00001784
Iteration 188/1000 | Loss: 0.00001784
Iteration 189/1000 | Loss: 0.00001784
Iteration 190/1000 | Loss: 0.00001784
Iteration 191/1000 | Loss: 0.00001784
Iteration 192/1000 | Loss: 0.00001784
Iteration 193/1000 | Loss: 0.00001784
Iteration 194/1000 | Loss: 0.00001784
Iteration 195/1000 | Loss: 0.00001784
Iteration 196/1000 | Loss: 0.00001784
Iteration 197/1000 | Loss: 0.00001784
Iteration 198/1000 | Loss: 0.00001784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.784225423762109e-05, 1.784225423762109e-05, 1.784225423762109e-05, 1.784225423762109e-05, 1.784225423762109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.784225423762109e-05

Optimization complete. Final v2v error: 3.6147854328155518 mm

Highest mean error: 5.104291915893555 mm for frame 125

Lowest mean error: 3.27219820022583 mm for frame 153

Saving results

Total time: 69.12176871299744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073690
Iteration 2/25 | Loss: 0.01073690
Iteration 3/25 | Loss: 0.01073690
Iteration 4/25 | Loss: 0.01073689
Iteration 5/25 | Loss: 0.01073689
Iteration 6/25 | Loss: 0.01073689
Iteration 7/25 | Loss: 0.01073689
Iteration 8/25 | Loss: 0.01073689
Iteration 9/25 | Loss: 0.01073689
Iteration 10/25 | Loss: 0.01073689
Iteration 11/25 | Loss: 0.01073689
Iteration 12/25 | Loss: 0.01073689
Iteration 13/25 | Loss: 0.01073689
Iteration 14/25 | Loss: 0.01073689
Iteration 15/25 | Loss: 0.01073689
Iteration 16/25 | Loss: 0.01073689
Iteration 17/25 | Loss: 0.01073689
Iteration 18/25 | Loss: 0.01073689
Iteration 19/25 | Loss: 0.01073689
Iteration 20/25 | Loss: 0.01073689
Iteration 21/25 | Loss: 0.01073689
Iteration 22/25 | Loss: 0.01073688
Iteration 23/25 | Loss: 0.01073688
Iteration 24/25 | Loss: 0.01073688
Iteration 25/25 | Loss: 0.01073688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40011370
Iteration 2/25 | Loss: 0.09928805
Iteration 3/25 | Loss: 0.09860405
Iteration 4/25 | Loss: 0.09785020
Iteration 5/25 | Loss: 0.09785011
Iteration 6/25 | Loss: 0.09785011
Iteration 7/25 | Loss: 0.09785010
Iteration 8/25 | Loss: 0.09785010
Iteration 9/25 | Loss: 0.09785008
Iteration 10/25 | Loss: 0.09785008
Iteration 11/25 | Loss: 0.09785007
Iteration 12/25 | Loss: 0.09785007
Iteration 13/25 | Loss: 0.09785007
Iteration 14/25 | Loss: 0.09785007
Iteration 15/25 | Loss: 0.09785007
Iteration 16/25 | Loss: 0.09785007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.09785006940364838, 0.09785006940364838, 0.09785006940364838, 0.09785006940364838, 0.09785006940364838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09785006940364838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09785007
Iteration 2/1000 | Loss: 0.00430234
Iteration 3/1000 | Loss: 0.00207024
Iteration 4/1000 | Loss: 0.00149672
Iteration 5/1000 | Loss: 0.00034146
Iteration 6/1000 | Loss: 0.00139238
Iteration 7/1000 | Loss: 0.00015721
Iteration 8/1000 | Loss: 0.00030077
Iteration 9/1000 | Loss: 0.00014235
Iteration 10/1000 | Loss: 0.00008392
Iteration 11/1000 | Loss: 0.00059363
Iteration 12/1000 | Loss: 0.00022244
Iteration 13/1000 | Loss: 0.00007775
Iteration 14/1000 | Loss: 0.00005406
Iteration 15/1000 | Loss: 0.00108336
Iteration 16/1000 | Loss: 0.00079934
Iteration 17/1000 | Loss: 0.00048461
Iteration 18/1000 | Loss: 0.00004489
Iteration 19/1000 | Loss: 0.00003704
Iteration 20/1000 | Loss: 0.00006536
Iteration 21/1000 | Loss: 0.00003231
Iteration 22/1000 | Loss: 0.00013505
Iteration 23/1000 | Loss: 0.00003076
Iteration 24/1000 | Loss: 0.00002958
Iteration 25/1000 | Loss: 0.00008389
Iteration 26/1000 | Loss: 0.00003048
Iteration 27/1000 | Loss: 0.00009841
Iteration 28/1000 | Loss: 0.00002607
Iteration 29/1000 | Loss: 0.00011684
Iteration 30/1000 | Loss: 0.00010773
Iteration 31/1000 | Loss: 0.00002437
Iteration 32/1000 | Loss: 0.00002371
Iteration 33/1000 | Loss: 0.00002299
Iteration 34/1000 | Loss: 0.00002456
Iteration 35/1000 | Loss: 0.00002300
Iteration 36/1000 | Loss: 0.00005031
Iteration 37/1000 | Loss: 0.00008264
Iteration 38/1000 | Loss: 0.00005477
Iteration 39/1000 | Loss: 0.00003365
Iteration 40/1000 | Loss: 0.00009286
Iteration 41/1000 | Loss: 0.00004973
Iteration 42/1000 | Loss: 0.00004599
Iteration 43/1000 | Loss: 0.00003138
Iteration 44/1000 | Loss: 0.00002714
Iteration 45/1000 | Loss: 0.00002473
Iteration 46/1000 | Loss: 0.00003769
Iteration 47/1000 | Loss: 0.00007107
Iteration 48/1000 | Loss: 0.00002950
Iteration 49/1000 | Loss: 0.00003658
Iteration 50/1000 | Loss: 0.00006095
Iteration 51/1000 | Loss: 0.00004087
Iteration 52/1000 | Loss: 0.00004928
Iteration 53/1000 | Loss: 0.00003051
Iteration 54/1000 | Loss: 0.00004608
Iteration 55/1000 | Loss: 0.00002982
Iteration 56/1000 | Loss: 0.00004741
Iteration 57/1000 | Loss: 0.00015427
Iteration 58/1000 | Loss: 0.00005819
Iteration 59/1000 | Loss: 0.00005164
Iteration 60/1000 | Loss: 0.00004641
Iteration 61/1000 | Loss: 0.00003604
Iteration 62/1000 | Loss: 0.00004819
Iteration 63/1000 | Loss: 0.00002427
Iteration 64/1000 | Loss: 0.00002231
Iteration 65/1000 | Loss: 0.00002229
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002084
Iteration 69/1000 | Loss: 0.00018235
Iteration 70/1000 | Loss: 0.00002081
Iteration 71/1000 | Loss: 0.00002051
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002046
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00005096
Iteration 80/1000 | Loss: 0.00003211
Iteration 81/1000 | Loss: 0.00002613
Iteration 82/1000 | Loss: 0.00003411
Iteration 83/1000 | Loss: 0.00002921
Iteration 84/1000 | Loss: 0.00002249
Iteration 85/1000 | Loss: 0.00002427
Iteration 86/1000 | Loss: 0.00002136
Iteration 87/1000 | Loss: 0.00002090
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002018
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00003429
Iteration 93/1000 | Loss: 0.00003973
Iteration 94/1000 | Loss: 0.00002886
Iteration 95/1000 | Loss: 0.00003438
Iteration 96/1000 | Loss: 0.00007993
Iteration 97/1000 | Loss: 0.00002025
Iteration 98/1000 | Loss: 0.00002002
Iteration 99/1000 | Loss: 0.00003427
Iteration 100/1000 | Loss: 0.00006052
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00005414
Iteration 103/1000 | Loss: 0.00002193
Iteration 104/1000 | Loss: 0.00003144
Iteration 105/1000 | Loss: 0.00002203
Iteration 106/1000 | Loss: 0.00005784
Iteration 107/1000 | Loss: 0.00002234
Iteration 108/1000 | Loss: 0.00004099
Iteration 109/1000 | Loss: 0.00002100
Iteration 110/1000 | Loss: 0.00004987
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002013
Iteration 113/1000 | Loss: 0.00002089
Iteration 114/1000 | Loss: 0.00002089
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002136
Iteration 117/1000 | Loss: 0.00002030
Iteration 118/1000 | Loss: 0.00002087
Iteration 119/1000 | Loss: 0.00002047
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002068
Iteration 122/1000 | Loss: 0.00007907
Iteration 123/1000 | Loss: 0.00002487
Iteration 124/1000 | Loss: 0.00004042
Iteration 125/1000 | Loss: 0.00002640
Iteration 126/1000 | Loss: 0.00002174
Iteration 127/1000 | Loss: 0.00002009
Iteration 128/1000 | Loss: 0.00001992
Iteration 129/1000 | Loss: 0.00001991
Iteration 130/1000 | Loss: 0.00001990
Iteration 131/1000 | Loss: 0.00001990
Iteration 132/1000 | Loss: 0.00001989
Iteration 133/1000 | Loss: 0.00001989
Iteration 134/1000 | Loss: 0.00001989
Iteration 135/1000 | Loss: 0.00001989
Iteration 136/1000 | Loss: 0.00001989
Iteration 137/1000 | Loss: 0.00001988
Iteration 138/1000 | Loss: 0.00001988
Iteration 139/1000 | Loss: 0.00001988
Iteration 140/1000 | Loss: 0.00001988
Iteration 141/1000 | Loss: 0.00001988
Iteration 142/1000 | Loss: 0.00001988
Iteration 143/1000 | Loss: 0.00001988
Iteration 144/1000 | Loss: 0.00001987
Iteration 145/1000 | Loss: 0.00001987
Iteration 146/1000 | Loss: 0.00001987
Iteration 147/1000 | Loss: 0.00001987
Iteration 148/1000 | Loss: 0.00001987
Iteration 149/1000 | Loss: 0.00001987
Iteration 150/1000 | Loss: 0.00001987
Iteration 151/1000 | Loss: 0.00001987
Iteration 152/1000 | Loss: 0.00001986
Iteration 153/1000 | Loss: 0.00001986
Iteration 154/1000 | Loss: 0.00001986
Iteration 155/1000 | Loss: 0.00001986
Iteration 156/1000 | Loss: 0.00001986
Iteration 157/1000 | Loss: 0.00001986
Iteration 158/1000 | Loss: 0.00001985
Iteration 159/1000 | Loss: 0.00001985
Iteration 160/1000 | Loss: 0.00001985
Iteration 161/1000 | Loss: 0.00001985
Iteration 162/1000 | Loss: 0.00001984
Iteration 163/1000 | Loss: 0.00001984
Iteration 164/1000 | Loss: 0.00008346
Iteration 165/1000 | Loss: 0.00002066
Iteration 166/1000 | Loss: 0.00001995
Iteration 167/1000 | Loss: 0.00001987
Iteration 168/1000 | Loss: 0.00001985
Iteration 169/1000 | Loss: 0.00001984
Iteration 170/1000 | Loss: 0.00001984
Iteration 171/1000 | Loss: 0.00001983
Iteration 172/1000 | Loss: 0.00001983
Iteration 173/1000 | Loss: 0.00001983
Iteration 174/1000 | Loss: 0.00001983
Iteration 175/1000 | Loss: 0.00001982
Iteration 176/1000 | Loss: 0.00001982
Iteration 177/1000 | Loss: 0.00001981
Iteration 178/1000 | Loss: 0.00001981
Iteration 179/1000 | Loss: 0.00001981
Iteration 180/1000 | Loss: 0.00001980
Iteration 181/1000 | Loss: 0.00001980
Iteration 182/1000 | Loss: 0.00001980
Iteration 183/1000 | Loss: 0.00001979
Iteration 184/1000 | Loss: 0.00001979
Iteration 185/1000 | Loss: 0.00001979
Iteration 186/1000 | Loss: 0.00001978
Iteration 187/1000 | Loss: 0.00001978
Iteration 188/1000 | Loss: 0.00001978
Iteration 189/1000 | Loss: 0.00001978
Iteration 190/1000 | Loss: 0.00001978
Iteration 191/1000 | Loss: 0.00001977
Iteration 192/1000 | Loss: 0.00001977
Iteration 193/1000 | Loss: 0.00001977
Iteration 194/1000 | Loss: 0.00001977
Iteration 195/1000 | Loss: 0.00001976
Iteration 196/1000 | Loss: 0.00001976
Iteration 197/1000 | Loss: 0.00001976
Iteration 198/1000 | Loss: 0.00001976
Iteration 199/1000 | Loss: 0.00001976
Iteration 200/1000 | Loss: 0.00001976
Iteration 201/1000 | Loss: 0.00001976
Iteration 202/1000 | Loss: 0.00001976
Iteration 203/1000 | Loss: 0.00001976
Iteration 204/1000 | Loss: 0.00001975
Iteration 205/1000 | Loss: 0.00001975
Iteration 206/1000 | Loss: 0.00001975
Iteration 207/1000 | Loss: 0.00001975
Iteration 208/1000 | Loss: 0.00001975
Iteration 209/1000 | Loss: 0.00001975
Iteration 210/1000 | Loss: 0.00001975
Iteration 211/1000 | Loss: 0.00001975
Iteration 212/1000 | Loss: 0.00001975
Iteration 213/1000 | Loss: 0.00001975
Iteration 214/1000 | Loss: 0.00001975
Iteration 215/1000 | Loss: 0.00001975
Iteration 216/1000 | Loss: 0.00001975
Iteration 217/1000 | Loss: 0.00001975
Iteration 218/1000 | Loss: 0.00001975
Iteration 219/1000 | Loss: 0.00001975
Iteration 220/1000 | Loss: 0.00001974
Iteration 221/1000 | Loss: 0.00001974
Iteration 222/1000 | Loss: 0.00001974
Iteration 223/1000 | Loss: 0.00001974
Iteration 224/1000 | Loss: 0.00001974
Iteration 225/1000 | Loss: 0.00001974
Iteration 226/1000 | Loss: 0.00001974
Iteration 227/1000 | Loss: 0.00001974
Iteration 228/1000 | Loss: 0.00001974
Iteration 229/1000 | Loss: 0.00001974
Iteration 230/1000 | Loss: 0.00001974
Iteration 231/1000 | Loss: 0.00001974
Iteration 232/1000 | Loss: 0.00001974
Iteration 233/1000 | Loss: 0.00001974
Iteration 234/1000 | Loss: 0.00001974
Iteration 235/1000 | Loss: 0.00001974
Iteration 236/1000 | Loss: 0.00001974
Iteration 237/1000 | Loss: 0.00001974
Iteration 238/1000 | Loss: 0.00001973
Iteration 239/1000 | Loss: 0.00001973
Iteration 240/1000 | Loss: 0.00001973
Iteration 241/1000 | Loss: 0.00001973
Iteration 242/1000 | Loss: 0.00001973
Iteration 243/1000 | Loss: 0.00001973
Iteration 244/1000 | Loss: 0.00001973
Iteration 245/1000 | Loss: 0.00001973
Iteration 246/1000 | Loss: 0.00001973
Iteration 247/1000 | Loss: 0.00001973
Iteration 248/1000 | Loss: 0.00001973
Iteration 249/1000 | Loss: 0.00001973
Iteration 250/1000 | Loss: 0.00001973
Iteration 251/1000 | Loss: 0.00001973
Iteration 252/1000 | Loss: 0.00001973
Iteration 253/1000 | Loss: 0.00001972
Iteration 254/1000 | Loss: 0.00001972
Iteration 255/1000 | Loss: 0.00001972
Iteration 256/1000 | Loss: 0.00001972
Iteration 257/1000 | Loss: 0.00001972
Iteration 258/1000 | Loss: 0.00001972
Iteration 259/1000 | Loss: 0.00001972
Iteration 260/1000 | Loss: 0.00001972
Iteration 261/1000 | Loss: 0.00001972
Iteration 262/1000 | Loss: 0.00001971
Iteration 263/1000 | Loss: 0.00001971
Iteration 264/1000 | Loss: 0.00001971
Iteration 265/1000 | Loss: 0.00001971
Iteration 266/1000 | Loss: 0.00001971
Iteration 267/1000 | Loss: 0.00001971
Iteration 268/1000 | Loss: 0.00001971
Iteration 269/1000 | Loss: 0.00001971
Iteration 270/1000 | Loss: 0.00001971
Iteration 271/1000 | Loss: 0.00001971
Iteration 272/1000 | Loss: 0.00001971
Iteration 273/1000 | Loss: 0.00001971
Iteration 274/1000 | Loss: 0.00001971
Iteration 275/1000 | Loss: 0.00001971
Iteration 276/1000 | Loss: 0.00001970
Iteration 277/1000 | Loss: 0.00001970
Iteration 278/1000 | Loss: 0.00001970
Iteration 279/1000 | Loss: 0.00001970
Iteration 280/1000 | Loss: 0.00001970
Iteration 281/1000 | Loss: 0.00001970
Iteration 282/1000 | Loss: 0.00001970
Iteration 283/1000 | Loss: 0.00001970
Iteration 284/1000 | Loss: 0.00001970
Iteration 285/1000 | Loss: 0.00001970
Iteration 286/1000 | Loss: 0.00001970
Iteration 287/1000 | Loss: 0.00001970
Iteration 288/1000 | Loss: 0.00001970
Iteration 289/1000 | Loss: 0.00001970
Iteration 290/1000 | Loss: 0.00001969
Iteration 291/1000 | Loss: 0.00001969
Iteration 292/1000 | Loss: 0.00001969
Iteration 293/1000 | Loss: 0.00001969
Iteration 294/1000 | Loss: 0.00001969
Iteration 295/1000 | Loss: 0.00001969
Iteration 296/1000 | Loss: 0.00001969
Iteration 297/1000 | Loss: 0.00001969
Iteration 298/1000 | Loss: 0.00001969
Iteration 299/1000 | Loss: 0.00001969
Iteration 300/1000 | Loss: 0.00001969
Iteration 301/1000 | Loss: 0.00001969
Iteration 302/1000 | Loss: 0.00001968
Iteration 303/1000 | Loss: 0.00001968
Iteration 304/1000 | Loss: 0.00001968
Iteration 305/1000 | Loss: 0.00001968
Iteration 306/1000 | Loss: 0.00001968
Iteration 307/1000 | Loss: 0.00001968
Iteration 308/1000 | Loss: 0.00001968
Iteration 309/1000 | Loss: 0.00001968
Iteration 310/1000 | Loss: 0.00002130
Iteration 311/1000 | Loss: 0.00002008
Iteration 312/1000 | Loss: 0.00001975
Iteration 313/1000 | Loss: 0.00001970
Iteration 314/1000 | Loss: 0.00001969
Iteration 315/1000 | Loss: 0.00001969
Iteration 316/1000 | Loss: 0.00001968
Iteration 317/1000 | Loss: 0.00001967
Iteration 318/1000 | Loss: 0.00001967
Iteration 319/1000 | Loss: 0.00001966
Iteration 320/1000 | Loss: 0.00001966
Iteration 321/1000 | Loss: 0.00001966
Iteration 322/1000 | Loss: 0.00001965
Iteration 323/1000 | Loss: 0.00001964
Iteration 324/1000 | Loss: 0.00001964
Iteration 325/1000 | Loss: 0.00001964
Iteration 326/1000 | Loss: 0.00001964
Iteration 327/1000 | Loss: 0.00001964
Iteration 328/1000 | Loss: 0.00001963
Iteration 329/1000 | Loss: 0.00001963
Iteration 330/1000 | Loss: 0.00001963
Iteration 331/1000 | Loss: 0.00001963
Iteration 332/1000 | Loss: 0.00001963
Iteration 333/1000 | Loss: 0.00001963
Iteration 334/1000 | Loss: 0.00001963
Iteration 335/1000 | Loss: 0.00001963
Iteration 336/1000 | Loss: 0.00001963
Iteration 337/1000 | Loss: 0.00001963
Iteration 338/1000 | Loss: 0.00001963
Iteration 339/1000 | Loss: 0.00001963
Iteration 340/1000 | Loss: 0.00001962
Iteration 341/1000 | Loss: 0.00001962
Iteration 342/1000 | Loss: 0.00001962
Iteration 343/1000 | Loss: 0.00001962
Iteration 344/1000 | Loss: 0.00001962
Iteration 345/1000 | Loss: 0.00001962
Iteration 346/1000 | Loss: 0.00001962
Iteration 347/1000 | Loss: 0.00001962
Iteration 348/1000 | Loss: 0.00001962
Iteration 349/1000 | Loss: 0.00001962
Iteration 350/1000 | Loss: 0.00001962
Iteration 351/1000 | Loss: 0.00001962
Iteration 352/1000 | Loss: 0.00001962
Iteration 353/1000 | Loss: 0.00001962
Iteration 354/1000 | Loss: 0.00001962
Iteration 355/1000 | Loss: 0.00001962
Iteration 356/1000 | Loss: 0.00001962
Iteration 357/1000 | Loss: 0.00001962
Iteration 358/1000 | Loss: 0.00001962
Iteration 359/1000 | Loss: 0.00001962
Iteration 360/1000 | Loss: 0.00001962
Iteration 361/1000 | Loss: 0.00001961
Iteration 362/1000 | Loss: 0.00001961
Iteration 363/1000 | Loss: 0.00001961
Iteration 364/1000 | Loss: 0.00001961
Iteration 365/1000 | Loss: 0.00001961
Iteration 366/1000 | Loss: 0.00001961
Iteration 367/1000 | Loss: 0.00001961
Iteration 368/1000 | Loss: 0.00001961
Iteration 369/1000 | Loss: 0.00001961
Iteration 370/1000 | Loss: 0.00001961
Iteration 371/1000 | Loss: 0.00001961
Iteration 372/1000 | Loss: 0.00001961
Iteration 373/1000 | Loss: 0.00001961
Iteration 374/1000 | Loss: 0.00001961
Iteration 375/1000 | Loss: 0.00001961
Iteration 376/1000 | Loss: 0.00001961
Iteration 377/1000 | Loss: 0.00001961
Iteration 378/1000 | Loss: 0.00001961
Iteration 379/1000 | Loss: 0.00001960
Iteration 380/1000 | Loss: 0.00001960
Iteration 381/1000 | Loss: 0.00001960
Iteration 382/1000 | Loss: 0.00001960
Iteration 383/1000 | Loss: 0.00001960
Iteration 384/1000 | Loss: 0.00001960
Iteration 385/1000 | Loss: 0.00001960
Iteration 386/1000 | Loss: 0.00001960
Iteration 387/1000 | Loss: 0.00001960
Iteration 388/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 388. Stopping optimization.
Last 5 losses: [1.9604221961344592e-05, 1.9604221961344592e-05, 1.9604221961344592e-05, 1.9604221961344592e-05, 1.9604221961344592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9604221961344592e-05

Optimization complete. Final v2v error: 3.4755632877349854 mm

Highest mean error: 9.428267478942871 mm for frame 133

Lowest mean error: 2.8373210430145264 mm for frame 107

Saving results

Total time: 229.87888503074646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850625
Iteration 2/25 | Loss: 0.00128636
Iteration 3/25 | Loss: 0.00119851
Iteration 4/25 | Loss: 0.00118133
Iteration 5/25 | Loss: 0.00117461
Iteration 6/25 | Loss: 0.00117273
Iteration 7/25 | Loss: 0.00117273
Iteration 8/25 | Loss: 0.00117273
Iteration 9/25 | Loss: 0.00117273
Iteration 10/25 | Loss: 0.00117273
Iteration 11/25 | Loss: 0.00117273
Iteration 12/25 | Loss: 0.00117273
Iteration 13/25 | Loss: 0.00117273
Iteration 14/25 | Loss: 0.00117273
Iteration 15/25 | Loss: 0.00117273
Iteration 16/25 | Loss: 0.00117273
Iteration 17/25 | Loss: 0.00117273
Iteration 18/25 | Loss: 0.00117273
Iteration 19/25 | Loss: 0.00117273
Iteration 20/25 | Loss: 0.00117273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011727336095646024, 0.0011727336095646024, 0.0011727336095646024, 0.0011727336095646024, 0.0011727336095646024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011727336095646024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21678925
Iteration 2/25 | Loss: 0.00236839
Iteration 3/25 | Loss: 0.00236839
Iteration 4/25 | Loss: 0.00236839
Iteration 5/25 | Loss: 0.00236839
Iteration 6/25 | Loss: 0.00236839
Iteration 7/25 | Loss: 0.00236839
Iteration 8/25 | Loss: 0.00236839
Iteration 9/25 | Loss: 0.00236839
Iteration 10/25 | Loss: 0.00236839
Iteration 11/25 | Loss: 0.00236839
Iteration 12/25 | Loss: 0.00236839
Iteration 13/25 | Loss: 0.00236839
Iteration 14/25 | Loss: 0.00236839
Iteration 15/25 | Loss: 0.00236839
Iteration 16/25 | Loss: 0.00236839
Iteration 17/25 | Loss: 0.00236839
Iteration 18/25 | Loss: 0.00236839
Iteration 19/25 | Loss: 0.00236839
Iteration 20/25 | Loss: 0.00236839
Iteration 21/25 | Loss: 0.00236839
Iteration 22/25 | Loss: 0.00236839
Iteration 23/25 | Loss: 0.00236839
Iteration 24/25 | Loss: 0.00236839
Iteration 25/25 | Loss: 0.00236839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236839
Iteration 2/1000 | Loss: 0.00006592
Iteration 3/1000 | Loss: 0.00003460
Iteration 4/1000 | Loss: 0.00002637
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002032
Iteration 8/1000 | Loss: 0.00001982
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001908
Iteration 11/1000 | Loss: 0.00001880
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001840
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001826
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001810
Iteration 21/1000 | Loss: 0.00001809
Iteration 22/1000 | Loss: 0.00001809
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001800
Iteration 30/1000 | Loss: 0.00001799
Iteration 31/1000 | Loss: 0.00001799
Iteration 32/1000 | Loss: 0.00001798
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001793
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001790
Iteration 43/1000 | Loss: 0.00001790
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001786
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001786
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001766
Iteration 137/1000 | Loss: 0.00001766
Iteration 138/1000 | Loss: 0.00001766
Iteration 139/1000 | Loss: 0.00001766
Iteration 140/1000 | Loss: 0.00001766
Iteration 141/1000 | Loss: 0.00001766
Iteration 142/1000 | Loss: 0.00001766
Iteration 143/1000 | Loss: 0.00001766
Iteration 144/1000 | Loss: 0.00001766
Iteration 145/1000 | Loss: 0.00001766
Iteration 146/1000 | Loss: 0.00001766
Iteration 147/1000 | Loss: 0.00001766
Iteration 148/1000 | Loss: 0.00001766
Iteration 149/1000 | Loss: 0.00001766
Iteration 150/1000 | Loss: 0.00001765
Iteration 151/1000 | Loss: 0.00001765
Iteration 152/1000 | Loss: 0.00001765
Iteration 153/1000 | Loss: 0.00001765
Iteration 154/1000 | Loss: 0.00001765
Iteration 155/1000 | Loss: 0.00001765
Iteration 156/1000 | Loss: 0.00001765
Iteration 157/1000 | Loss: 0.00001765
Iteration 158/1000 | Loss: 0.00001765
Iteration 159/1000 | Loss: 0.00001765
Iteration 160/1000 | Loss: 0.00001765
Iteration 161/1000 | Loss: 0.00001765
Iteration 162/1000 | Loss: 0.00001764
Iteration 163/1000 | Loss: 0.00001764
Iteration 164/1000 | Loss: 0.00001764
Iteration 165/1000 | Loss: 0.00001764
Iteration 166/1000 | Loss: 0.00001764
Iteration 167/1000 | Loss: 0.00001764
Iteration 168/1000 | Loss: 0.00001764
Iteration 169/1000 | Loss: 0.00001764
Iteration 170/1000 | Loss: 0.00001764
Iteration 171/1000 | Loss: 0.00001764
Iteration 172/1000 | Loss: 0.00001764
Iteration 173/1000 | Loss: 0.00001764
Iteration 174/1000 | Loss: 0.00001764
Iteration 175/1000 | Loss: 0.00001764
Iteration 176/1000 | Loss: 0.00001764
Iteration 177/1000 | Loss: 0.00001764
Iteration 178/1000 | Loss: 0.00001764
Iteration 179/1000 | Loss: 0.00001763
Iteration 180/1000 | Loss: 0.00001763
Iteration 181/1000 | Loss: 0.00001763
Iteration 182/1000 | Loss: 0.00001763
Iteration 183/1000 | Loss: 0.00001763
Iteration 184/1000 | Loss: 0.00001763
Iteration 185/1000 | Loss: 0.00001763
Iteration 186/1000 | Loss: 0.00001763
Iteration 187/1000 | Loss: 0.00001763
Iteration 188/1000 | Loss: 0.00001763
Iteration 189/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.763129148457665e-05, 1.763129148457665e-05, 1.763129148457665e-05, 1.763129148457665e-05, 1.763129148457665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.763129148457665e-05

Optimization complete. Final v2v error: 3.464160680770874 mm

Highest mean error: 4.385237216949463 mm for frame 26

Lowest mean error: 3.0893189907073975 mm for frame 79

Saving results

Total time: 51.508195877075195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082861
Iteration 2/25 | Loss: 0.01082861
Iteration 3/25 | Loss: 0.01082860
Iteration 4/25 | Loss: 0.01082860
Iteration 5/25 | Loss: 0.01082860
Iteration 6/25 | Loss: 0.01082860
Iteration 7/25 | Loss: 0.00285547
Iteration 8/25 | Loss: 0.00189581
Iteration 9/25 | Loss: 0.00150271
Iteration 10/25 | Loss: 0.00135949
Iteration 11/25 | Loss: 0.00133780
Iteration 12/25 | Loss: 0.00132315
Iteration 13/25 | Loss: 0.00129548
Iteration 14/25 | Loss: 0.00129060
Iteration 15/25 | Loss: 0.00129480
Iteration 16/25 | Loss: 0.00127873
Iteration 17/25 | Loss: 0.00127394
Iteration 18/25 | Loss: 0.00126337
Iteration 19/25 | Loss: 0.00125694
Iteration 20/25 | Loss: 0.00125542
Iteration 21/25 | Loss: 0.00125460
Iteration 22/25 | Loss: 0.00125423
Iteration 23/25 | Loss: 0.00125407
Iteration 24/25 | Loss: 0.00125317
Iteration 25/25 | Loss: 0.00125221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20790815
Iteration 2/25 | Loss: 0.00536535
Iteration 3/25 | Loss: 0.00315584
Iteration 4/25 | Loss: 0.00315583
Iteration 5/25 | Loss: 0.00315583
Iteration 6/25 | Loss: 0.00315583
Iteration 7/25 | Loss: 0.00315583
Iteration 8/25 | Loss: 0.00315583
Iteration 9/25 | Loss: 0.00315583
Iteration 10/25 | Loss: 0.00315583
Iteration 11/25 | Loss: 0.00315583
Iteration 12/25 | Loss: 0.00315583
Iteration 13/25 | Loss: 0.00315583
Iteration 14/25 | Loss: 0.00315583
Iteration 15/25 | Loss: 0.00315583
Iteration 16/25 | Loss: 0.00315583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003155831713229418, 0.003155831713229418, 0.003155831713229418, 0.003155831713229418, 0.003155831713229418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003155831713229418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315583
Iteration 2/1000 | Loss: 0.00032972
Iteration 3/1000 | Loss: 0.00070060
Iteration 4/1000 | Loss: 0.00096623
Iteration 5/1000 | Loss: 0.00068590
Iteration 6/1000 | Loss: 0.00016651
Iteration 7/1000 | Loss: 0.00022778
Iteration 8/1000 | Loss: 0.00104474
Iteration 9/1000 | Loss: 0.00018531
Iteration 10/1000 | Loss: 0.00026960
Iteration 11/1000 | Loss: 0.00011703
Iteration 12/1000 | Loss: 0.00025256
Iteration 13/1000 | Loss: 0.00011757
Iteration 14/1000 | Loss: 0.00010470
Iteration 15/1000 | Loss: 0.00009299
Iteration 16/1000 | Loss: 0.00008962
Iteration 17/1000 | Loss: 0.00012921
Iteration 18/1000 | Loss: 0.00034338
Iteration 19/1000 | Loss: 0.00009720
Iteration 20/1000 | Loss: 0.00008913
Iteration 21/1000 | Loss: 0.00083929
Iteration 22/1000 | Loss: 0.00226612
Iteration 23/1000 | Loss: 0.00161916
Iteration 24/1000 | Loss: 0.00353313
Iteration 25/1000 | Loss: 0.00179959
Iteration 26/1000 | Loss: 0.00181674
Iteration 27/1000 | Loss: 0.00427030
Iteration 28/1000 | Loss: 0.00285297
Iteration 29/1000 | Loss: 0.00156102
Iteration 30/1000 | Loss: 0.00133512
Iteration 31/1000 | Loss: 0.00063038
Iteration 32/1000 | Loss: 0.00369495
Iteration 33/1000 | Loss: 0.00124309
Iteration 34/1000 | Loss: 0.00068989
Iteration 35/1000 | Loss: 0.00084557
Iteration 36/1000 | Loss: 0.00169791
Iteration 37/1000 | Loss: 0.00076633
Iteration 38/1000 | Loss: 0.00079606
Iteration 39/1000 | Loss: 0.00077251
Iteration 40/1000 | Loss: 0.00029568
Iteration 41/1000 | Loss: 0.00008479
Iteration 42/1000 | Loss: 0.00006569
Iteration 43/1000 | Loss: 0.00084632
Iteration 44/1000 | Loss: 0.00039146
Iteration 45/1000 | Loss: 0.00077601
Iteration 46/1000 | Loss: 0.00062675
Iteration 47/1000 | Loss: 0.00019704
Iteration 48/1000 | Loss: 0.00016173
Iteration 49/1000 | Loss: 0.00017173
Iteration 50/1000 | Loss: 0.00042619
Iteration 51/1000 | Loss: 0.00104271
Iteration 52/1000 | Loss: 0.00084540
Iteration 53/1000 | Loss: 0.00050950
Iteration 54/1000 | Loss: 0.00056788
Iteration 55/1000 | Loss: 0.00066819
Iteration 56/1000 | Loss: 0.00030130
Iteration 57/1000 | Loss: 0.00042718
Iteration 58/1000 | Loss: 0.00036585
Iteration 59/1000 | Loss: 0.00023610
Iteration 60/1000 | Loss: 0.00015499
Iteration 61/1000 | Loss: 0.00026277
Iteration 62/1000 | Loss: 0.00029612
Iteration 63/1000 | Loss: 0.00019406
Iteration 64/1000 | Loss: 0.00016099
Iteration 65/1000 | Loss: 0.00017071
Iteration 66/1000 | Loss: 0.00005518
Iteration 67/1000 | Loss: 0.00004719
Iteration 68/1000 | Loss: 0.00004225
Iteration 69/1000 | Loss: 0.00003843
Iteration 70/1000 | Loss: 0.00032533
Iteration 71/1000 | Loss: 0.00009652
Iteration 72/1000 | Loss: 0.00004736
Iteration 73/1000 | Loss: 0.00004019
Iteration 74/1000 | Loss: 0.00003941
Iteration 75/1000 | Loss: 0.00003427
Iteration 76/1000 | Loss: 0.00003228
Iteration 77/1000 | Loss: 0.00029867
Iteration 78/1000 | Loss: 0.00025852
Iteration 79/1000 | Loss: 0.00011289
Iteration 80/1000 | Loss: 0.00015182
Iteration 81/1000 | Loss: 0.00009319
Iteration 82/1000 | Loss: 0.00033259
Iteration 83/1000 | Loss: 0.00004313
Iteration 84/1000 | Loss: 0.00003609
Iteration 85/1000 | Loss: 0.00003337
Iteration 86/1000 | Loss: 0.00052926
Iteration 87/1000 | Loss: 0.00003758
Iteration 88/1000 | Loss: 0.00003105
Iteration 89/1000 | Loss: 0.00002883
Iteration 90/1000 | Loss: 0.00002779
Iteration 91/1000 | Loss: 0.00002691
Iteration 92/1000 | Loss: 0.00002649
Iteration 93/1000 | Loss: 0.00002642
Iteration 94/1000 | Loss: 0.00002596
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002563
Iteration 97/1000 | Loss: 0.00002557
Iteration 98/1000 | Loss: 0.00002552
Iteration 99/1000 | Loss: 0.00002542
Iteration 100/1000 | Loss: 0.00002541
Iteration 101/1000 | Loss: 0.00002537
Iteration 102/1000 | Loss: 0.00002537
Iteration 103/1000 | Loss: 0.00002537
Iteration 104/1000 | Loss: 0.00002537
Iteration 105/1000 | Loss: 0.00002537
Iteration 106/1000 | Loss: 0.00002537
Iteration 107/1000 | Loss: 0.00002537
Iteration 108/1000 | Loss: 0.00002537
Iteration 109/1000 | Loss: 0.00002537
Iteration 110/1000 | Loss: 0.00002537
Iteration 111/1000 | Loss: 0.00002536
Iteration 112/1000 | Loss: 0.00002536
Iteration 113/1000 | Loss: 0.00002532
Iteration 114/1000 | Loss: 0.00002531
Iteration 115/1000 | Loss: 0.00002529
Iteration 116/1000 | Loss: 0.00002528
Iteration 117/1000 | Loss: 0.00002528
Iteration 118/1000 | Loss: 0.00002527
Iteration 119/1000 | Loss: 0.00002527
Iteration 120/1000 | Loss: 0.00002527
Iteration 121/1000 | Loss: 0.00002526
Iteration 122/1000 | Loss: 0.00002526
Iteration 123/1000 | Loss: 0.00002526
Iteration 124/1000 | Loss: 0.00002526
Iteration 125/1000 | Loss: 0.00002526
Iteration 126/1000 | Loss: 0.00002526
Iteration 127/1000 | Loss: 0.00002526
Iteration 128/1000 | Loss: 0.00002525
Iteration 129/1000 | Loss: 0.00002525
Iteration 130/1000 | Loss: 0.00002525
Iteration 131/1000 | Loss: 0.00002525
Iteration 132/1000 | Loss: 0.00002525
Iteration 133/1000 | Loss: 0.00002525
Iteration 134/1000 | Loss: 0.00002525
Iteration 135/1000 | Loss: 0.00002525
Iteration 136/1000 | Loss: 0.00002525
Iteration 137/1000 | Loss: 0.00002525
Iteration 138/1000 | Loss: 0.00002525
Iteration 139/1000 | Loss: 0.00002525
Iteration 140/1000 | Loss: 0.00002524
Iteration 141/1000 | Loss: 0.00002524
Iteration 142/1000 | Loss: 0.00002524
Iteration 143/1000 | Loss: 0.00002524
Iteration 144/1000 | Loss: 0.00002524
Iteration 145/1000 | Loss: 0.00002524
Iteration 146/1000 | Loss: 0.00002524
Iteration 147/1000 | Loss: 0.00002524
Iteration 148/1000 | Loss: 0.00002524
Iteration 149/1000 | Loss: 0.00002524
Iteration 150/1000 | Loss: 0.00002524
Iteration 151/1000 | Loss: 0.00002524
Iteration 152/1000 | Loss: 0.00002523
Iteration 153/1000 | Loss: 0.00002523
Iteration 154/1000 | Loss: 0.00002522
Iteration 155/1000 | Loss: 0.00002522
Iteration 156/1000 | Loss: 0.00002522
Iteration 157/1000 | Loss: 0.00002522
Iteration 158/1000 | Loss: 0.00002522
Iteration 159/1000 | Loss: 0.00002522
Iteration 160/1000 | Loss: 0.00002522
Iteration 161/1000 | Loss: 0.00002522
Iteration 162/1000 | Loss: 0.00002553
Iteration 163/1000 | Loss: 0.00002553
Iteration 164/1000 | Loss: 0.00002527
Iteration 165/1000 | Loss: 0.00002520
Iteration 166/1000 | Loss: 0.00002520
Iteration 167/1000 | Loss: 0.00002520
Iteration 168/1000 | Loss: 0.00002519
Iteration 169/1000 | Loss: 0.00002519
Iteration 170/1000 | Loss: 0.00002519
Iteration 171/1000 | Loss: 0.00002519
Iteration 172/1000 | Loss: 0.00002519
Iteration 173/1000 | Loss: 0.00002519
Iteration 174/1000 | Loss: 0.00002519
Iteration 175/1000 | Loss: 0.00002519
Iteration 176/1000 | Loss: 0.00002519
Iteration 177/1000 | Loss: 0.00002519
Iteration 178/1000 | Loss: 0.00002519
Iteration 179/1000 | Loss: 0.00002519
Iteration 180/1000 | Loss: 0.00002518
Iteration 181/1000 | Loss: 0.00002518
Iteration 182/1000 | Loss: 0.00002518
Iteration 183/1000 | Loss: 0.00002518
Iteration 184/1000 | Loss: 0.00002518
Iteration 185/1000 | Loss: 0.00002518
Iteration 186/1000 | Loss: 0.00002518
Iteration 187/1000 | Loss: 0.00002518
Iteration 188/1000 | Loss: 0.00002518
Iteration 189/1000 | Loss: 0.00002517
Iteration 190/1000 | Loss: 0.00002517
Iteration 191/1000 | Loss: 0.00002516
Iteration 192/1000 | Loss: 0.00002516
Iteration 193/1000 | Loss: 0.00002516
Iteration 194/1000 | Loss: 0.00002516
Iteration 195/1000 | Loss: 0.00002515
Iteration 196/1000 | Loss: 0.00002515
Iteration 197/1000 | Loss: 0.00002515
Iteration 198/1000 | Loss: 0.00002515
Iteration 199/1000 | Loss: 0.00002515
Iteration 200/1000 | Loss: 0.00002515
Iteration 201/1000 | Loss: 0.00002515
Iteration 202/1000 | Loss: 0.00002515
Iteration 203/1000 | Loss: 0.00002515
Iteration 204/1000 | Loss: 0.00002515
Iteration 205/1000 | Loss: 0.00002514
Iteration 206/1000 | Loss: 0.00002514
Iteration 207/1000 | Loss: 0.00002514
Iteration 208/1000 | Loss: 0.00002514
Iteration 209/1000 | Loss: 0.00002514
Iteration 210/1000 | Loss: 0.00002514
Iteration 211/1000 | Loss: 0.00002514
Iteration 212/1000 | Loss: 0.00002514
Iteration 213/1000 | Loss: 0.00002514
Iteration 214/1000 | Loss: 0.00002513
Iteration 215/1000 | Loss: 0.00002513
Iteration 216/1000 | Loss: 0.00002513
Iteration 217/1000 | Loss: 0.00002513
Iteration 218/1000 | Loss: 0.00002513
Iteration 219/1000 | Loss: 0.00002513
Iteration 220/1000 | Loss: 0.00002513
Iteration 221/1000 | Loss: 0.00002513
Iteration 222/1000 | Loss: 0.00002513
Iteration 223/1000 | Loss: 0.00002513
Iteration 224/1000 | Loss: 0.00002513
Iteration 225/1000 | Loss: 0.00002512
Iteration 226/1000 | Loss: 0.00002512
Iteration 227/1000 | Loss: 0.00002512
Iteration 228/1000 | Loss: 0.00002512
Iteration 229/1000 | Loss: 0.00002512
Iteration 230/1000 | Loss: 0.00002511
Iteration 231/1000 | Loss: 0.00002511
Iteration 232/1000 | Loss: 0.00002511
Iteration 233/1000 | Loss: 0.00002511
Iteration 234/1000 | Loss: 0.00002511
Iteration 235/1000 | Loss: 0.00002511
Iteration 236/1000 | Loss: 0.00002510
Iteration 237/1000 | Loss: 0.00002510
Iteration 238/1000 | Loss: 0.00002510
Iteration 239/1000 | Loss: 0.00002510
Iteration 240/1000 | Loss: 0.00002510
Iteration 241/1000 | Loss: 0.00002510
Iteration 242/1000 | Loss: 0.00002510
Iteration 243/1000 | Loss: 0.00002510
Iteration 244/1000 | Loss: 0.00002510
Iteration 245/1000 | Loss: 0.00002510
Iteration 246/1000 | Loss: 0.00002510
Iteration 247/1000 | Loss: 0.00002509
Iteration 248/1000 | Loss: 0.00002509
Iteration 249/1000 | Loss: 0.00002509
Iteration 250/1000 | Loss: 0.00002509
Iteration 251/1000 | Loss: 0.00002509
Iteration 252/1000 | Loss: 0.00002509
Iteration 253/1000 | Loss: 0.00002509
Iteration 254/1000 | Loss: 0.00002508
Iteration 255/1000 | Loss: 0.00002508
Iteration 256/1000 | Loss: 0.00002508
Iteration 257/1000 | Loss: 0.00002508
Iteration 258/1000 | Loss: 0.00002508
Iteration 259/1000 | Loss: 0.00002508
Iteration 260/1000 | Loss: 0.00002508
Iteration 261/1000 | Loss: 0.00002508
Iteration 262/1000 | Loss: 0.00002508
Iteration 263/1000 | Loss: 0.00002508
Iteration 264/1000 | Loss: 0.00002508
Iteration 265/1000 | Loss: 0.00002508
Iteration 266/1000 | Loss: 0.00002508
Iteration 267/1000 | Loss: 0.00002508
Iteration 268/1000 | Loss: 0.00002508
Iteration 269/1000 | Loss: 0.00002508
Iteration 270/1000 | Loss: 0.00002508
Iteration 271/1000 | Loss: 0.00002508
Iteration 272/1000 | Loss: 0.00002508
Iteration 273/1000 | Loss: 0.00002508
Iteration 274/1000 | Loss: 0.00002508
Iteration 275/1000 | Loss: 0.00002508
Iteration 276/1000 | Loss: 0.00002508
Iteration 277/1000 | Loss: 0.00002507
Iteration 278/1000 | Loss: 0.00002533
Iteration 279/1000 | Loss: 0.00002533
Iteration 280/1000 | Loss: 0.00002512
Iteration 281/1000 | Loss: 0.00002510
Iteration 282/1000 | Loss: 0.00002509
Iteration 283/1000 | Loss: 0.00002508
Iteration 284/1000 | Loss: 0.00002507
Iteration 285/1000 | Loss: 0.00002507
Iteration 286/1000 | Loss: 0.00002507
Iteration 287/1000 | Loss: 0.00002507
Iteration 288/1000 | Loss: 0.00002507
Iteration 289/1000 | Loss: 0.00002507
Iteration 290/1000 | Loss: 0.00002507
Iteration 291/1000 | Loss: 0.00002507
Iteration 292/1000 | Loss: 0.00002507
Iteration 293/1000 | Loss: 0.00002507
Iteration 294/1000 | Loss: 0.00002507
Iteration 295/1000 | Loss: 0.00002507
Iteration 296/1000 | Loss: 0.00002507
Iteration 297/1000 | Loss: 0.00002507
Iteration 298/1000 | Loss: 0.00002507
Iteration 299/1000 | Loss: 0.00002507
Iteration 300/1000 | Loss: 0.00002507
Iteration 301/1000 | Loss: 0.00002507
Iteration 302/1000 | Loss: 0.00002507
Iteration 303/1000 | Loss: 0.00002507
Iteration 304/1000 | Loss: 0.00002507
Iteration 305/1000 | Loss: 0.00002507
Iteration 306/1000 | Loss: 0.00002507
Iteration 307/1000 | Loss: 0.00002507
Iteration 308/1000 | Loss: 0.00002507
Iteration 309/1000 | Loss: 0.00002507
Iteration 310/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [2.5065397494472563e-05, 2.5065397494472563e-05, 2.5065397494472563e-05, 2.5065397494472563e-05, 2.5065397494472563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5065397494472563e-05

Optimization complete. Final v2v error: 3.364342451095581 mm

Highest mean error: 20.65926742553711 mm for frame 80

Lowest mean error: 2.662680149078369 mm for frame 65

Saving results

Total time: 222.81358742713928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084284
Iteration 2/25 | Loss: 0.00227088
Iteration 3/25 | Loss: 0.00174132
Iteration 4/25 | Loss: 0.00190579
Iteration 5/25 | Loss: 0.00145186
Iteration 6/25 | Loss: 0.00137916
Iteration 7/25 | Loss: 0.00136630
Iteration 8/25 | Loss: 0.00136426
Iteration 9/25 | Loss: 0.00136310
Iteration 10/25 | Loss: 0.00136226
Iteration 11/25 | Loss: 0.00135897
Iteration 12/25 | Loss: 0.00135634
Iteration 13/25 | Loss: 0.00135594
Iteration 14/25 | Loss: 0.00135583
Iteration 15/25 | Loss: 0.00135578
Iteration 16/25 | Loss: 0.00135577
Iteration 17/25 | Loss: 0.00135577
Iteration 18/25 | Loss: 0.00135577
Iteration 19/25 | Loss: 0.00135577
Iteration 20/25 | Loss: 0.00135577
Iteration 21/25 | Loss: 0.00135577
Iteration 22/25 | Loss: 0.00135577
Iteration 23/25 | Loss: 0.00135577
Iteration 24/25 | Loss: 0.00135576
Iteration 25/25 | Loss: 0.00135576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23151553
Iteration 2/25 | Loss: 0.00178986
Iteration 3/25 | Loss: 0.00178986
Iteration 4/25 | Loss: 0.00178986
Iteration 5/25 | Loss: 0.00178986
Iteration 6/25 | Loss: 0.00178985
Iteration 7/25 | Loss: 0.00178985
Iteration 8/25 | Loss: 0.00178985
Iteration 9/25 | Loss: 0.00178985
Iteration 10/25 | Loss: 0.00178985
Iteration 11/25 | Loss: 0.00178985
Iteration 12/25 | Loss: 0.00178985
Iteration 13/25 | Loss: 0.00178985
Iteration 14/25 | Loss: 0.00178985
Iteration 15/25 | Loss: 0.00178985
Iteration 16/25 | Loss: 0.00178985
Iteration 17/25 | Loss: 0.00178985
Iteration 18/25 | Loss: 0.00178985
Iteration 19/25 | Loss: 0.00178985
Iteration 20/25 | Loss: 0.00178985
Iteration 21/25 | Loss: 0.00178985
Iteration 22/25 | Loss: 0.00178985
Iteration 23/25 | Loss: 0.00178985
Iteration 24/25 | Loss: 0.00178985
Iteration 25/25 | Loss: 0.00178985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017898534424602985, 0.0017898534424602985, 0.0017898534424602985, 0.0017898534424602985, 0.0017898534424602985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017898534424602985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178985
Iteration 2/1000 | Loss: 0.00016654
Iteration 3/1000 | Loss: 0.00009930
Iteration 4/1000 | Loss: 0.00010240
Iteration 5/1000 | Loss: 0.00004778
Iteration 6/1000 | Loss: 0.00004288
Iteration 7/1000 | Loss: 0.00003822
Iteration 8/1000 | Loss: 0.00005838
Iteration 9/1000 | Loss: 0.00003915
Iteration 10/1000 | Loss: 0.00003522
Iteration 11/1000 | Loss: 0.00006498
Iteration 12/1000 | Loss: 0.00003747
Iteration 13/1000 | Loss: 0.00003565
Iteration 14/1000 | Loss: 0.00003297
Iteration 15/1000 | Loss: 0.00003155
Iteration 16/1000 | Loss: 0.00003027
Iteration 17/1000 | Loss: 0.00002962
Iteration 18/1000 | Loss: 0.00002906
Iteration 19/1000 | Loss: 0.00002872
Iteration 20/1000 | Loss: 0.00002848
Iteration 21/1000 | Loss: 0.00002837
Iteration 22/1000 | Loss: 0.00002818
Iteration 23/1000 | Loss: 0.00004228
Iteration 24/1000 | Loss: 0.00003912
Iteration 25/1000 | Loss: 0.00002832
Iteration 26/1000 | Loss: 0.00004021
Iteration 27/1000 | Loss: 0.00004982
Iteration 28/1000 | Loss: 0.00004375
Iteration 29/1000 | Loss: 0.00003121
Iteration 30/1000 | Loss: 0.00003946
Iteration 31/1000 | Loss: 0.00003815
Iteration 32/1000 | Loss: 0.00005160
Iteration 33/1000 | Loss: 0.00004747
Iteration 34/1000 | Loss: 0.00003007
Iteration 35/1000 | Loss: 0.00003741
Iteration 36/1000 | Loss: 0.00004565
Iteration 37/1000 | Loss: 0.00003850
Iteration 38/1000 | Loss: 0.00004163
Iteration 39/1000 | Loss: 0.00003621
Iteration 40/1000 | Loss: 0.00004193
Iteration 41/1000 | Loss: 0.00003779
Iteration 42/1000 | Loss: 0.00004173
Iteration 43/1000 | Loss: 0.00004187
Iteration 44/1000 | Loss: 0.00004125
Iteration 45/1000 | Loss: 0.00004829
Iteration 46/1000 | Loss: 0.00003900
Iteration 47/1000 | Loss: 0.00003507
Iteration 48/1000 | Loss: 0.00003591
Iteration 49/1000 | Loss: 0.00003933
Iteration 50/1000 | Loss: 0.00003528
Iteration 51/1000 | Loss: 0.00003628
Iteration 52/1000 | Loss: 0.00003486
Iteration 53/1000 | Loss: 0.00003403
Iteration 54/1000 | Loss: 0.00003555
Iteration 55/1000 | Loss: 0.00003925
Iteration 56/1000 | Loss: 0.00003708
Iteration 57/1000 | Loss: 0.00003836
Iteration 58/1000 | Loss: 0.00004127
Iteration 59/1000 | Loss: 0.00003453
Iteration 60/1000 | Loss: 0.00003766
Iteration 61/1000 | Loss: 0.00003901
Iteration 62/1000 | Loss: 0.00003632
Iteration 63/1000 | Loss: 0.00003612
Iteration 64/1000 | Loss: 0.00003732
Iteration 65/1000 | Loss: 0.00003750
Iteration 66/1000 | Loss: 0.00004345
Iteration 67/1000 | Loss: 0.00003476
Iteration 68/1000 | Loss: 0.00003759
Iteration 69/1000 | Loss: 0.00004612
Iteration 70/1000 | Loss: 0.00003733
Iteration 71/1000 | Loss: 0.00004564
Iteration 72/1000 | Loss: 0.00003722
Iteration 73/1000 | Loss: 0.00003944
Iteration 74/1000 | Loss: 0.00003655
Iteration 75/1000 | Loss: 0.00003585
Iteration 76/1000 | Loss: 0.00003530
Iteration 77/1000 | Loss: 0.00003516
Iteration 78/1000 | Loss: 0.00003762
Iteration 79/1000 | Loss: 0.00003612
Iteration 80/1000 | Loss: 0.00003998
Iteration 81/1000 | Loss: 0.00003462
Iteration 82/1000 | Loss: 0.00004433
Iteration 83/1000 | Loss: 0.00003969
Iteration 84/1000 | Loss: 0.00004076
Iteration 85/1000 | Loss: 0.00004205
Iteration 86/1000 | Loss: 0.00003622
Iteration 87/1000 | Loss: 0.00003282
Iteration 88/1000 | Loss: 0.00003963
Iteration 89/1000 | Loss: 0.00004440
Iteration 90/1000 | Loss: 0.00004840
Iteration 91/1000 | Loss: 0.00004051
Iteration 92/1000 | Loss: 0.00003956
Iteration 93/1000 | Loss: 0.00003493
Iteration 94/1000 | Loss: 0.00003687
Iteration 95/1000 | Loss: 0.00003559
Iteration 96/1000 | Loss: 0.00002992
Iteration 97/1000 | Loss: 0.00003921
Iteration 98/1000 | Loss: 0.00003685
Iteration 99/1000 | Loss: 0.00004305
Iteration 100/1000 | Loss: 0.00006080
Iteration 101/1000 | Loss: 0.00003526
Iteration 102/1000 | Loss: 0.00003254
Iteration 103/1000 | Loss: 0.00003060
Iteration 104/1000 | Loss: 0.00002904
Iteration 105/1000 | Loss: 0.00002841
Iteration 106/1000 | Loss: 0.00002786
Iteration 107/1000 | Loss: 0.00002755
Iteration 108/1000 | Loss: 0.00002724
Iteration 109/1000 | Loss: 0.00002709
Iteration 110/1000 | Loss: 0.00002700
Iteration 111/1000 | Loss: 0.00002700
Iteration 112/1000 | Loss: 0.00002699
Iteration 113/1000 | Loss: 0.00002696
Iteration 114/1000 | Loss: 0.00002696
Iteration 115/1000 | Loss: 0.00002695
Iteration 116/1000 | Loss: 0.00002694
Iteration 117/1000 | Loss: 0.00002693
Iteration 118/1000 | Loss: 0.00002693
Iteration 119/1000 | Loss: 0.00002692
Iteration 120/1000 | Loss: 0.00002692
Iteration 121/1000 | Loss: 0.00002691
Iteration 122/1000 | Loss: 0.00002691
Iteration 123/1000 | Loss: 0.00002691
Iteration 124/1000 | Loss: 0.00002690
Iteration 125/1000 | Loss: 0.00002689
Iteration 126/1000 | Loss: 0.00002689
Iteration 127/1000 | Loss: 0.00002688
Iteration 128/1000 | Loss: 0.00002687
Iteration 129/1000 | Loss: 0.00002687
Iteration 130/1000 | Loss: 0.00002686
Iteration 131/1000 | Loss: 0.00002686
Iteration 132/1000 | Loss: 0.00002685
Iteration 133/1000 | Loss: 0.00002685
Iteration 134/1000 | Loss: 0.00002685
Iteration 135/1000 | Loss: 0.00002684
Iteration 136/1000 | Loss: 0.00002683
Iteration 137/1000 | Loss: 0.00002683
Iteration 138/1000 | Loss: 0.00002683
Iteration 139/1000 | Loss: 0.00002682
Iteration 140/1000 | Loss: 0.00002682
Iteration 141/1000 | Loss: 0.00002682
Iteration 142/1000 | Loss: 0.00002682
Iteration 143/1000 | Loss: 0.00002681
Iteration 144/1000 | Loss: 0.00002681
Iteration 145/1000 | Loss: 0.00002681
Iteration 146/1000 | Loss: 0.00002681
Iteration 147/1000 | Loss: 0.00002681
Iteration 148/1000 | Loss: 0.00002680
Iteration 149/1000 | Loss: 0.00002680
Iteration 150/1000 | Loss: 0.00002680
Iteration 151/1000 | Loss: 0.00002679
Iteration 152/1000 | Loss: 0.00002679
Iteration 153/1000 | Loss: 0.00002678
Iteration 154/1000 | Loss: 0.00002678
Iteration 155/1000 | Loss: 0.00002677
Iteration 156/1000 | Loss: 0.00002677
Iteration 157/1000 | Loss: 0.00002677
Iteration 158/1000 | Loss: 0.00002677
Iteration 159/1000 | Loss: 0.00002677
Iteration 160/1000 | Loss: 0.00002677
Iteration 161/1000 | Loss: 0.00002677
Iteration 162/1000 | Loss: 0.00002677
Iteration 163/1000 | Loss: 0.00002676
Iteration 164/1000 | Loss: 0.00002676
Iteration 165/1000 | Loss: 0.00002676
Iteration 166/1000 | Loss: 0.00002676
Iteration 167/1000 | Loss: 0.00002676
Iteration 168/1000 | Loss: 0.00002676
Iteration 169/1000 | Loss: 0.00002676
Iteration 170/1000 | Loss: 0.00002675
Iteration 171/1000 | Loss: 0.00002675
Iteration 172/1000 | Loss: 0.00002675
Iteration 173/1000 | Loss: 0.00002674
Iteration 174/1000 | Loss: 0.00002674
Iteration 175/1000 | Loss: 0.00002674
Iteration 176/1000 | Loss: 0.00002673
Iteration 177/1000 | Loss: 0.00002673
Iteration 178/1000 | Loss: 0.00002672
Iteration 179/1000 | Loss: 0.00002672
Iteration 180/1000 | Loss: 0.00013019
Iteration 181/1000 | Loss: 0.00003482
Iteration 182/1000 | Loss: 0.00004471
Iteration 183/1000 | Loss: 0.00004590
Iteration 184/1000 | Loss: 0.00002927
Iteration 185/1000 | Loss: 0.00002749
Iteration 186/1000 | Loss: 0.00002712
Iteration 187/1000 | Loss: 0.00002691
Iteration 188/1000 | Loss: 0.00002669
Iteration 189/1000 | Loss: 0.00002651
Iteration 190/1000 | Loss: 0.00002636
Iteration 191/1000 | Loss: 0.00002628
Iteration 192/1000 | Loss: 0.00002626
Iteration 193/1000 | Loss: 0.00002623
Iteration 194/1000 | Loss: 0.00002622
Iteration 195/1000 | Loss: 0.00002621
Iteration 196/1000 | Loss: 0.00002621
Iteration 197/1000 | Loss: 0.00002617
Iteration 198/1000 | Loss: 0.00002616
Iteration 199/1000 | Loss: 0.00002616
Iteration 200/1000 | Loss: 0.00002616
Iteration 201/1000 | Loss: 0.00002615
Iteration 202/1000 | Loss: 0.00002615
Iteration 203/1000 | Loss: 0.00002613
Iteration 204/1000 | Loss: 0.00002613
Iteration 205/1000 | Loss: 0.00002613
Iteration 206/1000 | Loss: 0.00002613
Iteration 207/1000 | Loss: 0.00002612
Iteration 208/1000 | Loss: 0.00002612
Iteration 209/1000 | Loss: 0.00002612
Iteration 210/1000 | Loss: 0.00002612
Iteration 211/1000 | Loss: 0.00002612
Iteration 212/1000 | Loss: 0.00002612
Iteration 213/1000 | Loss: 0.00002612
Iteration 214/1000 | Loss: 0.00002611
Iteration 215/1000 | Loss: 0.00002611
Iteration 216/1000 | Loss: 0.00002610
Iteration 217/1000 | Loss: 0.00002610
Iteration 218/1000 | Loss: 0.00002610
Iteration 219/1000 | Loss: 0.00002610
Iteration 220/1000 | Loss: 0.00002610
Iteration 221/1000 | Loss: 0.00002610
Iteration 222/1000 | Loss: 0.00002610
Iteration 223/1000 | Loss: 0.00002610
Iteration 224/1000 | Loss: 0.00002610
Iteration 225/1000 | Loss: 0.00002609
Iteration 226/1000 | Loss: 0.00002609
Iteration 227/1000 | Loss: 0.00002609
Iteration 228/1000 | Loss: 0.00002609
Iteration 229/1000 | Loss: 0.00002609
Iteration 230/1000 | Loss: 0.00002609
Iteration 231/1000 | Loss: 0.00002609
Iteration 232/1000 | Loss: 0.00002609
Iteration 233/1000 | Loss: 0.00002609
Iteration 234/1000 | Loss: 0.00002608
Iteration 235/1000 | Loss: 0.00002608
Iteration 236/1000 | Loss: 0.00002608
Iteration 237/1000 | Loss: 0.00002608
Iteration 238/1000 | Loss: 0.00002608
Iteration 239/1000 | Loss: 0.00002608
Iteration 240/1000 | Loss: 0.00002608
Iteration 241/1000 | Loss: 0.00002608
Iteration 242/1000 | Loss: 0.00002608
Iteration 243/1000 | Loss: 0.00002608
Iteration 244/1000 | Loss: 0.00002608
Iteration 245/1000 | Loss: 0.00002608
Iteration 246/1000 | Loss: 0.00002608
Iteration 247/1000 | Loss: 0.00002608
Iteration 248/1000 | Loss: 0.00002608
Iteration 249/1000 | Loss: 0.00002608
Iteration 250/1000 | Loss: 0.00002608
Iteration 251/1000 | Loss: 0.00002608
Iteration 252/1000 | Loss: 0.00002608
Iteration 253/1000 | Loss: 0.00002608
Iteration 254/1000 | Loss: 0.00002608
Iteration 255/1000 | Loss: 0.00002608
Iteration 256/1000 | Loss: 0.00002608
Iteration 257/1000 | Loss: 0.00002608
Iteration 258/1000 | Loss: 0.00002608
Iteration 259/1000 | Loss: 0.00002608
Iteration 260/1000 | Loss: 0.00002608
Iteration 261/1000 | Loss: 0.00002608
Iteration 262/1000 | Loss: 0.00002608
Iteration 263/1000 | Loss: 0.00002608
Iteration 264/1000 | Loss: 0.00002608
Iteration 265/1000 | Loss: 0.00002608
Iteration 266/1000 | Loss: 0.00002608
Iteration 267/1000 | Loss: 0.00002608
Iteration 268/1000 | Loss: 0.00002608
Iteration 269/1000 | Loss: 0.00002608
Iteration 270/1000 | Loss: 0.00002608
Iteration 271/1000 | Loss: 0.00002608
Iteration 272/1000 | Loss: 0.00002608
Iteration 273/1000 | Loss: 0.00002608
Iteration 274/1000 | Loss: 0.00002608
Iteration 275/1000 | Loss: 0.00002608
Iteration 276/1000 | Loss: 0.00002608
Iteration 277/1000 | Loss: 0.00002608
Iteration 278/1000 | Loss: 0.00002608
Iteration 279/1000 | Loss: 0.00002608
Iteration 280/1000 | Loss: 0.00002608
Iteration 281/1000 | Loss: 0.00002608
Iteration 282/1000 | Loss: 0.00002608
Iteration 283/1000 | Loss: 0.00002608
Iteration 284/1000 | Loss: 0.00002608
Iteration 285/1000 | Loss: 0.00002608
Iteration 286/1000 | Loss: 0.00002608
Iteration 287/1000 | Loss: 0.00002608
Iteration 288/1000 | Loss: 0.00002608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [2.607717578939628e-05, 2.607717578939628e-05, 2.607717578939628e-05, 2.607717578939628e-05, 2.607717578939628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.607717578939628e-05

Optimization complete. Final v2v error: 4.456822872161865 mm

Highest mean error: 5.169372081756592 mm for frame 41

Lowest mean error: 4.182875633239746 mm for frame 7

Saving results

Total time: 236.72989320755005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403630
Iteration 2/25 | Loss: 0.00123029
Iteration 3/25 | Loss: 0.00114691
Iteration 4/25 | Loss: 0.00113711
Iteration 5/25 | Loss: 0.00113464
Iteration 6/25 | Loss: 0.00113388
Iteration 7/25 | Loss: 0.00113381
Iteration 8/25 | Loss: 0.00113381
Iteration 9/25 | Loss: 0.00113381
Iteration 10/25 | Loss: 0.00113381
Iteration 11/25 | Loss: 0.00113381
Iteration 12/25 | Loss: 0.00113381
Iteration 13/25 | Loss: 0.00113381
Iteration 14/25 | Loss: 0.00113381
Iteration 15/25 | Loss: 0.00113381
Iteration 16/25 | Loss: 0.00113381
Iteration 17/25 | Loss: 0.00113381
Iteration 18/25 | Loss: 0.00113381
Iteration 19/25 | Loss: 0.00113381
Iteration 20/25 | Loss: 0.00113381
Iteration 21/25 | Loss: 0.00113381
Iteration 22/25 | Loss: 0.00113381
Iteration 23/25 | Loss: 0.00113381
Iteration 24/25 | Loss: 0.00113381
Iteration 25/25 | Loss: 0.00113381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33089423
Iteration 2/25 | Loss: 0.00156782
Iteration 3/25 | Loss: 0.00156781
Iteration 4/25 | Loss: 0.00156781
Iteration 5/25 | Loss: 0.00156781
Iteration 6/25 | Loss: 0.00156781
Iteration 7/25 | Loss: 0.00156781
Iteration 8/25 | Loss: 0.00156781
Iteration 9/25 | Loss: 0.00156781
Iteration 10/25 | Loss: 0.00156781
Iteration 11/25 | Loss: 0.00156781
Iteration 12/25 | Loss: 0.00156781
Iteration 13/25 | Loss: 0.00156781
Iteration 14/25 | Loss: 0.00156781
Iteration 15/25 | Loss: 0.00156781
Iteration 16/25 | Loss: 0.00156781
Iteration 17/25 | Loss: 0.00156781
Iteration 18/25 | Loss: 0.00156781
Iteration 19/25 | Loss: 0.00156781
Iteration 20/25 | Loss: 0.00156781
Iteration 21/25 | Loss: 0.00156781
Iteration 22/25 | Loss: 0.00156781
Iteration 23/25 | Loss: 0.00156781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015678120544180274, 0.0015678120544180274, 0.0015678120544180274, 0.0015678120544180274, 0.0015678120544180274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015678120544180274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156781
Iteration 2/1000 | Loss: 0.00003656
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00001645
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001320
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001292
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001285
Iteration 21/1000 | Loss: 0.00001284
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001279
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001271
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001267
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001259
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001257
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001255
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001253
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001253
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001253
Iteration 92/1000 | Loss: 0.00001253
Iteration 93/1000 | Loss: 0.00001253
Iteration 94/1000 | Loss: 0.00001253
Iteration 95/1000 | Loss: 0.00001253
Iteration 96/1000 | Loss: 0.00001253
Iteration 97/1000 | Loss: 0.00001253
Iteration 98/1000 | Loss: 0.00001252
Iteration 99/1000 | Loss: 0.00001252
Iteration 100/1000 | Loss: 0.00001252
Iteration 101/1000 | Loss: 0.00001252
Iteration 102/1000 | Loss: 0.00001252
Iteration 103/1000 | Loss: 0.00001252
Iteration 104/1000 | Loss: 0.00001252
Iteration 105/1000 | Loss: 0.00001252
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001251
Iteration 109/1000 | Loss: 0.00001251
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001250
Iteration 113/1000 | Loss: 0.00001250
Iteration 114/1000 | Loss: 0.00001250
Iteration 115/1000 | Loss: 0.00001250
Iteration 116/1000 | Loss: 0.00001250
Iteration 117/1000 | Loss: 0.00001250
Iteration 118/1000 | Loss: 0.00001250
Iteration 119/1000 | Loss: 0.00001250
Iteration 120/1000 | Loss: 0.00001250
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001250
Iteration 123/1000 | Loss: 0.00001250
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001249
Iteration 127/1000 | Loss: 0.00001249
Iteration 128/1000 | Loss: 0.00001249
Iteration 129/1000 | Loss: 0.00001249
Iteration 130/1000 | Loss: 0.00001249
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Iteration 138/1000 | Loss: 0.00001248
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001247
Iteration 145/1000 | Loss: 0.00001247
Iteration 146/1000 | Loss: 0.00001247
Iteration 147/1000 | Loss: 0.00001247
Iteration 148/1000 | Loss: 0.00001247
Iteration 149/1000 | Loss: 0.00001247
Iteration 150/1000 | Loss: 0.00001247
Iteration 151/1000 | Loss: 0.00001247
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001247
Iteration 154/1000 | Loss: 0.00001247
Iteration 155/1000 | Loss: 0.00001247
Iteration 156/1000 | Loss: 0.00001247
Iteration 157/1000 | Loss: 0.00001247
Iteration 158/1000 | Loss: 0.00001247
Iteration 159/1000 | Loss: 0.00001246
Iteration 160/1000 | Loss: 0.00001246
Iteration 161/1000 | Loss: 0.00001246
Iteration 162/1000 | Loss: 0.00001246
Iteration 163/1000 | Loss: 0.00001246
Iteration 164/1000 | Loss: 0.00001246
Iteration 165/1000 | Loss: 0.00001246
Iteration 166/1000 | Loss: 0.00001246
Iteration 167/1000 | Loss: 0.00001246
Iteration 168/1000 | Loss: 0.00001246
Iteration 169/1000 | Loss: 0.00001246
Iteration 170/1000 | Loss: 0.00001246
Iteration 171/1000 | Loss: 0.00001246
Iteration 172/1000 | Loss: 0.00001246
Iteration 173/1000 | Loss: 0.00001246
Iteration 174/1000 | Loss: 0.00001246
Iteration 175/1000 | Loss: 0.00001246
Iteration 176/1000 | Loss: 0.00001246
Iteration 177/1000 | Loss: 0.00001246
Iteration 178/1000 | Loss: 0.00001246
Iteration 179/1000 | Loss: 0.00001246
Iteration 180/1000 | Loss: 0.00001246
Iteration 181/1000 | Loss: 0.00001246
Iteration 182/1000 | Loss: 0.00001246
Iteration 183/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.2463327038858552e-05, 1.2463327038858552e-05, 1.2463327038858552e-05, 1.2463327038858552e-05, 1.2463327038858552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2463327038858552e-05

Optimization complete. Final v2v error: 2.9994328022003174 mm

Highest mean error: 3.464062213897705 mm for frame 66

Lowest mean error: 2.815793991088867 mm for frame 106

Saving results

Total time: 37.34960651397705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864318
Iteration 2/25 | Loss: 0.00131539
Iteration 3/25 | Loss: 0.00119566
Iteration 4/25 | Loss: 0.00118569
Iteration 5/25 | Loss: 0.00118531
Iteration 6/25 | Loss: 0.00118531
Iteration 7/25 | Loss: 0.00118531
Iteration 8/25 | Loss: 0.00118531
Iteration 9/25 | Loss: 0.00118531
Iteration 10/25 | Loss: 0.00118531
Iteration 11/25 | Loss: 0.00118531
Iteration 12/25 | Loss: 0.00118531
Iteration 13/25 | Loss: 0.00118531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011853133328258991, 0.0011853133328258991, 0.0011853133328258991, 0.0011853133328258991, 0.0011853133328258991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011853133328258991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21388388
Iteration 2/25 | Loss: 0.00169328
Iteration 3/25 | Loss: 0.00169327
Iteration 4/25 | Loss: 0.00169327
Iteration 5/25 | Loss: 0.00169327
Iteration 6/25 | Loss: 0.00169327
Iteration 7/25 | Loss: 0.00169327
Iteration 8/25 | Loss: 0.00169327
Iteration 9/25 | Loss: 0.00169327
Iteration 10/25 | Loss: 0.00169327
Iteration 11/25 | Loss: 0.00169327
Iteration 12/25 | Loss: 0.00169327
Iteration 13/25 | Loss: 0.00169327
Iteration 14/25 | Loss: 0.00169327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016932726139202714, 0.0016932726139202714, 0.0016932726139202714, 0.0016932726139202714, 0.0016932726139202714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016932726139202714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169327
Iteration 2/1000 | Loss: 0.00003550
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001691
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001670
Iteration 14/1000 | Loss: 0.00001664
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001652
Iteration 26/1000 | Loss: 0.00001652
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001651
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001650
Iteration 32/1000 | Loss: 0.00001650
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001650
Iteration 37/1000 | Loss: 0.00001650
Iteration 38/1000 | Loss: 0.00001650
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001649
Iteration 41/1000 | Loss: 0.00001649
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001645
Iteration 52/1000 | Loss: 0.00001644
Iteration 53/1000 | Loss: 0.00001644
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001640
Iteration 56/1000 | Loss: 0.00001640
Iteration 57/1000 | Loss: 0.00001640
Iteration 58/1000 | Loss: 0.00001640
Iteration 59/1000 | Loss: 0.00001640
Iteration 60/1000 | Loss: 0.00001640
Iteration 61/1000 | Loss: 0.00001640
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001636
Iteration 75/1000 | Loss: 0.00001636
Iteration 76/1000 | Loss: 0.00001636
Iteration 77/1000 | Loss: 0.00001636
Iteration 78/1000 | Loss: 0.00001636
Iteration 79/1000 | Loss: 0.00001636
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001635
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001634
Iteration 90/1000 | Loss: 0.00001634
Iteration 91/1000 | Loss: 0.00001634
Iteration 92/1000 | Loss: 0.00001634
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001634
Iteration 96/1000 | Loss: 0.00001634
Iteration 97/1000 | Loss: 0.00001634
Iteration 98/1000 | Loss: 0.00001634
Iteration 99/1000 | Loss: 0.00001634
Iteration 100/1000 | Loss: 0.00001634
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001634
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001634
Iteration 110/1000 | Loss: 0.00001634
Iteration 111/1000 | Loss: 0.00001634
Iteration 112/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.633605825190898e-05, 1.633605825190898e-05, 1.633605825190898e-05, 1.633605825190898e-05, 1.633605825190898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.633605825190898e-05

Optimization complete. Final v2v error: 3.4964494705200195 mm

Highest mean error: 3.610295057296753 mm for frame 176

Lowest mean error: 3.330146074295044 mm for frame 1

Saving results

Total time: 35.86986064910889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616945
Iteration 2/25 | Loss: 0.00152819
Iteration 3/25 | Loss: 0.00122686
Iteration 4/25 | Loss: 0.00121402
Iteration 5/25 | Loss: 0.00120963
Iteration 6/25 | Loss: 0.00120829
Iteration 7/25 | Loss: 0.00120823
Iteration 8/25 | Loss: 0.00120823
Iteration 9/25 | Loss: 0.00120823
Iteration 10/25 | Loss: 0.00120823
Iteration 11/25 | Loss: 0.00120823
Iteration 12/25 | Loss: 0.00120823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012082320172339678, 0.0012082320172339678, 0.0012082320172339678, 0.0012082320172339678, 0.0012082320172339678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012082320172339678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88569665
Iteration 2/25 | Loss: 0.00135453
Iteration 3/25 | Loss: 0.00135453
Iteration 4/25 | Loss: 0.00135453
Iteration 5/25 | Loss: 0.00135453
Iteration 6/25 | Loss: 0.00135453
Iteration 7/25 | Loss: 0.00135453
Iteration 8/25 | Loss: 0.00135453
Iteration 9/25 | Loss: 0.00135453
Iteration 10/25 | Loss: 0.00135453
Iteration 11/25 | Loss: 0.00135453
Iteration 12/25 | Loss: 0.00135453
Iteration 13/25 | Loss: 0.00135453
Iteration 14/25 | Loss: 0.00135453
Iteration 15/25 | Loss: 0.00135453
Iteration 16/25 | Loss: 0.00135453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013545297551900148, 0.0013545297551900148, 0.0013545297551900148, 0.0013545297551900148, 0.0013545297551900148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013545297551900148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135453
Iteration 2/1000 | Loss: 0.00006717
Iteration 3/1000 | Loss: 0.00005130
Iteration 4/1000 | Loss: 0.00004687
Iteration 5/1000 | Loss: 0.00004451
Iteration 6/1000 | Loss: 0.00004310
Iteration 7/1000 | Loss: 0.00004231
Iteration 8/1000 | Loss: 0.00004172
Iteration 9/1000 | Loss: 0.00004113
Iteration 10/1000 | Loss: 0.00004062
Iteration 11/1000 | Loss: 0.00004001
Iteration 12/1000 | Loss: 0.00003959
Iteration 13/1000 | Loss: 0.00003921
Iteration 14/1000 | Loss: 0.00003894
Iteration 15/1000 | Loss: 0.00003868
Iteration 16/1000 | Loss: 0.00003840
Iteration 17/1000 | Loss: 0.00003810
Iteration 18/1000 | Loss: 0.00003790
Iteration 19/1000 | Loss: 0.00003783
Iteration 20/1000 | Loss: 0.00003777
Iteration 21/1000 | Loss: 0.00003777
Iteration 22/1000 | Loss: 0.00003774
Iteration 23/1000 | Loss: 0.00003774
Iteration 24/1000 | Loss: 0.00003772
Iteration 25/1000 | Loss: 0.00003772
Iteration 26/1000 | Loss: 0.00003770
Iteration 27/1000 | Loss: 0.00003770
Iteration 28/1000 | Loss: 0.00003769
Iteration 29/1000 | Loss: 0.00003769
Iteration 30/1000 | Loss: 0.00003768
Iteration 31/1000 | Loss: 0.00003768
Iteration 32/1000 | Loss: 0.00003767
Iteration 33/1000 | Loss: 0.00003767
Iteration 34/1000 | Loss: 0.00003763
Iteration 35/1000 | Loss: 0.00003763
Iteration 36/1000 | Loss: 0.00003763
Iteration 37/1000 | Loss: 0.00003760
Iteration 38/1000 | Loss: 0.00003759
Iteration 39/1000 | Loss: 0.00003758
Iteration 40/1000 | Loss: 0.00003757
Iteration 41/1000 | Loss: 0.00003756
Iteration 42/1000 | Loss: 0.00003756
Iteration 43/1000 | Loss: 0.00003756
Iteration 44/1000 | Loss: 0.00003756
Iteration 45/1000 | Loss: 0.00003756
Iteration 46/1000 | Loss: 0.00003756
Iteration 47/1000 | Loss: 0.00003756
Iteration 48/1000 | Loss: 0.00003755
Iteration 49/1000 | Loss: 0.00003755
Iteration 50/1000 | Loss: 0.00003755
Iteration 51/1000 | Loss: 0.00003755
Iteration 52/1000 | Loss: 0.00003755
Iteration 53/1000 | Loss: 0.00003755
Iteration 54/1000 | Loss: 0.00003754
Iteration 55/1000 | Loss: 0.00003754
Iteration 56/1000 | Loss: 0.00003754
Iteration 57/1000 | Loss: 0.00003754
Iteration 58/1000 | Loss: 0.00003754
Iteration 59/1000 | Loss: 0.00003754
Iteration 60/1000 | Loss: 0.00003754
Iteration 61/1000 | Loss: 0.00003754
Iteration 62/1000 | Loss: 0.00003754
Iteration 63/1000 | Loss: 0.00003754
Iteration 64/1000 | Loss: 0.00003753
Iteration 65/1000 | Loss: 0.00003753
Iteration 66/1000 | Loss: 0.00003753
Iteration 67/1000 | Loss: 0.00003753
Iteration 68/1000 | Loss: 0.00003753
Iteration 69/1000 | Loss: 0.00003753
Iteration 70/1000 | Loss: 0.00003753
Iteration 71/1000 | Loss: 0.00003753
Iteration 72/1000 | Loss: 0.00003753
Iteration 73/1000 | Loss: 0.00003753
Iteration 74/1000 | Loss: 0.00003752
Iteration 75/1000 | Loss: 0.00003752
Iteration 76/1000 | Loss: 0.00003752
Iteration 77/1000 | Loss: 0.00003752
Iteration 78/1000 | Loss: 0.00003752
Iteration 79/1000 | Loss: 0.00003752
Iteration 80/1000 | Loss: 0.00003752
Iteration 81/1000 | Loss: 0.00003752
Iteration 82/1000 | Loss: 0.00003752
Iteration 83/1000 | Loss: 0.00003752
Iteration 84/1000 | Loss: 0.00003752
Iteration 85/1000 | Loss: 0.00003752
Iteration 86/1000 | Loss: 0.00003752
Iteration 87/1000 | Loss: 0.00003752
Iteration 88/1000 | Loss: 0.00003752
Iteration 89/1000 | Loss: 0.00003752
Iteration 90/1000 | Loss: 0.00003752
Iteration 91/1000 | Loss: 0.00003752
Iteration 92/1000 | Loss: 0.00003752
Iteration 93/1000 | Loss: 0.00003752
Iteration 94/1000 | Loss: 0.00003752
Iteration 95/1000 | Loss: 0.00003752
Iteration 96/1000 | Loss: 0.00003752
Iteration 97/1000 | Loss: 0.00003752
Iteration 98/1000 | Loss: 0.00003752
Iteration 99/1000 | Loss: 0.00003752
Iteration 100/1000 | Loss: 0.00003752
Iteration 101/1000 | Loss: 0.00003752
Iteration 102/1000 | Loss: 0.00003752
Iteration 103/1000 | Loss: 0.00003752
Iteration 104/1000 | Loss: 0.00003752
Iteration 105/1000 | Loss: 0.00003752
Iteration 106/1000 | Loss: 0.00003752
Iteration 107/1000 | Loss: 0.00003752
Iteration 108/1000 | Loss: 0.00003752
Iteration 109/1000 | Loss: 0.00003752
Iteration 110/1000 | Loss: 0.00003752
Iteration 111/1000 | Loss: 0.00003752
Iteration 112/1000 | Loss: 0.00003752
Iteration 113/1000 | Loss: 0.00003752
Iteration 114/1000 | Loss: 0.00003752
Iteration 115/1000 | Loss: 0.00003752
Iteration 116/1000 | Loss: 0.00003752
Iteration 117/1000 | Loss: 0.00003752
Iteration 118/1000 | Loss: 0.00003752
Iteration 119/1000 | Loss: 0.00003752
Iteration 120/1000 | Loss: 0.00003752
Iteration 121/1000 | Loss: 0.00003752
Iteration 122/1000 | Loss: 0.00003752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.751742042368278e-05, 3.751742042368278e-05, 3.751742042368278e-05, 3.751742042368278e-05, 3.751742042368278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.751742042368278e-05

Optimization complete. Final v2v error: 4.535629749298096 mm

Highest mean error: 5.722715377807617 mm for frame 95

Lowest mean error: 3.159428119659424 mm for frame 0

Saving results

Total time: 43.91707944869995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882451
Iteration 2/25 | Loss: 0.00141696
Iteration 3/25 | Loss: 0.00126282
Iteration 4/25 | Loss: 0.00124469
Iteration 5/25 | Loss: 0.00124020
Iteration 6/25 | Loss: 0.00123965
Iteration 7/25 | Loss: 0.00123965
Iteration 8/25 | Loss: 0.00123965
Iteration 9/25 | Loss: 0.00123965
Iteration 10/25 | Loss: 0.00123965
Iteration 11/25 | Loss: 0.00123965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012396469246596098, 0.0012396469246596098, 0.0012396469246596098, 0.0012396469246596098, 0.0012396469246596098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012396469246596098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23695624
Iteration 2/25 | Loss: 0.00208353
Iteration 3/25 | Loss: 0.00208349
Iteration 4/25 | Loss: 0.00208349
Iteration 5/25 | Loss: 0.00208349
Iteration 6/25 | Loss: 0.00208349
Iteration 7/25 | Loss: 0.00208349
Iteration 8/25 | Loss: 0.00208349
Iteration 9/25 | Loss: 0.00208349
Iteration 10/25 | Loss: 0.00208349
Iteration 11/25 | Loss: 0.00208349
Iteration 12/25 | Loss: 0.00208349
Iteration 13/25 | Loss: 0.00208349
Iteration 14/25 | Loss: 0.00208349
Iteration 15/25 | Loss: 0.00208349
Iteration 16/25 | Loss: 0.00208349
Iteration 17/25 | Loss: 0.00208349
Iteration 18/25 | Loss: 0.00208349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020834861788898706, 0.0020834861788898706, 0.0020834861788898706, 0.0020834861788898706, 0.0020834861788898706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020834861788898706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208349
Iteration 2/1000 | Loss: 0.00006141
Iteration 3/1000 | Loss: 0.00003758
Iteration 4/1000 | Loss: 0.00002850
Iteration 5/1000 | Loss: 0.00002537
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002332
Iteration 8/1000 | Loss: 0.00002272
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00002192
Iteration 11/1000 | Loss: 0.00002165
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002140
Iteration 15/1000 | Loss: 0.00002139
Iteration 16/1000 | Loss: 0.00002136
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002124
Iteration 19/1000 | Loss: 0.00002115
Iteration 20/1000 | Loss: 0.00002103
Iteration 21/1000 | Loss: 0.00002103
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002089
Iteration 32/1000 | Loss: 0.00002089
Iteration 33/1000 | Loss: 0.00002088
Iteration 34/1000 | Loss: 0.00002088
Iteration 35/1000 | Loss: 0.00002088
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002087
Iteration 38/1000 | Loss: 0.00002086
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002084
Iteration 46/1000 | Loss: 0.00002084
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002083
Iteration 51/1000 | Loss: 0.00002083
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002081
Iteration 56/1000 | Loss: 0.00002080
Iteration 57/1000 | Loss: 0.00002080
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002079
Iteration 60/1000 | Loss: 0.00002079
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002077
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002077
Iteration 68/1000 | Loss: 0.00002076
Iteration 69/1000 | Loss: 0.00002076
Iteration 70/1000 | Loss: 0.00002076
Iteration 71/1000 | Loss: 0.00002076
Iteration 72/1000 | Loss: 0.00002076
Iteration 73/1000 | Loss: 0.00002076
Iteration 74/1000 | Loss: 0.00002076
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002075
Iteration 78/1000 | Loss: 0.00002075
Iteration 79/1000 | Loss: 0.00002075
Iteration 80/1000 | Loss: 0.00002075
Iteration 81/1000 | Loss: 0.00002075
Iteration 82/1000 | Loss: 0.00002074
Iteration 83/1000 | Loss: 0.00002074
Iteration 84/1000 | Loss: 0.00002074
Iteration 85/1000 | Loss: 0.00002074
Iteration 86/1000 | Loss: 0.00002074
Iteration 87/1000 | Loss: 0.00002074
Iteration 88/1000 | Loss: 0.00002073
Iteration 89/1000 | Loss: 0.00002073
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002072
Iteration 92/1000 | Loss: 0.00002072
Iteration 93/1000 | Loss: 0.00002072
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002071
Iteration 97/1000 | Loss: 0.00002070
Iteration 98/1000 | Loss: 0.00002070
Iteration 99/1000 | Loss: 0.00002070
Iteration 100/1000 | Loss: 0.00002070
Iteration 101/1000 | Loss: 0.00002070
Iteration 102/1000 | Loss: 0.00002070
Iteration 103/1000 | Loss: 0.00002069
Iteration 104/1000 | Loss: 0.00002069
Iteration 105/1000 | Loss: 0.00002069
Iteration 106/1000 | Loss: 0.00002068
Iteration 107/1000 | Loss: 0.00002068
Iteration 108/1000 | Loss: 0.00002068
Iteration 109/1000 | Loss: 0.00002068
Iteration 110/1000 | Loss: 0.00002068
Iteration 111/1000 | Loss: 0.00002068
Iteration 112/1000 | Loss: 0.00002068
Iteration 113/1000 | Loss: 0.00002068
Iteration 114/1000 | Loss: 0.00002068
Iteration 115/1000 | Loss: 0.00002068
Iteration 116/1000 | Loss: 0.00002068
Iteration 117/1000 | Loss: 0.00002068
Iteration 118/1000 | Loss: 0.00002068
Iteration 119/1000 | Loss: 0.00002068
Iteration 120/1000 | Loss: 0.00002068
Iteration 121/1000 | Loss: 0.00002068
Iteration 122/1000 | Loss: 0.00002068
Iteration 123/1000 | Loss: 0.00002068
Iteration 124/1000 | Loss: 0.00002068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.0675155610661022e-05, 2.0675155610661022e-05, 2.0675155610661022e-05, 2.0675155610661022e-05, 2.0675155610661022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0675155610661022e-05

Optimization complete. Final v2v error: 3.8111143112182617 mm

Highest mean error: 4.398510456085205 mm for frame 220

Lowest mean error: 3.3360471725463867 mm for frame 11

Saving results

Total time: 43.530924558639526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874204
Iteration 2/25 | Loss: 0.00154963
Iteration 3/25 | Loss: 0.00128636
Iteration 4/25 | Loss: 0.00126314
Iteration 5/25 | Loss: 0.00125691
Iteration 6/25 | Loss: 0.00125574
Iteration 7/25 | Loss: 0.00125574
Iteration 8/25 | Loss: 0.00125574
Iteration 9/25 | Loss: 0.00125574
Iteration 10/25 | Loss: 0.00125574
Iteration 11/25 | Loss: 0.00125574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012557427398860455, 0.0012557427398860455, 0.0012557427398860455, 0.0012557427398860455, 0.0012557427398860455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012557427398860455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12842977
Iteration 2/25 | Loss: 0.00079145
Iteration 3/25 | Loss: 0.00079142
Iteration 4/25 | Loss: 0.00079142
Iteration 5/25 | Loss: 0.00079142
Iteration 6/25 | Loss: 0.00079142
Iteration 7/25 | Loss: 0.00079142
Iteration 8/25 | Loss: 0.00079142
Iteration 9/25 | Loss: 0.00079142
Iteration 10/25 | Loss: 0.00079142
Iteration 11/25 | Loss: 0.00079142
Iteration 12/25 | Loss: 0.00079142
Iteration 13/25 | Loss: 0.00079142
Iteration 14/25 | Loss: 0.00079142
Iteration 15/25 | Loss: 0.00079142
Iteration 16/25 | Loss: 0.00079142
Iteration 17/25 | Loss: 0.00079142
Iteration 18/25 | Loss: 0.00079142
Iteration 19/25 | Loss: 0.00079142
Iteration 20/25 | Loss: 0.00079142
Iteration 21/25 | Loss: 0.00079142
Iteration 22/25 | Loss: 0.00079142
Iteration 23/25 | Loss: 0.00079142
Iteration 24/25 | Loss: 0.00079142
Iteration 25/25 | Loss: 0.00079142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079142
Iteration 2/1000 | Loss: 0.00006442
Iteration 3/1000 | Loss: 0.00004633
Iteration 4/1000 | Loss: 0.00003867
Iteration 5/1000 | Loss: 0.00003633
Iteration 6/1000 | Loss: 0.00003518
Iteration 7/1000 | Loss: 0.00003437
Iteration 8/1000 | Loss: 0.00003362
Iteration 9/1000 | Loss: 0.00003302
Iteration 10/1000 | Loss: 0.00003272
Iteration 11/1000 | Loss: 0.00003247
Iteration 12/1000 | Loss: 0.00003223
Iteration 13/1000 | Loss: 0.00003204
Iteration 14/1000 | Loss: 0.00003185
Iteration 15/1000 | Loss: 0.00003167
Iteration 16/1000 | Loss: 0.00003152
Iteration 17/1000 | Loss: 0.00003135
Iteration 18/1000 | Loss: 0.00003127
Iteration 19/1000 | Loss: 0.00003112
Iteration 20/1000 | Loss: 0.00003099
Iteration 21/1000 | Loss: 0.00003096
Iteration 22/1000 | Loss: 0.00003095
Iteration 23/1000 | Loss: 0.00003094
Iteration 24/1000 | Loss: 0.00003088
Iteration 25/1000 | Loss: 0.00003087
Iteration 26/1000 | Loss: 0.00003084
Iteration 27/1000 | Loss: 0.00003083
Iteration 28/1000 | Loss: 0.00003083
Iteration 29/1000 | Loss: 0.00003078
Iteration 30/1000 | Loss: 0.00003078
Iteration 31/1000 | Loss: 0.00003078
Iteration 32/1000 | Loss: 0.00003078
Iteration 33/1000 | Loss: 0.00003078
Iteration 34/1000 | Loss: 0.00003077
Iteration 35/1000 | Loss: 0.00003077
Iteration 36/1000 | Loss: 0.00003077
Iteration 37/1000 | Loss: 0.00003076
Iteration 38/1000 | Loss: 0.00003076
Iteration 39/1000 | Loss: 0.00003074
Iteration 40/1000 | Loss: 0.00003073
Iteration 41/1000 | Loss: 0.00003073
Iteration 42/1000 | Loss: 0.00003073
Iteration 43/1000 | Loss: 0.00003073
Iteration 44/1000 | Loss: 0.00003073
Iteration 45/1000 | Loss: 0.00003073
Iteration 46/1000 | Loss: 0.00003073
Iteration 47/1000 | Loss: 0.00003073
Iteration 48/1000 | Loss: 0.00003073
Iteration 49/1000 | Loss: 0.00003072
Iteration 50/1000 | Loss: 0.00003072
Iteration 51/1000 | Loss: 0.00003072
Iteration 52/1000 | Loss: 0.00003071
Iteration 53/1000 | Loss: 0.00003071
Iteration 54/1000 | Loss: 0.00003071
Iteration 55/1000 | Loss: 0.00003070
Iteration 56/1000 | Loss: 0.00003069
Iteration 57/1000 | Loss: 0.00003069
Iteration 58/1000 | Loss: 0.00003068
Iteration 59/1000 | Loss: 0.00003068
Iteration 60/1000 | Loss: 0.00003068
Iteration 61/1000 | Loss: 0.00003068
Iteration 62/1000 | Loss: 0.00003068
Iteration 63/1000 | Loss: 0.00003067
Iteration 64/1000 | Loss: 0.00003067
Iteration 65/1000 | Loss: 0.00003066
Iteration 66/1000 | Loss: 0.00003066
Iteration 67/1000 | Loss: 0.00003066
Iteration 68/1000 | Loss: 0.00003065
Iteration 69/1000 | Loss: 0.00003065
Iteration 70/1000 | Loss: 0.00003065
Iteration 71/1000 | Loss: 0.00003065
Iteration 72/1000 | Loss: 0.00003064
Iteration 73/1000 | Loss: 0.00003064
Iteration 74/1000 | Loss: 0.00003064
Iteration 75/1000 | Loss: 0.00003064
Iteration 76/1000 | Loss: 0.00003064
Iteration 77/1000 | Loss: 0.00003064
Iteration 78/1000 | Loss: 0.00003064
Iteration 79/1000 | Loss: 0.00003064
Iteration 80/1000 | Loss: 0.00003063
Iteration 81/1000 | Loss: 0.00003063
Iteration 82/1000 | Loss: 0.00003063
Iteration 83/1000 | Loss: 0.00003063
Iteration 84/1000 | Loss: 0.00003063
Iteration 85/1000 | Loss: 0.00003063
Iteration 86/1000 | Loss: 0.00003063
Iteration 87/1000 | Loss: 0.00003063
Iteration 88/1000 | Loss: 0.00003062
Iteration 89/1000 | Loss: 0.00003062
Iteration 90/1000 | Loss: 0.00003062
Iteration 91/1000 | Loss: 0.00003062
Iteration 92/1000 | Loss: 0.00003062
Iteration 93/1000 | Loss: 0.00003062
Iteration 94/1000 | Loss: 0.00003061
Iteration 95/1000 | Loss: 0.00003061
Iteration 96/1000 | Loss: 0.00003061
Iteration 97/1000 | Loss: 0.00003061
Iteration 98/1000 | Loss: 0.00003060
Iteration 99/1000 | Loss: 0.00003060
Iteration 100/1000 | Loss: 0.00003060
Iteration 101/1000 | Loss: 0.00003060
Iteration 102/1000 | Loss: 0.00003060
Iteration 103/1000 | Loss: 0.00003060
Iteration 104/1000 | Loss: 0.00003060
Iteration 105/1000 | Loss: 0.00003060
Iteration 106/1000 | Loss: 0.00003060
Iteration 107/1000 | Loss: 0.00003060
Iteration 108/1000 | Loss: 0.00003060
Iteration 109/1000 | Loss: 0.00003060
Iteration 110/1000 | Loss: 0.00003060
Iteration 111/1000 | Loss: 0.00003059
Iteration 112/1000 | Loss: 0.00003059
Iteration 113/1000 | Loss: 0.00003059
Iteration 114/1000 | Loss: 0.00003059
Iteration 115/1000 | Loss: 0.00003059
Iteration 116/1000 | Loss: 0.00003059
Iteration 117/1000 | Loss: 0.00003059
Iteration 118/1000 | Loss: 0.00003059
Iteration 119/1000 | Loss: 0.00003059
Iteration 120/1000 | Loss: 0.00003058
Iteration 121/1000 | Loss: 0.00003058
Iteration 122/1000 | Loss: 0.00003058
Iteration 123/1000 | Loss: 0.00003058
Iteration 124/1000 | Loss: 0.00003058
Iteration 125/1000 | Loss: 0.00003058
Iteration 126/1000 | Loss: 0.00003058
Iteration 127/1000 | Loss: 0.00003058
Iteration 128/1000 | Loss: 0.00003057
Iteration 129/1000 | Loss: 0.00003057
Iteration 130/1000 | Loss: 0.00003057
Iteration 131/1000 | Loss: 0.00003057
Iteration 132/1000 | Loss: 0.00003057
Iteration 133/1000 | Loss: 0.00003057
Iteration 134/1000 | Loss: 0.00003057
Iteration 135/1000 | Loss: 0.00003056
Iteration 136/1000 | Loss: 0.00003056
Iteration 137/1000 | Loss: 0.00003056
Iteration 138/1000 | Loss: 0.00003055
Iteration 139/1000 | Loss: 0.00003055
Iteration 140/1000 | Loss: 0.00003055
Iteration 141/1000 | Loss: 0.00003055
Iteration 142/1000 | Loss: 0.00003055
Iteration 143/1000 | Loss: 0.00003055
Iteration 144/1000 | Loss: 0.00003055
Iteration 145/1000 | Loss: 0.00003055
Iteration 146/1000 | Loss: 0.00003055
Iteration 147/1000 | Loss: 0.00003055
Iteration 148/1000 | Loss: 0.00003055
Iteration 149/1000 | Loss: 0.00003055
Iteration 150/1000 | Loss: 0.00003055
Iteration 151/1000 | Loss: 0.00003054
Iteration 152/1000 | Loss: 0.00003054
Iteration 153/1000 | Loss: 0.00003054
Iteration 154/1000 | Loss: 0.00003054
Iteration 155/1000 | Loss: 0.00003054
Iteration 156/1000 | Loss: 0.00003054
Iteration 157/1000 | Loss: 0.00003054
Iteration 158/1000 | Loss: 0.00003054
Iteration 159/1000 | Loss: 0.00003053
Iteration 160/1000 | Loss: 0.00003053
Iteration 161/1000 | Loss: 0.00003053
Iteration 162/1000 | Loss: 0.00003053
Iteration 163/1000 | Loss: 0.00003053
Iteration 164/1000 | Loss: 0.00003053
Iteration 165/1000 | Loss: 0.00003053
Iteration 166/1000 | Loss: 0.00003053
Iteration 167/1000 | Loss: 0.00003053
Iteration 168/1000 | Loss: 0.00003053
Iteration 169/1000 | Loss: 0.00003053
Iteration 170/1000 | Loss: 0.00003053
Iteration 171/1000 | Loss: 0.00003052
Iteration 172/1000 | Loss: 0.00003052
Iteration 173/1000 | Loss: 0.00003052
Iteration 174/1000 | Loss: 0.00003052
Iteration 175/1000 | Loss: 0.00003052
Iteration 176/1000 | Loss: 0.00003052
Iteration 177/1000 | Loss: 0.00003052
Iteration 178/1000 | Loss: 0.00003052
Iteration 179/1000 | Loss: 0.00003052
Iteration 180/1000 | Loss: 0.00003052
Iteration 181/1000 | Loss: 0.00003052
Iteration 182/1000 | Loss: 0.00003052
Iteration 183/1000 | Loss: 0.00003052
Iteration 184/1000 | Loss: 0.00003052
Iteration 185/1000 | Loss: 0.00003052
Iteration 186/1000 | Loss: 0.00003052
Iteration 187/1000 | Loss: 0.00003052
Iteration 188/1000 | Loss: 0.00003052
Iteration 189/1000 | Loss: 0.00003052
Iteration 190/1000 | Loss: 0.00003052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [3.052283500437625e-05, 3.052283500437625e-05, 3.052283500437625e-05, 3.052283500437625e-05, 3.052283500437625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.052283500437625e-05

Optimization complete. Final v2v error: 4.496862411499023 mm

Highest mean error: 5.450993061065674 mm for frame 60

Lowest mean error: 3.4719364643096924 mm for frame 3

Saving results

Total time: 51.52032923698425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443546
Iteration 2/25 | Loss: 0.00141401
Iteration 3/25 | Loss: 0.00121374
Iteration 4/25 | Loss: 0.00119204
Iteration 5/25 | Loss: 0.00118561
Iteration 6/25 | Loss: 0.00118404
Iteration 7/25 | Loss: 0.00118385
Iteration 8/25 | Loss: 0.00118385
Iteration 9/25 | Loss: 0.00118385
Iteration 10/25 | Loss: 0.00118385
Iteration 11/25 | Loss: 0.00118385
Iteration 12/25 | Loss: 0.00118385
Iteration 13/25 | Loss: 0.00118385
Iteration 14/25 | Loss: 0.00118385
Iteration 15/25 | Loss: 0.00118385
Iteration 16/25 | Loss: 0.00118385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001183854416012764, 0.001183854416012764, 0.001183854416012764, 0.001183854416012764, 0.001183854416012764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001183854416012764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91149998
Iteration 2/25 | Loss: 0.00067219
Iteration 3/25 | Loss: 0.00067215
Iteration 4/25 | Loss: 0.00067215
Iteration 5/25 | Loss: 0.00067215
Iteration 6/25 | Loss: 0.00067215
Iteration 7/25 | Loss: 0.00067215
Iteration 8/25 | Loss: 0.00067215
Iteration 9/25 | Loss: 0.00067215
Iteration 10/25 | Loss: 0.00067215
Iteration 11/25 | Loss: 0.00067215
Iteration 12/25 | Loss: 0.00067215
Iteration 13/25 | Loss: 0.00067215
Iteration 14/25 | Loss: 0.00067215
Iteration 15/25 | Loss: 0.00067215
Iteration 16/25 | Loss: 0.00067215
Iteration 17/25 | Loss: 0.00067215
Iteration 18/25 | Loss: 0.00067215
Iteration 19/25 | Loss: 0.00067215
Iteration 20/25 | Loss: 0.00067215
Iteration 21/25 | Loss: 0.00067215
Iteration 22/25 | Loss: 0.00067215
Iteration 23/25 | Loss: 0.00067215
Iteration 24/25 | Loss: 0.00067215
Iteration 25/25 | Loss: 0.00067215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006721462123095989, 0.0006721462123095989, 0.0006721462123095989, 0.0006721462123095989, 0.0006721462123095989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006721462123095989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067215
Iteration 2/1000 | Loss: 0.00004319
Iteration 3/1000 | Loss: 0.00003044
Iteration 4/1000 | Loss: 0.00002638
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002258
Iteration 8/1000 | Loss: 0.00002213
Iteration 9/1000 | Loss: 0.00002175
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002131
Iteration 12/1000 | Loss: 0.00002129
Iteration 13/1000 | Loss: 0.00002114
Iteration 14/1000 | Loss: 0.00002113
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002107
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002099
Iteration 19/1000 | Loss: 0.00002099
Iteration 20/1000 | Loss: 0.00002098
Iteration 21/1000 | Loss: 0.00002097
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002096
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002095
Iteration 27/1000 | Loss: 0.00002094
Iteration 28/1000 | Loss: 0.00002094
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002094
Iteration 32/1000 | Loss: 0.00002094
Iteration 33/1000 | Loss: 0.00002094
Iteration 34/1000 | Loss: 0.00002094
Iteration 35/1000 | Loss: 0.00002094
Iteration 36/1000 | Loss: 0.00002094
Iteration 37/1000 | Loss: 0.00002094
Iteration 38/1000 | Loss: 0.00002093
Iteration 39/1000 | Loss: 0.00002093
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002092
Iteration 42/1000 | Loss: 0.00002092
Iteration 43/1000 | Loss: 0.00002092
Iteration 44/1000 | Loss: 0.00002092
Iteration 45/1000 | Loss: 0.00002091
Iteration 46/1000 | Loss: 0.00002091
Iteration 47/1000 | Loss: 0.00002091
Iteration 48/1000 | Loss: 0.00002091
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002090
Iteration 51/1000 | Loss: 0.00002090
Iteration 52/1000 | Loss: 0.00002090
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002089
Iteration 55/1000 | Loss: 0.00002089
Iteration 56/1000 | Loss: 0.00002089
Iteration 57/1000 | Loss: 0.00002089
Iteration 58/1000 | Loss: 0.00002088
Iteration 59/1000 | Loss: 0.00002088
Iteration 60/1000 | Loss: 0.00002088
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002087
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002084
Iteration 81/1000 | Loss: 0.00002084
Iteration 82/1000 | Loss: 0.00002084
Iteration 83/1000 | Loss: 0.00002084
Iteration 84/1000 | Loss: 0.00002083
Iteration 85/1000 | Loss: 0.00002083
Iteration 86/1000 | Loss: 0.00002083
Iteration 87/1000 | Loss: 0.00002082
Iteration 88/1000 | Loss: 0.00002082
Iteration 89/1000 | Loss: 0.00002082
Iteration 90/1000 | Loss: 0.00002082
Iteration 91/1000 | Loss: 0.00002082
Iteration 92/1000 | Loss: 0.00002082
Iteration 93/1000 | Loss: 0.00002082
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00002082
Iteration 96/1000 | Loss: 0.00002082
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002080
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002080
Iteration 112/1000 | Loss: 0.00002080
Iteration 113/1000 | Loss: 0.00002080
Iteration 114/1000 | Loss: 0.00002080
Iteration 115/1000 | Loss: 0.00002080
Iteration 116/1000 | Loss: 0.00002080
Iteration 117/1000 | Loss: 0.00002080
Iteration 118/1000 | Loss: 0.00002080
Iteration 119/1000 | Loss: 0.00002080
Iteration 120/1000 | Loss: 0.00002080
Iteration 121/1000 | Loss: 0.00002080
Iteration 122/1000 | Loss: 0.00002080
Iteration 123/1000 | Loss: 0.00002080
Iteration 124/1000 | Loss: 0.00002080
Iteration 125/1000 | Loss: 0.00002080
Iteration 126/1000 | Loss: 0.00002080
Iteration 127/1000 | Loss: 0.00002080
Iteration 128/1000 | Loss: 0.00002080
Iteration 129/1000 | Loss: 0.00002080
Iteration 130/1000 | Loss: 0.00002080
Iteration 131/1000 | Loss: 0.00002080
Iteration 132/1000 | Loss: 0.00002080
Iteration 133/1000 | Loss: 0.00002080
Iteration 134/1000 | Loss: 0.00002080
Iteration 135/1000 | Loss: 0.00002080
Iteration 136/1000 | Loss: 0.00002080
Iteration 137/1000 | Loss: 0.00002080
Iteration 138/1000 | Loss: 0.00002080
Iteration 139/1000 | Loss: 0.00002080
Iteration 140/1000 | Loss: 0.00002080
Iteration 141/1000 | Loss: 0.00002080
Iteration 142/1000 | Loss: 0.00002080
Iteration 143/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.0796023818547837e-05, 2.0796023818547837e-05, 2.0796023818547837e-05, 2.0796023818547837e-05, 2.0796023818547837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0796023818547837e-05

Optimization complete. Final v2v error: 3.915956735610962 mm

Highest mean error: 4.417119026184082 mm for frame 31

Lowest mean error: 3.686417818069458 mm for frame 69

Saving results

Total time: 36.559693574905396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00634352
Iteration 2/25 | Loss: 0.00123564
Iteration 3/25 | Loss: 0.00114536
Iteration 4/25 | Loss: 0.00113799
Iteration 5/25 | Loss: 0.00113661
Iteration 6/25 | Loss: 0.00113661
Iteration 7/25 | Loss: 0.00113661
Iteration 8/25 | Loss: 0.00113661
Iteration 9/25 | Loss: 0.00113661
Iteration 10/25 | Loss: 0.00113661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011366145918145776, 0.0011366145918145776, 0.0011366145918145776, 0.0011366145918145776, 0.0011366145918145776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011366145918145776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.01625967
Iteration 2/25 | Loss: 0.00062588
Iteration 3/25 | Loss: 0.00062587
Iteration 4/25 | Loss: 0.00062587
Iteration 5/25 | Loss: 0.00062587
Iteration 6/25 | Loss: 0.00062587
Iteration 7/25 | Loss: 0.00062587
Iteration 8/25 | Loss: 0.00062586
Iteration 9/25 | Loss: 0.00062586
Iteration 10/25 | Loss: 0.00062586
Iteration 11/25 | Loss: 0.00062586
Iteration 12/25 | Loss: 0.00062586
Iteration 13/25 | Loss: 0.00062586
Iteration 14/25 | Loss: 0.00062586
Iteration 15/25 | Loss: 0.00062586
Iteration 16/25 | Loss: 0.00062586
Iteration 17/25 | Loss: 0.00062586
Iteration 18/25 | Loss: 0.00062586
Iteration 19/25 | Loss: 0.00062586
Iteration 20/25 | Loss: 0.00062586
Iteration 21/25 | Loss: 0.00062586
Iteration 22/25 | Loss: 0.00062586
Iteration 23/25 | Loss: 0.00062586
Iteration 24/25 | Loss: 0.00062586
Iteration 25/25 | Loss: 0.00062586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062586
Iteration 2/1000 | Loss: 0.00003112
Iteration 3/1000 | Loss: 0.00001831
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001541
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001445
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001435
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001430
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001429
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001427
Iteration 23/1000 | Loss: 0.00001427
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001422
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001418
Iteration 34/1000 | Loss: 0.00001418
Iteration 35/1000 | Loss: 0.00001417
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001414
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001413
Iteration 46/1000 | Loss: 0.00001413
Iteration 47/1000 | Loss: 0.00001413
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001410
Iteration 59/1000 | Loss: 0.00001410
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001409
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001409
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001407
Iteration 81/1000 | Loss: 0.00001407
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001407
Iteration 85/1000 | Loss: 0.00001407
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001406
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001406
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001406
Iteration 106/1000 | Loss: 0.00001406
Iteration 107/1000 | Loss: 0.00001406
Iteration 108/1000 | Loss: 0.00001406
Iteration 109/1000 | Loss: 0.00001406
Iteration 110/1000 | Loss: 0.00001406
Iteration 111/1000 | Loss: 0.00001406
Iteration 112/1000 | Loss: 0.00001406
Iteration 113/1000 | Loss: 0.00001406
Iteration 114/1000 | Loss: 0.00001406
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001406
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.4063220078242011e-05, 1.4063220078242011e-05, 1.4063220078242011e-05, 1.4063220078242011e-05, 1.4063220078242011e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4063220078242011e-05

Optimization complete. Final v2v error: 3.2489359378814697 mm

Highest mean error: 3.5556230545043945 mm for frame 16

Lowest mean error: 2.8766398429870605 mm for frame 42

Saving results

Total time: 30.463655471801758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893439
Iteration 2/25 | Loss: 0.00148735
Iteration 3/25 | Loss: 0.00121100
Iteration 4/25 | Loss: 0.00118049
Iteration 5/25 | Loss: 0.00117694
Iteration 6/25 | Loss: 0.00117591
Iteration 7/25 | Loss: 0.00117591
Iteration 8/25 | Loss: 0.00117591
Iteration 9/25 | Loss: 0.00117591
Iteration 10/25 | Loss: 0.00117591
Iteration 11/25 | Loss: 0.00117591
Iteration 12/25 | Loss: 0.00117591
Iteration 13/25 | Loss: 0.00117591
Iteration 14/25 | Loss: 0.00117591
Iteration 15/25 | Loss: 0.00117591
Iteration 16/25 | Loss: 0.00117591
Iteration 17/25 | Loss: 0.00117591
Iteration 18/25 | Loss: 0.00117591
Iteration 19/25 | Loss: 0.00117591
Iteration 20/25 | Loss: 0.00117591
Iteration 21/25 | Loss: 0.00117591
Iteration 22/25 | Loss: 0.00117591
Iteration 23/25 | Loss: 0.00117591
Iteration 24/25 | Loss: 0.00117591
Iteration 25/25 | Loss: 0.00117591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92544150
Iteration 2/25 | Loss: 0.00045540
Iteration 3/25 | Loss: 0.00045539
Iteration 4/25 | Loss: 0.00045539
Iteration 5/25 | Loss: 0.00045539
Iteration 6/25 | Loss: 0.00045539
Iteration 7/25 | Loss: 0.00045539
Iteration 8/25 | Loss: 0.00045539
Iteration 9/25 | Loss: 0.00045539
Iteration 10/25 | Loss: 0.00045539
Iteration 11/25 | Loss: 0.00045539
Iteration 12/25 | Loss: 0.00045539
Iteration 13/25 | Loss: 0.00045539
Iteration 14/25 | Loss: 0.00045539
Iteration 15/25 | Loss: 0.00045539
Iteration 16/25 | Loss: 0.00045539
Iteration 17/25 | Loss: 0.00045539
Iteration 18/25 | Loss: 0.00045539
Iteration 19/25 | Loss: 0.00045539
Iteration 20/25 | Loss: 0.00045539
Iteration 21/25 | Loss: 0.00045539
Iteration 22/25 | Loss: 0.00045539
Iteration 23/25 | Loss: 0.00045539
Iteration 24/25 | Loss: 0.00045539
Iteration 25/25 | Loss: 0.00045539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045539
Iteration 2/1000 | Loss: 0.00003602
Iteration 3/1000 | Loss: 0.00002692
Iteration 4/1000 | Loss: 0.00002396
Iteration 5/1000 | Loss: 0.00002289
Iteration 6/1000 | Loss: 0.00002229
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002145
Iteration 9/1000 | Loss: 0.00002127
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002125
Iteration 12/1000 | Loss: 0.00002121
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002107
Iteration 15/1000 | Loss: 0.00002106
Iteration 16/1000 | Loss: 0.00002106
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002106
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002104
Iteration 21/1000 | Loss: 0.00002104
Iteration 22/1000 | Loss: 0.00002103
Iteration 23/1000 | Loss: 0.00002102
Iteration 24/1000 | Loss: 0.00002097
Iteration 25/1000 | Loss: 0.00002097
Iteration 26/1000 | Loss: 0.00002097
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002097
Iteration 29/1000 | Loss: 0.00002097
Iteration 30/1000 | Loss: 0.00002097
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002095
Iteration 34/1000 | Loss: 0.00002094
Iteration 35/1000 | Loss: 0.00002091
Iteration 36/1000 | Loss: 0.00002091
Iteration 37/1000 | Loss: 0.00002091
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002091
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002090
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002090
Iteration 45/1000 | Loss: 0.00002090
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002090
Iteration 48/1000 | Loss: 0.00002090
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002089
Iteration 51/1000 | Loss: 0.00002089
Iteration 52/1000 | Loss: 0.00002089
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002088
Iteration 56/1000 | Loss: 0.00002088
Iteration 57/1000 | Loss: 0.00002088
Iteration 58/1000 | Loss: 0.00002088
Iteration 59/1000 | Loss: 0.00002088
Iteration 60/1000 | Loss: 0.00002088
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002088
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00002088
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002087
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002086
Iteration 77/1000 | Loss: 0.00002086
Iteration 78/1000 | Loss: 0.00002086
Iteration 79/1000 | Loss: 0.00002086
Iteration 80/1000 | Loss: 0.00002086
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002085
Iteration 89/1000 | Loss: 0.00002085
Iteration 90/1000 | Loss: 0.00002085
Iteration 91/1000 | Loss: 0.00002085
Iteration 92/1000 | Loss: 0.00002085
Iteration 93/1000 | Loss: 0.00002085
Iteration 94/1000 | Loss: 0.00002085
Iteration 95/1000 | Loss: 0.00002085
Iteration 96/1000 | Loss: 0.00002085
Iteration 97/1000 | Loss: 0.00002085
Iteration 98/1000 | Loss: 0.00002085
Iteration 99/1000 | Loss: 0.00002085
Iteration 100/1000 | Loss: 0.00002085
Iteration 101/1000 | Loss: 0.00002085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.085054620692972e-05, 2.085054620692972e-05, 2.085054620692972e-05, 2.085054620692972e-05, 2.085054620692972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.085054620692972e-05

Optimization complete. Final v2v error: 3.8549423217773438 mm

Highest mean error: 4.122716903686523 mm for frame 21

Lowest mean error: 3.6468443870544434 mm for frame 141

Saving results

Total time: 31.08080506324768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096334
Iteration 2/25 | Loss: 0.00212371
Iteration 3/25 | Loss: 0.00170062
Iteration 4/25 | Loss: 0.00158446
Iteration 5/25 | Loss: 0.00145750
Iteration 6/25 | Loss: 0.00146294
Iteration 7/25 | Loss: 0.00129759
Iteration 8/25 | Loss: 0.00125018
Iteration 9/25 | Loss: 0.00120423
Iteration 10/25 | Loss: 0.00117724
Iteration 11/25 | Loss: 0.00113285
Iteration 12/25 | Loss: 0.00110488
Iteration 13/25 | Loss: 0.00108779
Iteration 14/25 | Loss: 0.00107273
Iteration 15/25 | Loss: 0.00107403
Iteration 16/25 | Loss: 0.00107181
Iteration 17/25 | Loss: 0.00106006
Iteration 18/25 | Loss: 0.00105835
Iteration 19/25 | Loss: 0.00104970
Iteration 20/25 | Loss: 0.00104448
Iteration 21/25 | Loss: 0.00104300
Iteration 22/25 | Loss: 0.00104274
Iteration 23/25 | Loss: 0.00104273
Iteration 24/25 | Loss: 0.00104273
Iteration 25/25 | Loss: 0.00104273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51694870
Iteration 2/25 | Loss: 0.00090516
Iteration 3/25 | Loss: 0.00090516
Iteration 4/25 | Loss: 0.00090516
Iteration 5/25 | Loss: 0.00090516
Iteration 6/25 | Loss: 0.00090516
Iteration 7/25 | Loss: 0.00090516
Iteration 8/25 | Loss: 0.00090516
Iteration 9/25 | Loss: 0.00090516
Iteration 10/25 | Loss: 0.00090516
Iteration 11/25 | Loss: 0.00090516
Iteration 12/25 | Loss: 0.00090516
Iteration 13/25 | Loss: 0.00090516
Iteration 14/25 | Loss: 0.00090516
Iteration 15/25 | Loss: 0.00090516
Iteration 16/25 | Loss: 0.00090516
Iteration 17/25 | Loss: 0.00090516
Iteration 18/25 | Loss: 0.00090516
Iteration 19/25 | Loss: 0.00090516
Iteration 20/25 | Loss: 0.00090516
Iteration 21/25 | Loss: 0.00090516
Iteration 22/25 | Loss: 0.00090516
Iteration 23/25 | Loss: 0.00090516
Iteration 24/25 | Loss: 0.00090516
Iteration 25/25 | Loss: 0.00090516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090516
Iteration 2/1000 | Loss: 0.00005365
Iteration 3/1000 | Loss: 0.00004100
Iteration 4/1000 | Loss: 0.00022038
Iteration 5/1000 | Loss: 0.00003196
Iteration 6/1000 | Loss: 0.00002856
Iteration 7/1000 | Loss: 0.00002674
Iteration 8/1000 | Loss: 0.00002566
Iteration 9/1000 | Loss: 0.00002483
Iteration 10/1000 | Loss: 0.00002414
Iteration 11/1000 | Loss: 0.00093838
Iteration 12/1000 | Loss: 0.00002535
Iteration 13/1000 | Loss: 0.00002077
Iteration 14/1000 | Loss: 0.00001874
Iteration 15/1000 | Loss: 0.00001723
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001543
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001438
Iteration 37/1000 | Loss: 0.00001438
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001431
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001428
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001426
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001425
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001423
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001423
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001420
Iteration 87/1000 | Loss: 0.00001420
Iteration 88/1000 | Loss: 0.00001420
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001418
Iteration 109/1000 | Loss: 0.00001418
Iteration 110/1000 | Loss: 0.00001418
Iteration 111/1000 | Loss: 0.00001418
Iteration 112/1000 | Loss: 0.00001417
Iteration 113/1000 | Loss: 0.00001417
Iteration 114/1000 | Loss: 0.00001417
Iteration 115/1000 | Loss: 0.00001417
Iteration 116/1000 | Loss: 0.00001417
Iteration 117/1000 | Loss: 0.00001417
Iteration 118/1000 | Loss: 0.00001417
Iteration 119/1000 | Loss: 0.00001417
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001416
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001416
Iteration 129/1000 | Loss: 0.00001416
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001415
Iteration 135/1000 | Loss: 0.00001415
Iteration 136/1000 | Loss: 0.00001415
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001415
Iteration 141/1000 | Loss: 0.00001415
Iteration 142/1000 | Loss: 0.00001415
Iteration 143/1000 | Loss: 0.00001415
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001415
Iteration 148/1000 | Loss: 0.00001415
Iteration 149/1000 | Loss: 0.00001415
Iteration 150/1000 | Loss: 0.00001415
Iteration 151/1000 | Loss: 0.00001415
Iteration 152/1000 | Loss: 0.00001415
Iteration 153/1000 | Loss: 0.00001415
Iteration 154/1000 | Loss: 0.00001415
Iteration 155/1000 | Loss: 0.00001415
Iteration 156/1000 | Loss: 0.00001415
Iteration 157/1000 | Loss: 0.00001415
Iteration 158/1000 | Loss: 0.00001415
Iteration 159/1000 | Loss: 0.00001415
Iteration 160/1000 | Loss: 0.00001415
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001415
Iteration 166/1000 | Loss: 0.00001415
Iteration 167/1000 | Loss: 0.00001415
Iteration 168/1000 | Loss: 0.00001415
Iteration 169/1000 | Loss: 0.00001415
Iteration 170/1000 | Loss: 0.00001415
Iteration 171/1000 | Loss: 0.00001415
Iteration 172/1000 | Loss: 0.00001415
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.4145258319331333e-05, 1.4145258319331333e-05, 1.4145258319331333e-05, 1.4145258319331333e-05, 1.4145258319331333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4145258319331333e-05

Optimization complete. Final v2v error: 3.0918960571289062 mm

Highest mean error: 8.69871997833252 mm for frame 57

Lowest mean error: 2.618868112564087 mm for frame 23

Saving results

Total time: 83.71972823143005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945915
Iteration 2/25 | Loss: 0.00137858
Iteration 3/25 | Loss: 0.00112225
Iteration 4/25 | Loss: 0.00110909
Iteration 5/25 | Loss: 0.00110471
Iteration 6/25 | Loss: 0.00110312
Iteration 7/25 | Loss: 0.00110312
Iteration 8/25 | Loss: 0.00110312
Iteration 9/25 | Loss: 0.00110312
Iteration 10/25 | Loss: 0.00110312
Iteration 11/25 | Loss: 0.00110312
Iteration 12/25 | Loss: 0.00110312
Iteration 13/25 | Loss: 0.00110312
Iteration 14/25 | Loss: 0.00110312
Iteration 15/25 | Loss: 0.00110312
Iteration 16/25 | Loss: 0.00110312
Iteration 17/25 | Loss: 0.00110312
Iteration 18/25 | Loss: 0.00110312
Iteration 19/25 | Loss: 0.00110312
Iteration 20/25 | Loss: 0.00110312
Iteration 21/25 | Loss: 0.00110312
Iteration 22/25 | Loss: 0.00110312
Iteration 23/25 | Loss: 0.00110312
Iteration 24/25 | Loss: 0.00110312
Iteration 25/25 | Loss: 0.00110312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33467185
Iteration 2/25 | Loss: 0.00072477
Iteration 3/25 | Loss: 0.00072477
Iteration 4/25 | Loss: 0.00072477
Iteration 5/25 | Loss: 0.00072477
Iteration 6/25 | Loss: 0.00072477
Iteration 7/25 | Loss: 0.00072477
Iteration 8/25 | Loss: 0.00072477
Iteration 9/25 | Loss: 0.00072477
Iteration 10/25 | Loss: 0.00072477
Iteration 11/25 | Loss: 0.00072477
Iteration 12/25 | Loss: 0.00072477
Iteration 13/25 | Loss: 0.00072477
Iteration 14/25 | Loss: 0.00072477
Iteration 15/25 | Loss: 0.00072477
Iteration 16/25 | Loss: 0.00072477
Iteration 17/25 | Loss: 0.00072477
Iteration 18/25 | Loss: 0.00072477
Iteration 19/25 | Loss: 0.00072477
Iteration 20/25 | Loss: 0.00072477
Iteration 21/25 | Loss: 0.00072477
Iteration 22/25 | Loss: 0.00072477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007247683824971318, 0.0007247683824971318, 0.0007247683824971318, 0.0007247683824971318, 0.0007247683824971318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007247683824971318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072477
Iteration 2/1000 | Loss: 0.00003312
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002358
Iteration 6/1000 | Loss: 0.00002308
Iteration 7/1000 | Loss: 0.00002273
Iteration 8/1000 | Loss: 0.00002252
Iteration 9/1000 | Loss: 0.00002244
Iteration 10/1000 | Loss: 0.00002228
Iteration 11/1000 | Loss: 0.00002218
Iteration 12/1000 | Loss: 0.00002214
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002213
Iteration 15/1000 | Loss: 0.00002206
Iteration 16/1000 | Loss: 0.00002206
Iteration 17/1000 | Loss: 0.00002205
Iteration 18/1000 | Loss: 0.00002204
Iteration 19/1000 | Loss: 0.00002201
Iteration 20/1000 | Loss: 0.00002201
Iteration 21/1000 | Loss: 0.00002201
Iteration 22/1000 | Loss: 0.00002201
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00002198
Iteration 25/1000 | Loss: 0.00002193
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002188
Iteration 32/1000 | Loss: 0.00002188
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00002183
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00002181
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002178
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002178
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002178
Iteration 51/1000 | Loss: 0.00002178
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002178
Iteration 55/1000 | Loss: 0.00002178
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.1782670955872163e-05, 2.1782670955872163e-05, 2.1782670955872163e-05, 2.1782670955872163e-05, 2.1782670955872163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1782670955872163e-05

Optimization complete. Final v2v error: 3.6346402168273926 mm

Highest mean error: 4.402561664581299 mm for frame 182

Lowest mean error: 2.648218870162964 mm for frame 211

Saving results

Total time: 34.68785333633423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834726
Iteration 2/25 | Loss: 0.00134129
Iteration 3/25 | Loss: 0.00114180
Iteration 4/25 | Loss: 0.00112840
Iteration 5/25 | Loss: 0.00112580
Iteration 6/25 | Loss: 0.00112562
Iteration 7/25 | Loss: 0.00112562
Iteration 8/25 | Loss: 0.00112562
Iteration 9/25 | Loss: 0.00112562
Iteration 10/25 | Loss: 0.00112562
Iteration 11/25 | Loss: 0.00112562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011256230063736439, 0.0011256230063736439, 0.0011256230063736439, 0.0011256230063736439, 0.0011256230063736439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011256230063736439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93726981
Iteration 2/25 | Loss: 0.00050406
Iteration 3/25 | Loss: 0.00050404
Iteration 4/25 | Loss: 0.00050403
Iteration 5/25 | Loss: 0.00050403
Iteration 6/25 | Loss: 0.00050403
Iteration 7/25 | Loss: 0.00050403
Iteration 8/25 | Loss: 0.00050403
Iteration 9/25 | Loss: 0.00050403
Iteration 10/25 | Loss: 0.00050403
Iteration 11/25 | Loss: 0.00050403
Iteration 12/25 | Loss: 0.00050403
Iteration 13/25 | Loss: 0.00050403
Iteration 14/25 | Loss: 0.00050403
Iteration 15/25 | Loss: 0.00050403
Iteration 16/25 | Loss: 0.00050403
Iteration 17/25 | Loss: 0.00050403
Iteration 18/25 | Loss: 0.00050403
Iteration 19/25 | Loss: 0.00050403
Iteration 20/25 | Loss: 0.00050403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005040326504968107, 0.0005040326504968107, 0.0005040326504968107, 0.0005040326504968107, 0.0005040326504968107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005040326504968107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050403
Iteration 2/1000 | Loss: 0.00003146
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001960
Iteration 9/1000 | Loss: 0.00001953
Iteration 10/1000 | Loss: 0.00001943
Iteration 11/1000 | Loss: 0.00001937
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001927
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001909
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001906
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001905
Iteration 30/1000 | Loss: 0.00001905
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001905
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001904
Iteration 35/1000 | Loss: 0.00001904
Iteration 36/1000 | Loss: 0.00001902
Iteration 37/1000 | Loss: 0.00001902
Iteration 38/1000 | Loss: 0.00001902
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001902
Iteration 42/1000 | Loss: 0.00001902
Iteration 43/1000 | Loss: 0.00001902
Iteration 44/1000 | Loss: 0.00001902
Iteration 45/1000 | Loss: 0.00001902
Iteration 46/1000 | Loss: 0.00001902
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00001902
Iteration 49/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.901934592751786e-05, 1.901934592751786e-05, 1.901934592751786e-05, 1.901934592751786e-05, 1.901934592751786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.901934592751786e-05

Optimization complete. Final v2v error: 3.6373915672302246 mm

Highest mean error: 4.185360431671143 mm for frame 171

Lowest mean error: 3.216224193572998 mm for frame 44

Saving results

Total time: 31.080726385116577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810347
Iteration 2/25 | Loss: 0.00166398
Iteration 3/25 | Loss: 0.00118276
Iteration 4/25 | Loss: 0.00114936
Iteration 5/25 | Loss: 0.00108590
Iteration 6/25 | Loss: 0.00108604
Iteration 7/25 | Loss: 0.00107108
Iteration 8/25 | Loss: 0.00107390
Iteration 9/25 | Loss: 0.00107801
Iteration 10/25 | Loss: 0.00108528
Iteration 11/25 | Loss: 0.00108726
Iteration 12/25 | Loss: 0.00108331
Iteration 13/25 | Loss: 0.00107752
Iteration 14/25 | Loss: 0.00107291
Iteration 15/25 | Loss: 0.00106792
Iteration 16/25 | Loss: 0.00106535
Iteration 17/25 | Loss: 0.00106482
Iteration 18/25 | Loss: 0.00106458
Iteration 19/25 | Loss: 0.00106434
Iteration 20/25 | Loss: 0.00106412
Iteration 21/25 | Loss: 0.00106452
Iteration 22/25 | Loss: 0.00106202
Iteration 23/25 | Loss: 0.00106150
Iteration 24/25 | Loss: 0.00106123
Iteration 25/25 | Loss: 0.00106110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16638732
Iteration 2/25 | Loss: 0.00076949
Iteration 3/25 | Loss: 0.00076926
Iteration 4/25 | Loss: 0.00076926
Iteration 5/25 | Loss: 0.00076926
Iteration 6/25 | Loss: 0.00076926
Iteration 7/25 | Loss: 0.00076926
Iteration 8/25 | Loss: 0.00076926
Iteration 9/25 | Loss: 0.00076926
Iteration 10/25 | Loss: 0.00076926
Iteration 11/25 | Loss: 0.00076926
Iteration 12/25 | Loss: 0.00076926
Iteration 13/25 | Loss: 0.00076926
Iteration 14/25 | Loss: 0.00076926
Iteration 15/25 | Loss: 0.00076926
Iteration 16/25 | Loss: 0.00076926
Iteration 17/25 | Loss: 0.00076926
Iteration 18/25 | Loss: 0.00076926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007692566141486168, 0.0007692566141486168, 0.0007692566141486168, 0.0007692566141486168, 0.0007692566141486168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007692566141486168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076926
Iteration 2/1000 | Loss: 0.00002319
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001663
Iteration 5/1000 | Loss: 0.00001572
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001429
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001428
Iteration 17/1000 | Loss: 0.00001426
Iteration 18/1000 | Loss: 0.00001426
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001422
Iteration 27/1000 | Loss: 0.00001422
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001421
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001420
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001418
Iteration 37/1000 | Loss: 0.00001417
Iteration 38/1000 | Loss: 0.00001417
Iteration 39/1000 | Loss: 0.00001417
Iteration 40/1000 | Loss: 0.00001417
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001416
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001414
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001414
Iteration 51/1000 | Loss: 0.00001413
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001411
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001410
Iteration 59/1000 | Loss: 0.00001410
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001409
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001409
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001407
Iteration 81/1000 | Loss: 0.00001407
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001405
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001405
Iteration 117/1000 | Loss: 0.00001405
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4053940503799822e-05, 1.4053940503799822e-05, 1.4053940503799822e-05, 1.4053940503799822e-05, 1.4053940503799822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4053940503799822e-05

Optimization complete. Final v2v error: 3.157780408859253 mm

Highest mean error: 8.769685745239258 mm for frame 192

Lowest mean error: 2.6145429611206055 mm for frame 215

Saving results

Total time: 77.15540099143982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009053
Iteration 2/25 | Loss: 0.00279018
Iteration 3/25 | Loss: 0.00182088
Iteration 4/25 | Loss: 0.00164583
Iteration 5/25 | Loss: 0.00167703
Iteration 6/25 | Loss: 0.00159428
Iteration 7/25 | Loss: 0.00148748
Iteration 8/25 | Loss: 0.00142583
Iteration 9/25 | Loss: 0.00139110
Iteration 10/25 | Loss: 0.00135985
Iteration 11/25 | Loss: 0.00135058
Iteration 12/25 | Loss: 0.00134933
Iteration 13/25 | Loss: 0.00135832
Iteration 14/25 | Loss: 0.00135241
Iteration 15/25 | Loss: 0.00133777
Iteration 16/25 | Loss: 0.00134515
Iteration 17/25 | Loss: 0.00133342
Iteration 18/25 | Loss: 0.00130790
Iteration 19/25 | Loss: 0.00129134
Iteration 20/25 | Loss: 0.00128712
Iteration 21/25 | Loss: 0.00128656
Iteration 22/25 | Loss: 0.00128019
Iteration 23/25 | Loss: 0.00128435
Iteration 24/25 | Loss: 0.00127900
Iteration 25/25 | Loss: 0.00127123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15388739
Iteration 2/25 | Loss: 0.00202279
Iteration 3/25 | Loss: 0.00202278
Iteration 4/25 | Loss: 0.00202278
Iteration 5/25 | Loss: 0.00202277
Iteration 6/25 | Loss: 0.00202277
Iteration 7/25 | Loss: 0.00202277
Iteration 8/25 | Loss: 0.00202277
Iteration 9/25 | Loss: 0.00202277
Iteration 10/25 | Loss: 0.00202277
Iteration 11/25 | Loss: 0.00202277
Iteration 12/25 | Loss: 0.00202277
Iteration 13/25 | Loss: 0.00202277
Iteration 14/25 | Loss: 0.00202277
Iteration 15/25 | Loss: 0.00202277
Iteration 16/25 | Loss: 0.00202277
Iteration 17/25 | Loss: 0.00202277
Iteration 18/25 | Loss: 0.00202277
Iteration 19/25 | Loss: 0.00202277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020227734930813313, 0.0020227734930813313, 0.0020227734930813313, 0.0020227734930813313, 0.0020227734930813313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020227734930813313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202277
Iteration 2/1000 | Loss: 0.00064582
Iteration 3/1000 | Loss: 0.00026803
Iteration 4/1000 | Loss: 0.00066980
Iteration 5/1000 | Loss: 0.00051611
Iteration 6/1000 | Loss: 0.00062500
Iteration 7/1000 | Loss: 0.00067913
Iteration 8/1000 | Loss: 0.00041644
Iteration 9/1000 | Loss: 0.00046663
Iteration 10/1000 | Loss: 0.00036473
Iteration 11/1000 | Loss: 0.00055493
Iteration 12/1000 | Loss: 0.00046868
Iteration 13/1000 | Loss: 0.00045437
Iteration 14/1000 | Loss: 0.00040881
Iteration 15/1000 | Loss: 0.00054695
Iteration 16/1000 | Loss: 0.00042541
Iteration 17/1000 | Loss: 0.00075840
Iteration 18/1000 | Loss: 0.00044256
Iteration 19/1000 | Loss: 0.00112557
Iteration 20/1000 | Loss: 0.00117309
Iteration 21/1000 | Loss: 0.00087869
Iteration 22/1000 | Loss: 0.00078239
Iteration 23/1000 | Loss: 0.00085633
Iteration 24/1000 | Loss: 0.00105687
Iteration 25/1000 | Loss: 0.00066453
Iteration 26/1000 | Loss: 0.00048863
Iteration 27/1000 | Loss: 0.00032627
Iteration 28/1000 | Loss: 0.00013977
Iteration 29/1000 | Loss: 0.00033067
Iteration 30/1000 | Loss: 0.00045385
Iteration 31/1000 | Loss: 0.00053263
Iteration 32/1000 | Loss: 0.00031098
Iteration 33/1000 | Loss: 0.00036004
Iteration 34/1000 | Loss: 0.00073157
Iteration 35/1000 | Loss: 0.00025740
Iteration 36/1000 | Loss: 0.00041366
Iteration 37/1000 | Loss: 0.00016989
Iteration 38/1000 | Loss: 0.00027846
Iteration 39/1000 | Loss: 0.00026307
Iteration 40/1000 | Loss: 0.00025388
Iteration 41/1000 | Loss: 0.00043204
Iteration 42/1000 | Loss: 0.00049802
Iteration 43/1000 | Loss: 0.00054778
Iteration 44/1000 | Loss: 0.00088322
Iteration 45/1000 | Loss: 0.00025680
Iteration 46/1000 | Loss: 0.00029888
Iteration 47/1000 | Loss: 0.00032978
Iteration 48/1000 | Loss: 0.00024455
Iteration 49/1000 | Loss: 0.00017546
Iteration 50/1000 | Loss: 0.00016806
Iteration 51/1000 | Loss: 0.00017780
Iteration 52/1000 | Loss: 0.00027574
Iteration 53/1000 | Loss: 0.00029630
Iteration 54/1000 | Loss: 0.00034258
Iteration 55/1000 | Loss: 0.00027760
Iteration 56/1000 | Loss: 0.00029401
Iteration 57/1000 | Loss: 0.00032028
Iteration 58/1000 | Loss: 0.00011619
Iteration 59/1000 | Loss: 0.00062231
Iteration 60/1000 | Loss: 0.00032898
Iteration 61/1000 | Loss: 0.00026963
Iteration 62/1000 | Loss: 0.00060725
Iteration 63/1000 | Loss: 0.00022101
Iteration 64/1000 | Loss: 0.00029600
Iteration 65/1000 | Loss: 0.00036038
Iteration 66/1000 | Loss: 0.00040352
Iteration 67/1000 | Loss: 0.00010634
Iteration 68/1000 | Loss: 0.00030324
Iteration 69/1000 | Loss: 0.00029383
Iteration 70/1000 | Loss: 0.00035833
Iteration 71/1000 | Loss: 0.00032973
Iteration 72/1000 | Loss: 0.00033846
Iteration 73/1000 | Loss: 0.00039475
Iteration 74/1000 | Loss: 0.00082598
Iteration 75/1000 | Loss: 0.00078986
Iteration 76/1000 | Loss: 0.00026433
Iteration 77/1000 | Loss: 0.00035070
Iteration 78/1000 | Loss: 0.00031581
Iteration 79/1000 | Loss: 0.00036300
Iteration 80/1000 | Loss: 0.00030187
Iteration 81/1000 | Loss: 0.00024147
Iteration 82/1000 | Loss: 0.00017425
Iteration 83/1000 | Loss: 0.00015492
Iteration 84/1000 | Loss: 0.00010807
Iteration 85/1000 | Loss: 0.00018693
Iteration 86/1000 | Loss: 0.00016605
Iteration 87/1000 | Loss: 0.00018465
Iteration 88/1000 | Loss: 0.00008173
Iteration 89/1000 | Loss: 0.00019217
Iteration 90/1000 | Loss: 0.00020345
Iteration 91/1000 | Loss: 0.00017651
Iteration 92/1000 | Loss: 0.00020775
Iteration 93/1000 | Loss: 0.00021034
Iteration 94/1000 | Loss: 0.00036526
Iteration 95/1000 | Loss: 0.00130248
Iteration 96/1000 | Loss: 0.00096097
Iteration 97/1000 | Loss: 0.00061506
Iteration 98/1000 | Loss: 0.00104277
Iteration 99/1000 | Loss: 0.00033086
Iteration 100/1000 | Loss: 0.00050100
Iteration 101/1000 | Loss: 0.00039263
Iteration 102/1000 | Loss: 0.00024540
Iteration 103/1000 | Loss: 0.00039123
Iteration 104/1000 | Loss: 0.00037411
Iteration 105/1000 | Loss: 0.00107632
Iteration 106/1000 | Loss: 0.00040902
Iteration 107/1000 | Loss: 0.00043693
Iteration 108/1000 | Loss: 0.00029771
Iteration 109/1000 | Loss: 0.00033709
Iteration 110/1000 | Loss: 0.00031132
Iteration 111/1000 | Loss: 0.00030308
Iteration 112/1000 | Loss: 0.00037833
Iteration 113/1000 | Loss: 0.00028942
Iteration 114/1000 | Loss: 0.00028812
Iteration 115/1000 | Loss: 0.00038419
Iteration 116/1000 | Loss: 0.00013223
Iteration 117/1000 | Loss: 0.00025977
Iteration 118/1000 | Loss: 0.00020082
Iteration 119/1000 | Loss: 0.00046201
Iteration 120/1000 | Loss: 0.00018707
Iteration 121/1000 | Loss: 0.00035243
Iteration 122/1000 | Loss: 0.00029482
Iteration 123/1000 | Loss: 0.00021888
Iteration 124/1000 | Loss: 0.00016343
Iteration 125/1000 | Loss: 0.00018261
Iteration 126/1000 | Loss: 0.00039397
Iteration 127/1000 | Loss: 0.00061511
Iteration 128/1000 | Loss: 0.00031474
Iteration 129/1000 | Loss: 0.00033020
Iteration 130/1000 | Loss: 0.00024118
Iteration 131/1000 | Loss: 0.00039288
Iteration 132/1000 | Loss: 0.00023402
Iteration 133/1000 | Loss: 0.00030080
Iteration 134/1000 | Loss: 0.00011032
Iteration 135/1000 | Loss: 0.00025313
Iteration 136/1000 | Loss: 0.00026050
Iteration 137/1000 | Loss: 0.00028215
Iteration 138/1000 | Loss: 0.00048040
Iteration 139/1000 | Loss: 0.00029639
Iteration 140/1000 | Loss: 0.00024314
Iteration 141/1000 | Loss: 0.00030844
Iteration 142/1000 | Loss: 0.00023870
Iteration 143/1000 | Loss: 0.00023491
Iteration 144/1000 | Loss: 0.00034451
Iteration 145/1000 | Loss: 0.00032119
Iteration 146/1000 | Loss: 0.00029773
Iteration 147/1000 | Loss: 0.00041365
Iteration 148/1000 | Loss: 0.00032248
Iteration 149/1000 | Loss: 0.00020760
Iteration 150/1000 | Loss: 0.00026131
Iteration 151/1000 | Loss: 0.00031680
Iteration 152/1000 | Loss: 0.00034295
Iteration 153/1000 | Loss: 0.00028018
Iteration 154/1000 | Loss: 0.00029911
Iteration 155/1000 | Loss: 0.00023075
Iteration 156/1000 | Loss: 0.00025549
Iteration 157/1000 | Loss: 0.00027241
Iteration 158/1000 | Loss: 0.00029482
Iteration 159/1000 | Loss: 0.00021386
Iteration 160/1000 | Loss: 0.00037379
Iteration 161/1000 | Loss: 0.00030345
Iteration 162/1000 | Loss: 0.00036013
Iteration 163/1000 | Loss: 0.00034836
Iteration 164/1000 | Loss: 0.00026573
Iteration 165/1000 | Loss: 0.00030398
Iteration 166/1000 | Loss: 0.00029091
Iteration 167/1000 | Loss: 0.00041993
Iteration 168/1000 | Loss: 0.00031441
Iteration 169/1000 | Loss: 0.00030316
Iteration 170/1000 | Loss: 0.00042894
Iteration 171/1000 | Loss: 0.00013325
Iteration 172/1000 | Loss: 0.00027100
Iteration 173/1000 | Loss: 0.00012333
Iteration 174/1000 | Loss: 0.00075820
Iteration 175/1000 | Loss: 0.00150420
Iteration 176/1000 | Loss: 0.00116771
Iteration 177/1000 | Loss: 0.00078912
Iteration 178/1000 | Loss: 0.00036499
Iteration 179/1000 | Loss: 0.00020107
Iteration 180/1000 | Loss: 0.00021200
Iteration 181/1000 | Loss: 0.00018866
Iteration 182/1000 | Loss: 0.00025677
Iteration 183/1000 | Loss: 0.00031432
Iteration 184/1000 | Loss: 0.00036248
Iteration 185/1000 | Loss: 0.00040640
Iteration 186/1000 | Loss: 0.00024337
Iteration 187/1000 | Loss: 0.00023390
Iteration 188/1000 | Loss: 0.00033094
Iteration 189/1000 | Loss: 0.00027547
Iteration 190/1000 | Loss: 0.00027413
Iteration 191/1000 | Loss: 0.00031882
Iteration 192/1000 | Loss: 0.00034041
Iteration 193/1000 | Loss: 0.00038681
Iteration 194/1000 | Loss: 0.00075150
Iteration 195/1000 | Loss: 0.00043213
Iteration 196/1000 | Loss: 0.00036564
Iteration 197/1000 | Loss: 0.00041154
Iteration 198/1000 | Loss: 0.00026636
Iteration 199/1000 | Loss: 0.00026781
Iteration 200/1000 | Loss: 0.00021389
Iteration 201/1000 | Loss: 0.00036216
Iteration 202/1000 | Loss: 0.00104911
Iteration 203/1000 | Loss: 0.00061550
Iteration 204/1000 | Loss: 0.00025320
Iteration 205/1000 | Loss: 0.00030199
Iteration 206/1000 | Loss: 0.00028217
Iteration 207/1000 | Loss: 0.00032578
Iteration 208/1000 | Loss: 0.00025717
Iteration 209/1000 | Loss: 0.00028795
Iteration 210/1000 | Loss: 0.00029535
Iteration 211/1000 | Loss: 0.00029464
Iteration 212/1000 | Loss: 0.00033477
Iteration 213/1000 | Loss: 0.00030305
Iteration 214/1000 | Loss: 0.00028997
Iteration 215/1000 | Loss: 0.00014066
Iteration 216/1000 | Loss: 0.00023412
Iteration 217/1000 | Loss: 0.00026842
Iteration 218/1000 | Loss: 0.00022923
Iteration 219/1000 | Loss: 0.00021418
Iteration 220/1000 | Loss: 0.00036140
Iteration 221/1000 | Loss: 0.00038911
Iteration 222/1000 | Loss: 0.00030910
Iteration 223/1000 | Loss: 0.00022631
Iteration 224/1000 | Loss: 0.00032027
Iteration 225/1000 | Loss: 0.00010099
Iteration 226/1000 | Loss: 0.00052529
Iteration 227/1000 | Loss: 0.00018410
Iteration 228/1000 | Loss: 0.00016614
Iteration 229/1000 | Loss: 0.00020948
Iteration 230/1000 | Loss: 0.00036906
Iteration 231/1000 | Loss: 0.00112767
Iteration 232/1000 | Loss: 0.00039493
Iteration 233/1000 | Loss: 0.00101212
Iteration 234/1000 | Loss: 0.00066714
Iteration 235/1000 | Loss: 0.00028772
Iteration 236/1000 | Loss: 0.00012747
Iteration 237/1000 | Loss: 0.00024945
Iteration 238/1000 | Loss: 0.00016740
Iteration 239/1000 | Loss: 0.00067871
Iteration 240/1000 | Loss: 0.00021683
Iteration 241/1000 | Loss: 0.00038248
Iteration 242/1000 | Loss: 0.00065945
Iteration 243/1000 | Loss: 0.00061073
Iteration 244/1000 | Loss: 0.00028155
Iteration 245/1000 | Loss: 0.00059412
Iteration 246/1000 | Loss: 0.00038948
Iteration 247/1000 | Loss: 0.00025528
Iteration 248/1000 | Loss: 0.00033819
Iteration 249/1000 | Loss: 0.00035241
Iteration 250/1000 | Loss: 0.00044556
Iteration 251/1000 | Loss: 0.00030805
Iteration 252/1000 | Loss: 0.00038472
Iteration 253/1000 | Loss: 0.00033186
Iteration 254/1000 | Loss: 0.00036547
Iteration 255/1000 | Loss: 0.00043051
Iteration 256/1000 | Loss: 0.00024472
Iteration 257/1000 | Loss: 0.00020781
Iteration 258/1000 | Loss: 0.00015535
Iteration 259/1000 | Loss: 0.00019760
Iteration 260/1000 | Loss: 0.00015216
Iteration 261/1000 | Loss: 0.00017955
Iteration 262/1000 | Loss: 0.00010400
Iteration 263/1000 | Loss: 0.00022454
Iteration 264/1000 | Loss: 0.00017373
Iteration 265/1000 | Loss: 0.00015573
Iteration 266/1000 | Loss: 0.00027698
Iteration 267/1000 | Loss: 0.00021630
Iteration 268/1000 | Loss: 0.00018932
Iteration 269/1000 | Loss: 0.00022350
Iteration 270/1000 | Loss: 0.00020912
Iteration 271/1000 | Loss: 0.00034883
Iteration 272/1000 | Loss: 0.00010748
Iteration 273/1000 | Loss: 0.00005469
Iteration 274/1000 | Loss: 0.00013112
Iteration 275/1000 | Loss: 0.00016909
Iteration 276/1000 | Loss: 0.00009361
Iteration 277/1000 | Loss: 0.00014790
Iteration 278/1000 | Loss: 0.00014943
Iteration 279/1000 | Loss: 0.00014854
Iteration 280/1000 | Loss: 0.00011641
Iteration 281/1000 | Loss: 0.00030225
Iteration 282/1000 | Loss: 0.00007710
Iteration 283/1000 | Loss: 0.00014190
Iteration 284/1000 | Loss: 0.00012929
Iteration 285/1000 | Loss: 0.00010154
Iteration 286/1000 | Loss: 0.00017018
Iteration 287/1000 | Loss: 0.00006472
Iteration 288/1000 | Loss: 0.00007780
Iteration 289/1000 | Loss: 0.00006169
Iteration 290/1000 | Loss: 0.00006059
Iteration 291/1000 | Loss: 0.00005910
Iteration 292/1000 | Loss: 0.00006369
Iteration 293/1000 | Loss: 0.00006797
Iteration 294/1000 | Loss: 0.00006649
Iteration 295/1000 | Loss: 0.00094312
Iteration 296/1000 | Loss: 0.00051184
Iteration 297/1000 | Loss: 0.00024941
Iteration 298/1000 | Loss: 0.00006939
Iteration 299/1000 | Loss: 0.00006633
Iteration 300/1000 | Loss: 0.00006391
Iteration 301/1000 | Loss: 0.00006375
Iteration 302/1000 | Loss: 0.00006412
Iteration 303/1000 | Loss: 0.00004782
Iteration 304/1000 | Loss: 0.00006564
Iteration 305/1000 | Loss: 0.00006469
Iteration 306/1000 | Loss: 0.00006532
Iteration 307/1000 | Loss: 0.00006632
Iteration 308/1000 | Loss: 0.00005585
Iteration 309/1000 | Loss: 0.00005595
Iteration 310/1000 | Loss: 0.00005566
Iteration 311/1000 | Loss: 0.00005742
Iteration 312/1000 | Loss: 0.00006613
Iteration 313/1000 | Loss: 0.00027496
Iteration 314/1000 | Loss: 0.00016506
Iteration 315/1000 | Loss: 0.00023644
Iteration 316/1000 | Loss: 0.00015566
Iteration 317/1000 | Loss: 0.00021508
Iteration 318/1000 | Loss: 0.00015365
Iteration 319/1000 | Loss: 0.00021076
Iteration 320/1000 | Loss: 0.00021463
Iteration 321/1000 | Loss: 0.00020270
Iteration 322/1000 | Loss: 0.00007793
Iteration 323/1000 | Loss: 0.00006782
Iteration 324/1000 | Loss: 0.00004992
Iteration 325/1000 | Loss: 0.00006786
Iteration 326/1000 | Loss: 0.00007002
Iteration 327/1000 | Loss: 0.00006136
Iteration 328/1000 | Loss: 0.00006846
Iteration 329/1000 | Loss: 0.00005963
Iteration 330/1000 | Loss: 0.00005616
Iteration 331/1000 | Loss: 0.00007288
Iteration 332/1000 | Loss: 0.00005882
Iteration 333/1000 | Loss: 0.00031989
Iteration 334/1000 | Loss: 0.00015075
Iteration 335/1000 | Loss: 0.00027889
Iteration 336/1000 | Loss: 0.00014419
Iteration 337/1000 | Loss: 0.00024306
Iteration 338/1000 | Loss: 0.00014003
Iteration 339/1000 | Loss: 0.00025345
Iteration 340/1000 | Loss: 0.00026960
Iteration 341/1000 | Loss: 0.00005566
Iteration 342/1000 | Loss: 0.00005076
Iteration 343/1000 | Loss: 0.00004672
Iteration 344/1000 | Loss: 0.00004436
Iteration 345/1000 | Loss: 0.00004276
Iteration 346/1000 | Loss: 0.00004182
Iteration 347/1000 | Loss: 0.00004071
Iteration 348/1000 | Loss: 0.00004038
Iteration 349/1000 | Loss: 0.00004014
Iteration 350/1000 | Loss: 0.00003974
Iteration 351/1000 | Loss: 0.00003946
Iteration 352/1000 | Loss: 0.00003925
Iteration 353/1000 | Loss: 0.00003925
Iteration 354/1000 | Loss: 0.00003923
Iteration 355/1000 | Loss: 0.00003920
Iteration 356/1000 | Loss: 0.00003917
Iteration 357/1000 | Loss: 0.00003917
Iteration 358/1000 | Loss: 0.00003916
Iteration 359/1000 | Loss: 0.00003915
Iteration 360/1000 | Loss: 0.00003915
Iteration 361/1000 | Loss: 0.00003914
Iteration 362/1000 | Loss: 0.00003914
Iteration 363/1000 | Loss: 0.00003913
Iteration 364/1000 | Loss: 0.00003913
Iteration 365/1000 | Loss: 0.00003913
Iteration 366/1000 | Loss: 0.00003912
Iteration 367/1000 | Loss: 0.00003912
Iteration 368/1000 | Loss: 0.00003911
Iteration 369/1000 | Loss: 0.00003911
Iteration 370/1000 | Loss: 0.00003911
Iteration 371/1000 | Loss: 0.00003910
Iteration 372/1000 | Loss: 0.00003910
Iteration 373/1000 | Loss: 0.00003910
Iteration 374/1000 | Loss: 0.00003909
Iteration 375/1000 | Loss: 0.00003909
Iteration 376/1000 | Loss: 0.00003909
Iteration 377/1000 | Loss: 0.00003909
Iteration 378/1000 | Loss: 0.00003909
Iteration 379/1000 | Loss: 0.00003909
Iteration 380/1000 | Loss: 0.00003909
Iteration 381/1000 | Loss: 0.00003908
Iteration 382/1000 | Loss: 0.00003908
Iteration 383/1000 | Loss: 0.00003908
Iteration 384/1000 | Loss: 0.00003908
Iteration 385/1000 | Loss: 0.00003908
Iteration 386/1000 | Loss: 0.00003908
Iteration 387/1000 | Loss: 0.00003908
Iteration 388/1000 | Loss: 0.00003908
Iteration 389/1000 | Loss: 0.00003908
Iteration 390/1000 | Loss: 0.00003908
Iteration 391/1000 | Loss: 0.00003907
Iteration 392/1000 | Loss: 0.00003907
Iteration 393/1000 | Loss: 0.00003907
Iteration 394/1000 | Loss: 0.00003907
Iteration 395/1000 | Loss: 0.00003907
Iteration 396/1000 | Loss: 0.00003906
Iteration 397/1000 | Loss: 0.00003905
Iteration 398/1000 | Loss: 0.00003904
Iteration 399/1000 | Loss: 0.00003904
Iteration 400/1000 | Loss: 0.00003903
Iteration 401/1000 | Loss: 0.00003902
Iteration 402/1000 | Loss: 0.00003901
Iteration 403/1000 | Loss: 0.00003901
Iteration 404/1000 | Loss: 0.00003901
Iteration 405/1000 | Loss: 0.00003900
Iteration 406/1000 | Loss: 0.00003900
Iteration 407/1000 | Loss: 0.00003898
Iteration 408/1000 | Loss: 0.00003896
Iteration 409/1000 | Loss: 0.00003896
Iteration 410/1000 | Loss: 0.00003896
Iteration 411/1000 | Loss: 0.00003896
Iteration 412/1000 | Loss: 0.00003896
Iteration 413/1000 | Loss: 0.00003896
Iteration 414/1000 | Loss: 0.00003896
Iteration 415/1000 | Loss: 0.00003895
Iteration 416/1000 | Loss: 0.00003895
Iteration 417/1000 | Loss: 0.00003895
Iteration 418/1000 | Loss: 0.00003895
Iteration 419/1000 | Loss: 0.00003895
Iteration 420/1000 | Loss: 0.00003895
Iteration 421/1000 | Loss: 0.00003895
Iteration 422/1000 | Loss: 0.00003895
Iteration 423/1000 | Loss: 0.00003895
Iteration 424/1000 | Loss: 0.00003895
Iteration 425/1000 | Loss: 0.00003895
Iteration 426/1000 | Loss: 0.00003894
Iteration 427/1000 | Loss: 0.00003894
Iteration 428/1000 | Loss: 0.00003894
Iteration 429/1000 | Loss: 0.00003894
Iteration 430/1000 | Loss: 0.00003894
Iteration 431/1000 | Loss: 0.00003894
Iteration 432/1000 | Loss: 0.00003894
Iteration 433/1000 | Loss: 0.00003894
Iteration 434/1000 | Loss: 0.00003894
Iteration 435/1000 | Loss: 0.00003894
Iteration 436/1000 | Loss: 0.00003894
Iteration 437/1000 | Loss: 0.00003893
Iteration 438/1000 | Loss: 0.00003893
Iteration 439/1000 | Loss: 0.00003893
Iteration 440/1000 | Loss: 0.00003893
Iteration 441/1000 | Loss: 0.00003893
Iteration 442/1000 | Loss: 0.00003893
Iteration 443/1000 | Loss: 0.00003893
Iteration 444/1000 | Loss: 0.00003893
Iteration 445/1000 | Loss: 0.00003893
Iteration 446/1000 | Loss: 0.00003892
Iteration 447/1000 | Loss: 0.00003892
Iteration 448/1000 | Loss: 0.00003892
Iteration 449/1000 | Loss: 0.00003892
Iteration 450/1000 | Loss: 0.00003892
Iteration 451/1000 | Loss: 0.00003891
Iteration 452/1000 | Loss: 0.00003891
Iteration 453/1000 | Loss: 0.00003891
Iteration 454/1000 | Loss: 0.00003891
Iteration 455/1000 | Loss: 0.00003891
Iteration 456/1000 | Loss: 0.00003890
Iteration 457/1000 | Loss: 0.00003890
Iteration 458/1000 | Loss: 0.00003890
Iteration 459/1000 | Loss: 0.00003890
Iteration 460/1000 | Loss: 0.00003890
Iteration 461/1000 | Loss: 0.00003890
Iteration 462/1000 | Loss: 0.00003890
Iteration 463/1000 | Loss: 0.00003890
Iteration 464/1000 | Loss: 0.00003890
Iteration 465/1000 | Loss: 0.00003890
Iteration 466/1000 | Loss: 0.00003890
Iteration 467/1000 | Loss: 0.00003889
Iteration 468/1000 | Loss: 0.00003889
Iteration 469/1000 | Loss: 0.00003889
Iteration 470/1000 | Loss: 0.00003889
Iteration 471/1000 | Loss: 0.00003889
Iteration 472/1000 | Loss: 0.00003889
Iteration 473/1000 | Loss: 0.00003889
Iteration 474/1000 | Loss: 0.00003889
Iteration 475/1000 | Loss: 0.00003889
Iteration 476/1000 | Loss: 0.00003889
Iteration 477/1000 | Loss: 0.00003889
Iteration 478/1000 | Loss: 0.00003888
Iteration 479/1000 | Loss: 0.00003888
Iteration 480/1000 | Loss: 0.00003888
Iteration 481/1000 | Loss: 0.00003888
Iteration 482/1000 | Loss: 0.00003888
Iteration 483/1000 | Loss: 0.00003888
Iteration 484/1000 | Loss: 0.00003888
Iteration 485/1000 | Loss: 0.00003888
Iteration 486/1000 | Loss: 0.00003887
Iteration 487/1000 | Loss: 0.00003887
Iteration 488/1000 | Loss: 0.00003887
Iteration 489/1000 | Loss: 0.00003887
Iteration 490/1000 | Loss: 0.00003887
Iteration 491/1000 | Loss: 0.00003887
Iteration 492/1000 | Loss: 0.00003887
Iteration 493/1000 | Loss: 0.00003887
Iteration 494/1000 | Loss: 0.00003887
Iteration 495/1000 | Loss: 0.00003887
Iteration 496/1000 | Loss: 0.00003887
Iteration 497/1000 | Loss: 0.00003886
Iteration 498/1000 | Loss: 0.00003886
Iteration 499/1000 | Loss: 0.00003886
Iteration 500/1000 | Loss: 0.00003886
Iteration 501/1000 | Loss: 0.00003886
Iteration 502/1000 | Loss: 0.00003886
Iteration 503/1000 | Loss: 0.00003886
Iteration 504/1000 | Loss: 0.00003886
Iteration 505/1000 | Loss: 0.00003886
Iteration 506/1000 | Loss: 0.00003886
Iteration 507/1000 | Loss: 0.00003886
Iteration 508/1000 | Loss: 0.00003886
Iteration 509/1000 | Loss: 0.00003886
Iteration 510/1000 | Loss: 0.00003886
Iteration 511/1000 | Loss: 0.00003886
Iteration 512/1000 | Loss: 0.00003886
Iteration 513/1000 | Loss: 0.00003886
Iteration 514/1000 | Loss: 0.00003885
Iteration 515/1000 | Loss: 0.00003885
Iteration 516/1000 | Loss: 0.00003885
Iteration 517/1000 | Loss: 0.00003885
Iteration 518/1000 | Loss: 0.00003885
Iteration 519/1000 | Loss: 0.00003885
Iteration 520/1000 | Loss: 0.00003885
Iteration 521/1000 | Loss: 0.00003885
Iteration 522/1000 | Loss: 0.00003885
Iteration 523/1000 | Loss: 0.00003885
Iteration 524/1000 | Loss: 0.00003885
Iteration 525/1000 | Loss: 0.00003885
Iteration 526/1000 | Loss: 0.00003885
Iteration 527/1000 | Loss: 0.00003885
Iteration 528/1000 | Loss: 0.00003885
Iteration 529/1000 | Loss: 0.00003885
Iteration 530/1000 | Loss: 0.00003885
Iteration 531/1000 | Loss: 0.00003884
Iteration 532/1000 | Loss: 0.00003884
Iteration 533/1000 | Loss: 0.00003884
Iteration 534/1000 | Loss: 0.00003884
Iteration 535/1000 | Loss: 0.00003884
Iteration 536/1000 | Loss: 0.00003884
Iteration 537/1000 | Loss: 0.00003884
Iteration 538/1000 | Loss: 0.00003884
Iteration 539/1000 | Loss: 0.00003884
Iteration 540/1000 | Loss: 0.00003884
Iteration 541/1000 | Loss: 0.00003884
Iteration 542/1000 | Loss: 0.00003884
Iteration 543/1000 | Loss: 0.00003884
Iteration 544/1000 | Loss: 0.00003884
Iteration 545/1000 | Loss: 0.00003884
Iteration 546/1000 | Loss: 0.00003884
Iteration 547/1000 | Loss: 0.00003884
Iteration 548/1000 | Loss: 0.00003884
Iteration 549/1000 | Loss: 0.00003884
Iteration 550/1000 | Loss: 0.00003884
Iteration 551/1000 | Loss: 0.00003884
Iteration 552/1000 | Loss: 0.00003884
Iteration 553/1000 | Loss: 0.00003884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 553. Stopping optimization.
Last 5 losses: [3.8836125895613804e-05, 3.8836125895613804e-05, 3.8836125895613804e-05, 3.8836125895613804e-05, 3.8836125895613804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8836125895613804e-05

Optimization complete. Final v2v error: 4.134160995483398 mm

Highest mean error: 12.762601852416992 mm for frame 21

Lowest mean error: 2.8842239379882812 mm for frame 91

Saving results

Total time: 570.9093055725098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412625
Iteration 2/25 | Loss: 0.00120013
Iteration 3/25 | Loss: 0.00113253
Iteration 4/25 | Loss: 0.00112627
Iteration 5/25 | Loss: 0.00112375
Iteration 6/25 | Loss: 0.00112335
Iteration 7/25 | Loss: 0.00112335
Iteration 8/25 | Loss: 0.00112335
Iteration 9/25 | Loss: 0.00112335
Iteration 10/25 | Loss: 0.00112335
Iteration 11/25 | Loss: 0.00112335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001123345224186778, 0.001123345224186778, 0.001123345224186778, 0.001123345224186778, 0.001123345224186778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001123345224186778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.77888823
Iteration 2/25 | Loss: 0.00084751
Iteration 3/25 | Loss: 0.00084750
Iteration 4/25 | Loss: 0.00084750
Iteration 5/25 | Loss: 0.00084750
Iteration 6/25 | Loss: 0.00084750
Iteration 7/25 | Loss: 0.00084750
Iteration 8/25 | Loss: 0.00084750
Iteration 9/25 | Loss: 0.00084750
Iteration 10/25 | Loss: 0.00084750
Iteration 11/25 | Loss: 0.00084750
Iteration 12/25 | Loss: 0.00084750
Iteration 13/25 | Loss: 0.00084750
Iteration 14/25 | Loss: 0.00084750
Iteration 15/25 | Loss: 0.00084750
Iteration 16/25 | Loss: 0.00084750
Iteration 17/25 | Loss: 0.00084750
Iteration 18/25 | Loss: 0.00084750
Iteration 19/25 | Loss: 0.00084750
Iteration 20/25 | Loss: 0.00084750
Iteration 21/25 | Loss: 0.00084750
Iteration 22/25 | Loss: 0.00084750
Iteration 23/25 | Loss: 0.00084750
Iteration 24/25 | Loss: 0.00084750
Iteration 25/25 | Loss: 0.00084750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084750
Iteration 2/1000 | Loss: 0.00003534
Iteration 3/1000 | Loss: 0.00001987
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001504
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001462
Iteration 11/1000 | Loss: 0.00001459
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001436
Iteration 69/1000 | Loss: 0.00001436
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001436
Iteration 75/1000 | Loss: 0.00001436
Iteration 76/1000 | Loss: 0.00001436
Iteration 77/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.4364899470820092e-05, 1.4364899470820092e-05, 1.4364899470820092e-05, 1.4364899470820092e-05, 1.4364899470820092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4364899470820092e-05

Optimization complete. Final v2v error: 3.260383129119873 mm

Highest mean error: 3.6979458332061768 mm for frame 43

Lowest mean error: 2.9346492290496826 mm for frame 85

Saving results

Total time: 26.58776831626892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880427
Iteration 2/25 | Loss: 0.00121134
Iteration 3/25 | Loss: 0.00109482
Iteration 4/25 | Loss: 0.00108350
Iteration 5/25 | Loss: 0.00108007
Iteration 6/25 | Loss: 0.00107927
Iteration 7/25 | Loss: 0.00107927
Iteration 8/25 | Loss: 0.00107927
Iteration 9/25 | Loss: 0.00107927
Iteration 10/25 | Loss: 0.00107927
Iteration 11/25 | Loss: 0.00107927
Iteration 12/25 | Loss: 0.00107927
Iteration 13/25 | Loss: 0.00107927
Iteration 14/25 | Loss: 0.00107927
Iteration 15/25 | Loss: 0.00107927
Iteration 16/25 | Loss: 0.00107927
Iteration 17/25 | Loss: 0.00107927
Iteration 18/25 | Loss: 0.00107927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010792678222060204, 0.0010792678222060204, 0.0010792678222060204, 0.0010792678222060204, 0.0010792678222060204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010792678222060204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32998466
Iteration 2/25 | Loss: 0.00073484
Iteration 3/25 | Loss: 0.00073482
Iteration 4/25 | Loss: 0.00073482
Iteration 5/25 | Loss: 0.00073482
Iteration 6/25 | Loss: 0.00073482
Iteration 7/25 | Loss: 0.00073482
Iteration 8/25 | Loss: 0.00073482
Iteration 9/25 | Loss: 0.00073481
Iteration 10/25 | Loss: 0.00073481
Iteration 11/25 | Loss: 0.00073481
Iteration 12/25 | Loss: 0.00073481
Iteration 13/25 | Loss: 0.00073481
Iteration 14/25 | Loss: 0.00073481
Iteration 15/25 | Loss: 0.00073481
Iteration 16/25 | Loss: 0.00073481
Iteration 17/25 | Loss: 0.00073481
Iteration 18/25 | Loss: 0.00073481
Iteration 19/25 | Loss: 0.00073481
Iteration 20/25 | Loss: 0.00073481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000734814559109509, 0.000734814559109509, 0.000734814559109509, 0.000734814559109509, 0.000734814559109509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000734814559109509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073481
Iteration 2/1000 | Loss: 0.00003426
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001411
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001249
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001175
Iteration 10/1000 | Loss: 0.00001152
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001138
Iteration 14/1000 | Loss: 0.00001123
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001122
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001120
Iteration 19/1000 | Loss: 0.00001120
Iteration 20/1000 | Loss: 0.00001119
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001113
Iteration 23/1000 | Loss: 0.00001108
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001108
Iteration 28/1000 | Loss: 0.00001107
Iteration 29/1000 | Loss: 0.00001107
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001106
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001105
Iteration 34/1000 | Loss: 0.00001105
Iteration 35/1000 | Loss: 0.00001104
Iteration 36/1000 | Loss: 0.00001104
Iteration 37/1000 | Loss: 0.00001104
Iteration 38/1000 | Loss: 0.00001103
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001102
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001100
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001100
Iteration 61/1000 | Loss: 0.00001100
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001099
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001098
Iteration 69/1000 | Loss: 0.00001098
Iteration 70/1000 | Loss: 0.00001098
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001098
Iteration 74/1000 | Loss: 0.00001097
Iteration 75/1000 | Loss: 0.00001097
Iteration 76/1000 | Loss: 0.00001097
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001095
Iteration 102/1000 | Loss: 0.00001095
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001095
Iteration 108/1000 | Loss: 0.00001095
Iteration 109/1000 | Loss: 0.00001095
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001095
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001094
Iteration 116/1000 | Loss: 0.00001094
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001094
Iteration 119/1000 | Loss: 0.00001094
Iteration 120/1000 | Loss: 0.00001094
Iteration 121/1000 | Loss: 0.00001094
Iteration 122/1000 | Loss: 0.00001094
Iteration 123/1000 | Loss: 0.00001094
Iteration 124/1000 | Loss: 0.00001094
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001093
Iteration 131/1000 | Loss: 0.00001093
Iteration 132/1000 | Loss: 0.00001093
Iteration 133/1000 | Loss: 0.00001093
Iteration 134/1000 | Loss: 0.00001093
Iteration 135/1000 | Loss: 0.00001093
Iteration 136/1000 | Loss: 0.00001093
Iteration 137/1000 | Loss: 0.00001093
Iteration 138/1000 | Loss: 0.00001093
Iteration 139/1000 | Loss: 0.00001093
Iteration 140/1000 | Loss: 0.00001093
Iteration 141/1000 | Loss: 0.00001093
Iteration 142/1000 | Loss: 0.00001093
Iteration 143/1000 | Loss: 0.00001093
Iteration 144/1000 | Loss: 0.00001093
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001093
Iteration 154/1000 | Loss: 0.00001093
Iteration 155/1000 | Loss: 0.00001093
Iteration 156/1000 | Loss: 0.00001093
Iteration 157/1000 | Loss: 0.00001093
Iteration 158/1000 | Loss: 0.00001093
Iteration 159/1000 | Loss: 0.00001093
Iteration 160/1000 | Loss: 0.00001093
Iteration 161/1000 | Loss: 0.00001093
Iteration 162/1000 | Loss: 0.00001093
Iteration 163/1000 | Loss: 0.00001093
Iteration 164/1000 | Loss: 0.00001093
Iteration 165/1000 | Loss: 0.00001093
Iteration 166/1000 | Loss: 0.00001093
Iteration 167/1000 | Loss: 0.00001093
Iteration 168/1000 | Loss: 0.00001093
Iteration 169/1000 | Loss: 0.00001093
Iteration 170/1000 | Loss: 0.00001093
Iteration 171/1000 | Loss: 0.00001093
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001093
Iteration 174/1000 | Loss: 0.00001093
Iteration 175/1000 | Loss: 0.00001093
Iteration 176/1000 | Loss: 0.00001093
Iteration 177/1000 | Loss: 0.00001093
Iteration 178/1000 | Loss: 0.00001093
Iteration 179/1000 | Loss: 0.00001093
Iteration 180/1000 | Loss: 0.00001093
Iteration 181/1000 | Loss: 0.00001093
Iteration 182/1000 | Loss: 0.00001093
Iteration 183/1000 | Loss: 0.00001093
Iteration 184/1000 | Loss: 0.00001093
Iteration 185/1000 | Loss: 0.00001093
Iteration 186/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.0927482435363345e-05, 1.0927482435363345e-05, 1.0927482435363345e-05, 1.0927482435363345e-05, 1.0927482435363345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0927482435363345e-05

Optimization complete. Final v2v error: 2.88698673248291 mm

Highest mean error: 3.0896592140197754 mm for frame 128

Lowest mean error: 2.6886706352233887 mm for frame 28

Saving results

Total time: 36.33107399940491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871150
Iteration 2/25 | Loss: 0.00145406
Iteration 3/25 | Loss: 0.00117138
Iteration 4/25 | Loss: 0.00114148
Iteration 5/25 | Loss: 0.00113896
Iteration 6/25 | Loss: 0.00113879
Iteration 7/25 | Loss: 0.00113803
Iteration 8/25 | Loss: 0.00113838
Iteration 9/25 | Loss: 0.00113297
Iteration 10/25 | Loss: 0.00113047
Iteration 11/25 | Loss: 0.00113076
Iteration 12/25 | Loss: 0.00113432
Iteration 13/25 | Loss: 0.00113241
Iteration 14/25 | Loss: 0.00112886
Iteration 15/25 | Loss: 0.00112513
Iteration 16/25 | Loss: 0.00112304
Iteration 17/25 | Loss: 0.00112277
Iteration 18/25 | Loss: 0.00112257
Iteration 19/25 | Loss: 0.00112399
Iteration 20/25 | Loss: 0.00112257
Iteration 21/25 | Loss: 0.00112373
Iteration 22/25 | Loss: 0.00112390
Iteration 23/25 | Loss: 0.00112323
Iteration 24/25 | Loss: 0.00112331
Iteration 25/25 | Loss: 0.00112318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44840539
Iteration 2/25 | Loss: 0.00081986
Iteration 3/25 | Loss: 0.00081986
Iteration 4/25 | Loss: 0.00081986
Iteration 5/25 | Loss: 0.00081986
Iteration 6/25 | Loss: 0.00081986
Iteration 7/25 | Loss: 0.00081986
Iteration 8/25 | Loss: 0.00081985
Iteration 9/25 | Loss: 0.00081985
Iteration 10/25 | Loss: 0.00081985
Iteration 11/25 | Loss: 0.00081985
Iteration 12/25 | Loss: 0.00081985
Iteration 13/25 | Loss: 0.00081985
Iteration 14/25 | Loss: 0.00081985
Iteration 15/25 | Loss: 0.00081985
Iteration 16/25 | Loss: 0.00081985
Iteration 17/25 | Loss: 0.00081985
Iteration 18/25 | Loss: 0.00081985
Iteration 19/25 | Loss: 0.00081985
Iteration 20/25 | Loss: 0.00081985
Iteration 21/25 | Loss: 0.00081985
Iteration 22/25 | Loss: 0.00081985
Iteration 23/25 | Loss: 0.00081985
Iteration 24/25 | Loss: 0.00081985
Iteration 25/25 | Loss: 0.00081985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081985
Iteration 2/1000 | Loss: 0.00003738
Iteration 3/1000 | Loss: 0.00003492
Iteration 4/1000 | Loss: 0.00002791
Iteration 5/1000 | Loss: 0.00003500
Iteration 6/1000 | Loss: 0.00002319
Iteration 7/1000 | Loss: 0.00002029
Iteration 8/1000 | Loss: 0.00003231
Iteration 9/1000 | Loss: 0.00002099
Iteration 10/1000 | Loss: 0.00002079
Iteration 11/1000 | Loss: 0.00002775
Iteration 12/1000 | Loss: 0.00003005
Iteration 13/1000 | Loss: 0.00003064
Iteration 14/1000 | Loss: 0.00002695
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002395
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002281
Iteration 19/1000 | Loss: 0.00002444
Iteration 20/1000 | Loss: 0.00003086
Iteration 21/1000 | Loss: 0.00003087
Iteration 22/1000 | Loss: 0.00003215
Iteration 23/1000 | Loss: 0.00003056
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00002921
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002176
Iteration 28/1000 | Loss: 0.00002857
Iteration 29/1000 | Loss: 0.00002966
Iteration 30/1000 | Loss: 0.00002890
Iteration 31/1000 | Loss: 0.00002706
Iteration 32/1000 | Loss: 0.00002284
Iteration 33/1000 | Loss: 0.00003205
Iteration 34/1000 | Loss: 0.00002978
Iteration 35/1000 | Loss: 0.00002255
Iteration 36/1000 | Loss: 0.00003255
Iteration 37/1000 | Loss: 0.00003124
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00003021
Iteration 40/1000 | Loss: 0.00002420
Iteration 41/1000 | Loss: 0.00003022
Iteration 42/1000 | Loss: 0.00002794
Iteration 43/1000 | Loss: 0.00002858
Iteration 44/1000 | Loss: 0.00002662
Iteration 45/1000 | Loss: 0.00002852
Iteration 46/1000 | Loss: 0.00002628
Iteration 47/1000 | Loss: 0.00002389
Iteration 48/1000 | Loss: 0.00002897
Iteration 49/1000 | Loss: 0.00003157
Iteration 50/1000 | Loss: 0.00002867
Iteration 51/1000 | Loss: 0.00004584
Iteration 52/1000 | Loss: 0.00003000
Iteration 53/1000 | Loss: 0.00003920
Iteration 54/1000 | Loss: 0.00002935
Iteration 55/1000 | Loss: 0.00002605
Iteration 56/1000 | Loss: 0.00002764
Iteration 57/1000 | Loss: 0.00002867
Iteration 58/1000 | Loss: 0.00002711
Iteration 59/1000 | Loss: 0.00002788
Iteration 60/1000 | Loss: 0.00003451
Iteration 61/1000 | Loss: 0.00002698
Iteration 62/1000 | Loss: 0.00002669
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00002098
Iteration 66/1000 | Loss: 0.00003647
Iteration 67/1000 | Loss: 0.00002125
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001603
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001486
Iteration 96/1000 | Loss: 0.00001486
Iteration 97/1000 | Loss: 0.00001486
Iteration 98/1000 | Loss: 0.00001485
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001476
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001473
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001469
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001469
Iteration 122/1000 | Loss: 0.00001469
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001468
Iteration 125/1000 | Loss: 0.00001468
Iteration 126/1000 | Loss: 0.00001468
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001461
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001460
Iteration 177/1000 | Loss: 0.00001460
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Iteration 180/1000 | Loss: 0.00001459
Iteration 181/1000 | Loss: 0.00001459
Iteration 182/1000 | Loss: 0.00001459
Iteration 183/1000 | Loss: 0.00001459
Iteration 184/1000 | Loss: 0.00001459
Iteration 185/1000 | Loss: 0.00001459
Iteration 186/1000 | Loss: 0.00001459
Iteration 187/1000 | Loss: 0.00001459
Iteration 188/1000 | Loss: 0.00001459
Iteration 189/1000 | Loss: 0.00001459
Iteration 190/1000 | Loss: 0.00001459
Iteration 191/1000 | Loss: 0.00001459
Iteration 192/1000 | Loss: 0.00001459
Iteration 193/1000 | Loss: 0.00001459
Iteration 194/1000 | Loss: 0.00001459
Iteration 195/1000 | Loss: 0.00001459
Iteration 196/1000 | Loss: 0.00001459
Iteration 197/1000 | Loss: 0.00001459
Iteration 198/1000 | Loss: 0.00001458
Iteration 199/1000 | Loss: 0.00001458
Iteration 200/1000 | Loss: 0.00001458
Iteration 201/1000 | Loss: 0.00001458
Iteration 202/1000 | Loss: 0.00001458
Iteration 203/1000 | Loss: 0.00001458
Iteration 204/1000 | Loss: 0.00001458
Iteration 205/1000 | Loss: 0.00001458
Iteration 206/1000 | Loss: 0.00001458
Iteration 207/1000 | Loss: 0.00001458
Iteration 208/1000 | Loss: 0.00001458
Iteration 209/1000 | Loss: 0.00001458
Iteration 210/1000 | Loss: 0.00001458
Iteration 211/1000 | Loss: 0.00001458
Iteration 212/1000 | Loss: 0.00001458
Iteration 213/1000 | Loss: 0.00001458
Iteration 214/1000 | Loss: 0.00001458
Iteration 215/1000 | Loss: 0.00001458
Iteration 216/1000 | Loss: 0.00001458
Iteration 217/1000 | Loss: 0.00001458
Iteration 218/1000 | Loss: 0.00001458
Iteration 219/1000 | Loss: 0.00001458
Iteration 220/1000 | Loss: 0.00001458
Iteration 221/1000 | Loss: 0.00001458
Iteration 222/1000 | Loss: 0.00001458
Iteration 223/1000 | Loss: 0.00001458
Iteration 224/1000 | Loss: 0.00001458
Iteration 225/1000 | Loss: 0.00001458
Iteration 226/1000 | Loss: 0.00001458
Iteration 227/1000 | Loss: 0.00001458
Iteration 228/1000 | Loss: 0.00001458
Iteration 229/1000 | Loss: 0.00001458
Iteration 230/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.4582486073777545e-05, 1.4582486073777545e-05, 1.4582486073777545e-05, 1.4582486073777545e-05, 1.4582486073777545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4582486073777545e-05

Optimization complete. Final v2v error: 3.2472753524780273 mm

Highest mean error: 4.585002899169922 mm for frame 109

Lowest mean error: 2.728043794631958 mm for frame 76

Saving results

Total time: 176.02542161941528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995678
Iteration 2/25 | Loss: 0.00147819
Iteration 3/25 | Loss: 0.00137605
Iteration 4/25 | Loss: 0.00118365
Iteration 5/25 | Loss: 0.00119594
Iteration 6/25 | Loss: 0.00116277
Iteration 7/25 | Loss: 0.00116534
Iteration 8/25 | Loss: 0.00115809
Iteration 9/25 | Loss: 0.00115788
Iteration 10/25 | Loss: 0.00115786
Iteration 11/25 | Loss: 0.00115786
Iteration 12/25 | Loss: 0.00115786
Iteration 13/25 | Loss: 0.00115786
Iteration 14/25 | Loss: 0.00115786
Iteration 15/25 | Loss: 0.00115786
Iteration 16/25 | Loss: 0.00115786
Iteration 17/25 | Loss: 0.00115786
Iteration 18/25 | Loss: 0.00115786
Iteration 19/25 | Loss: 0.00115786
Iteration 20/25 | Loss: 0.00115785
Iteration 21/25 | Loss: 0.00115785
Iteration 22/25 | Loss: 0.00115785
Iteration 23/25 | Loss: 0.00115785
Iteration 24/25 | Loss: 0.00115785
Iteration 25/25 | Loss: 0.00115785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45235050
Iteration 2/25 | Loss: 0.00062885
Iteration 3/25 | Loss: 0.00062883
Iteration 4/25 | Loss: 0.00062883
Iteration 5/25 | Loss: 0.00062883
Iteration 6/25 | Loss: 0.00062883
Iteration 7/25 | Loss: 0.00062883
Iteration 8/25 | Loss: 0.00062882
Iteration 9/25 | Loss: 0.00062882
Iteration 10/25 | Loss: 0.00062882
Iteration 11/25 | Loss: 0.00062882
Iteration 12/25 | Loss: 0.00062882
Iteration 13/25 | Loss: 0.00062882
Iteration 14/25 | Loss: 0.00062882
Iteration 15/25 | Loss: 0.00062882
Iteration 16/25 | Loss: 0.00062882
Iteration 17/25 | Loss: 0.00062882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006288241711445153, 0.0006288241711445153, 0.0006288241711445153, 0.0006288241711445153, 0.0006288241711445153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006288241711445153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062882
Iteration 2/1000 | Loss: 0.00004749
Iteration 3/1000 | Loss: 0.00003036
Iteration 4/1000 | Loss: 0.00009859
Iteration 5/1000 | Loss: 0.00002158
Iteration 6/1000 | Loss: 0.00010825
Iteration 7/1000 | Loss: 0.00001989
Iteration 8/1000 | Loss: 0.00001944
Iteration 9/1000 | Loss: 0.00001913
Iteration 10/1000 | Loss: 0.00016438
Iteration 11/1000 | Loss: 0.00040238
Iteration 12/1000 | Loss: 0.00001884
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001870
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001861
Iteration 19/1000 | Loss: 0.00013708
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001850
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001845
Iteration 25/1000 | Loss: 0.00001844
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001839
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001835
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001832
Iteration 36/1000 | Loss: 0.00001832
Iteration 37/1000 | Loss: 0.00001831
Iteration 38/1000 | Loss: 0.00001831
Iteration 39/1000 | Loss: 0.00001830
Iteration 40/1000 | Loss: 0.00001830
Iteration 41/1000 | Loss: 0.00001830
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001829
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001828
Iteration 47/1000 | Loss: 0.00001828
Iteration 48/1000 | Loss: 0.00001827
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001827
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001825
Iteration 57/1000 | Loss: 0.00001825
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001824
Iteration 60/1000 | Loss: 0.00001824
Iteration 61/1000 | Loss: 0.00001824
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001824
Iteration 65/1000 | Loss: 0.00001823
Iteration 66/1000 | Loss: 0.00001823
Iteration 67/1000 | Loss: 0.00001823
Iteration 68/1000 | Loss: 0.00001823
Iteration 69/1000 | Loss: 0.00001822
Iteration 70/1000 | Loss: 0.00001822
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001820
Iteration 76/1000 | Loss: 0.00001820
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001819
Iteration 79/1000 | Loss: 0.00001819
Iteration 80/1000 | Loss: 0.00001819
Iteration 81/1000 | Loss: 0.00001819
Iteration 82/1000 | Loss: 0.00001819
Iteration 83/1000 | Loss: 0.00001819
Iteration 84/1000 | Loss: 0.00001819
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001818
Iteration 87/1000 | Loss: 0.00001818
Iteration 88/1000 | Loss: 0.00001818
Iteration 89/1000 | Loss: 0.00001818
Iteration 90/1000 | Loss: 0.00001818
Iteration 91/1000 | Loss: 0.00001818
Iteration 92/1000 | Loss: 0.00001818
Iteration 93/1000 | Loss: 0.00001818
Iteration 94/1000 | Loss: 0.00001818
Iteration 95/1000 | Loss: 0.00001818
Iteration 96/1000 | Loss: 0.00001818
Iteration 97/1000 | Loss: 0.00001817
Iteration 98/1000 | Loss: 0.00001817
Iteration 99/1000 | Loss: 0.00001817
Iteration 100/1000 | Loss: 0.00001817
Iteration 101/1000 | Loss: 0.00001817
Iteration 102/1000 | Loss: 0.00001817
Iteration 103/1000 | Loss: 0.00001817
Iteration 104/1000 | Loss: 0.00001817
Iteration 105/1000 | Loss: 0.00001817
Iteration 106/1000 | Loss: 0.00001817
Iteration 107/1000 | Loss: 0.00001817
Iteration 108/1000 | Loss: 0.00001816
Iteration 109/1000 | Loss: 0.00001816
Iteration 110/1000 | Loss: 0.00001816
Iteration 111/1000 | Loss: 0.00001816
Iteration 112/1000 | Loss: 0.00001816
Iteration 113/1000 | Loss: 0.00001816
Iteration 114/1000 | Loss: 0.00001816
Iteration 115/1000 | Loss: 0.00001816
Iteration 116/1000 | Loss: 0.00001816
Iteration 117/1000 | Loss: 0.00001815
Iteration 118/1000 | Loss: 0.00001815
Iteration 119/1000 | Loss: 0.00001815
Iteration 120/1000 | Loss: 0.00001815
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001815
Iteration 127/1000 | Loss: 0.00001815
Iteration 128/1000 | Loss: 0.00001815
Iteration 129/1000 | Loss: 0.00001815
Iteration 130/1000 | Loss: 0.00001815
Iteration 131/1000 | Loss: 0.00001815
Iteration 132/1000 | Loss: 0.00001815
Iteration 133/1000 | Loss: 0.00001815
Iteration 134/1000 | Loss: 0.00001815
Iteration 135/1000 | Loss: 0.00001815
Iteration 136/1000 | Loss: 0.00001815
Iteration 137/1000 | Loss: 0.00001815
Iteration 138/1000 | Loss: 0.00001815
Iteration 139/1000 | Loss: 0.00001815
Iteration 140/1000 | Loss: 0.00001815
Iteration 141/1000 | Loss: 0.00001815
Iteration 142/1000 | Loss: 0.00001815
Iteration 143/1000 | Loss: 0.00001815
Iteration 144/1000 | Loss: 0.00001815
Iteration 145/1000 | Loss: 0.00001815
Iteration 146/1000 | Loss: 0.00001815
Iteration 147/1000 | Loss: 0.00001815
Iteration 148/1000 | Loss: 0.00001815
Iteration 149/1000 | Loss: 0.00001815
Iteration 150/1000 | Loss: 0.00001815
Iteration 151/1000 | Loss: 0.00001815
Iteration 152/1000 | Loss: 0.00001815
Iteration 153/1000 | Loss: 0.00001815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8146152797271498e-05, 1.8146152797271498e-05, 1.8146152797271498e-05, 1.8146152797271498e-05, 1.8146152797271498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8146152797271498e-05

Optimization complete. Final v2v error: 3.714580774307251 mm

Highest mean error: 4.175151824951172 mm for frame 34

Lowest mean error: 3.294842004776001 mm for frame 0

Saving results

Total time: 50.31654334068298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00579323
Iteration 2/25 | Loss: 0.00135870
Iteration 3/25 | Loss: 0.00119026
Iteration 4/25 | Loss: 0.00117685
Iteration 5/25 | Loss: 0.00117154
Iteration 6/25 | Loss: 0.00117054
Iteration 7/25 | Loss: 0.00117054
Iteration 8/25 | Loss: 0.00117054
Iteration 9/25 | Loss: 0.00117054
Iteration 10/25 | Loss: 0.00117054
Iteration 11/25 | Loss: 0.00117054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011705424403771758, 0.0011705424403771758, 0.0011705424403771758, 0.0011705424403771758, 0.0011705424403771758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011705424403771758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59532636
Iteration 2/25 | Loss: 0.00072970
Iteration 3/25 | Loss: 0.00072969
Iteration 4/25 | Loss: 0.00072969
Iteration 5/25 | Loss: 0.00072969
Iteration 6/25 | Loss: 0.00072969
Iteration 7/25 | Loss: 0.00072969
Iteration 8/25 | Loss: 0.00072969
Iteration 9/25 | Loss: 0.00072969
Iteration 10/25 | Loss: 0.00072969
Iteration 11/25 | Loss: 0.00072969
Iteration 12/25 | Loss: 0.00072969
Iteration 13/25 | Loss: 0.00072969
Iteration 14/25 | Loss: 0.00072969
Iteration 15/25 | Loss: 0.00072969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007296893163584173, 0.0007296893163584173, 0.0007296893163584173, 0.0007296893163584173, 0.0007296893163584173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007296893163584173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072969
Iteration 2/1000 | Loss: 0.00004774
Iteration 3/1000 | Loss: 0.00003700
Iteration 4/1000 | Loss: 0.00003436
Iteration 5/1000 | Loss: 0.00003313
Iteration 6/1000 | Loss: 0.00003255
Iteration 7/1000 | Loss: 0.00003192
Iteration 8/1000 | Loss: 0.00003149
Iteration 9/1000 | Loss: 0.00003110
Iteration 10/1000 | Loss: 0.00003082
Iteration 11/1000 | Loss: 0.00003043
Iteration 12/1000 | Loss: 0.00003005
Iteration 13/1000 | Loss: 0.00002974
Iteration 14/1000 | Loss: 0.00002951
Iteration 15/1000 | Loss: 0.00002930
Iteration 16/1000 | Loss: 0.00002903
Iteration 17/1000 | Loss: 0.00002876
Iteration 18/1000 | Loss: 0.00002864
Iteration 19/1000 | Loss: 0.00002862
Iteration 20/1000 | Loss: 0.00002849
Iteration 21/1000 | Loss: 0.00002834
Iteration 22/1000 | Loss: 0.00002823
Iteration 23/1000 | Loss: 0.00002823
Iteration 24/1000 | Loss: 0.00002823
Iteration 25/1000 | Loss: 0.00002823
Iteration 26/1000 | Loss: 0.00002823
Iteration 27/1000 | Loss: 0.00002823
Iteration 28/1000 | Loss: 0.00002823
Iteration 29/1000 | Loss: 0.00002823
Iteration 30/1000 | Loss: 0.00002823
Iteration 31/1000 | Loss: 0.00002823
Iteration 32/1000 | Loss: 0.00002823
Iteration 33/1000 | Loss: 0.00002822
Iteration 34/1000 | Loss: 0.00002822
Iteration 35/1000 | Loss: 0.00002822
Iteration 36/1000 | Loss: 0.00002822
Iteration 37/1000 | Loss: 0.00002819
Iteration 38/1000 | Loss: 0.00002818
Iteration 39/1000 | Loss: 0.00002818
Iteration 40/1000 | Loss: 0.00002818
Iteration 41/1000 | Loss: 0.00002818
Iteration 42/1000 | Loss: 0.00002818
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002818
Iteration 46/1000 | Loss: 0.00002818
Iteration 47/1000 | Loss: 0.00002818
Iteration 48/1000 | Loss: 0.00002817
Iteration 49/1000 | Loss: 0.00002814
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00002812
Iteration 52/1000 | Loss: 0.00002810
Iteration 53/1000 | Loss: 0.00002810
Iteration 54/1000 | Loss: 0.00002810
Iteration 55/1000 | Loss: 0.00002810
Iteration 56/1000 | Loss: 0.00002810
Iteration 57/1000 | Loss: 0.00002809
Iteration 58/1000 | Loss: 0.00002809
Iteration 59/1000 | Loss: 0.00002809
Iteration 60/1000 | Loss: 0.00002809
Iteration 61/1000 | Loss: 0.00002809
Iteration 62/1000 | Loss: 0.00002809
Iteration 63/1000 | Loss: 0.00002808
Iteration 64/1000 | Loss: 0.00002808
Iteration 65/1000 | Loss: 0.00002807
Iteration 66/1000 | Loss: 0.00002807
Iteration 67/1000 | Loss: 0.00002807
Iteration 68/1000 | Loss: 0.00002807
Iteration 69/1000 | Loss: 0.00002806
Iteration 70/1000 | Loss: 0.00002806
Iteration 71/1000 | Loss: 0.00002806
Iteration 72/1000 | Loss: 0.00002806
Iteration 73/1000 | Loss: 0.00002806
Iteration 74/1000 | Loss: 0.00002805
Iteration 75/1000 | Loss: 0.00002805
Iteration 76/1000 | Loss: 0.00002805
Iteration 77/1000 | Loss: 0.00002805
Iteration 78/1000 | Loss: 0.00002805
Iteration 79/1000 | Loss: 0.00002805
Iteration 80/1000 | Loss: 0.00002805
Iteration 81/1000 | Loss: 0.00002805
Iteration 82/1000 | Loss: 0.00002804
Iteration 83/1000 | Loss: 0.00002804
Iteration 84/1000 | Loss: 0.00002804
Iteration 85/1000 | Loss: 0.00002804
Iteration 86/1000 | Loss: 0.00002803
Iteration 87/1000 | Loss: 0.00002803
Iteration 88/1000 | Loss: 0.00002802
Iteration 89/1000 | Loss: 0.00002802
Iteration 90/1000 | Loss: 0.00002802
Iteration 91/1000 | Loss: 0.00002801
Iteration 92/1000 | Loss: 0.00002801
Iteration 93/1000 | Loss: 0.00002801
Iteration 94/1000 | Loss: 0.00002801
Iteration 95/1000 | Loss: 0.00002801
Iteration 96/1000 | Loss: 0.00002801
Iteration 97/1000 | Loss: 0.00002801
Iteration 98/1000 | Loss: 0.00002800
Iteration 99/1000 | Loss: 0.00002800
Iteration 100/1000 | Loss: 0.00002800
Iteration 101/1000 | Loss: 0.00002800
Iteration 102/1000 | Loss: 0.00002800
Iteration 103/1000 | Loss: 0.00002800
Iteration 104/1000 | Loss: 0.00002800
Iteration 105/1000 | Loss: 0.00002800
Iteration 106/1000 | Loss: 0.00002799
Iteration 107/1000 | Loss: 0.00002798
Iteration 108/1000 | Loss: 0.00002798
Iteration 109/1000 | Loss: 0.00002798
Iteration 110/1000 | Loss: 0.00002798
Iteration 111/1000 | Loss: 0.00002798
Iteration 112/1000 | Loss: 0.00002798
Iteration 113/1000 | Loss: 0.00002797
Iteration 114/1000 | Loss: 0.00002797
Iteration 115/1000 | Loss: 0.00002797
Iteration 116/1000 | Loss: 0.00002797
Iteration 117/1000 | Loss: 0.00002797
Iteration 118/1000 | Loss: 0.00002797
Iteration 119/1000 | Loss: 0.00002797
Iteration 120/1000 | Loss: 0.00002797
Iteration 121/1000 | Loss: 0.00002797
Iteration 122/1000 | Loss: 0.00002797
Iteration 123/1000 | Loss: 0.00002797
Iteration 124/1000 | Loss: 0.00002797
Iteration 125/1000 | Loss: 0.00002797
Iteration 126/1000 | Loss: 0.00002797
Iteration 127/1000 | Loss: 0.00002797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.7971200324827805e-05, 2.7971200324827805e-05, 2.7971200324827805e-05, 2.7971200324827805e-05, 2.7971200324827805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7971200324827805e-05

Optimization complete. Final v2v error: 4.241125583648682 mm

Highest mean error: 4.420476913452148 mm for frame 54

Lowest mean error: 4.11706018447876 mm for frame 9

Saving results

Total time: 53.91066575050354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912111
Iteration 2/25 | Loss: 0.00126833
Iteration 3/25 | Loss: 0.00115727
Iteration 4/25 | Loss: 0.00114552
Iteration 5/25 | Loss: 0.00114001
Iteration 6/25 | Loss: 0.00113865
Iteration 7/25 | Loss: 0.00113865
Iteration 8/25 | Loss: 0.00113865
Iteration 9/25 | Loss: 0.00113865
Iteration 10/25 | Loss: 0.00113865
Iteration 11/25 | Loss: 0.00113865
Iteration 12/25 | Loss: 0.00113865
Iteration 13/25 | Loss: 0.00113865
Iteration 14/25 | Loss: 0.00113865
Iteration 15/25 | Loss: 0.00113865
Iteration 16/25 | Loss: 0.00113865
Iteration 17/25 | Loss: 0.00113865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011386454571038485, 0.0011386454571038485, 0.0011386454571038485, 0.0011386454571038485, 0.0011386454571038485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011386454571038485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95904279
Iteration 2/25 | Loss: 0.00083386
Iteration 3/25 | Loss: 0.00083386
Iteration 4/25 | Loss: 0.00083386
Iteration 5/25 | Loss: 0.00083386
Iteration 6/25 | Loss: 0.00083386
Iteration 7/25 | Loss: 0.00083386
Iteration 8/25 | Loss: 0.00083386
Iteration 9/25 | Loss: 0.00083386
Iteration 10/25 | Loss: 0.00083386
Iteration 11/25 | Loss: 0.00083386
Iteration 12/25 | Loss: 0.00083386
Iteration 13/25 | Loss: 0.00083386
Iteration 14/25 | Loss: 0.00083386
Iteration 15/25 | Loss: 0.00083386
Iteration 16/25 | Loss: 0.00083386
Iteration 17/25 | Loss: 0.00083386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008338554762303829, 0.0008338554762303829, 0.0008338554762303829, 0.0008338554762303829, 0.0008338554762303829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008338554762303829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083386
Iteration 2/1000 | Loss: 0.00004145
Iteration 3/1000 | Loss: 0.00002626
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001869
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001685
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001603
Iteration 10/1000 | Loss: 0.00001590
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001584
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001571
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001569
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001557
Iteration 23/1000 | Loss: 0.00001557
Iteration 24/1000 | Loss: 0.00001556
Iteration 25/1000 | Loss: 0.00001556
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001553
Iteration 31/1000 | Loss: 0.00001553
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001551
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001549
Iteration 42/1000 | Loss: 0.00001549
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001548
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001545
Iteration 60/1000 | Loss: 0.00001545
Iteration 61/1000 | Loss: 0.00001545
Iteration 62/1000 | Loss: 0.00001545
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001544
Iteration 70/1000 | Loss: 0.00001544
Iteration 71/1000 | Loss: 0.00001544
Iteration 72/1000 | Loss: 0.00001544
Iteration 73/1000 | Loss: 0.00001543
Iteration 74/1000 | Loss: 0.00001543
Iteration 75/1000 | Loss: 0.00001543
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001541
Iteration 114/1000 | Loss: 0.00001541
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5412077118526213e-05, 1.5412077118526213e-05, 1.5412077118526213e-05, 1.5412077118526213e-05, 1.5412077118526213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5412077118526213e-05

Optimization complete. Final v2v error: 3.3854620456695557 mm

Highest mean error: 4.256381511688232 mm for frame 57

Lowest mean error: 2.858150005340576 mm for frame 100

Saving results

Total time: 32.999720096588135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820858
Iteration 2/25 | Loss: 0.00142745
Iteration 3/25 | Loss: 0.00119022
Iteration 4/25 | Loss: 0.00116435
Iteration 5/25 | Loss: 0.00115837
Iteration 6/25 | Loss: 0.00115683
Iteration 7/25 | Loss: 0.00115683
Iteration 8/25 | Loss: 0.00115683
Iteration 9/25 | Loss: 0.00115683
Iteration 10/25 | Loss: 0.00115683
Iteration 11/25 | Loss: 0.00115683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011568337213248014, 0.0011568337213248014, 0.0011568337213248014, 0.0011568337213248014, 0.0011568337213248014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011568337213248014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13405848
Iteration 2/25 | Loss: 0.00058970
Iteration 3/25 | Loss: 0.00058969
Iteration 4/25 | Loss: 0.00058969
Iteration 5/25 | Loss: 0.00058969
Iteration 6/25 | Loss: 0.00058969
Iteration 7/25 | Loss: 0.00058969
Iteration 8/25 | Loss: 0.00058969
Iteration 9/25 | Loss: 0.00058969
Iteration 10/25 | Loss: 0.00058969
Iteration 11/25 | Loss: 0.00058969
Iteration 12/25 | Loss: 0.00058969
Iteration 13/25 | Loss: 0.00058969
Iteration 14/25 | Loss: 0.00058969
Iteration 15/25 | Loss: 0.00058969
Iteration 16/25 | Loss: 0.00058969
Iteration 17/25 | Loss: 0.00058969
Iteration 18/25 | Loss: 0.00058969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005896915099583566, 0.0005896915099583566, 0.0005896915099583566, 0.0005896915099583566, 0.0005896915099583566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005896915099583566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058969
Iteration 2/1000 | Loss: 0.00003986
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00002259
Iteration 6/1000 | Loss: 0.00002166
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002071
Iteration 9/1000 | Loss: 0.00002037
Iteration 10/1000 | Loss: 0.00002014
Iteration 11/1000 | Loss: 0.00002008
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001996
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001989
Iteration 16/1000 | Loss: 0.00001975
Iteration 17/1000 | Loss: 0.00001962
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001950
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001947
Iteration 37/1000 | Loss: 0.00001947
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001944
Iteration 41/1000 | Loss: 0.00001944
Iteration 42/1000 | Loss: 0.00001944
Iteration 43/1000 | Loss: 0.00001944
Iteration 44/1000 | Loss: 0.00001944
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001944
Iteration 49/1000 | Loss: 0.00001943
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001941
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001938
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001937
Iteration 64/1000 | Loss: 0.00001937
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001936
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001935
Iteration 72/1000 | Loss: 0.00001935
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001933
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001932
Iteration 93/1000 | Loss: 0.00001932
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001930
Iteration 99/1000 | Loss: 0.00001930
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001929
Iteration 102/1000 | Loss: 0.00001929
Iteration 103/1000 | Loss: 0.00001929
Iteration 104/1000 | Loss: 0.00001929
Iteration 105/1000 | Loss: 0.00001929
Iteration 106/1000 | Loss: 0.00001929
Iteration 107/1000 | Loss: 0.00001928
Iteration 108/1000 | Loss: 0.00001928
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001928
Iteration 111/1000 | Loss: 0.00001928
Iteration 112/1000 | Loss: 0.00001928
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001928
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001928
Iteration 120/1000 | Loss: 0.00001928
Iteration 121/1000 | Loss: 0.00001928
Iteration 122/1000 | Loss: 0.00001928
Iteration 123/1000 | Loss: 0.00001928
Iteration 124/1000 | Loss: 0.00001928
Iteration 125/1000 | Loss: 0.00001928
Iteration 126/1000 | Loss: 0.00001928
Iteration 127/1000 | Loss: 0.00001928
Iteration 128/1000 | Loss: 0.00001928
Iteration 129/1000 | Loss: 0.00001928
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.9275992599432357e-05, 1.9275992599432357e-05, 1.9275992599432357e-05, 1.9275992599432357e-05, 1.9275992599432357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9275992599432357e-05

Optimization complete. Final v2v error: 3.568105697631836 mm

Highest mean error: 4.752634048461914 mm for frame 211

Lowest mean error: 2.9585678577423096 mm for frame 125

Saving results

Total time: 42.923442363739014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068757
Iteration 2/25 | Loss: 0.00245380
Iteration 3/25 | Loss: 0.00181951
Iteration 4/25 | Loss: 0.00151141
Iteration 5/25 | Loss: 0.00153041
Iteration 6/25 | Loss: 0.00139697
Iteration 7/25 | Loss: 0.00135766
Iteration 8/25 | Loss: 0.00137594
Iteration 9/25 | Loss: 0.00128285
Iteration 10/25 | Loss: 0.00123206
Iteration 11/25 | Loss: 0.00120389
Iteration 12/25 | Loss: 0.00118409
Iteration 13/25 | Loss: 0.00117027
Iteration 14/25 | Loss: 0.00116482
Iteration 15/25 | Loss: 0.00116199
Iteration 16/25 | Loss: 0.00115012
Iteration 17/25 | Loss: 0.00114028
Iteration 18/25 | Loss: 0.00113992
Iteration 19/25 | Loss: 0.00113639
Iteration 20/25 | Loss: 0.00113685
Iteration 21/25 | Loss: 0.00113500
Iteration 22/25 | Loss: 0.00113352
Iteration 23/25 | Loss: 0.00113103
Iteration 24/25 | Loss: 0.00112941
Iteration 25/25 | Loss: 0.00112850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30273354
Iteration 2/25 | Loss: 0.00161288
Iteration 3/25 | Loss: 0.00161288
Iteration 4/25 | Loss: 0.00161288
Iteration 5/25 | Loss: 0.00161288
Iteration 6/25 | Loss: 0.00161288
Iteration 7/25 | Loss: 0.00161288
Iteration 8/25 | Loss: 0.00161288
Iteration 9/25 | Loss: 0.00161288
Iteration 10/25 | Loss: 0.00161288
Iteration 11/25 | Loss: 0.00161288
Iteration 12/25 | Loss: 0.00161288
Iteration 13/25 | Loss: 0.00161288
Iteration 14/25 | Loss: 0.00161288
Iteration 15/25 | Loss: 0.00161288
Iteration 16/25 | Loss: 0.00161288
Iteration 17/25 | Loss: 0.00161288
Iteration 18/25 | Loss: 0.00161288
Iteration 19/25 | Loss: 0.00161288
Iteration 20/25 | Loss: 0.00161288
Iteration 21/25 | Loss: 0.00161288
Iteration 22/25 | Loss: 0.00161288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001612876309081912, 0.001612876309081912, 0.001612876309081912, 0.001612876309081912, 0.001612876309081912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001612876309081912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161288
Iteration 2/1000 | Loss: 0.00035939
Iteration 3/1000 | Loss: 0.00015150
Iteration 4/1000 | Loss: 0.00012242
Iteration 5/1000 | Loss: 0.00010460
Iteration 6/1000 | Loss: 0.00009812
Iteration 7/1000 | Loss: 0.00009175
Iteration 8/1000 | Loss: 0.00008717
Iteration 9/1000 | Loss: 0.00008438
Iteration 10/1000 | Loss: 0.00008249
Iteration 11/1000 | Loss: 0.00008059
Iteration 12/1000 | Loss: 0.00148989
Iteration 13/1000 | Loss: 0.00465830
Iteration 14/1000 | Loss: 0.00164989
Iteration 15/1000 | Loss: 0.00019404
Iteration 16/1000 | Loss: 0.00026815
Iteration 17/1000 | Loss: 0.00016046
Iteration 18/1000 | Loss: 0.00008243
Iteration 19/1000 | Loss: 0.00006172
Iteration 20/1000 | Loss: 0.00004760
Iteration 21/1000 | Loss: 0.00032644
Iteration 22/1000 | Loss: 0.00037647
Iteration 23/1000 | Loss: 0.00021472
Iteration 24/1000 | Loss: 0.00147477
Iteration 25/1000 | Loss: 0.00349922
Iteration 26/1000 | Loss: 0.00003477
Iteration 27/1000 | Loss: 0.00069376
Iteration 28/1000 | Loss: 0.00140605
Iteration 29/1000 | Loss: 0.00003207
Iteration 30/1000 | Loss: 0.00002643
Iteration 31/1000 | Loss: 0.00002470
Iteration 32/1000 | Loss: 0.00002297
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00001938
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001835
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001828
Iteration 46/1000 | Loss: 0.00001828
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001826
Iteration 49/1000 | Loss: 0.00001824
Iteration 50/1000 | Loss: 0.00001821
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001816
Iteration 58/1000 | Loss: 0.00001815
Iteration 59/1000 | Loss: 0.00001814
Iteration 60/1000 | Loss: 0.00001814
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001814
Iteration 63/1000 | Loss: 0.00001814
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001809
Iteration 74/1000 | Loss: 0.00001809
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001808
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001805
Iteration 89/1000 | Loss: 0.00001805
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001805
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001804
Iteration 102/1000 | Loss: 0.00001804
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001803
Iteration 114/1000 | Loss: 0.00001803
Iteration 115/1000 | Loss: 0.00001803
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Iteration 136/1000 | Loss: 0.00001801
Iteration 137/1000 | Loss: 0.00001801
Iteration 138/1000 | Loss: 0.00001801
Iteration 139/1000 | Loss: 0.00001801
Iteration 140/1000 | Loss: 0.00001801
Iteration 141/1000 | Loss: 0.00001801
Iteration 142/1000 | Loss: 0.00001801
Iteration 143/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8011065549217165e-05, 1.8011065549217165e-05, 1.8011065549217165e-05, 1.8011065549217165e-05, 1.8011065549217165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8011065549217165e-05

Optimization complete. Final v2v error: 2.8632194995880127 mm

Highest mean error: 21.440488815307617 mm for frame 57

Lowest mean error: 2.370518684387207 mm for frame 12

Saving results

Total time: 110.12897181510925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829706
Iteration 2/25 | Loss: 0.00120119
Iteration 3/25 | Loss: 0.00110823
Iteration 4/25 | Loss: 0.00110004
Iteration 5/25 | Loss: 0.00109691
Iteration 6/25 | Loss: 0.00109597
Iteration 7/25 | Loss: 0.00109592
Iteration 8/25 | Loss: 0.00109592
Iteration 9/25 | Loss: 0.00109592
Iteration 10/25 | Loss: 0.00109592
Iteration 11/25 | Loss: 0.00109592
Iteration 12/25 | Loss: 0.00109592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010959237115457654, 0.0010959237115457654, 0.0010959237115457654, 0.0010959237115457654, 0.0010959237115457654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010959237115457654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31155717
Iteration 2/25 | Loss: 0.00079203
Iteration 3/25 | Loss: 0.00079203
Iteration 4/25 | Loss: 0.00079203
Iteration 5/25 | Loss: 0.00079203
Iteration 6/25 | Loss: 0.00079203
Iteration 7/25 | Loss: 0.00079203
Iteration 8/25 | Loss: 0.00079203
Iteration 9/25 | Loss: 0.00079203
Iteration 10/25 | Loss: 0.00079203
Iteration 11/25 | Loss: 0.00079203
Iteration 12/25 | Loss: 0.00079203
Iteration 13/25 | Loss: 0.00079203
Iteration 14/25 | Loss: 0.00079203
Iteration 15/25 | Loss: 0.00079203
Iteration 16/25 | Loss: 0.00079203
Iteration 17/25 | Loss: 0.00079203
Iteration 18/25 | Loss: 0.00079203
Iteration 19/25 | Loss: 0.00079203
Iteration 20/25 | Loss: 0.00079203
Iteration 21/25 | Loss: 0.00079203
Iteration 22/25 | Loss: 0.00079203
Iteration 23/25 | Loss: 0.00079203
Iteration 24/25 | Loss: 0.00079203
Iteration 25/25 | Loss: 0.00079203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079203
Iteration 2/1000 | Loss: 0.00004194
Iteration 3/1000 | Loss: 0.00002150
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001388
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001305
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001290
Iteration 16/1000 | Loss: 0.00001290
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001274
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001272
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001272
Iteration 30/1000 | Loss: 0.00001272
Iteration 31/1000 | Loss: 0.00001272
Iteration 32/1000 | Loss: 0.00001272
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001271
Iteration 36/1000 | Loss: 0.00001271
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001270
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001269
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001264
Iteration 59/1000 | Loss: 0.00001264
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001264
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001260
Iteration 87/1000 | Loss: 0.00001260
Iteration 88/1000 | Loss: 0.00001260
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001259
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001258
Iteration 100/1000 | Loss: 0.00001258
Iteration 101/1000 | Loss: 0.00001258
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001257
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001257
Iteration 131/1000 | Loss: 0.00001257
Iteration 132/1000 | Loss: 0.00001257
Iteration 133/1000 | Loss: 0.00001257
Iteration 134/1000 | Loss: 0.00001257
Iteration 135/1000 | Loss: 0.00001257
Iteration 136/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.2571432307595387e-05, 1.2571432307595387e-05, 1.2571432307595387e-05, 1.2571432307595387e-05, 1.2571432307595387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2571432307595387e-05

Optimization complete. Final v2v error: 3.048410177230835 mm

Highest mean error: 3.605949640274048 mm for frame 103

Lowest mean error: 2.629141092300415 mm for frame 11

Saving results

Total time: 35.38079285621643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746957
Iteration 2/25 | Loss: 0.00145817
Iteration 3/25 | Loss: 0.00119752
Iteration 4/25 | Loss: 0.00117283
Iteration 5/25 | Loss: 0.00117119
Iteration 6/25 | Loss: 0.00117119
Iteration 7/25 | Loss: 0.00117119
Iteration 8/25 | Loss: 0.00117119
Iteration 9/25 | Loss: 0.00117119
Iteration 10/25 | Loss: 0.00117119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011711905244737864, 0.0011711905244737864, 0.0011711905244737864, 0.0011711905244737864, 0.0011711905244737864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011711905244737864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28111756
Iteration 2/25 | Loss: 0.00062120
Iteration 3/25 | Loss: 0.00062119
Iteration 4/25 | Loss: 0.00062119
Iteration 5/25 | Loss: 0.00062119
Iteration 6/25 | Loss: 0.00062119
Iteration 7/25 | Loss: 0.00062119
Iteration 8/25 | Loss: 0.00062119
Iteration 9/25 | Loss: 0.00062119
Iteration 10/25 | Loss: 0.00062119
Iteration 11/25 | Loss: 0.00062119
Iteration 12/25 | Loss: 0.00062119
Iteration 13/25 | Loss: 0.00062119
Iteration 14/25 | Loss: 0.00062119
Iteration 15/25 | Loss: 0.00062119
Iteration 16/25 | Loss: 0.00062119
Iteration 17/25 | Loss: 0.00062119
Iteration 18/25 | Loss: 0.00062119
Iteration 19/25 | Loss: 0.00062119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006211863947100937, 0.0006211863947100937, 0.0006211863947100937, 0.0006211863947100937, 0.0006211863947100937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006211863947100937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062119
Iteration 2/1000 | Loss: 0.00003127
Iteration 3/1000 | Loss: 0.00002264
Iteration 4/1000 | Loss: 0.00001925
Iteration 5/1000 | Loss: 0.00001805
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001718
Iteration 10/1000 | Loss: 0.00001713
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001710
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001709
Iteration 16/1000 | Loss: 0.00001706
Iteration 17/1000 | Loss: 0.00001706
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001705
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001696
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001679
Iteration 29/1000 | Loss: 0.00001679
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001672
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00001669
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001669
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001667
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001667
Iteration 76/1000 | Loss: 0.00001667
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001666
Iteration 81/1000 | Loss: 0.00001666
Iteration 82/1000 | Loss: 0.00001666
Iteration 83/1000 | Loss: 0.00001666
Iteration 84/1000 | Loss: 0.00001666
Iteration 85/1000 | Loss: 0.00001666
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001666
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001662
Iteration 117/1000 | Loss: 0.00001662
Iteration 118/1000 | Loss: 0.00001662
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001661
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001659
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001658
Iteration 137/1000 | Loss: 0.00001658
Iteration 138/1000 | Loss: 0.00001658
Iteration 139/1000 | Loss: 0.00001658
Iteration 140/1000 | Loss: 0.00001658
Iteration 141/1000 | Loss: 0.00001658
Iteration 142/1000 | Loss: 0.00001658
Iteration 143/1000 | Loss: 0.00001658
Iteration 144/1000 | Loss: 0.00001658
Iteration 145/1000 | Loss: 0.00001658
Iteration 146/1000 | Loss: 0.00001657
Iteration 147/1000 | Loss: 0.00001657
Iteration 148/1000 | Loss: 0.00001657
Iteration 149/1000 | Loss: 0.00001657
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001656
Iteration 154/1000 | Loss: 0.00001656
Iteration 155/1000 | Loss: 0.00001656
Iteration 156/1000 | Loss: 0.00001656
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001654
Iteration 166/1000 | Loss: 0.00001654
Iteration 167/1000 | Loss: 0.00001654
Iteration 168/1000 | Loss: 0.00001654
Iteration 169/1000 | Loss: 0.00001654
Iteration 170/1000 | Loss: 0.00001654
Iteration 171/1000 | Loss: 0.00001654
Iteration 172/1000 | Loss: 0.00001654
Iteration 173/1000 | Loss: 0.00001654
Iteration 174/1000 | Loss: 0.00001653
Iteration 175/1000 | Loss: 0.00001653
Iteration 176/1000 | Loss: 0.00001653
Iteration 177/1000 | Loss: 0.00001653
Iteration 178/1000 | Loss: 0.00001652
Iteration 179/1000 | Loss: 0.00001652
Iteration 180/1000 | Loss: 0.00001652
Iteration 181/1000 | Loss: 0.00001652
Iteration 182/1000 | Loss: 0.00001651
Iteration 183/1000 | Loss: 0.00001651
Iteration 184/1000 | Loss: 0.00001651
Iteration 185/1000 | Loss: 0.00001651
Iteration 186/1000 | Loss: 0.00001651
Iteration 187/1000 | Loss: 0.00001651
Iteration 188/1000 | Loss: 0.00001650
Iteration 189/1000 | Loss: 0.00001650
Iteration 190/1000 | Loss: 0.00001650
Iteration 191/1000 | Loss: 0.00001650
Iteration 192/1000 | Loss: 0.00001650
Iteration 193/1000 | Loss: 0.00001650
Iteration 194/1000 | Loss: 0.00001649
Iteration 195/1000 | Loss: 0.00001649
Iteration 196/1000 | Loss: 0.00001649
Iteration 197/1000 | Loss: 0.00001649
Iteration 198/1000 | Loss: 0.00001649
Iteration 199/1000 | Loss: 0.00001649
Iteration 200/1000 | Loss: 0.00001649
Iteration 201/1000 | Loss: 0.00001649
Iteration 202/1000 | Loss: 0.00001649
Iteration 203/1000 | Loss: 0.00001649
Iteration 204/1000 | Loss: 0.00001649
Iteration 205/1000 | Loss: 0.00001649
Iteration 206/1000 | Loss: 0.00001648
Iteration 207/1000 | Loss: 0.00001648
Iteration 208/1000 | Loss: 0.00001648
Iteration 209/1000 | Loss: 0.00001648
Iteration 210/1000 | Loss: 0.00001648
Iteration 211/1000 | Loss: 0.00001648
Iteration 212/1000 | Loss: 0.00001648
Iteration 213/1000 | Loss: 0.00001648
Iteration 214/1000 | Loss: 0.00001648
Iteration 215/1000 | Loss: 0.00001648
Iteration 216/1000 | Loss: 0.00001648
Iteration 217/1000 | Loss: 0.00001648
Iteration 218/1000 | Loss: 0.00001648
Iteration 219/1000 | Loss: 0.00001648
Iteration 220/1000 | Loss: 0.00001648
Iteration 221/1000 | Loss: 0.00001648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.6479780242661946e-05, 1.6479780242661946e-05, 1.6479780242661946e-05, 1.6479780242661946e-05, 1.6479780242661946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6479780242661946e-05

Optimization complete. Final v2v error: 3.405120611190796 mm

Highest mean error: 3.752578020095825 mm for frame 38

Lowest mean error: 3.11474347114563 mm for frame 193

Saving results

Total time: 42.0254328250885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500760
Iteration 2/25 | Loss: 0.00125553
Iteration 3/25 | Loss: 0.00114497
Iteration 4/25 | Loss: 0.00113378
Iteration 5/25 | Loss: 0.00113048
Iteration 6/25 | Loss: 0.00113048
Iteration 7/25 | Loss: 0.00113048
Iteration 8/25 | Loss: 0.00113048
Iteration 9/25 | Loss: 0.00113048
Iteration 10/25 | Loss: 0.00113048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011304827639833093, 0.0011304827639833093, 0.0011304827639833093, 0.0011304827639833093, 0.0011304827639833093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011304827639833093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82045239
Iteration 2/25 | Loss: 0.00068362
Iteration 3/25 | Loss: 0.00068362
Iteration 4/25 | Loss: 0.00068362
Iteration 5/25 | Loss: 0.00068361
Iteration 6/25 | Loss: 0.00068361
Iteration 7/25 | Loss: 0.00068361
Iteration 8/25 | Loss: 0.00068361
Iteration 9/25 | Loss: 0.00068361
Iteration 10/25 | Loss: 0.00068361
Iteration 11/25 | Loss: 0.00068361
Iteration 12/25 | Loss: 0.00068361
Iteration 13/25 | Loss: 0.00068361
Iteration 14/25 | Loss: 0.00068361
Iteration 15/25 | Loss: 0.00068361
Iteration 16/25 | Loss: 0.00068361
Iteration 17/25 | Loss: 0.00068361
Iteration 18/25 | Loss: 0.00068361
Iteration 19/25 | Loss: 0.00068361
Iteration 20/25 | Loss: 0.00068361
Iteration 21/25 | Loss: 0.00068361
Iteration 22/25 | Loss: 0.00068361
Iteration 23/25 | Loss: 0.00068361
Iteration 24/25 | Loss: 0.00068361
Iteration 25/25 | Loss: 0.00068361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068361
Iteration 2/1000 | Loss: 0.00004307
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002389
Iteration 5/1000 | Loss: 0.00002229
Iteration 6/1000 | Loss: 0.00002144
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00002026
Iteration 9/1000 | Loss: 0.00002001
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001960
Iteration 12/1000 | Loss: 0.00001942
Iteration 13/1000 | Loss: 0.00001922
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001911
Iteration 17/1000 | Loss: 0.00001911
Iteration 18/1000 | Loss: 0.00001908
Iteration 19/1000 | Loss: 0.00001900
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001886
Iteration 22/1000 | Loss: 0.00001877
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001831
Iteration 29/1000 | Loss: 0.00001830
Iteration 30/1000 | Loss: 0.00001830
Iteration 31/1000 | Loss: 0.00001830
Iteration 32/1000 | Loss: 0.00001829
Iteration 33/1000 | Loss: 0.00001825
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001820
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001817
Iteration 43/1000 | Loss: 0.00001814
Iteration 44/1000 | Loss: 0.00001814
Iteration 45/1000 | Loss: 0.00001814
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001813
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001811
Iteration 52/1000 | Loss: 0.00001811
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001810
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001808
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001807
Iteration 70/1000 | Loss: 0.00001806
Iteration 71/1000 | Loss: 0.00001806
Iteration 72/1000 | Loss: 0.00001806
Iteration 73/1000 | Loss: 0.00001806
Iteration 74/1000 | Loss: 0.00001806
Iteration 75/1000 | Loss: 0.00001805
Iteration 76/1000 | Loss: 0.00001805
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001805
Iteration 80/1000 | Loss: 0.00001805
Iteration 81/1000 | Loss: 0.00001801
Iteration 82/1000 | Loss: 0.00001801
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001797
Iteration 94/1000 | Loss: 0.00001797
Iteration 95/1000 | Loss: 0.00001797
Iteration 96/1000 | Loss: 0.00001797
Iteration 97/1000 | Loss: 0.00001797
Iteration 98/1000 | Loss: 0.00001797
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001793
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001792
Iteration 112/1000 | Loss: 0.00001792
Iteration 113/1000 | Loss: 0.00001792
Iteration 114/1000 | Loss: 0.00001791
Iteration 115/1000 | Loss: 0.00001791
Iteration 116/1000 | Loss: 0.00001791
Iteration 117/1000 | Loss: 0.00001791
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001791
Iteration 120/1000 | Loss: 0.00001791
Iteration 121/1000 | Loss: 0.00001791
Iteration 122/1000 | Loss: 0.00001791
Iteration 123/1000 | Loss: 0.00001790
Iteration 124/1000 | Loss: 0.00001790
Iteration 125/1000 | Loss: 0.00001790
Iteration 126/1000 | Loss: 0.00001790
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001790
Iteration 129/1000 | Loss: 0.00001789
Iteration 130/1000 | Loss: 0.00001789
Iteration 131/1000 | Loss: 0.00001789
Iteration 132/1000 | Loss: 0.00001789
Iteration 133/1000 | Loss: 0.00001789
Iteration 134/1000 | Loss: 0.00001789
Iteration 135/1000 | Loss: 0.00001789
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001789
Iteration 138/1000 | Loss: 0.00001789
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001789
Iteration 142/1000 | Loss: 0.00001789
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001789
Iteration 145/1000 | Loss: 0.00001789
Iteration 146/1000 | Loss: 0.00001789
Iteration 147/1000 | Loss: 0.00001789
Iteration 148/1000 | Loss: 0.00001789
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001789
Iteration 152/1000 | Loss: 0.00001789
Iteration 153/1000 | Loss: 0.00001789
Iteration 154/1000 | Loss: 0.00001789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.7887998183141463e-05, 1.7887998183141463e-05, 1.7887998183141463e-05, 1.7887998183141463e-05, 1.7887998183141463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7887998183141463e-05

Optimization complete. Final v2v error: 3.5639402866363525 mm

Highest mean error: 3.816664218902588 mm for frame 23

Lowest mean error: 3.391014814376831 mm for frame 203

Saving results

Total time: 56.136892557144165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394515
Iteration 2/25 | Loss: 0.00116049
Iteration 3/25 | Loss: 0.00106360
Iteration 4/25 | Loss: 0.00105663
Iteration 5/25 | Loss: 0.00105516
Iteration 6/25 | Loss: 0.00105500
Iteration 7/25 | Loss: 0.00105500
Iteration 8/25 | Loss: 0.00105500
Iteration 9/25 | Loss: 0.00105500
Iteration 10/25 | Loss: 0.00105500
Iteration 11/25 | Loss: 0.00105500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001055001514032483, 0.001055001514032483, 0.001055001514032483, 0.001055001514032483, 0.001055001514032483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001055001514032483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37160921
Iteration 2/25 | Loss: 0.00063722
Iteration 3/25 | Loss: 0.00063722
Iteration 4/25 | Loss: 0.00063722
Iteration 5/25 | Loss: 0.00063722
Iteration 6/25 | Loss: 0.00063722
Iteration 7/25 | Loss: 0.00063722
Iteration 8/25 | Loss: 0.00063722
Iteration 9/25 | Loss: 0.00063722
Iteration 10/25 | Loss: 0.00063722
Iteration 11/25 | Loss: 0.00063722
Iteration 12/25 | Loss: 0.00063722
Iteration 13/25 | Loss: 0.00063722
Iteration 14/25 | Loss: 0.00063722
Iteration 15/25 | Loss: 0.00063722
Iteration 16/25 | Loss: 0.00063722
Iteration 17/25 | Loss: 0.00063722
Iteration 18/25 | Loss: 0.00063722
Iteration 19/25 | Loss: 0.00063722
Iteration 20/25 | Loss: 0.00063722
Iteration 21/25 | Loss: 0.00063722
Iteration 22/25 | Loss: 0.00063722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006372160860337317, 0.0006372160860337317, 0.0006372160860337317, 0.0006372160860337317, 0.0006372160860337317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006372160860337317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063722
Iteration 2/1000 | Loss: 0.00002188
Iteration 3/1000 | Loss: 0.00001366
Iteration 4/1000 | Loss: 0.00001244
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001131
Iteration 7/1000 | Loss: 0.00001108
Iteration 8/1000 | Loss: 0.00001107
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001098
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001093
Iteration 13/1000 | Loss: 0.00001092
Iteration 14/1000 | Loss: 0.00001092
Iteration 15/1000 | Loss: 0.00001092
Iteration 16/1000 | Loss: 0.00001092
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001091
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001089
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001088
Iteration 23/1000 | Loss: 0.00001087
Iteration 24/1000 | Loss: 0.00001087
Iteration 25/1000 | Loss: 0.00001086
Iteration 26/1000 | Loss: 0.00001086
Iteration 27/1000 | Loss: 0.00001085
Iteration 28/1000 | Loss: 0.00001085
Iteration 29/1000 | Loss: 0.00001084
Iteration 30/1000 | Loss: 0.00001084
Iteration 31/1000 | Loss: 0.00001084
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001083
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001082
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001081
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001081
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001080
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001070
Iteration 92/1000 | Loss: 0.00001070
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001068
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001066
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001066
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001066
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001066
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001066
Iteration 126/1000 | Loss: 0.00001066
Iteration 127/1000 | Loss: 0.00001066
Iteration 128/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.0659874533303082e-05, 1.0659874533303082e-05, 1.0659874533303082e-05, 1.0659874533303082e-05, 1.0659874533303082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0659874533303082e-05

Optimization complete. Final v2v error: 2.776627779006958 mm

Highest mean error: 3.3109302520751953 mm for frame 79

Lowest mean error: 2.433666706085205 mm for frame 102

Saving results

Total time: 29.013875246047974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866948
Iteration 2/25 | Loss: 0.00131780
Iteration 3/25 | Loss: 0.00112939
Iteration 4/25 | Loss: 0.00113395
Iteration 5/25 | Loss: 0.00111200
Iteration 6/25 | Loss: 0.00111033
Iteration 7/25 | Loss: 0.00110978
Iteration 8/25 | Loss: 0.00110962
Iteration 9/25 | Loss: 0.00110957
Iteration 10/25 | Loss: 0.00110957
Iteration 11/25 | Loss: 0.00110956
Iteration 12/25 | Loss: 0.00110955
Iteration 13/25 | Loss: 0.00110955
Iteration 14/25 | Loss: 0.00110955
Iteration 15/25 | Loss: 0.00110954
Iteration 16/25 | Loss: 0.00110954
Iteration 17/25 | Loss: 0.00110954
Iteration 18/25 | Loss: 0.00110954
Iteration 19/25 | Loss: 0.00110954
Iteration 20/25 | Loss: 0.00110954
Iteration 21/25 | Loss: 0.00110954
Iteration 22/25 | Loss: 0.00110954
Iteration 23/25 | Loss: 0.00110954
Iteration 24/25 | Loss: 0.00110954
Iteration 25/25 | Loss: 0.00110953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41174817
Iteration 2/25 | Loss: 0.00069188
Iteration 3/25 | Loss: 0.00069188
Iteration 4/25 | Loss: 0.00069187
Iteration 5/25 | Loss: 0.00069187
Iteration 6/25 | Loss: 0.00069187
Iteration 7/25 | Loss: 0.00069187
Iteration 8/25 | Loss: 0.00069187
Iteration 9/25 | Loss: 0.00069187
Iteration 10/25 | Loss: 0.00069187
Iteration 11/25 | Loss: 0.00069187
Iteration 12/25 | Loss: 0.00069187
Iteration 13/25 | Loss: 0.00069187
Iteration 14/25 | Loss: 0.00069187
Iteration 15/25 | Loss: 0.00069187
Iteration 16/25 | Loss: 0.00069187
Iteration 17/25 | Loss: 0.00069187
Iteration 18/25 | Loss: 0.00069187
Iteration 19/25 | Loss: 0.00069187
Iteration 20/25 | Loss: 0.00069187
Iteration 21/25 | Loss: 0.00069187
Iteration 22/25 | Loss: 0.00069187
Iteration 23/25 | Loss: 0.00069187
Iteration 24/25 | Loss: 0.00069187
Iteration 25/25 | Loss: 0.00069187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069187
Iteration 2/1000 | Loss: 0.00002867
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001378
Iteration 9/1000 | Loss: 0.00001371
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001357
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001345
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001332
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001327
Iteration 43/1000 | Loss: 0.00001327
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001322
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001322
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001320
Iteration 71/1000 | Loss: 0.00001320
Iteration 72/1000 | Loss: 0.00001320
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001319
Iteration 83/1000 | Loss: 0.00001319
Iteration 84/1000 | Loss: 0.00001319
Iteration 85/1000 | Loss: 0.00001319
Iteration 86/1000 | Loss: 0.00001319
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001319
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001319
Iteration 118/1000 | Loss: 0.00001319
Iteration 119/1000 | Loss: 0.00001319
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.3191688594815787e-05, 1.3191688594815787e-05, 1.3191688594815787e-05, 1.3191688594815787e-05, 1.3191688594815787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3191688594815787e-05

Optimization complete. Final v2v error: 3.1287269592285156 mm

Highest mean error: 3.821584939956665 mm for frame 68

Lowest mean error: 2.7070837020874023 mm for frame 125

Saving results

Total time: 38.38188314437866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01237466
Iteration 2/25 | Loss: 0.00447404
Iteration 3/25 | Loss: 0.00293457
Iteration 4/25 | Loss: 0.00258768
Iteration 5/25 | Loss: 0.00238358
Iteration 6/25 | Loss: 0.00214571
Iteration 7/25 | Loss: 0.00203100
Iteration 8/25 | Loss: 0.00195671
Iteration 9/25 | Loss: 0.00190288
Iteration 10/25 | Loss: 0.00189086
Iteration 11/25 | Loss: 0.00188210
Iteration 12/25 | Loss: 0.00188130
Iteration 13/25 | Loss: 0.00187528
Iteration 14/25 | Loss: 0.00187476
Iteration 15/25 | Loss: 0.00187104
Iteration 16/25 | Loss: 0.00187047
Iteration 17/25 | Loss: 0.00187172
Iteration 18/25 | Loss: 0.00187257
Iteration 19/25 | Loss: 0.00187060
Iteration 20/25 | Loss: 0.00186983
Iteration 21/25 | Loss: 0.00186927
Iteration 22/25 | Loss: 0.00186900
Iteration 23/25 | Loss: 0.00186976
Iteration 24/25 | Loss: 0.00186987
Iteration 25/25 | Loss: 0.00186920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52100629
Iteration 2/25 | Loss: 0.00626882
Iteration 3/25 | Loss: 0.00573435
Iteration 4/25 | Loss: 0.00573434
Iteration 5/25 | Loss: 0.00573434
Iteration 6/25 | Loss: 0.00573434
Iteration 7/25 | Loss: 0.00573434
Iteration 8/25 | Loss: 0.00573434
Iteration 9/25 | Loss: 0.00573434
Iteration 10/25 | Loss: 0.00573434
Iteration 11/25 | Loss: 0.00573434
Iteration 12/25 | Loss: 0.00573434
Iteration 13/25 | Loss: 0.00573434
Iteration 14/25 | Loss: 0.00573434
Iteration 15/25 | Loss: 0.00573434
Iteration 16/25 | Loss: 0.00573434
Iteration 17/25 | Loss: 0.00573434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.005734342150390148, 0.005734342150390148, 0.005734342150390148, 0.005734342150390148, 0.005734342150390148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005734342150390148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00573434
Iteration 2/1000 | Loss: 0.00200525
Iteration 3/1000 | Loss: 0.00203947
Iteration 4/1000 | Loss: 0.00180274
Iteration 5/1000 | Loss: 0.00575535
Iteration 6/1000 | Loss: 0.00166335
Iteration 7/1000 | Loss: 0.00290500
Iteration 8/1000 | Loss: 0.00184783
Iteration 9/1000 | Loss: 0.01355673
Iteration 10/1000 | Loss: 0.00473197
Iteration 11/1000 | Loss: 0.01219513
Iteration 12/1000 | Loss: 0.00422184
Iteration 13/1000 | Loss: 0.00136570
Iteration 14/1000 | Loss: 0.00892091
Iteration 15/1000 | Loss: 0.00241990
Iteration 16/1000 | Loss: 0.00156812
Iteration 17/1000 | Loss: 0.00056506
Iteration 18/1000 | Loss: 0.00202940
Iteration 19/1000 | Loss: 0.00823093
Iteration 20/1000 | Loss: 0.00218694
Iteration 21/1000 | Loss: 0.00086983
Iteration 22/1000 | Loss: 0.00056208
Iteration 23/1000 | Loss: 0.00060761
Iteration 24/1000 | Loss: 0.00070278
Iteration 25/1000 | Loss: 0.00050507
Iteration 26/1000 | Loss: 0.00039963
Iteration 27/1000 | Loss: 0.00038101
Iteration 28/1000 | Loss: 0.00359892
Iteration 29/1000 | Loss: 0.02724662
Iteration 30/1000 | Loss: 0.00571353
Iteration 31/1000 | Loss: 0.00077877
Iteration 32/1000 | Loss: 0.00072263
Iteration 33/1000 | Loss: 0.00046262
Iteration 34/1000 | Loss: 0.00079021
Iteration 35/1000 | Loss: 0.00043190
Iteration 36/1000 | Loss: 0.00048376
Iteration 37/1000 | Loss: 0.00084503
Iteration 38/1000 | Loss: 0.00114993
Iteration 39/1000 | Loss: 0.00087096
Iteration 40/1000 | Loss: 0.00158199
Iteration 41/1000 | Loss: 0.00064840
Iteration 42/1000 | Loss: 0.00124750
Iteration 43/1000 | Loss: 0.00057943
Iteration 44/1000 | Loss: 0.00116473
Iteration 45/1000 | Loss: 0.00199438
Iteration 46/1000 | Loss: 0.00088268
Iteration 47/1000 | Loss: 0.00097424
Iteration 48/1000 | Loss: 0.00053277
Iteration 49/1000 | Loss: 0.00034210
Iteration 50/1000 | Loss: 0.00016455
Iteration 51/1000 | Loss: 0.00036432
Iteration 52/1000 | Loss: 0.00048861
Iteration 53/1000 | Loss: 0.00011481
Iteration 54/1000 | Loss: 0.00010439
Iteration 55/1000 | Loss: 0.00027556
Iteration 56/1000 | Loss: 0.00030543
Iteration 57/1000 | Loss: 0.00019607
Iteration 58/1000 | Loss: 0.00024973
Iteration 59/1000 | Loss: 0.00020934
Iteration 60/1000 | Loss: 0.00020096
Iteration 61/1000 | Loss: 0.00016611
Iteration 62/1000 | Loss: 0.00015021
Iteration 63/1000 | Loss: 0.00017956
Iteration 64/1000 | Loss: 0.00014430
Iteration 65/1000 | Loss: 0.00025729
Iteration 66/1000 | Loss: 0.00014717
Iteration 67/1000 | Loss: 0.00009293
Iteration 68/1000 | Loss: 0.00008751
Iteration 69/1000 | Loss: 0.00008296
Iteration 70/1000 | Loss: 0.00011462
Iteration 71/1000 | Loss: 0.00014232
Iteration 72/1000 | Loss: 0.00011571
Iteration 73/1000 | Loss: 0.00017359
Iteration 74/1000 | Loss: 0.00025993
Iteration 75/1000 | Loss: 0.00007888
Iteration 76/1000 | Loss: 0.00021459
Iteration 77/1000 | Loss: 0.00029426
Iteration 78/1000 | Loss: 0.00016521
Iteration 79/1000 | Loss: 0.00022918
Iteration 80/1000 | Loss: 0.00020570
Iteration 81/1000 | Loss: 0.00033972
Iteration 82/1000 | Loss: 0.00008970
Iteration 83/1000 | Loss: 0.00008347
Iteration 84/1000 | Loss: 0.00007878
Iteration 85/1000 | Loss: 0.00048467
Iteration 86/1000 | Loss: 0.00010405
Iteration 87/1000 | Loss: 0.00007511
Iteration 88/1000 | Loss: 0.00021825
Iteration 89/1000 | Loss: 0.00054092
Iteration 90/1000 | Loss: 0.00044388
Iteration 91/1000 | Loss: 0.00007440
Iteration 92/1000 | Loss: 0.00018990
Iteration 93/1000 | Loss: 0.00007399
Iteration 94/1000 | Loss: 0.00007252
Iteration 95/1000 | Loss: 0.00020483
Iteration 96/1000 | Loss: 0.00019934
Iteration 97/1000 | Loss: 0.00027998
Iteration 98/1000 | Loss: 0.00053763
Iteration 99/1000 | Loss: 0.00024049
Iteration 100/1000 | Loss: 0.00049959
Iteration 101/1000 | Loss: 0.00026620
Iteration 102/1000 | Loss: 0.00016442
Iteration 103/1000 | Loss: 0.00052811
Iteration 104/1000 | Loss: 0.00017260
Iteration 105/1000 | Loss: 0.00018824
Iteration 106/1000 | Loss: 0.00025536
Iteration 107/1000 | Loss: 0.00020633
Iteration 108/1000 | Loss: 0.00010520
Iteration 109/1000 | Loss: 0.00024149
Iteration 110/1000 | Loss: 0.00023958
Iteration 111/1000 | Loss: 0.00026566
Iteration 112/1000 | Loss: 0.00024990
Iteration 113/1000 | Loss: 0.00028487
Iteration 114/1000 | Loss: 0.00025028
Iteration 115/1000 | Loss: 0.00034801
Iteration 116/1000 | Loss: 0.00030985
Iteration 117/1000 | Loss: 0.00032368
Iteration 118/1000 | Loss: 0.00021879
Iteration 119/1000 | Loss: 0.00008318
Iteration 120/1000 | Loss: 0.00053962
Iteration 121/1000 | Loss: 0.00008200
Iteration 122/1000 | Loss: 0.00007681
Iteration 123/1000 | Loss: 0.00007512
Iteration 124/1000 | Loss: 0.00007336
Iteration 125/1000 | Loss: 0.00014107
Iteration 126/1000 | Loss: 0.00008772
Iteration 127/1000 | Loss: 0.00015291
Iteration 128/1000 | Loss: 0.00017580
Iteration 129/1000 | Loss: 0.00014861
Iteration 130/1000 | Loss: 0.00023568
Iteration 131/1000 | Loss: 0.00031118
Iteration 132/1000 | Loss: 0.00024702
Iteration 133/1000 | Loss: 0.00007692
Iteration 134/1000 | Loss: 0.00026745
Iteration 135/1000 | Loss: 0.00044358
Iteration 136/1000 | Loss: 0.00017029
Iteration 137/1000 | Loss: 0.00014013
Iteration 138/1000 | Loss: 0.00008903
Iteration 139/1000 | Loss: 0.00010349
Iteration 140/1000 | Loss: 0.00009061
Iteration 141/1000 | Loss: 0.00007232
Iteration 142/1000 | Loss: 0.00007142
Iteration 143/1000 | Loss: 0.00018754
Iteration 144/1000 | Loss: 0.00021041
Iteration 145/1000 | Loss: 0.00008642
Iteration 146/1000 | Loss: 0.00013924
Iteration 147/1000 | Loss: 0.00017632
Iteration 148/1000 | Loss: 0.00016280
Iteration 149/1000 | Loss: 0.00024899
Iteration 150/1000 | Loss: 0.00035015
Iteration 151/1000 | Loss: 0.00036116
Iteration 152/1000 | Loss: 0.00039064
Iteration 153/1000 | Loss: 0.00020499
Iteration 154/1000 | Loss: 0.00023487
Iteration 155/1000 | Loss: 0.00012324
Iteration 156/1000 | Loss: 0.00036219
Iteration 157/1000 | Loss: 0.00031966
Iteration 158/1000 | Loss: 0.00033233
Iteration 159/1000 | Loss: 0.00008268
Iteration 160/1000 | Loss: 0.00007420
Iteration 161/1000 | Loss: 0.00007125
Iteration 162/1000 | Loss: 0.00007056
Iteration 163/1000 | Loss: 0.00007752
Iteration 164/1000 | Loss: 0.00029791
Iteration 165/1000 | Loss: 0.00018445
Iteration 166/1000 | Loss: 0.00010521
Iteration 167/1000 | Loss: 0.00008197
Iteration 168/1000 | Loss: 0.00035916
Iteration 169/1000 | Loss: 0.00032628
Iteration 170/1000 | Loss: 0.00041453
Iteration 171/1000 | Loss: 0.00035314
Iteration 172/1000 | Loss: 0.00038349
Iteration 173/1000 | Loss: 0.00031981
Iteration 174/1000 | Loss: 0.00024961
Iteration 175/1000 | Loss: 0.00007525
Iteration 176/1000 | Loss: 0.00021148
Iteration 177/1000 | Loss: 0.00043627
Iteration 178/1000 | Loss: 0.00016429
Iteration 179/1000 | Loss: 0.00017195
Iteration 180/1000 | Loss: 0.00022958
Iteration 181/1000 | Loss: 0.00023337
Iteration 182/1000 | Loss: 0.00020595
Iteration 183/1000 | Loss: 0.00029214
Iteration 184/1000 | Loss: 0.00028744
Iteration 185/1000 | Loss: 0.00034717
Iteration 186/1000 | Loss: 0.00028397
Iteration 187/1000 | Loss: 0.00018264
Iteration 188/1000 | Loss: 0.00032688
Iteration 189/1000 | Loss: 0.00036036
Iteration 190/1000 | Loss: 0.00035892
Iteration 191/1000 | Loss: 0.00020071
Iteration 192/1000 | Loss: 0.00028581
Iteration 193/1000 | Loss: 0.00027688
Iteration 194/1000 | Loss: 0.00020437
Iteration 195/1000 | Loss: 0.00037966
Iteration 196/1000 | Loss: 0.00010590
Iteration 197/1000 | Loss: 0.00008462
Iteration 198/1000 | Loss: 0.00018379
Iteration 199/1000 | Loss: 0.00036978
Iteration 200/1000 | Loss: 0.00040816
Iteration 201/1000 | Loss: 0.00012035
Iteration 202/1000 | Loss: 0.00026122
Iteration 203/1000 | Loss: 0.00026919
Iteration 204/1000 | Loss: 0.00008257
Iteration 205/1000 | Loss: 0.00007815
Iteration 206/1000 | Loss: 0.00056687
Iteration 207/1000 | Loss: 0.00053577
Iteration 208/1000 | Loss: 0.00026331
Iteration 209/1000 | Loss: 0.00024985
Iteration 210/1000 | Loss: 0.00017943
Iteration 211/1000 | Loss: 0.00024906
Iteration 212/1000 | Loss: 0.00044868
Iteration 213/1000 | Loss: 0.00014137
Iteration 214/1000 | Loss: 0.00024432
Iteration 215/1000 | Loss: 0.00031141
Iteration 216/1000 | Loss: 0.00040724
Iteration 217/1000 | Loss: 0.00023889
Iteration 218/1000 | Loss: 0.00063620
Iteration 219/1000 | Loss: 0.00026913
Iteration 220/1000 | Loss: 0.00027006
Iteration 221/1000 | Loss: 0.00029630
Iteration 222/1000 | Loss: 0.00013679
Iteration 223/1000 | Loss: 0.00008699
Iteration 224/1000 | Loss: 0.00007541
Iteration 225/1000 | Loss: 0.00007401
Iteration 226/1000 | Loss: 0.00007283
Iteration 227/1000 | Loss: 0.00007234
Iteration 228/1000 | Loss: 0.00007186
Iteration 229/1000 | Loss: 0.00007159
Iteration 230/1000 | Loss: 0.00007116
Iteration 231/1000 | Loss: 0.00007091
Iteration 232/1000 | Loss: 0.00007069
Iteration 233/1000 | Loss: 0.00007061
Iteration 234/1000 | Loss: 0.00007041
Iteration 235/1000 | Loss: 0.00007025
Iteration 236/1000 | Loss: 0.00007014
Iteration 237/1000 | Loss: 0.00007013
Iteration 238/1000 | Loss: 0.00011456
Iteration 239/1000 | Loss: 0.00051037
Iteration 240/1000 | Loss: 0.00011928
Iteration 241/1000 | Loss: 0.00007401
Iteration 242/1000 | Loss: 0.00006961
Iteration 243/1000 | Loss: 0.00006858
Iteration 244/1000 | Loss: 0.00006808
Iteration 245/1000 | Loss: 0.00006735
Iteration 246/1000 | Loss: 0.00006701
Iteration 247/1000 | Loss: 0.00006687
Iteration 248/1000 | Loss: 0.00006686
Iteration 249/1000 | Loss: 0.00006683
Iteration 250/1000 | Loss: 0.00006675
Iteration 251/1000 | Loss: 0.00006674
Iteration 252/1000 | Loss: 0.00006674
Iteration 253/1000 | Loss: 0.00006672
Iteration 254/1000 | Loss: 0.00006670
Iteration 255/1000 | Loss: 0.00006670
Iteration 256/1000 | Loss: 0.00006670
Iteration 257/1000 | Loss: 0.00006669
Iteration 258/1000 | Loss: 0.00006669
Iteration 259/1000 | Loss: 0.00006669
Iteration 260/1000 | Loss: 0.00006669
Iteration 261/1000 | Loss: 0.00006669
Iteration 262/1000 | Loss: 0.00006669
Iteration 263/1000 | Loss: 0.00006669
Iteration 264/1000 | Loss: 0.00006669
Iteration 265/1000 | Loss: 0.00006669
Iteration 266/1000 | Loss: 0.00006669
Iteration 267/1000 | Loss: 0.00006668
Iteration 268/1000 | Loss: 0.00006668
Iteration 269/1000 | Loss: 0.00006668
Iteration 270/1000 | Loss: 0.00006668
Iteration 271/1000 | Loss: 0.00006668
Iteration 272/1000 | Loss: 0.00006667
Iteration 273/1000 | Loss: 0.00006667
Iteration 274/1000 | Loss: 0.00006667
Iteration 275/1000 | Loss: 0.00006667
Iteration 276/1000 | Loss: 0.00006667
Iteration 277/1000 | Loss: 0.00006667
Iteration 278/1000 | Loss: 0.00006667
Iteration 279/1000 | Loss: 0.00006667
Iteration 280/1000 | Loss: 0.00006667
Iteration 281/1000 | Loss: 0.00006667
Iteration 282/1000 | Loss: 0.00006667
Iteration 283/1000 | Loss: 0.00006667
Iteration 284/1000 | Loss: 0.00006667
Iteration 285/1000 | Loss: 0.00006667
Iteration 286/1000 | Loss: 0.00006667
Iteration 287/1000 | Loss: 0.00006667
Iteration 288/1000 | Loss: 0.00006667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [6.66680425638333e-05, 6.66680425638333e-05, 6.66680425638333e-05, 6.66680425638333e-05, 6.66680425638333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.66680425638333e-05

Optimization complete. Final v2v error: 6.2651753425598145 mm

Highest mean error: 11.6504545211792 mm for frame 61

Lowest mean error: 4.002950191497803 mm for frame 2

Saving results

Total time: 454.4560182094574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098206
Iteration 2/25 | Loss: 0.00377991
Iteration 3/25 | Loss: 0.00297907
Iteration 4/25 | Loss: 0.00259528
Iteration 5/25 | Loss: 0.00223038
Iteration 6/25 | Loss: 0.00223304
Iteration 7/25 | Loss: 0.00210692
Iteration 8/25 | Loss: 0.00204368
Iteration 9/25 | Loss: 0.00188987
Iteration 10/25 | Loss: 0.00185309
Iteration 11/25 | Loss: 0.00182078
Iteration 12/25 | Loss: 0.00180989
Iteration 13/25 | Loss: 0.00179494
Iteration 14/25 | Loss: 0.00176796
Iteration 15/25 | Loss: 0.00176005
Iteration 16/25 | Loss: 0.00176729
Iteration 17/25 | Loss: 0.00177156
Iteration 18/25 | Loss: 0.00175281
Iteration 19/25 | Loss: 0.00174030
Iteration 20/25 | Loss: 0.00174666
Iteration 21/25 | Loss: 0.00173770
Iteration 22/25 | Loss: 0.00173802
Iteration 23/25 | Loss: 0.00173522
Iteration 24/25 | Loss: 0.00173469
Iteration 25/25 | Loss: 0.00173481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29333782
Iteration 2/25 | Loss: 0.01150321
Iteration 3/25 | Loss: 0.00718641
Iteration 4/25 | Loss: 0.00710822
Iteration 5/25 | Loss: 0.00654388
Iteration 6/25 | Loss: 0.00641092
Iteration 7/25 | Loss: 0.00641092
Iteration 8/25 | Loss: 0.00641092
Iteration 9/25 | Loss: 0.00641092
Iteration 10/25 | Loss: 0.00641092
Iteration 11/25 | Loss: 0.00641092
Iteration 12/25 | Loss: 0.00641092
Iteration 13/25 | Loss: 0.00641092
Iteration 14/25 | Loss: 0.00641092
Iteration 15/25 | Loss: 0.00641092
Iteration 16/25 | Loss: 0.00641092
Iteration 17/25 | Loss: 0.00641092
Iteration 18/25 | Loss: 0.00641092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.006410915404558182, 0.006410915404558182, 0.006410915404558182, 0.006410915404558182, 0.006410915404558182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006410915404558182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00641092
Iteration 2/1000 | Loss: 0.00460183
Iteration 3/1000 | Loss: 0.00330051
Iteration 4/1000 | Loss: 0.00577299
Iteration 5/1000 | Loss: 0.00125967
Iteration 6/1000 | Loss: 0.00882876
Iteration 7/1000 | Loss: 0.00180665
Iteration 8/1000 | Loss: 0.00393288
Iteration 9/1000 | Loss: 0.00107695
Iteration 10/1000 | Loss: 0.00063013
Iteration 11/1000 | Loss: 0.00230280
Iteration 12/1000 | Loss: 0.00258689
Iteration 13/1000 | Loss: 0.00040544
Iteration 14/1000 | Loss: 0.00145201
Iteration 15/1000 | Loss: 0.00184938
Iteration 16/1000 | Loss: 0.00035932
Iteration 17/1000 | Loss: 0.00033435
Iteration 18/1000 | Loss: 0.00113827
Iteration 19/1000 | Loss: 0.00101728
Iteration 20/1000 | Loss: 0.00108387
Iteration 21/1000 | Loss: 0.00046959
Iteration 22/1000 | Loss: 0.00041454
Iteration 23/1000 | Loss: 0.00025991
Iteration 24/1000 | Loss: 0.00033422
Iteration 25/1000 | Loss: 0.00024553
Iteration 26/1000 | Loss: 0.00099482
Iteration 27/1000 | Loss: 0.00075149
Iteration 28/1000 | Loss: 0.00026161
Iteration 29/1000 | Loss: 0.00023580
Iteration 30/1000 | Loss: 0.00025416
Iteration 31/1000 | Loss: 0.00064317
Iteration 32/1000 | Loss: 0.00022792
Iteration 33/1000 | Loss: 0.00022515
Iteration 34/1000 | Loss: 0.00061390
Iteration 35/1000 | Loss: 0.00098310
Iteration 36/1000 | Loss: 0.00113517
Iteration 37/1000 | Loss: 0.00034697
Iteration 38/1000 | Loss: 0.00073679
Iteration 39/1000 | Loss: 0.00064897
Iteration 40/1000 | Loss: 0.00022273
Iteration 41/1000 | Loss: 0.00021818
Iteration 42/1000 | Loss: 0.00021605
Iteration 43/1000 | Loss: 0.00050177
Iteration 44/1000 | Loss: 0.00042400
Iteration 45/1000 | Loss: 0.00021226
Iteration 46/1000 | Loss: 0.00032832
Iteration 47/1000 | Loss: 0.00022477
Iteration 48/1000 | Loss: 0.00020977
Iteration 49/1000 | Loss: 0.00038809
Iteration 50/1000 | Loss: 0.00233754
Iteration 51/1000 | Loss: 0.01037219
Iteration 52/1000 | Loss: 0.00675698
Iteration 53/1000 | Loss: 0.00738398
Iteration 54/1000 | Loss: 0.00180679
Iteration 55/1000 | Loss: 0.00090876
Iteration 56/1000 | Loss: 0.00093858
Iteration 57/1000 | Loss: 0.00155886
Iteration 58/1000 | Loss: 0.00112189
Iteration 59/1000 | Loss: 0.00156160
Iteration 60/1000 | Loss: 0.00155621
Iteration 61/1000 | Loss: 0.00223124
Iteration 62/1000 | Loss: 0.00099380
Iteration 63/1000 | Loss: 0.00192084
Iteration 64/1000 | Loss: 0.00899508
Iteration 65/1000 | Loss: 0.00215994
Iteration 66/1000 | Loss: 0.00186229
Iteration 67/1000 | Loss: 0.00176585
Iteration 68/1000 | Loss: 0.00054026
Iteration 69/1000 | Loss: 0.00099922
Iteration 70/1000 | Loss: 0.00087594
Iteration 71/1000 | Loss: 0.00027222
Iteration 72/1000 | Loss: 0.00075235
Iteration 73/1000 | Loss: 0.00038777
Iteration 74/1000 | Loss: 0.00037982
Iteration 75/1000 | Loss: 0.00107726
Iteration 76/1000 | Loss: 0.00214242
Iteration 77/1000 | Loss: 0.00070333
Iteration 78/1000 | Loss: 0.00049763
Iteration 79/1000 | Loss: 0.00024869
Iteration 80/1000 | Loss: 0.00033908
Iteration 81/1000 | Loss: 0.00033551
Iteration 82/1000 | Loss: 0.00031766
Iteration 83/1000 | Loss: 0.00100346
Iteration 84/1000 | Loss: 0.00152063
Iteration 85/1000 | Loss: 0.00176422
Iteration 86/1000 | Loss: 0.00090026
Iteration 87/1000 | Loss: 0.00035248
Iteration 88/1000 | Loss: 0.00025015
Iteration 89/1000 | Loss: 0.00031419
Iteration 90/1000 | Loss: 0.00031501
Iteration 91/1000 | Loss: 0.00104949
Iteration 92/1000 | Loss: 0.00020173
Iteration 93/1000 | Loss: 0.00026486
Iteration 94/1000 | Loss: 0.00063484
Iteration 95/1000 | Loss: 0.00049231
Iteration 96/1000 | Loss: 0.00024206
Iteration 97/1000 | Loss: 0.00078488
Iteration 98/1000 | Loss: 0.00030455
Iteration 99/1000 | Loss: 0.00027382
Iteration 100/1000 | Loss: 0.00035528
Iteration 101/1000 | Loss: 0.00064954
Iteration 102/1000 | Loss: 0.00101892
Iteration 103/1000 | Loss: 0.00017438
Iteration 104/1000 | Loss: 0.00016156
Iteration 105/1000 | Loss: 0.00024915
Iteration 106/1000 | Loss: 0.00050586
Iteration 107/1000 | Loss: 0.00016990
Iteration 108/1000 | Loss: 0.00015127
Iteration 109/1000 | Loss: 0.00014844
Iteration 110/1000 | Loss: 0.00014702
Iteration 111/1000 | Loss: 0.00038659
Iteration 112/1000 | Loss: 0.00053095
Iteration 113/1000 | Loss: 0.00079677
Iteration 114/1000 | Loss: 0.00014523
Iteration 115/1000 | Loss: 0.00014381
Iteration 116/1000 | Loss: 0.00014303
Iteration 117/1000 | Loss: 0.00032732
Iteration 118/1000 | Loss: 0.00015919
Iteration 119/1000 | Loss: 0.00026796
Iteration 120/1000 | Loss: 0.00024751
Iteration 121/1000 | Loss: 0.00015289
Iteration 122/1000 | Loss: 0.00014598
Iteration 123/1000 | Loss: 0.00027405
Iteration 124/1000 | Loss: 0.00028614
Iteration 125/1000 | Loss: 0.00018090
Iteration 126/1000 | Loss: 0.00015471
Iteration 127/1000 | Loss: 0.00014628
Iteration 128/1000 | Loss: 0.00049830
Iteration 129/1000 | Loss: 0.00041065
Iteration 130/1000 | Loss: 0.00035460
Iteration 131/1000 | Loss: 0.00024573
Iteration 132/1000 | Loss: 0.00018260
Iteration 133/1000 | Loss: 0.00017987
Iteration 134/1000 | Loss: 0.00014741
Iteration 135/1000 | Loss: 0.00051311
Iteration 136/1000 | Loss: 0.00044000
Iteration 137/1000 | Loss: 0.00066225
Iteration 138/1000 | Loss: 0.00054385
Iteration 139/1000 | Loss: 0.00048204
Iteration 140/1000 | Loss: 0.00019824
Iteration 141/1000 | Loss: 0.00023089
Iteration 142/1000 | Loss: 0.00026979
Iteration 143/1000 | Loss: 0.00035212
Iteration 144/1000 | Loss: 0.00031312
Iteration 145/1000 | Loss: 0.00018198
Iteration 146/1000 | Loss: 0.00016136
Iteration 147/1000 | Loss: 0.00015090
Iteration 148/1000 | Loss: 0.00014501
Iteration 149/1000 | Loss: 0.00013920
Iteration 150/1000 | Loss: 0.00013657
Iteration 151/1000 | Loss: 0.00013471
Iteration 152/1000 | Loss: 0.00018966
Iteration 153/1000 | Loss: 0.00031870
Iteration 154/1000 | Loss: 0.00036449
Iteration 155/1000 | Loss: 0.00017555
Iteration 156/1000 | Loss: 0.00015227
Iteration 157/1000 | Loss: 0.00013938
Iteration 158/1000 | Loss: 0.00013570
Iteration 159/1000 | Loss: 0.00013419
Iteration 160/1000 | Loss: 0.00018804
Iteration 161/1000 | Loss: 0.00019299
Iteration 162/1000 | Loss: 0.00020234
Iteration 163/1000 | Loss: 0.00023347
Iteration 164/1000 | Loss: 0.00020825
Iteration 165/1000 | Loss: 0.00032812
Iteration 166/1000 | Loss: 0.00021371
Iteration 167/1000 | Loss: 0.00020727
Iteration 168/1000 | Loss: 0.00019700
Iteration 169/1000 | Loss: 0.00018625
Iteration 170/1000 | Loss: 0.00016992
Iteration 171/1000 | Loss: 0.00023223
Iteration 172/1000 | Loss: 0.00026558
Iteration 173/1000 | Loss: 0.00020453
Iteration 174/1000 | Loss: 0.00026140
Iteration 175/1000 | Loss: 0.00020269
Iteration 176/1000 | Loss: 0.00015160
Iteration 177/1000 | Loss: 0.00014552
Iteration 178/1000 | Loss: 0.00017266
Iteration 179/1000 | Loss: 0.00019159
Iteration 180/1000 | Loss: 0.00020992
Iteration 181/1000 | Loss: 0.00025217
Iteration 182/1000 | Loss: 0.00028184
Iteration 183/1000 | Loss: 0.00021047
Iteration 184/1000 | Loss: 0.00033854
Iteration 185/1000 | Loss: 0.00028743
Iteration 186/1000 | Loss: 0.00034290
Iteration 187/1000 | Loss: 0.00040603
Iteration 188/1000 | Loss: 0.00037767
Iteration 189/1000 | Loss: 0.00028151
Iteration 190/1000 | Loss: 0.00024418
Iteration 191/1000 | Loss: 0.00030785
Iteration 192/1000 | Loss: 0.00049148
Iteration 193/1000 | Loss: 0.00033868
Iteration 194/1000 | Loss: 0.00029040
Iteration 195/1000 | Loss: 0.00029973
Iteration 196/1000 | Loss: 0.00026426
Iteration 197/1000 | Loss: 0.00026608
Iteration 198/1000 | Loss: 0.00129761
Iteration 199/1000 | Loss: 0.00024188
Iteration 200/1000 | Loss: 0.00026615
Iteration 201/1000 | Loss: 0.00022530
Iteration 202/1000 | Loss: 0.00023444
Iteration 203/1000 | Loss: 0.00019167
Iteration 204/1000 | Loss: 0.00028418
Iteration 205/1000 | Loss: 0.00031805
Iteration 206/1000 | Loss: 0.00025753
Iteration 207/1000 | Loss: 0.00027822
Iteration 208/1000 | Loss: 0.00023689
Iteration 209/1000 | Loss: 0.00019255
Iteration 210/1000 | Loss: 0.00020650
Iteration 211/1000 | Loss: 0.00016963
Iteration 212/1000 | Loss: 0.00013920
Iteration 213/1000 | Loss: 0.00013168
Iteration 214/1000 | Loss: 0.00012575
Iteration 215/1000 | Loss: 0.00018502
Iteration 216/1000 | Loss: 0.00031708
Iteration 217/1000 | Loss: 0.00026146
Iteration 218/1000 | Loss: 0.00020055
Iteration 219/1000 | Loss: 0.00016509
Iteration 220/1000 | Loss: 0.00016795
Iteration 221/1000 | Loss: 0.00018452
Iteration 222/1000 | Loss: 0.00013031
Iteration 223/1000 | Loss: 0.00011983
Iteration 224/1000 | Loss: 0.00011909
Iteration 225/1000 | Loss: 0.00011787
Iteration 226/1000 | Loss: 0.00011688
Iteration 227/1000 | Loss: 0.00011603
Iteration 228/1000 | Loss: 0.00011556
Iteration 229/1000 | Loss: 0.00019084
Iteration 230/1000 | Loss: 0.00012582
Iteration 231/1000 | Loss: 0.00011815
Iteration 232/1000 | Loss: 0.00011687
Iteration 233/1000 | Loss: 0.00017701
Iteration 234/1000 | Loss: 0.00025629
Iteration 235/1000 | Loss: 0.00019608
Iteration 236/1000 | Loss: 0.00032000
Iteration 237/1000 | Loss: 0.00058993
Iteration 238/1000 | Loss: 0.00022238
Iteration 239/1000 | Loss: 0.00022207
Iteration 240/1000 | Loss: 0.00029647
Iteration 241/1000 | Loss: 0.00027778
Iteration 242/1000 | Loss: 0.00021961
Iteration 243/1000 | Loss: 0.00022660
Iteration 244/1000 | Loss: 0.00020414
Iteration 245/1000 | Loss: 0.00021434
Iteration 246/1000 | Loss: 0.00022536
Iteration 247/1000 | Loss: 0.00028874
Iteration 248/1000 | Loss: 0.00032404
Iteration 249/1000 | Loss: 0.00026530
Iteration 250/1000 | Loss: 0.00040043
Iteration 251/1000 | Loss: 0.00024523
Iteration 252/1000 | Loss: 0.00016034
Iteration 253/1000 | Loss: 0.00017121
Iteration 254/1000 | Loss: 0.00021373
Iteration 255/1000 | Loss: 0.00019172
Iteration 256/1000 | Loss: 0.00012481
Iteration 257/1000 | Loss: 0.00021441
Iteration 258/1000 | Loss: 0.00014322
Iteration 259/1000 | Loss: 0.00018146
Iteration 260/1000 | Loss: 0.00012802
Iteration 261/1000 | Loss: 0.00020514
Iteration 262/1000 | Loss: 0.00023288
Iteration 263/1000 | Loss: 0.00028360
Iteration 264/1000 | Loss: 0.00035618
Iteration 265/1000 | Loss: 0.00040094
Iteration 266/1000 | Loss: 0.00064240
Iteration 267/1000 | Loss: 0.00029179
Iteration 268/1000 | Loss: 0.00019599
Iteration 269/1000 | Loss: 0.00036287
Iteration 270/1000 | Loss: 0.00026328
Iteration 271/1000 | Loss: 0.00024335
Iteration 272/1000 | Loss: 0.00017971
Iteration 273/1000 | Loss: 0.00015444
Iteration 274/1000 | Loss: 0.00017378
Iteration 275/1000 | Loss: 0.00030434
Iteration 276/1000 | Loss: 0.00022605
Iteration 277/1000 | Loss: 0.00027671
Iteration 278/1000 | Loss: 0.00024580
Iteration 279/1000 | Loss: 0.00028224
Iteration 280/1000 | Loss: 0.00019545
Iteration 281/1000 | Loss: 0.00018426
Iteration 282/1000 | Loss: 0.00021033
Iteration 283/1000 | Loss: 0.00017901
Iteration 284/1000 | Loss: 0.00017582
Iteration 285/1000 | Loss: 0.00020548
Iteration 286/1000 | Loss: 0.00020214
Iteration 287/1000 | Loss: 0.00019876
Iteration 288/1000 | Loss: 0.00014974
Iteration 289/1000 | Loss: 0.00018638
Iteration 290/1000 | Loss: 0.00015014
Iteration 291/1000 | Loss: 0.00014569
Iteration 292/1000 | Loss: 0.00015524
Iteration 293/1000 | Loss: 0.00011861
Iteration 294/1000 | Loss: 0.00019211
Iteration 295/1000 | Loss: 0.00019059
Iteration 296/1000 | Loss: 0.00017290
Iteration 297/1000 | Loss: 0.00021612
Iteration 298/1000 | Loss: 0.00027315
Iteration 299/1000 | Loss: 0.00019891
Iteration 300/1000 | Loss: 0.00017309
Iteration 301/1000 | Loss: 0.00034012
Iteration 302/1000 | Loss: 0.00019901
Iteration 303/1000 | Loss: 0.00012937
Iteration 304/1000 | Loss: 0.00027372
Iteration 305/1000 | Loss: 0.00013398
Iteration 306/1000 | Loss: 0.00019082
Iteration 307/1000 | Loss: 0.00017898
Iteration 308/1000 | Loss: 0.00019009
Iteration 309/1000 | Loss: 0.00011458
Iteration 310/1000 | Loss: 0.00023074
Iteration 311/1000 | Loss: 0.00011511
Iteration 312/1000 | Loss: 0.00025784
Iteration 313/1000 | Loss: 0.00023210
Iteration 314/1000 | Loss: 0.00024028
Iteration 315/1000 | Loss: 0.00017809
Iteration 316/1000 | Loss: 0.00018113
Iteration 317/1000 | Loss: 0.00011918
Iteration 318/1000 | Loss: 0.00011442
Iteration 319/1000 | Loss: 0.00011836
Iteration 320/1000 | Loss: 0.00023455
Iteration 321/1000 | Loss: 0.00024863
Iteration 322/1000 | Loss: 0.00011078
Iteration 323/1000 | Loss: 0.00014190
Iteration 324/1000 | Loss: 0.00011410
Iteration 325/1000 | Loss: 0.00011171
Iteration 326/1000 | Loss: 0.00010881
Iteration 327/1000 | Loss: 0.00018307
Iteration 328/1000 | Loss: 0.00011110
Iteration 329/1000 | Loss: 0.00010899
Iteration 330/1000 | Loss: 0.00010808
Iteration 331/1000 | Loss: 0.00016842
Iteration 332/1000 | Loss: 0.00012549
Iteration 333/1000 | Loss: 0.00011536
Iteration 334/1000 | Loss: 0.00011076
Iteration 335/1000 | Loss: 0.00010898
Iteration 336/1000 | Loss: 0.00010834
Iteration 337/1000 | Loss: 0.00010792
Iteration 338/1000 | Loss: 0.00010756
Iteration 339/1000 | Loss: 0.00010740
Iteration 340/1000 | Loss: 0.00010737
Iteration 341/1000 | Loss: 0.00010733
Iteration 342/1000 | Loss: 0.00010725
Iteration 343/1000 | Loss: 0.00010725
Iteration 344/1000 | Loss: 0.00010725
Iteration 345/1000 | Loss: 0.00010725
Iteration 346/1000 | Loss: 0.00010724
Iteration 347/1000 | Loss: 0.00010724
Iteration 348/1000 | Loss: 0.00010724
Iteration 349/1000 | Loss: 0.00010724
Iteration 350/1000 | Loss: 0.00010724
Iteration 351/1000 | Loss: 0.00010722
Iteration 352/1000 | Loss: 0.00010721
Iteration 353/1000 | Loss: 0.00025677
Iteration 354/1000 | Loss: 0.00010740
Iteration 355/1000 | Loss: 0.00010722
Iteration 356/1000 | Loss: 0.00010715
Iteration 357/1000 | Loss: 0.00010714
Iteration 358/1000 | Loss: 0.00010714
Iteration 359/1000 | Loss: 0.00010714
Iteration 360/1000 | Loss: 0.00010714
Iteration 361/1000 | Loss: 0.00010713
Iteration 362/1000 | Loss: 0.00010713
Iteration 363/1000 | Loss: 0.00010713
Iteration 364/1000 | Loss: 0.00010713
Iteration 365/1000 | Loss: 0.00010710
Iteration 366/1000 | Loss: 0.00010710
Iteration 367/1000 | Loss: 0.00010710
Iteration 368/1000 | Loss: 0.00010709
Iteration 369/1000 | Loss: 0.00010709
Iteration 370/1000 | Loss: 0.00010709
Iteration 371/1000 | Loss: 0.00010709
Iteration 372/1000 | Loss: 0.00010709
Iteration 373/1000 | Loss: 0.00010709
Iteration 374/1000 | Loss: 0.00010709
Iteration 375/1000 | Loss: 0.00010708
Iteration 376/1000 | Loss: 0.00010708
Iteration 377/1000 | Loss: 0.00010708
Iteration 378/1000 | Loss: 0.00010707
Iteration 379/1000 | Loss: 0.00010706
Iteration 380/1000 | Loss: 0.00010706
Iteration 381/1000 | Loss: 0.00010706
Iteration 382/1000 | Loss: 0.00010705
Iteration 383/1000 | Loss: 0.00010705
Iteration 384/1000 | Loss: 0.00023309
Iteration 385/1000 | Loss: 0.00042019
Iteration 386/1000 | Loss: 0.00023249
Iteration 387/1000 | Loss: 0.00016886
Iteration 388/1000 | Loss: 0.00019377
Iteration 389/1000 | Loss: 0.00017413
Iteration 390/1000 | Loss: 0.00016884
Iteration 391/1000 | Loss: 0.00014629
Iteration 392/1000 | Loss: 0.00021424
Iteration 393/1000 | Loss: 0.00017390
Iteration 394/1000 | Loss: 0.00011323
Iteration 395/1000 | Loss: 0.00014094
Iteration 396/1000 | Loss: 0.00018187
Iteration 397/1000 | Loss: 0.00022207
Iteration 398/1000 | Loss: 0.00016298
Iteration 399/1000 | Loss: 0.00016618
Iteration 400/1000 | Loss: 0.00020245
Iteration 401/1000 | Loss: 0.00012212
Iteration 402/1000 | Loss: 0.00010381
Iteration 403/1000 | Loss: 0.00010149
Iteration 404/1000 | Loss: 0.00010055
Iteration 405/1000 | Loss: 0.00010010
Iteration 406/1000 | Loss: 0.00021015
Iteration 407/1000 | Loss: 0.00012608
Iteration 408/1000 | Loss: 0.00009970
Iteration 409/1000 | Loss: 0.00009955
Iteration 410/1000 | Loss: 0.00009953
Iteration 411/1000 | Loss: 0.00021286
Iteration 412/1000 | Loss: 0.00010009
Iteration 413/1000 | Loss: 0.00009947
Iteration 414/1000 | Loss: 0.00009943
Iteration 415/1000 | Loss: 0.00009943
Iteration 416/1000 | Loss: 0.00009942
Iteration 417/1000 | Loss: 0.00009942
Iteration 418/1000 | Loss: 0.00009941
Iteration 419/1000 | Loss: 0.00009941
Iteration 420/1000 | Loss: 0.00009941
Iteration 421/1000 | Loss: 0.00009940
Iteration 422/1000 | Loss: 0.00009940
Iteration 423/1000 | Loss: 0.00009940
Iteration 424/1000 | Loss: 0.00009937
Iteration 425/1000 | Loss: 0.00009934
Iteration 426/1000 | Loss: 0.00009934
Iteration 427/1000 | Loss: 0.00009933
Iteration 428/1000 | Loss: 0.00009933
Iteration 429/1000 | Loss: 0.00009933
Iteration 430/1000 | Loss: 0.00009933
Iteration 431/1000 | Loss: 0.00009932
Iteration 432/1000 | Loss: 0.00009932
Iteration 433/1000 | Loss: 0.00009932
Iteration 434/1000 | Loss: 0.00009932
Iteration 435/1000 | Loss: 0.00009931
Iteration 436/1000 | Loss: 0.00009931
Iteration 437/1000 | Loss: 0.00009931
Iteration 438/1000 | Loss: 0.00009930
Iteration 439/1000 | Loss: 0.00009930
Iteration 440/1000 | Loss: 0.00009930
Iteration 441/1000 | Loss: 0.00009929
Iteration 442/1000 | Loss: 0.00009929
Iteration 443/1000 | Loss: 0.00009929
Iteration 444/1000 | Loss: 0.00009928
Iteration 445/1000 | Loss: 0.00009928
Iteration 446/1000 | Loss: 0.00009928
Iteration 447/1000 | Loss: 0.00009928
Iteration 448/1000 | Loss: 0.00009927
Iteration 449/1000 | Loss: 0.00009927
Iteration 450/1000 | Loss: 0.00009927
Iteration 451/1000 | Loss: 0.00009927
Iteration 452/1000 | Loss: 0.00009927
Iteration 453/1000 | Loss: 0.00009927
Iteration 454/1000 | Loss: 0.00009927
Iteration 455/1000 | Loss: 0.00009927
Iteration 456/1000 | Loss: 0.00009927
Iteration 457/1000 | Loss: 0.00009927
Iteration 458/1000 | Loss: 0.00009926
Iteration 459/1000 | Loss: 0.00009926
Iteration 460/1000 | Loss: 0.00009925
Iteration 461/1000 | Loss: 0.00009923
Iteration 462/1000 | Loss: 0.00009923
Iteration 463/1000 | Loss: 0.00009923
Iteration 464/1000 | Loss: 0.00009923
Iteration 465/1000 | Loss: 0.00009923
Iteration 466/1000 | Loss: 0.00009923
Iteration 467/1000 | Loss: 0.00009923
Iteration 468/1000 | Loss: 0.00009923
Iteration 469/1000 | Loss: 0.00009923
Iteration 470/1000 | Loss: 0.00009923
Iteration 471/1000 | Loss: 0.00009922
Iteration 472/1000 | Loss: 0.00009922
Iteration 473/1000 | Loss: 0.00009921
Iteration 474/1000 | Loss: 0.00009921
Iteration 475/1000 | Loss: 0.00009920
Iteration 476/1000 | Loss: 0.00009919
Iteration 477/1000 | Loss: 0.00009918
Iteration 478/1000 | Loss: 0.00009918
Iteration 479/1000 | Loss: 0.00009917
Iteration 480/1000 | Loss: 0.00009916
Iteration 481/1000 | Loss: 0.00015460
Iteration 482/1000 | Loss: 0.00011508
Iteration 483/1000 | Loss: 0.00010836
Iteration 484/1000 | Loss: 0.00010587
Iteration 485/1000 | Loss: 0.00010200
Iteration 486/1000 | Loss: 0.00019895
Iteration 487/1000 | Loss: 0.00009963
Iteration 488/1000 | Loss: 0.00009669
Iteration 489/1000 | Loss: 0.00009629
Iteration 490/1000 | Loss: 0.00009591
Iteration 491/1000 | Loss: 0.00009572
Iteration 492/1000 | Loss: 0.00009559
Iteration 493/1000 | Loss: 0.00009558
Iteration 494/1000 | Loss: 0.00009557
Iteration 495/1000 | Loss: 0.00009554
Iteration 496/1000 | Loss: 0.00009554
Iteration 497/1000 | Loss: 0.00009554
Iteration 498/1000 | Loss: 0.00009554
Iteration 499/1000 | Loss: 0.00009554
Iteration 500/1000 | Loss: 0.00009553
Iteration 501/1000 | Loss: 0.00009552
Iteration 502/1000 | Loss: 0.00009551
Iteration 503/1000 | Loss: 0.00009550
Iteration 504/1000 | Loss: 0.00009550
Iteration 505/1000 | Loss: 0.00009550
Iteration 506/1000 | Loss: 0.00009550
Iteration 507/1000 | Loss: 0.00009550
Iteration 508/1000 | Loss: 0.00009550
Iteration 509/1000 | Loss: 0.00009550
Iteration 510/1000 | Loss: 0.00009550
Iteration 511/1000 | Loss: 0.00009550
Iteration 512/1000 | Loss: 0.00009550
Iteration 513/1000 | Loss: 0.00009550
Iteration 514/1000 | Loss: 0.00009549
Iteration 515/1000 | Loss: 0.00009549
Iteration 516/1000 | Loss: 0.00009548
Iteration 517/1000 | Loss: 0.00009548
Iteration 518/1000 | Loss: 0.00009548
Iteration 519/1000 | Loss: 0.00009548
Iteration 520/1000 | Loss: 0.00009548
Iteration 521/1000 | Loss: 0.00009548
Iteration 522/1000 | Loss: 0.00009548
Iteration 523/1000 | Loss: 0.00009548
Iteration 524/1000 | Loss: 0.00009548
Iteration 525/1000 | Loss: 0.00009548
Iteration 526/1000 | Loss: 0.00009548
Iteration 527/1000 | Loss: 0.00009548
Iteration 528/1000 | Loss: 0.00009548
Iteration 529/1000 | Loss: 0.00009548
Iteration 530/1000 | Loss: 0.00009548
Iteration 531/1000 | Loss: 0.00009548
Iteration 532/1000 | Loss: 0.00009547
Iteration 533/1000 | Loss: 0.00009547
Iteration 534/1000 | Loss: 0.00009547
Iteration 535/1000 | Loss: 0.00009547
Iteration 536/1000 | Loss: 0.00009547
Iteration 537/1000 | Loss: 0.00009547
Iteration 538/1000 | Loss: 0.00009546
Iteration 539/1000 | Loss: 0.00009546
Iteration 540/1000 | Loss: 0.00009546
Iteration 541/1000 | Loss: 0.00009546
Iteration 542/1000 | Loss: 0.00009546
Iteration 543/1000 | Loss: 0.00009546
Iteration 544/1000 | Loss: 0.00009546
Iteration 545/1000 | Loss: 0.00009546
Iteration 546/1000 | Loss: 0.00009545
Iteration 547/1000 | Loss: 0.00009545
Iteration 548/1000 | Loss: 0.00009545
Iteration 549/1000 | Loss: 0.00009545
Iteration 550/1000 | Loss: 0.00009545
Iteration 551/1000 | Loss: 0.00009545
Iteration 552/1000 | Loss: 0.00009545
Iteration 553/1000 | Loss: 0.00009545
Iteration 554/1000 | Loss: 0.00009545
Iteration 555/1000 | Loss: 0.00009545
Iteration 556/1000 | Loss: 0.00009544
Iteration 557/1000 | Loss: 0.00009544
Iteration 558/1000 | Loss: 0.00009544
Iteration 559/1000 | Loss: 0.00009544
Iteration 560/1000 | Loss: 0.00009544
Iteration 561/1000 | Loss: 0.00009544
Iteration 562/1000 | Loss: 0.00009544
Iteration 563/1000 | Loss: 0.00009544
Iteration 564/1000 | Loss: 0.00009544
Iteration 565/1000 | Loss: 0.00009544
Iteration 566/1000 | Loss: 0.00009544
Iteration 567/1000 | Loss: 0.00009544
Iteration 568/1000 | Loss: 0.00009544
Iteration 569/1000 | Loss: 0.00009543
Iteration 570/1000 | Loss: 0.00009543
Iteration 571/1000 | Loss: 0.00009543
Iteration 572/1000 | Loss: 0.00009543
Iteration 573/1000 | Loss: 0.00009543
Iteration 574/1000 | Loss: 0.00009543
Iteration 575/1000 | Loss: 0.00009543
Iteration 576/1000 | Loss: 0.00009543
Iteration 577/1000 | Loss: 0.00009543
Iteration 578/1000 | Loss: 0.00009543
Iteration 579/1000 | Loss: 0.00009543
Iteration 580/1000 | Loss: 0.00009543
Iteration 581/1000 | Loss: 0.00009543
Iteration 582/1000 | Loss: 0.00009543
Iteration 583/1000 | Loss: 0.00009542
Iteration 584/1000 | Loss: 0.00009542
Iteration 585/1000 | Loss: 0.00009542
Iteration 586/1000 | Loss: 0.00009542
Iteration 587/1000 | Loss: 0.00009542
Iteration 588/1000 | Loss: 0.00009542
Iteration 589/1000 | Loss: 0.00009542
Iteration 590/1000 | Loss: 0.00009542
Iteration 591/1000 | Loss: 0.00009542
Iteration 592/1000 | Loss: 0.00009542
Iteration 593/1000 | Loss: 0.00009542
Iteration 594/1000 | Loss: 0.00009542
Iteration 595/1000 | Loss: 0.00009542
Iteration 596/1000 | Loss: 0.00009542
Iteration 597/1000 | Loss: 0.00009542
Iteration 598/1000 | Loss: 0.00009542
Iteration 599/1000 | Loss: 0.00009542
Iteration 600/1000 | Loss: 0.00009542
Iteration 601/1000 | Loss: 0.00009542
Iteration 602/1000 | Loss: 0.00009542
Iteration 603/1000 | Loss: 0.00009542
Iteration 604/1000 | Loss: 0.00009542
Iteration 605/1000 | Loss: 0.00009542
Iteration 606/1000 | Loss: 0.00009542
Iteration 607/1000 | Loss: 0.00009542
Iteration 608/1000 | Loss: 0.00009542
Iteration 609/1000 | Loss: 0.00009542
Iteration 610/1000 | Loss: 0.00009542
Iteration 611/1000 | Loss: 0.00009542
Iteration 612/1000 | Loss: 0.00009542
Iteration 613/1000 | Loss: 0.00009542
Iteration 614/1000 | Loss: 0.00009542
Iteration 615/1000 | Loss: 0.00009542
Iteration 616/1000 | Loss: 0.00009542
Iteration 617/1000 | Loss: 0.00009542
Iteration 618/1000 | Loss: 0.00009542
Iteration 619/1000 | Loss: 0.00009542
Iteration 620/1000 | Loss: 0.00009542
Iteration 621/1000 | Loss: 0.00009542
Iteration 622/1000 | Loss: 0.00009542
Iteration 623/1000 | Loss: 0.00009542
Iteration 624/1000 | Loss: 0.00009542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 624. Stopping optimization.
Last 5 losses: [9.541789768263698e-05, 9.541789768263698e-05, 9.541789768263698e-05, 9.541789768263698e-05, 9.541789768263698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.541789768263698e-05

Optimization complete. Final v2v error: 5.632586479187012 mm

Highest mean error: 12.343937873840332 mm for frame 104

Lowest mean error: 3.6578776836395264 mm for frame 118

Saving results

Total time: 611.406004190445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2456/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2456/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097211
Iteration 2/25 | Loss: 0.00228026
Iteration 3/25 | Loss: 0.00167700
Iteration 4/25 | Loss: 0.00155104
Iteration 5/25 | Loss: 0.00148881
Iteration 6/25 | Loss: 0.00141465
Iteration 7/25 | Loss: 0.00136709
Iteration 8/25 | Loss: 0.00129613
Iteration 9/25 | Loss: 0.00125128
Iteration 10/25 | Loss: 0.00124089
Iteration 11/25 | Loss: 0.00123859
Iteration 12/25 | Loss: 0.00124016
Iteration 13/25 | Loss: 0.00122861
Iteration 14/25 | Loss: 0.00122513
Iteration 15/25 | Loss: 0.00122465
Iteration 16/25 | Loss: 0.00122434
Iteration 17/25 | Loss: 0.00122428
Iteration 18/25 | Loss: 0.00122428
Iteration 19/25 | Loss: 0.00122428
Iteration 20/25 | Loss: 0.00122427
Iteration 21/25 | Loss: 0.00122427
Iteration 22/25 | Loss: 0.00122427
Iteration 23/25 | Loss: 0.00122427
Iteration 24/25 | Loss: 0.00122427
Iteration 25/25 | Loss: 0.00122427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36126363
Iteration 2/25 | Loss: 0.00150889
Iteration 3/25 | Loss: 0.00143422
Iteration 4/25 | Loss: 0.00143422
Iteration 5/25 | Loss: 0.00143422
Iteration 6/25 | Loss: 0.00143422
Iteration 7/25 | Loss: 0.00143422
Iteration 8/25 | Loss: 0.00143422
Iteration 9/25 | Loss: 0.00143422
Iteration 10/25 | Loss: 0.00143422
Iteration 11/25 | Loss: 0.00143422
Iteration 12/25 | Loss: 0.00143422
Iteration 13/25 | Loss: 0.00143422
Iteration 14/25 | Loss: 0.00143422
Iteration 15/25 | Loss: 0.00143422
Iteration 16/25 | Loss: 0.00143422
Iteration 17/25 | Loss: 0.00143422
Iteration 18/25 | Loss: 0.00143422
Iteration 19/25 | Loss: 0.00143422
Iteration 20/25 | Loss: 0.00143422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014342190697789192, 0.0014342190697789192, 0.0014342190697789192, 0.0014342190697789192, 0.0014342190697789192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014342190697789192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143422
Iteration 2/1000 | Loss: 0.00022621
Iteration 3/1000 | Loss: 0.00011764
Iteration 4/1000 | Loss: 0.00023501
Iteration 5/1000 | Loss: 0.00010143
Iteration 6/1000 | Loss: 0.00008779
Iteration 7/1000 | Loss: 0.00008140
Iteration 8/1000 | Loss: 0.00011511
Iteration 9/1000 | Loss: 0.00007487
Iteration 10/1000 | Loss: 0.00012024
Iteration 11/1000 | Loss: 0.00255939
Iteration 12/1000 | Loss: 0.00136793
Iteration 13/1000 | Loss: 0.00012944
Iteration 14/1000 | Loss: 0.00082607
Iteration 15/1000 | Loss: 0.00007461
Iteration 16/1000 | Loss: 0.00033183
Iteration 17/1000 | Loss: 0.00039909
Iteration 18/1000 | Loss: 0.00145505
Iteration 19/1000 | Loss: 0.00007501
Iteration 20/1000 | Loss: 0.00015568
Iteration 21/1000 | Loss: 0.00008137
Iteration 22/1000 | Loss: 0.00002835
Iteration 23/1000 | Loss: 0.00002616
Iteration 24/1000 | Loss: 0.00013720
Iteration 25/1000 | Loss: 0.00016010
Iteration 26/1000 | Loss: 0.00018526
Iteration 27/1000 | Loss: 0.00005759
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002112
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001839
Iteration 34/1000 | Loss: 0.00006120
Iteration 35/1000 | Loss: 0.00001824
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001792
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00006299
Iteration 53/1000 | Loss: 0.00006392
Iteration 54/1000 | Loss: 0.00002910
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001769
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001768
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001767
Iteration 82/1000 | Loss: 0.00001767
Iteration 83/1000 | Loss: 0.00001767
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001766
Iteration 91/1000 | Loss: 0.00001766
Iteration 92/1000 | Loss: 0.00001766
Iteration 93/1000 | Loss: 0.00001766
Iteration 94/1000 | Loss: 0.00001765
Iteration 95/1000 | Loss: 0.00001765
Iteration 96/1000 | Loss: 0.00001765
Iteration 97/1000 | Loss: 0.00001765
Iteration 98/1000 | Loss: 0.00001764
Iteration 99/1000 | Loss: 0.00001764
Iteration 100/1000 | Loss: 0.00001764
Iteration 101/1000 | Loss: 0.00001764
Iteration 102/1000 | Loss: 0.00001764
Iteration 103/1000 | Loss: 0.00001763
Iteration 104/1000 | Loss: 0.00001763
Iteration 105/1000 | Loss: 0.00001763
Iteration 106/1000 | Loss: 0.00001763
Iteration 107/1000 | Loss: 0.00001762
Iteration 108/1000 | Loss: 0.00001762
Iteration 109/1000 | Loss: 0.00001762
Iteration 110/1000 | Loss: 0.00001762
Iteration 111/1000 | Loss: 0.00001761
Iteration 112/1000 | Loss: 0.00001761
Iteration 113/1000 | Loss: 0.00001761
Iteration 114/1000 | Loss: 0.00001761
Iteration 115/1000 | Loss: 0.00001761
Iteration 116/1000 | Loss: 0.00001761
Iteration 117/1000 | Loss: 0.00001761
Iteration 118/1000 | Loss: 0.00001761
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001761
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001761
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001761
Iteration 132/1000 | Loss: 0.00001761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.7610445866012014e-05, 1.7610445866012014e-05, 1.7610445866012014e-05, 1.7610445866012014e-05, 1.7610445866012014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7610445866012014e-05

Optimization complete. Final v2v error: 3.4826347827911377 mm

Highest mean error: 9.653613090515137 mm for frame 35

Lowest mean error: 2.919647455215454 mm for frame 153

Saving results

Total time: 97.31771516799927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814600
Iteration 2/25 | Loss: 0.00158199
Iteration 3/25 | Loss: 0.00131968
Iteration 4/25 | Loss: 0.00129876
Iteration 5/25 | Loss: 0.00129580
Iteration 6/25 | Loss: 0.00129562
Iteration 7/25 | Loss: 0.00129562
Iteration 8/25 | Loss: 0.00129562
Iteration 9/25 | Loss: 0.00129562
Iteration 10/25 | Loss: 0.00129562
Iteration 11/25 | Loss: 0.00129562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001295622088946402, 0.001295622088946402, 0.001295622088946402, 0.001295622088946402, 0.001295622088946402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001295622088946402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40668643
Iteration 2/25 | Loss: 0.00066213
Iteration 3/25 | Loss: 0.00066213
Iteration 4/25 | Loss: 0.00066213
Iteration 5/25 | Loss: 0.00066213
Iteration 6/25 | Loss: 0.00066212
Iteration 7/25 | Loss: 0.00066212
Iteration 8/25 | Loss: 0.00066212
Iteration 9/25 | Loss: 0.00066212
Iteration 10/25 | Loss: 0.00066212
Iteration 11/25 | Loss: 0.00066212
Iteration 12/25 | Loss: 0.00066212
Iteration 13/25 | Loss: 0.00066212
Iteration 14/25 | Loss: 0.00066212
Iteration 15/25 | Loss: 0.00066212
Iteration 16/25 | Loss: 0.00066212
Iteration 17/25 | Loss: 0.00066212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006621235515922308, 0.0006621235515922308, 0.0006621235515922308, 0.0006621235515922308, 0.0006621235515922308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006621235515922308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066212
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00002660
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00002301
Iteration 6/1000 | Loss: 0.00002195
Iteration 7/1000 | Loss: 0.00002133
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002060
Iteration 10/1000 | Loss: 0.00002032
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00002020
Iteration 13/1000 | Loss: 0.00001987
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00001940
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001891
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001890
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001888
Iteration 32/1000 | Loss: 0.00001888
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001888
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001887
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001887
Iteration 42/1000 | Loss: 0.00001885
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001882
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001877
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001876
Iteration 68/1000 | Loss: 0.00001876
Iteration 69/1000 | Loss: 0.00001876
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001875
Iteration 77/1000 | Loss: 0.00001875
Iteration 78/1000 | Loss: 0.00001875
Iteration 79/1000 | Loss: 0.00001875
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001874
Iteration 84/1000 | Loss: 0.00001874
Iteration 85/1000 | Loss: 0.00001874
Iteration 86/1000 | Loss: 0.00001874
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001873
Iteration 99/1000 | Loss: 0.00001873
Iteration 100/1000 | Loss: 0.00001873
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001869
Iteration 127/1000 | Loss: 0.00001869
Iteration 128/1000 | Loss: 0.00001869
Iteration 129/1000 | Loss: 0.00001869
Iteration 130/1000 | Loss: 0.00001869
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001869
Iteration 134/1000 | Loss: 0.00001869
Iteration 135/1000 | Loss: 0.00001869
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001869
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001868
Iteration 146/1000 | Loss: 0.00001868
Iteration 147/1000 | Loss: 0.00001868
Iteration 148/1000 | Loss: 0.00001868
Iteration 149/1000 | Loss: 0.00001868
Iteration 150/1000 | Loss: 0.00001868
Iteration 151/1000 | Loss: 0.00001868
Iteration 152/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.868390063464176e-05, 1.868390063464176e-05, 1.868390063464176e-05, 1.868390063464176e-05, 1.868390063464176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.868390063464176e-05

Optimization complete. Final v2v error: 3.681486129760742 mm

Highest mean error: 3.8263847827911377 mm for frame 52

Lowest mean error: 3.4201486110687256 mm for frame 170

Saving results

Total time: 42.95234274864197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340206
Iteration 2/25 | Loss: 0.00131125
Iteration 3/25 | Loss: 0.00121258
Iteration 4/25 | Loss: 0.00120273
Iteration 5/25 | Loss: 0.00119867
Iteration 6/25 | Loss: 0.00119845
Iteration 7/25 | Loss: 0.00119845
Iteration 8/25 | Loss: 0.00119845
Iteration 9/25 | Loss: 0.00119845
Iteration 10/25 | Loss: 0.00119845
Iteration 11/25 | Loss: 0.00119845
Iteration 12/25 | Loss: 0.00119845
Iteration 13/25 | Loss: 0.00119845
Iteration 14/25 | Loss: 0.00119845
Iteration 15/25 | Loss: 0.00119845
Iteration 16/25 | Loss: 0.00119845
Iteration 17/25 | Loss: 0.00119845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011984547600150108, 0.0011984547600150108, 0.0011984547600150108, 0.0011984547600150108, 0.0011984547600150108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011984547600150108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41076076
Iteration 2/25 | Loss: 0.00076911
Iteration 3/25 | Loss: 0.00076911
Iteration 4/25 | Loss: 0.00076911
Iteration 5/25 | Loss: 0.00076911
Iteration 6/25 | Loss: 0.00076911
Iteration 7/25 | Loss: 0.00076911
Iteration 8/25 | Loss: 0.00076911
Iteration 9/25 | Loss: 0.00076911
Iteration 10/25 | Loss: 0.00076911
Iteration 11/25 | Loss: 0.00076911
Iteration 12/25 | Loss: 0.00076911
Iteration 13/25 | Loss: 0.00076911
Iteration 14/25 | Loss: 0.00076911
Iteration 15/25 | Loss: 0.00076911
Iteration 16/25 | Loss: 0.00076911
Iteration 17/25 | Loss: 0.00076911
Iteration 18/25 | Loss: 0.00076911
Iteration 19/25 | Loss: 0.00076911
Iteration 20/25 | Loss: 0.00076911
Iteration 21/25 | Loss: 0.00076911
Iteration 22/25 | Loss: 0.00076911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007691080681979656, 0.0007691080681979656, 0.0007691080681979656, 0.0007691080681979656, 0.0007691080681979656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007691080681979656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076911
Iteration 2/1000 | Loss: 0.00003436
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001860
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001633
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001420
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001400
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001392
Iteration 32/1000 | Loss: 0.00001392
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001386
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001384
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001381
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001375
Iteration 63/1000 | Loss: 0.00001375
Iteration 64/1000 | Loss: 0.00001375
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001374
Iteration 67/1000 | Loss: 0.00001374
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001373
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001372
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001361
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001360
Iteration 132/1000 | Loss: 0.00001360
Iteration 133/1000 | Loss: 0.00001360
Iteration 134/1000 | Loss: 0.00001360
Iteration 135/1000 | Loss: 0.00001360
Iteration 136/1000 | Loss: 0.00001360
Iteration 137/1000 | Loss: 0.00001360
Iteration 138/1000 | Loss: 0.00001360
Iteration 139/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.3600542843050789e-05, 1.3600542843050789e-05, 1.3600542843050789e-05, 1.3600542843050789e-05, 1.3600542843050789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3600542843050789e-05

Optimization complete. Final v2v error: 3.1794514656066895 mm

Highest mean error: 3.4647836685180664 mm for frame 115

Lowest mean error: 2.9331400394439697 mm for frame 44

Saving results

Total time: 42.7066867351532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00748681
Iteration 2/25 | Loss: 0.00162431
Iteration 3/25 | Loss: 0.00141553
Iteration 4/25 | Loss: 0.00130803
Iteration 5/25 | Loss: 0.00130236
Iteration 6/25 | Loss: 0.00127941
Iteration 7/25 | Loss: 0.00126542
Iteration 8/25 | Loss: 0.00125771
Iteration 9/25 | Loss: 0.00125263
Iteration 10/25 | Loss: 0.00125075
Iteration 11/25 | Loss: 0.00125274
Iteration 12/25 | Loss: 0.00124987
Iteration 13/25 | Loss: 0.00124977
Iteration 14/25 | Loss: 0.00124977
Iteration 15/25 | Loss: 0.00124977
Iteration 16/25 | Loss: 0.00124977
Iteration 17/25 | Loss: 0.00124977
Iteration 18/25 | Loss: 0.00124977
Iteration 19/25 | Loss: 0.00124976
Iteration 20/25 | Loss: 0.00124976
Iteration 21/25 | Loss: 0.00124976
Iteration 22/25 | Loss: 0.00124976
Iteration 23/25 | Loss: 0.00124976
Iteration 24/25 | Loss: 0.00124976
Iteration 25/25 | Loss: 0.00124976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.76604080
Iteration 2/25 | Loss: 0.00083228
Iteration 3/25 | Loss: 0.00083225
Iteration 4/25 | Loss: 0.00083225
Iteration 5/25 | Loss: 0.00083225
Iteration 6/25 | Loss: 0.00083225
Iteration 7/25 | Loss: 0.00083225
Iteration 8/25 | Loss: 0.00083225
Iteration 9/25 | Loss: 0.00083225
Iteration 10/25 | Loss: 0.00083225
Iteration 11/25 | Loss: 0.00083225
Iteration 12/25 | Loss: 0.00083225
Iteration 13/25 | Loss: 0.00083225
Iteration 14/25 | Loss: 0.00083225
Iteration 15/25 | Loss: 0.00083225
Iteration 16/25 | Loss: 0.00083225
Iteration 17/25 | Loss: 0.00083225
Iteration 18/25 | Loss: 0.00083225
Iteration 19/25 | Loss: 0.00083225
Iteration 20/25 | Loss: 0.00083225
Iteration 21/25 | Loss: 0.00083225
Iteration 22/25 | Loss: 0.00083225
Iteration 23/25 | Loss: 0.00083225
Iteration 24/25 | Loss: 0.00083225
Iteration 25/25 | Loss: 0.00083225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083225
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00006363
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001813
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00027423
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001717
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001608
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001559
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001529
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001512
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001509
Iteration 55/1000 | Loss: 0.00001508
Iteration 56/1000 | Loss: 0.00001508
Iteration 57/1000 | Loss: 0.00001508
Iteration 58/1000 | Loss: 0.00001508
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001506
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001505
Iteration 73/1000 | Loss: 0.00001505
Iteration 74/1000 | Loss: 0.00001505
Iteration 75/1000 | Loss: 0.00001505
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001505
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001504
Iteration 81/1000 | Loss: 0.00001504
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001503
Iteration 86/1000 | Loss: 0.00001503
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001502
Iteration 91/1000 | Loss: 0.00001502
Iteration 92/1000 | Loss: 0.00001502
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001502
Iteration 105/1000 | Loss: 0.00001502
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001501
Iteration 113/1000 | Loss: 0.00001501
Iteration 114/1000 | Loss: 0.00001501
Iteration 115/1000 | Loss: 0.00001501
Iteration 116/1000 | Loss: 0.00001501
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001500
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001500
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001500
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001500
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001499
Iteration 133/1000 | Loss: 0.00001499
Iteration 134/1000 | Loss: 0.00001499
Iteration 135/1000 | Loss: 0.00001499
Iteration 136/1000 | Loss: 0.00001499
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001498
Iteration 157/1000 | Loss: 0.00001498
Iteration 158/1000 | Loss: 0.00001498
Iteration 159/1000 | Loss: 0.00001498
Iteration 160/1000 | Loss: 0.00001498
Iteration 161/1000 | Loss: 0.00001498
Iteration 162/1000 | Loss: 0.00001498
Iteration 163/1000 | Loss: 0.00001498
Iteration 164/1000 | Loss: 0.00001498
Iteration 165/1000 | Loss: 0.00001498
Iteration 166/1000 | Loss: 0.00001498
Iteration 167/1000 | Loss: 0.00001498
Iteration 168/1000 | Loss: 0.00001498
Iteration 169/1000 | Loss: 0.00001498
Iteration 170/1000 | Loss: 0.00001498
Iteration 171/1000 | Loss: 0.00001498
Iteration 172/1000 | Loss: 0.00001498
Iteration 173/1000 | Loss: 0.00001498
Iteration 174/1000 | Loss: 0.00001498
Iteration 175/1000 | Loss: 0.00001498
Iteration 176/1000 | Loss: 0.00001498
Iteration 177/1000 | Loss: 0.00001498
Iteration 178/1000 | Loss: 0.00001498
Iteration 179/1000 | Loss: 0.00001498
Iteration 180/1000 | Loss: 0.00001498
Iteration 181/1000 | Loss: 0.00001498
Iteration 182/1000 | Loss: 0.00001498
Iteration 183/1000 | Loss: 0.00001498
Iteration 184/1000 | Loss: 0.00001498
Iteration 185/1000 | Loss: 0.00001498
Iteration 186/1000 | Loss: 0.00001498
Iteration 187/1000 | Loss: 0.00001498
Iteration 188/1000 | Loss: 0.00001498
Iteration 189/1000 | Loss: 0.00001498
Iteration 190/1000 | Loss: 0.00001498
Iteration 191/1000 | Loss: 0.00001498
Iteration 192/1000 | Loss: 0.00001498
Iteration 193/1000 | Loss: 0.00001498
Iteration 194/1000 | Loss: 0.00001498
Iteration 195/1000 | Loss: 0.00001498
Iteration 196/1000 | Loss: 0.00001498
Iteration 197/1000 | Loss: 0.00001498
Iteration 198/1000 | Loss: 0.00001498
Iteration 199/1000 | Loss: 0.00001498
Iteration 200/1000 | Loss: 0.00001498
Iteration 201/1000 | Loss: 0.00001498
Iteration 202/1000 | Loss: 0.00001498
Iteration 203/1000 | Loss: 0.00001498
Iteration 204/1000 | Loss: 0.00001498
Iteration 205/1000 | Loss: 0.00001498
Iteration 206/1000 | Loss: 0.00001498
Iteration 207/1000 | Loss: 0.00001498
Iteration 208/1000 | Loss: 0.00001498
Iteration 209/1000 | Loss: 0.00001498
Iteration 210/1000 | Loss: 0.00001498
Iteration 211/1000 | Loss: 0.00001498
Iteration 212/1000 | Loss: 0.00001498
Iteration 213/1000 | Loss: 0.00001498
Iteration 214/1000 | Loss: 0.00001498
Iteration 215/1000 | Loss: 0.00001498
Iteration 216/1000 | Loss: 0.00001498
Iteration 217/1000 | Loss: 0.00001498
Iteration 218/1000 | Loss: 0.00001498
Iteration 219/1000 | Loss: 0.00001498
Iteration 220/1000 | Loss: 0.00001498
Iteration 221/1000 | Loss: 0.00001498
Iteration 222/1000 | Loss: 0.00001498
Iteration 223/1000 | Loss: 0.00001498
Iteration 224/1000 | Loss: 0.00001498
Iteration 225/1000 | Loss: 0.00001498
Iteration 226/1000 | Loss: 0.00001498
Iteration 227/1000 | Loss: 0.00001498
Iteration 228/1000 | Loss: 0.00001498
Iteration 229/1000 | Loss: 0.00001498
Iteration 230/1000 | Loss: 0.00001498
Iteration 231/1000 | Loss: 0.00001498
Iteration 232/1000 | Loss: 0.00001498
Iteration 233/1000 | Loss: 0.00001498
Iteration 234/1000 | Loss: 0.00001498
Iteration 235/1000 | Loss: 0.00001498
Iteration 236/1000 | Loss: 0.00001498
Iteration 237/1000 | Loss: 0.00001498
Iteration 238/1000 | Loss: 0.00001498
Iteration 239/1000 | Loss: 0.00001498
Iteration 240/1000 | Loss: 0.00001498
Iteration 241/1000 | Loss: 0.00001498
Iteration 242/1000 | Loss: 0.00001498
Iteration 243/1000 | Loss: 0.00001498
Iteration 244/1000 | Loss: 0.00001498
Iteration 245/1000 | Loss: 0.00001498
Iteration 246/1000 | Loss: 0.00001498
Iteration 247/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.4981264030211605e-05, 1.4981264030211605e-05, 1.4981264030211605e-05, 1.4981264030211605e-05, 1.4981264030211605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4981264030211605e-05

Optimization complete. Final v2v error: 3.288724660873413 mm

Highest mean error: 3.819214344024658 mm for frame 195

Lowest mean error: 2.8910176753997803 mm for frame 199

Saving results

Total time: 71.16555166244507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409946
Iteration 2/25 | Loss: 0.00135134
Iteration 3/25 | Loss: 0.00124886
Iteration 4/25 | Loss: 0.00123894
Iteration 5/25 | Loss: 0.00123506
Iteration 6/25 | Loss: 0.00123427
Iteration 7/25 | Loss: 0.00123427
Iteration 8/25 | Loss: 0.00123427
Iteration 9/25 | Loss: 0.00123427
Iteration 10/25 | Loss: 0.00123427
Iteration 11/25 | Loss: 0.00123427
Iteration 12/25 | Loss: 0.00123427
Iteration 13/25 | Loss: 0.00123427
Iteration 14/25 | Loss: 0.00123427
Iteration 15/25 | Loss: 0.00123427
Iteration 16/25 | Loss: 0.00123427
Iteration 17/25 | Loss: 0.00123427
Iteration 18/25 | Loss: 0.00123427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001234270865097642, 0.001234270865097642, 0.001234270865097642, 0.001234270865097642, 0.001234270865097642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001234270865097642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48838258
Iteration 2/25 | Loss: 0.00078492
Iteration 3/25 | Loss: 0.00078491
Iteration 4/25 | Loss: 0.00078491
Iteration 5/25 | Loss: 0.00078491
Iteration 6/25 | Loss: 0.00078491
Iteration 7/25 | Loss: 0.00078491
Iteration 8/25 | Loss: 0.00078491
Iteration 9/25 | Loss: 0.00078491
Iteration 10/25 | Loss: 0.00078491
Iteration 11/25 | Loss: 0.00078491
Iteration 12/25 | Loss: 0.00078491
Iteration 13/25 | Loss: 0.00078491
Iteration 14/25 | Loss: 0.00078491
Iteration 15/25 | Loss: 0.00078491
Iteration 16/25 | Loss: 0.00078491
Iteration 17/25 | Loss: 0.00078491
Iteration 18/25 | Loss: 0.00078491
Iteration 19/25 | Loss: 0.00078491
Iteration 20/25 | Loss: 0.00078491
Iteration 21/25 | Loss: 0.00078491
Iteration 22/25 | Loss: 0.00078491
Iteration 23/25 | Loss: 0.00078491
Iteration 24/25 | Loss: 0.00078491
Iteration 25/25 | Loss: 0.00078491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078491
Iteration 2/1000 | Loss: 0.00004195
Iteration 3/1000 | Loss: 0.00002583
Iteration 4/1000 | Loss: 0.00002085
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001715
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001585
Iteration 12/1000 | Loss: 0.00001564
Iteration 13/1000 | Loss: 0.00001558
Iteration 14/1000 | Loss: 0.00001554
Iteration 15/1000 | Loss: 0.00001548
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001540
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001537
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001530
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001528
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001512
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001510
Iteration 52/1000 | Loss: 0.00001510
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001509
Iteration 55/1000 | Loss: 0.00001509
Iteration 56/1000 | Loss: 0.00001509
Iteration 57/1000 | Loss: 0.00001509
Iteration 58/1000 | Loss: 0.00001509
Iteration 59/1000 | Loss: 0.00001509
Iteration 60/1000 | Loss: 0.00001509
Iteration 61/1000 | Loss: 0.00001509
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001509
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001506
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001506
Iteration 83/1000 | Loss: 0.00001506
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001503
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001503
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001502
Iteration 105/1000 | Loss: 0.00001502
Iteration 106/1000 | Loss: 0.00001502
Iteration 107/1000 | Loss: 0.00001502
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001500
Iteration 112/1000 | Loss: 0.00001500
Iteration 113/1000 | Loss: 0.00001500
Iteration 114/1000 | Loss: 0.00001500
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001499
Iteration 119/1000 | Loss: 0.00001499
Iteration 120/1000 | Loss: 0.00001499
Iteration 121/1000 | Loss: 0.00001499
Iteration 122/1000 | Loss: 0.00001499
Iteration 123/1000 | Loss: 0.00001499
Iteration 124/1000 | Loss: 0.00001499
Iteration 125/1000 | Loss: 0.00001499
Iteration 126/1000 | Loss: 0.00001499
Iteration 127/1000 | Loss: 0.00001499
Iteration 128/1000 | Loss: 0.00001498
Iteration 129/1000 | Loss: 0.00001498
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001497
Iteration 136/1000 | Loss: 0.00001497
Iteration 137/1000 | Loss: 0.00001497
Iteration 138/1000 | Loss: 0.00001497
Iteration 139/1000 | Loss: 0.00001497
Iteration 140/1000 | Loss: 0.00001497
Iteration 141/1000 | Loss: 0.00001497
Iteration 142/1000 | Loss: 0.00001497
Iteration 143/1000 | Loss: 0.00001496
Iteration 144/1000 | Loss: 0.00001496
Iteration 145/1000 | Loss: 0.00001496
Iteration 146/1000 | Loss: 0.00001496
Iteration 147/1000 | Loss: 0.00001496
Iteration 148/1000 | Loss: 0.00001496
Iteration 149/1000 | Loss: 0.00001496
Iteration 150/1000 | Loss: 0.00001496
Iteration 151/1000 | Loss: 0.00001496
Iteration 152/1000 | Loss: 0.00001496
Iteration 153/1000 | Loss: 0.00001496
Iteration 154/1000 | Loss: 0.00001496
Iteration 155/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4959284271753859e-05, 1.4959284271753859e-05, 1.4959284271753859e-05, 1.4959284271753859e-05, 1.4959284271753859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4959284271753859e-05

Optimization complete. Final v2v error: 3.297447919845581 mm

Highest mean error: 4.011343002319336 mm for frame 107

Lowest mean error: 2.8052151203155518 mm for frame 160

Saving results

Total time: 38.774110317230225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708506
Iteration 2/25 | Loss: 0.00135474
Iteration 3/25 | Loss: 0.00127444
Iteration 4/25 | Loss: 0.00125885
Iteration 5/25 | Loss: 0.00125272
Iteration 6/25 | Loss: 0.00125167
Iteration 7/25 | Loss: 0.00125167
Iteration 8/25 | Loss: 0.00125167
Iteration 9/25 | Loss: 0.00125167
Iteration 10/25 | Loss: 0.00125167
Iteration 11/25 | Loss: 0.00125167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001251671463251114, 0.001251671463251114, 0.001251671463251114, 0.001251671463251114, 0.001251671463251114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001251671463251114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.27964020
Iteration 2/25 | Loss: 0.00080683
Iteration 3/25 | Loss: 0.00080682
Iteration 4/25 | Loss: 0.00080682
Iteration 5/25 | Loss: 0.00080682
Iteration 6/25 | Loss: 0.00080682
Iteration 7/25 | Loss: 0.00080682
Iteration 8/25 | Loss: 0.00080682
Iteration 9/25 | Loss: 0.00080682
Iteration 10/25 | Loss: 0.00080682
Iteration 11/25 | Loss: 0.00080682
Iteration 12/25 | Loss: 0.00080682
Iteration 13/25 | Loss: 0.00080682
Iteration 14/25 | Loss: 0.00080682
Iteration 15/25 | Loss: 0.00080682
Iteration 16/25 | Loss: 0.00080682
Iteration 17/25 | Loss: 0.00080682
Iteration 18/25 | Loss: 0.00080682
Iteration 19/25 | Loss: 0.00080682
Iteration 20/25 | Loss: 0.00080682
Iteration 21/25 | Loss: 0.00080682
Iteration 22/25 | Loss: 0.00080682
Iteration 23/25 | Loss: 0.00080682
Iteration 24/25 | Loss: 0.00080682
Iteration 25/25 | Loss: 0.00080682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080682
Iteration 2/1000 | Loss: 0.00003304
Iteration 3/1000 | Loss: 0.00002176
Iteration 4/1000 | Loss: 0.00001947
Iteration 5/1000 | Loss: 0.00001848
Iteration 6/1000 | Loss: 0.00001773
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001634
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001599
Iteration 26/1000 | Loss: 0.00001595
Iteration 27/1000 | Loss: 0.00001594
Iteration 28/1000 | Loss: 0.00001594
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001592
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001591
Iteration 33/1000 | Loss: 0.00001590
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001589
Iteration 36/1000 | Loss: 0.00001589
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001583
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001579
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001578
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001577
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001576
Iteration 54/1000 | Loss: 0.00001576
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001569
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001568
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001557
Iteration 109/1000 | Loss: 0.00001557
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Iteration 114/1000 | Loss: 0.00001556
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001555
Iteration 125/1000 | Loss: 0.00001555
Iteration 126/1000 | Loss: 0.00001555
Iteration 127/1000 | Loss: 0.00001555
Iteration 128/1000 | Loss: 0.00001555
Iteration 129/1000 | Loss: 0.00001555
Iteration 130/1000 | Loss: 0.00001555
Iteration 131/1000 | Loss: 0.00001555
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001555
Iteration 135/1000 | Loss: 0.00001555
Iteration 136/1000 | Loss: 0.00001555
Iteration 137/1000 | Loss: 0.00001555
Iteration 138/1000 | Loss: 0.00001555
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001555
Iteration 142/1000 | Loss: 0.00001555
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00001555
Iteration 145/1000 | Loss: 0.00001555
Iteration 146/1000 | Loss: 0.00001555
Iteration 147/1000 | Loss: 0.00001555
Iteration 148/1000 | Loss: 0.00001555
Iteration 149/1000 | Loss: 0.00001555
Iteration 150/1000 | Loss: 0.00001555
Iteration 151/1000 | Loss: 0.00001555
Iteration 152/1000 | Loss: 0.00001555
Iteration 153/1000 | Loss: 0.00001555
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001555
Iteration 156/1000 | Loss: 0.00001555
Iteration 157/1000 | Loss: 0.00001555
Iteration 158/1000 | Loss: 0.00001555
Iteration 159/1000 | Loss: 0.00001555
Iteration 160/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.5549641830148175e-05, 1.5549641830148175e-05, 1.5549641830148175e-05, 1.5549641830148175e-05, 1.5549641830148175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5549641830148175e-05

Optimization complete. Final v2v error: 3.388979196548462 mm

Highest mean error: 3.626234292984009 mm for frame 97

Lowest mean error: 3.1504065990448 mm for frame 0

Saving results

Total time: 38.30159330368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876204
Iteration 2/25 | Loss: 0.00154114
Iteration 3/25 | Loss: 0.00135138
Iteration 4/25 | Loss: 0.00132101
Iteration 5/25 | Loss: 0.00131328
Iteration 6/25 | Loss: 0.00131218
Iteration 7/25 | Loss: 0.00131218
Iteration 8/25 | Loss: 0.00131218
Iteration 9/25 | Loss: 0.00131218
Iteration 10/25 | Loss: 0.00131218
Iteration 11/25 | Loss: 0.00131218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013121769297868013, 0.0013121769297868013, 0.0013121769297868013, 0.0013121769297868013, 0.0013121769297868013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013121769297868013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25965917
Iteration 2/25 | Loss: 0.00079074
Iteration 3/25 | Loss: 0.00079074
Iteration 4/25 | Loss: 0.00079074
Iteration 5/25 | Loss: 0.00079074
Iteration 6/25 | Loss: 0.00079074
Iteration 7/25 | Loss: 0.00079074
Iteration 8/25 | Loss: 0.00079074
Iteration 9/25 | Loss: 0.00079074
Iteration 10/25 | Loss: 0.00079074
Iteration 11/25 | Loss: 0.00079074
Iteration 12/25 | Loss: 0.00079074
Iteration 13/25 | Loss: 0.00079074
Iteration 14/25 | Loss: 0.00079074
Iteration 15/25 | Loss: 0.00079074
Iteration 16/25 | Loss: 0.00079074
Iteration 17/25 | Loss: 0.00079074
Iteration 18/25 | Loss: 0.00079074
Iteration 19/25 | Loss: 0.00079074
Iteration 20/25 | Loss: 0.00079074
Iteration 21/25 | Loss: 0.00079074
Iteration 22/25 | Loss: 0.00079074
Iteration 23/25 | Loss: 0.00079074
Iteration 24/25 | Loss: 0.00079074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007907413528300822, 0.0007907413528300822, 0.0007907413528300822, 0.0007907413528300822, 0.0007907413528300822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007907413528300822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079074
Iteration 2/1000 | Loss: 0.00006050
Iteration 3/1000 | Loss: 0.00004124
Iteration 4/1000 | Loss: 0.00003421
Iteration 5/1000 | Loss: 0.00003190
Iteration 6/1000 | Loss: 0.00003024
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002763
Iteration 9/1000 | Loss: 0.00002699
Iteration 10/1000 | Loss: 0.00002656
Iteration 11/1000 | Loss: 0.00002625
Iteration 12/1000 | Loss: 0.00002599
Iteration 13/1000 | Loss: 0.00002576
Iteration 14/1000 | Loss: 0.00002558
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002542
Iteration 17/1000 | Loss: 0.00002541
Iteration 18/1000 | Loss: 0.00002531
Iteration 19/1000 | Loss: 0.00002530
Iteration 20/1000 | Loss: 0.00002528
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002527
Iteration 23/1000 | Loss: 0.00002524
Iteration 24/1000 | Loss: 0.00002524
Iteration 25/1000 | Loss: 0.00002523
Iteration 26/1000 | Loss: 0.00002522
Iteration 27/1000 | Loss: 0.00002522
Iteration 28/1000 | Loss: 0.00002521
Iteration 29/1000 | Loss: 0.00002521
Iteration 30/1000 | Loss: 0.00002521
Iteration 31/1000 | Loss: 0.00002521
Iteration 32/1000 | Loss: 0.00002520
Iteration 33/1000 | Loss: 0.00002520
Iteration 34/1000 | Loss: 0.00002520
Iteration 35/1000 | Loss: 0.00002520
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002519
Iteration 38/1000 | Loss: 0.00002519
Iteration 39/1000 | Loss: 0.00002518
Iteration 40/1000 | Loss: 0.00002518
Iteration 41/1000 | Loss: 0.00002518
Iteration 42/1000 | Loss: 0.00002517
Iteration 43/1000 | Loss: 0.00002517
Iteration 44/1000 | Loss: 0.00002517
Iteration 45/1000 | Loss: 0.00002517
Iteration 46/1000 | Loss: 0.00002516
Iteration 47/1000 | Loss: 0.00002516
Iteration 48/1000 | Loss: 0.00002515
Iteration 49/1000 | Loss: 0.00002515
Iteration 50/1000 | Loss: 0.00002515
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002513
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00002512
Iteration 56/1000 | Loss: 0.00002512
Iteration 57/1000 | Loss: 0.00002512
Iteration 58/1000 | Loss: 0.00002511
Iteration 59/1000 | Loss: 0.00002511
Iteration 60/1000 | Loss: 0.00002511
Iteration 61/1000 | Loss: 0.00002510
Iteration 62/1000 | Loss: 0.00002510
Iteration 63/1000 | Loss: 0.00002510
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002509
Iteration 67/1000 | Loss: 0.00002509
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002508
Iteration 70/1000 | Loss: 0.00002508
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002508
Iteration 73/1000 | Loss: 0.00002508
Iteration 74/1000 | Loss: 0.00002507
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002506
Iteration 77/1000 | Loss: 0.00002506
Iteration 78/1000 | Loss: 0.00002505
Iteration 79/1000 | Loss: 0.00002505
Iteration 80/1000 | Loss: 0.00002505
Iteration 81/1000 | Loss: 0.00002504
Iteration 82/1000 | Loss: 0.00002504
Iteration 83/1000 | Loss: 0.00002504
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002503
Iteration 87/1000 | Loss: 0.00002503
Iteration 88/1000 | Loss: 0.00002503
Iteration 89/1000 | Loss: 0.00002503
Iteration 90/1000 | Loss: 0.00002503
Iteration 91/1000 | Loss: 0.00002503
Iteration 92/1000 | Loss: 0.00002502
Iteration 93/1000 | Loss: 0.00002502
Iteration 94/1000 | Loss: 0.00002502
Iteration 95/1000 | Loss: 0.00002502
Iteration 96/1000 | Loss: 0.00002502
Iteration 97/1000 | Loss: 0.00002502
Iteration 98/1000 | Loss: 0.00002501
Iteration 99/1000 | Loss: 0.00002501
Iteration 100/1000 | Loss: 0.00002501
Iteration 101/1000 | Loss: 0.00002501
Iteration 102/1000 | Loss: 0.00002501
Iteration 103/1000 | Loss: 0.00002501
Iteration 104/1000 | Loss: 0.00002501
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002500
Iteration 107/1000 | Loss: 0.00002500
Iteration 108/1000 | Loss: 0.00002500
Iteration 109/1000 | Loss: 0.00002500
Iteration 110/1000 | Loss: 0.00002500
Iteration 111/1000 | Loss: 0.00002500
Iteration 112/1000 | Loss: 0.00002500
Iteration 113/1000 | Loss: 0.00002500
Iteration 114/1000 | Loss: 0.00002499
Iteration 115/1000 | Loss: 0.00002499
Iteration 116/1000 | Loss: 0.00002499
Iteration 117/1000 | Loss: 0.00002499
Iteration 118/1000 | Loss: 0.00002499
Iteration 119/1000 | Loss: 0.00002499
Iteration 120/1000 | Loss: 0.00002499
Iteration 121/1000 | Loss: 0.00002499
Iteration 122/1000 | Loss: 0.00002499
Iteration 123/1000 | Loss: 0.00002499
Iteration 124/1000 | Loss: 0.00002499
Iteration 125/1000 | Loss: 0.00002499
Iteration 126/1000 | Loss: 0.00002499
Iteration 127/1000 | Loss: 0.00002499
Iteration 128/1000 | Loss: 0.00002499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.498641151760239e-05, 2.498641151760239e-05, 2.498641151760239e-05, 2.498641151760239e-05, 2.498641151760239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.498641151760239e-05

Optimization complete. Final v2v error: 4.133777618408203 mm

Highest mean error: 4.484072208404541 mm for frame 190

Lowest mean error: 3.348473072052002 mm for frame 237

Saving results

Total time: 46.93252396583557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982908
Iteration 2/25 | Loss: 0.00215056
Iteration 3/25 | Loss: 0.00180945
Iteration 4/25 | Loss: 0.00166019
Iteration 5/25 | Loss: 0.00180971
Iteration 6/25 | Loss: 0.00165279
Iteration 7/25 | Loss: 0.00148203
Iteration 8/25 | Loss: 0.00141722
Iteration 9/25 | Loss: 0.00150461
Iteration 10/25 | Loss: 0.00146510
Iteration 11/25 | Loss: 0.00138085
Iteration 12/25 | Loss: 0.00132483
Iteration 13/25 | Loss: 0.00129725
Iteration 14/25 | Loss: 0.00127984
Iteration 15/25 | Loss: 0.00127871
Iteration 16/25 | Loss: 0.00127986
Iteration 17/25 | Loss: 0.00127886
Iteration 18/25 | Loss: 0.00127586
Iteration 19/25 | Loss: 0.00127215
Iteration 20/25 | Loss: 0.00127039
Iteration 21/25 | Loss: 0.00126942
Iteration 22/25 | Loss: 0.00127328
Iteration 23/25 | Loss: 0.00127242
Iteration 24/25 | Loss: 0.00127061
Iteration 25/25 | Loss: 0.00126948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81201220
Iteration 2/25 | Loss: 0.00125192
Iteration 3/25 | Loss: 0.00125192
Iteration 4/25 | Loss: 0.00125192
Iteration 5/25 | Loss: 0.00125192
Iteration 6/25 | Loss: 0.00125192
Iteration 7/25 | Loss: 0.00125191
Iteration 8/25 | Loss: 0.00125191
Iteration 9/25 | Loss: 0.00125191
Iteration 10/25 | Loss: 0.00125191
Iteration 11/25 | Loss: 0.00125191
Iteration 12/25 | Loss: 0.00125191
Iteration 13/25 | Loss: 0.00125191
Iteration 14/25 | Loss: 0.00125191
Iteration 15/25 | Loss: 0.00125191
Iteration 16/25 | Loss: 0.00125191
Iteration 17/25 | Loss: 0.00125191
Iteration 18/25 | Loss: 0.00125191
Iteration 19/25 | Loss: 0.00125191
Iteration 20/25 | Loss: 0.00125191
Iteration 21/25 | Loss: 0.00125191
Iteration 22/25 | Loss: 0.00125191
Iteration 23/25 | Loss: 0.00125191
Iteration 24/25 | Loss: 0.00125191
Iteration 25/25 | Loss: 0.00125191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125191
Iteration 2/1000 | Loss: 0.00011678
Iteration 3/1000 | Loss: 0.00007049
Iteration 4/1000 | Loss: 0.00209434
Iteration 5/1000 | Loss: 0.00067511
Iteration 6/1000 | Loss: 0.00152205
Iteration 7/1000 | Loss: 0.00146035
Iteration 8/1000 | Loss: 0.00006659
Iteration 9/1000 | Loss: 0.00130031
Iteration 10/1000 | Loss: 0.00073327
Iteration 11/1000 | Loss: 0.00005140
Iteration 12/1000 | Loss: 0.00263560
Iteration 13/1000 | Loss: 0.00044041
Iteration 14/1000 | Loss: 0.00152363
Iteration 15/1000 | Loss: 0.00018630
Iteration 16/1000 | Loss: 0.00016857
Iteration 17/1000 | Loss: 0.00032147
Iteration 18/1000 | Loss: 0.00004806
Iteration 19/1000 | Loss: 0.00003291
Iteration 20/1000 | Loss: 0.00130338
Iteration 21/1000 | Loss: 0.00052454
Iteration 22/1000 | Loss: 0.00002888
Iteration 23/1000 | Loss: 0.00101133
Iteration 24/1000 | Loss: 0.00065949
Iteration 25/1000 | Loss: 0.00005182
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00080442
Iteration 28/1000 | Loss: 0.00122691
Iteration 29/1000 | Loss: 0.00004526
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00001860
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001604
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001476
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001461
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001460
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001459
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001457
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001455
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001455
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001454
Iteration 85/1000 | Loss: 0.00001454
Iteration 86/1000 | Loss: 0.00001453
Iteration 87/1000 | Loss: 0.00001453
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001452
Iteration 90/1000 | Loss: 0.00001452
Iteration 91/1000 | Loss: 0.00001452
Iteration 92/1000 | Loss: 0.00001452
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001449
Iteration 105/1000 | Loss: 0.00001449
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001448
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001446
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001445
Iteration 124/1000 | Loss: 0.00001445
Iteration 125/1000 | Loss: 0.00001445
Iteration 126/1000 | Loss: 0.00001445
Iteration 127/1000 | Loss: 0.00001445
Iteration 128/1000 | Loss: 0.00001445
Iteration 129/1000 | Loss: 0.00001444
Iteration 130/1000 | Loss: 0.00001444
Iteration 131/1000 | Loss: 0.00001444
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001442
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001442
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001441
Iteration 145/1000 | Loss: 0.00001441
Iteration 146/1000 | Loss: 0.00001441
Iteration 147/1000 | Loss: 0.00001441
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001438
Iteration 165/1000 | Loss: 0.00001438
Iteration 166/1000 | Loss: 0.00001438
Iteration 167/1000 | Loss: 0.00001438
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Iteration 170/1000 | Loss: 0.00001437
Iteration 171/1000 | Loss: 0.00001437
Iteration 172/1000 | Loss: 0.00001437
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001436
Iteration 175/1000 | Loss: 0.00001436
Iteration 176/1000 | Loss: 0.00001436
Iteration 177/1000 | Loss: 0.00001436
Iteration 178/1000 | Loss: 0.00001436
Iteration 179/1000 | Loss: 0.00001436
Iteration 180/1000 | Loss: 0.00001436
Iteration 181/1000 | Loss: 0.00001436
Iteration 182/1000 | Loss: 0.00001436
Iteration 183/1000 | Loss: 0.00001436
Iteration 184/1000 | Loss: 0.00001436
Iteration 185/1000 | Loss: 0.00001436
Iteration 186/1000 | Loss: 0.00001436
Iteration 187/1000 | Loss: 0.00001436
Iteration 188/1000 | Loss: 0.00001436
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001435
Iteration 191/1000 | Loss: 0.00001435
Iteration 192/1000 | Loss: 0.00001435
Iteration 193/1000 | Loss: 0.00001435
Iteration 194/1000 | Loss: 0.00001435
Iteration 195/1000 | Loss: 0.00001435
Iteration 196/1000 | Loss: 0.00001435
Iteration 197/1000 | Loss: 0.00001435
Iteration 198/1000 | Loss: 0.00001435
Iteration 199/1000 | Loss: 0.00001435
Iteration 200/1000 | Loss: 0.00001435
Iteration 201/1000 | Loss: 0.00001434
Iteration 202/1000 | Loss: 0.00001434
Iteration 203/1000 | Loss: 0.00001434
Iteration 204/1000 | Loss: 0.00001434
Iteration 205/1000 | Loss: 0.00001433
Iteration 206/1000 | Loss: 0.00001433
Iteration 207/1000 | Loss: 0.00001433
Iteration 208/1000 | Loss: 0.00001432
Iteration 209/1000 | Loss: 0.00001432
Iteration 210/1000 | Loss: 0.00001432
Iteration 211/1000 | Loss: 0.00001432
Iteration 212/1000 | Loss: 0.00001431
Iteration 213/1000 | Loss: 0.00001431
Iteration 214/1000 | Loss: 0.00001431
Iteration 215/1000 | Loss: 0.00001430
Iteration 216/1000 | Loss: 0.00001430
Iteration 217/1000 | Loss: 0.00001430
Iteration 218/1000 | Loss: 0.00001429
Iteration 219/1000 | Loss: 0.00001429
Iteration 220/1000 | Loss: 0.00001428
Iteration 221/1000 | Loss: 0.00001428
Iteration 222/1000 | Loss: 0.00001428
Iteration 223/1000 | Loss: 0.00001428
Iteration 224/1000 | Loss: 0.00001428
Iteration 225/1000 | Loss: 0.00001428
Iteration 226/1000 | Loss: 0.00001427
Iteration 227/1000 | Loss: 0.00001427
Iteration 228/1000 | Loss: 0.00001427
Iteration 229/1000 | Loss: 0.00001427
Iteration 230/1000 | Loss: 0.00001427
Iteration 231/1000 | Loss: 0.00001427
Iteration 232/1000 | Loss: 0.00001427
Iteration 233/1000 | Loss: 0.00001426
Iteration 234/1000 | Loss: 0.00001426
Iteration 235/1000 | Loss: 0.00001426
Iteration 236/1000 | Loss: 0.00001426
Iteration 237/1000 | Loss: 0.00001426
Iteration 238/1000 | Loss: 0.00001426
Iteration 239/1000 | Loss: 0.00001426
Iteration 240/1000 | Loss: 0.00001426
Iteration 241/1000 | Loss: 0.00001425
Iteration 242/1000 | Loss: 0.00001425
Iteration 243/1000 | Loss: 0.00001425
Iteration 244/1000 | Loss: 0.00001425
Iteration 245/1000 | Loss: 0.00001425
Iteration 246/1000 | Loss: 0.00001425
Iteration 247/1000 | Loss: 0.00001425
Iteration 248/1000 | Loss: 0.00001425
Iteration 249/1000 | Loss: 0.00001425
Iteration 250/1000 | Loss: 0.00001424
Iteration 251/1000 | Loss: 0.00001424
Iteration 252/1000 | Loss: 0.00001424
Iteration 253/1000 | Loss: 0.00001424
Iteration 254/1000 | Loss: 0.00001424
Iteration 255/1000 | Loss: 0.00001424
Iteration 256/1000 | Loss: 0.00001424
Iteration 257/1000 | Loss: 0.00001424
Iteration 258/1000 | Loss: 0.00001424
Iteration 259/1000 | Loss: 0.00001424
Iteration 260/1000 | Loss: 0.00001424
Iteration 261/1000 | Loss: 0.00001424
Iteration 262/1000 | Loss: 0.00001424
Iteration 263/1000 | Loss: 0.00001424
Iteration 264/1000 | Loss: 0.00001424
Iteration 265/1000 | Loss: 0.00001424
Iteration 266/1000 | Loss: 0.00001424
Iteration 267/1000 | Loss: 0.00001424
Iteration 268/1000 | Loss: 0.00001424
Iteration 269/1000 | Loss: 0.00001423
Iteration 270/1000 | Loss: 0.00001423
Iteration 271/1000 | Loss: 0.00001423
Iteration 272/1000 | Loss: 0.00001423
Iteration 273/1000 | Loss: 0.00001423
Iteration 274/1000 | Loss: 0.00001423
Iteration 275/1000 | Loss: 0.00001423
Iteration 276/1000 | Loss: 0.00001423
Iteration 277/1000 | Loss: 0.00001423
Iteration 278/1000 | Loss: 0.00001423
Iteration 279/1000 | Loss: 0.00001423
Iteration 280/1000 | Loss: 0.00001423
Iteration 281/1000 | Loss: 0.00001423
Iteration 282/1000 | Loss: 0.00001423
Iteration 283/1000 | Loss: 0.00001423
Iteration 284/1000 | Loss: 0.00001423
Iteration 285/1000 | Loss: 0.00001423
Iteration 286/1000 | Loss: 0.00001423
Iteration 287/1000 | Loss: 0.00001423
Iteration 288/1000 | Loss: 0.00001423
Iteration 289/1000 | Loss: 0.00001423
Iteration 290/1000 | Loss: 0.00001423
Iteration 291/1000 | Loss: 0.00001423
Iteration 292/1000 | Loss: 0.00001423
Iteration 293/1000 | Loss: 0.00001423
Iteration 294/1000 | Loss: 0.00001423
Iteration 295/1000 | Loss: 0.00001423
Iteration 296/1000 | Loss: 0.00001423
Iteration 297/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.4230121450964361e-05, 1.4230121450964361e-05, 1.4230121450964361e-05, 1.4230121450964361e-05, 1.4230121450964361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4230121450964361e-05

Optimization complete. Final v2v error: 3.208418846130371 mm

Highest mean error: 4.061530113220215 mm for frame 48

Lowest mean error: 2.8209452629089355 mm for frame 90

Saving results

Total time: 121.92102456092834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799266
Iteration 2/25 | Loss: 0.00152627
Iteration 3/25 | Loss: 0.00133442
Iteration 4/25 | Loss: 0.00130784
Iteration 5/25 | Loss: 0.00130150
Iteration 6/25 | Loss: 0.00129666
Iteration 7/25 | Loss: 0.00129038
Iteration 8/25 | Loss: 0.00128573
Iteration 9/25 | Loss: 0.00128742
Iteration 10/25 | Loss: 0.00128603
Iteration 11/25 | Loss: 0.00128286
Iteration 12/25 | Loss: 0.00128527
Iteration 13/25 | Loss: 0.00128368
Iteration 14/25 | Loss: 0.00128140
Iteration 15/25 | Loss: 0.00128031
Iteration 16/25 | Loss: 0.00127944
Iteration 17/25 | Loss: 0.00128215
Iteration 18/25 | Loss: 0.00128356
Iteration 19/25 | Loss: 0.00127842
Iteration 20/25 | Loss: 0.00127690
Iteration 21/25 | Loss: 0.00127676
Iteration 22/25 | Loss: 0.00127671
Iteration 23/25 | Loss: 0.00127671
Iteration 24/25 | Loss: 0.00127671
Iteration 25/25 | Loss: 0.00127671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50138903
Iteration 2/25 | Loss: 0.00085267
Iteration 3/25 | Loss: 0.00085266
Iteration 4/25 | Loss: 0.00085266
Iteration 5/25 | Loss: 0.00085266
Iteration 6/25 | Loss: 0.00085266
Iteration 7/25 | Loss: 0.00085266
Iteration 8/25 | Loss: 0.00085266
Iteration 9/25 | Loss: 0.00085266
Iteration 10/25 | Loss: 0.00085266
Iteration 11/25 | Loss: 0.00085266
Iteration 12/25 | Loss: 0.00085266
Iteration 13/25 | Loss: 0.00085266
Iteration 14/25 | Loss: 0.00085266
Iteration 15/25 | Loss: 0.00085266
Iteration 16/25 | Loss: 0.00085266
Iteration 17/25 | Loss: 0.00085266
Iteration 18/25 | Loss: 0.00085266
Iteration 19/25 | Loss: 0.00085266
Iteration 20/25 | Loss: 0.00085266
Iteration 21/25 | Loss: 0.00085266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008526589954271913, 0.0008526589954271913, 0.0008526589954271913, 0.0008526589954271913, 0.0008526589954271913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008526589954271913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085266
Iteration 2/1000 | Loss: 0.00007490
Iteration 3/1000 | Loss: 0.00005304
Iteration 4/1000 | Loss: 0.00004732
Iteration 5/1000 | Loss: 0.00004382
Iteration 6/1000 | Loss: 0.00004163
Iteration 7/1000 | Loss: 0.00003989
Iteration 8/1000 | Loss: 0.00003855
Iteration 9/1000 | Loss: 0.00003722
Iteration 10/1000 | Loss: 0.00003633
Iteration 11/1000 | Loss: 0.00003566
Iteration 12/1000 | Loss: 0.00048528
Iteration 13/1000 | Loss: 0.00078138
Iteration 14/1000 | Loss: 0.00006684
Iteration 15/1000 | Loss: 0.00039178
Iteration 16/1000 | Loss: 0.00004947
Iteration 17/1000 | Loss: 0.00011520
Iteration 18/1000 | Loss: 0.00032516
Iteration 19/1000 | Loss: 0.00012796
Iteration 20/1000 | Loss: 0.00014793
Iteration 21/1000 | Loss: 0.00011509
Iteration 22/1000 | Loss: 0.00012479
Iteration 23/1000 | Loss: 0.00004876
Iteration 24/1000 | Loss: 0.00004304
Iteration 25/1000 | Loss: 0.00003960
Iteration 26/1000 | Loss: 0.00003794
Iteration 27/1000 | Loss: 0.00004178
Iteration 28/1000 | Loss: 0.00003518
Iteration 29/1000 | Loss: 0.00003386
Iteration 30/1000 | Loss: 0.00003319
Iteration 31/1000 | Loss: 0.00003275
Iteration 32/1000 | Loss: 0.00003228
Iteration 33/1000 | Loss: 0.00003186
Iteration 34/1000 | Loss: 0.00027046
Iteration 35/1000 | Loss: 0.00037956
Iteration 36/1000 | Loss: 0.00005676
Iteration 37/1000 | Loss: 0.00004135
Iteration 38/1000 | Loss: 0.00003691
Iteration 39/1000 | Loss: 0.00003526
Iteration 40/1000 | Loss: 0.00003356
Iteration 41/1000 | Loss: 0.00003268
Iteration 42/1000 | Loss: 0.00003185
Iteration 43/1000 | Loss: 0.00064778
Iteration 44/1000 | Loss: 0.00100751
Iteration 45/1000 | Loss: 0.00039692
Iteration 46/1000 | Loss: 0.00016819
Iteration 47/1000 | Loss: 0.00015448
Iteration 48/1000 | Loss: 0.00004136
Iteration 49/1000 | Loss: 0.00003663
Iteration 50/1000 | Loss: 0.00021152
Iteration 51/1000 | Loss: 0.00003658
Iteration 52/1000 | Loss: 0.00003106
Iteration 53/1000 | Loss: 0.00002868
Iteration 54/1000 | Loss: 0.00002689
Iteration 55/1000 | Loss: 0.00002571
Iteration 56/1000 | Loss: 0.00002492
Iteration 57/1000 | Loss: 0.00002405
Iteration 58/1000 | Loss: 0.00002358
Iteration 59/1000 | Loss: 0.00002328
Iteration 60/1000 | Loss: 0.00002294
Iteration 61/1000 | Loss: 0.00002268
Iteration 62/1000 | Loss: 0.00002244
Iteration 63/1000 | Loss: 0.00002240
Iteration 64/1000 | Loss: 0.00002236
Iteration 65/1000 | Loss: 0.00002234
Iteration 66/1000 | Loss: 0.00002233
Iteration 67/1000 | Loss: 0.00002232
Iteration 68/1000 | Loss: 0.00002232
Iteration 69/1000 | Loss: 0.00002228
Iteration 70/1000 | Loss: 0.00002228
Iteration 71/1000 | Loss: 0.00002227
Iteration 72/1000 | Loss: 0.00002226
Iteration 73/1000 | Loss: 0.00002226
Iteration 74/1000 | Loss: 0.00002225
Iteration 75/1000 | Loss: 0.00002225
Iteration 76/1000 | Loss: 0.00002225
Iteration 77/1000 | Loss: 0.00002224
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002224
Iteration 80/1000 | Loss: 0.00002224
Iteration 81/1000 | Loss: 0.00002224
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002223
Iteration 84/1000 | Loss: 0.00002223
Iteration 85/1000 | Loss: 0.00002222
Iteration 86/1000 | Loss: 0.00002222
Iteration 87/1000 | Loss: 0.00002221
Iteration 88/1000 | Loss: 0.00002219
Iteration 89/1000 | Loss: 0.00002218
Iteration 90/1000 | Loss: 0.00002218
Iteration 91/1000 | Loss: 0.00002217
Iteration 92/1000 | Loss: 0.00002217
Iteration 93/1000 | Loss: 0.00002216
Iteration 94/1000 | Loss: 0.00002216
Iteration 95/1000 | Loss: 0.00002216
Iteration 96/1000 | Loss: 0.00002216
Iteration 97/1000 | Loss: 0.00002215
Iteration 98/1000 | Loss: 0.00002215
Iteration 99/1000 | Loss: 0.00002215
Iteration 100/1000 | Loss: 0.00002214
Iteration 101/1000 | Loss: 0.00002214
Iteration 102/1000 | Loss: 0.00002214
Iteration 103/1000 | Loss: 0.00002214
Iteration 104/1000 | Loss: 0.00002213
Iteration 105/1000 | Loss: 0.00002213
Iteration 106/1000 | Loss: 0.00002213
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002212
Iteration 109/1000 | Loss: 0.00002212
Iteration 110/1000 | Loss: 0.00002212
Iteration 111/1000 | Loss: 0.00002211
Iteration 112/1000 | Loss: 0.00002211
Iteration 113/1000 | Loss: 0.00002211
Iteration 114/1000 | Loss: 0.00002211
Iteration 115/1000 | Loss: 0.00002211
Iteration 116/1000 | Loss: 0.00002211
Iteration 117/1000 | Loss: 0.00002210
Iteration 118/1000 | Loss: 0.00002210
Iteration 119/1000 | Loss: 0.00002210
Iteration 120/1000 | Loss: 0.00002210
Iteration 121/1000 | Loss: 0.00002210
Iteration 122/1000 | Loss: 0.00002210
Iteration 123/1000 | Loss: 0.00002210
Iteration 124/1000 | Loss: 0.00002209
Iteration 125/1000 | Loss: 0.00002209
Iteration 126/1000 | Loss: 0.00002209
Iteration 127/1000 | Loss: 0.00002209
Iteration 128/1000 | Loss: 0.00002209
Iteration 129/1000 | Loss: 0.00002209
Iteration 130/1000 | Loss: 0.00002209
Iteration 131/1000 | Loss: 0.00002209
Iteration 132/1000 | Loss: 0.00002209
Iteration 133/1000 | Loss: 0.00002209
Iteration 134/1000 | Loss: 0.00002208
Iteration 135/1000 | Loss: 0.00002208
Iteration 136/1000 | Loss: 0.00002208
Iteration 137/1000 | Loss: 0.00002208
Iteration 138/1000 | Loss: 0.00002208
Iteration 139/1000 | Loss: 0.00002208
Iteration 140/1000 | Loss: 0.00002208
Iteration 141/1000 | Loss: 0.00002208
Iteration 142/1000 | Loss: 0.00002207
Iteration 143/1000 | Loss: 0.00002207
Iteration 144/1000 | Loss: 0.00002207
Iteration 145/1000 | Loss: 0.00002207
Iteration 146/1000 | Loss: 0.00002207
Iteration 147/1000 | Loss: 0.00002207
Iteration 148/1000 | Loss: 0.00002207
Iteration 149/1000 | Loss: 0.00002207
Iteration 150/1000 | Loss: 0.00002207
Iteration 151/1000 | Loss: 0.00002207
Iteration 152/1000 | Loss: 0.00002207
Iteration 153/1000 | Loss: 0.00002206
Iteration 154/1000 | Loss: 0.00002206
Iteration 155/1000 | Loss: 0.00002206
Iteration 156/1000 | Loss: 0.00002206
Iteration 157/1000 | Loss: 0.00002206
Iteration 158/1000 | Loss: 0.00002206
Iteration 159/1000 | Loss: 0.00002206
Iteration 160/1000 | Loss: 0.00002206
Iteration 161/1000 | Loss: 0.00002206
Iteration 162/1000 | Loss: 0.00002206
Iteration 163/1000 | Loss: 0.00002206
Iteration 164/1000 | Loss: 0.00002206
Iteration 165/1000 | Loss: 0.00002206
Iteration 166/1000 | Loss: 0.00002206
Iteration 167/1000 | Loss: 0.00002206
Iteration 168/1000 | Loss: 0.00002206
Iteration 169/1000 | Loss: 0.00002206
Iteration 170/1000 | Loss: 0.00002206
Iteration 171/1000 | Loss: 0.00002206
Iteration 172/1000 | Loss: 0.00002206
Iteration 173/1000 | Loss: 0.00002206
Iteration 174/1000 | Loss: 0.00002206
Iteration 175/1000 | Loss: 0.00002206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.2058215108700097e-05, 2.2058215108700097e-05, 2.2058215108700097e-05, 2.2058215108700097e-05, 2.2058215108700097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2058215108700097e-05

Optimization complete. Final v2v error: 3.8004958629608154 mm

Highest mean error: 11.711745262145996 mm for frame 168

Lowest mean error: 3.09938383102417 mm for frame 148

Saving results

Total time: 154.9031126499176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864400
Iteration 2/25 | Loss: 0.00244134
Iteration 3/25 | Loss: 0.00201125
Iteration 4/25 | Loss: 0.00201132
Iteration 5/25 | Loss: 0.00174626
Iteration 6/25 | Loss: 0.00168944
Iteration 7/25 | Loss: 0.00167466
Iteration 8/25 | Loss: 0.00166628
Iteration 9/25 | Loss: 0.00166704
Iteration 10/25 | Loss: 0.00165516
Iteration 11/25 | Loss: 0.00164823
Iteration 12/25 | Loss: 0.00164511
Iteration 13/25 | Loss: 0.00164462
Iteration 14/25 | Loss: 0.00164448
Iteration 15/25 | Loss: 0.00164394
Iteration 16/25 | Loss: 0.00164302
Iteration 17/25 | Loss: 0.00164299
Iteration 18/25 | Loss: 0.00164299
Iteration 19/25 | Loss: 0.00164299
Iteration 20/25 | Loss: 0.00164299
Iteration 21/25 | Loss: 0.00164299
Iteration 22/25 | Loss: 0.00164299
Iteration 23/25 | Loss: 0.00164299
Iteration 24/25 | Loss: 0.00164299
Iteration 25/25 | Loss: 0.00164298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58291519
Iteration 2/25 | Loss: 0.00230722
Iteration 3/25 | Loss: 0.00230693
Iteration 4/25 | Loss: 0.00230693
Iteration 5/25 | Loss: 0.00230693
Iteration 6/25 | Loss: 0.00230693
Iteration 7/25 | Loss: 0.00230693
Iteration 8/25 | Loss: 0.00230693
Iteration 9/25 | Loss: 0.00230693
Iteration 10/25 | Loss: 0.00230693
Iteration 11/25 | Loss: 0.00230693
Iteration 12/25 | Loss: 0.00230693
Iteration 13/25 | Loss: 0.00230693
Iteration 14/25 | Loss: 0.00230693
Iteration 15/25 | Loss: 0.00230693
Iteration 16/25 | Loss: 0.00230693
Iteration 17/25 | Loss: 0.00230693
Iteration 18/25 | Loss: 0.00230693
Iteration 19/25 | Loss: 0.00230693
Iteration 20/25 | Loss: 0.00230693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023069276940077543, 0.0023069276940077543, 0.0023069276940077543, 0.0023069276940077543, 0.0023069276940077543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023069276940077543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230693
Iteration 2/1000 | Loss: 0.00458264
Iteration 3/1000 | Loss: 0.00025997
Iteration 4/1000 | Loss: 0.00013565
Iteration 5/1000 | Loss: 0.00010137
Iteration 6/1000 | Loss: 0.00008264
Iteration 7/1000 | Loss: 0.00007326
Iteration 8/1000 | Loss: 0.00006852
Iteration 9/1000 | Loss: 0.00006482
Iteration 10/1000 | Loss: 0.00006272
Iteration 11/1000 | Loss: 0.00006092
Iteration 12/1000 | Loss: 0.00005979
Iteration 13/1000 | Loss: 0.00005876
Iteration 14/1000 | Loss: 0.00005808
Iteration 15/1000 | Loss: 0.00005741
Iteration 16/1000 | Loss: 0.00005687
Iteration 17/1000 | Loss: 0.00006860
Iteration 18/1000 | Loss: 0.00005857
Iteration 19/1000 | Loss: 0.00005711
Iteration 20/1000 | Loss: 0.00005636
Iteration 21/1000 | Loss: 0.00005586
Iteration 22/1000 | Loss: 0.00005549
Iteration 23/1000 | Loss: 0.00005526
Iteration 24/1000 | Loss: 0.00005505
Iteration 25/1000 | Loss: 0.00005481
Iteration 26/1000 | Loss: 0.00005471
Iteration 27/1000 | Loss: 0.00005470
Iteration 28/1000 | Loss: 0.00005469
Iteration 29/1000 | Loss: 0.00005469
Iteration 30/1000 | Loss: 0.00005468
Iteration 31/1000 | Loss: 0.00005468
Iteration 32/1000 | Loss: 0.00005468
Iteration 33/1000 | Loss: 0.00005468
Iteration 34/1000 | Loss: 0.00005467
Iteration 35/1000 | Loss: 0.00005467
Iteration 36/1000 | Loss: 0.00005467
Iteration 37/1000 | Loss: 0.00005467
Iteration 38/1000 | Loss: 0.00005466
Iteration 39/1000 | Loss: 0.00005466
Iteration 40/1000 | Loss: 0.00005465
Iteration 41/1000 | Loss: 0.00005465
Iteration 42/1000 | Loss: 0.00005460
Iteration 43/1000 | Loss: 0.00005457
Iteration 44/1000 | Loss: 0.00005455
Iteration 45/1000 | Loss: 0.00006981
Iteration 46/1000 | Loss: 0.00006651
Iteration 47/1000 | Loss: 0.00005980
Iteration 48/1000 | Loss: 0.00005663
Iteration 49/1000 | Loss: 0.00005569
Iteration 50/1000 | Loss: 0.00005511
Iteration 51/1000 | Loss: 0.00005471
Iteration 52/1000 | Loss: 0.00005450
Iteration 53/1000 | Loss: 0.00005449
Iteration 54/1000 | Loss: 0.00005446
Iteration 55/1000 | Loss: 0.00005446
Iteration 56/1000 | Loss: 0.00005446
Iteration 57/1000 | Loss: 0.00005444
Iteration 58/1000 | Loss: 0.00005443
Iteration 59/1000 | Loss: 0.00005442
Iteration 60/1000 | Loss: 0.00005442
Iteration 61/1000 | Loss: 0.00005442
Iteration 62/1000 | Loss: 0.00005442
Iteration 63/1000 | Loss: 0.00005442
Iteration 64/1000 | Loss: 0.00005442
Iteration 65/1000 | Loss: 0.00005442
Iteration 66/1000 | Loss: 0.00005442
Iteration 67/1000 | Loss: 0.00005442
Iteration 68/1000 | Loss: 0.00005442
Iteration 69/1000 | Loss: 0.00005442
Iteration 70/1000 | Loss: 0.00005441
Iteration 71/1000 | Loss: 0.00005441
Iteration 72/1000 | Loss: 0.00005441
Iteration 73/1000 | Loss: 0.00005441
Iteration 74/1000 | Loss: 0.00005441
Iteration 75/1000 | Loss: 0.00005439
Iteration 76/1000 | Loss: 0.00005439
Iteration 77/1000 | Loss: 0.00005437
Iteration 78/1000 | Loss: 0.00005434
Iteration 79/1000 | Loss: 0.00005433
Iteration 80/1000 | Loss: 0.00005429
Iteration 81/1000 | Loss: 0.00005428
Iteration 82/1000 | Loss: 0.00005427
Iteration 83/1000 | Loss: 0.00005426
Iteration 84/1000 | Loss: 0.00005426
Iteration 85/1000 | Loss: 0.00005425
Iteration 86/1000 | Loss: 0.00005425
Iteration 87/1000 | Loss: 0.00005425
Iteration 88/1000 | Loss: 0.00005424
Iteration 89/1000 | Loss: 0.00005424
Iteration 90/1000 | Loss: 0.00005424
Iteration 91/1000 | Loss: 0.00005423
Iteration 92/1000 | Loss: 0.00005423
Iteration 93/1000 | Loss: 0.00005423
Iteration 94/1000 | Loss: 0.00005423
Iteration 95/1000 | Loss: 0.00005422
Iteration 96/1000 | Loss: 0.00005422
Iteration 97/1000 | Loss: 0.00005422
Iteration 98/1000 | Loss: 0.00005422
Iteration 99/1000 | Loss: 0.00005422
Iteration 100/1000 | Loss: 0.00005421
Iteration 101/1000 | Loss: 0.00005421
Iteration 102/1000 | Loss: 0.00005421
Iteration 103/1000 | Loss: 0.00005421
Iteration 104/1000 | Loss: 0.00005421
Iteration 105/1000 | Loss: 0.00005421
Iteration 106/1000 | Loss: 0.00005421
Iteration 107/1000 | Loss: 0.00005421
Iteration 108/1000 | Loss: 0.00005421
Iteration 109/1000 | Loss: 0.00005421
Iteration 110/1000 | Loss: 0.00005421
Iteration 111/1000 | Loss: 0.00005421
Iteration 112/1000 | Loss: 0.00005421
Iteration 113/1000 | Loss: 0.00005421
Iteration 114/1000 | Loss: 0.00005421
Iteration 115/1000 | Loss: 0.00005421
Iteration 116/1000 | Loss: 0.00005421
Iteration 117/1000 | Loss: 0.00005420
Iteration 118/1000 | Loss: 0.00005420
Iteration 119/1000 | Loss: 0.00005420
Iteration 120/1000 | Loss: 0.00005420
Iteration 121/1000 | Loss: 0.00005420
Iteration 122/1000 | Loss: 0.00005420
Iteration 123/1000 | Loss: 0.00005420
Iteration 124/1000 | Loss: 0.00005420
Iteration 125/1000 | Loss: 0.00005420
Iteration 126/1000 | Loss: 0.00005420
Iteration 127/1000 | Loss: 0.00005420
Iteration 128/1000 | Loss: 0.00005420
Iteration 129/1000 | Loss: 0.00005420
Iteration 130/1000 | Loss: 0.00005420
Iteration 131/1000 | Loss: 0.00005419
Iteration 132/1000 | Loss: 0.00005419
Iteration 133/1000 | Loss: 0.00005419
Iteration 134/1000 | Loss: 0.00005419
Iteration 135/1000 | Loss: 0.00005419
Iteration 136/1000 | Loss: 0.00005419
Iteration 137/1000 | Loss: 0.00005419
Iteration 138/1000 | Loss: 0.00005419
Iteration 139/1000 | Loss: 0.00005419
Iteration 140/1000 | Loss: 0.00005419
Iteration 141/1000 | Loss: 0.00005419
Iteration 142/1000 | Loss: 0.00005418
Iteration 143/1000 | Loss: 0.00005418
Iteration 144/1000 | Loss: 0.00005418
Iteration 145/1000 | Loss: 0.00005418
Iteration 146/1000 | Loss: 0.00005418
Iteration 147/1000 | Loss: 0.00005418
Iteration 148/1000 | Loss: 0.00005418
Iteration 149/1000 | Loss: 0.00005418
Iteration 150/1000 | Loss: 0.00005418
Iteration 151/1000 | Loss: 0.00005418
Iteration 152/1000 | Loss: 0.00005418
Iteration 153/1000 | Loss: 0.00005418
Iteration 154/1000 | Loss: 0.00005417
Iteration 155/1000 | Loss: 0.00005417
Iteration 156/1000 | Loss: 0.00005417
Iteration 157/1000 | Loss: 0.00005417
Iteration 158/1000 | Loss: 0.00005417
Iteration 159/1000 | Loss: 0.00005417
Iteration 160/1000 | Loss: 0.00005417
Iteration 161/1000 | Loss: 0.00005417
Iteration 162/1000 | Loss: 0.00005417
Iteration 163/1000 | Loss: 0.00005417
Iteration 164/1000 | Loss: 0.00005417
Iteration 165/1000 | Loss: 0.00005417
Iteration 166/1000 | Loss: 0.00005417
Iteration 167/1000 | Loss: 0.00005416
Iteration 168/1000 | Loss: 0.00005416
Iteration 169/1000 | Loss: 0.00005416
Iteration 170/1000 | Loss: 0.00005416
Iteration 171/1000 | Loss: 0.00005416
Iteration 172/1000 | Loss: 0.00005416
Iteration 173/1000 | Loss: 0.00005416
Iteration 174/1000 | Loss: 0.00005416
Iteration 175/1000 | Loss: 0.00005416
Iteration 176/1000 | Loss: 0.00005415
Iteration 177/1000 | Loss: 0.00005415
Iteration 178/1000 | Loss: 0.00005415
Iteration 179/1000 | Loss: 0.00005415
Iteration 180/1000 | Loss: 0.00005415
Iteration 181/1000 | Loss: 0.00005415
Iteration 182/1000 | Loss: 0.00005415
Iteration 183/1000 | Loss: 0.00005415
Iteration 184/1000 | Loss: 0.00005415
Iteration 185/1000 | Loss: 0.00005415
Iteration 186/1000 | Loss: 0.00005415
Iteration 187/1000 | Loss: 0.00005415
Iteration 188/1000 | Loss: 0.00005414
Iteration 189/1000 | Loss: 0.00005414
Iteration 190/1000 | Loss: 0.00005414
Iteration 191/1000 | Loss: 0.00005414
Iteration 192/1000 | Loss: 0.00006532
Iteration 193/1000 | Loss: 0.00005551
Iteration 194/1000 | Loss: 0.00005935
Iteration 195/1000 | Loss: 0.00006431
Iteration 196/1000 | Loss: 0.00005887
Iteration 197/1000 | Loss: 0.00005413
Iteration 198/1000 | Loss: 0.00005412
Iteration 199/1000 | Loss: 0.00005412
Iteration 200/1000 | Loss: 0.00005412
Iteration 201/1000 | Loss: 0.00005412
Iteration 202/1000 | Loss: 0.00005412
Iteration 203/1000 | Loss: 0.00005411
Iteration 204/1000 | Loss: 0.00005411
Iteration 205/1000 | Loss: 0.00005411
Iteration 206/1000 | Loss: 0.00005411
Iteration 207/1000 | Loss: 0.00005411
Iteration 208/1000 | Loss: 0.00005411
Iteration 209/1000 | Loss: 0.00005411
Iteration 210/1000 | Loss: 0.00005411
Iteration 211/1000 | Loss: 0.00005411
Iteration 212/1000 | Loss: 0.00005411
Iteration 213/1000 | Loss: 0.00005411
Iteration 214/1000 | Loss: 0.00005411
Iteration 215/1000 | Loss: 0.00005411
Iteration 216/1000 | Loss: 0.00005411
Iteration 217/1000 | Loss: 0.00005411
Iteration 218/1000 | Loss: 0.00005411
Iteration 219/1000 | Loss: 0.00005411
Iteration 220/1000 | Loss: 0.00005411
Iteration 221/1000 | Loss: 0.00005411
Iteration 222/1000 | Loss: 0.00005411
Iteration 223/1000 | Loss: 0.00005411
Iteration 224/1000 | Loss: 0.00005411
Iteration 225/1000 | Loss: 0.00005411
Iteration 226/1000 | Loss: 0.00005411
Iteration 227/1000 | Loss: 0.00005411
Iteration 228/1000 | Loss: 0.00005411
Iteration 229/1000 | Loss: 0.00005411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [5.4107396863400936e-05, 5.4107396863400936e-05, 5.4107396863400936e-05, 5.4107396863400936e-05, 5.4107396863400936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.4107396863400936e-05

Optimization complete. Final v2v error: 5.56686544418335 mm

Highest mean error: 11.443312644958496 mm for frame 42

Lowest mean error: 3.5580132007598877 mm for frame 83

Saving results

Total time: 112.13094067573547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838690
Iteration 2/25 | Loss: 0.00160780
Iteration 3/25 | Loss: 0.00131905
Iteration 4/25 | Loss: 0.00128822
Iteration 5/25 | Loss: 0.00128375
Iteration 6/25 | Loss: 0.00128341
Iteration 7/25 | Loss: 0.00128341
Iteration 8/25 | Loss: 0.00128341
Iteration 9/25 | Loss: 0.00128341
Iteration 10/25 | Loss: 0.00128341
Iteration 11/25 | Loss: 0.00128341
Iteration 12/25 | Loss: 0.00128341
Iteration 13/25 | Loss: 0.00128341
Iteration 14/25 | Loss: 0.00128341
Iteration 15/25 | Loss: 0.00128341
Iteration 16/25 | Loss: 0.00128341
Iteration 17/25 | Loss: 0.00128341
Iteration 18/25 | Loss: 0.00128341
Iteration 19/25 | Loss: 0.00128341
Iteration 20/25 | Loss: 0.00128341
Iteration 21/25 | Loss: 0.00128341
Iteration 22/25 | Loss: 0.00128341
Iteration 23/25 | Loss: 0.00128341
Iteration 24/25 | Loss: 0.00128341
Iteration 25/25 | Loss: 0.00128341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39497244
Iteration 2/25 | Loss: 0.00076130
Iteration 3/25 | Loss: 0.00076129
Iteration 4/25 | Loss: 0.00076129
Iteration 5/25 | Loss: 0.00076129
Iteration 6/25 | Loss: 0.00076129
Iteration 7/25 | Loss: 0.00076129
Iteration 8/25 | Loss: 0.00076129
Iteration 9/25 | Loss: 0.00076129
Iteration 10/25 | Loss: 0.00076129
Iteration 11/25 | Loss: 0.00076129
Iteration 12/25 | Loss: 0.00076129
Iteration 13/25 | Loss: 0.00076129
Iteration 14/25 | Loss: 0.00076129
Iteration 15/25 | Loss: 0.00076129
Iteration 16/25 | Loss: 0.00076129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007612886838614941, 0.0007612886838614941, 0.0007612886838614941, 0.0007612886838614941, 0.0007612886838614941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007612886838614941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076129
Iteration 2/1000 | Loss: 0.00003717
Iteration 3/1000 | Loss: 0.00002309
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001702
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001650
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001615
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001609
Iteration 23/1000 | Loss: 0.00001605
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001565
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.5568657545372844e-05, 1.5568657545372844e-05, 1.5568657545372844e-05, 1.5568657545372844e-05, 1.5568657545372844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5568657545372844e-05

Optimization complete. Final v2v error: 3.344158411026001 mm

Highest mean error: 3.9248359203338623 mm for frame 1

Lowest mean error: 2.9749677181243896 mm for frame 239

Saving results

Total time: 40.90523838996887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982511
Iteration 2/25 | Loss: 0.00261119
Iteration 3/25 | Loss: 0.00226633
Iteration 4/25 | Loss: 0.00227776
Iteration 5/25 | Loss: 0.00224084
Iteration 6/25 | Loss: 0.00220478
Iteration 7/25 | Loss: 0.00214481
Iteration 8/25 | Loss: 0.00210918
Iteration 9/25 | Loss: 0.00205531
Iteration 10/25 | Loss: 0.00203292
Iteration 11/25 | Loss: 0.00197756
Iteration 12/25 | Loss: 0.00198081
Iteration 13/25 | Loss: 0.00195968
Iteration 14/25 | Loss: 0.00192047
Iteration 15/25 | Loss: 0.00190320
Iteration 16/25 | Loss: 0.00191420
Iteration 17/25 | Loss: 0.00189633
Iteration 18/25 | Loss: 0.00189189
Iteration 19/25 | Loss: 0.00189274
Iteration 20/25 | Loss: 0.00189078
Iteration 21/25 | Loss: 0.00189239
Iteration 22/25 | Loss: 0.00188903
Iteration 23/25 | Loss: 0.00188702
Iteration 24/25 | Loss: 0.00188577
Iteration 25/25 | Loss: 0.00188373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41833377
Iteration 2/25 | Loss: 0.00417197
Iteration 3/25 | Loss: 0.00417196
Iteration 4/25 | Loss: 0.00417196
Iteration 5/25 | Loss: 0.00417196
Iteration 6/25 | Loss: 0.00417196
Iteration 7/25 | Loss: 0.00417196
Iteration 8/25 | Loss: 0.00417196
Iteration 9/25 | Loss: 0.00417196
Iteration 10/25 | Loss: 0.00417196
Iteration 11/25 | Loss: 0.00417196
Iteration 12/25 | Loss: 0.00417196
Iteration 13/25 | Loss: 0.00417196
Iteration 14/25 | Loss: 0.00417196
Iteration 15/25 | Loss: 0.00417196
Iteration 16/25 | Loss: 0.00417196
Iteration 17/25 | Loss: 0.00417196
Iteration 18/25 | Loss: 0.00417196
Iteration 19/25 | Loss: 0.00417196
Iteration 20/25 | Loss: 0.00417196
Iteration 21/25 | Loss: 0.00417196
Iteration 22/25 | Loss: 0.00417196
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0041719600558280945, 0.0041719600558280945, 0.0041719600558280945, 0.0041719600558280945, 0.0041719600558280945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0041719600558280945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00417196
Iteration 2/1000 | Loss: 0.00162041
Iteration 3/1000 | Loss: 0.00104226
Iteration 4/1000 | Loss: 0.00061327
Iteration 5/1000 | Loss: 0.00045750
Iteration 6/1000 | Loss: 0.00040486
Iteration 7/1000 | Loss: 0.00035738
Iteration 8/1000 | Loss: 0.00175956
Iteration 9/1000 | Loss: 0.00313335
Iteration 10/1000 | Loss: 0.01023785
Iteration 11/1000 | Loss: 0.00601476
Iteration 12/1000 | Loss: 0.00084455
Iteration 13/1000 | Loss: 0.00064900
Iteration 14/1000 | Loss: 0.00036203
Iteration 15/1000 | Loss: 0.00152787
Iteration 16/1000 | Loss: 0.00077790
Iteration 17/1000 | Loss: 0.00136324
Iteration 18/1000 | Loss: 0.00020451
Iteration 19/1000 | Loss: 0.00038640
Iteration 20/1000 | Loss: 0.00010724
Iteration 21/1000 | Loss: 0.00008794
Iteration 22/1000 | Loss: 0.00008557
Iteration 23/1000 | Loss: 0.00008413
Iteration 24/1000 | Loss: 0.00005598
Iteration 25/1000 | Loss: 0.00025797
Iteration 26/1000 | Loss: 0.00018250
Iteration 27/1000 | Loss: 0.00005873
Iteration 28/1000 | Loss: 0.00006699
Iteration 29/1000 | Loss: 0.00004183
Iteration 30/1000 | Loss: 0.00004629
Iteration 31/1000 | Loss: 0.00003808
Iteration 32/1000 | Loss: 0.00005428
Iteration 33/1000 | Loss: 0.00003200
Iteration 34/1000 | Loss: 0.00005461
Iteration 35/1000 | Loss: 0.00003486
Iteration 36/1000 | Loss: 0.00003120
Iteration 37/1000 | Loss: 0.00002978
Iteration 38/1000 | Loss: 0.00004545
Iteration 39/1000 | Loss: 0.00003135
Iteration 40/1000 | Loss: 0.00004712
Iteration 41/1000 | Loss: 0.00004147
Iteration 42/1000 | Loss: 0.00005128
Iteration 43/1000 | Loss: 0.00003407
Iteration 44/1000 | Loss: 0.00002941
Iteration 45/1000 | Loss: 0.00002723
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00002454
Iteration 48/1000 | Loss: 0.00002436
Iteration 49/1000 | Loss: 0.00002410
Iteration 50/1000 | Loss: 0.00002394
Iteration 51/1000 | Loss: 0.00002374
Iteration 52/1000 | Loss: 0.00002355
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00052746
Iteration 55/1000 | Loss: 0.00034808
Iteration 56/1000 | Loss: 0.00003874
Iteration 57/1000 | Loss: 0.00003046
Iteration 58/1000 | Loss: 0.00002704
Iteration 59/1000 | Loss: 0.00002493
Iteration 60/1000 | Loss: 0.00002312
Iteration 61/1000 | Loss: 0.00002120
Iteration 62/1000 | Loss: 0.00002041
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001898
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001891
Iteration 71/1000 | Loss: 0.00001891
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001871
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001864
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001859
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001858
Iteration 84/1000 | Loss: 0.00001858
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001855
Iteration 100/1000 | Loss: 0.00001855
Iteration 101/1000 | Loss: 0.00001855
Iteration 102/1000 | Loss: 0.00001855
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001853
Iteration 116/1000 | Loss: 0.00001853
Iteration 117/1000 | Loss: 0.00001853
Iteration 118/1000 | Loss: 0.00001853
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001853
Iteration 123/1000 | Loss: 0.00001853
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001851
Iteration 135/1000 | Loss: 0.00001851
Iteration 136/1000 | Loss: 0.00001851
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001851
Iteration 139/1000 | Loss: 0.00001851
Iteration 140/1000 | Loss: 0.00001851
Iteration 141/1000 | Loss: 0.00001851
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001851
Iteration 155/1000 | Loss: 0.00001851
Iteration 156/1000 | Loss: 0.00001851
Iteration 157/1000 | Loss: 0.00001851
Iteration 158/1000 | Loss: 0.00001851
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.851173692557495e-05, 1.851173692557495e-05, 1.851173692557495e-05, 1.851173692557495e-05, 1.851173692557495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.851173692557495e-05

Optimization complete. Final v2v error: 3.6368439197540283 mm

Highest mean error: 4.622437000274658 mm for frame 6

Lowest mean error: 3.5002403259277344 mm for frame 1

Saving results

Total time: 146.08650732040405
