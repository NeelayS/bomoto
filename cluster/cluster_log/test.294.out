Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=294, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16464-16519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430917
Iteration 2/25 | Loss: 0.00134358
Iteration 3/25 | Loss: 0.00118571
Iteration 4/25 | Loss: 0.00116900
Iteration 5/25 | Loss: 0.00116532
Iteration 6/25 | Loss: 0.00116431
Iteration 7/25 | Loss: 0.00116431
Iteration 8/25 | Loss: 0.00116431
Iteration 9/25 | Loss: 0.00116431
Iteration 10/25 | Loss: 0.00116431
Iteration 11/25 | Loss: 0.00116431
Iteration 12/25 | Loss: 0.00116431
Iteration 13/25 | Loss: 0.00116431
Iteration 14/25 | Loss: 0.00116431
Iteration 15/25 | Loss: 0.00116431
Iteration 16/25 | Loss: 0.00116431
Iteration 17/25 | Loss: 0.00116431
Iteration 18/25 | Loss: 0.00116431
Iteration 19/25 | Loss: 0.00116431
Iteration 20/25 | Loss: 0.00116431
Iteration 21/25 | Loss: 0.00116431
Iteration 22/25 | Loss: 0.00116431
Iteration 23/25 | Loss: 0.00116431
Iteration 24/25 | Loss: 0.00116431
Iteration 25/25 | Loss: 0.00116431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60941732
Iteration 2/25 | Loss: 0.00080172
Iteration 3/25 | Loss: 0.00080171
Iteration 4/25 | Loss: 0.00080171
Iteration 5/25 | Loss: 0.00080171
Iteration 6/25 | Loss: 0.00080171
Iteration 7/25 | Loss: 0.00080171
Iteration 8/25 | Loss: 0.00080171
Iteration 9/25 | Loss: 0.00080171
Iteration 10/25 | Loss: 0.00080171
Iteration 11/25 | Loss: 0.00080171
Iteration 12/25 | Loss: 0.00080171
Iteration 13/25 | Loss: 0.00080171
Iteration 14/25 | Loss: 0.00080171
Iteration 15/25 | Loss: 0.00080171
Iteration 16/25 | Loss: 0.00080171
Iteration 17/25 | Loss: 0.00080171
Iteration 18/25 | Loss: 0.00080171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008017082000151277, 0.0008017082000151277, 0.0008017082000151277, 0.0008017082000151277, 0.0008017082000151277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008017082000151277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080171
Iteration 2/1000 | Loss: 0.00003007
Iteration 3/1000 | Loss: 0.00001844
Iteration 4/1000 | Loss: 0.00001657
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001490
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001386
Iteration 11/1000 | Loss: 0.00001371
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001344
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001335
Iteration 20/1000 | Loss: 0.00001335
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001322
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001321
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001316
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001315
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001315
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001313
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001311
Iteration 60/1000 | Loss: 0.00001311
Iteration 61/1000 | Loss: 0.00001311
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001310
Iteration 64/1000 | Loss: 0.00001310
Iteration 65/1000 | Loss: 0.00001310
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001306
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001299
Iteration 97/1000 | Loss: 0.00001299
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001295
Iteration 120/1000 | Loss: 0.00001295
Iteration 121/1000 | Loss: 0.00001295
Iteration 122/1000 | Loss: 0.00001295
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001293
Iteration 132/1000 | Loss: 0.00001293
Iteration 133/1000 | Loss: 0.00001293
Iteration 134/1000 | Loss: 0.00001293
Iteration 135/1000 | Loss: 0.00001293
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001292
Iteration 138/1000 | Loss: 0.00001292
Iteration 139/1000 | Loss: 0.00001292
Iteration 140/1000 | Loss: 0.00001292
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001290
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001289
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001289
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001289
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001288
Iteration 170/1000 | Loss: 0.00001288
Iteration 171/1000 | Loss: 0.00001288
Iteration 172/1000 | Loss: 0.00001288
Iteration 173/1000 | Loss: 0.00001288
Iteration 174/1000 | Loss: 0.00001288
Iteration 175/1000 | Loss: 0.00001288
Iteration 176/1000 | Loss: 0.00001288
Iteration 177/1000 | Loss: 0.00001288
Iteration 178/1000 | Loss: 0.00001288
Iteration 179/1000 | Loss: 0.00001288
Iteration 180/1000 | Loss: 0.00001288
Iteration 181/1000 | Loss: 0.00001288
Iteration 182/1000 | Loss: 0.00001288
Iteration 183/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.2879690075351391e-05, 1.2879690075351391e-05, 1.2879690075351391e-05, 1.2879690075351391e-05, 1.2879690075351391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2879690075351391e-05

Optimization complete. Final v2v error: 3.0319879055023193 mm

Highest mean error: 3.6409549713134766 mm for frame 84

Lowest mean error: 2.54815673828125 mm for frame 189

Saving results

Total time: 47.772440671920776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944616
Iteration 2/25 | Loss: 0.00338169
Iteration 3/25 | Loss: 0.00243044
Iteration 4/25 | Loss: 0.00207649
Iteration 5/25 | Loss: 0.00193736
Iteration 6/25 | Loss: 0.00187747
Iteration 7/25 | Loss: 0.00182078
Iteration 8/25 | Loss: 0.00173340
Iteration 9/25 | Loss: 0.00170988
Iteration 10/25 | Loss: 0.00167311
Iteration 11/25 | Loss: 0.00161699
Iteration 12/25 | Loss: 0.00160271
Iteration 13/25 | Loss: 0.00159644
Iteration 14/25 | Loss: 0.00157712
Iteration 15/25 | Loss: 0.00156027
Iteration 16/25 | Loss: 0.00157335
Iteration 17/25 | Loss: 0.00154437
Iteration 18/25 | Loss: 0.00153020
Iteration 19/25 | Loss: 0.00152280
Iteration 20/25 | Loss: 0.00152067
Iteration 21/25 | Loss: 0.00152026
Iteration 22/25 | Loss: 0.00152127
Iteration 23/25 | Loss: 0.00151940
Iteration 24/25 | Loss: 0.00151796
Iteration 25/25 | Loss: 0.00151760

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33852684
Iteration 2/25 | Loss: 0.00367130
Iteration 3/25 | Loss: 0.00322846
Iteration 4/25 | Loss: 0.00320472
Iteration 5/25 | Loss: 0.00320471
Iteration 6/25 | Loss: 0.00320471
Iteration 7/25 | Loss: 0.00320471
Iteration 8/25 | Loss: 0.00320471
Iteration 9/25 | Loss: 0.00320471
Iteration 10/25 | Loss: 0.00320471
Iteration 11/25 | Loss: 0.00320471
Iteration 12/25 | Loss: 0.00320471
Iteration 13/25 | Loss: 0.00320471
Iteration 14/25 | Loss: 0.00320471
Iteration 15/25 | Loss: 0.00320471
Iteration 16/25 | Loss: 0.00320471
Iteration 17/25 | Loss: 0.00320471
Iteration 18/25 | Loss: 0.00320471
Iteration 19/25 | Loss: 0.00320471
Iteration 20/25 | Loss: 0.00320471
Iteration 21/25 | Loss: 0.00320471
Iteration 22/25 | Loss: 0.00320471
Iteration 23/25 | Loss: 0.00320471
Iteration 24/25 | Loss: 0.00320471
Iteration 25/25 | Loss: 0.00320471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320471
Iteration 2/1000 | Loss: 0.00128056
Iteration 3/1000 | Loss: 0.00091807
Iteration 4/1000 | Loss: 0.00061126
Iteration 5/1000 | Loss: 0.00033627
Iteration 6/1000 | Loss: 0.00025605
Iteration 7/1000 | Loss: 0.00061140
Iteration 8/1000 | Loss: 0.00066657
Iteration 9/1000 | Loss: 0.00050431
Iteration 10/1000 | Loss: 0.00032216
Iteration 11/1000 | Loss: 0.00049593
Iteration 12/1000 | Loss: 0.00044136
Iteration 13/1000 | Loss: 0.00063757
Iteration 14/1000 | Loss: 0.00022171
Iteration 15/1000 | Loss: 0.00019261
Iteration 16/1000 | Loss: 0.00033063
Iteration 17/1000 | Loss: 0.00015102
Iteration 18/1000 | Loss: 0.00045950
Iteration 19/1000 | Loss: 0.00064915
Iteration 20/1000 | Loss: 0.00014332
Iteration 21/1000 | Loss: 0.00013828
Iteration 22/1000 | Loss: 0.00017278
Iteration 23/1000 | Loss: 0.00013183
Iteration 24/1000 | Loss: 0.00023411
Iteration 25/1000 | Loss: 0.00017550
Iteration 26/1000 | Loss: 0.00018259
Iteration 27/1000 | Loss: 0.00041613
Iteration 28/1000 | Loss: 0.00013554
Iteration 29/1000 | Loss: 0.00012890
Iteration 30/1000 | Loss: 0.00012516
Iteration 31/1000 | Loss: 0.00019380
Iteration 32/1000 | Loss: 0.00013118
Iteration 33/1000 | Loss: 0.00012422
Iteration 34/1000 | Loss: 0.00016689
Iteration 35/1000 | Loss: 0.00012086
Iteration 36/1000 | Loss: 0.00012016
Iteration 37/1000 | Loss: 0.00011965
Iteration 38/1000 | Loss: 0.00037495
Iteration 39/1000 | Loss: 0.00229208
Iteration 40/1000 | Loss: 0.00192558
Iteration 41/1000 | Loss: 0.00254907
Iteration 42/1000 | Loss: 0.00134968
Iteration 43/1000 | Loss: 0.00096268
Iteration 44/1000 | Loss: 0.00058429
Iteration 45/1000 | Loss: 0.00045077
Iteration 46/1000 | Loss: 0.00015397
Iteration 47/1000 | Loss: 0.00012643
Iteration 48/1000 | Loss: 0.00018046
Iteration 49/1000 | Loss: 0.00038510
Iteration 50/1000 | Loss: 0.00009055
Iteration 51/1000 | Loss: 0.00008150
Iteration 52/1000 | Loss: 0.00007661
Iteration 53/1000 | Loss: 0.00057410
Iteration 54/1000 | Loss: 0.00029363
Iteration 55/1000 | Loss: 0.00010017
Iteration 56/1000 | Loss: 0.00007174
Iteration 57/1000 | Loss: 0.00006901
Iteration 58/1000 | Loss: 0.00025094
Iteration 59/1000 | Loss: 0.00054365
Iteration 60/1000 | Loss: 0.00013058
Iteration 61/1000 | Loss: 0.00006664
Iteration 62/1000 | Loss: 0.00006541
Iteration 63/1000 | Loss: 0.00016683
Iteration 64/1000 | Loss: 0.00019733
Iteration 65/1000 | Loss: 0.00006923
Iteration 66/1000 | Loss: 0.00006490
Iteration 67/1000 | Loss: 0.00006357
Iteration 68/1000 | Loss: 0.00006297
Iteration 69/1000 | Loss: 0.00006256
Iteration 70/1000 | Loss: 0.00006220
Iteration 71/1000 | Loss: 0.00016391
Iteration 72/1000 | Loss: 0.00006185
Iteration 73/1000 | Loss: 0.00006177
Iteration 74/1000 | Loss: 0.00006174
Iteration 75/1000 | Loss: 0.00006173
Iteration 76/1000 | Loss: 0.00006173
Iteration 77/1000 | Loss: 0.00006173
Iteration 78/1000 | Loss: 0.00006172
Iteration 79/1000 | Loss: 0.00006166
Iteration 80/1000 | Loss: 0.00006165
Iteration 81/1000 | Loss: 0.00006162
Iteration 82/1000 | Loss: 0.00006161
Iteration 83/1000 | Loss: 0.00006161
Iteration 84/1000 | Loss: 0.00006161
Iteration 85/1000 | Loss: 0.00006161
Iteration 86/1000 | Loss: 0.00006160
Iteration 87/1000 | Loss: 0.00006160
Iteration 88/1000 | Loss: 0.00006160
Iteration 89/1000 | Loss: 0.00006160
Iteration 90/1000 | Loss: 0.00006160
Iteration 91/1000 | Loss: 0.00006159
Iteration 92/1000 | Loss: 0.00006159
Iteration 93/1000 | Loss: 0.00006158
Iteration 94/1000 | Loss: 0.00006158
Iteration 95/1000 | Loss: 0.00006158
Iteration 96/1000 | Loss: 0.00006158
Iteration 97/1000 | Loss: 0.00006158
Iteration 98/1000 | Loss: 0.00006158
Iteration 99/1000 | Loss: 0.00006158
Iteration 100/1000 | Loss: 0.00014180
Iteration 101/1000 | Loss: 0.00006176
Iteration 102/1000 | Loss: 0.00006156
Iteration 103/1000 | Loss: 0.00006155
Iteration 104/1000 | Loss: 0.00006155
Iteration 105/1000 | Loss: 0.00006155
Iteration 106/1000 | Loss: 0.00006155
Iteration 107/1000 | Loss: 0.00006155
Iteration 108/1000 | Loss: 0.00006155
Iteration 109/1000 | Loss: 0.00006155
Iteration 110/1000 | Loss: 0.00006155
Iteration 111/1000 | Loss: 0.00006155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [6.155165465315804e-05, 6.155165465315804e-05, 6.155165465315804e-05, 6.155165465315804e-05, 6.155165465315804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.155165465315804e-05

Optimization complete. Final v2v error: 4.360905170440674 mm

Highest mean error: 10.367908477783203 mm for frame 64

Lowest mean error: 2.8712799549102783 mm for frame 15

Saving results

Total time: 155.96539735794067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699258
Iteration 2/25 | Loss: 0.00160557
Iteration 3/25 | Loss: 0.00126107
Iteration 4/25 | Loss: 0.00121198
Iteration 5/25 | Loss: 0.00119977
Iteration 6/25 | Loss: 0.00119808
Iteration 7/25 | Loss: 0.00119267
Iteration 8/25 | Loss: 0.00119083
Iteration 9/25 | Loss: 0.00118803
Iteration 10/25 | Loss: 0.00118701
Iteration 11/25 | Loss: 0.00118660
Iteration 12/25 | Loss: 0.00118648
Iteration 13/25 | Loss: 0.00118646
Iteration 14/25 | Loss: 0.00118646
Iteration 15/25 | Loss: 0.00118645
Iteration 16/25 | Loss: 0.00118645
Iteration 17/25 | Loss: 0.00118645
Iteration 18/25 | Loss: 0.00118645
Iteration 19/25 | Loss: 0.00118645
Iteration 20/25 | Loss: 0.00118645
Iteration 21/25 | Loss: 0.00118645
Iteration 22/25 | Loss: 0.00118645
Iteration 23/25 | Loss: 0.00118645
Iteration 24/25 | Loss: 0.00118644
Iteration 25/25 | Loss: 0.00118644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.51052809
Iteration 2/25 | Loss: 0.00086817
Iteration 3/25 | Loss: 0.00086817
Iteration 4/25 | Loss: 0.00086817
Iteration 5/25 | Loss: 0.00086817
Iteration 6/25 | Loss: 0.00086817
Iteration 7/25 | Loss: 0.00086817
Iteration 8/25 | Loss: 0.00086817
Iteration 9/25 | Loss: 0.00086817
Iteration 10/25 | Loss: 0.00086817
Iteration 11/25 | Loss: 0.00086817
Iteration 12/25 | Loss: 0.00086817
Iteration 13/25 | Loss: 0.00086817
Iteration 14/25 | Loss: 0.00086817
Iteration 15/25 | Loss: 0.00086817
Iteration 16/25 | Loss: 0.00086817
Iteration 17/25 | Loss: 0.00086817
Iteration 18/25 | Loss: 0.00086817
Iteration 19/25 | Loss: 0.00086817
Iteration 20/25 | Loss: 0.00086817
Iteration 21/25 | Loss: 0.00086817
Iteration 22/25 | Loss: 0.00086817
Iteration 23/25 | Loss: 0.00086817
Iteration 24/25 | Loss: 0.00086817
Iteration 25/25 | Loss: 0.00086817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086817
Iteration 2/1000 | Loss: 0.00003746
Iteration 3/1000 | Loss: 0.00007931
Iteration 4/1000 | Loss: 0.00005931
Iteration 5/1000 | Loss: 0.00007968
Iteration 6/1000 | Loss: 0.00005001
Iteration 7/1000 | Loss: 0.00008039
Iteration 8/1000 | Loss: 0.00005477
Iteration 9/1000 | Loss: 0.00007691
Iteration 10/1000 | Loss: 0.00003424
Iteration 11/1000 | Loss: 0.00002471
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00002039
Iteration 14/1000 | Loss: 0.00001969
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001832
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00007416
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001724
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001709
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001707
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001700
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001697
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001696
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00001693
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001692
Iteration 124/1000 | Loss: 0.00001692
Iteration 125/1000 | Loss: 0.00001692
Iteration 126/1000 | Loss: 0.00001692
Iteration 127/1000 | Loss: 0.00001692
Iteration 128/1000 | Loss: 0.00001692
Iteration 129/1000 | Loss: 0.00001692
Iteration 130/1000 | Loss: 0.00001692
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001691
Iteration 133/1000 | Loss: 0.00001691
Iteration 134/1000 | Loss: 0.00001691
Iteration 135/1000 | Loss: 0.00001691
Iteration 136/1000 | Loss: 0.00001691
Iteration 137/1000 | Loss: 0.00001691
Iteration 138/1000 | Loss: 0.00001691
Iteration 139/1000 | Loss: 0.00001691
Iteration 140/1000 | Loss: 0.00001690
Iteration 141/1000 | Loss: 0.00001690
Iteration 142/1000 | Loss: 0.00001690
Iteration 143/1000 | Loss: 0.00001690
Iteration 144/1000 | Loss: 0.00001690
Iteration 145/1000 | Loss: 0.00001690
Iteration 146/1000 | Loss: 0.00001690
Iteration 147/1000 | Loss: 0.00001690
Iteration 148/1000 | Loss: 0.00001689
Iteration 149/1000 | Loss: 0.00001689
Iteration 150/1000 | Loss: 0.00001689
Iteration 151/1000 | Loss: 0.00001689
Iteration 152/1000 | Loss: 0.00001689
Iteration 153/1000 | Loss: 0.00001689
Iteration 154/1000 | Loss: 0.00001689
Iteration 155/1000 | Loss: 0.00001689
Iteration 156/1000 | Loss: 0.00001689
Iteration 157/1000 | Loss: 0.00001689
Iteration 158/1000 | Loss: 0.00001689
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001689
Iteration 161/1000 | Loss: 0.00001689
Iteration 162/1000 | Loss: 0.00001689
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001689
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001689
Iteration 172/1000 | Loss: 0.00001689
Iteration 173/1000 | Loss: 0.00001689
Iteration 174/1000 | Loss: 0.00001689
Iteration 175/1000 | Loss: 0.00001689
Iteration 176/1000 | Loss: 0.00001689
Iteration 177/1000 | Loss: 0.00001689
Iteration 178/1000 | Loss: 0.00001689
Iteration 179/1000 | Loss: 0.00001689
Iteration 180/1000 | Loss: 0.00001689
Iteration 181/1000 | Loss: 0.00001689
Iteration 182/1000 | Loss: 0.00001689
Iteration 183/1000 | Loss: 0.00001689
Iteration 184/1000 | Loss: 0.00001689
Iteration 185/1000 | Loss: 0.00001689
Iteration 186/1000 | Loss: 0.00001689
Iteration 187/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.6891712220967747e-05, 1.6891712220967747e-05, 1.6891712220967747e-05, 1.6891712220967747e-05, 1.6891712220967747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6891712220967747e-05

Optimization complete. Final v2v error: 3.391144275665283 mm

Highest mean error: 5.420645713806152 mm for frame 67

Lowest mean error: 2.5558416843414307 mm for frame 197

Saving results

Total time: 76.89239645004272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858655
Iteration 2/25 | Loss: 0.00134209
Iteration 3/25 | Loss: 0.00126250
Iteration 4/25 | Loss: 0.00124379
Iteration 5/25 | Loss: 0.00123798
Iteration 6/25 | Loss: 0.00123731
Iteration 7/25 | Loss: 0.00123731
Iteration 8/25 | Loss: 0.00123731
Iteration 9/25 | Loss: 0.00123731
Iteration 10/25 | Loss: 0.00123731
Iteration 11/25 | Loss: 0.00123731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012373082572594285, 0.0012373082572594285, 0.0012373082572594285, 0.0012373082572594285, 0.0012373082572594285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012373082572594285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29351079
Iteration 2/25 | Loss: 0.00105085
Iteration 3/25 | Loss: 0.00105078
Iteration 4/25 | Loss: 0.00105078
Iteration 5/25 | Loss: 0.00105078
Iteration 6/25 | Loss: 0.00105078
Iteration 7/25 | Loss: 0.00105078
Iteration 8/25 | Loss: 0.00105078
Iteration 9/25 | Loss: 0.00105078
Iteration 10/25 | Loss: 0.00105078
Iteration 11/25 | Loss: 0.00105078
Iteration 12/25 | Loss: 0.00105078
Iteration 13/25 | Loss: 0.00105078
Iteration 14/25 | Loss: 0.00105078
Iteration 15/25 | Loss: 0.00105078
Iteration 16/25 | Loss: 0.00105078
Iteration 17/25 | Loss: 0.00105078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010507775004953146, 0.0010507775004953146, 0.0010507775004953146, 0.0010507775004953146, 0.0010507775004953146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010507775004953146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105078
Iteration 2/1000 | Loss: 0.00005376
Iteration 3/1000 | Loss: 0.00003298
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002640
Iteration 6/1000 | Loss: 0.00002533
Iteration 7/1000 | Loss: 0.00002461
Iteration 8/1000 | Loss: 0.00002390
Iteration 9/1000 | Loss: 0.00002341
Iteration 10/1000 | Loss: 0.00002305
Iteration 11/1000 | Loss: 0.00002277
Iteration 12/1000 | Loss: 0.00002256
Iteration 13/1000 | Loss: 0.00002249
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002225
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002221
Iteration 19/1000 | Loss: 0.00002220
Iteration 20/1000 | Loss: 0.00002220
Iteration 21/1000 | Loss: 0.00002217
Iteration 22/1000 | Loss: 0.00002215
Iteration 23/1000 | Loss: 0.00002214
Iteration 24/1000 | Loss: 0.00002214
Iteration 25/1000 | Loss: 0.00002213
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002212
Iteration 28/1000 | Loss: 0.00002211
Iteration 29/1000 | Loss: 0.00002211
Iteration 30/1000 | Loss: 0.00002210
Iteration 31/1000 | Loss: 0.00002210
Iteration 32/1000 | Loss: 0.00002210
Iteration 33/1000 | Loss: 0.00002209
Iteration 34/1000 | Loss: 0.00002209
Iteration 35/1000 | Loss: 0.00002209
Iteration 36/1000 | Loss: 0.00002208
Iteration 37/1000 | Loss: 0.00002208
Iteration 38/1000 | Loss: 0.00002206
Iteration 39/1000 | Loss: 0.00002206
Iteration 40/1000 | Loss: 0.00002206
Iteration 41/1000 | Loss: 0.00002206
Iteration 42/1000 | Loss: 0.00002206
Iteration 43/1000 | Loss: 0.00002206
Iteration 44/1000 | Loss: 0.00002205
Iteration 45/1000 | Loss: 0.00002205
Iteration 46/1000 | Loss: 0.00002205
Iteration 47/1000 | Loss: 0.00002205
Iteration 48/1000 | Loss: 0.00002205
Iteration 49/1000 | Loss: 0.00002204
Iteration 50/1000 | Loss: 0.00002204
Iteration 51/1000 | Loss: 0.00002204
Iteration 52/1000 | Loss: 0.00002203
Iteration 53/1000 | Loss: 0.00002203
Iteration 54/1000 | Loss: 0.00002202
Iteration 55/1000 | Loss: 0.00002201
Iteration 56/1000 | Loss: 0.00002200
Iteration 57/1000 | Loss: 0.00002200
Iteration 58/1000 | Loss: 0.00002200
Iteration 59/1000 | Loss: 0.00002200
Iteration 60/1000 | Loss: 0.00002200
Iteration 61/1000 | Loss: 0.00002200
Iteration 62/1000 | Loss: 0.00002200
Iteration 63/1000 | Loss: 0.00002200
Iteration 64/1000 | Loss: 0.00002199
Iteration 65/1000 | Loss: 0.00002199
Iteration 66/1000 | Loss: 0.00002199
Iteration 67/1000 | Loss: 0.00002199
Iteration 68/1000 | Loss: 0.00002199
Iteration 69/1000 | Loss: 0.00002199
Iteration 70/1000 | Loss: 0.00002199
Iteration 71/1000 | Loss: 0.00002199
Iteration 72/1000 | Loss: 0.00002199
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002199
Iteration 75/1000 | Loss: 0.00002198
Iteration 76/1000 | Loss: 0.00002198
Iteration 77/1000 | Loss: 0.00002198
Iteration 78/1000 | Loss: 0.00002196
Iteration 79/1000 | Loss: 0.00002196
Iteration 80/1000 | Loss: 0.00002195
Iteration 81/1000 | Loss: 0.00002195
Iteration 82/1000 | Loss: 0.00002195
Iteration 83/1000 | Loss: 0.00002194
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002193
Iteration 86/1000 | Loss: 0.00002193
Iteration 87/1000 | Loss: 0.00002192
Iteration 88/1000 | Loss: 0.00002192
Iteration 89/1000 | Loss: 0.00002191
Iteration 90/1000 | Loss: 0.00002191
Iteration 91/1000 | Loss: 0.00002190
Iteration 92/1000 | Loss: 0.00002190
Iteration 93/1000 | Loss: 0.00002190
Iteration 94/1000 | Loss: 0.00002190
Iteration 95/1000 | Loss: 0.00002189
Iteration 96/1000 | Loss: 0.00002189
Iteration 97/1000 | Loss: 0.00002189
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002188
Iteration 100/1000 | Loss: 0.00002187
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002186
Iteration 103/1000 | Loss: 0.00002186
Iteration 104/1000 | Loss: 0.00002186
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002182
Iteration 110/1000 | Loss: 0.00002182
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002180
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002179
Iteration 116/1000 | Loss: 0.00002179
Iteration 117/1000 | Loss: 0.00002179
Iteration 118/1000 | Loss: 0.00002179
Iteration 119/1000 | Loss: 0.00002178
Iteration 120/1000 | Loss: 0.00002178
Iteration 121/1000 | Loss: 0.00002177
Iteration 122/1000 | Loss: 0.00002177
Iteration 123/1000 | Loss: 0.00002177
Iteration 124/1000 | Loss: 0.00002177
Iteration 125/1000 | Loss: 0.00002176
Iteration 126/1000 | Loss: 0.00002176
Iteration 127/1000 | Loss: 0.00002176
Iteration 128/1000 | Loss: 0.00002176
Iteration 129/1000 | Loss: 0.00002176
Iteration 130/1000 | Loss: 0.00002175
Iteration 131/1000 | Loss: 0.00002175
Iteration 132/1000 | Loss: 0.00002175
Iteration 133/1000 | Loss: 0.00002175
Iteration 134/1000 | Loss: 0.00002175
Iteration 135/1000 | Loss: 0.00002175
Iteration 136/1000 | Loss: 0.00002174
Iteration 137/1000 | Loss: 0.00002174
Iteration 138/1000 | Loss: 0.00002174
Iteration 139/1000 | Loss: 0.00002174
Iteration 140/1000 | Loss: 0.00002174
Iteration 141/1000 | Loss: 0.00002174
Iteration 142/1000 | Loss: 0.00002174
Iteration 143/1000 | Loss: 0.00002174
Iteration 144/1000 | Loss: 0.00002174
Iteration 145/1000 | Loss: 0.00002174
Iteration 146/1000 | Loss: 0.00002173
Iteration 147/1000 | Loss: 0.00002173
Iteration 148/1000 | Loss: 0.00002173
Iteration 149/1000 | Loss: 0.00002173
Iteration 150/1000 | Loss: 0.00002173
Iteration 151/1000 | Loss: 0.00002173
Iteration 152/1000 | Loss: 0.00002173
Iteration 153/1000 | Loss: 0.00002173
Iteration 154/1000 | Loss: 0.00002173
Iteration 155/1000 | Loss: 0.00002173
Iteration 156/1000 | Loss: 0.00002172
Iteration 157/1000 | Loss: 0.00002172
Iteration 158/1000 | Loss: 0.00002172
Iteration 159/1000 | Loss: 0.00002172
Iteration 160/1000 | Loss: 0.00002172
Iteration 161/1000 | Loss: 0.00002172
Iteration 162/1000 | Loss: 0.00002172
Iteration 163/1000 | Loss: 0.00002172
Iteration 164/1000 | Loss: 0.00002172
Iteration 165/1000 | Loss: 0.00002171
Iteration 166/1000 | Loss: 0.00002171
Iteration 167/1000 | Loss: 0.00002171
Iteration 168/1000 | Loss: 0.00002171
Iteration 169/1000 | Loss: 0.00002171
Iteration 170/1000 | Loss: 0.00002171
Iteration 171/1000 | Loss: 0.00002171
Iteration 172/1000 | Loss: 0.00002171
Iteration 173/1000 | Loss: 0.00002171
Iteration 174/1000 | Loss: 0.00002170
Iteration 175/1000 | Loss: 0.00002170
Iteration 176/1000 | Loss: 0.00002170
Iteration 177/1000 | Loss: 0.00002170
Iteration 178/1000 | Loss: 0.00002170
Iteration 179/1000 | Loss: 0.00002169
Iteration 180/1000 | Loss: 0.00002169
Iteration 181/1000 | Loss: 0.00002169
Iteration 182/1000 | Loss: 0.00002169
Iteration 183/1000 | Loss: 0.00002169
Iteration 184/1000 | Loss: 0.00002169
Iteration 185/1000 | Loss: 0.00002169
Iteration 186/1000 | Loss: 0.00002168
Iteration 187/1000 | Loss: 0.00002168
Iteration 188/1000 | Loss: 0.00002168
Iteration 189/1000 | Loss: 0.00002168
Iteration 190/1000 | Loss: 0.00002168
Iteration 191/1000 | Loss: 0.00002168
Iteration 192/1000 | Loss: 0.00002168
Iteration 193/1000 | Loss: 0.00002168
Iteration 194/1000 | Loss: 0.00002168
Iteration 195/1000 | Loss: 0.00002168
Iteration 196/1000 | Loss: 0.00002168
Iteration 197/1000 | Loss: 0.00002167
Iteration 198/1000 | Loss: 0.00002167
Iteration 199/1000 | Loss: 0.00002167
Iteration 200/1000 | Loss: 0.00002167
Iteration 201/1000 | Loss: 0.00002167
Iteration 202/1000 | Loss: 0.00002167
Iteration 203/1000 | Loss: 0.00002167
Iteration 204/1000 | Loss: 0.00002167
Iteration 205/1000 | Loss: 0.00002167
Iteration 206/1000 | Loss: 0.00002166
Iteration 207/1000 | Loss: 0.00002166
Iteration 208/1000 | Loss: 0.00002166
Iteration 209/1000 | Loss: 0.00002166
Iteration 210/1000 | Loss: 0.00002166
Iteration 211/1000 | Loss: 0.00002166
Iteration 212/1000 | Loss: 0.00002166
Iteration 213/1000 | Loss: 0.00002166
Iteration 214/1000 | Loss: 0.00002166
Iteration 215/1000 | Loss: 0.00002166
Iteration 216/1000 | Loss: 0.00002166
Iteration 217/1000 | Loss: 0.00002166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.166302510886453e-05, 2.166302510886453e-05, 2.166302510886453e-05, 2.166302510886453e-05, 2.166302510886453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.166302510886453e-05

Optimization complete. Final v2v error: 3.940175771713257 mm

Highest mean error: 4.245877265930176 mm for frame 110

Lowest mean error: 3.5267250537872314 mm for frame 0

Saving results

Total time: 44.546242475509644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058646
Iteration 2/25 | Loss: 0.01058646
Iteration 3/25 | Loss: 0.01058646
Iteration 4/25 | Loss: 0.01058646
Iteration 5/25 | Loss: 0.01058646
Iteration 6/25 | Loss: 0.01058646
Iteration 7/25 | Loss: 0.01058646
Iteration 8/25 | Loss: 0.01058646
Iteration 9/25 | Loss: 0.01058646
Iteration 10/25 | Loss: 0.01058646
Iteration 11/25 | Loss: 0.01058646
Iteration 12/25 | Loss: 0.01058645
Iteration 13/25 | Loss: 0.01058645
Iteration 14/25 | Loss: 0.01058645
Iteration 15/25 | Loss: 0.01058645
Iteration 16/25 | Loss: 0.01058645
Iteration 17/25 | Loss: 0.01058645
Iteration 18/25 | Loss: 0.01058645
Iteration 19/25 | Loss: 0.01058645
Iteration 20/25 | Loss: 0.01058645
Iteration 21/25 | Loss: 0.01058645
Iteration 22/25 | Loss: 0.01058645
Iteration 23/25 | Loss: 0.01058645
Iteration 24/25 | Loss: 0.01058645
Iteration 25/25 | Loss: 0.01058645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75075161
Iteration 2/25 | Loss: 0.06080464
Iteration 3/25 | Loss: 0.06081985
Iteration 4/25 | Loss: 0.05975161
Iteration 5/25 | Loss: 0.05975161
Iteration 6/25 | Loss: 0.05975161
Iteration 7/25 | Loss: 0.05975161
Iteration 8/25 | Loss: 0.05975160
Iteration 9/25 | Loss: 0.05975161
Iteration 10/25 | Loss: 0.05975161
Iteration 11/25 | Loss: 0.05975161
Iteration 12/25 | Loss: 0.05975160
Iteration 13/25 | Loss: 0.05975161
Iteration 14/25 | Loss: 0.05975160
Iteration 15/25 | Loss: 0.05975160
Iteration 16/25 | Loss: 0.05975160
Iteration 17/25 | Loss: 0.05975160
Iteration 18/25 | Loss: 0.05975160
Iteration 19/25 | Loss: 0.05975160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.05975160375237465, 0.05975160375237465, 0.05975160375237465, 0.05975160375237465, 0.05975160375237465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05975160375237465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05975160
Iteration 2/1000 | Loss: 0.00278387
Iteration 3/1000 | Loss: 0.00079635
Iteration 4/1000 | Loss: 0.00113631
Iteration 5/1000 | Loss: 0.00022485
Iteration 6/1000 | Loss: 0.00021477
Iteration 7/1000 | Loss: 0.00011068
Iteration 8/1000 | Loss: 0.00032173
Iteration 9/1000 | Loss: 0.00008653
Iteration 10/1000 | Loss: 0.00006802
Iteration 11/1000 | Loss: 0.00018231
Iteration 12/1000 | Loss: 0.00004256
Iteration 13/1000 | Loss: 0.00003842
Iteration 14/1000 | Loss: 0.00015915
Iteration 15/1000 | Loss: 0.00008977
Iteration 16/1000 | Loss: 0.00008248
Iteration 17/1000 | Loss: 0.00015413
Iteration 18/1000 | Loss: 0.00002968
Iteration 19/1000 | Loss: 0.00004070
Iteration 20/1000 | Loss: 0.00010191
Iteration 21/1000 | Loss: 0.00009027
Iteration 22/1000 | Loss: 0.00021917
Iteration 23/1000 | Loss: 0.00012347
Iteration 24/1000 | Loss: 0.00008610
Iteration 25/1000 | Loss: 0.00007674
Iteration 26/1000 | Loss: 0.00005296
Iteration 27/1000 | Loss: 0.00004731
Iteration 28/1000 | Loss: 0.00002250
Iteration 29/1000 | Loss: 0.00007072
Iteration 30/1000 | Loss: 0.00009014
Iteration 31/1000 | Loss: 0.00003547
Iteration 32/1000 | Loss: 0.00009161
Iteration 33/1000 | Loss: 0.00002075
Iteration 34/1000 | Loss: 0.00008909
Iteration 35/1000 | Loss: 0.00007613
Iteration 36/1000 | Loss: 0.00002943
Iteration 37/1000 | Loss: 0.00002290
Iteration 38/1000 | Loss: 0.00001981
Iteration 39/1000 | Loss: 0.00003199
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00002715
Iteration 42/1000 | Loss: 0.00004939
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001904
Iteration 45/1000 | Loss: 0.00004078
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001893
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001886
Iteration 67/1000 | Loss: 0.00001886
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001881
Iteration 79/1000 | Loss: 0.00001881
Iteration 80/1000 | Loss: 0.00001881
Iteration 81/1000 | Loss: 0.00001881
Iteration 82/1000 | Loss: 0.00001879
Iteration 83/1000 | Loss: 0.00001879
Iteration 84/1000 | Loss: 0.00001879
Iteration 85/1000 | Loss: 0.00007695
Iteration 86/1000 | Loss: 0.00004930
Iteration 87/1000 | Loss: 0.00002388
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00002671
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00001949
Iteration 96/1000 | Loss: 0.00002540
Iteration 97/1000 | Loss: 0.00001868
Iteration 98/1000 | Loss: 0.00001868
Iteration 99/1000 | Loss: 0.00001868
Iteration 100/1000 | Loss: 0.00001868
Iteration 101/1000 | Loss: 0.00001867
Iteration 102/1000 | Loss: 0.00001867
Iteration 103/1000 | Loss: 0.00001866
Iteration 104/1000 | Loss: 0.00001866
Iteration 105/1000 | Loss: 0.00001866
Iteration 106/1000 | Loss: 0.00001866
Iteration 107/1000 | Loss: 0.00001866
Iteration 108/1000 | Loss: 0.00001865
Iteration 109/1000 | Loss: 0.00001865
Iteration 110/1000 | Loss: 0.00001865
Iteration 111/1000 | Loss: 0.00001865
Iteration 112/1000 | Loss: 0.00001865
Iteration 113/1000 | Loss: 0.00001865
Iteration 114/1000 | Loss: 0.00001864
Iteration 115/1000 | Loss: 0.00001864
Iteration 116/1000 | Loss: 0.00001864
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00002951
Iteration 123/1000 | Loss: 0.00001995
Iteration 124/1000 | Loss: 0.00001862
Iteration 125/1000 | Loss: 0.00001862
Iteration 126/1000 | Loss: 0.00001862
Iteration 127/1000 | Loss: 0.00001862
Iteration 128/1000 | Loss: 0.00001862
Iteration 129/1000 | Loss: 0.00001862
Iteration 130/1000 | Loss: 0.00001862
Iteration 131/1000 | Loss: 0.00001862
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001861
Iteration 134/1000 | Loss: 0.00001861
Iteration 135/1000 | Loss: 0.00001861
Iteration 136/1000 | Loss: 0.00001861
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001861
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001861
Iteration 143/1000 | Loss: 0.00001861
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001860
Iteration 147/1000 | Loss: 0.00001860
Iteration 148/1000 | Loss: 0.00002087
Iteration 149/1000 | Loss: 0.00001859
Iteration 150/1000 | Loss: 0.00001859
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001859
Iteration 153/1000 | Loss: 0.00001859
Iteration 154/1000 | Loss: 0.00001859
Iteration 155/1000 | Loss: 0.00001859
Iteration 156/1000 | Loss: 0.00001859
Iteration 157/1000 | Loss: 0.00001859
Iteration 158/1000 | Loss: 0.00001859
Iteration 159/1000 | Loss: 0.00001859
Iteration 160/1000 | Loss: 0.00001859
Iteration 161/1000 | Loss: 0.00001859
Iteration 162/1000 | Loss: 0.00001859
Iteration 163/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.859079384303186e-05, 1.859079384303186e-05, 1.859079384303186e-05, 1.859079384303186e-05, 1.859079384303186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.859079384303186e-05

Optimization complete. Final v2v error: 3.7013938426971436 mm

Highest mean error: 4.1275434494018555 mm for frame 153

Lowest mean error: 3.363988161087036 mm for frame 167

Saving results

Total time: 99.80143880844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486941
Iteration 2/25 | Loss: 0.00131612
Iteration 3/25 | Loss: 0.00121351
Iteration 4/25 | Loss: 0.00119450
Iteration 5/25 | Loss: 0.00118886
Iteration 6/25 | Loss: 0.00118849
Iteration 7/25 | Loss: 0.00118849
Iteration 8/25 | Loss: 0.00118849
Iteration 9/25 | Loss: 0.00118849
Iteration 10/25 | Loss: 0.00118849
Iteration 11/25 | Loss: 0.00118849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011884879786521196, 0.0011884879786521196, 0.0011884879786521196, 0.0011884879786521196, 0.0011884879786521196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011884879786521196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34818959
Iteration 2/25 | Loss: 0.00088368
Iteration 3/25 | Loss: 0.00088367
Iteration 4/25 | Loss: 0.00088367
Iteration 5/25 | Loss: 0.00088367
Iteration 6/25 | Loss: 0.00088367
Iteration 7/25 | Loss: 0.00088367
Iteration 8/25 | Loss: 0.00088367
Iteration 9/25 | Loss: 0.00088367
Iteration 10/25 | Loss: 0.00088367
Iteration 11/25 | Loss: 0.00088367
Iteration 12/25 | Loss: 0.00088367
Iteration 13/25 | Loss: 0.00088367
Iteration 14/25 | Loss: 0.00088367
Iteration 15/25 | Loss: 0.00088367
Iteration 16/25 | Loss: 0.00088367
Iteration 17/25 | Loss: 0.00088367
Iteration 18/25 | Loss: 0.00088367
Iteration 19/25 | Loss: 0.00088367
Iteration 20/25 | Loss: 0.00088367
Iteration 21/25 | Loss: 0.00088367
Iteration 22/25 | Loss: 0.00088367
Iteration 23/25 | Loss: 0.00088367
Iteration 24/25 | Loss: 0.00088367
Iteration 25/25 | Loss: 0.00088367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088367
Iteration 2/1000 | Loss: 0.00004033
Iteration 3/1000 | Loss: 0.00002940
Iteration 4/1000 | Loss: 0.00002525
Iteration 5/1000 | Loss: 0.00002402
Iteration 6/1000 | Loss: 0.00002335
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002223
Iteration 9/1000 | Loss: 0.00002205
Iteration 10/1000 | Loss: 0.00002175
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002132
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002121
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002119
Iteration 18/1000 | Loss: 0.00002119
Iteration 19/1000 | Loss: 0.00002116
Iteration 20/1000 | Loss: 0.00002115
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00002106
Iteration 23/1000 | Loss: 0.00002105
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002103
Iteration 26/1000 | Loss: 0.00002102
Iteration 27/1000 | Loss: 0.00002100
Iteration 28/1000 | Loss: 0.00002099
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002099
Iteration 31/1000 | Loss: 0.00002097
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002094
Iteration 34/1000 | Loss: 0.00002094
Iteration 35/1000 | Loss: 0.00002093
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002088
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002075
Iteration 45/1000 | Loss: 0.00002075
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002075
Iteration 49/1000 | Loss: 0.00002075
Iteration 50/1000 | Loss: 0.00002074
Iteration 51/1000 | Loss: 0.00002074
Iteration 52/1000 | Loss: 0.00002074
Iteration 53/1000 | Loss: 0.00002074
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002073
Iteration 56/1000 | Loss: 0.00002073
Iteration 57/1000 | Loss: 0.00002073
Iteration 58/1000 | Loss: 0.00002072
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002072
Iteration 61/1000 | Loss: 0.00002071
Iteration 62/1000 | Loss: 0.00002071
Iteration 63/1000 | Loss: 0.00002071
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002070
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002066
Iteration 86/1000 | Loss: 0.00002066
Iteration 87/1000 | Loss: 0.00002066
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002064
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002062
Iteration 97/1000 | Loss: 0.00002062
Iteration 98/1000 | Loss: 0.00002062
Iteration 99/1000 | Loss: 0.00002062
Iteration 100/1000 | Loss: 0.00002062
Iteration 101/1000 | Loss: 0.00002062
Iteration 102/1000 | Loss: 0.00002062
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002061
Iteration 105/1000 | Loss: 0.00002061
Iteration 106/1000 | Loss: 0.00002061
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002061
Iteration 109/1000 | Loss: 0.00002061
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002060
Iteration 113/1000 | Loss: 0.00002060
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002060
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002058
Iteration 123/1000 | Loss: 0.00002058
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002057
Iteration 126/1000 | Loss: 0.00002057
Iteration 127/1000 | Loss: 0.00002057
Iteration 128/1000 | Loss: 0.00002057
Iteration 129/1000 | Loss: 0.00002057
Iteration 130/1000 | Loss: 0.00002057
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002056
Iteration 135/1000 | Loss: 0.00002056
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002056
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002056
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002055
Iteration 145/1000 | Loss: 0.00002055
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002054
Iteration 148/1000 | Loss: 0.00002054
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002054
Iteration 155/1000 | Loss: 0.00002054
Iteration 156/1000 | Loss: 0.00002053
Iteration 157/1000 | Loss: 0.00002053
Iteration 158/1000 | Loss: 0.00002053
Iteration 159/1000 | Loss: 0.00002053
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002053
Iteration 162/1000 | Loss: 0.00002053
Iteration 163/1000 | Loss: 0.00002053
Iteration 164/1000 | Loss: 0.00002052
Iteration 165/1000 | Loss: 0.00002052
Iteration 166/1000 | Loss: 0.00002052
Iteration 167/1000 | Loss: 0.00002052
Iteration 168/1000 | Loss: 0.00002052
Iteration 169/1000 | Loss: 0.00002052
Iteration 170/1000 | Loss: 0.00002052
Iteration 171/1000 | Loss: 0.00002052
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002051
Iteration 174/1000 | Loss: 0.00002051
Iteration 175/1000 | Loss: 0.00002051
Iteration 176/1000 | Loss: 0.00002051
Iteration 177/1000 | Loss: 0.00002051
Iteration 178/1000 | Loss: 0.00002051
Iteration 179/1000 | Loss: 0.00002051
Iteration 180/1000 | Loss: 0.00002051
Iteration 181/1000 | Loss: 0.00002051
Iteration 182/1000 | Loss: 0.00002051
Iteration 183/1000 | Loss: 0.00002051
Iteration 184/1000 | Loss: 0.00002050
Iteration 185/1000 | Loss: 0.00002050
Iteration 186/1000 | Loss: 0.00002050
Iteration 187/1000 | Loss: 0.00002050
Iteration 188/1000 | Loss: 0.00002050
Iteration 189/1000 | Loss: 0.00002050
Iteration 190/1000 | Loss: 0.00002050
Iteration 191/1000 | Loss: 0.00002050
Iteration 192/1000 | Loss: 0.00002050
Iteration 193/1000 | Loss: 0.00002050
Iteration 194/1000 | Loss: 0.00002050
Iteration 195/1000 | Loss: 0.00002050
Iteration 196/1000 | Loss: 0.00002050
Iteration 197/1000 | Loss: 0.00002050
Iteration 198/1000 | Loss: 0.00002050
Iteration 199/1000 | Loss: 0.00002050
Iteration 200/1000 | Loss: 0.00002050
Iteration 201/1000 | Loss: 0.00002050
Iteration 202/1000 | Loss: 0.00002050
Iteration 203/1000 | Loss: 0.00002050
Iteration 204/1000 | Loss: 0.00002050
Iteration 205/1000 | Loss: 0.00002050
Iteration 206/1000 | Loss: 0.00002050
Iteration 207/1000 | Loss: 0.00002050
Iteration 208/1000 | Loss: 0.00002050
Iteration 209/1000 | Loss: 0.00002050
Iteration 210/1000 | Loss: 0.00002050
Iteration 211/1000 | Loss: 0.00002050
Iteration 212/1000 | Loss: 0.00002050
Iteration 213/1000 | Loss: 0.00002050
Iteration 214/1000 | Loss: 0.00002050
Iteration 215/1000 | Loss: 0.00002050
Iteration 216/1000 | Loss: 0.00002050
Iteration 217/1000 | Loss: 0.00002050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.0495224816841073e-05, 2.0495224816841073e-05, 2.0495224816841073e-05, 2.0495224816841073e-05, 2.0495224816841073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0495224816841073e-05

Optimization complete. Final v2v error: 3.5306930541992188 mm

Highest mean error: 4.556436061859131 mm for frame 77

Lowest mean error: 3.069526195526123 mm for frame 114

Saving results

Total time: 43.22480082511902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762389
Iteration 2/25 | Loss: 0.00129288
Iteration 3/25 | Loss: 0.00119689
Iteration 4/25 | Loss: 0.00118265
Iteration 5/25 | Loss: 0.00118023
Iteration 6/25 | Loss: 0.00118015
Iteration 7/25 | Loss: 0.00118015
Iteration 8/25 | Loss: 0.00118015
Iteration 9/25 | Loss: 0.00118015
Iteration 10/25 | Loss: 0.00118015
Iteration 11/25 | Loss: 0.00118015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011801487999036908, 0.0011801487999036908, 0.0011801487999036908, 0.0011801487999036908, 0.0011801487999036908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011801487999036908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34565127
Iteration 2/25 | Loss: 0.00078512
Iteration 3/25 | Loss: 0.00078503
Iteration 4/25 | Loss: 0.00078503
Iteration 5/25 | Loss: 0.00078503
Iteration 6/25 | Loss: 0.00078503
Iteration 7/25 | Loss: 0.00078503
Iteration 8/25 | Loss: 0.00078503
Iteration 9/25 | Loss: 0.00078503
Iteration 10/25 | Loss: 0.00078503
Iteration 11/25 | Loss: 0.00078503
Iteration 12/25 | Loss: 0.00078503
Iteration 13/25 | Loss: 0.00078503
Iteration 14/25 | Loss: 0.00078503
Iteration 15/25 | Loss: 0.00078503
Iteration 16/25 | Loss: 0.00078503
Iteration 17/25 | Loss: 0.00078503
Iteration 18/25 | Loss: 0.00078503
Iteration 19/25 | Loss: 0.00078503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007850279216654599, 0.0007850279216654599, 0.0007850279216654599, 0.0007850279216654599, 0.0007850279216654599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007850279216654599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078503
Iteration 2/1000 | Loss: 0.00003331
Iteration 3/1000 | Loss: 0.00001870
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001279
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001266
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001253
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001236
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001216
Iteration 29/1000 | Loss: 0.00001216
Iteration 30/1000 | Loss: 0.00001215
Iteration 31/1000 | Loss: 0.00001215
Iteration 32/1000 | Loss: 0.00001214
Iteration 33/1000 | Loss: 0.00001214
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001213
Iteration 36/1000 | Loss: 0.00001213
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001209
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001209
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.1997838555544149e-05, 1.1997838555544149e-05, 1.1997838555544149e-05, 1.1997838555544149e-05, 1.1997838555544149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1997838555544149e-05

Optimization complete. Final v2v error: 2.907365560531616 mm

Highest mean error: 3.346656560897827 mm for frame 22

Lowest mean error: 2.606410264968872 mm for frame 187

Saving results

Total time: 41.98985242843628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600232
Iteration 2/25 | Loss: 0.00125238
Iteration 3/25 | Loss: 0.00120381
Iteration 4/25 | Loss: 0.00119872
Iteration 5/25 | Loss: 0.00119699
Iteration 6/25 | Loss: 0.00119699
Iteration 7/25 | Loss: 0.00119699
Iteration 8/25 | Loss: 0.00119699
Iteration 9/25 | Loss: 0.00119699
Iteration 10/25 | Loss: 0.00119699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011969880433753133, 0.0011969880433753133, 0.0011969880433753133, 0.0011969880433753133, 0.0011969880433753133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011969880433753133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.10059786
Iteration 2/25 | Loss: 0.00080346
Iteration 3/25 | Loss: 0.00080345
Iteration 4/25 | Loss: 0.00080344
Iteration 5/25 | Loss: 0.00080344
Iteration 6/25 | Loss: 0.00080344
Iteration 7/25 | Loss: 0.00080344
Iteration 8/25 | Loss: 0.00080344
Iteration 9/25 | Loss: 0.00080344
Iteration 10/25 | Loss: 0.00080344
Iteration 11/25 | Loss: 0.00080344
Iteration 12/25 | Loss: 0.00080344
Iteration 13/25 | Loss: 0.00080344
Iteration 14/25 | Loss: 0.00080344
Iteration 15/25 | Loss: 0.00080344
Iteration 16/25 | Loss: 0.00080344
Iteration 17/25 | Loss: 0.00080344
Iteration 18/25 | Loss: 0.00080344
Iteration 19/25 | Loss: 0.00080344
Iteration 20/25 | Loss: 0.00080344
Iteration 21/25 | Loss: 0.00080344
Iteration 22/25 | Loss: 0.00080344
Iteration 23/25 | Loss: 0.00080344
Iteration 24/25 | Loss: 0.00080344
Iteration 25/25 | Loss: 0.00080344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080344
Iteration 2/1000 | Loss: 0.00002480
Iteration 3/1000 | Loss: 0.00001590
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001325
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001216
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001176
Iteration 18/1000 | Loss: 0.00001175
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001172
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001167
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001163
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001154
Iteration 39/1000 | Loss: 0.00001153
Iteration 40/1000 | Loss: 0.00001153
Iteration 41/1000 | Loss: 0.00001152
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001150
Iteration 44/1000 | Loss: 0.00001150
Iteration 45/1000 | Loss: 0.00001150
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001150
Iteration 48/1000 | Loss: 0.00001149
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001149
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001146
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001145
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001144
Iteration 62/1000 | Loss: 0.00001144
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001144
Iteration 65/1000 | Loss: 0.00001143
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001142
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001139
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001134
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001125
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001122
Iteration 88/1000 | Loss: 0.00001122
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001112
Iteration 128/1000 | Loss: 0.00001112
Iteration 129/1000 | Loss: 0.00001112
Iteration 130/1000 | Loss: 0.00001111
Iteration 131/1000 | Loss: 0.00001111
Iteration 132/1000 | Loss: 0.00001111
Iteration 133/1000 | Loss: 0.00001111
Iteration 134/1000 | Loss: 0.00001111
Iteration 135/1000 | Loss: 0.00001111
Iteration 136/1000 | Loss: 0.00001111
Iteration 137/1000 | Loss: 0.00001111
Iteration 138/1000 | Loss: 0.00001111
Iteration 139/1000 | Loss: 0.00001111
Iteration 140/1000 | Loss: 0.00001111
Iteration 141/1000 | Loss: 0.00001111
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001110
Iteration 146/1000 | Loss: 0.00001110
Iteration 147/1000 | Loss: 0.00001110
Iteration 148/1000 | Loss: 0.00001110
Iteration 149/1000 | Loss: 0.00001110
Iteration 150/1000 | Loss: 0.00001110
Iteration 151/1000 | Loss: 0.00001110
Iteration 152/1000 | Loss: 0.00001110
Iteration 153/1000 | Loss: 0.00001110
Iteration 154/1000 | Loss: 0.00001110
Iteration 155/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.1099943549197633e-05, 1.1099943549197633e-05, 1.1099943549197633e-05, 1.1099943549197633e-05, 1.1099943549197633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1099943549197633e-05

Optimization complete. Final v2v error: 2.837960958480835 mm

Highest mean error: 2.9804646968841553 mm for frame 133

Lowest mean error: 2.592498779296875 mm for frame 82

Saving results

Total time: 39.816081523895264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932604
Iteration 2/25 | Loss: 0.00243118
Iteration 3/25 | Loss: 0.00181868
Iteration 4/25 | Loss: 0.00170975
Iteration 5/25 | Loss: 0.00169270
Iteration 6/25 | Loss: 0.00159438
Iteration 7/25 | Loss: 0.00152436
Iteration 8/25 | Loss: 0.00149115
Iteration 9/25 | Loss: 0.00144696
Iteration 10/25 | Loss: 0.00139715
Iteration 11/25 | Loss: 0.00136469
Iteration 12/25 | Loss: 0.00135990
Iteration 13/25 | Loss: 0.00137576
Iteration 14/25 | Loss: 0.00138130
Iteration 15/25 | Loss: 0.00134867
Iteration 16/25 | Loss: 0.00132014
Iteration 17/25 | Loss: 0.00130296
Iteration 18/25 | Loss: 0.00130497
Iteration 19/25 | Loss: 0.00129837
Iteration 20/25 | Loss: 0.00128670
Iteration 21/25 | Loss: 0.00128715
Iteration 22/25 | Loss: 0.00128532
Iteration 23/25 | Loss: 0.00127589
Iteration 24/25 | Loss: 0.00126826
Iteration 25/25 | Loss: 0.00127067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16233742
Iteration 2/25 | Loss: 0.00160338
Iteration 3/25 | Loss: 0.00160338
Iteration 4/25 | Loss: 0.00160338
Iteration 5/25 | Loss: 0.00160337
Iteration 6/25 | Loss: 0.00160337
Iteration 7/25 | Loss: 0.00160337
Iteration 8/25 | Loss: 0.00160337
Iteration 9/25 | Loss: 0.00160337
Iteration 10/25 | Loss: 0.00160337
Iteration 11/25 | Loss: 0.00160337
Iteration 12/25 | Loss: 0.00160337
Iteration 13/25 | Loss: 0.00160337
Iteration 14/25 | Loss: 0.00160337
Iteration 15/25 | Loss: 0.00160337
Iteration 16/25 | Loss: 0.00160337
Iteration 17/25 | Loss: 0.00160337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001603373559191823, 0.001603373559191823, 0.001603373559191823, 0.001603373559191823, 0.001603373559191823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001603373559191823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160337
Iteration 2/1000 | Loss: 0.00015061
Iteration 3/1000 | Loss: 0.00009890
Iteration 4/1000 | Loss: 0.00018787
Iteration 5/1000 | Loss: 0.00076154
Iteration 6/1000 | Loss: 0.00030703
Iteration 7/1000 | Loss: 0.00021977
Iteration 8/1000 | Loss: 0.00009771
Iteration 9/1000 | Loss: 0.00011170
Iteration 10/1000 | Loss: 0.00007814
Iteration 11/1000 | Loss: 0.00038356
Iteration 12/1000 | Loss: 0.00027972
Iteration 13/1000 | Loss: 0.00018707
Iteration 14/1000 | Loss: 0.00014057
Iteration 15/1000 | Loss: 0.00015762
Iteration 16/1000 | Loss: 0.00018031
Iteration 17/1000 | Loss: 0.00008110
Iteration 18/1000 | Loss: 0.00005863
Iteration 19/1000 | Loss: 0.00010254
Iteration 20/1000 | Loss: 0.00015353
Iteration 21/1000 | Loss: 0.00007777
Iteration 22/1000 | Loss: 0.00007224
Iteration 23/1000 | Loss: 0.00006436
Iteration 24/1000 | Loss: 0.00006808
Iteration 25/1000 | Loss: 0.00005969
Iteration 26/1000 | Loss: 0.00007016
Iteration 27/1000 | Loss: 0.00039611
Iteration 28/1000 | Loss: 0.00040003
Iteration 29/1000 | Loss: 0.00008740
Iteration 30/1000 | Loss: 0.00015736
Iteration 31/1000 | Loss: 0.00049328
Iteration 32/1000 | Loss: 0.00005000
Iteration 33/1000 | Loss: 0.00040078
Iteration 34/1000 | Loss: 0.00005202
Iteration 35/1000 | Loss: 0.00004750
Iteration 36/1000 | Loss: 0.00006288
Iteration 37/1000 | Loss: 0.00004594
Iteration 38/1000 | Loss: 0.00004349
Iteration 39/1000 | Loss: 0.00004189
Iteration 40/1000 | Loss: 0.00010733
Iteration 41/1000 | Loss: 0.00005715
Iteration 42/1000 | Loss: 0.00006092
Iteration 43/1000 | Loss: 0.00004026
Iteration 44/1000 | Loss: 0.00003963
Iteration 45/1000 | Loss: 0.00005856
Iteration 46/1000 | Loss: 0.00148452
Iteration 47/1000 | Loss: 0.00124816
Iteration 48/1000 | Loss: 0.00067029
Iteration 49/1000 | Loss: 0.00109809
Iteration 50/1000 | Loss: 0.00010559
Iteration 51/1000 | Loss: 0.00005089
Iteration 52/1000 | Loss: 0.00003978
Iteration 53/1000 | Loss: 0.00009101
Iteration 54/1000 | Loss: 0.00003474
Iteration 55/1000 | Loss: 0.00003179
Iteration 56/1000 | Loss: 0.00003020
Iteration 57/1000 | Loss: 0.00002943
Iteration 58/1000 | Loss: 0.00002891
Iteration 59/1000 | Loss: 0.00002857
Iteration 60/1000 | Loss: 0.00010214
Iteration 61/1000 | Loss: 0.00002843
Iteration 62/1000 | Loss: 0.00002814
Iteration 63/1000 | Loss: 0.00002798
Iteration 64/1000 | Loss: 0.00002861
Iteration 65/1000 | Loss: 0.00002861
Iteration 66/1000 | Loss: 0.00010062
Iteration 67/1000 | Loss: 0.00003431
Iteration 68/1000 | Loss: 0.00003119
Iteration 69/1000 | Loss: 0.00002782
Iteration 70/1000 | Loss: 0.00003144
Iteration 71/1000 | Loss: 0.00002766
Iteration 72/1000 | Loss: 0.00002760
Iteration 73/1000 | Loss: 0.00002760
Iteration 74/1000 | Loss: 0.00002760
Iteration 75/1000 | Loss: 0.00002760
Iteration 76/1000 | Loss: 0.00002760
Iteration 77/1000 | Loss: 0.00002760
Iteration 78/1000 | Loss: 0.00002760
Iteration 79/1000 | Loss: 0.00002760
Iteration 80/1000 | Loss: 0.00002760
Iteration 81/1000 | Loss: 0.00002760
Iteration 82/1000 | Loss: 0.00002760
Iteration 83/1000 | Loss: 0.00002760
Iteration 84/1000 | Loss: 0.00002760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.76011269306764e-05, 2.76011269306764e-05, 2.76011269306764e-05, 2.76011269306764e-05, 2.76011269306764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.76011269306764e-05

Optimization complete. Final v2v error: 3.879262685775757 mm

Highest mean error: 11.732275009155273 mm for frame 21

Lowest mean error: 2.6617729663848877 mm for frame 93

Saving results

Total time: 141.8221151828766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837136
Iteration 2/25 | Loss: 0.00121099
Iteration 3/25 | Loss: 0.00113775
Iteration 4/25 | Loss: 0.00113140
Iteration 5/25 | Loss: 0.00113067
Iteration 6/25 | Loss: 0.00113067
Iteration 7/25 | Loss: 0.00113067
Iteration 8/25 | Loss: 0.00113067
Iteration 9/25 | Loss: 0.00113067
Iteration 10/25 | Loss: 0.00113067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011306668166071177, 0.0011306668166071177, 0.0011306668166071177, 0.0011306668166071177, 0.0011306668166071177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011306668166071177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98816633
Iteration 2/25 | Loss: 0.00082537
Iteration 3/25 | Loss: 0.00082537
Iteration 4/25 | Loss: 0.00082537
Iteration 5/25 | Loss: 0.00082537
Iteration 6/25 | Loss: 0.00082536
Iteration 7/25 | Loss: 0.00082536
Iteration 8/25 | Loss: 0.00082536
Iteration 9/25 | Loss: 0.00082536
Iteration 10/25 | Loss: 0.00082536
Iteration 11/25 | Loss: 0.00082536
Iteration 12/25 | Loss: 0.00082536
Iteration 13/25 | Loss: 0.00082536
Iteration 14/25 | Loss: 0.00082536
Iteration 15/25 | Loss: 0.00082536
Iteration 16/25 | Loss: 0.00082536
Iteration 17/25 | Loss: 0.00082536
Iteration 18/25 | Loss: 0.00082536
Iteration 19/25 | Loss: 0.00082536
Iteration 20/25 | Loss: 0.00082536
Iteration 21/25 | Loss: 0.00082536
Iteration 22/25 | Loss: 0.00082536
Iteration 23/25 | Loss: 0.00082536
Iteration 24/25 | Loss: 0.00082536
Iteration 25/25 | Loss: 0.00082536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082536
Iteration 2/1000 | Loss: 0.00002161
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001278
Iteration 5/1000 | Loss: 0.00001186
Iteration 6/1000 | Loss: 0.00001123
Iteration 7/1000 | Loss: 0.00001079
Iteration 8/1000 | Loss: 0.00001076
Iteration 9/1000 | Loss: 0.00001076
Iteration 10/1000 | Loss: 0.00001062
Iteration 11/1000 | Loss: 0.00001034
Iteration 12/1000 | Loss: 0.00001017
Iteration 13/1000 | Loss: 0.00001013
Iteration 14/1000 | Loss: 0.00001008
Iteration 15/1000 | Loss: 0.00001006
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00001000
Iteration 18/1000 | Loss: 0.00000999
Iteration 19/1000 | Loss: 0.00000997
Iteration 20/1000 | Loss: 0.00000991
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000990
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000987
Iteration 26/1000 | Loss: 0.00000986
Iteration 27/1000 | Loss: 0.00000985
Iteration 28/1000 | Loss: 0.00000984
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000978
Iteration 32/1000 | Loss: 0.00000973
Iteration 33/1000 | Loss: 0.00000972
Iteration 34/1000 | Loss: 0.00000971
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000961
Iteration 42/1000 | Loss: 0.00000961
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000956
Iteration 45/1000 | Loss: 0.00000956
Iteration 46/1000 | Loss: 0.00000955
Iteration 47/1000 | Loss: 0.00000953
Iteration 48/1000 | Loss: 0.00000953
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000953
Iteration 51/1000 | Loss: 0.00000953
Iteration 52/1000 | Loss: 0.00000953
Iteration 53/1000 | Loss: 0.00000953
Iteration 54/1000 | Loss: 0.00000951
Iteration 55/1000 | Loss: 0.00000950
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000948
Iteration 67/1000 | Loss: 0.00000946
Iteration 68/1000 | Loss: 0.00000946
Iteration 69/1000 | Loss: 0.00000946
Iteration 70/1000 | Loss: 0.00000946
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000945
Iteration 78/1000 | Loss: 0.00000945
Iteration 79/1000 | Loss: 0.00000945
Iteration 80/1000 | Loss: 0.00000944
Iteration 81/1000 | Loss: 0.00000944
Iteration 82/1000 | Loss: 0.00000944
Iteration 83/1000 | Loss: 0.00000943
Iteration 84/1000 | Loss: 0.00000943
Iteration 85/1000 | Loss: 0.00000942
Iteration 86/1000 | Loss: 0.00000942
Iteration 87/1000 | Loss: 0.00000941
Iteration 88/1000 | Loss: 0.00000941
Iteration 89/1000 | Loss: 0.00000941
Iteration 90/1000 | Loss: 0.00000941
Iteration 91/1000 | Loss: 0.00000941
Iteration 92/1000 | Loss: 0.00000941
Iteration 93/1000 | Loss: 0.00000941
Iteration 94/1000 | Loss: 0.00000941
Iteration 95/1000 | Loss: 0.00000941
Iteration 96/1000 | Loss: 0.00000940
Iteration 97/1000 | Loss: 0.00000940
Iteration 98/1000 | Loss: 0.00000940
Iteration 99/1000 | Loss: 0.00000939
Iteration 100/1000 | Loss: 0.00000937
Iteration 101/1000 | Loss: 0.00000936
Iteration 102/1000 | Loss: 0.00000936
Iteration 103/1000 | Loss: 0.00000936
Iteration 104/1000 | Loss: 0.00000936
Iteration 105/1000 | Loss: 0.00000936
Iteration 106/1000 | Loss: 0.00000936
Iteration 107/1000 | Loss: 0.00000936
Iteration 108/1000 | Loss: 0.00000936
Iteration 109/1000 | Loss: 0.00000936
Iteration 110/1000 | Loss: 0.00000936
Iteration 111/1000 | Loss: 0.00000936
Iteration 112/1000 | Loss: 0.00000936
Iteration 113/1000 | Loss: 0.00000935
Iteration 114/1000 | Loss: 0.00000935
Iteration 115/1000 | Loss: 0.00000935
Iteration 116/1000 | Loss: 0.00000934
Iteration 117/1000 | Loss: 0.00000934
Iteration 118/1000 | Loss: 0.00000933
Iteration 119/1000 | Loss: 0.00000933
Iteration 120/1000 | Loss: 0.00000933
Iteration 121/1000 | Loss: 0.00000933
Iteration 122/1000 | Loss: 0.00000933
Iteration 123/1000 | Loss: 0.00000932
Iteration 124/1000 | Loss: 0.00000932
Iteration 125/1000 | Loss: 0.00000932
Iteration 126/1000 | Loss: 0.00000932
Iteration 127/1000 | Loss: 0.00000932
Iteration 128/1000 | Loss: 0.00000932
Iteration 129/1000 | Loss: 0.00000932
Iteration 130/1000 | Loss: 0.00000931
Iteration 131/1000 | Loss: 0.00000931
Iteration 132/1000 | Loss: 0.00000930
Iteration 133/1000 | Loss: 0.00000930
Iteration 134/1000 | Loss: 0.00000930
Iteration 135/1000 | Loss: 0.00000930
Iteration 136/1000 | Loss: 0.00000930
Iteration 137/1000 | Loss: 0.00000930
Iteration 138/1000 | Loss: 0.00000930
Iteration 139/1000 | Loss: 0.00000930
Iteration 140/1000 | Loss: 0.00000930
Iteration 141/1000 | Loss: 0.00000929
Iteration 142/1000 | Loss: 0.00000929
Iteration 143/1000 | Loss: 0.00000929
Iteration 144/1000 | Loss: 0.00000929
Iteration 145/1000 | Loss: 0.00000929
Iteration 146/1000 | Loss: 0.00000929
Iteration 147/1000 | Loss: 0.00000929
Iteration 148/1000 | Loss: 0.00000929
Iteration 149/1000 | Loss: 0.00000929
Iteration 150/1000 | Loss: 0.00000928
Iteration 151/1000 | Loss: 0.00000928
Iteration 152/1000 | Loss: 0.00000928
Iteration 153/1000 | Loss: 0.00000927
Iteration 154/1000 | Loss: 0.00000927
Iteration 155/1000 | Loss: 0.00000927
Iteration 156/1000 | Loss: 0.00000926
Iteration 157/1000 | Loss: 0.00000926
Iteration 158/1000 | Loss: 0.00000926
Iteration 159/1000 | Loss: 0.00000925
Iteration 160/1000 | Loss: 0.00000925
Iteration 161/1000 | Loss: 0.00000925
Iteration 162/1000 | Loss: 0.00000924
Iteration 163/1000 | Loss: 0.00000924
Iteration 164/1000 | Loss: 0.00000924
Iteration 165/1000 | Loss: 0.00000924
Iteration 166/1000 | Loss: 0.00000924
Iteration 167/1000 | Loss: 0.00000924
Iteration 168/1000 | Loss: 0.00000924
Iteration 169/1000 | Loss: 0.00000924
Iteration 170/1000 | Loss: 0.00000923
Iteration 171/1000 | Loss: 0.00000923
Iteration 172/1000 | Loss: 0.00000923
Iteration 173/1000 | Loss: 0.00000923
Iteration 174/1000 | Loss: 0.00000923
Iteration 175/1000 | Loss: 0.00000923
Iteration 176/1000 | Loss: 0.00000923
Iteration 177/1000 | Loss: 0.00000923
Iteration 178/1000 | Loss: 0.00000923
Iteration 179/1000 | Loss: 0.00000923
Iteration 180/1000 | Loss: 0.00000922
Iteration 181/1000 | Loss: 0.00000922
Iteration 182/1000 | Loss: 0.00000922
Iteration 183/1000 | Loss: 0.00000922
Iteration 184/1000 | Loss: 0.00000922
Iteration 185/1000 | Loss: 0.00000922
Iteration 186/1000 | Loss: 0.00000922
Iteration 187/1000 | Loss: 0.00000921
Iteration 188/1000 | Loss: 0.00000921
Iteration 189/1000 | Loss: 0.00000921
Iteration 190/1000 | Loss: 0.00000921
Iteration 191/1000 | Loss: 0.00000921
Iteration 192/1000 | Loss: 0.00000921
Iteration 193/1000 | Loss: 0.00000921
Iteration 194/1000 | Loss: 0.00000921
Iteration 195/1000 | Loss: 0.00000921
Iteration 196/1000 | Loss: 0.00000920
Iteration 197/1000 | Loss: 0.00000920
Iteration 198/1000 | Loss: 0.00000920
Iteration 199/1000 | Loss: 0.00000920
Iteration 200/1000 | Loss: 0.00000920
Iteration 201/1000 | Loss: 0.00000920
Iteration 202/1000 | Loss: 0.00000920
Iteration 203/1000 | Loss: 0.00000920
Iteration 204/1000 | Loss: 0.00000920
Iteration 205/1000 | Loss: 0.00000920
Iteration 206/1000 | Loss: 0.00000920
Iteration 207/1000 | Loss: 0.00000920
Iteration 208/1000 | Loss: 0.00000920
Iteration 209/1000 | Loss: 0.00000920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.199095075018704e-06, 9.199095075018704e-06, 9.199095075018704e-06, 9.199095075018704e-06, 9.199095075018704e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.199095075018704e-06

Optimization complete. Final v2v error: 2.615276575088501 mm

Highest mean error: 2.8407649993896484 mm for frame 235

Lowest mean error: 2.420783519744873 mm for frame 120

Saving results

Total time: 45.18963074684143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996544
Iteration 2/25 | Loss: 0.00996544
Iteration 3/25 | Loss: 0.00247471
Iteration 4/25 | Loss: 0.00185735
Iteration 5/25 | Loss: 0.00175737
Iteration 6/25 | Loss: 0.00168926
Iteration 7/25 | Loss: 0.00160462
Iteration 8/25 | Loss: 0.00153263
Iteration 9/25 | Loss: 0.00149729
Iteration 10/25 | Loss: 0.00146048
Iteration 11/25 | Loss: 0.00143495
Iteration 12/25 | Loss: 0.00142208
Iteration 13/25 | Loss: 0.00141678
Iteration 14/25 | Loss: 0.00141252
Iteration 15/25 | Loss: 0.00141004
Iteration 16/25 | Loss: 0.00141330
Iteration 17/25 | Loss: 0.00141529
Iteration 18/25 | Loss: 0.00141765
Iteration 19/25 | Loss: 0.00140800
Iteration 20/25 | Loss: 0.00140441
Iteration 21/25 | Loss: 0.00140542
Iteration 22/25 | Loss: 0.00139963
Iteration 23/25 | Loss: 0.00139676
Iteration 24/25 | Loss: 0.00139589
Iteration 25/25 | Loss: 0.00139548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32062840
Iteration 2/25 | Loss: 0.00333114
Iteration 3/25 | Loss: 0.00333113
Iteration 4/25 | Loss: 0.00333113
Iteration 5/25 | Loss: 0.00333112
Iteration 6/25 | Loss: 0.00333112
Iteration 7/25 | Loss: 0.00333112
Iteration 8/25 | Loss: 0.00333112
Iteration 9/25 | Loss: 0.00333112
Iteration 10/25 | Loss: 0.00333112
Iteration 11/25 | Loss: 0.00333112
Iteration 12/25 | Loss: 0.00333112
Iteration 13/25 | Loss: 0.00333112
Iteration 14/25 | Loss: 0.00333112
Iteration 15/25 | Loss: 0.00333112
Iteration 16/25 | Loss: 0.00333112
Iteration 17/25 | Loss: 0.00333112
Iteration 18/25 | Loss: 0.00333112
Iteration 19/25 | Loss: 0.00333112
Iteration 20/25 | Loss: 0.00333112
Iteration 21/25 | Loss: 0.00333112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003331122687086463, 0.003331122687086463, 0.003331122687086463, 0.003331122687086463, 0.003331122687086463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003331122687086463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333112
Iteration 2/1000 | Loss: 0.00222444
Iteration 3/1000 | Loss: 0.00043986
Iteration 4/1000 | Loss: 0.00031683
Iteration 5/1000 | Loss: 0.00021072
Iteration 6/1000 | Loss: 0.00057205
Iteration 7/1000 | Loss: 0.00073353
Iteration 8/1000 | Loss: 0.00017493
Iteration 9/1000 | Loss: 0.00028343
Iteration 10/1000 | Loss: 0.00186715
Iteration 11/1000 | Loss: 0.00184836
Iteration 12/1000 | Loss: 0.00143558
Iteration 13/1000 | Loss: 0.00095060
Iteration 14/1000 | Loss: 0.00081886
Iteration 15/1000 | Loss: 0.00042252
Iteration 16/1000 | Loss: 0.00048505
Iteration 17/1000 | Loss: 0.00049443
Iteration 18/1000 | Loss: 0.00028016
Iteration 19/1000 | Loss: 0.00045883
Iteration 20/1000 | Loss: 0.00046841
Iteration 21/1000 | Loss: 0.00045389
Iteration 22/1000 | Loss: 0.00023350
Iteration 23/1000 | Loss: 0.00010936
Iteration 24/1000 | Loss: 0.00018822
Iteration 25/1000 | Loss: 0.00035062
Iteration 26/1000 | Loss: 0.00014073
Iteration 27/1000 | Loss: 0.00009433
Iteration 28/1000 | Loss: 0.00016799
Iteration 29/1000 | Loss: 0.00006866
Iteration 30/1000 | Loss: 0.00007225
Iteration 31/1000 | Loss: 0.00006581
Iteration 32/1000 | Loss: 0.00028395
Iteration 33/1000 | Loss: 0.00045260
Iteration 34/1000 | Loss: 0.00008535
Iteration 35/1000 | Loss: 0.00006132
Iteration 36/1000 | Loss: 0.00014173
Iteration 37/1000 | Loss: 0.00016897
Iteration 38/1000 | Loss: 0.00022894
Iteration 39/1000 | Loss: 0.00011802
Iteration 40/1000 | Loss: 0.00019571
Iteration 41/1000 | Loss: 0.00058892
Iteration 42/1000 | Loss: 0.00210547
Iteration 43/1000 | Loss: 0.00024254
Iteration 44/1000 | Loss: 0.00009260
Iteration 45/1000 | Loss: 0.00011630
Iteration 46/1000 | Loss: 0.00006598
Iteration 47/1000 | Loss: 0.00005638
Iteration 48/1000 | Loss: 0.00005068
Iteration 49/1000 | Loss: 0.00005235
Iteration 50/1000 | Loss: 0.00004567
Iteration 51/1000 | Loss: 0.00012183
Iteration 52/1000 | Loss: 0.00036875
Iteration 53/1000 | Loss: 0.00029712
Iteration 54/1000 | Loss: 0.00029672
Iteration 55/1000 | Loss: 0.00006586
Iteration 56/1000 | Loss: 0.00010740
Iteration 57/1000 | Loss: 0.00004888
Iteration 58/1000 | Loss: 0.00005165
Iteration 59/1000 | Loss: 0.00005138
Iteration 60/1000 | Loss: 0.00011071
Iteration 61/1000 | Loss: 0.00012364
Iteration 62/1000 | Loss: 0.00007712
Iteration 63/1000 | Loss: 0.00010828
Iteration 64/1000 | Loss: 0.00009109
Iteration 65/1000 | Loss: 0.00011074
Iteration 66/1000 | Loss: 0.00009300
Iteration 67/1000 | Loss: 0.00017938
Iteration 68/1000 | Loss: 0.00013734
Iteration 69/1000 | Loss: 0.00009805
Iteration 70/1000 | Loss: 0.00003577
Iteration 71/1000 | Loss: 0.00003500
Iteration 72/1000 | Loss: 0.00003413
Iteration 73/1000 | Loss: 0.00003328
Iteration 74/1000 | Loss: 0.00003251
Iteration 75/1000 | Loss: 0.00003190
Iteration 76/1000 | Loss: 0.00007196
Iteration 77/1000 | Loss: 0.00005456
Iteration 78/1000 | Loss: 0.00003103
Iteration 79/1000 | Loss: 0.00003485
Iteration 80/1000 | Loss: 0.00003696
Iteration 81/1000 | Loss: 0.00003047
Iteration 82/1000 | Loss: 0.00003034
Iteration 83/1000 | Loss: 0.00007093
Iteration 84/1000 | Loss: 0.00007092
Iteration 85/1000 | Loss: 0.00008505
Iteration 86/1000 | Loss: 0.00003036
Iteration 87/1000 | Loss: 0.00003016
Iteration 88/1000 | Loss: 0.00003014
Iteration 89/1000 | Loss: 0.00003012
Iteration 90/1000 | Loss: 0.00003008
Iteration 91/1000 | Loss: 0.00003004
Iteration 92/1000 | Loss: 0.00003003
Iteration 93/1000 | Loss: 0.00003003
Iteration 94/1000 | Loss: 0.00003002
Iteration 95/1000 | Loss: 0.00003002
Iteration 96/1000 | Loss: 0.00003001
Iteration 97/1000 | Loss: 0.00003001
Iteration 98/1000 | Loss: 0.00003000
Iteration 99/1000 | Loss: 0.00003000
Iteration 100/1000 | Loss: 0.00003000
Iteration 101/1000 | Loss: 0.00002999
Iteration 102/1000 | Loss: 0.00002999
Iteration 103/1000 | Loss: 0.00002999
Iteration 104/1000 | Loss: 0.00006169
Iteration 105/1000 | Loss: 0.00003294
Iteration 106/1000 | Loss: 0.00006812
Iteration 107/1000 | Loss: 0.00003008
Iteration 108/1000 | Loss: 0.00003888
Iteration 109/1000 | Loss: 0.00002999
Iteration 110/1000 | Loss: 0.00002996
Iteration 111/1000 | Loss: 0.00003993
Iteration 112/1000 | Loss: 0.00002993
Iteration 113/1000 | Loss: 0.00002993
Iteration 114/1000 | Loss: 0.00002993
Iteration 115/1000 | Loss: 0.00002993
Iteration 116/1000 | Loss: 0.00002993
Iteration 117/1000 | Loss: 0.00002993
Iteration 118/1000 | Loss: 0.00002993
Iteration 119/1000 | Loss: 0.00002993
Iteration 120/1000 | Loss: 0.00002992
Iteration 121/1000 | Loss: 0.00002992
Iteration 122/1000 | Loss: 0.00002992
Iteration 123/1000 | Loss: 0.00002991
Iteration 124/1000 | Loss: 0.00002991
Iteration 125/1000 | Loss: 0.00002991
Iteration 126/1000 | Loss: 0.00002991
Iteration 127/1000 | Loss: 0.00002991
Iteration 128/1000 | Loss: 0.00002991
Iteration 129/1000 | Loss: 0.00002991
Iteration 130/1000 | Loss: 0.00002990
Iteration 131/1000 | Loss: 0.00002990
Iteration 132/1000 | Loss: 0.00002990
Iteration 133/1000 | Loss: 0.00002989
Iteration 134/1000 | Loss: 0.00002989
Iteration 135/1000 | Loss: 0.00002989
Iteration 136/1000 | Loss: 0.00002989
Iteration 137/1000 | Loss: 0.00002989
Iteration 138/1000 | Loss: 0.00002988
Iteration 139/1000 | Loss: 0.00002988
Iteration 140/1000 | Loss: 0.00002988
Iteration 141/1000 | Loss: 0.00002988
Iteration 142/1000 | Loss: 0.00002987
Iteration 143/1000 | Loss: 0.00002987
Iteration 144/1000 | Loss: 0.00002987
Iteration 145/1000 | Loss: 0.00002986
Iteration 146/1000 | Loss: 0.00002986
Iteration 147/1000 | Loss: 0.00002984
Iteration 148/1000 | Loss: 0.00002984
Iteration 149/1000 | Loss: 0.00002984
Iteration 150/1000 | Loss: 0.00002983
Iteration 151/1000 | Loss: 0.00002983
Iteration 152/1000 | Loss: 0.00002983
Iteration 153/1000 | Loss: 0.00002983
Iteration 154/1000 | Loss: 0.00002983
Iteration 155/1000 | Loss: 0.00002983
Iteration 156/1000 | Loss: 0.00002983
Iteration 157/1000 | Loss: 0.00002983
Iteration 158/1000 | Loss: 0.00002983
Iteration 159/1000 | Loss: 0.00002982
Iteration 160/1000 | Loss: 0.00002982
Iteration 161/1000 | Loss: 0.00002982
Iteration 162/1000 | Loss: 0.00002982
Iteration 163/1000 | Loss: 0.00002982
Iteration 164/1000 | Loss: 0.00002982
Iteration 165/1000 | Loss: 0.00002982
Iteration 166/1000 | Loss: 0.00002982
Iteration 167/1000 | Loss: 0.00002982
Iteration 168/1000 | Loss: 0.00002982
Iteration 169/1000 | Loss: 0.00002982
Iteration 170/1000 | Loss: 0.00002982
Iteration 171/1000 | Loss: 0.00002981
Iteration 172/1000 | Loss: 0.00002981
Iteration 173/1000 | Loss: 0.00002981
Iteration 174/1000 | Loss: 0.00002981
Iteration 175/1000 | Loss: 0.00002981
Iteration 176/1000 | Loss: 0.00002980
Iteration 177/1000 | Loss: 0.00002980
Iteration 178/1000 | Loss: 0.00002980
Iteration 179/1000 | Loss: 0.00002980
Iteration 180/1000 | Loss: 0.00002980
Iteration 181/1000 | Loss: 0.00002980
Iteration 182/1000 | Loss: 0.00002980
Iteration 183/1000 | Loss: 0.00002979
Iteration 184/1000 | Loss: 0.00002979
Iteration 185/1000 | Loss: 0.00002979
Iteration 186/1000 | Loss: 0.00002979
Iteration 187/1000 | Loss: 0.00002979
Iteration 188/1000 | Loss: 0.00002979
Iteration 189/1000 | Loss: 0.00002979
Iteration 190/1000 | Loss: 0.00002979
Iteration 191/1000 | Loss: 0.00002979
Iteration 192/1000 | Loss: 0.00002979
Iteration 193/1000 | Loss: 0.00002978
Iteration 194/1000 | Loss: 0.00002978
Iteration 195/1000 | Loss: 0.00002978
Iteration 196/1000 | Loss: 0.00002978
Iteration 197/1000 | Loss: 0.00002978
Iteration 198/1000 | Loss: 0.00002978
Iteration 199/1000 | Loss: 0.00002978
Iteration 200/1000 | Loss: 0.00002978
Iteration 201/1000 | Loss: 0.00002978
Iteration 202/1000 | Loss: 0.00002978
Iteration 203/1000 | Loss: 0.00002978
Iteration 204/1000 | Loss: 0.00002978
Iteration 205/1000 | Loss: 0.00002977
Iteration 206/1000 | Loss: 0.00002977
Iteration 207/1000 | Loss: 0.00002977
Iteration 208/1000 | Loss: 0.00002977
Iteration 209/1000 | Loss: 0.00002977
Iteration 210/1000 | Loss: 0.00002977
Iteration 211/1000 | Loss: 0.00002977
Iteration 212/1000 | Loss: 0.00002977
Iteration 213/1000 | Loss: 0.00002977
Iteration 214/1000 | Loss: 0.00002977
Iteration 215/1000 | Loss: 0.00002977
Iteration 216/1000 | Loss: 0.00002977
Iteration 217/1000 | Loss: 0.00002977
Iteration 218/1000 | Loss: 0.00002977
Iteration 219/1000 | Loss: 0.00002977
Iteration 220/1000 | Loss: 0.00002977
Iteration 221/1000 | Loss: 0.00002977
Iteration 222/1000 | Loss: 0.00002977
Iteration 223/1000 | Loss: 0.00002977
Iteration 224/1000 | Loss: 0.00002977
Iteration 225/1000 | Loss: 0.00002977
Iteration 226/1000 | Loss: 0.00002977
Iteration 227/1000 | Loss: 0.00003594
Iteration 228/1000 | Loss: 0.00003594
Iteration 229/1000 | Loss: 0.00008741
Iteration 230/1000 | Loss: 0.00005351
Iteration 231/1000 | Loss: 0.00002981
Iteration 232/1000 | Loss: 0.00003010
Iteration 233/1000 | Loss: 0.00002975
Iteration 234/1000 | Loss: 0.00002975
Iteration 235/1000 | Loss: 0.00002975
Iteration 236/1000 | Loss: 0.00002975
Iteration 237/1000 | Loss: 0.00002975
Iteration 238/1000 | Loss: 0.00002975
Iteration 239/1000 | Loss: 0.00002975
Iteration 240/1000 | Loss: 0.00002975
Iteration 241/1000 | Loss: 0.00002975
Iteration 242/1000 | Loss: 0.00002975
Iteration 243/1000 | Loss: 0.00002975
Iteration 244/1000 | Loss: 0.00002975
Iteration 245/1000 | Loss: 0.00002975
Iteration 246/1000 | Loss: 0.00002975
Iteration 247/1000 | Loss: 0.00002975
Iteration 248/1000 | Loss: 0.00002975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.9747094231424853e-05, 2.9747094231424853e-05, 2.9747094231424853e-05, 2.9747094231424853e-05, 2.9747094231424853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9747094231424853e-05

Optimization complete. Final v2v error: 3.9264140129089355 mm

Highest mean error: 12.76551342010498 mm for frame 234

Lowest mean error: 3.12141489982605 mm for frame 120

Saving results

Total time: 215.38785696029663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042989
Iteration 2/25 | Loss: 0.01042989
Iteration 3/25 | Loss: 0.01042989
Iteration 4/25 | Loss: 0.01042989
Iteration 5/25 | Loss: 0.00273666
Iteration 6/25 | Loss: 0.00211592
Iteration 7/25 | Loss: 0.00184460
Iteration 8/25 | Loss: 0.00168747
Iteration 9/25 | Loss: 0.00159099
Iteration 10/25 | Loss: 0.00147367
Iteration 11/25 | Loss: 0.00145864
Iteration 12/25 | Loss: 0.00140954
Iteration 13/25 | Loss: 0.00140841
Iteration 14/25 | Loss: 0.00141361
Iteration 15/25 | Loss: 0.00141239
Iteration 16/25 | Loss: 0.00138668
Iteration 17/25 | Loss: 0.00137241
Iteration 18/25 | Loss: 0.00136708
Iteration 19/25 | Loss: 0.00133005
Iteration 20/25 | Loss: 0.00135353
Iteration 21/25 | Loss: 0.00135073
Iteration 22/25 | Loss: 0.00133482
Iteration 23/25 | Loss: 0.00133713
Iteration 24/25 | Loss: 0.00132380
Iteration 25/25 | Loss: 0.00131654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40164351
Iteration 2/25 | Loss: 0.00323889
Iteration 3/25 | Loss: 0.00310782
Iteration 4/25 | Loss: 0.00310781
Iteration 5/25 | Loss: 0.00310781
Iteration 6/25 | Loss: 0.00310781
Iteration 7/25 | Loss: 0.00310781
Iteration 8/25 | Loss: 0.00310781
Iteration 9/25 | Loss: 0.00310781
Iteration 10/25 | Loss: 0.00310781
Iteration 11/25 | Loss: 0.00310781
Iteration 12/25 | Loss: 0.00310781
Iteration 13/25 | Loss: 0.00310781
Iteration 14/25 | Loss: 0.00310781
Iteration 15/25 | Loss: 0.00310781
Iteration 16/25 | Loss: 0.00310781
Iteration 17/25 | Loss: 0.00310781
Iteration 18/25 | Loss: 0.00310781
Iteration 19/25 | Loss: 0.00310781
Iteration 20/25 | Loss: 0.00310781
Iteration 21/25 | Loss: 0.00310781
Iteration 22/25 | Loss: 0.00310781
Iteration 23/25 | Loss: 0.00310781
Iteration 24/25 | Loss: 0.00310781
Iteration 25/25 | Loss: 0.00310781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310781
Iteration 2/1000 | Loss: 0.00356518
Iteration 3/1000 | Loss: 0.00374834
Iteration 4/1000 | Loss: 0.00414028
Iteration 5/1000 | Loss: 0.00473186
Iteration 6/1000 | Loss: 0.00460970
Iteration 7/1000 | Loss: 0.00538783
Iteration 8/1000 | Loss: 0.00600077
Iteration 9/1000 | Loss: 0.00518299
Iteration 10/1000 | Loss: 0.00560783
Iteration 11/1000 | Loss: 0.00564220
Iteration 12/1000 | Loss: 0.00558551
Iteration 13/1000 | Loss: 0.00497143
Iteration 14/1000 | Loss: 0.00566735
Iteration 15/1000 | Loss: 0.00498028
Iteration 16/1000 | Loss: 0.00565809
Iteration 17/1000 | Loss: 0.00699646
Iteration 18/1000 | Loss: 0.00632385
Iteration 19/1000 | Loss: 0.00572130
Iteration 20/1000 | Loss: 0.00539309
Iteration 21/1000 | Loss: 0.00560501
Iteration 22/1000 | Loss: 0.00690427
Iteration 23/1000 | Loss: 0.00605579
Iteration 24/1000 | Loss: 0.00624092
Iteration 25/1000 | Loss: 0.00576933
Iteration 26/1000 | Loss: 0.00539495
Iteration 27/1000 | Loss: 0.00682817
Iteration 28/1000 | Loss: 0.00555500
Iteration 29/1000 | Loss: 0.00666076
Iteration 30/1000 | Loss: 0.00649383
Iteration 31/1000 | Loss: 0.00718267
Iteration 32/1000 | Loss: 0.00638489
Iteration 33/1000 | Loss: 0.00715765
Iteration 34/1000 | Loss: 0.00634106
Iteration 35/1000 | Loss: 0.00534568
Iteration 36/1000 | Loss: 0.00698989
Iteration 37/1000 | Loss: 0.00558773
Iteration 38/1000 | Loss: 0.00565155
Iteration 39/1000 | Loss: 0.00540781
Iteration 40/1000 | Loss: 0.00603233
Iteration 41/1000 | Loss: 0.00649804
Iteration 42/1000 | Loss: 0.00844314
Iteration 43/1000 | Loss: 0.00828595
Iteration 44/1000 | Loss: 0.00597786
Iteration 45/1000 | Loss: 0.00603207
Iteration 46/1000 | Loss: 0.00546046
Iteration 47/1000 | Loss: 0.00591643
Iteration 48/1000 | Loss: 0.00609537
Iteration 49/1000 | Loss: 0.00705946
Iteration 50/1000 | Loss: 0.00579547
Iteration 51/1000 | Loss: 0.00661972
Iteration 52/1000 | Loss: 0.00663079
Iteration 53/1000 | Loss: 0.00737466
Iteration 54/1000 | Loss: 0.00590049
Iteration 55/1000 | Loss: 0.00537097
Iteration 56/1000 | Loss: 0.00592360
Iteration 57/1000 | Loss: 0.00560856
Iteration 58/1000 | Loss: 0.00571638
Iteration 59/1000 | Loss: 0.00564838
Iteration 60/1000 | Loss: 0.00627506
Iteration 61/1000 | Loss: 0.00725109
Iteration 62/1000 | Loss: 0.00518222
Iteration 63/1000 | Loss: 0.00678898
Iteration 64/1000 | Loss: 0.00547114
Iteration 65/1000 | Loss: 0.00525503
Iteration 66/1000 | Loss: 0.00510331
Iteration 67/1000 | Loss: 0.00434854
Iteration 68/1000 | Loss: 0.00461070
Iteration 69/1000 | Loss: 0.00549764
Iteration 70/1000 | Loss: 0.00536963
Iteration 71/1000 | Loss: 0.00647796
Iteration 72/1000 | Loss: 0.00487147
Iteration 73/1000 | Loss: 0.00499308
Iteration 74/1000 | Loss: 0.00534499
Iteration 75/1000 | Loss: 0.00518364
Iteration 76/1000 | Loss: 0.00379317
Iteration 77/1000 | Loss: 0.00348710
Iteration 78/1000 | Loss: 0.00415382
Iteration 79/1000 | Loss: 0.00476590
Iteration 80/1000 | Loss: 0.00474951
Iteration 81/1000 | Loss: 0.00506044
Iteration 82/1000 | Loss: 0.00446458
Iteration 83/1000 | Loss: 0.00503343
Iteration 84/1000 | Loss: 0.00525781
Iteration 85/1000 | Loss: 0.00529260
Iteration 86/1000 | Loss: 0.00525862
Iteration 87/1000 | Loss: 0.00505591
Iteration 88/1000 | Loss: 0.00504908
Iteration 89/1000 | Loss: 0.00598439
Iteration 90/1000 | Loss: 0.00523402
Iteration 91/1000 | Loss: 0.00594676
Iteration 92/1000 | Loss: 0.00526405
Iteration 93/1000 | Loss: 0.00608709
Iteration 94/1000 | Loss: 0.00559566
Iteration 95/1000 | Loss: 0.00678612
Iteration 96/1000 | Loss: 0.00628172
Iteration 97/1000 | Loss: 0.00514420
Iteration 98/1000 | Loss: 0.00442190
Iteration 99/1000 | Loss: 0.00694671
Iteration 100/1000 | Loss: 0.00628194
Iteration 101/1000 | Loss: 0.00498474
Iteration 102/1000 | Loss: 0.00505695
Iteration 103/1000 | Loss: 0.00496608
Iteration 104/1000 | Loss: 0.00470979
Iteration 105/1000 | Loss: 0.00506241
Iteration 106/1000 | Loss: 0.00549722
Iteration 107/1000 | Loss: 0.00789605
Iteration 108/1000 | Loss: 0.00525619
Iteration 109/1000 | Loss: 0.00426614
Iteration 110/1000 | Loss: 0.00576899
Iteration 111/1000 | Loss: 0.00556261
Iteration 112/1000 | Loss: 0.00597742
Iteration 113/1000 | Loss: 0.00508796
Iteration 114/1000 | Loss: 0.00588393
Iteration 115/1000 | Loss: 0.00598023
Iteration 116/1000 | Loss: 0.00469515
Iteration 117/1000 | Loss: 0.00432595
Iteration 118/1000 | Loss: 0.00487649
Iteration 119/1000 | Loss: 0.00456153
Iteration 120/1000 | Loss: 0.00774099
Iteration 121/1000 | Loss: 0.00710503
Iteration 122/1000 | Loss: 0.00611511
Iteration 123/1000 | Loss: 0.00576742
Iteration 124/1000 | Loss: 0.00482730
Iteration 125/1000 | Loss: 0.00526986
Iteration 126/1000 | Loss: 0.00550398
Iteration 127/1000 | Loss: 0.00578340
Iteration 128/1000 | Loss: 0.00486613
Iteration 129/1000 | Loss: 0.00535092
Iteration 130/1000 | Loss: 0.00523202
Iteration 131/1000 | Loss: 0.00497930
Iteration 132/1000 | Loss: 0.00580673
Iteration 133/1000 | Loss: 0.00480856
Iteration 134/1000 | Loss: 0.00674520
Iteration 135/1000 | Loss: 0.00502038
Iteration 136/1000 | Loss: 0.00424957
Iteration 137/1000 | Loss: 0.00513446
Iteration 138/1000 | Loss: 0.00488068
Iteration 139/1000 | Loss: 0.00635419
Iteration 140/1000 | Loss: 0.00554196
Iteration 141/1000 | Loss: 0.00655999
Iteration 142/1000 | Loss: 0.00513150
Iteration 143/1000 | Loss: 0.00678404
Iteration 144/1000 | Loss: 0.00511601
Iteration 145/1000 | Loss: 0.00540366
Iteration 146/1000 | Loss: 0.00481972
Iteration 147/1000 | Loss: 0.00507713
Iteration 148/1000 | Loss: 0.00520554
Iteration 149/1000 | Loss: 0.00694271
Iteration 150/1000 | Loss: 0.00557590
Iteration 151/1000 | Loss: 0.00583311
Iteration 152/1000 | Loss: 0.00556297
Iteration 153/1000 | Loss: 0.00550926
Iteration 154/1000 | Loss: 0.00525855
Iteration 155/1000 | Loss: 0.00460004
Iteration 156/1000 | Loss: 0.00627528
Iteration 157/1000 | Loss: 0.00458519
Iteration 158/1000 | Loss: 0.00569230
Iteration 159/1000 | Loss: 0.00533021
Iteration 160/1000 | Loss: 0.00619575
Iteration 161/1000 | Loss: 0.00484765
Iteration 162/1000 | Loss: 0.00480580
Iteration 163/1000 | Loss: 0.00500031
Iteration 164/1000 | Loss: 0.00499592
Iteration 165/1000 | Loss: 0.00548487
Iteration 166/1000 | Loss: 0.00614629
Iteration 167/1000 | Loss: 0.00618944
Iteration 168/1000 | Loss: 0.00563023
Iteration 169/1000 | Loss: 0.00581608
Iteration 170/1000 | Loss: 0.00583451
Iteration 171/1000 | Loss: 0.00609297
Iteration 172/1000 | Loss: 0.00508308
Iteration 173/1000 | Loss: 0.00645621
Iteration 174/1000 | Loss: 0.00588099
Iteration 175/1000 | Loss: 0.00551332
Iteration 176/1000 | Loss: 0.00400198
Iteration 177/1000 | Loss: 0.00433148
Iteration 178/1000 | Loss: 0.00541168
Iteration 179/1000 | Loss: 0.00456864
Iteration 180/1000 | Loss: 0.00496644
Iteration 181/1000 | Loss: 0.00438876
Iteration 182/1000 | Loss: 0.00573340
Iteration 183/1000 | Loss: 0.00517162
Iteration 184/1000 | Loss: 0.00479069
Iteration 185/1000 | Loss: 0.00501351
Iteration 186/1000 | Loss: 0.00441583
Iteration 187/1000 | Loss: 0.00408341
Iteration 188/1000 | Loss: 0.00444222
Iteration 189/1000 | Loss: 0.00473085
Iteration 190/1000 | Loss: 0.00541020
Iteration 191/1000 | Loss: 0.00521708
Iteration 192/1000 | Loss: 0.00569542
Iteration 193/1000 | Loss: 0.00518684
Iteration 194/1000 | Loss: 0.00560405
Iteration 195/1000 | Loss: 0.00529083
Iteration 196/1000 | Loss: 0.00464953
Iteration 197/1000 | Loss: 0.00514185
Iteration 198/1000 | Loss: 0.00512655
Iteration 199/1000 | Loss: 0.00643901
Iteration 200/1000 | Loss: 0.00893057
Iteration 201/1000 | Loss: 0.00438739
Iteration 202/1000 | Loss: 0.00740322
Iteration 203/1000 | Loss: 0.00806435
Iteration 204/1000 | Loss: 0.00742357
Iteration 205/1000 | Loss: 0.00616786
Iteration 206/1000 | Loss: 0.00526314
Iteration 207/1000 | Loss: 0.00382311
Iteration 208/1000 | Loss: 0.00381689
Iteration 209/1000 | Loss: 0.00450244
Iteration 210/1000 | Loss: 0.00663643
Iteration 211/1000 | Loss: 0.00474807
Iteration 212/1000 | Loss: 0.00558669
Iteration 213/1000 | Loss: 0.00638017
Iteration 214/1000 | Loss: 0.00370107
Iteration 215/1000 | Loss: 0.00386459
Iteration 216/1000 | Loss: 0.00691756
Iteration 217/1000 | Loss: 0.00569575
Iteration 218/1000 | Loss: 0.00533670
Iteration 219/1000 | Loss: 0.00480205
Iteration 220/1000 | Loss: 0.00482326
Iteration 221/1000 | Loss: 0.00448021
Iteration 222/1000 | Loss: 0.00490622
Iteration 223/1000 | Loss: 0.00478769
Iteration 224/1000 | Loss: 0.00610042
Iteration 225/1000 | Loss: 0.00461592
Iteration 226/1000 | Loss: 0.00624042
Iteration 227/1000 | Loss: 0.00694571
Iteration 228/1000 | Loss: 0.00439399
Iteration 229/1000 | Loss: 0.00602774
Iteration 230/1000 | Loss: 0.00506315
Iteration 231/1000 | Loss: 0.00418238
Iteration 232/1000 | Loss: 0.00432052
Iteration 233/1000 | Loss: 0.00432381
Iteration 234/1000 | Loss: 0.00478467
Iteration 235/1000 | Loss: 0.00673799
Iteration 236/1000 | Loss: 0.00483529
Iteration 237/1000 | Loss: 0.00386601
Iteration 238/1000 | Loss: 0.00416708
Iteration 239/1000 | Loss: 0.00393849
Iteration 240/1000 | Loss: 0.00405542
Iteration 241/1000 | Loss: 0.00469774
Iteration 242/1000 | Loss: 0.00509323
Iteration 243/1000 | Loss: 0.00355636
Iteration 244/1000 | Loss: 0.00488098
Iteration 245/1000 | Loss: 0.00414171
Iteration 246/1000 | Loss: 0.00351305
Iteration 247/1000 | Loss: 0.00440208
Iteration 248/1000 | Loss: 0.00605211
Iteration 249/1000 | Loss: 0.00536095
Iteration 250/1000 | Loss: 0.00433976
Iteration 251/1000 | Loss: 0.00357498
Iteration 252/1000 | Loss: 0.00392551
Iteration 253/1000 | Loss: 0.00370382
Iteration 254/1000 | Loss: 0.00381506
Iteration 255/1000 | Loss: 0.00407368
Iteration 256/1000 | Loss: 0.00416185
Iteration 257/1000 | Loss: 0.00465789
Iteration 258/1000 | Loss: 0.00519507
Iteration 259/1000 | Loss: 0.00403405
Iteration 260/1000 | Loss: 0.00333691
Iteration 261/1000 | Loss: 0.00339857
Iteration 262/1000 | Loss: 0.00398898
Iteration 263/1000 | Loss: 0.00463327
Iteration 264/1000 | Loss: 0.00594481
Iteration 265/1000 | Loss: 0.00418815
Iteration 266/1000 | Loss: 0.00421035
Iteration 267/1000 | Loss: 0.00416734
Iteration 268/1000 | Loss: 0.00421568
Iteration 269/1000 | Loss: 0.00379357
Iteration 270/1000 | Loss: 0.00388767
Iteration 271/1000 | Loss: 0.00369441
Iteration 272/1000 | Loss: 0.00437083
Iteration 273/1000 | Loss: 0.00400553
Iteration 274/1000 | Loss: 0.00437255
Iteration 275/1000 | Loss: 0.00601535
Iteration 276/1000 | Loss: 0.00419297
Iteration 277/1000 | Loss: 0.00376543
Iteration 278/1000 | Loss: 0.00396102
Iteration 279/1000 | Loss: 0.00416874
Iteration 280/1000 | Loss: 0.00395730
Iteration 281/1000 | Loss: 0.00418730
Iteration 282/1000 | Loss: 0.00339560
Iteration 283/1000 | Loss: 0.00341659
Iteration 284/1000 | Loss: 0.00393004
Iteration 285/1000 | Loss: 0.00387771
Iteration 286/1000 | Loss: 0.00377824
Iteration 287/1000 | Loss: 0.00353262
Iteration 288/1000 | Loss: 0.00330122
Iteration 289/1000 | Loss: 0.00378058
Iteration 290/1000 | Loss: 0.00410012
Iteration 291/1000 | Loss: 0.00331751
Iteration 292/1000 | Loss: 0.00316806
Iteration 293/1000 | Loss: 0.00356066
Iteration 294/1000 | Loss: 0.00325681
Iteration 295/1000 | Loss: 0.00330519
Iteration 296/1000 | Loss: 0.00363627
Iteration 297/1000 | Loss: 0.00339603
Iteration 298/1000 | Loss: 0.00335822
Iteration 299/1000 | Loss: 0.00311625
Iteration 300/1000 | Loss: 0.00497406
Iteration 301/1000 | Loss: 0.00444314
Iteration 302/1000 | Loss: 0.00637251
Iteration 303/1000 | Loss: 0.00348271
Iteration 304/1000 | Loss: 0.00322652
Iteration 305/1000 | Loss: 0.00472747
Iteration 306/1000 | Loss: 0.00354630
Iteration 307/1000 | Loss: 0.00423232
Iteration 308/1000 | Loss: 0.00370334
Iteration 309/1000 | Loss: 0.00313886
Iteration 310/1000 | Loss: 0.00369342
Iteration 311/1000 | Loss: 0.00370349
Iteration 312/1000 | Loss: 0.00302850
Iteration 313/1000 | Loss: 0.00494841
Iteration 314/1000 | Loss: 0.00282527
Iteration 315/1000 | Loss: 0.00284296
Iteration 316/1000 | Loss: 0.00318272
Iteration 317/1000 | Loss: 0.00365215
Iteration 318/1000 | Loss: 0.00568759
Iteration 319/1000 | Loss: 0.00387397
Iteration 320/1000 | Loss: 0.00317102
Iteration 321/1000 | Loss: 0.00344357
Iteration 322/1000 | Loss: 0.00389061
Iteration 323/1000 | Loss: 0.00339913
Iteration 324/1000 | Loss: 0.00401495
Iteration 325/1000 | Loss: 0.00436914
Iteration 326/1000 | Loss: 0.00538999
Iteration 327/1000 | Loss: 0.00423316
Iteration 328/1000 | Loss: 0.00433264
Iteration 329/1000 | Loss: 0.00362275
Iteration 330/1000 | Loss: 0.00309527
Iteration 331/1000 | Loss: 0.00311995
Iteration 332/1000 | Loss: 0.00420012
Iteration 333/1000 | Loss: 0.00514573
Iteration 334/1000 | Loss: 0.00335934
Iteration 335/1000 | Loss: 0.00248749
Iteration 336/1000 | Loss: 0.00261699
Iteration 337/1000 | Loss: 0.00243294
Iteration 338/1000 | Loss: 0.00285200
Iteration 339/1000 | Loss: 0.00395594
Iteration 340/1000 | Loss: 0.00360189
Iteration 341/1000 | Loss: 0.00263079
Iteration 342/1000 | Loss: 0.00310703
Iteration 343/1000 | Loss: 0.00279984
Iteration 344/1000 | Loss: 0.00298924
Iteration 345/1000 | Loss: 0.00410491
Iteration 346/1000 | Loss: 0.00300032
Iteration 347/1000 | Loss: 0.00274481
Iteration 348/1000 | Loss: 0.00399571
Iteration 349/1000 | Loss: 0.00317726
Iteration 350/1000 | Loss: 0.00315412
Iteration 351/1000 | Loss: 0.00289242
Iteration 352/1000 | Loss: 0.00358009
Iteration 353/1000 | Loss: 0.00305549
Iteration 354/1000 | Loss: 0.00437174
Iteration 355/1000 | Loss: 0.00319856
Iteration 356/1000 | Loss: 0.00328804
Iteration 357/1000 | Loss: 0.00377225
Iteration 358/1000 | Loss: 0.00309579
Iteration 359/1000 | Loss: 0.00312924
Iteration 360/1000 | Loss: 0.00307428
Iteration 361/1000 | Loss: 0.00308371
Iteration 362/1000 | Loss: 0.00455836
Iteration 363/1000 | Loss: 0.00383183
Iteration 364/1000 | Loss: 0.00314480
Iteration 365/1000 | Loss: 0.00312220
Iteration 366/1000 | Loss: 0.00306085
Iteration 367/1000 | Loss: 0.00315050
Iteration 368/1000 | Loss: 0.00323914
Iteration 369/1000 | Loss: 0.00389795
Iteration 370/1000 | Loss: 0.00395722
Iteration 371/1000 | Loss: 0.00267266
Iteration 372/1000 | Loss: 0.00266109
Iteration 373/1000 | Loss: 0.00248719
Iteration 374/1000 | Loss: 0.00288924
Iteration 375/1000 | Loss: 0.00390504
Iteration 376/1000 | Loss: 0.00318375
Iteration 377/1000 | Loss: 0.00405493
Iteration 378/1000 | Loss: 0.00482744
Iteration 379/1000 | Loss: 0.00384118
Iteration 380/1000 | Loss: 0.00212063
Iteration 381/1000 | Loss: 0.00219817
Iteration 382/1000 | Loss: 0.00250834
Iteration 383/1000 | Loss: 0.00502677
Iteration 384/1000 | Loss: 0.00383213
Iteration 385/1000 | Loss: 0.00470510
Iteration 386/1000 | Loss: 0.00349152
Iteration 387/1000 | Loss: 0.00453243
Iteration 388/1000 | Loss: 0.00676050
Iteration 389/1000 | Loss: 0.00427024
Iteration 390/1000 | Loss: 0.00324689
Iteration 391/1000 | Loss: 0.00297789
Iteration 392/1000 | Loss: 0.00266490
Iteration 393/1000 | Loss: 0.00287498
Iteration 394/1000 | Loss: 0.00294255
Iteration 395/1000 | Loss: 0.00541553
Iteration 396/1000 | Loss: 0.00266292
Iteration 397/1000 | Loss: 0.00266187
Iteration 398/1000 | Loss: 0.00369235
Iteration 399/1000 | Loss: 0.00297169
Iteration 400/1000 | Loss: 0.00247829
Iteration 401/1000 | Loss: 0.00252064
Iteration 402/1000 | Loss: 0.00262575
Iteration 403/1000 | Loss: 0.00297811
Iteration 404/1000 | Loss: 0.00310027
Iteration 405/1000 | Loss: 0.00329749
Iteration 406/1000 | Loss: 0.00379961
Iteration 407/1000 | Loss: 0.00446559
Iteration 408/1000 | Loss: 0.00246016
Iteration 409/1000 | Loss: 0.00428860
Iteration 410/1000 | Loss: 0.00447937
Iteration 411/1000 | Loss: 0.00644163
Iteration 412/1000 | Loss: 0.00443793
Iteration 413/1000 | Loss: 0.00511521
Iteration 414/1000 | Loss: 0.00439345
Iteration 415/1000 | Loss: 0.00263848
Iteration 416/1000 | Loss: 0.00234754
Iteration 417/1000 | Loss: 0.00250911
Iteration 418/1000 | Loss: 0.00432390
Iteration 419/1000 | Loss: 0.00278850
Iteration 420/1000 | Loss: 0.00182641
Iteration 421/1000 | Loss: 0.00212910
Iteration 422/1000 | Loss: 0.00269149
Iteration 423/1000 | Loss: 0.00248473
Iteration 424/1000 | Loss: 0.00251800
Iteration 425/1000 | Loss: 0.00237085
Iteration 426/1000 | Loss: 0.00299245
Iteration 427/1000 | Loss: 0.00265209
Iteration 428/1000 | Loss: 0.00281484
Iteration 429/1000 | Loss: 0.00367783
Iteration 430/1000 | Loss: 0.00229845
Iteration 431/1000 | Loss: 0.00257201
Iteration 432/1000 | Loss: 0.00302351
Iteration 433/1000 | Loss: 0.00314363
Iteration 434/1000 | Loss: 0.00381077
Iteration 435/1000 | Loss: 0.00268252
Iteration 436/1000 | Loss: 0.00259145
Iteration 437/1000 | Loss: 0.00378647
Iteration 438/1000 | Loss: 0.00303531
Iteration 439/1000 | Loss: 0.00235244
Iteration 440/1000 | Loss: 0.00291803
Iteration 441/1000 | Loss: 0.00246671
Iteration 442/1000 | Loss: 0.00340762
Iteration 443/1000 | Loss: 0.00487133
Iteration 444/1000 | Loss: 0.00358291
Iteration 445/1000 | Loss: 0.00214707
Iteration 446/1000 | Loss: 0.00245989
Iteration 447/1000 | Loss: 0.00395742
Iteration 448/1000 | Loss: 0.00200973
Iteration 449/1000 | Loss: 0.00231793
Iteration 450/1000 | Loss: 0.00227302
Iteration 451/1000 | Loss: 0.00285485
Iteration 452/1000 | Loss: 0.00187666
Iteration 453/1000 | Loss: 0.00182953
Iteration 454/1000 | Loss: 0.00172491
Iteration 455/1000 | Loss: 0.00187735
Iteration 456/1000 | Loss: 0.00202510
Iteration 457/1000 | Loss: 0.00214945
Iteration 458/1000 | Loss: 0.00203376
Iteration 459/1000 | Loss: 0.00157859
Iteration 460/1000 | Loss: 0.00208367
Iteration 461/1000 | Loss: 0.00176848
Iteration 462/1000 | Loss: 0.00198035
Iteration 463/1000 | Loss: 0.00196491
Iteration 464/1000 | Loss: 0.00183793
Iteration 465/1000 | Loss: 0.00269778
Iteration 466/1000 | Loss: 0.00217279
Iteration 467/1000 | Loss: 0.00233576
Iteration 468/1000 | Loss: 0.00270011
Iteration 469/1000 | Loss: 0.00195147
Iteration 470/1000 | Loss: 0.00187836
Iteration 471/1000 | Loss: 0.00212907
Iteration 472/1000 | Loss: 0.00257634
Iteration 473/1000 | Loss: 0.00210914
Iteration 474/1000 | Loss: 0.00233965
Iteration 475/1000 | Loss: 0.00235794
Iteration 476/1000 | Loss: 0.00237555
Iteration 477/1000 | Loss: 0.00181415
Iteration 478/1000 | Loss: 0.00185565
Iteration 479/1000 | Loss: 0.00169730
Iteration 480/1000 | Loss: 0.00174452
Iteration 481/1000 | Loss: 0.00188748
Iteration 482/1000 | Loss: 0.00217931
Iteration 483/1000 | Loss: 0.00183013
Iteration 484/1000 | Loss: 0.00182069
Iteration 485/1000 | Loss: 0.00203823
Iteration 486/1000 | Loss: 0.00197489
Iteration 487/1000 | Loss: 0.00233269
Iteration 488/1000 | Loss: 0.00203363
Iteration 489/1000 | Loss: 0.00361870
Iteration 490/1000 | Loss: 0.00219052
Iteration 491/1000 | Loss: 0.00195241
Iteration 492/1000 | Loss: 0.00191297
Iteration 493/1000 | Loss: 0.00196804
Iteration 494/1000 | Loss: 0.00175741
Iteration 495/1000 | Loss: 0.00249847
Iteration 496/1000 | Loss: 0.00249566
Iteration 497/1000 | Loss: 0.00228055
Iteration 498/1000 | Loss: 0.00170496
Iteration 499/1000 | Loss: 0.00136868
Iteration 500/1000 | Loss: 0.00143486
Iteration 501/1000 | Loss: 0.00156570
Iteration 502/1000 | Loss: 0.00190072
Iteration 503/1000 | Loss: 0.00196512
Iteration 504/1000 | Loss: 0.00184585
Iteration 505/1000 | Loss: 0.00194262
Iteration 506/1000 | Loss: 0.00238911
Iteration 507/1000 | Loss: 0.00196611
Iteration 508/1000 | Loss: 0.00183929
Iteration 509/1000 | Loss: 0.00182897
Iteration 510/1000 | Loss: 0.00224333
Iteration 511/1000 | Loss: 0.00196198
Iteration 512/1000 | Loss: 0.00200198
Iteration 513/1000 | Loss: 0.00178848
Iteration 514/1000 | Loss: 0.00151603
Iteration 515/1000 | Loss: 0.00170765
Iteration 516/1000 | Loss: 0.00246471
Iteration 517/1000 | Loss: 0.00231244
Iteration 518/1000 | Loss: 0.00214188
Iteration 519/1000 | Loss: 0.00247666
Iteration 520/1000 | Loss: 0.00231569
Iteration 521/1000 | Loss: 0.00182042
Iteration 522/1000 | Loss: 0.00125995
Iteration 523/1000 | Loss: 0.00144840
Iteration 524/1000 | Loss: 0.00193608
Iteration 525/1000 | Loss: 0.00155013
Iteration 526/1000 | Loss: 0.00204323
Iteration 527/1000 | Loss: 0.00144631
Iteration 528/1000 | Loss: 0.00107869
Iteration 529/1000 | Loss: 0.00161278
Iteration 530/1000 | Loss: 0.00118359
Iteration 531/1000 | Loss: 0.00123946
Iteration 532/1000 | Loss: 0.00125349
Iteration 533/1000 | Loss: 0.00126760
Iteration 534/1000 | Loss: 0.00128700
Iteration 535/1000 | Loss: 0.00136612
Iteration 536/1000 | Loss: 0.00119146
Iteration 537/1000 | Loss: 0.00159328
Iteration 538/1000 | Loss: 0.00157880
Iteration 539/1000 | Loss: 0.00137563
Iteration 540/1000 | Loss: 0.00146474
Iteration 541/1000 | Loss: 0.00175273
Iteration 542/1000 | Loss: 0.00237058
Iteration 543/1000 | Loss: 0.00215221
Iteration 544/1000 | Loss: 0.00144786
Iteration 545/1000 | Loss: 0.00100504
Iteration 546/1000 | Loss: 0.00096088
Iteration 547/1000 | Loss: 0.00108951
Iteration 548/1000 | Loss: 0.00121528
Iteration 549/1000 | Loss: 0.00127352
Iteration 550/1000 | Loss: 0.00137481
Iteration 551/1000 | Loss: 0.00104704
Iteration 552/1000 | Loss: 0.00083684
Iteration 553/1000 | Loss: 0.00097835
Iteration 554/1000 | Loss: 0.00156368
Iteration 555/1000 | Loss: 0.00126855
Iteration 556/1000 | Loss: 0.00126656
Iteration 557/1000 | Loss: 0.00102924
Iteration 558/1000 | Loss: 0.00102936
Iteration 559/1000 | Loss: 0.00104118
Iteration 560/1000 | Loss: 0.00152212
Iteration 561/1000 | Loss: 0.00149379
Iteration 562/1000 | Loss: 0.00107249
Iteration 563/1000 | Loss: 0.00152512
Iteration 564/1000 | Loss: 0.00127872
Iteration 565/1000 | Loss: 0.00115393
Iteration 566/1000 | Loss: 0.00120384
Iteration 567/1000 | Loss: 0.00113180
Iteration 568/1000 | Loss: 0.00116137
Iteration 569/1000 | Loss: 0.00274009
Iteration 570/1000 | Loss: 0.00121075
Iteration 571/1000 | Loss: 0.00113383
Iteration 572/1000 | Loss: 0.00162167
Iteration 573/1000 | Loss: 0.00143163
Iteration 574/1000 | Loss: 0.00151121
Iteration 575/1000 | Loss: 0.00152004
Iteration 576/1000 | Loss: 0.00116927
Iteration 577/1000 | Loss: 0.00081041
Iteration 578/1000 | Loss: 0.00088090
Iteration 579/1000 | Loss: 0.00126965
Iteration 580/1000 | Loss: 0.00165394
Iteration 581/1000 | Loss: 0.00131819
Iteration 582/1000 | Loss: 0.00142710
Iteration 583/1000 | Loss: 0.00179466
Iteration 584/1000 | Loss: 0.00215430
Iteration 585/1000 | Loss: 0.00122963
Iteration 586/1000 | Loss: 0.00177363
Iteration 587/1000 | Loss: 0.00211986
Iteration 588/1000 | Loss: 0.00140856
Iteration 589/1000 | Loss: 0.00106301
Iteration 590/1000 | Loss: 0.00125670
Iteration 591/1000 | Loss: 0.00104663
Iteration 592/1000 | Loss: 0.00068586
Iteration 593/1000 | Loss: 0.00075654
Iteration 594/1000 | Loss: 0.00099929
Iteration 595/1000 | Loss: 0.00097432
Iteration 596/1000 | Loss: 0.00093356
Iteration 597/1000 | Loss: 0.00098805
Iteration 598/1000 | Loss: 0.00104327
Iteration 599/1000 | Loss: 0.00105662
Iteration 600/1000 | Loss: 0.00105106
Iteration 601/1000 | Loss: 0.00162205
Iteration 602/1000 | Loss: 0.00114063
Iteration 603/1000 | Loss: 0.00126384
Iteration 604/1000 | Loss: 0.00128423
Iteration 605/1000 | Loss: 0.00089450
Iteration 606/1000 | Loss: 0.00107363
Iteration 607/1000 | Loss: 0.00078288
Iteration 608/1000 | Loss: 0.00080676
Iteration 609/1000 | Loss: 0.00049309
Iteration 610/1000 | Loss: 0.00089021
Iteration 611/1000 | Loss: 0.00092905
Iteration 612/1000 | Loss: 0.00078574
Iteration 613/1000 | Loss: 0.00087489
Iteration 614/1000 | Loss: 0.00089654
Iteration 615/1000 | Loss: 0.00123588
Iteration 616/1000 | Loss: 0.00085813
Iteration 617/1000 | Loss: 0.00113954
Iteration 618/1000 | Loss: 0.00097499
Iteration 619/1000 | Loss: 0.00092563
Iteration 620/1000 | Loss: 0.00093101
Iteration 621/1000 | Loss: 0.00155851
Iteration 622/1000 | Loss: 0.00099411
Iteration 623/1000 | Loss: 0.00062738
Iteration 624/1000 | Loss: 0.00077329
Iteration 625/1000 | Loss: 0.00074754
Iteration 626/1000 | Loss: 0.00077056
Iteration 627/1000 | Loss: 0.00139758
Iteration 628/1000 | Loss: 0.00085828
Iteration 629/1000 | Loss: 0.00101769
Iteration 630/1000 | Loss: 0.00044793
Iteration 631/1000 | Loss: 0.00069410
Iteration 632/1000 | Loss: 0.00088937
Iteration 633/1000 | Loss: 0.00075185
Iteration 634/1000 | Loss: 0.00074324
Iteration 635/1000 | Loss: 0.00098403
Iteration 636/1000 | Loss: 0.00135438
Iteration 637/1000 | Loss: 0.00080616
Iteration 638/1000 | Loss: 0.00082314
Iteration 639/1000 | Loss: 0.00075701
Iteration 640/1000 | Loss: 0.00096588
Iteration 641/1000 | Loss: 0.00111324
Iteration 642/1000 | Loss: 0.00135758
Iteration 643/1000 | Loss: 0.00107430
Iteration 644/1000 | Loss: 0.00119893
Iteration 645/1000 | Loss: 0.00074153
Iteration 646/1000 | Loss: 0.00087550
Iteration 647/1000 | Loss: 0.00084557
Iteration 648/1000 | Loss: 0.00072153
Iteration 649/1000 | Loss: 0.00082426
Iteration 650/1000 | Loss: 0.00090954
Iteration 651/1000 | Loss: 0.00108787
Iteration 652/1000 | Loss: 0.00086699
Iteration 653/1000 | Loss: 0.00068465
Iteration 654/1000 | Loss: 0.00085477
Iteration 655/1000 | Loss: 0.00082187
Iteration 656/1000 | Loss: 0.00080915
Iteration 657/1000 | Loss: 0.00089588
Iteration 658/1000 | Loss: 0.00085838
Iteration 659/1000 | Loss: 0.00077631
Iteration 660/1000 | Loss: 0.00126984
Iteration 661/1000 | Loss: 0.00062880
Iteration 662/1000 | Loss: 0.00116762
Iteration 663/1000 | Loss: 0.00110525
Iteration 664/1000 | Loss: 0.00107554
Iteration 665/1000 | Loss: 0.00103635
Iteration 666/1000 | Loss: 0.00093037
Iteration 667/1000 | Loss: 0.00101387
Iteration 668/1000 | Loss: 0.00065312
Iteration 669/1000 | Loss: 0.00063573
Iteration 670/1000 | Loss: 0.00047511
Iteration 671/1000 | Loss: 0.00053337
Iteration 672/1000 | Loss: 0.00054041
Iteration 673/1000 | Loss: 0.00055069
Iteration 674/1000 | Loss: 0.00041720
Iteration 675/1000 | Loss: 0.00044405
Iteration 676/1000 | Loss: 0.00046146
Iteration 677/1000 | Loss: 0.00049940
Iteration 678/1000 | Loss: 0.00052974
Iteration 679/1000 | Loss: 0.00053419
Iteration 680/1000 | Loss: 0.00044099
Iteration 681/1000 | Loss: 0.00066875
Iteration 682/1000 | Loss: 0.00049126
Iteration 683/1000 | Loss: 0.00054617
Iteration 684/1000 | Loss: 0.00064328
Iteration 685/1000 | Loss: 0.00052140
Iteration 686/1000 | Loss: 0.00072575
Iteration 687/1000 | Loss: 0.00061178
Iteration 688/1000 | Loss: 0.00083784
Iteration 689/1000 | Loss: 0.00062052
Iteration 690/1000 | Loss: 0.00104467
Iteration 691/1000 | Loss: 0.00083907
Iteration 692/1000 | Loss: 0.00058100
Iteration 693/1000 | Loss: 0.00055156
Iteration 694/1000 | Loss: 0.00044465
Iteration 695/1000 | Loss: 0.00045419
Iteration 696/1000 | Loss: 0.00037800
Iteration 697/1000 | Loss: 0.00051511
Iteration 698/1000 | Loss: 0.00065338
Iteration 699/1000 | Loss: 0.00033657
Iteration 700/1000 | Loss: 0.00045253
Iteration 701/1000 | Loss: 0.00049240
Iteration 702/1000 | Loss: 0.00053570
Iteration 703/1000 | Loss: 0.00040821
Iteration 704/1000 | Loss: 0.00066029
Iteration 705/1000 | Loss: 0.00056556
Iteration 706/1000 | Loss: 0.00051805
Iteration 707/1000 | Loss: 0.00043156
Iteration 708/1000 | Loss: 0.00053812
Iteration 709/1000 | Loss: 0.00064058
Iteration 710/1000 | Loss: 0.00030754
Iteration 711/1000 | Loss: 0.00045447
Iteration 712/1000 | Loss: 0.00035914
Iteration 713/1000 | Loss: 0.00031835
Iteration 714/1000 | Loss: 0.00045335
Iteration 715/1000 | Loss: 0.00052558
Iteration 716/1000 | Loss: 0.00046144
Iteration 717/1000 | Loss: 0.00058138
Iteration 718/1000 | Loss: 0.00042957
Iteration 719/1000 | Loss: 0.00043850
Iteration 720/1000 | Loss: 0.00037220
Iteration 721/1000 | Loss: 0.00037858
Iteration 722/1000 | Loss: 0.00043243
Iteration 723/1000 | Loss: 0.00044587
Iteration 724/1000 | Loss: 0.00056506
Iteration 725/1000 | Loss: 0.00047746
Iteration 726/1000 | Loss: 0.00034948
Iteration 727/1000 | Loss: 0.00043481
Iteration 728/1000 | Loss: 0.00032805
Iteration 729/1000 | Loss: 0.00030588
Iteration 730/1000 | Loss: 0.00040303
Iteration 731/1000 | Loss: 0.00040547
Iteration 732/1000 | Loss: 0.00039862
Iteration 733/1000 | Loss: 0.00058196
Iteration 734/1000 | Loss: 0.00039197
Iteration 735/1000 | Loss: 0.00038323
Iteration 736/1000 | Loss: 0.00044505
Iteration 737/1000 | Loss: 0.00047211
Iteration 738/1000 | Loss: 0.00038054
Iteration 739/1000 | Loss: 0.00034317
Iteration 740/1000 | Loss: 0.00029631
Iteration 741/1000 | Loss: 0.00034147
Iteration 742/1000 | Loss: 0.00049286
Iteration 743/1000 | Loss: 0.00053641
Iteration 744/1000 | Loss: 0.00074960
Iteration 745/1000 | Loss: 0.00041148
Iteration 746/1000 | Loss: 0.00050278
Iteration 747/1000 | Loss: 0.00050380
Iteration 748/1000 | Loss: 0.00039638
Iteration 749/1000 | Loss: 0.00049947
Iteration 750/1000 | Loss: 0.00047048
Iteration 751/1000 | Loss: 0.00042297
Iteration 752/1000 | Loss: 0.00062525
Iteration 753/1000 | Loss: 0.00025132
Iteration 754/1000 | Loss: 0.00046042
Iteration 755/1000 | Loss: 0.00064225
Iteration 756/1000 | Loss: 0.00032674
Iteration 757/1000 | Loss: 0.00037100
Iteration 758/1000 | Loss: 0.00033401
Iteration 759/1000 | Loss: 0.00046029
Iteration 760/1000 | Loss: 0.00044031
Iteration 761/1000 | Loss: 0.00044761
Iteration 762/1000 | Loss: 0.00067877
Iteration 763/1000 | Loss: 0.00047827
Iteration 764/1000 | Loss: 0.00038451
Iteration 765/1000 | Loss: 0.00044239
Iteration 766/1000 | Loss: 0.00024358
Iteration 767/1000 | Loss: 0.00024912
Iteration 768/1000 | Loss: 0.00042726
Iteration 769/1000 | Loss: 0.00038750
Iteration 770/1000 | Loss: 0.00040756
Iteration 771/1000 | Loss: 0.00050472
Iteration 772/1000 | Loss: 0.00031553
Iteration 773/1000 | Loss: 0.00051356
Iteration 774/1000 | Loss: 0.00069003
Iteration 775/1000 | Loss: 0.00087250
Iteration 776/1000 | Loss: 0.00046025
Iteration 777/1000 | Loss: 0.00071968
Iteration 778/1000 | Loss: 0.00065691
Iteration 779/1000 | Loss: 0.00061176
Iteration 780/1000 | Loss: 0.00109707
Iteration 781/1000 | Loss: 0.00045708
Iteration 782/1000 | Loss: 0.00035369
Iteration 783/1000 | Loss: 0.00053540
Iteration 784/1000 | Loss: 0.00058718
Iteration 785/1000 | Loss: 0.00043377
Iteration 786/1000 | Loss: 0.00051484
Iteration 787/1000 | Loss: 0.00038434
Iteration 788/1000 | Loss: 0.00040032
Iteration 789/1000 | Loss: 0.00041040
Iteration 790/1000 | Loss: 0.00096567
Iteration 791/1000 | Loss: 0.00070730
Iteration 792/1000 | Loss: 0.00039956
Iteration 793/1000 | Loss: 0.00040582
Iteration 794/1000 | Loss: 0.00094143
Iteration 795/1000 | Loss: 0.00042572
Iteration 796/1000 | Loss: 0.00054398
Iteration 797/1000 | Loss: 0.00040873
Iteration 798/1000 | Loss: 0.00046299
Iteration 799/1000 | Loss: 0.00041720
Iteration 800/1000 | Loss: 0.00074334
Iteration 801/1000 | Loss: 0.00056962
Iteration 802/1000 | Loss: 0.00069789
Iteration 803/1000 | Loss: 0.00038710
Iteration 804/1000 | Loss: 0.00082976
Iteration 805/1000 | Loss: 0.00048732
Iteration 806/1000 | Loss: 0.00053477
Iteration 807/1000 | Loss: 0.00041131
Iteration 808/1000 | Loss: 0.00039154
Iteration 809/1000 | Loss: 0.00047679
Iteration 810/1000 | Loss: 0.00049992
Iteration 811/1000 | Loss: 0.00075772
Iteration 812/1000 | Loss: 0.00051854
Iteration 813/1000 | Loss: 0.00050359
Iteration 814/1000 | Loss: 0.00055961
Iteration 815/1000 | Loss: 0.00039744
Iteration 816/1000 | Loss: 0.00107193
Iteration 817/1000 | Loss: 0.00091609
Iteration 818/1000 | Loss: 0.00037485
Iteration 819/1000 | Loss: 0.00039007
Iteration 820/1000 | Loss: 0.00041444
Iteration 821/1000 | Loss: 0.00029601
Iteration 822/1000 | Loss: 0.00039707
Iteration 823/1000 | Loss: 0.00035746
Iteration 824/1000 | Loss: 0.00036869
Iteration 825/1000 | Loss: 0.00028017
Iteration 826/1000 | Loss: 0.00053312
Iteration 827/1000 | Loss: 0.00046759
Iteration 828/1000 | Loss: 0.00045825
Iteration 829/1000 | Loss: 0.00031248
Iteration 830/1000 | Loss: 0.00034253
Iteration 831/1000 | Loss: 0.00037579
Iteration 832/1000 | Loss: 0.00032096
Iteration 833/1000 | Loss: 0.00033534
Iteration 834/1000 | Loss: 0.00040697
Iteration 835/1000 | Loss: 0.00039300
Iteration 836/1000 | Loss: 0.00037083
Iteration 837/1000 | Loss: 0.00039998
Iteration 838/1000 | Loss: 0.00071455
Iteration 839/1000 | Loss: 0.00063787
Iteration 840/1000 | Loss: 0.00057784
Iteration 841/1000 | Loss: 0.00060911
Iteration 842/1000 | Loss: 0.00051542
Iteration 843/1000 | Loss: 0.00044200
Iteration 844/1000 | Loss: 0.00020039
Iteration 845/1000 | Loss: 0.00020814
Iteration 846/1000 | Loss: 0.00027250
Iteration 847/1000 | Loss: 0.00024082
Iteration 848/1000 | Loss: 0.00067689
Iteration 849/1000 | Loss: 0.00031904
Iteration 850/1000 | Loss: 0.00027308
Iteration 851/1000 | Loss: 0.00021127
Iteration 852/1000 | Loss: 0.00025102
Iteration 853/1000 | Loss: 0.00024700
Iteration 854/1000 | Loss: 0.00026027
Iteration 855/1000 | Loss: 0.00022687
Iteration 856/1000 | Loss: 0.00023792
Iteration 857/1000 | Loss: 0.00025800
Iteration 858/1000 | Loss: 0.00022251
Iteration 859/1000 | Loss: 0.00023520
Iteration 860/1000 | Loss: 0.00042981
Iteration 861/1000 | Loss: 0.00022720
Iteration 862/1000 | Loss: 0.00029140
Iteration 863/1000 | Loss: 0.00025125
Iteration 864/1000 | Loss: 0.00029290
Iteration 865/1000 | Loss: 0.00021446
Iteration 866/1000 | Loss: 0.00049953
Iteration 867/1000 | Loss: 0.00017953
Iteration 868/1000 | Loss: 0.00009953
Iteration 869/1000 | Loss: 0.00042232
Iteration 870/1000 | Loss: 0.00061897
Iteration 871/1000 | Loss: 0.00033026
Iteration 872/1000 | Loss: 0.00027818
Iteration 873/1000 | Loss: 0.00033004
Iteration 874/1000 | Loss: 0.00032518
Iteration 875/1000 | Loss: 0.00053907
Iteration 876/1000 | Loss: 0.00046111
Iteration 877/1000 | Loss: 0.00029847
Iteration 878/1000 | Loss: 0.00026741
Iteration 879/1000 | Loss: 0.00021367
Iteration 880/1000 | Loss: 0.00024107
Iteration 881/1000 | Loss: 0.00035873
Iteration 882/1000 | Loss: 0.00021977
Iteration 883/1000 | Loss: 0.00032107
Iteration 884/1000 | Loss: 0.00030884
Iteration 885/1000 | Loss: 0.00027608
Iteration 886/1000 | Loss: 0.00041917
Iteration 887/1000 | Loss: 0.00037459
Iteration 888/1000 | Loss: 0.00032980
Iteration 889/1000 | Loss: 0.00050870
Iteration 890/1000 | Loss: 0.00024352
Iteration 891/1000 | Loss: 0.00034109
Iteration 892/1000 | Loss: 0.00029163
Iteration 893/1000 | Loss: 0.00026936
Iteration 894/1000 | Loss: 0.00024451
Iteration 895/1000 | Loss: 0.00045686
Iteration 896/1000 | Loss: 0.00034808
Iteration 897/1000 | Loss: 0.00030881
Iteration 898/1000 | Loss: 0.00032602
Iteration 899/1000 | Loss: 0.00024773
Iteration 900/1000 | Loss: 0.00032149
Iteration 901/1000 | Loss: 0.00037755
Iteration 902/1000 | Loss: 0.00027241
Iteration 903/1000 | Loss: 0.00026373
Iteration 904/1000 | Loss: 0.00049475
Iteration 905/1000 | Loss: 0.00034808
Iteration 906/1000 | Loss: 0.00033812
Iteration 907/1000 | Loss: 0.00038613
Iteration 908/1000 | Loss: 0.00026527
Iteration 909/1000 | Loss: 0.00032146
Iteration 910/1000 | Loss: 0.00025578
Iteration 911/1000 | Loss: 0.00038015
Iteration 912/1000 | Loss: 0.00028862
Iteration 913/1000 | Loss: 0.00032055
Iteration 914/1000 | Loss: 0.00028209
Iteration 915/1000 | Loss: 0.00032123
Iteration 916/1000 | Loss: 0.00034193
Iteration 917/1000 | Loss: 0.00033550
Iteration 918/1000 | Loss: 0.00025489
Iteration 919/1000 | Loss: 0.00008796
Iteration 920/1000 | Loss: 0.00029330
Iteration 921/1000 | Loss: 0.00028204
Iteration 922/1000 | Loss: 0.00031512
Iteration 923/1000 | Loss: 0.00038467
Iteration 924/1000 | Loss: 0.00044721
Iteration 925/1000 | Loss: 0.00023306
Iteration 926/1000 | Loss: 0.00020599
Iteration 927/1000 | Loss: 0.00036000
Iteration 928/1000 | Loss: 0.00048846
Iteration 929/1000 | Loss: 0.00029227
Iteration 930/1000 | Loss: 0.00018693
Iteration 931/1000 | Loss: 0.00030503
Iteration 932/1000 | Loss: 0.00025699
Iteration 933/1000 | Loss: 0.00031117
Iteration 934/1000 | Loss: 0.00044425
Iteration 935/1000 | Loss: 0.00033767
Iteration 936/1000 | Loss: 0.00028600
Iteration 937/1000 | Loss: 0.00022494
Iteration 938/1000 | Loss: 0.00033089
Iteration 939/1000 | Loss: 0.00030495
Iteration 940/1000 | Loss: 0.00031124
Iteration 941/1000 | Loss: 0.00021737
Iteration 942/1000 | Loss: 0.00026780
Iteration 943/1000 | Loss: 0.00032502
Iteration 944/1000 | Loss: 0.00041415
Iteration 945/1000 | Loss: 0.00028339
Iteration 946/1000 | Loss: 0.00024974
Iteration 947/1000 | Loss: 0.00016594
Iteration 948/1000 | Loss: 0.00020202
Iteration 949/1000 | Loss: 0.00024160
Iteration 950/1000 | Loss: 0.00039434
Iteration 951/1000 | Loss: 0.00019746
Iteration 952/1000 | Loss: 0.00030541
Iteration 953/1000 | Loss: 0.00039772
Iteration 954/1000 | Loss: 0.00008190
Iteration 955/1000 | Loss: 0.00017912
Iteration 956/1000 | Loss: 0.00011292
Iteration 957/1000 | Loss: 0.00013913
Iteration 958/1000 | Loss: 0.00025934
Iteration 959/1000 | Loss: 0.00019605
Iteration 960/1000 | Loss: 0.00012588
Iteration 961/1000 | Loss: 0.00016511
Iteration 962/1000 | Loss: 0.00004029
Iteration 963/1000 | Loss: 0.00003886
Iteration 964/1000 | Loss: 0.00004870
Iteration 965/1000 | Loss: 0.00011485
Iteration 966/1000 | Loss: 0.00014276
Iteration 967/1000 | Loss: 0.00009294
Iteration 968/1000 | Loss: 0.00010623
Iteration 969/1000 | Loss: 0.00015341
Iteration 970/1000 | Loss: 0.00013210
Iteration 971/1000 | Loss: 0.00010588
Iteration 972/1000 | Loss: 0.00011773
Iteration 973/1000 | Loss: 0.00016267
Iteration 974/1000 | Loss: 0.00003852
Iteration 975/1000 | Loss: 0.00006146
Iteration 976/1000 | Loss: 0.00009006
Iteration 977/1000 | Loss: 0.00007246
Iteration 978/1000 | Loss: 0.00014171
Iteration 979/1000 | Loss: 0.00014563
Iteration 980/1000 | Loss: 0.00012386
Iteration 981/1000 | Loss: 0.00010765
Iteration 982/1000 | Loss: 0.00017064
Iteration 983/1000 | Loss: 0.00006024
Iteration 984/1000 | Loss: 0.00012278
Iteration 985/1000 | Loss: 0.00055805
Iteration 986/1000 | Loss: 0.00007262
Iteration 987/1000 | Loss: 0.00004490
Iteration 988/1000 | Loss: 0.00009902
Iteration 989/1000 | Loss: 0.00024587
Iteration 990/1000 | Loss: 0.00012460
Iteration 991/1000 | Loss: 0.00012524
Iteration 992/1000 | Loss: 0.00010156
Iteration 993/1000 | Loss: 0.00010087
Iteration 994/1000 | Loss: 0.00010847
Iteration 995/1000 | Loss: 0.00020587
Iteration 996/1000 | Loss: 0.00014491
Iteration 997/1000 | Loss: 0.00014686
Iteration 998/1000 | Loss: 0.00019208
Iteration 999/1000 | Loss: 0.00014278
Iteration 1000/1000 | Loss: 0.00015844

Optimization complete. Final v2v error: 3.0462841987609863 mm

Highest mean error: 61.191810607910156 mm for frame 152

Lowest mean error: 2.2986221313476562 mm for frame 3

Saving results

Total time: 1511.5318093299866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812002
Iteration 2/25 | Loss: 0.00172890
Iteration 3/25 | Loss: 0.00127476
Iteration 4/25 | Loss: 0.00123329
Iteration 5/25 | Loss: 0.00122765
Iteration 6/25 | Loss: 0.00122645
Iteration 7/25 | Loss: 0.00122621
Iteration 8/25 | Loss: 0.00122621
Iteration 9/25 | Loss: 0.00122621
Iteration 10/25 | Loss: 0.00122621
Iteration 11/25 | Loss: 0.00122621
Iteration 12/25 | Loss: 0.00122621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012262077070772648, 0.0012262077070772648, 0.0012262077070772648, 0.0012262077070772648, 0.0012262077070772648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012262077070772648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34888828
Iteration 2/25 | Loss: 0.00058538
Iteration 3/25 | Loss: 0.00058538
Iteration 4/25 | Loss: 0.00058538
Iteration 5/25 | Loss: 0.00058538
Iteration 6/25 | Loss: 0.00058538
Iteration 7/25 | Loss: 0.00058538
Iteration 8/25 | Loss: 0.00058537
Iteration 9/25 | Loss: 0.00058537
Iteration 10/25 | Loss: 0.00058537
Iteration 11/25 | Loss: 0.00058537
Iteration 12/25 | Loss: 0.00058537
Iteration 13/25 | Loss: 0.00058537
Iteration 14/25 | Loss: 0.00058537
Iteration 15/25 | Loss: 0.00058537
Iteration 16/25 | Loss: 0.00058537
Iteration 17/25 | Loss: 0.00058537
Iteration 18/25 | Loss: 0.00058537
Iteration 19/25 | Loss: 0.00058537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005853743059560657, 0.0005853743059560657, 0.0005853743059560657, 0.0005853743059560657, 0.0005853743059560657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005853743059560657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058537
Iteration 2/1000 | Loss: 0.00003710
Iteration 3/1000 | Loss: 0.00002531
Iteration 4/1000 | Loss: 0.00002285
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002062
Iteration 7/1000 | Loss: 0.00001988
Iteration 8/1000 | Loss: 0.00001951
Iteration 9/1000 | Loss: 0.00001911
Iteration 10/1000 | Loss: 0.00001877
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001844
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001841
Iteration 16/1000 | Loss: 0.00001834
Iteration 17/1000 | Loss: 0.00001834
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001825
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001819
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001816
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001814
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001810
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001809
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001803
Iteration 74/1000 | Loss: 0.00001803
Iteration 75/1000 | Loss: 0.00001803
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001802
Iteration 78/1000 | Loss: 0.00001802
Iteration 79/1000 | Loss: 0.00001802
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001801
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001801
Iteration 87/1000 | Loss: 0.00001801
Iteration 88/1000 | Loss: 0.00001801
Iteration 89/1000 | Loss: 0.00001801
Iteration 90/1000 | Loss: 0.00001801
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001801
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00001800
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001800
Iteration 99/1000 | Loss: 0.00001800
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00001799
Iteration 102/1000 | Loss: 0.00001799
Iteration 103/1000 | Loss: 0.00001799
Iteration 104/1000 | Loss: 0.00001799
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001798
Iteration 108/1000 | Loss: 0.00001798
Iteration 109/1000 | Loss: 0.00001798
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001797
Iteration 113/1000 | Loss: 0.00001797
Iteration 114/1000 | Loss: 0.00001797
Iteration 115/1000 | Loss: 0.00001797
Iteration 116/1000 | Loss: 0.00001797
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001796
Iteration 121/1000 | Loss: 0.00001796
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001795
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001795
Iteration 128/1000 | Loss: 0.00001794
Iteration 129/1000 | Loss: 0.00001794
Iteration 130/1000 | Loss: 0.00001794
Iteration 131/1000 | Loss: 0.00001794
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001793
Iteration 136/1000 | Loss: 0.00001793
Iteration 137/1000 | Loss: 0.00001793
Iteration 138/1000 | Loss: 0.00001793
Iteration 139/1000 | Loss: 0.00001793
Iteration 140/1000 | Loss: 0.00001793
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001792
Iteration 143/1000 | Loss: 0.00001792
Iteration 144/1000 | Loss: 0.00001792
Iteration 145/1000 | Loss: 0.00001792
Iteration 146/1000 | Loss: 0.00001792
Iteration 147/1000 | Loss: 0.00001792
Iteration 148/1000 | Loss: 0.00001791
Iteration 149/1000 | Loss: 0.00001791
Iteration 150/1000 | Loss: 0.00001791
Iteration 151/1000 | Loss: 0.00001791
Iteration 152/1000 | Loss: 0.00001791
Iteration 153/1000 | Loss: 0.00001791
Iteration 154/1000 | Loss: 0.00001790
Iteration 155/1000 | Loss: 0.00001790
Iteration 156/1000 | Loss: 0.00001790
Iteration 157/1000 | Loss: 0.00001790
Iteration 158/1000 | Loss: 0.00001790
Iteration 159/1000 | Loss: 0.00001790
Iteration 160/1000 | Loss: 0.00001790
Iteration 161/1000 | Loss: 0.00001790
Iteration 162/1000 | Loss: 0.00001789
Iteration 163/1000 | Loss: 0.00001789
Iteration 164/1000 | Loss: 0.00001789
Iteration 165/1000 | Loss: 0.00001789
Iteration 166/1000 | Loss: 0.00001789
Iteration 167/1000 | Loss: 0.00001789
Iteration 168/1000 | Loss: 0.00001788
Iteration 169/1000 | Loss: 0.00001788
Iteration 170/1000 | Loss: 0.00001788
Iteration 171/1000 | Loss: 0.00001788
Iteration 172/1000 | Loss: 0.00001788
Iteration 173/1000 | Loss: 0.00001788
Iteration 174/1000 | Loss: 0.00001788
Iteration 175/1000 | Loss: 0.00001788
Iteration 176/1000 | Loss: 0.00001788
Iteration 177/1000 | Loss: 0.00001788
Iteration 178/1000 | Loss: 0.00001788
Iteration 179/1000 | Loss: 0.00001788
Iteration 180/1000 | Loss: 0.00001788
Iteration 181/1000 | Loss: 0.00001788
Iteration 182/1000 | Loss: 0.00001788
Iteration 183/1000 | Loss: 0.00001788
Iteration 184/1000 | Loss: 0.00001788
Iteration 185/1000 | Loss: 0.00001788
Iteration 186/1000 | Loss: 0.00001788
Iteration 187/1000 | Loss: 0.00001788
Iteration 188/1000 | Loss: 0.00001788
Iteration 189/1000 | Loss: 0.00001788
Iteration 190/1000 | Loss: 0.00001788
Iteration 191/1000 | Loss: 0.00001788
Iteration 192/1000 | Loss: 0.00001788
Iteration 193/1000 | Loss: 0.00001788
Iteration 194/1000 | Loss: 0.00001788
Iteration 195/1000 | Loss: 0.00001788
Iteration 196/1000 | Loss: 0.00001788
Iteration 197/1000 | Loss: 0.00001788
Iteration 198/1000 | Loss: 0.00001788
Iteration 199/1000 | Loss: 0.00001788
Iteration 200/1000 | Loss: 0.00001788
Iteration 201/1000 | Loss: 0.00001788
Iteration 202/1000 | Loss: 0.00001788
Iteration 203/1000 | Loss: 0.00001788
Iteration 204/1000 | Loss: 0.00001788
Iteration 205/1000 | Loss: 0.00001788
Iteration 206/1000 | Loss: 0.00001788
Iteration 207/1000 | Loss: 0.00001788
Iteration 208/1000 | Loss: 0.00001788
Iteration 209/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.7881808162201196e-05, 1.7881808162201196e-05, 1.7881808162201196e-05, 1.7881808162201196e-05, 1.7881808162201196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7881808162201196e-05

Optimization complete. Final v2v error: 3.592017650604248 mm

Highest mean error: 3.983663320541382 mm for frame 197

Lowest mean error: 3.160987377166748 mm for frame 63

Saving results

Total time: 46.99528455734253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980228
Iteration 2/25 | Loss: 0.00220608
Iteration 3/25 | Loss: 0.00168404
Iteration 4/25 | Loss: 0.00157301
Iteration 5/25 | Loss: 0.00153872
Iteration 6/25 | Loss: 0.00155759
Iteration 7/25 | Loss: 0.00160423
Iteration 8/25 | Loss: 0.00152761
Iteration 9/25 | Loss: 0.00139567
Iteration 10/25 | Loss: 0.00135505
Iteration 11/25 | Loss: 0.00129739
Iteration 12/25 | Loss: 0.00129194
Iteration 13/25 | Loss: 0.00129231
Iteration 14/25 | Loss: 0.00129011
Iteration 15/25 | Loss: 0.00128168
Iteration 16/25 | Loss: 0.00127079
Iteration 17/25 | Loss: 0.00126168
Iteration 18/25 | Loss: 0.00125408
Iteration 19/25 | Loss: 0.00126297
Iteration 20/25 | Loss: 0.00125917
Iteration 21/25 | Loss: 0.00123235
Iteration 22/25 | Loss: 0.00123918
Iteration 23/25 | Loss: 0.00123396
Iteration 24/25 | Loss: 0.00123180
Iteration 25/25 | Loss: 0.00122826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38649583
Iteration 2/25 | Loss: 0.00124206
Iteration 3/25 | Loss: 0.00103087
Iteration 4/25 | Loss: 0.00103087
Iteration 5/25 | Loss: 0.00103087
Iteration 6/25 | Loss: 0.00103087
Iteration 7/25 | Loss: 0.00103087
Iteration 8/25 | Loss: 0.00103087
Iteration 9/25 | Loss: 0.00103086
Iteration 10/25 | Loss: 0.00103086
Iteration 11/25 | Loss: 0.00103086
Iteration 12/25 | Loss: 0.00103086
Iteration 13/25 | Loss: 0.00103086
Iteration 14/25 | Loss: 0.00103086
Iteration 15/25 | Loss: 0.00103086
Iteration 16/25 | Loss: 0.00103086
Iteration 17/25 | Loss: 0.00103086
Iteration 18/25 | Loss: 0.00103086
Iteration 19/25 | Loss: 0.00103086
Iteration 20/25 | Loss: 0.00103086
Iteration 21/25 | Loss: 0.00103086
Iteration 22/25 | Loss: 0.00103086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010308637283742428, 0.0010308637283742428, 0.0010308637283742428, 0.0010308637283742428, 0.0010308637283742428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010308637283742428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103086
Iteration 2/1000 | Loss: 0.00017208
Iteration 3/1000 | Loss: 0.00012631
Iteration 4/1000 | Loss: 0.00006734
Iteration 5/1000 | Loss: 0.00009223
Iteration 6/1000 | Loss: 0.00004525
Iteration 7/1000 | Loss: 0.00020685
Iteration 8/1000 | Loss: 0.00032110
Iteration 9/1000 | Loss: 0.00017916
Iteration 10/1000 | Loss: 0.00022634
Iteration 11/1000 | Loss: 0.00005308
Iteration 12/1000 | Loss: 0.00003857
Iteration 13/1000 | Loss: 0.00025274
Iteration 14/1000 | Loss: 0.00023192
Iteration 15/1000 | Loss: 0.00023499
Iteration 16/1000 | Loss: 0.00003820
Iteration 17/1000 | Loss: 0.00009357
Iteration 18/1000 | Loss: 0.00003123
Iteration 19/1000 | Loss: 0.00002813
Iteration 20/1000 | Loss: 0.00002640
Iteration 21/1000 | Loss: 0.00024594
Iteration 22/1000 | Loss: 0.00004084
Iteration 23/1000 | Loss: 0.00003002
Iteration 24/1000 | Loss: 0.00002749
Iteration 25/1000 | Loss: 0.00009655
Iteration 26/1000 | Loss: 0.00020346
Iteration 27/1000 | Loss: 0.00002997
Iteration 28/1000 | Loss: 0.00002736
Iteration 29/1000 | Loss: 0.00035807
Iteration 30/1000 | Loss: 0.00038681
Iteration 31/1000 | Loss: 0.00048204
Iteration 32/1000 | Loss: 0.00013874
Iteration 33/1000 | Loss: 0.00037755
Iteration 34/1000 | Loss: 0.00003145
Iteration 35/1000 | Loss: 0.00002641
Iteration 36/1000 | Loss: 0.00006425
Iteration 37/1000 | Loss: 0.00032900
Iteration 38/1000 | Loss: 0.00002891
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00009763
Iteration 41/1000 | Loss: 0.00021926
Iteration 42/1000 | Loss: 0.00011118
Iteration 43/1000 | Loss: 0.00001906
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00013070
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00012589
Iteration 51/1000 | Loss: 0.00006651
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00009336
Iteration 54/1000 | Loss: 0.00009222
Iteration 55/1000 | Loss: 0.00008322
Iteration 56/1000 | Loss: 0.00010940
Iteration 57/1000 | Loss: 0.00007058
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00006101
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001630
Iteration 82/1000 | Loss: 0.00001628
Iteration 83/1000 | Loss: 0.00001627
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00004554
Iteration 88/1000 | Loss: 0.00002490
Iteration 89/1000 | Loss: 0.00001639
Iteration 90/1000 | Loss: 0.00007054
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00009604
Iteration 93/1000 | Loss: 0.00001847
Iteration 94/1000 | Loss: 0.00001661
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00001625
Iteration 98/1000 | Loss: 0.00001625
Iteration 99/1000 | Loss: 0.00001624
Iteration 100/1000 | Loss: 0.00001624
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001623
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00005617
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001625
Iteration 121/1000 | Loss: 0.00003817
Iteration 122/1000 | Loss: 0.00001628
Iteration 123/1000 | Loss: 0.00001625
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001622
Iteration 126/1000 | Loss: 0.00001622
Iteration 127/1000 | Loss: 0.00001622
Iteration 128/1000 | Loss: 0.00001622
Iteration 129/1000 | Loss: 0.00001622
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001622
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001619
Iteration 147/1000 | Loss: 0.00001619
Iteration 148/1000 | Loss: 0.00001619
Iteration 149/1000 | Loss: 0.00001619
Iteration 150/1000 | Loss: 0.00001618
Iteration 151/1000 | Loss: 0.00001618
Iteration 152/1000 | Loss: 0.00001618
Iteration 153/1000 | Loss: 0.00001618
Iteration 154/1000 | Loss: 0.00001618
Iteration 155/1000 | Loss: 0.00001618
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001617
Iteration 158/1000 | Loss: 0.00001617
Iteration 159/1000 | Loss: 0.00001617
Iteration 160/1000 | Loss: 0.00001617
Iteration 161/1000 | Loss: 0.00001616
Iteration 162/1000 | Loss: 0.00001616
Iteration 163/1000 | Loss: 0.00001616
Iteration 164/1000 | Loss: 0.00001616
Iteration 165/1000 | Loss: 0.00001616
Iteration 166/1000 | Loss: 0.00001616
Iteration 167/1000 | Loss: 0.00001616
Iteration 168/1000 | Loss: 0.00001616
Iteration 169/1000 | Loss: 0.00001616
Iteration 170/1000 | Loss: 0.00001616
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001615
Iteration 173/1000 | Loss: 0.00001615
Iteration 174/1000 | Loss: 0.00001615
Iteration 175/1000 | Loss: 0.00001615
Iteration 176/1000 | Loss: 0.00001615
Iteration 177/1000 | Loss: 0.00001615
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001613
Iteration 182/1000 | Loss: 0.00001613
Iteration 183/1000 | Loss: 0.00001613
Iteration 184/1000 | Loss: 0.00001613
Iteration 185/1000 | Loss: 0.00001613
Iteration 186/1000 | Loss: 0.00001613
Iteration 187/1000 | Loss: 0.00001612
Iteration 188/1000 | Loss: 0.00001612
Iteration 189/1000 | Loss: 0.00001612
Iteration 190/1000 | Loss: 0.00001612
Iteration 191/1000 | Loss: 0.00001612
Iteration 192/1000 | Loss: 0.00001612
Iteration 193/1000 | Loss: 0.00001612
Iteration 194/1000 | Loss: 0.00001612
Iteration 195/1000 | Loss: 0.00001612
Iteration 196/1000 | Loss: 0.00001612
Iteration 197/1000 | Loss: 0.00001612
Iteration 198/1000 | Loss: 0.00001612
Iteration 199/1000 | Loss: 0.00001612
Iteration 200/1000 | Loss: 0.00001612
Iteration 201/1000 | Loss: 0.00001612
Iteration 202/1000 | Loss: 0.00001612
Iteration 203/1000 | Loss: 0.00001612
Iteration 204/1000 | Loss: 0.00001612
Iteration 205/1000 | Loss: 0.00001612
Iteration 206/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.6118086932692677e-05, 1.6118086932692677e-05, 1.6118086932692677e-05, 1.6118086932692677e-05, 1.6118086932692677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6118086932692677e-05

Optimization complete. Final v2v error: 3.4017817974090576 mm

Highest mean error: 5.590278625488281 mm for frame 227

Lowest mean error: 2.958218574523926 mm for frame 5

Saving results

Total time: 179.7563214302063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388404
Iteration 2/25 | Loss: 0.00130122
Iteration 3/25 | Loss: 0.00120946
Iteration 4/25 | Loss: 0.00119644
Iteration 5/25 | Loss: 0.00119141
Iteration 6/25 | Loss: 0.00119097
Iteration 7/25 | Loss: 0.00119097
Iteration 8/25 | Loss: 0.00119097
Iteration 9/25 | Loss: 0.00119097
Iteration 10/25 | Loss: 0.00119097
Iteration 11/25 | Loss: 0.00119097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001190966460853815, 0.001190966460853815, 0.001190966460853815, 0.001190966460853815, 0.001190966460853815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190966460853815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81517237
Iteration 2/25 | Loss: 0.00104415
Iteration 3/25 | Loss: 0.00104415
Iteration 4/25 | Loss: 0.00104415
Iteration 5/25 | Loss: 0.00104415
Iteration 6/25 | Loss: 0.00104415
Iteration 7/25 | Loss: 0.00104415
Iteration 8/25 | Loss: 0.00104415
Iteration 9/25 | Loss: 0.00104415
Iteration 10/25 | Loss: 0.00104415
Iteration 11/25 | Loss: 0.00104415
Iteration 12/25 | Loss: 0.00104415
Iteration 13/25 | Loss: 0.00104415
Iteration 14/25 | Loss: 0.00104415
Iteration 15/25 | Loss: 0.00104415
Iteration 16/25 | Loss: 0.00104415
Iteration 17/25 | Loss: 0.00104415
Iteration 18/25 | Loss: 0.00104415
Iteration 19/25 | Loss: 0.00104415
Iteration 20/25 | Loss: 0.00104415
Iteration 21/25 | Loss: 0.00104415
Iteration 22/25 | Loss: 0.00104415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001044149394147098, 0.001044149394147098, 0.001044149394147098, 0.001044149394147098, 0.001044149394147098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001044149394147098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104415
Iteration 2/1000 | Loss: 0.00003753
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001425
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001305
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001286
Iteration 20/1000 | Loss: 0.00001286
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001275
Iteration 37/1000 | Loss: 0.00001275
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001270
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001260
Iteration 75/1000 | Loss: 0.00001260
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001254
Iteration 117/1000 | Loss: 0.00001254
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001252
Iteration 124/1000 | Loss: 0.00001252
Iteration 125/1000 | Loss: 0.00001252
Iteration 126/1000 | Loss: 0.00001252
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001251
Iteration 131/1000 | Loss: 0.00001251
Iteration 132/1000 | Loss: 0.00001251
Iteration 133/1000 | Loss: 0.00001251
Iteration 134/1000 | Loss: 0.00001251
Iteration 135/1000 | Loss: 0.00001251
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001250
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001247
Iteration 158/1000 | Loss: 0.00001247
Iteration 159/1000 | Loss: 0.00001247
Iteration 160/1000 | Loss: 0.00001247
Iteration 161/1000 | Loss: 0.00001247
Iteration 162/1000 | Loss: 0.00001246
Iteration 163/1000 | Loss: 0.00001246
Iteration 164/1000 | Loss: 0.00001246
Iteration 165/1000 | Loss: 0.00001245
Iteration 166/1000 | Loss: 0.00001245
Iteration 167/1000 | Loss: 0.00001245
Iteration 168/1000 | Loss: 0.00001245
Iteration 169/1000 | Loss: 0.00001245
Iteration 170/1000 | Loss: 0.00001245
Iteration 171/1000 | Loss: 0.00001245
Iteration 172/1000 | Loss: 0.00001244
Iteration 173/1000 | Loss: 0.00001244
Iteration 174/1000 | Loss: 0.00001244
Iteration 175/1000 | Loss: 0.00001244
Iteration 176/1000 | Loss: 0.00001244
Iteration 177/1000 | Loss: 0.00001244
Iteration 178/1000 | Loss: 0.00001243
Iteration 179/1000 | Loss: 0.00001243
Iteration 180/1000 | Loss: 0.00001243
Iteration 181/1000 | Loss: 0.00001243
Iteration 182/1000 | Loss: 0.00001243
Iteration 183/1000 | Loss: 0.00001243
Iteration 184/1000 | Loss: 0.00001243
Iteration 185/1000 | Loss: 0.00001243
Iteration 186/1000 | Loss: 0.00001243
Iteration 187/1000 | Loss: 0.00001243
Iteration 188/1000 | Loss: 0.00001243
Iteration 189/1000 | Loss: 0.00001243
Iteration 190/1000 | Loss: 0.00001243
Iteration 191/1000 | Loss: 0.00001243
Iteration 192/1000 | Loss: 0.00001243
Iteration 193/1000 | Loss: 0.00001243
Iteration 194/1000 | Loss: 0.00001243
Iteration 195/1000 | Loss: 0.00001243
Iteration 196/1000 | Loss: 0.00001243
Iteration 197/1000 | Loss: 0.00001243
Iteration 198/1000 | Loss: 0.00001243
Iteration 199/1000 | Loss: 0.00001243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.2432463336153887e-05, 1.2432463336153887e-05, 1.2432463336153887e-05, 1.2432463336153887e-05, 1.2432463336153887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2432463336153887e-05

Optimization complete. Final v2v error: 2.9724981784820557 mm

Highest mean error: 3.230483293533325 mm for frame 200

Lowest mean error: 2.854405641555786 mm for frame 182

Saving results

Total time: 46.477622985839844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398133
Iteration 2/25 | Loss: 0.00123439
Iteration 3/25 | Loss: 0.00114479
Iteration 4/25 | Loss: 0.00113703
Iteration 5/25 | Loss: 0.00113423
Iteration 6/25 | Loss: 0.00113423
Iteration 7/25 | Loss: 0.00113423
Iteration 8/25 | Loss: 0.00113423
Iteration 9/25 | Loss: 0.00113423
Iteration 10/25 | Loss: 0.00113411
Iteration 11/25 | Loss: 0.00113411
Iteration 12/25 | Loss: 0.00113411
Iteration 13/25 | Loss: 0.00113411
Iteration 14/25 | Loss: 0.00113411
Iteration 15/25 | Loss: 0.00113411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011341089848428965, 0.0011341089848428965, 0.0011341089848428965, 0.0011341089848428965, 0.0011341089848428965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011341089848428965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59843624
Iteration 2/25 | Loss: 0.00074943
Iteration 3/25 | Loss: 0.00074943
Iteration 4/25 | Loss: 0.00074943
Iteration 5/25 | Loss: 0.00074943
Iteration 6/25 | Loss: 0.00074943
Iteration 7/25 | Loss: 0.00074943
Iteration 8/25 | Loss: 0.00074943
Iteration 9/25 | Loss: 0.00074943
Iteration 10/25 | Loss: 0.00074943
Iteration 11/25 | Loss: 0.00074943
Iteration 12/25 | Loss: 0.00074943
Iteration 13/25 | Loss: 0.00074943
Iteration 14/25 | Loss: 0.00074943
Iteration 15/25 | Loss: 0.00074943
Iteration 16/25 | Loss: 0.00074943
Iteration 17/25 | Loss: 0.00074943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007494286401197314, 0.0007494286401197314, 0.0007494286401197314, 0.0007494286401197314, 0.0007494286401197314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007494286401197314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074943
Iteration 2/1000 | Loss: 0.00002599
Iteration 3/1000 | Loss: 0.00001593
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001179
Iteration 7/1000 | Loss: 0.00001137
Iteration 8/1000 | Loss: 0.00001112
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001030
Iteration 13/1000 | Loss: 0.00001023
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001002
Iteration 20/1000 | Loss: 0.00001000
Iteration 21/1000 | Loss: 0.00001000
Iteration 22/1000 | Loss: 0.00001000
Iteration 23/1000 | Loss: 0.00001000
Iteration 24/1000 | Loss: 0.00000999
Iteration 25/1000 | Loss: 0.00000999
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000998
Iteration 35/1000 | Loss: 0.00000997
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000995
Iteration 39/1000 | Loss: 0.00000995
Iteration 40/1000 | Loss: 0.00000995
Iteration 41/1000 | Loss: 0.00000995
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000994
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000992
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000991
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000990
Iteration 64/1000 | Loss: 0.00000990
Iteration 65/1000 | Loss: 0.00000988
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000988
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000987
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000982
Iteration 98/1000 | Loss: 0.00000982
Iteration 99/1000 | Loss: 0.00000982
Iteration 100/1000 | Loss: 0.00000981
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000981
Iteration 104/1000 | Loss: 0.00000981
Iteration 105/1000 | Loss: 0.00000981
Iteration 106/1000 | Loss: 0.00000980
Iteration 107/1000 | Loss: 0.00000980
Iteration 108/1000 | Loss: 0.00000980
Iteration 109/1000 | Loss: 0.00000980
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000979
Iteration 112/1000 | Loss: 0.00000979
Iteration 113/1000 | Loss: 0.00000979
Iteration 114/1000 | Loss: 0.00000979
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000979
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000978
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000977
Iteration 125/1000 | Loss: 0.00000977
Iteration 126/1000 | Loss: 0.00000977
Iteration 127/1000 | Loss: 0.00000977
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000974
Iteration 147/1000 | Loss: 0.00000974
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000974
Iteration 150/1000 | Loss: 0.00000974
Iteration 151/1000 | Loss: 0.00000974
Iteration 152/1000 | Loss: 0.00000974
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000974
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Iteration 165/1000 | Loss: 0.00000973
Iteration 166/1000 | Loss: 0.00000973
Iteration 167/1000 | Loss: 0.00000973
Iteration 168/1000 | Loss: 0.00000973
Iteration 169/1000 | Loss: 0.00000973
Iteration 170/1000 | Loss: 0.00000973
Iteration 171/1000 | Loss: 0.00000973
Iteration 172/1000 | Loss: 0.00000973
Iteration 173/1000 | Loss: 0.00000973
Iteration 174/1000 | Loss: 0.00000973
Iteration 175/1000 | Loss: 0.00000973
Iteration 176/1000 | Loss: 0.00000973
Iteration 177/1000 | Loss: 0.00000973
Iteration 178/1000 | Loss: 0.00000973
Iteration 179/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [9.728176337375771e-06, 9.728176337375771e-06, 9.728176337375771e-06, 9.728176337375771e-06, 9.728176337375771e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.728176337375771e-06

Optimization complete. Final v2v error: 2.688265085220337 mm

Highest mean error: 2.911691427230835 mm for frame 105

Lowest mean error: 2.5414774417877197 mm for frame 138

Saving results

Total time: 41.978585958480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413887
Iteration 2/25 | Loss: 0.00123316
Iteration 3/25 | Loss: 0.00114843
Iteration 4/25 | Loss: 0.00113296
Iteration 5/25 | Loss: 0.00112798
Iteration 6/25 | Loss: 0.00112746
Iteration 7/25 | Loss: 0.00112746
Iteration 8/25 | Loss: 0.00112746
Iteration 9/25 | Loss: 0.00112746
Iteration 10/25 | Loss: 0.00112746
Iteration 11/25 | Loss: 0.00112746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011274635326117277, 0.0011274635326117277, 0.0011274635326117277, 0.0011274635326117277, 0.0011274635326117277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011274635326117277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49942112
Iteration 2/25 | Loss: 0.00077530
Iteration 3/25 | Loss: 0.00077530
Iteration 4/25 | Loss: 0.00077530
Iteration 5/25 | Loss: 0.00077530
Iteration 6/25 | Loss: 0.00077530
Iteration 7/25 | Loss: 0.00077530
Iteration 8/25 | Loss: 0.00077530
Iteration 9/25 | Loss: 0.00077530
Iteration 10/25 | Loss: 0.00077530
Iteration 11/25 | Loss: 0.00077530
Iteration 12/25 | Loss: 0.00077530
Iteration 13/25 | Loss: 0.00077530
Iteration 14/25 | Loss: 0.00077530
Iteration 15/25 | Loss: 0.00077530
Iteration 16/25 | Loss: 0.00077530
Iteration 17/25 | Loss: 0.00077530
Iteration 18/25 | Loss: 0.00077530
Iteration 19/25 | Loss: 0.00077530
Iteration 20/25 | Loss: 0.00077530
Iteration 21/25 | Loss: 0.00077530
Iteration 22/25 | Loss: 0.00077530
Iteration 23/25 | Loss: 0.00077530
Iteration 24/25 | Loss: 0.00077530
Iteration 25/25 | Loss: 0.00077530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077530
Iteration 2/1000 | Loss: 0.00002248
Iteration 3/1000 | Loss: 0.00001555
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001394
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001260
Iteration 10/1000 | Loss: 0.00001241
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001192
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001185
Iteration 35/1000 | Loss: 0.00001185
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001167
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001167
Iteration 115/1000 | Loss: 0.00001167
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001166
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001164
Iteration 122/1000 | Loss: 0.00001164
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001161
Iteration 130/1000 | Loss: 0.00001161
Iteration 131/1000 | Loss: 0.00001161
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001160
Iteration 135/1000 | Loss: 0.00001160
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001157
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00001157
Iteration 168/1000 | Loss: 0.00001157
Iteration 169/1000 | Loss: 0.00001157
Iteration 170/1000 | Loss: 0.00001157
Iteration 171/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.157177030108869e-05, 1.157177030108869e-05, 1.157177030108869e-05, 1.157177030108869e-05, 1.157177030108869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.157177030108869e-05

Optimization complete. Final v2v error: 2.9288573265075684 mm

Highest mean error: 3.314239501953125 mm for frame 89

Lowest mean error: 2.7185609340667725 mm for frame 74

Saving results

Total time: 38.14617943763733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846136
Iteration 2/25 | Loss: 0.00158538
Iteration 3/25 | Loss: 0.00129012
Iteration 4/25 | Loss: 0.00125598
Iteration 5/25 | Loss: 0.00124998
Iteration 6/25 | Loss: 0.00124967
Iteration 7/25 | Loss: 0.00124967
Iteration 8/25 | Loss: 0.00124967
Iteration 9/25 | Loss: 0.00124967
Iteration 10/25 | Loss: 0.00124967
Iteration 11/25 | Loss: 0.00124967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012496651615947485, 0.0012496651615947485, 0.0012496651615947485, 0.0012496651615947485, 0.0012496651615947485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012496651615947485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15210724
Iteration 2/25 | Loss: 0.00067219
Iteration 3/25 | Loss: 0.00067218
Iteration 4/25 | Loss: 0.00067218
Iteration 5/25 | Loss: 0.00067218
Iteration 6/25 | Loss: 0.00067218
Iteration 7/25 | Loss: 0.00067218
Iteration 8/25 | Loss: 0.00067218
Iteration 9/25 | Loss: 0.00067218
Iteration 10/25 | Loss: 0.00067218
Iteration 11/25 | Loss: 0.00067218
Iteration 12/25 | Loss: 0.00067218
Iteration 13/25 | Loss: 0.00067218
Iteration 14/25 | Loss: 0.00067218
Iteration 15/25 | Loss: 0.00067218
Iteration 16/25 | Loss: 0.00067218
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006721820682287216, 0.0006721820682287216, 0.0006721820682287216, 0.0006721820682287216, 0.0006721820682287216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006721820682287216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067218
Iteration 2/1000 | Loss: 0.00005965
Iteration 3/1000 | Loss: 0.00003625
Iteration 4/1000 | Loss: 0.00002798
Iteration 5/1000 | Loss: 0.00002621
Iteration 6/1000 | Loss: 0.00002500
Iteration 7/1000 | Loss: 0.00002446
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00002364
Iteration 10/1000 | Loss: 0.00002338
Iteration 11/1000 | Loss: 0.00002318
Iteration 12/1000 | Loss: 0.00002297
Iteration 13/1000 | Loss: 0.00002295
Iteration 14/1000 | Loss: 0.00002282
Iteration 15/1000 | Loss: 0.00002281
Iteration 16/1000 | Loss: 0.00002281
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002271
Iteration 19/1000 | Loss: 0.00002270
Iteration 20/1000 | Loss: 0.00002268
Iteration 21/1000 | Loss: 0.00002268
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00002268
Iteration 24/1000 | Loss: 0.00002268
Iteration 25/1000 | Loss: 0.00002268
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002268
Iteration 29/1000 | Loss: 0.00002268
Iteration 30/1000 | Loss: 0.00002268
Iteration 31/1000 | Loss: 0.00002268
Iteration 32/1000 | Loss: 0.00002267
Iteration 33/1000 | Loss: 0.00002267
Iteration 34/1000 | Loss: 0.00002267
Iteration 35/1000 | Loss: 0.00002267
Iteration 36/1000 | Loss: 0.00002267
Iteration 37/1000 | Loss: 0.00002267
Iteration 38/1000 | Loss: 0.00002267
Iteration 39/1000 | Loss: 0.00002266
Iteration 40/1000 | Loss: 0.00002266
Iteration 41/1000 | Loss: 0.00002266
Iteration 42/1000 | Loss: 0.00002266
Iteration 43/1000 | Loss: 0.00002266
Iteration 44/1000 | Loss: 0.00002266
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002262
Iteration 48/1000 | Loss: 0.00002261
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002261
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002261
Iteration 55/1000 | Loss: 0.00002261
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00002260
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002259
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002256
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002255
Iteration 73/1000 | Loss: 0.00002255
Iteration 74/1000 | Loss: 0.00002255
Iteration 75/1000 | Loss: 0.00002254
Iteration 76/1000 | Loss: 0.00002254
Iteration 77/1000 | Loss: 0.00002254
Iteration 78/1000 | Loss: 0.00002253
Iteration 79/1000 | Loss: 0.00002253
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002253
Iteration 84/1000 | Loss: 0.00002253
Iteration 85/1000 | Loss: 0.00002253
Iteration 86/1000 | Loss: 0.00002253
Iteration 87/1000 | Loss: 0.00002253
Iteration 88/1000 | Loss: 0.00002253
Iteration 89/1000 | Loss: 0.00002253
Iteration 90/1000 | Loss: 0.00002253
Iteration 91/1000 | Loss: 0.00002253
Iteration 92/1000 | Loss: 0.00002253
Iteration 93/1000 | Loss: 0.00002253
Iteration 94/1000 | Loss: 0.00002253
Iteration 95/1000 | Loss: 0.00002253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.252828016935382e-05, 2.252828016935382e-05, 2.252828016935382e-05, 2.252828016935382e-05, 2.252828016935382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.252828016935382e-05

Optimization complete. Final v2v error: 4.044356822967529 mm

Highest mean error: 4.240324020385742 mm for frame 51

Lowest mean error: 3.6225390434265137 mm for frame 102

Saving results

Total time: 36.3393440246582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960056
Iteration 2/25 | Loss: 0.00210593
Iteration 3/25 | Loss: 0.00141716
Iteration 4/25 | Loss: 0.00135645
Iteration 5/25 | Loss: 0.00135852
Iteration 6/25 | Loss: 0.00125672
Iteration 7/25 | Loss: 0.00123454
Iteration 8/25 | Loss: 0.00123085
Iteration 9/25 | Loss: 0.00122203
Iteration 10/25 | Loss: 0.00121647
Iteration 11/25 | Loss: 0.00121519
Iteration 12/25 | Loss: 0.00121134
Iteration 13/25 | Loss: 0.00120979
Iteration 14/25 | Loss: 0.00120912
Iteration 15/25 | Loss: 0.00120892
Iteration 16/25 | Loss: 0.00120881
Iteration 17/25 | Loss: 0.00120873
Iteration 18/25 | Loss: 0.00120869
Iteration 19/25 | Loss: 0.00120868
Iteration 20/25 | Loss: 0.00120868
Iteration 21/25 | Loss: 0.00120868
Iteration 22/25 | Loss: 0.00120868
Iteration 23/25 | Loss: 0.00120868
Iteration 24/25 | Loss: 0.00120868
Iteration 25/25 | Loss: 0.00120868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35248888
Iteration 2/25 | Loss: 0.00074314
Iteration 3/25 | Loss: 0.00065989
Iteration 4/25 | Loss: 0.00065989
Iteration 5/25 | Loss: 0.00065989
Iteration 6/25 | Loss: 0.00065989
Iteration 7/25 | Loss: 0.00065989
Iteration 8/25 | Loss: 0.00065989
Iteration 9/25 | Loss: 0.00065989
Iteration 10/25 | Loss: 0.00065989
Iteration 11/25 | Loss: 0.00065989
Iteration 12/25 | Loss: 0.00065989
Iteration 13/25 | Loss: 0.00065989
Iteration 14/25 | Loss: 0.00065989
Iteration 15/25 | Loss: 0.00065989
Iteration 16/25 | Loss: 0.00065989
Iteration 17/25 | Loss: 0.00065989
Iteration 18/25 | Loss: 0.00065989
Iteration 19/25 | Loss: 0.00065989
Iteration 20/25 | Loss: 0.00065989
Iteration 21/25 | Loss: 0.00065989
Iteration 22/25 | Loss: 0.00065989
Iteration 23/25 | Loss: 0.00065989
Iteration 24/25 | Loss: 0.00065989
Iteration 25/25 | Loss: 0.00065989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065989
Iteration 2/1000 | Loss: 0.00003378
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00008260
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001814
Iteration 9/1000 | Loss: 0.00006100
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001731
Iteration 12/1000 | Loss: 0.00006985
Iteration 13/1000 | Loss: 0.00012621
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001593
Iteration 18/1000 | Loss: 0.00009179
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00003981
Iteration 21/1000 | Loss: 0.00002059
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001516
Iteration 30/1000 | Loss: 0.00001515
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001501
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001496
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001495
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001493
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001493
Iteration 74/1000 | Loss: 0.00001493
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001493
Iteration 78/1000 | Loss: 0.00001493
Iteration 79/1000 | Loss: 0.00001493
Iteration 80/1000 | Loss: 0.00001493
Iteration 81/1000 | Loss: 0.00001493
Iteration 82/1000 | Loss: 0.00001493
Iteration 83/1000 | Loss: 0.00001493
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001493
Iteration 91/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.493225045123836e-05, 1.493225045123836e-05, 1.493225045123836e-05, 1.493225045123836e-05, 1.493225045123836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.493225045123836e-05

Optimization complete. Final v2v error: 3.2278220653533936 mm

Highest mean error: 4.402163982391357 mm for frame 63

Lowest mean error: 2.7274396419525146 mm for frame 187

Saving results

Total time: 82.49960780143738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020319
Iteration 2/25 | Loss: 0.00238196
Iteration 3/25 | Loss: 0.00174649
Iteration 4/25 | Loss: 0.00154407
Iteration 5/25 | Loss: 0.00150393
Iteration 6/25 | Loss: 0.00138111
Iteration 7/25 | Loss: 0.00134150
Iteration 8/25 | Loss: 0.00130609
Iteration 9/25 | Loss: 0.00128861
Iteration 10/25 | Loss: 0.00128071
Iteration 11/25 | Loss: 0.00127508
Iteration 12/25 | Loss: 0.00127316
Iteration 13/25 | Loss: 0.00127239
Iteration 14/25 | Loss: 0.00127635
Iteration 15/25 | Loss: 0.00127368
Iteration 16/25 | Loss: 0.00127200
Iteration 17/25 | Loss: 0.00127176
Iteration 18/25 | Loss: 0.00127133
Iteration 19/25 | Loss: 0.00127113
Iteration 20/25 | Loss: 0.00127108
Iteration 21/25 | Loss: 0.00127108
Iteration 22/25 | Loss: 0.00127108
Iteration 23/25 | Loss: 0.00127108
Iteration 24/25 | Loss: 0.00127108
Iteration 25/25 | Loss: 0.00127108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34137762
Iteration 2/25 | Loss: 0.00090716
Iteration 3/25 | Loss: 0.00090715
Iteration 4/25 | Loss: 0.00090715
Iteration 5/25 | Loss: 0.00090715
Iteration 6/25 | Loss: 0.00090715
Iteration 7/25 | Loss: 0.00090715
Iteration 8/25 | Loss: 0.00090715
Iteration 9/25 | Loss: 0.00090715
Iteration 10/25 | Loss: 0.00090715
Iteration 11/25 | Loss: 0.00090715
Iteration 12/25 | Loss: 0.00090715
Iteration 13/25 | Loss: 0.00090715
Iteration 14/25 | Loss: 0.00090715
Iteration 15/25 | Loss: 0.00090715
Iteration 16/25 | Loss: 0.00090715
Iteration 17/25 | Loss: 0.00090715
Iteration 18/25 | Loss: 0.00090715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000907145265955478, 0.000907145265955478, 0.000907145265955478, 0.000907145265955478, 0.000907145265955478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000907145265955478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090715
Iteration 2/1000 | Loss: 0.00021507
Iteration 3/1000 | Loss: 0.00006488
Iteration 4/1000 | Loss: 0.00005288
Iteration 5/1000 | Loss: 0.00004693
Iteration 6/1000 | Loss: 0.00004207
Iteration 7/1000 | Loss: 0.00003980
Iteration 8/1000 | Loss: 0.00003833
Iteration 9/1000 | Loss: 0.00003739
Iteration 10/1000 | Loss: 0.00003671
Iteration 11/1000 | Loss: 0.00003619
Iteration 12/1000 | Loss: 0.00043153
Iteration 13/1000 | Loss: 0.00017763
Iteration 14/1000 | Loss: 0.00003799
Iteration 15/1000 | Loss: 0.00003269
Iteration 16/1000 | Loss: 0.00002836
Iteration 17/1000 | Loss: 0.00002535
Iteration 18/1000 | Loss: 0.00002418
Iteration 19/1000 | Loss: 0.00002328
Iteration 20/1000 | Loss: 0.00002246
Iteration 21/1000 | Loss: 0.00002180
Iteration 22/1000 | Loss: 0.00002134
Iteration 23/1000 | Loss: 0.00002103
Iteration 24/1000 | Loss: 0.00002077
Iteration 25/1000 | Loss: 0.00002068
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002049
Iteration 28/1000 | Loss: 0.00002043
Iteration 29/1000 | Loss: 0.00002033
Iteration 30/1000 | Loss: 0.00002030
Iteration 31/1000 | Loss: 0.00002030
Iteration 32/1000 | Loss: 0.00002028
Iteration 33/1000 | Loss: 0.00002027
Iteration 34/1000 | Loss: 0.00002027
Iteration 35/1000 | Loss: 0.00002026
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002026
Iteration 38/1000 | Loss: 0.00002026
Iteration 39/1000 | Loss: 0.00002026
Iteration 40/1000 | Loss: 0.00002026
Iteration 41/1000 | Loss: 0.00002026
Iteration 42/1000 | Loss: 0.00002025
Iteration 43/1000 | Loss: 0.00002025
Iteration 44/1000 | Loss: 0.00002025
Iteration 45/1000 | Loss: 0.00002025
Iteration 46/1000 | Loss: 0.00002025
Iteration 47/1000 | Loss: 0.00002025
Iteration 48/1000 | Loss: 0.00002025
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002024
Iteration 51/1000 | Loss: 0.00002024
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002023
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002022
Iteration 59/1000 | Loss: 0.00002021
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002021
Iteration 62/1000 | Loss: 0.00002021
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002020
Iteration 66/1000 | Loss: 0.00002020
Iteration 67/1000 | Loss: 0.00002020
Iteration 68/1000 | Loss: 0.00002020
Iteration 69/1000 | Loss: 0.00002020
Iteration 70/1000 | Loss: 0.00002020
Iteration 71/1000 | Loss: 0.00002020
Iteration 72/1000 | Loss: 0.00002019
Iteration 73/1000 | Loss: 0.00002019
Iteration 74/1000 | Loss: 0.00002019
Iteration 75/1000 | Loss: 0.00002019
Iteration 76/1000 | Loss: 0.00002019
Iteration 77/1000 | Loss: 0.00002019
Iteration 78/1000 | Loss: 0.00002019
Iteration 79/1000 | Loss: 0.00002019
Iteration 80/1000 | Loss: 0.00002019
Iteration 81/1000 | Loss: 0.00002019
Iteration 82/1000 | Loss: 0.00002019
Iteration 83/1000 | Loss: 0.00002019
Iteration 84/1000 | Loss: 0.00002019
Iteration 85/1000 | Loss: 0.00002019
Iteration 86/1000 | Loss: 0.00002018
Iteration 87/1000 | Loss: 0.00002018
Iteration 88/1000 | Loss: 0.00002018
Iteration 89/1000 | Loss: 0.00002018
Iteration 90/1000 | Loss: 0.00002018
Iteration 91/1000 | Loss: 0.00002018
Iteration 92/1000 | Loss: 0.00002018
Iteration 93/1000 | Loss: 0.00002018
Iteration 94/1000 | Loss: 0.00002018
Iteration 95/1000 | Loss: 0.00002018
Iteration 96/1000 | Loss: 0.00002017
Iteration 97/1000 | Loss: 0.00002017
Iteration 98/1000 | Loss: 0.00002017
Iteration 99/1000 | Loss: 0.00002017
Iteration 100/1000 | Loss: 0.00002017
Iteration 101/1000 | Loss: 0.00002017
Iteration 102/1000 | Loss: 0.00002017
Iteration 103/1000 | Loss: 0.00002017
Iteration 104/1000 | Loss: 0.00002017
Iteration 105/1000 | Loss: 0.00002016
Iteration 106/1000 | Loss: 0.00002016
Iteration 107/1000 | Loss: 0.00002016
Iteration 108/1000 | Loss: 0.00002016
Iteration 109/1000 | Loss: 0.00002016
Iteration 110/1000 | Loss: 0.00002016
Iteration 111/1000 | Loss: 0.00002016
Iteration 112/1000 | Loss: 0.00002016
Iteration 113/1000 | Loss: 0.00002016
Iteration 114/1000 | Loss: 0.00002016
Iteration 115/1000 | Loss: 0.00002016
Iteration 116/1000 | Loss: 0.00002016
Iteration 117/1000 | Loss: 0.00002016
Iteration 118/1000 | Loss: 0.00002016
Iteration 119/1000 | Loss: 0.00002016
Iteration 120/1000 | Loss: 0.00002016
Iteration 121/1000 | Loss: 0.00002016
Iteration 122/1000 | Loss: 0.00002016
Iteration 123/1000 | Loss: 0.00002016
Iteration 124/1000 | Loss: 0.00002016
Iteration 125/1000 | Loss: 0.00002016
Iteration 126/1000 | Loss: 0.00002016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.0155635866103694e-05, 2.0155635866103694e-05, 2.0155635866103694e-05, 2.0155635866103694e-05, 2.0155635866103694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0155635866103694e-05

Optimization complete. Final v2v error: 3.7387304306030273 mm

Highest mean error: 3.9502875804901123 mm for frame 117

Lowest mean error: 3.382387399673462 mm for frame 0

Saving results

Total time: 89.71447968482971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952741
Iteration 2/25 | Loss: 0.00206669
Iteration 3/25 | Loss: 0.00145721
Iteration 4/25 | Loss: 0.00144002
Iteration 5/25 | Loss: 0.00143700
Iteration 6/25 | Loss: 0.00143606
Iteration 7/25 | Loss: 0.00143606
Iteration 8/25 | Loss: 0.00143606
Iteration 9/25 | Loss: 0.00143606
Iteration 10/25 | Loss: 0.00143606
Iteration 11/25 | Loss: 0.00143606
Iteration 12/25 | Loss: 0.00143606
Iteration 13/25 | Loss: 0.00143606
Iteration 14/25 | Loss: 0.00143606
Iteration 15/25 | Loss: 0.00143606
Iteration 16/25 | Loss: 0.00143606
Iteration 17/25 | Loss: 0.00143606
Iteration 18/25 | Loss: 0.00143606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014360587811097503, 0.0014360587811097503, 0.0014360587811097503, 0.0014360587811097503, 0.0014360587811097503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014360587811097503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66687965
Iteration 2/25 | Loss: 0.00094751
Iteration 3/25 | Loss: 0.00094751
Iteration 4/25 | Loss: 0.00094751
Iteration 5/25 | Loss: 0.00094751
Iteration 6/25 | Loss: 0.00094751
Iteration 7/25 | Loss: 0.00094751
Iteration 8/25 | Loss: 0.00094751
Iteration 9/25 | Loss: 0.00094751
Iteration 10/25 | Loss: 0.00094751
Iteration 11/25 | Loss: 0.00094751
Iteration 12/25 | Loss: 0.00094751
Iteration 13/25 | Loss: 0.00094751
Iteration 14/25 | Loss: 0.00094751
Iteration 15/25 | Loss: 0.00094751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009475132101215422, 0.0009475132101215422, 0.0009475132101215422, 0.0009475132101215422, 0.0009475132101215422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009475132101215422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094751
Iteration 2/1000 | Loss: 0.00008135
Iteration 3/1000 | Loss: 0.00005694
Iteration 4/1000 | Loss: 0.00004617
Iteration 5/1000 | Loss: 0.00004281
Iteration 6/1000 | Loss: 0.00004154
Iteration 7/1000 | Loss: 0.00004029
Iteration 8/1000 | Loss: 0.00003971
Iteration 9/1000 | Loss: 0.00003858
Iteration 10/1000 | Loss: 0.00003780
Iteration 11/1000 | Loss: 0.00003707
Iteration 12/1000 | Loss: 0.00003661
Iteration 13/1000 | Loss: 0.00003624
Iteration 14/1000 | Loss: 0.00003593
Iteration 15/1000 | Loss: 0.00003558
Iteration 16/1000 | Loss: 0.00003534
Iteration 17/1000 | Loss: 0.00003512
Iteration 18/1000 | Loss: 0.00003499
Iteration 19/1000 | Loss: 0.00003484
Iteration 20/1000 | Loss: 0.00003481
Iteration 21/1000 | Loss: 0.00003477
Iteration 22/1000 | Loss: 0.00003467
Iteration 23/1000 | Loss: 0.00003462
Iteration 24/1000 | Loss: 0.00003445
Iteration 25/1000 | Loss: 0.00003433
Iteration 26/1000 | Loss: 0.00003421
Iteration 27/1000 | Loss: 0.00003418
Iteration 28/1000 | Loss: 0.00003416
Iteration 29/1000 | Loss: 0.00003407
Iteration 30/1000 | Loss: 0.00003406
Iteration 31/1000 | Loss: 0.00003406
Iteration 32/1000 | Loss: 0.00003405
Iteration 33/1000 | Loss: 0.00003405
Iteration 34/1000 | Loss: 0.00003405
Iteration 35/1000 | Loss: 0.00003404
Iteration 36/1000 | Loss: 0.00003404
Iteration 37/1000 | Loss: 0.00003403
Iteration 38/1000 | Loss: 0.00003402
Iteration 39/1000 | Loss: 0.00003397
Iteration 40/1000 | Loss: 0.00003394
Iteration 41/1000 | Loss: 0.00003394
Iteration 42/1000 | Loss: 0.00003394
Iteration 43/1000 | Loss: 0.00003394
Iteration 44/1000 | Loss: 0.00003394
Iteration 45/1000 | Loss: 0.00003394
Iteration 46/1000 | Loss: 0.00003394
Iteration 47/1000 | Loss: 0.00003393
Iteration 48/1000 | Loss: 0.00003393
Iteration 49/1000 | Loss: 0.00003393
Iteration 50/1000 | Loss: 0.00003393
Iteration 51/1000 | Loss: 0.00003393
Iteration 52/1000 | Loss: 0.00003393
Iteration 53/1000 | Loss: 0.00003393
Iteration 54/1000 | Loss: 0.00003393
Iteration 55/1000 | Loss: 0.00003393
Iteration 56/1000 | Loss: 0.00003392
Iteration 57/1000 | Loss: 0.00003392
Iteration 58/1000 | Loss: 0.00003392
Iteration 59/1000 | Loss: 0.00003392
Iteration 60/1000 | Loss: 0.00003392
Iteration 61/1000 | Loss: 0.00003390
Iteration 62/1000 | Loss: 0.00003389
Iteration 63/1000 | Loss: 0.00003389
Iteration 64/1000 | Loss: 0.00003389
Iteration 65/1000 | Loss: 0.00003389
Iteration 66/1000 | Loss: 0.00003389
Iteration 67/1000 | Loss: 0.00003388
Iteration 68/1000 | Loss: 0.00003388
Iteration 69/1000 | Loss: 0.00003388
Iteration 70/1000 | Loss: 0.00003387
Iteration 71/1000 | Loss: 0.00003386
Iteration 72/1000 | Loss: 0.00003386
Iteration 73/1000 | Loss: 0.00003386
Iteration 74/1000 | Loss: 0.00003386
Iteration 75/1000 | Loss: 0.00003386
Iteration 76/1000 | Loss: 0.00003386
Iteration 77/1000 | Loss: 0.00003386
Iteration 78/1000 | Loss: 0.00003386
Iteration 79/1000 | Loss: 0.00003386
Iteration 80/1000 | Loss: 0.00003386
Iteration 81/1000 | Loss: 0.00003386
Iteration 82/1000 | Loss: 0.00003386
Iteration 83/1000 | Loss: 0.00003386
Iteration 84/1000 | Loss: 0.00003386
Iteration 85/1000 | Loss: 0.00003386
Iteration 86/1000 | Loss: 0.00003386
Iteration 87/1000 | Loss: 0.00003386
Iteration 88/1000 | Loss: 0.00003386
Iteration 89/1000 | Loss: 0.00003386
Iteration 90/1000 | Loss: 0.00003386
Iteration 91/1000 | Loss: 0.00003386
Iteration 92/1000 | Loss: 0.00003386
Iteration 93/1000 | Loss: 0.00003386
Iteration 94/1000 | Loss: 0.00003386
Iteration 95/1000 | Loss: 0.00003386
Iteration 96/1000 | Loss: 0.00003386
Iteration 97/1000 | Loss: 0.00003386
Iteration 98/1000 | Loss: 0.00003386
Iteration 99/1000 | Loss: 0.00003386
Iteration 100/1000 | Loss: 0.00003386
Iteration 101/1000 | Loss: 0.00003386
Iteration 102/1000 | Loss: 0.00003386
Iteration 103/1000 | Loss: 0.00003386
Iteration 104/1000 | Loss: 0.00003386
Iteration 105/1000 | Loss: 0.00003386
Iteration 106/1000 | Loss: 0.00003386
Iteration 107/1000 | Loss: 0.00003386
Iteration 108/1000 | Loss: 0.00003386
Iteration 109/1000 | Loss: 0.00003386
Iteration 110/1000 | Loss: 0.00003386
Iteration 111/1000 | Loss: 0.00003386
Iteration 112/1000 | Loss: 0.00003386
Iteration 113/1000 | Loss: 0.00003386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.38577083311975e-05, 3.38577083311975e-05, 3.38577083311975e-05, 3.38577083311975e-05, 3.38577083311975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.38577083311975e-05

Optimization complete. Final v2v error: 4.792710304260254 mm

Highest mean error: 5.4224395751953125 mm for frame 58

Lowest mean error: 4.368414878845215 mm for frame 113

Saving results

Total time: 48.09060502052307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00612533
Iteration 2/25 | Loss: 0.00143059
Iteration 3/25 | Loss: 0.00129865
Iteration 4/25 | Loss: 0.00127410
Iteration 5/25 | Loss: 0.00126906
Iteration 6/25 | Loss: 0.00126766
Iteration 7/25 | Loss: 0.00126766
Iteration 8/25 | Loss: 0.00126766
Iteration 9/25 | Loss: 0.00126766
Iteration 10/25 | Loss: 0.00126766
Iteration 11/25 | Loss: 0.00126766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012676637852564454, 0.0012676637852564454, 0.0012676637852564454, 0.0012676637852564454, 0.0012676637852564454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012676637852564454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70117474
Iteration 2/25 | Loss: 0.00121211
Iteration 3/25 | Loss: 0.00121209
Iteration 4/25 | Loss: 0.00121209
Iteration 5/25 | Loss: 0.00121209
Iteration 6/25 | Loss: 0.00121209
Iteration 7/25 | Loss: 0.00121209
Iteration 8/25 | Loss: 0.00121209
Iteration 9/25 | Loss: 0.00121209
Iteration 10/25 | Loss: 0.00121209
Iteration 11/25 | Loss: 0.00121209
Iteration 12/25 | Loss: 0.00121209
Iteration 13/25 | Loss: 0.00121209
Iteration 14/25 | Loss: 0.00121209
Iteration 15/25 | Loss: 0.00121209
Iteration 16/25 | Loss: 0.00121209
Iteration 17/25 | Loss: 0.00121209
Iteration 18/25 | Loss: 0.00121209
Iteration 19/25 | Loss: 0.00121209
Iteration 20/25 | Loss: 0.00121209
Iteration 21/25 | Loss: 0.00121209
Iteration 22/25 | Loss: 0.00121209
Iteration 23/25 | Loss: 0.00121209
Iteration 24/25 | Loss: 0.00121209
Iteration 25/25 | Loss: 0.00121209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121209
Iteration 2/1000 | Loss: 0.00004322
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002181
Iteration 5/1000 | Loss: 0.00002034
Iteration 6/1000 | Loss: 0.00001932
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001803
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001764
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001763
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001763
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001745
Iteration 50/1000 | Loss: 0.00001745
Iteration 51/1000 | Loss: 0.00001745
Iteration 52/1000 | Loss: 0.00001745
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001744
Iteration 56/1000 | Loss: 0.00001744
Iteration 57/1000 | Loss: 0.00001744
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001744
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001742
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.7416150512872264e-05, 1.7416150512872264e-05, 1.7416150512872264e-05, 1.7416150512872264e-05, 1.7416150512872264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7416150512872264e-05

Optimization complete. Final v2v error: 3.531540632247925 mm

Highest mean error: 4.413354396820068 mm for frame 0

Lowest mean error: 3.412628173828125 mm for frame 29

Saving results

Total time: 30.799399614334106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100620
Iteration 2/25 | Loss: 0.00360514
Iteration 3/25 | Loss: 0.00255876
Iteration 4/25 | Loss: 0.00230724
Iteration 5/25 | Loss: 0.00155374
Iteration 6/25 | Loss: 0.00148172
Iteration 7/25 | Loss: 0.00145867
Iteration 8/25 | Loss: 0.00143623
Iteration 9/25 | Loss: 0.00143039
Iteration 10/25 | Loss: 0.00142446
Iteration 11/25 | Loss: 0.00142309
Iteration 12/25 | Loss: 0.00142286
Iteration 13/25 | Loss: 0.00142285
Iteration 14/25 | Loss: 0.00142285
Iteration 15/25 | Loss: 0.00142284
Iteration 16/25 | Loss: 0.00142284
Iteration 17/25 | Loss: 0.00142284
Iteration 18/25 | Loss: 0.00142284
Iteration 19/25 | Loss: 0.00142284
Iteration 20/25 | Loss: 0.00142283
Iteration 21/25 | Loss: 0.00142283
Iteration 22/25 | Loss: 0.00142283
Iteration 23/25 | Loss: 0.00142283
Iteration 24/25 | Loss: 0.00142283
Iteration 25/25 | Loss: 0.00142283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64053690
Iteration 2/25 | Loss: 0.00436375
Iteration 3/25 | Loss: 0.00114911
Iteration 4/25 | Loss: 0.00114911
Iteration 5/25 | Loss: 0.00114911
Iteration 6/25 | Loss: 0.00114911
Iteration 7/25 | Loss: 0.00114911
Iteration 8/25 | Loss: 0.00114910
Iteration 9/25 | Loss: 0.00114910
Iteration 10/25 | Loss: 0.00114910
Iteration 11/25 | Loss: 0.00114910
Iteration 12/25 | Loss: 0.00114910
Iteration 13/25 | Loss: 0.00114910
Iteration 14/25 | Loss: 0.00114910
Iteration 15/25 | Loss: 0.00114910
Iteration 16/25 | Loss: 0.00114910
Iteration 17/25 | Loss: 0.00114910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011491036275401711, 0.0011491036275401711, 0.0011491036275401711, 0.0011491036275401711, 0.0011491036275401711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011491036275401711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114910
Iteration 2/1000 | Loss: 0.00067269
Iteration 3/1000 | Loss: 0.00003524
Iteration 4/1000 | Loss: 0.00002903
Iteration 5/1000 | Loss: 0.00002737
Iteration 6/1000 | Loss: 0.00002660
Iteration 7/1000 | Loss: 0.00002898
Iteration 8/1000 | Loss: 0.00003082
Iteration 9/1000 | Loss: 0.00002855
Iteration 10/1000 | Loss: 0.00002659
Iteration 11/1000 | Loss: 0.00002592
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002458
Iteration 15/1000 | Loss: 0.00005414
Iteration 16/1000 | Loss: 0.00005355
Iteration 17/1000 | Loss: 0.00004836
Iteration 18/1000 | Loss: 0.00003134
Iteration 19/1000 | Loss: 0.00002645
Iteration 20/1000 | Loss: 0.00002469
Iteration 21/1000 | Loss: 0.00002393
Iteration 22/1000 | Loss: 0.00002370
Iteration 23/1000 | Loss: 0.00002361
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002343
Iteration 26/1000 | Loss: 0.00002330
Iteration 27/1000 | Loss: 0.00002325
Iteration 28/1000 | Loss: 0.00002324
Iteration 29/1000 | Loss: 0.00002317
Iteration 30/1000 | Loss: 0.00002312
Iteration 31/1000 | Loss: 0.00002312
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00002312
Iteration 34/1000 | Loss: 0.00002312
Iteration 35/1000 | Loss: 0.00002312
Iteration 36/1000 | Loss: 0.00002312
Iteration 37/1000 | Loss: 0.00002312
Iteration 38/1000 | Loss: 0.00002312
Iteration 39/1000 | Loss: 0.00002312
Iteration 40/1000 | Loss: 0.00002311
Iteration 41/1000 | Loss: 0.00002311
Iteration 42/1000 | Loss: 0.00002311
Iteration 43/1000 | Loss: 0.00002311
Iteration 44/1000 | Loss: 0.00002309
Iteration 45/1000 | Loss: 0.00002308
Iteration 46/1000 | Loss: 0.00002308
Iteration 47/1000 | Loss: 0.00002307
Iteration 48/1000 | Loss: 0.00002306
Iteration 49/1000 | Loss: 0.00002306
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002305
Iteration 53/1000 | Loss: 0.00002305
Iteration 54/1000 | Loss: 0.00002305
Iteration 55/1000 | Loss: 0.00002305
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002305
Iteration 59/1000 | Loss: 0.00002305
Iteration 60/1000 | Loss: 0.00002305
Iteration 61/1000 | Loss: 0.00002305
Iteration 62/1000 | Loss: 0.00002305
Iteration 63/1000 | Loss: 0.00002305
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.3048592993291095e-05, 2.3048592993291095e-05, 2.3048592993291095e-05, 2.3048592993291095e-05, 2.3048592993291095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3048592993291095e-05

Optimization complete. Final v2v error: 3.7582895755767822 mm

Highest mean error: 9.735393524169922 mm for frame 125

Lowest mean error: 3.2658114433288574 mm for frame 3

Saving results

Total time: 61.033915519714355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003396
Iteration 2/25 | Loss: 0.01003396
Iteration 3/25 | Loss: 0.01003396
Iteration 4/25 | Loss: 0.01003395
Iteration 5/25 | Loss: 0.01003395
Iteration 6/25 | Loss: 0.01003395
Iteration 7/25 | Loss: 0.01003395
Iteration 8/25 | Loss: 0.01003395
Iteration 9/25 | Loss: 0.01003395
Iteration 10/25 | Loss: 0.01003395
Iteration 11/25 | Loss: 0.01003395
Iteration 12/25 | Loss: 0.01003395
Iteration 13/25 | Loss: 0.01003395
Iteration 14/25 | Loss: 0.01003395
Iteration 15/25 | Loss: 0.01003395
Iteration 16/25 | Loss: 0.01003395
Iteration 17/25 | Loss: 0.01003395
Iteration 18/25 | Loss: 0.01003395
Iteration 19/25 | Loss: 0.01003394
Iteration 20/25 | Loss: 0.01003394
Iteration 21/25 | Loss: 0.01003394
Iteration 22/25 | Loss: 0.01003394
Iteration 23/25 | Loss: 0.01003394
Iteration 24/25 | Loss: 0.01003394
Iteration 25/25 | Loss: 0.01003394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60128748
Iteration 2/25 | Loss: 0.11833933
Iteration 3/25 | Loss: 0.11771718
Iteration 4/25 | Loss: 0.11740689
Iteration 5/25 | Loss: 0.11729484
Iteration 6/25 | Loss: 0.11729483
Iteration 7/25 | Loss: 0.11729482
Iteration 8/25 | Loss: 0.11729481
Iteration 9/25 | Loss: 0.11729481
Iteration 10/25 | Loss: 0.11729480
Iteration 11/25 | Loss: 0.11729481
Iteration 12/25 | Loss: 0.11729481
Iteration 13/25 | Loss: 0.11729480
Iteration 14/25 | Loss: 0.11729480
Iteration 15/25 | Loss: 0.11729480
Iteration 16/25 | Loss: 0.11729480
Iteration 17/25 | Loss: 0.11729481
Iteration 18/25 | Loss: 0.11729481
Iteration 19/25 | Loss: 0.11729481
Iteration 20/25 | Loss: 0.11729481
Iteration 21/25 | Loss: 0.11729481
Iteration 22/25 | Loss: 0.11729481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.1172948107123375, 0.1172948107123375, 0.1172948107123375, 0.1172948107123375, 0.1172948107123375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1172948107123375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11729481
Iteration 2/1000 | Loss: 0.00315328
Iteration 3/1000 | Loss: 0.00592005
Iteration 4/1000 | Loss: 0.00177411
Iteration 5/1000 | Loss: 0.00064965
Iteration 6/1000 | Loss: 0.00041170
Iteration 7/1000 | Loss: 0.00021253
Iteration 8/1000 | Loss: 0.00008210
Iteration 9/1000 | Loss: 0.00009909
Iteration 10/1000 | Loss: 0.00005161
Iteration 11/1000 | Loss: 0.00034306
Iteration 12/1000 | Loss: 0.00008135
Iteration 13/1000 | Loss: 0.00028072
Iteration 14/1000 | Loss: 0.00025139
Iteration 15/1000 | Loss: 0.00050461
Iteration 16/1000 | Loss: 0.00003498
Iteration 17/1000 | Loss: 0.00006517
Iteration 18/1000 | Loss: 0.00004358
Iteration 19/1000 | Loss: 0.00002397
Iteration 20/1000 | Loss: 0.00014134
Iteration 21/1000 | Loss: 0.00043128
Iteration 22/1000 | Loss: 0.00004729
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00009797
Iteration 25/1000 | Loss: 0.00004553
Iteration 26/1000 | Loss: 0.00081228
Iteration 27/1000 | Loss: 0.00004322
Iteration 28/1000 | Loss: 0.00013366
Iteration 29/1000 | Loss: 0.00003154
Iteration 30/1000 | Loss: 0.00004190
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00002123
Iteration 33/1000 | Loss: 0.00001919
Iteration 34/1000 | Loss: 0.00007763
Iteration 35/1000 | Loss: 0.00098793
Iteration 36/1000 | Loss: 0.00003323
Iteration 37/1000 | Loss: 0.00004788
Iteration 38/1000 | Loss: 0.00007045
Iteration 39/1000 | Loss: 0.00017141
Iteration 40/1000 | Loss: 0.00003304
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00002294
Iteration 43/1000 | Loss: 0.00017032
Iteration 44/1000 | Loss: 0.00002662
Iteration 45/1000 | Loss: 0.00006058
Iteration 46/1000 | Loss: 0.00002672
Iteration 47/1000 | Loss: 0.00002280
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00002237
Iteration 50/1000 | Loss: 0.00004698
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001501
Iteration 56/1000 | Loss: 0.00004545
Iteration 57/1000 | Loss: 0.00003656
Iteration 58/1000 | Loss: 0.00005625
Iteration 59/1000 | Loss: 0.00005852
Iteration 60/1000 | Loss: 0.00002223
Iteration 61/1000 | Loss: 0.00004392
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00005352
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001473
Iteration 93/1000 | Loss: 0.00001473
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001471
Iteration 105/1000 | Loss: 0.00001471
Iteration 106/1000 | Loss: 0.00001471
Iteration 107/1000 | Loss: 0.00001471
Iteration 108/1000 | Loss: 0.00001471
Iteration 109/1000 | Loss: 0.00001471
Iteration 110/1000 | Loss: 0.00001921
Iteration 111/1000 | Loss: 0.00004988
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00002097
Iteration 114/1000 | Loss: 0.00001469
Iteration 115/1000 | Loss: 0.00001947
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001491
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001841
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001464
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001463
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00002233
Iteration 158/1000 | Loss: 0.00002347
Iteration 159/1000 | Loss: 0.00002382
Iteration 160/1000 | Loss: 0.00002759
Iteration 161/1000 | Loss: 0.00001824
Iteration 162/1000 | Loss: 0.00005069
Iteration 163/1000 | Loss: 0.00001748
Iteration 164/1000 | Loss: 0.00001462
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001461
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001461
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001460
Iteration 184/1000 | Loss: 0.00001460
Iteration 185/1000 | Loss: 0.00001460
Iteration 186/1000 | Loss: 0.00001460
Iteration 187/1000 | Loss: 0.00001460
Iteration 188/1000 | Loss: 0.00001460
Iteration 189/1000 | Loss: 0.00001460
Iteration 190/1000 | Loss: 0.00001460
Iteration 191/1000 | Loss: 0.00001460
Iteration 192/1000 | Loss: 0.00001460
Iteration 193/1000 | Loss: 0.00001460
Iteration 194/1000 | Loss: 0.00001460
Iteration 195/1000 | Loss: 0.00001460
Iteration 196/1000 | Loss: 0.00001460
Iteration 197/1000 | Loss: 0.00001460
Iteration 198/1000 | Loss: 0.00001460
Iteration 199/1000 | Loss: 0.00001460
Iteration 200/1000 | Loss: 0.00001460
Iteration 201/1000 | Loss: 0.00001460
Iteration 202/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.460065868741367e-05, 1.460065868741367e-05, 1.460065868741367e-05, 1.460065868741367e-05, 1.460065868741367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.460065868741367e-05

Optimization complete. Final v2v error: 3.2300946712493896 mm

Highest mean error: 3.4013583660125732 mm for frame 86

Lowest mean error: 3.12422776222229 mm for frame 225

Saving results

Total time: 137.0561056137085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487458
Iteration 2/25 | Loss: 0.00126424
Iteration 3/25 | Loss: 0.00118741
Iteration 4/25 | Loss: 0.00117565
Iteration 5/25 | Loss: 0.00117128
Iteration 6/25 | Loss: 0.00117081
Iteration 7/25 | Loss: 0.00117081
Iteration 8/25 | Loss: 0.00117081
Iteration 9/25 | Loss: 0.00117081
Iteration 10/25 | Loss: 0.00117081
Iteration 11/25 | Loss: 0.00117081
Iteration 12/25 | Loss: 0.00117081
Iteration 13/25 | Loss: 0.00117081
Iteration 14/25 | Loss: 0.00117081
Iteration 15/25 | Loss: 0.00117081
Iteration 16/25 | Loss: 0.00117081
Iteration 17/25 | Loss: 0.00117081
Iteration 18/25 | Loss: 0.00117081
Iteration 19/25 | Loss: 0.00117081
Iteration 20/25 | Loss: 0.00117081
Iteration 21/25 | Loss: 0.00117081
Iteration 22/25 | Loss: 0.00117081
Iteration 23/25 | Loss: 0.00117081
Iteration 24/25 | Loss: 0.00117081
Iteration 25/25 | Loss: 0.00117081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54831004
Iteration 2/25 | Loss: 0.00087517
Iteration 3/25 | Loss: 0.00087516
Iteration 4/25 | Loss: 0.00087516
Iteration 5/25 | Loss: 0.00087516
Iteration 6/25 | Loss: 0.00087516
Iteration 7/25 | Loss: 0.00087516
Iteration 8/25 | Loss: 0.00087516
Iteration 9/25 | Loss: 0.00087516
Iteration 10/25 | Loss: 0.00087516
Iteration 11/25 | Loss: 0.00087516
Iteration 12/25 | Loss: 0.00087516
Iteration 13/25 | Loss: 0.00087516
Iteration 14/25 | Loss: 0.00087516
Iteration 15/25 | Loss: 0.00087516
Iteration 16/25 | Loss: 0.00087516
Iteration 17/25 | Loss: 0.00087516
Iteration 18/25 | Loss: 0.00087516
Iteration 19/25 | Loss: 0.00087516
Iteration 20/25 | Loss: 0.00087516
Iteration 21/25 | Loss: 0.00087516
Iteration 22/25 | Loss: 0.00087516
Iteration 23/25 | Loss: 0.00087516
Iteration 24/25 | Loss: 0.00087516
Iteration 25/25 | Loss: 0.00087516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087516
Iteration 2/1000 | Loss: 0.00003307
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001786
Iteration 5/1000 | Loss: 0.00001675
Iteration 6/1000 | Loss: 0.00001600
Iteration 7/1000 | Loss: 0.00001555
Iteration 8/1000 | Loss: 0.00001537
Iteration 9/1000 | Loss: 0.00001517
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001475
Iteration 13/1000 | Loss: 0.00001469
Iteration 14/1000 | Loss: 0.00001467
Iteration 15/1000 | Loss: 0.00001460
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001442
Iteration 35/1000 | Loss: 0.00001441
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001423
Iteration 77/1000 | Loss: 0.00001423
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001420
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001419
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001417
Iteration 98/1000 | Loss: 0.00001417
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001416
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001416
Iteration 103/1000 | Loss: 0.00001416
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001414
Iteration 110/1000 | Loss: 0.00001414
Iteration 111/1000 | Loss: 0.00001414
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001412
Iteration 116/1000 | Loss: 0.00001412
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001408
Iteration 130/1000 | Loss: 0.00001407
Iteration 131/1000 | Loss: 0.00001407
Iteration 132/1000 | Loss: 0.00001407
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001406
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001405
Iteration 143/1000 | Loss: 0.00001405
Iteration 144/1000 | Loss: 0.00001405
Iteration 145/1000 | Loss: 0.00001405
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001404
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001404
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001404
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001402
Iteration 170/1000 | Loss: 0.00001402
Iteration 171/1000 | Loss: 0.00001401
Iteration 172/1000 | Loss: 0.00001401
Iteration 173/1000 | Loss: 0.00001401
Iteration 174/1000 | Loss: 0.00001401
Iteration 175/1000 | Loss: 0.00001401
Iteration 176/1000 | Loss: 0.00001401
Iteration 177/1000 | Loss: 0.00001401
Iteration 178/1000 | Loss: 0.00001401
Iteration 179/1000 | Loss: 0.00001401
Iteration 180/1000 | Loss: 0.00001401
Iteration 181/1000 | Loss: 0.00001401
Iteration 182/1000 | Loss: 0.00001401
Iteration 183/1000 | Loss: 0.00001401
Iteration 184/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4009867300046608e-05, 1.4009867300046608e-05, 1.4009867300046608e-05, 1.4009867300046608e-05, 1.4009867300046608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4009867300046608e-05

Optimization complete. Final v2v error: 3.198946237564087 mm

Highest mean error: 3.923123598098755 mm for frame 227

Lowest mean error: 2.904334783554077 mm for frame 191

Saving results

Total time: 45.162089824676514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091303
Iteration 2/25 | Loss: 0.00170899
Iteration 3/25 | Loss: 0.00134284
Iteration 4/25 | Loss: 0.00131330
Iteration 5/25 | Loss: 0.00130624
Iteration 6/25 | Loss: 0.00130536
Iteration 7/25 | Loss: 0.00130536
Iteration 8/25 | Loss: 0.00130536
Iteration 9/25 | Loss: 0.00130536
Iteration 10/25 | Loss: 0.00130536
Iteration 11/25 | Loss: 0.00130536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013053551083430648, 0.0013053551083430648, 0.0013053551083430648, 0.0013053551083430648, 0.0013053551083430648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013053551083430648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15872371
Iteration 2/25 | Loss: 0.00101671
Iteration 3/25 | Loss: 0.00101669
Iteration 4/25 | Loss: 0.00101669
Iteration 5/25 | Loss: 0.00101669
Iteration 6/25 | Loss: 0.00101669
Iteration 7/25 | Loss: 0.00101668
Iteration 8/25 | Loss: 0.00101668
Iteration 9/25 | Loss: 0.00101668
Iteration 10/25 | Loss: 0.00101668
Iteration 11/25 | Loss: 0.00101668
Iteration 12/25 | Loss: 0.00101668
Iteration 13/25 | Loss: 0.00101668
Iteration 14/25 | Loss: 0.00101668
Iteration 15/25 | Loss: 0.00101668
Iteration 16/25 | Loss: 0.00101668
Iteration 17/25 | Loss: 0.00101668
Iteration 18/25 | Loss: 0.00101668
Iteration 19/25 | Loss: 0.00101668
Iteration 20/25 | Loss: 0.00101668
Iteration 21/25 | Loss: 0.00101668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010166831780225039, 0.0010166831780225039, 0.0010166831780225039, 0.0010166831780225039, 0.0010166831780225039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010166831780225039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101668
Iteration 2/1000 | Loss: 0.00007704
Iteration 3/1000 | Loss: 0.00004735
Iteration 4/1000 | Loss: 0.00003370
Iteration 5/1000 | Loss: 0.00003134
Iteration 6/1000 | Loss: 0.00002964
Iteration 7/1000 | Loss: 0.00002879
Iteration 8/1000 | Loss: 0.00002814
Iteration 9/1000 | Loss: 0.00002769
Iteration 10/1000 | Loss: 0.00002732
Iteration 11/1000 | Loss: 0.00002696
Iteration 12/1000 | Loss: 0.00002672
Iteration 13/1000 | Loss: 0.00002658
Iteration 14/1000 | Loss: 0.00002652
Iteration 15/1000 | Loss: 0.00002631
Iteration 16/1000 | Loss: 0.00002615
Iteration 17/1000 | Loss: 0.00002607
Iteration 18/1000 | Loss: 0.00002600
Iteration 19/1000 | Loss: 0.00002592
Iteration 20/1000 | Loss: 0.00002579
Iteration 21/1000 | Loss: 0.00002577
Iteration 22/1000 | Loss: 0.00002573
Iteration 23/1000 | Loss: 0.00002573
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002571
Iteration 27/1000 | Loss: 0.00002568
Iteration 28/1000 | Loss: 0.00002568
Iteration 29/1000 | Loss: 0.00002567
Iteration 30/1000 | Loss: 0.00002566
Iteration 31/1000 | Loss: 0.00002566
Iteration 32/1000 | Loss: 0.00002566
Iteration 33/1000 | Loss: 0.00002565
Iteration 34/1000 | Loss: 0.00002565
Iteration 35/1000 | Loss: 0.00002564
Iteration 36/1000 | Loss: 0.00002563
Iteration 37/1000 | Loss: 0.00002562
Iteration 38/1000 | Loss: 0.00002562
Iteration 39/1000 | Loss: 0.00002561
Iteration 40/1000 | Loss: 0.00002561
Iteration 41/1000 | Loss: 0.00002561
Iteration 42/1000 | Loss: 0.00002561
Iteration 43/1000 | Loss: 0.00002561
Iteration 44/1000 | Loss: 0.00002561
Iteration 45/1000 | Loss: 0.00002561
Iteration 46/1000 | Loss: 0.00002561
Iteration 47/1000 | Loss: 0.00002561
Iteration 48/1000 | Loss: 0.00002560
Iteration 49/1000 | Loss: 0.00002558
Iteration 50/1000 | Loss: 0.00002557
Iteration 51/1000 | Loss: 0.00002555
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002555
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002553
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002552
Iteration 58/1000 | Loss: 0.00002551
Iteration 59/1000 | Loss: 0.00002551
Iteration 60/1000 | Loss: 0.00002551
Iteration 61/1000 | Loss: 0.00002550
Iteration 62/1000 | Loss: 0.00002550
Iteration 63/1000 | Loss: 0.00002550
Iteration 64/1000 | Loss: 0.00002550
Iteration 65/1000 | Loss: 0.00002549
Iteration 66/1000 | Loss: 0.00002549
Iteration 67/1000 | Loss: 0.00002549
Iteration 68/1000 | Loss: 0.00002549
Iteration 69/1000 | Loss: 0.00002549
Iteration 70/1000 | Loss: 0.00002548
Iteration 71/1000 | Loss: 0.00002548
Iteration 72/1000 | Loss: 0.00002547
Iteration 73/1000 | Loss: 0.00002546
Iteration 74/1000 | Loss: 0.00002546
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002545
Iteration 77/1000 | Loss: 0.00002544
Iteration 78/1000 | Loss: 0.00002544
Iteration 79/1000 | Loss: 0.00002544
Iteration 80/1000 | Loss: 0.00002544
Iteration 81/1000 | Loss: 0.00002544
Iteration 82/1000 | Loss: 0.00002544
Iteration 83/1000 | Loss: 0.00002543
Iteration 84/1000 | Loss: 0.00002542
Iteration 85/1000 | Loss: 0.00002542
Iteration 86/1000 | Loss: 0.00002541
Iteration 87/1000 | Loss: 0.00002541
Iteration 88/1000 | Loss: 0.00002541
Iteration 89/1000 | Loss: 0.00002541
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002541
Iteration 92/1000 | Loss: 0.00002541
Iteration 93/1000 | Loss: 0.00002541
Iteration 94/1000 | Loss: 0.00002541
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002540
Iteration 97/1000 | Loss: 0.00002540
Iteration 98/1000 | Loss: 0.00002539
Iteration 99/1000 | Loss: 0.00002539
Iteration 100/1000 | Loss: 0.00002539
Iteration 101/1000 | Loss: 0.00002539
Iteration 102/1000 | Loss: 0.00002539
Iteration 103/1000 | Loss: 0.00002539
Iteration 104/1000 | Loss: 0.00002538
Iteration 105/1000 | Loss: 0.00002538
Iteration 106/1000 | Loss: 0.00002538
Iteration 107/1000 | Loss: 0.00002538
Iteration 108/1000 | Loss: 0.00002538
Iteration 109/1000 | Loss: 0.00002538
Iteration 110/1000 | Loss: 0.00002538
Iteration 111/1000 | Loss: 0.00002538
Iteration 112/1000 | Loss: 0.00002538
Iteration 113/1000 | Loss: 0.00002538
Iteration 114/1000 | Loss: 0.00002538
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00002538
Iteration 117/1000 | Loss: 0.00002537
Iteration 118/1000 | Loss: 0.00002537
Iteration 119/1000 | Loss: 0.00002537
Iteration 120/1000 | Loss: 0.00002537
Iteration 121/1000 | Loss: 0.00002536
Iteration 122/1000 | Loss: 0.00002536
Iteration 123/1000 | Loss: 0.00002536
Iteration 124/1000 | Loss: 0.00002536
Iteration 125/1000 | Loss: 0.00002536
Iteration 126/1000 | Loss: 0.00002536
Iteration 127/1000 | Loss: 0.00002536
Iteration 128/1000 | Loss: 0.00002536
Iteration 129/1000 | Loss: 0.00002536
Iteration 130/1000 | Loss: 0.00002536
Iteration 131/1000 | Loss: 0.00002536
Iteration 132/1000 | Loss: 0.00002536
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002535
Iteration 135/1000 | Loss: 0.00002535
Iteration 136/1000 | Loss: 0.00002535
Iteration 137/1000 | Loss: 0.00002535
Iteration 138/1000 | Loss: 0.00002535
Iteration 139/1000 | Loss: 0.00002535
Iteration 140/1000 | Loss: 0.00002535
Iteration 141/1000 | Loss: 0.00002535
Iteration 142/1000 | Loss: 0.00002535
Iteration 143/1000 | Loss: 0.00002535
Iteration 144/1000 | Loss: 0.00002535
Iteration 145/1000 | Loss: 0.00002535
Iteration 146/1000 | Loss: 0.00002534
Iteration 147/1000 | Loss: 0.00002534
Iteration 148/1000 | Loss: 0.00002534
Iteration 149/1000 | Loss: 0.00002534
Iteration 150/1000 | Loss: 0.00002534
Iteration 151/1000 | Loss: 0.00002533
Iteration 152/1000 | Loss: 0.00002533
Iteration 153/1000 | Loss: 0.00002533
Iteration 154/1000 | Loss: 0.00002533
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00002532
Iteration 157/1000 | Loss: 0.00002532
Iteration 158/1000 | Loss: 0.00002532
Iteration 159/1000 | Loss: 0.00002532
Iteration 160/1000 | Loss: 0.00002532
Iteration 161/1000 | Loss: 0.00002532
Iteration 162/1000 | Loss: 0.00002532
Iteration 163/1000 | Loss: 0.00002532
Iteration 164/1000 | Loss: 0.00002532
Iteration 165/1000 | Loss: 0.00002532
Iteration 166/1000 | Loss: 0.00002531
Iteration 167/1000 | Loss: 0.00002531
Iteration 168/1000 | Loss: 0.00002531
Iteration 169/1000 | Loss: 0.00002531
Iteration 170/1000 | Loss: 0.00002531
Iteration 171/1000 | Loss: 0.00002531
Iteration 172/1000 | Loss: 0.00002531
Iteration 173/1000 | Loss: 0.00002531
Iteration 174/1000 | Loss: 0.00002531
Iteration 175/1000 | Loss: 0.00002531
Iteration 176/1000 | Loss: 0.00002531
Iteration 177/1000 | Loss: 0.00002531
Iteration 178/1000 | Loss: 0.00002531
Iteration 179/1000 | Loss: 0.00002531
Iteration 180/1000 | Loss: 0.00002531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.5308941985713318e-05, 2.5308941985713318e-05, 2.5308941985713318e-05, 2.5308941985713318e-05, 2.5308941985713318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5308941985713318e-05

Optimization complete. Final v2v error: 4.105676651000977 mm

Highest mean error: 5.437535762786865 mm for frame 149

Lowest mean error: 3.3236067295074463 mm for frame 55

Saving results

Total time: 51.94955849647522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983533
Iteration 2/25 | Loss: 0.00188344
Iteration 3/25 | Loss: 0.00144032
Iteration 4/25 | Loss: 0.00135513
Iteration 5/25 | Loss: 0.00130729
Iteration 6/25 | Loss: 0.00131053
Iteration 7/25 | Loss: 0.00127056
Iteration 8/25 | Loss: 0.00125688
Iteration 9/25 | Loss: 0.00125463
Iteration 10/25 | Loss: 0.00125269
Iteration 11/25 | Loss: 0.00125309
Iteration 12/25 | Loss: 0.00125121
Iteration 13/25 | Loss: 0.00125058
Iteration 14/25 | Loss: 0.00124436
Iteration 15/25 | Loss: 0.00124084
Iteration 16/25 | Loss: 0.00123358
Iteration 17/25 | Loss: 0.00123143
Iteration 18/25 | Loss: 0.00122812
Iteration 19/25 | Loss: 0.00122775
Iteration 20/25 | Loss: 0.00122750
Iteration 21/25 | Loss: 0.00123051
Iteration 22/25 | Loss: 0.00122824
Iteration 23/25 | Loss: 0.00122668
Iteration 24/25 | Loss: 0.00122611
Iteration 25/25 | Loss: 0.00122587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28130984
Iteration 2/25 | Loss: 0.00118756
Iteration 3/25 | Loss: 0.00120977
Iteration 4/25 | Loss: 0.00114363
Iteration 5/25 | Loss: 0.00114363
Iteration 6/25 | Loss: 0.00114363
Iteration 7/25 | Loss: 0.00114363
Iteration 8/25 | Loss: 0.00114363
Iteration 9/25 | Loss: 0.00114363
Iteration 10/25 | Loss: 0.00114363
Iteration 11/25 | Loss: 0.00114363
Iteration 12/25 | Loss: 0.00114363
Iteration 13/25 | Loss: 0.00114363
Iteration 14/25 | Loss: 0.00114363
Iteration 15/25 | Loss: 0.00114363
Iteration 16/25 | Loss: 0.00114363
Iteration 17/25 | Loss: 0.00114363
Iteration 18/25 | Loss: 0.00114363
Iteration 19/25 | Loss: 0.00114363
Iteration 20/25 | Loss: 0.00114363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011436260538175702, 0.0011436260538175702, 0.0011436260538175702, 0.0011436260538175702, 0.0011436260538175702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011436260538175702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114363
Iteration 2/1000 | Loss: 0.00030460
Iteration 3/1000 | Loss: 0.00073309
Iteration 4/1000 | Loss: 0.00032255
Iteration 5/1000 | Loss: 0.00020314
Iteration 6/1000 | Loss: 0.00004408
Iteration 7/1000 | Loss: 0.00007323
Iteration 8/1000 | Loss: 0.00019800
Iteration 9/1000 | Loss: 0.00002829
Iteration 10/1000 | Loss: 0.00002590
Iteration 11/1000 | Loss: 0.00012877
Iteration 12/1000 | Loss: 0.00006147
Iteration 13/1000 | Loss: 0.00022292
Iteration 14/1000 | Loss: 0.00074632
Iteration 15/1000 | Loss: 0.00002588
Iteration 16/1000 | Loss: 0.00002229
Iteration 17/1000 | Loss: 0.00002087
Iteration 18/1000 | Loss: 0.00012737
Iteration 19/1000 | Loss: 0.00002505
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00018544
Iteration 22/1000 | Loss: 0.00007165
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00001827
Iteration 30/1000 | Loss: 0.00001824
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001804
Iteration 33/1000 | Loss: 0.00001798
Iteration 34/1000 | Loss: 0.00014209
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001779
Iteration 48/1000 | Loss: 0.00001779
Iteration 49/1000 | Loss: 0.00001779
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00001778
Iteration 56/1000 | Loss: 0.00001778
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001778
Iteration 59/1000 | Loss: 0.00001777
Iteration 60/1000 | Loss: 0.00001777
Iteration 61/1000 | Loss: 0.00001777
Iteration 62/1000 | Loss: 0.00001777
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001776
Iteration 67/1000 | Loss: 0.00001776
Iteration 68/1000 | Loss: 0.00001775
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001769
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001766
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00017419
Iteration 86/1000 | Loss: 0.00012102
Iteration 87/1000 | Loss: 0.00018986
Iteration 88/1000 | Loss: 0.00008688
Iteration 89/1000 | Loss: 0.00002058
Iteration 90/1000 | Loss: 0.00001818
Iteration 91/1000 | Loss: 0.00012969
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00002313
Iteration 94/1000 | Loss: 0.00001653
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00006196
Iteration 97/1000 | Loss: 0.00033668
Iteration 98/1000 | Loss: 0.00029829
Iteration 99/1000 | Loss: 0.00013764
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001669
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00006644
Iteration 106/1000 | Loss: 0.00001773
Iteration 107/1000 | Loss: 0.00002425
Iteration 108/1000 | Loss: 0.00001850
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001609
Iteration 129/1000 | Loss: 0.00001609
Iteration 130/1000 | Loss: 0.00001608
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001606
Iteration 133/1000 | Loss: 0.00001606
Iteration 134/1000 | Loss: 0.00001606
Iteration 135/1000 | Loss: 0.00001606
Iteration 136/1000 | Loss: 0.00001606
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001606
Iteration 139/1000 | Loss: 0.00001605
Iteration 140/1000 | Loss: 0.00001605
Iteration 141/1000 | Loss: 0.00001605
Iteration 142/1000 | Loss: 0.00001604
Iteration 143/1000 | Loss: 0.00001604
Iteration 144/1000 | Loss: 0.00001604
Iteration 145/1000 | Loss: 0.00001604
Iteration 146/1000 | Loss: 0.00001604
Iteration 147/1000 | Loss: 0.00001603
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001603
Iteration 151/1000 | Loss: 0.00001603
Iteration 152/1000 | Loss: 0.00001603
Iteration 153/1000 | Loss: 0.00001603
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001602
Iteration 156/1000 | Loss: 0.00001602
Iteration 157/1000 | Loss: 0.00001602
Iteration 158/1000 | Loss: 0.00004592
Iteration 159/1000 | Loss: 0.00001613
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001603
Iteration 162/1000 | Loss: 0.00001603
Iteration 163/1000 | Loss: 0.00001603
Iteration 164/1000 | Loss: 0.00001603
Iteration 165/1000 | Loss: 0.00001603
Iteration 166/1000 | Loss: 0.00001603
Iteration 167/1000 | Loss: 0.00001603
Iteration 168/1000 | Loss: 0.00001603
Iteration 169/1000 | Loss: 0.00001603
Iteration 170/1000 | Loss: 0.00001602
Iteration 171/1000 | Loss: 0.00001602
Iteration 172/1000 | Loss: 0.00001602
Iteration 173/1000 | Loss: 0.00001602
Iteration 174/1000 | Loss: 0.00001602
Iteration 175/1000 | Loss: 0.00001602
Iteration 176/1000 | Loss: 0.00001602
Iteration 177/1000 | Loss: 0.00001602
Iteration 178/1000 | Loss: 0.00001602
Iteration 179/1000 | Loss: 0.00001602
Iteration 180/1000 | Loss: 0.00001602
Iteration 181/1000 | Loss: 0.00001602
Iteration 182/1000 | Loss: 0.00001601
Iteration 183/1000 | Loss: 0.00001601
Iteration 184/1000 | Loss: 0.00001601
Iteration 185/1000 | Loss: 0.00001601
Iteration 186/1000 | Loss: 0.00001600
Iteration 187/1000 | Loss: 0.00001600
Iteration 188/1000 | Loss: 0.00001599
Iteration 189/1000 | Loss: 0.00001599
Iteration 190/1000 | Loss: 0.00001599
Iteration 191/1000 | Loss: 0.00001599
Iteration 192/1000 | Loss: 0.00001598
Iteration 193/1000 | Loss: 0.00001598
Iteration 194/1000 | Loss: 0.00001598
Iteration 195/1000 | Loss: 0.00001597
Iteration 196/1000 | Loss: 0.00001597
Iteration 197/1000 | Loss: 0.00001597
Iteration 198/1000 | Loss: 0.00001597
Iteration 199/1000 | Loss: 0.00001597
Iteration 200/1000 | Loss: 0.00001597
Iteration 201/1000 | Loss: 0.00001597
Iteration 202/1000 | Loss: 0.00001597
Iteration 203/1000 | Loss: 0.00001597
Iteration 204/1000 | Loss: 0.00001597
Iteration 205/1000 | Loss: 0.00001597
Iteration 206/1000 | Loss: 0.00001597
Iteration 207/1000 | Loss: 0.00001597
Iteration 208/1000 | Loss: 0.00001597
Iteration 209/1000 | Loss: 0.00001597
Iteration 210/1000 | Loss: 0.00001597
Iteration 211/1000 | Loss: 0.00001597
Iteration 212/1000 | Loss: 0.00001597
Iteration 213/1000 | Loss: 0.00001597
Iteration 214/1000 | Loss: 0.00001597
Iteration 215/1000 | Loss: 0.00001597
Iteration 216/1000 | Loss: 0.00001597
Iteration 217/1000 | Loss: 0.00001597
Iteration 218/1000 | Loss: 0.00001597
Iteration 219/1000 | Loss: 0.00001597
Iteration 220/1000 | Loss: 0.00001597
Iteration 221/1000 | Loss: 0.00001597
Iteration 222/1000 | Loss: 0.00001597
Iteration 223/1000 | Loss: 0.00001597
Iteration 224/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.596953188709449e-05, 1.596953188709449e-05, 1.596953188709449e-05, 1.596953188709449e-05, 1.596953188709449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.596953188709449e-05

Optimization complete. Final v2v error: 3.350158452987671 mm

Highest mean error: 3.978015899658203 mm for frame 135

Lowest mean error: 2.8685193061828613 mm for frame 18

Saving results

Total time: 152.14390325546265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973798
Iteration 2/25 | Loss: 0.00148697
Iteration 3/25 | Loss: 0.00126758
Iteration 4/25 | Loss: 0.00124239
Iteration 5/25 | Loss: 0.00124083
Iteration 6/25 | Loss: 0.00122736
Iteration 7/25 | Loss: 0.00122306
Iteration 8/25 | Loss: 0.00121887
Iteration 9/25 | Loss: 0.00121803
Iteration 10/25 | Loss: 0.00121942
Iteration 11/25 | Loss: 0.00121603
Iteration 12/25 | Loss: 0.00121538
Iteration 13/25 | Loss: 0.00121520
Iteration 14/25 | Loss: 0.00121519
Iteration 15/25 | Loss: 0.00121519
Iteration 16/25 | Loss: 0.00121519
Iteration 17/25 | Loss: 0.00121519
Iteration 18/25 | Loss: 0.00121519
Iteration 19/25 | Loss: 0.00121519
Iteration 20/25 | Loss: 0.00121519
Iteration 21/25 | Loss: 0.00121519
Iteration 22/25 | Loss: 0.00121519
Iteration 23/25 | Loss: 0.00121519
Iteration 24/25 | Loss: 0.00121519
Iteration 25/25 | Loss: 0.00121519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.56094742
Iteration 2/25 | Loss: 0.00125096
Iteration 3/25 | Loss: 0.00125090
Iteration 4/25 | Loss: 0.00125090
Iteration 5/25 | Loss: 0.00125090
Iteration 6/25 | Loss: 0.00125090
Iteration 7/25 | Loss: 0.00125090
Iteration 8/25 | Loss: 0.00125090
Iteration 9/25 | Loss: 0.00125090
Iteration 10/25 | Loss: 0.00125090
Iteration 11/25 | Loss: 0.00125090
Iteration 12/25 | Loss: 0.00125090
Iteration 13/25 | Loss: 0.00125090
Iteration 14/25 | Loss: 0.00125090
Iteration 15/25 | Loss: 0.00125090
Iteration 16/25 | Loss: 0.00125090
Iteration 17/25 | Loss: 0.00125090
Iteration 18/25 | Loss: 0.00125090
Iteration 19/25 | Loss: 0.00125090
Iteration 20/25 | Loss: 0.00125090
Iteration 21/25 | Loss: 0.00125090
Iteration 22/25 | Loss: 0.00125090
Iteration 23/25 | Loss: 0.00125090
Iteration 24/25 | Loss: 0.00125090
Iteration 25/25 | Loss: 0.00125090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125090
Iteration 2/1000 | Loss: 0.00010908
Iteration 3/1000 | Loss: 0.00004982
Iteration 4/1000 | Loss: 0.00003260
Iteration 5/1000 | Loss: 0.00002717
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00005648
Iteration 8/1000 | Loss: 0.00004884
Iteration 9/1000 | Loss: 0.00004907
Iteration 10/1000 | Loss: 0.00050317
Iteration 11/1000 | Loss: 0.00008542
Iteration 12/1000 | Loss: 0.00027748
Iteration 13/1000 | Loss: 0.00026773
Iteration 14/1000 | Loss: 0.00003771
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00003930
Iteration 17/1000 | Loss: 0.00004844
Iteration 18/1000 | Loss: 0.00002710
Iteration 19/1000 | Loss: 0.00003628
Iteration 20/1000 | Loss: 0.00004692
Iteration 21/1000 | Loss: 0.00003250
Iteration 22/1000 | Loss: 0.00001873
Iteration 23/1000 | Loss: 0.00003498
Iteration 24/1000 | Loss: 0.00003594
Iteration 25/1000 | Loss: 0.00003156
Iteration 26/1000 | Loss: 0.00003440
Iteration 27/1000 | Loss: 0.00003504
Iteration 28/1000 | Loss: 0.00003821
Iteration 29/1000 | Loss: 0.00004229
Iteration 30/1000 | Loss: 0.00005263
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00002010
Iteration 33/1000 | Loss: 0.00004496
Iteration 34/1000 | Loss: 0.00003251
Iteration 35/1000 | Loss: 0.00004016
Iteration 36/1000 | Loss: 0.00003750
Iteration 37/1000 | Loss: 0.00003754
Iteration 38/1000 | Loss: 0.00003659
Iteration 39/1000 | Loss: 0.00003693
Iteration 40/1000 | Loss: 0.00003092
Iteration 41/1000 | Loss: 0.00003125
Iteration 42/1000 | Loss: 0.00003393
Iteration 43/1000 | Loss: 0.00004204
Iteration 44/1000 | Loss: 0.00003865
Iteration 45/1000 | Loss: 0.00003262
Iteration 46/1000 | Loss: 0.00003587
Iteration 47/1000 | Loss: 0.00003985
Iteration 48/1000 | Loss: 0.00003203
Iteration 49/1000 | Loss: 0.00003235
Iteration 50/1000 | Loss: 0.00003907
Iteration 51/1000 | Loss: 0.00004251
Iteration 52/1000 | Loss: 0.00003810
Iteration 53/1000 | Loss: 0.00002989
Iteration 54/1000 | Loss: 0.00004478
Iteration 55/1000 | Loss: 0.00002860
Iteration 56/1000 | Loss: 0.00004442
Iteration 57/1000 | Loss: 0.00003962
Iteration 58/1000 | Loss: 0.00002999
Iteration 59/1000 | Loss: 0.00003874
Iteration 60/1000 | Loss: 0.00003861
Iteration 61/1000 | Loss: 0.00002957
Iteration 62/1000 | Loss: 0.00003899
Iteration 63/1000 | Loss: 0.00003611
Iteration 64/1000 | Loss: 0.00003796
Iteration 65/1000 | Loss: 0.00003826
Iteration 66/1000 | Loss: 0.00003457
Iteration 67/1000 | Loss: 0.00004500
Iteration 68/1000 | Loss: 0.00003236
Iteration 69/1000 | Loss: 0.00002088
Iteration 70/1000 | Loss: 0.00003825
Iteration 71/1000 | Loss: 0.00003403
Iteration 72/1000 | Loss: 0.00002336
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001516
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001498
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001498
Iteration 84/1000 | Loss: 0.00001498
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001497
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001497
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001495
Iteration 99/1000 | Loss: 0.00001495
Iteration 100/1000 | Loss: 0.00001495
Iteration 101/1000 | Loss: 0.00001494
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001490
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001488
Iteration 121/1000 | Loss: 0.00001488
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001488
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001487
Iteration 129/1000 | Loss: 0.00001487
Iteration 130/1000 | Loss: 0.00001487
Iteration 131/1000 | Loss: 0.00001487
Iteration 132/1000 | Loss: 0.00001487
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001487
Iteration 136/1000 | Loss: 0.00001487
Iteration 137/1000 | Loss: 0.00001486
Iteration 138/1000 | Loss: 0.00001486
Iteration 139/1000 | Loss: 0.00001486
Iteration 140/1000 | Loss: 0.00001486
Iteration 141/1000 | Loss: 0.00001486
Iteration 142/1000 | Loss: 0.00001486
Iteration 143/1000 | Loss: 0.00001486
Iteration 144/1000 | Loss: 0.00001486
Iteration 145/1000 | Loss: 0.00001486
Iteration 146/1000 | Loss: 0.00001485
Iteration 147/1000 | Loss: 0.00001485
Iteration 148/1000 | Loss: 0.00001485
Iteration 149/1000 | Loss: 0.00001485
Iteration 150/1000 | Loss: 0.00001485
Iteration 151/1000 | Loss: 0.00001485
Iteration 152/1000 | Loss: 0.00001485
Iteration 153/1000 | Loss: 0.00001485
Iteration 154/1000 | Loss: 0.00001485
Iteration 155/1000 | Loss: 0.00001485
Iteration 156/1000 | Loss: 0.00001485
Iteration 157/1000 | Loss: 0.00001485
Iteration 158/1000 | Loss: 0.00001484
Iteration 159/1000 | Loss: 0.00001484
Iteration 160/1000 | Loss: 0.00001484
Iteration 161/1000 | Loss: 0.00001484
Iteration 162/1000 | Loss: 0.00001484
Iteration 163/1000 | Loss: 0.00001484
Iteration 164/1000 | Loss: 0.00001484
Iteration 165/1000 | Loss: 0.00001484
Iteration 166/1000 | Loss: 0.00001484
Iteration 167/1000 | Loss: 0.00001484
Iteration 168/1000 | Loss: 0.00001484
Iteration 169/1000 | Loss: 0.00001484
Iteration 170/1000 | Loss: 0.00001484
Iteration 171/1000 | Loss: 0.00001484
Iteration 172/1000 | Loss: 0.00001484
Iteration 173/1000 | Loss: 0.00001483
Iteration 174/1000 | Loss: 0.00001483
Iteration 175/1000 | Loss: 0.00001483
Iteration 176/1000 | Loss: 0.00001483
Iteration 177/1000 | Loss: 0.00001483
Iteration 178/1000 | Loss: 0.00001483
Iteration 179/1000 | Loss: 0.00001483
Iteration 180/1000 | Loss: 0.00001483
Iteration 181/1000 | Loss: 0.00001483
Iteration 182/1000 | Loss: 0.00001483
Iteration 183/1000 | Loss: 0.00001483
Iteration 184/1000 | Loss: 0.00001483
Iteration 185/1000 | Loss: 0.00001483
Iteration 186/1000 | Loss: 0.00001483
Iteration 187/1000 | Loss: 0.00001483
Iteration 188/1000 | Loss: 0.00001483
Iteration 189/1000 | Loss: 0.00001483
Iteration 190/1000 | Loss: 0.00001483
Iteration 191/1000 | Loss: 0.00001483
Iteration 192/1000 | Loss: 0.00001483
Iteration 193/1000 | Loss: 0.00001483
Iteration 194/1000 | Loss: 0.00001483
Iteration 195/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.4834899047855288e-05, 1.4834899047855288e-05, 1.4834899047855288e-05, 1.4834899047855288e-05, 1.4834899047855288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4834899047855288e-05

Optimization complete. Final v2v error: 3.1988437175750732 mm

Highest mean error: 5.225546360015869 mm for frame 34

Lowest mean error: 2.608208179473877 mm for frame 71

Saving results

Total time: 140.20808911323547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564989
Iteration 2/25 | Loss: 0.00119031
Iteration 3/25 | Loss: 0.00112984
Iteration 4/25 | Loss: 0.00112095
Iteration 5/25 | Loss: 0.00111752
Iteration 6/25 | Loss: 0.00111718
Iteration 7/25 | Loss: 0.00111718
Iteration 8/25 | Loss: 0.00111718
Iteration 9/25 | Loss: 0.00111718
Iteration 10/25 | Loss: 0.00111718
Iteration 11/25 | Loss: 0.00111718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011171818478032947, 0.0011171818478032947, 0.0011171818478032947, 0.0011171818478032947, 0.0011171818478032947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011171818478032947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71090078
Iteration 2/25 | Loss: 0.00086205
Iteration 3/25 | Loss: 0.00086205
Iteration 4/25 | Loss: 0.00086205
Iteration 5/25 | Loss: 0.00086205
Iteration 6/25 | Loss: 0.00086205
Iteration 7/25 | Loss: 0.00086205
Iteration 8/25 | Loss: 0.00086205
Iteration 9/25 | Loss: 0.00086205
Iteration 10/25 | Loss: 0.00086205
Iteration 11/25 | Loss: 0.00086205
Iteration 12/25 | Loss: 0.00086205
Iteration 13/25 | Loss: 0.00086205
Iteration 14/25 | Loss: 0.00086205
Iteration 15/25 | Loss: 0.00086205
Iteration 16/25 | Loss: 0.00086205
Iteration 17/25 | Loss: 0.00086205
Iteration 18/25 | Loss: 0.00086205
Iteration 19/25 | Loss: 0.00086205
Iteration 20/25 | Loss: 0.00086205
Iteration 21/25 | Loss: 0.00086205
Iteration 22/25 | Loss: 0.00086205
Iteration 23/25 | Loss: 0.00086205
Iteration 24/25 | Loss: 0.00086205
Iteration 25/25 | Loss: 0.00086205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086205
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001470
Iteration 4/1000 | Loss: 0.00001284
Iteration 5/1000 | Loss: 0.00001203
Iteration 6/1000 | Loss: 0.00001158
Iteration 7/1000 | Loss: 0.00001119
Iteration 8/1000 | Loss: 0.00001101
Iteration 9/1000 | Loss: 0.00001082
Iteration 10/1000 | Loss: 0.00001058
Iteration 11/1000 | Loss: 0.00001055
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001048
Iteration 14/1000 | Loss: 0.00001048
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001034
Iteration 18/1000 | Loss: 0.00001033
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001019
Iteration 24/1000 | Loss: 0.00001018
Iteration 25/1000 | Loss: 0.00001018
Iteration 26/1000 | Loss: 0.00001017
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001015
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00001014
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001012
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001007
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000997
Iteration 83/1000 | Loss: 0.00000997
Iteration 84/1000 | Loss: 0.00000997
Iteration 85/1000 | Loss: 0.00000996
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000995
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000993
Iteration 101/1000 | Loss: 0.00000993
Iteration 102/1000 | Loss: 0.00000993
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000992
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000990
Iteration 112/1000 | Loss: 0.00000990
Iteration 113/1000 | Loss: 0.00000990
Iteration 114/1000 | Loss: 0.00000989
Iteration 115/1000 | Loss: 0.00000989
Iteration 116/1000 | Loss: 0.00000989
Iteration 117/1000 | Loss: 0.00000989
Iteration 118/1000 | Loss: 0.00000989
Iteration 119/1000 | Loss: 0.00000989
Iteration 120/1000 | Loss: 0.00000989
Iteration 121/1000 | Loss: 0.00000989
Iteration 122/1000 | Loss: 0.00000988
Iteration 123/1000 | Loss: 0.00000988
Iteration 124/1000 | Loss: 0.00000988
Iteration 125/1000 | Loss: 0.00000988
Iteration 126/1000 | Loss: 0.00000988
Iteration 127/1000 | Loss: 0.00000988
Iteration 128/1000 | Loss: 0.00000988
Iteration 129/1000 | Loss: 0.00000988
Iteration 130/1000 | Loss: 0.00000987
Iteration 131/1000 | Loss: 0.00000987
Iteration 132/1000 | Loss: 0.00000987
Iteration 133/1000 | Loss: 0.00000987
Iteration 134/1000 | Loss: 0.00000987
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000986
Iteration 137/1000 | Loss: 0.00000986
Iteration 138/1000 | Loss: 0.00000986
Iteration 139/1000 | Loss: 0.00000986
Iteration 140/1000 | Loss: 0.00000986
Iteration 141/1000 | Loss: 0.00000986
Iteration 142/1000 | Loss: 0.00000986
Iteration 143/1000 | Loss: 0.00000986
Iteration 144/1000 | Loss: 0.00000986
Iteration 145/1000 | Loss: 0.00000986
Iteration 146/1000 | Loss: 0.00000986
Iteration 147/1000 | Loss: 0.00000986
Iteration 148/1000 | Loss: 0.00000986
Iteration 149/1000 | Loss: 0.00000986
Iteration 150/1000 | Loss: 0.00000985
Iteration 151/1000 | Loss: 0.00000985
Iteration 152/1000 | Loss: 0.00000985
Iteration 153/1000 | Loss: 0.00000985
Iteration 154/1000 | Loss: 0.00000985
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000984
Iteration 161/1000 | Loss: 0.00000984
Iteration 162/1000 | Loss: 0.00000984
Iteration 163/1000 | Loss: 0.00000984
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000984
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000984
Iteration 169/1000 | Loss: 0.00000984
Iteration 170/1000 | Loss: 0.00000984
Iteration 171/1000 | Loss: 0.00000984
Iteration 172/1000 | Loss: 0.00000984
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000983
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000983
Iteration 182/1000 | Loss: 0.00000983
Iteration 183/1000 | Loss: 0.00000983
Iteration 184/1000 | Loss: 0.00000983
Iteration 185/1000 | Loss: 0.00000983
Iteration 186/1000 | Loss: 0.00000983
Iteration 187/1000 | Loss: 0.00000982
Iteration 188/1000 | Loss: 0.00000982
Iteration 189/1000 | Loss: 0.00000982
Iteration 190/1000 | Loss: 0.00000982
Iteration 191/1000 | Loss: 0.00000982
Iteration 192/1000 | Loss: 0.00000982
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000982
Iteration 195/1000 | Loss: 0.00000982
Iteration 196/1000 | Loss: 0.00000982
Iteration 197/1000 | Loss: 0.00000982
Iteration 198/1000 | Loss: 0.00000982
Iteration 199/1000 | Loss: 0.00000982
Iteration 200/1000 | Loss: 0.00000982
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000982
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000982
Iteration 210/1000 | Loss: 0.00000982
Iteration 211/1000 | Loss: 0.00000982
Iteration 212/1000 | Loss: 0.00000982
Iteration 213/1000 | Loss: 0.00000982
Iteration 214/1000 | Loss: 0.00000982
Iteration 215/1000 | Loss: 0.00000982
Iteration 216/1000 | Loss: 0.00000982
Iteration 217/1000 | Loss: 0.00000982
Iteration 218/1000 | Loss: 0.00000982
Iteration 219/1000 | Loss: 0.00000982
Iteration 220/1000 | Loss: 0.00000982
Iteration 221/1000 | Loss: 0.00000982
Iteration 222/1000 | Loss: 0.00000982
Iteration 223/1000 | Loss: 0.00000982
Iteration 224/1000 | Loss: 0.00000982
Iteration 225/1000 | Loss: 0.00000982
Iteration 226/1000 | Loss: 0.00000982
Iteration 227/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [9.823595974012278e-06, 9.823595974012278e-06, 9.823595974012278e-06, 9.823595974012278e-06, 9.823595974012278e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.823595974012278e-06

Optimization complete. Final v2v error: 2.691744804382324 mm

Highest mean error: 2.9219748973846436 mm for frame 60

Lowest mean error: 2.5473501682281494 mm for frame 30

Saving results

Total time: 40.20985388755798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867359
Iteration 2/25 | Loss: 0.00166300
Iteration 3/25 | Loss: 0.00135790
Iteration 4/25 | Loss: 0.00132502
Iteration 5/25 | Loss: 0.00132951
Iteration 6/25 | Loss: 0.00129271
Iteration 7/25 | Loss: 0.00125080
Iteration 8/25 | Loss: 0.00123348
Iteration 9/25 | Loss: 0.00122486
Iteration 10/25 | Loss: 0.00121812
Iteration 11/25 | Loss: 0.00121681
Iteration 12/25 | Loss: 0.00121633
Iteration 13/25 | Loss: 0.00121311
Iteration 14/25 | Loss: 0.00120888
Iteration 15/25 | Loss: 0.00120758
Iteration 16/25 | Loss: 0.00120726
Iteration 17/25 | Loss: 0.00120718
Iteration 18/25 | Loss: 0.00120717
Iteration 19/25 | Loss: 0.00120716
Iteration 20/25 | Loss: 0.00120716
Iteration 21/25 | Loss: 0.00120716
Iteration 22/25 | Loss: 0.00120716
Iteration 23/25 | Loss: 0.00120716
Iteration 24/25 | Loss: 0.00120716
Iteration 25/25 | Loss: 0.00120716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92000651
Iteration 2/25 | Loss: 0.00045889
Iteration 3/25 | Loss: 0.00045888
Iteration 4/25 | Loss: 0.00045888
Iteration 5/25 | Loss: 0.00045888
Iteration 6/25 | Loss: 0.00045888
Iteration 7/25 | Loss: 0.00045888
Iteration 8/25 | Loss: 0.00045888
Iteration 9/25 | Loss: 0.00045888
Iteration 10/25 | Loss: 0.00045888
Iteration 11/25 | Loss: 0.00045888
Iteration 12/25 | Loss: 0.00045888
Iteration 13/25 | Loss: 0.00045888
Iteration 14/25 | Loss: 0.00045888
Iteration 15/25 | Loss: 0.00045888
Iteration 16/25 | Loss: 0.00045888
Iteration 17/25 | Loss: 0.00045888
Iteration 18/25 | Loss: 0.00045888
Iteration 19/25 | Loss: 0.00045888
Iteration 20/25 | Loss: 0.00045888
Iteration 21/25 | Loss: 0.00045888
Iteration 22/25 | Loss: 0.00045888
Iteration 23/25 | Loss: 0.00045888
Iteration 24/25 | Loss: 0.00045888
Iteration 25/25 | Loss: 0.00045888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045888
Iteration 2/1000 | Loss: 0.00003912
Iteration 3/1000 | Loss: 0.00003180
Iteration 4/1000 | Loss: 0.00002930
Iteration 5/1000 | Loss: 0.00002805
Iteration 6/1000 | Loss: 0.00002744
Iteration 7/1000 | Loss: 0.00002661
Iteration 8/1000 | Loss: 0.00002620
Iteration 9/1000 | Loss: 0.00002587
Iteration 10/1000 | Loss: 0.00002563
Iteration 11/1000 | Loss: 0.00002551
Iteration 12/1000 | Loss: 0.00002551
Iteration 13/1000 | Loss: 0.00002551
Iteration 14/1000 | Loss: 0.00002551
Iteration 15/1000 | Loss: 0.00002550
Iteration 16/1000 | Loss: 0.00002550
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00002550
Iteration 20/1000 | Loss: 0.00002550
Iteration 21/1000 | Loss: 0.00002550
Iteration 22/1000 | Loss: 0.00002550
Iteration 23/1000 | Loss: 0.00002541
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002533
Iteration 26/1000 | Loss: 0.00002533
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002532
Iteration 29/1000 | Loss: 0.00002532
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002530
Iteration 32/1000 | Loss: 0.00002520
Iteration 33/1000 | Loss: 0.00002520
Iteration 34/1000 | Loss: 0.00002520
Iteration 35/1000 | Loss: 0.00002520
Iteration 36/1000 | Loss: 0.00002520
Iteration 37/1000 | Loss: 0.00002520
Iteration 38/1000 | Loss: 0.00002519
Iteration 39/1000 | Loss: 0.00002519
Iteration 40/1000 | Loss: 0.00002518
Iteration 41/1000 | Loss: 0.00002518
Iteration 42/1000 | Loss: 0.00002516
Iteration 43/1000 | Loss: 0.00002516
Iteration 44/1000 | Loss: 0.00002516
Iteration 45/1000 | Loss: 0.00002516
Iteration 46/1000 | Loss: 0.00002516
Iteration 47/1000 | Loss: 0.00002516
Iteration 48/1000 | Loss: 0.00002516
Iteration 49/1000 | Loss: 0.00002515
Iteration 50/1000 | Loss: 0.00002515
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00002513
Iteration 56/1000 | Loss: 0.00002512
Iteration 57/1000 | Loss: 0.00002512
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00002512
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002511
Iteration 63/1000 | Loss: 0.00002511
Iteration 64/1000 | Loss: 0.00002511
Iteration 65/1000 | Loss: 0.00002511
Iteration 66/1000 | Loss: 0.00002511
Iteration 67/1000 | Loss: 0.00002511
Iteration 68/1000 | Loss: 0.00002511
Iteration 69/1000 | Loss: 0.00002511
Iteration 70/1000 | Loss: 0.00002511
Iteration 71/1000 | Loss: 0.00002511
Iteration 72/1000 | Loss: 0.00002511
Iteration 73/1000 | Loss: 0.00002511
Iteration 74/1000 | Loss: 0.00002511
Iteration 75/1000 | Loss: 0.00002511
Iteration 76/1000 | Loss: 0.00002511
Iteration 77/1000 | Loss: 0.00002511
Iteration 78/1000 | Loss: 0.00002511
Iteration 79/1000 | Loss: 0.00002511
Iteration 80/1000 | Loss: 0.00002511
Iteration 81/1000 | Loss: 0.00002511
Iteration 82/1000 | Loss: 0.00002511
Iteration 83/1000 | Loss: 0.00002511
Iteration 84/1000 | Loss: 0.00002511
Iteration 85/1000 | Loss: 0.00002511
Iteration 86/1000 | Loss: 0.00002511
Iteration 87/1000 | Loss: 0.00002511
Iteration 88/1000 | Loss: 0.00002511
Iteration 89/1000 | Loss: 0.00002511
Iteration 90/1000 | Loss: 0.00002511
Iteration 91/1000 | Loss: 0.00002511
Iteration 92/1000 | Loss: 0.00002511
Iteration 93/1000 | Loss: 0.00002511
Iteration 94/1000 | Loss: 0.00002511
Iteration 95/1000 | Loss: 0.00002511
Iteration 96/1000 | Loss: 0.00002511
Iteration 97/1000 | Loss: 0.00002511
Iteration 98/1000 | Loss: 0.00002511
Iteration 99/1000 | Loss: 0.00002511
Iteration 100/1000 | Loss: 0.00002511
Iteration 101/1000 | Loss: 0.00002511
Iteration 102/1000 | Loss: 0.00002511
Iteration 103/1000 | Loss: 0.00002511
Iteration 104/1000 | Loss: 0.00002511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.510571175662335e-05, 2.510571175662335e-05, 2.510571175662335e-05, 2.510571175662335e-05, 2.510571175662335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.510571175662335e-05

Optimization complete. Final v2v error: 4.198992729187012 mm

Highest mean error: 4.302638530731201 mm for frame 42

Lowest mean error: 4.119778633117676 mm for frame 15

Saving results

Total time: 51.82560133934021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486687
Iteration 2/25 | Loss: 0.00139090
Iteration 3/25 | Loss: 0.00123575
Iteration 4/25 | Loss: 0.00121999
Iteration 5/25 | Loss: 0.00121710
Iteration 6/25 | Loss: 0.00121654
Iteration 7/25 | Loss: 0.00121654
Iteration 8/25 | Loss: 0.00121654
Iteration 9/25 | Loss: 0.00121654
Iteration 10/25 | Loss: 0.00121654
Iteration 11/25 | Loss: 0.00121654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012165404623374343, 0.0012165404623374343, 0.0012165404623374343, 0.0012165404623374343, 0.0012165404623374343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012165404623374343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46936738
Iteration 2/25 | Loss: 0.00089645
Iteration 3/25 | Loss: 0.00089641
Iteration 4/25 | Loss: 0.00089641
Iteration 5/25 | Loss: 0.00089641
Iteration 6/25 | Loss: 0.00089641
Iteration 7/25 | Loss: 0.00089641
Iteration 8/25 | Loss: 0.00089641
Iteration 9/25 | Loss: 0.00089641
Iteration 10/25 | Loss: 0.00089641
Iteration 11/25 | Loss: 0.00089641
Iteration 12/25 | Loss: 0.00089641
Iteration 13/25 | Loss: 0.00089641
Iteration 14/25 | Loss: 0.00089641
Iteration 15/25 | Loss: 0.00089641
Iteration 16/25 | Loss: 0.00089641
Iteration 17/25 | Loss: 0.00089641
Iteration 18/25 | Loss: 0.00089641
Iteration 19/25 | Loss: 0.00089641
Iteration 20/25 | Loss: 0.00089641
Iteration 21/25 | Loss: 0.00089641
Iteration 22/25 | Loss: 0.00089641
Iteration 23/25 | Loss: 0.00089641
Iteration 24/25 | Loss: 0.00089641
Iteration 25/25 | Loss: 0.00089641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089641
Iteration 2/1000 | Loss: 0.00003507
Iteration 3/1000 | Loss: 0.00002430
Iteration 4/1000 | Loss: 0.00002009
Iteration 5/1000 | Loss: 0.00001880
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001591
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001545
Iteration 13/1000 | Loss: 0.00001529
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001509
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001506
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001505
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001501
Iteration 48/1000 | Loss: 0.00001501
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001500
Iteration 51/1000 | Loss: 0.00001500
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001499
Iteration 63/1000 | Loss: 0.00001499
Iteration 64/1000 | Loss: 0.00001499
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001499
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001499
Iteration 72/1000 | Loss: 0.00001499
Iteration 73/1000 | Loss: 0.00001499
Iteration 74/1000 | Loss: 0.00001499
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.4993061085988302e-05, 1.4993061085988302e-05, 1.4993061085988302e-05, 1.4993061085988302e-05, 1.4993061085988302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4993061085988302e-05

Optimization complete. Final v2v error: 3.2483932971954346 mm

Highest mean error: 3.816439151763916 mm for frame 69

Lowest mean error: 2.720284938812256 mm for frame 93

Saving results

Total time: 32.30883073806763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980182
Iteration 2/25 | Loss: 0.00192141
Iteration 3/25 | Loss: 0.00141989
Iteration 4/25 | Loss: 0.00132864
Iteration 5/25 | Loss: 0.00126382
Iteration 6/25 | Loss: 0.00124679
Iteration 7/25 | Loss: 0.00123841
Iteration 8/25 | Loss: 0.00121027
Iteration 9/25 | Loss: 0.00119319
Iteration 10/25 | Loss: 0.00118965
Iteration 11/25 | Loss: 0.00118762
Iteration 12/25 | Loss: 0.00119594
Iteration 13/25 | Loss: 0.00119746
Iteration 14/25 | Loss: 0.00118818
Iteration 15/25 | Loss: 0.00118661
Iteration 16/25 | Loss: 0.00118171
Iteration 17/25 | Loss: 0.00117910
Iteration 18/25 | Loss: 0.00117802
Iteration 19/25 | Loss: 0.00117766
Iteration 20/25 | Loss: 0.00117756
Iteration 21/25 | Loss: 0.00117747
Iteration 22/25 | Loss: 0.00117737
Iteration 23/25 | Loss: 0.00117810
Iteration 24/25 | Loss: 0.00117643
Iteration 25/25 | Loss: 0.00117614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37173510
Iteration 2/25 | Loss: 0.00100506
Iteration 3/25 | Loss: 0.00099389
Iteration 4/25 | Loss: 0.00099389
Iteration 5/25 | Loss: 0.00099389
Iteration 6/25 | Loss: 0.00099389
Iteration 7/25 | Loss: 0.00099389
Iteration 8/25 | Loss: 0.00099389
Iteration 9/25 | Loss: 0.00099389
Iteration 10/25 | Loss: 0.00099389
Iteration 11/25 | Loss: 0.00099389
Iteration 12/25 | Loss: 0.00099389
Iteration 13/25 | Loss: 0.00099389
Iteration 14/25 | Loss: 0.00099389
Iteration 15/25 | Loss: 0.00099389
Iteration 16/25 | Loss: 0.00099389
Iteration 17/25 | Loss: 0.00099389
Iteration 18/25 | Loss: 0.00099389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009938867297023535, 0.0009938867297023535, 0.0009938867297023535, 0.0009938867297023535, 0.0009938867297023535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009938867297023535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099389
Iteration 2/1000 | Loss: 0.00004764
Iteration 3/1000 | Loss: 0.00002251
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001733
Iteration 6/1000 | Loss: 0.00001619
Iteration 7/1000 | Loss: 0.00002497
Iteration 8/1000 | Loss: 0.00001842
Iteration 9/1000 | Loss: 0.00001466
Iteration 10/1000 | Loss: 0.00002502
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001382
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00003107
Iteration 21/1000 | Loss: 0.00001391
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001346
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00003141
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001333
Iteration 50/1000 | Loss: 0.00001333
Iteration 51/1000 | Loss: 0.00001332
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001329
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001326
Iteration 101/1000 | Loss: 0.00001326
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001326
Iteration 120/1000 | Loss: 0.00001326
Iteration 121/1000 | Loss: 0.00001326
Iteration 122/1000 | Loss: 0.00001326
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.3260850209917407e-05, 1.3260850209917407e-05, 1.3260850209917407e-05, 1.3260850209917407e-05, 1.3260850209917407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3260850209917407e-05

Optimization complete. Final v2v error: 3.021127223968506 mm

Highest mean error: 6.0079169273376465 mm for frame 1

Lowest mean error: 2.4577789306640625 mm for frame 237

Saving results

Total time: 87.60335779190063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791692
Iteration 2/25 | Loss: 0.00126601
Iteration 3/25 | Loss: 0.00117271
Iteration 4/25 | Loss: 0.00115906
Iteration 5/25 | Loss: 0.00115559
Iteration 6/25 | Loss: 0.00115551
Iteration 7/25 | Loss: 0.00115551
Iteration 8/25 | Loss: 0.00115551
Iteration 9/25 | Loss: 0.00115551
Iteration 10/25 | Loss: 0.00115551
Iteration 11/25 | Loss: 0.00115551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011555103119462729, 0.0011555103119462729, 0.0011555103119462729, 0.0011555103119462729, 0.0011555103119462729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011555103119462729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33227932
Iteration 2/25 | Loss: 0.00069182
Iteration 3/25 | Loss: 0.00069180
Iteration 4/25 | Loss: 0.00069180
Iteration 5/25 | Loss: 0.00069180
Iteration 6/25 | Loss: 0.00069180
Iteration 7/25 | Loss: 0.00069180
Iteration 8/25 | Loss: 0.00069180
Iteration 9/25 | Loss: 0.00069180
Iteration 10/25 | Loss: 0.00069180
Iteration 11/25 | Loss: 0.00069180
Iteration 12/25 | Loss: 0.00069180
Iteration 13/25 | Loss: 0.00069180
Iteration 14/25 | Loss: 0.00069180
Iteration 15/25 | Loss: 0.00069180
Iteration 16/25 | Loss: 0.00069180
Iteration 17/25 | Loss: 0.00069180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006917973514646292, 0.0006917973514646292, 0.0006917973514646292, 0.0006917973514646292, 0.0006917973514646292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006917973514646292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069180
Iteration 2/1000 | Loss: 0.00002847
Iteration 3/1000 | Loss: 0.00001998
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001738
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001569
Iteration 9/1000 | Loss: 0.00001538
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001498
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001496
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001461
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001455
Iteration 33/1000 | Loss: 0.00001454
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001442
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001418
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001411
Iteration 73/1000 | Loss: 0.00001411
Iteration 74/1000 | Loss: 0.00001410
Iteration 75/1000 | Loss: 0.00001410
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Iteration 78/1000 | Loss: 0.00001409
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00001409
Iteration 81/1000 | Loss: 0.00001409
Iteration 82/1000 | Loss: 0.00001409
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001409
Iteration 90/1000 | Loss: 0.00001409
Iteration 91/1000 | Loss: 0.00001409
Iteration 92/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.408785828971304e-05, 1.408785828971304e-05, 1.408785828971304e-05, 1.408785828971304e-05, 1.408785828971304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.408785828971304e-05

Optimization complete. Final v2v error: 3.1729984283447266 mm

Highest mean error: 3.3936595916748047 mm for frame 64

Lowest mean error: 2.932725191116333 mm for frame 0

Saving results

Total time: 36.59693479537964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021179
Iteration 2/25 | Loss: 0.01021179
Iteration 3/25 | Loss: 0.00187799
Iteration 4/25 | Loss: 0.00148639
Iteration 5/25 | Loss: 0.00140700
Iteration 6/25 | Loss: 0.00145874
Iteration 7/25 | Loss: 0.00147362
Iteration 8/25 | Loss: 0.00140372
Iteration 9/25 | Loss: 0.00129694
Iteration 10/25 | Loss: 0.00125773
Iteration 11/25 | Loss: 0.00124175
Iteration 12/25 | Loss: 0.00123670
Iteration 13/25 | Loss: 0.00123364
Iteration 14/25 | Loss: 0.00122661
Iteration 15/25 | Loss: 0.00122360
Iteration 16/25 | Loss: 0.00121695
Iteration 17/25 | Loss: 0.00120841
Iteration 18/25 | Loss: 0.00121742
Iteration 19/25 | Loss: 0.00121254
Iteration 20/25 | Loss: 0.00120656
Iteration 21/25 | Loss: 0.00120343
Iteration 22/25 | Loss: 0.00120122
Iteration 23/25 | Loss: 0.00120768
Iteration 24/25 | Loss: 0.00120644
Iteration 25/25 | Loss: 0.00121603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39061117
Iteration 2/25 | Loss: 0.00151600
Iteration 3/25 | Loss: 0.00147651
Iteration 4/25 | Loss: 0.00147651
Iteration 5/25 | Loss: 0.00147651
Iteration 6/25 | Loss: 0.00147651
Iteration 7/25 | Loss: 0.00147651
Iteration 8/25 | Loss: 0.00147651
Iteration 9/25 | Loss: 0.00147651
Iteration 10/25 | Loss: 0.00147651
Iteration 11/25 | Loss: 0.00147651
Iteration 12/25 | Loss: 0.00147650
Iteration 13/25 | Loss: 0.00147650
Iteration 14/25 | Loss: 0.00147650
Iteration 15/25 | Loss: 0.00147650
Iteration 16/25 | Loss: 0.00147650
Iteration 17/25 | Loss: 0.00147650
Iteration 18/25 | Loss: 0.00147650
Iteration 19/25 | Loss: 0.00147650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001476504490710795, 0.001476504490710795, 0.001476504490710795, 0.001476504490710795, 0.001476504490710795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001476504490710795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147650
Iteration 2/1000 | Loss: 0.00036940
Iteration 3/1000 | Loss: 0.00035933
Iteration 4/1000 | Loss: 0.00033386
Iteration 5/1000 | Loss: 0.00054472
Iteration 6/1000 | Loss: 0.00051351
Iteration 7/1000 | Loss: 0.00049330
Iteration 8/1000 | Loss: 0.00059733
Iteration 9/1000 | Loss: 0.00050253
Iteration 10/1000 | Loss: 0.00043703
Iteration 11/1000 | Loss: 0.00052549
Iteration 12/1000 | Loss: 0.00033698
Iteration 13/1000 | Loss: 0.00048095
Iteration 14/1000 | Loss: 0.00065106
Iteration 15/1000 | Loss: 0.00061367
Iteration 16/1000 | Loss: 0.00041827
Iteration 17/1000 | Loss: 0.00031324
Iteration 18/1000 | Loss: 0.00033168
Iteration 19/1000 | Loss: 0.00029283
Iteration 20/1000 | Loss: 0.00037361
Iteration 21/1000 | Loss: 0.00081607
Iteration 22/1000 | Loss: 0.00036325
Iteration 23/1000 | Loss: 0.00032149
Iteration 24/1000 | Loss: 0.00045620
Iteration 25/1000 | Loss: 0.00029216
Iteration 26/1000 | Loss: 0.00020568
Iteration 27/1000 | Loss: 0.00017198
Iteration 28/1000 | Loss: 0.00071203
Iteration 29/1000 | Loss: 0.00047335
Iteration 30/1000 | Loss: 0.00005381
Iteration 31/1000 | Loss: 0.00065000
Iteration 32/1000 | Loss: 0.00063160
Iteration 33/1000 | Loss: 0.00046626
Iteration 34/1000 | Loss: 0.00017643
Iteration 35/1000 | Loss: 0.00030234
Iteration 36/1000 | Loss: 0.00014719
Iteration 37/1000 | Loss: 0.00012444
Iteration 38/1000 | Loss: 0.00020028
Iteration 39/1000 | Loss: 0.00018779
Iteration 40/1000 | Loss: 0.00011345
Iteration 41/1000 | Loss: 0.00019968
Iteration 42/1000 | Loss: 0.00011776
Iteration 43/1000 | Loss: 0.00009803
Iteration 44/1000 | Loss: 0.00020474
Iteration 45/1000 | Loss: 0.00042506
Iteration 46/1000 | Loss: 0.00026017
Iteration 47/1000 | Loss: 0.00011599
Iteration 48/1000 | Loss: 0.00020949
Iteration 49/1000 | Loss: 0.00024756
Iteration 50/1000 | Loss: 0.00027835
Iteration 51/1000 | Loss: 0.00015358
Iteration 52/1000 | Loss: 0.00013284
Iteration 53/1000 | Loss: 0.00007743
Iteration 54/1000 | Loss: 0.00009697
Iteration 55/1000 | Loss: 0.00012149
Iteration 56/1000 | Loss: 0.00011636
Iteration 57/1000 | Loss: 0.00008882
Iteration 58/1000 | Loss: 0.00011518
Iteration 59/1000 | Loss: 0.00025726
Iteration 60/1000 | Loss: 0.00027826
Iteration 61/1000 | Loss: 0.00034870
Iteration 62/1000 | Loss: 0.00022661
Iteration 63/1000 | Loss: 0.00013378
Iteration 64/1000 | Loss: 0.00018756
Iteration 65/1000 | Loss: 0.00039248
Iteration 66/1000 | Loss: 0.00007653
Iteration 67/1000 | Loss: 0.00008115
Iteration 68/1000 | Loss: 0.00006689
Iteration 69/1000 | Loss: 0.00030162
Iteration 70/1000 | Loss: 0.00050940
Iteration 71/1000 | Loss: 0.00062034
Iteration 72/1000 | Loss: 0.00036969
Iteration 73/1000 | Loss: 0.00031909
Iteration 74/1000 | Loss: 0.00032156
Iteration 75/1000 | Loss: 0.00020381
Iteration 76/1000 | Loss: 0.00026267
Iteration 77/1000 | Loss: 0.00024336
Iteration 78/1000 | Loss: 0.00024821
Iteration 79/1000 | Loss: 0.00021251
Iteration 80/1000 | Loss: 0.00013416
Iteration 81/1000 | Loss: 0.00013588
Iteration 82/1000 | Loss: 0.00004613
Iteration 83/1000 | Loss: 0.00017017
Iteration 84/1000 | Loss: 0.00077213
Iteration 85/1000 | Loss: 0.00041299
Iteration 86/1000 | Loss: 0.00022448
Iteration 87/1000 | Loss: 0.00018364
Iteration 88/1000 | Loss: 0.00004082
Iteration 89/1000 | Loss: 0.00005249
Iteration 90/1000 | Loss: 0.00006733
Iteration 91/1000 | Loss: 0.00004848
Iteration 92/1000 | Loss: 0.00052285
Iteration 93/1000 | Loss: 0.00052770
Iteration 94/1000 | Loss: 0.00011033
Iteration 95/1000 | Loss: 0.00054210
Iteration 96/1000 | Loss: 0.00006892
Iteration 97/1000 | Loss: 0.00006231
Iteration 98/1000 | Loss: 0.00005721
Iteration 99/1000 | Loss: 0.00007009
Iteration 100/1000 | Loss: 0.00005702
Iteration 101/1000 | Loss: 0.00005165
Iteration 102/1000 | Loss: 0.00003855
Iteration 103/1000 | Loss: 0.00004724
Iteration 104/1000 | Loss: 0.00006852
Iteration 105/1000 | Loss: 0.00004786
Iteration 106/1000 | Loss: 0.00006868
Iteration 107/1000 | Loss: 0.00004774
Iteration 108/1000 | Loss: 0.00044752
Iteration 109/1000 | Loss: 0.00029328
Iteration 110/1000 | Loss: 0.00005505
Iteration 111/1000 | Loss: 0.00004572
Iteration 112/1000 | Loss: 0.00004455
Iteration 113/1000 | Loss: 0.00004081
Iteration 114/1000 | Loss: 0.00004015
Iteration 115/1000 | Loss: 0.00004176
Iteration 116/1000 | Loss: 0.00004416
Iteration 117/1000 | Loss: 0.00004460
Iteration 118/1000 | Loss: 0.00006003
Iteration 119/1000 | Loss: 0.00004315
Iteration 120/1000 | Loss: 0.00004875
Iteration 121/1000 | Loss: 0.00004522
Iteration 122/1000 | Loss: 0.00004863
Iteration 123/1000 | Loss: 0.00006225
Iteration 124/1000 | Loss: 0.00029431
Iteration 125/1000 | Loss: 0.00026798
Iteration 126/1000 | Loss: 0.00003857
Iteration 127/1000 | Loss: 0.00004500
Iteration 128/1000 | Loss: 0.00004660
Iteration 129/1000 | Loss: 0.00004321
Iteration 130/1000 | Loss: 0.00005036
Iteration 131/1000 | Loss: 0.00005136
Iteration 132/1000 | Loss: 0.00004472
Iteration 133/1000 | Loss: 0.00004394
Iteration 134/1000 | Loss: 0.00005399
Iteration 135/1000 | Loss: 0.00005482
Iteration 136/1000 | Loss: 0.00004577
Iteration 137/1000 | Loss: 0.00004794
Iteration 138/1000 | Loss: 0.00004768
Iteration 139/1000 | Loss: 0.00004957
Iteration 140/1000 | Loss: 0.00005048
Iteration 141/1000 | Loss: 0.00026234
Iteration 142/1000 | Loss: 0.00046121
Iteration 143/1000 | Loss: 0.00018329
Iteration 144/1000 | Loss: 0.00017608
Iteration 145/1000 | Loss: 0.00027771
Iteration 146/1000 | Loss: 0.00009560
Iteration 147/1000 | Loss: 0.00017583
Iteration 148/1000 | Loss: 0.00017376
Iteration 149/1000 | Loss: 0.00013569
Iteration 150/1000 | Loss: 0.00014053
Iteration 151/1000 | Loss: 0.00022240
Iteration 152/1000 | Loss: 0.00067718
Iteration 153/1000 | Loss: 0.00006575
Iteration 154/1000 | Loss: 0.00005689
Iteration 155/1000 | Loss: 0.00002834
Iteration 156/1000 | Loss: 0.00002476
Iteration 157/1000 | Loss: 0.00019618
Iteration 158/1000 | Loss: 0.00026791
Iteration 159/1000 | Loss: 0.00008173
Iteration 160/1000 | Loss: 0.00002677
Iteration 161/1000 | Loss: 0.00002490
Iteration 162/1000 | Loss: 0.00002293
Iteration 163/1000 | Loss: 0.00002183
Iteration 164/1000 | Loss: 0.00002055
Iteration 165/1000 | Loss: 0.00001890
Iteration 166/1000 | Loss: 0.00001802
Iteration 167/1000 | Loss: 0.00001761
Iteration 168/1000 | Loss: 0.00001707
Iteration 169/1000 | Loss: 0.00001672
Iteration 170/1000 | Loss: 0.00001648
Iteration 171/1000 | Loss: 0.00001646
Iteration 172/1000 | Loss: 0.00001644
Iteration 173/1000 | Loss: 0.00001640
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001620
Iteration 176/1000 | Loss: 0.00001605
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001592
Iteration 179/1000 | Loss: 0.00001592
Iteration 180/1000 | Loss: 0.00001590
Iteration 181/1000 | Loss: 0.00001590
Iteration 182/1000 | Loss: 0.00001589
Iteration 183/1000 | Loss: 0.00001589
Iteration 184/1000 | Loss: 0.00001588
Iteration 185/1000 | Loss: 0.00001588
Iteration 186/1000 | Loss: 0.00001587
Iteration 187/1000 | Loss: 0.00001586
Iteration 188/1000 | Loss: 0.00001586
Iteration 189/1000 | Loss: 0.00001586
Iteration 190/1000 | Loss: 0.00001585
Iteration 191/1000 | Loss: 0.00001585
Iteration 192/1000 | Loss: 0.00001585
Iteration 193/1000 | Loss: 0.00001585
Iteration 194/1000 | Loss: 0.00001585
Iteration 195/1000 | Loss: 0.00001585
Iteration 196/1000 | Loss: 0.00001585
Iteration 197/1000 | Loss: 0.00001585
Iteration 198/1000 | Loss: 0.00001584
Iteration 199/1000 | Loss: 0.00001584
Iteration 200/1000 | Loss: 0.00001583
Iteration 201/1000 | Loss: 0.00001583
Iteration 202/1000 | Loss: 0.00001582
Iteration 203/1000 | Loss: 0.00001582
Iteration 204/1000 | Loss: 0.00001582
Iteration 205/1000 | Loss: 0.00001582
Iteration 206/1000 | Loss: 0.00001582
Iteration 207/1000 | Loss: 0.00001582
Iteration 208/1000 | Loss: 0.00001582
Iteration 209/1000 | Loss: 0.00001582
Iteration 210/1000 | Loss: 0.00001582
Iteration 211/1000 | Loss: 0.00001582
Iteration 212/1000 | Loss: 0.00001582
Iteration 213/1000 | Loss: 0.00001582
Iteration 214/1000 | Loss: 0.00001581
Iteration 215/1000 | Loss: 0.00001581
Iteration 216/1000 | Loss: 0.00001581
Iteration 217/1000 | Loss: 0.00001581
Iteration 218/1000 | Loss: 0.00001581
Iteration 219/1000 | Loss: 0.00001580
Iteration 220/1000 | Loss: 0.00001580
Iteration 221/1000 | Loss: 0.00001580
Iteration 222/1000 | Loss: 0.00001579
Iteration 223/1000 | Loss: 0.00001579
Iteration 224/1000 | Loss: 0.00001579
Iteration 225/1000 | Loss: 0.00001579
Iteration 226/1000 | Loss: 0.00001579
Iteration 227/1000 | Loss: 0.00001579
Iteration 228/1000 | Loss: 0.00001579
Iteration 229/1000 | Loss: 0.00001579
Iteration 230/1000 | Loss: 0.00001579
Iteration 231/1000 | Loss: 0.00001579
Iteration 232/1000 | Loss: 0.00001579
Iteration 233/1000 | Loss: 0.00001578
Iteration 234/1000 | Loss: 0.00001578
Iteration 235/1000 | Loss: 0.00001578
Iteration 236/1000 | Loss: 0.00001578
Iteration 237/1000 | Loss: 0.00001578
Iteration 238/1000 | Loss: 0.00001578
Iteration 239/1000 | Loss: 0.00001578
Iteration 240/1000 | Loss: 0.00001578
Iteration 241/1000 | Loss: 0.00001578
Iteration 242/1000 | Loss: 0.00001578
Iteration 243/1000 | Loss: 0.00001578
Iteration 244/1000 | Loss: 0.00001577
Iteration 245/1000 | Loss: 0.00001577
Iteration 246/1000 | Loss: 0.00001577
Iteration 247/1000 | Loss: 0.00001577
Iteration 248/1000 | Loss: 0.00001576
Iteration 249/1000 | Loss: 0.00001576
Iteration 250/1000 | Loss: 0.00001576
Iteration 251/1000 | Loss: 0.00001576
Iteration 252/1000 | Loss: 0.00001576
Iteration 253/1000 | Loss: 0.00001576
Iteration 254/1000 | Loss: 0.00001576
Iteration 255/1000 | Loss: 0.00001576
Iteration 256/1000 | Loss: 0.00001576
Iteration 257/1000 | Loss: 0.00001576
Iteration 258/1000 | Loss: 0.00001576
Iteration 259/1000 | Loss: 0.00001576
Iteration 260/1000 | Loss: 0.00001576
Iteration 261/1000 | Loss: 0.00001576
Iteration 262/1000 | Loss: 0.00001575
Iteration 263/1000 | Loss: 0.00001575
Iteration 264/1000 | Loss: 0.00001575
Iteration 265/1000 | Loss: 0.00001575
Iteration 266/1000 | Loss: 0.00001575
Iteration 267/1000 | Loss: 0.00001575
Iteration 268/1000 | Loss: 0.00001575
Iteration 269/1000 | Loss: 0.00001575
Iteration 270/1000 | Loss: 0.00001575
Iteration 271/1000 | Loss: 0.00001575
Iteration 272/1000 | Loss: 0.00001575
Iteration 273/1000 | Loss: 0.00001575
Iteration 274/1000 | Loss: 0.00001575
Iteration 275/1000 | Loss: 0.00001575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.5746902136015706e-05, 1.5746902136015706e-05, 1.5746902136015706e-05, 1.5746902136015706e-05, 1.5746902136015706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5746902136015706e-05

Optimization complete. Final v2v error: 3.0370514392852783 mm

Highest mean error: 11.577601432800293 mm for frame 217

Lowest mean error: 2.6918435096740723 mm for frame 233

Saving results

Total time: 333.2979199886322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626623
Iteration 2/25 | Loss: 0.00120072
Iteration 3/25 | Loss: 0.00113352
Iteration 4/25 | Loss: 0.00112487
Iteration 5/25 | Loss: 0.00112148
Iteration 6/25 | Loss: 0.00112079
Iteration 7/25 | Loss: 0.00112079
Iteration 8/25 | Loss: 0.00112079
Iteration 9/25 | Loss: 0.00112079
Iteration 10/25 | Loss: 0.00112079
Iteration 11/25 | Loss: 0.00112079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011207923525944352, 0.0011207923525944352, 0.0011207923525944352, 0.0011207923525944352, 0.0011207923525944352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011207923525944352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39705706
Iteration 2/25 | Loss: 0.00083743
Iteration 3/25 | Loss: 0.00083743
Iteration 4/25 | Loss: 0.00083743
Iteration 5/25 | Loss: 0.00083743
Iteration 6/25 | Loss: 0.00083743
Iteration 7/25 | Loss: 0.00083743
Iteration 8/25 | Loss: 0.00083743
Iteration 9/25 | Loss: 0.00083743
Iteration 10/25 | Loss: 0.00083743
Iteration 11/25 | Loss: 0.00083743
Iteration 12/25 | Loss: 0.00083743
Iteration 13/25 | Loss: 0.00083743
Iteration 14/25 | Loss: 0.00083743
Iteration 15/25 | Loss: 0.00083743
Iteration 16/25 | Loss: 0.00083743
Iteration 17/25 | Loss: 0.00083743
Iteration 18/25 | Loss: 0.00083743
Iteration 19/25 | Loss: 0.00083743
Iteration 20/25 | Loss: 0.00083743
Iteration 21/25 | Loss: 0.00083743
Iteration 22/25 | Loss: 0.00083743
Iteration 23/25 | Loss: 0.00083743
Iteration 24/25 | Loss: 0.00083743
Iteration 25/25 | Loss: 0.00083743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083743
Iteration 2/1000 | Loss: 0.00001964
Iteration 3/1000 | Loss: 0.00001250
Iteration 4/1000 | Loss: 0.00001135
Iteration 5/1000 | Loss: 0.00001078
Iteration 6/1000 | Loss: 0.00001043
Iteration 7/1000 | Loss: 0.00001003
Iteration 8/1000 | Loss: 0.00001003
Iteration 9/1000 | Loss: 0.00000982
Iteration 10/1000 | Loss: 0.00000960
Iteration 11/1000 | Loss: 0.00000953
Iteration 12/1000 | Loss: 0.00000949
Iteration 13/1000 | Loss: 0.00000949
Iteration 14/1000 | Loss: 0.00000948
Iteration 15/1000 | Loss: 0.00000939
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000936
Iteration 18/1000 | Loss: 0.00000930
Iteration 19/1000 | Loss: 0.00000924
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000923
Iteration 22/1000 | Loss: 0.00000922
Iteration 23/1000 | Loss: 0.00000920
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000918
Iteration 26/1000 | Loss: 0.00000918
Iteration 27/1000 | Loss: 0.00000918
Iteration 28/1000 | Loss: 0.00000914
Iteration 29/1000 | Loss: 0.00000912
Iteration 30/1000 | Loss: 0.00000911
Iteration 31/1000 | Loss: 0.00000910
Iteration 32/1000 | Loss: 0.00000910
Iteration 33/1000 | Loss: 0.00000909
Iteration 34/1000 | Loss: 0.00000909
Iteration 35/1000 | Loss: 0.00000909
Iteration 36/1000 | Loss: 0.00000909
Iteration 37/1000 | Loss: 0.00000909
Iteration 38/1000 | Loss: 0.00000909
Iteration 39/1000 | Loss: 0.00000909
Iteration 40/1000 | Loss: 0.00000909
Iteration 41/1000 | Loss: 0.00000908
Iteration 42/1000 | Loss: 0.00000908
Iteration 43/1000 | Loss: 0.00000908
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000908
Iteration 46/1000 | Loss: 0.00000908
Iteration 47/1000 | Loss: 0.00000908
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000908
Iteration 50/1000 | Loss: 0.00000907
Iteration 51/1000 | Loss: 0.00000907
Iteration 52/1000 | Loss: 0.00000907
Iteration 53/1000 | Loss: 0.00000907
Iteration 54/1000 | Loss: 0.00000907
Iteration 55/1000 | Loss: 0.00000907
Iteration 56/1000 | Loss: 0.00000907
Iteration 57/1000 | Loss: 0.00000907
Iteration 58/1000 | Loss: 0.00000907
Iteration 59/1000 | Loss: 0.00000906
Iteration 60/1000 | Loss: 0.00000906
Iteration 61/1000 | Loss: 0.00000906
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000905
Iteration 71/1000 | Loss: 0.00000904
Iteration 72/1000 | Loss: 0.00000904
Iteration 73/1000 | Loss: 0.00000904
Iteration 74/1000 | Loss: 0.00000904
Iteration 75/1000 | Loss: 0.00000904
Iteration 76/1000 | Loss: 0.00000904
Iteration 77/1000 | Loss: 0.00000904
Iteration 78/1000 | Loss: 0.00000904
Iteration 79/1000 | Loss: 0.00000904
Iteration 80/1000 | Loss: 0.00000903
Iteration 81/1000 | Loss: 0.00000903
Iteration 82/1000 | Loss: 0.00000903
Iteration 83/1000 | Loss: 0.00000903
Iteration 84/1000 | Loss: 0.00000903
Iteration 85/1000 | Loss: 0.00000903
Iteration 86/1000 | Loss: 0.00000903
Iteration 87/1000 | Loss: 0.00000903
Iteration 88/1000 | Loss: 0.00000903
Iteration 89/1000 | Loss: 0.00000903
Iteration 90/1000 | Loss: 0.00000903
Iteration 91/1000 | Loss: 0.00000903
Iteration 92/1000 | Loss: 0.00000903
Iteration 93/1000 | Loss: 0.00000903
Iteration 94/1000 | Loss: 0.00000902
Iteration 95/1000 | Loss: 0.00000902
Iteration 96/1000 | Loss: 0.00000902
Iteration 97/1000 | Loss: 0.00000901
Iteration 98/1000 | Loss: 0.00000901
Iteration 99/1000 | Loss: 0.00000901
Iteration 100/1000 | Loss: 0.00000901
Iteration 101/1000 | Loss: 0.00000901
Iteration 102/1000 | Loss: 0.00000900
Iteration 103/1000 | Loss: 0.00000900
Iteration 104/1000 | Loss: 0.00000900
Iteration 105/1000 | Loss: 0.00000900
Iteration 106/1000 | Loss: 0.00000900
Iteration 107/1000 | Loss: 0.00000900
Iteration 108/1000 | Loss: 0.00000900
Iteration 109/1000 | Loss: 0.00000900
Iteration 110/1000 | Loss: 0.00000900
Iteration 111/1000 | Loss: 0.00000900
Iteration 112/1000 | Loss: 0.00000900
Iteration 113/1000 | Loss: 0.00000899
Iteration 114/1000 | Loss: 0.00000899
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000899
Iteration 119/1000 | Loss: 0.00000898
Iteration 120/1000 | Loss: 0.00000898
Iteration 121/1000 | Loss: 0.00000898
Iteration 122/1000 | Loss: 0.00000898
Iteration 123/1000 | Loss: 0.00000898
Iteration 124/1000 | Loss: 0.00000898
Iteration 125/1000 | Loss: 0.00000898
Iteration 126/1000 | Loss: 0.00000897
Iteration 127/1000 | Loss: 0.00000897
Iteration 128/1000 | Loss: 0.00000897
Iteration 129/1000 | Loss: 0.00000897
Iteration 130/1000 | Loss: 0.00000897
Iteration 131/1000 | Loss: 0.00000897
Iteration 132/1000 | Loss: 0.00000897
Iteration 133/1000 | Loss: 0.00000897
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000897
Iteration 136/1000 | Loss: 0.00000896
Iteration 137/1000 | Loss: 0.00000896
Iteration 138/1000 | Loss: 0.00000896
Iteration 139/1000 | Loss: 0.00000896
Iteration 140/1000 | Loss: 0.00000896
Iteration 141/1000 | Loss: 0.00000896
Iteration 142/1000 | Loss: 0.00000896
Iteration 143/1000 | Loss: 0.00000896
Iteration 144/1000 | Loss: 0.00000896
Iteration 145/1000 | Loss: 0.00000896
Iteration 146/1000 | Loss: 0.00000895
Iteration 147/1000 | Loss: 0.00000895
Iteration 148/1000 | Loss: 0.00000895
Iteration 149/1000 | Loss: 0.00000895
Iteration 150/1000 | Loss: 0.00000895
Iteration 151/1000 | Loss: 0.00000895
Iteration 152/1000 | Loss: 0.00000895
Iteration 153/1000 | Loss: 0.00000895
Iteration 154/1000 | Loss: 0.00000894
Iteration 155/1000 | Loss: 0.00000894
Iteration 156/1000 | Loss: 0.00000894
Iteration 157/1000 | Loss: 0.00000894
Iteration 158/1000 | Loss: 0.00000894
Iteration 159/1000 | Loss: 0.00000893
Iteration 160/1000 | Loss: 0.00000893
Iteration 161/1000 | Loss: 0.00000893
Iteration 162/1000 | Loss: 0.00000893
Iteration 163/1000 | Loss: 0.00000893
Iteration 164/1000 | Loss: 0.00000893
Iteration 165/1000 | Loss: 0.00000893
Iteration 166/1000 | Loss: 0.00000893
Iteration 167/1000 | Loss: 0.00000893
Iteration 168/1000 | Loss: 0.00000893
Iteration 169/1000 | Loss: 0.00000892
Iteration 170/1000 | Loss: 0.00000892
Iteration 171/1000 | Loss: 0.00000892
Iteration 172/1000 | Loss: 0.00000892
Iteration 173/1000 | Loss: 0.00000892
Iteration 174/1000 | Loss: 0.00000892
Iteration 175/1000 | Loss: 0.00000892
Iteration 176/1000 | Loss: 0.00000892
Iteration 177/1000 | Loss: 0.00000892
Iteration 178/1000 | Loss: 0.00000892
Iteration 179/1000 | Loss: 0.00000892
Iteration 180/1000 | Loss: 0.00000891
Iteration 181/1000 | Loss: 0.00000891
Iteration 182/1000 | Loss: 0.00000891
Iteration 183/1000 | Loss: 0.00000891
Iteration 184/1000 | Loss: 0.00000891
Iteration 185/1000 | Loss: 0.00000891
Iteration 186/1000 | Loss: 0.00000891
Iteration 187/1000 | Loss: 0.00000891
Iteration 188/1000 | Loss: 0.00000891
Iteration 189/1000 | Loss: 0.00000891
Iteration 190/1000 | Loss: 0.00000891
Iteration 191/1000 | Loss: 0.00000891
Iteration 192/1000 | Loss: 0.00000891
Iteration 193/1000 | Loss: 0.00000891
Iteration 194/1000 | Loss: 0.00000891
Iteration 195/1000 | Loss: 0.00000891
Iteration 196/1000 | Loss: 0.00000891
Iteration 197/1000 | Loss: 0.00000891
Iteration 198/1000 | Loss: 0.00000891
Iteration 199/1000 | Loss: 0.00000891
Iteration 200/1000 | Loss: 0.00000891
Iteration 201/1000 | Loss: 0.00000891
Iteration 202/1000 | Loss: 0.00000891
Iteration 203/1000 | Loss: 0.00000891
Iteration 204/1000 | Loss: 0.00000891
Iteration 205/1000 | Loss: 0.00000891
Iteration 206/1000 | Loss: 0.00000891
Iteration 207/1000 | Loss: 0.00000891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [8.905100912670605e-06, 8.905100912670605e-06, 8.905100912670605e-06, 8.905100912670605e-06, 8.905100912670605e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.905100912670605e-06

Optimization complete. Final v2v error: 2.5654489994049072 mm

Highest mean error: 3.198108196258545 mm for frame 80

Lowest mean error: 2.368178367614746 mm for frame 105

Saving results

Total time: 38.06049394607544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852126
Iteration 2/25 | Loss: 0.00156258
Iteration 3/25 | Loss: 0.00126055
Iteration 4/25 | Loss: 0.00123321
Iteration 5/25 | Loss: 0.00122362
Iteration 6/25 | Loss: 0.00122056
Iteration 7/25 | Loss: 0.00121960
Iteration 8/25 | Loss: 0.00121923
Iteration 9/25 | Loss: 0.00121923
Iteration 10/25 | Loss: 0.00121923
Iteration 11/25 | Loss: 0.00121923
Iteration 12/25 | Loss: 0.00121923
Iteration 13/25 | Loss: 0.00121923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012192336143925786, 0.0012192336143925786, 0.0012192336143925786, 0.0012192336143925786, 0.0012192336143925786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012192336143925786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16753244
Iteration 2/25 | Loss: 0.00093307
Iteration 3/25 | Loss: 0.00093307
Iteration 4/25 | Loss: 0.00093307
Iteration 5/25 | Loss: 0.00093307
Iteration 6/25 | Loss: 0.00093307
Iteration 7/25 | Loss: 0.00093307
Iteration 8/25 | Loss: 0.00093307
Iteration 9/25 | Loss: 0.00093307
Iteration 10/25 | Loss: 0.00093307
Iteration 11/25 | Loss: 0.00093307
Iteration 12/25 | Loss: 0.00093307
Iteration 13/25 | Loss: 0.00093307
Iteration 14/25 | Loss: 0.00093307
Iteration 15/25 | Loss: 0.00093307
Iteration 16/25 | Loss: 0.00093307
Iteration 17/25 | Loss: 0.00093307
Iteration 18/25 | Loss: 0.00093307
Iteration 19/25 | Loss: 0.00093307
Iteration 20/25 | Loss: 0.00093307
Iteration 21/25 | Loss: 0.00093307
Iteration 22/25 | Loss: 0.00093307
Iteration 23/25 | Loss: 0.00093307
Iteration 24/25 | Loss: 0.00093307
Iteration 25/25 | Loss: 0.00093307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093307
Iteration 2/1000 | Loss: 0.00005743
Iteration 3/1000 | Loss: 0.00003523
Iteration 4/1000 | Loss: 0.00002781
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002377
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002211
Iteration 9/1000 | Loss: 0.00002163
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002093
Iteration 12/1000 | Loss: 0.00002073
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002037
Iteration 15/1000 | Loss: 0.00002021
Iteration 16/1000 | Loss: 0.00002015
Iteration 17/1000 | Loss: 0.00002012
Iteration 18/1000 | Loss: 0.00002011
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002010
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00001994
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001988
Iteration 29/1000 | Loss: 0.00001988
Iteration 30/1000 | Loss: 0.00001988
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00001987
Iteration 33/1000 | Loss: 0.00001987
Iteration 34/1000 | Loss: 0.00001985
Iteration 35/1000 | Loss: 0.00001985
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00001985
Iteration 39/1000 | Loss: 0.00001985
Iteration 40/1000 | Loss: 0.00001985
Iteration 41/1000 | Loss: 0.00001985
Iteration 42/1000 | Loss: 0.00001984
Iteration 43/1000 | Loss: 0.00001984
Iteration 44/1000 | Loss: 0.00001983
Iteration 45/1000 | Loss: 0.00001983
Iteration 46/1000 | Loss: 0.00001983
Iteration 47/1000 | Loss: 0.00001983
Iteration 48/1000 | Loss: 0.00001983
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001982
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00001981
Iteration 55/1000 | Loss: 0.00001980
Iteration 56/1000 | Loss: 0.00001980
Iteration 57/1000 | Loss: 0.00001980
Iteration 58/1000 | Loss: 0.00001980
Iteration 59/1000 | Loss: 0.00001980
Iteration 60/1000 | Loss: 0.00001979
Iteration 61/1000 | Loss: 0.00001979
Iteration 62/1000 | Loss: 0.00001979
Iteration 63/1000 | Loss: 0.00001978
Iteration 64/1000 | Loss: 0.00001978
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001978
Iteration 67/1000 | Loss: 0.00001977
Iteration 68/1000 | Loss: 0.00001977
Iteration 69/1000 | Loss: 0.00001977
Iteration 70/1000 | Loss: 0.00001977
Iteration 71/1000 | Loss: 0.00001977
Iteration 72/1000 | Loss: 0.00001976
Iteration 73/1000 | Loss: 0.00001976
Iteration 74/1000 | Loss: 0.00001976
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00001971
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001969
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001969
Iteration 106/1000 | Loss: 0.00001968
Iteration 107/1000 | Loss: 0.00001968
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001968
Iteration 111/1000 | Loss: 0.00001968
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001967
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001967
Iteration 119/1000 | Loss: 0.00001967
Iteration 120/1000 | Loss: 0.00001967
Iteration 121/1000 | Loss: 0.00001967
Iteration 122/1000 | Loss: 0.00001967
Iteration 123/1000 | Loss: 0.00001967
Iteration 124/1000 | Loss: 0.00001966
Iteration 125/1000 | Loss: 0.00001966
Iteration 126/1000 | Loss: 0.00001966
Iteration 127/1000 | Loss: 0.00001966
Iteration 128/1000 | Loss: 0.00001966
Iteration 129/1000 | Loss: 0.00001966
Iteration 130/1000 | Loss: 0.00001966
Iteration 131/1000 | Loss: 0.00001966
Iteration 132/1000 | Loss: 0.00001966
Iteration 133/1000 | Loss: 0.00001966
Iteration 134/1000 | Loss: 0.00001966
Iteration 135/1000 | Loss: 0.00001966
Iteration 136/1000 | Loss: 0.00001966
Iteration 137/1000 | Loss: 0.00001966
Iteration 138/1000 | Loss: 0.00001966
Iteration 139/1000 | Loss: 0.00001965
Iteration 140/1000 | Loss: 0.00001965
Iteration 141/1000 | Loss: 0.00001965
Iteration 142/1000 | Loss: 0.00001965
Iteration 143/1000 | Loss: 0.00001965
Iteration 144/1000 | Loss: 0.00001965
Iteration 145/1000 | Loss: 0.00001965
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001964
Iteration 149/1000 | Loss: 0.00001964
Iteration 150/1000 | Loss: 0.00001964
Iteration 151/1000 | Loss: 0.00001964
Iteration 152/1000 | Loss: 0.00001964
Iteration 153/1000 | Loss: 0.00001964
Iteration 154/1000 | Loss: 0.00001964
Iteration 155/1000 | Loss: 0.00001964
Iteration 156/1000 | Loss: 0.00001964
Iteration 157/1000 | Loss: 0.00001964
Iteration 158/1000 | Loss: 0.00001964
Iteration 159/1000 | Loss: 0.00001963
Iteration 160/1000 | Loss: 0.00001963
Iteration 161/1000 | Loss: 0.00001963
Iteration 162/1000 | Loss: 0.00001963
Iteration 163/1000 | Loss: 0.00001963
Iteration 164/1000 | Loss: 0.00001963
Iteration 165/1000 | Loss: 0.00001963
Iteration 166/1000 | Loss: 0.00001963
Iteration 167/1000 | Loss: 0.00001963
Iteration 168/1000 | Loss: 0.00001963
Iteration 169/1000 | Loss: 0.00001962
Iteration 170/1000 | Loss: 0.00001962
Iteration 171/1000 | Loss: 0.00001962
Iteration 172/1000 | Loss: 0.00001962
Iteration 173/1000 | Loss: 0.00001962
Iteration 174/1000 | Loss: 0.00001961
Iteration 175/1000 | Loss: 0.00001961
Iteration 176/1000 | Loss: 0.00001961
Iteration 177/1000 | Loss: 0.00001961
Iteration 178/1000 | Loss: 0.00001961
Iteration 179/1000 | Loss: 0.00001961
Iteration 180/1000 | Loss: 0.00001961
Iteration 181/1000 | Loss: 0.00001960
Iteration 182/1000 | Loss: 0.00001960
Iteration 183/1000 | Loss: 0.00001960
Iteration 184/1000 | Loss: 0.00001960
Iteration 185/1000 | Loss: 0.00001960
Iteration 186/1000 | Loss: 0.00001960
Iteration 187/1000 | Loss: 0.00001960
Iteration 188/1000 | Loss: 0.00001960
Iteration 189/1000 | Loss: 0.00001959
Iteration 190/1000 | Loss: 0.00001959
Iteration 191/1000 | Loss: 0.00001959
Iteration 192/1000 | Loss: 0.00001959
Iteration 193/1000 | Loss: 0.00001959
Iteration 194/1000 | Loss: 0.00001959
Iteration 195/1000 | Loss: 0.00001959
Iteration 196/1000 | Loss: 0.00001959
Iteration 197/1000 | Loss: 0.00001959
Iteration 198/1000 | Loss: 0.00001959
Iteration 199/1000 | Loss: 0.00001959
Iteration 200/1000 | Loss: 0.00001959
Iteration 201/1000 | Loss: 0.00001958
Iteration 202/1000 | Loss: 0.00001958
Iteration 203/1000 | Loss: 0.00001958
Iteration 204/1000 | Loss: 0.00001958
Iteration 205/1000 | Loss: 0.00001958
Iteration 206/1000 | Loss: 0.00001958
Iteration 207/1000 | Loss: 0.00001958
Iteration 208/1000 | Loss: 0.00001958
Iteration 209/1000 | Loss: 0.00001958
Iteration 210/1000 | Loss: 0.00001958
Iteration 211/1000 | Loss: 0.00001958
Iteration 212/1000 | Loss: 0.00001958
Iteration 213/1000 | Loss: 0.00001958
Iteration 214/1000 | Loss: 0.00001958
Iteration 215/1000 | Loss: 0.00001958
Iteration 216/1000 | Loss: 0.00001958
Iteration 217/1000 | Loss: 0.00001958
Iteration 218/1000 | Loss: 0.00001958
Iteration 219/1000 | Loss: 0.00001957
Iteration 220/1000 | Loss: 0.00001957
Iteration 221/1000 | Loss: 0.00001957
Iteration 222/1000 | Loss: 0.00001957
Iteration 223/1000 | Loss: 0.00001957
Iteration 224/1000 | Loss: 0.00001957
Iteration 225/1000 | Loss: 0.00001957
Iteration 226/1000 | Loss: 0.00001957
Iteration 227/1000 | Loss: 0.00001957
Iteration 228/1000 | Loss: 0.00001957
Iteration 229/1000 | Loss: 0.00001957
Iteration 230/1000 | Loss: 0.00001957
Iteration 231/1000 | Loss: 0.00001957
Iteration 232/1000 | Loss: 0.00001957
Iteration 233/1000 | Loss: 0.00001957
Iteration 234/1000 | Loss: 0.00001957
Iteration 235/1000 | Loss: 0.00001956
Iteration 236/1000 | Loss: 0.00001956
Iteration 237/1000 | Loss: 0.00001956
Iteration 238/1000 | Loss: 0.00001956
Iteration 239/1000 | Loss: 0.00001956
Iteration 240/1000 | Loss: 0.00001956
Iteration 241/1000 | Loss: 0.00001956
Iteration 242/1000 | Loss: 0.00001956
Iteration 243/1000 | Loss: 0.00001956
Iteration 244/1000 | Loss: 0.00001956
Iteration 245/1000 | Loss: 0.00001956
Iteration 246/1000 | Loss: 0.00001956
Iteration 247/1000 | Loss: 0.00001956
Iteration 248/1000 | Loss: 0.00001956
Iteration 249/1000 | Loss: 0.00001956
Iteration 250/1000 | Loss: 0.00001956
Iteration 251/1000 | Loss: 0.00001955
Iteration 252/1000 | Loss: 0.00001955
Iteration 253/1000 | Loss: 0.00001955
Iteration 254/1000 | Loss: 0.00001955
Iteration 255/1000 | Loss: 0.00001955
Iteration 256/1000 | Loss: 0.00001955
Iteration 257/1000 | Loss: 0.00001955
Iteration 258/1000 | Loss: 0.00001955
Iteration 259/1000 | Loss: 0.00001955
Iteration 260/1000 | Loss: 0.00001955
Iteration 261/1000 | Loss: 0.00001955
Iteration 262/1000 | Loss: 0.00001955
Iteration 263/1000 | Loss: 0.00001955
Iteration 264/1000 | Loss: 0.00001955
Iteration 265/1000 | Loss: 0.00001954
Iteration 266/1000 | Loss: 0.00001954
Iteration 267/1000 | Loss: 0.00001954
Iteration 268/1000 | Loss: 0.00001954
Iteration 269/1000 | Loss: 0.00001954
Iteration 270/1000 | Loss: 0.00001954
Iteration 271/1000 | Loss: 0.00001954
Iteration 272/1000 | Loss: 0.00001954
Iteration 273/1000 | Loss: 0.00001954
Iteration 274/1000 | Loss: 0.00001954
Iteration 275/1000 | Loss: 0.00001954
Iteration 276/1000 | Loss: 0.00001954
Iteration 277/1000 | Loss: 0.00001954
Iteration 278/1000 | Loss: 0.00001954
Iteration 279/1000 | Loss: 0.00001954
Iteration 280/1000 | Loss: 0.00001954
Iteration 281/1000 | Loss: 0.00001954
Iteration 282/1000 | Loss: 0.00001954
Iteration 283/1000 | Loss: 0.00001954
Iteration 284/1000 | Loss: 0.00001954
Iteration 285/1000 | Loss: 0.00001954
Iteration 286/1000 | Loss: 0.00001954
Iteration 287/1000 | Loss: 0.00001953
Iteration 288/1000 | Loss: 0.00001953
Iteration 289/1000 | Loss: 0.00001953
Iteration 290/1000 | Loss: 0.00001953
Iteration 291/1000 | Loss: 0.00001953
Iteration 292/1000 | Loss: 0.00001953
Iteration 293/1000 | Loss: 0.00001953
Iteration 294/1000 | Loss: 0.00001953
Iteration 295/1000 | Loss: 0.00001953
Iteration 296/1000 | Loss: 0.00001953
Iteration 297/1000 | Loss: 0.00001953
Iteration 298/1000 | Loss: 0.00001953
Iteration 299/1000 | Loss: 0.00001953
Iteration 300/1000 | Loss: 0.00001953
Iteration 301/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.9530161807779223e-05, 1.9530161807779223e-05, 1.9530161807779223e-05, 1.9530161807779223e-05, 1.9530161807779223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9530161807779223e-05

Optimization complete. Final v2v error: 3.615539789199829 mm

Highest mean error: 5.5948309898376465 mm for frame 91

Lowest mean error: 2.6700549125671387 mm for frame 10

Saving results

Total time: 54.459696531295776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973389
Iteration 2/25 | Loss: 0.00384282
Iteration 3/25 | Loss: 0.00223042
Iteration 4/25 | Loss: 0.00198268
Iteration 5/25 | Loss: 0.00183453
Iteration 6/25 | Loss: 0.00177843
Iteration 7/25 | Loss: 0.00175477
Iteration 8/25 | Loss: 0.00173645
Iteration 9/25 | Loss: 0.00172460
Iteration 10/25 | Loss: 0.00171681
Iteration 11/25 | Loss: 0.00170818
Iteration 12/25 | Loss: 0.00170716
Iteration 13/25 | Loss: 0.00169920
Iteration 14/25 | Loss: 0.00169499
Iteration 15/25 | Loss: 0.00169046
Iteration 16/25 | Loss: 0.00169573
Iteration 17/25 | Loss: 0.00169332
Iteration 18/25 | Loss: 0.00168612
Iteration 19/25 | Loss: 0.00168185
Iteration 20/25 | Loss: 0.00167875
Iteration 21/25 | Loss: 0.00173754
Iteration 22/25 | Loss: 0.00165066
Iteration 23/25 | Loss: 0.00156819
Iteration 24/25 | Loss: 0.00152241
Iteration 25/25 | Loss: 0.00149892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35127223
Iteration 2/25 | Loss: 0.00437406
Iteration 3/25 | Loss: 0.00423378
Iteration 4/25 | Loss: 0.00423378
Iteration 5/25 | Loss: 0.00423378
Iteration 6/25 | Loss: 0.00423378
Iteration 7/25 | Loss: 0.00423378
Iteration 8/25 | Loss: 0.00423378
Iteration 9/25 | Loss: 0.00423378
Iteration 10/25 | Loss: 0.00423378
Iteration 11/25 | Loss: 0.00423378
Iteration 12/25 | Loss: 0.00423378
Iteration 13/25 | Loss: 0.00423378
Iteration 14/25 | Loss: 0.00423378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004233777057379484, 0.004233777057379484, 0.004233777057379484, 0.004233777057379484, 0.004233777057379484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004233777057379484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00423378
Iteration 2/1000 | Loss: 0.00101502
Iteration 3/1000 | Loss: 0.00605007
Iteration 4/1000 | Loss: 0.00155840
Iteration 5/1000 | Loss: 0.00038904
Iteration 6/1000 | Loss: 0.00179247
Iteration 7/1000 | Loss: 0.00040447
Iteration 8/1000 | Loss: 0.00070494
Iteration 9/1000 | Loss: 0.00057380
Iteration 10/1000 | Loss: 0.00085023
Iteration 11/1000 | Loss: 0.00101854
Iteration 12/1000 | Loss: 0.00061947
Iteration 13/1000 | Loss: 0.00078527
Iteration 14/1000 | Loss: 0.00074308
Iteration 15/1000 | Loss: 0.00039542
Iteration 16/1000 | Loss: 0.00024813
Iteration 17/1000 | Loss: 0.00052797
Iteration 18/1000 | Loss: 0.00024877
Iteration 19/1000 | Loss: 0.00073600
Iteration 20/1000 | Loss: 0.00049019
Iteration 21/1000 | Loss: 0.00178851
Iteration 22/1000 | Loss: 0.00034517
Iteration 23/1000 | Loss: 0.00089517
Iteration 24/1000 | Loss: 0.00075857
Iteration 25/1000 | Loss: 0.00022330
Iteration 26/1000 | Loss: 0.00031075
Iteration 27/1000 | Loss: 0.00150138
Iteration 28/1000 | Loss: 0.00035430
Iteration 29/1000 | Loss: 0.00078922
Iteration 30/1000 | Loss: 0.00049117
Iteration 31/1000 | Loss: 0.00086967
Iteration 32/1000 | Loss: 0.00054342
Iteration 33/1000 | Loss: 0.00083944
Iteration 34/1000 | Loss: 0.00019704
Iteration 35/1000 | Loss: 0.00113800
Iteration 36/1000 | Loss: 0.00020818
Iteration 37/1000 | Loss: 0.00036893
Iteration 38/1000 | Loss: 0.00085461
Iteration 39/1000 | Loss: 0.00022872
Iteration 40/1000 | Loss: 0.00018595
Iteration 41/1000 | Loss: 0.00018522
Iteration 42/1000 | Loss: 0.00036079
Iteration 43/1000 | Loss: 0.00156824
Iteration 44/1000 | Loss: 0.00030935
Iteration 45/1000 | Loss: 0.00092596
Iteration 46/1000 | Loss: 0.00126191
Iteration 47/1000 | Loss: 0.00033956
Iteration 48/1000 | Loss: 0.00017510
Iteration 49/1000 | Loss: 0.00017041
Iteration 50/1000 | Loss: 0.00061603
Iteration 51/1000 | Loss: 0.00019261
Iteration 52/1000 | Loss: 0.00017814
Iteration 53/1000 | Loss: 0.00016050
Iteration 54/1000 | Loss: 0.00030023
Iteration 55/1000 | Loss: 0.00029136
Iteration 56/1000 | Loss: 0.00016807
Iteration 57/1000 | Loss: 0.00015890
Iteration 58/1000 | Loss: 0.00058664
Iteration 59/1000 | Loss: 0.00021631
Iteration 60/1000 | Loss: 0.00015881
Iteration 61/1000 | Loss: 0.00016567
Iteration 62/1000 | Loss: 0.00014390
Iteration 63/1000 | Loss: 0.00016599
Iteration 64/1000 | Loss: 0.00013818
Iteration 65/1000 | Loss: 0.00028195
Iteration 66/1000 | Loss: 0.00082478
Iteration 67/1000 | Loss: 0.00065975
Iteration 68/1000 | Loss: 0.00040300
Iteration 69/1000 | Loss: 0.00014501
Iteration 70/1000 | Loss: 0.00062772
Iteration 71/1000 | Loss: 0.00027378
Iteration 72/1000 | Loss: 0.00038956
Iteration 73/1000 | Loss: 0.00013127
Iteration 74/1000 | Loss: 0.00013054
Iteration 75/1000 | Loss: 0.00013673
Iteration 76/1000 | Loss: 0.00014225
Iteration 77/1000 | Loss: 0.00027781
Iteration 78/1000 | Loss: 0.00014354
Iteration 79/1000 | Loss: 0.00014133
Iteration 80/1000 | Loss: 0.00015808
Iteration 81/1000 | Loss: 0.00014886
Iteration 82/1000 | Loss: 0.00013930
Iteration 83/1000 | Loss: 0.00014045
Iteration 84/1000 | Loss: 0.00012657
Iteration 85/1000 | Loss: 0.00012767
Iteration 86/1000 | Loss: 0.00013220
Iteration 87/1000 | Loss: 0.00013267
Iteration 88/1000 | Loss: 0.00012799
Iteration 89/1000 | Loss: 0.00013942
Iteration 90/1000 | Loss: 0.00012423
Iteration 91/1000 | Loss: 0.00011930
Iteration 92/1000 | Loss: 0.00028044
Iteration 93/1000 | Loss: 0.00013246
Iteration 94/1000 | Loss: 0.00013806
Iteration 95/1000 | Loss: 0.00038387
Iteration 96/1000 | Loss: 0.00029884
Iteration 97/1000 | Loss: 0.00013449
Iteration 98/1000 | Loss: 0.00013269
Iteration 99/1000 | Loss: 0.00014102
Iteration 100/1000 | Loss: 0.00013226
Iteration 101/1000 | Loss: 0.00014182
Iteration 102/1000 | Loss: 0.00012600
Iteration 103/1000 | Loss: 0.00012691
Iteration 104/1000 | Loss: 0.00012652
Iteration 105/1000 | Loss: 0.00014133
Iteration 106/1000 | Loss: 0.00029562
Iteration 107/1000 | Loss: 0.00012528
Iteration 108/1000 | Loss: 0.00013436
Iteration 109/1000 | Loss: 0.00013994
Iteration 110/1000 | Loss: 0.00014121
Iteration 111/1000 | Loss: 0.00012642
Iteration 112/1000 | Loss: 0.00013465
Iteration 113/1000 | Loss: 0.00013852
Iteration 114/1000 | Loss: 0.00020160
Iteration 115/1000 | Loss: 0.00018493
Iteration 116/1000 | Loss: 0.00012678
Iteration 117/1000 | Loss: 0.00012500
Iteration 118/1000 | Loss: 0.00012338
Iteration 119/1000 | Loss: 0.00011513
Iteration 120/1000 | Loss: 0.00013139
Iteration 121/1000 | Loss: 0.00012325
Iteration 122/1000 | Loss: 0.00011972
Iteration 123/1000 | Loss: 0.00014057
Iteration 124/1000 | Loss: 0.00012554
Iteration 125/1000 | Loss: 0.00012412
Iteration 126/1000 | Loss: 0.00012932
Iteration 127/1000 | Loss: 0.00012768
Iteration 128/1000 | Loss: 0.00013056
Iteration 129/1000 | Loss: 0.00013050
Iteration 130/1000 | Loss: 0.00013279
Iteration 131/1000 | Loss: 0.00012606
Iteration 132/1000 | Loss: 0.00013676
Iteration 133/1000 | Loss: 0.00012830
Iteration 134/1000 | Loss: 0.00029995
Iteration 135/1000 | Loss: 0.00012579
Iteration 136/1000 | Loss: 0.00016421
Iteration 137/1000 | Loss: 0.00012928
Iteration 138/1000 | Loss: 0.00012363
Iteration 139/1000 | Loss: 0.00012649
Iteration 140/1000 | Loss: 0.00012675
Iteration 141/1000 | Loss: 0.00012388
Iteration 142/1000 | Loss: 0.00012524
Iteration 143/1000 | Loss: 0.00012346
Iteration 144/1000 | Loss: 0.00012370
Iteration 145/1000 | Loss: 0.00012160
Iteration 146/1000 | Loss: 0.00012378
Iteration 147/1000 | Loss: 0.00012359
Iteration 148/1000 | Loss: 0.00016505
Iteration 149/1000 | Loss: 0.00013013
Iteration 150/1000 | Loss: 0.00012154
Iteration 151/1000 | Loss: 0.00012123
Iteration 152/1000 | Loss: 0.00018284
Iteration 153/1000 | Loss: 0.00043238
Iteration 154/1000 | Loss: 0.00128401
Iteration 155/1000 | Loss: 0.00012988
Iteration 156/1000 | Loss: 0.00107961
Iteration 157/1000 | Loss: 0.00043812
Iteration 158/1000 | Loss: 0.00016905
Iteration 159/1000 | Loss: 0.00015265
Iteration 160/1000 | Loss: 0.00020921
Iteration 161/1000 | Loss: 0.00012281
Iteration 162/1000 | Loss: 0.00012330
Iteration 163/1000 | Loss: 0.00017645
Iteration 164/1000 | Loss: 0.00013518
Iteration 165/1000 | Loss: 0.00011207
Iteration 166/1000 | Loss: 0.00012185
Iteration 167/1000 | Loss: 0.00013557
Iteration 168/1000 | Loss: 0.00012393
Iteration 169/1000 | Loss: 0.00013839
Iteration 170/1000 | Loss: 0.00012174
Iteration 171/1000 | Loss: 0.00012783
Iteration 172/1000 | Loss: 0.00011940
Iteration 173/1000 | Loss: 0.00031957
Iteration 174/1000 | Loss: 0.00014879
Iteration 175/1000 | Loss: 0.00018424
Iteration 176/1000 | Loss: 0.00013873
Iteration 177/1000 | Loss: 0.00011958
Iteration 178/1000 | Loss: 0.00012495
Iteration 179/1000 | Loss: 0.00012015
Iteration 180/1000 | Loss: 0.00011219
Iteration 181/1000 | Loss: 0.00011271
Iteration 182/1000 | Loss: 0.00011420
Iteration 183/1000 | Loss: 0.00012333
Iteration 184/1000 | Loss: 0.00011438
Iteration 185/1000 | Loss: 0.00012757
Iteration 186/1000 | Loss: 0.00011817
Iteration 187/1000 | Loss: 0.00011508
Iteration 188/1000 | Loss: 0.00010947
Iteration 189/1000 | Loss: 0.00011755
Iteration 190/1000 | Loss: 0.00010941
Iteration 191/1000 | Loss: 0.00012121
Iteration 192/1000 | Loss: 0.00011370
Iteration 193/1000 | Loss: 0.00011068
Iteration 194/1000 | Loss: 0.00011462
Iteration 195/1000 | Loss: 0.00014692
Iteration 196/1000 | Loss: 0.00013860
Iteration 197/1000 | Loss: 0.00037434
Iteration 198/1000 | Loss: 0.00012135
Iteration 199/1000 | Loss: 0.00011813
Iteration 200/1000 | Loss: 0.00011877
Iteration 201/1000 | Loss: 0.00011849
Iteration 202/1000 | Loss: 0.00011687
Iteration 203/1000 | Loss: 0.00011689
Iteration 204/1000 | Loss: 0.00011143
Iteration 205/1000 | Loss: 0.00011788
Iteration 206/1000 | Loss: 0.00010930
Iteration 207/1000 | Loss: 0.00011438
Iteration 208/1000 | Loss: 0.00010560
Iteration 209/1000 | Loss: 0.00010832
Iteration 210/1000 | Loss: 0.00011674
Iteration 211/1000 | Loss: 0.00011770
Iteration 212/1000 | Loss: 0.00011674
Iteration 213/1000 | Loss: 0.00012116
Iteration 214/1000 | Loss: 0.00012650
Iteration 215/1000 | Loss: 0.00011539
Iteration 216/1000 | Loss: 0.00011377
Iteration 217/1000 | Loss: 0.00010738
Iteration 218/1000 | Loss: 0.00010804
Iteration 219/1000 | Loss: 0.00010685
Iteration 220/1000 | Loss: 0.00011090
Iteration 221/1000 | Loss: 0.00012296
Iteration 222/1000 | Loss: 0.00011410
Iteration 223/1000 | Loss: 0.00011933
Iteration 224/1000 | Loss: 0.00011585
Iteration 225/1000 | Loss: 0.00010637
Iteration 226/1000 | Loss: 0.00011300
Iteration 227/1000 | Loss: 0.00010668
Iteration 228/1000 | Loss: 0.00010724
Iteration 229/1000 | Loss: 0.00010632
Iteration 230/1000 | Loss: 0.00011344
Iteration 231/1000 | Loss: 0.00010653
Iteration 232/1000 | Loss: 0.00011354
Iteration 233/1000 | Loss: 0.00010661
Iteration 234/1000 | Loss: 0.00011347
Iteration 235/1000 | Loss: 0.00011067
Iteration 236/1000 | Loss: 0.00010996
Iteration 237/1000 | Loss: 0.00011812
Iteration 238/1000 | Loss: 0.00011051
Iteration 239/1000 | Loss: 0.00011684
Iteration 240/1000 | Loss: 0.00011589
Iteration 241/1000 | Loss: 0.00012257
Iteration 242/1000 | Loss: 0.00035609
Iteration 243/1000 | Loss: 0.00162872
Iteration 244/1000 | Loss: 0.00057754
Iteration 245/1000 | Loss: 0.00104156
Iteration 246/1000 | Loss: 0.00032475
Iteration 247/1000 | Loss: 0.00034983
Iteration 248/1000 | Loss: 0.00012687
Iteration 249/1000 | Loss: 0.00012488
Iteration 250/1000 | Loss: 0.00014534
Iteration 251/1000 | Loss: 0.00020247
Iteration 252/1000 | Loss: 0.00011059
Iteration 253/1000 | Loss: 0.00047592
Iteration 254/1000 | Loss: 0.00010694
Iteration 255/1000 | Loss: 0.00010584
Iteration 256/1000 | Loss: 0.00009776
Iteration 257/1000 | Loss: 0.00010198
Iteration 258/1000 | Loss: 0.00010275
Iteration 259/1000 | Loss: 0.00010548
Iteration 260/1000 | Loss: 0.00009993
Iteration 261/1000 | Loss: 0.00010014
Iteration 262/1000 | Loss: 0.00010405
Iteration 263/1000 | Loss: 0.00013053
Iteration 264/1000 | Loss: 0.00011294
Iteration 265/1000 | Loss: 0.00010495
Iteration 266/1000 | Loss: 0.00010591
Iteration 267/1000 | Loss: 0.00010581
Iteration 268/1000 | Loss: 0.00011710
Iteration 269/1000 | Loss: 0.00010649
Iteration 270/1000 | Loss: 0.00011369
Iteration 271/1000 | Loss: 0.00010559
Iteration 272/1000 | Loss: 0.00019147
Iteration 273/1000 | Loss: 0.00010840
Iteration 274/1000 | Loss: 0.00017537
Iteration 275/1000 | Loss: 0.00010473
Iteration 276/1000 | Loss: 0.00009610
Iteration 277/1000 | Loss: 0.00011084
Iteration 278/1000 | Loss: 0.00018587
Iteration 279/1000 | Loss: 0.00017478
Iteration 280/1000 | Loss: 0.00037150
Iteration 281/1000 | Loss: 0.00016364
Iteration 282/1000 | Loss: 0.00056236
Iteration 283/1000 | Loss: 0.00022902
Iteration 284/1000 | Loss: 0.00037719
Iteration 285/1000 | Loss: 0.00026415
Iteration 286/1000 | Loss: 0.00010511
Iteration 287/1000 | Loss: 0.00011861
Iteration 288/1000 | Loss: 0.00009982
Iteration 289/1000 | Loss: 0.00010301
Iteration 290/1000 | Loss: 0.00009481
Iteration 291/1000 | Loss: 0.00010521
Iteration 292/1000 | Loss: 0.00013037
Iteration 293/1000 | Loss: 0.00010698
Iteration 294/1000 | Loss: 0.00009173
Iteration 295/1000 | Loss: 0.00009527
Iteration 296/1000 | Loss: 0.00010676
Iteration 297/1000 | Loss: 0.00035423
Iteration 298/1000 | Loss: 0.00022425
Iteration 299/1000 | Loss: 0.00012386
Iteration 300/1000 | Loss: 0.00011900
Iteration 301/1000 | Loss: 0.00043359
Iteration 302/1000 | Loss: 0.00190865
Iteration 303/1000 | Loss: 0.00050338
Iteration 304/1000 | Loss: 0.00036882
Iteration 305/1000 | Loss: 0.00066128
Iteration 306/1000 | Loss: 0.00043965
Iteration 307/1000 | Loss: 0.00107019
Iteration 308/1000 | Loss: 0.00044168
Iteration 309/1000 | Loss: 0.00042538
Iteration 310/1000 | Loss: 0.00059723
Iteration 311/1000 | Loss: 0.00026418
Iteration 312/1000 | Loss: 0.00026813
Iteration 313/1000 | Loss: 0.00024158
Iteration 314/1000 | Loss: 0.00012567
Iteration 315/1000 | Loss: 0.00009395
Iteration 316/1000 | Loss: 0.00033904
Iteration 317/1000 | Loss: 0.00113565
Iteration 318/1000 | Loss: 0.00059773
Iteration 319/1000 | Loss: 0.00051528
Iteration 320/1000 | Loss: 0.00022233
Iteration 321/1000 | Loss: 0.00024914
Iteration 322/1000 | Loss: 0.00035256
Iteration 323/1000 | Loss: 0.00016010
Iteration 324/1000 | Loss: 0.00028685
Iteration 325/1000 | Loss: 0.00013472
Iteration 326/1000 | Loss: 0.00008911
Iteration 327/1000 | Loss: 0.00009415
Iteration 328/1000 | Loss: 0.00008110
Iteration 329/1000 | Loss: 0.00007869
Iteration 330/1000 | Loss: 0.00007740
Iteration 331/1000 | Loss: 0.00064530
Iteration 332/1000 | Loss: 0.00022625
Iteration 333/1000 | Loss: 0.00049506
Iteration 334/1000 | Loss: 0.00015832
Iteration 335/1000 | Loss: 0.00021796
Iteration 336/1000 | Loss: 0.00010394
Iteration 337/1000 | Loss: 0.00008006
Iteration 338/1000 | Loss: 0.00007889
Iteration 339/1000 | Loss: 0.00008243
Iteration 340/1000 | Loss: 0.00009263
Iteration 341/1000 | Loss: 0.00007502
Iteration 342/1000 | Loss: 0.00008912
Iteration 343/1000 | Loss: 0.00035847
Iteration 344/1000 | Loss: 0.00025227
Iteration 345/1000 | Loss: 0.00009251
Iteration 346/1000 | Loss: 0.00017412
Iteration 347/1000 | Loss: 0.00066096
Iteration 348/1000 | Loss: 0.00010892
Iteration 349/1000 | Loss: 0.00020360
Iteration 350/1000 | Loss: 0.00008965
Iteration 351/1000 | Loss: 0.00021532
Iteration 352/1000 | Loss: 0.00007863
Iteration 353/1000 | Loss: 0.00009206
Iteration 354/1000 | Loss: 0.00023272
Iteration 355/1000 | Loss: 0.00020701
Iteration 356/1000 | Loss: 0.00026984
Iteration 357/1000 | Loss: 0.00016421
Iteration 358/1000 | Loss: 0.00008607
Iteration 359/1000 | Loss: 0.00007957
Iteration 360/1000 | Loss: 0.00007764
Iteration 361/1000 | Loss: 0.00007249
Iteration 362/1000 | Loss: 0.00007368
Iteration 363/1000 | Loss: 0.00008510
Iteration 364/1000 | Loss: 0.00007004
Iteration 365/1000 | Loss: 0.00007130
Iteration 366/1000 | Loss: 0.00007391
Iteration 367/1000 | Loss: 0.00006933
Iteration 368/1000 | Loss: 0.00006984
Iteration 369/1000 | Loss: 0.00006976
Iteration 370/1000 | Loss: 0.00006909
Iteration 371/1000 | Loss: 0.00006908
Iteration 372/1000 | Loss: 0.00006899
Iteration 373/1000 | Loss: 0.00006898
Iteration 374/1000 | Loss: 0.00006898
Iteration 375/1000 | Loss: 0.00008070
Iteration 376/1000 | Loss: 0.00006881
Iteration 377/1000 | Loss: 0.00006871
Iteration 378/1000 | Loss: 0.00006869
Iteration 379/1000 | Loss: 0.00006868
Iteration 380/1000 | Loss: 0.00006868
Iteration 381/1000 | Loss: 0.00006867
Iteration 382/1000 | Loss: 0.00007094
Iteration 383/1000 | Loss: 0.00006862
Iteration 384/1000 | Loss: 0.00006862
Iteration 385/1000 | Loss: 0.00006862
Iteration 386/1000 | Loss: 0.00006862
Iteration 387/1000 | Loss: 0.00006862
Iteration 388/1000 | Loss: 0.00006862
Iteration 389/1000 | Loss: 0.00006861
Iteration 390/1000 | Loss: 0.00006861
Iteration 391/1000 | Loss: 0.00006861
Iteration 392/1000 | Loss: 0.00006861
Iteration 393/1000 | Loss: 0.00006861
Iteration 394/1000 | Loss: 0.00006861
Iteration 395/1000 | Loss: 0.00007462
Iteration 396/1000 | Loss: 0.00006859
Iteration 397/1000 | Loss: 0.00006857
Iteration 398/1000 | Loss: 0.00006855
Iteration 399/1000 | Loss: 0.00006855
Iteration 400/1000 | Loss: 0.00006855
Iteration 401/1000 | Loss: 0.00006855
Iteration 402/1000 | Loss: 0.00006854
Iteration 403/1000 | Loss: 0.00006854
Iteration 404/1000 | Loss: 0.00006853
Iteration 405/1000 | Loss: 0.00006853
Iteration 406/1000 | Loss: 0.00006853
Iteration 407/1000 | Loss: 0.00006852
Iteration 408/1000 | Loss: 0.00006852
Iteration 409/1000 | Loss: 0.00006844
Iteration 410/1000 | Loss: 0.00006844
Iteration 411/1000 | Loss: 0.00006844
Iteration 412/1000 | Loss: 0.00006842
Iteration 413/1000 | Loss: 0.00006841
Iteration 414/1000 | Loss: 0.00006841
Iteration 415/1000 | Loss: 0.00006841
Iteration 416/1000 | Loss: 0.00006841
Iteration 417/1000 | Loss: 0.00006840
Iteration 418/1000 | Loss: 0.00006840
Iteration 419/1000 | Loss: 0.00006840
Iteration 420/1000 | Loss: 0.00006840
Iteration 421/1000 | Loss: 0.00006840
Iteration 422/1000 | Loss: 0.00006840
Iteration 423/1000 | Loss: 0.00006840
Iteration 424/1000 | Loss: 0.00006839
Iteration 425/1000 | Loss: 0.00006839
Iteration 426/1000 | Loss: 0.00006839
Iteration 427/1000 | Loss: 0.00006839
Iteration 428/1000 | Loss: 0.00006839
Iteration 429/1000 | Loss: 0.00006838
Iteration 430/1000 | Loss: 0.00006838
Iteration 431/1000 | Loss: 0.00006838
Iteration 432/1000 | Loss: 0.00006838
Iteration 433/1000 | Loss: 0.00006837
Iteration 434/1000 | Loss: 0.00006837
Iteration 435/1000 | Loss: 0.00006837
Iteration 436/1000 | Loss: 0.00006837
Iteration 437/1000 | Loss: 0.00006836
Iteration 438/1000 | Loss: 0.00006836
Iteration 439/1000 | Loss: 0.00006836
Iteration 440/1000 | Loss: 0.00006836
Iteration 441/1000 | Loss: 0.00006836
Iteration 442/1000 | Loss: 0.00006836
Iteration 443/1000 | Loss: 0.00006836
Iteration 444/1000 | Loss: 0.00006836
Iteration 445/1000 | Loss: 0.00006835
Iteration 446/1000 | Loss: 0.00006835
Iteration 447/1000 | Loss: 0.00006835
Iteration 448/1000 | Loss: 0.00006835
Iteration 449/1000 | Loss: 0.00006835
Iteration 450/1000 | Loss: 0.00006835
Iteration 451/1000 | Loss: 0.00006835
Iteration 452/1000 | Loss: 0.00006834
Iteration 453/1000 | Loss: 0.00006834
Iteration 454/1000 | Loss: 0.00006834
Iteration 455/1000 | Loss: 0.00006834
Iteration 456/1000 | Loss: 0.00006834
Iteration 457/1000 | Loss: 0.00006834
Iteration 458/1000 | Loss: 0.00006834
Iteration 459/1000 | Loss: 0.00006834
Iteration 460/1000 | Loss: 0.00006834
Iteration 461/1000 | Loss: 0.00006834
Iteration 462/1000 | Loss: 0.00006834
Iteration 463/1000 | Loss: 0.00006834
Iteration 464/1000 | Loss: 0.00006834
Iteration 465/1000 | Loss: 0.00006833
Iteration 466/1000 | Loss: 0.00006833
Iteration 467/1000 | Loss: 0.00006833
Iteration 468/1000 | Loss: 0.00006833
Iteration 469/1000 | Loss: 0.00006833
Iteration 470/1000 | Loss: 0.00006833
Iteration 471/1000 | Loss: 0.00006833
Iteration 472/1000 | Loss: 0.00006833
Iteration 473/1000 | Loss: 0.00006833
Iteration 474/1000 | Loss: 0.00006833
Iteration 475/1000 | Loss: 0.00006833
Iteration 476/1000 | Loss: 0.00006833
Iteration 477/1000 | Loss: 0.00006833
Iteration 478/1000 | Loss: 0.00006832
Iteration 479/1000 | Loss: 0.00006832
Iteration 480/1000 | Loss: 0.00006832
Iteration 481/1000 | Loss: 0.00006832
Iteration 482/1000 | Loss: 0.00006832
Iteration 483/1000 | Loss: 0.00006832
Iteration 484/1000 | Loss: 0.00006832
Iteration 485/1000 | Loss: 0.00006832
Iteration 486/1000 | Loss: 0.00006832
Iteration 487/1000 | Loss: 0.00006832
Iteration 488/1000 | Loss: 0.00006832
Iteration 489/1000 | Loss: 0.00006832
Iteration 490/1000 | Loss: 0.00006832
Iteration 491/1000 | Loss: 0.00006832
Iteration 492/1000 | Loss: 0.00006832
Iteration 493/1000 | Loss: 0.00006831
Iteration 494/1000 | Loss: 0.00006831
Iteration 495/1000 | Loss: 0.00006831
Iteration 496/1000 | Loss: 0.00006831
Iteration 497/1000 | Loss: 0.00006831
Iteration 498/1000 | Loss: 0.00006831
Iteration 499/1000 | Loss: 0.00006831
Iteration 500/1000 | Loss: 0.00006831
Iteration 501/1000 | Loss: 0.00006831
Iteration 502/1000 | Loss: 0.00006831
Iteration 503/1000 | Loss: 0.00006831
Iteration 504/1000 | Loss: 0.00006831
Iteration 505/1000 | Loss: 0.00006831
Iteration 506/1000 | Loss: 0.00006830
Iteration 507/1000 | Loss: 0.00006830
Iteration 508/1000 | Loss: 0.00006830
Iteration 509/1000 | Loss: 0.00006830
Iteration 510/1000 | Loss: 0.00006830
Iteration 511/1000 | Loss: 0.00006830
Iteration 512/1000 | Loss: 0.00006830
Iteration 513/1000 | Loss: 0.00006830
Iteration 514/1000 | Loss: 0.00006830
Iteration 515/1000 | Loss: 0.00006830
Iteration 516/1000 | Loss: 0.00006830
Iteration 517/1000 | Loss: 0.00006829
Iteration 518/1000 | Loss: 0.00006829
Iteration 519/1000 | Loss: 0.00006829
Iteration 520/1000 | Loss: 0.00006829
Iteration 521/1000 | Loss: 0.00006829
Iteration 522/1000 | Loss: 0.00006829
Iteration 523/1000 | Loss: 0.00006829
Iteration 524/1000 | Loss: 0.00006829
Iteration 525/1000 | Loss: 0.00006829
Iteration 526/1000 | Loss: 0.00006829
Iteration 527/1000 | Loss: 0.00006829
Iteration 528/1000 | Loss: 0.00006829
Iteration 529/1000 | Loss: 0.00006829
Iteration 530/1000 | Loss: 0.00006829
Iteration 531/1000 | Loss: 0.00006829
Iteration 532/1000 | Loss: 0.00006828
Iteration 533/1000 | Loss: 0.00006828
Iteration 534/1000 | Loss: 0.00006828
Iteration 535/1000 | Loss: 0.00006828
Iteration 536/1000 | Loss: 0.00006828
Iteration 537/1000 | Loss: 0.00006828
Iteration 538/1000 | Loss: 0.00006828
Iteration 539/1000 | Loss: 0.00006828
Iteration 540/1000 | Loss: 0.00006828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 540. Stopping optimization.
Last 5 losses: [6.828373443568125e-05, 6.828373443568125e-05, 6.828373443568125e-05, 6.828373443568125e-05, 6.828373443568125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.828373443568125e-05

Optimization complete. Final v2v error: 4.312831878662109 mm

Highest mean error: 11.13971996307373 mm for frame 15

Lowest mean error: 2.87735652923584 mm for frame 110

Saving results

Total time: 665.0731482505798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459727
Iteration 2/25 | Loss: 0.00132080
Iteration 3/25 | Loss: 0.00119292
Iteration 4/25 | Loss: 0.00118236
Iteration 5/25 | Loss: 0.00117922
Iteration 6/25 | Loss: 0.00117859
Iteration 7/25 | Loss: 0.00117859
Iteration 8/25 | Loss: 0.00117859
Iteration 9/25 | Loss: 0.00117859
Iteration 10/25 | Loss: 0.00117859
Iteration 11/25 | Loss: 0.00117859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011785876704379916, 0.0011785876704379916, 0.0011785876704379916, 0.0011785876704379916, 0.0011785876704379916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011785876704379916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48141325
Iteration 2/25 | Loss: 0.00073884
Iteration 3/25 | Loss: 0.00073882
Iteration 4/25 | Loss: 0.00073882
Iteration 5/25 | Loss: 0.00073882
Iteration 6/25 | Loss: 0.00073882
Iteration 7/25 | Loss: 0.00073882
Iteration 8/25 | Loss: 0.00073882
Iteration 9/25 | Loss: 0.00073882
Iteration 10/25 | Loss: 0.00073882
Iteration 11/25 | Loss: 0.00073882
Iteration 12/25 | Loss: 0.00073882
Iteration 13/25 | Loss: 0.00073882
Iteration 14/25 | Loss: 0.00073882
Iteration 15/25 | Loss: 0.00073882
Iteration 16/25 | Loss: 0.00073882
Iteration 17/25 | Loss: 0.00073882
Iteration 18/25 | Loss: 0.00073882
Iteration 19/25 | Loss: 0.00073882
Iteration 20/25 | Loss: 0.00073882
Iteration 21/25 | Loss: 0.00073882
Iteration 22/25 | Loss: 0.00073882
Iteration 23/25 | Loss: 0.00073882
Iteration 24/25 | Loss: 0.00073882
Iteration 25/25 | Loss: 0.00073882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073882
Iteration 2/1000 | Loss: 0.00003077
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001944
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001732
Iteration 11/1000 | Loss: 0.00001716
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001705
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001686
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001682
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001672
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001670
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001670
Iteration 30/1000 | Loss: 0.00001669
Iteration 31/1000 | Loss: 0.00001669
Iteration 32/1000 | Loss: 0.00001669
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001666
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001662
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00001654
Iteration 54/1000 | Loss: 0.00001653
Iteration 55/1000 | Loss: 0.00001653
Iteration 56/1000 | Loss: 0.00001652
Iteration 57/1000 | Loss: 0.00001652
Iteration 58/1000 | Loss: 0.00001652
Iteration 59/1000 | Loss: 0.00001650
Iteration 60/1000 | Loss: 0.00001650
Iteration 61/1000 | Loss: 0.00001650
Iteration 62/1000 | Loss: 0.00001650
Iteration 63/1000 | Loss: 0.00001650
Iteration 64/1000 | Loss: 0.00001650
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00001649
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001637
Iteration 130/1000 | Loss: 0.00001637
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Iteration 135/1000 | Loss: 0.00001636
Iteration 136/1000 | Loss: 0.00001636
Iteration 137/1000 | Loss: 0.00001636
Iteration 138/1000 | Loss: 0.00001636
Iteration 139/1000 | Loss: 0.00001635
Iteration 140/1000 | Loss: 0.00001635
Iteration 141/1000 | Loss: 0.00001635
Iteration 142/1000 | Loss: 0.00001635
Iteration 143/1000 | Loss: 0.00001635
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001635
Iteration 149/1000 | Loss: 0.00001635
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001635
Iteration 154/1000 | Loss: 0.00001635
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001635
Iteration 158/1000 | Loss: 0.00001635
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001635
Iteration 161/1000 | Loss: 0.00001635
Iteration 162/1000 | Loss: 0.00001635
Iteration 163/1000 | Loss: 0.00001635
Iteration 164/1000 | Loss: 0.00001635
Iteration 165/1000 | Loss: 0.00001635
Iteration 166/1000 | Loss: 0.00001635
Iteration 167/1000 | Loss: 0.00001635
Iteration 168/1000 | Loss: 0.00001635
Iteration 169/1000 | Loss: 0.00001635
Iteration 170/1000 | Loss: 0.00001635
Iteration 171/1000 | Loss: 0.00001635
Iteration 172/1000 | Loss: 0.00001635
Iteration 173/1000 | Loss: 0.00001635
Iteration 174/1000 | Loss: 0.00001635
Iteration 175/1000 | Loss: 0.00001635
Iteration 176/1000 | Loss: 0.00001635
Iteration 177/1000 | Loss: 0.00001635
Iteration 178/1000 | Loss: 0.00001635
Iteration 179/1000 | Loss: 0.00001635
Iteration 180/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6351081285392866e-05, 1.6351081285392866e-05, 1.6351081285392866e-05, 1.6351081285392866e-05, 1.6351081285392866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6351081285392866e-05

Optimization complete. Final v2v error: 3.3295650482177734 mm

Highest mean error: 4.171104907989502 mm for frame 147

Lowest mean error: 2.5336990356445312 mm for frame 3

Saving results

Total time: 40.832252740859985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00656315
Iteration 2/25 | Loss: 0.00129101
Iteration 3/25 | Loss: 0.00119518
Iteration 4/25 | Loss: 0.00118058
Iteration 5/25 | Loss: 0.00117571
Iteration 6/25 | Loss: 0.00117571
Iteration 7/25 | Loss: 0.00117571
Iteration 8/25 | Loss: 0.00117571
Iteration 9/25 | Loss: 0.00117571
Iteration 10/25 | Loss: 0.00117571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001175707788206637, 0.001175707788206637, 0.001175707788206637, 0.001175707788206637, 0.001175707788206637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175707788206637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36140251
Iteration 2/25 | Loss: 0.00085668
Iteration 3/25 | Loss: 0.00085667
Iteration 4/25 | Loss: 0.00085666
Iteration 5/25 | Loss: 0.00085666
Iteration 6/25 | Loss: 0.00085666
Iteration 7/25 | Loss: 0.00085666
Iteration 8/25 | Loss: 0.00085666
Iteration 9/25 | Loss: 0.00085666
Iteration 10/25 | Loss: 0.00085666
Iteration 11/25 | Loss: 0.00085666
Iteration 12/25 | Loss: 0.00085666
Iteration 13/25 | Loss: 0.00085666
Iteration 14/25 | Loss: 0.00085666
Iteration 15/25 | Loss: 0.00085666
Iteration 16/25 | Loss: 0.00085666
Iteration 17/25 | Loss: 0.00085666
Iteration 18/25 | Loss: 0.00085666
Iteration 19/25 | Loss: 0.00085666
Iteration 20/25 | Loss: 0.00085666
Iteration 21/25 | Loss: 0.00085666
Iteration 22/25 | Loss: 0.00085666
Iteration 23/25 | Loss: 0.00085666
Iteration 24/25 | Loss: 0.00085666
Iteration 25/25 | Loss: 0.00085666
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008566623437218368, 0.0008566623437218368, 0.0008566623437218368, 0.0008566623437218368, 0.0008566623437218368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008566623437218368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085666
Iteration 2/1000 | Loss: 0.00004032
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002226
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002110
Iteration 7/1000 | Loss: 0.00002065
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001917
Iteration 16/1000 | Loss: 0.00001913
Iteration 17/1000 | Loss: 0.00001910
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001900
Iteration 22/1000 | Loss: 0.00001900
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001899
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001898
Iteration 29/1000 | Loss: 0.00001898
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001898
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001895
Iteration 56/1000 | Loss: 0.00001895
Iteration 57/1000 | Loss: 0.00001895
Iteration 58/1000 | Loss: 0.00001895
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001894
Iteration 61/1000 | Loss: 0.00001894
Iteration 62/1000 | Loss: 0.00001894
Iteration 63/1000 | Loss: 0.00001894
Iteration 64/1000 | Loss: 0.00001894
Iteration 65/1000 | Loss: 0.00001894
Iteration 66/1000 | Loss: 0.00001894
Iteration 67/1000 | Loss: 0.00001894
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001893
Iteration 71/1000 | Loss: 0.00001893
Iteration 72/1000 | Loss: 0.00001893
Iteration 73/1000 | Loss: 0.00001893
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001893
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001893
Iteration 79/1000 | Loss: 0.00001892
Iteration 80/1000 | Loss: 0.00001892
Iteration 81/1000 | Loss: 0.00001892
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8911261577159166e-05, 1.8911261577159166e-05, 1.8911261577159166e-05, 1.8911261577159166e-05, 1.8911261577159166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8911261577159166e-05

Optimization complete. Final v2v error: 3.679253339767456 mm

Highest mean error: 4.019877910614014 mm for frame 217

Lowest mean error: 3.3846418857574463 mm for frame 230

Saving results

Total time: 37.658053398132324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642178
Iteration 2/25 | Loss: 0.00131080
Iteration 3/25 | Loss: 0.00120619
Iteration 4/25 | Loss: 0.00119522
Iteration 5/25 | Loss: 0.00119226
Iteration 6/25 | Loss: 0.00119200
Iteration 7/25 | Loss: 0.00119200
Iteration 8/25 | Loss: 0.00119200
Iteration 9/25 | Loss: 0.00119200
Iteration 10/25 | Loss: 0.00119200
Iteration 11/25 | Loss: 0.00119200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011920006945729256, 0.0011920006945729256, 0.0011920006945729256, 0.0011920006945729256, 0.0011920006945729256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011920006945729256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07675886
Iteration 2/25 | Loss: 0.00079685
Iteration 3/25 | Loss: 0.00079678
Iteration 4/25 | Loss: 0.00079678
Iteration 5/25 | Loss: 0.00079678
Iteration 6/25 | Loss: 0.00079678
Iteration 7/25 | Loss: 0.00079678
Iteration 8/25 | Loss: 0.00079678
Iteration 9/25 | Loss: 0.00079678
Iteration 10/25 | Loss: 0.00079678
Iteration 11/25 | Loss: 0.00079678
Iteration 12/25 | Loss: 0.00079678
Iteration 13/25 | Loss: 0.00079678
Iteration 14/25 | Loss: 0.00079678
Iteration 15/25 | Loss: 0.00079678
Iteration 16/25 | Loss: 0.00079678
Iteration 17/25 | Loss: 0.00079678
Iteration 18/25 | Loss: 0.00079678
Iteration 19/25 | Loss: 0.00079678
Iteration 20/25 | Loss: 0.00079678
Iteration 21/25 | Loss: 0.00079678
Iteration 22/25 | Loss: 0.00079678
Iteration 23/25 | Loss: 0.00079678
Iteration 24/25 | Loss: 0.00079678
Iteration 25/25 | Loss: 0.00079678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079678
Iteration 2/1000 | Loss: 0.00002491
Iteration 3/1000 | Loss: 0.00001908
Iteration 4/1000 | Loss: 0.00001709
Iteration 5/1000 | Loss: 0.00001631
Iteration 6/1000 | Loss: 0.00001580
Iteration 7/1000 | Loss: 0.00001544
Iteration 8/1000 | Loss: 0.00001511
Iteration 9/1000 | Loss: 0.00001481
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001444
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001387
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001384
Iteration 29/1000 | Loss: 0.00001382
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001378
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Iteration 44/1000 | Loss: 0.00001372
Iteration 45/1000 | Loss: 0.00001372
Iteration 46/1000 | Loss: 0.00001372
Iteration 47/1000 | Loss: 0.00001372
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001371
Iteration 50/1000 | Loss: 0.00001371
Iteration 51/1000 | Loss: 0.00001371
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001369
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001366
Iteration 71/1000 | Loss: 0.00001366
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001364
Iteration 74/1000 | Loss: 0.00001364
Iteration 75/1000 | Loss: 0.00001364
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001363
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001362
Iteration 84/1000 | Loss: 0.00001362
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001357
Iteration 109/1000 | Loss: 0.00001357
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001357
Iteration 112/1000 | Loss: 0.00001357
Iteration 113/1000 | Loss: 0.00001357
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001356
Iteration 118/1000 | Loss: 0.00001356
Iteration 119/1000 | Loss: 0.00001356
Iteration 120/1000 | Loss: 0.00001356
Iteration 121/1000 | Loss: 0.00001356
Iteration 122/1000 | Loss: 0.00001356
Iteration 123/1000 | Loss: 0.00001355
Iteration 124/1000 | Loss: 0.00001355
Iteration 125/1000 | Loss: 0.00001355
Iteration 126/1000 | Loss: 0.00001355
Iteration 127/1000 | Loss: 0.00001355
Iteration 128/1000 | Loss: 0.00001355
Iteration 129/1000 | Loss: 0.00001355
Iteration 130/1000 | Loss: 0.00001355
Iteration 131/1000 | Loss: 0.00001355
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001355
Iteration 137/1000 | Loss: 0.00001355
Iteration 138/1000 | Loss: 0.00001355
Iteration 139/1000 | Loss: 0.00001355
Iteration 140/1000 | Loss: 0.00001355
Iteration 141/1000 | Loss: 0.00001355
Iteration 142/1000 | Loss: 0.00001355
Iteration 143/1000 | Loss: 0.00001355
Iteration 144/1000 | Loss: 0.00001355
Iteration 145/1000 | Loss: 0.00001355
Iteration 146/1000 | Loss: 0.00001355
Iteration 147/1000 | Loss: 0.00001355
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001355
Iteration 150/1000 | Loss: 0.00001355
Iteration 151/1000 | Loss: 0.00001355
Iteration 152/1000 | Loss: 0.00001355
Iteration 153/1000 | Loss: 0.00001355
Iteration 154/1000 | Loss: 0.00001355
Iteration 155/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3546107766160276e-05, 1.3546107766160276e-05, 1.3546107766160276e-05, 1.3546107766160276e-05, 1.3546107766160276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3546107766160276e-05

Optimization complete. Final v2v error: 3.1066877841949463 mm

Highest mean error: 3.7090022563934326 mm for frame 140

Lowest mean error: 2.8764915466308594 mm for frame 15

Saving results

Total time: 40.17923831939697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880983
Iteration 2/25 | Loss: 0.00165738
Iteration 3/25 | Loss: 0.00134939
Iteration 4/25 | Loss: 0.00129757
Iteration 5/25 | Loss: 0.00128698
Iteration 6/25 | Loss: 0.00129065
Iteration 7/25 | Loss: 0.00128107
Iteration 8/25 | Loss: 0.00125873
Iteration 9/25 | Loss: 0.00124138
Iteration 10/25 | Loss: 0.00123849
Iteration 11/25 | Loss: 0.00122873
Iteration 12/25 | Loss: 0.00122815
Iteration 13/25 | Loss: 0.00123076
Iteration 14/25 | Loss: 0.00123966
Iteration 15/25 | Loss: 0.00123460
Iteration 16/25 | Loss: 0.00122894
Iteration 17/25 | Loss: 0.00122398
Iteration 18/25 | Loss: 0.00122076
Iteration 19/25 | Loss: 0.00121841
Iteration 20/25 | Loss: 0.00121793
Iteration 21/25 | Loss: 0.00121779
Iteration 22/25 | Loss: 0.00121778
Iteration 23/25 | Loss: 0.00121778
Iteration 24/25 | Loss: 0.00121778
Iteration 25/25 | Loss: 0.00121778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24844503
Iteration 2/25 | Loss: 0.00114555
Iteration 3/25 | Loss: 0.00114554
Iteration 4/25 | Loss: 0.00114553
Iteration 5/25 | Loss: 0.00114553
Iteration 6/25 | Loss: 0.00114553
Iteration 7/25 | Loss: 0.00114553
Iteration 8/25 | Loss: 0.00114553
Iteration 9/25 | Loss: 0.00114553
Iteration 10/25 | Loss: 0.00114553
Iteration 11/25 | Loss: 0.00114553
Iteration 12/25 | Loss: 0.00114553
Iteration 13/25 | Loss: 0.00114553
Iteration 14/25 | Loss: 0.00114553
Iteration 15/25 | Loss: 0.00114553
Iteration 16/25 | Loss: 0.00114553
Iteration 17/25 | Loss: 0.00114553
Iteration 18/25 | Loss: 0.00114553
Iteration 19/25 | Loss: 0.00114553
Iteration 20/25 | Loss: 0.00114553
Iteration 21/25 | Loss: 0.00114553
Iteration 22/25 | Loss: 0.00114553
Iteration 23/25 | Loss: 0.00114553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011455310741439462, 0.0011455310741439462, 0.0011455310741439462, 0.0011455310741439462, 0.0011455310741439462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011455310741439462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114553
Iteration 2/1000 | Loss: 0.00013904
Iteration 3/1000 | Loss: 0.00029225
Iteration 4/1000 | Loss: 0.00004373
Iteration 5/1000 | Loss: 0.00003070
Iteration 6/1000 | Loss: 0.00002651
Iteration 7/1000 | Loss: 0.00002503
Iteration 8/1000 | Loss: 0.00002430
Iteration 9/1000 | Loss: 0.00002336
Iteration 10/1000 | Loss: 0.00002266
Iteration 11/1000 | Loss: 0.00002209
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002151
Iteration 14/1000 | Loss: 0.00002132
Iteration 15/1000 | Loss: 0.00002119
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002084
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002057
Iteration 24/1000 | Loss: 0.00002057
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002052
Iteration 28/1000 | Loss: 0.00002050
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002048
Iteration 31/1000 | Loss: 0.00002047
Iteration 32/1000 | Loss: 0.00002046
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002045
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002044
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002043
Iteration 41/1000 | Loss: 0.00002042
Iteration 42/1000 | Loss: 0.00002042
Iteration 43/1000 | Loss: 0.00002042
Iteration 44/1000 | Loss: 0.00002041
Iteration 45/1000 | Loss: 0.00002041
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002040
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002040
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002038
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002036
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002036
Iteration 71/1000 | Loss: 0.00002036
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002035
Iteration 74/1000 | Loss: 0.00002035
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00002034
Iteration 77/1000 | Loss: 0.00002034
Iteration 78/1000 | Loss: 0.00002034
Iteration 79/1000 | Loss: 0.00002034
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002034
Iteration 86/1000 | Loss: 0.00002034
Iteration 87/1000 | Loss: 0.00002034
Iteration 88/1000 | Loss: 0.00002034
Iteration 89/1000 | Loss: 0.00002034
Iteration 90/1000 | Loss: 0.00002033
Iteration 91/1000 | Loss: 0.00002033
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002032
Iteration 94/1000 | Loss: 0.00002032
Iteration 95/1000 | Loss: 0.00002032
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002031
Iteration 100/1000 | Loss: 0.00002031
Iteration 101/1000 | Loss: 0.00002031
Iteration 102/1000 | Loss: 0.00002031
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002029
Iteration 109/1000 | Loss: 0.00002029
Iteration 110/1000 | Loss: 0.00002029
Iteration 111/1000 | Loss: 0.00002029
Iteration 112/1000 | Loss: 0.00002029
Iteration 113/1000 | Loss: 0.00002029
Iteration 114/1000 | Loss: 0.00002029
Iteration 115/1000 | Loss: 0.00002029
Iteration 116/1000 | Loss: 0.00002028
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002028
Iteration 119/1000 | Loss: 0.00002027
Iteration 120/1000 | Loss: 0.00002027
Iteration 121/1000 | Loss: 0.00002027
Iteration 122/1000 | Loss: 0.00002027
Iteration 123/1000 | Loss: 0.00002027
Iteration 124/1000 | Loss: 0.00002027
Iteration 125/1000 | Loss: 0.00002026
Iteration 126/1000 | Loss: 0.00002026
Iteration 127/1000 | Loss: 0.00002026
Iteration 128/1000 | Loss: 0.00002025
Iteration 129/1000 | Loss: 0.00002025
Iteration 130/1000 | Loss: 0.00002025
Iteration 131/1000 | Loss: 0.00002025
Iteration 132/1000 | Loss: 0.00002025
Iteration 133/1000 | Loss: 0.00002025
Iteration 134/1000 | Loss: 0.00002025
Iteration 135/1000 | Loss: 0.00002025
Iteration 136/1000 | Loss: 0.00002025
Iteration 137/1000 | Loss: 0.00002025
Iteration 138/1000 | Loss: 0.00002025
Iteration 139/1000 | Loss: 0.00002025
Iteration 140/1000 | Loss: 0.00002025
Iteration 141/1000 | Loss: 0.00002025
Iteration 142/1000 | Loss: 0.00002025
Iteration 143/1000 | Loss: 0.00002025
Iteration 144/1000 | Loss: 0.00002025
Iteration 145/1000 | Loss: 0.00002025
Iteration 146/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.0245983250788413e-05, 2.0245983250788413e-05, 2.0245983250788413e-05, 2.0245983250788413e-05, 2.0245983250788413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0245983250788413e-05

Optimization complete. Final v2v error: 3.7469351291656494 mm

Highest mean error: 5.729187965393066 mm for frame 106

Lowest mean error: 3.2015466690063477 mm for frame 134

Saving results

Total time: 76.87185549736023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114833
Iteration 2/25 | Loss: 0.00195287
Iteration 3/25 | Loss: 0.00161808
Iteration 4/25 | Loss: 0.00140846
Iteration 5/25 | Loss: 0.00138238
Iteration 6/25 | Loss: 0.00135901
Iteration 7/25 | Loss: 0.00134675
Iteration 8/25 | Loss: 0.00134370
Iteration 9/25 | Loss: 0.00134458
Iteration 10/25 | Loss: 0.00134104
Iteration 11/25 | Loss: 0.00133812
Iteration 12/25 | Loss: 0.00133578
Iteration 13/25 | Loss: 0.00133391
Iteration 14/25 | Loss: 0.00133727
Iteration 15/25 | Loss: 0.00133620
Iteration 16/25 | Loss: 0.00133482
Iteration 17/25 | Loss: 0.00133394
Iteration 18/25 | Loss: 0.00133517
Iteration 19/25 | Loss: 0.00133377
Iteration 20/25 | Loss: 0.00133247
Iteration 21/25 | Loss: 0.00133224
Iteration 22/25 | Loss: 0.00133223
Iteration 23/25 | Loss: 0.00133222
Iteration 24/25 | Loss: 0.00133222
Iteration 25/25 | Loss: 0.00133222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84568959
Iteration 2/25 | Loss: 0.00088705
Iteration 3/25 | Loss: 0.00088705
Iteration 4/25 | Loss: 0.00088705
Iteration 5/25 | Loss: 0.00088705
Iteration 6/25 | Loss: 0.00088705
Iteration 7/25 | Loss: 0.00088705
Iteration 8/25 | Loss: 0.00088704
Iteration 9/25 | Loss: 0.00088704
Iteration 10/25 | Loss: 0.00088704
Iteration 11/25 | Loss: 0.00088704
Iteration 12/25 | Loss: 0.00088704
Iteration 13/25 | Loss: 0.00088704
Iteration 14/25 | Loss: 0.00088704
Iteration 15/25 | Loss: 0.00088704
Iteration 16/25 | Loss: 0.00088704
Iteration 17/25 | Loss: 0.00088704
Iteration 18/25 | Loss: 0.00088704
Iteration 19/25 | Loss: 0.00088704
Iteration 20/25 | Loss: 0.00088704
Iteration 21/25 | Loss: 0.00088704
Iteration 22/25 | Loss: 0.00088704
Iteration 23/25 | Loss: 0.00088704
Iteration 24/25 | Loss: 0.00088704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008870439487509429, 0.0008870439487509429, 0.0008870439487509429, 0.0008870439487509429, 0.0008870439487509429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008870439487509429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088704
Iteration 2/1000 | Loss: 0.00022937
Iteration 3/1000 | Loss: 0.00005945
Iteration 4/1000 | Loss: 0.00005204
Iteration 5/1000 | Loss: 0.00011874
Iteration 6/1000 | Loss: 0.00004528
Iteration 7/1000 | Loss: 0.00026990
Iteration 8/1000 | Loss: 0.00024502
Iteration 9/1000 | Loss: 0.00005247
Iteration 10/1000 | Loss: 0.00004613
Iteration 11/1000 | Loss: 0.00004214
Iteration 12/1000 | Loss: 0.00004040
Iteration 13/1000 | Loss: 0.00003887
Iteration 14/1000 | Loss: 0.00003795
Iteration 15/1000 | Loss: 0.00003741
Iteration 16/1000 | Loss: 0.00003714
Iteration 17/1000 | Loss: 0.00003684
Iteration 18/1000 | Loss: 0.00003656
Iteration 19/1000 | Loss: 0.00003635
Iteration 20/1000 | Loss: 0.00003611
Iteration 21/1000 | Loss: 0.00003594
Iteration 22/1000 | Loss: 0.00003573
Iteration 23/1000 | Loss: 0.00003555
Iteration 24/1000 | Loss: 0.00003548
Iteration 25/1000 | Loss: 0.00003540
Iteration 26/1000 | Loss: 0.00003540
Iteration 27/1000 | Loss: 0.00003538
Iteration 28/1000 | Loss: 0.00003537
Iteration 29/1000 | Loss: 0.00003536
Iteration 30/1000 | Loss: 0.00003536
Iteration 31/1000 | Loss: 0.00003535
Iteration 32/1000 | Loss: 0.00003535
Iteration 33/1000 | Loss: 0.00003534
Iteration 34/1000 | Loss: 0.00003534
Iteration 35/1000 | Loss: 0.00003534
Iteration 36/1000 | Loss: 0.00003533
Iteration 37/1000 | Loss: 0.00003532
Iteration 38/1000 | Loss: 0.00003532
Iteration 39/1000 | Loss: 0.00003531
Iteration 40/1000 | Loss: 0.00003529
Iteration 41/1000 | Loss: 0.00003529
Iteration 42/1000 | Loss: 0.00003528
Iteration 43/1000 | Loss: 0.00003526
Iteration 44/1000 | Loss: 0.00003526
Iteration 45/1000 | Loss: 0.00003521
Iteration 46/1000 | Loss: 0.00003520
Iteration 47/1000 | Loss: 0.00003519
Iteration 48/1000 | Loss: 0.00003513
Iteration 49/1000 | Loss: 0.00003511
Iteration 50/1000 | Loss: 0.00003505
Iteration 51/1000 | Loss: 0.00003494
Iteration 52/1000 | Loss: 0.00003490
Iteration 53/1000 | Loss: 0.00003490
Iteration 54/1000 | Loss: 0.00003490
Iteration 55/1000 | Loss: 0.00003490
Iteration 56/1000 | Loss: 0.00003490
Iteration 57/1000 | Loss: 0.00003489
Iteration 58/1000 | Loss: 0.00003489
Iteration 59/1000 | Loss: 0.00003489
Iteration 60/1000 | Loss: 0.00003489
Iteration 61/1000 | Loss: 0.00003488
Iteration 62/1000 | Loss: 0.00003486
Iteration 63/1000 | Loss: 0.00003483
Iteration 64/1000 | Loss: 0.00003483
Iteration 65/1000 | Loss: 0.00003483
Iteration 66/1000 | Loss: 0.00003483
Iteration 67/1000 | Loss: 0.00003483
Iteration 68/1000 | Loss: 0.00003483
Iteration 69/1000 | Loss: 0.00003482
Iteration 70/1000 | Loss: 0.00003482
Iteration 71/1000 | Loss: 0.00003482
Iteration 72/1000 | Loss: 0.00003481
Iteration 73/1000 | Loss: 0.00003480
Iteration 74/1000 | Loss: 0.00003479
Iteration 75/1000 | Loss: 0.00003479
Iteration 76/1000 | Loss: 0.00003478
Iteration 77/1000 | Loss: 0.00003477
Iteration 78/1000 | Loss: 0.00003477
Iteration 79/1000 | Loss: 0.00003477
Iteration 80/1000 | Loss: 0.00003476
Iteration 81/1000 | Loss: 0.00003476
Iteration 82/1000 | Loss: 0.00003476
Iteration 83/1000 | Loss: 0.00003476
Iteration 84/1000 | Loss: 0.00003476
Iteration 85/1000 | Loss: 0.00003476
Iteration 86/1000 | Loss: 0.00003475
Iteration 87/1000 | Loss: 0.00003475
Iteration 88/1000 | Loss: 0.00003475
Iteration 89/1000 | Loss: 0.00003475
Iteration 90/1000 | Loss: 0.00003475
Iteration 91/1000 | Loss: 0.00003475
Iteration 92/1000 | Loss: 0.00003475
Iteration 93/1000 | Loss: 0.00003475
Iteration 94/1000 | Loss: 0.00003474
Iteration 95/1000 | Loss: 0.00003474
Iteration 96/1000 | Loss: 0.00003474
Iteration 97/1000 | Loss: 0.00003474
Iteration 98/1000 | Loss: 0.00003474
Iteration 99/1000 | Loss: 0.00003474
Iteration 100/1000 | Loss: 0.00003474
Iteration 101/1000 | Loss: 0.00003474
Iteration 102/1000 | Loss: 0.00003474
Iteration 103/1000 | Loss: 0.00003474
Iteration 104/1000 | Loss: 0.00003474
Iteration 105/1000 | Loss: 0.00003474
Iteration 106/1000 | Loss: 0.00003474
Iteration 107/1000 | Loss: 0.00003473
Iteration 108/1000 | Loss: 0.00003473
Iteration 109/1000 | Loss: 0.00003473
Iteration 110/1000 | Loss: 0.00003473
Iteration 111/1000 | Loss: 0.00003472
Iteration 112/1000 | Loss: 0.00003472
Iteration 113/1000 | Loss: 0.00003472
Iteration 114/1000 | Loss: 0.00003472
Iteration 115/1000 | Loss: 0.00003472
Iteration 116/1000 | Loss: 0.00003472
Iteration 117/1000 | Loss: 0.00003472
Iteration 118/1000 | Loss: 0.00003472
Iteration 119/1000 | Loss: 0.00003472
Iteration 120/1000 | Loss: 0.00003471
Iteration 121/1000 | Loss: 0.00003471
Iteration 122/1000 | Loss: 0.00003471
Iteration 123/1000 | Loss: 0.00003471
Iteration 124/1000 | Loss: 0.00003471
Iteration 125/1000 | Loss: 0.00003471
Iteration 126/1000 | Loss: 0.00003471
Iteration 127/1000 | Loss: 0.00003471
Iteration 128/1000 | Loss: 0.00003470
Iteration 129/1000 | Loss: 0.00003470
Iteration 130/1000 | Loss: 0.00003470
Iteration 131/1000 | Loss: 0.00003470
Iteration 132/1000 | Loss: 0.00003469
Iteration 133/1000 | Loss: 0.00003469
Iteration 134/1000 | Loss: 0.00003469
Iteration 135/1000 | Loss: 0.00003469
Iteration 136/1000 | Loss: 0.00003469
Iteration 137/1000 | Loss: 0.00003469
Iteration 138/1000 | Loss: 0.00003469
Iteration 139/1000 | Loss: 0.00003469
Iteration 140/1000 | Loss: 0.00003469
Iteration 141/1000 | Loss: 0.00003469
Iteration 142/1000 | Loss: 0.00003469
Iteration 143/1000 | Loss: 0.00003468
Iteration 144/1000 | Loss: 0.00003468
Iteration 145/1000 | Loss: 0.00003468
Iteration 146/1000 | Loss: 0.00003468
Iteration 147/1000 | Loss: 0.00003468
Iteration 148/1000 | Loss: 0.00003468
Iteration 149/1000 | Loss: 0.00003468
Iteration 150/1000 | Loss: 0.00003468
Iteration 151/1000 | Loss: 0.00003468
Iteration 152/1000 | Loss: 0.00003468
Iteration 153/1000 | Loss: 0.00003468
Iteration 154/1000 | Loss: 0.00003468
Iteration 155/1000 | Loss: 0.00003468
Iteration 156/1000 | Loss: 0.00003468
Iteration 157/1000 | Loss: 0.00003468
Iteration 158/1000 | Loss: 0.00003468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.4683122066780925e-05, 3.4683122066780925e-05, 3.4683122066780925e-05, 3.4683122066780925e-05, 3.4683122066780925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4683122066780925e-05

Optimization complete. Final v2v error: 4.561267375946045 mm

Highest mean error: 5.109465599060059 mm for frame 0

Lowest mean error: 3.8739185333251953 mm for frame 139

Saving results

Total time: 97.83255457878113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842368
Iteration 2/25 | Loss: 0.00123481
Iteration 3/25 | Loss: 0.00114919
Iteration 4/25 | Loss: 0.00113903
Iteration 5/25 | Loss: 0.00113661
Iteration 6/25 | Loss: 0.00113661
Iteration 7/25 | Loss: 0.00113661
Iteration 8/25 | Loss: 0.00113661
Iteration 9/25 | Loss: 0.00113661
Iteration 10/25 | Loss: 0.00113661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001136614358983934, 0.001136614358983934, 0.001136614358983934, 0.001136614358983934, 0.001136614358983934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001136614358983934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.48133516
Iteration 2/25 | Loss: 0.00081299
Iteration 3/25 | Loss: 0.00081298
Iteration 4/25 | Loss: 0.00081298
Iteration 5/25 | Loss: 0.00081298
Iteration 6/25 | Loss: 0.00081298
Iteration 7/25 | Loss: 0.00081298
Iteration 8/25 | Loss: 0.00081298
Iteration 9/25 | Loss: 0.00081298
Iteration 10/25 | Loss: 0.00081298
Iteration 11/25 | Loss: 0.00081298
Iteration 12/25 | Loss: 0.00081298
Iteration 13/25 | Loss: 0.00081298
Iteration 14/25 | Loss: 0.00081298
Iteration 15/25 | Loss: 0.00081298
Iteration 16/25 | Loss: 0.00081298
Iteration 17/25 | Loss: 0.00081298
Iteration 18/25 | Loss: 0.00081298
Iteration 19/25 | Loss: 0.00081298
Iteration 20/25 | Loss: 0.00081298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008129761554300785, 0.0008129761554300785, 0.0008129761554300785, 0.0008129761554300785, 0.0008129761554300785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008129761554300785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081298
Iteration 2/1000 | Loss: 0.00002598
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001441
Iteration 10/1000 | Loss: 0.00001415
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001386
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001368
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001323
Iteration 89/1000 | Loss: 0.00001323
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001322
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001322
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001317
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001316
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001315
Iteration 150/1000 | Loss: 0.00001315
Iteration 151/1000 | Loss: 0.00001315
Iteration 152/1000 | Loss: 0.00001315
Iteration 153/1000 | Loss: 0.00001315
Iteration 154/1000 | Loss: 0.00001315
Iteration 155/1000 | Loss: 0.00001315
Iteration 156/1000 | Loss: 0.00001315
Iteration 157/1000 | Loss: 0.00001315
Iteration 158/1000 | Loss: 0.00001315
Iteration 159/1000 | Loss: 0.00001315
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Iteration 162/1000 | Loss: 0.00001314
Iteration 163/1000 | Loss: 0.00001314
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001314
Iteration 169/1000 | Loss: 0.00001314
Iteration 170/1000 | Loss: 0.00001314
Iteration 171/1000 | Loss: 0.00001314
Iteration 172/1000 | Loss: 0.00001313
Iteration 173/1000 | Loss: 0.00001313
Iteration 174/1000 | Loss: 0.00001313
Iteration 175/1000 | Loss: 0.00001313
Iteration 176/1000 | Loss: 0.00001313
Iteration 177/1000 | Loss: 0.00001313
Iteration 178/1000 | Loss: 0.00001313
Iteration 179/1000 | Loss: 0.00001313
Iteration 180/1000 | Loss: 0.00001313
Iteration 181/1000 | Loss: 0.00001313
Iteration 182/1000 | Loss: 0.00001313
Iteration 183/1000 | Loss: 0.00001313
Iteration 184/1000 | Loss: 0.00001313
Iteration 185/1000 | Loss: 0.00001313
Iteration 186/1000 | Loss: 0.00001313
Iteration 187/1000 | Loss: 0.00001313
Iteration 188/1000 | Loss: 0.00001313
Iteration 189/1000 | Loss: 0.00001313
Iteration 190/1000 | Loss: 0.00001313
Iteration 191/1000 | Loss: 0.00001313
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001313
Iteration 194/1000 | Loss: 0.00001313
Iteration 195/1000 | Loss: 0.00001313
Iteration 196/1000 | Loss: 0.00001313
Iteration 197/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.3133148968336172e-05, 1.3133148968336172e-05, 1.3133148968336172e-05, 1.3133148968336172e-05, 1.3133148968336172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3133148968336172e-05

Optimization complete. Final v2v error: 3.0923268795013428 mm

Highest mean error: 3.4544320106506348 mm for frame 111

Lowest mean error: 2.723932981491089 mm for frame 167

Saving results

Total time: 41.18831133842468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774362
Iteration 2/25 | Loss: 0.00181543
Iteration 3/25 | Loss: 0.00131351
Iteration 4/25 | Loss: 0.00118031
Iteration 5/25 | Loss: 0.00115541
Iteration 6/25 | Loss: 0.00115092
Iteration 7/25 | Loss: 0.00114133
Iteration 8/25 | Loss: 0.00112630
Iteration 9/25 | Loss: 0.00112085
Iteration 10/25 | Loss: 0.00111848
Iteration 11/25 | Loss: 0.00111762
Iteration 12/25 | Loss: 0.00111730
Iteration 13/25 | Loss: 0.00111725
Iteration 14/25 | Loss: 0.00111725
Iteration 15/25 | Loss: 0.00111725
Iteration 16/25 | Loss: 0.00111724
Iteration 17/25 | Loss: 0.00111724
Iteration 18/25 | Loss: 0.00111724
Iteration 19/25 | Loss: 0.00111724
Iteration 20/25 | Loss: 0.00111724
Iteration 21/25 | Loss: 0.00111724
Iteration 22/25 | Loss: 0.00111724
Iteration 23/25 | Loss: 0.00111724
Iteration 24/25 | Loss: 0.00111724
Iteration 25/25 | Loss: 0.00111724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89243305
Iteration 2/25 | Loss: 0.00093544
Iteration 3/25 | Loss: 0.00085245
Iteration 4/25 | Loss: 0.00085245
Iteration 5/25 | Loss: 0.00085245
Iteration 6/25 | Loss: 0.00085245
Iteration 7/25 | Loss: 0.00085245
Iteration 8/25 | Loss: 0.00085245
Iteration 9/25 | Loss: 0.00085245
Iteration 10/25 | Loss: 0.00085245
Iteration 11/25 | Loss: 0.00085245
Iteration 12/25 | Loss: 0.00085244
Iteration 13/25 | Loss: 0.00085244
Iteration 14/25 | Loss: 0.00085244
Iteration 15/25 | Loss: 0.00085244
Iteration 16/25 | Loss: 0.00085244
Iteration 17/25 | Loss: 0.00085244
Iteration 18/25 | Loss: 0.00085244
Iteration 19/25 | Loss: 0.00085244
Iteration 20/25 | Loss: 0.00085244
Iteration 21/25 | Loss: 0.00085244
Iteration 22/25 | Loss: 0.00085244
Iteration 23/25 | Loss: 0.00085244
Iteration 24/25 | Loss: 0.00085244
Iteration 25/25 | Loss: 0.00085244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085244
Iteration 2/1000 | Loss: 0.00007739
Iteration 3/1000 | Loss: 0.00006202
Iteration 4/1000 | Loss: 0.00002580
Iteration 5/1000 | Loss: 0.00001875
Iteration 6/1000 | Loss: 0.00019834
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00002261
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00002593
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001130
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001104
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001792
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001636
Iteration 23/1000 | Loss: 0.00003126
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001129
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001068
Iteration 28/1000 | Loss: 0.00001067
Iteration 29/1000 | Loss: 0.00001067
Iteration 30/1000 | Loss: 0.00001067
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001066
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001066
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001066
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001066
Iteration 43/1000 | Loss: 0.00001066
Iteration 44/1000 | Loss: 0.00001066
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001062
Iteration 66/1000 | Loss: 0.00001062
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001062
Iteration 69/1000 | Loss: 0.00001062
Iteration 70/1000 | Loss: 0.00001074
Iteration 71/1000 | Loss: 0.00001061
Iteration 72/1000 | Loss: 0.00001061
Iteration 73/1000 | Loss: 0.00001061
Iteration 74/1000 | Loss: 0.00001061
Iteration 75/1000 | Loss: 0.00001061
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001060
Iteration 80/1000 | Loss: 0.00001058
Iteration 81/1000 | Loss: 0.00001058
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001055
Iteration 93/1000 | Loss: 0.00001054
Iteration 94/1000 | Loss: 0.00001054
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001053
Iteration 97/1000 | Loss: 0.00001053
Iteration 98/1000 | Loss: 0.00001053
Iteration 99/1000 | Loss: 0.00001053
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001052
Iteration 102/1000 | Loss: 0.00001052
Iteration 103/1000 | Loss: 0.00001052
Iteration 104/1000 | Loss: 0.00001051
Iteration 105/1000 | Loss: 0.00001051
Iteration 106/1000 | Loss: 0.00001051
Iteration 107/1000 | Loss: 0.00001051
Iteration 108/1000 | Loss: 0.00001051
Iteration 109/1000 | Loss: 0.00001051
Iteration 110/1000 | Loss: 0.00001051
Iteration 111/1000 | Loss: 0.00001051
Iteration 112/1000 | Loss: 0.00001050
Iteration 113/1000 | Loss: 0.00001050
Iteration 114/1000 | Loss: 0.00001050
Iteration 115/1000 | Loss: 0.00001050
Iteration 116/1000 | Loss: 0.00001050
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001050
Iteration 120/1000 | Loss: 0.00001049
Iteration 121/1000 | Loss: 0.00001049
Iteration 122/1000 | Loss: 0.00001049
Iteration 123/1000 | Loss: 0.00001049
Iteration 124/1000 | Loss: 0.00001049
Iteration 125/1000 | Loss: 0.00001049
Iteration 126/1000 | Loss: 0.00001049
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001048
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001047
Iteration 132/1000 | Loss: 0.00001047
Iteration 133/1000 | Loss: 0.00001047
Iteration 134/1000 | Loss: 0.00001047
Iteration 135/1000 | Loss: 0.00001047
Iteration 136/1000 | Loss: 0.00001047
Iteration 137/1000 | Loss: 0.00001047
Iteration 138/1000 | Loss: 0.00001047
Iteration 139/1000 | Loss: 0.00001047
Iteration 140/1000 | Loss: 0.00001047
Iteration 141/1000 | Loss: 0.00001046
Iteration 142/1000 | Loss: 0.00001046
Iteration 143/1000 | Loss: 0.00001046
Iteration 144/1000 | Loss: 0.00001046
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001046
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001046
Iteration 149/1000 | Loss: 0.00001046
Iteration 150/1000 | Loss: 0.00001046
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001046
Iteration 153/1000 | Loss: 0.00001045
Iteration 154/1000 | Loss: 0.00001045
Iteration 155/1000 | Loss: 0.00001045
Iteration 156/1000 | Loss: 0.00001045
Iteration 157/1000 | Loss: 0.00001045
Iteration 158/1000 | Loss: 0.00001045
Iteration 159/1000 | Loss: 0.00001045
Iteration 160/1000 | Loss: 0.00001045
Iteration 161/1000 | Loss: 0.00001045
Iteration 162/1000 | Loss: 0.00001044
Iteration 163/1000 | Loss: 0.00001044
Iteration 164/1000 | Loss: 0.00001044
Iteration 165/1000 | Loss: 0.00001044
Iteration 166/1000 | Loss: 0.00001044
Iteration 167/1000 | Loss: 0.00001044
Iteration 168/1000 | Loss: 0.00001044
Iteration 169/1000 | Loss: 0.00001044
Iteration 170/1000 | Loss: 0.00001044
Iteration 171/1000 | Loss: 0.00001044
Iteration 172/1000 | Loss: 0.00001044
Iteration 173/1000 | Loss: 0.00001044
Iteration 174/1000 | Loss: 0.00001043
Iteration 175/1000 | Loss: 0.00001043
Iteration 176/1000 | Loss: 0.00001043
Iteration 177/1000 | Loss: 0.00001043
Iteration 178/1000 | Loss: 0.00001042
Iteration 179/1000 | Loss: 0.00001042
Iteration 180/1000 | Loss: 0.00001042
Iteration 181/1000 | Loss: 0.00001042
Iteration 182/1000 | Loss: 0.00001042
Iteration 183/1000 | Loss: 0.00001042
Iteration 184/1000 | Loss: 0.00001042
Iteration 185/1000 | Loss: 0.00001042
Iteration 186/1000 | Loss: 0.00001042
Iteration 187/1000 | Loss: 0.00001042
Iteration 188/1000 | Loss: 0.00001042
Iteration 189/1000 | Loss: 0.00001042
Iteration 190/1000 | Loss: 0.00001042
Iteration 191/1000 | Loss: 0.00001042
Iteration 192/1000 | Loss: 0.00001042
Iteration 193/1000 | Loss: 0.00001042
Iteration 194/1000 | Loss: 0.00001042
Iteration 195/1000 | Loss: 0.00001041
Iteration 196/1000 | Loss: 0.00001041
Iteration 197/1000 | Loss: 0.00001041
Iteration 198/1000 | Loss: 0.00001041
Iteration 199/1000 | Loss: 0.00001040
Iteration 200/1000 | Loss: 0.00001040
Iteration 201/1000 | Loss: 0.00001040
Iteration 202/1000 | Loss: 0.00001040
Iteration 203/1000 | Loss: 0.00001040
Iteration 204/1000 | Loss: 0.00001039
Iteration 205/1000 | Loss: 0.00001039
Iteration 206/1000 | Loss: 0.00001039
Iteration 207/1000 | Loss: 0.00001039
Iteration 208/1000 | Loss: 0.00001039
Iteration 209/1000 | Loss: 0.00001039
Iteration 210/1000 | Loss: 0.00001039
Iteration 211/1000 | Loss: 0.00001039
Iteration 212/1000 | Loss: 0.00001038
Iteration 213/1000 | Loss: 0.00001038
Iteration 214/1000 | Loss: 0.00001038
Iteration 215/1000 | Loss: 0.00001038
Iteration 216/1000 | Loss: 0.00001038
Iteration 217/1000 | Loss: 0.00001038
Iteration 218/1000 | Loss: 0.00001038
Iteration 219/1000 | Loss: 0.00001038
Iteration 220/1000 | Loss: 0.00001038
Iteration 221/1000 | Loss: 0.00001038
Iteration 222/1000 | Loss: 0.00001038
Iteration 223/1000 | Loss: 0.00001038
Iteration 224/1000 | Loss: 0.00001038
Iteration 225/1000 | Loss: 0.00001038
Iteration 226/1000 | Loss: 0.00001038
Iteration 227/1000 | Loss: 0.00001038
Iteration 228/1000 | Loss: 0.00001038
Iteration 229/1000 | Loss: 0.00001038
Iteration 230/1000 | Loss: 0.00001038
Iteration 231/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.0377721082477365e-05, 1.0377721082477365e-05, 1.0377721082477365e-05, 1.0377721082477365e-05, 1.0377721082477365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0377721082477365e-05

Optimization complete. Final v2v error: 2.7728326320648193 mm

Highest mean error: 3.124162435531616 mm for frame 78

Lowest mean error: 2.555665969848633 mm for frame 3

Saving results

Total time: 69.44803309440613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598873
Iteration 2/25 | Loss: 0.00119797
Iteration 3/25 | Loss: 0.00113154
Iteration 4/25 | Loss: 0.00112063
Iteration 5/25 | Loss: 0.00111610
Iteration 6/25 | Loss: 0.00111507
Iteration 7/25 | Loss: 0.00111507
Iteration 8/25 | Loss: 0.00111507
Iteration 9/25 | Loss: 0.00111507
Iteration 10/25 | Loss: 0.00111507
Iteration 11/25 | Loss: 0.00111507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011150736827403307, 0.0011150736827403307, 0.0011150736827403307, 0.0011150736827403307, 0.0011150736827403307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011150736827403307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90351105
Iteration 2/25 | Loss: 0.00085763
Iteration 3/25 | Loss: 0.00085762
Iteration 4/25 | Loss: 0.00085762
Iteration 5/25 | Loss: 0.00085762
Iteration 6/25 | Loss: 0.00085762
Iteration 7/25 | Loss: 0.00085762
Iteration 8/25 | Loss: 0.00085762
Iteration 9/25 | Loss: 0.00085762
Iteration 10/25 | Loss: 0.00085762
Iteration 11/25 | Loss: 0.00085762
Iteration 12/25 | Loss: 0.00085762
Iteration 13/25 | Loss: 0.00085762
Iteration 14/25 | Loss: 0.00085762
Iteration 15/25 | Loss: 0.00085762
Iteration 16/25 | Loss: 0.00085762
Iteration 17/25 | Loss: 0.00085762
Iteration 18/25 | Loss: 0.00085762
Iteration 19/25 | Loss: 0.00085762
Iteration 20/25 | Loss: 0.00085762
Iteration 21/25 | Loss: 0.00085762
Iteration 22/25 | Loss: 0.00085762
Iteration 23/25 | Loss: 0.00085762
Iteration 24/25 | Loss: 0.00085762
Iteration 25/25 | Loss: 0.00085762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085762
Iteration 2/1000 | Loss: 0.00001937
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001227
Iteration 5/1000 | Loss: 0.00001167
Iteration 6/1000 | Loss: 0.00001138
Iteration 7/1000 | Loss: 0.00001104
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001102
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001088
Iteration 13/1000 | Loss: 0.00001066
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001048
Iteration 16/1000 | Loss: 0.00001045
Iteration 17/1000 | Loss: 0.00001033
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001017
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001014
Iteration 25/1000 | Loss: 0.00001014
Iteration 26/1000 | Loss: 0.00001014
Iteration 27/1000 | Loss: 0.00001014
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001010
Iteration 34/1000 | Loss: 0.00001010
Iteration 35/1000 | Loss: 0.00001009
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001006
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001005
Iteration 45/1000 | Loss: 0.00001005
Iteration 46/1000 | Loss: 0.00001005
Iteration 47/1000 | Loss: 0.00001005
Iteration 48/1000 | Loss: 0.00001005
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001001
Iteration 57/1000 | Loss: 0.00001001
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000997
Iteration 60/1000 | Loss: 0.00000997
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000997
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000996
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000996
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000995
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000993
Iteration 83/1000 | Loss: 0.00000993
Iteration 84/1000 | Loss: 0.00000993
Iteration 85/1000 | Loss: 0.00000992
Iteration 86/1000 | Loss: 0.00000992
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000991
Iteration 89/1000 | Loss: 0.00000990
Iteration 90/1000 | Loss: 0.00000990
Iteration 91/1000 | Loss: 0.00000989
Iteration 92/1000 | Loss: 0.00000989
Iteration 93/1000 | Loss: 0.00000989
Iteration 94/1000 | Loss: 0.00000989
Iteration 95/1000 | Loss: 0.00000989
Iteration 96/1000 | Loss: 0.00000988
Iteration 97/1000 | Loss: 0.00000988
Iteration 98/1000 | Loss: 0.00000988
Iteration 99/1000 | Loss: 0.00000988
Iteration 100/1000 | Loss: 0.00000987
Iteration 101/1000 | Loss: 0.00000987
Iteration 102/1000 | Loss: 0.00000987
Iteration 103/1000 | Loss: 0.00000987
Iteration 104/1000 | Loss: 0.00000986
Iteration 105/1000 | Loss: 0.00000986
Iteration 106/1000 | Loss: 0.00000986
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000985
Iteration 109/1000 | Loss: 0.00000985
Iteration 110/1000 | Loss: 0.00000985
Iteration 111/1000 | Loss: 0.00000984
Iteration 112/1000 | Loss: 0.00000984
Iteration 113/1000 | Loss: 0.00000984
Iteration 114/1000 | Loss: 0.00000984
Iteration 115/1000 | Loss: 0.00000984
Iteration 116/1000 | Loss: 0.00000984
Iteration 117/1000 | Loss: 0.00000984
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000984
Iteration 121/1000 | Loss: 0.00000984
Iteration 122/1000 | Loss: 0.00000984
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000984
Iteration 126/1000 | Loss: 0.00000984
Iteration 127/1000 | Loss: 0.00000984
Iteration 128/1000 | Loss: 0.00000984
Iteration 129/1000 | Loss: 0.00000983
Iteration 130/1000 | Loss: 0.00000983
Iteration 131/1000 | Loss: 0.00000983
Iteration 132/1000 | Loss: 0.00000983
Iteration 133/1000 | Loss: 0.00000983
Iteration 134/1000 | Loss: 0.00000983
Iteration 135/1000 | Loss: 0.00000982
Iteration 136/1000 | Loss: 0.00000982
Iteration 137/1000 | Loss: 0.00000982
Iteration 138/1000 | Loss: 0.00000982
Iteration 139/1000 | Loss: 0.00000982
Iteration 140/1000 | Loss: 0.00000982
Iteration 141/1000 | Loss: 0.00000982
Iteration 142/1000 | Loss: 0.00000982
Iteration 143/1000 | Loss: 0.00000982
Iteration 144/1000 | Loss: 0.00000982
Iteration 145/1000 | Loss: 0.00000982
Iteration 146/1000 | Loss: 0.00000982
Iteration 147/1000 | Loss: 0.00000982
Iteration 148/1000 | Loss: 0.00000982
Iteration 149/1000 | Loss: 0.00000982
Iteration 150/1000 | Loss: 0.00000982
Iteration 151/1000 | Loss: 0.00000982
Iteration 152/1000 | Loss: 0.00000982
Iteration 153/1000 | Loss: 0.00000982
Iteration 154/1000 | Loss: 0.00000982
Iteration 155/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [9.818004400585778e-06, 9.818004400585778e-06, 9.818004400585778e-06, 9.818004400585778e-06, 9.818004400585778e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.818004400585778e-06

Optimization complete. Final v2v error: 2.696253776550293 mm

Highest mean error: 2.9114623069763184 mm for frame 99

Lowest mean error: 2.543896198272705 mm for frame 122

Saving results

Total time: 36.24406599998474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954233
Iteration 2/25 | Loss: 0.00300383
Iteration 3/25 | Loss: 0.00206861
Iteration 4/25 | Loss: 0.00193895
Iteration 5/25 | Loss: 0.00176766
Iteration 6/25 | Loss: 0.00170575
Iteration 7/25 | Loss: 0.00160844
Iteration 8/25 | Loss: 0.00156667
Iteration 9/25 | Loss: 0.00157330
Iteration 10/25 | Loss: 0.00161984
Iteration 11/25 | Loss: 0.00151125
Iteration 12/25 | Loss: 0.00144402
Iteration 13/25 | Loss: 0.00142523
Iteration 14/25 | Loss: 0.00141692
Iteration 15/25 | Loss: 0.00141189
Iteration 16/25 | Loss: 0.00140965
Iteration 17/25 | Loss: 0.00140173
Iteration 18/25 | Loss: 0.00139662
Iteration 19/25 | Loss: 0.00139287
Iteration 20/25 | Loss: 0.00139655
Iteration 21/25 | Loss: 0.00139750
Iteration 22/25 | Loss: 0.00139184
Iteration 23/25 | Loss: 0.00138162
Iteration 24/25 | Loss: 0.00137134
Iteration 25/25 | Loss: 0.00135854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35546601
Iteration 2/25 | Loss: 0.00230533
Iteration 3/25 | Loss: 0.00219396
Iteration 4/25 | Loss: 0.00219390
Iteration 5/25 | Loss: 0.00219390
Iteration 6/25 | Loss: 0.00219390
Iteration 7/25 | Loss: 0.00219390
Iteration 8/25 | Loss: 0.00219390
Iteration 9/25 | Loss: 0.00219390
Iteration 10/25 | Loss: 0.00219390
Iteration 11/25 | Loss: 0.00219390
Iteration 12/25 | Loss: 0.00219390
Iteration 13/25 | Loss: 0.00219390
Iteration 14/25 | Loss: 0.00219390
Iteration 15/25 | Loss: 0.00219390
Iteration 16/25 | Loss: 0.00219390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002193897031247616, 0.002193897031247616, 0.002193897031247616, 0.002193897031247616, 0.002193897031247616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002193897031247616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219390
Iteration 2/1000 | Loss: 0.00160289
Iteration 3/1000 | Loss: 0.00121041
Iteration 4/1000 | Loss: 0.00112406
Iteration 5/1000 | Loss: 0.00142207
Iteration 6/1000 | Loss: 0.00034937
Iteration 7/1000 | Loss: 0.00067444
Iteration 8/1000 | Loss: 0.00362215
Iteration 9/1000 | Loss: 0.00019964
Iteration 10/1000 | Loss: 0.00034713
Iteration 11/1000 | Loss: 0.00033721
Iteration 12/1000 | Loss: 0.00031889
Iteration 13/1000 | Loss: 0.00016028
Iteration 14/1000 | Loss: 0.00047975
Iteration 15/1000 | Loss: 0.00127088
Iteration 16/1000 | Loss: 0.00033897
Iteration 17/1000 | Loss: 0.00034954
Iteration 18/1000 | Loss: 0.00026889
Iteration 19/1000 | Loss: 0.00138881
Iteration 20/1000 | Loss: 0.00082313
Iteration 21/1000 | Loss: 0.00049130
Iteration 22/1000 | Loss: 0.00024962
Iteration 23/1000 | Loss: 0.00047417
Iteration 24/1000 | Loss: 0.00022745
Iteration 25/1000 | Loss: 0.00012335
Iteration 26/1000 | Loss: 0.00066159
Iteration 27/1000 | Loss: 0.00061102
Iteration 28/1000 | Loss: 0.00027468
Iteration 29/1000 | Loss: 0.00066622
Iteration 30/1000 | Loss: 0.00032483
Iteration 31/1000 | Loss: 0.00010646
Iteration 32/1000 | Loss: 0.00030763
Iteration 33/1000 | Loss: 0.00028002
Iteration 34/1000 | Loss: 0.00032785
Iteration 35/1000 | Loss: 0.00010054
Iteration 36/1000 | Loss: 0.00009531
Iteration 37/1000 | Loss: 0.00009386
Iteration 38/1000 | Loss: 0.00020123
Iteration 39/1000 | Loss: 0.00009940
Iteration 40/1000 | Loss: 0.00008193
Iteration 41/1000 | Loss: 0.00054957
Iteration 42/1000 | Loss: 0.00021465
Iteration 43/1000 | Loss: 0.00029738
Iteration 44/1000 | Loss: 0.00018756
Iteration 45/1000 | Loss: 0.00007905
Iteration 46/1000 | Loss: 0.00047124
Iteration 47/1000 | Loss: 0.00065831
Iteration 48/1000 | Loss: 0.00012262
Iteration 49/1000 | Loss: 0.00009027
Iteration 50/1000 | Loss: 0.00007940
Iteration 51/1000 | Loss: 0.00036074
Iteration 52/1000 | Loss: 0.00009630
Iteration 53/1000 | Loss: 0.00007607
Iteration 54/1000 | Loss: 0.00006928
Iteration 55/1000 | Loss: 0.00029501
Iteration 56/1000 | Loss: 0.00062989
Iteration 57/1000 | Loss: 0.00063017
Iteration 58/1000 | Loss: 0.00062452
Iteration 59/1000 | Loss: 0.00008832
Iteration 60/1000 | Loss: 0.00022491
Iteration 61/1000 | Loss: 0.00044134
Iteration 62/1000 | Loss: 0.00012533
Iteration 63/1000 | Loss: 0.00009845
Iteration 64/1000 | Loss: 0.00007981
Iteration 65/1000 | Loss: 0.00005369
Iteration 66/1000 | Loss: 0.00005171
Iteration 67/1000 | Loss: 0.00005050
Iteration 68/1000 | Loss: 0.00004940
Iteration 69/1000 | Loss: 0.00004857
Iteration 70/1000 | Loss: 0.00004756
Iteration 71/1000 | Loss: 0.00004689
Iteration 72/1000 | Loss: 0.00004640
Iteration 73/1000 | Loss: 0.00004614
Iteration 74/1000 | Loss: 0.00004586
Iteration 75/1000 | Loss: 0.00004556
Iteration 76/1000 | Loss: 0.00005122
Iteration 77/1000 | Loss: 0.00031849
Iteration 78/1000 | Loss: 0.00012685
Iteration 79/1000 | Loss: 0.00004773
Iteration 80/1000 | Loss: 0.00004965
Iteration 81/1000 | Loss: 0.00033746
Iteration 82/1000 | Loss: 0.00043481
Iteration 83/1000 | Loss: 0.00006672
Iteration 84/1000 | Loss: 0.00004751
Iteration 85/1000 | Loss: 0.00004508
Iteration 86/1000 | Loss: 0.00004468
Iteration 87/1000 | Loss: 0.00004441
Iteration 88/1000 | Loss: 0.00004436
Iteration 89/1000 | Loss: 0.00004421
Iteration 90/1000 | Loss: 0.00004415
Iteration 91/1000 | Loss: 0.00004400
Iteration 92/1000 | Loss: 0.00004399
Iteration 93/1000 | Loss: 0.00004736
Iteration 94/1000 | Loss: 0.00004442
Iteration 95/1000 | Loss: 0.00004453
Iteration 96/1000 | Loss: 0.00004384
Iteration 97/1000 | Loss: 0.00004384
Iteration 98/1000 | Loss: 0.00004384
Iteration 99/1000 | Loss: 0.00004384
Iteration 100/1000 | Loss: 0.00004384
Iteration 101/1000 | Loss: 0.00004384
Iteration 102/1000 | Loss: 0.00004384
Iteration 103/1000 | Loss: 0.00004384
Iteration 104/1000 | Loss: 0.00004384
Iteration 105/1000 | Loss: 0.00004384
Iteration 106/1000 | Loss: 0.00004384
Iteration 107/1000 | Loss: 0.00004383
Iteration 108/1000 | Loss: 0.00004383
Iteration 109/1000 | Loss: 0.00004383
Iteration 110/1000 | Loss: 0.00004383
Iteration 111/1000 | Loss: 0.00004383
Iteration 112/1000 | Loss: 0.00004382
Iteration 113/1000 | Loss: 0.00004382
Iteration 114/1000 | Loss: 0.00004382
Iteration 115/1000 | Loss: 0.00004382
Iteration 116/1000 | Loss: 0.00004381
Iteration 117/1000 | Loss: 0.00004546
Iteration 118/1000 | Loss: 0.00004419
Iteration 119/1000 | Loss: 0.00004409
Iteration 120/1000 | Loss: 0.00004408
Iteration 121/1000 | Loss: 0.00004408
Iteration 122/1000 | Loss: 0.00004408
Iteration 123/1000 | Loss: 0.00004408
Iteration 124/1000 | Loss: 0.00004408
Iteration 125/1000 | Loss: 0.00004408
Iteration 126/1000 | Loss: 0.00004407
Iteration 127/1000 | Loss: 0.00004403
Iteration 128/1000 | Loss: 0.00004403
Iteration 129/1000 | Loss: 0.00004479
Iteration 130/1000 | Loss: 0.00004436
Iteration 131/1000 | Loss: 0.00004403
Iteration 132/1000 | Loss: 0.00004397
Iteration 133/1000 | Loss: 0.00004397
Iteration 134/1000 | Loss: 0.00004397
Iteration 135/1000 | Loss: 0.00004397
Iteration 136/1000 | Loss: 0.00004396
Iteration 137/1000 | Loss: 0.00004396
Iteration 138/1000 | Loss: 0.00004396
Iteration 139/1000 | Loss: 0.00004396
Iteration 140/1000 | Loss: 0.00004396
Iteration 141/1000 | Loss: 0.00004396
Iteration 142/1000 | Loss: 0.00004396
Iteration 143/1000 | Loss: 0.00004396
Iteration 144/1000 | Loss: 0.00004396
Iteration 145/1000 | Loss: 0.00004396
Iteration 146/1000 | Loss: 0.00004396
Iteration 147/1000 | Loss: 0.00004396
Iteration 148/1000 | Loss: 0.00004395
Iteration 149/1000 | Loss: 0.00004395
Iteration 150/1000 | Loss: 0.00004395
Iteration 151/1000 | Loss: 0.00004395
Iteration 152/1000 | Loss: 0.00004395
Iteration 153/1000 | Loss: 0.00004394
Iteration 154/1000 | Loss: 0.00004393
Iteration 155/1000 | Loss: 0.00004393
Iteration 156/1000 | Loss: 0.00004392
Iteration 157/1000 | Loss: 0.00004392
Iteration 158/1000 | Loss: 0.00004392
Iteration 159/1000 | Loss: 0.00004391
Iteration 160/1000 | Loss: 0.00004391
Iteration 161/1000 | Loss: 0.00004391
Iteration 162/1000 | Loss: 0.00004391
Iteration 163/1000 | Loss: 0.00004390
Iteration 164/1000 | Loss: 0.00004390
Iteration 165/1000 | Loss: 0.00004384
Iteration 166/1000 | Loss: 0.00004379
Iteration 167/1000 | Loss: 0.00004372
Iteration 168/1000 | Loss: 0.00004371
Iteration 169/1000 | Loss: 0.00004360
Iteration 170/1000 | Loss: 0.00004356
Iteration 171/1000 | Loss: 0.00004340
Iteration 172/1000 | Loss: 0.00004333
Iteration 173/1000 | Loss: 0.00004317
Iteration 174/1000 | Loss: 0.00004313
Iteration 175/1000 | Loss: 0.00004312
Iteration 176/1000 | Loss: 0.00004312
Iteration 177/1000 | Loss: 0.00004311
Iteration 178/1000 | Loss: 0.00004308
Iteration 179/1000 | Loss: 0.00004306
Iteration 180/1000 | Loss: 0.00004305
Iteration 181/1000 | Loss: 0.00004305
Iteration 182/1000 | Loss: 0.00004305
Iteration 183/1000 | Loss: 0.00004304
Iteration 184/1000 | Loss: 0.00004304
Iteration 185/1000 | Loss: 0.00004304
Iteration 186/1000 | Loss: 0.00004303
Iteration 187/1000 | Loss: 0.00004301
Iteration 188/1000 | Loss: 0.00004300
Iteration 189/1000 | Loss: 0.00004300
Iteration 190/1000 | Loss: 0.00004298
Iteration 191/1000 | Loss: 0.00004297
Iteration 192/1000 | Loss: 0.00004297
Iteration 193/1000 | Loss: 0.00004296
Iteration 194/1000 | Loss: 0.00004296
Iteration 195/1000 | Loss: 0.00004295
Iteration 196/1000 | Loss: 0.00004295
Iteration 197/1000 | Loss: 0.00004295
Iteration 198/1000 | Loss: 0.00004295
Iteration 199/1000 | Loss: 0.00004295
Iteration 200/1000 | Loss: 0.00004295
Iteration 201/1000 | Loss: 0.00004295
Iteration 202/1000 | Loss: 0.00004295
Iteration 203/1000 | Loss: 0.00004294
Iteration 204/1000 | Loss: 0.00004294
Iteration 205/1000 | Loss: 0.00004294
Iteration 206/1000 | Loss: 0.00004294
Iteration 207/1000 | Loss: 0.00004294
Iteration 208/1000 | Loss: 0.00004294
Iteration 209/1000 | Loss: 0.00004293
Iteration 210/1000 | Loss: 0.00004293
Iteration 211/1000 | Loss: 0.00004293
Iteration 212/1000 | Loss: 0.00004292
Iteration 213/1000 | Loss: 0.00004292
Iteration 214/1000 | Loss: 0.00004292
Iteration 215/1000 | Loss: 0.00004291
Iteration 216/1000 | Loss: 0.00004291
Iteration 217/1000 | Loss: 0.00004291
Iteration 218/1000 | Loss: 0.00004291
Iteration 219/1000 | Loss: 0.00004290
Iteration 220/1000 | Loss: 0.00004290
Iteration 221/1000 | Loss: 0.00004290
Iteration 222/1000 | Loss: 0.00004289
Iteration 223/1000 | Loss: 0.00004289
Iteration 224/1000 | Loss: 0.00004289
Iteration 225/1000 | Loss: 0.00004289
Iteration 226/1000 | Loss: 0.00004289
Iteration 227/1000 | Loss: 0.00004289
Iteration 228/1000 | Loss: 0.00004289
Iteration 229/1000 | Loss: 0.00004289
Iteration 230/1000 | Loss: 0.00004289
Iteration 231/1000 | Loss: 0.00004289
Iteration 232/1000 | Loss: 0.00004289
Iteration 233/1000 | Loss: 0.00004289
Iteration 234/1000 | Loss: 0.00004289
Iteration 235/1000 | Loss: 0.00004289
Iteration 236/1000 | Loss: 0.00004288
Iteration 237/1000 | Loss: 0.00004288
Iteration 238/1000 | Loss: 0.00004288
Iteration 239/1000 | Loss: 0.00004288
Iteration 240/1000 | Loss: 0.00004288
Iteration 241/1000 | Loss: 0.00004288
Iteration 242/1000 | Loss: 0.00004288
Iteration 243/1000 | Loss: 0.00004288
Iteration 244/1000 | Loss: 0.00004288
Iteration 245/1000 | Loss: 0.00004288
Iteration 246/1000 | Loss: 0.00004288
Iteration 247/1000 | Loss: 0.00004288
Iteration 248/1000 | Loss: 0.00004288
Iteration 249/1000 | Loss: 0.00004287
Iteration 250/1000 | Loss: 0.00004287
Iteration 251/1000 | Loss: 0.00004287
Iteration 252/1000 | Loss: 0.00004287
Iteration 253/1000 | Loss: 0.00004287
Iteration 254/1000 | Loss: 0.00004287
Iteration 255/1000 | Loss: 0.00004287
Iteration 256/1000 | Loss: 0.00004287
Iteration 257/1000 | Loss: 0.00004287
Iteration 258/1000 | Loss: 0.00004287
Iteration 259/1000 | Loss: 0.00004287
Iteration 260/1000 | Loss: 0.00004287
Iteration 261/1000 | Loss: 0.00004287
Iteration 262/1000 | Loss: 0.00004287
Iteration 263/1000 | Loss: 0.00004286
Iteration 264/1000 | Loss: 0.00004286
Iteration 265/1000 | Loss: 0.00004286
Iteration 266/1000 | Loss: 0.00004286
Iteration 267/1000 | Loss: 0.00004285
Iteration 268/1000 | Loss: 0.00004285
Iteration 269/1000 | Loss: 0.00004285
Iteration 270/1000 | Loss: 0.00004285
Iteration 271/1000 | Loss: 0.00004285
Iteration 272/1000 | Loss: 0.00004285
Iteration 273/1000 | Loss: 0.00004285
Iteration 274/1000 | Loss: 0.00004285
Iteration 275/1000 | Loss: 0.00004285
Iteration 276/1000 | Loss: 0.00004285
Iteration 277/1000 | Loss: 0.00004285
Iteration 278/1000 | Loss: 0.00004285
Iteration 279/1000 | Loss: 0.00004285
Iteration 280/1000 | Loss: 0.00004285
Iteration 281/1000 | Loss: 0.00004285
Iteration 282/1000 | Loss: 0.00004285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [4.285402246750891e-05, 4.285402246750891e-05, 4.285402246750891e-05, 4.285402246750891e-05, 4.285402246750891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.285402246750891e-05

Optimization complete. Final v2v error: 3.9197309017181396 mm

Highest mean error: 10.648674011230469 mm for frame 127

Lowest mean error: 2.935239553451538 mm for frame 10

Saving results

Total time: 207.02105140686035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609809
Iteration 2/25 | Loss: 0.00148338
Iteration 3/25 | Loss: 0.00125145
Iteration 4/25 | Loss: 0.00119602
Iteration 5/25 | Loss: 0.00119739
Iteration 6/25 | Loss: 0.00118692
Iteration 7/25 | Loss: 0.00117228
Iteration 8/25 | Loss: 0.00116602
Iteration 9/25 | Loss: 0.00116224
Iteration 10/25 | Loss: 0.00115764
Iteration 11/25 | Loss: 0.00115766
Iteration 12/25 | Loss: 0.00115521
Iteration 13/25 | Loss: 0.00115567
Iteration 14/25 | Loss: 0.00115462
Iteration 15/25 | Loss: 0.00115396
Iteration 16/25 | Loss: 0.00115391
Iteration 17/25 | Loss: 0.00115391
Iteration 18/25 | Loss: 0.00115391
Iteration 19/25 | Loss: 0.00115390
Iteration 20/25 | Loss: 0.00115390
Iteration 21/25 | Loss: 0.00115491
Iteration 22/25 | Loss: 0.00115502
Iteration 23/25 | Loss: 0.00115604
Iteration 24/25 | Loss: 0.00115456
Iteration 25/25 | Loss: 0.00115376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.57627583
Iteration 2/25 | Loss: 0.00080751
Iteration 3/25 | Loss: 0.00077867
Iteration 4/25 | Loss: 0.00077867
Iteration 5/25 | Loss: 0.00077867
Iteration 6/25 | Loss: 0.00077867
Iteration 7/25 | Loss: 0.00077867
Iteration 8/25 | Loss: 0.00077867
Iteration 9/25 | Loss: 0.00077867
Iteration 10/25 | Loss: 0.00077867
Iteration 11/25 | Loss: 0.00077867
Iteration 12/25 | Loss: 0.00077867
Iteration 13/25 | Loss: 0.00077867
Iteration 14/25 | Loss: 0.00077867
Iteration 15/25 | Loss: 0.00077867
Iteration 16/25 | Loss: 0.00077867
Iteration 17/25 | Loss: 0.00077867
Iteration 18/25 | Loss: 0.00077867
Iteration 19/25 | Loss: 0.00077867
Iteration 20/25 | Loss: 0.00077867
Iteration 21/25 | Loss: 0.00077867
Iteration 22/25 | Loss: 0.00077867
Iteration 23/25 | Loss: 0.00077867
Iteration 24/25 | Loss: 0.00077867
Iteration 25/25 | Loss: 0.00077867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077867
Iteration 2/1000 | Loss: 0.00006367
Iteration 3/1000 | Loss: 0.00002483
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00001907
Iteration 6/1000 | Loss: 0.00002457
Iteration 7/1000 | Loss: 0.00001817
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00002235
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001877
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001682
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001678
Iteration 22/1000 | Loss: 0.00001678
Iteration 23/1000 | Loss: 0.00001678
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001677
Iteration 28/1000 | Loss: 0.00001677
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001676
Iteration 33/1000 | Loss: 0.00001676
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001666
Iteration 53/1000 | Loss: 0.00001666
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001658
Iteration 63/1000 | Loss: 0.00001658
Iteration 64/1000 | Loss: 0.00001658
Iteration 65/1000 | Loss: 0.00001657
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001656
Iteration 68/1000 | Loss: 0.00001656
Iteration 69/1000 | Loss: 0.00001656
Iteration 70/1000 | Loss: 0.00001656
Iteration 71/1000 | Loss: 0.00001656
Iteration 72/1000 | Loss: 0.00001656
Iteration 73/1000 | Loss: 0.00001655
Iteration 74/1000 | Loss: 0.00001655
Iteration 75/1000 | Loss: 0.00001655
Iteration 76/1000 | Loss: 0.00001655
Iteration 77/1000 | Loss: 0.00001655
Iteration 78/1000 | Loss: 0.00001655
Iteration 79/1000 | Loss: 0.00001655
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001653
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001649
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001649
Iteration 99/1000 | Loss: 0.00001649
Iteration 100/1000 | Loss: 0.00001649
Iteration 101/1000 | Loss: 0.00001649
Iteration 102/1000 | Loss: 0.00001649
Iteration 103/1000 | Loss: 0.00001649
Iteration 104/1000 | Loss: 0.00001649
Iteration 105/1000 | Loss: 0.00001649
Iteration 106/1000 | Loss: 0.00001649
Iteration 107/1000 | Loss: 0.00001649
Iteration 108/1000 | Loss: 0.00001649
Iteration 109/1000 | Loss: 0.00001649
Iteration 110/1000 | Loss: 0.00001649
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001649
Iteration 115/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.6493493603775278e-05, 1.6493493603775278e-05, 1.6493493603775278e-05, 1.6493493603775278e-05, 1.6493493603775278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6493493603775278e-05

Optimization complete. Final v2v error: 3.0425617694854736 mm

Highest mean error: 21.276941299438477 mm for frame 214

Lowest mean error: 2.7238903045654297 mm for frame 126

Saving results

Total time: 75.39218974113464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582259
Iteration 2/25 | Loss: 0.00140208
Iteration 3/25 | Loss: 0.00122811
Iteration 4/25 | Loss: 0.00120750
Iteration 5/25 | Loss: 0.00120188
Iteration 6/25 | Loss: 0.00120013
Iteration 7/25 | Loss: 0.00119954
Iteration 8/25 | Loss: 0.00119934
Iteration 9/25 | Loss: 0.00119925
Iteration 10/25 | Loss: 0.00119923
Iteration 11/25 | Loss: 0.00119922
Iteration 12/25 | Loss: 0.00119922
Iteration 13/25 | Loss: 0.00119922
Iteration 14/25 | Loss: 0.00119922
Iteration 15/25 | Loss: 0.00119922
Iteration 16/25 | Loss: 0.00119922
Iteration 17/25 | Loss: 0.00119922
Iteration 18/25 | Loss: 0.00119922
Iteration 19/25 | Loss: 0.00119922
Iteration 20/25 | Loss: 0.00119922
Iteration 21/25 | Loss: 0.00119922
Iteration 22/25 | Loss: 0.00119922
Iteration 23/25 | Loss: 0.00119922
Iteration 24/25 | Loss: 0.00119922
Iteration 25/25 | Loss: 0.00119922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05061698
Iteration 2/25 | Loss: 0.00094427
Iteration 3/25 | Loss: 0.00094424
Iteration 4/25 | Loss: 0.00094424
Iteration 5/25 | Loss: 0.00094424
Iteration 6/25 | Loss: 0.00094424
Iteration 7/25 | Loss: 0.00094424
Iteration 8/25 | Loss: 0.00094424
Iteration 9/25 | Loss: 0.00094424
Iteration 10/25 | Loss: 0.00094424
Iteration 11/25 | Loss: 0.00094424
Iteration 12/25 | Loss: 0.00094424
Iteration 13/25 | Loss: 0.00094424
Iteration 14/25 | Loss: 0.00094424
Iteration 15/25 | Loss: 0.00094424
Iteration 16/25 | Loss: 0.00094424
Iteration 17/25 | Loss: 0.00094424
Iteration 18/25 | Loss: 0.00094424
Iteration 19/25 | Loss: 0.00094424
Iteration 20/25 | Loss: 0.00094424
Iteration 21/25 | Loss: 0.00094424
Iteration 22/25 | Loss: 0.00094424
Iteration 23/25 | Loss: 0.00094424
Iteration 24/25 | Loss: 0.00094424
Iteration 25/25 | Loss: 0.00094424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094424
Iteration 2/1000 | Loss: 0.00003735
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00005179
Iteration 5/1000 | Loss: 0.00002279
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001784
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001586
Iteration 11/1000 | Loss: 0.00001542
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001477
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001409
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001393
Iteration 19/1000 | Loss: 0.00001390
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001314
Iteration 27/1000 | Loss: 0.00001306
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001301
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001299
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001299
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001298
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001298
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001297
Iteration 89/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.2974634955753572e-05, 1.2974634955753572e-05, 1.2974634955753572e-05, 1.2974634955753572e-05, 1.2974634955753572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2974634955753572e-05

Optimization complete. Final v2v error: 3.045954465866089 mm

Highest mean error: 3.9985511302948 mm for frame 88

Lowest mean error: 2.6887710094451904 mm for frame 0

Saving results

Total time: 48.70687770843506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551841
Iteration 2/25 | Loss: 0.00146880
Iteration 3/25 | Loss: 0.00124931
Iteration 4/25 | Loss: 0.00123230
Iteration 5/25 | Loss: 0.00122717
Iteration 6/25 | Loss: 0.00122612
Iteration 7/25 | Loss: 0.00122612
Iteration 8/25 | Loss: 0.00122612
Iteration 9/25 | Loss: 0.00122612
Iteration 10/25 | Loss: 0.00122612
Iteration 11/25 | Loss: 0.00122612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001226117485202849, 0.001226117485202849, 0.001226117485202849, 0.001226117485202849, 0.001226117485202849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001226117485202849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38476288
Iteration 2/25 | Loss: 0.00095352
Iteration 3/25 | Loss: 0.00095351
Iteration 4/25 | Loss: 0.00095350
Iteration 5/25 | Loss: 0.00095350
Iteration 6/25 | Loss: 0.00095350
Iteration 7/25 | Loss: 0.00095350
Iteration 8/25 | Loss: 0.00095350
Iteration 9/25 | Loss: 0.00095350
Iteration 10/25 | Loss: 0.00095350
Iteration 11/25 | Loss: 0.00095350
Iteration 12/25 | Loss: 0.00095350
Iteration 13/25 | Loss: 0.00095350
Iteration 14/25 | Loss: 0.00095350
Iteration 15/25 | Loss: 0.00095350
Iteration 16/25 | Loss: 0.00095350
Iteration 17/25 | Loss: 0.00095350
Iteration 18/25 | Loss: 0.00095350
Iteration 19/25 | Loss: 0.00095350
Iteration 20/25 | Loss: 0.00095350
Iteration 21/25 | Loss: 0.00095350
Iteration 22/25 | Loss: 0.00095350
Iteration 23/25 | Loss: 0.00095350
Iteration 24/25 | Loss: 0.00095350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009535030112601817, 0.0009535030112601817, 0.0009535030112601817, 0.0009535030112601817, 0.0009535030112601817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009535030112601817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095350
Iteration 2/1000 | Loss: 0.00003898
Iteration 3/1000 | Loss: 0.00003012
Iteration 4/1000 | Loss: 0.00002791
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002600
Iteration 7/1000 | Loss: 0.00002560
Iteration 8/1000 | Loss: 0.00002530
Iteration 9/1000 | Loss: 0.00002501
Iteration 10/1000 | Loss: 0.00002476
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002438
Iteration 13/1000 | Loss: 0.00002417
Iteration 14/1000 | Loss: 0.00002397
Iteration 15/1000 | Loss: 0.00002379
Iteration 16/1000 | Loss: 0.00002376
Iteration 17/1000 | Loss: 0.00002368
Iteration 18/1000 | Loss: 0.00002361
Iteration 19/1000 | Loss: 0.00002358
Iteration 20/1000 | Loss: 0.00002358
Iteration 21/1000 | Loss: 0.00002357
Iteration 22/1000 | Loss: 0.00002357
Iteration 23/1000 | Loss: 0.00002357
Iteration 24/1000 | Loss: 0.00002356
Iteration 25/1000 | Loss: 0.00002355
Iteration 26/1000 | Loss: 0.00002355
Iteration 27/1000 | Loss: 0.00002353
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002349
Iteration 30/1000 | Loss: 0.00002349
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002349
Iteration 33/1000 | Loss: 0.00002349
Iteration 34/1000 | Loss: 0.00002348
Iteration 35/1000 | Loss: 0.00002348
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002347
Iteration 39/1000 | Loss: 0.00002347
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002346
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002345
Iteration 45/1000 | Loss: 0.00002345
Iteration 46/1000 | Loss: 0.00002345
Iteration 47/1000 | Loss: 0.00002344
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002340
Iteration 53/1000 | Loss: 0.00002339
Iteration 54/1000 | Loss: 0.00002339
Iteration 55/1000 | Loss: 0.00002338
Iteration 56/1000 | Loss: 0.00002338
Iteration 57/1000 | Loss: 0.00002337
Iteration 58/1000 | Loss: 0.00002337
Iteration 59/1000 | Loss: 0.00002337
Iteration 60/1000 | Loss: 0.00002334
Iteration 61/1000 | Loss: 0.00002334
Iteration 62/1000 | Loss: 0.00002334
Iteration 63/1000 | Loss: 0.00002334
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00002332
Iteration 66/1000 | Loss: 0.00002332
Iteration 67/1000 | Loss: 0.00002332
Iteration 68/1000 | Loss: 0.00002331
Iteration 69/1000 | Loss: 0.00002331
Iteration 70/1000 | Loss: 0.00002331
Iteration 71/1000 | Loss: 0.00002331
Iteration 72/1000 | Loss: 0.00002331
Iteration 73/1000 | Loss: 0.00002331
Iteration 74/1000 | Loss: 0.00002331
Iteration 75/1000 | Loss: 0.00002330
Iteration 76/1000 | Loss: 0.00002330
Iteration 77/1000 | Loss: 0.00002330
Iteration 78/1000 | Loss: 0.00002329
Iteration 79/1000 | Loss: 0.00002329
Iteration 80/1000 | Loss: 0.00002329
Iteration 81/1000 | Loss: 0.00002328
Iteration 82/1000 | Loss: 0.00002328
Iteration 83/1000 | Loss: 0.00002328
Iteration 84/1000 | Loss: 0.00002328
Iteration 85/1000 | Loss: 0.00002328
Iteration 86/1000 | Loss: 0.00002327
Iteration 87/1000 | Loss: 0.00002327
Iteration 88/1000 | Loss: 0.00002327
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002326
Iteration 91/1000 | Loss: 0.00002326
Iteration 92/1000 | Loss: 0.00002326
Iteration 93/1000 | Loss: 0.00002326
Iteration 94/1000 | Loss: 0.00002325
Iteration 95/1000 | Loss: 0.00002325
Iteration 96/1000 | Loss: 0.00002325
Iteration 97/1000 | Loss: 0.00002325
Iteration 98/1000 | Loss: 0.00002325
Iteration 99/1000 | Loss: 0.00002325
Iteration 100/1000 | Loss: 0.00002324
Iteration 101/1000 | Loss: 0.00002324
Iteration 102/1000 | Loss: 0.00002324
Iteration 103/1000 | Loss: 0.00002324
Iteration 104/1000 | Loss: 0.00002324
Iteration 105/1000 | Loss: 0.00002324
Iteration 106/1000 | Loss: 0.00002324
Iteration 107/1000 | Loss: 0.00002324
Iteration 108/1000 | Loss: 0.00002324
Iteration 109/1000 | Loss: 0.00002324
Iteration 110/1000 | Loss: 0.00002324
Iteration 111/1000 | Loss: 0.00002323
Iteration 112/1000 | Loss: 0.00002323
Iteration 113/1000 | Loss: 0.00002323
Iteration 114/1000 | Loss: 0.00002323
Iteration 115/1000 | Loss: 0.00002323
Iteration 116/1000 | Loss: 0.00002323
Iteration 117/1000 | Loss: 0.00002323
Iteration 118/1000 | Loss: 0.00002322
Iteration 119/1000 | Loss: 0.00002322
Iteration 120/1000 | Loss: 0.00002322
Iteration 121/1000 | Loss: 0.00002322
Iteration 122/1000 | Loss: 0.00002322
Iteration 123/1000 | Loss: 0.00002322
Iteration 124/1000 | Loss: 0.00002322
Iteration 125/1000 | Loss: 0.00002322
Iteration 126/1000 | Loss: 0.00002322
Iteration 127/1000 | Loss: 0.00002322
Iteration 128/1000 | Loss: 0.00002321
Iteration 129/1000 | Loss: 0.00002321
Iteration 130/1000 | Loss: 0.00002321
Iteration 131/1000 | Loss: 0.00002321
Iteration 132/1000 | Loss: 0.00002321
Iteration 133/1000 | Loss: 0.00002321
Iteration 134/1000 | Loss: 0.00002321
Iteration 135/1000 | Loss: 0.00002321
Iteration 136/1000 | Loss: 0.00002321
Iteration 137/1000 | Loss: 0.00002321
Iteration 138/1000 | Loss: 0.00002321
Iteration 139/1000 | Loss: 0.00002320
Iteration 140/1000 | Loss: 0.00002320
Iteration 141/1000 | Loss: 0.00002320
Iteration 142/1000 | Loss: 0.00002320
Iteration 143/1000 | Loss: 0.00002320
Iteration 144/1000 | Loss: 0.00002320
Iteration 145/1000 | Loss: 0.00002320
Iteration 146/1000 | Loss: 0.00002320
Iteration 147/1000 | Loss: 0.00002320
Iteration 148/1000 | Loss: 0.00002320
Iteration 149/1000 | Loss: 0.00002320
Iteration 150/1000 | Loss: 0.00002320
Iteration 151/1000 | Loss: 0.00002320
Iteration 152/1000 | Loss: 0.00002320
Iteration 153/1000 | Loss: 0.00002320
Iteration 154/1000 | Loss: 0.00002320
Iteration 155/1000 | Loss: 0.00002320
Iteration 156/1000 | Loss: 0.00002320
Iteration 157/1000 | Loss: 0.00002320
Iteration 158/1000 | Loss: 0.00002320
Iteration 159/1000 | Loss: 0.00002320
Iteration 160/1000 | Loss: 0.00002320
Iteration 161/1000 | Loss: 0.00002320
Iteration 162/1000 | Loss: 0.00002320
Iteration 163/1000 | Loss: 0.00002320
Iteration 164/1000 | Loss: 0.00002320
Iteration 165/1000 | Loss: 0.00002320
Iteration 166/1000 | Loss: 0.00002320
Iteration 167/1000 | Loss: 0.00002320
Iteration 168/1000 | Loss: 0.00002320
Iteration 169/1000 | Loss: 0.00002320
Iteration 170/1000 | Loss: 0.00002320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.3195647372631356e-05, 2.3195647372631356e-05, 2.3195647372631356e-05, 2.3195647372631356e-05, 2.3195647372631356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3195647372631356e-05

Optimization complete. Final v2v error: 3.8976879119873047 mm

Highest mean error: 4.317100524902344 mm for frame 132

Lowest mean error: 3.44451642036438 mm for frame 76

Saving results

Total time: 48.855730295181274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401997
Iteration 2/25 | Loss: 0.00124092
Iteration 3/25 | Loss: 0.00114525
Iteration 4/25 | Loss: 0.00113915
Iteration 5/25 | Loss: 0.00113716
Iteration 6/25 | Loss: 0.00113716
Iteration 7/25 | Loss: 0.00113716
Iteration 8/25 | Loss: 0.00113716
Iteration 9/25 | Loss: 0.00113716
Iteration 10/25 | Loss: 0.00113716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011371552245691419, 0.0011371552245691419, 0.0011371552245691419, 0.0011371552245691419, 0.0011371552245691419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011371552245691419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66113412
Iteration 2/25 | Loss: 0.00074484
Iteration 3/25 | Loss: 0.00074484
Iteration 4/25 | Loss: 0.00074484
Iteration 5/25 | Loss: 0.00074484
Iteration 6/25 | Loss: 0.00074484
Iteration 7/25 | Loss: 0.00074484
Iteration 8/25 | Loss: 0.00074484
Iteration 9/25 | Loss: 0.00074484
Iteration 10/25 | Loss: 0.00074484
Iteration 11/25 | Loss: 0.00074484
Iteration 12/25 | Loss: 0.00074484
Iteration 13/25 | Loss: 0.00074484
Iteration 14/25 | Loss: 0.00074484
Iteration 15/25 | Loss: 0.00074484
Iteration 16/25 | Loss: 0.00074484
Iteration 17/25 | Loss: 0.00074484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007448366959579289, 0.0007448366959579289, 0.0007448366959579289, 0.0007448366959579289, 0.0007448366959579289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007448366959579289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074484
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001407
Iteration 4/1000 | Loss: 0.00001301
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001110
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001047
Iteration 10/1000 | Loss: 0.00001026
Iteration 11/1000 | Loss: 0.00001022
Iteration 12/1000 | Loss: 0.00001019
Iteration 13/1000 | Loss: 0.00001018
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001012
Iteration 16/1000 | Loss: 0.00001004
Iteration 17/1000 | Loss: 0.00000997
Iteration 18/1000 | Loss: 0.00000990
Iteration 19/1000 | Loss: 0.00000990
Iteration 20/1000 | Loss: 0.00000989
Iteration 21/1000 | Loss: 0.00000989
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00000986
Iteration 25/1000 | Loss: 0.00000985
Iteration 26/1000 | Loss: 0.00000984
Iteration 27/1000 | Loss: 0.00000984
Iteration 28/1000 | Loss: 0.00000984
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000983
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000978
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000977
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000974
Iteration 60/1000 | Loss: 0.00000974
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000973
Iteration 63/1000 | Loss: 0.00000971
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000967
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000967
Iteration 74/1000 | Loss: 0.00000967
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000966
Iteration 77/1000 | Loss: 0.00000966
Iteration 78/1000 | Loss: 0.00000966
Iteration 79/1000 | Loss: 0.00000966
Iteration 80/1000 | Loss: 0.00000966
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000965
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000965
Iteration 87/1000 | Loss: 0.00000965
Iteration 88/1000 | Loss: 0.00000965
Iteration 89/1000 | Loss: 0.00000965
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000963
Iteration 95/1000 | Loss: 0.00000962
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000959
Iteration 104/1000 | Loss: 0.00000959
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000958
Iteration 113/1000 | Loss: 0.00000958
Iteration 114/1000 | Loss: 0.00000958
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000958
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000956
Iteration 124/1000 | Loss: 0.00000956
Iteration 125/1000 | Loss: 0.00000956
Iteration 126/1000 | Loss: 0.00000956
Iteration 127/1000 | Loss: 0.00000956
Iteration 128/1000 | Loss: 0.00000956
Iteration 129/1000 | Loss: 0.00000956
Iteration 130/1000 | Loss: 0.00000956
Iteration 131/1000 | Loss: 0.00000956
Iteration 132/1000 | Loss: 0.00000956
Iteration 133/1000 | Loss: 0.00000956
Iteration 134/1000 | Loss: 0.00000956
Iteration 135/1000 | Loss: 0.00000956
Iteration 136/1000 | Loss: 0.00000956
Iteration 137/1000 | Loss: 0.00000956
Iteration 138/1000 | Loss: 0.00000956
Iteration 139/1000 | Loss: 0.00000956
Iteration 140/1000 | Loss: 0.00000956
Iteration 141/1000 | Loss: 0.00000956
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000956
Iteration 147/1000 | Loss: 0.00000956
Iteration 148/1000 | Loss: 0.00000956
Iteration 149/1000 | Loss: 0.00000956
Iteration 150/1000 | Loss: 0.00000956
Iteration 151/1000 | Loss: 0.00000956
Iteration 152/1000 | Loss: 0.00000956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.556494660500903e-06, 9.556494660500903e-06, 9.556494660500903e-06, 9.556494660500903e-06, 9.556494660500903e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.556494660500903e-06

Optimization complete. Final v2v error: 2.677398443222046 mm

Highest mean error: 2.8479599952697754 mm for frame 19

Lowest mean error: 2.5340170860290527 mm for frame 11

Saving results

Total time: 39.07386541366577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770694
Iteration 2/25 | Loss: 0.00175991
Iteration 3/25 | Loss: 0.00141096
Iteration 4/25 | Loss: 0.00131712
Iteration 5/25 | Loss: 0.00128243
Iteration 6/25 | Loss: 0.00125698
Iteration 7/25 | Loss: 0.00125497
Iteration 8/25 | Loss: 0.00125137
Iteration 9/25 | Loss: 0.00125084
Iteration 10/25 | Loss: 0.00125074
Iteration 11/25 | Loss: 0.00125074
Iteration 12/25 | Loss: 0.00125073
Iteration 13/25 | Loss: 0.00125073
Iteration 14/25 | Loss: 0.00125073
Iteration 15/25 | Loss: 0.00125073
Iteration 16/25 | Loss: 0.00125073
Iteration 17/25 | Loss: 0.00125073
Iteration 18/25 | Loss: 0.00125073
Iteration 19/25 | Loss: 0.00125073
Iteration 20/25 | Loss: 0.00125073
Iteration 21/25 | Loss: 0.00125073
Iteration 22/25 | Loss: 0.00125073
Iteration 23/25 | Loss: 0.00125073
Iteration 24/25 | Loss: 0.00125073
Iteration 25/25 | Loss: 0.00125073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32463098
Iteration 2/25 | Loss: 0.00070498
Iteration 3/25 | Loss: 0.00070496
Iteration 4/25 | Loss: 0.00070496
Iteration 5/25 | Loss: 0.00070496
Iteration 6/25 | Loss: 0.00070496
Iteration 7/25 | Loss: 0.00070496
Iteration 8/25 | Loss: 0.00070496
Iteration 9/25 | Loss: 0.00070496
Iteration 10/25 | Loss: 0.00070496
Iteration 11/25 | Loss: 0.00070496
Iteration 12/25 | Loss: 0.00070496
Iteration 13/25 | Loss: 0.00070496
Iteration 14/25 | Loss: 0.00070496
Iteration 15/25 | Loss: 0.00070496
Iteration 16/25 | Loss: 0.00070496
Iteration 17/25 | Loss: 0.00070496
Iteration 18/25 | Loss: 0.00070496
Iteration 19/25 | Loss: 0.00070496
Iteration 20/25 | Loss: 0.00070496
Iteration 21/25 | Loss: 0.00070496
Iteration 22/25 | Loss: 0.00070496
Iteration 23/25 | Loss: 0.00070496
Iteration 24/25 | Loss: 0.00070496
Iteration 25/25 | Loss: 0.00070496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070496
Iteration 2/1000 | Loss: 0.00003330
Iteration 3/1000 | Loss: 0.00002203
Iteration 4/1000 | Loss: 0.00001970
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001749
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001733
Iteration 15/1000 | Loss: 0.00001724
Iteration 16/1000 | Loss: 0.00001720
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001714
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001708
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001706
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001695
Iteration 73/1000 | Loss: 0.00001694
Iteration 74/1000 | Loss: 0.00001694
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001693
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001687
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.6874075299710967e-05, 1.6874075299710967e-05, 1.6874075299710967e-05, 1.6874075299710967e-05, 1.6874075299710967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6874075299710967e-05

Optimization complete. Final v2v error: 3.449341058731079 mm

Highest mean error: 3.9747607707977295 mm for frame 181

Lowest mean error: 3.2642197608947754 mm for frame 128

Saving results

Total time: 46.67149066925049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053028
Iteration 2/25 | Loss: 0.00557093
Iteration 3/25 | Loss: 0.00534652
Iteration 4/25 | Loss: 0.00241954
Iteration 5/25 | Loss: 0.00216170
Iteration 6/25 | Loss: 0.00252005
Iteration 7/25 | Loss: 0.00224405
Iteration 8/25 | Loss: 0.00170324
Iteration 9/25 | Loss: 0.00150544
Iteration 10/25 | Loss: 0.00146530
Iteration 11/25 | Loss: 0.00144236
Iteration 12/25 | Loss: 0.00146214
Iteration 13/25 | Loss: 0.00142933
Iteration 14/25 | Loss: 0.00142468
Iteration 15/25 | Loss: 0.00142461
Iteration 16/25 | Loss: 0.00142461
Iteration 17/25 | Loss: 0.00142461
Iteration 18/25 | Loss: 0.00142461
Iteration 19/25 | Loss: 0.00142595
Iteration 20/25 | Loss: 0.00142462
Iteration 21/25 | Loss: 0.00142461
Iteration 22/25 | Loss: 0.00142461
Iteration 23/25 | Loss: 0.00142461
Iteration 24/25 | Loss: 0.00142461
Iteration 25/25 | Loss: 0.00142461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43764156
Iteration 2/25 | Loss: 0.00065569
Iteration 3/25 | Loss: 0.00059688
Iteration 4/25 | Loss: 0.00059687
Iteration 5/25 | Loss: 0.00059687
Iteration 6/25 | Loss: 0.00059687
Iteration 7/25 | Loss: 0.00059687
Iteration 8/25 | Loss: 0.00059687
Iteration 9/25 | Loss: 0.00059687
Iteration 10/25 | Loss: 0.00059687
Iteration 11/25 | Loss: 0.00059687
Iteration 12/25 | Loss: 0.00059687
Iteration 13/25 | Loss: 0.00059687
Iteration 14/25 | Loss: 0.00059687
Iteration 15/25 | Loss: 0.00059687
Iteration 16/25 | Loss: 0.00059687
Iteration 17/25 | Loss: 0.00059687
Iteration 18/25 | Loss: 0.00059687
Iteration 19/25 | Loss: 0.00059687
Iteration 20/25 | Loss: 0.00059687
Iteration 21/25 | Loss: 0.00059687
Iteration 22/25 | Loss: 0.00059687
Iteration 23/25 | Loss: 0.00059687
Iteration 24/25 | Loss: 0.00059687
Iteration 25/25 | Loss: 0.00059687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059687
Iteration 2/1000 | Loss: 0.00015498
Iteration 3/1000 | Loss: 0.00025272
Iteration 4/1000 | Loss: 0.00011982
Iteration 5/1000 | Loss: 0.00003966
Iteration 6/1000 | Loss: 0.00003708
Iteration 7/1000 | Loss: 0.00003665
Iteration 8/1000 | Loss: 0.00003485
Iteration 9/1000 | Loss: 0.00003451
Iteration 10/1000 | Loss: 0.00003486
Iteration 11/1000 | Loss: 0.00003372
Iteration 12/1000 | Loss: 0.00003338
Iteration 13/1000 | Loss: 0.00003416
Iteration 14/1000 | Loss: 0.00003430
Iteration 15/1000 | Loss: 0.00003333
Iteration 16/1000 | Loss: 0.00003256
Iteration 17/1000 | Loss: 0.00003285
Iteration 18/1000 | Loss: 0.00003229
Iteration 19/1000 | Loss: 0.00003228
Iteration 20/1000 | Loss: 0.00003440
Iteration 21/1000 | Loss: 0.00003206
Iteration 22/1000 | Loss: 0.00003206
Iteration 23/1000 | Loss: 0.00003206
Iteration 24/1000 | Loss: 0.00003206
Iteration 25/1000 | Loss: 0.00003206
Iteration 26/1000 | Loss: 0.00003206
Iteration 27/1000 | Loss: 0.00003206
Iteration 28/1000 | Loss: 0.00003206
Iteration 29/1000 | Loss: 0.00003206
Iteration 30/1000 | Loss: 0.00003205
Iteration 31/1000 | Loss: 0.00003205
Iteration 32/1000 | Loss: 0.00003205
Iteration 33/1000 | Loss: 0.00003205
Iteration 34/1000 | Loss: 0.00003205
Iteration 35/1000 | Loss: 0.00003196
Iteration 36/1000 | Loss: 0.00003234
Iteration 37/1000 | Loss: 0.00003208
Iteration 38/1000 | Loss: 0.00003208
Iteration 39/1000 | Loss: 0.00003186
Iteration 40/1000 | Loss: 0.00003186
Iteration 41/1000 | Loss: 0.00003186
Iteration 42/1000 | Loss: 0.00003186
Iteration 43/1000 | Loss: 0.00003186
Iteration 44/1000 | Loss: 0.00003186
Iteration 45/1000 | Loss: 0.00003186
Iteration 46/1000 | Loss: 0.00003186
Iteration 47/1000 | Loss: 0.00003186
Iteration 48/1000 | Loss: 0.00003186
Iteration 49/1000 | Loss: 0.00003186
Iteration 50/1000 | Loss: 0.00003186
Iteration 51/1000 | Loss: 0.00003185
Iteration 52/1000 | Loss: 0.00003185
Iteration 53/1000 | Loss: 0.00003370
Iteration 54/1000 | Loss: 0.00003808
Iteration 55/1000 | Loss: 0.00003175
Iteration 56/1000 | Loss: 0.00003174
Iteration 57/1000 | Loss: 0.00003174
Iteration 58/1000 | Loss: 0.00003174
Iteration 59/1000 | Loss: 0.00003174
Iteration 60/1000 | Loss: 0.00003173
Iteration 61/1000 | Loss: 0.00003173
Iteration 62/1000 | Loss: 0.00003173
Iteration 63/1000 | Loss: 0.00003173
Iteration 64/1000 | Loss: 0.00003173
Iteration 65/1000 | Loss: 0.00003173
Iteration 66/1000 | Loss: 0.00003173
Iteration 67/1000 | Loss: 0.00003173
Iteration 68/1000 | Loss: 0.00003173
Iteration 69/1000 | Loss: 0.00003172
Iteration 70/1000 | Loss: 0.00003171
Iteration 71/1000 | Loss: 0.00003170
Iteration 72/1000 | Loss: 0.00003170
Iteration 73/1000 | Loss: 0.00003251
Iteration 74/1000 | Loss: 0.00003461
Iteration 75/1000 | Loss: 0.00003163
Iteration 76/1000 | Loss: 0.00003159
Iteration 77/1000 | Loss: 0.00003159
Iteration 78/1000 | Loss: 0.00003159
Iteration 79/1000 | Loss: 0.00003159
Iteration 80/1000 | Loss: 0.00003159
Iteration 81/1000 | Loss: 0.00003159
Iteration 82/1000 | Loss: 0.00003158
Iteration 83/1000 | Loss: 0.00003158
Iteration 84/1000 | Loss: 0.00003158
Iteration 85/1000 | Loss: 0.00003158
Iteration 86/1000 | Loss: 0.00003158
Iteration 87/1000 | Loss: 0.00003158
Iteration 88/1000 | Loss: 0.00003157
Iteration 89/1000 | Loss: 0.00003157
Iteration 90/1000 | Loss: 0.00003157
Iteration 91/1000 | Loss: 0.00003156
Iteration 92/1000 | Loss: 0.00003151
Iteration 93/1000 | Loss: 0.00003149
Iteration 94/1000 | Loss: 0.00003147
Iteration 95/1000 | Loss: 0.00003147
Iteration 96/1000 | Loss: 0.00003147
Iteration 97/1000 | Loss: 0.00003147
Iteration 98/1000 | Loss: 0.00003147
Iteration 99/1000 | Loss: 0.00003147
Iteration 100/1000 | Loss: 0.00003147
Iteration 101/1000 | Loss: 0.00003147
Iteration 102/1000 | Loss: 0.00003147
Iteration 103/1000 | Loss: 0.00003146
Iteration 104/1000 | Loss: 0.00003146
Iteration 105/1000 | Loss: 0.00003146
Iteration 106/1000 | Loss: 0.00003146
Iteration 107/1000 | Loss: 0.00003146
Iteration 108/1000 | Loss: 0.00003144
Iteration 109/1000 | Loss: 0.00003143
Iteration 110/1000 | Loss: 0.00003487
Iteration 111/1000 | Loss: 0.00003260
Iteration 112/1000 | Loss: 0.00003148
Iteration 113/1000 | Loss: 0.00003130
Iteration 114/1000 | Loss: 0.00003130
Iteration 115/1000 | Loss: 0.00003130
Iteration 116/1000 | Loss: 0.00003130
Iteration 117/1000 | Loss: 0.00003130
Iteration 118/1000 | Loss: 0.00003130
Iteration 119/1000 | Loss: 0.00003130
Iteration 120/1000 | Loss: 0.00003130
Iteration 121/1000 | Loss: 0.00003129
Iteration 122/1000 | Loss: 0.00003129
Iteration 123/1000 | Loss: 0.00003129
Iteration 124/1000 | Loss: 0.00003129
Iteration 125/1000 | Loss: 0.00003127
Iteration 126/1000 | Loss: 0.00003126
Iteration 127/1000 | Loss: 0.00003126
Iteration 128/1000 | Loss: 0.00003126
Iteration 129/1000 | Loss: 0.00003220
Iteration 130/1000 | Loss: 0.00003113
Iteration 131/1000 | Loss: 0.00003113
Iteration 132/1000 | Loss: 0.00003113
Iteration 133/1000 | Loss: 0.00003113
Iteration 134/1000 | Loss: 0.00003113
Iteration 135/1000 | Loss: 0.00003113
Iteration 136/1000 | Loss: 0.00003112
Iteration 137/1000 | Loss: 0.00003112
Iteration 138/1000 | Loss: 0.00003112
Iteration 139/1000 | Loss: 0.00003112
Iteration 140/1000 | Loss: 0.00003112
Iteration 141/1000 | Loss: 0.00003112
Iteration 142/1000 | Loss: 0.00003112
Iteration 143/1000 | Loss: 0.00003112
Iteration 144/1000 | Loss: 0.00003111
Iteration 145/1000 | Loss: 0.00003111
Iteration 146/1000 | Loss: 0.00003111
Iteration 147/1000 | Loss: 0.00003111
Iteration 148/1000 | Loss: 0.00003110
Iteration 149/1000 | Loss: 0.00003110
Iteration 150/1000 | Loss: 0.00003110
Iteration 151/1000 | Loss: 0.00003108
Iteration 152/1000 | Loss: 0.00003101
Iteration 153/1000 | Loss: 0.00003101
Iteration 154/1000 | Loss: 0.00003101
Iteration 155/1000 | Loss: 0.00003101
Iteration 156/1000 | Loss: 0.00003101
Iteration 157/1000 | Loss: 0.00003101
Iteration 158/1000 | Loss: 0.00003101
Iteration 159/1000 | Loss: 0.00003101
Iteration 160/1000 | Loss: 0.00003101
Iteration 161/1000 | Loss: 0.00003101
Iteration 162/1000 | Loss: 0.00003101
Iteration 163/1000 | Loss: 0.00003101
Iteration 164/1000 | Loss: 0.00003101
Iteration 165/1000 | Loss: 0.00003101
Iteration 166/1000 | Loss: 0.00003101
Iteration 167/1000 | Loss: 0.00003100
Iteration 168/1000 | Loss: 0.00003100
Iteration 169/1000 | Loss: 0.00003100
Iteration 170/1000 | Loss: 0.00003100
Iteration 171/1000 | Loss: 0.00003100
Iteration 172/1000 | Loss: 0.00003100
Iteration 173/1000 | Loss: 0.00003100
Iteration 174/1000 | Loss: 0.00003099
Iteration 175/1000 | Loss: 0.00003099
Iteration 176/1000 | Loss: 0.00003099
Iteration 177/1000 | Loss: 0.00003099
Iteration 178/1000 | Loss: 0.00003098
Iteration 179/1000 | Loss: 0.00003098
Iteration 180/1000 | Loss: 0.00003098
Iteration 181/1000 | Loss: 0.00003098
Iteration 182/1000 | Loss: 0.00003098
Iteration 183/1000 | Loss: 0.00003098
Iteration 184/1000 | Loss: 0.00003098
Iteration 185/1000 | Loss: 0.00003097
Iteration 186/1000 | Loss: 0.00003097
Iteration 187/1000 | Loss: 0.00003097
Iteration 188/1000 | Loss: 0.00003097
Iteration 189/1000 | Loss: 0.00003097
Iteration 190/1000 | Loss: 0.00003097
Iteration 191/1000 | Loss: 0.00003097
Iteration 192/1000 | Loss: 0.00003097
Iteration 193/1000 | Loss: 0.00003096
Iteration 194/1000 | Loss: 0.00003096
Iteration 195/1000 | Loss: 0.00003093
Iteration 196/1000 | Loss: 0.00003093
Iteration 197/1000 | Loss: 0.00003093
Iteration 198/1000 | Loss: 0.00003093
Iteration 199/1000 | Loss: 0.00003093
Iteration 200/1000 | Loss: 0.00003093
Iteration 201/1000 | Loss: 0.00003093
Iteration 202/1000 | Loss: 0.00003093
Iteration 203/1000 | Loss: 0.00003093
Iteration 204/1000 | Loss: 0.00003092
Iteration 205/1000 | Loss: 0.00003092
Iteration 206/1000 | Loss: 0.00003092
Iteration 207/1000 | Loss: 0.00003092
Iteration 208/1000 | Loss: 0.00003092
Iteration 209/1000 | Loss: 0.00003092
Iteration 210/1000 | Loss: 0.00003092
Iteration 211/1000 | Loss: 0.00003092
Iteration 212/1000 | Loss: 0.00003092
Iteration 213/1000 | Loss: 0.00003092
Iteration 214/1000 | Loss: 0.00003092
Iteration 215/1000 | Loss: 0.00003091
Iteration 216/1000 | Loss: 0.00003091
Iteration 217/1000 | Loss: 0.00003091
Iteration 218/1000 | Loss: 0.00003091
Iteration 219/1000 | Loss: 0.00003091
Iteration 220/1000 | Loss: 0.00003091
Iteration 221/1000 | Loss: 0.00003091
Iteration 222/1000 | Loss: 0.00003091
Iteration 223/1000 | Loss: 0.00003091
Iteration 224/1000 | Loss: 0.00003091
Iteration 225/1000 | Loss: 0.00003090
Iteration 226/1000 | Loss: 0.00003090
Iteration 227/1000 | Loss: 0.00003090
Iteration 228/1000 | Loss: 0.00003090
Iteration 229/1000 | Loss: 0.00003090
Iteration 230/1000 | Loss: 0.00003090
Iteration 231/1000 | Loss: 0.00003090
Iteration 232/1000 | Loss: 0.00003090
Iteration 233/1000 | Loss: 0.00003090
Iteration 234/1000 | Loss: 0.00003090
Iteration 235/1000 | Loss: 0.00003090
Iteration 236/1000 | Loss: 0.00003090
Iteration 237/1000 | Loss: 0.00003090
Iteration 238/1000 | Loss: 0.00003090
Iteration 239/1000 | Loss: 0.00003090
Iteration 240/1000 | Loss: 0.00003090
Iteration 241/1000 | Loss: 0.00003090
Iteration 242/1000 | Loss: 0.00003090
Iteration 243/1000 | Loss: 0.00003090
Iteration 244/1000 | Loss: 0.00003090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [3.090118116233498e-05, 3.090118116233498e-05, 3.090118116233498e-05, 3.090118116233498e-05, 3.090118116233498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.090118116233498e-05

Optimization complete. Final v2v error: 4.610078811645508 mm

Highest mean error: 4.7527971267700195 mm for frame 52

Lowest mean error: 4.185427665710449 mm for frame 9

Saving results

Total time: 86.75126552581787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871890
Iteration 2/25 | Loss: 0.00166281
Iteration 3/25 | Loss: 0.00135746
Iteration 4/25 | Loss: 0.00134340
Iteration 5/25 | Loss: 0.00133799
Iteration 6/25 | Loss: 0.00133681
Iteration 7/25 | Loss: 0.00133681
Iteration 8/25 | Loss: 0.00133681
Iteration 9/25 | Loss: 0.00133681
Iteration 10/25 | Loss: 0.00133681
Iteration 11/25 | Loss: 0.00133681
Iteration 12/25 | Loss: 0.00133681
Iteration 13/25 | Loss: 0.00133681
Iteration 14/25 | Loss: 0.00133681
Iteration 15/25 | Loss: 0.00133681
Iteration 16/25 | Loss: 0.00133681
Iteration 17/25 | Loss: 0.00133681
Iteration 18/25 | Loss: 0.00133681
Iteration 19/25 | Loss: 0.00133681
Iteration 20/25 | Loss: 0.00133681
Iteration 21/25 | Loss: 0.00133681
Iteration 22/25 | Loss: 0.00133681
Iteration 23/25 | Loss: 0.00133681
Iteration 24/25 | Loss: 0.00133681
Iteration 25/25 | Loss: 0.00133681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86361539
Iteration 2/25 | Loss: 0.00111991
Iteration 3/25 | Loss: 0.00111990
Iteration 4/25 | Loss: 0.00111990
Iteration 5/25 | Loss: 0.00111990
Iteration 6/25 | Loss: 0.00111990
Iteration 7/25 | Loss: 0.00111990
Iteration 8/25 | Loss: 0.00111990
Iteration 9/25 | Loss: 0.00111990
Iteration 10/25 | Loss: 0.00111990
Iteration 11/25 | Loss: 0.00111990
Iteration 12/25 | Loss: 0.00111990
Iteration 13/25 | Loss: 0.00111990
Iteration 14/25 | Loss: 0.00111990
Iteration 15/25 | Loss: 0.00111990
Iteration 16/25 | Loss: 0.00111990
Iteration 17/25 | Loss: 0.00111990
Iteration 18/25 | Loss: 0.00111990
Iteration 19/25 | Loss: 0.00111990
Iteration 20/25 | Loss: 0.00111990
Iteration 21/25 | Loss: 0.00111990
Iteration 22/25 | Loss: 0.00111990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001119895838201046, 0.001119895838201046, 0.001119895838201046, 0.001119895838201046, 0.001119895838201046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001119895838201046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111990
Iteration 2/1000 | Loss: 0.00006213
Iteration 3/1000 | Loss: 0.00004022
Iteration 4/1000 | Loss: 0.00003246
Iteration 5/1000 | Loss: 0.00003031
Iteration 6/1000 | Loss: 0.00002895
Iteration 7/1000 | Loss: 0.00002812
Iteration 8/1000 | Loss: 0.00002723
Iteration 9/1000 | Loss: 0.00002676
Iteration 10/1000 | Loss: 0.00002642
Iteration 11/1000 | Loss: 0.00002605
Iteration 12/1000 | Loss: 0.00002576
Iteration 13/1000 | Loss: 0.00002552
Iteration 14/1000 | Loss: 0.00002531
Iteration 15/1000 | Loss: 0.00002516
Iteration 16/1000 | Loss: 0.00002503
Iteration 17/1000 | Loss: 0.00002499
Iteration 18/1000 | Loss: 0.00002494
Iteration 19/1000 | Loss: 0.00002487
Iteration 20/1000 | Loss: 0.00002485
Iteration 21/1000 | Loss: 0.00002484
Iteration 22/1000 | Loss: 0.00002483
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002482
Iteration 26/1000 | Loss: 0.00002481
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002478
Iteration 29/1000 | Loss: 0.00002478
Iteration 30/1000 | Loss: 0.00002477
Iteration 31/1000 | Loss: 0.00002477
Iteration 32/1000 | Loss: 0.00002475
Iteration 33/1000 | Loss: 0.00002473
Iteration 34/1000 | Loss: 0.00002473
Iteration 35/1000 | Loss: 0.00002471
Iteration 36/1000 | Loss: 0.00002469
Iteration 37/1000 | Loss: 0.00002469
Iteration 38/1000 | Loss: 0.00002469
Iteration 39/1000 | Loss: 0.00002468
Iteration 40/1000 | Loss: 0.00002468
Iteration 41/1000 | Loss: 0.00002468
Iteration 42/1000 | Loss: 0.00002466
Iteration 43/1000 | Loss: 0.00002465
Iteration 44/1000 | Loss: 0.00002465
Iteration 45/1000 | Loss: 0.00002465
Iteration 46/1000 | Loss: 0.00002464
Iteration 47/1000 | Loss: 0.00002464
Iteration 48/1000 | Loss: 0.00002463
Iteration 49/1000 | Loss: 0.00002463
Iteration 50/1000 | Loss: 0.00002461
Iteration 51/1000 | Loss: 0.00002461
Iteration 52/1000 | Loss: 0.00002461
Iteration 53/1000 | Loss: 0.00002461
Iteration 54/1000 | Loss: 0.00002461
Iteration 55/1000 | Loss: 0.00002461
Iteration 56/1000 | Loss: 0.00002461
Iteration 57/1000 | Loss: 0.00002460
Iteration 58/1000 | Loss: 0.00002460
Iteration 59/1000 | Loss: 0.00002460
Iteration 60/1000 | Loss: 0.00002460
Iteration 61/1000 | Loss: 0.00002460
Iteration 62/1000 | Loss: 0.00002460
Iteration 63/1000 | Loss: 0.00002460
Iteration 64/1000 | Loss: 0.00002460
Iteration 65/1000 | Loss: 0.00002460
Iteration 66/1000 | Loss: 0.00002460
Iteration 67/1000 | Loss: 0.00002460
Iteration 68/1000 | Loss: 0.00002459
Iteration 69/1000 | Loss: 0.00002459
Iteration 70/1000 | Loss: 0.00002458
Iteration 71/1000 | Loss: 0.00002458
Iteration 72/1000 | Loss: 0.00002458
Iteration 73/1000 | Loss: 0.00002458
Iteration 74/1000 | Loss: 0.00002458
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002458
Iteration 80/1000 | Loss: 0.00002458
Iteration 81/1000 | Loss: 0.00002458
Iteration 82/1000 | Loss: 0.00002458
Iteration 83/1000 | Loss: 0.00002457
Iteration 84/1000 | Loss: 0.00002457
Iteration 85/1000 | Loss: 0.00002457
Iteration 86/1000 | Loss: 0.00002457
Iteration 87/1000 | Loss: 0.00002457
Iteration 88/1000 | Loss: 0.00002457
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002456
Iteration 93/1000 | Loss: 0.00002456
Iteration 94/1000 | Loss: 0.00002456
Iteration 95/1000 | Loss: 0.00002456
Iteration 96/1000 | Loss: 0.00002456
Iteration 97/1000 | Loss: 0.00002456
Iteration 98/1000 | Loss: 0.00002456
Iteration 99/1000 | Loss: 0.00002456
Iteration 100/1000 | Loss: 0.00002456
Iteration 101/1000 | Loss: 0.00002456
Iteration 102/1000 | Loss: 0.00002456
Iteration 103/1000 | Loss: 0.00002456
Iteration 104/1000 | Loss: 0.00002456
Iteration 105/1000 | Loss: 0.00002456
Iteration 106/1000 | Loss: 0.00002456
Iteration 107/1000 | Loss: 0.00002456
Iteration 108/1000 | Loss: 0.00002456
Iteration 109/1000 | Loss: 0.00002456
Iteration 110/1000 | Loss: 0.00002456
Iteration 111/1000 | Loss: 0.00002456
Iteration 112/1000 | Loss: 0.00002456
Iteration 113/1000 | Loss: 0.00002456
Iteration 114/1000 | Loss: 0.00002456
Iteration 115/1000 | Loss: 0.00002456
Iteration 116/1000 | Loss: 0.00002456
Iteration 117/1000 | Loss: 0.00002456
Iteration 118/1000 | Loss: 0.00002456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.4557639335398562e-05, 2.4557639335398562e-05, 2.4557639335398562e-05, 2.4557639335398562e-05, 2.4557639335398562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4557639335398562e-05

Optimization complete. Final v2v error: 4.0836052894592285 mm

Highest mean error: 4.913766384124756 mm for frame 25

Lowest mean error: 3.1140193939208984 mm for frame 0

Saving results

Total time: 41.89125466346741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019957
Iteration 2/25 | Loss: 0.00174801
Iteration 3/25 | Loss: 0.00134276
Iteration 4/25 | Loss: 0.00131185
Iteration 5/25 | Loss: 0.00130252
Iteration 6/25 | Loss: 0.00129979
Iteration 7/25 | Loss: 0.00129949
Iteration 8/25 | Loss: 0.00129949
Iteration 9/25 | Loss: 0.00129949
Iteration 10/25 | Loss: 0.00129949
Iteration 11/25 | Loss: 0.00129949
Iteration 12/25 | Loss: 0.00129949
Iteration 13/25 | Loss: 0.00129949
Iteration 14/25 | Loss: 0.00129949
Iteration 15/25 | Loss: 0.00129949
Iteration 16/25 | Loss: 0.00129949
Iteration 17/25 | Loss: 0.00129949
Iteration 18/25 | Loss: 0.00129949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00129948859103024, 0.00129948859103024, 0.00129948859103024, 0.00129948859103024, 0.00129948859103024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00129948859103024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92379904
Iteration 2/25 | Loss: 0.00111996
Iteration 3/25 | Loss: 0.00111994
Iteration 4/25 | Loss: 0.00111994
Iteration 5/25 | Loss: 0.00111994
Iteration 6/25 | Loss: 0.00111994
Iteration 7/25 | Loss: 0.00111994
Iteration 8/25 | Loss: 0.00111994
Iteration 9/25 | Loss: 0.00111994
Iteration 10/25 | Loss: 0.00111994
Iteration 11/25 | Loss: 0.00111994
Iteration 12/25 | Loss: 0.00111994
Iteration 13/25 | Loss: 0.00111994
Iteration 14/25 | Loss: 0.00111994
Iteration 15/25 | Loss: 0.00111994
Iteration 16/25 | Loss: 0.00111994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011199383297935128, 0.0011199383297935128, 0.0011199383297935128, 0.0011199383297935128, 0.0011199383297935128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011199383297935128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111994
Iteration 2/1000 | Loss: 0.00007373
Iteration 3/1000 | Loss: 0.00004395
Iteration 4/1000 | Loss: 0.00003121
Iteration 5/1000 | Loss: 0.00002911
Iteration 6/1000 | Loss: 0.00002794
Iteration 7/1000 | Loss: 0.00002726
Iteration 8/1000 | Loss: 0.00002664
Iteration 9/1000 | Loss: 0.00002623
Iteration 10/1000 | Loss: 0.00002579
Iteration 11/1000 | Loss: 0.00002542
Iteration 12/1000 | Loss: 0.00002520
Iteration 13/1000 | Loss: 0.00002500
Iteration 14/1000 | Loss: 0.00002482
Iteration 15/1000 | Loss: 0.00002479
Iteration 16/1000 | Loss: 0.00002471
Iteration 17/1000 | Loss: 0.00002463
Iteration 18/1000 | Loss: 0.00002458
Iteration 19/1000 | Loss: 0.00002451
Iteration 20/1000 | Loss: 0.00002446
Iteration 21/1000 | Loss: 0.00002442
Iteration 22/1000 | Loss: 0.00002442
Iteration 23/1000 | Loss: 0.00002440
Iteration 24/1000 | Loss: 0.00002440
Iteration 25/1000 | Loss: 0.00002439
Iteration 26/1000 | Loss: 0.00002438
Iteration 27/1000 | Loss: 0.00002436
Iteration 28/1000 | Loss: 0.00002430
Iteration 29/1000 | Loss: 0.00002429
Iteration 30/1000 | Loss: 0.00002427
Iteration 31/1000 | Loss: 0.00002427
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002425
Iteration 34/1000 | Loss: 0.00002425
Iteration 35/1000 | Loss: 0.00002425
Iteration 36/1000 | Loss: 0.00002425
Iteration 37/1000 | Loss: 0.00002425
Iteration 38/1000 | Loss: 0.00002425
Iteration 39/1000 | Loss: 0.00002425
Iteration 40/1000 | Loss: 0.00002425
Iteration 41/1000 | Loss: 0.00002425
Iteration 42/1000 | Loss: 0.00002425
Iteration 43/1000 | Loss: 0.00002424
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002424
Iteration 46/1000 | Loss: 0.00002424
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002421
Iteration 49/1000 | Loss: 0.00002420
Iteration 50/1000 | Loss: 0.00002420
Iteration 51/1000 | Loss: 0.00002420
Iteration 52/1000 | Loss: 0.00002419
Iteration 53/1000 | Loss: 0.00002418
Iteration 54/1000 | Loss: 0.00002418
Iteration 55/1000 | Loss: 0.00002418
Iteration 56/1000 | Loss: 0.00002418
Iteration 57/1000 | Loss: 0.00002418
Iteration 58/1000 | Loss: 0.00002418
Iteration 59/1000 | Loss: 0.00002418
Iteration 60/1000 | Loss: 0.00002417
Iteration 61/1000 | Loss: 0.00002417
Iteration 62/1000 | Loss: 0.00002417
Iteration 63/1000 | Loss: 0.00002417
Iteration 64/1000 | Loss: 0.00002417
Iteration 65/1000 | Loss: 0.00002417
Iteration 66/1000 | Loss: 0.00002417
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002416
Iteration 69/1000 | Loss: 0.00002415
Iteration 70/1000 | Loss: 0.00002415
Iteration 71/1000 | Loss: 0.00002415
Iteration 72/1000 | Loss: 0.00002414
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002414
Iteration 78/1000 | Loss: 0.00002414
Iteration 79/1000 | Loss: 0.00002414
Iteration 80/1000 | Loss: 0.00002414
Iteration 81/1000 | Loss: 0.00002414
Iteration 82/1000 | Loss: 0.00002414
Iteration 83/1000 | Loss: 0.00002414
Iteration 84/1000 | Loss: 0.00002414
Iteration 85/1000 | Loss: 0.00002414
Iteration 86/1000 | Loss: 0.00002414
Iteration 87/1000 | Loss: 0.00002414
Iteration 88/1000 | Loss: 0.00002414
Iteration 89/1000 | Loss: 0.00002413
Iteration 90/1000 | Loss: 0.00002412
Iteration 91/1000 | Loss: 0.00002412
Iteration 92/1000 | Loss: 0.00002412
Iteration 93/1000 | Loss: 0.00002412
Iteration 94/1000 | Loss: 0.00002412
Iteration 95/1000 | Loss: 0.00002411
Iteration 96/1000 | Loss: 0.00002411
Iteration 97/1000 | Loss: 0.00002409
Iteration 98/1000 | Loss: 0.00002409
Iteration 99/1000 | Loss: 0.00002409
Iteration 100/1000 | Loss: 0.00002408
Iteration 101/1000 | Loss: 0.00002408
Iteration 102/1000 | Loss: 0.00002408
Iteration 103/1000 | Loss: 0.00002408
Iteration 104/1000 | Loss: 0.00002407
Iteration 105/1000 | Loss: 0.00002407
Iteration 106/1000 | Loss: 0.00002407
Iteration 107/1000 | Loss: 0.00002406
Iteration 108/1000 | Loss: 0.00002406
Iteration 109/1000 | Loss: 0.00002406
Iteration 110/1000 | Loss: 0.00002406
Iteration 111/1000 | Loss: 0.00002406
Iteration 112/1000 | Loss: 0.00002406
Iteration 113/1000 | Loss: 0.00002406
Iteration 114/1000 | Loss: 0.00002406
Iteration 115/1000 | Loss: 0.00002405
Iteration 116/1000 | Loss: 0.00002405
Iteration 117/1000 | Loss: 0.00002405
Iteration 118/1000 | Loss: 0.00002405
Iteration 119/1000 | Loss: 0.00002405
Iteration 120/1000 | Loss: 0.00002404
Iteration 121/1000 | Loss: 0.00002404
Iteration 122/1000 | Loss: 0.00002404
Iteration 123/1000 | Loss: 0.00002404
Iteration 124/1000 | Loss: 0.00002404
Iteration 125/1000 | Loss: 0.00002404
Iteration 126/1000 | Loss: 0.00002404
Iteration 127/1000 | Loss: 0.00002404
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002403
Iteration 130/1000 | Loss: 0.00002403
Iteration 131/1000 | Loss: 0.00002403
Iteration 132/1000 | Loss: 0.00002403
Iteration 133/1000 | Loss: 0.00002403
Iteration 134/1000 | Loss: 0.00002403
Iteration 135/1000 | Loss: 0.00002403
Iteration 136/1000 | Loss: 0.00002403
Iteration 137/1000 | Loss: 0.00002403
Iteration 138/1000 | Loss: 0.00002403
Iteration 139/1000 | Loss: 0.00002403
Iteration 140/1000 | Loss: 0.00002403
Iteration 141/1000 | Loss: 0.00002403
Iteration 142/1000 | Loss: 0.00002403
Iteration 143/1000 | Loss: 0.00002403
Iteration 144/1000 | Loss: 0.00002403
Iteration 145/1000 | Loss: 0.00002403
Iteration 146/1000 | Loss: 0.00002403
Iteration 147/1000 | Loss: 0.00002403
Iteration 148/1000 | Loss: 0.00002403
Iteration 149/1000 | Loss: 0.00002403
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.402507743681781e-05, 2.402507743681781e-05, 2.402507743681781e-05, 2.402507743681781e-05, 2.402507743681781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.402507743681781e-05

Optimization complete. Final v2v error: 4.072569370269775 mm

Highest mean error: 4.882961273193359 mm for frame 66

Lowest mean error: 3.4159882068634033 mm for frame 28

Saving results

Total time: 48.52811551094055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427593
Iteration 2/25 | Loss: 0.00124894
Iteration 3/25 | Loss: 0.00113833
Iteration 4/25 | Loss: 0.00112527
Iteration 5/25 | Loss: 0.00112125
Iteration 6/25 | Loss: 0.00112081
Iteration 7/25 | Loss: 0.00112081
Iteration 8/25 | Loss: 0.00112081
Iteration 9/25 | Loss: 0.00112081
Iteration 10/25 | Loss: 0.00112081
Iteration 11/25 | Loss: 0.00112081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001120812026783824, 0.001120812026783824, 0.001120812026783824, 0.001120812026783824, 0.001120812026783824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001120812026783824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10600102
Iteration 2/25 | Loss: 0.00057250
Iteration 3/25 | Loss: 0.00057249
Iteration 4/25 | Loss: 0.00057249
Iteration 5/25 | Loss: 0.00057249
Iteration 6/25 | Loss: 0.00057249
Iteration 7/25 | Loss: 0.00057249
Iteration 8/25 | Loss: 0.00057249
Iteration 9/25 | Loss: 0.00057249
Iteration 10/25 | Loss: 0.00057249
Iteration 11/25 | Loss: 0.00057249
Iteration 12/25 | Loss: 0.00057249
Iteration 13/25 | Loss: 0.00057249
Iteration 14/25 | Loss: 0.00057249
Iteration 15/25 | Loss: 0.00057249
Iteration 16/25 | Loss: 0.00057249
Iteration 17/25 | Loss: 0.00057249
Iteration 18/25 | Loss: 0.00057249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005724910297431052, 0.0005724910297431052, 0.0005724910297431052, 0.0005724910297431052, 0.0005724910297431052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005724910297431052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057249
Iteration 2/1000 | Loss: 0.00003482
Iteration 3/1000 | Loss: 0.00002008
Iteration 4/1000 | Loss: 0.00001716
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001316
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001297
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001261
Iteration 47/1000 | Loss: 0.00001261
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001260
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001257
Iteration 78/1000 | Loss: 0.00001257
Iteration 79/1000 | Loss: 0.00001257
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001257
Iteration 82/1000 | Loss: 0.00001256
Iteration 83/1000 | Loss: 0.00001256
Iteration 84/1000 | Loss: 0.00001256
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001255
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001255
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001250
Iteration 120/1000 | Loss: 0.00001250
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001250
Iteration 123/1000 | Loss: 0.00001250
Iteration 124/1000 | Loss: 0.00001250
Iteration 125/1000 | Loss: 0.00001250
Iteration 126/1000 | Loss: 0.00001250
Iteration 127/1000 | Loss: 0.00001250
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001249
Iteration 143/1000 | Loss: 0.00001249
Iteration 144/1000 | Loss: 0.00001249
Iteration 145/1000 | Loss: 0.00001249
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001248
Iteration 160/1000 | Loss: 0.00001248
Iteration 161/1000 | Loss: 0.00001248
Iteration 162/1000 | Loss: 0.00001248
Iteration 163/1000 | Loss: 0.00001248
Iteration 164/1000 | Loss: 0.00001248
Iteration 165/1000 | Loss: 0.00001248
Iteration 166/1000 | Loss: 0.00001248
Iteration 167/1000 | Loss: 0.00001248
Iteration 168/1000 | Loss: 0.00001248
Iteration 169/1000 | Loss: 0.00001248
Iteration 170/1000 | Loss: 0.00001248
Iteration 171/1000 | Loss: 0.00001248
Iteration 172/1000 | Loss: 0.00001248
Iteration 173/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.2481139492592774e-05, 1.2481139492592774e-05, 1.2481139492592774e-05, 1.2481139492592774e-05, 1.2481139492592774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2481139492592774e-05

Optimization complete. Final v2v error: 3.0466344356536865 mm

Highest mean error: 3.1015920639038086 mm for frame 83

Lowest mean error: 3.0154953002929688 mm for frame 19

Saving results

Total time: 35.36997890472412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050879
Iteration 2/25 | Loss: 0.01050879
Iteration 3/25 | Loss: 0.01050879
Iteration 4/25 | Loss: 0.01050879
Iteration 5/25 | Loss: 0.01050879
Iteration 6/25 | Loss: 0.01050878
Iteration 7/25 | Loss: 0.01050878
Iteration 8/25 | Loss: 0.01050878
Iteration 9/25 | Loss: 0.01050878
Iteration 10/25 | Loss: 0.01050878
Iteration 11/25 | Loss: 0.01050878
Iteration 12/25 | Loss: 0.01050878
Iteration 13/25 | Loss: 0.01050878
Iteration 14/25 | Loss: 0.01050878
Iteration 15/25 | Loss: 0.01050878
Iteration 16/25 | Loss: 0.01050877
Iteration 17/25 | Loss: 0.01050877
Iteration 18/25 | Loss: 0.01050877
Iteration 19/25 | Loss: 0.01050877
Iteration 20/25 | Loss: 0.01050877
Iteration 21/25 | Loss: 0.01050877
Iteration 22/25 | Loss: 0.01050877
Iteration 23/25 | Loss: 0.01050877
Iteration 24/25 | Loss: 0.01050877
Iteration 25/25 | Loss: 0.01050877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72432089
Iteration 2/25 | Loss: 0.05959563
Iteration 3/25 | Loss: 0.05959558
Iteration 4/25 | Loss: 0.05959558
Iteration 5/25 | Loss: 0.05959558
Iteration 6/25 | Loss: 0.05959558
Iteration 7/25 | Loss: 0.05959558
Iteration 8/25 | Loss: 0.05959557
Iteration 9/25 | Loss: 0.05959557
Iteration 10/25 | Loss: 0.05959557
Iteration 11/25 | Loss: 0.05959557
Iteration 12/25 | Loss: 0.05959557
Iteration 13/25 | Loss: 0.05959557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.05959556996822357, 0.05959556996822357, 0.05959556996822357, 0.05959556996822357, 0.05959556996822357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05959556996822357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05959557
Iteration 2/1000 | Loss: 0.00806092
Iteration 3/1000 | Loss: 0.00035867
Iteration 4/1000 | Loss: 0.00111884
Iteration 5/1000 | Loss: 0.00034371
Iteration 6/1000 | Loss: 0.00014487
Iteration 7/1000 | Loss: 0.00003953
Iteration 8/1000 | Loss: 0.00050701
Iteration 9/1000 | Loss: 0.00006593
Iteration 10/1000 | Loss: 0.00042296
Iteration 11/1000 | Loss: 0.00003043
Iteration 12/1000 | Loss: 0.00011402
Iteration 13/1000 | Loss: 0.00003184
Iteration 14/1000 | Loss: 0.00010003
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00023768
Iteration 17/1000 | Loss: 0.00007518
Iteration 18/1000 | Loss: 0.00005344
Iteration 19/1000 | Loss: 0.00013770
Iteration 20/1000 | Loss: 0.00004132
Iteration 21/1000 | Loss: 0.00002585
Iteration 22/1000 | Loss: 0.00001882
Iteration 23/1000 | Loss: 0.00008339
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00003616
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00003633
Iteration 28/1000 | Loss: 0.00028611
Iteration 29/1000 | Loss: 0.00026301
Iteration 30/1000 | Loss: 0.00003242
Iteration 31/1000 | Loss: 0.00003194
Iteration 32/1000 | Loss: 0.00002812
Iteration 33/1000 | Loss: 0.00002629
Iteration 34/1000 | Loss: 0.00007620
Iteration 35/1000 | Loss: 0.00002506
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00002528
Iteration 38/1000 | Loss: 0.00004341
Iteration 39/1000 | Loss: 0.00002322
Iteration 40/1000 | Loss: 0.00017732
Iteration 41/1000 | Loss: 0.00004366
Iteration 42/1000 | Loss: 0.00008566
Iteration 43/1000 | Loss: 0.00030597
Iteration 44/1000 | Loss: 0.00001919
Iteration 45/1000 | Loss: 0.00009821
Iteration 46/1000 | Loss: 0.00017083
Iteration 47/1000 | Loss: 0.00027144
Iteration 48/1000 | Loss: 0.00049100
Iteration 49/1000 | Loss: 0.00114228
Iteration 50/1000 | Loss: 0.00070953
Iteration 51/1000 | Loss: 0.00019090
Iteration 52/1000 | Loss: 0.00005458
Iteration 53/1000 | Loss: 0.00002730
Iteration 54/1000 | Loss: 0.00128220
Iteration 55/1000 | Loss: 0.00002137
Iteration 56/1000 | Loss: 0.00004362
Iteration 57/1000 | Loss: 0.00002696
Iteration 58/1000 | Loss: 0.00002594
Iteration 59/1000 | Loss: 0.00004196
Iteration 60/1000 | Loss: 0.00002685
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00003709
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001573
Iteration 65/1000 | Loss: 0.00003611
Iteration 66/1000 | Loss: 0.00020253
Iteration 67/1000 | Loss: 0.00003380
Iteration 68/1000 | Loss: 0.00014815
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00003640
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001264
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00002306
Iteration 101/1000 | Loss: 0.00002305
Iteration 102/1000 | Loss: 0.00017434
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001923
Iteration 105/1000 | Loss: 0.00012029
Iteration 106/1000 | Loss: 0.00003450
Iteration 107/1000 | Loss: 0.00001369
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00002608
Iteration 111/1000 | Loss: 0.00009111
Iteration 112/1000 | Loss: 0.00007107
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00002284
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001655
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001156
Iteration 154/1000 | Loss: 0.00001156
Iteration 155/1000 | Loss: 0.00001156
Iteration 156/1000 | Loss: 0.00001156
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Iteration 163/1000 | Loss: 0.00001155
Iteration 164/1000 | Loss: 0.00001155
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001155
Iteration 168/1000 | Loss: 0.00001155
Iteration 169/1000 | Loss: 0.00001155
Iteration 170/1000 | Loss: 0.00001155
Iteration 171/1000 | Loss: 0.00001154
Iteration 172/1000 | Loss: 0.00001154
Iteration 173/1000 | Loss: 0.00001154
Iteration 174/1000 | Loss: 0.00001154
Iteration 175/1000 | Loss: 0.00001154
Iteration 176/1000 | Loss: 0.00001154
Iteration 177/1000 | Loss: 0.00001154
Iteration 178/1000 | Loss: 0.00001154
Iteration 179/1000 | Loss: 0.00001154
Iteration 180/1000 | Loss: 0.00001154
Iteration 181/1000 | Loss: 0.00001154
Iteration 182/1000 | Loss: 0.00001154
Iteration 183/1000 | Loss: 0.00001153
Iteration 184/1000 | Loss: 0.00001153
Iteration 185/1000 | Loss: 0.00001153
Iteration 186/1000 | Loss: 0.00001153
Iteration 187/1000 | Loss: 0.00001153
Iteration 188/1000 | Loss: 0.00001153
Iteration 189/1000 | Loss: 0.00001152
Iteration 190/1000 | Loss: 0.00001152
Iteration 191/1000 | Loss: 0.00001152
Iteration 192/1000 | Loss: 0.00001152
Iteration 193/1000 | Loss: 0.00001152
Iteration 194/1000 | Loss: 0.00001152
Iteration 195/1000 | Loss: 0.00001152
Iteration 196/1000 | Loss: 0.00001152
Iteration 197/1000 | Loss: 0.00001152
Iteration 198/1000 | Loss: 0.00001152
Iteration 199/1000 | Loss: 0.00001152
Iteration 200/1000 | Loss: 0.00001152
Iteration 201/1000 | Loss: 0.00003037
Iteration 202/1000 | Loss: 0.00002459
Iteration 203/1000 | Loss: 0.00002004
Iteration 204/1000 | Loss: 0.00001150
Iteration 205/1000 | Loss: 0.00001149
Iteration 206/1000 | Loss: 0.00001149
Iteration 207/1000 | Loss: 0.00001149
Iteration 208/1000 | Loss: 0.00001148
Iteration 209/1000 | Loss: 0.00001148
Iteration 210/1000 | Loss: 0.00001148
Iteration 211/1000 | Loss: 0.00001148
Iteration 212/1000 | Loss: 0.00001148
Iteration 213/1000 | Loss: 0.00001148
Iteration 214/1000 | Loss: 0.00001148
Iteration 215/1000 | Loss: 0.00001148
Iteration 216/1000 | Loss: 0.00001148
Iteration 217/1000 | Loss: 0.00001148
Iteration 218/1000 | Loss: 0.00001148
Iteration 219/1000 | Loss: 0.00001148
Iteration 220/1000 | Loss: 0.00001148
Iteration 221/1000 | Loss: 0.00001148
Iteration 222/1000 | Loss: 0.00001148
Iteration 223/1000 | Loss: 0.00001148
Iteration 224/1000 | Loss: 0.00001148
Iteration 225/1000 | Loss: 0.00001147
Iteration 226/1000 | Loss: 0.00001147
Iteration 227/1000 | Loss: 0.00001147
Iteration 228/1000 | Loss: 0.00001147
Iteration 229/1000 | Loss: 0.00001147
Iteration 230/1000 | Loss: 0.00001147
Iteration 231/1000 | Loss: 0.00001147
Iteration 232/1000 | Loss: 0.00001147
Iteration 233/1000 | Loss: 0.00001147
Iteration 234/1000 | Loss: 0.00001146
Iteration 235/1000 | Loss: 0.00001146
Iteration 236/1000 | Loss: 0.00001146
Iteration 237/1000 | Loss: 0.00001146
Iteration 238/1000 | Loss: 0.00001146
Iteration 239/1000 | Loss: 0.00001146
Iteration 240/1000 | Loss: 0.00001146
Iteration 241/1000 | Loss: 0.00001146
Iteration 242/1000 | Loss: 0.00001146
Iteration 243/1000 | Loss: 0.00001146
Iteration 244/1000 | Loss: 0.00001146
Iteration 245/1000 | Loss: 0.00001146
Iteration 246/1000 | Loss: 0.00001146
Iteration 247/1000 | Loss: 0.00001146
Iteration 248/1000 | Loss: 0.00001146
Iteration 249/1000 | Loss: 0.00001146
Iteration 250/1000 | Loss: 0.00001146
Iteration 251/1000 | Loss: 0.00001146
Iteration 252/1000 | Loss: 0.00001146
Iteration 253/1000 | Loss: 0.00001146
Iteration 254/1000 | Loss: 0.00001146
Iteration 255/1000 | Loss: 0.00001146
Iteration 256/1000 | Loss: 0.00001146
Iteration 257/1000 | Loss: 0.00001146
Iteration 258/1000 | Loss: 0.00001146
Iteration 259/1000 | Loss: 0.00001146
Iteration 260/1000 | Loss: 0.00001146
Iteration 261/1000 | Loss: 0.00001146
Iteration 262/1000 | Loss: 0.00001146
Iteration 263/1000 | Loss: 0.00001146
Iteration 264/1000 | Loss: 0.00001146
Iteration 265/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.145797887147637e-05, 1.145797887147637e-05, 1.145797887147637e-05, 1.145797887147637e-05, 1.145797887147637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.145797887147637e-05

Optimization complete. Final v2v error: 2.896563768386841 mm

Highest mean error: 3.3873744010925293 mm for frame 17

Lowest mean error: 2.4805030822753906 mm for frame 252

Saving results

Total time: 162.11770701408386
