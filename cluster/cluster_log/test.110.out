Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=110, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6160-6215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540990
Iteration 2/25 | Loss: 0.00116590
Iteration 3/25 | Loss: 0.00097538
Iteration 4/25 | Loss: 0.00094188
Iteration 5/25 | Loss: 0.00093132
Iteration 6/25 | Loss: 0.00092941
Iteration 7/25 | Loss: 0.00092898
Iteration 8/25 | Loss: 0.00092877
Iteration 9/25 | Loss: 0.00092872
Iteration 10/25 | Loss: 0.00092872
Iteration 11/25 | Loss: 0.00092872
Iteration 12/25 | Loss: 0.00092872
Iteration 13/25 | Loss: 0.00092872
Iteration 14/25 | Loss: 0.00092872
Iteration 15/25 | Loss: 0.00092871
Iteration 16/25 | Loss: 0.00092871
Iteration 17/25 | Loss: 0.00092871
Iteration 18/25 | Loss: 0.00092871
Iteration 19/25 | Loss: 0.00092871
Iteration 20/25 | Loss: 0.00092871
Iteration 21/25 | Loss: 0.00092879
Iteration 22/25 | Loss: 0.00092879
Iteration 23/25 | Loss: 0.00092878
Iteration 24/25 | Loss: 0.00092878
Iteration 25/25 | Loss: 0.00092870

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.89225483
Iteration 2/25 | Loss: 0.00065922
Iteration 3/25 | Loss: 0.00064135
Iteration 4/25 | Loss: 0.00064135
Iteration 5/25 | Loss: 0.00064135
Iteration 6/25 | Loss: 0.00064135
Iteration 7/25 | Loss: 0.00064135
Iteration 8/25 | Loss: 0.00064135
Iteration 9/25 | Loss: 0.00064135
Iteration 10/25 | Loss: 0.00064135
Iteration 11/25 | Loss: 0.00064135
Iteration 12/25 | Loss: 0.00064135
Iteration 13/25 | Loss: 0.00064135
Iteration 14/25 | Loss: 0.00064135
Iteration 15/25 | Loss: 0.00064135
Iteration 16/25 | Loss: 0.00064135
Iteration 17/25 | Loss: 0.00064135
Iteration 18/25 | Loss: 0.00064135
Iteration 19/25 | Loss: 0.00064135
Iteration 20/25 | Loss: 0.00064135
Iteration 21/25 | Loss: 0.00064135
Iteration 22/25 | Loss: 0.00064135
Iteration 23/25 | Loss: 0.00064135
Iteration 24/25 | Loss: 0.00064135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006413466180674732, 0.0006413466180674732, 0.0006413466180674732, 0.0006413466180674732, 0.0006413466180674732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006413466180674732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064135
Iteration 2/1000 | Loss: 0.00004441
Iteration 3/1000 | Loss: 0.00003496
Iteration 4/1000 | Loss: 0.00003520
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001295
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001158
Iteration 18/1000 | Loss: 0.00001158
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00003069
Iteration 22/1000 | Loss: 0.00031775
Iteration 23/1000 | Loss: 0.00017818
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00025663
Iteration 28/1000 | Loss: 0.00004790
Iteration 29/1000 | Loss: 0.00002872
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00003095
Iteration 32/1000 | Loss: 0.00003618
Iteration 33/1000 | Loss: 0.00002034
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00004054
Iteration 36/1000 | Loss: 0.00010470
Iteration 37/1000 | Loss: 0.00002209
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001043
Iteration 41/1000 | Loss: 0.00001025
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001012
Iteration 46/1000 | Loss: 0.00001011
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001018
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001007
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001003
Iteration 76/1000 | Loss: 0.00001003
Iteration 77/1000 | Loss: 0.00001003
Iteration 78/1000 | Loss: 0.00001002
Iteration 79/1000 | Loss: 0.00001002
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001001
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00001000
Iteration 92/1000 | Loss: 0.00001000
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000998
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00000998
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000998
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000994
Iteration 132/1000 | Loss: 0.00000994
Iteration 133/1000 | Loss: 0.00000994
Iteration 134/1000 | Loss: 0.00000994
Iteration 135/1000 | Loss: 0.00000994
Iteration 136/1000 | Loss: 0.00000994
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000994
Iteration 139/1000 | Loss: 0.00002685
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000994
Iteration 143/1000 | Loss: 0.00000993
Iteration 144/1000 | Loss: 0.00000993
Iteration 145/1000 | Loss: 0.00000992
Iteration 146/1000 | Loss: 0.00000992
Iteration 147/1000 | Loss: 0.00000992
Iteration 148/1000 | Loss: 0.00000992
Iteration 149/1000 | Loss: 0.00000992
Iteration 150/1000 | Loss: 0.00000992
Iteration 151/1000 | Loss: 0.00000992
Iteration 152/1000 | Loss: 0.00000992
Iteration 153/1000 | Loss: 0.00000992
Iteration 154/1000 | Loss: 0.00000992
Iteration 155/1000 | Loss: 0.00000992
Iteration 156/1000 | Loss: 0.00000992
Iteration 157/1000 | Loss: 0.00000991
Iteration 158/1000 | Loss: 0.00000991
Iteration 159/1000 | Loss: 0.00000991
Iteration 160/1000 | Loss: 0.00000991
Iteration 161/1000 | Loss: 0.00000991
Iteration 162/1000 | Loss: 0.00000991
Iteration 163/1000 | Loss: 0.00000991
Iteration 164/1000 | Loss: 0.00000991
Iteration 165/1000 | Loss: 0.00000991
Iteration 166/1000 | Loss: 0.00000991
Iteration 167/1000 | Loss: 0.00000991
Iteration 168/1000 | Loss: 0.00000991
Iteration 169/1000 | Loss: 0.00000991
Iteration 170/1000 | Loss: 0.00000991
Iteration 171/1000 | Loss: 0.00000991
Iteration 172/1000 | Loss: 0.00000991
Iteration 173/1000 | Loss: 0.00000991
Iteration 174/1000 | Loss: 0.00000991
Iteration 175/1000 | Loss: 0.00000991
Iteration 176/1000 | Loss: 0.00000991
Iteration 177/1000 | Loss: 0.00000991
Iteration 178/1000 | Loss: 0.00000991
Iteration 179/1000 | Loss: 0.00000991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [9.909967957355548e-06, 9.909967957355548e-06, 9.909967957355548e-06, 9.909967957355548e-06, 9.909967957355548e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.909967957355548e-06

Optimization complete. Final v2v error: 2.678809881210327 mm

Highest mean error: 3.7669260501861572 mm for frame 237

Lowest mean error: 2.3419108390808105 mm for frame 160

Saving results

Total time: 85.78068470954895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844665
Iteration 2/25 | Loss: 0.00103214
Iteration 3/25 | Loss: 0.00092865
Iteration 4/25 | Loss: 0.00091870
Iteration 5/25 | Loss: 0.00091712
Iteration 6/25 | Loss: 0.00091712
Iteration 7/25 | Loss: 0.00091712
Iteration 8/25 | Loss: 0.00091712
Iteration 9/25 | Loss: 0.00091712
Iteration 10/25 | Loss: 0.00091712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009171243291348219, 0.0009171243291348219, 0.0009171243291348219, 0.0009171243291348219, 0.0009171243291348219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009171243291348219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31209207
Iteration 2/25 | Loss: 0.00068176
Iteration 3/25 | Loss: 0.00068176
Iteration 4/25 | Loss: 0.00068176
Iteration 5/25 | Loss: 0.00068176
Iteration 6/25 | Loss: 0.00068176
Iteration 7/25 | Loss: 0.00068176
Iteration 8/25 | Loss: 0.00068176
Iteration 9/25 | Loss: 0.00068176
Iteration 10/25 | Loss: 0.00068176
Iteration 11/25 | Loss: 0.00068176
Iteration 12/25 | Loss: 0.00068176
Iteration 13/25 | Loss: 0.00068176
Iteration 14/25 | Loss: 0.00068176
Iteration 15/25 | Loss: 0.00068176
Iteration 16/25 | Loss: 0.00068176
Iteration 17/25 | Loss: 0.00068176
Iteration 18/25 | Loss: 0.00068176
Iteration 19/25 | Loss: 0.00068176
Iteration 20/25 | Loss: 0.00068176
Iteration 21/25 | Loss: 0.00068176
Iteration 22/25 | Loss: 0.00068176
Iteration 23/25 | Loss: 0.00068176
Iteration 24/25 | Loss: 0.00068176
Iteration 25/25 | Loss: 0.00068176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068176
Iteration 2/1000 | Loss: 0.00002320
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001302
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00001194
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001144
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001128
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001112
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001111
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001109
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001106
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001105
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001104
Iteration 56/1000 | Loss: 0.00001104
Iteration 57/1000 | Loss: 0.00001104
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001102
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001095
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001091
Iteration 87/1000 | Loss: 0.00001091
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001088
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001087
Iteration 104/1000 | Loss: 0.00001087
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001086
Iteration 107/1000 | Loss: 0.00001086
Iteration 108/1000 | Loss: 0.00001086
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001085
Iteration 113/1000 | Loss: 0.00001085
Iteration 114/1000 | Loss: 0.00001085
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001084
Iteration 120/1000 | Loss: 0.00001084
Iteration 121/1000 | Loss: 0.00001084
Iteration 122/1000 | Loss: 0.00001084
Iteration 123/1000 | Loss: 0.00001084
Iteration 124/1000 | Loss: 0.00001084
Iteration 125/1000 | Loss: 0.00001084
Iteration 126/1000 | Loss: 0.00001084
Iteration 127/1000 | Loss: 0.00001083
Iteration 128/1000 | Loss: 0.00001083
Iteration 129/1000 | Loss: 0.00001083
Iteration 130/1000 | Loss: 0.00001083
Iteration 131/1000 | Loss: 0.00001083
Iteration 132/1000 | Loss: 0.00001083
Iteration 133/1000 | Loss: 0.00001083
Iteration 134/1000 | Loss: 0.00001083
Iteration 135/1000 | Loss: 0.00001082
Iteration 136/1000 | Loss: 0.00001082
Iteration 137/1000 | Loss: 0.00001082
Iteration 138/1000 | Loss: 0.00001082
Iteration 139/1000 | Loss: 0.00001082
Iteration 140/1000 | Loss: 0.00001082
Iteration 141/1000 | Loss: 0.00001082
Iteration 142/1000 | Loss: 0.00001081
Iteration 143/1000 | Loss: 0.00001081
Iteration 144/1000 | Loss: 0.00001081
Iteration 145/1000 | Loss: 0.00001081
Iteration 146/1000 | Loss: 0.00001081
Iteration 147/1000 | Loss: 0.00001081
Iteration 148/1000 | Loss: 0.00001081
Iteration 149/1000 | Loss: 0.00001081
Iteration 150/1000 | Loss: 0.00001081
Iteration 151/1000 | Loss: 0.00001080
Iteration 152/1000 | Loss: 0.00001080
Iteration 153/1000 | Loss: 0.00001080
Iteration 154/1000 | Loss: 0.00001080
Iteration 155/1000 | Loss: 0.00001080
Iteration 156/1000 | Loss: 0.00001080
Iteration 157/1000 | Loss: 0.00001079
Iteration 158/1000 | Loss: 0.00001079
Iteration 159/1000 | Loss: 0.00001079
Iteration 160/1000 | Loss: 0.00001079
Iteration 161/1000 | Loss: 0.00001079
Iteration 162/1000 | Loss: 0.00001078
Iteration 163/1000 | Loss: 0.00001078
Iteration 164/1000 | Loss: 0.00001078
Iteration 165/1000 | Loss: 0.00001078
Iteration 166/1000 | Loss: 0.00001078
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001077
Iteration 169/1000 | Loss: 0.00001077
Iteration 170/1000 | Loss: 0.00001077
Iteration 171/1000 | Loss: 0.00001077
Iteration 172/1000 | Loss: 0.00001077
Iteration 173/1000 | Loss: 0.00001077
Iteration 174/1000 | Loss: 0.00001077
Iteration 175/1000 | Loss: 0.00001077
Iteration 176/1000 | Loss: 0.00001077
Iteration 177/1000 | Loss: 0.00001077
Iteration 178/1000 | Loss: 0.00001077
Iteration 179/1000 | Loss: 0.00001077
Iteration 180/1000 | Loss: 0.00001077
Iteration 181/1000 | Loss: 0.00001077
Iteration 182/1000 | Loss: 0.00001077
Iteration 183/1000 | Loss: 0.00001077
Iteration 184/1000 | Loss: 0.00001076
Iteration 185/1000 | Loss: 0.00001076
Iteration 186/1000 | Loss: 0.00001076
Iteration 187/1000 | Loss: 0.00001076
Iteration 188/1000 | Loss: 0.00001076
Iteration 189/1000 | Loss: 0.00001076
Iteration 190/1000 | Loss: 0.00001076
Iteration 191/1000 | Loss: 0.00001076
Iteration 192/1000 | Loss: 0.00001076
Iteration 193/1000 | Loss: 0.00001076
Iteration 194/1000 | Loss: 0.00001076
Iteration 195/1000 | Loss: 0.00001076
Iteration 196/1000 | Loss: 0.00001076
Iteration 197/1000 | Loss: 0.00001076
Iteration 198/1000 | Loss: 0.00001076
Iteration 199/1000 | Loss: 0.00001076
Iteration 200/1000 | Loss: 0.00001076
Iteration 201/1000 | Loss: 0.00001076
Iteration 202/1000 | Loss: 0.00001075
Iteration 203/1000 | Loss: 0.00001075
Iteration 204/1000 | Loss: 0.00001075
Iteration 205/1000 | Loss: 0.00001075
Iteration 206/1000 | Loss: 0.00001075
Iteration 207/1000 | Loss: 0.00001075
Iteration 208/1000 | Loss: 0.00001075
Iteration 209/1000 | Loss: 0.00001075
Iteration 210/1000 | Loss: 0.00001075
Iteration 211/1000 | Loss: 0.00001074
Iteration 212/1000 | Loss: 0.00001074
Iteration 213/1000 | Loss: 0.00001074
Iteration 214/1000 | Loss: 0.00001074
Iteration 215/1000 | Loss: 0.00001074
Iteration 216/1000 | Loss: 0.00001074
Iteration 217/1000 | Loss: 0.00001074
Iteration 218/1000 | Loss: 0.00001073
Iteration 219/1000 | Loss: 0.00001073
Iteration 220/1000 | Loss: 0.00001073
Iteration 221/1000 | Loss: 0.00001073
Iteration 222/1000 | Loss: 0.00001073
Iteration 223/1000 | Loss: 0.00001073
Iteration 224/1000 | Loss: 0.00001073
Iteration 225/1000 | Loss: 0.00001073
Iteration 226/1000 | Loss: 0.00001072
Iteration 227/1000 | Loss: 0.00001072
Iteration 228/1000 | Loss: 0.00001072
Iteration 229/1000 | Loss: 0.00001072
Iteration 230/1000 | Loss: 0.00001072
Iteration 231/1000 | Loss: 0.00001072
Iteration 232/1000 | Loss: 0.00001072
Iteration 233/1000 | Loss: 0.00001072
Iteration 234/1000 | Loss: 0.00001072
Iteration 235/1000 | Loss: 0.00001072
Iteration 236/1000 | Loss: 0.00001072
Iteration 237/1000 | Loss: 0.00001071
Iteration 238/1000 | Loss: 0.00001071
Iteration 239/1000 | Loss: 0.00001071
Iteration 240/1000 | Loss: 0.00001071
Iteration 241/1000 | Loss: 0.00001071
Iteration 242/1000 | Loss: 0.00001071
Iteration 243/1000 | Loss: 0.00001071
Iteration 244/1000 | Loss: 0.00001071
Iteration 245/1000 | Loss: 0.00001071
Iteration 246/1000 | Loss: 0.00001071
Iteration 247/1000 | Loss: 0.00001071
Iteration 248/1000 | Loss: 0.00001071
Iteration 249/1000 | Loss: 0.00001071
Iteration 250/1000 | Loss: 0.00001070
Iteration 251/1000 | Loss: 0.00001070
Iteration 252/1000 | Loss: 0.00001070
Iteration 253/1000 | Loss: 0.00001070
Iteration 254/1000 | Loss: 0.00001070
Iteration 255/1000 | Loss: 0.00001070
Iteration 256/1000 | Loss: 0.00001070
Iteration 257/1000 | Loss: 0.00001070
Iteration 258/1000 | Loss: 0.00001070
Iteration 259/1000 | Loss: 0.00001069
Iteration 260/1000 | Loss: 0.00001069
Iteration 261/1000 | Loss: 0.00001069
Iteration 262/1000 | Loss: 0.00001069
Iteration 263/1000 | Loss: 0.00001069
Iteration 264/1000 | Loss: 0.00001069
Iteration 265/1000 | Loss: 0.00001069
Iteration 266/1000 | Loss: 0.00001069
Iteration 267/1000 | Loss: 0.00001069
Iteration 268/1000 | Loss: 0.00001069
Iteration 269/1000 | Loss: 0.00001069
Iteration 270/1000 | Loss: 0.00001069
Iteration 271/1000 | Loss: 0.00001069
Iteration 272/1000 | Loss: 0.00001069
Iteration 273/1000 | Loss: 0.00001069
Iteration 274/1000 | Loss: 0.00001069
Iteration 275/1000 | Loss: 0.00001069
Iteration 276/1000 | Loss: 0.00001068
Iteration 277/1000 | Loss: 0.00001068
Iteration 278/1000 | Loss: 0.00001068
Iteration 279/1000 | Loss: 0.00001068
Iteration 280/1000 | Loss: 0.00001068
Iteration 281/1000 | Loss: 0.00001068
Iteration 282/1000 | Loss: 0.00001068
Iteration 283/1000 | Loss: 0.00001068
Iteration 284/1000 | Loss: 0.00001068
Iteration 285/1000 | Loss: 0.00001068
Iteration 286/1000 | Loss: 0.00001068
Iteration 287/1000 | Loss: 0.00001068
Iteration 288/1000 | Loss: 0.00001068
Iteration 289/1000 | Loss: 0.00001067
Iteration 290/1000 | Loss: 0.00001067
Iteration 291/1000 | Loss: 0.00001067
Iteration 292/1000 | Loss: 0.00001067
Iteration 293/1000 | Loss: 0.00001067
Iteration 294/1000 | Loss: 0.00001067
Iteration 295/1000 | Loss: 0.00001067
Iteration 296/1000 | Loss: 0.00001067
Iteration 297/1000 | Loss: 0.00001067
Iteration 298/1000 | Loss: 0.00001067
Iteration 299/1000 | Loss: 0.00001067
Iteration 300/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.067312950908672e-05, 1.067312950908672e-05, 1.067312950908672e-05, 1.067312950908672e-05, 1.067312950908672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.067312950908672e-05

Optimization complete. Final v2v error: 2.7620761394500732 mm

Highest mean error: 3.4723167419433594 mm for frame 139

Lowest mean error: 2.2494471073150635 mm for frame 57

Saving results

Total time: 44.19257593154907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536013
Iteration 2/25 | Loss: 0.00132716
Iteration 3/25 | Loss: 0.00112000
Iteration 4/25 | Loss: 0.00109921
Iteration 5/25 | Loss: 0.00109735
Iteration 6/25 | Loss: 0.00109722
Iteration 7/25 | Loss: 0.00109722
Iteration 8/25 | Loss: 0.00109722
Iteration 9/25 | Loss: 0.00109722
Iteration 10/25 | Loss: 0.00109722
Iteration 11/25 | Loss: 0.00109722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010972240706905723, 0.0010972240706905723, 0.0010972240706905723, 0.0010972240706905723, 0.0010972240706905723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010972240706905723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25878930
Iteration 2/25 | Loss: 0.00068877
Iteration 3/25 | Loss: 0.00068873
Iteration 4/25 | Loss: 0.00068873
Iteration 5/25 | Loss: 0.00068873
Iteration 6/25 | Loss: 0.00068873
Iteration 7/25 | Loss: 0.00068873
Iteration 8/25 | Loss: 0.00068873
Iteration 9/25 | Loss: 0.00068873
Iteration 10/25 | Loss: 0.00068873
Iteration 11/25 | Loss: 0.00068873
Iteration 12/25 | Loss: 0.00068873
Iteration 13/25 | Loss: 0.00068873
Iteration 14/25 | Loss: 0.00068873
Iteration 15/25 | Loss: 0.00068873
Iteration 16/25 | Loss: 0.00068873
Iteration 17/25 | Loss: 0.00068873
Iteration 18/25 | Loss: 0.00068873
Iteration 19/25 | Loss: 0.00068873
Iteration 20/25 | Loss: 0.00068873
Iteration 21/25 | Loss: 0.00068873
Iteration 22/25 | Loss: 0.00068873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006887255585752428, 0.0006887255585752428, 0.0006887255585752428, 0.0006887255585752428, 0.0006887255585752428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006887255585752428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068873
Iteration 2/1000 | Loss: 0.00005469
Iteration 3/1000 | Loss: 0.00003133
Iteration 4/1000 | Loss: 0.00002649
Iteration 5/1000 | Loss: 0.00002426
Iteration 6/1000 | Loss: 0.00002285
Iteration 7/1000 | Loss: 0.00002204
Iteration 8/1000 | Loss: 0.00002145
Iteration 9/1000 | Loss: 0.00002103
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002052
Iteration 12/1000 | Loss: 0.00002046
Iteration 13/1000 | Loss: 0.00002039
Iteration 14/1000 | Loss: 0.00002036
Iteration 15/1000 | Loss: 0.00002033
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002023
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002022
Iteration 22/1000 | Loss: 0.00002022
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002020
Iteration 26/1000 | Loss: 0.00002020
Iteration 27/1000 | Loss: 0.00002019
Iteration 28/1000 | Loss: 0.00002019
Iteration 29/1000 | Loss: 0.00002019
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002018
Iteration 32/1000 | Loss: 0.00002017
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002017
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002016
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002013
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002012
Iteration 44/1000 | Loss: 0.00002012
Iteration 45/1000 | Loss: 0.00002012
Iteration 46/1000 | Loss: 0.00002012
Iteration 47/1000 | Loss: 0.00002011
Iteration 48/1000 | Loss: 0.00002011
Iteration 49/1000 | Loss: 0.00002011
Iteration 50/1000 | Loss: 0.00002011
Iteration 51/1000 | Loss: 0.00002011
Iteration 52/1000 | Loss: 0.00002011
Iteration 53/1000 | Loss: 0.00002011
Iteration 54/1000 | Loss: 0.00002010
Iteration 55/1000 | Loss: 0.00002010
Iteration 56/1000 | Loss: 0.00002010
Iteration 57/1000 | Loss: 0.00002010
Iteration 58/1000 | Loss: 0.00002010
Iteration 59/1000 | Loss: 0.00002010
Iteration 60/1000 | Loss: 0.00002010
Iteration 61/1000 | Loss: 0.00002010
Iteration 62/1000 | Loss: 0.00002010
Iteration 63/1000 | Loss: 0.00002010
Iteration 64/1000 | Loss: 0.00002010
Iteration 65/1000 | Loss: 0.00002010
Iteration 66/1000 | Loss: 0.00002010
Iteration 67/1000 | Loss: 0.00002009
Iteration 68/1000 | Loss: 0.00002009
Iteration 69/1000 | Loss: 0.00002008
Iteration 70/1000 | Loss: 0.00002008
Iteration 71/1000 | Loss: 0.00002008
Iteration 72/1000 | Loss: 0.00002008
Iteration 73/1000 | Loss: 0.00002008
Iteration 74/1000 | Loss: 0.00002008
Iteration 75/1000 | Loss: 0.00002008
Iteration 76/1000 | Loss: 0.00002008
Iteration 77/1000 | Loss: 0.00002008
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002007
Iteration 80/1000 | Loss: 0.00002007
Iteration 81/1000 | Loss: 0.00002007
Iteration 82/1000 | Loss: 0.00002007
Iteration 83/1000 | Loss: 0.00002007
Iteration 84/1000 | Loss: 0.00002007
Iteration 85/1000 | Loss: 0.00002007
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002006
Iteration 89/1000 | Loss: 0.00002006
Iteration 90/1000 | Loss: 0.00002006
Iteration 91/1000 | Loss: 0.00002006
Iteration 92/1000 | Loss: 0.00002006
Iteration 93/1000 | Loss: 0.00002006
Iteration 94/1000 | Loss: 0.00002006
Iteration 95/1000 | Loss: 0.00002006
Iteration 96/1000 | Loss: 0.00002006
Iteration 97/1000 | Loss: 0.00002006
Iteration 98/1000 | Loss: 0.00002006
Iteration 99/1000 | Loss: 0.00002006
Iteration 100/1000 | Loss: 0.00002006
Iteration 101/1000 | Loss: 0.00002006
Iteration 102/1000 | Loss: 0.00002006
Iteration 103/1000 | Loss: 0.00002006
Iteration 104/1000 | Loss: 0.00002005
Iteration 105/1000 | Loss: 0.00002005
Iteration 106/1000 | Loss: 0.00002005
Iteration 107/1000 | Loss: 0.00002005
Iteration 108/1000 | Loss: 0.00002005
Iteration 109/1000 | Loss: 0.00002005
Iteration 110/1000 | Loss: 0.00002005
Iteration 111/1000 | Loss: 0.00002005
Iteration 112/1000 | Loss: 0.00002004
Iteration 113/1000 | Loss: 0.00002004
Iteration 114/1000 | Loss: 0.00002004
Iteration 115/1000 | Loss: 0.00002004
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002004
Iteration 119/1000 | Loss: 0.00002004
Iteration 120/1000 | Loss: 0.00002004
Iteration 121/1000 | Loss: 0.00002004
Iteration 122/1000 | Loss: 0.00002004
Iteration 123/1000 | Loss: 0.00002004
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002003
Iteration 126/1000 | Loss: 0.00002003
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002003
Iteration 130/1000 | Loss: 0.00002003
Iteration 131/1000 | Loss: 0.00002003
Iteration 132/1000 | Loss: 0.00002003
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002003
Iteration 136/1000 | Loss: 0.00002003
Iteration 137/1000 | Loss: 0.00002003
Iteration 138/1000 | Loss: 0.00002003
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002002
Iteration 143/1000 | Loss: 0.00002002
Iteration 144/1000 | Loss: 0.00002002
Iteration 145/1000 | Loss: 0.00002002
Iteration 146/1000 | Loss: 0.00002002
Iteration 147/1000 | Loss: 0.00002002
Iteration 148/1000 | Loss: 0.00002002
Iteration 149/1000 | Loss: 0.00002002
Iteration 150/1000 | Loss: 0.00002002
Iteration 151/1000 | Loss: 0.00002002
Iteration 152/1000 | Loss: 0.00002001
Iteration 153/1000 | Loss: 0.00002001
Iteration 154/1000 | Loss: 0.00002001
Iteration 155/1000 | Loss: 0.00002001
Iteration 156/1000 | Loss: 0.00002001
Iteration 157/1000 | Loss: 0.00002001
Iteration 158/1000 | Loss: 0.00002001
Iteration 159/1000 | Loss: 0.00002001
Iteration 160/1000 | Loss: 0.00002001
Iteration 161/1000 | Loss: 0.00002001
Iteration 162/1000 | Loss: 0.00002001
Iteration 163/1000 | Loss: 0.00002001
Iteration 164/1000 | Loss: 0.00002001
Iteration 165/1000 | Loss: 0.00002001
Iteration 166/1000 | Loss: 0.00002001
Iteration 167/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.000814856728539e-05, 2.000814856728539e-05, 2.000814856728539e-05, 2.000814856728539e-05, 2.000814856728539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.000814856728539e-05

Optimization complete. Final v2v error: 3.6630282402038574 mm

Highest mean error: 4.305950164794922 mm for frame 139

Lowest mean error: 2.99432110786438 mm for frame 24

Saving results

Total time: 37.22551512718201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116261
Iteration 2/25 | Loss: 0.00158048
Iteration 3/25 | Loss: 0.00119160
Iteration 4/25 | Loss: 0.00116011
Iteration 5/25 | Loss: 0.00115245
Iteration 6/25 | Loss: 0.00114972
Iteration 7/25 | Loss: 0.00114966
Iteration 8/25 | Loss: 0.00114966
Iteration 9/25 | Loss: 0.00114966
Iteration 10/25 | Loss: 0.00114966
Iteration 11/25 | Loss: 0.00114966
Iteration 12/25 | Loss: 0.00114966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011496624210849404, 0.0011496624210849404, 0.0011496624210849404, 0.0011496624210849404, 0.0011496624210849404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011496624210849404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15397847
Iteration 2/25 | Loss: 0.00086905
Iteration 3/25 | Loss: 0.00086904
Iteration 4/25 | Loss: 0.00086904
Iteration 5/25 | Loss: 0.00086904
Iteration 6/25 | Loss: 0.00086904
Iteration 7/25 | Loss: 0.00086904
Iteration 8/25 | Loss: 0.00086904
Iteration 9/25 | Loss: 0.00086904
Iteration 10/25 | Loss: 0.00086904
Iteration 11/25 | Loss: 0.00086904
Iteration 12/25 | Loss: 0.00086904
Iteration 13/25 | Loss: 0.00086904
Iteration 14/25 | Loss: 0.00086904
Iteration 15/25 | Loss: 0.00086904
Iteration 16/25 | Loss: 0.00086904
Iteration 17/25 | Loss: 0.00086904
Iteration 18/25 | Loss: 0.00086904
Iteration 19/25 | Loss: 0.00086904
Iteration 20/25 | Loss: 0.00086904
Iteration 21/25 | Loss: 0.00086904
Iteration 22/25 | Loss: 0.00086904
Iteration 23/25 | Loss: 0.00086904
Iteration 24/25 | Loss: 0.00086904
Iteration 25/25 | Loss: 0.00086904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086904
Iteration 2/1000 | Loss: 0.00007677
Iteration 3/1000 | Loss: 0.00004810
Iteration 4/1000 | Loss: 0.00003442
Iteration 5/1000 | Loss: 0.00003243
Iteration 6/1000 | Loss: 0.00003086
Iteration 7/1000 | Loss: 0.00003006
Iteration 8/1000 | Loss: 0.00002943
Iteration 9/1000 | Loss: 0.00002897
Iteration 10/1000 | Loss: 0.00002862
Iteration 11/1000 | Loss: 0.00002828
Iteration 12/1000 | Loss: 0.00002802
Iteration 13/1000 | Loss: 0.00002783
Iteration 14/1000 | Loss: 0.00002766
Iteration 15/1000 | Loss: 0.00002755
Iteration 16/1000 | Loss: 0.00002745
Iteration 17/1000 | Loss: 0.00002730
Iteration 18/1000 | Loss: 0.00002730
Iteration 19/1000 | Loss: 0.00002726
Iteration 20/1000 | Loss: 0.00002724
Iteration 21/1000 | Loss: 0.00002720
Iteration 22/1000 | Loss: 0.00002719
Iteration 23/1000 | Loss: 0.00002718
Iteration 24/1000 | Loss: 0.00002717
Iteration 25/1000 | Loss: 0.00002713
Iteration 26/1000 | Loss: 0.00002712
Iteration 27/1000 | Loss: 0.00002712
Iteration 28/1000 | Loss: 0.00002711
Iteration 29/1000 | Loss: 0.00002711
Iteration 30/1000 | Loss: 0.00002711
Iteration 31/1000 | Loss: 0.00002710
Iteration 32/1000 | Loss: 0.00002710
Iteration 33/1000 | Loss: 0.00002710
Iteration 34/1000 | Loss: 0.00002709
Iteration 35/1000 | Loss: 0.00002709
Iteration 36/1000 | Loss: 0.00002707
Iteration 37/1000 | Loss: 0.00002706
Iteration 38/1000 | Loss: 0.00002702
Iteration 39/1000 | Loss: 0.00002702
Iteration 40/1000 | Loss: 0.00002702
Iteration 41/1000 | Loss: 0.00002702
Iteration 42/1000 | Loss: 0.00002702
Iteration 43/1000 | Loss: 0.00002702
Iteration 44/1000 | Loss: 0.00002702
Iteration 45/1000 | Loss: 0.00002702
Iteration 46/1000 | Loss: 0.00002702
Iteration 47/1000 | Loss: 0.00002702
Iteration 48/1000 | Loss: 0.00002702
Iteration 49/1000 | Loss: 0.00002701
Iteration 50/1000 | Loss: 0.00002701
Iteration 51/1000 | Loss: 0.00002701
Iteration 52/1000 | Loss: 0.00002699
Iteration 53/1000 | Loss: 0.00002699
Iteration 54/1000 | Loss: 0.00002699
Iteration 55/1000 | Loss: 0.00002699
Iteration 56/1000 | Loss: 0.00002699
Iteration 57/1000 | Loss: 0.00002699
Iteration 58/1000 | Loss: 0.00002699
Iteration 59/1000 | Loss: 0.00002699
Iteration 60/1000 | Loss: 0.00002699
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002698
Iteration 64/1000 | Loss: 0.00002698
Iteration 65/1000 | Loss: 0.00002698
Iteration 66/1000 | Loss: 0.00002698
Iteration 67/1000 | Loss: 0.00002698
Iteration 68/1000 | Loss: 0.00002697
Iteration 69/1000 | Loss: 0.00002697
Iteration 70/1000 | Loss: 0.00002697
Iteration 71/1000 | Loss: 0.00002697
Iteration 72/1000 | Loss: 0.00002697
Iteration 73/1000 | Loss: 0.00002697
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002697
Iteration 76/1000 | Loss: 0.00002696
Iteration 77/1000 | Loss: 0.00002696
Iteration 78/1000 | Loss: 0.00002696
Iteration 79/1000 | Loss: 0.00002696
Iteration 80/1000 | Loss: 0.00002696
Iteration 81/1000 | Loss: 0.00002696
Iteration 82/1000 | Loss: 0.00002696
Iteration 83/1000 | Loss: 0.00002696
Iteration 84/1000 | Loss: 0.00002695
Iteration 85/1000 | Loss: 0.00002695
Iteration 86/1000 | Loss: 0.00002695
Iteration 87/1000 | Loss: 0.00002694
Iteration 88/1000 | Loss: 0.00002694
Iteration 89/1000 | Loss: 0.00002694
Iteration 90/1000 | Loss: 0.00002694
Iteration 91/1000 | Loss: 0.00002694
Iteration 92/1000 | Loss: 0.00002693
Iteration 93/1000 | Loss: 0.00002693
Iteration 94/1000 | Loss: 0.00002693
Iteration 95/1000 | Loss: 0.00002693
Iteration 96/1000 | Loss: 0.00002692
Iteration 97/1000 | Loss: 0.00002692
Iteration 98/1000 | Loss: 0.00002692
Iteration 99/1000 | Loss: 0.00002692
Iteration 100/1000 | Loss: 0.00002692
Iteration 101/1000 | Loss: 0.00002692
Iteration 102/1000 | Loss: 0.00002691
Iteration 103/1000 | Loss: 0.00002691
Iteration 104/1000 | Loss: 0.00002691
Iteration 105/1000 | Loss: 0.00002691
Iteration 106/1000 | Loss: 0.00002690
Iteration 107/1000 | Loss: 0.00002690
Iteration 108/1000 | Loss: 0.00002690
Iteration 109/1000 | Loss: 0.00002690
Iteration 110/1000 | Loss: 0.00002690
Iteration 111/1000 | Loss: 0.00002690
Iteration 112/1000 | Loss: 0.00002689
Iteration 113/1000 | Loss: 0.00002689
Iteration 114/1000 | Loss: 0.00002689
Iteration 115/1000 | Loss: 0.00002689
Iteration 116/1000 | Loss: 0.00002689
Iteration 117/1000 | Loss: 0.00002689
Iteration 118/1000 | Loss: 0.00002688
Iteration 119/1000 | Loss: 0.00002688
Iteration 120/1000 | Loss: 0.00002688
Iteration 121/1000 | Loss: 0.00002688
Iteration 122/1000 | Loss: 0.00002688
Iteration 123/1000 | Loss: 0.00002687
Iteration 124/1000 | Loss: 0.00002687
Iteration 125/1000 | Loss: 0.00002687
Iteration 126/1000 | Loss: 0.00002687
Iteration 127/1000 | Loss: 0.00002687
Iteration 128/1000 | Loss: 0.00002687
Iteration 129/1000 | Loss: 0.00002686
Iteration 130/1000 | Loss: 0.00002686
Iteration 131/1000 | Loss: 0.00002686
Iteration 132/1000 | Loss: 0.00002686
Iteration 133/1000 | Loss: 0.00002686
Iteration 134/1000 | Loss: 0.00002686
Iteration 135/1000 | Loss: 0.00002685
Iteration 136/1000 | Loss: 0.00002685
Iteration 137/1000 | Loss: 0.00002685
Iteration 138/1000 | Loss: 0.00002685
Iteration 139/1000 | Loss: 0.00002685
Iteration 140/1000 | Loss: 0.00002685
Iteration 141/1000 | Loss: 0.00002685
Iteration 142/1000 | Loss: 0.00002685
Iteration 143/1000 | Loss: 0.00002685
Iteration 144/1000 | Loss: 0.00002685
Iteration 145/1000 | Loss: 0.00002684
Iteration 146/1000 | Loss: 0.00002684
Iteration 147/1000 | Loss: 0.00002684
Iteration 148/1000 | Loss: 0.00002684
Iteration 149/1000 | Loss: 0.00002684
Iteration 150/1000 | Loss: 0.00002684
Iteration 151/1000 | Loss: 0.00002684
Iteration 152/1000 | Loss: 0.00002684
Iteration 153/1000 | Loss: 0.00002684
Iteration 154/1000 | Loss: 0.00002684
Iteration 155/1000 | Loss: 0.00002684
Iteration 156/1000 | Loss: 0.00002684
Iteration 157/1000 | Loss: 0.00002684
Iteration 158/1000 | Loss: 0.00002684
Iteration 159/1000 | Loss: 0.00002683
Iteration 160/1000 | Loss: 0.00002683
Iteration 161/1000 | Loss: 0.00002683
Iteration 162/1000 | Loss: 0.00002683
Iteration 163/1000 | Loss: 0.00002683
Iteration 164/1000 | Loss: 0.00002683
Iteration 165/1000 | Loss: 0.00002683
Iteration 166/1000 | Loss: 0.00002683
Iteration 167/1000 | Loss: 0.00002683
Iteration 168/1000 | Loss: 0.00002683
Iteration 169/1000 | Loss: 0.00002683
Iteration 170/1000 | Loss: 0.00002683
Iteration 171/1000 | Loss: 0.00002683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.683381717361044e-05, 2.683381717361044e-05, 2.683381717361044e-05, 2.683381717361044e-05, 2.683381717361044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.683381717361044e-05

Optimization complete. Final v2v error: 4.1981520652771 mm

Highest mean error: 5.504697799682617 mm for frame 150

Lowest mean error: 3.387795925140381 mm for frame 2

Saving results

Total time: 53.062344551086426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405669
Iteration 2/25 | Loss: 0.00108501
Iteration 3/25 | Loss: 0.00098392
Iteration 4/25 | Loss: 0.00096943
Iteration 5/25 | Loss: 0.00096635
Iteration 6/25 | Loss: 0.00096589
Iteration 7/25 | Loss: 0.00096589
Iteration 8/25 | Loss: 0.00096589
Iteration 9/25 | Loss: 0.00096589
Iteration 10/25 | Loss: 0.00096589
Iteration 11/25 | Loss: 0.00096589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009658915805630386, 0.0009658915805630386, 0.0009658915805630386, 0.0009658915805630386, 0.0009658915805630386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009658915805630386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42449999
Iteration 2/25 | Loss: 0.00068814
Iteration 3/25 | Loss: 0.00068811
Iteration 4/25 | Loss: 0.00068811
Iteration 5/25 | Loss: 0.00068811
Iteration 6/25 | Loss: 0.00068811
Iteration 7/25 | Loss: 0.00068811
Iteration 8/25 | Loss: 0.00068811
Iteration 9/25 | Loss: 0.00068811
Iteration 10/25 | Loss: 0.00068811
Iteration 11/25 | Loss: 0.00068811
Iteration 12/25 | Loss: 0.00068811
Iteration 13/25 | Loss: 0.00068811
Iteration 14/25 | Loss: 0.00068811
Iteration 15/25 | Loss: 0.00068811
Iteration 16/25 | Loss: 0.00068811
Iteration 17/25 | Loss: 0.00068811
Iteration 18/25 | Loss: 0.00068811
Iteration 19/25 | Loss: 0.00068811
Iteration 20/25 | Loss: 0.00068811
Iteration 21/25 | Loss: 0.00068811
Iteration 22/25 | Loss: 0.00068811
Iteration 23/25 | Loss: 0.00068811
Iteration 24/25 | Loss: 0.00068811
Iteration 25/25 | Loss: 0.00068811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068811
Iteration 2/1000 | Loss: 0.00002934
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001924
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001906
Iteration 8/1000 | Loss: 0.00001869
Iteration 9/1000 | Loss: 0.00001839
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001797
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001777
Iteration 17/1000 | Loss: 0.00001772
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001764
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001761
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001761
Iteration 28/1000 | Loss: 0.00001761
Iteration 29/1000 | Loss: 0.00001761
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001755
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001755
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001752
Iteration 54/1000 | Loss: 0.00001752
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001752
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001747
Iteration 96/1000 | Loss: 0.00001747
Iteration 97/1000 | Loss: 0.00001747
Iteration 98/1000 | Loss: 0.00001747
Iteration 99/1000 | Loss: 0.00001747
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001746
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001746
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001746
Iteration 116/1000 | Loss: 0.00001746
Iteration 117/1000 | Loss: 0.00001746
Iteration 118/1000 | Loss: 0.00001746
Iteration 119/1000 | Loss: 0.00001746
Iteration 120/1000 | Loss: 0.00001746
Iteration 121/1000 | Loss: 0.00001746
Iteration 122/1000 | Loss: 0.00001746
Iteration 123/1000 | Loss: 0.00001746
Iteration 124/1000 | Loss: 0.00001746
Iteration 125/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.7459877199144103e-05, 1.7459877199144103e-05, 1.7459877199144103e-05, 1.7459877199144103e-05, 1.7459877199144103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7459877199144103e-05

Optimization complete. Final v2v error: 3.538848638534546 mm

Highest mean error: 3.8206396102905273 mm for frame 133

Lowest mean error: 3.1849000453948975 mm for frame 19

Saving results

Total time: 32.87175917625427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085801
Iteration 2/25 | Loss: 0.01085800
Iteration 3/25 | Loss: 0.01085800
Iteration 4/25 | Loss: 0.01085800
Iteration 5/25 | Loss: 0.00243268
Iteration 6/25 | Loss: 0.00166733
Iteration 7/25 | Loss: 0.00126785
Iteration 8/25 | Loss: 0.00116166
Iteration 9/25 | Loss: 0.00111160
Iteration 10/25 | Loss: 0.00110061
Iteration 11/25 | Loss: 0.00109224
Iteration 12/25 | Loss: 0.00108926
Iteration 13/25 | Loss: 0.00109174
Iteration 14/25 | Loss: 0.00109392
Iteration 15/25 | Loss: 0.00108992
Iteration 16/25 | Loss: 0.00108332
Iteration 17/25 | Loss: 0.00108046
Iteration 18/25 | Loss: 0.00108561
Iteration 19/25 | Loss: 0.00108295
Iteration 20/25 | Loss: 0.00108024
Iteration 21/25 | Loss: 0.00108006
Iteration 22/25 | Loss: 0.00107994
Iteration 23/25 | Loss: 0.00107967
Iteration 24/25 | Loss: 0.00107930
Iteration 25/25 | Loss: 0.00107914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28461742
Iteration 2/25 | Loss: 0.00140650
Iteration 3/25 | Loss: 0.00140650
Iteration 4/25 | Loss: 0.00140650
Iteration 5/25 | Loss: 0.00140650
Iteration 6/25 | Loss: 0.00140649
Iteration 7/25 | Loss: 0.00140649
Iteration 8/25 | Loss: 0.00140649
Iteration 9/25 | Loss: 0.00140649
Iteration 10/25 | Loss: 0.00140649
Iteration 11/25 | Loss: 0.00140649
Iteration 12/25 | Loss: 0.00140649
Iteration 13/25 | Loss: 0.00140649
Iteration 14/25 | Loss: 0.00140649
Iteration 15/25 | Loss: 0.00140649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001406493945978582, 0.001406493945978582, 0.001406493945978582, 0.001406493945978582, 0.001406493945978582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001406493945978582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140649
Iteration 2/1000 | Loss: 0.00012447
Iteration 3/1000 | Loss: 0.00113525
Iteration 4/1000 | Loss: 0.00116407
Iteration 5/1000 | Loss: 0.00007815
Iteration 6/1000 | Loss: 0.00007172
Iteration 7/1000 | Loss: 0.00010094
Iteration 8/1000 | Loss: 0.00004363
Iteration 9/1000 | Loss: 0.00017535
Iteration 10/1000 | Loss: 0.00004142
Iteration 11/1000 | Loss: 0.00019511
Iteration 12/1000 | Loss: 0.00003840
Iteration 13/1000 | Loss: 0.00003758
Iteration 14/1000 | Loss: 0.00003620
Iteration 15/1000 | Loss: 0.00003556
Iteration 16/1000 | Loss: 0.00013133
Iteration 17/1000 | Loss: 0.00003492
Iteration 18/1000 | Loss: 0.00003442
Iteration 19/1000 | Loss: 0.00003407
Iteration 20/1000 | Loss: 0.00003386
Iteration 21/1000 | Loss: 0.00003361
Iteration 22/1000 | Loss: 0.00004557
Iteration 23/1000 | Loss: 0.00003337
Iteration 24/1000 | Loss: 0.00003337
Iteration 25/1000 | Loss: 0.00021950
Iteration 26/1000 | Loss: 0.00044620
Iteration 27/1000 | Loss: 0.00021796
Iteration 28/1000 | Loss: 0.00026596
Iteration 29/1000 | Loss: 0.00021418
Iteration 30/1000 | Loss: 0.00003678
Iteration 31/1000 | Loss: 0.00004337
Iteration 32/1000 | Loss: 0.00006670
Iteration 33/1000 | Loss: 0.00004155
Iteration 34/1000 | Loss: 0.00007920
Iteration 35/1000 | Loss: 0.00006458
Iteration 36/1000 | Loss: 0.00003357
Iteration 37/1000 | Loss: 0.00004662
Iteration 38/1000 | Loss: 0.00003503
Iteration 39/1000 | Loss: 0.00003325
Iteration 40/1000 | Loss: 0.00003320
Iteration 41/1000 | Loss: 0.00003320
Iteration 42/1000 | Loss: 0.00003320
Iteration 43/1000 | Loss: 0.00003320
Iteration 44/1000 | Loss: 0.00003320
Iteration 45/1000 | Loss: 0.00003319
Iteration 46/1000 | Loss: 0.00003319
Iteration 47/1000 | Loss: 0.00003319
Iteration 48/1000 | Loss: 0.00003835
Iteration 49/1000 | Loss: 0.00003315
Iteration 50/1000 | Loss: 0.00003906
Iteration 51/1000 | Loss: 0.00003308
Iteration 52/1000 | Loss: 0.00003308
Iteration 53/1000 | Loss: 0.00003308
Iteration 54/1000 | Loss: 0.00003308
Iteration 55/1000 | Loss: 0.00003308
Iteration 56/1000 | Loss: 0.00003308
Iteration 57/1000 | Loss: 0.00003308
Iteration 58/1000 | Loss: 0.00003308
Iteration 59/1000 | Loss: 0.00003308
Iteration 60/1000 | Loss: 0.00003307
Iteration 61/1000 | Loss: 0.00003355
Iteration 62/1000 | Loss: 0.00003307
Iteration 63/1000 | Loss: 0.00003307
Iteration 64/1000 | Loss: 0.00003307
Iteration 65/1000 | Loss: 0.00003307
Iteration 66/1000 | Loss: 0.00003306
Iteration 67/1000 | Loss: 0.00003306
Iteration 68/1000 | Loss: 0.00003306
Iteration 69/1000 | Loss: 0.00003306
Iteration 70/1000 | Loss: 0.00003306
Iteration 71/1000 | Loss: 0.00003306
Iteration 72/1000 | Loss: 0.00003306
Iteration 73/1000 | Loss: 0.00003306
Iteration 74/1000 | Loss: 0.00003306
Iteration 75/1000 | Loss: 0.00003306
Iteration 76/1000 | Loss: 0.00003306
Iteration 77/1000 | Loss: 0.00003306
Iteration 78/1000 | Loss: 0.00003305
Iteration 79/1000 | Loss: 0.00003305
Iteration 80/1000 | Loss: 0.00003305
Iteration 81/1000 | Loss: 0.00003315
Iteration 82/1000 | Loss: 0.00003305
Iteration 83/1000 | Loss: 0.00003305
Iteration 84/1000 | Loss: 0.00003305
Iteration 85/1000 | Loss: 0.00003305
Iteration 86/1000 | Loss: 0.00003305
Iteration 87/1000 | Loss: 0.00003305
Iteration 88/1000 | Loss: 0.00003304
Iteration 89/1000 | Loss: 0.00003304
Iteration 90/1000 | Loss: 0.00003304
Iteration 91/1000 | Loss: 0.00003304
Iteration 92/1000 | Loss: 0.00003304
Iteration 93/1000 | Loss: 0.00003304
Iteration 94/1000 | Loss: 0.00003304
Iteration 95/1000 | Loss: 0.00003304
Iteration 96/1000 | Loss: 0.00003304
Iteration 97/1000 | Loss: 0.00003304
Iteration 98/1000 | Loss: 0.00003304
Iteration 99/1000 | Loss: 0.00003304
Iteration 100/1000 | Loss: 0.00003304
Iteration 101/1000 | Loss: 0.00003304
Iteration 102/1000 | Loss: 0.00003304
Iteration 103/1000 | Loss: 0.00003304
Iteration 104/1000 | Loss: 0.00003304
Iteration 105/1000 | Loss: 0.00003304
Iteration 106/1000 | Loss: 0.00003304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.3038948458852246e-05, 3.3038948458852246e-05, 3.3038948458852246e-05, 3.3038948458852246e-05, 3.3038948458852246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3038948458852246e-05

Optimization complete. Final v2v error: 4.331180095672607 mm

Highest mean error: 19.17671012878418 mm for frame 12

Lowest mean error: 3.3551297187805176 mm for frame 3

Saving results

Total time: 106.55863666534424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029322
Iteration 2/25 | Loss: 0.00262631
Iteration 3/25 | Loss: 0.00176605
Iteration 4/25 | Loss: 0.00158021
Iteration 5/25 | Loss: 0.00139417
Iteration 6/25 | Loss: 0.00137580
Iteration 7/25 | Loss: 0.00129671
Iteration 8/25 | Loss: 0.00127677
Iteration 9/25 | Loss: 0.00123290
Iteration 10/25 | Loss: 0.00123176
Iteration 11/25 | Loss: 0.00121168
Iteration 12/25 | Loss: 0.00120699
Iteration 13/25 | Loss: 0.00120616
Iteration 14/25 | Loss: 0.00120557
Iteration 15/25 | Loss: 0.00120943
Iteration 16/25 | Loss: 0.00120955
Iteration 17/25 | Loss: 0.00120498
Iteration 18/25 | Loss: 0.00120464
Iteration 19/25 | Loss: 0.00120039
Iteration 20/25 | Loss: 0.00119954
Iteration 21/25 | Loss: 0.00119944
Iteration 22/25 | Loss: 0.00119944
Iteration 23/25 | Loss: 0.00119944
Iteration 24/25 | Loss: 0.00119944
Iteration 25/25 | Loss: 0.00119944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34701586
Iteration 2/25 | Loss: 0.00160160
Iteration 3/25 | Loss: 0.00144579
Iteration 4/25 | Loss: 0.00144579
Iteration 5/25 | Loss: 0.00144579
Iteration 6/25 | Loss: 0.00144579
Iteration 7/25 | Loss: 0.00144579
Iteration 8/25 | Loss: 0.00144579
Iteration 9/25 | Loss: 0.00144579
Iteration 10/25 | Loss: 0.00144579
Iteration 11/25 | Loss: 0.00144579
Iteration 12/25 | Loss: 0.00144579
Iteration 13/25 | Loss: 0.00144579
Iteration 14/25 | Loss: 0.00144579
Iteration 15/25 | Loss: 0.00144579
Iteration 16/25 | Loss: 0.00144579
Iteration 17/25 | Loss: 0.00144579
Iteration 18/25 | Loss: 0.00144579
Iteration 19/25 | Loss: 0.00144579
Iteration 20/25 | Loss: 0.00144579
Iteration 21/25 | Loss: 0.00144579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014457899378612638, 0.0014457899378612638, 0.0014457899378612638, 0.0014457899378612638, 0.0014457899378612638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014457899378612638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144579
Iteration 2/1000 | Loss: 0.00039285
Iteration 3/1000 | Loss: 0.00058666
Iteration 4/1000 | Loss: 0.00015659
Iteration 5/1000 | Loss: 0.00009327
Iteration 6/1000 | Loss: 0.00023766
Iteration 7/1000 | Loss: 0.00024449
Iteration 8/1000 | Loss: 0.00021971
Iteration 9/1000 | Loss: 0.00008274
Iteration 10/1000 | Loss: 0.00008945
Iteration 11/1000 | Loss: 0.00007329
Iteration 12/1000 | Loss: 0.00044449
Iteration 13/1000 | Loss: 0.00264026
Iteration 14/1000 | Loss: 0.00077105
Iteration 15/1000 | Loss: 0.00082265
Iteration 16/1000 | Loss: 0.00413439
Iteration 17/1000 | Loss: 0.00267331
Iteration 18/1000 | Loss: 0.00395293
Iteration 19/1000 | Loss: 0.00155940
Iteration 20/1000 | Loss: 0.00119396
Iteration 21/1000 | Loss: 0.00080405
Iteration 22/1000 | Loss: 0.00195334
Iteration 23/1000 | Loss: 0.00271796
Iteration 24/1000 | Loss: 0.00254535
Iteration 25/1000 | Loss: 0.00587986
Iteration 26/1000 | Loss: 0.00171388
Iteration 27/1000 | Loss: 0.00071832
Iteration 28/1000 | Loss: 0.00088943
Iteration 29/1000 | Loss: 0.00106479
Iteration 30/1000 | Loss: 0.00016491
Iteration 31/1000 | Loss: 0.00014542
Iteration 32/1000 | Loss: 0.00009900
Iteration 33/1000 | Loss: 0.00018135
Iteration 34/1000 | Loss: 0.00009442
Iteration 35/1000 | Loss: 0.00052480
Iteration 36/1000 | Loss: 0.00012682
Iteration 37/1000 | Loss: 0.00008587
Iteration 38/1000 | Loss: 0.00019877
Iteration 39/1000 | Loss: 0.00005156
Iteration 40/1000 | Loss: 0.00008036
Iteration 41/1000 | Loss: 0.00004274
Iteration 42/1000 | Loss: 0.00046111
Iteration 43/1000 | Loss: 0.00028481
Iteration 44/1000 | Loss: 0.00074457
Iteration 45/1000 | Loss: 0.00047867
Iteration 46/1000 | Loss: 0.00033235
Iteration 47/1000 | Loss: 0.00037234
Iteration 48/1000 | Loss: 0.00003689
Iteration 49/1000 | Loss: 0.00048447
Iteration 50/1000 | Loss: 0.00032540
Iteration 51/1000 | Loss: 0.00034031
Iteration 52/1000 | Loss: 0.00012787
Iteration 53/1000 | Loss: 0.00003521
Iteration 54/1000 | Loss: 0.00002681
Iteration 55/1000 | Loss: 0.00008993
Iteration 56/1000 | Loss: 0.00002483
Iteration 57/1000 | Loss: 0.00034631
Iteration 58/1000 | Loss: 0.00013110
Iteration 59/1000 | Loss: 0.00047457
Iteration 60/1000 | Loss: 0.00050911
Iteration 61/1000 | Loss: 0.00005167
Iteration 62/1000 | Loss: 0.00017041
Iteration 63/1000 | Loss: 0.00004842
Iteration 64/1000 | Loss: 0.00012283
Iteration 65/1000 | Loss: 0.00007578
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002393
Iteration 68/1000 | Loss: 0.00009625
Iteration 69/1000 | Loss: 0.00006161
Iteration 70/1000 | Loss: 0.00002304
Iteration 71/1000 | Loss: 0.00002243
Iteration 72/1000 | Loss: 0.00011913
Iteration 73/1000 | Loss: 0.00002346
Iteration 74/1000 | Loss: 0.00002524
Iteration 75/1000 | Loss: 0.00018753
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00006350
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00007041
Iteration 80/1000 | Loss: 0.00012389
Iteration 81/1000 | Loss: 0.00001935
Iteration 82/1000 | Loss: 0.00001889
Iteration 83/1000 | Loss: 0.00005380
Iteration 84/1000 | Loss: 0.00010342
Iteration 85/1000 | Loss: 0.00007039
Iteration 86/1000 | Loss: 0.00007385
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001905
Iteration 89/1000 | Loss: 0.00006822
Iteration 90/1000 | Loss: 0.00004795
Iteration 91/1000 | Loss: 0.00001879
Iteration 92/1000 | Loss: 0.00005446
Iteration 93/1000 | Loss: 0.00004191
Iteration 94/1000 | Loss: 0.00004067
Iteration 95/1000 | Loss: 0.00004930
Iteration 96/1000 | Loss: 0.00002020
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001852
Iteration 101/1000 | Loss: 0.00001852
Iteration 102/1000 | Loss: 0.00001852
Iteration 103/1000 | Loss: 0.00001851
Iteration 104/1000 | Loss: 0.00001851
Iteration 105/1000 | Loss: 0.00001851
Iteration 106/1000 | Loss: 0.00003875
Iteration 107/1000 | Loss: 0.00001848
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001847
Iteration 115/1000 | Loss: 0.00001847
Iteration 116/1000 | Loss: 0.00001847
Iteration 117/1000 | Loss: 0.00001847
Iteration 118/1000 | Loss: 0.00001847
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001844
Iteration 131/1000 | Loss: 0.00001844
Iteration 132/1000 | Loss: 0.00001844
Iteration 133/1000 | Loss: 0.00001844
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001843
Iteration 139/1000 | Loss: 0.00001843
Iteration 140/1000 | Loss: 0.00001843
Iteration 141/1000 | Loss: 0.00001843
Iteration 142/1000 | Loss: 0.00001842
Iteration 143/1000 | Loss: 0.00001842
Iteration 144/1000 | Loss: 0.00001842
Iteration 145/1000 | Loss: 0.00001842
Iteration 146/1000 | Loss: 0.00001842
Iteration 147/1000 | Loss: 0.00001842
Iteration 148/1000 | Loss: 0.00001842
Iteration 149/1000 | Loss: 0.00001842
Iteration 150/1000 | Loss: 0.00001842
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001841
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Iteration 158/1000 | Loss: 0.00001841
Iteration 159/1000 | Loss: 0.00001841
Iteration 160/1000 | Loss: 0.00001841
Iteration 161/1000 | Loss: 0.00001841
Iteration 162/1000 | Loss: 0.00001841
Iteration 163/1000 | Loss: 0.00001841
Iteration 164/1000 | Loss: 0.00001841
Iteration 165/1000 | Loss: 0.00001841
Iteration 166/1000 | Loss: 0.00001841
Iteration 167/1000 | Loss: 0.00001841
Iteration 168/1000 | Loss: 0.00001841
Iteration 169/1000 | Loss: 0.00001841
Iteration 170/1000 | Loss: 0.00001840
Iteration 171/1000 | Loss: 0.00001840
Iteration 172/1000 | Loss: 0.00001840
Iteration 173/1000 | Loss: 0.00001840
Iteration 174/1000 | Loss: 0.00001840
Iteration 175/1000 | Loss: 0.00001840
Iteration 176/1000 | Loss: 0.00001840
Iteration 177/1000 | Loss: 0.00001840
Iteration 178/1000 | Loss: 0.00001840
Iteration 179/1000 | Loss: 0.00001840
Iteration 180/1000 | Loss: 0.00001840
Iteration 181/1000 | Loss: 0.00001840
Iteration 182/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.8404143702355213e-05, 1.8404143702355213e-05, 1.8404143702355213e-05, 1.8404143702355213e-05, 1.8404143702355213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8404143702355213e-05

Optimization complete. Final v2v error: 3.5901927947998047 mm

Highest mean error: 8.874512672424316 mm for frame 124

Lowest mean error: 3.051241159439087 mm for frame 102

Saving results

Total time: 178.05627870559692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547255
Iteration 2/25 | Loss: 0.00122917
Iteration 3/25 | Loss: 0.00100021
Iteration 4/25 | Loss: 0.00095633
Iteration 5/25 | Loss: 0.00094614
Iteration 6/25 | Loss: 0.00092426
Iteration 7/25 | Loss: 0.00091996
Iteration 8/25 | Loss: 0.00091870
Iteration 9/25 | Loss: 0.00092777
Iteration 10/25 | Loss: 0.00093118
Iteration 11/25 | Loss: 0.00091038
Iteration 12/25 | Loss: 0.00090974
Iteration 13/25 | Loss: 0.00090971
Iteration 14/25 | Loss: 0.00090970
Iteration 15/25 | Loss: 0.00090970
Iteration 16/25 | Loss: 0.00090970
Iteration 17/25 | Loss: 0.00090970
Iteration 18/25 | Loss: 0.00090970
Iteration 19/25 | Loss: 0.00090970
Iteration 20/25 | Loss: 0.00090970
Iteration 21/25 | Loss: 0.00090968
Iteration 22/25 | Loss: 0.00090968
Iteration 23/25 | Loss: 0.00090968
Iteration 24/25 | Loss: 0.00090968
Iteration 25/25 | Loss: 0.00090968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70268929
Iteration 2/25 | Loss: 0.00079374
Iteration 3/25 | Loss: 0.00079372
Iteration 4/25 | Loss: 0.00079372
Iteration 5/25 | Loss: 0.00079372
Iteration 6/25 | Loss: 0.00079371
Iteration 7/25 | Loss: 0.00079371
Iteration 8/25 | Loss: 0.00079371
Iteration 9/25 | Loss: 0.00079371
Iteration 10/25 | Loss: 0.00079371
Iteration 11/25 | Loss: 0.00079371
Iteration 12/25 | Loss: 0.00079371
Iteration 13/25 | Loss: 0.00079371
Iteration 14/25 | Loss: 0.00079371
Iteration 15/25 | Loss: 0.00079371
Iteration 16/25 | Loss: 0.00079371
Iteration 17/25 | Loss: 0.00079371
Iteration 18/25 | Loss: 0.00079371
Iteration 19/25 | Loss: 0.00079371
Iteration 20/25 | Loss: 0.00079371
Iteration 21/25 | Loss: 0.00079371
Iteration 22/25 | Loss: 0.00079371
Iteration 23/25 | Loss: 0.00079371
Iteration 24/25 | Loss: 0.00079371
Iteration 25/25 | Loss: 0.00079371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000793713319581002, 0.000793713319581002, 0.000793713319581002, 0.000793713319581002, 0.000793713319581002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000793713319581002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079371
Iteration 2/1000 | Loss: 0.00003426
Iteration 3/1000 | Loss: 0.00002255
Iteration 4/1000 | Loss: 0.00001845
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001616
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001520
Iteration 9/1000 | Loss: 0.00001491
Iteration 10/1000 | Loss: 0.00001466
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001445
Iteration 13/1000 | Loss: 0.00001439
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001437
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001427
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001409
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001408
Iteration 48/1000 | Loss: 0.00001408
Iteration 49/1000 | Loss: 0.00001408
Iteration 50/1000 | Loss: 0.00001408
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001405
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001398
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001396
Iteration 90/1000 | Loss: 0.00001396
Iteration 91/1000 | Loss: 0.00001396
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001395
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001395
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001392
Iteration 114/1000 | Loss: 0.00001392
Iteration 115/1000 | Loss: 0.00001392
Iteration 116/1000 | Loss: 0.00001392
Iteration 117/1000 | Loss: 0.00001392
Iteration 118/1000 | Loss: 0.00001392
Iteration 119/1000 | Loss: 0.00001392
Iteration 120/1000 | Loss: 0.00001392
Iteration 121/1000 | Loss: 0.00001392
Iteration 122/1000 | Loss: 0.00001392
Iteration 123/1000 | Loss: 0.00001392
Iteration 124/1000 | Loss: 0.00001392
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001392
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.3915641829953529e-05, 1.3915641829953529e-05, 1.3915641829953529e-05, 1.3915641829953529e-05, 1.3915641829953529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3915641829953529e-05

Optimization complete. Final v2v error: 3.004570245742798 mm

Highest mean error: 4.4918293952941895 mm for frame 107

Lowest mean error: 2.4053664207458496 mm for frame 126

Saving results

Total time: 57.190969467163086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405501
Iteration 2/25 | Loss: 0.00111964
Iteration 3/25 | Loss: 0.00099311
Iteration 4/25 | Loss: 0.00096993
Iteration 5/25 | Loss: 0.00096448
Iteration 6/25 | Loss: 0.00096307
Iteration 7/25 | Loss: 0.00096298
Iteration 8/25 | Loss: 0.00096298
Iteration 9/25 | Loss: 0.00096298
Iteration 10/25 | Loss: 0.00096298
Iteration 11/25 | Loss: 0.00096298
Iteration 12/25 | Loss: 0.00096298
Iteration 13/25 | Loss: 0.00096298
Iteration 14/25 | Loss: 0.00096298
Iteration 15/25 | Loss: 0.00096298
Iteration 16/25 | Loss: 0.00096298
Iteration 17/25 | Loss: 0.00096298
Iteration 18/25 | Loss: 0.00096298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009629760752432048, 0.0009629760752432048, 0.0009629760752432048, 0.0009629760752432048, 0.0009629760752432048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009629760752432048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94817865
Iteration 2/25 | Loss: 0.00065646
Iteration 3/25 | Loss: 0.00065646
Iteration 4/25 | Loss: 0.00065646
Iteration 5/25 | Loss: 0.00065645
Iteration 6/25 | Loss: 0.00065645
Iteration 7/25 | Loss: 0.00065645
Iteration 8/25 | Loss: 0.00065645
Iteration 9/25 | Loss: 0.00065645
Iteration 10/25 | Loss: 0.00065645
Iteration 11/25 | Loss: 0.00065645
Iteration 12/25 | Loss: 0.00065645
Iteration 13/25 | Loss: 0.00065645
Iteration 14/25 | Loss: 0.00065645
Iteration 15/25 | Loss: 0.00065645
Iteration 16/25 | Loss: 0.00065645
Iteration 17/25 | Loss: 0.00065645
Iteration 18/25 | Loss: 0.00065645
Iteration 19/25 | Loss: 0.00065645
Iteration 20/25 | Loss: 0.00065645
Iteration 21/25 | Loss: 0.00065645
Iteration 22/25 | Loss: 0.00065645
Iteration 23/25 | Loss: 0.00065645
Iteration 24/25 | Loss: 0.00065645
Iteration 25/25 | Loss: 0.00065645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065645
Iteration 2/1000 | Loss: 0.00003349
Iteration 3/1000 | Loss: 0.00002301
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00002062
Iteration 6/1000 | Loss: 0.00001992
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001864
Iteration 11/1000 | Loss: 0.00001855
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001836
Iteration 15/1000 | Loss: 0.00001826
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001823
Iteration 18/1000 | Loss: 0.00001817
Iteration 19/1000 | Loss: 0.00001817
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001813
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001812
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001810
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001809
Iteration 39/1000 | Loss: 0.00001809
Iteration 40/1000 | Loss: 0.00001809
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001803
Iteration 83/1000 | Loss: 0.00001803
Iteration 84/1000 | Loss: 0.00001803
Iteration 85/1000 | Loss: 0.00001803
Iteration 86/1000 | Loss: 0.00001803
Iteration 87/1000 | Loss: 0.00001803
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001801
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001800
Iteration 107/1000 | Loss: 0.00001800
Iteration 108/1000 | Loss: 0.00001800
Iteration 109/1000 | Loss: 0.00001800
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001800
Iteration 112/1000 | Loss: 0.00001800
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001800
Iteration 115/1000 | Loss: 0.00001800
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001800
Iteration 118/1000 | Loss: 0.00001800
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001799
Iteration 121/1000 | Loss: 0.00001799
Iteration 122/1000 | Loss: 0.00001799
Iteration 123/1000 | Loss: 0.00001799
Iteration 124/1000 | Loss: 0.00001798
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001798
Iteration 127/1000 | Loss: 0.00001798
Iteration 128/1000 | Loss: 0.00001798
Iteration 129/1000 | Loss: 0.00001798
Iteration 130/1000 | Loss: 0.00001798
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001797
Iteration 143/1000 | Loss: 0.00001797
Iteration 144/1000 | Loss: 0.00001797
Iteration 145/1000 | Loss: 0.00001797
Iteration 146/1000 | Loss: 0.00001797
Iteration 147/1000 | Loss: 0.00001797
Iteration 148/1000 | Loss: 0.00001797
Iteration 149/1000 | Loss: 0.00001797
Iteration 150/1000 | Loss: 0.00001797
Iteration 151/1000 | Loss: 0.00001797
Iteration 152/1000 | Loss: 0.00001797
Iteration 153/1000 | Loss: 0.00001796
Iteration 154/1000 | Loss: 0.00001796
Iteration 155/1000 | Loss: 0.00001796
Iteration 156/1000 | Loss: 0.00001796
Iteration 157/1000 | Loss: 0.00001796
Iteration 158/1000 | Loss: 0.00001796
Iteration 159/1000 | Loss: 0.00001796
Iteration 160/1000 | Loss: 0.00001796
Iteration 161/1000 | Loss: 0.00001796
Iteration 162/1000 | Loss: 0.00001796
Iteration 163/1000 | Loss: 0.00001796
Iteration 164/1000 | Loss: 0.00001796
Iteration 165/1000 | Loss: 0.00001796
Iteration 166/1000 | Loss: 0.00001796
Iteration 167/1000 | Loss: 0.00001796
Iteration 168/1000 | Loss: 0.00001796
Iteration 169/1000 | Loss: 0.00001796
Iteration 170/1000 | Loss: 0.00001796
Iteration 171/1000 | Loss: 0.00001796
Iteration 172/1000 | Loss: 0.00001796
Iteration 173/1000 | Loss: 0.00001796
Iteration 174/1000 | Loss: 0.00001796
Iteration 175/1000 | Loss: 0.00001796
Iteration 176/1000 | Loss: 0.00001796
Iteration 177/1000 | Loss: 0.00001796
Iteration 178/1000 | Loss: 0.00001796
Iteration 179/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.796349897631444e-05, 1.796349897631444e-05, 1.796349897631444e-05, 1.796349897631444e-05, 1.796349897631444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.796349897631444e-05

Optimization complete. Final v2v error: 3.531346321105957 mm

Highest mean error: 3.8737874031066895 mm for frame 28

Lowest mean error: 3.15358304977417 mm for frame 43

Saving results

Total time: 39.06862998008728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865491
Iteration 2/25 | Loss: 0.00106571
Iteration 3/25 | Loss: 0.00095022
Iteration 4/25 | Loss: 0.00093555
Iteration 5/25 | Loss: 0.00093147
Iteration 6/25 | Loss: 0.00093079
Iteration 7/25 | Loss: 0.00093079
Iteration 8/25 | Loss: 0.00093079
Iteration 9/25 | Loss: 0.00093079
Iteration 10/25 | Loss: 0.00093079
Iteration 11/25 | Loss: 0.00093079
Iteration 12/25 | Loss: 0.00093079
Iteration 13/25 | Loss: 0.00093079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009307944565080106, 0.0009307944565080106, 0.0009307944565080106, 0.0009307944565080106, 0.0009307944565080106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009307944565080106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.43374443
Iteration 2/25 | Loss: 0.00073162
Iteration 3/25 | Loss: 0.00073161
Iteration 4/25 | Loss: 0.00073161
Iteration 5/25 | Loss: 0.00073161
Iteration 6/25 | Loss: 0.00073161
Iteration 7/25 | Loss: 0.00073161
Iteration 8/25 | Loss: 0.00073161
Iteration 9/25 | Loss: 0.00073161
Iteration 10/25 | Loss: 0.00073161
Iteration 11/25 | Loss: 0.00073161
Iteration 12/25 | Loss: 0.00073161
Iteration 13/25 | Loss: 0.00073161
Iteration 14/25 | Loss: 0.00073161
Iteration 15/25 | Loss: 0.00073161
Iteration 16/25 | Loss: 0.00073161
Iteration 17/25 | Loss: 0.00073161
Iteration 18/25 | Loss: 0.00073161
Iteration 19/25 | Loss: 0.00073161
Iteration 20/25 | Loss: 0.00073161
Iteration 21/25 | Loss: 0.00073161
Iteration 22/25 | Loss: 0.00073161
Iteration 23/25 | Loss: 0.00073161
Iteration 24/25 | Loss: 0.00073161
Iteration 25/25 | Loss: 0.00073161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073161
Iteration 2/1000 | Loss: 0.00003105
Iteration 3/1000 | Loss: 0.00001635
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001385
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001253
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001250
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001249
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001249
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001246
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001241
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001241
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001241
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.2407590475049801e-05, 1.2407590475049801e-05, 1.2407590475049801e-05, 1.2407590475049801e-05, 1.2407590475049801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2407590475049801e-05

Optimization complete. Final v2v error: 2.9909310340881348 mm

Highest mean error: 3.3558788299560547 mm for frame 122

Lowest mean error: 2.317695379257202 mm for frame 178

Saving results

Total time: 31.92212152481079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538096
Iteration 2/25 | Loss: 0.00104192
Iteration 3/25 | Loss: 0.00095900
Iteration 4/25 | Loss: 0.00094500
Iteration 5/25 | Loss: 0.00094097
Iteration 6/25 | Loss: 0.00093958
Iteration 7/25 | Loss: 0.00093958
Iteration 8/25 | Loss: 0.00093958
Iteration 9/25 | Loss: 0.00093958
Iteration 10/25 | Loss: 0.00093958
Iteration 11/25 | Loss: 0.00093958
Iteration 12/25 | Loss: 0.00093958
Iteration 13/25 | Loss: 0.00093958
Iteration 14/25 | Loss: 0.00093958
Iteration 15/25 | Loss: 0.00093958
Iteration 16/25 | Loss: 0.00093958
Iteration 17/25 | Loss: 0.00093958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009395848028361797, 0.0009395848028361797, 0.0009395848028361797, 0.0009395848028361797, 0.0009395848028361797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009395848028361797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.59438014
Iteration 2/25 | Loss: 0.00069908
Iteration 3/25 | Loss: 0.00069907
Iteration 4/25 | Loss: 0.00069907
Iteration 5/25 | Loss: 0.00069907
Iteration 6/25 | Loss: 0.00069907
Iteration 7/25 | Loss: 0.00069907
Iteration 8/25 | Loss: 0.00069907
Iteration 9/25 | Loss: 0.00069907
Iteration 10/25 | Loss: 0.00069907
Iteration 11/25 | Loss: 0.00069907
Iteration 12/25 | Loss: 0.00069907
Iteration 13/25 | Loss: 0.00069907
Iteration 14/25 | Loss: 0.00069907
Iteration 15/25 | Loss: 0.00069907
Iteration 16/25 | Loss: 0.00069907
Iteration 17/25 | Loss: 0.00069907
Iteration 18/25 | Loss: 0.00069907
Iteration 19/25 | Loss: 0.00069907
Iteration 20/25 | Loss: 0.00069907
Iteration 21/25 | Loss: 0.00069907
Iteration 22/25 | Loss: 0.00069907
Iteration 23/25 | Loss: 0.00069907
Iteration 24/25 | Loss: 0.00069907
Iteration 25/25 | Loss: 0.00069907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069907
Iteration 2/1000 | Loss: 0.00003318
Iteration 3/1000 | Loss: 0.00001825
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001579
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001361
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001338
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001318
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001317
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001316
Iteration 146/1000 | Loss: 0.00001316
Iteration 147/1000 | Loss: 0.00001316
Iteration 148/1000 | Loss: 0.00001316
Iteration 149/1000 | Loss: 0.00001316
Iteration 150/1000 | Loss: 0.00001316
Iteration 151/1000 | Loss: 0.00001315
Iteration 152/1000 | Loss: 0.00001315
Iteration 153/1000 | Loss: 0.00001315
Iteration 154/1000 | Loss: 0.00001315
Iteration 155/1000 | Loss: 0.00001315
Iteration 156/1000 | Loss: 0.00001315
Iteration 157/1000 | Loss: 0.00001315
Iteration 158/1000 | Loss: 0.00001315
Iteration 159/1000 | Loss: 0.00001315
Iteration 160/1000 | Loss: 0.00001315
Iteration 161/1000 | Loss: 0.00001315
Iteration 162/1000 | Loss: 0.00001315
Iteration 163/1000 | Loss: 0.00001314
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001313
Iteration 169/1000 | Loss: 0.00001313
Iteration 170/1000 | Loss: 0.00001313
Iteration 171/1000 | Loss: 0.00001313
Iteration 172/1000 | Loss: 0.00001313
Iteration 173/1000 | Loss: 0.00001313
Iteration 174/1000 | Loss: 0.00001313
Iteration 175/1000 | Loss: 0.00001313
Iteration 176/1000 | Loss: 0.00001313
Iteration 177/1000 | Loss: 0.00001313
Iteration 178/1000 | Loss: 0.00001313
Iteration 179/1000 | Loss: 0.00001313
Iteration 180/1000 | Loss: 0.00001313
Iteration 181/1000 | Loss: 0.00001313
Iteration 182/1000 | Loss: 0.00001313
Iteration 183/1000 | Loss: 0.00001313
Iteration 184/1000 | Loss: 0.00001313
Iteration 185/1000 | Loss: 0.00001313
Iteration 186/1000 | Loss: 0.00001313
Iteration 187/1000 | Loss: 0.00001313
Iteration 188/1000 | Loss: 0.00001313
Iteration 189/1000 | Loss: 0.00001313
Iteration 190/1000 | Loss: 0.00001313
Iteration 191/1000 | Loss: 0.00001313
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001313
Iteration 194/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.3132796084391885e-05, 1.3132796084391885e-05, 1.3132796084391885e-05, 1.3132796084391885e-05, 1.3132796084391885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3132796084391885e-05

Optimization complete. Final v2v error: 3.12437105178833 mm

Highest mean error: 3.511732816696167 mm for frame 152

Lowest mean error: 2.780752182006836 mm for frame 44

Saving results

Total time: 39.4936683177948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447311
Iteration 2/25 | Loss: 0.00105750
Iteration 3/25 | Loss: 0.00098277
Iteration 4/25 | Loss: 0.00097589
Iteration 5/25 | Loss: 0.00097383
Iteration 6/25 | Loss: 0.00097350
Iteration 7/25 | Loss: 0.00097350
Iteration 8/25 | Loss: 0.00097350
Iteration 9/25 | Loss: 0.00097350
Iteration 10/25 | Loss: 0.00097350
Iteration 11/25 | Loss: 0.00097350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009734951891005039, 0.0009734951891005039, 0.0009734951891005039, 0.0009734951891005039, 0.0009734951891005039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009734951891005039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42203009
Iteration 2/25 | Loss: 0.00058318
Iteration 3/25 | Loss: 0.00058316
Iteration 4/25 | Loss: 0.00058316
Iteration 5/25 | Loss: 0.00058316
Iteration 6/25 | Loss: 0.00058316
Iteration 7/25 | Loss: 0.00058316
Iteration 8/25 | Loss: 0.00058316
Iteration 9/25 | Loss: 0.00058316
Iteration 10/25 | Loss: 0.00058316
Iteration 11/25 | Loss: 0.00058316
Iteration 12/25 | Loss: 0.00058316
Iteration 13/25 | Loss: 0.00058316
Iteration 14/25 | Loss: 0.00058316
Iteration 15/25 | Loss: 0.00058316
Iteration 16/25 | Loss: 0.00058316
Iteration 17/25 | Loss: 0.00058316
Iteration 18/25 | Loss: 0.00058316
Iteration 19/25 | Loss: 0.00058316
Iteration 20/25 | Loss: 0.00058316
Iteration 21/25 | Loss: 0.00058316
Iteration 22/25 | Loss: 0.00058316
Iteration 23/25 | Loss: 0.00058316
Iteration 24/25 | Loss: 0.00058316
Iteration 25/25 | Loss: 0.00058316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058316
Iteration 2/1000 | Loss: 0.00002678
Iteration 3/1000 | Loss: 0.00001801
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001529
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001390
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001377
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001376
Iteration 31/1000 | Loss: 0.00001376
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001375
Iteration 34/1000 | Loss: 0.00001375
Iteration 35/1000 | Loss: 0.00001374
Iteration 36/1000 | Loss: 0.00001374
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001365
Iteration 49/1000 | Loss: 0.00001365
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001364
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001363
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001354
Iteration 97/1000 | Loss: 0.00001354
Iteration 98/1000 | Loss: 0.00001354
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001351
Iteration 121/1000 | Loss: 0.00001351
Iteration 122/1000 | Loss: 0.00001351
Iteration 123/1000 | Loss: 0.00001351
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001350
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001350
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001349
Iteration 137/1000 | Loss: 0.00001349
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001349
Iteration 140/1000 | Loss: 0.00001349
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001348
Iteration 147/1000 | Loss: 0.00001348
Iteration 148/1000 | Loss: 0.00001348
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001347
Iteration 154/1000 | Loss: 0.00001347
Iteration 155/1000 | Loss: 0.00001347
Iteration 156/1000 | Loss: 0.00001346
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001346
Iteration 163/1000 | Loss: 0.00001346
Iteration 164/1000 | Loss: 0.00001345
Iteration 165/1000 | Loss: 0.00001345
Iteration 166/1000 | Loss: 0.00001345
Iteration 167/1000 | Loss: 0.00001345
Iteration 168/1000 | Loss: 0.00001345
Iteration 169/1000 | Loss: 0.00001345
Iteration 170/1000 | Loss: 0.00001344
Iteration 171/1000 | Loss: 0.00001344
Iteration 172/1000 | Loss: 0.00001344
Iteration 173/1000 | Loss: 0.00001344
Iteration 174/1000 | Loss: 0.00001343
Iteration 175/1000 | Loss: 0.00001343
Iteration 176/1000 | Loss: 0.00001343
Iteration 177/1000 | Loss: 0.00001342
Iteration 178/1000 | Loss: 0.00001342
Iteration 179/1000 | Loss: 0.00001342
Iteration 180/1000 | Loss: 0.00001342
Iteration 181/1000 | Loss: 0.00001342
Iteration 182/1000 | Loss: 0.00001342
Iteration 183/1000 | Loss: 0.00001342
Iteration 184/1000 | Loss: 0.00001342
Iteration 185/1000 | Loss: 0.00001342
Iteration 186/1000 | Loss: 0.00001341
Iteration 187/1000 | Loss: 0.00001341
Iteration 188/1000 | Loss: 0.00001341
Iteration 189/1000 | Loss: 0.00001341
Iteration 190/1000 | Loss: 0.00001341
Iteration 191/1000 | Loss: 0.00001341
Iteration 192/1000 | Loss: 0.00001340
Iteration 193/1000 | Loss: 0.00001340
Iteration 194/1000 | Loss: 0.00001340
Iteration 195/1000 | Loss: 0.00001339
Iteration 196/1000 | Loss: 0.00001339
Iteration 197/1000 | Loss: 0.00001339
Iteration 198/1000 | Loss: 0.00001338
Iteration 199/1000 | Loss: 0.00001338
Iteration 200/1000 | Loss: 0.00001338
Iteration 201/1000 | Loss: 0.00001338
Iteration 202/1000 | Loss: 0.00001338
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001338
Iteration 205/1000 | Loss: 0.00001338
Iteration 206/1000 | Loss: 0.00001338
Iteration 207/1000 | Loss: 0.00001338
Iteration 208/1000 | Loss: 0.00001338
Iteration 209/1000 | Loss: 0.00001337
Iteration 210/1000 | Loss: 0.00001337
Iteration 211/1000 | Loss: 0.00001337
Iteration 212/1000 | Loss: 0.00001337
Iteration 213/1000 | Loss: 0.00001337
Iteration 214/1000 | Loss: 0.00001337
Iteration 215/1000 | Loss: 0.00001337
Iteration 216/1000 | Loss: 0.00001337
Iteration 217/1000 | Loss: 0.00001337
Iteration 218/1000 | Loss: 0.00001337
Iteration 219/1000 | Loss: 0.00001337
Iteration 220/1000 | Loss: 0.00001337
Iteration 221/1000 | Loss: 0.00001336
Iteration 222/1000 | Loss: 0.00001336
Iteration 223/1000 | Loss: 0.00001336
Iteration 224/1000 | Loss: 0.00001336
Iteration 225/1000 | Loss: 0.00001336
Iteration 226/1000 | Loss: 0.00001336
Iteration 227/1000 | Loss: 0.00001336
Iteration 228/1000 | Loss: 0.00001336
Iteration 229/1000 | Loss: 0.00001336
Iteration 230/1000 | Loss: 0.00001336
Iteration 231/1000 | Loss: 0.00001336
Iteration 232/1000 | Loss: 0.00001336
Iteration 233/1000 | Loss: 0.00001336
Iteration 234/1000 | Loss: 0.00001336
Iteration 235/1000 | Loss: 0.00001336
Iteration 236/1000 | Loss: 0.00001336
Iteration 237/1000 | Loss: 0.00001336
Iteration 238/1000 | Loss: 0.00001336
Iteration 239/1000 | Loss: 0.00001336
Iteration 240/1000 | Loss: 0.00001336
Iteration 241/1000 | Loss: 0.00001336
Iteration 242/1000 | Loss: 0.00001336
Iteration 243/1000 | Loss: 0.00001336
Iteration 244/1000 | Loss: 0.00001336
Iteration 245/1000 | Loss: 0.00001336
Iteration 246/1000 | Loss: 0.00001336
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001336
Iteration 249/1000 | Loss: 0.00001336
Iteration 250/1000 | Loss: 0.00001336
Iteration 251/1000 | Loss: 0.00001336
Iteration 252/1000 | Loss: 0.00001336
Iteration 253/1000 | Loss: 0.00001336
Iteration 254/1000 | Loss: 0.00001336
Iteration 255/1000 | Loss: 0.00001336
Iteration 256/1000 | Loss: 0.00001336
Iteration 257/1000 | Loss: 0.00001336
Iteration 258/1000 | Loss: 0.00001336
Iteration 259/1000 | Loss: 0.00001336
Iteration 260/1000 | Loss: 0.00001336
Iteration 261/1000 | Loss: 0.00001336
Iteration 262/1000 | Loss: 0.00001336
Iteration 263/1000 | Loss: 0.00001336
Iteration 264/1000 | Loss: 0.00001336
Iteration 265/1000 | Loss: 0.00001336
Iteration 266/1000 | Loss: 0.00001336
Iteration 267/1000 | Loss: 0.00001336
Iteration 268/1000 | Loss: 0.00001336
Iteration 269/1000 | Loss: 0.00001336
Iteration 270/1000 | Loss: 0.00001336
Iteration 271/1000 | Loss: 0.00001336
Iteration 272/1000 | Loss: 0.00001336
Iteration 273/1000 | Loss: 0.00001336
Iteration 274/1000 | Loss: 0.00001336
Iteration 275/1000 | Loss: 0.00001336
Iteration 276/1000 | Loss: 0.00001336
Iteration 277/1000 | Loss: 0.00001336
Iteration 278/1000 | Loss: 0.00001336
Iteration 279/1000 | Loss: 0.00001336
Iteration 280/1000 | Loss: 0.00001336
Iteration 281/1000 | Loss: 0.00001336
Iteration 282/1000 | Loss: 0.00001336
Iteration 283/1000 | Loss: 0.00001336
Iteration 284/1000 | Loss: 0.00001336
Iteration 285/1000 | Loss: 0.00001336
Iteration 286/1000 | Loss: 0.00001336
Iteration 287/1000 | Loss: 0.00001336
Iteration 288/1000 | Loss: 0.00001336
Iteration 289/1000 | Loss: 0.00001336
Iteration 290/1000 | Loss: 0.00001336
Iteration 291/1000 | Loss: 0.00001336
Iteration 292/1000 | Loss: 0.00001336
Iteration 293/1000 | Loss: 0.00001336
Iteration 294/1000 | Loss: 0.00001336
Iteration 295/1000 | Loss: 0.00001336
Iteration 296/1000 | Loss: 0.00001336
Iteration 297/1000 | Loss: 0.00001336
Iteration 298/1000 | Loss: 0.00001336
Iteration 299/1000 | Loss: 0.00001336
Iteration 300/1000 | Loss: 0.00001336
Iteration 301/1000 | Loss: 0.00001336
Iteration 302/1000 | Loss: 0.00001336
Iteration 303/1000 | Loss: 0.00001336
Iteration 304/1000 | Loss: 0.00001336
Iteration 305/1000 | Loss: 0.00001336
Iteration 306/1000 | Loss: 0.00001336
Iteration 307/1000 | Loss: 0.00001336
Iteration 308/1000 | Loss: 0.00001336
Iteration 309/1000 | Loss: 0.00001336
Iteration 310/1000 | Loss: 0.00001336
Iteration 311/1000 | Loss: 0.00001336
Iteration 312/1000 | Loss: 0.00001336
Iteration 313/1000 | Loss: 0.00001336
Iteration 314/1000 | Loss: 0.00001336
Iteration 315/1000 | Loss: 0.00001336
Iteration 316/1000 | Loss: 0.00001336
Iteration 317/1000 | Loss: 0.00001336
Iteration 318/1000 | Loss: 0.00001336
Iteration 319/1000 | Loss: 0.00001336
Iteration 320/1000 | Loss: 0.00001336
Iteration 321/1000 | Loss: 0.00001336
Iteration 322/1000 | Loss: 0.00001336
Iteration 323/1000 | Loss: 0.00001336
Iteration 324/1000 | Loss: 0.00001336
Iteration 325/1000 | Loss: 0.00001336
Iteration 326/1000 | Loss: 0.00001336
Iteration 327/1000 | Loss: 0.00001336
Iteration 328/1000 | Loss: 0.00001336
Iteration 329/1000 | Loss: 0.00001336
Iteration 330/1000 | Loss: 0.00001336
Iteration 331/1000 | Loss: 0.00001336
Iteration 332/1000 | Loss: 0.00001336
Iteration 333/1000 | Loss: 0.00001336
Iteration 334/1000 | Loss: 0.00001336
Iteration 335/1000 | Loss: 0.00001336
Iteration 336/1000 | Loss: 0.00001336
Iteration 337/1000 | Loss: 0.00001336
Iteration 338/1000 | Loss: 0.00001336
Iteration 339/1000 | Loss: 0.00001336
Iteration 340/1000 | Loss: 0.00001336
Iteration 341/1000 | Loss: 0.00001336
Iteration 342/1000 | Loss: 0.00001336
Iteration 343/1000 | Loss: 0.00001336
Iteration 344/1000 | Loss: 0.00001336
Iteration 345/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [1.3360194316192064e-05, 1.3360194316192064e-05, 1.3360194316192064e-05, 1.3360194316192064e-05, 1.3360194316192064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3360194316192064e-05

Optimization complete. Final v2v error: 3.0995049476623535 mm

Highest mean error: 3.7188425064086914 mm for frame 148

Lowest mean error: 2.437666416168213 mm for frame 4

Saving results

Total time: 43.00091218948364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01228083
Iteration 2/25 | Loss: 0.00352155
Iteration 3/25 | Loss: 0.00266492
Iteration 4/25 | Loss: 0.00225756
Iteration 5/25 | Loss: 0.00194047
Iteration 6/25 | Loss: 0.00171159
Iteration 7/25 | Loss: 0.00163902
Iteration 8/25 | Loss: 0.00161996
Iteration 9/25 | Loss: 0.00160242
Iteration 10/25 | Loss: 0.00160496
Iteration 11/25 | Loss: 0.00159082
Iteration 12/25 | Loss: 0.00156916
Iteration 13/25 | Loss: 0.00156471
Iteration 14/25 | Loss: 0.00156099
Iteration 15/25 | Loss: 0.00155651
Iteration 16/25 | Loss: 0.00154158
Iteration 17/25 | Loss: 0.00154020
Iteration 18/25 | Loss: 0.00155124
Iteration 19/25 | Loss: 0.00153470
Iteration 20/25 | Loss: 0.00154329
Iteration 21/25 | Loss: 0.00153357
Iteration 22/25 | Loss: 0.00151785
Iteration 23/25 | Loss: 0.00150466
Iteration 24/25 | Loss: 0.00148404
Iteration 25/25 | Loss: 0.00149064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58303744
Iteration 2/25 | Loss: 0.00395388
Iteration 3/25 | Loss: 0.00386052
Iteration 4/25 | Loss: 0.00386052
Iteration 5/25 | Loss: 0.00386052
Iteration 6/25 | Loss: 0.00386051
Iteration 7/25 | Loss: 0.00386051
Iteration 8/25 | Loss: 0.00386051
Iteration 9/25 | Loss: 0.00386051
Iteration 10/25 | Loss: 0.00386051
Iteration 11/25 | Loss: 0.00386051
Iteration 12/25 | Loss: 0.00386051
Iteration 13/25 | Loss: 0.00386051
Iteration 14/25 | Loss: 0.00386051
Iteration 15/25 | Loss: 0.00386051
Iteration 16/25 | Loss: 0.00386051
Iteration 17/25 | Loss: 0.00386051
Iteration 18/25 | Loss: 0.00386051
Iteration 19/25 | Loss: 0.00386051
Iteration 20/25 | Loss: 0.00386051
Iteration 21/25 | Loss: 0.00386051
Iteration 22/25 | Loss: 0.00386051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.003860512049868703, 0.003860512049868703, 0.003860512049868703, 0.003860512049868703, 0.003860512049868703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003860512049868703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00386051
Iteration 2/1000 | Loss: 0.00121260
Iteration 3/1000 | Loss: 0.00086137
Iteration 4/1000 | Loss: 0.00147205
Iteration 5/1000 | Loss: 0.00186826
Iteration 6/1000 | Loss: 0.00113646
Iteration 7/1000 | Loss: 0.00060478
Iteration 8/1000 | Loss: 0.00046985
Iteration 9/1000 | Loss: 0.00054383
Iteration 10/1000 | Loss: 0.00099894
Iteration 11/1000 | Loss: 0.00191013
Iteration 12/1000 | Loss: 0.00074580
Iteration 13/1000 | Loss: 0.00034987
Iteration 14/1000 | Loss: 0.00051347
Iteration 15/1000 | Loss: 0.00034044
Iteration 16/1000 | Loss: 0.00018794
Iteration 17/1000 | Loss: 0.00130567
Iteration 18/1000 | Loss: 0.00018563
Iteration 19/1000 | Loss: 0.00030595
Iteration 20/1000 | Loss: 0.00041149
Iteration 21/1000 | Loss: 0.00047437
Iteration 22/1000 | Loss: 0.00031733
Iteration 23/1000 | Loss: 0.00034520
Iteration 24/1000 | Loss: 0.00035466
Iteration 25/1000 | Loss: 0.00038295
Iteration 26/1000 | Loss: 0.00140652
Iteration 27/1000 | Loss: 0.00043806
Iteration 28/1000 | Loss: 0.00046752
Iteration 29/1000 | Loss: 0.00042706
Iteration 30/1000 | Loss: 0.00091556
Iteration 31/1000 | Loss: 0.00075247
Iteration 32/1000 | Loss: 0.00031057
Iteration 33/1000 | Loss: 0.00022740
Iteration 34/1000 | Loss: 0.00027045
Iteration 35/1000 | Loss: 0.00026537
Iteration 36/1000 | Loss: 0.00028635
Iteration 37/1000 | Loss: 0.00023532
Iteration 38/1000 | Loss: 0.00032685
Iteration 39/1000 | Loss: 0.00037011
Iteration 40/1000 | Loss: 0.00030107
Iteration 41/1000 | Loss: 0.00037893
Iteration 42/1000 | Loss: 0.00040421
Iteration 43/1000 | Loss: 0.00039467
Iteration 44/1000 | Loss: 0.00030528
Iteration 45/1000 | Loss: 0.00028810
Iteration 46/1000 | Loss: 0.00024040
Iteration 47/1000 | Loss: 0.00039550
Iteration 48/1000 | Loss: 0.00027387
Iteration 49/1000 | Loss: 0.00036102
Iteration 50/1000 | Loss: 0.00026094
Iteration 51/1000 | Loss: 0.00044822
Iteration 52/1000 | Loss: 0.00026019
Iteration 53/1000 | Loss: 0.00037250
Iteration 54/1000 | Loss: 0.00028468
Iteration 55/1000 | Loss: 0.00019868
Iteration 56/1000 | Loss: 0.00025327
Iteration 57/1000 | Loss: 0.00027980
Iteration 58/1000 | Loss: 0.00047247
Iteration 59/1000 | Loss: 0.00032089
Iteration 60/1000 | Loss: 0.00035482
Iteration 61/1000 | Loss: 0.00036573
Iteration 62/1000 | Loss: 0.00019980
Iteration 63/1000 | Loss: 0.00043265
Iteration 64/1000 | Loss: 0.00037832
Iteration 65/1000 | Loss: 0.00047799
Iteration 66/1000 | Loss: 0.00045592
Iteration 67/1000 | Loss: 0.00015920
Iteration 68/1000 | Loss: 0.00012450
Iteration 69/1000 | Loss: 0.00012573
Iteration 70/1000 | Loss: 0.00011743
Iteration 71/1000 | Loss: 0.00012334
Iteration 72/1000 | Loss: 0.00030892
Iteration 73/1000 | Loss: 0.00022243
Iteration 74/1000 | Loss: 0.00013141
Iteration 75/1000 | Loss: 0.00030147
Iteration 76/1000 | Loss: 0.00032008
Iteration 77/1000 | Loss: 0.00014484
Iteration 78/1000 | Loss: 0.00106205
Iteration 79/1000 | Loss: 0.00250968
Iteration 80/1000 | Loss: 0.00039457
Iteration 81/1000 | Loss: 0.00014545
Iteration 82/1000 | Loss: 0.00014862
Iteration 83/1000 | Loss: 0.00011840
Iteration 84/1000 | Loss: 0.00064333
Iteration 85/1000 | Loss: 0.00012048
Iteration 86/1000 | Loss: 0.00109831
Iteration 87/1000 | Loss: 0.00190789
Iteration 88/1000 | Loss: 0.00075761
Iteration 89/1000 | Loss: 0.00052593
Iteration 90/1000 | Loss: 0.00053058
Iteration 91/1000 | Loss: 0.00121079
Iteration 92/1000 | Loss: 0.00089512
Iteration 93/1000 | Loss: 0.00226127
Iteration 94/1000 | Loss: 0.00051727
Iteration 95/1000 | Loss: 0.00035334
Iteration 96/1000 | Loss: 0.00037629
Iteration 97/1000 | Loss: 0.00029213
Iteration 98/1000 | Loss: 0.00036688
Iteration 99/1000 | Loss: 0.00012620
Iteration 100/1000 | Loss: 0.00012033
Iteration 101/1000 | Loss: 0.00091802
Iteration 102/1000 | Loss: 0.00012003
Iteration 103/1000 | Loss: 0.00012568
Iteration 104/1000 | Loss: 0.00012232
Iteration 105/1000 | Loss: 0.00010132
Iteration 106/1000 | Loss: 0.00009598
Iteration 107/1000 | Loss: 0.00010973
Iteration 108/1000 | Loss: 0.00206531
Iteration 109/1000 | Loss: 0.00116797
Iteration 110/1000 | Loss: 0.00019970
Iteration 111/1000 | Loss: 0.00111560
Iteration 112/1000 | Loss: 0.00030403
Iteration 113/1000 | Loss: 0.00011693
Iteration 114/1000 | Loss: 0.00009845
Iteration 115/1000 | Loss: 0.00010152
Iteration 116/1000 | Loss: 0.00008781
Iteration 117/1000 | Loss: 0.00009272
Iteration 118/1000 | Loss: 0.00019706
Iteration 119/1000 | Loss: 0.00018053
Iteration 120/1000 | Loss: 0.00021501
Iteration 121/1000 | Loss: 0.00017475
Iteration 122/1000 | Loss: 0.00022370
Iteration 123/1000 | Loss: 0.00022241
Iteration 124/1000 | Loss: 0.00023292
Iteration 125/1000 | Loss: 0.00018222
Iteration 126/1000 | Loss: 0.00009670
Iteration 127/1000 | Loss: 0.00009653
Iteration 128/1000 | Loss: 0.00021902
Iteration 129/1000 | Loss: 0.00009446
Iteration 130/1000 | Loss: 0.00009261
Iteration 131/1000 | Loss: 0.00009912
Iteration 132/1000 | Loss: 0.00009709
Iteration 133/1000 | Loss: 0.00008265
Iteration 134/1000 | Loss: 0.00008060
Iteration 135/1000 | Loss: 0.00008227
Iteration 136/1000 | Loss: 0.00009510
Iteration 137/1000 | Loss: 0.00009509
Iteration 138/1000 | Loss: 0.00008965
Iteration 139/1000 | Loss: 0.00024530
Iteration 140/1000 | Loss: 0.00122984
Iteration 141/1000 | Loss: 0.00112981
Iteration 142/1000 | Loss: 0.00356352
Iteration 143/1000 | Loss: 0.00059842
Iteration 144/1000 | Loss: 0.00024859
Iteration 145/1000 | Loss: 0.00012298
Iteration 146/1000 | Loss: 0.00015786
Iteration 147/1000 | Loss: 0.00017720
Iteration 148/1000 | Loss: 0.00009834
Iteration 149/1000 | Loss: 0.00010074
Iteration 150/1000 | Loss: 0.00009252
Iteration 151/1000 | Loss: 0.00008793
Iteration 152/1000 | Loss: 0.00008031
Iteration 153/1000 | Loss: 0.00009018
Iteration 154/1000 | Loss: 0.00007927
Iteration 155/1000 | Loss: 0.00007684
Iteration 156/1000 | Loss: 0.00040303
Iteration 157/1000 | Loss: 0.00008740
Iteration 158/1000 | Loss: 0.00007386
Iteration 159/1000 | Loss: 0.00007681
Iteration 160/1000 | Loss: 0.00007424
Iteration 161/1000 | Loss: 0.00007987
Iteration 162/1000 | Loss: 0.00007349
Iteration 163/1000 | Loss: 0.00008068
Iteration 164/1000 | Loss: 0.00008456
Iteration 165/1000 | Loss: 0.00006961
Iteration 166/1000 | Loss: 0.00007838
Iteration 167/1000 | Loss: 0.00008250
Iteration 168/1000 | Loss: 0.00007756
Iteration 169/1000 | Loss: 0.00007715
Iteration 170/1000 | Loss: 0.00007131
Iteration 171/1000 | Loss: 0.00007229
Iteration 172/1000 | Loss: 0.00007809
Iteration 173/1000 | Loss: 0.00007621
Iteration 174/1000 | Loss: 0.00007846
Iteration 175/1000 | Loss: 0.00007789
Iteration 176/1000 | Loss: 0.00117463
Iteration 177/1000 | Loss: 0.00009799
Iteration 178/1000 | Loss: 0.00007147
Iteration 179/1000 | Loss: 0.00009515
Iteration 180/1000 | Loss: 0.00008743
Iteration 181/1000 | Loss: 0.00008503
Iteration 182/1000 | Loss: 0.00009619
Iteration 183/1000 | Loss: 0.00009871
Iteration 184/1000 | Loss: 0.00009064
Iteration 185/1000 | Loss: 0.00009344
Iteration 186/1000 | Loss: 0.00008992
Iteration 187/1000 | Loss: 0.00009696
Iteration 188/1000 | Loss: 0.00008696
Iteration 189/1000 | Loss: 0.00007972
Iteration 190/1000 | Loss: 0.00007463
Iteration 191/1000 | Loss: 0.00007370
Iteration 192/1000 | Loss: 0.00009224
Iteration 193/1000 | Loss: 0.00008053
Iteration 194/1000 | Loss: 0.00006521
Iteration 195/1000 | Loss: 0.00008530
Iteration 196/1000 | Loss: 0.00007675
Iteration 197/1000 | Loss: 0.00006735
Iteration 198/1000 | Loss: 0.00008392
Iteration 199/1000 | Loss: 0.00007548
Iteration 200/1000 | Loss: 0.00008662
Iteration 201/1000 | Loss: 0.00008399
Iteration 202/1000 | Loss: 0.00009975
Iteration 203/1000 | Loss: 0.00008474
Iteration 204/1000 | Loss: 0.00008799
Iteration 205/1000 | Loss: 0.00007750
Iteration 206/1000 | Loss: 0.00008806
Iteration 207/1000 | Loss: 0.00007992
Iteration 208/1000 | Loss: 0.00008646
Iteration 209/1000 | Loss: 0.00008424
Iteration 210/1000 | Loss: 0.00008233
Iteration 211/1000 | Loss: 0.00007294
Iteration 212/1000 | Loss: 0.00008013
Iteration 213/1000 | Loss: 0.00008091
Iteration 214/1000 | Loss: 0.00006778
Iteration 215/1000 | Loss: 0.00007735
Iteration 216/1000 | Loss: 0.00010390
Iteration 217/1000 | Loss: 0.00007930
Iteration 218/1000 | Loss: 0.00008265
Iteration 219/1000 | Loss: 0.00007256
Iteration 220/1000 | Loss: 0.00007567
Iteration 221/1000 | Loss: 0.00007590
Iteration 222/1000 | Loss: 0.00008942
Iteration 223/1000 | Loss: 0.00007561
Iteration 224/1000 | Loss: 0.00008578
Iteration 225/1000 | Loss: 0.00007264
Iteration 226/1000 | Loss: 0.00007527
Iteration 227/1000 | Loss: 0.00007244
Iteration 228/1000 | Loss: 0.00007444
Iteration 229/1000 | Loss: 0.00007166
Iteration 230/1000 | Loss: 0.00006706
Iteration 231/1000 | Loss: 0.00007155
Iteration 232/1000 | Loss: 0.00007870
Iteration 233/1000 | Loss: 0.00007157
Iteration 234/1000 | Loss: 0.00006930
Iteration 235/1000 | Loss: 0.00006675
Iteration 236/1000 | Loss: 0.00007030
Iteration 237/1000 | Loss: 0.00006901
Iteration 238/1000 | Loss: 0.00007355
Iteration 239/1000 | Loss: 0.00007225
Iteration 240/1000 | Loss: 0.00007710
Iteration 241/1000 | Loss: 0.00007206
Iteration 242/1000 | Loss: 0.00007781
Iteration 243/1000 | Loss: 0.00007188
Iteration 244/1000 | Loss: 0.00008010
Iteration 245/1000 | Loss: 0.00007425
Iteration 246/1000 | Loss: 0.00007215
Iteration 247/1000 | Loss: 0.00007644
Iteration 248/1000 | Loss: 0.00007129
Iteration 249/1000 | Loss: 0.00007976
Iteration 250/1000 | Loss: 0.00007883
Iteration 251/1000 | Loss: 0.00006968
Iteration 252/1000 | Loss: 0.00006773
Iteration 253/1000 | Loss: 0.00006504
Iteration 254/1000 | Loss: 0.00006402
Iteration 255/1000 | Loss: 0.00006338
Iteration 256/1000 | Loss: 0.00006303
Iteration 257/1000 | Loss: 0.00006273
Iteration 258/1000 | Loss: 0.00006242
Iteration 259/1000 | Loss: 0.00006219
Iteration 260/1000 | Loss: 0.00006213
Iteration 261/1000 | Loss: 0.00006212
Iteration 262/1000 | Loss: 0.00006212
Iteration 263/1000 | Loss: 0.00006211
Iteration 264/1000 | Loss: 0.00006211
Iteration 265/1000 | Loss: 0.00006211
Iteration 266/1000 | Loss: 0.00006211
Iteration 267/1000 | Loss: 0.00006211
Iteration 268/1000 | Loss: 0.00006211
Iteration 269/1000 | Loss: 0.00006210
Iteration 270/1000 | Loss: 0.00006210
Iteration 271/1000 | Loss: 0.00006210
Iteration 272/1000 | Loss: 0.00006210
Iteration 273/1000 | Loss: 0.00006210
Iteration 274/1000 | Loss: 0.00006210
Iteration 275/1000 | Loss: 0.00006210
Iteration 276/1000 | Loss: 0.00006209
Iteration 277/1000 | Loss: 0.00006209
Iteration 278/1000 | Loss: 0.00006209
Iteration 279/1000 | Loss: 0.00006209
Iteration 280/1000 | Loss: 0.00006208
Iteration 281/1000 | Loss: 0.00006208
Iteration 282/1000 | Loss: 0.00006208
Iteration 283/1000 | Loss: 0.00006208
Iteration 284/1000 | Loss: 0.00006207
Iteration 285/1000 | Loss: 0.00006207
Iteration 286/1000 | Loss: 0.00006207
Iteration 287/1000 | Loss: 0.00006207
Iteration 288/1000 | Loss: 0.00006206
Iteration 289/1000 | Loss: 0.00006206
Iteration 290/1000 | Loss: 0.00006206
Iteration 291/1000 | Loss: 0.00006206
Iteration 292/1000 | Loss: 0.00006206
Iteration 293/1000 | Loss: 0.00006206
Iteration 294/1000 | Loss: 0.00006206
Iteration 295/1000 | Loss: 0.00006206
Iteration 296/1000 | Loss: 0.00006206
Iteration 297/1000 | Loss: 0.00006206
Iteration 298/1000 | Loss: 0.00006206
Iteration 299/1000 | Loss: 0.00006206
Iteration 300/1000 | Loss: 0.00006206
Iteration 301/1000 | Loss: 0.00006206
Iteration 302/1000 | Loss: 0.00006206
Iteration 303/1000 | Loss: 0.00006206
Iteration 304/1000 | Loss: 0.00006206
Iteration 305/1000 | Loss: 0.00006206
Iteration 306/1000 | Loss: 0.00006205
Iteration 307/1000 | Loss: 0.00006205
Iteration 308/1000 | Loss: 0.00006205
Iteration 309/1000 | Loss: 0.00006205
Iteration 310/1000 | Loss: 0.00006205
Iteration 311/1000 | Loss: 0.00006205
Iteration 312/1000 | Loss: 0.00006205
Iteration 313/1000 | Loss: 0.00006205
Iteration 314/1000 | Loss: 0.00006205
Iteration 315/1000 | Loss: 0.00006205
Iteration 316/1000 | Loss: 0.00006205
Iteration 317/1000 | Loss: 0.00006205
Iteration 318/1000 | Loss: 0.00006205
Iteration 319/1000 | Loss: 0.00006205
Iteration 320/1000 | Loss: 0.00006205
Iteration 321/1000 | Loss: 0.00006205
Iteration 322/1000 | Loss: 0.00006205
Iteration 323/1000 | Loss: 0.00006205
Iteration 324/1000 | Loss: 0.00006205
Iteration 325/1000 | Loss: 0.00006205
Iteration 326/1000 | Loss: 0.00006205
Iteration 327/1000 | Loss: 0.00006205
Iteration 328/1000 | Loss: 0.00006204
Iteration 329/1000 | Loss: 0.00006204
Iteration 330/1000 | Loss: 0.00006204
Iteration 331/1000 | Loss: 0.00006204
Iteration 332/1000 | Loss: 0.00006204
Iteration 333/1000 | Loss: 0.00006204
Iteration 334/1000 | Loss: 0.00006204
Iteration 335/1000 | Loss: 0.00006204
Iteration 336/1000 | Loss: 0.00006204
Iteration 337/1000 | Loss: 0.00006204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [6.204441888257861e-05, 6.204441888257861e-05, 6.204441888257861e-05, 6.204441888257861e-05, 6.204441888257861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.204441888257861e-05

Optimization complete. Final v2v error: 5.045688152313232 mm

Highest mean error: 12.238118171691895 mm for frame 75

Lowest mean error: 4.052420139312744 mm for frame 45

Saving results

Total time: 427.98722791671753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386253
Iteration 2/25 | Loss: 0.00098149
Iteration 3/25 | Loss: 0.00090711
Iteration 4/25 | Loss: 0.00089782
Iteration 5/25 | Loss: 0.00089541
Iteration 6/25 | Loss: 0.00089541
Iteration 7/25 | Loss: 0.00089541
Iteration 8/25 | Loss: 0.00089541
Iteration 9/25 | Loss: 0.00089541
Iteration 10/25 | Loss: 0.00089541
Iteration 11/25 | Loss: 0.00089541
Iteration 12/25 | Loss: 0.00089541
Iteration 13/25 | Loss: 0.00089541
Iteration 14/25 | Loss: 0.00089541
Iteration 15/25 | Loss: 0.00089541
Iteration 16/25 | Loss: 0.00089541
Iteration 17/25 | Loss: 0.00089541
Iteration 18/25 | Loss: 0.00089541
Iteration 19/25 | Loss: 0.00089541
Iteration 20/25 | Loss: 0.00089541
Iteration 21/25 | Loss: 0.00089541
Iteration 22/25 | Loss: 0.00089541
Iteration 23/25 | Loss: 0.00089541
Iteration 24/25 | Loss: 0.00089541
Iteration 25/25 | Loss: 0.00089541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38334846
Iteration 2/25 | Loss: 0.00074312
Iteration 3/25 | Loss: 0.00074312
Iteration 4/25 | Loss: 0.00074312
Iteration 5/25 | Loss: 0.00074312
Iteration 6/25 | Loss: 0.00074312
Iteration 7/25 | Loss: 0.00074312
Iteration 8/25 | Loss: 0.00074312
Iteration 9/25 | Loss: 0.00074312
Iteration 10/25 | Loss: 0.00074312
Iteration 11/25 | Loss: 0.00074312
Iteration 12/25 | Loss: 0.00074312
Iteration 13/25 | Loss: 0.00074312
Iteration 14/25 | Loss: 0.00074312
Iteration 15/25 | Loss: 0.00074312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007431191625073552, 0.0007431191625073552, 0.0007431191625073552, 0.0007431191625073552, 0.0007431191625073552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007431191625073552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074312
Iteration 2/1000 | Loss: 0.00001752
Iteration 3/1000 | Loss: 0.00001354
Iteration 4/1000 | Loss: 0.00001260
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001151
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001138
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001100
Iteration 13/1000 | Loss: 0.00001092
Iteration 14/1000 | Loss: 0.00001091
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001091
Iteration 17/1000 | Loss: 0.00001088
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001069
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001068
Iteration 22/1000 | Loss: 0.00001068
Iteration 23/1000 | Loss: 0.00001065
Iteration 24/1000 | Loss: 0.00001059
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001052
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001051
Iteration 36/1000 | Loss: 0.00001050
Iteration 37/1000 | Loss: 0.00001048
Iteration 38/1000 | Loss: 0.00001048
Iteration 39/1000 | Loss: 0.00001048
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001045
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001042
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001040
Iteration 54/1000 | Loss: 0.00001040
Iteration 55/1000 | Loss: 0.00001040
Iteration 56/1000 | Loss: 0.00001040
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001039
Iteration 60/1000 | Loss: 0.00001039
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001037
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001036
Iteration 70/1000 | Loss: 0.00001036
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001035
Iteration 73/1000 | Loss: 0.00001035
Iteration 74/1000 | Loss: 0.00001035
Iteration 75/1000 | Loss: 0.00001035
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001035
Iteration 78/1000 | Loss: 0.00001035
Iteration 79/1000 | Loss: 0.00001035
Iteration 80/1000 | Loss: 0.00001034
Iteration 81/1000 | Loss: 0.00001034
Iteration 82/1000 | Loss: 0.00001034
Iteration 83/1000 | Loss: 0.00001034
Iteration 84/1000 | Loss: 0.00001033
Iteration 85/1000 | Loss: 0.00001033
Iteration 86/1000 | Loss: 0.00001033
Iteration 87/1000 | Loss: 0.00001032
Iteration 88/1000 | Loss: 0.00001032
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001032
Iteration 92/1000 | Loss: 0.00001032
Iteration 93/1000 | Loss: 0.00001031
Iteration 94/1000 | Loss: 0.00001031
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001031
Iteration 98/1000 | Loss: 0.00001031
Iteration 99/1000 | Loss: 0.00001031
Iteration 100/1000 | Loss: 0.00001031
Iteration 101/1000 | Loss: 0.00001030
Iteration 102/1000 | Loss: 0.00001030
Iteration 103/1000 | Loss: 0.00001029
Iteration 104/1000 | Loss: 0.00001029
Iteration 105/1000 | Loss: 0.00001029
Iteration 106/1000 | Loss: 0.00001029
Iteration 107/1000 | Loss: 0.00001029
Iteration 108/1000 | Loss: 0.00001029
Iteration 109/1000 | Loss: 0.00001029
Iteration 110/1000 | Loss: 0.00001029
Iteration 111/1000 | Loss: 0.00001029
Iteration 112/1000 | Loss: 0.00001029
Iteration 113/1000 | Loss: 0.00001029
Iteration 114/1000 | Loss: 0.00001029
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001028
Iteration 121/1000 | Loss: 0.00001028
Iteration 122/1000 | Loss: 0.00001028
Iteration 123/1000 | Loss: 0.00001028
Iteration 124/1000 | Loss: 0.00001028
Iteration 125/1000 | Loss: 0.00001028
Iteration 126/1000 | Loss: 0.00001028
Iteration 127/1000 | Loss: 0.00001028
Iteration 128/1000 | Loss: 0.00001028
Iteration 129/1000 | Loss: 0.00001028
Iteration 130/1000 | Loss: 0.00001028
Iteration 131/1000 | Loss: 0.00001028
Iteration 132/1000 | Loss: 0.00001028
Iteration 133/1000 | Loss: 0.00001028
Iteration 134/1000 | Loss: 0.00001028
Iteration 135/1000 | Loss: 0.00001028
Iteration 136/1000 | Loss: 0.00001028
Iteration 137/1000 | Loss: 0.00001028
Iteration 138/1000 | Loss: 0.00001028
Iteration 139/1000 | Loss: 0.00001028
Iteration 140/1000 | Loss: 0.00001028
Iteration 141/1000 | Loss: 0.00001028
Iteration 142/1000 | Loss: 0.00001028
Iteration 143/1000 | Loss: 0.00001028
Iteration 144/1000 | Loss: 0.00001028
Iteration 145/1000 | Loss: 0.00001028
Iteration 146/1000 | Loss: 0.00001028
Iteration 147/1000 | Loss: 0.00001028
Iteration 148/1000 | Loss: 0.00001028
Iteration 149/1000 | Loss: 0.00001028
Iteration 150/1000 | Loss: 0.00001028
Iteration 151/1000 | Loss: 0.00001028
Iteration 152/1000 | Loss: 0.00001028
Iteration 153/1000 | Loss: 0.00001028
Iteration 154/1000 | Loss: 0.00001028
Iteration 155/1000 | Loss: 0.00001028
Iteration 156/1000 | Loss: 0.00001028
Iteration 157/1000 | Loss: 0.00001028
Iteration 158/1000 | Loss: 0.00001028
Iteration 159/1000 | Loss: 0.00001028
Iteration 160/1000 | Loss: 0.00001028
Iteration 161/1000 | Loss: 0.00001028
Iteration 162/1000 | Loss: 0.00001028
Iteration 163/1000 | Loss: 0.00001028
Iteration 164/1000 | Loss: 0.00001028
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Iteration 167/1000 | Loss: 0.00001028
Iteration 168/1000 | Loss: 0.00001028
Iteration 169/1000 | Loss: 0.00001028
Iteration 170/1000 | Loss: 0.00001028
Iteration 171/1000 | Loss: 0.00001028
Iteration 172/1000 | Loss: 0.00001028
Iteration 173/1000 | Loss: 0.00001028
Iteration 174/1000 | Loss: 0.00001028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.0282135917805135e-05, 1.0282135917805135e-05, 1.0282135917805135e-05, 1.0282135917805135e-05, 1.0282135917805135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0282135917805135e-05

Optimization complete. Final v2v error: 2.767914056777954 mm

Highest mean error: 3.131298065185547 mm for frame 80

Lowest mean error: 2.3997154235839844 mm for frame 20

Saving results

Total time: 38.17384672164917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444097
Iteration 2/25 | Loss: 0.00105342
Iteration 3/25 | Loss: 0.00097163
Iteration 4/25 | Loss: 0.00095856
Iteration 5/25 | Loss: 0.00095349
Iteration 6/25 | Loss: 0.00095180
Iteration 7/25 | Loss: 0.00095180
Iteration 8/25 | Loss: 0.00095180
Iteration 9/25 | Loss: 0.00095180
Iteration 10/25 | Loss: 0.00095180
Iteration 11/25 | Loss: 0.00095180
Iteration 12/25 | Loss: 0.00095180
Iteration 13/25 | Loss: 0.00095180
Iteration 14/25 | Loss: 0.00095180
Iteration 15/25 | Loss: 0.00095180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009518048027530313, 0.0009518048027530313, 0.0009518048027530313, 0.0009518048027530313, 0.0009518048027530313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009518048027530313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43227994
Iteration 2/25 | Loss: 0.00073203
Iteration 3/25 | Loss: 0.00073203
Iteration 4/25 | Loss: 0.00073203
Iteration 5/25 | Loss: 0.00073202
Iteration 6/25 | Loss: 0.00073202
Iteration 7/25 | Loss: 0.00073202
Iteration 8/25 | Loss: 0.00073202
Iteration 9/25 | Loss: 0.00073202
Iteration 10/25 | Loss: 0.00073202
Iteration 11/25 | Loss: 0.00073202
Iteration 12/25 | Loss: 0.00073202
Iteration 13/25 | Loss: 0.00073202
Iteration 14/25 | Loss: 0.00073202
Iteration 15/25 | Loss: 0.00073202
Iteration 16/25 | Loss: 0.00073202
Iteration 17/25 | Loss: 0.00073202
Iteration 18/25 | Loss: 0.00073202
Iteration 19/25 | Loss: 0.00073202
Iteration 20/25 | Loss: 0.00073202
Iteration 21/25 | Loss: 0.00073202
Iteration 22/25 | Loss: 0.00073202
Iteration 23/25 | Loss: 0.00073202
Iteration 24/25 | Loss: 0.00073202
Iteration 25/25 | Loss: 0.00073202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073202
Iteration 2/1000 | Loss: 0.00002392
Iteration 3/1000 | Loss: 0.00001460
Iteration 4/1000 | Loss: 0.00001324
Iteration 5/1000 | Loss: 0.00001263
Iteration 6/1000 | Loss: 0.00001239
Iteration 7/1000 | Loss: 0.00001221
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001189
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001182
Iteration 19/1000 | Loss: 0.00001181
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001180
Iteration 22/1000 | Loss: 0.00001180
Iteration 23/1000 | Loss: 0.00001180
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001176
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001156
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001156
Iteration 113/1000 | Loss: 0.00001156
Iteration 114/1000 | Loss: 0.00001156
Iteration 115/1000 | Loss: 0.00001156
Iteration 116/1000 | Loss: 0.00001156
Iteration 117/1000 | Loss: 0.00001156
Iteration 118/1000 | Loss: 0.00001156
Iteration 119/1000 | Loss: 0.00001155
Iteration 120/1000 | Loss: 0.00001155
Iteration 121/1000 | Loss: 0.00001155
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001155
Iteration 124/1000 | Loss: 0.00001155
Iteration 125/1000 | Loss: 0.00001155
Iteration 126/1000 | Loss: 0.00001155
Iteration 127/1000 | Loss: 0.00001155
Iteration 128/1000 | Loss: 0.00001155
Iteration 129/1000 | Loss: 0.00001155
Iteration 130/1000 | Loss: 0.00001154
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001154
Iteration 133/1000 | Loss: 0.00001154
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001154
Iteration 136/1000 | Loss: 0.00001154
Iteration 137/1000 | Loss: 0.00001154
Iteration 138/1000 | Loss: 0.00001154
Iteration 139/1000 | Loss: 0.00001154
Iteration 140/1000 | Loss: 0.00001154
Iteration 141/1000 | Loss: 0.00001154
Iteration 142/1000 | Loss: 0.00001154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.1540614650584757e-05, 1.1540614650584757e-05, 1.1540614650584757e-05, 1.1540614650584757e-05, 1.1540614650584757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1540614650584757e-05

Optimization complete. Final v2v error: 2.893512010574341 mm

Highest mean error: 3.398061990737915 mm for frame 187

Lowest mean error: 2.521930694580078 mm for frame 97

Saving results

Total time: 33.89070272445679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881597
Iteration 2/25 | Loss: 0.00124866
Iteration 3/25 | Loss: 0.00099276
Iteration 4/25 | Loss: 0.00096894
Iteration 5/25 | Loss: 0.00096383
Iteration 6/25 | Loss: 0.00096201
Iteration 7/25 | Loss: 0.00096191
Iteration 8/25 | Loss: 0.00096191
Iteration 9/25 | Loss: 0.00096191
Iteration 10/25 | Loss: 0.00096191
Iteration 11/25 | Loss: 0.00096191
Iteration 12/25 | Loss: 0.00096191
Iteration 13/25 | Loss: 0.00096191
Iteration 14/25 | Loss: 0.00096191
Iteration 15/25 | Loss: 0.00096191
Iteration 16/25 | Loss: 0.00096191
Iteration 17/25 | Loss: 0.00096191
Iteration 18/25 | Loss: 0.00096191
Iteration 19/25 | Loss: 0.00096191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000961906451266259, 0.000961906451266259, 0.000961906451266259, 0.000961906451266259, 0.000961906451266259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000961906451266259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59490252
Iteration 2/25 | Loss: 0.00119071
Iteration 3/25 | Loss: 0.00119071
Iteration 4/25 | Loss: 0.00119071
Iteration 5/25 | Loss: 0.00119071
Iteration 6/25 | Loss: 0.00119071
Iteration 7/25 | Loss: 0.00119071
Iteration 8/25 | Loss: 0.00119071
Iteration 9/25 | Loss: 0.00119071
Iteration 10/25 | Loss: 0.00119071
Iteration 11/25 | Loss: 0.00119071
Iteration 12/25 | Loss: 0.00119071
Iteration 13/25 | Loss: 0.00119071
Iteration 14/25 | Loss: 0.00119071
Iteration 15/25 | Loss: 0.00119071
Iteration 16/25 | Loss: 0.00119071
Iteration 17/25 | Loss: 0.00119071
Iteration 18/25 | Loss: 0.00119071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011907076695933938, 0.0011907076695933938, 0.0011907076695933938, 0.0011907076695933938, 0.0011907076695933938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011907076695933938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119071
Iteration 2/1000 | Loss: 0.00001904
Iteration 3/1000 | Loss: 0.00001475
Iteration 4/1000 | Loss: 0.00001327
Iteration 5/1000 | Loss: 0.00001234
Iteration 6/1000 | Loss: 0.00001194
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001147
Iteration 9/1000 | Loss: 0.00001132
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001114
Iteration 12/1000 | Loss: 0.00001113
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001089
Iteration 54/1000 | Loss: 0.00001089
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001088
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001087
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001080
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001076
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001076
Iteration 107/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.0759582437458448e-05, 1.0759582437458448e-05, 1.0759582437458448e-05, 1.0759582437458448e-05, 1.0759582437458448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0759582437458448e-05

Optimization complete. Final v2v error: 2.7959046363830566 mm

Highest mean error: 3.4092442989349365 mm for frame 182

Lowest mean error: 2.387948513031006 mm for frame 48

Saving results

Total time: 34.97117757797241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041991
Iteration 2/25 | Loss: 0.01041991
Iteration 3/25 | Loss: 0.01041991
Iteration 4/25 | Loss: 0.01041991
Iteration 5/25 | Loss: 0.01041991
Iteration 6/25 | Loss: 0.01041991
Iteration 7/25 | Loss: 0.01041990
Iteration 8/25 | Loss: 0.01041990
Iteration 9/25 | Loss: 0.01041990
Iteration 10/25 | Loss: 0.01041990
Iteration 11/25 | Loss: 0.01041990
Iteration 12/25 | Loss: 0.01041989
Iteration 13/25 | Loss: 0.01041989
Iteration 14/25 | Loss: 0.01041989
Iteration 15/25 | Loss: 0.01041989
Iteration 16/25 | Loss: 0.01041989
Iteration 17/25 | Loss: 0.01041989
Iteration 18/25 | Loss: 0.01041988
Iteration 19/25 | Loss: 0.01041988
Iteration 20/25 | Loss: 0.01041988
Iteration 21/25 | Loss: 0.01041988
Iteration 22/25 | Loss: 0.01041988
Iteration 23/25 | Loss: 0.01041988
Iteration 24/25 | Loss: 0.01041987
Iteration 25/25 | Loss: 0.01041987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54776978
Iteration 2/25 | Loss: 0.12601310
Iteration 3/25 | Loss: 0.12568492
Iteration 4/25 | Loss: 0.12554686
Iteration 5/25 | Loss: 0.12545890
Iteration 6/25 | Loss: 0.12545890
Iteration 7/25 | Loss: 0.12545888
Iteration 8/25 | Loss: 0.12545887
Iteration 9/25 | Loss: 0.12545887
Iteration 10/25 | Loss: 0.12545887
Iteration 11/25 | Loss: 0.12545887
Iteration 12/25 | Loss: 0.12545887
Iteration 13/25 | Loss: 0.12545887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.12545886635780334, 0.12545886635780334, 0.12545886635780334, 0.12545886635780334, 0.12545886635780334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12545886635780334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12545887
Iteration 2/1000 | Loss: 0.00224601
Iteration 3/1000 | Loss: 0.00072201
Iteration 4/1000 | Loss: 0.00093995
Iteration 5/1000 | Loss: 0.00032309
Iteration 6/1000 | Loss: 0.00154038
Iteration 7/1000 | Loss: 0.00074821
Iteration 8/1000 | Loss: 0.00029083
Iteration 9/1000 | Loss: 0.00009372
Iteration 10/1000 | Loss: 0.00031531
Iteration 11/1000 | Loss: 0.00041575
Iteration 12/1000 | Loss: 0.00163987
Iteration 13/1000 | Loss: 0.00007134
Iteration 14/1000 | Loss: 0.00023818
Iteration 15/1000 | Loss: 0.00021620
Iteration 16/1000 | Loss: 0.00094762
Iteration 17/1000 | Loss: 0.00002482
Iteration 18/1000 | Loss: 0.00002298
Iteration 19/1000 | Loss: 0.00002164
Iteration 20/1000 | Loss: 0.00013039
Iteration 21/1000 | Loss: 0.00002145
Iteration 22/1000 | Loss: 0.00002229
Iteration 23/1000 | Loss: 0.00009038
Iteration 24/1000 | Loss: 0.00001984
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001707
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001559
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001529
Iteration 34/1000 | Loss: 0.00009680
Iteration 35/1000 | Loss: 0.00003302
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001497
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00007754
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001482
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001462
Iteration 50/1000 | Loss: 0.00001461
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001451
Iteration 100/1000 | Loss: 0.00001451
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001450
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00009167
Iteration 116/1000 | Loss: 0.00004230
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001450
Iteration 121/1000 | Loss: 0.00001450
Iteration 122/1000 | Loss: 0.00001450
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001449
Iteration 130/1000 | Loss: 0.00001449
Iteration 131/1000 | Loss: 0.00001449
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001449
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001447
Iteration 149/1000 | Loss: 0.00001447
Iteration 150/1000 | Loss: 0.00001447
Iteration 151/1000 | Loss: 0.00001447
Iteration 152/1000 | Loss: 0.00001447
Iteration 153/1000 | Loss: 0.00001447
Iteration 154/1000 | Loss: 0.00001447
Iteration 155/1000 | Loss: 0.00001447
Iteration 156/1000 | Loss: 0.00001447
Iteration 157/1000 | Loss: 0.00001447
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001446
Iteration 163/1000 | Loss: 0.00001446
Iteration 164/1000 | Loss: 0.00001446
Iteration 165/1000 | Loss: 0.00001446
Iteration 166/1000 | Loss: 0.00001446
Iteration 167/1000 | Loss: 0.00001446
Iteration 168/1000 | Loss: 0.00001446
Iteration 169/1000 | Loss: 0.00001446
Iteration 170/1000 | Loss: 0.00011228
Iteration 171/1000 | Loss: 0.00003446
Iteration 172/1000 | Loss: 0.00001450
Iteration 173/1000 | Loss: 0.00001449
Iteration 174/1000 | Loss: 0.00001449
Iteration 175/1000 | Loss: 0.00001448
Iteration 176/1000 | Loss: 0.00005849
Iteration 177/1000 | Loss: 0.00001456
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001447
Iteration 180/1000 | Loss: 0.00001446
Iteration 181/1000 | Loss: 0.00001446
Iteration 182/1000 | Loss: 0.00001446
Iteration 183/1000 | Loss: 0.00001446
Iteration 184/1000 | Loss: 0.00001446
Iteration 185/1000 | Loss: 0.00001446
Iteration 186/1000 | Loss: 0.00001446
Iteration 187/1000 | Loss: 0.00001446
Iteration 188/1000 | Loss: 0.00001445
Iteration 189/1000 | Loss: 0.00001445
Iteration 190/1000 | Loss: 0.00001445
Iteration 191/1000 | Loss: 0.00001445
Iteration 192/1000 | Loss: 0.00001445
Iteration 193/1000 | Loss: 0.00001445
Iteration 194/1000 | Loss: 0.00001445
Iteration 195/1000 | Loss: 0.00001445
Iteration 196/1000 | Loss: 0.00001445
Iteration 197/1000 | Loss: 0.00001444
Iteration 198/1000 | Loss: 0.00001444
Iteration 199/1000 | Loss: 0.00001444
Iteration 200/1000 | Loss: 0.00001444
Iteration 201/1000 | Loss: 0.00001444
Iteration 202/1000 | Loss: 0.00001444
Iteration 203/1000 | Loss: 0.00001444
Iteration 204/1000 | Loss: 0.00001444
Iteration 205/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.444352437829366e-05, 1.444352437829366e-05, 1.444352437829366e-05, 1.444352437829366e-05, 1.444352437829366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.444352437829366e-05

Optimization complete. Final v2v error: 3.122504711151123 mm

Highest mean error: 8.615479469299316 mm for frame 140

Lowest mean error: 2.3122036457061768 mm for frame 1

Saving results

Total time: 94.4972448348999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821795
Iteration 2/25 | Loss: 0.00168442
Iteration 3/25 | Loss: 0.00115064
Iteration 4/25 | Loss: 0.00111667
Iteration 5/25 | Loss: 0.00110906
Iteration 6/25 | Loss: 0.00110720
Iteration 7/25 | Loss: 0.00110674
Iteration 8/25 | Loss: 0.00110674
Iteration 9/25 | Loss: 0.00110674
Iteration 10/25 | Loss: 0.00110674
Iteration 11/25 | Loss: 0.00110674
Iteration 12/25 | Loss: 0.00110674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011067402083426714, 0.0011067402083426714, 0.0011067402083426714, 0.0011067402083426714, 0.0011067402083426714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011067402083426714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.36584294
Iteration 2/25 | Loss: 0.00077469
Iteration 3/25 | Loss: 0.00077467
Iteration 4/25 | Loss: 0.00077467
Iteration 5/25 | Loss: 0.00077467
Iteration 6/25 | Loss: 0.00077467
Iteration 7/25 | Loss: 0.00077467
Iteration 8/25 | Loss: 0.00077466
Iteration 9/25 | Loss: 0.00077466
Iteration 10/25 | Loss: 0.00077466
Iteration 11/25 | Loss: 0.00077466
Iteration 12/25 | Loss: 0.00077466
Iteration 13/25 | Loss: 0.00077466
Iteration 14/25 | Loss: 0.00077466
Iteration 15/25 | Loss: 0.00077466
Iteration 16/25 | Loss: 0.00077466
Iteration 17/25 | Loss: 0.00077466
Iteration 18/25 | Loss: 0.00077466
Iteration 19/25 | Loss: 0.00077466
Iteration 20/25 | Loss: 0.00077466
Iteration 21/25 | Loss: 0.00077466
Iteration 22/25 | Loss: 0.00077466
Iteration 23/25 | Loss: 0.00077466
Iteration 24/25 | Loss: 0.00077466
Iteration 25/25 | Loss: 0.00077466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077466
Iteration 2/1000 | Loss: 0.00008693
Iteration 3/1000 | Loss: 0.00005282
Iteration 4/1000 | Loss: 0.00004483
Iteration 5/1000 | Loss: 0.00004220
Iteration 6/1000 | Loss: 0.00004098
Iteration 7/1000 | Loss: 0.00003999
Iteration 8/1000 | Loss: 0.00003915
Iteration 9/1000 | Loss: 0.00003820
Iteration 10/1000 | Loss: 0.00003750
Iteration 11/1000 | Loss: 0.00003697
Iteration 12/1000 | Loss: 0.00003662
Iteration 13/1000 | Loss: 0.00003634
Iteration 14/1000 | Loss: 0.00003599
Iteration 15/1000 | Loss: 0.00003567
Iteration 16/1000 | Loss: 0.00003548
Iteration 17/1000 | Loss: 0.00003524
Iteration 18/1000 | Loss: 0.00003498
Iteration 19/1000 | Loss: 0.00003477
Iteration 20/1000 | Loss: 0.00003474
Iteration 21/1000 | Loss: 0.00003465
Iteration 22/1000 | Loss: 0.00003457
Iteration 23/1000 | Loss: 0.00003446
Iteration 24/1000 | Loss: 0.00003444
Iteration 25/1000 | Loss: 0.00003439
Iteration 26/1000 | Loss: 0.00003439
Iteration 27/1000 | Loss: 0.00003439
Iteration 28/1000 | Loss: 0.00003437
Iteration 29/1000 | Loss: 0.00003435
Iteration 30/1000 | Loss: 0.00003435
Iteration 31/1000 | Loss: 0.00003434
Iteration 32/1000 | Loss: 0.00003434
Iteration 33/1000 | Loss: 0.00003433
Iteration 34/1000 | Loss: 0.00003433
Iteration 35/1000 | Loss: 0.00003433
Iteration 36/1000 | Loss: 0.00003432
Iteration 37/1000 | Loss: 0.00003430
Iteration 38/1000 | Loss: 0.00003430
Iteration 39/1000 | Loss: 0.00003430
Iteration 40/1000 | Loss: 0.00003430
Iteration 41/1000 | Loss: 0.00003429
Iteration 42/1000 | Loss: 0.00003428
Iteration 43/1000 | Loss: 0.00003428
Iteration 44/1000 | Loss: 0.00003428
Iteration 45/1000 | Loss: 0.00003427
Iteration 46/1000 | Loss: 0.00003427
Iteration 47/1000 | Loss: 0.00003426
Iteration 48/1000 | Loss: 0.00003426
Iteration 49/1000 | Loss: 0.00003424
Iteration 50/1000 | Loss: 0.00003423
Iteration 51/1000 | Loss: 0.00003423
Iteration 52/1000 | Loss: 0.00003423
Iteration 53/1000 | Loss: 0.00003421
Iteration 54/1000 | Loss: 0.00003421
Iteration 55/1000 | Loss: 0.00003420
Iteration 56/1000 | Loss: 0.00003420
Iteration 57/1000 | Loss: 0.00003420
Iteration 58/1000 | Loss: 0.00003420
Iteration 59/1000 | Loss: 0.00003420
Iteration 60/1000 | Loss: 0.00003420
Iteration 61/1000 | Loss: 0.00003419
Iteration 62/1000 | Loss: 0.00003419
Iteration 63/1000 | Loss: 0.00003419
Iteration 64/1000 | Loss: 0.00003419
Iteration 65/1000 | Loss: 0.00003419
Iteration 66/1000 | Loss: 0.00003419
Iteration 67/1000 | Loss: 0.00003419
Iteration 68/1000 | Loss: 0.00003419
Iteration 69/1000 | Loss: 0.00003419
Iteration 70/1000 | Loss: 0.00003419
Iteration 71/1000 | Loss: 0.00003418
Iteration 72/1000 | Loss: 0.00003418
Iteration 73/1000 | Loss: 0.00003418
Iteration 74/1000 | Loss: 0.00003418
Iteration 75/1000 | Loss: 0.00003418
Iteration 76/1000 | Loss: 0.00003418
Iteration 77/1000 | Loss: 0.00003418
Iteration 78/1000 | Loss: 0.00003417
Iteration 79/1000 | Loss: 0.00003417
Iteration 80/1000 | Loss: 0.00003417
Iteration 81/1000 | Loss: 0.00003417
Iteration 82/1000 | Loss: 0.00003417
Iteration 83/1000 | Loss: 0.00003417
Iteration 84/1000 | Loss: 0.00003417
Iteration 85/1000 | Loss: 0.00003417
Iteration 86/1000 | Loss: 0.00003417
Iteration 87/1000 | Loss: 0.00003417
Iteration 88/1000 | Loss: 0.00003417
Iteration 89/1000 | Loss: 0.00003417
Iteration 90/1000 | Loss: 0.00003417
Iteration 91/1000 | Loss: 0.00003417
Iteration 92/1000 | Loss: 0.00003417
Iteration 93/1000 | Loss: 0.00003417
Iteration 94/1000 | Loss: 0.00003417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [3.4169475839007646e-05, 3.4169475839007646e-05, 3.4169475839007646e-05, 3.4169475839007646e-05, 3.4169475839007646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4169475839007646e-05

Optimization complete. Final v2v error: 4.534531593322754 mm

Highest mean error: 6.073220729827881 mm for frame 59

Lowest mean error: 3.131683826446533 mm for frame 97

Saving results

Total time: 49.25744271278381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855199
Iteration 2/25 | Loss: 0.00138526
Iteration 3/25 | Loss: 0.00102273
Iteration 4/25 | Loss: 0.00097171
Iteration 5/25 | Loss: 0.00096217
Iteration 6/25 | Loss: 0.00095969
Iteration 7/25 | Loss: 0.00095836
Iteration 8/25 | Loss: 0.00095709
Iteration 9/25 | Loss: 0.00096517
Iteration 10/25 | Loss: 0.00096023
Iteration 11/25 | Loss: 0.00096000
Iteration 12/25 | Loss: 0.00095461
Iteration 13/25 | Loss: 0.00095170
Iteration 14/25 | Loss: 0.00094635
Iteration 15/25 | Loss: 0.00094161
Iteration 16/25 | Loss: 0.00093993
Iteration 17/25 | Loss: 0.00093929
Iteration 18/25 | Loss: 0.00093930
Iteration 19/25 | Loss: 0.00093897
Iteration 20/25 | Loss: 0.00093914
Iteration 21/25 | Loss: 0.00093928
Iteration 22/25 | Loss: 0.00093912
Iteration 23/25 | Loss: 0.00093865
Iteration 24/25 | Loss: 0.00093800
Iteration 25/25 | Loss: 0.00093923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24670446
Iteration 2/25 | Loss: 0.00116422
Iteration 3/25 | Loss: 0.00116422
Iteration 4/25 | Loss: 0.00116422
Iteration 5/25 | Loss: 0.00116421
Iteration 6/25 | Loss: 0.00116421
Iteration 7/25 | Loss: 0.00116421
Iteration 8/25 | Loss: 0.00116421
Iteration 9/25 | Loss: 0.00116421
Iteration 10/25 | Loss: 0.00116421
Iteration 11/25 | Loss: 0.00116421
Iteration 12/25 | Loss: 0.00116421
Iteration 13/25 | Loss: 0.00116421
Iteration 14/25 | Loss: 0.00116421
Iteration 15/25 | Loss: 0.00116421
Iteration 16/25 | Loss: 0.00116421
Iteration 17/25 | Loss: 0.00116421
Iteration 18/25 | Loss: 0.00116421
Iteration 19/25 | Loss: 0.00116421
Iteration 20/25 | Loss: 0.00116421
Iteration 21/25 | Loss: 0.00116421
Iteration 22/25 | Loss: 0.00116421
Iteration 23/25 | Loss: 0.00116421
Iteration 24/25 | Loss: 0.00116421
Iteration 25/25 | Loss: 0.00116421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001164212473668158, 0.001164212473668158, 0.001164212473668158, 0.001164212473668158, 0.001164212473668158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001164212473668158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116421
Iteration 2/1000 | Loss: 0.00002220
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00003745
Iteration 5/1000 | Loss: 0.00004255
Iteration 6/1000 | Loss: 0.00003838
Iteration 7/1000 | Loss: 0.00004214
Iteration 8/1000 | Loss: 0.00003979
Iteration 9/1000 | Loss: 0.00003272
Iteration 10/1000 | Loss: 0.00003240
Iteration 11/1000 | Loss: 0.00004715
Iteration 12/1000 | Loss: 0.00003738
Iteration 13/1000 | Loss: 0.00003602
Iteration 14/1000 | Loss: 0.00003472
Iteration 15/1000 | Loss: 0.00003989
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001184
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00000954
Iteration 20/1000 | Loss: 0.00000922
Iteration 21/1000 | Loss: 0.00000904
Iteration 22/1000 | Loss: 0.00000902
Iteration 23/1000 | Loss: 0.00000899
Iteration 24/1000 | Loss: 0.00000896
Iteration 25/1000 | Loss: 0.00000895
Iteration 26/1000 | Loss: 0.00000894
Iteration 27/1000 | Loss: 0.00000894
Iteration 28/1000 | Loss: 0.00000893
Iteration 29/1000 | Loss: 0.00000892
Iteration 30/1000 | Loss: 0.00000892
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000890
Iteration 35/1000 | Loss: 0.00000890
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000889
Iteration 38/1000 | Loss: 0.00000889
Iteration 39/1000 | Loss: 0.00000889
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000889
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000888
Iteration 44/1000 | Loss: 0.00000888
Iteration 45/1000 | Loss: 0.00000887
Iteration 46/1000 | Loss: 0.00000887
Iteration 47/1000 | Loss: 0.00000887
Iteration 48/1000 | Loss: 0.00000886
Iteration 49/1000 | Loss: 0.00000886
Iteration 50/1000 | Loss: 0.00000886
Iteration 51/1000 | Loss: 0.00000885
Iteration 52/1000 | Loss: 0.00000885
Iteration 53/1000 | Loss: 0.00000885
Iteration 54/1000 | Loss: 0.00000884
Iteration 55/1000 | Loss: 0.00000883
Iteration 56/1000 | Loss: 0.00000883
Iteration 57/1000 | Loss: 0.00000883
Iteration 58/1000 | Loss: 0.00000883
Iteration 59/1000 | Loss: 0.00000883
Iteration 60/1000 | Loss: 0.00000882
Iteration 61/1000 | Loss: 0.00000882
Iteration 62/1000 | Loss: 0.00000882
Iteration 63/1000 | Loss: 0.00000882
Iteration 64/1000 | Loss: 0.00000882
Iteration 65/1000 | Loss: 0.00000881
Iteration 66/1000 | Loss: 0.00000881
Iteration 67/1000 | Loss: 0.00000880
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000879
Iteration 70/1000 | Loss: 0.00000879
Iteration 71/1000 | Loss: 0.00000879
Iteration 72/1000 | Loss: 0.00000878
Iteration 73/1000 | Loss: 0.00000878
Iteration 74/1000 | Loss: 0.00000878
Iteration 75/1000 | Loss: 0.00000878
Iteration 76/1000 | Loss: 0.00000878
Iteration 77/1000 | Loss: 0.00000877
Iteration 78/1000 | Loss: 0.00000877
Iteration 79/1000 | Loss: 0.00000877
Iteration 80/1000 | Loss: 0.00000876
Iteration 81/1000 | Loss: 0.00000876
Iteration 82/1000 | Loss: 0.00000876
Iteration 83/1000 | Loss: 0.00000875
Iteration 84/1000 | Loss: 0.00000875
Iteration 85/1000 | Loss: 0.00000875
Iteration 86/1000 | Loss: 0.00000875
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000874
Iteration 90/1000 | Loss: 0.00000872
Iteration 91/1000 | Loss: 0.00000872
Iteration 92/1000 | Loss: 0.00000870
Iteration 93/1000 | Loss: 0.00000870
Iteration 94/1000 | Loss: 0.00000870
Iteration 95/1000 | Loss: 0.00000870
Iteration 96/1000 | Loss: 0.00000869
Iteration 97/1000 | Loss: 0.00000869
Iteration 98/1000 | Loss: 0.00000869
Iteration 99/1000 | Loss: 0.00000868
Iteration 100/1000 | Loss: 0.00000868
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000866
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000866
Iteration 108/1000 | Loss: 0.00000866
Iteration 109/1000 | Loss: 0.00000866
Iteration 110/1000 | Loss: 0.00000864
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000860
Iteration 113/1000 | Loss: 0.00000860
Iteration 114/1000 | Loss: 0.00000859
Iteration 115/1000 | Loss: 0.00000859
Iteration 116/1000 | Loss: 0.00000859
Iteration 117/1000 | Loss: 0.00000859
Iteration 118/1000 | Loss: 0.00000858
Iteration 119/1000 | Loss: 0.00000858
Iteration 120/1000 | Loss: 0.00000858
Iteration 121/1000 | Loss: 0.00000857
Iteration 122/1000 | Loss: 0.00000857
Iteration 123/1000 | Loss: 0.00000856
Iteration 124/1000 | Loss: 0.00000856
Iteration 125/1000 | Loss: 0.00000856
Iteration 126/1000 | Loss: 0.00000856
Iteration 127/1000 | Loss: 0.00000856
Iteration 128/1000 | Loss: 0.00000856
Iteration 129/1000 | Loss: 0.00000856
Iteration 130/1000 | Loss: 0.00000856
Iteration 131/1000 | Loss: 0.00000856
Iteration 132/1000 | Loss: 0.00000855
Iteration 133/1000 | Loss: 0.00000855
Iteration 134/1000 | Loss: 0.00000855
Iteration 135/1000 | Loss: 0.00000855
Iteration 136/1000 | Loss: 0.00000855
Iteration 137/1000 | Loss: 0.00000855
Iteration 138/1000 | Loss: 0.00000855
Iteration 139/1000 | Loss: 0.00000855
Iteration 140/1000 | Loss: 0.00000855
Iteration 141/1000 | Loss: 0.00000855
Iteration 142/1000 | Loss: 0.00000854
Iteration 143/1000 | Loss: 0.00000854
Iteration 144/1000 | Loss: 0.00000854
Iteration 145/1000 | Loss: 0.00000854
Iteration 146/1000 | Loss: 0.00000854
Iteration 147/1000 | Loss: 0.00000854
Iteration 148/1000 | Loss: 0.00000854
Iteration 149/1000 | Loss: 0.00000854
Iteration 150/1000 | Loss: 0.00000853
Iteration 151/1000 | Loss: 0.00000853
Iteration 152/1000 | Loss: 0.00000853
Iteration 153/1000 | Loss: 0.00000853
Iteration 154/1000 | Loss: 0.00000853
Iteration 155/1000 | Loss: 0.00000852
Iteration 156/1000 | Loss: 0.00000852
Iteration 157/1000 | Loss: 0.00000852
Iteration 158/1000 | Loss: 0.00000852
Iteration 159/1000 | Loss: 0.00000852
Iteration 160/1000 | Loss: 0.00000852
Iteration 161/1000 | Loss: 0.00000852
Iteration 162/1000 | Loss: 0.00000851
Iteration 163/1000 | Loss: 0.00000851
Iteration 164/1000 | Loss: 0.00000851
Iteration 165/1000 | Loss: 0.00000851
Iteration 166/1000 | Loss: 0.00000851
Iteration 167/1000 | Loss: 0.00000851
Iteration 168/1000 | Loss: 0.00000851
Iteration 169/1000 | Loss: 0.00000851
Iteration 170/1000 | Loss: 0.00000851
Iteration 171/1000 | Loss: 0.00000851
Iteration 172/1000 | Loss: 0.00000851
Iteration 173/1000 | Loss: 0.00000851
Iteration 174/1000 | Loss: 0.00000851
Iteration 175/1000 | Loss: 0.00000851
Iteration 176/1000 | Loss: 0.00000851
Iteration 177/1000 | Loss: 0.00000851
Iteration 178/1000 | Loss: 0.00000851
Iteration 179/1000 | Loss: 0.00000851
Iteration 180/1000 | Loss: 0.00000851
Iteration 181/1000 | Loss: 0.00000851
Iteration 182/1000 | Loss: 0.00000851
Iteration 183/1000 | Loss: 0.00000851
Iteration 184/1000 | Loss: 0.00000851
Iteration 185/1000 | Loss: 0.00000851
Iteration 186/1000 | Loss: 0.00000851
Iteration 187/1000 | Loss: 0.00000851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [8.507396159984637e-06, 8.507396159984637e-06, 8.507396159984637e-06, 8.507396159984637e-06, 8.507396159984637e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.507396159984637e-06

Optimization complete. Final v2v error: 2.4176688194274902 mm

Highest mean error: 3.7738585472106934 mm for frame 91

Lowest mean error: 2.0261011123657227 mm for frame 14

Saving results

Total time: 94.13050699234009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406188
Iteration 2/25 | Loss: 0.00109803
Iteration 3/25 | Loss: 0.00097614
Iteration 4/25 | Loss: 0.00096042
Iteration 5/25 | Loss: 0.00095834
Iteration 6/25 | Loss: 0.00095815
Iteration 7/25 | Loss: 0.00095815
Iteration 8/25 | Loss: 0.00095815
Iteration 9/25 | Loss: 0.00095815
Iteration 10/25 | Loss: 0.00095815
Iteration 11/25 | Loss: 0.00095815
Iteration 12/25 | Loss: 0.00095815
Iteration 13/25 | Loss: 0.00095815
Iteration 14/25 | Loss: 0.00095815
Iteration 15/25 | Loss: 0.00095815
Iteration 16/25 | Loss: 0.00095815
Iteration 17/25 | Loss: 0.00095815
Iteration 18/25 | Loss: 0.00095815
Iteration 19/25 | Loss: 0.00095815
Iteration 20/25 | Loss: 0.00095815
Iteration 21/25 | Loss: 0.00095815
Iteration 22/25 | Loss: 0.00095815
Iteration 23/25 | Loss: 0.00095815
Iteration 24/25 | Loss: 0.00095815
Iteration 25/25 | Loss: 0.00095815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23409855
Iteration 2/25 | Loss: 0.00121792
Iteration 3/25 | Loss: 0.00121792
Iteration 4/25 | Loss: 0.00121792
Iteration 5/25 | Loss: 0.00121792
Iteration 6/25 | Loss: 0.00121792
Iteration 7/25 | Loss: 0.00121791
Iteration 8/25 | Loss: 0.00121791
Iteration 9/25 | Loss: 0.00121791
Iteration 10/25 | Loss: 0.00121791
Iteration 11/25 | Loss: 0.00121791
Iteration 12/25 | Loss: 0.00121791
Iteration 13/25 | Loss: 0.00121791
Iteration 14/25 | Loss: 0.00121791
Iteration 15/25 | Loss: 0.00121791
Iteration 16/25 | Loss: 0.00121791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012179138138890266, 0.0012179138138890266, 0.0012179138138890266, 0.0012179138138890266, 0.0012179138138890266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012179138138890266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121791
Iteration 2/1000 | Loss: 0.00002131
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001179
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001097
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001063
Iteration 12/1000 | Loss: 0.00001061
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001055
Iteration 15/1000 | Loss: 0.00001053
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001043
Iteration 18/1000 | Loss: 0.00001037
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001036
Iteration 21/1000 | Loss: 0.00001035
Iteration 22/1000 | Loss: 0.00001034
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001032
Iteration 26/1000 | Loss: 0.00001031
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001030
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001029
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001028
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001027
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001021
Iteration 51/1000 | Loss: 0.00001021
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001021
Iteration 55/1000 | Loss: 0.00001021
Iteration 56/1000 | Loss: 0.00001021
Iteration 57/1000 | Loss: 0.00001021
Iteration 58/1000 | Loss: 0.00001021
Iteration 59/1000 | Loss: 0.00001021
Iteration 60/1000 | Loss: 0.00001020
Iteration 61/1000 | Loss: 0.00001020
Iteration 62/1000 | Loss: 0.00001020
Iteration 63/1000 | Loss: 0.00001020
Iteration 64/1000 | Loss: 0.00001019
Iteration 65/1000 | Loss: 0.00001019
Iteration 66/1000 | Loss: 0.00001018
Iteration 67/1000 | Loss: 0.00001018
Iteration 68/1000 | Loss: 0.00001017
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001016
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001011
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001010
Iteration 85/1000 | Loss: 0.00001010
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001009
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001008
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001007
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001007
Iteration 96/1000 | Loss: 0.00001006
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001005
Iteration 101/1000 | Loss: 0.00001005
Iteration 102/1000 | Loss: 0.00001005
Iteration 103/1000 | Loss: 0.00001005
Iteration 104/1000 | Loss: 0.00001005
Iteration 105/1000 | Loss: 0.00001004
Iteration 106/1000 | Loss: 0.00001004
Iteration 107/1000 | Loss: 0.00001004
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001002
Iteration 118/1000 | Loss: 0.00001002
Iteration 119/1000 | Loss: 0.00001002
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001001
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001001
Iteration 129/1000 | Loss: 0.00001001
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00000999
Iteration 143/1000 | Loss: 0.00000999
Iteration 144/1000 | Loss: 0.00000999
Iteration 145/1000 | Loss: 0.00000999
Iteration 146/1000 | Loss: 0.00000999
Iteration 147/1000 | Loss: 0.00000999
Iteration 148/1000 | Loss: 0.00000999
Iteration 149/1000 | Loss: 0.00000999
Iteration 150/1000 | Loss: 0.00000999
Iteration 151/1000 | Loss: 0.00000999
Iteration 152/1000 | Loss: 0.00000999
Iteration 153/1000 | Loss: 0.00000999
Iteration 154/1000 | Loss: 0.00000999
Iteration 155/1000 | Loss: 0.00000999
Iteration 156/1000 | Loss: 0.00000999
Iteration 157/1000 | Loss: 0.00000999
Iteration 158/1000 | Loss: 0.00000999
Iteration 159/1000 | Loss: 0.00000999
Iteration 160/1000 | Loss: 0.00000999
Iteration 161/1000 | Loss: 0.00000999
Iteration 162/1000 | Loss: 0.00000999
Iteration 163/1000 | Loss: 0.00000999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [9.986161785491277e-06, 9.986161785491277e-06, 9.986161785491277e-06, 9.986161785491277e-06, 9.986161785491277e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.986161785491277e-06

Optimization complete. Final v2v error: 2.6060619354248047 mm

Highest mean error: 3.0397040843963623 mm for frame 22

Lowest mean error: 2.245069980621338 mm for frame 41

Saving results

Total time: 37.95559072494507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353580
Iteration 2/25 | Loss: 0.00112001
Iteration 3/25 | Loss: 0.00101213
Iteration 4/25 | Loss: 0.00099305
Iteration 5/25 | Loss: 0.00098473
Iteration 6/25 | Loss: 0.00098252
Iteration 7/25 | Loss: 0.00098167
Iteration 8/25 | Loss: 0.00098167
Iteration 9/25 | Loss: 0.00098167
Iteration 10/25 | Loss: 0.00098167
Iteration 11/25 | Loss: 0.00098167
Iteration 12/25 | Loss: 0.00098167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009816678939387202, 0.0009816678939387202, 0.0009816678939387202, 0.0009816678939387202, 0.0009816678939387202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009816678939387202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25201321
Iteration 2/25 | Loss: 0.00180168
Iteration 3/25 | Loss: 0.00180168
Iteration 4/25 | Loss: 0.00180168
Iteration 5/25 | Loss: 0.00180168
Iteration 6/25 | Loss: 0.00180168
Iteration 7/25 | Loss: 0.00180168
Iteration 8/25 | Loss: 0.00180168
Iteration 9/25 | Loss: 0.00180168
Iteration 10/25 | Loss: 0.00180168
Iteration 11/25 | Loss: 0.00180168
Iteration 12/25 | Loss: 0.00180168
Iteration 13/25 | Loss: 0.00180168
Iteration 14/25 | Loss: 0.00180168
Iteration 15/25 | Loss: 0.00180168
Iteration 16/25 | Loss: 0.00180168
Iteration 17/25 | Loss: 0.00180168
Iteration 18/25 | Loss: 0.00180168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018016810063272715, 0.0018016810063272715, 0.0018016810063272715, 0.0018016810063272715, 0.0018016810063272715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018016810063272715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180168
Iteration 2/1000 | Loss: 0.00004249
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001649
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001350
Iteration 15/1000 | Loss: 0.00001349
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001347
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001345
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001324
Iteration 43/1000 | Loss: 0.00001322
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001313
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001312
Iteration 81/1000 | Loss: 0.00001312
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001309
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001307
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001306
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001302
Iteration 135/1000 | Loss: 0.00001302
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001302
Iteration 141/1000 | Loss: 0.00001302
Iteration 142/1000 | Loss: 0.00001302
Iteration 143/1000 | Loss: 0.00001302
Iteration 144/1000 | Loss: 0.00001301
Iteration 145/1000 | Loss: 0.00001301
Iteration 146/1000 | Loss: 0.00001301
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001301
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001298
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001297
Iteration 176/1000 | Loss: 0.00001297
Iteration 177/1000 | Loss: 0.00001297
Iteration 178/1000 | Loss: 0.00001297
Iteration 179/1000 | Loss: 0.00001297
Iteration 180/1000 | Loss: 0.00001297
Iteration 181/1000 | Loss: 0.00001297
Iteration 182/1000 | Loss: 0.00001297
Iteration 183/1000 | Loss: 0.00001297
Iteration 184/1000 | Loss: 0.00001296
Iteration 185/1000 | Loss: 0.00001296
Iteration 186/1000 | Loss: 0.00001296
Iteration 187/1000 | Loss: 0.00001296
Iteration 188/1000 | Loss: 0.00001296
Iteration 189/1000 | Loss: 0.00001296
Iteration 190/1000 | Loss: 0.00001296
Iteration 191/1000 | Loss: 0.00001296
Iteration 192/1000 | Loss: 0.00001295
Iteration 193/1000 | Loss: 0.00001295
Iteration 194/1000 | Loss: 0.00001295
Iteration 195/1000 | Loss: 0.00001295
Iteration 196/1000 | Loss: 0.00001295
Iteration 197/1000 | Loss: 0.00001295
Iteration 198/1000 | Loss: 0.00001295
Iteration 199/1000 | Loss: 0.00001294
Iteration 200/1000 | Loss: 0.00001294
Iteration 201/1000 | Loss: 0.00001294
Iteration 202/1000 | Loss: 0.00001294
Iteration 203/1000 | Loss: 0.00001294
Iteration 204/1000 | Loss: 0.00001294
Iteration 205/1000 | Loss: 0.00001293
Iteration 206/1000 | Loss: 0.00001293
Iteration 207/1000 | Loss: 0.00001293
Iteration 208/1000 | Loss: 0.00001293
Iteration 209/1000 | Loss: 0.00001293
Iteration 210/1000 | Loss: 0.00001293
Iteration 211/1000 | Loss: 0.00001292
Iteration 212/1000 | Loss: 0.00001292
Iteration 213/1000 | Loss: 0.00001292
Iteration 214/1000 | Loss: 0.00001292
Iteration 215/1000 | Loss: 0.00001292
Iteration 216/1000 | Loss: 0.00001292
Iteration 217/1000 | Loss: 0.00001292
Iteration 218/1000 | Loss: 0.00001292
Iteration 219/1000 | Loss: 0.00001292
Iteration 220/1000 | Loss: 0.00001291
Iteration 221/1000 | Loss: 0.00001291
Iteration 222/1000 | Loss: 0.00001291
Iteration 223/1000 | Loss: 0.00001291
Iteration 224/1000 | Loss: 0.00001291
Iteration 225/1000 | Loss: 0.00001291
Iteration 226/1000 | Loss: 0.00001291
Iteration 227/1000 | Loss: 0.00001291
Iteration 228/1000 | Loss: 0.00001291
Iteration 229/1000 | Loss: 0.00001291
Iteration 230/1000 | Loss: 0.00001291
Iteration 231/1000 | Loss: 0.00001290
Iteration 232/1000 | Loss: 0.00001290
Iteration 233/1000 | Loss: 0.00001290
Iteration 234/1000 | Loss: 0.00001290
Iteration 235/1000 | Loss: 0.00001290
Iteration 236/1000 | Loss: 0.00001290
Iteration 237/1000 | Loss: 0.00001290
Iteration 238/1000 | Loss: 0.00001290
Iteration 239/1000 | Loss: 0.00001290
Iteration 240/1000 | Loss: 0.00001290
Iteration 241/1000 | Loss: 0.00001290
Iteration 242/1000 | Loss: 0.00001290
Iteration 243/1000 | Loss: 0.00001290
Iteration 244/1000 | Loss: 0.00001290
Iteration 245/1000 | Loss: 0.00001290
Iteration 246/1000 | Loss: 0.00001290
Iteration 247/1000 | Loss: 0.00001290
Iteration 248/1000 | Loss: 0.00001290
Iteration 249/1000 | Loss: 0.00001289
Iteration 250/1000 | Loss: 0.00001289
Iteration 251/1000 | Loss: 0.00001289
Iteration 252/1000 | Loss: 0.00001289
Iteration 253/1000 | Loss: 0.00001289
Iteration 254/1000 | Loss: 0.00001289
Iteration 255/1000 | Loss: 0.00001289
Iteration 256/1000 | Loss: 0.00001289
Iteration 257/1000 | Loss: 0.00001289
Iteration 258/1000 | Loss: 0.00001289
Iteration 259/1000 | Loss: 0.00001289
Iteration 260/1000 | Loss: 0.00001289
Iteration 261/1000 | Loss: 0.00001289
Iteration 262/1000 | Loss: 0.00001289
Iteration 263/1000 | Loss: 0.00001289
Iteration 264/1000 | Loss: 0.00001289
Iteration 265/1000 | Loss: 0.00001288
Iteration 266/1000 | Loss: 0.00001288
Iteration 267/1000 | Loss: 0.00001288
Iteration 268/1000 | Loss: 0.00001288
Iteration 269/1000 | Loss: 0.00001288
Iteration 270/1000 | Loss: 0.00001288
Iteration 271/1000 | Loss: 0.00001288
Iteration 272/1000 | Loss: 0.00001288
Iteration 273/1000 | Loss: 0.00001288
Iteration 274/1000 | Loss: 0.00001288
Iteration 275/1000 | Loss: 0.00001288
Iteration 276/1000 | Loss: 0.00001288
Iteration 277/1000 | Loss: 0.00001288
Iteration 278/1000 | Loss: 0.00001288
Iteration 279/1000 | Loss: 0.00001288
Iteration 280/1000 | Loss: 0.00001288
Iteration 281/1000 | Loss: 0.00001288
Iteration 282/1000 | Loss: 0.00001287
Iteration 283/1000 | Loss: 0.00001287
Iteration 284/1000 | Loss: 0.00001287
Iteration 285/1000 | Loss: 0.00001287
Iteration 286/1000 | Loss: 0.00001287
Iteration 287/1000 | Loss: 0.00001287
Iteration 288/1000 | Loss: 0.00001287
Iteration 289/1000 | Loss: 0.00001287
Iteration 290/1000 | Loss: 0.00001287
Iteration 291/1000 | Loss: 0.00001287
Iteration 292/1000 | Loss: 0.00001287
Iteration 293/1000 | Loss: 0.00001287
Iteration 294/1000 | Loss: 0.00001287
Iteration 295/1000 | Loss: 0.00001287
Iteration 296/1000 | Loss: 0.00001287
Iteration 297/1000 | Loss: 0.00001287
Iteration 298/1000 | Loss: 0.00001287
Iteration 299/1000 | Loss: 0.00001287
Iteration 300/1000 | Loss: 0.00001287
Iteration 301/1000 | Loss: 0.00001287
Iteration 302/1000 | Loss: 0.00001286
Iteration 303/1000 | Loss: 0.00001286
Iteration 304/1000 | Loss: 0.00001286
Iteration 305/1000 | Loss: 0.00001286
Iteration 306/1000 | Loss: 0.00001286
Iteration 307/1000 | Loss: 0.00001286
Iteration 308/1000 | Loss: 0.00001286
Iteration 309/1000 | Loss: 0.00001286
Iteration 310/1000 | Loss: 0.00001286
Iteration 311/1000 | Loss: 0.00001286
Iteration 312/1000 | Loss: 0.00001286
Iteration 313/1000 | Loss: 0.00001286
Iteration 314/1000 | Loss: 0.00001285
Iteration 315/1000 | Loss: 0.00001285
Iteration 316/1000 | Loss: 0.00001285
Iteration 317/1000 | Loss: 0.00001285
Iteration 318/1000 | Loss: 0.00001285
Iteration 319/1000 | Loss: 0.00001285
Iteration 320/1000 | Loss: 0.00001285
Iteration 321/1000 | Loss: 0.00001285
Iteration 322/1000 | Loss: 0.00001284
Iteration 323/1000 | Loss: 0.00001284
Iteration 324/1000 | Loss: 0.00001284
Iteration 325/1000 | Loss: 0.00001284
Iteration 326/1000 | Loss: 0.00001284
Iteration 327/1000 | Loss: 0.00001284
Iteration 328/1000 | Loss: 0.00001284
Iteration 329/1000 | Loss: 0.00001284
Iteration 330/1000 | Loss: 0.00001284
Iteration 331/1000 | Loss: 0.00001284
Iteration 332/1000 | Loss: 0.00001284
Iteration 333/1000 | Loss: 0.00001284
Iteration 334/1000 | Loss: 0.00001284
Iteration 335/1000 | Loss: 0.00001284
Iteration 336/1000 | Loss: 0.00001284
Iteration 337/1000 | Loss: 0.00001283
Iteration 338/1000 | Loss: 0.00001283
Iteration 339/1000 | Loss: 0.00001283
Iteration 340/1000 | Loss: 0.00001283
Iteration 341/1000 | Loss: 0.00001283
Iteration 342/1000 | Loss: 0.00001283
Iteration 343/1000 | Loss: 0.00001283
Iteration 344/1000 | Loss: 0.00001283
Iteration 345/1000 | Loss: 0.00001283
Iteration 346/1000 | Loss: 0.00001283
Iteration 347/1000 | Loss: 0.00001283
Iteration 348/1000 | Loss: 0.00001283
Iteration 349/1000 | Loss: 0.00001283
Iteration 350/1000 | Loss: 0.00001282
Iteration 351/1000 | Loss: 0.00001282
Iteration 352/1000 | Loss: 0.00001282
Iteration 353/1000 | Loss: 0.00001282
Iteration 354/1000 | Loss: 0.00001282
Iteration 355/1000 | Loss: 0.00001282
Iteration 356/1000 | Loss: 0.00001282
Iteration 357/1000 | Loss: 0.00001282
Iteration 358/1000 | Loss: 0.00001282
Iteration 359/1000 | Loss: 0.00001282
Iteration 360/1000 | Loss: 0.00001282
Iteration 361/1000 | Loss: 0.00001282
Iteration 362/1000 | Loss: 0.00001282
Iteration 363/1000 | Loss: 0.00001282
Iteration 364/1000 | Loss: 0.00001282
Iteration 365/1000 | Loss: 0.00001281
Iteration 366/1000 | Loss: 0.00001281
Iteration 367/1000 | Loss: 0.00001281
Iteration 368/1000 | Loss: 0.00001281
Iteration 369/1000 | Loss: 0.00001281
Iteration 370/1000 | Loss: 0.00001281
Iteration 371/1000 | Loss: 0.00001280
Iteration 372/1000 | Loss: 0.00001280
Iteration 373/1000 | Loss: 0.00001280
Iteration 374/1000 | Loss: 0.00001280
Iteration 375/1000 | Loss: 0.00001280
Iteration 376/1000 | Loss: 0.00001280
Iteration 377/1000 | Loss: 0.00001280
Iteration 378/1000 | Loss: 0.00001280
Iteration 379/1000 | Loss: 0.00001280
Iteration 380/1000 | Loss: 0.00001280
Iteration 381/1000 | Loss: 0.00001280
Iteration 382/1000 | Loss: 0.00001280
Iteration 383/1000 | Loss: 0.00001280
Iteration 384/1000 | Loss: 0.00001280
Iteration 385/1000 | Loss: 0.00001280
Iteration 386/1000 | Loss: 0.00001280
Iteration 387/1000 | Loss: 0.00001280
Iteration 388/1000 | Loss: 0.00001280
Iteration 389/1000 | Loss: 0.00001280
Iteration 390/1000 | Loss: 0.00001280
Iteration 391/1000 | Loss: 0.00001280
Iteration 392/1000 | Loss: 0.00001280
Iteration 393/1000 | Loss: 0.00001280
Iteration 394/1000 | Loss: 0.00001280
Iteration 395/1000 | Loss: 0.00001280
Iteration 396/1000 | Loss: 0.00001280
Iteration 397/1000 | Loss: 0.00001280
Iteration 398/1000 | Loss: 0.00001280
Iteration 399/1000 | Loss: 0.00001280
Iteration 400/1000 | Loss: 0.00001280
Iteration 401/1000 | Loss: 0.00001280
Iteration 402/1000 | Loss: 0.00001280
Iteration 403/1000 | Loss: 0.00001280
Iteration 404/1000 | Loss: 0.00001280
Iteration 405/1000 | Loss: 0.00001280
Iteration 406/1000 | Loss: 0.00001280
Iteration 407/1000 | Loss: 0.00001280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 407. Stopping optimization.
Last 5 losses: [1.2798320312867872e-05, 1.2798320312867872e-05, 1.2798320312867872e-05, 1.2798320312867872e-05, 1.2798320312867872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2798320312867872e-05

Optimization complete. Final v2v error: 2.9688336849212646 mm

Highest mean error: 4.432997226715088 mm for frame 29

Lowest mean error: 2.2301642894744873 mm for frame 175

Saving results

Total time: 55.37420964241028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793525
Iteration 2/25 | Loss: 0.00152527
Iteration 3/25 | Loss: 0.00118212
Iteration 4/25 | Loss: 0.00112812
Iteration 5/25 | Loss: 0.00115873
Iteration 6/25 | Loss: 0.00117721
Iteration 7/25 | Loss: 0.00112659
Iteration 8/25 | Loss: 0.00109454
Iteration 9/25 | Loss: 0.00107381
Iteration 10/25 | Loss: 0.00106221
Iteration 11/25 | Loss: 0.00105993
Iteration 12/25 | Loss: 0.00105961
Iteration 13/25 | Loss: 0.00105948
Iteration 14/25 | Loss: 0.00105941
Iteration 15/25 | Loss: 0.00105933
Iteration 16/25 | Loss: 0.00105924
Iteration 17/25 | Loss: 0.00105921
Iteration 18/25 | Loss: 0.00105921
Iteration 19/25 | Loss: 0.00105921
Iteration 20/25 | Loss: 0.00105921
Iteration 21/25 | Loss: 0.00105921
Iteration 22/25 | Loss: 0.00105921
Iteration 23/25 | Loss: 0.00105921
Iteration 24/25 | Loss: 0.00105921
Iteration 25/25 | Loss: 0.00105921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21589625
Iteration 2/25 | Loss: 0.00150269
Iteration 3/25 | Loss: 0.00150268
Iteration 4/25 | Loss: 0.00150268
Iteration 5/25 | Loss: 0.00150268
Iteration 6/25 | Loss: 0.00150268
Iteration 7/25 | Loss: 0.00150268
Iteration 8/25 | Loss: 0.00150268
Iteration 9/25 | Loss: 0.00150268
Iteration 10/25 | Loss: 0.00150267
Iteration 11/25 | Loss: 0.00150267
Iteration 12/25 | Loss: 0.00150267
Iteration 13/25 | Loss: 0.00150267
Iteration 14/25 | Loss: 0.00150267
Iteration 15/25 | Loss: 0.00150267
Iteration 16/25 | Loss: 0.00150267
Iteration 17/25 | Loss: 0.00150267
Iteration 18/25 | Loss: 0.00150267
Iteration 19/25 | Loss: 0.00150267
Iteration 20/25 | Loss: 0.00150267
Iteration 21/25 | Loss: 0.00150267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015026743058115244, 0.0015026743058115244, 0.0015026743058115244, 0.0015026743058115244, 0.0015026743058115244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015026743058115244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150267
Iteration 2/1000 | Loss: 0.00005705
Iteration 3/1000 | Loss: 0.00004273
Iteration 4/1000 | Loss: 0.00003855
Iteration 5/1000 | Loss: 0.00003611
Iteration 6/1000 | Loss: 0.00003472
Iteration 7/1000 | Loss: 0.00003385
Iteration 8/1000 | Loss: 0.00003330
Iteration 9/1000 | Loss: 0.00003290
Iteration 10/1000 | Loss: 0.00003253
Iteration 11/1000 | Loss: 0.00003223
Iteration 12/1000 | Loss: 0.00003196
Iteration 13/1000 | Loss: 0.00003179
Iteration 14/1000 | Loss: 0.00003172
Iteration 15/1000 | Loss: 0.00003166
Iteration 16/1000 | Loss: 0.00003164
Iteration 17/1000 | Loss: 0.00003161
Iteration 18/1000 | Loss: 0.00003160
Iteration 19/1000 | Loss: 0.00003154
Iteration 20/1000 | Loss: 0.00003143
Iteration 21/1000 | Loss: 0.00003138
Iteration 22/1000 | Loss: 0.00003138
Iteration 23/1000 | Loss: 0.00003137
Iteration 24/1000 | Loss: 0.00003135
Iteration 25/1000 | Loss: 0.00003135
Iteration 26/1000 | Loss: 0.00003133
Iteration 27/1000 | Loss: 0.00003133
Iteration 28/1000 | Loss: 0.00003131
Iteration 29/1000 | Loss: 0.00003131
Iteration 30/1000 | Loss: 0.00003125
Iteration 31/1000 | Loss: 0.00003122
Iteration 32/1000 | Loss: 0.00003122
Iteration 33/1000 | Loss: 0.00003122
Iteration 34/1000 | Loss: 0.00003122
Iteration 35/1000 | Loss: 0.00003121
Iteration 36/1000 | Loss: 0.00003120
Iteration 37/1000 | Loss: 0.00003119
Iteration 38/1000 | Loss: 0.00003118
Iteration 39/1000 | Loss: 0.00003118
Iteration 40/1000 | Loss: 0.00003117
Iteration 41/1000 | Loss: 0.00003115
Iteration 42/1000 | Loss: 0.00003115
Iteration 43/1000 | Loss: 0.00003114
Iteration 44/1000 | Loss: 0.00003113
Iteration 45/1000 | Loss: 0.00003112
Iteration 46/1000 | Loss: 0.00003112
Iteration 47/1000 | Loss: 0.00003111
Iteration 48/1000 | Loss: 0.00003111
Iteration 49/1000 | Loss: 0.00003110
Iteration 50/1000 | Loss: 0.00003110
Iteration 51/1000 | Loss: 0.00003109
Iteration 52/1000 | Loss: 0.00003109
Iteration 53/1000 | Loss: 0.00003109
Iteration 54/1000 | Loss: 0.00003109
Iteration 55/1000 | Loss: 0.00003109
Iteration 56/1000 | Loss: 0.00003108
Iteration 57/1000 | Loss: 0.00003108
Iteration 58/1000 | Loss: 0.00003107
Iteration 59/1000 | Loss: 0.00003107
Iteration 60/1000 | Loss: 0.00003107
Iteration 61/1000 | Loss: 0.00003104
Iteration 62/1000 | Loss: 0.00003102
Iteration 63/1000 | Loss: 0.00003101
Iteration 64/1000 | Loss: 0.00003101
Iteration 65/1000 | Loss: 0.00003100
Iteration 66/1000 | Loss: 0.00003100
Iteration 67/1000 | Loss: 0.00003098
Iteration 68/1000 | Loss: 0.00003096
Iteration 69/1000 | Loss: 0.00003095
Iteration 70/1000 | Loss: 0.00003092
Iteration 71/1000 | Loss: 0.00003092
Iteration 72/1000 | Loss: 0.00003092
Iteration 73/1000 | Loss: 0.00003092
Iteration 74/1000 | Loss: 0.00003092
Iteration 75/1000 | Loss: 0.00003092
Iteration 76/1000 | Loss: 0.00003092
Iteration 77/1000 | Loss: 0.00003092
Iteration 78/1000 | Loss: 0.00003091
Iteration 79/1000 | Loss: 0.00003091
Iteration 80/1000 | Loss: 0.00003091
Iteration 81/1000 | Loss: 0.00003090
Iteration 82/1000 | Loss: 0.00003090
Iteration 83/1000 | Loss: 0.00003089
Iteration 84/1000 | Loss: 0.00003089
Iteration 85/1000 | Loss: 0.00003089
Iteration 86/1000 | Loss: 0.00003089
Iteration 87/1000 | Loss: 0.00003088
Iteration 88/1000 | Loss: 0.00003088
Iteration 89/1000 | Loss: 0.00003088
Iteration 90/1000 | Loss: 0.00003088
Iteration 91/1000 | Loss: 0.00003088
Iteration 92/1000 | Loss: 0.00003088
Iteration 93/1000 | Loss: 0.00003088
Iteration 94/1000 | Loss: 0.00003087
Iteration 95/1000 | Loss: 0.00003087
Iteration 96/1000 | Loss: 0.00003087
Iteration 97/1000 | Loss: 0.00003087
Iteration 98/1000 | Loss: 0.00003087
Iteration 99/1000 | Loss: 0.00003087
Iteration 100/1000 | Loss: 0.00003087
Iteration 101/1000 | Loss: 0.00003087
Iteration 102/1000 | Loss: 0.00003086
Iteration 103/1000 | Loss: 0.00003086
Iteration 104/1000 | Loss: 0.00003086
Iteration 105/1000 | Loss: 0.00003085
Iteration 106/1000 | Loss: 0.00003085
Iteration 107/1000 | Loss: 0.00003085
Iteration 108/1000 | Loss: 0.00003085
Iteration 109/1000 | Loss: 0.00003085
Iteration 110/1000 | Loss: 0.00003084
Iteration 111/1000 | Loss: 0.00003084
Iteration 112/1000 | Loss: 0.00003083
Iteration 113/1000 | Loss: 0.00003083
Iteration 114/1000 | Loss: 0.00003083
Iteration 115/1000 | Loss: 0.00003082
Iteration 116/1000 | Loss: 0.00003082
Iteration 117/1000 | Loss: 0.00003082
Iteration 118/1000 | Loss: 0.00003082
Iteration 119/1000 | Loss: 0.00003081
Iteration 120/1000 | Loss: 0.00003081
Iteration 121/1000 | Loss: 0.00003081
Iteration 122/1000 | Loss: 0.00003081
Iteration 123/1000 | Loss: 0.00003080
Iteration 124/1000 | Loss: 0.00003080
Iteration 125/1000 | Loss: 0.00003080
Iteration 126/1000 | Loss: 0.00003080
Iteration 127/1000 | Loss: 0.00003080
Iteration 128/1000 | Loss: 0.00003080
Iteration 129/1000 | Loss: 0.00003080
Iteration 130/1000 | Loss: 0.00003080
Iteration 131/1000 | Loss: 0.00003080
Iteration 132/1000 | Loss: 0.00003079
Iteration 133/1000 | Loss: 0.00003079
Iteration 134/1000 | Loss: 0.00003079
Iteration 135/1000 | Loss: 0.00003079
Iteration 136/1000 | Loss: 0.00003079
Iteration 137/1000 | Loss: 0.00003079
Iteration 138/1000 | Loss: 0.00003079
Iteration 139/1000 | Loss: 0.00003078
Iteration 140/1000 | Loss: 0.00003078
Iteration 141/1000 | Loss: 0.00003078
Iteration 142/1000 | Loss: 0.00003078
Iteration 143/1000 | Loss: 0.00003078
Iteration 144/1000 | Loss: 0.00003078
Iteration 145/1000 | Loss: 0.00003078
Iteration 146/1000 | Loss: 0.00003078
Iteration 147/1000 | Loss: 0.00003078
Iteration 148/1000 | Loss: 0.00003078
Iteration 149/1000 | Loss: 0.00003078
Iteration 150/1000 | Loss: 0.00003077
Iteration 151/1000 | Loss: 0.00003077
Iteration 152/1000 | Loss: 0.00003077
Iteration 153/1000 | Loss: 0.00003077
Iteration 154/1000 | Loss: 0.00003077
Iteration 155/1000 | Loss: 0.00003077
Iteration 156/1000 | Loss: 0.00003076
Iteration 157/1000 | Loss: 0.00003076
Iteration 158/1000 | Loss: 0.00003076
Iteration 159/1000 | Loss: 0.00003076
Iteration 160/1000 | Loss: 0.00003075
Iteration 161/1000 | Loss: 0.00003075
Iteration 162/1000 | Loss: 0.00003075
Iteration 163/1000 | Loss: 0.00003075
Iteration 164/1000 | Loss: 0.00003074
Iteration 165/1000 | Loss: 0.00003074
Iteration 166/1000 | Loss: 0.00003073
Iteration 167/1000 | Loss: 0.00003073
Iteration 168/1000 | Loss: 0.00003073
Iteration 169/1000 | Loss: 0.00003073
Iteration 170/1000 | Loss: 0.00003072
Iteration 171/1000 | Loss: 0.00003072
Iteration 172/1000 | Loss: 0.00003072
Iteration 173/1000 | Loss: 0.00003071
Iteration 174/1000 | Loss: 0.00003071
Iteration 175/1000 | Loss: 0.00003071
Iteration 176/1000 | Loss: 0.00003070
Iteration 177/1000 | Loss: 0.00003070
Iteration 178/1000 | Loss: 0.00003070
Iteration 179/1000 | Loss: 0.00003069
Iteration 180/1000 | Loss: 0.00003069
Iteration 181/1000 | Loss: 0.00003069
Iteration 182/1000 | Loss: 0.00003069
Iteration 183/1000 | Loss: 0.00003068
Iteration 184/1000 | Loss: 0.00003068
Iteration 185/1000 | Loss: 0.00003068
Iteration 186/1000 | Loss: 0.00003068
Iteration 187/1000 | Loss: 0.00003068
Iteration 188/1000 | Loss: 0.00003067
Iteration 189/1000 | Loss: 0.00003067
Iteration 190/1000 | Loss: 0.00003067
Iteration 191/1000 | Loss: 0.00003067
Iteration 192/1000 | Loss: 0.00003067
Iteration 193/1000 | Loss: 0.00003067
Iteration 194/1000 | Loss: 0.00003066
Iteration 195/1000 | Loss: 0.00003066
Iteration 196/1000 | Loss: 0.00003066
Iteration 197/1000 | Loss: 0.00003066
Iteration 198/1000 | Loss: 0.00003066
Iteration 199/1000 | Loss: 0.00003066
Iteration 200/1000 | Loss: 0.00003066
Iteration 201/1000 | Loss: 0.00003065
Iteration 202/1000 | Loss: 0.00003065
Iteration 203/1000 | Loss: 0.00003065
Iteration 204/1000 | Loss: 0.00003064
Iteration 205/1000 | Loss: 0.00003064
Iteration 206/1000 | Loss: 0.00003064
Iteration 207/1000 | Loss: 0.00003064
Iteration 208/1000 | Loss: 0.00003063
Iteration 209/1000 | Loss: 0.00003063
Iteration 210/1000 | Loss: 0.00003063
Iteration 211/1000 | Loss: 0.00003062
Iteration 212/1000 | Loss: 0.00003062
Iteration 213/1000 | Loss: 0.00003062
Iteration 214/1000 | Loss: 0.00003062
Iteration 215/1000 | Loss: 0.00003061
Iteration 216/1000 | Loss: 0.00003061
Iteration 217/1000 | Loss: 0.00003061
Iteration 218/1000 | Loss: 0.00003060
Iteration 219/1000 | Loss: 0.00003060
Iteration 220/1000 | Loss: 0.00003060
Iteration 221/1000 | Loss: 0.00003059
Iteration 222/1000 | Loss: 0.00003059
Iteration 223/1000 | Loss: 0.00003059
Iteration 224/1000 | Loss: 0.00003059
Iteration 225/1000 | Loss: 0.00003059
Iteration 226/1000 | Loss: 0.00003058
Iteration 227/1000 | Loss: 0.00003058
Iteration 228/1000 | Loss: 0.00003058
Iteration 229/1000 | Loss: 0.00003057
Iteration 230/1000 | Loss: 0.00003057
Iteration 231/1000 | Loss: 0.00003057
Iteration 232/1000 | Loss: 0.00003057
Iteration 233/1000 | Loss: 0.00003056
Iteration 234/1000 | Loss: 0.00003056
Iteration 235/1000 | Loss: 0.00003056
Iteration 236/1000 | Loss: 0.00003056
Iteration 237/1000 | Loss: 0.00003056
Iteration 238/1000 | Loss: 0.00003056
Iteration 239/1000 | Loss: 0.00003056
Iteration 240/1000 | Loss: 0.00003055
Iteration 241/1000 | Loss: 0.00003055
Iteration 242/1000 | Loss: 0.00003055
Iteration 243/1000 | Loss: 0.00003054
Iteration 244/1000 | Loss: 0.00003054
Iteration 245/1000 | Loss: 0.00003054
Iteration 246/1000 | Loss: 0.00003053
Iteration 247/1000 | Loss: 0.00003053
Iteration 248/1000 | Loss: 0.00003052
Iteration 249/1000 | Loss: 0.00003052
Iteration 250/1000 | Loss: 0.00003052
Iteration 251/1000 | Loss: 0.00003051
Iteration 252/1000 | Loss: 0.00003051
Iteration 253/1000 | Loss: 0.00003051
Iteration 254/1000 | Loss: 0.00003049
Iteration 255/1000 | Loss: 0.00003048
Iteration 256/1000 | Loss: 0.00003048
Iteration 257/1000 | Loss: 0.00003048
Iteration 258/1000 | Loss: 0.00003046
Iteration 259/1000 | Loss: 0.00003046
Iteration 260/1000 | Loss: 0.00003045
Iteration 261/1000 | Loss: 0.00003045
Iteration 262/1000 | Loss: 0.00003044
Iteration 263/1000 | Loss: 0.00003042
Iteration 264/1000 | Loss: 0.00003041
Iteration 265/1000 | Loss: 0.00003040
Iteration 266/1000 | Loss: 0.00003039
Iteration 267/1000 | Loss: 0.00003038
Iteration 268/1000 | Loss: 0.00003038
Iteration 269/1000 | Loss: 0.00003037
Iteration 270/1000 | Loss: 0.00003036
Iteration 271/1000 | Loss: 0.00003036
Iteration 272/1000 | Loss: 0.00003034
Iteration 273/1000 | Loss: 0.00003033
Iteration 274/1000 | Loss: 0.00003032
Iteration 275/1000 | Loss: 0.00003032
Iteration 276/1000 | Loss: 0.00003031
Iteration 277/1000 | Loss: 0.00003031
Iteration 278/1000 | Loss: 0.00003031
Iteration 279/1000 | Loss: 0.00003030
Iteration 280/1000 | Loss: 0.00003029
Iteration 281/1000 | Loss: 0.00003029
Iteration 282/1000 | Loss: 0.00003029
Iteration 283/1000 | Loss: 0.00003028
Iteration 284/1000 | Loss: 0.00003028
Iteration 285/1000 | Loss: 0.00003028
Iteration 286/1000 | Loss: 0.00003027
Iteration 287/1000 | Loss: 0.00003027
Iteration 288/1000 | Loss: 0.00003026
Iteration 289/1000 | Loss: 0.00003026
Iteration 290/1000 | Loss: 0.00003026
Iteration 291/1000 | Loss: 0.00003026
Iteration 292/1000 | Loss: 0.00003025
Iteration 293/1000 | Loss: 0.00003025
Iteration 294/1000 | Loss: 0.00003025
Iteration 295/1000 | Loss: 0.00003025
Iteration 296/1000 | Loss: 0.00003024
Iteration 297/1000 | Loss: 0.00003024
Iteration 298/1000 | Loss: 0.00003024
Iteration 299/1000 | Loss: 0.00003024
Iteration 300/1000 | Loss: 0.00003023
Iteration 301/1000 | Loss: 0.00003023
Iteration 302/1000 | Loss: 0.00032989
Iteration 303/1000 | Loss: 0.00132058
Iteration 304/1000 | Loss: 0.00113689
Iteration 305/1000 | Loss: 0.00036753
Iteration 306/1000 | Loss: 0.00004193
Iteration 307/1000 | Loss: 0.00037968
Iteration 308/1000 | Loss: 0.00027626
Iteration 309/1000 | Loss: 0.00036295
Iteration 310/1000 | Loss: 0.00028613
Iteration 311/1000 | Loss: 0.00003444
Iteration 312/1000 | Loss: 0.00028091
Iteration 313/1000 | Loss: 0.00125916
Iteration 314/1000 | Loss: 0.00027648
Iteration 315/1000 | Loss: 0.00031676
Iteration 316/1000 | Loss: 0.00027347
Iteration 317/1000 | Loss: 0.00004453
Iteration 318/1000 | Loss: 0.00003892
Iteration 319/1000 | Loss: 0.00003462
Iteration 320/1000 | Loss: 0.00003243
Iteration 321/1000 | Loss: 0.00003108
Iteration 322/1000 | Loss: 0.00003035
Iteration 323/1000 | Loss: 0.00002953
Iteration 324/1000 | Loss: 0.00002888
Iteration 325/1000 | Loss: 0.00002847
Iteration 326/1000 | Loss: 0.00011885
Iteration 327/1000 | Loss: 0.00016264
Iteration 328/1000 | Loss: 0.00003719
Iteration 329/1000 | Loss: 0.00003070
Iteration 330/1000 | Loss: 0.00002885
Iteration 331/1000 | Loss: 0.00002800
Iteration 332/1000 | Loss: 0.00002762
Iteration 333/1000 | Loss: 0.00002737
Iteration 334/1000 | Loss: 0.00002721
Iteration 335/1000 | Loss: 0.00002715
Iteration 336/1000 | Loss: 0.00002705
Iteration 337/1000 | Loss: 0.00002700
Iteration 338/1000 | Loss: 0.00002697
Iteration 339/1000 | Loss: 0.00002697
Iteration 340/1000 | Loss: 0.00002692
Iteration 341/1000 | Loss: 0.00002687
Iteration 342/1000 | Loss: 0.00002687
Iteration 343/1000 | Loss: 0.00002686
Iteration 344/1000 | Loss: 0.00002686
Iteration 345/1000 | Loss: 0.00002685
Iteration 346/1000 | Loss: 0.00002685
Iteration 347/1000 | Loss: 0.00002684
Iteration 348/1000 | Loss: 0.00002684
Iteration 349/1000 | Loss: 0.00002684
Iteration 350/1000 | Loss: 0.00002684
Iteration 351/1000 | Loss: 0.00002684
Iteration 352/1000 | Loss: 0.00002683
Iteration 353/1000 | Loss: 0.00002683
Iteration 354/1000 | Loss: 0.00002683
Iteration 355/1000 | Loss: 0.00002683
Iteration 356/1000 | Loss: 0.00002683
Iteration 357/1000 | Loss: 0.00002683
Iteration 358/1000 | Loss: 0.00002683
Iteration 359/1000 | Loss: 0.00002683
Iteration 360/1000 | Loss: 0.00002683
Iteration 361/1000 | Loss: 0.00002683
Iteration 362/1000 | Loss: 0.00002683
Iteration 363/1000 | Loss: 0.00002683
Iteration 364/1000 | Loss: 0.00002683
Iteration 365/1000 | Loss: 0.00002683
Iteration 366/1000 | Loss: 0.00002683
Iteration 367/1000 | Loss: 0.00002683
Iteration 368/1000 | Loss: 0.00002683
Iteration 369/1000 | Loss: 0.00002683
Iteration 370/1000 | Loss: 0.00002683
Iteration 371/1000 | Loss: 0.00002683
Iteration 372/1000 | Loss: 0.00002683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 372. Stopping optimization.
Last 5 losses: [2.6826797693502158e-05, 2.6826797693502158e-05, 2.6826797693502158e-05, 2.6826797693502158e-05, 2.6826797693502158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6826797693502158e-05

Optimization complete. Final v2v error: 3.362255573272705 mm

Highest mean error: 11.411758422851562 mm for frame 15

Lowest mean error: 2.3293533325195312 mm for frame 134

Saving results

Total time: 148.90969347953796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929035
Iteration 2/25 | Loss: 0.00115586
Iteration 3/25 | Loss: 0.00101141
Iteration 4/25 | Loss: 0.00098738
Iteration 5/25 | Loss: 0.00097966
Iteration 6/25 | Loss: 0.00097730
Iteration 7/25 | Loss: 0.00097687
Iteration 8/25 | Loss: 0.00097687
Iteration 9/25 | Loss: 0.00097687
Iteration 10/25 | Loss: 0.00097687
Iteration 11/25 | Loss: 0.00097687
Iteration 12/25 | Loss: 0.00097687
Iteration 13/25 | Loss: 0.00097687
Iteration 14/25 | Loss: 0.00097687
Iteration 15/25 | Loss: 0.00097687
Iteration 16/25 | Loss: 0.00097687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009768687887117267, 0.0009768687887117267, 0.0009768687887117267, 0.0009768687887117267, 0.0009768687887117267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009768687887117267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28823709
Iteration 2/25 | Loss: 0.00111996
Iteration 3/25 | Loss: 0.00111995
Iteration 4/25 | Loss: 0.00111995
Iteration 5/25 | Loss: 0.00111995
Iteration 6/25 | Loss: 0.00111995
Iteration 7/25 | Loss: 0.00111995
Iteration 8/25 | Loss: 0.00111995
Iteration 9/25 | Loss: 0.00111995
Iteration 10/25 | Loss: 0.00111995
Iteration 11/25 | Loss: 0.00111995
Iteration 12/25 | Loss: 0.00111995
Iteration 13/25 | Loss: 0.00111995
Iteration 14/25 | Loss: 0.00111995
Iteration 15/25 | Loss: 0.00111995
Iteration 16/25 | Loss: 0.00111995
Iteration 17/25 | Loss: 0.00111995
Iteration 18/25 | Loss: 0.00111995
Iteration 19/25 | Loss: 0.00111995
Iteration 20/25 | Loss: 0.00111995
Iteration 21/25 | Loss: 0.00111995
Iteration 22/25 | Loss: 0.00111995
Iteration 23/25 | Loss: 0.00111995
Iteration 24/25 | Loss: 0.00111995
Iteration 25/25 | Loss: 0.00111995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111995
Iteration 2/1000 | Loss: 0.00004146
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002340
Iteration 5/1000 | Loss: 0.00002144
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001859
Iteration 8/1000 | Loss: 0.00001808
Iteration 9/1000 | Loss: 0.00001769
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001732
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001716
Iteration 15/1000 | Loss: 0.00001715
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001702
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001700
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001700
Iteration 23/1000 | Loss: 0.00001700
Iteration 24/1000 | Loss: 0.00001698
Iteration 25/1000 | Loss: 0.00001698
Iteration 26/1000 | Loss: 0.00001696
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001690
Iteration 66/1000 | Loss: 0.00001690
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001689
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001689
Iteration 78/1000 | Loss: 0.00001689
Iteration 79/1000 | Loss: 0.00001689
Iteration 80/1000 | Loss: 0.00001689
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001688
Iteration 83/1000 | Loss: 0.00001688
Iteration 84/1000 | Loss: 0.00001688
Iteration 85/1000 | Loss: 0.00001688
Iteration 86/1000 | Loss: 0.00001688
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001685
Iteration 119/1000 | Loss: 0.00001685
Iteration 120/1000 | Loss: 0.00001685
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001685
Iteration 126/1000 | Loss: 0.00001685
Iteration 127/1000 | Loss: 0.00001685
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001685
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6837135262903757e-05, 1.6837135262903757e-05, 1.6837135262903757e-05, 1.6837135262903757e-05, 1.6837135262903757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6837135262903757e-05

Optimization complete. Final v2v error: 3.395944118499756 mm

Highest mean error: 5.263882160186768 mm for frame 69

Lowest mean error: 2.5572946071624756 mm for frame 1

Saving results

Total time: 37.72667622566223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423479
Iteration 2/25 | Loss: 0.00115056
Iteration 3/25 | Loss: 0.00098115
Iteration 4/25 | Loss: 0.00096890
Iteration 5/25 | Loss: 0.00096581
Iteration 6/25 | Loss: 0.00096477
Iteration 7/25 | Loss: 0.00096461
Iteration 8/25 | Loss: 0.00096461
Iteration 9/25 | Loss: 0.00096461
Iteration 10/25 | Loss: 0.00096461
Iteration 11/25 | Loss: 0.00096461
Iteration 12/25 | Loss: 0.00096461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009646113030612469, 0.0009646113030612469, 0.0009646113030612469, 0.0009646113030612469, 0.0009646113030612469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009646113030612469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39623392
Iteration 2/25 | Loss: 0.00107833
Iteration 3/25 | Loss: 0.00107833
Iteration 4/25 | Loss: 0.00107833
Iteration 5/25 | Loss: 0.00107833
Iteration 6/25 | Loss: 0.00107833
Iteration 7/25 | Loss: 0.00107833
Iteration 8/25 | Loss: 0.00107833
Iteration 9/25 | Loss: 0.00107833
Iteration 10/25 | Loss: 0.00107833
Iteration 11/25 | Loss: 0.00107833
Iteration 12/25 | Loss: 0.00107833
Iteration 13/25 | Loss: 0.00107833
Iteration 14/25 | Loss: 0.00107832
Iteration 15/25 | Loss: 0.00107832
Iteration 16/25 | Loss: 0.00107832
Iteration 17/25 | Loss: 0.00107832
Iteration 18/25 | Loss: 0.00107832
Iteration 19/25 | Loss: 0.00107832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001078324974514544, 0.001078324974514544, 0.001078324974514544, 0.001078324974514544, 0.001078324974514544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001078324974514544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107832
Iteration 2/1000 | Loss: 0.00003070
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001404
Iteration 6/1000 | Loss: 0.00001310
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001208
Iteration 10/1000 | Loss: 0.00001207
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001182
Iteration 19/1000 | Loss: 0.00001177
Iteration 20/1000 | Loss: 0.00001175
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001174
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001170
Iteration 29/1000 | Loss: 0.00001170
Iteration 30/1000 | Loss: 0.00001170
Iteration 31/1000 | Loss: 0.00001170
Iteration 32/1000 | Loss: 0.00001170
Iteration 33/1000 | Loss: 0.00001170
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001168
Iteration 40/1000 | Loss: 0.00001168
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001167
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001166
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001152
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001152
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001150
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001149
Iteration 144/1000 | Loss: 0.00001149
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001149
Iteration 147/1000 | Loss: 0.00001149
Iteration 148/1000 | Loss: 0.00001149
Iteration 149/1000 | Loss: 0.00001149
Iteration 150/1000 | Loss: 0.00001149
Iteration 151/1000 | Loss: 0.00001149
Iteration 152/1000 | Loss: 0.00001149
Iteration 153/1000 | Loss: 0.00001149
Iteration 154/1000 | Loss: 0.00001149
Iteration 155/1000 | Loss: 0.00001149
Iteration 156/1000 | Loss: 0.00001149
Iteration 157/1000 | Loss: 0.00001148
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001148
Iteration 164/1000 | Loss: 0.00001148
Iteration 165/1000 | Loss: 0.00001148
Iteration 166/1000 | Loss: 0.00001148
Iteration 167/1000 | Loss: 0.00001147
Iteration 168/1000 | Loss: 0.00001147
Iteration 169/1000 | Loss: 0.00001147
Iteration 170/1000 | Loss: 0.00001147
Iteration 171/1000 | Loss: 0.00001147
Iteration 172/1000 | Loss: 0.00001147
Iteration 173/1000 | Loss: 0.00001147
Iteration 174/1000 | Loss: 0.00001147
Iteration 175/1000 | Loss: 0.00001147
Iteration 176/1000 | Loss: 0.00001147
Iteration 177/1000 | Loss: 0.00001147
Iteration 178/1000 | Loss: 0.00001147
Iteration 179/1000 | Loss: 0.00001147
Iteration 180/1000 | Loss: 0.00001147
Iteration 181/1000 | Loss: 0.00001147
Iteration 182/1000 | Loss: 0.00001147
Iteration 183/1000 | Loss: 0.00001147
Iteration 184/1000 | Loss: 0.00001147
Iteration 185/1000 | Loss: 0.00001146
Iteration 186/1000 | Loss: 0.00001146
Iteration 187/1000 | Loss: 0.00001146
Iteration 188/1000 | Loss: 0.00001146
Iteration 189/1000 | Loss: 0.00001146
Iteration 190/1000 | Loss: 0.00001146
Iteration 191/1000 | Loss: 0.00001146
Iteration 192/1000 | Loss: 0.00001146
Iteration 193/1000 | Loss: 0.00001146
Iteration 194/1000 | Loss: 0.00001146
Iteration 195/1000 | Loss: 0.00001146
Iteration 196/1000 | Loss: 0.00001146
Iteration 197/1000 | Loss: 0.00001146
Iteration 198/1000 | Loss: 0.00001146
Iteration 199/1000 | Loss: 0.00001146
Iteration 200/1000 | Loss: 0.00001145
Iteration 201/1000 | Loss: 0.00001145
Iteration 202/1000 | Loss: 0.00001145
Iteration 203/1000 | Loss: 0.00001145
Iteration 204/1000 | Loss: 0.00001145
Iteration 205/1000 | Loss: 0.00001145
Iteration 206/1000 | Loss: 0.00001145
Iteration 207/1000 | Loss: 0.00001145
Iteration 208/1000 | Loss: 0.00001145
Iteration 209/1000 | Loss: 0.00001145
Iteration 210/1000 | Loss: 0.00001145
Iteration 211/1000 | Loss: 0.00001145
Iteration 212/1000 | Loss: 0.00001145
Iteration 213/1000 | Loss: 0.00001145
Iteration 214/1000 | Loss: 0.00001144
Iteration 215/1000 | Loss: 0.00001144
Iteration 216/1000 | Loss: 0.00001144
Iteration 217/1000 | Loss: 0.00001144
Iteration 218/1000 | Loss: 0.00001144
Iteration 219/1000 | Loss: 0.00001144
Iteration 220/1000 | Loss: 0.00001144
Iteration 221/1000 | Loss: 0.00001144
Iteration 222/1000 | Loss: 0.00001144
Iteration 223/1000 | Loss: 0.00001144
Iteration 224/1000 | Loss: 0.00001144
Iteration 225/1000 | Loss: 0.00001144
Iteration 226/1000 | Loss: 0.00001144
Iteration 227/1000 | Loss: 0.00001144
Iteration 228/1000 | Loss: 0.00001144
Iteration 229/1000 | Loss: 0.00001143
Iteration 230/1000 | Loss: 0.00001143
Iteration 231/1000 | Loss: 0.00001143
Iteration 232/1000 | Loss: 0.00001143
Iteration 233/1000 | Loss: 0.00001143
Iteration 234/1000 | Loss: 0.00001143
Iteration 235/1000 | Loss: 0.00001143
Iteration 236/1000 | Loss: 0.00001143
Iteration 237/1000 | Loss: 0.00001143
Iteration 238/1000 | Loss: 0.00001143
Iteration 239/1000 | Loss: 0.00001143
Iteration 240/1000 | Loss: 0.00001143
Iteration 241/1000 | Loss: 0.00001143
Iteration 242/1000 | Loss: 0.00001143
Iteration 243/1000 | Loss: 0.00001143
Iteration 244/1000 | Loss: 0.00001143
Iteration 245/1000 | Loss: 0.00001143
Iteration 246/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.1434851330704987e-05, 1.1434851330704987e-05, 1.1434851330704987e-05, 1.1434851330704987e-05, 1.1434851330704987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1434851330704987e-05

Optimization complete. Final v2v error: 2.8316104412078857 mm

Highest mean error: 3.766555070877075 mm for frame 47

Lowest mean error: 2.422307014465332 mm for frame 93

Saving results

Total time: 38.42047691345215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374963
Iteration 2/25 | Loss: 0.00110475
Iteration 3/25 | Loss: 0.00100550
Iteration 4/25 | Loss: 0.00099503
Iteration 5/25 | Loss: 0.00099181
Iteration 6/25 | Loss: 0.00099092
Iteration 7/25 | Loss: 0.00099092
Iteration 8/25 | Loss: 0.00099092
Iteration 9/25 | Loss: 0.00099092
Iteration 10/25 | Loss: 0.00099092
Iteration 11/25 | Loss: 0.00099092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000990921282209456, 0.000990921282209456, 0.000990921282209456, 0.000990921282209456, 0.000990921282209456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000990921282209456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34548688
Iteration 2/25 | Loss: 0.00141313
Iteration 3/25 | Loss: 0.00141312
Iteration 4/25 | Loss: 0.00141312
Iteration 5/25 | Loss: 0.00141312
Iteration 6/25 | Loss: 0.00141312
Iteration 7/25 | Loss: 0.00141312
Iteration 8/25 | Loss: 0.00141312
Iteration 9/25 | Loss: 0.00141312
Iteration 10/25 | Loss: 0.00141312
Iteration 11/25 | Loss: 0.00141312
Iteration 12/25 | Loss: 0.00141312
Iteration 13/25 | Loss: 0.00141312
Iteration 14/25 | Loss: 0.00141312
Iteration 15/25 | Loss: 0.00141312
Iteration 16/25 | Loss: 0.00141312
Iteration 17/25 | Loss: 0.00141312
Iteration 18/25 | Loss: 0.00141312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014131177449598908, 0.0014131177449598908, 0.0014131177449598908, 0.0014131177449598908, 0.0014131177449598908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014131177449598908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141312
Iteration 2/1000 | Loss: 0.00003336
Iteration 3/1000 | Loss: 0.00002322
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001790
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001626
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001575
Iteration 13/1000 | Loss: 0.00001574
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001572
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001571
Iteration 18/1000 | Loss: 0.00001571
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001566
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001564
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001561
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00001559
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001559
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00001555
Iteration 57/1000 | Loss: 0.00001555
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001552
Iteration 70/1000 | Loss: 0.00001552
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001546
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001537
Iteration 127/1000 | Loss: 0.00001537
Iteration 128/1000 | Loss: 0.00001537
Iteration 129/1000 | Loss: 0.00001537
Iteration 130/1000 | Loss: 0.00001536
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001536
Iteration 133/1000 | Loss: 0.00001536
Iteration 134/1000 | Loss: 0.00001536
Iteration 135/1000 | Loss: 0.00001535
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001534
Iteration 140/1000 | Loss: 0.00001534
Iteration 141/1000 | Loss: 0.00001533
Iteration 142/1000 | Loss: 0.00001533
Iteration 143/1000 | Loss: 0.00001532
Iteration 144/1000 | Loss: 0.00001532
Iteration 145/1000 | Loss: 0.00001532
Iteration 146/1000 | Loss: 0.00001531
Iteration 147/1000 | Loss: 0.00001531
Iteration 148/1000 | Loss: 0.00001530
Iteration 149/1000 | Loss: 0.00001529
Iteration 150/1000 | Loss: 0.00001529
Iteration 151/1000 | Loss: 0.00001529
Iteration 152/1000 | Loss: 0.00001529
Iteration 153/1000 | Loss: 0.00001529
Iteration 154/1000 | Loss: 0.00001529
Iteration 155/1000 | Loss: 0.00001529
Iteration 156/1000 | Loss: 0.00001529
Iteration 157/1000 | Loss: 0.00001528
Iteration 158/1000 | Loss: 0.00001528
Iteration 159/1000 | Loss: 0.00001528
Iteration 160/1000 | Loss: 0.00001528
Iteration 161/1000 | Loss: 0.00001528
Iteration 162/1000 | Loss: 0.00001528
Iteration 163/1000 | Loss: 0.00001527
Iteration 164/1000 | Loss: 0.00001527
Iteration 165/1000 | Loss: 0.00001527
Iteration 166/1000 | Loss: 0.00001527
Iteration 167/1000 | Loss: 0.00001527
Iteration 168/1000 | Loss: 0.00001527
Iteration 169/1000 | Loss: 0.00001527
Iteration 170/1000 | Loss: 0.00001527
Iteration 171/1000 | Loss: 0.00001527
Iteration 172/1000 | Loss: 0.00001526
Iteration 173/1000 | Loss: 0.00001526
Iteration 174/1000 | Loss: 0.00001526
Iteration 175/1000 | Loss: 0.00001526
Iteration 176/1000 | Loss: 0.00001526
Iteration 177/1000 | Loss: 0.00001526
Iteration 178/1000 | Loss: 0.00001526
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001526
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001525
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001524
Iteration 185/1000 | Loss: 0.00001524
Iteration 186/1000 | Loss: 0.00001524
Iteration 187/1000 | Loss: 0.00001524
Iteration 188/1000 | Loss: 0.00001524
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00001523
Iteration 198/1000 | Loss: 0.00001522
Iteration 199/1000 | Loss: 0.00001522
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001522
Iteration 204/1000 | Loss: 0.00001522
Iteration 205/1000 | Loss: 0.00001522
Iteration 206/1000 | Loss: 0.00001522
Iteration 207/1000 | Loss: 0.00001521
Iteration 208/1000 | Loss: 0.00001521
Iteration 209/1000 | Loss: 0.00001521
Iteration 210/1000 | Loss: 0.00001521
Iteration 211/1000 | Loss: 0.00001521
Iteration 212/1000 | Loss: 0.00001521
Iteration 213/1000 | Loss: 0.00001521
Iteration 214/1000 | Loss: 0.00001521
Iteration 215/1000 | Loss: 0.00001521
Iteration 216/1000 | Loss: 0.00001521
Iteration 217/1000 | Loss: 0.00001521
Iteration 218/1000 | Loss: 0.00001521
Iteration 219/1000 | Loss: 0.00001521
Iteration 220/1000 | Loss: 0.00001521
Iteration 221/1000 | Loss: 0.00001521
Iteration 222/1000 | Loss: 0.00001521
Iteration 223/1000 | Loss: 0.00001521
Iteration 224/1000 | Loss: 0.00001521
Iteration 225/1000 | Loss: 0.00001521
Iteration 226/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.5209499906632118e-05, 1.5209499906632118e-05, 1.5209499906632118e-05, 1.5209499906632118e-05, 1.5209499906632118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5209499906632118e-05

Optimization complete. Final v2v error: 3.095414638519287 mm

Highest mean error: 3.592442750930786 mm for frame 43

Lowest mean error: 2.5505902767181396 mm for frame 5

Saving results

Total time: 39.7278847694397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061186
Iteration 2/25 | Loss: 0.00305778
Iteration 3/25 | Loss: 0.00161221
Iteration 4/25 | Loss: 0.00147940
Iteration 5/25 | Loss: 0.00142911
Iteration 6/25 | Loss: 0.00141829
Iteration 7/25 | Loss: 0.00138201
Iteration 8/25 | Loss: 0.00135710
Iteration 9/25 | Loss: 0.00133892
Iteration 10/25 | Loss: 0.00132199
Iteration 11/25 | Loss: 0.00130895
Iteration 12/25 | Loss: 0.00130062
Iteration 13/25 | Loss: 0.00129336
Iteration 14/25 | Loss: 0.00127381
Iteration 15/25 | Loss: 0.00127455
Iteration 16/25 | Loss: 0.00126192
Iteration 17/25 | Loss: 0.00125849
Iteration 18/25 | Loss: 0.00125324
Iteration 19/25 | Loss: 0.00125238
Iteration 20/25 | Loss: 0.00125051
Iteration 21/25 | Loss: 0.00124570
Iteration 22/25 | Loss: 0.00124259
Iteration 23/25 | Loss: 0.00124201
Iteration 24/25 | Loss: 0.00123883
Iteration 25/25 | Loss: 0.00123749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28169858
Iteration 2/25 | Loss: 0.00268152
Iteration 3/25 | Loss: 0.00268152
Iteration 4/25 | Loss: 0.00268152
Iteration 5/25 | Loss: 0.00268152
Iteration 6/25 | Loss: 0.00268152
Iteration 7/25 | Loss: 0.00268151
Iteration 8/25 | Loss: 0.00268151
Iteration 9/25 | Loss: 0.00268151
Iteration 10/25 | Loss: 0.00268151
Iteration 11/25 | Loss: 0.00268151
Iteration 12/25 | Loss: 0.00268151
Iteration 13/25 | Loss: 0.00268151
Iteration 14/25 | Loss: 0.00268151
Iteration 15/25 | Loss: 0.00268151
Iteration 16/25 | Loss: 0.00268151
Iteration 17/25 | Loss: 0.00268151
Iteration 18/25 | Loss: 0.00268151
Iteration 19/25 | Loss: 0.00268151
Iteration 20/25 | Loss: 0.00268151
Iteration 21/25 | Loss: 0.00268151
Iteration 22/25 | Loss: 0.00268151
Iteration 23/25 | Loss: 0.00268151
Iteration 24/25 | Loss: 0.00268151
Iteration 25/25 | Loss: 0.00268151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268151
Iteration 2/1000 | Loss: 0.00031234
Iteration 3/1000 | Loss: 0.00025762
Iteration 4/1000 | Loss: 0.00022060
Iteration 5/1000 | Loss: 0.00268479
Iteration 6/1000 | Loss: 0.00049217
Iteration 7/1000 | Loss: 0.00116635
Iteration 8/1000 | Loss: 0.00035892
Iteration 9/1000 | Loss: 0.00019439
Iteration 10/1000 | Loss: 0.00015958
Iteration 11/1000 | Loss: 0.00014282
Iteration 12/1000 | Loss: 0.00012536
Iteration 13/1000 | Loss: 0.00011483
Iteration 14/1000 | Loss: 0.00010728
Iteration 15/1000 | Loss: 0.00010233
Iteration 16/1000 | Loss: 0.00009855
Iteration 17/1000 | Loss: 0.00041478
Iteration 18/1000 | Loss: 0.00010342
Iteration 19/1000 | Loss: 0.00072856
Iteration 20/1000 | Loss: 0.00259578
Iteration 21/1000 | Loss: 0.00187710
Iteration 22/1000 | Loss: 0.00100419
Iteration 23/1000 | Loss: 0.00085061
Iteration 24/1000 | Loss: 0.00018260
Iteration 25/1000 | Loss: 0.00013592
Iteration 26/1000 | Loss: 0.00012069
Iteration 27/1000 | Loss: 0.00077168
Iteration 28/1000 | Loss: 0.00149888
Iteration 29/1000 | Loss: 0.00021735
Iteration 30/1000 | Loss: 0.00287451
Iteration 31/1000 | Loss: 0.00122620
Iteration 32/1000 | Loss: 0.00287194
Iteration 33/1000 | Loss: 0.00299920
Iteration 34/1000 | Loss: 0.00088206
Iteration 35/1000 | Loss: 0.00408772
Iteration 36/1000 | Loss: 0.00058377
Iteration 37/1000 | Loss: 0.00088651
Iteration 38/1000 | Loss: 0.00064601
Iteration 39/1000 | Loss: 0.00017904
Iteration 40/1000 | Loss: 0.00032237
Iteration 41/1000 | Loss: 0.00038773
Iteration 42/1000 | Loss: 0.00040343
Iteration 43/1000 | Loss: 0.00040685
Iteration 44/1000 | Loss: 0.00158147
Iteration 45/1000 | Loss: 0.00034726
Iteration 46/1000 | Loss: 0.00111321
Iteration 47/1000 | Loss: 0.00027252
Iteration 48/1000 | Loss: 0.00039522
Iteration 49/1000 | Loss: 0.00024051
Iteration 50/1000 | Loss: 0.00006549
Iteration 51/1000 | Loss: 0.00032042
Iteration 52/1000 | Loss: 0.00006770
Iteration 53/1000 | Loss: 0.00066754
Iteration 54/1000 | Loss: 0.00006161
Iteration 55/1000 | Loss: 0.00015123
Iteration 56/1000 | Loss: 0.00051793
Iteration 57/1000 | Loss: 0.00016015
Iteration 58/1000 | Loss: 0.00019032
Iteration 59/1000 | Loss: 0.00023666
Iteration 60/1000 | Loss: 0.00004322
Iteration 61/1000 | Loss: 0.00003848
Iteration 62/1000 | Loss: 0.00003382
Iteration 63/1000 | Loss: 0.00068253
Iteration 64/1000 | Loss: 0.00048244
Iteration 65/1000 | Loss: 0.00034677
Iteration 66/1000 | Loss: 0.00019347
Iteration 67/1000 | Loss: 0.00016442
Iteration 68/1000 | Loss: 0.00017545
Iteration 69/1000 | Loss: 0.00015171
Iteration 70/1000 | Loss: 0.00018820
Iteration 71/1000 | Loss: 0.00016956
Iteration 72/1000 | Loss: 0.00017748
Iteration 73/1000 | Loss: 0.00016318
Iteration 74/1000 | Loss: 0.00037001
Iteration 75/1000 | Loss: 0.00007986
Iteration 76/1000 | Loss: 0.00042753
Iteration 77/1000 | Loss: 0.00031681
Iteration 78/1000 | Loss: 0.00019589
Iteration 79/1000 | Loss: 0.00049981
Iteration 80/1000 | Loss: 0.00019067
Iteration 81/1000 | Loss: 0.00019521
Iteration 82/1000 | Loss: 0.00007071
Iteration 83/1000 | Loss: 0.00017083
Iteration 84/1000 | Loss: 0.00003909
Iteration 85/1000 | Loss: 0.00010750
Iteration 86/1000 | Loss: 0.00003325
Iteration 87/1000 | Loss: 0.00003160
Iteration 88/1000 | Loss: 0.00012022
Iteration 89/1000 | Loss: 0.00007964
Iteration 90/1000 | Loss: 0.00009640
Iteration 91/1000 | Loss: 0.00003164
Iteration 92/1000 | Loss: 0.00029481
Iteration 93/1000 | Loss: 0.00011821
Iteration 94/1000 | Loss: 0.00025705
Iteration 95/1000 | Loss: 0.00003186
Iteration 96/1000 | Loss: 0.00067226
Iteration 97/1000 | Loss: 0.00003676
Iteration 98/1000 | Loss: 0.00002906
Iteration 99/1000 | Loss: 0.00003258
Iteration 100/1000 | Loss: 0.00002824
Iteration 101/1000 | Loss: 0.00002551
Iteration 102/1000 | Loss: 0.00002286
Iteration 103/1000 | Loss: 0.00002162
Iteration 104/1000 | Loss: 0.00002055
Iteration 105/1000 | Loss: 0.00066628
Iteration 106/1000 | Loss: 0.00002463
Iteration 107/1000 | Loss: 0.00002019
Iteration 108/1000 | Loss: 0.00001875
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001673
Iteration 111/1000 | Loss: 0.00001627
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001575
Iteration 116/1000 | Loss: 0.00001574
Iteration 117/1000 | Loss: 0.00001574
Iteration 118/1000 | Loss: 0.00001574
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001572
Iteration 121/1000 | Loss: 0.00001571
Iteration 122/1000 | Loss: 0.00001571
Iteration 123/1000 | Loss: 0.00001570
Iteration 124/1000 | Loss: 0.00001566
Iteration 125/1000 | Loss: 0.00001562
Iteration 126/1000 | Loss: 0.00001562
Iteration 127/1000 | Loss: 0.00001561
Iteration 128/1000 | Loss: 0.00001561
Iteration 129/1000 | Loss: 0.00001561
Iteration 130/1000 | Loss: 0.00001561
Iteration 131/1000 | Loss: 0.00001561
Iteration 132/1000 | Loss: 0.00001560
Iteration 133/1000 | Loss: 0.00001560
Iteration 134/1000 | Loss: 0.00001560
Iteration 135/1000 | Loss: 0.00001560
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001557
Iteration 144/1000 | Loss: 0.00001557
Iteration 145/1000 | Loss: 0.00001557
Iteration 146/1000 | Loss: 0.00001556
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001555
Iteration 150/1000 | Loss: 0.00001554
Iteration 151/1000 | Loss: 0.00001554
Iteration 152/1000 | Loss: 0.00001554
Iteration 153/1000 | Loss: 0.00001553
Iteration 154/1000 | Loss: 0.00001553
Iteration 155/1000 | Loss: 0.00001553
Iteration 156/1000 | Loss: 0.00001552
Iteration 157/1000 | Loss: 0.00001552
Iteration 158/1000 | Loss: 0.00001552
Iteration 159/1000 | Loss: 0.00001551
Iteration 160/1000 | Loss: 0.00001551
Iteration 161/1000 | Loss: 0.00001551
Iteration 162/1000 | Loss: 0.00001551
Iteration 163/1000 | Loss: 0.00001551
Iteration 164/1000 | Loss: 0.00001551
Iteration 165/1000 | Loss: 0.00001551
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001551
Iteration 168/1000 | Loss: 0.00001551
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Iteration 171/1000 | Loss: 0.00001551
Iteration 172/1000 | Loss: 0.00001551
Iteration 173/1000 | Loss: 0.00001550
Iteration 174/1000 | Loss: 0.00001550
Iteration 175/1000 | Loss: 0.00001550
Iteration 176/1000 | Loss: 0.00001550
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001549
Iteration 184/1000 | Loss: 0.00001549
Iteration 185/1000 | Loss: 0.00001549
Iteration 186/1000 | Loss: 0.00001549
Iteration 187/1000 | Loss: 0.00001549
Iteration 188/1000 | Loss: 0.00001549
Iteration 189/1000 | Loss: 0.00001549
Iteration 190/1000 | Loss: 0.00001549
Iteration 191/1000 | Loss: 0.00001549
Iteration 192/1000 | Loss: 0.00001549
Iteration 193/1000 | Loss: 0.00001549
Iteration 194/1000 | Loss: 0.00001549
Iteration 195/1000 | Loss: 0.00001549
Iteration 196/1000 | Loss: 0.00001549
Iteration 197/1000 | Loss: 0.00001549
Iteration 198/1000 | Loss: 0.00001549
Iteration 199/1000 | Loss: 0.00001549
Iteration 200/1000 | Loss: 0.00001549
Iteration 201/1000 | Loss: 0.00001549
Iteration 202/1000 | Loss: 0.00001549
Iteration 203/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.5486240954487585e-05, 1.5486240954487585e-05, 1.5486240954487585e-05, 1.5486240954487585e-05, 1.5486240954487585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5486240954487585e-05

Optimization complete. Final v2v error: 2.754974126815796 mm

Highest mean error: 11.479788780212402 mm for frame 98

Lowest mean error: 2.0997421741485596 mm for frame 17

Saving results

Total time: 224.8464035987854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821022
Iteration 2/25 | Loss: 0.00115855
Iteration 3/25 | Loss: 0.00103723
Iteration 4/25 | Loss: 0.00101906
Iteration 5/25 | Loss: 0.00101264
Iteration 6/25 | Loss: 0.00101109
Iteration 7/25 | Loss: 0.00101084
Iteration 8/25 | Loss: 0.00101084
Iteration 9/25 | Loss: 0.00101084
Iteration 10/25 | Loss: 0.00101084
Iteration 11/25 | Loss: 0.00101084
Iteration 12/25 | Loss: 0.00101084
Iteration 13/25 | Loss: 0.00101084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001010842970572412, 0.001010842970572412, 0.001010842970572412, 0.001010842970572412, 0.001010842970572412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001010842970572412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.05848408
Iteration 2/25 | Loss: 0.00162930
Iteration 3/25 | Loss: 0.00162930
Iteration 4/25 | Loss: 0.00162930
Iteration 5/25 | Loss: 0.00162929
Iteration 6/25 | Loss: 0.00162929
Iteration 7/25 | Loss: 0.00162929
Iteration 8/25 | Loss: 0.00162929
Iteration 9/25 | Loss: 0.00162929
Iteration 10/25 | Loss: 0.00162929
Iteration 11/25 | Loss: 0.00162929
Iteration 12/25 | Loss: 0.00162929
Iteration 13/25 | Loss: 0.00162929
Iteration 14/25 | Loss: 0.00162929
Iteration 15/25 | Loss: 0.00162929
Iteration 16/25 | Loss: 0.00162929
Iteration 17/25 | Loss: 0.00162929
Iteration 18/25 | Loss: 0.00162929
Iteration 19/25 | Loss: 0.00162929
Iteration 20/25 | Loss: 0.00162929
Iteration 21/25 | Loss: 0.00162929
Iteration 22/25 | Loss: 0.00162929
Iteration 23/25 | Loss: 0.00162929
Iteration 24/25 | Loss: 0.00162929
Iteration 25/25 | Loss: 0.00162929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162929
Iteration 2/1000 | Loss: 0.00004690
Iteration 3/1000 | Loss: 0.00002824
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001602
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001437
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001436
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001436
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001435
Iteration 29/1000 | Loss: 0.00001435
Iteration 30/1000 | Loss: 0.00001434
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001432
Iteration 33/1000 | Loss: 0.00001431
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001424
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001420
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001420
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001420
Iteration 57/1000 | Loss: 0.00001419
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001417
Iteration 61/1000 | Loss: 0.00001417
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001416
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001415
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001414
Iteration 75/1000 | Loss: 0.00001414
Iteration 76/1000 | Loss: 0.00001414
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001413
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001413
Iteration 85/1000 | Loss: 0.00001413
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001408
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001405
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001404
Iteration 133/1000 | Loss: 0.00001404
Iteration 134/1000 | Loss: 0.00001404
Iteration 135/1000 | Loss: 0.00001404
Iteration 136/1000 | Loss: 0.00001404
Iteration 137/1000 | Loss: 0.00001404
Iteration 138/1000 | Loss: 0.00001404
Iteration 139/1000 | Loss: 0.00001403
Iteration 140/1000 | Loss: 0.00001403
Iteration 141/1000 | Loss: 0.00001403
Iteration 142/1000 | Loss: 0.00001403
Iteration 143/1000 | Loss: 0.00001403
Iteration 144/1000 | Loss: 0.00001403
Iteration 145/1000 | Loss: 0.00001403
Iteration 146/1000 | Loss: 0.00001403
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001402
Iteration 149/1000 | Loss: 0.00001402
Iteration 150/1000 | Loss: 0.00001402
Iteration 151/1000 | Loss: 0.00001402
Iteration 152/1000 | Loss: 0.00001402
Iteration 153/1000 | Loss: 0.00001402
Iteration 154/1000 | Loss: 0.00001402
Iteration 155/1000 | Loss: 0.00001402
Iteration 156/1000 | Loss: 0.00001401
Iteration 157/1000 | Loss: 0.00001401
Iteration 158/1000 | Loss: 0.00001401
Iteration 159/1000 | Loss: 0.00001401
Iteration 160/1000 | Loss: 0.00001401
Iteration 161/1000 | Loss: 0.00001401
Iteration 162/1000 | Loss: 0.00001401
Iteration 163/1000 | Loss: 0.00001401
Iteration 164/1000 | Loss: 0.00001401
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001399
Iteration 171/1000 | Loss: 0.00001399
Iteration 172/1000 | Loss: 0.00001399
Iteration 173/1000 | Loss: 0.00001399
Iteration 174/1000 | Loss: 0.00001399
Iteration 175/1000 | Loss: 0.00001398
Iteration 176/1000 | Loss: 0.00001398
Iteration 177/1000 | Loss: 0.00001398
Iteration 178/1000 | Loss: 0.00001398
Iteration 179/1000 | Loss: 0.00001398
Iteration 180/1000 | Loss: 0.00001398
Iteration 181/1000 | Loss: 0.00001398
Iteration 182/1000 | Loss: 0.00001398
Iteration 183/1000 | Loss: 0.00001398
Iteration 184/1000 | Loss: 0.00001397
Iteration 185/1000 | Loss: 0.00001397
Iteration 186/1000 | Loss: 0.00001397
Iteration 187/1000 | Loss: 0.00001397
Iteration 188/1000 | Loss: 0.00001397
Iteration 189/1000 | Loss: 0.00001397
Iteration 190/1000 | Loss: 0.00001397
Iteration 191/1000 | Loss: 0.00001397
Iteration 192/1000 | Loss: 0.00001397
Iteration 193/1000 | Loss: 0.00001397
Iteration 194/1000 | Loss: 0.00001397
Iteration 195/1000 | Loss: 0.00001396
Iteration 196/1000 | Loss: 0.00001396
Iteration 197/1000 | Loss: 0.00001396
Iteration 198/1000 | Loss: 0.00001396
Iteration 199/1000 | Loss: 0.00001396
Iteration 200/1000 | Loss: 0.00001396
Iteration 201/1000 | Loss: 0.00001396
Iteration 202/1000 | Loss: 0.00001396
Iteration 203/1000 | Loss: 0.00001396
Iteration 204/1000 | Loss: 0.00001396
Iteration 205/1000 | Loss: 0.00001395
Iteration 206/1000 | Loss: 0.00001395
Iteration 207/1000 | Loss: 0.00001395
Iteration 208/1000 | Loss: 0.00001395
Iteration 209/1000 | Loss: 0.00001395
Iteration 210/1000 | Loss: 0.00001394
Iteration 211/1000 | Loss: 0.00001394
Iteration 212/1000 | Loss: 0.00001394
Iteration 213/1000 | Loss: 0.00001394
Iteration 214/1000 | Loss: 0.00001394
Iteration 215/1000 | Loss: 0.00001394
Iteration 216/1000 | Loss: 0.00001394
Iteration 217/1000 | Loss: 0.00001394
Iteration 218/1000 | Loss: 0.00001394
Iteration 219/1000 | Loss: 0.00001394
Iteration 220/1000 | Loss: 0.00001394
Iteration 221/1000 | Loss: 0.00001394
Iteration 222/1000 | Loss: 0.00001394
Iteration 223/1000 | Loss: 0.00001394
Iteration 224/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.3939931704953779e-05, 1.3939931704953779e-05, 1.3939931704953779e-05, 1.3939931704953779e-05, 1.3939931704953779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3939931704953779e-05

Optimization complete. Final v2v error: 3.106752872467041 mm

Highest mean error: 4.340821266174316 mm for frame 55

Lowest mean error: 2.654567003250122 mm for frame 41

Saving results

Total time: 41.93371272087097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064120
Iteration 2/25 | Loss: 0.00186360
Iteration 3/25 | Loss: 0.00134843
Iteration 4/25 | Loss: 0.00128195
Iteration 5/25 | Loss: 0.00119251
Iteration 6/25 | Loss: 0.00114083
Iteration 7/25 | Loss: 0.00108138
Iteration 8/25 | Loss: 0.00106230
Iteration 9/25 | Loss: 0.00105064
Iteration 10/25 | Loss: 0.00103244
Iteration 11/25 | Loss: 0.00102734
Iteration 12/25 | Loss: 0.00102100
Iteration 13/25 | Loss: 0.00101749
Iteration 14/25 | Loss: 0.00101596
Iteration 15/25 | Loss: 0.00102502
Iteration 16/25 | Loss: 0.00100916
Iteration 17/25 | Loss: 0.00100630
Iteration 18/25 | Loss: 0.00100554
Iteration 19/25 | Loss: 0.00100542
Iteration 20/25 | Loss: 0.00100539
Iteration 21/25 | Loss: 0.00100539
Iteration 22/25 | Loss: 0.00100539
Iteration 23/25 | Loss: 0.00100539
Iteration 24/25 | Loss: 0.00100539
Iteration 25/25 | Loss: 0.00100539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35707474
Iteration 2/25 | Loss: 0.00125977
Iteration 3/25 | Loss: 0.00125977
Iteration 4/25 | Loss: 0.00125977
Iteration 5/25 | Loss: 0.00125977
Iteration 6/25 | Loss: 0.00125977
Iteration 7/25 | Loss: 0.00123673
Iteration 8/25 | Loss: 0.00123672
Iteration 9/25 | Loss: 0.00123672
Iteration 10/25 | Loss: 0.00123672
Iteration 11/25 | Loss: 0.00123672
Iteration 12/25 | Loss: 0.00123672
Iteration 13/25 | Loss: 0.00123672
Iteration 14/25 | Loss: 0.00123672
Iteration 15/25 | Loss: 0.00123672
Iteration 16/25 | Loss: 0.00123672
Iteration 17/25 | Loss: 0.00123672
Iteration 18/25 | Loss: 0.00123672
Iteration 19/25 | Loss: 0.00123672
Iteration 20/25 | Loss: 0.00123672
Iteration 21/25 | Loss: 0.00123672
Iteration 22/25 | Loss: 0.00123672
Iteration 23/25 | Loss: 0.00123672
Iteration 24/25 | Loss: 0.00123672
Iteration 25/25 | Loss: 0.00123672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001236721291206777, 0.001236721291206777, 0.001236721291206777, 0.001236721291206777, 0.001236721291206777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001236721291206777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123672
Iteration 2/1000 | Loss: 0.00012027
Iteration 3/1000 | Loss: 0.00002570
Iteration 4/1000 | Loss: 0.00005083
Iteration 5/1000 | Loss: 0.00002417
Iteration 6/1000 | Loss: 0.00002019
Iteration 7/1000 | Loss: 0.00018444
Iteration 8/1000 | Loss: 0.00002323
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001809
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001668
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001625
Iteration 16/1000 | Loss: 0.00001609
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001572
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001570
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001562
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001551
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001550
Iteration 72/1000 | Loss: 0.00001550
Iteration 73/1000 | Loss: 0.00001550
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001550
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001546
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001545
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001542
Iteration 130/1000 | Loss: 0.00001542
Iteration 131/1000 | Loss: 0.00001542
Iteration 132/1000 | Loss: 0.00001542
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001541
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001540
Iteration 143/1000 | Loss: 0.00001540
Iteration 144/1000 | Loss: 0.00001540
Iteration 145/1000 | Loss: 0.00001540
Iteration 146/1000 | Loss: 0.00001540
Iteration 147/1000 | Loss: 0.00001540
Iteration 148/1000 | Loss: 0.00001540
Iteration 149/1000 | Loss: 0.00001540
Iteration 150/1000 | Loss: 0.00001540
Iteration 151/1000 | Loss: 0.00001540
Iteration 152/1000 | Loss: 0.00001540
Iteration 153/1000 | Loss: 0.00001540
Iteration 154/1000 | Loss: 0.00001540
Iteration 155/1000 | Loss: 0.00001540
Iteration 156/1000 | Loss: 0.00001540
Iteration 157/1000 | Loss: 0.00001539
Iteration 158/1000 | Loss: 0.00001539
Iteration 159/1000 | Loss: 0.00001539
Iteration 160/1000 | Loss: 0.00001539
Iteration 161/1000 | Loss: 0.00001539
Iteration 162/1000 | Loss: 0.00001539
Iteration 163/1000 | Loss: 0.00001539
Iteration 164/1000 | Loss: 0.00001539
Iteration 165/1000 | Loss: 0.00001539
Iteration 166/1000 | Loss: 0.00001539
Iteration 167/1000 | Loss: 0.00001539
Iteration 168/1000 | Loss: 0.00001539
Iteration 169/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.539291042718105e-05, 1.539291042718105e-05, 1.539291042718105e-05, 1.539291042718105e-05, 1.539291042718105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.539291042718105e-05

Optimization complete. Final v2v error: 2.9993484020233154 mm

Highest mean error: 11.071442604064941 mm for frame 174

Lowest mean error: 2.55902099609375 mm for frame 18

Saving results

Total time: 74.7435393333435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405131
Iteration 2/25 | Loss: 0.00109060
Iteration 3/25 | Loss: 0.00100437
Iteration 4/25 | Loss: 0.00099405
Iteration 5/25 | Loss: 0.00099127
Iteration 6/25 | Loss: 0.00099058
Iteration 7/25 | Loss: 0.00099058
Iteration 8/25 | Loss: 0.00099058
Iteration 9/25 | Loss: 0.00099058
Iteration 10/25 | Loss: 0.00099058
Iteration 11/25 | Loss: 0.00099058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000990577507764101, 0.000990577507764101, 0.000990577507764101, 0.000990577507764101, 0.000990577507764101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000990577507764101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27868426
Iteration 2/25 | Loss: 0.00138727
Iteration 3/25 | Loss: 0.00138727
Iteration 4/25 | Loss: 0.00138727
Iteration 5/25 | Loss: 0.00138727
Iteration 6/25 | Loss: 0.00138727
Iteration 7/25 | Loss: 0.00138727
Iteration 8/25 | Loss: 0.00138727
Iteration 9/25 | Loss: 0.00138727
Iteration 10/25 | Loss: 0.00138727
Iteration 11/25 | Loss: 0.00138727
Iteration 12/25 | Loss: 0.00138727
Iteration 13/25 | Loss: 0.00138727
Iteration 14/25 | Loss: 0.00138727
Iteration 15/25 | Loss: 0.00138727
Iteration 16/25 | Loss: 0.00138727
Iteration 17/25 | Loss: 0.00138727
Iteration 18/25 | Loss: 0.00138727
Iteration 19/25 | Loss: 0.00138727
Iteration 20/25 | Loss: 0.00138727
Iteration 21/25 | Loss: 0.00138727
Iteration 22/25 | Loss: 0.00138727
Iteration 23/25 | Loss: 0.00138727
Iteration 24/25 | Loss: 0.00138727
Iteration 25/25 | Loss: 0.00138727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138727
Iteration 2/1000 | Loss: 0.00003038
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001255
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001237
Iteration 18/1000 | Loss: 0.00001237
Iteration 19/1000 | Loss: 0.00001236
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001218
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001204
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001204
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001204
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001203
Iteration 145/1000 | Loss: 0.00001203
Iteration 146/1000 | Loss: 0.00001203
Iteration 147/1000 | Loss: 0.00001203
Iteration 148/1000 | Loss: 0.00001203
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Iteration 151/1000 | Loss: 0.00001203
Iteration 152/1000 | Loss: 0.00001203
Iteration 153/1000 | Loss: 0.00001203
Iteration 154/1000 | Loss: 0.00001202
Iteration 155/1000 | Loss: 0.00001202
Iteration 156/1000 | Loss: 0.00001202
Iteration 157/1000 | Loss: 0.00001202
Iteration 158/1000 | Loss: 0.00001202
Iteration 159/1000 | Loss: 0.00001202
Iteration 160/1000 | Loss: 0.00001202
Iteration 161/1000 | Loss: 0.00001202
Iteration 162/1000 | Loss: 0.00001202
Iteration 163/1000 | Loss: 0.00001202
Iteration 164/1000 | Loss: 0.00001201
Iteration 165/1000 | Loss: 0.00001201
Iteration 166/1000 | Loss: 0.00001201
Iteration 167/1000 | Loss: 0.00001201
Iteration 168/1000 | Loss: 0.00001201
Iteration 169/1000 | Loss: 0.00001201
Iteration 170/1000 | Loss: 0.00001201
Iteration 171/1000 | Loss: 0.00001201
Iteration 172/1000 | Loss: 0.00001201
Iteration 173/1000 | Loss: 0.00001201
Iteration 174/1000 | Loss: 0.00001201
Iteration 175/1000 | Loss: 0.00001201
Iteration 176/1000 | Loss: 0.00001201
Iteration 177/1000 | Loss: 0.00001201
Iteration 178/1000 | Loss: 0.00001201
Iteration 179/1000 | Loss: 0.00001201
Iteration 180/1000 | Loss: 0.00001201
Iteration 181/1000 | Loss: 0.00001201
Iteration 182/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2010539649054408e-05, 1.2010539649054408e-05, 1.2010539649054408e-05, 1.2010539649054408e-05, 1.2010539649054408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2010539649054408e-05

Optimization complete. Final v2v error: 2.939232110977173 mm

Highest mean error: 3.1681385040283203 mm for frame 61

Lowest mean error: 2.8298728466033936 mm for frame 128

Saving results

Total time: 36.027480363845825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603048
Iteration 2/25 | Loss: 0.00147154
Iteration 3/25 | Loss: 0.00109849
Iteration 4/25 | Loss: 0.00105990
Iteration 5/25 | Loss: 0.00105642
Iteration 6/25 | Loss: 0.00105563
Iteration 7/25 | Loss: 0.00105563
Iteration 8/25 | Loss: 0.00105563
Iteration 9/25 | Loss: 0.00105563
Iteration 10/25 | Loss: 0.00105563
Iteration 11/25 | Loss: 0.00105563
Iteration 12/25 | Loss: 0.00105563
Iteration 13/25 | Loss: 0.00105563
Iteration 14/25 | Loss: 0.00105563
Iteration 15/25 | Loss: 0.00105563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001055631204508245, 0.001055631204508245, 0.001055631204508245, 0.001055631204508245, 0.001055631204508245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001055631204508245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27669871
Iteration 2/25 | Loss: 0.00077368
Iteration 3/25 | Loss: 0.00077367
Iteration 4/25 | Loss: 0.00077367
Iteration 5/25 | Loss: 0.00077367
Iteration 6/25 | Loss: 0.00077367
Iteration 7/25 | Loss: 0.00077367
Iteration 8/25 | Loss: 0.00077367
Iteration 9/25 | Loss: 0.00077367
Iteration 10/25 | Loss: 0.00077367
Iteration 11/25 | Loss: 0.00077367
Iteration 12/25 | Loss: 0.00077367
Iteration 13/25 | Loss: 0.00077367
Iteration 14/25 | Loss: 0.00077367
Iteration 15/25 | Loss: 0.00077367
Iteration 16/25 | Loss: 0.00077367
Iteration 17/25 | Loss: 0.00077367
Iteration 18/25 | Loss: 0.00077367
Iteration 19/25 | Loss: 0.00077367
Iteration 20/25 | Loss: 0.00077367
Iteration 21/25 | Loss: 0.00077367
Iteration 22/25 | Loss: 0.00077367
Iteration 23/25 | Loss: 0.00077367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000773665145970881, 0.000773665145970881, 0.000773665145970881, 0.000773665145970881, 0.000773665145970881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000773665145970881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077367
Iteration 2/1000 | Loss: 0.00002883
Iteration 3/1000 | Loss: 0.00002134
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001622
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001517
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001502
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001486
Iteration 19/1000 | Loss: 0.00001485
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001482
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001482
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001481
Iteration 27/1000 | Loss: 0.00001479
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001477
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001470
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001468
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001465
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.45762560350704e-05, 1.45762560350704e-05, 1.45762560350704e-05, 1.45762560350704e-05, 1.45762560350704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.45762560350704e-05

Optimization complete. Final v2v error: 3.1901257038116455 mm

Highest mean error: 3.5083117485046387 mm for frame 11

Lowest mean error: 2.833362102508545 mm for frame 24

Saving results

Total time: 30.183334350585938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049005
Iteration 2/25 | Loss: 0.00175518
Iteration 3/25 | Loss: 0.00155780
Iteration 4/25 | Loss: 0.00125311
Iteration 5/25 | Loss: 0.00134669
Iteration 6/25 | Loss: 0.00126645
Iteration 7/25 | Loss: 0.00118751
Iteration 8/25 | Loss: 0.00113107
Iteration 9/25 | Loss: 0.00111835
Iteration 10/25 | Loss: 0.00110821
Iteration 11/25 | Loss: 0.00110746
Iteration 12/25 | Loss: 0.00111075
Iteration 13/25 | Loss: 0.00110703
Iteration 14/25 | Loss: 0.00114774
Iteration 15/25 | Loss: 0.00109850
Iteration 16/25 | Loss: 0.00109282
Iteration 17/25 | Loss: 0.00109457
Iteration 18/25 | Loss: 0.00109800
Iteration 19/25 | Loss: 0.00109483
Iteration 20/25 | Loss: 0.00108266
Iteration 21/25 | Loss: 0.00108185
Iteration 22/25 | Loss: 0.00108183
Iteration 23/25 | Loss: 0.00108183
Iteration 24/25 | Loss: 0.00108182
Iteration 25/25 | Loss: 0.00108182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73693693
Iteration 2/25 | Loss: 0.00181493
Iteration 3/25 | Loss: 0.00181490
Iteration 4/25 | Loss: 0.00181490
Iteration 5/25 | Loss: 0.00181490
Iteration 6/25 | Loss: 0.00181490
Iteration 7/25 | Loss: 0.00181490
Iteration 8/25 | Loss: 0.00181490
Iteration 9/25 | Loss: 0.00181490
Iteration 10/25 | Loss: 0.00181490
Iteration 11/25 | Loss: 0.00181490
Iteration 12/25 | Loss: 0.00181490
Iteration 13/25 | Loss: 0.00181490
Iteration 14/25 | Loss: 0.00181490
Iteration 15/25 | Loss: 0.00181490
Iteration 16/25 | Loss: 0.00181490
Iteration 17/25 | Loss: 0.00181490
Iteration 18/25 | Loss: 0.00181490
Iteration 19/25 | Loss: 0.00181490
Iteration 20/25 | Loss: 0.00181490
Iteration 21/25 | Loss: 0.00181490
Iteration 22/25 | Loss: 0.00181490
Iteration 23/25 | Loss: 0.00181490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018148989183828235, 0.0018148989183828235, 0.0018148989183828235, 0.0018148989183828235, 0.0018148989183828235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018148989183828235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181490
Iteration 2/1000 | Loss: 0.00015872
Iteration 3/1000 | Loss: 0.00072599
Iteration 4/1000 | Loss: 0.00042961
Iteration 5/1000 | Loss: 0.00032282
Iteration 6/1000 | Loss: 0.00033376
Iteration 7/1000 | Loss: 0.00011706
Iteration 8/1000 | Loss: 0.00009043
Iteration 9/1000 | Loss: 0.00038006
Iteration 10/1000 | Loss: 0.00008814
Iteration 11/1000 | Loss: 0.00006788
Iteration 12/1000 | Loss: 0.00005896
Iteration 13/1000 | Loss: 0.00005136
Iteration 14/1000 | Loss: 0.00004696
Iteration 15/1000 | Loss: 0.00047063
Iteration 16/1000 | Loss: 0.00005358
Iteration 17/1000 | Loss: 0.00004377
Iteration 18/1000 | Loss: 0.00004046
Iteration 19/1000 | Loss: 0.00003797
Iteration 20/1000 | Loss: 0.00003615
Iteration 21/1000 | Loss: 0.00003538
Iteration 22/1000 | Loss: 0.00003484
Iteration 23/1000 | Loss: 0.00003436
Iteration 24/1000 | Loss: 0.00003400
Iteration 25/1000 | Loss: 0.00003361
Iteration 26/1000 | Loss: 0.00003330
Iteration 27/1000 | Loss: 0.00003302
Iteration 28/1000 | Loss: 0.00034013
Iteration 29/1000 | Loss: 0.00132152
Iteration 30/1000 | Loss: 0.00035127
Iteration 31/1000 | Loss: 0.00028856
Iteration 32/1000 | Loss: 0.00062478
Iteration 33/1000 | Loss: 0.00007990
Iteration 34/1000 | Loss: 0.00005641
Iteration 35/1000 | Loss: 0.00004175
Iteration 36/1000 | Loss: 0.00003253
Iteration 37/1000 | Loss: 0.00002639
Iteration 38/1000 | Loss: 0.00063620
Iteration 39/1000 | Loss: 0.00004006
Iteration 40/1000 | Loss: 0.00003193
Iteration 41/1000 | Loss: 0.00002744
Iteration 42/1000 | Loss: 0.00002368
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00048603
Iteration 46/1000 | Loss: 0.00055992
Iteration 47/1000 | Loss: 0.00025640
Iteration 48/1000 | Loss: 0.00043637
Iteration 49/1000 | Loss: 0.00031308
Iteration 50/1000 | Loss: 0.00003229
Iteration 51/1000 | Loss: 0.00002584
Iteration 52/1000 | Loss: 0.00002281
Iteration 53/1000 | Loss: 0.00002058
Iteration 54/1000 | Loss: 0.00026439
Iteration 55/1000 | Loss: 0.00003644
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00010779
Iteration 58/1000 | Loss: 0.00004386
Iteration 59/1000 | Loss: 0.00007162
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001814
Iteration 63/1000 | Loss: 0.00001811
Iteration 64/1000 | Loss: 0.00001801
Iteration 65/1000 | Loss: 0.00001800
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001772
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001753
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001749
Iteration 87/1000 | Loss: 0.00001749
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00010176
Iteration 93/1000 | Loss: 0.00043842
Iteration 94/1000 | Loss: 0.00035052
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001836
Iteration 101/1000 | Loss: 0.00005275
Iteration 102/1000 | Loss: 0.00042350
Iteration 103/1000 | Loss: 0.00054517
Iteration 104/1000 | Loss: 0.00017505
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002015
Iteration 107/1000 | Loss: 0.00027862
Iteration 108/1000 | Loss: 0.00026526
Iteration 109/1000 | Loss: 0.00056541
Iteration 110/1000 | Loss: 0.00031974
Iteration 111/1000 | Loss: 0.00006910
Iteration 112/1000 | Loss: 0.00002995
Iteration 113/1000 | Loss: 0.00002523
Iteration 114/1000 | Loss: 0.00027566
Iteration 115/1000 | Loss: 0.00003026
Iteration 116/1000 | Loss: 0.00002407
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00001972
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001681
Iteration 121/1000 | Loss: 0.00001552
Iteration 122/1000 | Loss: 0.00001492
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001383
Iteration 131/1000 | Loss: 0.00001382
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001378
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001378
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001377
Iteration 148/1000 | Loss: 0.00001377
Iteration 149/1000 | Loss: 0.00001377
Iteration 150/1000 | Loss: 0.00001377
Iteration 151/1000 | Loss: 0.00001377
Iteration 152/1000 | Loss: 0.00001377
Iteration 153/1000 | Loss: 0.00001377
Iteration 154/1000 | Loss: 0.00001377
Iteration 155/1000 | Loss: 0.00001377
Iteration 156/1000 | Loss: 0.00001376
Iteration 157/1000 | Loss: 0.00001376
Iteration 158/1000 | Loss: 0.00001376
Iteration 159/1000 | Loss: 0.00001376
Iteration 160/1000 | Loss: 0.00001376
Iteration 161/1000 | Loss: 0.00001376
Iteration 162/1000 | Loss: 0.00001376
Iteration 163/1000 | Loss: 0.00001375
Iteration 164/1000 | Loss: 0.00001375
Iteration 165/1000 | Loss: 0.00001375
Iteration 166/1000 | Loss: 0.00001375
Iteration 167/1000 | Loss: 0.00001375
Iteration 168/1000 | Loss: 0.00001374
Iteration 169/1000 | Loss: 0.00001374
Iteration 170/1000 | Loss: 0.00001374
Iteration 171/1000 | Loss: 0.00001374
Iteration 172/1000 | Loss: 0.00001374
Iteration 173/1000 | Loss: 0.00001374
Iteration 174/1000 | Loss: 0.00001374
Iteration 175/1000 | Loss: 0.00001374
Iteration 176/1000 | Loss: 0.00001374
Iteration 177/1000 | Loss: 0.00001374
Iteration 178/1000 | Loss: 0.00001374
Iteration 179/1000 | Loss: 0.00001373
Iteration 180/1000 | Loss: 0.00001373
Iteration 181/1000 | Loss: 0.00001373
Iteration 182/1000 | Loss: 0.00001373
Iteration 183/1000 | Loss: 0.00001373
Iteration 184/1000 | Loss: 0.00001373
Iteration 185/1000 | Loss: 0.00001373
Iteration 186/1000 | Loss: 0.00001373
Iteration 187/1000 | Loss: 0.00001373
Iteration 188/1000 | Loss: 0.00001373
Iteration 189/1000 | Loss: 0.00001373
Iteration 190/1000 | Loss: 0.00001373
Iteration 191/1000 | Loss: 0.00001373
Iteration 192/1000 | Loss: 0.00001373
Iteration 193/1000 | Loss: 0.00001372
Iteration 194/1000 | Loss: 0.00001372
Iteration 195/1000 | Loss: 0.00001372
Iteration 196/1000 | Loss: 0.00001372
Iteration 197/1000 | Loss: 0.00001372
Iteration 198/1000 | Loss: 0.00001372
Iteration 199/1000 | Loss: 0.00001372
Iteration 200/1000 | Loss: 0.00001372
Iteration 201/1000 | Loss: 0.00001372
Iteration 202/1000 | Loss: 0.00001372
Iteration 203/1000 | Loss: 0.00001372
Iteration 204/1000 | Loss: 0.00001372
Iteration 205/1000 | Loss: 0.00001372
Iteration 206/1000 | Loss: 0.00001372
Iteration 207/1000 | Loss: 0.00001372
Iteration 208/1000 | Loss: 0.00001372
Iteration 209/1000 | Loss: 0.00001372
Iteration 210/1000 | Loss: 0.00001372
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001372
Iteration 213/1000 | Loss: 0.00001372
Iteration 214/1000 | Loss: 0.00001372
Iteration 215/1000 | Loss: 0.00001372
Iteration 216/1000 | Loss: 0.00001372
Iteration 217/1000 | Loss: 0.00001372
Iteration 218/1000 | Loss: 0.00001372
Iteration 219/1000 | Loss: 0.00001372
Iteration 220/1000 | Loss: 0.00001372
Iteration 221/1000 | Loss: 0.00001372
Iteration 222/1000 | Loss: 0.00001372
Iteration 223/1000 | Loss: 0.00001372
Iteration 224/1000 | Loss: 0.00001372
Iteration 225/1000 | Loss: 0.00001372
Iteration 226/1000 | Loss: 0.00001372
Iteration 227/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.3719771231990308e-05, 1.3719771231990308e-05, 1.3719771231990308e-05, 1.3719771231990308e-05, 1.3719771231990308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3719771231990308e-05

Optimization complete. Final v2v error: 3.03855037689209 mm

Highest mean error: 3.9287259578704834 mm for frame 38

Lowest mean error: 2.171839714050293 mm for frame 1

Saving results

Total time: 181.94071078300476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017298
Iteration 2/25 | Loss: 0.00264592
Iteration 3/25 | Loss: 0.00182622
Iteration 4/25 | Loss: 0.00171739
Iteration 5/25 | Loss: 0.00155427
Iteration 6/25 | Loss: 0.00146533
Iteration 7/25 | Loss: 0.00143743
Iteration 8/25 | Loss: 0.00144216
Iteration 9/25 | Loss: 0.00144577
Iteration 10/25 | Loss: 0.00134377
Iteration 11/25 | Loss: 0.00131668
Iteration 12/25 | Loss: 0.00130942
Iteration 13/25 | Loss: 0.00130188
Iteration 14/25 | Loss: 0.00129590
Iteration 15/25 | Loss: 0.00129312
Iteration 16/25 | Loss: 0.00129611
Iteration 17/25 | Loss: 0.00130179
Iteration 18/25 | Loss: 0.00129761
Iteration 19/25 | Loss: 0.00129458
Iteration 20/25 | Loss: 0.00129351
Iteration 21/25 | Loss: 0.00129322
Iteration 22/25 | Loss: 0.00129199
Iteration 23/25 | Loss: 0.00129199
Iteration 24/25 | Loss: 0.00129267
Iteration 25/25 | Loss: 0.00129436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21974003
Iteration 2/25 | Loss: 0.00375701
Iteration 3/25 | Loss: 0.00367764
Iteration 4/25 | Loss: 0.00367764
Iteration 5/25 | Loss: 0.00367764
Iteration 6/25 | Loss: 0.00367764
Iteration 7/25 | Loss: 0.00367764
Iteration 8/25 | Loss: 0.00367764
Iteration 9/25 | Loss: 0.00367764
Iteration 10/25 | Loss: 0.00367764
Iteration 11/25 | Loss: 0.00367764
Iteration 12/25 | Loss: 0.00367764
Iteration 13/25 | Loss: 0.00367764
Iteration 14/25 | Loss: 0.00367764
Iteration 15/25 | Loss: 0.00367764
Iteration 16/25 | Loss: 0.00367764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0036776375491172075, 0.0036776375491172075, 0.0036776375491172075, 0.0036776375491172075, 0.0036776375491172075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036776375491172075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00367764
Iteration 2/1000 | Loss: 0.00083317
Iteration 3/1000 | Loss: 0.00036073
Iteration 4/1000 | Loss: 0.00041726
Iteration 5/1000 | Loss: 0.00114072
Iteration 6/1000 | Loss: 0.00031416
Iteration 7/1000 | Loss: 0.00021509
Iteration 8/1000 | Loss: 0.00026841
Iteration 9/1000 | Loss: 0.00019857
Iteration 10/1000 | Loss: 0.00060761
Iteration 11/1000 | Loss: 0.00019358
Iteration 12/1000 | Loss: 0.00018569
Iteration 13/1000 | Loss: 0.00018948
Iteration 14/1000 | Loss: 0.00016627
Iteration 15/1000 | Loss: 0.00068183
Iteration 16/1000 | Loss: 0.00017731
Iteration 17/1000 | Loss: 0.00017111
Iteration 18/1000 | Loss: 0.00020385
Iteration 19/1000 | Loss: 0.00015613
Iteration 20/1000 | Loss: 0.00015990
Iteration 21/1000 | Loss: 0.00015351
Iteration 22/1000 | Loss: 0.00016552
Iteration 23/1000 | Loss: 0.00014744
Iteration 24/1000 | Loss: 0.00016362
Iteration 25/1000 | Loss: 0.00016030
Iteration 26/1000 | Loss: 0.00015943
Iteration 27/1000 | Loss: 0.00049014
Iteration 28/1000 | Loss: 0.01478983
Iteration 29/1000 | Loss: 0.00583678
Iteration 30/1000 | Loss: 0.00243750
Iteration 31/1000 | Loss: 0.00114755
Iteration 32/1000 | Loss: 0.00045336
Iteration 33/1000 | Loss: 0.00039521
Iteration 34/1000 | Loss: 0.00089896
Iteration 35/1000 | Loss: 0.00034937
Iteration 36/1000 | Loss: 0.00035798
Iteration 37/1000 | Loss: 0.00020716
Iteration 38/1000 | Loss: 0.00043952
Iteration 39/1000 | Loss: 0.00024684
Iteration 40/1000 | Loss: 0.00010790
Iteration 41/1000 | Loss: 0.00018418
Iteration 42/1000 | Loss: 0.00007532
Iteration 43/1000 | Loss: 0.00012153
Iteration 44/1000 | Loss: 0.00006230
Iteration 45/1000 | Loss: 0.00021986
Iteration 46/1000 | Loss: 0.00085677
Iteration 47/1000 | Loss: 0.00073018
Iteration 48/1000 | Loss: 0.00103246
Iteration 49/1000 | Loss: 0.00103188
Iteration 50/1000 | Loss: 0.00065247
Iteration 51/1000 | Loss: 0.00043584
Iteration 52/1000 | Loss: 0.00006128
Iteration 53/1000 | Loss: 0.00024155
Iteration 54/1000 | Loss: 0.00022059
Iteration 55/1000 | Loss: 0.00019850
Iteration 56/1000 | Loss: 0.00027392
Iteration 57/1000 | Loss: 0.00009267
Iteration 58/1000 | Loss: 0.00030349
Iteration 59/1000 | Loss: 0.00066579
Iteration 60/1000 | Loss: 0.00041021
Iteration 61/1000 | Loss: 0.00007194
Iteration 62/1000 | Loss: 0.00005962
Iteration 63/1000 | Loss: 0.00019012
Iteration 64/1000 | Loss: 0.00004411
Iteration 65/1000 | Loss: 0.00004087
Iteration 66/1000 | Loss: 0.00003913
Iteration 67/1000 | Loss: 0.00004015
Iteration 68/1000 | Loss: 0.00003892
Iteration 69/1000 | Loss: 0.00003503
Iteration 70/1000 | Loss: 0.00003398
Iteration 71/1000 | Loss: 0.00003306
Iteration 72/1000 | Loss: 0.00004697
Iteration 73/1000 | Loss: 0.00003403
Iteration 74/1000 | Loss: 0.00004133
Iteration 75/1000 | Loss: 0.00003212
Iteration 76/1000 | Loss: 0.00003169
Iteration 77/1000 | Loss: 0.00003115
Iteration 78/1000 | Loss: 0.00003085
Iteration 79/1000 | Loss: 0.00003057
Iteration 80/1000 | Loss: 0.00003046
Iteration 81/1000 | Loss: 0.00003034
Iteration 82/1000 | Loss: 0.00003032
Iteration 83/1000 | Loss: 0.00003021
Iteration 84/1000 | Loss: 0.00003020
Iteration 85/1000 | Loss: 0.00003020
Iteration 86/1000 | Loss: 0.00003012
Iteration 87/1000 | Loss: 0.00003010
Iteration 88/1000 | Loss: 0.00003010
Iteration 89/1000 | Loss: 0.00003009
Iteration 90/1000 | Loss: 0.00003008
Iteration 91/1000 | Loss: 0.00003005
Iteration 92/1000 | Loss: 0.00003005
Iteration 93/1000 | Loss: 0.00003000
Iteration 94/1000 | Loss: 0.00002999
Iteration 95/1000 | Loss: 0.00002994
Iteration 96/1000 | Loss: 0.00002993
Iteration 97/1000 | Loss: 0.00002991
Iteration 98/1000 | Loss: 0.00002990
Iteration 99/1000 | Loss: 0.00002990
Iteration 100/1000 | Loss: 0.00002990
Iteration 101/1000 | Loss: 0.00002990
Iteration 102/1000 | Loss: 0.00002989
Iteration 103/1000 | Loss: 0.00002989
Iteration 104/1000 | Loss: 0.00002989
Iteration 105/1000 | Loss: 0.00002988
Iteration 106/1000 | Loss: 0.00002988
Iteration 107/1000 | Loss: 0.00002987
Iteration 108/1000 | Loss: 0.00002987
Iteration 109/1000 | Loss: 0.00002987
Iteration 110/1000 | Loss: 0.00002986
Iteration 111/1000 | Loss: 0.00002986
Iteration 112/1000 | Loss: 0.00002986
Iteration 113/1000 | Loss: 0.00002986
Iteration 114/1000 | Loss: 0.00002985
Iteration 115/1000 | Loss: 0.00002985
Iteration 116/1000 | Loss: 0.00002985
Iteration 117/1000 | Loss: 0.00002985
Iteration 118/1000 | Loss: 0.00002985
Iteration 119/1000 | Loss: 0.00002985
Iteration 120/1000 | Loss: 0.00002985
Iteration 121/1000 | Loss: 0.00002985
Iteration 122/1000 | Loss: 0.00002985
Iteration 123/1000 | Loss: 0.00002984
Iteration 124/1000 | Loss: 0.00002984
Iteration 125/1000 | Loss: 0.00002984
Iteration 126/1000 | Loss: 0.00002984
Iteration 127/1000 | Loss: 0.00002984
Iteration 128/1000 | Loss: 0.00002983
Iteration 129/1000 | Loss: 0.00002983
Iteration 130/1000 | Loss: 0.00002983
Iteration 131/1000 | Loss: 0.00002983
Iteration 132/1000 | Loss: 0.00002982
Iteration 133/1000 | Loss: 0.00002982
Iteration 134/1000 | Loss: 0.00002982
Iteration 135/1000 | Loss: 0.00002982
Iteration 136/1000 | Loss: 0.00002981
Iteration 137/1000 | Loss: 0.00002981
Iteration 138/1000 | Loss: 0.00002981
Iteration 139/1000 | Loss: 0.00002981
Iteration 140/1000 | Loss: 0.00002981
Iteration 141/1000 | Loss: 0.00002981
Iteration 142/1000 | Loss: 0.00002981
Iteration 143/1000 | Loss: 0.00002981
Iteration 144/1000 | Loss: 0.00002980
Iteration 145/1000 | Loss: 0.00002980
Iteration 146/1000 | Loss: 0.00002980
Iteration 147/1000 | Loss: 0.00002980
Iteration 148/1000 | Loss: 0.00002979
Iteration 149/1000 | Loss: 0.00002979
Iteration 150/1000 | Loss: 0.00002979
Iteration 151/1000 | Loss: 0.00002979
Iteration 152/1000 | Loss: 0.00002978
Iteration 153/1000 | Loss: 0.00002978
Iteration 154/1000 | Loss: 0.00002978
Iteration 155/1000 | Loss: 0.00002978
Iteration 156/1000 | Loss: 0.00002977
Iteration 157/1000 | Loss: 0.00002977
Iteration 158/1000 | Loss: 0.00002977
Iteration 159/1000 | Loss: 0.00002977
Iteration 160/1000 | Loss: 0.00002977
Iteration 161/1000 | Loss: 0.00002977
Iteration 162/1000 | Loss: 0.00002977
Iteration 163/1000 | Loss: 0.00002977
Iteration 164/1000 | Loss: 0.00004523
Iteration 165/1000 | Loss: 0.00059652
Iteration 166/1000 | Loss: 0.00041433
Iteration 167/1000 | Loss: 0.00050301
Iteration 168/1000 | Loss: 0.00003696
Iteration 169/1000 | Loss: 0.00003281
Iteration 170/1000 | Loss: 0.00003980
Iteration 171/1000 | Loss: 0.00002983
Iteration 172/1000 | Loss: 0.00002687
Iteration 173/1000 | Loss: 0.00002628
Iteration 174/1000 | Loss: 0.00002580
Iteration 175/1000 | Loss: 0.00002547
Iteration 176/1000 | Loss: 0.00002517
Iteration 177/1000 | Loss: 0.00002499
Iteration 178/1000 | Loss: 0.00002498
Iteration 179/1000 | Loss: 0.00002493
Iteration 180/1000 | Loss: 0.00002487
Iteration 181/1000 | Loss: 0.00002472
Iteration 182/1000 | Loss: 0.00002470
Iteration 183/1000 | Loss: 0.00002469
Iteration 184/1000 | Loss: 0.00002469
Iteration 185/1000 | Loss: 0.00002468
Iteration 186/1000 | Loss: 0.00002467
Iteration 187/1000 | Loss: 0.00002466
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002466
Iteration 192/1000 | Loss: 0.00002466
Iteration 193/1000 | Loss: 0.00002466
Iteration 194/1000 | Loss: 0.00002466
Iteration 195/1000 | Loss: 0.00002466
Iteration 196/1000 | Loss: 0.00002466
Iteration 197/1000 | Loss: 0.00002466
Iteration 198/1000 | Loss: 0.00002464
Iteration 199/1000 | Loss: 0.00002464
Iteration 200/1000 | Loss: 0.00002463
Iteration 201/1000 | Loss: 0.00002463
Iteration 202/1000 | Loss: 0.00002463
Iteration 203/1000 | Loss: 0.00002462
Iteration 204/1000 | Loss: 0.00002462
Iteration 205/1000 | Loss: 0.00002462
Iteration 206/1000 | Loss: 0.00002462
Iteration 207/1000 | Loss: 0.00002462
Iteration 208/1000 | Loss: 0.00002462
Iteration 209/1000 | Loss: 0.00002461
Iteration 210/1000 | Loss: 0.00002461
Iteration 211/1000 | Loss: 0.00002461
Iteration 212/1000 | Loss: 0.00002461
Iteration 213/1000 | Loss: 0.00002461
Iteration 214/1000 | Loss: 0.00002461
Iteration 215/1000 | Loss: 0.00002461
Iteration 216/1000 | Loss: 0.00002460
Iteration 217/1000 | Loss: 0.00002460
Iteration 218/1000 | Loss: 0.00002460
Iteration 219/1000 | Loss: 0.00002460
Iteration 220/1000 | Loss: 0.00002460
Iteration 221/1000 | Loss: 0.00002460
Iteration 222/1000 | Loss: 0.00002460
Iteration 223/1000 | Loss: 0.00002460
Iteration 224/1000 | Loss: 0.00002460
Iteration 225/1000 | Loss: 0.00002459
Iteration 226/1000 | Loss: 0.00002459
Iteration 227/1000 | Loss: 0.00002459
Iteration 228/1000 | Loss: 0.00002459
Iteration 229/1000 | Loss: 0.00002459
Iteration 230/1000 | Loss: 0.00002459
Iteration 231/1000 | Loss: 0.00002459
Iteration 232/1000 | Loss: 0.00002459
Iteration 233/1000 | Loss: 0.00002459
Iteration 234/1000 | Loss: 0.00002459
Iteration 235/1000 | Loss: 0.00002459
Iteration 236/1000 | Loss: 0.00002459
Iteration 237/1000 | Loss: 0.00002459
Iteration 238/1000 | Loss: 0.00002459
Iteration 239/1000 | Loss: 0.00002459
Iteration 240/1000 | Loss: 0.00002459
Iteration 241/1000 | Loss: 0.00002459
Iteration 242/1000 | Loss: 0.00002458
Iteration 243/1000 | Loss: 0.00002458
Iteration 244/1000 | Loss: 0.00002458
Iteration 245/1000 | Loss: 0.00002458
Iteration 246/1000 | Loss: 0.00002458
Iteration 247/1000 | Loss: 0.00002458
Iteration 248/1000 | Loss: 0.00002458
Iteration 249/1000 | Loss: 0.00002458
Iteration 250/1000 | Loss: 0.00002458
Iteration 251/1000 | Loss: 0.00002458
Iteration 252/1000 | Loss: 0.00002458
Iteration 253/1000 | Loss: 0.00002458
Iteration 254/1000 | Loss: 0.00002458
Iteration 255/1000 | Loss: 0.00002458
Iteration 256/1000 | Loss: 0.00002458
Iteration 257/1000 | Loss: 0.00002458
Iteration 258/1000 | Loss: 0.00002458
Iteration 259/1000 | Loss: 0.00002458
Iteration 260/1000 | Loss: 0.00002458
Iteration 261/1000 | Loss: 0.00002458
Iteration 262/1000 | Loss: 0.00002458
Iteration 263/1000 | Loss: 0.00002457
Iteration 264/1000 | Loss: 0.00002457
Iteration 265/1000 | Loss: 0.00002457
Iteration 266/1000 | Loss: 0.00002457
Iteration 267/1000 | Loss: 0.00002457
Iteration 268/1000 | Loss: 0.00002457
Iteration 269/1000 | Loss: 0.00002457
Iteration 270/1000 | Loss: 0.00002457
Iteration 271/1000 | Loss: 0.00002457
Iteration 272/1000 | Loss: 0.00002457
Iteration 273/1000 | Loss: 0.00002457
Iteration 274/1000 | Loss: 0.00002457
Iteration 275/1000 | Loss: 0.00002457
Iteration 276/1000 | Loss: 0.00002457
Iteration 277/1000 | Loss: 0.00002457
Iteration 278/1000 | Loss: 0.00002457
Iteration 279/1000 | Loss: 0.00002457
Iteration 280/1000 | Loss: 0.00002457
Iteration 281/1000 | Loss: 0.00002457
Iteration 282/1000 | Loss: 0.00002457
Iteration 283/1000 | Loss: 0.00002457
Iteration 284/1000 | Loss: 0.00002457
Iteration 285/1000 | Loss: 0.00002457
Iteration 286/1000 | Loss: 0.00002457
Iteration 287/1000 | Loss: 0.00002457
Iteration 288/1000 | Loss: 0.00002457
Iteration 289/1000 | Loss: 0.00002457
Iteration 290/1000 | Loss: 0.00002457
Iteration 291/1000 | Loss: 0.00002457
Iteration 292/1000 | Loss: 0.00002457
Iteration 293/1000 | Loss: 0.00002457
Iteration 294/1000 | Loss: 0.00002457
Iteration 295/1000 | Loss: 0.00002457
Iteration 296/1000 | Loss: 0.00002457
Iteration 297/1000 | Loss: 0.00002457
Iteration 298/1000 | Loss: 0.00002457
Iteration 299/1000 | Loss: 0.00002457
Iteration 300/1000 | Loss: 0.00002457
Iteration 301/1000 | Loss: 0.00002457
Iteration 302/1000 | Loss: 0.00002457
Iteration 303/1000 | Loss: 0.00002457
Iteration 304/1000 | Loss: 0.00002457
Iteration 305/1000 | Loss: 0.00002457
Iteration 306/1000 | Loss: 0.00002457
Iteration 307/1000 | Loss: 0.00002457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.457484879414551e-05, 2.457484879414551e-05, 2.457484879414551e-05, 2.457484879414551e-05, 2.457484879414551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.457484879414551e-05

Optimization complete. Final v2v error: 3.7102112770080566 mm

Highest mean error: 12.405709266662598 mm for frame 89

Lowest mean error: 2.5288169384002686 mm for frame 16

Saving results

Total time: 197.6968924999237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060025
Iteration 2/25 | Loss: 0.00210702
Iteration 3/25 | Loss: 0.00190480
Iteration 4/25 | Loss: 0.00191641
Iteration 5/25 | Loss: 0.00170055
Iteration 6/25 | Loss: 0.00133119
Iteration 7/25 | Loss: 0.00118673
Iteration 8/25 | Loss: 0.00112922
Iteration 9/25 | Loss: 0.00111111
Iteration 10/25 | Loss: 0.00110525
Iteration 11/25 | Loss: 0.00110178
Iteration 12/25 | Loss: 0.00110017
Iteration 13/25 | Loss: 0.00110045
Iteration 14/25 | Loss: 0.00109867
Iteration 15/25 | Loss: 0.00109728
Iteration 16/25 | Loss: 0.00109638
Iteration 17/25 | Loss: 0.00109584
Iteration 18/25 | Loss: 0.00109578
Iteration 19/25 | Loss: 0.00109450
Iteration 20/25 | Loss: 0.00109534
Iteration 21/25 | Loss: 0.00109577
Iteration 22/25 | Loss: 0.00109548
Iteration 23/25 | Loss: 0.00109517
Iteration 24/25 | Loss: 0.00109520
Iteration 25/25 | Loss: 0.00109594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21293986
Iteration 2/25 | Loss: 0.00090772
Iteration 3/25 | Loss: 0.00090771
Iteration 4/25 | Loss: 0.00090771
Iteration 5/25 | Loss: 0.00090770
Iteration 6/25 | Loss: 0.00090770
Iteration 7/25 | Loss: 0.00090770
Iteration 8/25 | Loss: 0.00090770
Iteration 9/25 | Loss: 0.00090770
Iteration 10/25 | Loss: 0.00090770
Iteration 11/25 | Loss: 0.00090770
Iteration 12/25 | Loss: 0.00090770
Iteration 13/25 | Loss: 0.00090770
Iteration 14/25 | Loss: 0.00090770
Iteration 15/25 | Loss: 0.00090770
Iteration 16/25 | Loss: 0.00090770
Iteration 17/25 | Loss: 0.00090770
Iteration 18/25 | Loss: 0.00090770
Iteration 19/25 | Loss: 0.00090770
Iteration 20/25 | Loss: 0.00090770
Iteration 21/25 | Loss: 0.00090770
Iteration 22/25 | Loss: 0.00090770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009077026043087244, 0.0009077026043087244, 0.0009077026043087244, 0.0009077026043087244, 0.0009077026043087244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009077026043087244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090770
Iteration 2/1000 | Loss: 0.00004565
Iteration 3/1000 | Loss: 0.00004265
Iteration 4/1000 | Loss: 0.00003630
Iteration 5/1000 | Loss: 0.00003074
Iteration 6/1000 | Loss: 0.00003948
Iteration 7/1000 | Loss: 0.00003897
Iteration 8/1000 | Loss: 0.00004152
Iteration 9/1000 | Loss: 0.00002855
Iteration 10/1000 | Loss: 0.00004339
Iteration 11/1000 | Loss: 0.00003633
Iteration 12/1000 | Loss: 0.00003575
Iteration 13/1000 | Loss: 0.00003969
Iteration 14/1000 | Loss: 0.00003455
Iteration 15/1000 | Loss: 0.00003713
Iteration 16/1000 | Loss: 0.00003605
Iteration 17/1000 | Loss: 0.00003598
Iteration 18/1000 | Loss: 0.00004584
Iteration 19/1000 | Loss: 0.00002770
Iteration 20/1000 | Loss: 0.00002579
Iteration 21/1000 | Loss: 0.00002502
Iteration 22/1000 | Loss: 0.00002454
Iteration 23/1000 | Loss: 0.00002432
Iteration 24/1000 | Loss: 0.00002424
Iteration 25/1000 | Loss: 0.00002419
Iteration 26/1000 | Loss: 0.00002404
Iteration 27/1000 | Loss: 0.00002403
Iteration 28/1000 | Loss: 0.00002402
Iteration 29/1000 | Loss: 0.00002402
Iteration 30/1000 | Loss: 0.00002401
Iteration 31/1000 | Loss: 0.00002401
Iteration 32/1000 | Loss: 0.00002401
Iteration 33/1000 | Loss: 0.00002400
Iteration 34/1000 | Loss: 0.00002400
Iteration 35/1000 | Loss: 0.00002400
Iteration 36/1000 | Loss: 0.00002399
Iteration 37/1000 | Loss: 0.00002399
Iteration 38/1000 | Loss: 0.00002399
Iteration 39/1000 | Loss: 0.00002399
Iteration 40/1000 | Loss: 0.00002399
Iteration 41/1000 | Loss: 0.00002399
Iteration 42/1000 | Loss: 0.00002399
Iteration 43/1000 | Loss: 0.00002399
Iteration 44/1000 | Loss: 0.00002399
Iteration 45/1000 | Loss: 0.00002398
Iteration 46/1000 | Loss: 0.00002398
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002394
Iteration 49/1000 | Loss: 0.00002394
Iteration 50/1000 | Loss: 0.00002394
Iteration 51/1000 | Loss: 0.00002394
Iteration 52/1000 | Loss: 0.00002394
Iteration 53/1000 | Loss: 0.00002394
Iteration 54/1000 | Loss: 0.00002394
Iteration 55/1000 | Loss: 0.00002394
Iteration 56/1000 | Loss: 0.00002393
Iteration 57/1000 | Loss: 0.00002392
Iteration 58/1000 | Loss: 0.00002392
Iteration 59/1000 | Loss: 0.00002391
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00002389
Iteration 62/1000 | Loss: 0.00002388
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002381
Iteration 65/1000 | Loss: 0.00002379
Iteration 66/1000 | Loss: 0.00002378
Iteration 67/1000 | Loss: 0.00002377
Iteration 68/1000 | Loss: 0.00002377
Iteration 69/1000 | Loss: 0.00002376
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002375
Iteration 72/1000 | Loss: 0.00002374
Iteration 73/1000 | Loss: 0.00002374
Iteration 74/1000 | Loss: 0.00002373
Iteration 75/1000 | Loss: 0.00002367
Iteration 76/1000 | Loss: 0.00002367
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002363
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002362
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002361
Iteration 88/1000 | Loss: 0.00002361
Iteration 89/1000 | Loss: 0.00002361
Iteration 90/1000 | Loss: 0.00002361
Iteration 91/1000 | Loss: 0.00002361
Iteration 92/1000 | Loss: 0.00002361
Iteration 93/1000 | Loss: 0.00002361
Iteration 94/1000 | Loss: 0.00002361
Iteration 95/1000 | Loss: 0.00002361
Iteration 96/1000 | Loss: 0.00002361
Iteration 97/1000 | Loss: 0.00002361
Iteration 98/1000 | Loss: 0.00002361
Iteration 99/1000 | Loss: 0.00002361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.360685721214395e-05, 2.360685721214395e-05, 2.360685721214395e-05, 2.360685721214395e-05, 2.360685721214395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.360685721214395e-05

Optimization complete. Final v2v error: 4.212832450866699 mm

Highest mean error: 5.633741855621338 mm for frame 210

Lowest mean error: 3.855717182159424 mm for frame 34

Saving results

Total time: 102.91051650047302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374966
Iteration 2/25 | Loss: 0.00104984
Iteration 3/25 | Loss: 0.00097095
Iteration 4/25 | Loss: 0.00096528
Iteration 5/25 | Loss: 0.00096291
Iteration 6/25 | Loss: 0.00096284
Iteration 7/25 | Loss: 0.00096284
Iteration 8/25 | Loss: 0.00096284
Iteration 9/25 | Loss: 0.00096284
Iteration 10/25 | Loss: 0.00096284
Iteration 11/25 | Loss: 0.00096284
Iteration 12/25 | Loss: 0.00096284
Iteration 13/25 | Loss: 0.00096284
Iteration 14/25 | Loss: 0.00096284
Iteration 15/25 | Loss: 0.00096284
Iteration 16/25 | Loss: 0.00096284
Iteration 17/25 | Loss: 0.00096284
Iteration 18/25 | Loss: 0.00096284
Iteration 19/25 | Loss: 0.00096284
Iteration 20/25 | Loss: 0.00096284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009628375410102308, 0.0009628375410102308, 0.0009628375410102308, 0.0009628375410102308, 0.0009628375410102308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628375410102308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41237581
Iteration 2/25 | Loss: 0.00117765
Iteration 3/25 | Loss: 0.00117765
Iteration 4/25 | Loss: 0.00117765
Iteration 5/25 | Loss: 0.00117765
Iteration 6/25 | Loss: 0.00117765
Iteration 7/25 | Loss: 0.00117765
Iteration 8/25 | Loss: 0.00117765
Iteration 9/25 | Loss: 0.00117765
Iteration 10/25 | Loss: 0.00117765
Iteration 11/25 | Loss: 0.00117765
Iteration 12/25 | Loss: 0.00117765
Iteration 13/25 | Loss: 0.00117765
Iteration 14/25 | Loss: 0.00117765
Iteration 15/25 | Loss: 0.00117765
Iteration 16/25 | Loss: 0.00117765
Iteration 17/25 | Loss: 0.00117765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011776466853916645, 0.0011776466853916645, 0.0011776466853916645, 0.0011776466853916645, 0.0011776466853916645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011776466853916645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117765
Iteration 2/1000 | Loss: 0.00003010
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001488
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001219
Iteration 7/1000 | Loss: 0.00001161
Iteration 8/1000 | Loss: 0.00001122
Iteration 9/1000 | Loss: 0.00001091
Iteration 10/1000 | Loss: 0.00001069
Iteration 11/1000 | Loss: 0.00001058
Iteration 12/1000 | Loss: 0.00001044
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001034
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001032
Iteration 19/1000 | Loss: 0.00001026
Iteration 20/1000 | Loss: 0.00001025
Iteration 21/1000 | Loss: 0.00001025
Iteration 22/1000 | Loss: 0.00001024
Iteration 23/1000 | Loss: 0.00001024
Iteration 24/1000 | Loss: 0.00001023
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001022
Iteration 27/1000 | Loss: 0.00001022
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001021
Iteration 30/1000 | Loss: 0.00001020
Iteration 31/1000 | Loss: 0.00001020
Iteration 32/1000 | Loss: 0.00001020
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001019
Iteration 35/1000 | Loss: 0.00001019
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001017
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001016
Iteration 43/1000 | Loss: 0.00001016
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001016
Iteration 46/1000 | Loss: 0.00001016
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001014
Iteration 51/1000 | Loss: 0.00001013
Iteration 52/1000 | Loss: 0.00001013
Iteration 53/1000 | Loss: 0.00001013
Iteration 54/1000 | Loss: 0.00001012
Iteration 55/1000 | Loss: 0.00001012
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001011
Iteration 59/1000 | Loss: 0.00001010
Iteration 60/1000 | Loss: 0.00001008
Iteration 61/1000 | Loss: 0.00001008
Iteration 62/1000 | Loss: 0.00001008
Iteration 63/1000 | Loss: 0.00001008
Iteration 64/1000 | Loss: 0.00001008
Iteration 65/1000 | Loss: 0.00001008
Iteration 66/1000 | Loss: 0.00001008
Iteration 67/1000 | Loss: 0.00001008
Iteration 68/1000 | Loss: 0.00001008
Iteration 69/1000 | Loss: 0.00001008
Iteration 70/1000 | Loss: 0.00001007
Iteration 71/1000 | Loss: 0.00001007
Iteration 72/1000 | Loss: 0.00001007
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001006
Iteration 76/1000 | Loss: 0.00001005
Iteration 77/1000 | Loss: 0.00001004
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001003
Iteration 80/1000 | Loss: 0.00001003
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001003
Iteration 84/1000 | Loss: 0.00001003
Iteration 85/1000 | Loss: 0.00001002
Iteration 86/1000 | Loss: 0.00001002
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001002
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001001
Iteration 91/1000 | Loss: 0.00001001
Iteration 92/1000 | Loss: 0.00001001
Iteration 93/1000 | Loss: 0.00001000
Iteration 94/1000 | Loss: 0.00001000
Iteration 95/1000 | Loss: 0.00001000
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000996
Iteration 104/1000 | Loss: 0.00000996
Iteration 105/1000 | Loss: 0.00000996
Iteration 106/1000 | Loss: 0.00000996
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000995
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000992
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000992
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000992
Iteration 131/1000 | Loss: 0.00000992
Iteration 132/1000 | Loss: 0.00000992
Iteration 133/1000 | Loss: 0.00000992
Iteration 134/1000 | Loss: 0.00000992
Iteration 135/1000 | Loss: 0.00000992
Iteration 136/1000 | Loss: 0.00000992
Iteration 137/1000 | Loss: 0.00000992
Iteration 138/1000 | Loss: 0.00000992
Iteration 139/1000 | Loss: 0.00000992
Iteration 140/1000 | Loss: 0.00000992
Iteration 141/1000 | Loss: 0.00000992
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000992
Iteration 144/1000 | Loss: 0.00000992
Iteration 145/1000 | Loss: 0.00000992
Iteration 146/1000 | Loss: 0.00000992
Iteration 147/1000 | Loss: 0.00000992
Iteration 148/1000 | Loss: 0.00000992
Iteration 149/1000 | Loss: 0.00000992
Iteration 150/1000 | Loss: 0.00000992
Iteration 151/1000 | Loss: 0.00000992
Iteration 152/1000 | Loss: 0.00000992
Iteration 153/1000 | Loss: 0.00000992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [9.916718227032106e-06, 9.916718227032106e-06, 9.916718227032106e-06, 9.916718227032106e-06, 9.916718227032106e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.916718227032106e-06

Optimization complete. Final v2v error: 2.689347982406616 mm

Highest mean error: 3.181936502456665 mm for frame 130

Lowest mean error: 2.3230643272399902 mm for frame 2

Saving results

Total time: 41.26570916175842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470205
Iteration 2/25 | Loss: 0.00105447
Iteration 3/25 | Loss: 0.00097271
Iteration 4/25 | Loss: 0.00096170
Iteration 5/25 | Loss: 0.00095747
Iteration 6/25 | Loss: 0.00095619
Iteration 7/25 | Loss: 0.00095599
Iteration 8/25 | Loss: 0.00095599
Iteration 9/25 | Loss: 0.00095599
Iteration 10/25 | Loss: 0.00095599
Iteration 11/25 | Loss: 0.00095599
Iteration 12/25 | Loss: 0.00095599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009559926693327725, 0.0009559926693327725, 0.0009559926693327725, 0.0009559926693327725, 0.0009559926693327725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009559926693327725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40602708
Iteration 2/25 | Loss: 0.00116711
Iteration 3/25 | Loss: 0.00116711
Iteration 4/25 | Loss: 0.00116711
Iteration 5/25 | Loss: 0.00116711
Iteration 6/25 | Loss: 0.00116711
Iteration 7/25 | Loss: 0.00116711
Iteration 8/25 | Loss: 0.00116711
Iteration 9/25 | Loss: 0.00116711
Iteration 10/25 | Loss: 0.00116711
Iteration 11/25 | Loss: 0.00116711
Iteration 12/25 | Loss: 0.00116711
Iteration 13/25 | Loss: 0.00116711
Iteration 14/25 | Loss: 0.00116711
Iteration 15/25 | Loss: 0.00116711
Iteration 16/25 | Loss: 0.00116711
Iteration 17/25 | Loss: 0.00116711
Iteration 18/25 | Loss: 0.00116711
Iteration 19/25 | Loss: 0.00116711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011671086540445685, 0.0011671086540445685, 0.0011671086540445685, 0.0011671086540445685, 0.0011671086540445685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011671086540445685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116711
Iteration 2/1000 | Loss: 0.00002435
Iteration 3/1000 | Loss: 0.00001558
Iteration 4/1000 | Loss: 0.00001348
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001145
Iteration 9/1000 | Loss: 0.00001135
Iteration 10/1000 | Loss: 0.00001135
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001133
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001118
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001103
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001099
Iteration 37/1000 | Loss: 0.00001099
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001092
Iteration 53/1000 | Loss: 0.00001092
Iteration 54/1000 | Loss: 0.00001092
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001089
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001087
Iteration 74/1000 | Loss: 0.00001087
Iteration 75/1000 | Loss: 0.00001087
Iteration 76/1000 | Loss: 0.00001087
Iteration 77/1000 | Loss: 0.00001087
Iteration 78/1000 | Loss: 0.00001087
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001086
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001085
Iteration 85/1000 | Loss: 0.00001085
Iteration 86/1000 | Loss: 0.00001085
Iteration 87/1000 | Loss: 0.00001085
Iteration 88/1000 | Loss: 0.00001085
Iteration 89/1000 | Loss: 0.00001085
Iteration 90/1000 | Loss: 0.00001085
Iteration 91/1000 | Loss: 0.00001084
Iteration 92/1000 | Loss: 0.00001084
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001080
Iteration 105/1000 | Loss: 0.00001080
Iteration 106/1000 | Loss: 0.00001080
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001079
Iteration 110/1000 | Loss: 0.00001079
Iteration 111/1000 | Loss: 0.00001079
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001078
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001074
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001074
Iteration 147/1000 | Loss: 0.00001073
Iteration 148/1000 | Loss: 0.00001073
Iteration 149/1000 | Loss: 0.00001073
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001073
Iteration 159/1000 | Loss: 0.00001073
Iteration 160/1000 | Loss: 0.00001073
Iteration 161/1000 | Loss: 0.00001073
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001072
Iteration 166/1000 | Loss: 0.00001072
Iteration 167/1000 | Loss: 0.00001072
Iteration 168/1000 | Loss: 0.00001072
Iteration 169/1000 | Loss: 0.00001072
Iteration 170/1000 | Loss: 0.00001072
Iteration 171/1000 | Loss: 0.00001072
Iteration 172/1000 | Loss: 0.00001072
Iteration 173/1000 | Loss: 0.00001072
Iteration 174/1000 | Loss: 0.00001072
Iteration 175/1000 | Loss: 0.00001072
Iteration 176/1000 | Loss: 0.00001072
Iteration 177/1000 | Loss: 0.00001072
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Iteration 181/1000 | Loss: 0.00001071
Iteration 182/1000 | Loss: 0.00001071
Iteration 183/1000 | Loss: 0.00001071
Iteration 184/1000 | Loss: 0.00001071
Iteration 185/1000 | Loss: 0.00001071
Iteration 186/1000 | Loss: 0.00001071
Iteration 187/1000 | Loss: 0.00001071
Iteration 188/1000 | Loss: 0.00001071
Iteration 189/1000 | Loss: 0.00001071
Iteration 190/1000 | Loss: 0.00001071
Iteration 191/1000 | Loss: 0.00001071
Iteration 192/1000 | Loss: 0.00001071
Iteration 193/1000 | Loss: 0.00001071
Iteration 194/1000 | Loss: 0.00001071
Iteration 195/1000 | Loss: 0.00001071
Iteration 196/1000 | Loss: 0.00001071
Iteration 197/1000 | Loss: 0.00001071
Iteration 198/1000 | Loss: 0.00001071
Iteration 199/1000 | Loss: 0.00001071
Iteration 200/1000 | Loss: 0.00001071
Iteration 201/1000 | Loss: 0.00001071
Iteration 202/1000 | Loss: 0.00001071
Iteration 203/1000 | Loss: 0.00001071
Iteration 204/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0707332876336295e-05, 1.0707332876336295e-05, 1.0707332876336295e-05, 1.0707332876336295e-05, 1.0707332876336295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0707332876336295e-05

Optimization complete. Final v2v error: 2.6909539699554443 mm

Highest mean error: 3.42130446434021 mm for frame 54

Lowest mean error: 2.0714151859283447 mm for frame 2

Saving results

Total time: 37.03594160079956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852295
Iteration 2/25 | Loss: 0.00118576
Iteration 3/25 | Loss: 0.00103792
Iteration 4/25 | Loss: 0.00100715
Iteration 5/25 | Loss: 0.00099519
Iteration 6/25 | Loss: 0.00099210
Iteration 7/25 | Loss: 0.00099176
Iteration 8/25 | Loss: 0.00099176
Iteration 9/25 | Loss: 0.00099176
Iteration 10/25 | Loss: 0.00099176
Iteration 11/25 | Loss: 0.00099176
Iteration 12/25 | Loss: 0.00099176
Iteration 13/25 | Loss: 0.00099176
Iteration 14/25 | Loss: 0.00099176
Iteration 15/25 | Loss: 0.00099176
Iteration 16/25 | Loss: 0.00099176
Iteration 17/25 | Loss: 0.00099176
Iteration 18/25 | Loss: 0.00099176
Iteration 19/25 | Loss: 0.00099176
Iteration 20/25 | Loss: 0.00099176
Iteration 21/25 | Loss: 0.00099176
Iteration 22/25 | Loss: 0.00099176
Iteration 23/25 | Loss: 0.00099176
Iteration 24/25 | Loss: 0.00099176
Iteration 25/25 | Loss: 0.00099176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41535652
Iteration 2/25 | Loss: 0.00162991
Iteration 3/25 | Loss: 0.00162991
Iteration 4/25 | Loss: 0.00162991
Iteration 5/25 | Loss: 0.00162991
Iteration 6/25 | Loss: 0.00162991
Iteration 7/25 | Loss: 0.00162991
Iteration 8/25 | Loss: 0.00162991
Iteration 9/25 | Loss: 0.00162991
Iteration 10/25 | Loss: 0.00162991
Iteration 11/25 | Loss: 0.00162991
Iteration 12/25 | Loss: 0.00162991
Iteration 13/25 | Loss: 0.00162991
Iteration 14/25 | Loss: 0.00162991
Iteration 15/25 | Loss: 0.00162991
Iteration 16/25 | Loss: 0.00162991
Iteration 17/25 | Loss: 0.00162991
Iteration 18/25 | Loss: 0.00162991
Iteration 19/25 | Loss: 0.00162991
Iteration 20/25 | Loss: 0.00162991
Iteration 21/25 | Loss: 0.00162991
Iteration 22/25 | Loss: 0.00162991
Iteration 23/25 | Loss: 0.00162991
Iteration 24/25 | Loss: 0.00162991
Iteration 25/25 | Loss: 0.00162991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162991
Iteration 2/1000 | Loss: 0.00004365
Iteration 3/1000 | Loss: 0.00003043
Iteration 4/1000 | Loss: 0.00002569
Iteration 5/1000 | Loss: 0.00002325
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002115
Iteration 8/1000 | Loss: 0.00002054
Iteration 9/1000 | Loss: 0.00002011
Iteration 10/1000 | Loss: 0.00001982
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001941
Iteration 13/1000 | Loss: 0.00001927
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001921
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001914
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001897
Iteration 27/1000 | Loss: 0.00001897
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001895
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00001893
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001892
Iteration 40/1000 | Loss: 0.00001892
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001889
Iteration 52/1000 | Loss: 0.00001889
Iteration 53/1000 | Loss: 0.00001889
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001888
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001886
Iteration 64/1000 | Loss: 0.00001886
Iteration 65/1000 | Loss: 0.00001886
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001884
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001883
Iteration 87/1000 | Loss: 0.00001883
Iteration 88/1000 | Loss: 0.00001883
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001883
Iteration 92/1000 | Loss: 0.00001883
Iteration 93/1000 | Loss: 0.00001883
Iteration 94/1000 | Loss: 0.00001883
Iteration 95/1000 | Loss: 0.00001883
Iteration 96/1000 | Loss: 0.00001883
Iteration 97/1000 | Loss: 0.00001883
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.883252480183728e-05, 1.883252480183728e-05, 1.883252480183728e-05, 1.883252480183728e-05, 1.883252480183728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.883252480183728e-05

Optimization complete. Final v2v error: 3.512784719467163 mm

Highest mean error: 5.514291763305664 mm for frame 123

Lowest mean error: 2.448941230773926 mm for frame 0

Saving results

Total time: 38.10210394859314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385466
Iteration 2/25 | Loss: 0.00113050
Iteration 3/25 | Loss: 0.00100433
Iteration 4/25 | Loss: 0.00098686
Iteration 5/25 | Loss: 0.00098150
Iteration 6/25 | Loss: 0.00097995
Iteration 7/25 | Loss: 0.00097981
Iteration 8/25 | Loss: 0.00097981
Iteration 9/25 | Loss: 0.00097981
Iteration 10/25 | Loss: 0.00097981
Iteration 11/25 | Loss: 0.00097981
Iteration 12/25 | Loss: 0.00097981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009798143291845918, 0.0009798143291845918, 0.0009798143291845918, 0.0009798143291845918, 0.0009798143291845918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009798143291845918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63428831
Iteration 2/25 | Loss: 0.00166679
Iteration 3/25 | Loss: 0.00166663
Iteration 4/25 | Loss: 0.00166663
Iteration 5/25 | Loss: 0.00166663
Iteration 6/25 | Loss: 0.00166663
Iteration 7/25 | Loss: 0.00166662
Iteration 8/25 | Loss: 0.00166662
Iteration 9/25 | Loss: 0.00166662
Iteration 10/25 | Loss: 0.00166662
Iteration 11/25 | Loss: 0.00166662
Iteration 12/25 | Loss: 0.00166662
Iteration 13/25 | Loss: 0.00166662
Iteration 14/25 | Loss: 0.00166662
Iteration 15/25 | Loss: 0.00166662
Iteration 16/25 | Loss: 0.00166662
Iteration 17/25 | Loss: 0.00166662
Iteration 18/25 | Loss: 0.00166662
Iteration 19/25 | Loss: 0.00166662
Iteration 20/25 | Loss: 0.00166662
Iteration 21/25 | Loss: 0.00166662
Iteration 22/25 | Loss: 0.00166662
Iteration 23/25 | Loss: 0.00166662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016666235169395804, 0.0016666235169395804, 0.0016666235169395804, 0.0016666235169395804, 0.0016666235169395804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016666235169395804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166662
Iteration 2/1000 | Loss: 0.00003716
Iteration 3/1000 | Loss: 0.00002229
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00001314
Iteration 6/1000 | Loss: 0.00001219
Iteration 7/1000 | Loss: 0.00001148
Iteration 8/1000 | Loss: 0.00001106
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001051
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001048
Iteration 16/1000 | Loss: 0.00001046
Iteration 17/1000 | Loss: 0.00001046
Iteration 18/1000 | Loss: 0.00001045
Iteration 19/1000 | Loss: 0.00001045
Iteration 20/1000 | Loss: 0.00001044
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001044
Iteration 24/1000 | Loss: 0.00001044
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001043
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001042
Iteration 34/1000 | Loss: 0.00001042
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001041
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001040
Iteration 50/1000 | Loss: 0.00001040
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001038
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001035
Iteration 78/1000 | Loss: 0.00001035
Iteration 79/1000 | Loss: 0.00001034
Iteration 80/1000 | Loss: 0.00001034
Iteration 81/1000 | Loss: 0.00001033
Iteration 82/1000 | Loss: 0.00001033
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001032
Iteration 85/1000 | Loss: 0.00001032
Iteration 86/1000 | Loss: 0.00001031
Iteration 87/1000 | Loss: 0.00001031
Iteration 88/1000 | Loss: 0.00001031
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001030
Iteration 93/1000 | Loss: 0.00001030
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001026
Iteration 104/1000 | Loss: 0.00001026
Iteration 105/1000 | Loss: 0.00001026
Iteration 106/1000 | Loss: 0.00001026
Iteration 107/1000 | Loss: 0.00001026
Iteration 108/1000 | Loss: 0.00001026
Iteration 109/1000 | Loss: 0.00001025
Iteration 110/1000 | Loss: 0.00001025
Iteration 111/1000 | Loss: 0.00001025
Iteration 112/1000 | Loss: 0.00001025
Iteration 113/1000 | Loss: 0.00001025
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001024
Iteration 116/1000 | Loss: 0.00001024
Iteration 117/1000 | Loss: 0.00001024
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001023
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001022
Iteration 127/1000 | Loss: 0.00001022
Iteration 128/1000 | Loss: 0.00001022
Iteration 129/1000 | Loss: 0.00001021
Iteration 130/1000 | Loss: 0.00001021
Iteration 131/1000 | Loss: 0.00001021
Iteration 132/1000 | Loss: 0.00001021
Iteration 133/1000 | Loss: 0.00001021
Iteration 134/1000 | Loss: 0.00001021
Iteration 135/1000 | Loss: 0.00001021
Iteration 136/1000 | Loss: 0.00001021
Iteration 137/1000 | Loss: 0.00001021
Iteration 138/1000 | Loss: 0.00001021
Iteration 139/1000 | Loss: 0.00001020
Iteration 140/1000 | Loss: 0.00001020
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001019
Iteration 144/1000 | Loss: 0.00001019
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001018
Iteration 148/1000 | Loss: 0.00001018
Iteration 149/1000 | Loss: 0.00001018
Iteration 150/1000 | Loss: 0.00001017
Iteration 151/1000 | Loss: 0.00001017
Iteration 152/1000 | Loss: 0.00001017
Iteration 153/1000 | Loss: 0.00001017
Iteration 154/1000 | Loss: 0.00001017
Iteration 155/1000 | Loss: 0.00001016
Iteration 156/1000 | Loss: 0.00001016
Iteration 157/1000 | Loss: 0.00001016
Iteration 158/1000 | Loss: 0.00001015
Iteration 159/1000 | Loss: 0.00001015
Iteration 160/1000 | Loss: 0.00001015
Iteration 161/1000 | Loss: 0.00001015
Iteration 162/1000 | Loss: 0.00001015
Iteration 163/1000 | Loss: 0.00001014
Iteration 164/1000 | Loss: 0.00001014
Iteration 165/1000 | Loss: 0.00001014
Iteration 166/1000 | Loss: 0.00001014
Iteration 167/1000 | Loss: 0.00001014
Iteration 168/1000 | Loss: 0.00001014
Iteration 169/1000 | Loss: 0.00001014
Iteration 170/1000 | Loss: 0.00001014
Iteration 171/1000 | Loss: 0.00001014
Iteration 172/1000 | Loss: 0.00001013
Iteration 173/1000 | Loss: 0.00001013
Iteration 174/1000 | Loss: 0.00001013
Iteration 175/1000 | Loss: 0.00001013
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001012
Iteration 179/1000 | Loss: 0.00001012
Iteration 180/1000 | Loss: 0.00001012
Iteration 181/1000 | Loss: 0.00001012
Iteration 182/1000 | Loss: 0.00001012
Iteration 183/1000 | Loss: 0.00001012
Iteration 184/1000 | Loss: 0.00001012
Iteration 185/1000 | Loss: 0.00001012
Iteration 186/1000 | Loss: 0.00001012
Iteration 187/1000 | Loss: 0.00001012
Iteration 188/1000 | Loss: 0.00001012
Iteration 189/1000 | Loss: 0.00001012
Iteration 190/1000 | Loss: 0.00001012
Iteration 191/1000 | Loss: 0.00001011
Iteration 192/1000 | Loss: 0.00001011
Iteration 193/1000 | Loss: 0.00001011
Iteration 194/1000 | Loss: 0.00001011
Iteration 195/1000 | Loss: 0.00001011
Iteration 196/1000 | Loss: 0.00001011
Iteration 197/1000 | Loss: 0.00001011
Iteration 198/1000 | Loss: 0.00001011
Iteration 199/1000 | Loss: 0.00001011
Iteration 200/1000 | Loss: 0.00001011
Iteration 201/1000 | Loss: 0.00001011
Iteration 202/1000 | Loss: 0.00001011
Iteration 203/1000 | Loss: 0.00001011
Iteration 204/1000 | Loss: 0.00001011
Iteration 205/1000 | Loss: 0.00001011
Iteration 206/1000 | Loss: 0.00001011
Iteration 207/1000 | Loss: 0.00001011
Iteration 208/1000 | Loss: 0.00001011
Iteration 209/1000 | Loss: 0.00001011
Iteration 210/1000 | Loss: 0.00001011
Iteration 211/1000 | Loss: 0.00001011
Iteration 212/1000 | Loss: 0.00001011
Iteration 213/1000 | Loss: 0.00001011
Iteration 214/1000 | Loss: 0.00001011
Iteration 215/1000 | Loss: 0.00001011
Iteration 216/1000 | Loss: 0.00001011
Iteration 217/1000 | Loss: 0.00001011
Iteration 218/1000 | Loss: 0.00001011
Iteration 219/1000 | Loss: 0.00001011
Iteration 220/1000 | Loss: 0.00001011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.0106783520313911e-05, 1.0106783520313911e-05, 1.0106783520313911e-05, 1.0106783520313911e-05, 1.0106783520313911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0106783520313911e-05

Optimization complete. Final v2v error: 2.6253366470336914 mm

Highest mean error: 3.149778366088867 mm for frame 9

Lowest mean error: 2.1598246097564697 mm for frame 138

Saving results

Total time: 39.27251052856445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874386
Iteration 2/25 | Loss: 0.00130194
Iteration 3/25 | Loss: 0.00104127
Iteration 4/25 | Loss: 0.00100079
Iteration 5/25 | Loss: 0.00099800
Iteration 6/25 | Loss: 0.00099800
Iteration 7/25 | Loss: 0.00099800
Iteration 8/25 | Loss: 0.00099800
Iteration 9/25 | Loss: 0.00099800
Iteration 10/25 | Loss: 0.00099800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009980022441595793, 0.0009980022441595793, 0.0009980022441595793, 0.0009980022441595793, 0.0009980022441595793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009980022441595793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25231302
Iteration 2/25 | Loss: 0.00124743
Iteration 3/25 | Loss: 0.00124743
Iteration 4/25 | Loss: 0.00124743
Iteration 5/25 | Loss: 0.00124743
Iteration 6/25 | Loss: 0.00124743
Iteration 7/25 | Loss: 0.00124743
Iteration 8/25 | Loss: 0.00124743
Iteration 9/25 | Loss: 0.00124742
Iteration 10/25 | Loss: 0.00124742
Iteration 11/25 | Loss: 0.00124742
Iteration 12/25 | Loss: 0.00124742
Iteration 13/25 | Loss: 0.00124742
Iteration 14/25 | Loss: 0.00124742
Iteration 15/25 | Loss: 0.00124742
Iteration 16/25 | Loss: 0.00124742
Iteration 17/25 | Loss: 0.00124742
Iteration 18/25 | Loss: 0.00124742
Iteration 19/25 | Loss: 0.00124742
Iteration 20/25 | Loss: 0.00124742
Iteration 21/25 | Loss: 0.00124742
Iteration 22/25 | Loss: 0.00124742
Iteration 23/25 | Loss: 0.00124742
Iteration 24/25 | Loss: 0.00124742
Iteration 25/25 | Loss: 0.00124742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124742
Iteration 2/1000 | Loss: 0.00003337
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001661
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001135
Iteration 11/1000 | Loss: 0.00001112
Iteration 12/1000 | Loss: 0.00001099
Iteration 13/1000 | Loss: 0.00001094
Iteration 14/1000 | Loss: 0.00001092
Iteration 15/1000 | Loss: 0.00001086
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001077
Iteration 26/1000 | Loss: 0.00001076
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001068
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001064
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001063
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001062
Iteration 53/1000 | Loss: 0.00001061
Iteration 54/1000 | Loss: 0.00001061
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001060
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001059
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001057
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001054
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001053
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001052
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001050
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001048
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001047
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001046
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001045
Iteration 89/1000 | Loss: 0.00001045
Iteration 90/1000 | Loss: 0.00001045
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001045
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001042
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001041
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001040
Iteration 108/1000 | Loss: 0.00001040
Iteration 109/1000 | Loss: 0.00001040
Iteration 110/1000 | Loss: 0.00001040
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001039
Iteration 115/1000 | Loss: 0.00001039
Iteration 116/1000 | Loss: 0.00001039
Iteration 117/1000 | Loss: 0.00001039
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001038
Iteration 121/1000 | Loss: 0.00001038
Iteration 122/1000 | Loss: 0.00001038
Iteration 123/1000 | Loss: 0.00001038
Iteration 124/1000 | Loss: 0.00001038
Iteration 125/1000 | Loss: 0.00001038
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001037
Iteration 132/1000 | Loss: 0.00001037
Iteration 133/1000 | Loss: 0.00001037
Iteration 134/1000 | Loss: 0.00001037
Iteration 135/1000 | Loss: 0.00001037
Iteration 136/1000 | Loss: 0.00001037
Iteration 137/1000 | Loss: 0.00001036
Iteration 138/1000 | Loss: 0.00001036
Iteration 139/1000 | Loss: 0.00001036
Iteration 140/1000 | Loss: 0.00001036
Iteration 141/1000 | Loss: 0.00001036
Iteration 142/1000 | Loss: 0.00001035
Iteration 143/1000 | Loss: 0.00001035
Iteration 144/1000 | Loss: 0.00001035
Iteration 145/1000 | Loss: 0.00001035
Iteration 146/1000 | Loss: 0.00001035
Iteration 147/1000 | Loss: 0.00001035
Iteration 148/1000 | Loss: 0.00001035
Iteration 149/1000 | Loss: 0.00001035
Iteration 150/1000 | Loss: 0.00001035
Iteration 151/1000 | Loss: 0.00001035
Iteration 152/1000 | Loss: 0.00001035
Iteration 153/1000 | Loss: 0.00001035
Iteration 154/1000 | Loss: 0.00001035
Iteration 155/1000 | Loss: 0.00001035
Iteration 156/1000 | Loss: 0.00001035
Iteration 157/1000 | Loss: 0.00001035
Iteration 158/1000 | Loss: 0.00001035
Iteration 159/1000 | Loss: 0.00001035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.0351534001529217e-05, 1.0351534001529217e-05, 1.0351534001529217e-05, 1.0351534001529217e-05, 1.0351534001529217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0351534001529217e-05

Optimization complete. Final v2v error: 2.677086591720581 mm

Highest mean error: 3.382327079772949 mm for frame 164

Lowest mean error: 2.2568466663360596 mm for frame 56

Saving results

Total time: 42.44576025009155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01207366
Iteration 2/25 | Loss: 0.01207366
Iteration 3/25 | Loss: 0.00366368
Iteration 4/25 | Loss: 0.00233621
Iteration 5/25 | Loss: 0.00204820
Iteration 6/25 | Loss: 0.00179710
Iteration 7/25 | Loss: 0.00160439
Iteration 8/25 | Loss: 0.00143863
Iteration 9/25 | Loss: 0.00137673
Iteration 10/25 | Loss: 0.00134231
Iteration 11/25 | Loss: 0.00132985
Iteration 12/25 | Loss: 0.00131812
Iteration 13/25 | Loss: 0.00132144
Iteration 14/25 | Loss: 0.00130292
Iteration 15/25 | Loss: 0.00129161
Iteration 16/25 | Loss: 0.00128012
Iteration 17/25 | Loss: 0.00128229
Iteration 18/25 | Loss: 0.00128067
Iteration 19/25 | Loss: 0.00127136
Iteration 20/25 | Loss: 0.00126621
Iteration 21/25 | Loss: 0.00126666
Iteration 22/25 | Loss: 0.00126851
Iteration 23/25 | Loss: 0.00126701
Iteration 24/25 | Loss: 0.00126735
Iteration 25/25 | Loss: 0.00126552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62938374
Iteration 2/25 | Loss: 0.00150261
Iteration 3/25 | Loss: 0.00150261
Iteration 4/25 | Loss: 0.00150261
Iteration 5/25 | Loss: 0.00150261
Iteration 6/25 | Loss: 0.00150261
Iteration 7/25 | Loss: 0.00150261
Iteration 8/25 | Loss: 0.00150261
Iteration 9/25 | Loss: 0.00150261
Iteration 10/25 | Loss: 0.00150261
Iteration 11/25 | Loss: 0.00150261
Iteration 12/25 | Loss: 0.00150261
Iteration 13/25 | Loss: 0.00150261
Iteration 14/25 | Loss: 0.00150261
Iteration 15/25 | Loss: 0.00150261
Iteration 16/25 | Loss: 0.00150261
Iteration 17/25 | Loss: 0.00150261
Iteration 18/25 | Loss: 0.00150261
Iteration 19/25 | Loss: 0.00150261
Iteration 20/25 | Loss: 0.00150261
Iteration 21/25 | Loss: 0.00150261
Iteration 22/25 | Loss: 0.00150261
Iteration 23/25 | Loss: 0.00150261
Iteration 24/25 | Loss: 0.00150261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015026084147393703, 0.0015026084147393703, 0.0015026084147393703, 0.0015026084147393703, 0.0015026084147393703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015026084147393703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150261
Iteration 2/1000 | Loss: 0.00030055
Iteration 3/1000 | Loss: 0.00029038
Iteration 4/1000 | Loss: 0.00010901
Iteration 5/1000 | Loss: 0.00025524
Iteration 6/1000 | Loss: 0.00026667
Iteration 7/1000 | Loss: 0.00030689
Iteration 8/1000 | Loss: 0.00023526
Iteration 9/1000 | Loss: 0.00018269
Iteration 10/1000 | Loss: 0.00037116
Iteration 11/1000 | Loss: 0.00024309
Iteration 12/1000 | Loss: 0.00023764
Iteration 13/1000 | Loss: 0.00036704
Iteration 14/1000 | Loss: 0.00032310
Iteration 15/1000 | Loss: 0.00029590
Iteration 16/1000 | Loss: 0.00032003
Iteration 17/1000 | Loss: 0.00023677
Iteration 18/1000 | Loss: 0.00018627
Iteration 19/1000 | Loss: 0.00024767
Iteration 20/1000 | Loss: 0.00024887
Iteration 21/1000 | Loss: 0.00024658
Iteration 22/1000 | Loss: 0.00025941
Iteration 23/1000 | Loss: 0.00026746
Iteration 24/1000 | Loss: 0.00023399
Iteration 25/1000 | Loss: 0.00027601
Iteration 26/1000 | Loss: 0.00029554
Iteration 27/1000 | Loss: 0.00020243
Iteration 28/1000 | Loss: 0.00010797
Iteration 29/1000 | Loss: 0.00029209
Iteration 30/1000 | Loss: 0.00028469
Iteration 31/1000 | Loss: 0.00023096
Iteration 32/1000 | Loss: 0.00027903
Iteration 33/1000 | Loss: 0.00020937
Iteration 34/1000 | Loss: 0.00024859
Iteration 35/1000 | Loss: 0.00020512
Iteration 36/1000 | Loss: 0.00037332
Iteration 37/1000 | Loss: 0.00021457
Iteration 38/1000 | Loss: 0.00031419
Iteration 39/1000 | Loss: 0.00024284
Iteration 40/1000 | Loss: 0.00031497
Iteration 41/1000 | Loss: 0.00026023
Iteration 42/1000 | Loss: 0.00023002
Iteration 43/1000 | Loss: 0.00023124
Iteration 44/1000 | Loss: 0.00018355
Iteration 45/1000 | Loss: 0.00028674
Iteration 46/1000 | Loss: 0.00022014
Iteration 47/1000 | Loss: 0.00007525
Iteration 48/1000 | Loss: 0.00010199
Iteration 49/1000 | Loss: 0.00012647
Iteration 50/1000 | Loss: 0.00015719
Iteration 51/1000 | Loss: 0.00009636
Iteration 52/1000 | Loss: 0.00017722
Iteration 53/1000 | Loss: 0.00016090
Iteration 54/1000 | Loss: 0.00014966
Iteration 55/1000 | Loss: 0.00010260
Iteration 56/1000 | Loss: 0.00004694
Iteration 57/1000 | Loss: 0.00012308
Iteration 58/1000 | Loss: 0.00015512
Iteration 59/1000 | Loss: 0.00020825
Iteration 60/1000 | Loss: 0.00039659
Iteration 61/1000 | Loss: 0.00012171
Iteration 62/1000 | Loss: 0.00019102
Iteration 63/1000 | Loss: 0.00009689
Iteration 64/1000 | Loss: 0.00009097
Iteration 65/1000 | Loss: 0.00017189
Iteration 66/1000 | Loss: 0.00014115
Iteration 67/1000 | Loss: 0.00015164
Iteration 68/1000 | Loss: 0.00017725
Iteration 69/1000 | Loss: 0.00015706
Iteration 70/1000 | Loss: 0.00019788
Iteration 71/1000 | Loss: 0.00011534
Iteration 72/1000 | Loss: 0.00017359
Iteration 73/1000 | Loss: 0.00011038
Iteration 74/1000 | Loss: 0.00027816
Iteration 75/1000 | Loss: 0.00011021
Iteration 76/1000 | Loss: 0.00024691
Iteration 77/1000 | Loss: 0.00021392
Iteration 78/1000 | Loss: 0.00021200
Iteration 79/1000 | Loss: 0.00020251
Iteration 80/1000 | Loss: 0.00005452
Iteration 81/1000 | Loss: 0.00010343
Iteration 82/1000 | Loss: 0.00019981
Iteration 83/1000 | Loss: 0.00013112
Iteration 84/1000 | Loss: 0.00018495
Iteration 85/1000 | Loss: 0.00022237
Iteration 86/1000 | Loss: 0.00018968
Iteration 87/1000 | Loss: 0.00020682
Iteration 88/1000 | Loss: 0.00004811
Iteration 89/1000 | Loss: 0.00004585
Iteration 90/1000 | Loss: 0.00004472
Iteration 91/1000 | Loss: 0.00004348
Iteration 92/1000 | Loss: 0.00004275
Iteration 93/1000 | Loss: 0.00004222
Iteration 94/1000 | Loss: 0.00004193
Iteration 95/1000 | Loss: 0.00004172
Iteration 96/1000 | Loss: 0.00004154
Iteration 97/1000 | Loss: 0.00004137
Iteration 98/1000 | Loss: 0.00004121
Iteration 99/1000 | Loss: 0.00004104
Iteration 100/1000 | Loss: 0.00004102
Iteration 101/1000 | Loss: 0.00004100
Iteration 102/1000 | Loss: 0.00004096
Iteration 103/1000 | Loss: 0.00004092
Iteration 104/1000 | Loss: 0.00004092
Iteration 105/1000 | Loss: 0.00004091
Iteration 106/1000 | Loss: 0.00004091
Iteration 107/1000 | Loss: 0.00004091
Iteration 108/1000 | Loss: 0.00004091
Iteration 109/1000 | Loss: 0.00004090
Iteration 110/1000 | Loss: 0.00004090
Iteration 111/1000 | Loss: 0.00004090
Iteration 112/1000 | Loss: 0.00004090
Iteration 113/1000 | Loss: 0.00004090
Iteration 114/1000 | Loss: 0.00004090
Iteration 115/1000 | Loss: 0.00004090
Iteration 116/1000 | Loss: 0.00004090
Iteration 117/1000 | Loss: 0.00004090
Iteration 118/1000 | Loss: 0.00004090
Iteration 119/1000 | Loss: 0.00004090
Iteration 120/1000 | Loss: 0.00004089
Iteration 121/1000 | Loss: 0.00004089
Iteration 122/1000 | Loss: 0.00004089
Iteration 123/1000 | Loss: 0.00004089
Iteration 124/1000 | Loss: 0.00004089
Iteration 125/1000 | Loss: 0.00004088
Iteration 126/1000 | Loss: 0.00004088
Iteration 127/1000 | Loss: 0.00004088
Iteration 128/1000 | Loss: 0.00004087
Iteration 129/1000 | Loss: 0.00004087
Iteration 130/1000 | Loss: 0.00004086
Iteration 131/1000 | Loss: 0.00004086
Iteration 132/1000 | Loss: 0.00004086
Iteration 133/1000 | Loss: 0.00004086
Iteration 134/1000 | Loss: 0.00004086
Iteration 135/1000 | Loss: 0.00004086
Iteration 136/1000 | Loss: 0.00004085
Iteration 137/1000 | Loss: 0.00004085
Iteration 138/1000 | Loss: 0.00004085
Iteration 139/1000 | Loss: 0.00004085
Iteration 140/1000 | Loss: 0.00004085
Iteration 141/1000 | Loss: 0.00004085
Iteration 142/1000 | Loss: 0.00004085
Iteration 143/1000 | Loss: 0.00004084
Iteration 144/1000 | Loss: 0.00004084
Iteration 145/1000 | Loss: 0.00004084
Iteration 146/1000 | Loss: 0.00004084
Iteration 147/1000 | Loss: 0.00004083
Iteration 148/1000 | Loss: 0.00004083
Iteration 149/1000 | Loss: 0.00004083
Iteration 150/1000 | Loss: 0.00004083
Iteration 151/1000 | Loss: 0.00004083
Iteration 152/1000 | Loss: 0.00004083
Iteration 153/1000 | Loss: 0.00004082
Iteration 154/1000 | Loss: 0.00004082
Iteration 155/1000 | Loss: 0.00004081
Iteration 156/1000 | Loss: 0.00004081
Iteration 157/1000 | Loss: 0.00004081
Iteration 158/1000 | Loss: 0.00004081
Iteration 159/1000 | Loss: 0.00004080
Iteration 160/1000 | Loss: 0.00004079
Iteration 161/1000 | Loss: 0.00004079
Iteration 162/1000 | Loss: 0.00004078
Iteration 163/1000 | Loss: 0.00004078
Iteration 164/1000 | Loss: 0.00004077
Iteration 165/1000 | Loss: 0.00004075
Iteration 166/1000 | Loss: 0.00004075
Iteration 167/1000 | Loss: 0.00004075
Iteration 168/1000 | Loss: 0.00004074
Iteration 169/1000 | Loss: 0.00004074
Iteration 170/1000 | Loss: 0.00004074
Iteration 171/1000 | Loss: 0.00004074
Iteration 172/1000 | Loss: 0.00004074
Iteration 173/1000 | Loss: 0.00004074
Iteration 174/1000 | Loss: 0.00004074
Iteration 175/1000 | Loss: 0.00004073
Iteration 176/1000 | Loss: 0.00004073
Iteration 177/1000 | Loss: 0.00004073
Iteration 178/1000 | Loss: 0.00004072
Iteration 179/1000 | Loss: 0.00004072
Iteration 180/1000 | Loss: 0.00004072
Iteration 181/1000 | Loss: 0.00004072
Iteration 182/1000 | Loss: 0.00004072
Iteration 183/1000 | Loss: 0.00004071
Iteration 184/1000 | Loss: 0.00004071
Iteration 185/1000 | Loss: 0.00004071
Iteration 186/1000 | Loss: 0.00004071
Iteration 187/1000 | Loss: 0.00004071
Iteration 188/1000 | Loss: 0.00004071
Iteration 189/1000 | Loss: 0.00004070
Iteration 190/1000 | Loss: 0.00004070
Iteration 191/1000 | Loss: 0.00004070
Iteration 192/1000 | Loss: 0.00004070
Iteration 193/1000 | Loss: 0.00004070
Iteration 194/1000 | Loss: 0.00004070
Iteration 195/1000 | Loss: 0.00004070
Iteration 196/1000 | Loss: 0.00004070
Iteration 197/1000 | Loss: 0.00004070
Iteration 198/1000 | Loss: 0.00004070
Iteration 199/1000 | Loss: 0.00004070
Iteration 200/1000 | Loss: 0.00004069
Iteration 201/1000 | Loss: 0.00004069
Iteration 202/1000 | Loss: 0.00004068
Iteration 203/1000 | Loss: 0.00004068
Iteration 204/1000 | Loss: 0.00004068
Iteration 205/1000 | Loss: 0.00004067
Iteration 206/1000 | Loss: 0.00004067
Iteration 207/1000 | Loss: 0.00004067
Iteration 208/1000 | Loss: 0.00004067
Iteration 209/1000 | Loss: 0.00004067
Iteration 210/1000 | Loss: 0.00004066
Iteration 211/1000 | Loss: 0.00004066
Iteration 212/1000 | Loss: 0.00004066
Iteration 213/1000 | Loss: 0.00004066
Iteration 214/1000 | Loss: 0.00004065
Iteration 215/1000 | Loss: 0.00004065
Iteration 216/1000 | Loss: 0.00004065
Iteration 217/1000 | Loss: 0.00004065
Iteration 218/1000 | Loss: 0.00004064
Iteration 219/1000 | Loss: 0.00004064
Iteration 220/1000 | Loss: 0.00004064
Iteration 221/1000 | Loss: 0.00004064
Iteration 222/1000 | Loss: 0.00004064
Iteration 223/1000 | Loss: 0.00004063
Iteration 224/1000 | Loss: 0.00004063
Iteration 225/1000 | Loss: 0.00004063
Iteration 226/1000 | Loss: 0.00004063
Iteration 227/1000 | Loss: 0.00004063
Iteration 228/1000 | Loss: 0.00004063
Iteration 229/1000 | Loss: 0.00004062
Iteration 230/1000 | Loss: 0.00004062
Iteration 231/1000 | Loss: 0.00004062
Iteration 232/1000 | Loss: 0.00004062
Iteration 233/1000 | Loss: 0.00004062
Iteration 234/1000 | Loss: 0.00004062
Iteration 235/1000 | Loss: 0.00004061
Iteration 236/1000 | Loss: 0.00004061
Iteration 237/1000 | Loss: 0.00004061
Iteration 238/1000 | Loss: 0.00004061
Iteration 239/1000 | Loss: 0.00004061
Iteration 240/1000 | Loss: 0.00004060
Iteration 241/1000 | Loss: 0.00004060
Iteration 242/1000 | Loss: 0.00004060
Iteration 243/1000 | Loss: 0.00004060
Iteration 244/1000 | Loss: 0.00004060
Iteration 245/1000 | Loss: 0.00004060
Iteration 246/1000 | Loss: 0.00004060
Iteration 247/1000 | Loss: 0.00004060
Iteration 248/1000 | Loss: 0.00004060
Iteration 249/1000 | Loss: 0.00004060
Iteration 250/1000 | Loss: 0.00004060
Iteration 251/1000 | Loss: 0.00004059
Iteration 252/1000 | Loss: 0.00004059
Iteration 253/1000 | Loss: 0.00004059
Iteration 254/1000 | Loss: 0.00004059
Iteration 255/1000 | Loss: 0.00004059
Iteration 256/1000 | Loss: 0.00004059
Iteration 257/1000 | Loss: 0.00004058
Iteration 258/1000 | Loss: 0.00004058
Iteration 259/1000 | Loss: 0.00004058
Iteration 260/1000 | Loss: 0.00004058
Iteration 261/1000 | Loss: 0.00004058
Iteration 262/1000 | Loss: 0.00004058
Iteration 263/1000 | Loss: 0.00004058
Iteration 264/1000 | Loss: 0.00004058
Iteration 265/1000 | Loss: 0.00004058
Iteration 266/1000 | Loss: 0.00004058
Iteration 267/1000 | Loss: 0.00004058
Iteration 268/1000 | Loss: 0.00004057
Iteration 269/1000 | Loss: 0.00004057
Iteration 270/1000 | Loss: 0.00004057
Iteration 271/1000 | Loss: 0.00004057
Iteration 272/1000 | Loss: 0.00004057
Iteration 273/1000 | Loss: 0.00004057
Iteration 274/1000 | Loss: 0.00004057
Iteration 275/1000 | Loss: 0.00004057
Iteration 276/1000 | Loss: 0.00004057
Iteration 277/1000 | Loss: 0.00004057
Iteration 278/1000 | Loss: 0.00004057
Iteration 279/1000 | Loss: 0.00004057
Iteration 280/1000 | Loss: 0.00004057
Iteration 281/1000 | Loss: 0.00004057
Iteration 282/1000 | Loss: 0.00004057
Iteration 283/1000 | Loss: 0.00004057
Iteration 284/1000 | Loss: 0.00004056
Iteration 285/1000 | Loss: 0.00004056
Iteration 286/1000 | Loss: 0.00004056
Iteration 287/1000 | Loss: 0.00004056
Iteration 288/1000 | Loss: 0.00004056
Iteration 289/1000 | Loss: 0.00004056
Iteration 290/1000 | Loss: 0.00004056
Iteration 291/1000 | Loss: 0.00004056
Iteration 292/1000 | Loss: 0.00004056
Iteration 293/1000 | Loss: 0.00004056
Iteration 294/1000 | Loss: 0.00004056
Iteration 295/1000 | Loss: 0.00004056
Iteration 296/1000 | Loss: 0.00004056
Iteration 297/1000 | Loss: 0.00004056
Iteration 298/1000 | Loss: 0.00004056
Iteration 299/1000 | Loss: 0.00004056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [4.0563943912275136e-05, 4.0563943912275136e-05, 4.0563943912275136e-05, 4.0563943912275136e-05, 4.0563943912275136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0563943912275136e-05

Optimization complete. Final v2v error: 5.261534690856934 mm

Highest mean error: 11.376038551330566 mm for frame 2

Lowest mean error: 4.595810413360596 mm for frame 19

Saving results

Total time: 198.0174651145935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_1807/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_1807/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016674
Iteration 2/25 | Loss: 0.01016674
Iteration 3/25 | Loss: 0.01016674
Iteration 4/25 | Loss: 0.01016674
Iteration 5/25 | Loss: 0.01016674
Iteration 6/25 | Loss: 0.01016674
Iteration 7/25 | Loss: 0.01016673
Iteration 8/25 | Loss: 0.01016673
Iteration 9/25 | Loss: 0.01016673
Iteration 10/25 | Loss: 0.01016673
Iteration 11/25 | Loss: 0.01016672
Iteration 12/25 | Loss: 0.01016672
Iteration 13/25 | Loss: 0.01016672
Iteration 14/25 | Loss: 0.01016672
Iteration 15/25 | Loss: 0.01016671
Iteration 16/25 | Loss: 0.01016671
Iteration 17/25 | Loss: 0.01016671
Iteration 18/25 | Loss: 0.01016671
Iteration 19/25 | Loss: 0.01016671
Iteration 20/25 | Loss: 0.01016670
Iteration 21/25 | Loss: 0.01016670
Iteration 22/25 | Loss: 0.01016670
Iteration 23/25 | Loss: 0.01016670
Iteration 24/25 | Loss: 0.01016670
Iteration 25/25 | Loss: 0.01016670

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54155827
Iteration 2/25 | Loss: 0.18901983
Iteration 3/25 | Loss: 0.18896581
Iteration 4/25 | Loss: 0.18896578
Iteration 5/25 | Loss: 0.18896578
Iteration 6/25 | Loss: 0.18896574
Iteration 7/25 | Loss: 0.18896574
Iteration 8/25 | Loss: 0.18896574
Iteration 9/25 | Loss: 0.18896574
Iteration 10/25 | Loss: 0.18896574
Iteration 11/25 | Loss: 0.18896574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.18896573781967163, 0.18896573781967163, 0.18896573781967163, 0.18896573781967163, 0.18896573781967163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18896573781967163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18896574
Iteration 2/1000 | Loss: 0.00399040
Iteration 3/1000 | Loss: 0.00225430
Iteration 4/1000 | Loss: 0.00150382
Iteration 5/1000 | Loss: 0.00044830
Iteration 6/1000 | Loss: 0.00025589
Iteration 7/1000 | Loss: 0.00014219
Iteration 8/1000 | Loss: 0.00014940
Iteration 9/1000 | Loss: 0.00008550
Iteration 10/1000 | Loss: 0.00005386
Iteration 11/1000 | Loss: 0.00004275
Iteration 12/1000 | Loss: 0.00003528
Iteration 13/1000 | Loss: 0.00003040
Iteration 14/1000 | Loss: 0.00002629
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00001985
Iteration 18/1000 | Loss: 0.00001827
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001404
Iteration 26/1000 | Loss: 0.00001381
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001283
Iteration 89/1000 | Loss: 0.00001283
Iteration 90/1000 | Loss: 0.00001283
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.2820001757063437e-05, 1.2820001757063437e-05, 1.2820001757063437e-05, 1.2820001757063437e-05, 1.2820001757063437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2820001757063437e-05

Optimization complete. Final v2v error: 2.9680731296539307 mm

Highest mean error: 3.1792705059051514 mm for frame 239

Lowest mean error: 2.806743860244751 mm for frame 0

Saving results

Total time: 62.08634305000305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452816
Iteration 2/25 | Loss: 0.00118587
Iteration 3/25 | Loss: 0.00098564
Iteration 4/25 | Loss: 0.00096736
Iteration 5/25 | Loss: 0.00096498
Iteration 6/25 | Loss: 0.00096463
Iteration 7/25 | Loss: 0.00096463
Iteration 8/25 | Loss: 0.00096463
Iteration 9/25 | Loss: 0.00096463
Iteration 10/25 | Loss: 0.00096463
Iteration 11/25 | Loss: 0.00096463
Iteration 12/25 | Loss: 0.00096463
Iteration 13/25 | Loss: 0.00096463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009646288817748427, 0.0009646288817748427, 0.0009646288817748427, 0.0009646288817748427, 0.0009646288817748427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009646288817748427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22099185
Iteration 2/25 | Loss: 0.00097008
Iteration 3/25 | Loss: 0.00097008
Iteration 4/25 | Loss: 0.00097008
Iteration 5/25 | Loss: 0.00097007
Iteration 6/25 | Loss: 0.00097007
Iteration 7/25 | Loss: 0.00097007
Iteration 8/25 | Loss: 0.00097007
Iteration 9/25 | Loss: 0.00097007
Iteration 10/25 | Loss: 0.00097007
Iteration 11/25 | Loss: 0.00097007
Iteration 12/25 | Loss: 0.00097007
Iteration 13/25 | Loss: 0.00097007
Iteration 14/25 | Loss: 0.00097007
Iteration 15/25 | Loss: 0.00097007
Iteration 16/25 | Loss: 0.00097007
Iteration 17/25 | Loss: 0.00097007
Iteration 18/25 | Loss: 0.00097007
Iteration 19/25 | Loss: 0.00097007
Iteration 20/25 | Loss: 0.00097007
Iteration 21/25 | Loss: 0.00097007
Iteration 22/25 | Loss: 0.00097007
Iteration 23/25 | Loss: 0.00097007
Iteration 24/25 | Loss: 0.00097007
Iteration 25/25 | Loss: 0.00097007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097007
Iteration 2/1000 | Loss: 0.00004123
Iteration 3/1000 | Loss: 0.00002626
Iteration 4/1000 | Loss: 0.00001782
Iteration 5/1000 | Loss: 0.00001558
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001267
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001208
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00001207
Iteration 46/1000 | Loss: 0.00001207
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001184
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.176771638711216e-05, 1.176771638711216e-05, 1.176771638711216e-05, 1.176771638711216e-05, 1.176771638711216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.176771638711216e-05

Optimization complete. Final v2v error: 2.938814640045166 mm

Highest mean error: 3.1365983486175537 mm for frame 0

Lowest mean error: 2.782758951187134 mm for frame 164

Saving results

Total time: 42.12266492843628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403557
Iteration 2/25 | Loss: 0.00115881
Iteration 3/25 | Loss: 0.00098377
Iteration 4/25 | Loss: 0.00095843
Iteration 5/25 | Loss: 0.00095254
Iteration 6/25 | Loss: 0.00095117
Iteration 7/25 | Loss: 0.00095107
Iteration 8/25 | Loss: 0.00095107
Iteration 9/25 | Loss: 0.00095107
Iteration 10/25 | Loss: 0.00095107
Iteration 11/25 | Loss: 0.00095107
Iteration 12/25 | Loss: 0.00095107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009510737145319581, 0.0009510737145319581, 0.0009510737145319581, 0.0009510737145319581, 0.0009510737145319581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009510737145319581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25809419
Iteration 2/25 | Loss: 0.00124603
Iteration 3/25 | Loss: 0.00124601
Iteration 4/25 | Loss: 0.00124601
Iteration 5/25 | Loss: 0.00124601
Iteration 6/25 | Loss: 0.00124601
Iteration 7/25 | Loss: 0.00124601
Iteration 8/25 | Loss: 0.00124600
Iteration 9/25 | Loss: 0.00124600
Iteration 10/25 | Loss: 0.00124600
Iteration 11/25 | Loss: 0.00124600
Iteration 12/25 | Loss: 0.00124600
Iteration 13/25 | Loss: 0.00124600
Iteration 14/25 | Loss: 0.00124600
Iteration 15/25 | Loss: 0.00124600
Iteration 16/25 | Loss: 0.00124600
Iteration 17/25 | Loss: 0.00124600
Iteration 18/25 | Loss: 0.00124600
Iteration 19/25 | Loss: 0.00124600
Iteration 20/25 | Loss: 0.00124600
Iteration 21/25 | Loss: 0.00124600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001246004132553935, 0.001246004132553935, 0.001246004132553935, 0.001246004132553935, 0.001246004132553935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001246004132553935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124600
Iteration 2/1000 | Loss: 0.00004773
Iteration 3/1000 | Loss: 0.00003043
Iteration 4/1000 | Loss: 0.00002447
Iteration 5/1000 | Loss: 0.00002148
Iteration 6/1000 | Loss: 0.00001968
Iteration 7/1000 | Loss: 0.00001830
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001703
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001641
Iteration 13/1000 | Loss: 0.00001624
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001581
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001580
Iteration 22/1000 | Loss: 0.00001577
Iteration 23/1000 | Loss: 0.00001577
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001573
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001559
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001546
Iteration 93/1000 | Loss: 0.00001546
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001544
Iteration 96/1000 | Loss: 0.00001544
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001538
Iteration 119/1000 | Loss: 0.00001538
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001537
Iteration 122/1000 | Loss: 0.00001537
Iteration 123/1000 | Loss: 0.00001537
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001535
Iteration 128/1000 | Loss: 0.00001535
Iteration 129/1000 | Loss: 0.00001535
Iteration 130/1000 | Loss: 0.00001535
Iteration 131/1000 | Loss: 0.00001535
Iteration 132/1000 | Loss: 0.00001535
Iteration 133/1000 | Loss: 0.00001535
Iteration 134/1000 | Loss: 0.00001534
Iteration 135/1000 | Loss: 0.00001534
Iteration 136/1000 | Loss: 0.00001534
Iteration 137/1000 | Loss: 0.00001534
Iteration 138/1000 | Loss: 0.00001534
Iteration 139/1000 | Loss: 0.00001534
Iteration 140/1000 | Loss: 0.00001534
Iteration 141/1000 | Loss: 0.00001534
Iteration 142/1000 | Loss: 0.00001534
Iteration 143/1000 | Loss: 0.00001534
Iteration 144/1000 | Loss: 0.00001534
Iteration 145/1000 | Loss: 0.00001534
Iteration 146/1000 | Loss: 0.00001534
Iteration 147/1000 | Loss: 0.00001534
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001533
Iteration 150/1000 | Loss: 0.00001533
Iteration 151/1000 | Loss: 0.00001533
Iteration 152/1000 | Loss: 0.00001533
Iteration 153/1000 | Loss: 0.00001533
Iteration 154/1000 | Loss: 0.00001533
Iteration 155/1000 | Loss: 0.00001533
Iteration 156/1000 | Loss: 0.00001533
Iteration 157/1000 | Loss: 0.00001533
Iteration 158/1000 | Loss: 0.00001532
Iteration 159/1000 | Loss: 0.00001532
Iteration 160/1000 | Loss: 0.00001532
Iteration 161/1000 | Loss: 0.00001532
Iteration 162/1000 | Loss: 0.00001532
Iteration 163/1000 | Loss: 0.00001532
Iteration 164/1000 | Loss: 0.00001531
Iteration 165/1000 | Loss: 0.00001531
Iteration 166/1000 | Loss: 0.00001531
Iteration 167/1000 | Loss: 0.00001531
Iteration 168/1000 | Loss: 0.00001531
Iteration 169/1000 | Loss: 0.00001531
Iteration 170/1000 | Loss: 0.00001531
Iteration 171/1000 | Loss: 0.00001530
Iteration 172/1000 | Loss: 0.00001530
Iteration 173/1000 | Loss: 0.00001530
Iteration 174/1000 | Loss: 0.00001530
Iteration 175/1000 | Loss: 0.00001530
Iteration 176/1000 | Loss: 0.00001530
Iteration 177/1000 | Loss: 0.00001530
Iteration 178/1000 | Loss: 0.00001530
Iteration 179/1000 | Loss: 0.00001529
Iteration 180/1000 | Loss: 0.00001529
Iteration 181/1000 | Loss: 0.00001529
Iteration 182/1000 | Loss: 0.00001529
Iteration 183/1000 | Loss: 0.00001529
Iteration 184/1000 | Loss: 0.00001529
Iteration 185/1000 | Loss: 0.00001529
Iteration 186/1000 | Loss: 0.00001529
Iteration 187/1000 | Loss: 0.00001528
Iteration 188/1000 | Loss: 0.00001528
Iteration 189/1000 | Loss: 0.00001528
Iteration 190/1000 | Loss: 0.00001528
Iteration 191/1000 | Loss: 0.00001528
Iteration 192/1000 | Loss: 0.00001528
Iteration 193/1000 | Loss: 0.00001528
Iteration 194/1000 | Loss: 0.00001528
Iteration 195/1000 | Loss: 0.00001528
Iteration 196/1000 | Loss: 0.00001528
Iteration 197/1000 | Loss: 0.00001528
Iteration 198/1000 | Loss: 0.00001527
Iteration 199/1000 | Loss: 0.00001527
Iteration 200/1000 | Loss: 0.00001527
Iteration 201/1000 | Loss: 0.00001527
Iteration 202/1000 | Loss: 0.00001527
Iteration 203/1000 | Loss: 0.00001527
Iteration 204/1000 | Loss: 0.00001526
Iteration 205/1000 | Loss: 0.00001526
Iteration 206/1000 | Loss: 0.00001526
Iteration 207/1000 | Loss: 0.00001526
Iteration 208/1000 | Loss: 0.00001526
Iteration 209/1000 | Loss: 0.00001526
Iteration 210/1000 | Loss: 0.00001526
Iteration 211/1000 | Loss: 0.00001525
Iteration 212/1000 | Loss: 0.00001525
Iteration 213/1000 | Loss: 0.00001525
Iteration 214/1000 | Loss: 0.00001525
Iteration 215/1000 | Loss: 0.00001525
Iteration 216/1000 | Loss: 0.00001525
Iteration 217/1000 | Loss: 0.00001525
Iteration 218/1000 | Loss: 0.00001525
Iteration 219/1000 | Loss: 0.00001525
Iteration 220/1000 | Loss: 0.00001525
Iteration 221/1000 | Loss: 0.00001525
Iteration 222/1000 | Loss: 0.00001525
Iteration 223/1000 | Loss: 0.00001525
Iteration 224/1000 | Loss: 0.00001525
Iteration 225/1000 | Loss: 0.00001525
Iteration 226/1000 | Loss: 0.00001525
Iteration 227/1000 | Loss: 0.00001525
Iteration 228/1000 | Loss: 0.00001525
Iteration 229/1000 | Loss: 0.00001525
Iteration 230/1000 | Loss: 0.00001525
Iteration 231/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.524916569906054e-05, 1.524916569906054e-05, 1.524916569906054e-05, 1.524916569906054e-05, 1.524916569906054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.524916569906054e-05

Optimization complete. Final v2v error: 3.193864107131958 mm

Highest mean error: 4.869706153869629 mm for frame 87

Lowest mean error: 2.504039764404297 mm for frame 148

Saving results

Total time: 48.38142991065979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505528
Iteration 2/25 | Loss: 0.00117668
Iteration 3/25 | Loss: 0.00102697
Iteration 4/25 | Loss: 0.00100502
Iteration 5/25 | Loss: 0.00099803
Iteration 6/25 | Loss: 0.00099615
Iteration 7/25 | Loss: 0.00099564
Iteration 8/25 | Loss: 0.00099564
Iteration 9/25 | Loss: 0.00099564
Iteration 10/25 | Loss: 0.00099564
Iteration 11/25 | Loss: 0.00099564
Iteration 12/25 | Loss: 0.00099564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009956408757716417, 0.0009956408757716417, 0.0009956408757716417, 0.0009956408757716417, 0.0009956408757716417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009956408757716417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28213191
Iteration 2/25 | Loss: 0.00158800
Iteration 3/25 | Loss: 0.00158800
Iteration 4/25 | Loss: 0.00158800
Iteration 5/25 | Loss: 0.00158800
Iteration 6/25 | Loss: 0.00158800
Iteration 7/25 | Loss: 0.00158800
Iteration 8/25 | Loss: 0.00158799
Iteration 9/25 | Loss: 0.00158799
Iteration 10/25 | Loss: 0.00158799
Iteration 11/25 | Loss: 0.00158799
Iteration 12/25 | Loss: 0.00158799
Iteration 13/25 | Loss: 0.00158799
Iteration 14/25 | Loss: 0.00158799
Iteration 15/25 | Loss: 0.00158799
Iteration 16/25 | Loss: 0.00158799
Iteration 17/25 | Loss: 0.00158799
Iteration 18/25 | Loss: 0.00158799
Iteration 19/25 | Loss: 0.00158799
Iteration 20/25 | Loss: 0.00158799
Iteration 21/25 | Loss: 0.00158799
Iteration 22/25 | Loss: 0.00158799
Iteration 23/25 | Loss: 0.00158799
Iteration 24/25 | Loss: 0.00158799
Iteration 25/25 | Loss: 0.00158799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158799
Iteration 2/1000 | Loss: 0.00005733
Iteration 3/1000 | Loss: 0.00004014
Iteration 4/1000 | Loss: 0.00003014
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002506
Iteration 7/1000 | Loss: 0.00002335
Iteration 8/1000 | Loss: 0.00002237
Iteration 9/1000 | Loss: 0.00002191
Iteration 10/1000 | Loss: 0.00002155
Iteration 11/1000 | Loss: 0.00002137
Iteration 12/1000 | Loss: 0.00002116
Iteration 13/1000 | Loss: 0.00002112
Iteration 14/1000 | Loss: 0.00002110
Iteration 15/1000 | Loss: 0.00002091
Iteration 16/1000 | Loss: 0.00002085
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00002078
Iteration 20/1000 | Loss: 0.00002077
Iteration 21/1000 | Loss: 0.00002074
Iteration 22/1000 | Loss: 0.00002074
Iteration 23/1000 | Loss: 0.00002073
Iteration 24/1000 | Loss: 0.00002068
Iteration 25/1000 | Loss: 0.00002066
Iteration 26/1000 | Loss: 0.00002062
Iteration 27/1000 | Loss: 0.00002062
Iteration 28/1000 | Loss: 0.00002062
Iteration 29/1000 | Loss: 0.00002061
Iteration 30/1000 | Loss: 0.00002060
Iteration 31/1000 | Loss: 0.00002059
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00002059
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002055
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00002054
Iteration 48/1000 | Loss: 0.00002053
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002053
Iteration 51/1000 | Loss: 0.00002052
Iteration 52/1000 | Loss: 0.00002052
Iteration 53/1000 | Loss: 0.00002052
Iteration 54/1000 | Loss: 0.00002050
Iteration 55/1000 | Loss: 0.00002050
Iteration 56/1000 | Loss: 0.00002050
Iteration 57/1000 | Loss: 0.00002050
Iteration 58/1000 | Loss: 0.00002050
Iteration 59/1000 | Loss: 0.00002049
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002048
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002047
Iteration 66/1000 | Loss: 0.00002047
Iteration 67/1000 | Loss: 0.00002047
Iteration 68/1000 | Loss: 0.00002047
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002046
Iteration 71/1000 | Loss: 0.00002046
Iteration 72/1000 | Loss: 0.00002046
Iteration 73/1000 | Loss: 0.00002046
Iteration 74/1000 | Loss: 0.00002046
Iteration 75/1000 | Loss: 0.00002045
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002043
Iteration 81/1000 | Loss: 0.00002043
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002042
Iteration 84/1000 | Loss: 0.00002042
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002041
Iteration 90/1000 | Loss: 0.00002040
Iteration 91/1000 | Loss: 0.00002039
Iteration 92/1000 | Loss: 0.00002039
Iteration 93/1000 | Loss: 0.00002038
Iteration 94/1000 | Loss: 0.00002038
Iteration 95/1000 | Loss: 0.00002037
Iteration 96/1000 | Loss: 0.00002037
Iteration 97/1000 | Loss: 0.00002037
Iteration 98/1000 | Loss: 0.00002037
Iteration 99/1000 | Loss: 0.00002037
Iteration 100/1000 | Loss: 0.00002037
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002036
Iteration 103/1000 | Loss: 0.00002035
Iteration 104/1000 | Loss: 0.00002035
Iteration 105/1000 | Loss: 0.00002035
Iteration 106/1000 | Loss: 0.00002034
Iteration 107/1000 | Loss: 0.00002034
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002031
Iteration 116/1000 | Loss: 0.00002031
Iteration 117/1000 | Loss: 0.00002031
Iteration 118/1000 | Loss: 0.00002030
Iteration 119/1000 | Loss: 0.00002030
Iteration 120/1000 | Loss: 0.00002030
Iteration 121/1000 | Loss: 0.00002030
Iteration 122/1000 | Loss: 0.00002030
Iteration 123/1000 | Loss: 0.00002030
Iteration 124/1000 | Loss: 0.00002029
Iteration 125/1000 | Loss: 0.00002029
Iteration 126/1000 | Loss: 0.00002029
Iteration 127/1000 | Loss: 0.00002029
Iteration 128/1000 | Loss: 0.00002029
Iteration 129/1000 | Loss: 0.00002029
Iteration 130/1000 | Loss: 0.00002028
Iteration 131/1000 | Loss: 0.00002028
Iteration 132/1000 | Loss: 0.00002028
Iteration 133/1000 | Loss: 0.00002028
Iteration 134/1000 | Loss: 0.00002028
Iteration 135/1000 | Loss: 0.00002027
Iteration 136/1000 | Loss: 0.00002027
Iteration 137/1000 | Loss: 0.00002027
Iteration 138/1000 | Loss: 0.00002027
Iteration 139/1000 | Loss: 0.00002027
Iteration 140/1000 | Loss: 0.00002027
Iteration 141/1000 | Loss: 0.00002027
Iteration 142/1000 | Loss: 0.00002027
Iteration 143/1000 | Loss: 0.00002026
Iteration 144/1000 | Loss: 0.00002026
Iteration 145/1000 | Loss: 0.00002025
Iteration 146/1000 | Loss: 0.00002025
Iteration 147/1000 | Loss: 0.00002025
Iteration 148/1000 | Loss: 0.00002025
Iteration 149/1000 | Loss: 0.00002025
Iteration 150/1000 | Loss: 0.00002025
Iteration 151/1000 | Loss: 0.00002025
Iteration 152/1000 | Loss: 0.00002025
Iteration 153/1000 | Loss: 0.00002024
Iteration 154/1000 | Loss: 0.00002024
Iteration 155/1000 | Loss: 0.00002024
Iteration 156/1000 | Loss: 0.00002024
Iteration 157/1000 | Loss: 0.00002024
Iteration 158/1000 | Loss: 0.00002024
Iteration 159/1000 | Loss: 0.00002024
Iteration 160/1000 | Loss: 0.00002024
Iteration 161/1000 | Loss: 0.00002024
Iteration 162/1000 | Loss: 0.00002024
Iteration 163/1000 | Loss: 0.00002023
Iteration 164/1000 | Loss: 0.00002023
Iteration 165/1000 | Loss: 0.00002023
Iteration 166/1000 | Loss: 0.00002023
Iteration 167/1000 | Loss: 0.00002023
Iteration 168/1000 | Loss: 0.00002022
Iteration 169/1000 | Loss: 0.00002022
Iteration 170/1000 | Loss: 0.00002022
Iteration 171/1000 | Loss: 0.00002022
Iteration 172/1000 | Loss: 0.00002021
Iteration 173/1000 | Loss: 0.00002021
Iteration 174/1000 | Loss: 0.00002021
Iteration 175/1000 | Loss: 0.00002021
Iteration 176/1000 | Loss: 0.00002021
Iteration 177/1000 | Loss: 0.00002021
Iteration 178/1000 | Loss: 0.00002021
Iteration 179/1000 | Loss: 0.00002021
Iteration 180/1000 | Loss: 0.00002021
Iteration 181/1000 | Loss: 0.00002020
Iteration 182/1000 | Loss: 0.00002020
Iteration 183/1000 | Loss: 0.00002020
Iteration 184/1000 | Loss: 0.00002020
Iteration 185/1000 | Loss: 0.00002020
Iteration 186/1000 | Loss: 0.00002020
Iteration 187/1000 | Loss: 0.00002020
Iteration 188/1000 | Loss: 0.00002020
Iteration 189/1000 | Loss: 0.00002020
Iteration 190/1000 | Loss: 0.00002019
Iteration 191/1000 | Loss: 0.00002019
Iteration 192/1000 | Loss: 0.00002019
Iteration 193/1000 | Loss: 0.00002019
Iteration 194/1000 | Loss: 0.00002019
Iteration 195/1000 | Loss: 0.00002019
Iteration 196/1000 | Loss: 0.00002019
Iteration 197/1000 | Loss: 0.00002019
Iteration 198/1000 | Loss: 0.00002019
Iteration 199/1000 | Loss: 0.00002019
Iteration 200/1000 | Loss: 0.00002019
Iteration 201/1000 | Loss: 0.00002019
Iteration 202/1000 | Loss: 0.00002019
Iteration 203/1000 | Loss: 0.00002019
Iteration 204/1000 | Loss: 0.00002019
Iteration 205/1000 | Loss: 0.00002018
Iteration 206/1000 | Loss: 0.00002018
Iteration 207/1000 | Loss: 0.00002018
Iteration 208/1000 | Loss: 0.00002018
Iteration 209/1000 | Loss: 0.00002018
Iteration 210/1000 | Loss: 0.00002018
Iteration 211/1000 | Loss: 0.00002018
Iteration 212/1000 | Loss: 0.00002018
Iteration 213/1000 | Loss: 0.00002018
Iteration 214/1000 | Loss: 0.00002018
Iteration 215/1000 | Loss: 0.00002018
Iteration 216/1000 | Loss: 0.00002018
Iteration 217/1000 | Loss: 0.00002018
Iteration 218/1000 | Loss: 0.00002018
Iteration 219/1000 | Loss: 0.00002018
Iteration 220/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.017950828303583e-05, 2.017950828303583e-05, 2.017950828303583e-05, 2.017950828303583e-05, 2.017950828303583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.017950828303583e-05

Optimization complete. Final v2v error: 3.7605321407318115 mm

Highest mean error: 4.791445732116699 mm for frame 93

Lowest mean error: 2.9118802547454834 mm for frame 49

Saving results

Total time: 48.760425090789795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821591
Iteration 2/25 | Loss: 0.00145479
Iteration 3/25 | Loss: 0.00102675
Iteration 4/25 | Loss: 0.00097321
Iteration 5/25 | Loss: 0.00095898
Iteration 6/25 | Loss: 0.00095518
Iteration 7/25 | Loss: 0.00095391
Iteration 8/25 | Loss: 0.00095352
Iteration 9/25 | Loss: 0.00095344
Iteration 10/25 | Loss: 0.00095344
Iteration 11/25 | Loss: 0.00095344
Iteration 12/25 | Loss: 0.00095344
Iteration 13/25 | Loss: 0.00095344
Iteration 14/25 | Loss: 0.00095344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009534404380246997, 0.0009534404380246997, 0.0009534404380246997, 0.0009534404380246997, 0.0009534404380246997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009534404380246997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25001228
Iteration 2/25 | Loss: 0.00100310
Iteration 3/25 | Loss: 0.00100310
Iteration 4/25 | Loss: 0.00100310
Iteration 5/25 | Loss: 0.00100310
Iteration 6/25 | Loss: 0.00100309
Iteration 7/25 | Loss: 0.00100309
Iteration 8/25 | Loss: 0.00100309
Iteration 9/25 | Loss: 0.00100309
Iteration 10/25 | Loss: 0.00100309
Iteration 11/25 | Loss: 0.00100309
Iteration 12/25 | Loss: 0.00100309
Iteration 13/25 | Loss: 0.00100309
Iteration 14/25 | Loss: 0.00100309
Iteration 15/25 | Loss: 0.00100309
Iteration 16/25 | Loss: 0.00100309
Iteration 17/25 | Loss: 0.00100309
Iteration 18/25 | Loss: 0.00100309
Iteration 19/25 | Loss: 0.00100309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001003093784675002, 0.001003093784675002, 0.001003093784675002, 0.001003093784675002, 0.001003093784675002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001003093784675002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100309
Iteration 2/1000 | Loss: 0.00004053
Iteration 3/1000 | Loss: 0.00002926
Iteration 4/1000 | Loss: 0.00002496
Iteration 5/1000 | Loss: 0.00002288
Iteration 6/1000 | Loss: 0.00002171
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002047
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001948
Iteration 12/1000 | Loss: 0.00001941
Iteration 13/1000 | Loss: 0.00001939
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001933
Iteration 19/1000 | Loss: 0.00001933
Iteration 20/1000 | Loss: 0.00001932
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001932
Iteration 23/1000 | Loss: 0.00001931
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001925
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001922
Iteration 36/1000 | Loss: 0.00001922
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001922
Iteration 41/1000 | Loss: 0.00001922
Iteration 42/1000 | Loss: 0.00001922
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001922
Iteration 53/1000 | Loss: 0.00001922
Iteration 54/1000 | Loss: 0.00001922
Iteration 55/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.922234514495358e-05, 1.922234514495358e-05, 1.922234514495358e-05, 1.922234514495358e-05, 1.922234514495358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.922234514495358e-05

Optimization complete. Final v2v error: 3.6378109455108643 mm

Highest mean error: 4.243668556213379 mm for frame 104

Lowest mean error: 2.9024746417999268 mm for frame 0

Saving results

Total time: 37.992579221725464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404534
Iteration 2/25 | Loss: 0.00121619
Iteration 3/25 | Loss: 0.00100526
Iteration 4/25 | Loss: 0.00098794
Iteration 5/25 | Loss: 0.00098589
Iteration 6/25 | Loss: 0.00098553
Iteration 7/25 | Loss: 0.00098553
Iteration 8/25 | Loss: 0.00098553
Iteration 9/25 | Loss: 0.00098553
Iteration 10/25 | Loss: 0.00098553
Iteration 11/25 | Loss: 0.00098553
Iteration 12/25 | Loss: 0.00098553
Iteration 13/25 | Loss: 0.00098553
Iteration 14/25 | Loss: 0.00098553
Iteration 15/25 | Loss: 0.00098553
Iteration 16/25 | Loss: 0.00098553
Iteration 17/25 | Loss: 0.00098553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009855309035629034, 0.0009855309035629034, 0.0009855309035629034, 0.0009855309035629034, 0.0009855309035629034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009855309035629034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23953557
Iteration 2/25 | Loss: 0.00101550
Iteration 3/25 | Loss: 0.00101550
Iteration 4/25 | Loss: 0.00101550
Iteration 5/25 | Loss: 0.00101550
Iteration 6/25 | Loss: 0.00101550
Iteration 7/25 | Loss: 0.00101550
Iteration 8/25 | Loss: 0.00101550
Iteration 9/25 | Loss: 0.00101549
Iteration 10/25 | Loss: 0.00101549
Iteration 11/25 | Loss: 0.00101549
Iteration 12/25 | Loss: 0.00101549
Iteration 13/25 | Loss: 0.00101549
Iteration 14/25 | Loss: 0.00101549
Iteration 15/25 | Loss: 0.00101549
Iteration 16/25 | Loss: 0.00101549
Iteration 17/25 | Loss: 0.00101549
Iteration 18/25 | Loss: 0.00101549
Iteration 19/25 | Loss: 0.00101549
Iteration 20/25 | Loss: 0.00101549
Iteration 21/25 | Loss: 0.00101549
Iteration 22/25 | Loss: 0.00101549
Iteration 23/25 | Loss: 0.00101549
Iteration 24/25 | Loss: 0.00101549
Iteration 25/25 | Loss: 0.00101549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101549
Iteration 2/1000 | Loss: 0.00004466
Iteration 3/1000 | Loss: 0.00003068
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002286
Iteration 6/1000 | Loss: 0.00002140
Iteration 7/1000 | Loss: 0.00002034
Iteration 8/1000 | Loss: 0.00001981
Iteration 9/1000 | Loss: 0.00001949
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001895
Iteration 12/1000 | Loss: 0.00001869
Iteration 13/1000 | Loss: 0.00001858
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001850
Iteration 16/1000 | Loss: 0.00001849
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001833
Iteration 19/1000 | Loss: 0.00001832
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001812
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001796
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001794
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001793
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001786
Iteration 38/1000 | Loss: 0.00001786
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001782
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001781
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001780
Iteration 75/1000 | Loss: 0.00001780
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001779
Iteration 78/1000 | Loss: 0.00001779
Iteration 79/1000 | Loss: 0.00001779
Iteration 80/1000 | Loss: 0.00001779
Iteration 81/1000 | Loss: 0.00001779
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001777
Iteration 86/1000 | Loss: 0.00001777
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00001777
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001775
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001773
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001773
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001772
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001769
Iteration 118/1000 | Loss: 0.00001769
Iteration 119/1000 | Loss: 0.00001769
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001765
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001765
Iteration 142/1000 | Loss: 0.00001765
Iteration 143/1000 | Loss: 0.00001765
Iteration 144/1000 | Loss: 0.00001765
Iteration 145/1000 | Loss: 0.00001765
Iteration 146/1000 | Loss: 0.00001765
Iteration 147/1000 | Loss: 0.00001765
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001763
Iteration 151/1000 | Loss: 0.00001763
Iteration 152/1000 | Loss: 0.00001763
Iteration 153/1000 | Loss: 0.00001763
Iteration 154/1000 | Loss: 0.00001763
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001763
Iteration 157/1000 | Loss: 0.00001763
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001763
Iteration 165/1000 | Loss: 0.00001763
Iteration 166/1000 | Loss: 0.00001763
Iteration 167/1000 | Loss: 0.00001762
Iteration 168/1000 | Loss: 0.00001762
Iteration 169/1000 | Loss: 0.00001762
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00001761
Iteration 172/1000 | Loss: 0.00001761
Iteration 173/1000 | Loss: 0.00001761
Iteration 174/1000 | Loss: 0.00001761
Iteration 175/1000 | Loss: 0.00001760
Iteration 176/1000 | Loss: 0.00001760
Iteration 177/1000 | Loss: 0.00001760
Iteration 178/1000 | Loss: 0.00001760
Iteration 179/1000 | Loss: 0.00001760
Iteration 180/1000 | Loss: 0.00001759
Iteration 181/1000 | Loss: 0.00001759
Iteration 182/1000 | Loss: 0.00001759
Iteration 183/1000 | Loss: 0.00001759
Iteration 184/1000 | Loss: 0.00001759
Iteration 185/1000 | Loss: 0.00001759
Iteration 186/1000 | Loss: 0.00001759
Iteration 187/1000 | Loss: 0.00001758
Iteration 188/1000 | Loss: 0.00001758
Iteration 189/1000 | Loss: 0.00001758
Iteration 190/1000 | Loss: 0.00001758
Iteration 191/1000 | Loss: 0.00001758
Iteration 192/1000 | Loss: 0.00001758
Iteration 193/1000 | Loss: 0.00001758
Iteration 194/1000 | Loss: 0.00001758
Iteration 195/1000 | Loss: 0.00001758
Iteration 196/1000 | Loss: 0.00001758
Iteration 197/1000 | Loss: 0.00001758
Iteration 198/1000 | Loss: 0.00001758
Iteration 199/1000 | Loss: 0.00001757
Iteration 200/1000 | Loss: 0.00001757
Iteration 201/1000 | Loss: 0.00001757
Iteration 202/1000 | Loss: 0.00001757
Iteration 203/1000 | Loss: 0.00001757
Iteration 204/1000 | Loss: 0.00001757
Iteration 205/1000 | Loss: 0.00001757
Iteration 206/1000 | Loss: 0.00001757
Iteration 207/1000 | Loss: 0.00001757
Iteration 208/1000 | Loss: 0.00001757
Iteration 209/1000 | Loss: 0.00001757
Iteration 210/1000 | Loss: 0.00001757
Iteration 211/1000 | Loss: 0.00001757
Iteration 212/1000 | Loss: 0.00001756
Iteration 213/1000 | Loss: 0.00001756
Iteration 214/1000 | Loss: 0.00001756
Iteration 215/1000 | Loss: 0.00001756
Iteration 216/1000 | Loss: 0.00001756
Iteration 217/1000 | Loss: 0.00001756
Iteration 218/1000 | Loss: 0.00001756
Iteration 219/1000 | Loss: 0.00001755
Iteration 220/1000 | Loss: 0.00001755
Iteration 221/1000 | Loss: 0.00001755
Iteration 222/1000 | Loss: 0.00001755
Iteration 223/1000 | Loss: 0.00001755
Iteration 224/1000 | Loss: 0.00001755
Iteration 225/1000 | Loss: 0.00001755
Iteration 226/1000 | Loss: 0.00001755
Iteration 227/1000 | Loss: 0.00001755
Iteration 228/1000 | Loss: 0.00001755
Iteration 229/1000 | Loss: 0.00001755
Iteration 230/1000 | Loss: 0.00001755
Iteration 231/1000 | Loss: 0.00001755
Iteration 232/1000 | Loss: 0.00001755
Iteration 233/1000 | Loss: 0.00001755
Iteration 234/1000 | Loss: 0.00001754
Iteration 235/1000 | Loss: 0.00001754
Iteration 236/1000 | Loss: 0.00001754
Iteration 237/1000 | Loss: 0.00001754
Iteration 238/1000 | Loss: 0.00001754
Iteration 239/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.7544887668918818e-05, 1.7544887668918818e-05, 1.7544887668918818e-05, 1.7544887668918818e-05, 1.7544887668918818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7544887668918818e-05

Optimization complete. Final v2v error: 3.655986785888672 mm

Highest mean error: 3.881925582885742 mm for frame 22

Lowest mean error: 3.4201111793518066 mm for frame 134

Saving results

Total time: 47.91256046295166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919931
Iteration 2/25 | Loss: 0.00227456
Iteration 3/25 | Loss: 0.00121443
Iteration 4/25 | Loss: 0.00110144
Iteration 5/25 | Loss: 0.00109879
Iteration 6/25 | Loss: 0.00105955
Iteration 7/25 | Loss: 0.00102121
Iteration 8/25 | Loss: 0.00097171
Iteration 9/25 | Loss: 0.00097514
Iteration 10/25 | Loss: 0.00092420
Iteration 11/25 | Loss: 0.00091770
Iteration 12/25 | Loss: 0.00090504
Iteration 13/25 | Loss: 0.00090777
Iteration 14/25 | Loss: 0.00090243
Iteration 15/25 | Loss: 0.00090542
Iteration 16/25 | Loss: 0.00090111
Iteration 17/25 | Loss: 0.00090099
Iteration 18/25 | Loss: 0.00090089
Iteration 19/25 | Loss: 0.00090084
Iteration 20/25 | Loss: 0.00090084
Iteration 21/25 | Loss: 0.00090083
Iteration 22/25 | Loss: 0.00090083
Iteration 23/25 | Loss: 0.00090082
Iteration 24/25 | Loss: 0.00090082
Iteration 25/25 | Loss: 0.00090082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87493229
Iteration 2/25 | Loss: 0.00121683
Iteration 3/25 | Loss: 0.00118878
Iteration 4/25 | Loss: 0.00116160
Iteration 5/25 | Loss: 0.00116160
Iteration 6/25 | Loss: 0.00116160
Iteration 7/25 | Loss: 0.00116160
Iteration 8/25 | Loss: 0.00116160
Iteration 9/25 | Loss: 0.00116160
Iteration 10/25 | Loss: 0.00116160
Iteration 11/25 | Loss: 0.00116160
Iteration 12/25 | Loss: 0.00116160
Iteration 13/25 | Loss: 0.00116160
Iteration 14/25 | Loss: 0.00116160
Iteration 15/25 | Loss: 0.00116160
Iteration 16/25 | Loss: 0.00116160
Iteration 17/25 | Loss: 0.00116160
Iteration 18/25 | Loss: 0.00116160
Iteration 19/25 | Loss: 0.00116160
Iteration 20/25 | Loss: 0.00116160
Iteration 21/25 | Loss: 0.00116160
Iteration 22/25 | Loss: 0.00116160
Iteration 23/25 | Loss: 0.00116160
Iteration 24/25 | Loss: 0.00116160
Iteration 25/25 | Loss: 0.00116160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116160
Iteration 2/1000 | Loss: 0.00003554
Iteration 3/1000 | Loss: 0.00012630
Iteration 4/1000 | Loss: 0.00012852
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00019644
Iteration 8/1000 | Loss: 0.00001589
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00005088
Iteration 13/1000 | Loss: 0.00004756
Iteration 14/1000 | Loss: 0.00008935
Iteration 15/1000 | Loss: 0.00023365
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00002433
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00003627
Iteration 24/1000 | Loss: 0.00003776
Iteration 25/1000 | Loss: 0.00008545
Iteration 26/1000 | Loss: 0.00003186
Iteration 27/1000 | Loss: 0.00006095
Iteration 28/1000 | Loss: 0.00003980
Iteration 29/1000 | Loss: 0.00003517
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00003420
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001398
Iteration 39/1000 | Loss: 0.00001398
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00002596
Iteration 46/1000 | Loss: 0.00001394
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001393
Iteration 49/1000 | Loss: 0.00001393
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001391
Iteration 56/1000 | Loss: 0.00001391
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00004825
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00004935
Iteration 88/1000 | Loss: 0.00003190
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00006611
Iteration 91/1000 | Loss: 0.00001590
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00003113
Iteration 94/1000 | Loss: 0.00001402
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001386
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001385
Iteration 105/1000 | Loss: 0.00001385
Iteration 106/1000 | Loss: 0.00001384
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001383
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00004416
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001379
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001379
Iteration 142/1000 | Loss: 0.00001379
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Iteration 160/1000 | Loss: 0.00001378
Iteration 161/1000 | Loss: 0.00001378
Iteration 162/1000 | Loss: 0.00001378
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001378
Iteration 166/1000 | Loss: 0.00001378
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.3783962458546739e-05, 1.3783962458546739e-05, 1.3783962458546739e-05, 1.3783962458546739e-05, 1.3783962458546739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3783962458546739e-05

Optimization complete. Final v2v error: 3.1517317295074463 mm

Highest mean error: 8.383294105529785 mm for frame 24

Lowest mean error: 2.847982406616211 mm for frame 238

Saving results

Total time: 113.11337995529175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037901
Iteration 2/25 | Loss: 0.00249309
Iteration 3/25 | Loss: 0.00175781
Iteration 4/25 | Loss: 0.00161837
Iteration 5/25 | Loss: 0.00157879
Iteration 6/25 | Loss: 0.00148313
Iteration 7/25 | Loss: 0.00140156
Iteration 8/25 | Loss: 0.00139078
Iteration 9/25 | Loss: 0.00134364
Iteration 10/25 | Loss: 0.00128729
Iteration 11/25 | Loss: 0.00124134
Iteration 12/25 | Loss: 0.00119303
Iteration 13/25 | Loss: 0.00115413
Iteration 14/25 | Loss: 0.00114596
Iteration 15/25 | Loss: 0.00114279
Iteration 16/25 | Loss: 0.00113225
Iteration 17/25 | Loss: 0.00112267
Iteration 18/25 | Loss: 0.00110940
Iteration 19/25 | Loss: 0.00110588
Iteration 20/25 | Loss: 0.00110509
Iteration 21/25 | Loss: 0.00110493
Iteration 22/25 | Loss: 0.00110493
Iteration 23/25 | Loss: 0.00110493
Iteration 24/25 | Loss: 0.00110493
Iteration 25/25 | Loss: 0.00110493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22417068
Iteration 2/25 | Loss: 0.00253440
Iteration 3/25 | Loss: 0.00192558
Iteration 4/25 | Loss: 0.00192558
Iteration 5/25 | Loss: 0.00192558
Iteration 6/25 | Loss: 0.00192558
Iteration 7/25 | Loss: 0.00192558
Iteration 8/25 | Loss: 0.00192558
Iteration 9/25 | Loss: 0.00192558
Iteration 10/25 | Loss: 0.00192558
Iteration 11/25 | Loss: 0.00192558
Iteration 12/25 | Loss: 0.00192558
Iteration 13/25 | Loss: 0.00192558
Iteration 14/25 | Loss: 0.00192558
Iteration 15/25 | Loss: 0.00192558
Iteration 16/25 | Loss: 0.00192558
Iteration 17/25 | Loss: 0.00192558
Iteration 18/25 | Loss: 0.00192558
Iteration 19/25 | Loss: 0.00192558
Iteration 20/25 | Loss: 0.00192558
Iteration 21/25 | Loss: 0.00192558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019255756633356214, 0.0019255756633356214, 0.0019255756633356214, 0.0019255756633356214, 0.0019255756633356214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019255756633356214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192558
Iteration 2/1000 | Loss: 0.00052566
Iteration 3/1000 | Loss: 0.00023249
Iteration 4/1000 | Loss: 0.00037095
Iteration 5/1000 | Loss: 0.00013939
Iteration 6/1000 | Loss: 0.00006759
Iteration 7/1000 | Loss: 0.00005694
Iteration 8/1000 | Loss: 0.00005176
Iteration 9/1000 | Loss: 0.00004900
Iteration 10/1000 | Loss: 0.00004610
Iteration 11/1000 | Loss: 0.00004436
Iteration 12/1000 | Loss: 0.00004322
Iteration 13/1000 | Loss: 0.00004241
Iteration 14/1000 | Loss: 0.00004193
Iteration 15/1000 | Loss: 0.00004150
Iteration 16/1000 | Loss: 0.00004112
Iteration 17/1000 | Loss: 0.00004080
Iteration 18/1000 | Loss: 0.00004047
Iteration 19/1000 | Loss: 0.00004011
Iteration 20/1000 | Loss: 0.00003970
Iteration 21/1000 | Loss: 0.00003937
Iteration 22/1000 | Loss: 0.00003907
Iteration 23/1000 | Loss: 0.00003879
Iteration 24/1000 | Loss: 0.00003854
Iteration 25/1000 | Loss: 0.00003829
Iteration 26/1000 | Loss: 0.00007370
Iteration 27/1000 | Loss: 0.00073218
Iteration 28/1000 | Loss: 0.00042077
Iteration 29/1000 | Loss: 0.00036218
Iteration 30/1000 | Loss: 0.00040570
Iteration 31/1000 | Loss: 0.00047840
Iteration 32/1000 | Loss: 0.00010908
Iteration 33/1000 | Loss: 0.00005517
Iteration 34/1000 | Loss: 0.00030771
Iteration 35/1000 | Loss: 0.00005695
Iteration 36/1000 | Loss: 0.00004492
Iteration 37/1000 | Loss: 0.00026873
Iteration 38/1000 | Loss: 0.00006880
Iteration 39/1000 | Loss: 0.00012697
Iteration 40/1000 | Loss: 0.00004232
Iteration 41/1000 | Loss: 0.00017608
Iteration 42/1000 | Loss: 0.00004630
Iteration 43/1000 | Loss: 0.00003528
Iteration 44/1000 | Loss: 0.00003309
Iteration 45/1000 | Loss: 0.00003138
Iteration 46/1000 | Loss: 0.00003026
Iteration 47/1000 | Loss: 0.00002899
Iteration 48/1000 | Loss: 0.00002780
Iteration 49/1000 | Loss: 0.00002697
Iteration 50/1000 | Loss: 0.00002657
Iteration 51/1000 | Loss: 0.00002634
Iteration 52/1000 | Loss: 0.00002625
Iteration 53/1000 | Loss: 0.00002617
Iteration 54/1000 | Loss: 0.00002616
Iteration 55/1000 | Loss: 0.00002615
Iteration 56/1000 | Loss: 0.00002611
Iteration 57/1000 | Loss: 0.00002610
Iteration 58/1000 | Loss: 0.00002608
Iteration 59/1000 | Loss: 0.00002607
Iteration 60/1000 | Loss: 0.00002607
Iteration 61/1000 | Loss: 0.00002607
Iteration 62/1000 | Loss: 0.00002606
Iteration 63/1000 | Loss: 0.00002606
Iteration 64/1000 | Loss: 0.00002605
Iteration 65/1000 | Loss: 0.00002605
Iteration 66/1000 | Loss: 0.00002605
Iteration 67/1000 | Loss: 0.00002605
Iteration 68/1000 | Loss: 0.00002605
Iteration 69/1000 | Loss: 0.00002605
Iteration 70/1000 | Loss: 0.00002605
Iteration 71/1000 | Loss: 0.00002605
Iteration 72/1000 | Loss: 0.00002604
Iteration 73/1000 | Loss: 0.00002604
Iteration 74/1000 | Loss: 0.00002604
Iteration 75/1000 | Loss: 0.00002604
Iteration 76/1000 | Loss: 0.00002604
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002603
Iteration 79/1000 | Loss: 0.00002598
Iteration 80/1000 | Loss: 0.00002595
Iteration 81/1000 | Loss: 0.00002590
Iteration 82/1000 | Loss: 0.00002590
Iteration 83/1000 | Loss: 0.00002590
Iteration 84/1000 | Loss: 0.00002587
Iteration 85/1000 | Loss: 0.00002587
Iteration 86/1000 | Loss: 0.00002586
Iteration 87/1000 | Loss: 0.00002586
Iteration 88/1000 | Loss: 0.00002586
Iteration 89/1000 | Loss: 0.00002586
Iteration 90/1000 | Loss: 0.00002586
Iteration 91/1000 | Loss: 0.00002586
Iteration 92/1000 | Loss: 0.00002586
Iteration 93/1000 | Loss: 0.00002585
Iteration 94/1000 | Loss: 0.00002583
Iteration 95/1000 | Loss: 0.00002583
Iteration 96/1000 | Loss: 0.00002583
Iteration 97/1000 | Loss: 0.00002582
Iteration 98/1000 | Loss: 0.00002581
Iteration 99/1000 | Loss: 0.00002581
Iteration 100/1000 | Loss: 0.00002580
Iteration 101/1000 | Loss: 0.00002580
Iteration 102/1000 | Loss: 0.00002580
Iteration 103/1000 | Loss: 0.00002580
Iteration 104/1000 | Loss: 0.00002579
Iteration 105/1000 | Loss: 0.00002579
Iteration 106/1000 | Loss: 0.00002579
Iteration 107/1000 | Loss: 0.00002579
Iteration 108/1000 | Loss: 0.00002579
Iteration 109/1000 | Loss: 0.00002579
Iteration 110/1000 | Loss: 0.00002579
Iteration 111/1000 | Loss: 0.00002579
Iteration 112/1000 | Loss: 0.00002579
Iteration 113/1000 | Loss: 0.00002579
Iteration 114/1000 | Loss: 0.00002579
Iteration 115/1000 | Loss: 0.00002579
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002579
Iteration 118/1000 | Loss: 0.00002579
Iteration 119/1000 | Loss: 0.00002579
Iteration 120/1000 | Loss: 0.00002579
Iteration 121/1000 | Loss: 0.00002579
Iteration 122/1000 | Loss: 0.00002579
Iteration 123/1000 | Loss: 0.00002579
Iteration 124/1000 | Loss: 0.00002579
Iteration 125/1000 | Loss: 0.00002579
Iteration 126/1000 | Loss: 0.00002579
Iteration 127/1000 | Loss: 0.00002579
Iteration 128/1000 | Loss: 0.00002579
Iteration 129/1000 | Loss: 0.00002579
Iteration 130/1000 | Loss: 0.00002579
Iteration 131/1000 | Loss: 0.00002579
Iteration 132/1000 | Loss: 0.00002579
Iteration 133/1000 | Loss: 0.00002579
Iteration 134/1000 | Loss: 0.00002579
Iteration 135/1000 | Loss: 0.00002579
Iteration 136/1000 | Loss: 0.00002579
Iteration 137/1000 | Loss: 0.00002579
Iteration 138/1000 | Loss: 0.00002579
Iteration 139/1000 | Loss: 0.00002579
Iteration 140/1000 | Loss: 0.00002579
Iteration 141/1000 | Loss: 0.00002579
Iteration 142/1000 | Loss: 0.00002579
Iteration 143/1000 | Loss: 0.00002579
Iteration 144/1000 | Loss: 0.00002579
Iteration 145/1000 | Loss: 0.00002579
Iteration 146/1000 | Loss: 0.00002579
Iteration 147/1000 | Loss: 0.00002579
Iteration 148/1000 | Loss: 0.00002579
Iteration 149/1000 | Loss: 0.00002579
Iteration 150/1000 | Loss: 0.00002579
Iteration 151/1000 | Loss: 0.00002579
Iteration 152/1000 | Loss: 0.00002579
Iteration 153/1000 | Loss: 0.00002579
Iteration 154/1000 | Loss: 0.00002579
Iteration 155/1000 | Loss: 0.00002579
Iteration 156/1000 | Loss: 0.00002579
Iteration 157/1000 | Loss: 0.00002579
Iteration 158/1000 | Loss: 0.00002579
Iteration 159/1000 | Loss: 0.00002579
Iteration 160/1000 | Loss: 0.00002579
Iteration 161/1000 | Loss: 0.00002579
Iteration 162/1000 | Loss: 0.00002579
Iteration 163/1000 | Loss: 0.00002579
Iteration 164/1000 | Loss: 0.00002579
Iteration 165/1000 | Loss: 0.00002579
Iteration 166/1000 | Loss: 0.00002579
Iteration 167/1000 | Loss: 0.00002579
Iteration 168/1000 | Loss: 0.00002579
Iteration 169/1000 | Loss: 0.00002579
Iteration 170/1000 | Loss: 0.00002579
Iteration 171/1000 | Loss: 0.00002579
Iteration 172/1000 | Loss: 0.00002579
Iteration 173/1000 | Loss: 0.00002579
Iteration 174/1000 | Loss: 0.00002579
Iteration 175/1000 | Loss: 0.00002579
Iteration 176/1000 | Loss: 0.00002579
Iteration 177/1000 | Loss: 0.00002579
Iteration 178/1000 | Loss: 0.00002579
Iteration 179/1000 | Loss: 0.00002579
Iteration 180/1000 | Loss: 0.00002579
Iteration 181/1000 | Loss: 0.00002579
Iteration 182/1000 | Loss: 0.00002579
Iteration 183/1000 | Loss: 0.00002579
Iteration 184/1000 | Loss: 0.00002579
Iteration 185/1000 | Loss: 0.00002579
Iteration 186/1000 | Loss: 0.00002579
Iteration 187/1000 | Loss: 0.00002579
Iteration 188/1000 | Loss: 0.00002579
Iteration 189/1000 | Loss: 0.00002579
Iteration 190/1000 | Loss: 0.00002579
Iteration 191/1000 | Loss: 0.00002579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.5786153855733573e-05, 2.5786153855733573e-05, 2.5786153855733573e-05, 2.5786153855733573e-05, 2.5786153855733573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5786153855733573e-05

Optimization complete. Final v2v error: 3.800532341003418 mm

Highest mean error: 10.6446533203125 mm for frame 123

Lowest mean error: 3.0812880992889404 mm for frame 94

Saving results

Total time: 122.40079927444458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992721
Iteration 2/25 | Loss: 0.00266426
Iteration 3/25 | Loss: 0.00199637
Iteration 4/25 | Loss: 0.00185996
Iteration 5/25 | Loss: 0.00175038
Iteration 6/25 | Loss: 0.00158218
Iteration 7/25 | Loss: 0.00170622
Iteration 8/25 | Loss: 0.00198265
Iteration 9/25 | Loss: 0.00161966
Iteration 10/25 | Loss: 0.00111202
Iteration 11/25 | Loss: 0.00100535
Iteration 12/25 | Loss: 0.00098194
Iteration 13/25 | Loss: 0.00097852
Iteration 14/25 | Loss: 0.00097771
Iteration 15/25 | Loss: 0.00097745
Iteration 16/25 | Loss: 0.00097741
Iteration 17/25 | Loss: 0.00097741
Iteration 18/25 | Loss: 0.00097741
Iteration 19/25 | Loss: 0.00097741
Iteration 20/25 | Loss: 0.00097740
Iteration 21/25 | Loss: 0.00097740
Iteration 22/25 | Loss: 0.00097740
Iteration 23/25 | Loss: 0.00097740
Iteration 24/25 | Loss: 0.00097740
Iteration 25/25 | Loss: 0.00097740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.98604298
Iteration 2/25 | Loss: 0.00120090
Iteration 3/25 | Loss: 0.00075464
Iteration 4/25 | Loss: 0.00075464
Iteration 5/25 | Loss: 0.00075464
Iteration 6/25 | Loss: 0.00075464
Iteration 7/25 | Loss: 0.00075464
Iteration 8/25 | Loss: 0.00075463
Iteration 9/25 | Loss: 0.00075463
Iteration 10/25 | Loss: 0.00075463
Iteration 11/25 | Loss: 0.00075463
Iteration 12/25 | Loss: 0.00075463
Iteration 13/25 | Loss: 0.00075463
Iteration 14/25 | Loss: 0.00075463
Iteration 15/25 | Loss: 0.00075463
Iteration 16/25 | Loss: 0.00075463
Iteration 17/25 | Loss: 0.00075463
Iteration 18/25 | Loss: 0.00075463
Iteration 19/25 | Loss: 0.00075463
Iteration 20/25 | Loss: 0.00075463
Iteration 21/25 | Loss: 0.00075463
Iteration 22/25 | Loss: 0.00075463
Iteration 23/25 | Loss: 0.00075463
Iteration 24/25 | Loss: 0.00075463
Iteration 25/25 | Loss: 0.00075463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075463
Iteration 2/1000 | Loss: 0.00082757
Iteration 3/1000 | Loss: 0.00004479
Iteration 4/1000 | Loss: 0.00003538
Iteration 5/1000 | Loss: 0.00003130
Iteration 6/1000 | Loss: 0.00057856
Iteration 7/1000 | Loss: 0.00004318
Iteration 8/1000 | Loss: 0.00003003
Iteration 9/1000 | Loss: 0.00024922
Iteration 10/1000 | Loss: 0.00009694
Iteration 11/1000 | Loss: 0.00002425
Iteration 12/1000 | Loss: 0.00002283
Iteration 13/1000 | Loss: 0.00002221
Iteration 14/1000 | Loss: 0.00031117
Iteration 15/1000 | Loss: 0.00009464
Iteration 16/1000 | Loss: 0.00002217
Iteration 17/1000 | Loss: 0.00002161
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002090
Iteration 21/1000 | Loss: 0.00002087
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002085
Iteration 25/1000 | Loss: 0.00002079
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002077
Iteration 28/1000 | Loss: 0.00002077
Iteration 29/1000 | Loss: 0.00002076
Iteration 30/1000 | Loss: 0.00002076
Iteration 31/1000 | Loss: 0.00002076
Iteration 32/1000 | Loss: 0.00002076
Iteration 33/1000 | Loss: 0.00002075
Iteration 34/1000 | Loss: 0.00002075
Iteration 35/1000 | Loss: 0.00002074
Iteration 36/1000 | Loss: 0.00002074
Iteration 37/1000 | Loss: 0.00002074
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002073
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002073
Iteration 42/1000 | Loss: 0.00002073
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002072
Iteration 45/1000 | Loss: 0.00002072
Iteration 46/1000 | Loss: 0.00002072
Iteration 47/1000 | Loss: 0.00002072
Iteration 48/1000 | Loss: 0.00002071
Iteration 49/1000 | Loss: 0.00002071
Iteration 50/1000 | Loss: 0.00002071
Iteration 51/1000 | Loss: 0.00002071
Iteration 52/1000 | Loss: 0.00002071
Iteration 53/1000 | Loss: 0.00002070
Iteration 54/1000 | Loss: 0.00031568
Iteration 55/1000 | Loss: 0.00003808
Iteration 56/1000 | Loss: 0.00003100
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002090
Iteration 59/1000 | Loss: 0.00002069
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00030435
Iteration 62/1000 | Loss: 0.00009823
Iteration 63/1000 | Loss: 0.00003405
Iteration 64/1000 | Loss: 0.00003068
Iteration 65/1000 | Loss: 0.00014361
Iteration 66/1000 | Loss: 0.00004258
Iteration 67/1000 | Loss: 0.00003953
Iteration 68/1000 | Loss: 0.00002198
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00002064
Iteration 72/1000 | Loss: 0.00002064
Iteration 73/1000 | Loss: 0.00002062
Iteration 74/1000 | Loss: 0.00002061
Iteration 75/1000 | Loss: 0.00002061
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002059
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002058
Iteration 91/1000 | Loss: 0.00002058
Iteration 92/1000 | Loss: 0.00002058
Iteration 93/1000 | Loss: 0.00002058
Iteration 94/1000 | Loss: 0.00002058
Iteration 95/1000 | Loss: 0.00002058
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002058
Iteration 101/1000 | Loss: 0.00002057
Iteration 102/1000 | Loss: 0.00002057
Iteration 103/1000 | Loss: 0.00002057
Iteration 104/1000 | Loss: 0.00002057
Iteration 105/1000 | Loss: 0.00002057
Iteration 106/1000 | Loss: 0.00002056
Iteration 107/1000 | Loss: 0.00002056
Iteration 108/1000 | Loss: 0.00002056
Iteration 109/1000 | Loss: 0.00002056
Iteration 110/1000 | Loss: 0.00002056
Iteration 111/1000 | Loss: 0.00002056
Iteration 112/1000 | Loss: 0.00002056
Iteration 113/1000 | Loss: 0.00002056
Iteration 114/1000 | Loss: 0.00002056
Iteration 115/1000 | Loss: 0.00002056
Iteration 116/1000 | Loss: 0.00002056
Iteration 117/1000 | Loss: 0.00002056
Iteration 118/1000 | Loss: 0.00002056
Iteration 119/1000 | Loss: 0.00002056
Iteration 120/1000 | Loss: 0.00002056
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002056
Iteration 123/1000 | Loss: 0.00002056
Iteration 124/1000 | Loss: 0.00002056
Iteration 125/1000 | Loss: 0.00002056
Iteration 126/1000 | Loss: 0.00002056
Iteration 127/1000 | Loss: 0.00002056
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002056
Iteration 130/1000 | Loss: 0.00002056
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002056
Iteration 135/1000 | Loss: 0.00002056
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002056
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002056
Iteration 141/1000 | Loss: 0.00002056
Iteration 142/1000 | Loss: 0.00002056
Iteration 143/1000 | Loss: 0.00002056
Iteration 144/1000 | Loss: 0.00002056
Iteration 145/1000 | Loss: 0.00002056
Iteration 146/1000 | Loss: 0.00002056
Iteration 147/1000 | Loss: 0.00002056
Iteration 148/1000 | Loss: 0.00002056
Iteration 149/1000 | Loss: 0.00002056
Iteration 150/1000 | Loss: 0.00002056
Iteration 151/1000 | Loss: 0.00002056
Iteration 152/1000 | Loss: 0.00002056
Iteration 153/1000 | Loss: 0.00002056
Iteration 154/1000 | Loss: 0.00002056
Iteration 155/1000 | Loss: 0.00002056
Iteration 156/1000 | Loss: 0.00002056
Iteration 157/1000 | Loss: 0.00002056
Iteration 158/1000 | Loss: 0.00002056
Iteration 159/1000 | Loss: 0.00002056
Iteration 160/1000 | Loss: 0.00002056
Iteration 161/1000 | Loss: 0.00002056
Iteration 162/1000 | Loss: 0.00002056
Iteration 163/1000 | Loss: 0.00002056
Iteration 164/1000 | Loss: 0.00002056
Iteration 165/1000 | Loss: 0.00002056
Iteration 166/1000 | Loss: 0.00002056
Iteration 167/1000 | Loss: 0.00002056
Iteration 168/1000 | Loss: 0.00002056
Iteration 169/1000 | Loss: 0.00002056
Iteration 170/1000 | Loss: 0.00002056
Iteration 171/1000 | Loss: 0.00002056
Iteration 172/1000 | Loss: 0.00002056
Iteration 173/1000 | Loss: 0.00002056
Iteration 174/1000 | Loss: 0.00002056
Iteration 175/1000 | Loss: 0.00002056
Iteration 176/1000 | Loss: 0.00002056
Iteration 177/1000 | Loss: 0.00002056
Iteration 178/1000 | Loss: 0.00002056
Iteration 179/1000 | Loss: 0.00002056
Iteration 180/1000 | Loss: 0.00002056
Iteration 181/1000 | Loss: 0.00002056
Iteration 182/1000 | Loss: 0.00002056
Iteration 183/1000 | Loss: 0.00002056
Iteration 184/1000 | Loss: 0.00002056
Iteration 185/1000 | Loss: 0.00002056
Iteration 186/1000 | Loss: 0.00002056
Iteration 187/1000 | Loss: 0.00002056
Iteration 188/1000 | Loss: 0.00002056
Iteration 189/1000 | Loss: 0.00002056
Iteration 190/1000 | Loss: 0.00002056
Iteration 191/1000 | Loss: 0.00002056
Iteration 192/1000 | Loss: 0.00002056
Iteration 193/1000 | Loss: 0.00002056
Iteration 194/1000 | Loss: 0.00002056
Iteration 195/1000 | Loss: 0.00002056
Iteration 196/1000 | Loss: 0.00002056
Iteration 197/1000 | Loss: 0.00002056
Iteration 198/1000 | Loss: 0.00002056
Iteration 199/1000 | Loss: 0.00002056
Iteration 200/1000 | Loss: 0.00002056
Iteration 201/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.0562600184348412e-05, 2.0562600184348412e-05, 2.0562600184348412e-05, 2.0562600184348412e-05, 2.0562600184348412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0562600184348412e-05

Optimization complete. Final v2v error: 3.798529624938965 mm

Highest mean error: 4.222203254699707 mm for frame 109

Lowest mean error: 3.4284212589263916 mm for frame 77

Saving results

Total time: 85.28578519821167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958601
Iteration 2/25 | Loss: 0.00137251
Iteration 3/25 | Loss: 0.00112861
Iteration 4/25 | Loss: 0.00093608
Iteration 5/25 | Loss: 0.00092897
Iteration 6/25 | Loss: 0.00091975
Iteration 7/25 | Loss: 0.00091841
Iteration 8/25 | Loss: 0.00091761
Iteration 9/25 | Loss: 0.00092040
Iteration 10/25 | Loss: 0.00092103
Iteration 11/25 | Loss: 0.00092938
Iteration 12/25 | Loss: 0.00091734
Iteration 13/25 | Loss: 0.00091803
Iteration 14/25 | Loss: 0.00091252
Iteration 15/25 | Loss: 0.00091637
Iteration 16/25 | Loss: 0.00091552
Iteration 17/25 | Loss: 0.00091255
Iteration 18/25 | Loss: 0.00091228
Iteration 19/25 | Loss: 0.00091226
Iteration 20/25 | Loss: 0.00091226
Iteration 21/25 | Loss: 0.00091226
Iteration 22/25 | Loss: 0.00091226
Iteration 23/25 | Loss: 0.00091226
Iteration 24/25 | Loss: 0.00091226
Iteration 25/25 | Loss: 0.00091226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38458633
Iteration 2/25 | Loss: 0.00091848
Iteration 3/25 | Loss: 0.00091845
Iteration 4/25 | Loss: 0.00091845
Iteration 5/25 | Loss: 0.00091845
Iteration 6/25 | Loss: 0.00091845
Iteration 7/25 | Loss: 0.00091845
Iteration 8/25 | Loss: 0.00091845
Iteration 9/25 | Loss: 0.00091845
Iteration 10/25 | Loss: 0.00091845
Iteration 11/25 | Loss: 0.00091845
Iteration 12/25 | Loss: 0.00091845
Iteration 13/25 | Loss: 0.00091845
Iteration 14/25 | Loss: 0.00091845
Iteration 15/25 | Loss: 0.00091845
Iteration 16/25 | Loss: 0.00091845
Iteration 17/25 | Loss: 0.00091845
Iteration 18/25 | Loss: 0.00091845
Iteration 19/25 | Loss: 0.00091845
Iteration 20/25 | Loss: 0.00091845
Iteration 21/25 | Loss: 0.00091845
Iteration 22/25 | Loss: 0.00091845
Iteration 23/25 | Loss: 0.00091845
Iteration 24/25 | Loss: 0.00091845
Iteration 25/25 | Loss: 0.00091845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091845
Iteration 2/1000 | Loss: 0.00004416
Iteration 3/1000 | Loss: 0.00003084
Iteration 4/1000 | Loss: 0.00013815
Iteration 5/1000 | Loss: 0.00014771
Iteration 6/1000 | Loss: 0.00002112
Iteration 7/1000 | Loss: 0.00014904
Iteration 8/1000 | Loss: 0.00006980
Iteration 9/1000 | Loss: 0.00023514
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00006622
Iteration 13/1000 | Loss: 0.00003310
Iteration 14/1000 | Loss: 0.00004170
Iteration 15/1000 | Loss: 0.00014411
Iteration 16/1000 | Loss: 0.00003941
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001688
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001645
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001632
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001623
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001622
Iteration 30/1000 | Loss: 0.00001617
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001614
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001610
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00018017
Iteration 46/1000 | Loss: 0.00018017
Iteration 47/1000 | Loss: 0.00002165
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001612
Iteration 51/1000 | Loss: 0.00010238
Iteration 52/1000 | Loss: 0.00003330
Iteration 53/1000 | Loss: 0.00001607
Iteration 54/1000 | Loss: 0.00007729
Iteration 55/1000 | Loss: 0.00003608
Iteration 56/1000 | Loss: 0.00002172
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001603
Iteration 60/1000 | Loss: 0.00001602
Iteration 61/1000 | Loss: 0.00001602
Iteration 62/1000 | Loss: 0.00001601
Iteration 63/1000 | Loss: 0.00001601
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001599
Iteration 68/1000 | Loss: 0.00001599
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001598
Iteration 71/1000 | Loss: 0.00001598
Iteration 72/1000 | Loss: 0.00001598
Iteration 73/1000 | Loss: 0.00001598
Iteration 74/1000 | Loss: 0.00001598
Iteration 75/1000 | Loss: 0.00001598
Iteration 76/1000 | Loss: 0.00001597
Iteration 77/1000 | Loss: 0.00001597
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001596
Iteration 81/1000 | Loss: 0.00001596
Iteration 82/1000 | Loss: 0.00001596
Iteration 83/1000 | Loss: 0.00001596
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.5946803614497185e-05, 1.5946803614497185e-05, 1.5946803614497185e-05, 1.5946803614497185e-05, 1.5946803614497185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5946803614497185e-05

Optimization complete. Final v2v error: 3.3742482662200928 mm

Highest mean error: 4.248661994934082 mm for frame 38

Lowest mean error: 2.920701503753662 mm for frame 120

Saving results

Total time: 85.23614931106567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850653
Iteration 2/25 | Loss: 0.00128075
Iteration 3/25 | Loss: 0.00101969
Iteration 4/25 | Loss: 0.00100450
Iteration 5/25 | Loss: 0.00100377
Iteration 6/25 | Loss: 0.00100377
Iteration 7/25 | Loss: 0.00100377
Iteration 8/25 | Loss: 0.00100377
Iteration 9/25 | Loss: 0.00100377
Iteration 10/25 | Loss: 0.00100377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001003770506940782, 0.001003770506940782, 0.001003770506940782, 0.001003770506940782, 0.001003770506940782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001003770506940782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88699663
Iteration 2/25 | Loss: 0.00049406
Iteration 3/25 | Loss: 0.00049405
Iteration 4/25 | Loss: 0.00049405
Iteration 5/25 | Loss: 0.00049405
Iteration 6/25 | Loss: 0.00049405
Iteration 7/25 | Loss: 0.00049405
Iteration 8/25 | Loss: 0.00049405
Iteration 9/25 | Loss: 0.00049405
Iteration 10/25 | Loss: 0.00049405
Iteration 11/25 | Loss: 0.00049405
Iteration 12/25 | Loss: 0.00049405
Iteration 13/25 | Loss: 0.00049405
Iteration 14/25 | Loss: 0.00049405
Iteration 15/25 | Loss: 0.00049405
Iteration 16/25 | Loss: 0.00049405
Iteration 17/25 | Loss: 0.00049405
Iteration 18/25 | Loss: 0.00049405
Iteration 19/25 | Loss: 0.00049405
Iteration 20/25 | Loss: 0.00049405
Iteration 21/25 | Loss: 0.00049405
Iteration 22/25 | Loss: 0.00049405
Iteration 23/25 | Loss: 0.00049405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004940497456118464, 0.0004940497456118464, 0.0004940497456118464, 0.0004940497456118464, 0.0004940497456118464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004940497456118464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049405
Iteration 2/1000 | Loss: 0.00004522
Iteration 3/1000 | Loss: 0.00003526
Iteration 4/1000 | Loss: 0.00003041
Iteration 5/1000 | Loss: 0.00002839
Iteration 6/1000 | Loss: 0.00002664
Iteration 7/1000 | Loss: 0.00002570
Iteration 8/1000 | Loss: 0.00002509
Iteration 9/1000 | Loss: 0.00002482
Iteration 10/1000 | Loss: 0.00002465
Iteration 11/1000 | Loss: 0.00002445
Iteration 12/1000 | Loss: 0.00002438
Iteration 13/1000 | Loss: 0.00002421
Iteration 14/1000 | Loss: 0.00002419
Iteration 15/1000 | Loss: 0.00002416
Iteration 16/1000 | Loss: 0.00002416
Iteration 17/1000 | Loss: 0.00002410
Iteration 18/1000 | Loss: 0.00002410
Iteration 19/1000 | Loss: 0.00002408
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002408
Iteration 22/1000 | Loss: 0.00002408
Iteration 23/1000 | Loss: 0.00002408
Iteration 24/1000 | Loss: 0.00002408
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002408
Iteration 27/1000 | Loss: 0.00002408
Iteration 28/1000 | Loss: 0.00002408
Iteration 29/1000 | Loss: 0.00002408
Iteration 30/1000 | Loss: 0.00002408
Iteration 31/1000 | Loss: 0.00002408
Iteration 32/1000 | Loss: 0.00002407
Iteration 33/1000 | Loss: 0.00002406
Iteration 34/1000 | Loss: 0.00002406
Iteration 35/1000 | Loss: 0.00002406
Iteration 36/1000 | Loss: 0.00002405
Iteration 37/1000 | Loss: 0.00002405
Iteration 38/1000 | Loss: 0.00002405
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002404
Iteration 41/1000 | Loss: 0.00002404
Iteration 42/1000 | Loss: 0.00002404
Iteration 43/1000 | Loss: 0.00002404
Iteration 44/1000 | Loss: 0.00002404
Iteration 45/1000 | Loss: 0.00002404
Iteration 46/1000 | Loss: 0.00002404
Iteration 47/1000 | Loss: 0.00002404
Iteration 48/1000 | Loss: 0.00002404
Iteration 49/1000 | Loss: 0.00002403
Iteration 50/1000 | Loss: 0.00002403
Iteration 51/1000 | Loss: 0.00002402
Iteration 52/1000 | Loss: 0.00002402
Iteration 53/1000 | Loss: 0.00002402
Iteration 54/1000 | Loss: 0.00002402
Iteration 55/1000 | Loss: 0.00002401
Iteration 56/1000 | Loss: 0.00002401
Iteration 57/1000 | Loss: 0.00002401
Iteration 58/1000 | Loss: 0.00002401
Iteration 59/1000 | Loss: 0.00002401
Iteration 60/1000 | Loss: 0.00002401
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002401
Iteration 64/1000 | Loss: 0.00002401
Iteration 65/1000 | Loss: 0.00002401
Iteration 66/1000 | Loss: 0.00002401
Iteration 67/1000 | Loss: 0.00002401
Iteration 68/1000 | Loss: 0.00002401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.4013865186134353e-05, 2.4013865186134353e-05, 2.4013865186134353e-05, 2.4013865186134353e-05, 2.4013865186134353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4013865186134353e-05

Optimization complete. Final v2v error: 4.135903835296631 mm

Highest mean error: 4.332320690155029 mm for frame 102

Lowest mean error: 3.9141550064086914 mm for frame 67

Saving results

Total time: 32.931233167648315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055997
Iteration 2/25 | Loss: 0.00197748
Iteration 3/25 | Loss: 0.00159791
Iteration 4/25 | Loss: 0.00153579
Iteration 5/25 | Loss: 0.00146410
Iteration 6/25 | Loss: 0.00127768
Iteration 7/25 | Loss: 0.00111366
Iteration 8/25 | Loss: 0.00112332
Iteration 9/25 | Loss: 0.00108160
Iteration 10/25 | Loss: 0.00106245
Iteration 11/25 | Loss: 0.00106426
Iteration 12/25 | Loss: 0.00105383
Iteration 13/25 | Loss: 0.00107522
Iteration 14/25 | Loss: 0.00104710
Iteration 15/25 | Loss: 0.00104429
Iteration 16/25 | Loss: 0.00103869
Iteration 17/25 | Loss: 0.00104646
Iteration 18/25 | Loss: 0.00104185
Iteration 19/25 | Loss: 0.00104108
Iteration 20/25 | Loss: 0.00104575
Iteration 21/25 | Loss: 0.00104041
Iteration 22/25 | Loss: 0.00104776
Iteration 23/25 | Loss: 0.00104395
Iteration 24/25 | Loss: 0.00104251
Iteration 25/25 | Loss: 0.00104083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44335663
Iteration 2/25 | Loss: 0.00169496
Iteration 3/25 | Loss: 0.00140178
Iteration 4/25 | Loss: 0.00140178
Iteration 5/25 | Loss: 0.00140178
Iteration 6/25 | Loss: 0.00140178
Iteration 7/25 | Loss: 0.00140178
Iteration 8/25 | Loss: 0.00140178
Iteration 9/25 | Loss: 0.00140178
Iteration 10/25 | Loss: 0.00140178
Iteration 11/25 | Loss: 0.00140178
Iteration 12/25 | Loss: 0.00140178
Iteration 13/25 | Loss: 0.00140178
Iteration 14/25 | Loss: 0.00140178
Iteration 15/25 | Loss: 0.00140178
Iteration 16/25 | Loss: 0.00140178
Iteration 17/25 | Loss: 0.00140178
Iteration 18/25 | Loss: 0.00140178
Iteration 19/25 | Loss: 0.00140178
Iteration 20/25 | Loss: 0.00140178
Iteration 21/25 | Loss: 0.00140178
Iteration 22/25 | Loss: 0.00140178
Iteration 23/25 | Loss: 0.00140178
Iteration 24/25 | Loss: 0.00140178
Iteration 25/25 | Loss: 0.00140178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140178
Iteration 2/1000 | Loss: 0.00052359
Iteration 3/1000 | Loss: 0.00030851
Iteration 4/1000 | Loss: 0.00029553
Iteration 5/1000 | Loss: 0.00071970
Iteration 6/1000 | Loss: 0.00036056
Iteration 7/1000 | Loss: 0.00046442
Iteration 8/1000 | Loss: 0.00036035
Iteration 9/1000 | Loss: 0.00038649
Iteration 10/1000 | Loss: 0.00030437
Iteration 11/1000 | Loss: 0.00022344
Iteration 12/1000 | Loss: 0.00036667
Iteration 13/1000 | Loss: 0.00025481
Iteration 14/1000 | Loss: 0.00023928
Iteration 15/1000 | Loss: 0.00028178
Iteration 16/1000 | Loss: 0.00005121
Iteration 17/1000 | Loss: 0.00005866
Iteration 18/1000 | Loss: 0.00007128
Iteration 19/1000 | Loss: 0.00019747
Iteration 20/1000 | Loss: 0.00003015
Iteration 21/1000 | Loss: 0.00006576
Iteration 22/1000 | Loss: 0.00007694
Iteration 23/1000 | Loss: 0.00003377
Iteration 24/1000 | Loss: 0.00005882
Iteration 25/1000 | Loss: 0.00005937
Iteration 26/1000 | Loss: 0.00003141
Iteration 27/1000 | Loss: 0.00002701
Iteration 28/1000 | Loss: 0.00007230
Iteration 29/1000 | Loss: 0.00005902
Iteration 30/1000 | Loss: 0.00006288
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00024668
Iteration 33/1000 | Loss: 0.00018482
Iteration 34/1000 | Loss: 0.00025815
Iteration 35/1000 | Loss: 0.00015760
Iteration 36/1000 | Loss: 0.00002457
Iteration 37/1000 | Loss: 0.00015601
Iteration 38/1000 | Loss: 0.00002325
Iteration 39/1000 | Loss: 0.00002639
Iteration 40/1000 | Loss: 0.00002459
Iteration 41/1000 | Loss: 0.00002241
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002060
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00002039
Iteration 46/1000 | Loss: 0.00002027
Iteration 47/1000 | Loss: 0.00002027
Iteration 48/1000 | Loss: 0.00002027
Iteration 49/1000 | Loss: 0.00002027
Iteration 50/1000 | Loss: 0.00002027
Iteration 51/1000 | Loss: 0.00002027
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002026
Iteration 55/1000 | Loss: 0.00002026
Iteration 56/1000 | Loss: 0.00002026
Iteration 57/1000 | Loss: 0.00002026
Iteration 58/1000 | Loss: 0.00002025
Iteration 59/1000 | Loss: 0.00002025
Iteration 60/1000 | Loss: 0.00002025
Iteration 61/1000 | Loss: 0.00002787
Iteration 62/1000 | Loss: 0.00002521
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00002008
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002006
Iteration 70/1000 | Loss: 0.00002005
Iteration 71/1000 | Loss: 0.00002005
Iteration 72/1000 | Loss: 0.00002005
Iteration 73/1000 | Loss: 0.00002005
Iteration 74/1000 | Loss: 0.00002005
Iteration 75/1000 | Loss: 0.00002005
Iteration 76/1000 | Loss: 0.00002005
Iteration 77/1000 | Loss: 0.00002005
Iteration 78/1000 | Loss: 0.00002005
Iteration 79/1000 | Loss: 0.00002005
Iteration 80/1000 | Loss: 0.00002005
Iteration 81/1000 | Loss: 0.00002004
Iteration 82/1000 | Loss: 0.00002004
Iteration 83/1000 | Loss: 0.00002004
Iteration 84/1000 | Loss: 0.00002004
Iteration 85/1000 | Loss: 0.00002004
Iteration 86/1000 | Loss: 0.00002004
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002224
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002003
Iteration 93/1000 | Loss: 0.00002003
Iteration 94/1000 | Loss: 0.00002003
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002002
Iteration 99/1000 | Loss: 0.00002002
Iteration 100/1000 | Loss: 0.00002002
Iteration 101/1000 | Loss: 0.00002002
Iteration 102/1000 | Loss: 0.00002681
Iteration 103/1000 | Loss: 0.00002000
Iteration 104/1000 | Loss: 0.00001998
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001998
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001998
Iteration 112/1000 | Loss: 0.00001998
Iteration 113/1000 | Loss: 0.00001998
Iteration 114/1000 | Loss: 0.00001997
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001996
Iteration 118/1000 | Loss: 0.00001996
Iteration 119/1000 | Loss: 0.00001996
Iteration 120/1000 | Loss: 0.00001996
Iteration 121/1000 | Loss: 0.00001996
Iteration 122/1000 | Loss: 0.00001996
Iteration 123/1000 | Loss: 0.00001996
Iteration 124/1000 | Loss: 0.00001996
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001996
Iteration 127/1000 | Loss: 0.00001995
Iteration 128/1000 | Loss: 0.00001995
Iteration 129/1000 | Loss: 0.00001995
Iteration 130/1000 | Loss: 0.00001995
Iteration 131/1000 | Loss: 0.00001995
Iteration 132/1000 | Loss: 0.00001995
Iteration 133/1000 | Loss: 0.00001995
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002128
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002037
Iteration 138/1000 | Loss: 0.00002058
Iteration 139/1000 | Loss: 0.00002004
Iteration 140/1000 | Loss: 0.00001993
Iteration 141/1000 | Loss: 0.00001993
Iteration 142/1000 | Loss: 0.00001993
Iteration 143/1000 | Loss: 0.00001993
Iteration 144/1000 | Loss: 0.00001993
Iteration 145/1000 | Loss: 0.00001993
Iteration 146/1000 | Loss: 0.00001993
Iteration 147/1000 | Loss: 0.00001993
Iteration 148/1000 | Loss: 0.00001993
Iteration 149/1000 | Loss: 0.00001993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.993075966311153e-05, 1.993075966311153e-05, 1.993075966311153e-05, 1.993075966311153e-05, 1.993075966311153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.993075966311153e-05

Optimization complete. Final v2v error: 3.7938013076782227 mm

Highest mean error: 4.480844020843506 mm for frame 34

Lowest mean error: 3.2315635681152344 mm for frame 249

Saving results

Total time: 146.75892972946167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432143
Iteration 2/25 | Loss: 0.00102321
Iteration 3/25 | Loss: 0.00094062
Iteration 4/25 | Loss: 0.00092656
Iteration 5/25 | Loss: 0.00092295
Iteration 6/25 | Loss: 0.00092229
Iteration 7/25 | Loss: 0.00092229
Iteration 8/25 | Loss: 0.00092229
Iteration 9/25 | Loss: 0.00092229
Iteration 10/25 | Loss: 0.00092229
Iteration 11/25 | Loss: 0.00092229
Iteration 12/25 | Loss: 0.00092229
Iteration 13/25 | Loss: 0.00092229
Iteration 14/25 | Loss: 0.00092229
Iteration 15/25 | Loss: 0.00092229
Iteration 16/25 | Loss: 0.00092229
Iteration 17/25 | Loss: 0.00092229
Iteration 18/25 | Loss: 0.00092229
Iteration 19/25 | Loss: 0.00092229
Iteration 20/25 | Loss: 0.00092229
Iteration 21/25 | Loss: 0.00092229
Iteration 22/25 | Loss: 0.00092229
Iteration 23/25 | Loss: 0.00092229
Iteration 24/25 | Loss: 0.00092229
Iteration 25/25 | Loss: 0.00092229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91384190
Iteration 2/25 | Loss: 0.00076872
Iteration 3/25 | Loss: 0.00076872
Iteration 4/25 | Loss: 0.00076872
Iteration 5/25 | Loss: 0.00076871
Iteration 6/25 | Loss: 0.00076871
Iteration 7/25 | Loss: 0.00076871
Iteration 8/25 | Loss: 0.00076871
Iteration 9/25 | Loss: 0.00076871
Iteration 10/25 | Loss: 0.00076871
Iteration 11/25 | Loss: 0.00076871
Iteration 12/25 | Loss: 0.00076871
Iteration 13/25 | Loss: 0.00076871
Iteration 14/25 | Loss: 0.00076871
Iteration 15/25 | Loss: 0.00076871
Iteration 16/25 | Loss: 0.00076871
Iteration 17/25 | Loss: 0.00076871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007687125471420586, 0.0007687125471420586, 0.0007687125471420586, 0.0007687125471420586, 0.0007687125471420586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007687125471420586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076871
Iteration 2/1000 | Loss: 0.00005365
Iteration 3/1000 | Loss: 0.00002861
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00001990
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001774
Iteration 8/1000 | Loss: 0.00001726
Iteration 9/1000 | Loss: 0.00001688
Iteration 10/1000 | Loss: 0.00001673
Iteration 11/1000 | Loss: 0.00001660
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001618
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00001615
Iteration 32/1000 | Loss: 0.00001614
Iteration 33/1000 | Loss: 0.00001614
Iteration 34/1000 | Loss: 0.00001613
Iteration 35/1000 | Loss: 0.00001613
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001611
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001609
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00001609
Iteration 48/1000 | Loss: 0.00001609
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001608
Iteration 53/1000 | Loss: 0.00001608
Iteration 54/1000 | Loss: 0.00001608
Iteration 55/1000 | Loss: 0.00001608
Iteration 56/1000 | Loss: 0.00001608
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00001607
Iteration 60/1000 | Loss: 0.00001606
Iteration 61/1000 | Loss: 0.00001606
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001605
Iteration 65/1000 | Loss: 0.00001605
Iteration 66/1000 | Loss: 0.00001605
Iteration 67/1000 | Loss: 0.00001605
Iteration 68/1000 | Loss: 0.00001605
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001604
Iteration 73/1000 | Loss: 0.00001604
Iteration 74/1000 | Loss: 0.00001604
Iteration 75/1000 | Loss: 0.00001604
Iteration 76/1000 | Loss: 0.00001604
Iteration 77/1000 | Loss: 0.00001604
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001603
Iteration 96/1000 | Loss: 0.00001603
Iteration 97/1000 | Loss: 0.00001603
Iteration 98/1000 | Loss: 0.00001603
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001603
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001602
Iteration 106/1000 | Loss: 0.00001602
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00001602
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.6023155694711022e-05, 1.6023155694711022e-05, 1.6023155694711022e-05, 1.6023155694711022e-05, 1.6023155694711022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6023155694711022e-05

Optimization complete. Final v2v error: 3.3855559825897217 mm

Highest mean error: 3.5752062797546387 mm for frame 113

Lowest mean error: 3.228508949279785 mm for frame 39

Saving results

Total time: 34.969879150390625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108500
Iteration 2/25 | Loss: 0.01108500
Iteration 3/25 | Loss: 0.01108500
Iteration 4/25 | Loss: 0.01108500
Iteration 5/25 | Loss: 0.01108500
Iteration 6/25 | Loss: 0.01108499
Iteration 7/25 | Loss: 0.01108499
Iteration 8/25 | Loss: 0.00238531
Iteration 9/25 | Loss: 0.00158624
Iteration 10/25 | Loss: 0.00120088
Iteration 11/25 | Loss: 0.00108622
Iteration 12/25 | Loss: 0.00105810
Iteration 13/25 | Loss: 0.00105050
Iteration 14/25 | Loss: 0.00103123
Iteration 15/25 | Loss: 0.00102096
Iteration 16/25 | Loss: 0.00101762
Iteration 17/25 | Loss: 0.00102058
Iteration 18/25 | Loss: 0.00101383
Iteration 19/25 | Loss: 0.00101232
Iteration 20/25 | Loss: 0.00101074
Iteration 21/25 | Loss: 0.00100964
Iteration 22/25 | Loss: 0.00100936
Iteration 23/25 | Loss: 0.00100930
Iteration 24/25 | Loss: 0.00100930
Iteration 25/25 | Loss: 0.00100930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23817039
Iteration 2/25 | Loss: 0.00143172
Iteration 3/25 | Loss: 0.00143172
Iteration 4/25 | Loss: 0.00143172
Iteration 5/25 | Loss: 0.00143172
Iteration 6/25 | Loss: 0.00143172
Iteration 7/25 | Loss: 0.00143172
Iteration 8/25 | Loss: 0.00143172
Iteration 9/25 | Loss: 0.00143172
Iteration 10/25 | Loss: 0.00143172
Iteration 11/25 | Loss: 0.00143172
Iteration 12/25 | Loss: 0.00143172
Iteration 13/25 | Loss: 0.00143172
Iteration 14/25 | Loss: 0.00143172
Iteration 15/25 | Loss: 0.00143172
Iteration 16/25 | Loss: 0.00143172
Iteration 17/25 | Loss: 0.00143172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014317176537588239, 0.0014317176537588239, 0.0014317176537588239, 0.0014317176537588239, 0.0014317176537588239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014317176537588239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143172
Iteration 2/1000 | Loss: 0.00006461
Iteration 3/1000 | Loss: 0.00005379
Iteration 4/1000 | Loss: 0.00003255
Iteration 5/1000 | Loss: 0.00003170
Iteration 6/1000 | Loss: 0.00002478
Iteration 7/1000 | Loss: 0.00003089
Iteration 8/1000 | Loss: 0.00002633
Iteration 9/1000 | Loss: 0.00002286
Iteration 10/1000 | Loss: 0.00002700
Iteration 11/1000 | Loss: 0.00002622
Iteration 12/1000 | Loss: 0.00002214
Iteration 13/1000 | Loss: 0.00002386
Iteration 14/1000 | Loss: 0.00002501
Iteration 15/1000 | Loss: 0.00002184
Iteration 16/1000 | Loss: 0.00002180
Iteration 17/1000 | Loss: 0.00002172
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002164
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00002156
Iteration 22/1000 | Loss: 0.00002156
Iteration 23/1000 | Loss: 0.00002155
Iteration 24/1000 | Loss: 0.00002154
Iteration 25/1000 | Loss: 0.00002154
Iteration 26/1000 | Loss: 0.00002154
Iteration 27/1000 | Loss: 0.00002153
Iteration 28/1000 | Loss: 0.00002153
Iteration 29/1000 | Loss: 0.00002152
Iteration 30/1000 | Loss: 0.00002152
Iteration 31/1000 | Loss: 0.00002152
Iteration 32/1000 | Loss: 0.00002151
Iteration 33/1000 | Loss: 0.00002151
Iteration 34/1000 | Loss: 0.00002150
Iteration 35/1000 | Loss: 0.00002150
Iteration 36/1000 | Loss: 0.00002150
Iteration 37/1000 | Loss: 0.00002150
Iteration 38/1000 | Loss: 0.00002150
Iteration 39/1000 | Loss: 0.00002149
Iteration 40/1000 | Loss: 0.00002149
Iteration 41/1000 | Loss: 0.00002149
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002149
Iteration 46/1000 | Loss: 0.00002149
Iteration 47/1000 | Loss: 0.00002149
Iteration 48/1000 | Loss: 0.00002149
Iteration 49/1000 | Loss: 0.00002149
Iteration 50/1000 | Loss: 0.00002149
Iteration 51/1000 | Loss: 0.00002149
Iteration 52/1000 | Loss: 0.00002149
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002149
Iteration 65/1000 | Loss: 0.00002149
Iteration 66/1000 | Loss: 0.00002149
Iteration 67/1000 | Loss: 0.00002149
Iteration 68/1000 | Loss: 0.00002149
Iteration 69/1000 | Loss: 0.00002149
Iteration 70/1000 | Loss: 0.00002149
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002149
Iteration 76/1000 | Loss: 0.00002149
Iteration 77/1000 | Loss: 0.00002149
Iteration 78/1000 | Loss: 0.00002149
Iteration 79/1000 | Loss: 0.00002149
Iteration 80/1000 | Loss: 0.00002149
Iteration 81/1000 | Loss: 0.00002149
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002149
Iteration 84/1000 | Loss: 0.00002149
Iteration 85/1000 | Loss: 0.00002149
Iteration 86/1000 | Loss: 0.00002149
Iteration 87/1000 | Loss: 0.00002149
Iteration 88/1000 | Loss: 0.00002149
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.1492387531907298e-05, 2.1492387531907298e-05, 2.1492387531907298e-05, 2.1492387531907298e-05, 2.1492387531907298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1492387531907298e-05

Optimization complete. Final v2v error: 4.008789539337158 mm

Highest mean error: 4.431338787078857 mm for frame 142

Lowest mean error: 3.184497117996216 mm for frame 0

Saving results

Total time: 60.75955867767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615437
Iteration 2/25 | Loss: 0.00144212
Iteration 3/25 | Loss: 0.00107867
Iteration 4/25 | Loss: 0.00100918
Iteration 5/25 | Loss: 0.00099332
Iteration 6/25 | Loss: 0.00098740
Iteration 7/25 | Loss: 0.00098069
Iteration 8/25 | Loss: 0.00096743
Iteration 9/25 | Loss: 0.00096697
Iteration 10/25 | Loss: 0.00097996
Iteration 11/25 | Loss: 0.00095325
Iteration 12/25 | Loss: 0.00095492
Iteration 13/25 | Loss: 0.00095068
Iteration 14/25 | Loss: 0.00095022
Iteration 15/25 | Loss: 0.00094988
Iteration 16/25 | Loss: 0.00094969
Iteration 17/25 | Loss: 0.00094961
Iteration 18/25 | Loss: 0.00094961
Iteration 19/25 | Loss: 0.00094961
Iteration 20/25 | Loss: 0.00094968
Iteration 21/25 | Loss: 0.00094967
Iteration 22/25 | Loss: 0.00094965
Iteration 23/25 | Loss: 0.00094959
Iteration 24/25 | Loss: 0.00094958
Iteration 25/25 | Loss: 0.00094958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45971680
Iteration 2/25 | Loss: 0.00114894
Iteration 3/25 | Loss: 0.00112012
Iteration 4/25 | Loss: 0.00112011
Iteration 5/25 | Loss: 0.00112011
Iteration 6/25 | Loss: 0.00112011
Iteration 7/25 | Loss: 0.00112011
Iteration 8/25 | Loss: 0.00112011
Iteration 9/25 | Loss: 0.00112011
Iteration 10/25 | Loss: 0.00112011
Iteration 11/25 | Loss: 0.00112011
Iteration 12/25 | Loss: 0.00112011
Iteration 13/25 | Loss: 0.00112011
Iteration 14/25 | Loss: 0.00112011
Iteration 15/25 | Loss: 0.00112011
Iteration 16/25 | Loss: 0.00112011
Iteration 17/25 | Loss: 0.00112011
Iteration 18/25 | Loss: 0.00112011
Iteration 19/25 | Loss: 0.00112011
Iteration 20/25 | Loss: 0.00112011
Iteration 21/25 | Loss: 0.00112011
Iteration 22/25 | Loss: 0.00112011
Iteration 23/25 | Loss: 0.00112011
Iteration 24/25 | Loss: 0.00112011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011201093439012766, 0.0011201093439012766, 0.0011201093439012766, 0.0011201093439012766, 0.0011201093439012766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011201093439012766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112011
Iteration 2/1000 | Loss: 0.00007899
Iteration 3/1000 | Loss: 0.00004619
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00005192
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002300
Iteration 8/1000 | Loss: 0.00002237
Iteration 9/1000 | Loss: 0.00002188
Iteration 10/1000 | Loss: 0.00002145
Iteration 11/1000 | Loss: 0.00002600
Iteration 12/1000 | Loss: 0.00036225
Iteration 13/1000 | Loss: 0.00050286
Iteration 14/1000 | Loss: 0.00004547
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00003106
Iteration 17/1000 | Loss: 0.00002048
Iteration 18/1000 | Loss: 0.00003892
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00002850
Iteration 21/1000 | Loss: 0.00002888
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001908
Iteration 25/1000 | Loss: 0.00002332
Iteration 26/1000 | Loss: 0.00002332
Iteration 27/1000 | Loss: 0.00003467
Iteration 28/1000 | Loss: 0.00001899
Iteration 29/1000 | Loss: 0.00001877
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001868
Iteration 41/1000 | Loss: 0.00001868
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00003612
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001848
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00001848
Iteration 58/1000 | Loss: 0.00001848
Iteration 59/1000 | Loss: 0.00001848
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001847
Iteration 64/1000 | Loss: 0.00001847
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001847
Iteration 67/1000 | Loss: 0.00001847
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001846
Iteration 75/1000 | Loss: 0.00001846
Iteration 76/1000 | Loss: 0.00001846
Iteration 77/1000 | Loss: 0.00001846
Iteration 78/1000 | Loss: 0.00001846
Iteration 79/1000 | Loss: 0.00001846
Iteration 80/1000 | Loss: 0.00001846
Iteration 81/1000 | Loss: 0.00001845
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001845
Iteration 86/1000 | Loss: 0.00001845
Iteration 87/1000 | Loss: 0.00001845
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001844
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001844
Iteration 99/1000 | Loss: 0.00001844
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001844
Iteration 104/1000 | Loss: 0.00001844
Iteration 105/1000 | Loss: 0.00001844
Iteration 106/1000 | Loss: 0.00001844
Iteration 107/1000 | Loss: 0.00001844
Iteration 108/1000 | Loss: 0.00001844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8440752683090977e-05, 1.8440752683090977e-05, 1.8440752683090977e-05, 1.8440752683090977e-05, 1.8440752683090977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8440752683090977e-05

Optimization complete. Final v2v error: 3.3602428436279297 mm

Highest mean error: 19.210641860961914 mm for frame 191

Lowest mean error: 2.916386842727661 mm for frame 6

Saving results

Total time: 90.99575185775757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002635
Iteration 2/25 | Loss: 0.00346282
Iteration 3/25 | Loss: 0.00212864
Iteration 4/25 | Loss: 0.00184672
Iteration 5/25 | Loss: 0.00165648
Iteration 6/25 | Loss: 0.00154323
Iteration 7/25 | Loss: 0.00153847
Iteration 8/25 | Loss: 0.00153121
Iteration 9/25 | Loss: 0.00141330
Iteration 10/25 | Loss: 0.00133762
Iteration 11/25 | Loss: 0.00128818
Iteration 12/25 | Loss: 0.00125392
Iteration 13/25 | Loss: 0.00123825
Iteration 14/25 | Loss: 0.00121571
Iteration 15/25 | Loss: 0.00120933
Iteration 16/25 | Loss: 0.00120339
Iteration 17/25 | Loss: 0.00118252
Iteration 18/25 | Loss: 0.00117719
Iteration 19/25 | Loss: 0.00116659
Iteration 20/25 | Loss: 0.00116561
Iteration 21/25 | Loss: 0.00116791
Iteration 22/25 | Loss: 0.00116274
Iteration 23/25 | Loss: 0.00116446
Iteration 24/25 | Loss: 0.00115984
Iteration 25/25 | Loss: 0.00115604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24409771
Iteration 2/25 | Loss: 0.00341965
Iteration 3/25 | Loss: 0.00321959
Iteration 4/25 | Loss: 0.00321959
Iteration 5/25 | Loss: 0.00321959
Iteration 6/25 | Loss: 0.00321959
Iteration 7/25 | Loss: 0.00321959
Iteration 8/25 | Loss: 0.00321959
Iteration 9/25 | Loss: 0.00321959
Iteration 10/25 | Loss: 0.00321959
Iteration 11/25 | Loss: 0.00321959
Iteration 12/25 | Loss: 0.00321959
Iteration 13/25 | Loss: 0.00321959
Iteration 14/25 | Loss: 0.00321959
Iteration 15/25 | Loss: 0.00321959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0032195851672440767, 0.0032195851672440767, 0.0032195851672440767, 0.0032195851672440767, 0.0032195851672440767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032195851672440767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321959
Iteration 2/1000 | Loss: 0.00160740
Iteration 3/1000 | Loss: 0.00072612
Iteration 4/1000 | Loss: 0.00046442
Iteration 5/1000 | Loss: 0.00029161
Iteration 6/1000 | Loss: 0.00021522
Iteration 7/1000 | Loss: 0.00066113
Iteration 8/1000 | Loss: 0.00025314
Iteration 9/1000 | Loss: 0.00032952
Iteration 10/1000 | Loss: 0.00033378
Iteration 11/1000 | Loss: 0.00016226
Iteration 12/1000 | Loss: 0.00050750
Iteration 13/1000 | Loss: 0.00145720
Iteration 14/1000 | Loss: 0.00078375
Iteration 15/1000 | Loss: 0.00041415
Iteration 16/1000 | Loss: 0.00058535
Iteration 17/1000 | Loss: 0.00032290
Iteration 18/1000 | Loss: 0.00023057
Iteration 19/1000 | Loss: 0.00045405
Iteration 20/1000 | Loss: 0.00014350
Iteration 21/1000 | Loss: 0.00009432
Iteration 22/1000 | Loss: 0.00022890
Iteration 23/1000 | Loss: 0.00007314
Iteration 24/1000 | Loss: 0.00023983
Iteration 25/1000 | Loss: 0.00020329
Iteration 26/1000 | Loss: 0.00014073
Iteration 27/1000 | Loss: 0.00025676
Iteration 28/1000 | Loss: 0.00024950
Iteration 29/1000 | Loss: 0.00004481
Iteration 30/1000 | Loss: 0.00015550
Iteration 31/1000 | Loss: 0.00018762
Iteration 32/1000 | Loss: 0.00005416
Iteration 33/1000 | Loss: 0.00021978
Iteration 34/1000 | Loss: 0.00013797
Iteration 35/1000 | Loss: 0.00004214
Iteration 36/1000 | Loss: 0.00006975
Iteration 37/1000 | Loss: 0.00016788
Iteration 38/1000 | Loss: 0.00005928
Iteration 39/1000 | Loss: 0.00019218
Iteration 40/1000 | Loss: 0.00003156
Iteration 41/1000 | Loss: 0.00003807
Iteration 42/1000 | Loss: 0.00002950
Iteration 43/1000 | Loss: 0.00003697
Iteration 44/1000 | Loss: 0.00002960
Iteration 45/1000 | Loss: 0.00003129
Iteration 46/1000 | Loss: 0.00004916
Iteration 47/1000 | Loss: 0.00002973
Iteration 48/1000 | Loss: 0.00003331
Iteration 49/1000 | Loss: 0.00003193
Iteration 50/1000 | Loss: 0.00005775
Iteration 51/1000 | Loss: 0.00003495
Iteration 52/1000 | Loss: 0.00002718
Iteration 53/1000 | Loss: 0.00002601
Iteration 54/1000 | Loss: 0.00002600
Iteration 55/1000 | Loss: 0.00003526
Iteration 56/1000 | Loss: 0.00005490
Iteration 57/1000 | Loss: 0.00002586
Iteration 58/1000 | Loss: 0.00002911
Iteration 59/1000 | Loss: 0.00002579
Iteration 60/1000 | Loss: 0.00002579
Iteration 61/1000 | Loss: 0.00002579
Iteration 62/1000 | Loss: 0.00002579
Iteration 63/1000 | Loss: 0.00002579
Iteration 64/1000 | Loss: 0.00002579
Iteration 65/1000 | Loss: 0.00002578
Iteration 66/1000 | Loss: 0.00002577
Iteration 67/1000 | Loss: 0.00002577
Iteration 68/1000 | Loss: 0.00002576
Iteration 69/1000 | Loss: 0.00003315
Iteration 70/1000 | Loss: 0.00002773
Iteration 71/1000 | Loss: 0.00002609
Iteration 72/1000 | Loss: 0.00002570
Iteration 73/1000 | Loss: 0.00002570
Iteration 74/1000 | Loss: 0.00002570
Iteration 75/1000 | Loss: 0.00002570
Iteration 76/1000 | Loss: 0.00002570
Iteration 77/1000 | Loss: 0.00002570
Iteration 78/1000 | Loss: 0.00002570
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00003162
Iteration 81/1000 | Loss: 0.00002567
Iteration 82/1000 | Loss: 0.00002566
Iteration 83/1000 | Loss: 0.00002566
Iteration 84/1000 | Loss: 0.00002565
Iteration 85/1000 | Loss: 0.00002565
Iteration 86/1000 | Loss: 0.00002564
Iteration 87/1000 | Loss: 0.00002625
Iteration 88/1000 | Loss: 0.00002563
Iteration 89/1000 | Loss: 0.00002563
Iteration 90/1000 | Loss: 0.00002563
Iteration 91/1000 | Loss: 0.00002562
Iteration 92/1000 | Loss: 0.00002562
Iteration 93/1000 | Loss: 0.00002562
Iteration 94/1000 | Loss: 0.00002562
Iteration 95/1000 | Loss: 0.00002562
Iteration 96/1000 | Loss: 0.00002561
Iteration 97/1000 | Loss: 0.00002561
Iteration 98/1000 | Loss: 0.00002560
Iteration 99/1000 | Loss: 0.00002560
Iteration 100/1000 | Loss: 0.00002560
Iteration 101/1000 | Loss: 0.00002559
Iteration 102/1000 | Loss: 0.00002559
Iteration 103/1000 | Loss: 0.00002559
Iteration 104/1000 | Loss: 0.00003249
Iteration 105/1000 | Loss: 0.00004103
Iteration 106/1000 | Loss: 0.00003512
Iteration 107/1000 | Loss: 0.00002564
Iteration 108/1000 | Loss: 0.00003331
Iteration 109/1000 | Loss: 0.00005052
Iteration 110/1000 | Loss: 0.00003234
Iteration 111/1000 | Loss: 0.00002552
Iteration 112/1000 | Loss: 0.00002552
Iteration 113/1000 | Loss: 0.00002552
Iteration 114/1000 | Loss: 0.00002552
Iteration 115/1000 | Loss: 0.00002552
Iteration 116/1000 | Loss: 0.00002552
Iteration 117/1000 | Loss: 0.00002552
Iteration 118/1000 | Loss: 0.00002552
Iteration 119/1000 | Loss: 0.00002551
Iteration 120/1000 | Loss: 0.00002551
Iteration 121/1000 | Loss: 0.00003123
Iteration 122/1000 | Loss: 0.00027616
Iteration 123/1000 | Loss: 0.00026030
Iteration 124/1000 | Loss: 0.00007443
Iteration 125/1000 | Loss: 0.00007053
Iteration 126/1000 | Loss: 0.00002546
Iteration 127/1000 | Loss: 0.00002453
Iteration 128/1000 | Loss: 0.00004754
Iteration 129/1000 | Loss: 0.00014054
Iteration 130/1000 | Loss: 0.00003031
Iteration 131/1000 | Loss: 0.00002698
Iteration 132/1000 | Loss: 0.00002349
Iteration 133/1000 | Loss: 0.00004656
Iteration 134/1000 | Loss: 0.00002789
Iteration 135/1000 | Loss: 0.00002333
Iteration 136/1000 | Loss: 0.00002332
Iteration 137/1000 | Loss: 0.00002332
Iteration 138/1000 | Loss: 0.00002331
Iteration 139/1000 | Loss: 0.00002330
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00003288
Iteration 142/1000 | Loss: 0.00002330
Iteration 143/1000 | Loss: 0.00002329
Iteration 144/1000 | Loss: 0.00002329
Iteration 145/1000 | Loss: 0.00002328
Iteration 146/1000 | Loss: 0.00002327
Iteration 147/1000 | Loss: 0.00002484
Iteration 148/1000 | Loss: 0.00002326
Iteration 149/1000 | Loss: 0.00002326
Iteration 150/1000 | Loss: 0.00002326
Iteration 151/1000 | Loss: 0.00002326
Iteration 152/1000 | Loss: 0.00002325
Iteration 153/1000 | Loss: 0.00002325
Iteration 154/1000 | Loss: 0.00002325
Iteration 155/1000 | Loss: 0.00002325
Iteration 156/1000 | Loss: 0.00002325
Iteration 157/1000 | Loss: 0.00002325
Iteration 158/1000 | Loss: 0.00002325
Iteration 159/1000 | Loss: 0.00002325
Iteration 160/1000 | Loss: 0.00002664
Iteration 161/1000 | Loss: 0.00002353
Iteration 162/1000 | Loss: 0.00003129
Iteration 163/1000 | Loss: 0.00004315
Iteration 164/1000 | Loss: 0.00002716
Iteration 165/1000 | Loss: 0.00002321
Iteration 166/1000 | Loss: 0.00002320
Iteration 167/1000 | Loss: 0.00002320
Iteration 168/1000 | Loss: 0.00002320
Iteration 169/1000 | Loss: 0.00002320
Iteration 170/1000 | Loss: 0.00002320
Iteration 171/1000 | Loss: 0.00002320
Iteration 172/1000 | Loss: 0.00002320
Iteration 173/1000 | Loss: 0.00002320
Iteration 174/1000 | Loss: 0.00002320
Iteration 175/1000 | Loss: 0.00002320
Iteration 176/1000 | Loss: 0.00002649
Iteration 177/1000 | Loss: 0.00005285
Iteration 178/1000 | Loss: 0.00002677
Iteration 179/1000 | Loss: 0.00002454
Iteration 180/1000 | Loss: 0.00002644
Iteration 181/1000 | Loss: 0.00004825
Iteration 182/1000 | Loss: 0.00002353
Iteration 183/1000 | Loss: 0.00002509
Iteration 184/1000 | Loss: 0.00002705
Iteration 185/1000 | Loss: 0.00002733
Iteration 186/1000 | Loss: 0.00002926
Iteration 187/1000 | Loss: 0.00003144
Iteration 188/1000 | Loss: 0.00002668
Iteration 189/1000 | Loss: 0.00002321
Iteration 190/1000 | Loss: 0.00002499
Iteration 191/1000 | Loss: 0.00002735
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002769
Iteration 194/1000 | Loss: 0.00002403
Iteration 195/1000 | Loss: 0.00002398
Iteration 196/1000 | Loss: 0.00002543
Iteration 197/1000 | Loss: 0.00002360
Iteration 198/1000 | Loss: 0.00002338
Iteration 199/1000 | Loss: 0.00002450
Iteration 200/1000 | Loss: 0.00002330
Iteration 201/1000 | Loss: 0.00002499
Iteration 202/1000 | Loss: 0.00002413
Iteration 203/1000 | Loss: 0.00002680
Iteration 204/1000 | Loss: 0.00002324
Iteration 205/1000 | Loss: 0.00002534
Iteration 206/1000 | Loss: 0.00002663
Iteration 207/1000 | Loss: 0.00002317
Iteration 208/1000 | Loss: 0.00002406
Iteration 209/1000 | Loss: 0.00002334
Iteration 210/1000 | Loss: 0.00002315
Iteration 211/1000 | Loss: 0.00002315
Iteration 212/1000 | Loss: 0.00002315
Iteration 213/1000 | Loss: 0.00002315
Iteration 214/1000 | Loss: 0.00002315
Iteration 215/1000 | Loss: 0.00002315
Iteration 216/1000 | Loss: 0.00002315
Iteration 217/1000 | Loss: 0.00002315
Iteration 218/1000 | Loss: 0.00002340
Iteration 219/1000 | Loss: 0.00002322
Iteration 220/1000 | Loss: 0.00002318
Iteration 221/1000 | Loss: 0.00002314
Iteration 222/1000 | Loss: 0.00002314
Iteration 223/1000 | Loss: 0.00002314
Iteration 224/1000 | Loss: 0.00002314
Iteration 225/1000 | Loss: 0.00002314
Iteration 226/1000 | Loss: 0.00002314
Iteration 227/1000 | Loss: 0.00002314
Iteration 228/1000 | Loss: 0.00002314
Iteration 229/1000 | Loss: 0.00002314
Iteration 230/1000 | Loss: 0.00002314
Iteration 231/1000 | Loss: 0.00002314
Iteration 232/1000 | Loss: 0.00002314
Iteration 233/1000 | Loss: 0.00002314
Iteration 234/1000 | Loss: 0.00002314
Iteration 235/1000 | Loss: 0.00002314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.3144782971940003e-05, 2.3144782971940003e-05, 2.3144782971940003e-05, 2.3144782971940003e-05, 2.3144782971940003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3144782971940003e-05

Optimization complete. Final v2v error: 3.1851189136505127 mm

Highest mean error: 21.48383903503418 mm for frame 77

Lowest mean error: 2.5808236598968506 mm for frame 0

Saving results

Total time: 231.75734400749207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2796/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2796/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818326
Iteration 2/25 | Loss: 0.00110155
Iteration 3/25 | Loss: 0.00096715
Iteration 4/25 | Loss: 0.00094918
Iteration 5/25 | Loss: 0.00094525
Iteration 6/25 | Loss: 0.00094455
Iteration 7/25 | Loss: 0.00094455
Iteration 8/25 | Loss: 0.00094455
Iteration 9/25 | Loss: 0.00094455
Iteration 10/25 | Loss: 0.00094455
Iteration 11/25 | Loss: 0.00094455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009445503819733858, 0.0009445503819733858, 0.0009445503819733858, 0.0009445503819733858, 0.0009445503819733858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009445503819733858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24749720
Iteration 2/25 | Loss: 0.00120781
Iteration 3/25 | Loss: 0.00120781
Iteration 4/25 | Loss: 0.00120780
Iteration 5/25 | Loss: 0.00120780
Iteration 6/25 | Loss: 0.00120780
Iteration 7/25 | Loss: 0.00120780
Iteration 8/25 | Loss: 0.00120780
Iteration 9/25 | Loss: 0.00120780
Iteration 10/25 | Loss: 0.00120780
Iteration 11/25 | Loss: 0.00120780
Iteration 12/25 | Loss: 0.00120780
Iteration 13/25 | Loss: 0.00120780
Iteration 14/25 | Loss: 0.00120780
Iteration 15/25 | Loss: 0.00120780
Iteration 16/25 | Loss: 0.00120780
Iteration 17/25 | Loss: 0.00120780
Iteration 18/25 | Loss: 0.00120780
Iteration 19/25 | Loss: 0.00120780
Iteration 20/25 | Loss: 0.00120780
Iteration 21/25 | Loss: 0.00120780
Iteration 22/25 | Loss: 0.00120780
Iteration 23/25 | Loss: 0.00120780
Iteration 24/25 | Loss: 0.00120780
Iteration 25/25 | Loss: 0.00120780

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120780
Iteration 2/1000 | Loss: 0.00004050
Iteration 3/1000 | Loss: 0.00002276
Iteration 4/1000 | Loss: 0.00001791
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001419
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001320
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001308
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001299
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001291
Iteration 38/1000 | Loss: 0.00001290
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001286
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001284
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001277
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001274
Iteration 51/1000 | Loss: 0.00001274
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001272
Iteration 59/1000 | Loss: 0.00001271
Iteration 60/1000 | Loss: 0.00001271
Iteration 61/1000 | Loss: 0.00001270
Iteration 62/1000 | Loss: 0.00001270
Iteration 63/1000 | Loss: 0.00001270
Iteration 64/1000 | Loss: 0.00001270
Iteration 65/1000 | Loss: 0.00001270
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001266
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001265
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001265
Iteration 114/1000 | Loss: 0.00001265
Iteration 115/1000 | Loss: 0.00001265
Iteration 116/1000 | Loss: 0.00001265
Iteration 117/1000 | Loss: 0.00001265
Iteration 118/1000 | Loss: 0.00001265
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2649555173993576e-05, 1.2649555173993576e-05, 1.2649555173993576e-05, 1.2649555173993576e-05, 1.2649555173993576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2649555173993576e-05

Optimization complete. Final v2v error: 3.0290281772613525 mm

Highest mean error: 3.2049596309661865 mm for frame 94

Lowest mean error: 2.776822090148926 mm for frame 230

Saving results

Total time: 39.794642210006714
